{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 0.005\n",
    "label = \"1D_SODE_swish\"\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((layers[1],len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.890845 Test MSE 383.40289852526513 Test RE 0.9981372842176024\n",
      "1 Train Loss 4.890722 Test MSE 383.3609223797081 Test RE 0.9980826431299479\n",
      "2 Train Loss 4.8907127 Test MSE 383.3565934788127 Test RE 0.9980770079532875\n",
      "3 Train Loss 4.890395 Test MSE 383.2978703159622 Test RE 0.9980005615267145\n",
      "4 Train Loss 4.5180387 Test MSE 386.61011945587416 Test RE 1.0023033715553498\n",
      "5 Train Loss 4.447725 Test MSE 386.5271541848704 Test RE 1.0021958202673544\n",
      "6 Train Loss 3.310747 Test MSE 386.49231742063677 Test RE 1.0021506564983584\n",
      "7 Train Loss 2.5245805 Test MSE 381.0077428649025 Test RE 0.9950146688057215\n",
      "8 Train Loss 2.5104072 Test MSE 381.22769040060194 Test RE 0.9953018276011896\n",
      "9 Train Loss 2.3922408 Test MSE 382.3464295098612 Test RE 0.9967611489020299\n",
      "10 Train Loss 2.3815036 Test MSE 383.0975382789841 Test RE 0.9977397231142923\n",
      "11 Train Loss 2.3809688 Test MSE 383.2447952000481 Test RE 0.9979314627482396\n",
      "12 Train Loss 2.3809605 Test MSE 383.2542067220947 Test RE 0.9979437160083225\n",
      "13 Train Loss 2.3809583 Test MSE 383.2561694306732 Test RE 0.997946271322981\n",
      "14 Train Loss 2.3809578 Test MSE 383.25743298599315 Test RE 0.9979479163836237\n",
      "15 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "16 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "17 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "18 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "19 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "20 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "21 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "22 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "23 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "24 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "25 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "26 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "27 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "28 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "29 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "30 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "31 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "32 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "33 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "34 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "35 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "36 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "37 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "38 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "39 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "40 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "41 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "42 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "43 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "44 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "45 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "46 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "47 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "48 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "49 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "50 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "51 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "52 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "53 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "54 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "55 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "56 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "57 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "58 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "59 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "60 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "61 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "62 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "63 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "64 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "65 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "66 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "67 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "68 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "69 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "70 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "71 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "72 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "73 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "74 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "75 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "76 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "77 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "78 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "79 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "80 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "81 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "82 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "83 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "84 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "85 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "86 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "87 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "88 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "89 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "90 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "91 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "92 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "93 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "94 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "95 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "96 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "97 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "99 Train Loss 2.3809536 Test MSE 383.2641018016369 Test RE 0.9979565986701934\n",
      "Training time: 19.89\n",
      "Training time: 19.89\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 4.8909874 Test MSE 383.4216693875317 Test RE 0.9981617176116149\n",
      "1 Train Loss 4.8907213 Test MSE 383.35690971553265 Test RE 0.9980774196177007\n",
      "2 Train Loss 4.89041 Test MSE 383.2951200598428 Test RE 0.9979969810710329\n",
      "3 Train Loss 4.532765 Test MSE 386.36021389475724 Test RE 1.001979373728024\n",
      "4 Train Loss 4.3340178 Test MSE 386.6535597474495 Test RE 1.002359680386281\n",
      "5 Train Loss 2.5366719 Test MSE 383.66366757204906 Test RE 0.9984766648883022\n",
      "6 Train Loss 2.440178 Test MSE 381.5747332404265 Test RE 0.9957547508900989\n",
      "7 Train Loss 2.4231672 Test MSE 381.8503133276535 Test RE 0.9961142619230716\n",
      "8 Train Loss 2.3822558 Test MSE 382.9542640524892 Test RE 0.9975531338768895\n",
      "9 Train Loss 2.381211 Test MSE 383.15579728400405 Test RE 0.9978155851441809\n",
      "10 Train Loss 2.3811994 Test MSE 383.17218493285867 Test RE 0.9978369233004971\n",
      "11 Train Loss 2.3811955 Test MSE 383.1787259152759 Test RE 0.9978454401059611\n",
      "12 Train Loss 2.3811927 Test MSE 383.18465604941315 Test RE 0.9978531614824543\n",
      "13 Train Loss 2.3811917 Test MSE 383.18564050254224 Test RE 0.997854443291395\n",
      "14 Train Loss 2.3811884 Test MSE 383.19242723401845 Test RE 0.997863279922335\n",
      "15 Train Loss 2.3811824 Test MSE 383.2022642724096 Test RE 0.9978760880517491\n",
      "16 Train Loss 2.3811789 Test MSE 383.20824875696627 Test RE 0.997883879956025\n",
      "17 Train Loss 2.3811767 Test MSE 383.2103167243393 Test RE 0.997886572471816\n",
      "18 Train Loss 2.380941 Test MSE 383.31376127015994 Test RE 0.9980212491147102\n",
      "19 Train Loss 2.3800216 Test MSE 383.2512160082637 Test RE 0.997939822287335\n",
      "20 Train Loss 2.3798492 Test MSE 383.245507807104 Test RE 0.9979323905268814\n",
      "21 Train Loss 2.3797088 Test MSE 383.3201929969668 Test RE 0.998029622115818\n",
      "22 Train Loss 2.3796322 Test MSE 383.3938424957059 Test RE 0.9981254960775519\n",
      "23 Train Loss 2.3796263 Test MSE 383.40456574728944 Test RE 0.9981394544081007\n",
      "24 Train Loss 2.3796208 Test MSE 383.4112296007543 Test RE 0.9981481285694881\n",
      "25 Train Loss 2.3796124 Test MSE 383.4213078089464 Test RE 0.9981612469626976\n",
      "26 Train Loss 2.3796048 Test MSE 383.42713183113995 Test RE 0.998168827776624\n",
      "27 Train Loss 2.3791697 Test MSE 383.37230495690454 Test RE 0.9980974603266709\n",
      "28 Train Loss 2.357133 Test MSE 378.5030867724464 Test RE 0.9917387789422464\n",
      "29 Train Loss 2.3001137 Test MSE 363.3085100523053 Test RE 0.9716287747216727\n",
      "30 Train Loss 2.1666589 Test MSE 342.3136528384356 Test RE 0.9431368047099986\n",
      "31 Train Loss 2.0971262 Test MSE 330.560548508823 Test RE 0.9268044124505195\n",
      "32 Train Loss 2.0059133 Test MSE 315.7068640499479 Test RE 0.9057421833351075\n",
      "33 Train Loss 1.7621946 Test MSE 257.2523523828279 Test RE 0.8176025925544255\n",
      "34 Train Loss 1.5659094 Test MSE 212.02720954985273 Test RE 0.7422639721779565\n",
      "35 Train Loss 1.5417327 Test MSE 194.48327698398577 Test RE 0.7108921430737521\n",
      "36 Train Loss 1.4918455 Test MSE 196.5738540214805 Test RE 0.7147027592541431\n",
      "37 Train Loss 1.4116559 Test MSE 187.5697877547812 Test RE 0.6981424182995\n",
      "38 Train Loss 1.3681475 Test MSE 180.11869657074894 Test RE 0.68413526939738\n",
      "39 Train Loss 1.3308802 Test MSE 167.91363365080437 Test RE 0.6605497955240226\n",
      "40 Train Loss 1.2821554 Test MSE 152.07640329400252 Test RE 0.6286276781243514\n",
      "41 Train Loss 1.1663971 Test MSE 113.51238728512288 Test RE 0.5431055129537481\n",
      "42 Train Loss 1.103933 Test MSE 84.66621815104608 Test RE 0.4690484046464233\n",
      "43 Train Loss 1.0701101 Test MSE 67.51782465265036 Test RE 0.41886281124320623\n",
      "44 Train Loss 1.0342417 Test MSE 50.275856805481475 Test RE 0.36144508444601436\n",
      "45 Train Loss 0.98429847 Test MSE 32.02219488990208 Test RE 0.2884616814544022\n",
      "46 Train Loss 0.8967593 Test MSE 14.11231353603982 Test RE 0.19149687486062364\n",
      "47 Train Loss 0.75541383 Test MSE 1.6616469491350037 Test RE 0.06571007453077514\n",
      "48 Train Loss 0.684674 Test MSE 0.16424613322644313 Test RE 0.0206590410930886\n",
      "49 Train Loss 0.6810422 Test MSE 0.38854826779343976 Test RE 0.031774964286794306\n",
      "50 Train Loss 0.6808756 Test MSE 0.41492695108482414 Test RE 0.03283586072700524\n",
      "51 Train Loss 0.68006283 Test MSE 0.5332028532477093 Test RE 0.037222780806229816\n",
      "52 Train Loss 0.66464454 Test MSE 1.0985410396505757 Test RE 0.05342822245781155\n",
      "53 Train Loss 0.6420945 Test MSE 0.6489018410903558 Test RE 0.04106313337620232\n",
      "54 Train Loss 0.63276064 Test MSE 1.05231780433409 Test RE 0.05229209502525233\n",
      "55 Train Loss 0.5951717 Test MSE 1.8648073118006088 Test RE 0.06961127112723059\n",
      "56 Train Loss 0.56986284 Test MSE 1.619837180978211 Test RE 0.06487812112661129\n",
      "57 Train Loss 0.53555936 Test MSE 2.680525543303737 Test RE 0.08345888101567683\n",
      "58 Train Loss 0.5271208 Test MSE 3.5663066448766414 Test RE 0.09626577774683487\n",
      "59 Train Loss 0.51352423 Test MSE 4.019171566948526 Test RE 0.102195284075372\n",
      "60 Train Loss 0.49949566 Test MSE 3.8996753384964156 Test RE 0.10066460842954589\n",
      "61 Train Loss 0.48116523 Test MSE 3.450155096983181 Test RE 0.09468515443072496\n",
      "62 Train Loss 0.46277645 Test MSE 2.7377324942483323 Test RE 0.08434475632147995\n",
      "63 Train Loss 0.44548208 Test MSE 1.8072671912985154 Test RE 0.06852890064374978\n",
      "64 Train Loss 0.42505616 Test MSE 1.5085991958062133 Test RE 0.06261083827794517\n",
      "65 Train Loss 0.40298325 Test MSE 1.740416458451993 Test RE 0.06724951759389189\n",
      "66 Train Loss 0.3748901 Test MSE 0.7448068749073227 Test RE 0.04399308531373035\n",
      "67 Train Loss 0.35331047 Test MSE 0.025790247826063935 Test RE 0.00818635059688814\n",
      "68 Train Loss 0.31687468 Test MSE 0.012614218299020614 Test RE 0.0057252275677060205\n",
      "69 Train Loss 0.30578527 Test MSE 0.05520548952445381 Test RE 0.011977156247383118\n",
      "70 Train Loss 0.2879624 Test MSE 0.8553173828184211 Test RE 0.047143978684169\n",
      "71 Train Loss 0.279976 Test MSE 2.164883048097767 Test RE 0.075003201642256\n",
      "72 Train Loss 0.27779502 Test MSE 2.75217642025223 Test RE 0.08456695969383057\n",
      "73 Train Loss 0.27146772 Test MSE 2.8507065602009685 Test RE 0.08606743101357497\n",
      "74 Train Loss 0.25798002 Test MSE 2.3266726048600463 Test RE 0.07775533927397718\n",
      "75 Train Loss 0.24582228 Test MSE 3.0567569250491307 Test RE 0.08912366446301367\n",
      "76 Train Loss 0.22845729 Test MSE 3.3651477409374517 Test RE 0.09351141968690903\n",
      "77 Train Loss 0.2130963 Test MSE 2.826291144483038 Test RE 0.08569806804425556\n",
      "78 Train Loss 0.19479835 Test MSE 2.258156211150668 Test RE 0.07660190555510485\n",
      "79 Train Loss 0.17946097 Test MSE 1.5026539431691937 Test RE 0.06248734467038076\n",
      "80 Train Loss 0.16851895 Test MSE 1.0035275481930641 Test RE 0.05106545807206362\n",
      "81 Train Loss 0.16096616 Test MSE 0.6560385444099418 Test RE 0.041288324628210755\n",
      "82 Train Loss 0.15342656 Test MSE 0.3987313783737562 Test RE 0.03218865199372337\n",
      "83 Train Loss 0.14773701 Test MSE 0.3376246216614566 Test RE 0.029619630590532724\n",
      "84 Train Loss 0.14167288 Test MSE 0.4016026569917727 Test RE 0.03230433990421964\n",
      "85 Train Loss 0.13462086 Test MSE 0.7753053955001993 Test RE 0.04488476809729373\n",
      "86 Train Loss 0.12998217 Test MSE 1.3822072304353903 Test RE 0.05993067411333118\n",
      "87 Train Loss 0.1281541 Test MSE 2.3775748077192973 Test RE 0.07860129072770357\n",
      "88 Train Loss 0.12769237 Test MSE 2.8054367707973262 Test RE 0.08538131219155302\n",
      "89 Train Loss 0.11302756 Test MSE 2.7972730457370862 Test RE 0.08525699333549096\n",
      "90 Train Loss 0.04956964 Test MSE 0.3748601785955975 Test RE 0.03121024920585728\n",
      "91 Train Loss 0.026770467 Test MSE 0.1253355614458116 Test RE 0.01804678057555525\n",
      "92 Train Loss 0.011120692 Test MSE 0.034590781956074255 Test RE 0.009480751695843411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 Train Loss 0.010255459 Test MSE 0.003047264005367677 Test RE 0.0028139580801271567\n",
      "94 Train Loss 0.010247922 Test MSE 0.0025417067264031016 Test RE 0.002569953714522452\n",
      "95 Train Loss 0.010243212 Test MSE 0.002199275859903758 Test RE 0.002390575345178924\n",
      "96 Train Loss 0.010238941 Test MSE 0.002052469367461802 Test RE 0.0023094093755074876\n",
      "97 Train Loss 0.010211411 Test MSE 0.0009732297755588677 Test RE 0.0015902678399845375\n",
      "98 Train Loss 0.010077819 Test MSE 0.00032580339029196254 Test RE 0.000920111337692056\n",
      "99 Train Loss 0.008404539 Test MSE 0.0006695298929869886 Test RE 0.0013190084219442525\n",
      "Training time: 112.68\n",
      "Training time: 112.68\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 4.892478 Test MSE 383.40139712865926 Test RE 0.9981353298747901\n",
      "1 Train Loss 4.892334 Test MSE 383.35671947465335 Test RE 0.9980771719696936\n",
      "2 Train Loss 4.8923283 Test MSE 383.35395083266326 Test RE 0.9980735678547575\n",
      "3 Train Loss 4.8923225 Test MSE 383.3519139280723 Test RE 0.998070916279981\n",
      "4 Train Loss 4.892061 Test MSE 383.2815224774416 Test RE 0.9979792786978068\n",
      "5 Train Loss 4.8854628 Test MSE 383.045290845078 Test RE 0.9976716841522512\n",
      "6 Train Loss 3.7391722 Test MSE 378.30850810586844 Test RE 0.9914838325194815\n",
      "7 Train Loss 3.4527135 Test MSE 380.52129962004227 Test RE 0.9943792844049755\n",
      "8 Train Loss 2.558832 Test MSE 386.9341710950392 Test RE 1.002723342479895\n",
      "9 Train Loss 2.4318514 Test MSE 384.38637935874345 Test RE 0.999416643583686\n",
      "10 Train Loss 2.3811657 Test MSE 383.4311070345707 Test RE 0.998174002050345\n",
      "11 Train Loss 2.3798137 Test MSE 383.2655403906712 Test RE 0.9979584715928929\n",
      "12 Train Loss 2.3795187 Test MSE 383.0894292762689 Test RE 0.9977291635098452\n",
      "13 Train Loss 2.3770666 Test MSE 381.91452818807477 Test RE 0.996198015487049\n",
      "14 Train Loss 2.3736627 Test MSE 381.20751094236897 Test RE 0.9952754851843659\n",
      "15 Train Loss 2.367176 Test MSE 377.6766586409856 Test RE 0.9906555001986168\n",
      "16 Train Loss 2.362188 Test MSE 373.8125498781252 Test RE 0.9855746430130812\n",
      "17 Train Loss 2.3520727 Test MSE 375.0031040506944 Test RE 0.9871428718157869\n",
      "18 Train Loss 2.3329852 Test MSE 374.74142491352353 Test RE 0.9867983949684782\n",
      "19 Train Loss 2.1328175 Test MSE 338.9992766264961 Test RE 0.9385598421769575\n",
      "20 Train Loss 2.0571747 Test MSE 321.9610055993785 Test RE 0.9146695473996769\n",
      "21 Train Loss 2.0007515 Test MSE 311.20599342924066 Test RE 0.8992626560413493\n",
      "22 Train Loss 1.7140082 Test MSE 265.0212955377214 Test RE 0.8298564404410291\n",
      "23 Train Loss 1.1444983 Test MSE 138.65743884711438 Test RE 0.6002527653991433\n",
      "24 Train Loss 0.8450236 Test MSE 93.88807638765111 Test RE 0.49393272758164164\n",
      "25 Train Loss 0.64696705 Test MSE 85.42387434712563 Test RE 0.4711424270915663\n",
      "26 Train Loss 0.5837976 Test MSE 76.08775002660104 Test RE 0.4446516963207496\n",
      "27 Train Loss 0.5337585 Test MSE 69.00949837392822 Test RE 0.423464509279554\n",
      "28 Train Loss 0.47070897 Test MSE 56.52712862952089 Test RE 0.3832578379035148\n",
      "29 Train Loss 0.37430325 Test MSE 30.05766759175613 Test RE 0.27947323441945826\n",
      "30 Train Loss 0.24863921 Test MSE 16.31196078520399 Test RE 0.20588071454797918\n",
      "31 Train Loss 0.114730164 Test MSE 10.141066099934545 Test RE 0.16233209340394908\n",
      "32 Train Loss 0.0871474 Test MSE 2.772916505807709 Test RE 0.08488500508008928\n",
      "33 Train Loss 0.08636805 Test MSE 2.4689803408475997 Test RE 0.08009794952729249\n",
      "34 Train Loss 0.08585811 Test MSE 2.4318080458400995 Test RE 0.07949269630818939\n",
      "35 Train Loss 0.08304409 Test MSE 1.9415568466549977 Test RE 0.07102931688071987\n",
      "36 Train Loss 0.06652006 Test MSE 0.4565681261837357 Test RE 0.03444414249299727\n",
      "37 Train Loss 0.06260504 Test MSE 0.09032950028740312 Test RE 0.015320656883576986\n",
      "38 Train Loss 0.060251884 Test MSE 0.0779133133065724 Test RE 0.01422880472556683\n",
      "39 Train Loss 0.057673544 Test MSE 0.018745371403711488 Test RE 0.006979263693087005\n",
      "40 Train Loss 0.049013626 Test MSE 0.3288416557068958 Test RE 0.02923182941345532\n",
      "41 Train Loss 0.04165002 Test MSE 1.0345404373354423 Test RE 0.05184851448241913\n",
      "42 Train Loss 0.021660754 Test MSE 0.004460707863715505 Test RE 0.0034045872893466523\n",
      "43 Train Loss 0.010080424 Test MSE 0.16248353198003787 Test RE 0.020547891224488645\n",
      "44 Train Loss 0.009506627 Test MSE 0.1108257028572871 Test RE 0.01697003858065296\n",
      "45 Train Loss 0.008467451 Test MSE 0.01421648753815854 Test RE 0.00607797258542519\n",
      "46 Train Loss 0.008285705 Test MSE 0.008856603660492535 Test RE 0.004797292373610802\n",
      "47 Train Loss 0.008095394 Test MSE 0.011772739426777536 Test RE 0.005530970579196017\n",
      "48 Train Loss 0.0071229795 Test MSE 0.01473474338735013 Test RE 0.006187765844917349\n",
      "49 Train Loss 0.004036583 Test MSE 0.006642709353316008 Test RE 0.004154657312935664\n",
      "50 Train Loss 0.0023990143 Test MSE 0.0040812163973907726 Test RE 0.0032565473259886283\n",
      "51 Train Loss 0.0012968831 Test MSE 0.0008992724559692124 Test RE 0.0015286505900600344\n",
      "52 Train Loss 0.001105837 Test MSE 6.700349883153463e-05 Test RE 0.0004172643903757407\n",
      "53 Train Loss 0.0010017091 Test MSE 3.3771556200609115e-05 Test RE 0.000296236193962678\n",
      "54 Train Loss 0.0009940143 Test MSE 2.188685471018238e-05 Test RE 0.00023848126133094473\n",
      "55 Train Loss 0.0009852666 Test MSE 8.49835231277092e-06 Test RE 0.0001486038113653285\n",
      "56 Train Loss 0.0009800934 Test MSE 1.6080357736854427e-06 Test RE 6.464135264288904e-05\n",
      "57 Train Loss 0.00097843 Test MSE 4.727326978224913e-07 Test RE 3.5048578114503244e-05\n",
      "58 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "59 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "60 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "61 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "62 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "63 Train Loss 0.00097578776 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "64 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "65 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "66 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "67 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "68 Train Loss 0.00097578776 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "69 Train Loss 0.00097578776 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "70 Train Loss 0.00097578764 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "71 Train Loss 0.00097578764 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "72 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "73 Train Loss 0.00097578776 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "74 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "75 Train Loss 0.00097578764 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "76 Train Loss 0.00097578764 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "77 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "78 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "79 Train Loss 0.00097578764 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "80 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "81 Train Loss 0.00097578764 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "82 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "83 Train Loss 0.00097578764 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 Train Loss 0.00097578764 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "85 Train Loss 0.00097578776 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "86 Train Loss 0.00097578764 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "87 Train Loss 0.00097578776 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "88 Train Loss 0.00097578764 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "89 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "90 Train Loss 0.00097578776 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "91 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "92 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "93 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "94 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "95 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "96 Train Loss 0.00097578776 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "97 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "98 Train Loss 0.0009757877 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "99 Train Loss 0.00097578764 Test MSE 5.789761871240155e-07 Test RE 3.87876022505774e-05\n",
      "Training time: 67.06\n",
      "Training time: 67.06\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 4.8913717 Test MSE 383.41389062961616 Test RE 0.9981515923391496\n",
      "1 Train Loss 4.8911543 Test MSE 383.3560610046689 Test RE 0.9980763147991932\n",
      "2 Train Loss 4.8907843 Test MSE 383.2863656768765 Test RE 0.9979855839814251\n",
      "3 Train Loss 4.499363 Test MSE 386.88875411058456 Test RE 1.0026644926673305\n",
      "4 Train Loss 3.4132729 Test MSE 380.9857920079926 Test RE 0.9949860056884728\n",
      "5 Train Loss 2.4837513 Test MSE 381.08256781077415 Test RE 0.9951123679537593\n",
      "6 Train Loss 2.415188 Test MSE 381.997081985884 Test RE 0.9963056776467196\n",
      "7 Train Loss 2.3806267 Test MSE 383.2429716360483 Test RE 0.9979290885554557\n",
      "8 Train Loss 2.3797178 Test MSE 383.46369323709024 Test RE 0.9982164164613515\n",
      "9 Train Loss 2.3792787 Test MSE 383.40007917189075 Test RE 0.9981336143093875\n",
      "10 Train Loss 2.3784678 Test MSE 382.88925248270147 Test RE 0.9974684563397859\n",
      "11 Train Loss 2.3784647 Test MSE 382.88824336062106 Test RE 0.9974671419021247\n",
      "12 Train Loss 2.3784554 Test MSE 382.8841087128437 Test RE 0.9974617562752393\n",
      "13 Train Loss 2.3784487 Test MSE 382.8817402509123 Test RE 0.9974586711983223\n",
      "14 Train Loss 2.3784468 Test MSE 382.8814285556662 Test RE 0.9974582651941226\n",
      "15 Train Loss 2.3784404 Test MSE 382.8807965645464 Test RE 0.9974574419823147\n",
      "16 Train Loss 2.3784373 Test MSE 382.8808109882546 Test RE 0.9974574607701922\n",
      "17 Train Loss 2.378431 Test MSE 382.8813263976907 Test RE 0.9974581321263941\n",
      "18 Train Loss 2.3784282 Test MSE 382.88163167759615 Test RE 0.9974585297742325\n",
      "19 Train Loss 2.3784206 Test MSE 382.88212533490406 Test RE 0.9974591727961134\n",
      "20 Train Loss 2.3784175 Test MSE 382.8820037258496 Test RE 0.9974590143921706\n",
      "21 Train Loss 2.3740153 Test MSE 381.12383456537253 Test RE 0.9951662459708868\n",
      "22 Train Loss 2.3606496 Test MSE 380.1297951638774 Test RE 0.9938676125116528\n",
      "23 Train Loss 2.2159464 Test MSE 352.87120044632223 Test RE 0.957570349497064\n",
      "24 Train Loss 2.1625962 Test MSE 339.1357439072963 Test RE 0.9387487361226613\n",
      "25 Train Loss 2.0791364 Test MSE 319.3331772394279 Test RE 0.9109291577951882\n",
      "26 Train Loss 1.9355178 Test MSE 283.25614005483635 Test RE 0.8579307791768939\n",
      "27 Train Loss 1.8246067 Test MSE 277.1752232618164 Test RE 0.8486718274185323\n",
      "28 Train Loss 1.7319641 Test MSE 261.63556655521086 Test RE 0.824538565182015\n",
      "29 Train Loss 1.5703616 Test MSE 227.9703566854663 Test RE 0.7696650605001688\n",
      "30 Train Loss 1.345731 Test MSE 178.04670206896236 Test RE 0.6801889140692412\n",
      "31 Train Loss 1.1275957 Test MSE 109.72978046369785 Test RE 0.5339798114759864\n",
      "32 Train Loss 0.87690514 Test MSE 45.510444565011795 Test RE 0.3438888711209705\n",
      "33 Train Loss 0.40967607 Test MSE 4.975095307268092 Test RE 0.11370073836394998\n",
      "34 Train Loss 0.16224492 Test MSE 0.7317826728188898 Test RE 0.04360674232096844\n",
      "35 Train Loss 0.16085698 Test MSE 0.7598379753671309 Test RE 0.04443478468226171\n",
      "36 Train Loss 0.16077998 Test MSE 0.7591409200290391 Test RE 0.044414398355378\n",
      "37 Train Loss 0.1589659 Test MSE 0.6965911471427176 Test RE 0.04254529682336255\n",
      "38 Train Loss 0.15211336 Test MSE 0.7835860031908503 Test RE 0.04512382615997426\n",
      "39 Train Loss 0.14376664 Test MSE 0.8866732916673142 Test RE 0.0480003491010276\n",
      "40 Train Loss 0.13031799 Test MSE 0.5324684417679815 Test RE 0.03719713741646783\n",
      "41 Train Loss 0.114956304 Test MSE 0.2946024178063941 Test RE 0.027668189143996925\n",
      "42 Train Loss 0.10012322 Test MSE 0.14369860416797756 Test RE 0.019323636411666026\n",
      "43 Train Loss 0.07559611 Test MSE 0.0003980123766064195 Test RE 0.0010169763913518194\n",
      "44 Train Loss 0.054654494 Test MSE 0.11328349522699767 Test RE 0.017157179804406387\n",
      "45 Train Loss 0.044933338 Test MSE 0.3732064066092903 Test RE 0.03114132792292158\n",
      "46 Train Loss 0.028009491 Test MSE 0.0027876148880650684 Test RE 0.002691404475115239\n",
      "47 Train Loss 0.026033755 Test MSE 0.12314130259971959 Test RE 0.0178881098902319\n",
      "48 Train Loss 0.024241604 Test MSE 0.41791743028206335 Test RE 0.032953976294306846\n",
      "49 Train Loss 0.015647462 Test MSE 0.050015866431314226 Test RE 0.011400305271403528\n",
      "50 Train Loss 0.009972859 Test MSE 0.04645571147586914 Test RE 0.010987076297674888\n",
      "51 Train Loss 0.0050155357 Test MSE 0.03170513157362673 Test RE 0.009076687061030904\n",
      "52 Train Loss 0.004704993 Test MSE 0.0036952252662926744 Test RE 0.0030987250423474702\n",
      "53 Train Loss 0.004473638 Test MSE 3.955038728717067e-05 Test RE 0.000320581128098895\n",
      "54 Train Loss 0.004454362 Test MSE 9.970795376972395e-05 Test RE 0.0005090113710468504\n",
      "55 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "56 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "57 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "58 Train Loss 0.0044483184 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "59 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "60 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "61 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "62 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "63 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "64 Train Loss 0.0044483184 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "65 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "66 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "67 Train Loss 0.004448318 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "68 Train Loss 0.0044483184 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "69 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "70 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "71 Train Loss 0.0044483184 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "72 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "73 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "74 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 Train Loss 0.0044483184 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "76 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "77 Train Loss 0.0044483184 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "78 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "79 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "80 Train Loss 0.0044483184 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "81 Train Loss 0.0044483184 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "82 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "83 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "84 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "85 Train Loss 0.0044483184 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "86 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "87 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "88 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "89 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "90 Train Loss 0.0044483184 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "91 Train Loss 0.0044483184 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "92 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "93 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "94 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "95 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "96 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "97 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "98 Train Loss 0.0044483184 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "99 Train Loss 0.004448319 Test MSE 0.00017369225813253182 Test RE 0.0006718198215152721\n",
      "Training time: 60.30\n",
      "Training time: 60.30\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 4.8929405 Test MSE 383.40809461047064 Test RE 0.9981440478451382\n",
      "1 Train Loss 4.8926697 Test MSE 383.3351435130181 Test RE 0.9980490848426326\n",
      "2 Train Loss 4.891544 Test MSE 383.2330677017507 Test RE 0.9979161940098122\n",
      "3 Train Loss 4.822545 Test MSE 384.62167451629085 Test RE 0.9997224841822766\n",
      "4 Train Loss 4.3391085 Test MSE 387.84905968958776 Test RE 1.0039080898892039\n",
      "5 Train Loss 4.255644 Test MSE 385.8833970249193 Test RE 1.0013608988610707\n",
      "6 Train Loss 2.7 Test MSE 378.82780672160436 Test RE 0.992164096908358\n",
      "7 Train Loss 2.424874 Test MSE 381.9243807681999 Test RE 0.9962108652960274\n",
      "8 Train Loss 2.394563 Test MSE 382.8135179367543 Test RE 0.9973698030659439\n",
      "9 Train Loss 2.3806567 Test MSE 383.3811972655442 Test RE 0.9981090356792579\n",
      "10 Train Loss 2.3579807 Test MSE 377.62625349872985 Test RE 0.9905893910064697\n",
      "11 Train Loss 2.2841318 Test MSE 358.8239330664751 Test RE 0.9656133993444356\n",
      "12 Train Loss 2.209854 Test MSE 347.84678252410055 Test RE 0.9507286420400107\n",
      "13 Train Loss 2.0634563 Test MSE 323.2344981111951 Test RE 0.9164767153910626\n",
      "14 Train Loss 1.9904145 Test MSE 310.0915761025731 Test RE 0.89765109860768\n",
      "15 Train Loss 1.9575857 Test MSE 306.2013380417372 Test RE 0.8920026089341326\n",
      "16 Train Loss 1.9416964 Test MSE 303.2539400857501 Test RE 0.8876991594330806\n",
      "17 Train Loss 1.9041665 Test MSE 296.3118951879025 Test RE 0.8774797960527378\n",
      "18 Train Loss 1.8259681 Test MSE 280.93195581957565 Test RE 0.854403765873168\n",
      "19 Train Loss 1.709898 Test MSE 258.44692640654836 Test RE 0.8194986988519701\n",
      "20 Train Loss 1.5849221 Test MSE 219.9526289012338 Test RE 0.7560093401987761\n",
      "21 Train Loss 1.4776475 Test MSE 187.64666904496153 Test RE 0.6982854812938675\n",
      "22 Train Loss 1.345273 Test MSE 182.83523752133348 Test RE 0.689275009133706\n",
      "23 Train Loss 1.2915835 Test MSE 183.31237112459777 Test RE 0.6901738019232999\n",
      "24 Train Loss 1.2659457 Test MSE 181.1617934140217 Test RE 0.6861133792846514\n",
      "25 Train Loss 1.2246631 Test MSE 168.7785750266113 Test RE 0.6622488921693334\n",
      "26 Train Loss 1.2120128 Test MSE 157.48337285498874 Test RE 0.6397052814944623\n",
      "27 Train Loss 1.1981413 Test MSE 145.13038168646636 Test RE 0.6141037526251522\n",
      "28 Train Loss 1.1351986 Test MSE 96.14460181072548 Test RE 0.4998331258372647\n",
      "29 Train Loss 0.5245043 Test MSE 25.548804252063047 Test RE 0.25766051586847594\n",
      "30 Train Loss 0.11316392 Test MSE 7.8207548821451915 Test RE 0.14255646572716382\n",
      "31 Train Loss 0.051229186 Test MSE 2.0251802490556337 Test RE 0.07254281832453857\n",
      "32 Train Loss 0.04628108 Test MSE 0.8987654270183505 Test RE 0.04832654657999081\n",
      "33 Train Loss 0.03924561 Test MSE 0.030271902546140032 Test RE 0.008869159002887534\n",
      "34 Train Loss 0.03106845 Test MSE 0.07389423545457464 Test RE 0.013856956785338432\n",
      "35 Train Loss 0.030913262 Test MSE 0.048289470383896464 Test RE 0.011201825516747994\n",
      "36 Train Loss 0.030893859 Test MSE 0.04257685419673507 Test RE 0.01051839237775059\n",
      "37 Train Loss 0.030640377 Test MSE 0.011078322856767804 Test RE 0.005365368946924512\n",
      "38 Train Loss 0.028469587 Test MSE 0.026219762012098732 Test RE 0.008254237395918181\n",
      "39 Train Loss 0.018409807 Test MSE 0.005322370811444547 Test RE 0.003718905535256676\n",
      "40 Train Loss 0.013006087 Test MSE 0.04625908243583656 Test RE 0.010963799621262106\n",
      "41 Train Loss 0.010938205 Test MSE 0.004794352560089945 Test RE 0.003529616867741518\n",
      "42 Train Loss 0.010593816 Test MSE 0.00035416849720615757 Test RE 0.0009593289419360524\n",
      "43 Train Loss 0.010577837 Test MSE 0.0006604239597163111 Test RE 0.0013100081360068198\n",
      "44 Train Loss 0.010544178 Test MSE 0.0007215462121537362 Test RE 0.001369287545815633\n",
      "45 Train Loss 0.0104404045 Test MSE 0.0013733758311382923 Test RE 0.0018891101554613578\n",
      "46 Train Loss 0.00981518 Test MSE 0.008869539449658825 Test RE 0.004800794512912126\n",
      "47 Train Loss 0.009632409 Test MSE 0.005784389265701208 Test RE 0.0038769601609296715\n",
      "48 Train Loss 0.009628606 Test MSE 0.005690256564380679 Test RE 0.003845284760403898\n",
      "49 Train Loss 0.009627679 Test MSE 0.005671268351962236 Test RE 0.0038388636002037223\n",
      "50 Train Loss 0.009624791 Test MSE 0.005395158472059067 Test RE 0.003744248680391122\n",
      "51 Train Loss 0.009623215 Test MSE 0.005257885316960171 Test RE 0.0036963078713498467\n",
      "52 Train Loss 0.00961981 Test MSE 0.005046850695438812 Test RE 0.003621369261745569\n",
      "53 Train Loss 0.009618827 Test MSE 0.006073358885298714 Test RE 0.003972620277780417\n",
      "54 Train Loss 0.009612545 Test MSE 0.00538983372204052 Test RE 0.0037424005317210564\n",
      "55 Train Loss 0.009611897 Test MSE 0.005389293841938001 Test RE 0.003742213095680847\n",
      "56 Train Loss 0.009611748 Test MSE 0.005389295568565724 Test RE 0.0037422136951479152\n",
      "57 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "58 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "59 Train Loss 0.009611086 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "60 Train Loss 0.009611086 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "61 Train Loss 0.009611084 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "62 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "63 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "64 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "66 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "67 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "68 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "69 Train Loss 0.009611086 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "70 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "71 Train Loss 0.009611086 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "72 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "73 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "74 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "75 Train Loss 0.009611084 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "76 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "77 Train Loss 0.009611086 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "78 Train Loss 0.009611086 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "79 Train Loss 0.009611084 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "80 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "81 Train Loss 0.009611086 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "82 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "83 Train Loss 0.009611086 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "84 Train Loss 0.009611084 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "85 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "86 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "87 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "88 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "89 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "90 Train Loss 0.009611086 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "91 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "92 Train Loss 0.009611086 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "93 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "94 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "95 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "96 Train Loss 0.009611084 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "97 Train Loss 0.009611085 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "98 Train Loss 0.009611084 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "99 Train Loss 0.009611086 Test MSE 0.005390583733490379 Test RE 0.0037426609057655204\n",
      "Training time: 72.27\n",
      "Training time: 72.27\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 4.8919835 Test MSE 383.4084353230486 Test RE 0.9981444913414557\n",
      "1 Train Loss 4.891675 Test MSE 383.3341722603576 Test RE 0.9980478204680254\n",
      "2 Train Loss 4.889846 Test MSE 383.19706279423315 Test RE 0.9978693155861452\n",
      "3 Train Loss 4.6519294 Test MSE 385.70591331677974 Test RE 1.0011305887630177\n",
      "4 Train Loss 4.1814146 Test MSE 379.4845370190295 Test RE 0.9930237250312339\n",
      "5 Train Loss 3.347176 Test MSE 378.4856671708198 Test RE 0.9917159576049547\n",
      "6 Train Loss 2.6398246 Test MSE 381.9058366608627 Test RE 0.9961866797943958\n",
      "7 Train Loss 2.5043192 Test MSE 380.70938247912505 Test RE 0.9946250033622357\n",
      "8 Train Loss 2.3987825 Test MSE 383.81657825037126 Test RE 0.9986756184868193\n",
      "9 Train Loss 2.3916674 Test MSE 383.4318891924238 Test RE 0.9981750201331528\n",
      "10 Train Loss 2.377191 Test MSE 381.1173305831946 Test RE 0.9951577545429635\n",
      "11 Train Loss 2.3712375 Test MSE 381.361380193757 Test RE 0.9954763296576502\n",
      "12 Train Loss 2.3688276 Test MSE 380.66316422735343 Test RE 0.9945646276186735\n",
      "13 Train Loss 2.3649144 Test MSE 379.09646654960636 Test RE 0.9925158495890795\n",
      "14 Train Loss 2.3479054 Test MSE 375.62854441599984 Test RE 0.9879657207168158\n",
      "15 Train Loss 2.339252 Test MSE 373.9702905966956 Test RE 0.9857825665501183\n",
      "16 Train Loss 2.3302915 Test MSE 371.7815333377129 Test RE 0.9828935602451891\n",
      "17 Train Loss 2.2513723 Test MSE 353.24489779720136 Test RE 0.9580772579842378\n",
      "18 Train Loss 2.2326388 Test MSE 345.2079177017475 Test RE 0.9471155272549543\n",
      "19 Train Loss 2.1879914 Test MSE 326.0554909560634 Test RE 0.9204672514606294\n",
      "20 Train Loss 1.9765446 Test MSE 298.41731529642954 Test RE 0.8805917086932904\n",
      "21 Train Loss 1.8856293 Test MSE 284.37577480097065 Test RE 0.8596246908499172\n",
      "22 Train Loss 1.6056899 Test MSE 230.6435233514925 Test RE 0.7741644321621255\n",
      "23 Train Loss 1.45351 Test MSE 211.52748657019885 Test RE 0.741388742107367\n",
      "24 Train Loss 1.1332737 Test MSE 155.89155199496207 Test RE 0.6364640424962377\n",
      "25 Train Loss 0.9322839 Test MSE 140.88236517249746 Test RE 0.6050494905046359\n",
      "26 Train Loss 0.7178346 Test MSE 89.66312880904512 Test RE 0.4826913623773814\n",
      "27 Train Loss 0.65491086 Test MSE 74.60635684498703 Test RE 0.44030183873878254\n",
      "28 Train Loss 0.53644884 Test MSE 32.78984863338119 Test RE 0.2918987857888165\n",
      "29 Train Loss 0.3273741 Test MSE 13.75216913856474 Test RE 0.18903759572719261\n",
      "30 Train Loss 0.26411253 Test MSE 14.902907115857193 Test RE 0.19678775891562345\n",
      "31 Train Loss 0.2181619 Test MSE 15.431881574394497 Test RE 0.20024976885004708\n",
      "32 Train Loss 0.17904186 Test MSE 11.868521966789901 Test RE 0.17561471430019435\n",
      "33 Train Loss 0.1402967 Test MSE 7.4194887278724 Test RE 0.13885117914950695\n",
      "34 Train Loss 0.10549899 Test MSE 7.102107285563137 Test RE 0.1358489220878929\n",
      "35 Train Loss 0.08286453 Test MSE 5.571939880590644 Test RE 0.12032774931691624\n",
      "36 Train Loss 0.06929968 Test MSE 2.3206303567250024 Test RE 0.07765431034707167\n",
      "37 Train Loss 0.060486343 Test MSE 1.0498512931523991 Test RE 0.05223077576254939\n",
      "38 Train Loss 0.042953245 Test MSE 0.34761651113536696 Test RE 0.03005472653717654\n",
      "39 Train Loss 0.0290076 Test MSE 0.22291814194575935 Test RE 0.024067738811058798\n",
      "40 Train Loss 0.022834737 Test MSE 0.06034566849167176 Test RE 0.012522344108954173\n",
      "41 Train Loss 0.020638581 Test MSE 0.045987408508655944 Test RE 0.010931557686937536\n",
      "42 Train Loss 0.017687531 Test MSE 0.07829268295379585 Test RE 0.01426340357327009\n",
      "43 Train Loss 0.016707368 Test MSE 0.08259594446167917 Test RE 0.014650145714185164\n",
      "44 Train Loss 0.015391008 Test MSE 0.012250490261579523 Test RE 0.00564208100598525\n",
      "45 Train Loss 0.011151085 Test MSE 0.001994334839977081 Test RE 0.0022764683709504737\n",
      "46 Train Loss 0.0055676987 Test MSE 0.0008685117153638425 Test RE 0.0015022784003730547\n",
      "47 Train Loss 0.004493384 Test MSE 0.005291910718496843 Test RE 0.0037082485600663222\n",
      "48 Train Loss 0.0035797362 Test MSE 0.04140457452531084 Test RE 0.010372578866627609\n",
      "49 Train Loss 0.0019978117 Test MSE 0.027523248569653682 Test RE 0.008456924019303697\n",
      "50 Train Loss 0.0012214018 Test MSE 1.0565922803496822e-06 Test RE 5.239819167261661e-05\n",
      "51 Train Loss 0.00087413104 Test MSE 0.001137022523616036 Test RE 0.0017188861666157548\n",
      "52 Train Loss 0.0007046227 Test MSE 0.0035551101425302435 Test RE 0.003039408773122577\n",
      "53 Train Loss 0.00066198525 Test MSE 0.006839549130310034 Test RE 0.0042157642755196725\n",
      "54 Train Loss 0.0005956014 Test MSE 0.007476561127520771 Test RE 0.004407715214965536\n",
      "55 Train Loss 0.0005856551 Test MSE 0.006354891046691003 Test RE 0.004063653197497005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 Train Loss 0.0005762043 Test MSE 0.005355510629142768 Test RE 0.003730465477797664\n",
      "57 Train Loss 0.00056843925 Test MSE 0.004066079057353044 Test RE 0.0032505024053166246\n",
      "58 Train Loss 0.0005595529 Test MSE 0.002755641746883935 Test RE 0.002675925143531178\n",
      "59 Train Loss 0.000554244 Test MSE 0.001979077208148797 Test RE 0.0022677436064349626\n",
      "60 Train Loss 0.0005499283 Test MSE 0.0014646582234984556 Test RE 0.0019508807782118065\n",
      "61 Train Loss 0.0005476577 Test MSE 0.001323345186205077 Test RE 0.001854381788368499\n",
      "62 Train Loss 0.0005390431 Test MSE 0.0007692810160909873 Test RE 0.001413855710163247\n",
      "63 Train Loss 0.00051586743 Test MSE 8.025198824381064e-05 Test RE 0.00045665738244346125\n",
      "64 Train Loss 0.0005065315 Test MSE 1.4121313437369476e-05 Test RE 0.00019155792715675306\n",
      "65 Train Loss 0.0005024123 Test MSE 5.666015229210495e-06 Test RE 0.00012133929052324579\n",
      "66 Train Loss 0.0004932632 Test MSE 4.909829857140947e-07 Test RE 3.571871313192124e-05\n",
      "67 Train Loss 0.00048573097 Test MSE 1.6097390706143675e-06 Test RE 6.467557895742947e-05\n",
      "68 Train Loss 0.00047891255 Test MSE 1.5675162664004192e-05 Test RE 0.00020182204882826244\n",
      "69 Train Loss 0.00047732843 Test MSE 2.1357864283876017e-05 Test RE 0.00023558166874367202\n",
      "70 Train Loss 0.00046918128 Test MSE 0.00013973770901166968 Test RE 0.0006025864918733397\n",
      "71 Train Loss 0.00046623813 Test MSE 0.00022840381183673403 Test RE 0.0007703964206196161\n",
      "72 Train Loss 0.00046502895 Test MSE 0.00027053442364512393 Test RE 0.0008384435912716623\n",
      "73 Train Loss 0.00045867218 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "74 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "75 Train Loss 0.00045867218 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "76 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "77 Train Loss 0.00045867224 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "78 Train Loss 0.00045867218 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "79 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "80 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "81 Train Loss 0.00045867218 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "82 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "83 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "84 Train Loss 0.00045867218 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "85 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "86 Train Loss 0.00045867218 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "87 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "88 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "89 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "90 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "91 Train Loss 0.00045867218 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "92 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "93 Train Loss 0.00045867224 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "94 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "95 Train Loss 0.00045867218 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "96 Train Loss 0.00045867224 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "97 Train Loss 0.00045867218 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "98 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "99 Train Loss 0.0004586722 Test MSE 0.0005964529285212066 Test RE 0.0012449464472113218\n",
      "Training time: 66.61\n",
      "Training time: 66.61\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 4.8941727 Test MSE 383.4070316522906 Test RE 0.998142664220052\n",
      "1 Train Loss 4.8939867 Test MSE 383.36336739175255 Test RE 0.998085825927216\n",
      "2 Train Loss 4.893935 Test MSE 383.33869190979357 Test RE 0.998053704124244\n",
      "3 Train Loss 4.893931 Test MSE 383.3371896479511 Test RE 0.9980517484914094\n",
      "4 Train Loss 4.893615 Test MSE 383.28248585726664 Test RE 0.997980532909714\n",
      "5 Train Loss 4.8895736 Test MSE 383.2972611894078 Test RE 0.9979997685287201\n",
      "6 Train Loss 4.8890142 Test MSE 383.4038064526656 Test RE 0.9981384660496344\n",
      "7 Train Loss 4.881993 Test MSE 383.5884859250729 Test RE 0.9983788307638801\n",
      "8 Train Loss 4.463772 Test MSE 382.5517005356035 Test RE 0.9970286794884449\n",
      "9 Train Loss 3.6676397 Test MSE 379.4482526781061 Test RE 0.9929762500102649\n",
      "10 Train Loss 2.941876 Test MSE 381.2226467516744 Test RE 0.9952952436492293\n",
      "11 Train Loss 2.9157867 Test MSE 380.91703593255056 Test RE 0.9948962196372428\n",
      "12 Train Loss 2.5350285 Test MSE 388.18013132934976 Test RE 1.0043364712296483\n",
      "13 Train Loss 2.481556 Test MSE 387.3179619547666 Test RE 1.003220508021569\n",
      "14 Train Loss 2.4801164 Test MSE 387.24451848558505 Test RE 1.0031253878685635\n",
      "15 Train Loss 2.4757116 Test MSE 386.965392031547 Test RE 1.0027637955241993\n",
      "16 Train Loss 2.4352145 Test MSE 383.99088612478636 Test RE 0.9989023638577116\n",
      "17 Train Loss 2.3898015 Test MSE 383.6299478307255 Test RE 0.9984327864636503\n",
      "18 Train Loss 2.377156 Test MSE 382.95391518576884 Test RE 0.9975526794973435\n",
      "19 Train Loss 2.3751683 Test MSE 382.57899832428245 Test RE 0.9970642514028456\n",
      "20 Train Loss 2.3751395 Test MSE 382.5601678560356 Test RE 0.9970397134409584\n",
      "21 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "22 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "23 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "24 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "25 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "26 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "27 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "28 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "29 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "30 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "31 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "32 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "33 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "34 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "35 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "36 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "37 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "38 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "39 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "40 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "41 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "42 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "43 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "44 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "45 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "47 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "48 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "49 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "50 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "51 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "52 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "53 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "54 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "55 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "56 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "57 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "58 Train Loss 2.3751335 Test MSE 382.55815458350475 Test RE 0.9970370899120643\n",
      "59 Train Loss 2.3751333 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "60 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "61 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "62 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "63 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "64 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "65 Train Loss 2.3751333 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "66 Train Loss 2.3751333 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "67 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "68 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "69 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "70 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "71 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "72 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "73 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "74 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "75 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "76 Train Loss 2.3751333 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "77 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "78 Train Loss 2.3751333 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "79 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "80 Train Loss 2.3751335 Test MSE 382.5581561352646 Test RE 0.997037091934191\n",
      "81 Train Loss 2.3751333 Test MSE 382.55793125561337 Test RE 0.9970367988893521\n",
      "82 Train Loss 2.375127 Test MSE 382.5550323391406 Test RE 0.9970330212498892\n",
      "83 Train Loss 2.375127 Test MSE 382.5550323391406 Test RE 0.9970330212498892\n",
      "84 Train Loss 2.375127 Test MSE 382.5550323391406 Test RE 0.9970330212498892\n",
      "85 Train Loss 2.375127 Test MSE 382.5550323391406 Test RE 0.9970330212498892\n",
      "86 Train Loss 2.375127 Test MSE 382.5550323391406 Test RE 0.9970330212498892\n",
      "87 Train Loss 2.3751254 Test MSE 382.55503190745316 Test RE 0.9970330206873472\n",
      "88 Train Loss 2.3751254 Test MSE 382.55503190745316 Test RE 0.9970330206873472\n",
      "89 Train Loss 2.3751254 Test MSE 382.55503190745316 Test RE 0.9970330206873472\n",
      "90 Train Loss 2.3751254 Test MSE 382.55503190745316 Test RE 0.9970330206873472\n",
      "91 Train Loss 2.3751254 Test MSE 382.55503190745316 Test RE 0.9970330206873472\n",
      "92 Train Loss 2.3751254 Test MSE 382.55503190745316 Test RE 0.9970330206873472\n",
      "93 Train Loss 2.3751254 Test MSE 382.55503190745316 Test RE 0.9970330206873472\n",
      "94 Train Loss 2.3751254 Test MSE 382.55503190745316 Test RE 0.9970330206873472\n",
      "95 Train Loss 2.3751254 Test MSE 382.55503190745316 Test RE 0.9970330206873472\n",
      "96 Train Loss 2.3751254 Test MSE 382.55503190745316 Test RE 0.9970330206873472\n",
      "97 Train Loss 2.3751254 Test MSE 382.55503190745316 Test RE 0.9970330206873472\n",
      "98 Train Loss 2.3751254 Test MSE 382.55503190745316 Test RE 0.9970330206873472\n",
      "99 Train Loss 2.3751254 Test MSE 382.55503190745316 Test RE 0.9970330206873472\n",
      "Training time: 43.43\n",
      "Training time: 43.43\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 4.8912935 Test MSE 383.4033167685206 Test RE 0.9981378286371608\n",
      "1 Train Loss 4.8911705 Test MSE 383.3626246794556 Test RE 0.9980848591016944\n",
      "2 Train Loss 4.891162 Test MSE 383.3580516149842 Test RE 0.9980789060956803\n",
      "3 Train Loss 4.891155 Test MSE 383.3546345571582 Test RE 0.9980744579030277\n",
      "4 Train Loss 4.890932 Test MSE 383.2996039386778 Test RE 0.9980028184587527\n",
      "5 Train Loss 4.5740094 Test MSE 386.0595478610565 Test RE 1.0015894270065595\n",
      "6 Train Loss 4.269614 Test MSE 383.0553997041054 Test RE 0.9976848487258838\n",
      "7 Train Loss 3.1935346 Test MSE 378.83430803599686 Test RE 0.9921726104637705\n",
      "8 Train Loss 2.4269693 Test MSE 385.034527671131 Test RE 1.0002588916572546\n",
      "9 Train Loss 2.414245 Test MSE 384.81512686738847 Test RE 0.9999738667298639\n",
      "10 Train Loss 2.3774087 Test MSE 382.6530840978744 Test RE 0.9971607866294776\n",
      "11 Train Loss 2.3753054 Test MSE 382.4176429203972 Test RE 0.9968539697714476\n",
      "12 Train Loss 2.373783 Test MSE 381.88431504926825 Test RE 0.996158610251075\n",
      "13 Train Loss 2.3372538 Test MSE 362.6050707792763 Test RE 0.9706876833616974\n",
      "14 Train Loss 2.2659883 Test MSE 354.3450625156113 Test RE 0.9595680415554679\n",
      "15 Train Loss 2.2164812 Test MSE 347.4071385935673 Test RE 0.9501276385902252\n",
      "16 Train Loss 2.0568132 Test MSE 315.81666408864675 Test RE 0.9058996741796319\n",
      "17 Train Loss 1.7429717 Test MSE 259.0884144493749 Test RE 0.8205151025298356\n",
      "18 Train Loss 1.2253044 Test MSE 161.91680072512423 Test RE 0.6486471878613745\n",
      "19 Train Loss 0.5806607 Test MSE 56.211316696535995 Test RE 0.38218572499328174\n",
      "20 Train Loss 0.2943033 Test MSE 7.791870458556802 Test RE 0.14229297004266137\n",
      "21 Train Loss 0.22322103 Test MSE 0.24379647610706306 Test RE 0.025169598886155154\n",
      "22 Train Loss 0.16436535 Test MSE 1.345136274501988 Test RE 0.05912153819559768\n",
      "23 Train Loss 0.12434535 Test MSE 5.227193080725254 Test RE 0.1165458607379417\n",
      "24 Train Loss 0.10680906 Test MSE 7.212796204550744 Test RE 0.1369034565194968\n",
      "25 Train Loss 0.091001794 Test MSE 4.15467977131738 Test RE 0.10390378307729563\n",
      "26 Train Loss 0.08809506 Test MSE 2.7471152233594416 Test RE 0.08448916544578045\n",
      "27 Train Loss 0.059126727 Test MSE 0.7177446372801112 Test RE 0.043186455460340466\n",
      "28 Train Loss 0.016324218 Test MSE 0.03127634628043824 Test RE 0.00901510083968016\n",
      "29 Train Loss 0.0076239244 Test MSE 0.1129247312596222 Test RE 0.01712999023424887\n",
      "30 Train Loss 0.0034018366 Test MSE 0.0009616923281879328 Test RE 0.001580813580783272\n",
      "31 Train Loss 0.0019200955 Test MSE 0.007362104139455645 Test RE 0.004373846734751759\n",
      "32 Train Loss 0.0010175307 Test MSE 0.0017813277745259196 Test RE 0.0021514660744843188\n",
      "33 Train Loss 0.00063466403 Test MSE 0.007732434305333179 Test RE 0.0044825041673491085\n",
      "34 Train Loss 0.00021050134 Test MSE 0.0009435237544359949 Test RE 0.0015658097817498693\n",
      "35 Train Loss 0.00014784138 Test MSE 8.537884243520824e-05 Test RE 0.0004710182273799318\n",
      "36 Train Loss 0.00014110452 Test MSE 2.8952583111626872e-05 Test RE 0.0002742876420404435\n",
      "37 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "38 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "39 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "40 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "42 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "43 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "44 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "45 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "46 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "47 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "48 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "49 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "50 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "51 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "52 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "53 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "54 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "55 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "56 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "57 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "58 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "59 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "60 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "61 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "62 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "63 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "64 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "65 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "66 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "67 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "68 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "69 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "70 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "71 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "72 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "73 Train Loss 0.00013444902 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "74 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "75 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "76 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "77 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "78 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "79 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "80 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "81 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "82 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "83 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "84 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "85 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "86 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "87 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "88 Train Loss 0.00013444902 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "89 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "90 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "91 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "92 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "93 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "94 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "95 Train Loss 0.00013444904 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "96 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "97 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "98 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "99 Train Loss 0.00013444903 Test MSE 1.3830505270935213e-07 Test RE 1.8957523620915268e-05\n",
      "Training time: 39.16\n",
      "Training time: 39.16\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 4.8899965 Test MSE 383.40614995347335 Test RE 0.9981415165341208\n",
      "1 Train Loss 4.889867 Test MSE 383.36541448400135 Test RE 0.9980884907241216\n",
      "2 Train Loss 4.8897085 Test MSE 383.3152645628389 Test RE 0.9980232061492833\n",
      "3 Train Loss 4.638102 Test MSE 385.86396337508336 Test RE 1.0013356835470226\n",
      "4 Train Loss 4.4105887 Test MSE 387.04121630646955 Test RE 1.0028620344300747\n",
      "5 Train Loss 3.9576426 Test MSE 387.74042758419165 Test RE 1.0037674884307817\n",
      "6 Train Loss 2.387258 Test MSE 382.88416848842354 Test RE 0.997461834136727\n",
      "7 Train Loss 2.382136 Test MSE 382.8016040997975 Test RE 0.9973542829848661\n",
      "8 Train Loss 2.38152 Test MSE 382.8637255297773 Test RE 0.9974352055308819\n",
      "9 Train Loss 2.380547 Test MSE 383.07244624892206 Test RE 0.9977070477215038\n",
      "10 Train Loss 2.3805006 Test MSE 383.1041637489194 Test RE 0.997748350768215\n",
      "11 Train Loss 2.3804994 Test MSE 383.10569107735114 Test RE 0.9977503396371041\n",
      "12 Train Loss 2.380497 Test MSE 383.10839908591544 Test RE 0.9977538659638501\n",
      "13 Train Loss 2.3804955 Test MSE 383.1097493434299 Test RE 0.997755624243584\n",
      "14 Train Loss 2.3804896 Test MSE 383.1159521174645 Test RE 0.9977637013385151\n",
      "15 Train Loss 2.3804855 Test MSE 383.12110899665464 Test RE 0.9977704164459457\n",
      "16 Train Loss 2.380484 Test MSE 383.1221328977719 Test RE 0.9977717497287347\n",
      "17 Train Loss 2.3804834 Test MSE 383.1231558582853 Test RE 0.9977730817849283\n",
      "18 Train Loss 2.380477 Test MSE 383.1296028742594 Test RE 0.997781476777292\n",
      "19 Train Loss 2.380471 Test MSE 383.13560223849805 Test RE 0.9977892887957729\n",
      "20 Train Loss 2.3804646 Test MSE 383.14171325061767 Test RE 0.9977972461326629\n",
      "21 Train Loss 2.380457 Test MSE 383.1483991128869 Test RE 0.9978059519269247\n",
      "22 Train Loss 2.3802185 Test MSE 383.2274428629594 Test RE 0.9979088706095266\n",
      "23 Train Loss 2.3794777 Test MSE 383.2281950067705 Test RE 0.9979098498851849\n",
      "24 Train Loss 2.3794398 Test MSE 383.23604536585265 Test RE 0.9979200708326992\n",
      "25 Train Loss 2.3794358 Test MSE 383.23881305614407 Test RE 0.9979236742629329\n",
      "26 Train Loss 2.379432 Test MSE 383.24219464483883 Test RE 0.9979280769484858\n",
      "27 Train Loss 2.379428 Test MSE 383.24641361942895 Test RE 0.9979335698478469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 Train Loss 2.3794236 Test MSE 383.2516104315457 Test RE 0.9979403358024688\n",
      "29 Train Loss 2.3794172 Test MSE 383.2597477275701 Test RE 0.9979509300082097\n",
      "30 Train Loss 2.379409 Test MSE 383.27163157667127 Test RE 0.997966401769074\n",
      "31 Train Loss 2.3793993 Test MSE 383.28474881849 Test RE 0.9979834790238695\n",
      "32 Train Loss 2.3793914 Test MSE 383.2936149171868 Test RE 0.9979950215766082\n",
      "33 Train Loss 2.379382 Test MSE 383.300445675698 Test RE 0.998003914279683\n",
      "34 Train Loss 2.3793259 Test MSE 383.33395476983395 Test RE 0.9980475373391137\n",
      "35 Train Loss 2.3792112 Test MSE 383.3704311508278 Test RE 0.9980950211267764\n",
      "36 Train Loss 2.377277 Test MSE 382.52372715714154 Test RE 0.9969922258922065\n",
      "37 Train Loss 2.3377469 Test MSE 372.0401481711278 Test RE 0.983235355953853\n",
      "38 Train Loss 2.3260682 Test MSE 370.82504983376106 Test RE 0.9816283993447918\n",
      "39 Train Loss 2.3244748 Test MSE 370.08066411381003 Test RE 0.9806426552124425\n",
      "40 Train Loss 2.316585 Test MSE 366.12581807618335 Test RE 0.97538879015673\n",
      "41 Train Loss 2.2108207 Test MSE 343.19815832652597 Test RE 0.9443545059156087\n",
      "42 Train Loss 1.9900753 Test MSE 290.8872006579571 Test RE 0.8694105154769143\n",
      "43 Train Loss 1.9237796 Test MSE 288.14252482485034 Test RE 0.8652991183026543\n",
      "44 Train Loss 1.894838 Test MSE 285.9044629212598 Test RE 0.8619320898729218\n",
      "45 Train Loss 1.8570834 Test MSE 281.0825045932035 Test RE 0.8546326686524042\n",
      "46 Train Loss 1.7676017 Test MSE 266.1878478783927 Test RE 0.8316808372546872\n",
      "47 Train Loss 1.6362154 Test MSE 235.9512312253554 Test RE 0.783021535174706\n",
      "48 Train Loss 1.5160347 Test MSE 203.0510545921006 Test RE 0.7263822234068095\n",
      "49 Train Loss 1.4054154 Test MSE 165.86036663911563 Test RE 0.6564987342051458\n",
      "50 Train Loss 1.3684882 Test MSE 149.82008209141043 Test RE 0.6239468519560876\n",
      "51 Train Loss 1.3369164 Test MSE 137.02063217119698 Test RE 0.5966993516735736\n",
      "52 Train Loss 1.2636836 Test MSE 118.77304880766698 Test RE 0.5555479314076884\n",
      "53 Train Loss 1.1694877 Test MSE 103.43769613492267 Test RE 0.5184441782191224\n",
      "54 Train Loss 1.1142228 Test MSE 90.30067490644909 Test RE 0.48440440147709796\n",
      "55 Train Loss 1.106087 Test MSE 85.53643494539742 Test RE 0.4714527303625081\n",
      "56 Train Loss 1.1054054 Test MSE 84.32414182698768 Test RE 0.46809989913613687\n",
      "57 Train Loss 1.1054054 Test MSE 84.32414182698768 Test RE 0.46809989913613687\n",
      "58 Train Loss 1.1053864 Test MSE 84.31964472428872 Test RE 0.46808741681884497\n",
      "59 Train Loss 1.1053864 Test MSE 84.31964472428872 Test RE 0.46808741681884497\n",
      "60 Train Loss 1.1053864 Test MSE 84.31964472428872 Test RE 0.46808741681884497\n",
      "61 Train Loss 1.1053864 Test MSE 84.31964472428872 Test RE 0.46808741681884497\n",
      "62 Train Loss 1.1053864 Test MSE 84.31964472428872 Test RE 0.46808741681884497\n",
      "63 Train Loss 1.1053864 Test MSE 84.31964472428872 Test RE 0.46808741681884497\n",
      "64 Train Loss 1.1053864 Test MSE 84.31964472428872 Test RE 0.46808741681884497\n",
      "65 Train Loss 1.1053864 Test MSE 84.31964472428872 Test RE 0.46808741681884497\n",
      "66 Train Loss 1.1053863 Test MSE 84.31287917112459 Test RE 0.46806863748312866\n",
      "67 Train Loss 1.1053778 Test MSE 84.3128698238956 Test RE 0.4680686115372416\n",
      "68 Train Loss 1.1053777 Test MSE 84.31286842710799 Test RE 0.4680686076600612\n",
      "69 Train Loss 1.1053777 Test MSE 84.31286842710799 Test RE 0.4680686076600612\n",
      "70 Train Loss 1.1053777 Test MSE 84.31286842710799 Test RE 0.4680686076600612\n",
      "71 Train Loss 1.1053774 Test MSE 84.31286547781293 Test RE 0.4680685994734552\n",
      "72 Train Loss 1.1053774 Test MSE 84.31286547781293 Test RE 0.4680685994734552\n",
      "73 Train Loss 1.1053774 Test MSE 84.31286547781293 Test RE 0.4680685994734552\n",
      "74 Train Loss 1.1053772 Test MSE 84.31285688921075 Test RE 0.4680685756333505\n",
      "75 Train Loss 1.1053772 Test MSE 84.31285688921075 Test RE 0.4680685756333505\n",
      "76 Train Loss 1.1053772 Test MSE 84.31285688921075 Test RE 0.4680685756333505\n",
      "77 Train Loss 1.1053772 Test MSE 84.31285688921075 Test RE 0.4680685756333505\n",
      "78 Train Loss 1.1053772 Test MSE 84.31285688921075 Test RE 0.4680685756333505\n",
      "79 Train Loss 1.1053772 Test MSE 84.31285688921075 Test RE 0.4680685756333505\n",
      "80 Train Loss 1.1053772 Test MSE 84.31285688921075 Test RE 0.4680685756333505\n",
      "81 Train Loss 1.1053772 Test MSE 84.31285688921075 Test RE 0.4680685756333505\n",
      "82 Train Loss 1.105377 Test MSE 84.31285405261114 Test RE 0.4680685677595623\n",
      "83 Train Loss 1.105377 Test MSE 84.31285405261114 Test RE 0.4680685677595623\n",
      "84 Train Loss 1.1053768 Test MSE 84.31285504964397 Test RE 0.4680685705271102\n",
      "85 Train Loss 1.105377 Test MSE 84.31285504964397 Test RE 0.4680685705271102\n",
      "86 Train Loss 1.1053768 Test MSE 84.31285504964397 Test RE 0.4680685705271102\n",
      "87 Train Loss 1.1053768 Test MSE 84.31285504964397 Test RE 0.4680685705271102\n",
      "88 Train Loss 1.1053768 Test MSE 84.3128551137195 Test RE 0.4680685707049701\n",
      "89 Train Loss 1.1053767 Test MSE 84.3128551137195 Test RE 0.4680685707049701\n",
      "90 Train Loss 1.1053767 Test MSE 84.3128551137195 Test RE 0.4680685707049701\n",
      "91 Train Loss 1.1053767 Test MSE 84.3128551137195 Test RE 0.4680685707049701\n",
      "92 Train Loss 1.1053767 Test MSE 84.31285336172557 Test RE 0.4680685658418131\n",
      "93 Train Loss 1.1053767 Test MSE 84.31285336172557 Test RE 0.4680685658418131\n",
      "94 Train Loss 1.1053767 Test MSE 84.31285336172557 Test RE 0.4680685658418131\n",
      "95 Train Loss 1.1053767 Test MSE 84.31285336172557 Test RE 0.4680685658418131\n",
      "96 Train Loss 1.1053767 Test MSE 84.31285336172557 Test RE 0.4680685658418131\n",
      "97 Train Loss 1.1053767 Test MSE 84.31285336172557 Test RE 0.4680685658418131\n",
      "98 Train Loss 1.1053767 Test MSE 84.31285336172557 Test RE 0.4680685658418131\n",
      "99 Train Loss 1.1053767 Test MSE 84.31285336172557 Test RE 0.4680685658418131\n",
      "Training time: 53.04\n",
      "Training time: 53.04\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 4.8926497 Test MSE 383.39835403850134 Test RE 0.9981313687235681\n",
      "1 Train Loss 4.892529 Test MSE 383.35855630343184 Test RE 0.9980795630777445\n",
      "2 Train Loss 4.8925214 Test MSE 383.35423508781497 Test RE 0.9980739378881174\n",
      "3 Train Loss 4.892512 Test MSE 383.3497189998689 Test RE 0.9980680589874266\n",
      "4 Train Loss 4.892109 Test MSE 383.26562211371197 Test RE 0.9979585779893568\n",
      "5 Train Loss 4.7417474 Test MSE 384.96826428495314 Test RE 1.0001728170478723\n",
      "6 Train Loss 4.307799 Test MSE 382.7065147400143 Test RE 0.9972304020116973\n",
      "7 Train Loss 3.6652055 Test MSE 380.5281953839201 Test RE 0.9943882943785193\n",
      "8 Train Loss 2.58209 Test MSE 386.3246849197078 Test RE 1.0019333025781771\n",
      "9 Train Loss 2.4133773 Test MSE 385.26889035133746 Test RE 1.000563263962302\n",
      "10 Train Loss 2.3869905 Test MSE 384.49014755651973 Test RE 0.9995515347651455\n",
      "11 Train Loss 2.3855426 Test MSE 384.3645325616744 Test RE 0.9993882420012958\n",
      "12 Train Loss 2.3829138 Test MSE 383.55076711898414 Test RE 0.9983297435458316\n",
      "13 Train Loss 2.3800783 Test MSE 382.73268365278864 Test RE 0.9972644960084118\n",
      "14 Train Loss 2.3783367 Test MSE 381.91218280054267 Test RE 0.9961949565905949\n",
      "15 Train Loss 2.3773031 Test MSE 381.4439124979402 Test RE 0.9955840418105945\n",
      "16 Train Loss 2.374758 Test MSE 380.8606147346875 Test RE 0.9948225351995238\n",
      "17 Train Loss 2.3459506 Test MSE 372.2099691968091 Test RE 0.9834597336018129\n",
      "18 Train Loss 2.2942078 Test MSE 365.79403434316924 Test RE 0.9749467406385073\n",
      "19 Train Loss 2.2411325 Test MSE 355.9680008164381 Test RE 0.9617629932996945\n",
      "20 Train Loss 2.1879122 Test MSE 344.60417391402814 Test RE 0.9462869463892908\n",
      "21 Train Loss 2.1269684 Test MSE 335.87271680497366 Test RE 0.934221689928433\n",
      "22 Train Loss 1.8505461 Test MSE 285.88320596831267 Test RE 0.8619000470180047\n",
      "23 Train Loss 1.4695828 Test MSE 216.91700530278322 Test RE 0.750774273772525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 Train Loss 1.4370553 Test MSE 177.30505882682658 Test RE 0.6787707919565261\n",
      "25 Train Loss 0.78797007 Test MSE 76.19737229370322 Test RE 0.4449718936123293\n",
      "26 Train Loss 0.44500804 Test MSE 40.39305765651575 Test RE 0.32397831867254856\n",
      "27 Train Loss 0.3970383 Test MSE 35.83317275396194 Test RE 0.3051442664928644\n",
      "28 Train Loss 0.21816477 Test MSE 3.3353065834593703 Test RE 0.09309588029446135\n",
      "29 Train Loss 0.11419695 Test MSE 0.6440383976435335 Test RE 0.04090896226695658\n",
      "30 Train Loss 0.09541426 Test MSE 0.414024323298944 Test RE 0.032800125886693746\n",
      "31 Train Loss 0.07729113 Test MSE 1.0403580751286432 Test RE 0.05199409265395396\n",
      "32 Train Loss 0.066109985 Test MSE 1.7791552253062881 Test RE 0.06799382963726595\n",
      "33 Train Loss 0.02398222 Test MSE 0.266106009913868 Test RE 0.026296014106878637\n",
      "34 Train Loss 0.017295722 Test MSE 0.002631966749743081 Test RE 0.002615187198895757\n",
      "35 Train Loss 0.01691852 Test MSE 0.0005289687828600046 Test RE 0.001172404843530289\n",
      "36 Train Loss 0.016854722 Test MSE 0.0010564122042471806 Test RE 0.00165683510349544\n",
      "37 Train Loss 0.016623046 Test MSE 0.0005364597272775339 Test RE 0.0011806771132443145\n",
      "38 Train Loss 0.01609642 Test MSE 0.030608752236583436 Test RE 0.008918368139111496\n",
      "39 Train Loss 0.015539803 Test MSE 0.061514028701265364 Test RE 0.012642986319382451\n",
      "40 Train Loss 0.013367483 Test MSE 0.01044259802702945 Test RE 0.005209149995021233\n",
      "41 Train Loss 0.0070514292 Test MSE 1.7008000891658377e-06 Test RE 6.647972549378365e-05\n",
      "42 Train Loss 0.004569566 Test MSE 0.0029822305155645466 Test RE 0.0027837689563798454\n",
      "43 Train Loss 0.0043173805 Test MSE 0.004207214988327977 Test RE 0.0033064345853765944\n",
      "44 Train Loss 0.0038643454 Test MSE 0.004257671185328568 Test RE 0.0033262021635911887\n",
      "45 Train Loss 0.0035403818 Test MSE 0.006270114779498308 Test RE 0.004036456978537287\n",
      "46 Train Loss 0.0033932498 Test MSE 0.0060548540544161115 Test RE 0.003966563600624088\n",
      "47 Train Loss 0.0033851143 Test MSE 0.0059979941937732305 Test RE 0.003947895087065935\n",
      "48 Train Loss 0.003339817 Test MSE 0.006288799256270501 Test RE 0.004042466675891042\n",
      "49 Train Loss 0.0030847588 Test MSE 0.006278859973709092 Test RE 0.004039270906316647\n",
      "50 Train Loss 0.0029878472 Test MSE 0.0062403184472691045 Test RE 0.004026854694992914\n",
      "51 Train Loss 0.0027010306 Test MSE 0.009830878301973893 Test RE 0.005054273540299501\n",
      "52 Train Loss 0.0023993177 Test MSE 0.00898068651013185 Test RE 0.004830781015229276\n",
      "53 Train Loss 0.0014590254 Test MSE 0.001270347350226429 Test RE 0.0018168698812184713\n",
      "54 Train Loss 0.0006618011 Test MSE 3.432614150015371e-05 Test RE 0.00029865863560896795\n",
      "55 Train Loss 0.00034705517 Test MSE 3.863520685246393e-05 Test RE 0.00031685035892563626\n",
      "56 Train Loss 0.00027045736 Test MSE 1.0672410110265245e-05 Test RE 0.00016653051891276927\n",
      "57 Train Loss 0.0002477489 Test MSE 1.391437180277791e-07 Test RE 1.9014914828389764e-05\n",
      "58 Train Loss 0.00024331293 Test MSE 3.688294020003079e-06 Test RE 9.78983449463541e-05\n",
      "59 Train Loss 0.00022900605 Test MSE 9.708272669447553e-06 Test RE 0.00015883037647517294\n",
      "60 Train Loss 0.00022263665 Test MSE 3.904148804377003e-06 Test RE 0.00010072232998088348\n",
      "61 Train Loss 0.00022063914 Test MSE 2.2786499863256982e-07 Test RE 2.433332141146526e-05\n",
      "62 Train Loss 0.00021931026 Test MSE 3.156079023221422e-08 Test RE 9.05600214871812e-06\n",
      "63 Train Loss 0.00021644213 Test MSE 6.621911355139041e-06 Test RE 0.00013117596382549365\n",
      "64 Train Loss 0.00021519538 Test MSE 7.943064029695417e-06 Test RE 0.0001436668647566331\n",
      "65 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "66 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "67 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "68 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "69 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "70 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "71 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "72 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "73 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "74 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "75 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "76 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "77 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "78 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "79 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "80 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "81 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "82 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "83 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "84 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "85 Train Loss 0.00021046748 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "86 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "87 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "88 Train Loss 0.00021046748 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "89 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "90 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "91 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "92 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "93 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "94 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "95 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "96 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "97 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "98 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "99 Train Loss 0.0002104675 Test MSE 3.25661400737616e-05 Test RE 0.0002909013426947779\n",
      "Training time: 56.69\n",
      "Training time: 56.69\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    beta_val = []\n",
    "\n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "\n",
    "\n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.05, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)    \n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1D_SODE_swish_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_SODE_swish_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0b19103404f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1D_SODE_swish_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m \u001b[0;31m#WRONGLY SAVED AS STAN - DOESN'T MATTER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_SODE_swish_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"1D_SODE_swish_tune\"+str(tune_reps)+\".mat\" #WRONGLY SAVED AS STAN - DOESN'T MATTER\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
