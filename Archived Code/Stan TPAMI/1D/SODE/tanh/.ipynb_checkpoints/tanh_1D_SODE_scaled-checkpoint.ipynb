{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y/50\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"1D_SODE_tanh_scaled\"\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "\n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a/50\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.586071 Test MSE 386.34613658544964 Test RE 1.0019611196460274\n",
      "1 Train Loss 3.4952848 Test MSE 383.28540498099 Test RE 0.9979843332700354\n",
      "2 Train Loss 2.6853971 Test MSE 384.8542083769056 Test RE 1.0000246437046574\n",
      "3 Train Loss 2.4075994 Test MSE 384.1441515301631 Test RE 0.999101693981912\n",
      "4 Train Loss 2.389292 Test MSE 384.3347134776916 Test RE 0.9993494748708684\n",
      "5 Train Loss 2.3870459 Test MSE 384.2915591541046 Test RE 0.9992933682276758\n",
      "6 Train Loss 2.383028 Test MSE 384.05241895530867 Test RE 0.9989823954807225\n",
      "7 Train Loss 2.382046 Test MSE 383.96821772113236 Test RE 0.9988728789701717\n",
      "8 Train Loss 2.3816898 Test MSE 383.94739475037306 Test RE 0.9988457936779613\n",
      "9 Train Loss 2.3813884 Test MSE 383.916360980697 Test RE 0.9988054254177139\n",
      "10 Train Loss 2.3813515 Test MSE 383.9125025019447 Test RE 0.9988004062526212\n",
      "11 Train Loss 2.3813448 Test MSE 383.91185580653496 Test RE 0.9987995650193902\n",
      "12 Train Loss 2.381338 Test MSE 383.9095567952118 Test RE 0.9987965744176251\n",
      "13 Train Loss 2.3813293 Test MSE 383.90722947091575 Test RE 0.9987935469766352\n",
      "14 Train Loss 2.3813198 Test MSE 383.9051855101715 Test RE 0.9987908881343911\n",
      "15 Train Loss 2.3813102 Test MSE 383.9021360436751 Test RE 0.9987869212885219\n",
      "16 Train Loss 2.3813004 Test MSE 383.8993593044278 Test RE 0.998783309201484\n",
      "17 Train Loss 2.381281 Test MSE 383.8930923819694 Test RE 0.9987751569046059\n",
      "18 Train Loss 2.381273 Test MSE 383.89072084302893 Test RE 0.9987720718818712\n",
      "19 Train Loss 2.3812654 Test MSE 383.8878476084595 Test RE 0.9987683342142778\n",
      "20 Train Loss 2.3812566 Test MSE 383.8846106667653 Test RE 0.9987641233989325\n",
      "21 Train Loss 2.3812494 Test MSE 383.8809145656287 Test RE 0.9987593152586806\n",
      "22 Train Loss 2.3812423 Test MSE 383.87817585058 Test RE 0.9987557525365687\n",
      "23 Train Loss 2.3812375 Test MSE 383.8758414974717 Test RE 0.998752715828198\n",
      "24 Train Loss 2.3812337 Test MSE 383.87369780519583 Test RE 0.9987499271381584\n",
      "25 Train Loss 2.3812282 Test MSE 383.87171792277974 Test RE 0.9987473515384494\n",
      "26 Train Loss 2.3812225 Test MSE 383.86885399793283 Test RE 0.9987436258892842\n",
      "27 Train Loss 2.3812175 Test MSE 383.86598345256635 Test RE 0.9987398916136098\n",
      "28 Train Loss 2.3812108 Test MSE 383.8621564883976 Test RE 0.998734913116595\n",
      "29 Train Loss 2.3812032 Test MSE 383.8580139443841 Test RE 0.9987295240538446\n",
      "30 Train Loss 2.381194 Test MSE 383.85198408064866 Test RE 0.9987216797124865\n",
      "31 Train Loss 2.3808856 Test MSE 383.6962779485774 Test RE 0.9985190978978433\n",
      "32 Train Loss 2.3799057 Test MSE 383.5341180730176 Test RE 0.9983080757255698\n",
      "33 Train Loss 2.379167 Test MSE 383.3213050474331 Test RE 0.9980310698069674\n",
      "34 Train Loss 2.37681 Test MSE 382.6768249293357 Test RE 0.9971917194247165\n",
      "35 Train Loss 2.363896 Test MSE 379.3164410202566 Test RE 0.992803766392989\n",
      "36 Train Loss 2.3531802 Test MSE 378.51456745271497 Test RE 0.9917538194397907\n",
      "37 Train Loss 2.3409693 Test MSE 376.0328745222352 Test RE 0.9884973055122034\n",
      "38 Train Loss 2.332884 Test MSE 374.558452351623 Test RE 0.9865574567326229\n",
      "39 Train Loss 2.3234372 Test MSE 373.87610894974523 Test RE 0.985658427715891\n",
      "40 Train Loss 2.3123198 Test MSE 369.6673361132818 Test RE 0.9800948823296795\n",
      "41 Train Loss 2.2774768 Test MSE 366.11940700311044 Test RE 0.9753802503107611\n",
      "42 Train Loss 2.2592006 Test MSE 362.9191801297353 Test RE 0.9711080250777183\n",
      "43 Train Loss 2.2481534 Test MSE 360.2679887890707 Test RE 0.9675544619836423\n",
      "44 Train Loss 2.2430649 Test MSE 359.9410786191863 Test RE 0.9671153788704588\n",
      "45 Train Loss 2.2340355 Test MSE 357.76489503892094 Test RE 0.9641873838591138\n",
      "46 Train Loss 2.1993706 Test MSE 351.8035379671751 Test RE 0.956120618788194\n",
      "47 Train Loss 2.1637444 Test MSE 347.1564390308502 Test RE 0.9497847562124011\n",
      "48 Train Loss 2.1586823 Test MSE 345.5463634865807 Test RE 0.947579694953548\n",
      "49 Train Loss 2.1442227 Test MSE 342.60353672687756 Test RE 0.9435360618023796\n",
      "50 Train Loss 2.136311 Test MSE 342.2872100005754 Test RE 0.9431003765759745\n",
      "51 Train Loss 2.1168172 Test MSE 339.4330836301786 Test RE 0.9391601733408781\n",
      "52 Train Loss 2.1036656 Test MSE 336.122320143918 Test RE 0.9345687582296025\n",
      "53 Train Loss 2.0950704 Test MSE 334.8964787652136 Test RE 0.9328630109031725\n",
      "54 Train Loss 2.0829234 Test MSE 333.55559581278345 Test RE 0.9309936053229997\n",
      "55 Train Loss 2.0752769 Test MSE 331.86284084821745 Test RE 0.9286282596521073\n",
      "56 Train Loss 2.0675483 Test MSE 330.27929486144984 Test RE 0.926410048100952\n",
      "57 Train Loss 2.0625687 Test MSE 329.65307983087973 Test RE 0.9255313869926899\n",
      "58 Train Loss 2.0580878 Test MSE 327.66458596602047 Test RE 0.9227357248551332\n",
      "59 Train Loss 2.0498202 Test MSE 324.8759381031691 Test RE 0.9188007812769703\n",
      "60 Train Loss 2.0428689 Test MSE 323.471649285003 Test RE 0.9168128547565666\n",
      "61 Train Loss 2.02659 Test MSE 322.9907013354328 Test RE 0.9161310279470084\n",
      "62 Train Loss 2.019419 Test MSE 322.8417820732912 Test RE 0.9159198062532488\n",
      "63 Train Loss 2.012476 Test MSE 321.429786453691 Test RE 0.9139146569688458\n",
      "64 Train Loss 1.9999347 Test MSE 318.58492347557586 Test RE 0.9098612983167845\n",
      "65 Train Loss 1.9844106 Test MSE 316.7118544512554 Test RE 0.907182663484023\n",
      "66 Train Loss 1.9789327 Test MSE 315.63127943769746 Test RE 0.9056337532095826\n",
      "67 Train Loss 1.974669 Test MSE 314.4046508691438 Test RE 0.9038722712201394\n",
      "68 Train Loss 1.9685994 Test MSE 313.48986172407234 Test RE 0.9025563637306185\n",
      "69 Train Loss 1.9607322 Test MSE 312.8678993887706 Test RE 0.9016605855241098\n",
      "70 Train Loss 1.9585214 Test MSE 312.24986143464395 Test RE 0.9007695769876961\n",
      "71 Train Loss 1.9548991 Test MSE 311.2489383011472 Test RE 0.8993247007772858\n",
      "72 Train Loss 1.9469006 Test MSE 309.70742519322107 Test RE 0.8970949075077613\n",
      "73 Train Loss 1.9416305 Test MSE 309.1796181807004 Test RE 0.8963301617353063\n",
      "74 Train Loss 1.9388435 Test MSE 308.6059205171147 Test RE 0.8954981837327706\n",
      "75 Train Loss 1.9377865 Test MSE 308.31558106452815 Test RE 0.8950768379059095\n",
      "76 Train Loss 1.9349463 Test MSE 308.6867315714817 Test RE 0.8956154229216506\n",
      "77 Train Loss 1.9313784 Test MSE 308.5298521434147 Test RE 0.8953878111116537\n",
      "78 Train Loss 1.929096 Test MSE 307.9485196721473 Test RE 0.8945438677673486\n",
      "79 Train Loss 1.9285845 Test MSE 307.7937289075697 Test RE 0.8943190176301615\n",
      "80 Train Loss 1.927261 Test MSE 307.44889172754023 Test RE 0.89381790138723\n",
      "81 Train Loss 1.9174292 Test MSE 305.6649640643887 Test RE 0.8912210044214649\n",
      "82 Train Loss 1.9119062 Test MSE 304.3078707883618 Test RE 0.8892403792001838\n",
      "83 Train Loss 1.90878 Test MSE 303.47528258831784 Test RE 0.8880230624188268\n",
      "84 Train Loss 1.9072142 Test MSE 303.00505463370615 Test RE 0.8873348100754991\n",
      "85 Train Loss 1.9065477 Test MSE 303.10326702214775 Test RE 0.8874786034035994\n",
      "86 Train Loss 1.9062115 Test MSE 303.30874614168243 Test RE 0.8877793712381148\n",
      "87 Train Loss 1.9056766 Test MSE 303.13550650818996 Test RE 0.8875258003433517\n",
      "88 Train Loss 1.9047172 Test MSE 302.552836437609 Test RE 0.8866724139030049\n",
      "89 Train Loss 1.9032744 Test MSE 301.94584934343584 Test RE 0.8857825380626297\n",
      "90 Train Loss 1.9018301 Test MSE 302.01930133642645 Test RE 0.8858902701884221\n",
      "91 Train Loss 1.9011984 Test MSE 302.0927738925586 Test RE 0.8859980193691074\n",
      "92 Train Loss 1.9007819 Test MSE 302.1819435071994 Test RE 0.8861287710448816\n",
      "93 Train Loss 1.8999747 Test MSE 302.3328865860516 Test RE 0.8863500587640767\n",
      "94 Train Loss 1.8987854 Test MSE 302.5142484497547 Test RE 0.8866158684153398\n",
      "95 Train Loss 1.8959051 Test MSE 301.76869993510314 Test RE 0.8855226588906385\n",
      "96 Train Loss 1.8928206 Test MSE 300.70419756053934 Test RE 0.8839594189908859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 Train Loss 1.8869572 Test MSE 299.688553943787 Test RE 0.8824653475538154\n",
      "98 Train Loss 1.8837986 Test MSE 298.93691864155903 Test RE 0.8813580171017978\n",
      "99 Train Loss 1.88182 Test MSE 298.33013203356563 Test RE 0.8804630659124638\n",
      "Training time: 36.61\n",
      "Training time: 36.61\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 4.621859 Test MSE 386.7508869127144 Test RE 1.0024858277931912\n",
      "1 Train Loss 3.1967108 Test MSE 382.3885021823382 Test RE 0.9968159882421308\n",
      "2 Train Loss 2.5110314 Test MSE 383.2151655336918 Test RE 0.9978928856453246\n",
      "3 Train Loss 2.3845363 Test MSE 383.5764567494428 Test RE 0.9983631762683761\n",
      "4 Train Loss 2.3817742 Test MSE 383.7410498178314 Test RE 0.9985773526490866\n",
      "5 Train Loss 2.3811424 Test MSE 383.8359630841545 Test RE 0.9987008374553775\n",
      "6 Train Loss 2.3808935 Test MSE 383.827772977915 Test RE 0.998690182500684\n",
      "7 Train Loss 2.3807375 Test MSE 383.7662674908806 Test RE 0.9986101630345163\n",
      "8 Train Loss 2.3798578 Test MSE 383.4253999058899 Test RE 0.998166573429441\n",
      "9 Train Loss 2.3752775 Test MSE 382.428532991529 Test RE 0.996868163328617\n",
      "10 Train Loss 2.3696227 Test MSE 381.48201490851375 Test RE 0.9956337649853733\n",
      "11 Train Loss 2.3644423 Test MSE 380.55568472056177 Test RE 0.9944242110086863\n",
      "12 Train Loss 2.3608701 Test MSE 379.6930739105784 Test RE 0.9932965340908716\n",
      "13 Train Loss 2.3576496 Test MSE 379.3024314453758 Test RE 0.9927854322455183\n",
      "14 Train Loss 2.3524485 Test MSE 378.8724566531405 Test RE 0.9922225651001746\n",
      "15 Train Loss 2.3460088 Test MSE 377.41141687066715 Test RE 0.9903075711389311\n",
      "16 Train Loss 2.3381224 Test MSE 376.2198627351618 Test RE 0.9887430478190076\n",
      "17 Train Loss 2.3360398 Test MSE 375.46526003586996 Test RE 0.9877509647993746\n",
      "18 Train Loss 2.3315802 Test MSE 373.6903361300233 Test RE 0.9854135186503395\n",
      "19 Train Loss 2.3222842 Test MSE 372.9966276205736 Test RE 0.984498446435164\n",
      "20 Train Loss 2.315738 Test MSE 372.133427272648 Test RE 0.9833586081928495\n",
      "21 Train Loss 2.3099785 Test MSE 371.16024794117345 Test RE 0.9820719584550714\n",
      "22 Train Loss 2.307677 Test MSE 371.003596831691 Test RE 0.9818646909758755\n",
      "23 Train Loss 2.3054023 Test MSE 370.8197050957086 Test RE 0.9816213251657869\n",
      "24 Train Loss 2.3002925 Test MSE 369.9112246505679 Test RE 0.9804181384997107\n",
      "25 Train Loss 2.2890189 Test MSE 367.79356324503254 Test RE 0.9776077701058581\n",
      "26 Train Loss 2.281788 Test MSE 365.3831163556614 Test RE 0.9743989792084355\n",
      "27 Train Loss 2.2683086 Test MSE 363.3723107216504 Test RE 0.9717140849238324\n",
      "28 Train Loss 2.2568502 Test MSE 361.7428554753574 Test RE 0.9695329285563645\n",
      "29 Train Loss 2.244022 Test MSE 359.7466645130164 Test RE 0.9668541607450625\n",
      "30 Train Loss 2.2298837 Test MSE 357.66237823597186 Test RE 0.9640492309947465\n",
      "31 Train Loss 2.2129874 Test MSE 353.5713852456988 Test RE 0.9585199083355525\n",
      "32 Train Loss 2.1916723 Test MSE 351.0253858467666 Test RE 0.9550626147056906\n",
      "33 Train Loss 2.1795528 Test MSE 348.63421628554994 Test RE 0.9518041337053841\n",
      "34 Train Loss 2.1621943 Test MSE 345.7358481801048 Test RE 0.9478394679665593\n",
      "35 Train Loss 2.15142 Test MSE 345.15802888902294 Test RE 0.9470470870859926\n",
      "36 Train Loss 2.1433883 Test MSE 343.3123749183747 Test RE 0.9445116337701563\n",
      "37 Train Loss 2.1307945 Test MSE 339.24432365652183 Test RE 0.9388990018303189\n",
      "38 Train Loss 2.111163 Test MSE 337.2989969184855 Test RE 0.9362031700777069\n",
      "39 Train Loss 2.0923696 Test MSE 335.50891838702955 Test RE 0.9337156046805953\n",
      "40 Train Loss 2.079012 Test MSE 331.58122882403006 Test RE 0.9282341685489526\n",
      "41 Train Loss 2.0692391 Test MSE 328.7751872158319 Test RE 0.9242981832241063\n",
      "42 Train Loss 2.0666332 Test MSE 328.56675203824403 Test RE 0.9240051459026447\n",
      "43 Train Loss 2.0576427 Test MSE 326.99758962031547 Test RE 0.921796082746302\n",
      "44 Train Loss 2.0554976 Test MSE 327.18860849582325 Test RE 0.9220652815867308\n",
      "45 Train Loss 2.0511017 Test MSE 326.8330659476831 Test RE 0.9215641600004509\n",
      "46 Train Loss 2.0427861 Test MSE 325.8824105496789 Test RE 0.9202229126926901\n",
      "47 Train Loss 2.0393848 Test MSE 325.1458205597721 Test RE 0.9191823372895189\n",
      "48 Train Loss 2.0295644 Test MSE 321.9461622525527 Test RE 0.9146484626820052\n",
      "49 Train Loss 2.0174117 Test MSE 319.78479429877706 Test RE 0.9115730711544534\n",
      "50 Train Loss 2.012478 Test MSE 319.15268047147566 Test RE 0.9106716790537653\n",
      "51 Train Loss 1.9986486 Test MSE 318.61629939424495 Test RE 0.9099061011764688\n",
      "52 Train Loss 1.9697821 Test MSE 314.11451250539284 Test RE 0.9034551200252751\n",
      "53 Train Loss 1.9584824 Test MSE 311.4661664097462 Test RE 0.899638476191841\n",
      "54 Train Loss 1.9541401 Test MSE 310.51492902844876 Test RE 0.8982636492700581\n",
      "55 Train Loss 1.9522115 Test MSE 310.6735292587936 Test RE 0.8984930209064801\n",
      "56 Train Loss 1.9497657 Test MSE 310.050610129865 Test RE 0.8975918026321688\n",
      "57 Train Loss 1.9458591 Test MSE 308.66569877824367 Test RE 0.8955849104101435\n",
      "58 Train Loss 1.9440036 Test MSE 308.79306866836737 Test RE 0.8957696714425555\n",
      "59 Train Loss 1.9424825 Test MSE 309.0633550832261 Test RE 0.8961616190607532\n",
      "60 Train Loss 1.9405766 Test MSE 308.54219725324236 Test RE 0.8954057243706466\n",
      "61 Train Loss 1.934394 Test MSE 307.6217484108158 Test RE 0.8940691312529303\n",
      "62 Train Loss 1.9336852 Test MSE 307.4595344487407 Test RE 0.893833371555871\n",
      "63 Train Loss 1.9315016 Test MSE 307.7021122447643 Test RE 0.8941859079950928\n",
      "64 Train Loss 1.9292767 Test MSE 307.81734335534406 Test RE 0.894353323794809\n",
      "65 Train Loss 1.9244182 Test MSE 306.3504714746034 Test RE 0.8922198046172888\n",
      "66 Train Loss 1.9136618 Test MSE 304.3222887423979 Test RE 0.8892614448318766\n",
      "67 Train Loss 1.9084997 Test MSE 303.53633077630866 Test RE 0.8881123768965761\n",
      "68 Train Loss 1.9064281 Test MSE 303.2388399091576 Test RE 0.8876770581853062\n",
      "69 Train Loss 1.9054059 Test MSE 302.9686710181106 Test RE 0.8872815346971762\n",
      "70 Train Loss 1.9050997 Test MSE 302.785870897784 Test RE 0.8870138178342566\n",
      "71 Train Loss 1.9045372 Test MSE 302.4476956520055 Test RE 0.8865183357989638\n",
      "72 Train Loss 1.9025306 Test MSE 301.920513598746 Test RE 0.8857453750562996\n",
      "73 Train Loss 1.9001957 Test MSE 301.8529592724686 Test RE 0.8856462773189848\n",
      "74 Train Loss 1.895756 Test MSE 301.55803372704196 Test RE 0.8852135110732453\n",
      "75 Train Loss 1.8918215 Test MSE 300.97781993154075 Test RE 0.8843615019633136\n",
      "76 Train Loss 1.8896568 Test MSE 300.6695583699079 Test RE 0.8839085043030956\n",
      "77 Train Loss 1.8871952 Test MSE 300.1738847628145 Test RE 0.883179613025172\n",
      "78 Train Loss 1.8830469 Test MSE 299.6415073513605 Test RE 0.8823960779462444\n",
      "79 Train Loss 1.8799314 Test MSE 299.71507408119845 Test RE 0.8825043923959425\n",
      "80 Train Loss 1.8771187 Test MSE 299.61613085985346 Test RE 0.8823587123107384\n",
      "81 Train Loss 1.8752298 Test MSE 299.6044264234402 Test RE 0.8823414775706149\n",
      "82 Train Loss 1.8734535 Test MSE 299.44635628532177 Test RE 0.8821086868839957\n",
      "83 Train Loss 1.871623 Test MSE 299.093597747368 Test RE 0.8815889559523221\n",
      "84 Train Loss 1.8706875 Test MSE 298.86679049546086 Test RE 0.8812546313617448\n",
      "85 Train Loss 1.8691175 Test MSE 298.5831550024799 Test RE 0.8808363606939372\n",
      "86 Train Loss 1.8654958 Test MSE 298.3990151310545 Test RE 0.8805647075439983\n",
      "87 Train Loss 1.8621835 Test MSE 297.70289759520716 Test RE 0.8795369989807906\n",
      "88 Train Loss 1.859335 Test MSE 296.78322796205737 Test RE 0.8781774066704757\n",
      "89 Train Loss 1.8534026 Test MSE 295.48201192015125 Test RE 0.8762501518827227\n",
      "90 Train Loss 1.8522584 Test MSE 295.24376931275305 Test RE 0.8758968271294608\n",
      "91 Train Loss 1.8522226 Test MSE 295.2278906565199 Test RE 0.8758732732850387\n",
      "92 Train Loss 1.8522136 Test MSE 295.21823125350727 Test RE 0.8758589445539017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 Train Loss 1.8518542 Test MSE 295.01989724741094 Test RE 0.8755646846122135\n",
      "94 Train Loss 1.8487947 Test MSE 294.02885466342997 Test RE 0.8740928316226156\n",
      "95 Train Loss 1.8398497 Test MSE 292.82810224114075 Test RE 0.8723061994315203\n",
      "96 Train Loss 1.8360045 Test MSE 292.6476400425866 Test RE 0.87203736841084\n",
      "97 Train Loss 1.8334465 Test MSE 292.66085417422687 Test RE 0.8720570560572254\n",
      "98 Train Loss 1.8298953 Test MSE 292.1161206070273 Test RE 0.871245092312969\n",
      "99 Train Loss 1.8277386 Test MSE 291.7232741977919 Test RE 0.8706590571174344\n",
      "Training time: 43.11\n",
      "Training time: 43.11\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 4.435661 Test MSE 386.3010197904591 Test RE 1.0019026143447354\n",
      "1 Train Loss 3.3496523 Test MSE 384.54745422360713 Test RE 0.9996260215019184\n",
      "2 Train Loss 2.649419 Test MSE 384.66830945332634 Test RE 0.9997830899477098\n",
      "3 Train Loss 2.475064 Test MSE 384.0880354803703 Test RE 0.9990287165750443\n",
      "4 Train Loss 2.394098 Test MSE 384.4560319583218 Test RE 0.9995071889539279\n",
      "5 Train Loss 2.3878825 Test MSE 384.3389572864542 Test RE 0.9993549922444004\n",
      "6 Train Loss 2.3836708 Test MSE 384.11476655512394 Test RE 0.9990634802805093\n",
      "7 Train Loss 2.3823307 Test MSE 384.0191802707453 Test RE 0.9989391649507647\n",
      "8 Train Loss 2.3818035 Test MSE 383.98739772679914 Test RE 0.9988978265405051\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b065e1f1f657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-33fd3811f7f0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mx_coll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolloc_pts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mf_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_bc2_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbc2_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a9ac8e8143dc>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_coll, f_hat)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a9ac8e8143dc>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_bc2_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbc2_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    " \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
