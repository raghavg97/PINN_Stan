{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"1D_SODE_tanh\"\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "\n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.3177376 Test MSE 387.0594648563399 Test RE 1.0028856760493225\n",
      "1 Train Loss 2.4640522 Test MSE 384.79244941356643 Test RE 0.9999444016780717\n",
      "2 Train Loss 2.3845737 Test MSE 383.9334758980542 Test RE 0.9988276884462488\n",
      "3 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "4 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "5 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "6 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "7 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "8 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "9 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "10 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "11 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "12 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "13 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "14 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "15 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "16 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "17 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "18 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "19 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "20 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "21 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "22 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "23 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "24 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "25 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "26 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "27 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "28 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "29 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "30 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "31 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "32 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "33 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "34 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "35 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "36 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "37 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "38 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "39 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "40 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "41 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "42 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "43 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "44 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "45 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "46 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "47 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "48 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "49 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "50 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "51 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "52 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "53 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "54 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "55 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "56 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "57 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "58 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "59 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "60 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "61 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "62 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "63 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "64 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "65 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "66 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "67 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "68 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "69 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "70 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "71 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "72 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "73 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "74 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "75 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "76 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "77 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "78 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "79 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "80 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "81 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "82 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "83 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "84 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "85 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "86 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "87 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "88 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "89 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "90 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "91 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "92 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "93 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "94 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "95 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "96 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "97 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "99 Train Loss 2.3811092 Test MSE 383.8212663874822 Test RE 0.9986817176422458\n",
      "Training time: 6.34\n",
      "Training time: 6.34\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 4.299877 Test MSE 387.02055636389326 Test RE 1.0028352680965815\n",
      "1 Train Loss 3.569609 Test MSE 386.5351299047683 Test RE 1.0022061600220458\n",
      "2 Train Loss 2.3818064 Test MSE 383.643659126698 Test RE 0.9984506287687177\n",
      "3 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "4 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "5 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "6 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "7 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "8 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "9 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "10 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "11 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "12 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "13 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "14 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "15 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "16 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "17 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "18 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "19 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "20 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "21 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "22 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "23 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "24 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "25 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "26 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "27 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "28 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "29 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "30 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "31 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "32 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "33 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "34 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "35 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "36 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "37 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "38 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "39 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "40 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "41 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "42 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "43 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "44 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "45 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "46 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "47 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "48 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "49 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "50 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "51 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "52 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "53 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "54 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "55 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "56 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "57 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "58 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "59 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "60 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "61 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "62 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "63 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "64 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "65 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "66 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "67 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "68 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "69 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "70 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "71 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "72 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "73 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "74 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "75 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "76 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "77 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "78 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "79 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "80 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "81 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "82 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "83 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "84 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "85 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "86 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "87 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "88 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "89 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "90 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "91 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "92 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "93 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "94 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "95 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "97 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "98 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "99 Train Loss 2.3809154 Test MSE 383.7647236578968 Test RE 0.9986081544045393\n",
      "Training time: 6.66\n",
      "Training time: 6.66\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 3.8533778 Test MSE 387.17902574124076 Test RE 1.0030405574714347\n",
      "1 Train Loss 2.4368904 Test MSE 383.1604515920816 Test RE 0.9978216455079644\n",
      "2 Train Loss 2.3827617 Test MSE 383.96513470845684 Test RE 0.9988688688154417\n",
      "3 Train Loss 2.38251 Test MSE 383.96040321487953 Test RE 0.9988627144074372\n",
      "4 Train Loss 2.3825035 Test MSE 383.9608475508124 Test RE 0.9988632923712897\n",
      "5 Train Loss 2.3824937 Test MSE 383.96124925180027 Test RE 0.998863814878036\n",
      "6 Train Loss 2.3814783 Test MSE 383.8951847498036 Test RE 0.998777878758953\n",
      "7 Train Loss 2.381408 Test MSE 383.8625514525109 Test RE 0.998735426926489\n",
      "8 Train Loss 2.381408 Test MSE 383.8625514525109 Test RE 0.998735426926489\n",
      "9 Train Loss 2.381408 Test MSE 383.8625514525109 Test RE 0.998735426926489\n",
      "10 Train Loss 2.381408 Test MSE 383.8625514525109 Test RE 0.998735426926489\n",
      "11 Train Loss 2.381408 Test MSE 383.8625514525109 Test RE 0.998735426926489\n",
      "12 Train Loss 2.381408 Test MSE 383.8625514525109 Test RE 0.998735426926489\n",
      "13 Train Loss 2.381408 Test MSE 383.8625514525109 Test RE 0.998735426926489\n",
      "14 Train Loss 2.381408 Test MSE 383.8625514525109 Test RE 0.998735426926489\n",
      "15 Train Loss 2.381407 Test MSE 383.86253510820046 Test RE 0.9987354056641362\n",
      "16 Train Loss 2.381406 Test MSE 383.85153153367503 Test RE 0.9987210909847664\n",
      "17 Train Loss 2.3813884 Test MSE 383.8519719605465 Test RE 0.9987216639451992\n",
      "18 Train Loss 2.3810172 Test MSE 383.7430125126214 Test RE 0.9985799063243609\n",
      "19 Train Loss 2.381013 Test MSE 383.741110523702 Test RE 0.9985774316339975\n",
      "20 Train Loss 2.381013 Test MSE 383.741110523702 Test RE 0.9985774316339975\n",
      "21 Train Loss 2.381013 Test MSE 383.741110523702 Test RE 0.9985774316339975\n",
      "22 Train Loss 2.381013 Test MSE 383.741110523702 Test RE 0.9985774316339975\n",
      "23 Train Loss 2.3810127 Test MSE 383.74110953982995 Test RE 0.9985774303538734\n",
      "24 Train Loss 2.3810127 Test MSE 383.74110953982995 Test RE 0.9985774303538734\n",
      "25 Train Loss 2.3810127 Test MSE 383.74110953982995 Test RE 0.9985774303538734\n",
      "26 Train Loss 2.3810127 Test MSE 383.74110953982995 Test RE 0.9985774303538734\n",
      "27 Train Loss 2.3810127 Test MSE 383.74110953982995 Test RE 0.9985774303538734\n",
      "28 Train Loss 2.3810127 Test MSE 383.74110953982995 Test RE 0.9985774303538734\n",
      "29 Train Loss 2.3810124 Test MSE 383.74110961910026 Test RE 0.9985774304570127\n",
      "30 Train Loss 2.3810124 Test MSE 383.74110961910026 Test RE 0.9985774304570127\n",
      "31 Train Loss 2.3810124 Test MSE 383.74110961910026 Test RE 0.9985774304570127\n",
      "32 Train Loss 2.3810124 Test MSE 383.74110961910026 Test RE 0.9985774304570127\n",
      "33 Train Loss 2.3810124 Test MSE 383.74110961910026 Test RE 0.9985774304570127\n",
      "34 Train Loss 2.3810124 Test MSE 383.74110961910026 Test RE 0.9985774304570127\n",
      "35 Train Loss 2.3810124 Test MSE 383.74110961910026 Test RE 0.9985774304570127\n",
      "36 Train Loss 2.3810124 Test MSE 383.74110961910026 Test RE 0.9985774304570127\n",
      "37 Train Loss 2.3810124 Test MSE 383.74110961910026 Test RE 0.9985774304570127\n",
      "38 Train Loss 2.3810124 Test MSE 383.74110961910026 Test RE 0.9985774304570127\n",
      "39 Train Loss 2.3810124 Test MSE 383.74110961910026 Test RE 0.9985774304570127\n",
      "40 Train Loss 2.3810124 Test MSE 383.74110961910026 Test RE 0.9985774304570127\n",
      "41 Train Loss 2.3810124 Test MSE 383.74110961910026 Test RE 0.9985774304570127\n",
      "42 Train Loss 2.3810124 Test MSE 383.74110961910026 Test RE 0.9985774304570127\n",
      "43 Train Loss 2.3810043 Test MSE 383.7144637841427 Test RE 0.998542760740051\n",
      "44 Train Loss 2.3810034 Test MSE 383.7179541506979 Test RE 0.9985473022320446\n",
      "45 Train Loss 2.3810034 Test MSE 383.7179541506979 Test RE 0.9985473022320446\n",
      "46 Train Loss 2.3810034 Test MSE 383.7179541506979 Test RE 0.9985473022320446\n",
      "47 Train Loss 2.3810034 Test MSE 383.7179541506979 Test RE 0.9985473022320446\n",
      "48 Train Loss 2.3810034 Test MSE 383.7179541506979 Test RE 0.9985473022320446\n",
      "49 Train Loss 2.3810034 Test MSE 383.7179541506979 Test RE 0.9985473022320446\n",
      "50 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "51 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "52 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "53 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "54 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "55 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "56 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "57 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "58 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "59 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "60 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "61 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "62 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "63 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "64 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "65 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "66 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "67 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "68 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "69 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "70 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "71 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "72 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "73 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "74 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "75 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "76 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "77 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "78 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "79 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "80 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "81 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "82 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "83 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "84 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "85 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "86 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "87 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "88 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "89 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "90 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "91 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "93 Train Loss 2.3810022 Test MSE 383.71772038648754 Test RE 0.9985469980702839\n",
      "94 Train Loss 2.381002 Test MSE 383.71772167334314 Test RE 0.9985469997446731\n",
      "95 Train Loss 2.381002 Test MSE 383.71772167334314 Test RE 0.9985469997446731\n",
      "96 Train Loss 2.381002 Test MSE 383.71772167334314 Test RE 0.9985469997446731\n",
      "97 Train Loss 2.381002 Test MSE 383.71772167334314 Test RE 0.9985469997446731\n",
      "98 Train Loss 2.381002 Test MSE 383.71772167334314 Test RE 0.9985469997446731\n",
      "99 Train Loss 2.381002 Test MSE 383.71772167334314 Test RE 0.9985469997446731\n",
      "Training time: 18.81\n",
      "Training time: 18.81\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 3.8894794 Test MSE 388.15940541020643 Test RE 1.0043096588393754\n",
      "1 Train Loss 3.0068126 Test MSE 381.3541837392326 Test RE 0.9954669370784521\n",
      "2 Train Loss 2.3945553 Test MSE 383.1492064424029 Test RE 0.9978070031616078\n",
      "3 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "4 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "5 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "6 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "7 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "8 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "9 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "10 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "11 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "12 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "13 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "14 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "15 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "16 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "17 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "18 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "19 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "20 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "21 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "22 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "23 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "24 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "25 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "26 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "27 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "28 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "29 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "30 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "31 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "32 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "33 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "34 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "35 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "36 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "37 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "38 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "39 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "40 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "41 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "42 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "43 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "44 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "45 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "46 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "47 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "48 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "49 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "50 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "51 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "52 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "53 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "54 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "55 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "56 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "57 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "58 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "59 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "60 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "61 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "62 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "63 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "64 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "65 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "66 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "67 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "68 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "69 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "70 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "71 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "72 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "73 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "74 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "75 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "76 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "77 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "78 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "79 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "80 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "81 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "82 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "83 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "84 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "85 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "86 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "87 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "88 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "90 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "91 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "92 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "93 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "94 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "95 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "96 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "97 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "98 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "99 Train Loss 2.3806412 Test MSE 383.67203540025656 Test RE 0.9984875533778192\n",
      "Training time: 6.54\n",
      "Training time: 6.54\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 3.745811 Test MSE 385.46331909444945 Test RE 1.000815702875867\n",
      "1 Train Loss 2.399674 Test MSE 383.34050328347143 Test RE 0.9980560621513768\n",
      "2 Train Loss 2.3813312 Test MSE 383.8392899077968 Test RE 0.9987051654687452\n",
      "3 Train Loss 2.3812745 Test MSE 383.84064246318127 Test RE 0.9987069250630527\n",
      "4 Train Loss 2.3812745 Test MSE 383.84064246318127 Test RE 0.9987069250630527\n",
      "5 Train Loss 2.3812745 Test MSE 383.84064246318127 Test RE 0.9987069250630527\n",
      "6 Train Loss 2.3812742 Test MSE 383.84064246318127 Test RE 0.9987069250630527\n",
      "7 Train Loss 2.3812745 Test MSE 383.84064246318127 Test RE 0.9987069250630527\n",
      "8 Train Loss 2.3812745 Test MSE 383.84064246318127 Test RE 0.9987069250630527\n",
      "9 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "10 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "11 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "12 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "13 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "14 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "15 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "16 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "17 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "18 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "19 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "20 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "21 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "22 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "23 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "24 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "25 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "26 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "27 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "28 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "29 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "30 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "31 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "32 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "33 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "34 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "35 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "36 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "37 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "38 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "39 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "40 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "41 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "42 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "43 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "44 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "45 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "46 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "47 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "48 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "49 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "50 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "51 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "52 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "53 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "54 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "55 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "56 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "57 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "58 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "59 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "60 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "61 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "62 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "63 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "64 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "65 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "66 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "67 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "68 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "69 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "70 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "71 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "72 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "73 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "74 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "75 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "76 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "77 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "78 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "79 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "80 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "81 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "82 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "83 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "84 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "85 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "86 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "88 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "89 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "90 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "91 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "92 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "93 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "94 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "95 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "96 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "97 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "98 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "99 Train Loss 2.381274 Test MSE 383.8406379141078 Test RE 0.9987069191449833\n",
      "Training time: 20.72\n",
      "Training time: 20.72\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 4.0846376 Test MSE 386.1402731145719 Test RE 1.001694137969768\n",
      "1 Train Loss 2.5589614 Test MSE 382.483839729803 Test RE 0.996940244160251\n",
      "2 Train Loss 2.386414 Test MSE 383.4765329126546 Test RE 0.998233128170792\n",
      "3 Train Loss 2.381095 Test MSE 383.80944951502534 Test RE 0.9986663440994666\n",
      "4 Train Loss 2.381095 Test MSE 383.80944951502534 Test RE 0.9986663440994666\n",
      "5 Train Loss 2.3810914 Test MSE 383.80849019424915 Test RE 0.9986650960296599\n",
      "6 Train Loss 2.3810914 Test MSE 383.80849019424915 Test RE 0.9986650960296599\n",
      "7 Train Loss 2.3810914 Test MSE 383.80849019424915 Test RE 0.9986650960296599\n",
      "8 Train Loss 2.3810914 Test MSE 383.80849019424915 Test RE 0.9986650960296599\n",
      "9 Train Loss 2.3810914 Test MSE 383.80849019424915 Test RE 0.9986650960296599\n",
      "10 Train Loss 2.381091 Test MSE 383.80849252411934 Test RE 0.9986650990608071\n",
      "11 Train Loss 2.381091 Test MSE 383.80849252411934 Test RE 0.9986650990608071\n",
      "12 Train Loss 2.381091 Test MSE 383.80849252411934 Test RE 0.9986650990608071\n",
      "13 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "14 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "15 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "16 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "17 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "18 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "19 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "20 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "21 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "22 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "23 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "24 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "25 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "26 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "27 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "28 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "29 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "30 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "31 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "32 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "33 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "34 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "35 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "36 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "37 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "38 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "39 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "40 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "41 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "42 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "43 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "44 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "45 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "46 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "47 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "48 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "49 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "50 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "51 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "52 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "53 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "54 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "55 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "56 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "57 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "58 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "59 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "60 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "61 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "62 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "63 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "64 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "65 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "66 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "67 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "68 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "69 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "70 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "71 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "72 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "73 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "74 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "75 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "76 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "77 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "78 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "79 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "80 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "81 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "82 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "83 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "84 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "86 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "87 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "88 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "89 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "90 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "91 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "92 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "93 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "94 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "95 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "96 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "97 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "98 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "99 Train Loss 2.3810904 Test MSE 383.808493616344 Test RE 0.9986651004817848\n",
      "Training time: 17.35\n",
      "Training time: 17.35\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 4.3769217 Test MSE 386.80720614838197 Test RE 1.0025588168699195\n",
      "1 Train Loss 4.016939 Test MSE 386.1189750888225 Test RE 1.001666512773416\n",
      "2 Train Loss 2.384099 Test MSE 383.65126786725216 Test RE 0.998460529771827\n",
      "3 Train Loss 2.381725 Test MSE 383.63315356695153 Test RE 0.9984369580689751\n",
      "4 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "5 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "6 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "7 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "8 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "9 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "10 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "11 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "12 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "13 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "14 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "15 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "16 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "17 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "18 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "19 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "20 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "21 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "22 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "23 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "24 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "25 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "26 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "27 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "28 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "29 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "30 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "31 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "32 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "33 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "34 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "35 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "36 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "37 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "38 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "39 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "40 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "41 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "42 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "43 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "44 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "45 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "46 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "47 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "48 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "49 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "50 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "51 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "52 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "53 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "54 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "55 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "56 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "57 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "58 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "59 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "60 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "61 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "62 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "63 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "64 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "65 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "66 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "67 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "68 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "69 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "70 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "71 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "72 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "73 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "74 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "75 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "76 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "77 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "78 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "79 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "80 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "82 Train Loss 2.3810406 Test MSE 383.7578709780705 Test RE 0.9985992385609495\n",
      "83 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "84 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "85 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "86 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "87 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "88 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "89 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "90 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "91 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "92 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "93 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "94 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "95 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "96 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "97 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "98 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "99 Train Loss 2.3810403 Test MSE 383.7578933663244 Test RE 0.9985992676898542\n",
      "Training time: 20.56\n",
      "Training time: 20.56\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 4.3289404 Test MSE 387.1437598016318 Test RE 1.0029948757959108\n",
      "1 Train Loss 2.5510805 Test MSE 386.0439550334489 Test RE 1.001569199856297\n",
      "2 Train Loss 2.3845284 Test MSE 384.14939281994344 Test RE 0.9991085098655008\n",
      "3 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "4 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "5 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "6 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "7 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "8 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "9 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "10 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "11 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "12 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "13 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "14 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "15 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "16 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "17 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "18 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "19 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "20 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "21 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "22 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "23 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "24 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "25 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "26 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "27 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "28 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "29 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "30 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "31 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "32 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "33 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "34 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "35 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "36 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "37 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "38 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "39 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "40 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "41 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "42 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "43 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "44 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "45 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "46 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "47 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "48 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "49 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "50 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "51 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "52 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "53 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "54 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "55 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "56 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "57 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "58 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "59 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "60 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "61 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "62 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "63 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "64 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "65 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "66 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "67 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "68 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "69 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "70 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "71 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "72 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "73 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "74 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "75 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "76 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "78 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "79 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "80 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "81 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "82 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "83 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "84 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "85 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "86 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "87 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "88 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "89 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "90 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "91 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "92 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "93 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "94 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "95 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "96 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "97 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "98 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "99 Train Loss 2.3811867 Test MSE 383.84022536497184 Test RE 0.9987063824433595\n",
      "Training time: 19.93\n",
      "Training time: 19.93\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 4.2957 Test MSE 386.7851247093516 Test RE 1.002530200214141\n",
      "1 Train Loss 2.398517 Test MSE 384.57571466146067 Test RE 0.9996627521443197\n",
      "2 Train Loss 2.3816972 Test MSE 383.8154280626459 Test RE 0.9986741221141198\n",
      "3 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "4 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "5 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "6 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "7 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "8 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "9 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "10 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "11 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "12 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "13 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "14 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "15 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "16 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "17 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "18 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "19 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "20 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "21 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "22 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "23 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "24 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "25 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "26 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "27 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "28 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "29 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "30 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "31 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "32 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "33 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "34 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "35 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "36 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "37 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "38 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "39 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "40 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "41 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "42 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "43 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "44 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "45 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "46 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "47 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "48 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "49 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "50 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "51 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "52 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "53 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "54 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "55 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "56 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "57 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "58 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "59 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "60 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "61 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "62 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "63 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "64 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "65 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "66 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "67 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "68 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "69 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "70 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "71 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "72 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "73 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "75 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "76 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "77 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "78 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "79 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "80 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "81 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "82 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "83 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "84 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "85 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "86 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "87 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "88 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "89 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "90 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "91 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "92 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "93 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "94 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "95 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "96 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "97 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "98 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "99 Train Loss 2.3813286 Test MSE 383.85169479740284 Test RE 0.9987213033779675\n",
      "Training time: 6.31\n",
      "Training time: 6.31\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 4.351628 Test MSE 387.50127275843334 Test RE 1.0034578832810341\n",
      "1 Train Loss 3.1580095 Test MSE 384.57906496934083 Test RE 0.9996671065153501\n",
      "2 Train Loss 2.38499 Test MSE 383.58970976956954 Test RE 0.9983804234335087\n",
      "3 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "4 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "5 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "6 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "7 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "8 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "9 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "10 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "11 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "12 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "13 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "14 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "15 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "16 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "17 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "18 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "19 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "20 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "21 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "22 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "23 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "24 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "25 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "26 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "27 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "28 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "29 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "30 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "31 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "32 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "33 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "34 Train Loss 2.3809881 Test MSE 383.7900889609492 Test RE 0.9986411558483493\n",
      "35 Train Loss 2.3809876 Test MSE 383.7903896164888 Test RE 0.9986415470087069\n",
      "36 Train Loss 2.3809876 Test MSE 383.7903896164888 Test RE 0.9986415470087069\n",
      "37 Train Loss 2.3809867 Test MSE 383.7893428650659 Test RE 0.9986401851583302\n",
      "38 Train Loss 2.3809786 Test MSE 383.78300563715396 Test RE 0.9986319402230578\n",
      "39 Train Loss 2.3809721 Test MSE 383.77807281257475 Test RE 0.9986255224141177\n",
      "40 Train Loss 2.3808315 Test MSE 383.7170960778442 Test RE 0.9985461857520844\n",
      "41 Train Loss 2.3808315 Test MSE 383.7170960778442 Test RE 0.9985461857520844\n",
      "42 Train Loss 2.3808315 Test MSE 383.7170960778442 Test RE 0.9985461857520844\n",
      "43 Train Loss 2.3808284 Test MSE 383.7058758553553 Test RE 0.9985315864647603\n",
      "44 Train Loss 2.3808284 Test MSE 383.7058758553553 Test RE 0.9985315864647603\n",
      "45 Train Loss 2.3808122 Test MSE 383.69543870605304 Test RE 0.9985180058881019\n",
      "46 Train Loss 2.3808098 Test MSE 383.70914606950834 Test RE 0.9985358415535852\n",
      "47 Train Loss 2.3800695 Test MSE 383.44716389998445 Test RE 0.9981949019949821\n",
      "48 Train Loss 2.3790753 Test MSE 383.16969643048174 Test RE 0.9978336830814982\n",
      "49 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "50 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "51 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "52 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "53 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "54 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "55 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "56 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "57 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "58 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "59 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "60 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "61 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "62 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "63 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "64 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "65 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "66 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "67 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "68 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "69 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "71 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "72 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "73 Train Loss 2.3787093 Test MSE 383.0598570432949 Test RE 0.9976906533782158\n",
      "74 Train Loss 2.3787057 Test MSE 383.0782781640522 Test RE 0.9977146422649628\n",
      "75 Train Loss 2.3786998 Test MSE 383.05230517484915 Test RE 0.9976808187981098\n",
      "76 Train Loss 2.3786998 Test MSE 383.05230517484915 Test RE 0.9976808187981098\n",
      "77 Train Loss 2.3786998 Test MSE 383.05230517484915 Test RE 0.9976808187981098\n",
      "78 Train Loss 2.3786998 Test MSE 383.05230517484915 Test RE 0.9976808187981098\n",
      "79 Train Loss 2.3786945 Test MSE 383.0391788296758 Test RE 0.9976637245073079\n",
      "80 Train Loss 2.37869 Test MSE 383.0309847190216 Test RE 0.9976530532612224\n",
      "81 Train Loss 2.3777049 Test MSE 382.89731623879385 Test RE 0.9974789597685727\n",
      "82 Train Loss 2.3751698 Test MSE 382.3145688865208 Test RE 0.9967196183806464\n",
      "83 Train Loss 2.3745973 Test MSE 382.308490078711 Test RE 0.9967116944201465\n",
      "84 Train Loss 2.3742142 Test MSE 382.33001264083447 Test RE 0.9967397496268885\n",
      "85 Train Loss 2.373414 Test MSE 382.15622670048396 Test RE 0.9965131926768849\n",
      "86 Train Loss 2.37218 Test MSE 381.9210810305654 Test RE 0.9962065617718878\n",
      "87 Train Loss 2.3705432 Test MSE 381.71766519579296 Test RE 0.9959412305433749\n",
      "88 Train Loss 2.3684573 Test MSE 381.1132157670461 Test RE 0.9951523823098964\n",
      "89 Train Loss 2.3674808 Test MSE 380.43158689401685 Test RE 0.9942620587266443\n",
      "90 Train Loss 2.366366 Test MSE 379.69792926160613 Test RE 0.9933028849940895\n",
      "91 Train Loss 2.3655486 Test MSE 379.0795481991812 Test RE 0.9924937022996988\n",
      "92 Train Loss 2.3640635 Test MSE 378.3443325046861 Test RE 0.9915307763108975\n",
      "93 Train Loss 2.362849 Test MSE 378.3942225636512 Test RE 0.9915961478441874\n",
      "94 Train Loss 2.3596623 Test MSE 377.6501081954432 Test RE 0.9906206783389868\n",
      "95 Train Loss 2.3553267 Test MSE 376.60794766231663 Test RE 0.9892528791812653\n",
      "96 Train Loss 2.3475654 Test MSE 376.16429578221613 Test RE 0.9886700273957233\n",
      "97 Train Loss 2.3133912 Test MSE 368.9516338713489 Test RE 0.9791456559816701\n",
      "98 Train Loss 2.2686641 Test MSE 359.91699691184266 Test RE 0.967083026104317\n",
      "99 Train Loss 2.2474833 Test MSE 350.62744524048435 Test RE 0.9545211069034253\n",
      "Training time: 24.61\n",
      "Training time: 24.61\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    " \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9942244505210651\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
