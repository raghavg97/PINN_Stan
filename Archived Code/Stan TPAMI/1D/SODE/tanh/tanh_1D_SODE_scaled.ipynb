{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y/10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"1D_SODE_tanh_scaled\"\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0/10.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "\n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a/50\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 0.036655657 Test MSE 3.8837616761265634 Test RE 1.00459004105692\n",
      "1 Train Loss 0.024073191 Test MSE 3.8470622394994995 Test RE 0.9998323601080702\n",
      "2 Train Loss 0.023913605 Test MSE 3.842426539910834 Test RE 0.9992297808825481\n",
      "3 Train Loss 0.02390569 Test MSE 3.8420818159495105 Test RE 0.9991849568409049\n",
      "4 Train Loss 0.023898479 Test MSE 3.8418772785648003 Test RE 0.9991583601405025\n",
      "5 Train Loss 0.02389109 Test MSE 3.841586500058109 Test RE 0.9991205479913333\n",
      "6 Train Loss 0.02388241 Test MSE 3.841302259082927 Test RE 0.9990835845841269\n",
      "7 Train Loss 0.023873398 Test MSE 3.8409814250252525 Test RE 0.9990418608779317\n",
      "8 Train Loss 0.023863908 Test MSE 3.8406853847593627 Test RE 0.999003359999586\n",
      "9 Train Loss 0.023854299 Test MSE 3.8404116667322374 Test RE 0.9989677608716252\n",
      "10 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "11 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "12 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "13 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "14 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "15 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "16 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "17 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "18 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "19 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "20 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "21 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "22 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "23 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "24 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "25 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "26 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "27 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "28 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "29 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "30 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "31 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "32 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "33 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "34 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "35 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "36 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "37 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "38 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "39 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "40 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "41 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "42 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "43 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "44 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "45 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "46 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "47 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "48 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "49 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "50 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "51 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "52 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "53 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "54 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "55 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "56 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "57 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "58 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "59 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "60 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "61 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "62 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "63 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "64 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "65 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "66 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "67 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "68 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "69 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "70 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "71 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "72 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "73 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "74 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "75 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "76 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "77 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "78 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "79 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "80 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "81 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "82 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "83 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "84 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "85 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "86 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "87 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "88 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "89 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "90 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "91 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "92 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "93 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "94 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "95 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "96 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "97 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "99 Train Loss 0.02382048 Test MSE 3.83941368269108 Test RE 0.9988379546495351\n",
      "Training time: 6.78\n",
      "Training time: 6.78\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 0.026567467 Test MSE 3.8492728941508454 Test RE 1.0001195879308467\n",
      "1 Train Loss 0.024157144 Test MSE 3.847276605374356 Test RE 0.9998602160328778\n",
      "2 Train Loss 0.02396054 Test MSE 3.841706971240923 Test RE 0.99913621395259\n",
      "3 Train Loss 0.023827268 Test MSE 3.839643291196146 Test RE 0.9988678209624409\n",
      "4 Train Loss 0.023822377 Test MSE 3.8394753998057407 Test RE 0.9988459825873843\n",
      "5 Train Loss 0.023818864 Test MSE 3.83927046206769 Test RE 0.9988193247830431\n",
      "6 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "7 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "8 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "9 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "10 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "11 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "12 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "13 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "14 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "15 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "16 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "17 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "18 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "19 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "20 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "21 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "22 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "23 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "24 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "25 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "26 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "27 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "28 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "29 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "30 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "31 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "32 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "33 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "34 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "35 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "36 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "37 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "38 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "39 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "40 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "41 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "42 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "43 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "44 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "45 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "46 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "47 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "48 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "49 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "50 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "51 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "52 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "53 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "54 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "55 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "56 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "57 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "58 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "59 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "60 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "61 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "62 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "63 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "64 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "65 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "66 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "67 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "68 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "69 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "70 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "71 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "72 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "73 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "74 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "75 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "76 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "77 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "78 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "79 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "80 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "81 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "82 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "83 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "84 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "85 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "86 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "87 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "88 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "89 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "90 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "91 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "93 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "94 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "95 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "96 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "97 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "98 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "99 Train Loss 0.023816258 Test MSE 3.8391376291908363 Test RE 0.9988020458242806\n",
      "Training time: 6.76\n",
      "Training time: 6.76\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 0.02622377 Test MSE 3.835493181394581 Test RE 0.9983278577971266\n",
      "1 Train Loss 0.024025364 Test MSE 3.8429620004403504 Test RE 0.9992994021834298\n",
      "2 Train Loss 0.024018964 Test MSE 3.8429720065687536 Test RE 0.9993007031475845\n",
      "3 Train Loss 0.024013499 Test MSE 3.8429880626649817 Test RE 0.9993027907052694\n",
      "4 Train Loss 0.02400821 Test MSE 3.843010735666512 Test RE 0.9993057385624986\n",
      "5 Train Loss 0.024002519 Test MSE 3.8430410551128915 Test RE 0.9993096805677495\n",
      "6 Train Loss 0.0239959 Test MSE 3.8430795500765806 Test RE 0.9993146854965262\n",
      "7 Train Loss 0.023987832 Test MSE 3.8431244954720127 Test RE 0.9993205290474854\n",
      "8 Train Loss 0.023849152 Test MSE 3.841266655624983 Test RE 0.9990789545250466\n",
      "9 Train Loss 0.023845349 Test MSE 3.841217064640239 Test RE 0.9990725054193814\n",
      "10 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "11 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "12 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "13 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "14 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "15 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "16 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "17 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "18 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "19 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "20 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "21 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "22 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "23 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "24 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "25 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "26 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "27 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "28 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "29 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "30 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "31 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "32 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "33 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "34 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "35 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "36 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "37 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "38 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "39 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "40 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "41 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "42 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "43 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "44 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "45 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "46 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "47 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "48 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "49 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "50 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "51 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "52 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "53 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "54 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "55 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "56 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "57 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "58 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "59 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "60 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "61 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "62 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "63 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "64 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "65 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "66 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "67 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "68 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "69 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "70 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "71 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "72 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "73 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "74 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "75 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "76 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "77 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "78 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "79 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "80 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "81 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "82 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "83 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "84 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "85 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "86 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "88 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "89 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "90 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "91 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "92 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "93 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "94 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "95 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "96 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "97 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "98 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "99 Train Loss 0.023842957 Test MSE 3.8411615038408486 Test RE 0.9990652799141777\n",
      "Training time: 5.90\n",
      "Training time: 5.90\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 0.042735033 Test MSE 3.8744090812943393 Test RE 1.003379721315771\n",
      "1 Train Loss 0.02605124 Test MSE 3.8485452756853546 Test RE 1.0000250584064108\n",
      "2 Train Loss 0.024363024 Test MSE 3.850690451745825 Test RE 1.0003037261636347\n",
      "3 Train Loss 0.02393067 Test MSE 3.8442405927375667 Test RE 0.9994656268637852\n",
      "4 Train Loss 0.023926899 Test MSE 3.843855103605197 Test RE 0.9994155138717276\n",
      "5 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "6 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "7 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "8 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "9 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "10 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "11 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "12 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "13 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "14 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "15 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "16 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "17 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "18 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "19 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "20 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "21 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "22 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "23 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "24 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "25 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "26 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "27 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "28 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "29 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "30 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "31 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "32 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "33 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "34 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "35 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "36 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "37 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "38 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "39 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "40 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "41 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "42 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "43 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "44 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "45 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "46 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "47 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "48 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "49 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "50 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "51 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "52 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "53 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "54 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "55 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "56 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "57 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "58 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "59 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "60 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "61 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "62 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "63 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "64 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "65 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "66 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "67 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "68 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "69 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "70 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "71 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "72 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "73 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "74 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "75 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "76 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "77 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "78 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "79 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "80 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "81 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "82 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "83 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "84 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "85 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "86 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "88 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "89 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "90 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "91 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "92 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "93 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "94 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "95 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "96 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "97 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "98 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "99 Train Loss 0.02392435 Test MSE 3.843539627647774 Test RE 0.999374500614583\n",
      "Training time: 5.94\n",
      "Training time: 5.94\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 0.03230372 Test MSE 3.859817855316191 Test RE 1.0014885489815657\n",
      "1 Train Loss 0.024058092 Test MSE 3.8474886468380407 Test RE 0.9998877691469726\n",
      "2 Train Loss 0.023860019 Test MSE 3.841837601749241 Test RE 0.9991532007455699\n",
      "3 Train Loss 0.023857314 Test MSE 3.841648931209005 Test RE 0.9991286665111518\n",
      "4 Train Loss 0.023854477 Test MSE 3.8414854098984623 Test RE 0.9991074021308711\n",
      "5 Train Loss 0.023851527 Test MSE 3.841199143312961 Test RE 0.9990701748134918\n",
      "6 Train Loss 0.023847148 Test MSE 3.8410043967277816 Test RE 0.9990448483511304\n",
      "7 Train Loss 0.023840614 Test MSE 3.8405529444652506 Test RE 0.9989861352823551\n",
      "8 Train Loss 0.023833394 Test MSE 3.8400242058591556 Test RE 0.9989173664455092\n",
      "9 Train Loss 0.023824928 Test MSE 3.839292248555733 Test RE 0.9988221587504098\n",
      "10 Train Loss 0.023819419 Test MSE 3.83883081139293 Test RE 0.9987621336881652\n",
      "11 Train Loss 0.023816623 Test MSE 3.83863249682446 Test RE 0.9987363352550727\n",
      "12 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "13 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "14 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "15 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "16 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "17 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "18 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "19 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "20 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "21 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "22 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "23 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "24 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "25 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "26 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "27 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "28 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "29 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "30 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "31 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "32 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "33 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "34 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "35 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "36 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "37 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "38 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "39 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "40 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "41 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "42 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "43 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "44 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "45 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "46 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "47 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "48 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "49 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "50 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "51 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "52 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "53 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "54 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "55 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "56 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "57 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "58 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "59 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "60 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "61 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "62 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "63 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "64 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "65 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "66 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "67 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "68 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "69 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "70 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "71 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "72 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "73 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "74 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "75 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "76 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "77 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "78 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "79 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "81 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "82 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "83 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "84 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "85 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "86 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "87 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "88 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "89 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "90 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "91 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "92 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "93 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "94 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "95 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "96 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "97 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "98 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "99 Train Loss 0.023814902 Test MSE 3.8388103468902957 Test RE 0.9987594715237509\n",
      "Training time: 6.07\n",
      "Training time: 6.07\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 0.040525723 Test MSE 3.8918696397195185 Test RE 1.0056381142271225\n",
      "1 Train Loss 0.025199555 Test MSE 3.844857977682997 Test RE 0.9995458807193709\n",
      "2 Train Loss 0.02398199 Test MSE 3.8450048227599725 Test RE 0.9995649681591687\n",
      "3 Train Loss 0.023974922 Test MSE 3.8449681927891004 Test RE 0.9995602069007882\n",
      "4 Train Loss 0.023968596 Test MSE 3.844904785051307 Test RE 0.9995519649453763\n",
      "5 Train Loss 0.023962308 Test MSE 3.8448095773385846 Test RE 0.9995395893924333\n",
      "6 Train Loss 0.023955477 Test MSE 3.844673337714695 Test RE 0.9995218800493109\n",
      "7 Train Loss 0.023947615 Test MSE 3.844483607504976 Test RE 0.9994972171171261\n",
      "8 Train Loss 0.023938298 Test MSE 3.844226585988069 Test RE 0.9994638060519654\n",
      "9 Train Loss 0.023879558 Test MSE 3.841987154279191 Test RE 0.9991726477459782\n",
      "10 Train Loss 0.023874767 Test MSE 3.841674397277292 Test RE 0.9991319780886981\n",
      "11 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "12 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "13 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "14 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "15 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "16 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "17 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "18 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "19 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "20 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "21 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "22 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "23 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "24 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "25 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "26 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "27 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "28 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "29 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "30 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "31 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "32 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "33 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "34 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "35 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "36 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "37 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "38 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "39 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "40 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "41 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "42 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "43 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "44 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "45 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "46 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "47 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "48 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "49 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "50 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "51 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "52 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "53 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "54 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "55 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "56 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "57 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "58 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "59 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "60 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "61 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "62 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "63 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "64 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "65 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "66 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "67 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "68 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "69 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "70 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "71 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "72 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "73 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "75 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "76 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "77 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "78 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "79 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "80 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "81 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "82 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "83 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "84 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "85 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "86 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "87 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "88 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "89 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "90 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "91 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "92 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "93 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "94 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "95 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "96 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "97 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "98 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "99 Train Loss 0.023872074 Test MSE 3.8414363198625523 Test RE 0.9991010183535421\n",
      "Training time: 5.78\n",
      "Training time: 5.78\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 0.03198942 Test MSE 3.866766072348406 Test RE 1.0023895540642305\n",
      "1 Train Loss 0.024034875 Test MSE 3.844961682587704 Test RE 0.9995593606855944\n",
      "2 Train Loss 0.023836639 Test MSE 3.8406960959463676 Test RE 0.9990047530457844\n",
      "3 Train Loss 0.023832513 Test MSE 3.840580257698328 Test RE 0.9989896875693611\n",
      "4 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "5 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "6 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "7 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "8 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "9 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "10 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "11 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "12 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "13 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "14 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "15 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "16 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "17 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "18 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "19 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "20 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "21 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "22 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "23 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "24 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "25 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "26 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "27 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "28 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "29 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "30 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "31 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "32 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "33 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "34 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "35 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "36 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "37 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "38 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "39 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "40 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "41 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "42 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "43 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "44 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "45 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "46 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "47 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "48 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "49 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "50 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "51 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "52 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "53 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "54 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "55 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "56 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "57 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "58 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "59 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "60 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "61 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "62 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "63 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "64 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "65 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "66 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "67 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "68 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "70 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "71 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "72 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "73 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "74 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "75 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "76 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "77 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "78 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "79 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "80 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "81 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "82 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "83 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "84 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "85 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "86 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "87 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "88 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "89 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "90 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "91 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "92 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "93 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "94 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "95 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "96 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "97 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "98 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "99 Train Loss 0.02382989 Test MSE 3.8404698695386643 Test RE 0.9989753306990248\n",
      "Training time: 6.02\n",
      "Training time: 6.02\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 0.041629046 Test MSE 3.883814995042777 Test RE 1.0045969368802041\n",
      "1 Train Loss 0.026503094 Test MSE 3.8588740669862487 Test RE 1.0013661013653334\n",
      "2 Train Loss 0.024255203 Test MSE 3.8471623466625924 Test RE 0.9998453687005733\n",
      "3 Train Loss 0.023833005 Test MSE 3.840450313635442 Test RE 0.9989727872745973\n",
      "4 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "5 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "6 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "7 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "8 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "9 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "10 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "11 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "12 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "13 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "14 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "15 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "16 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "17 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "18 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "19 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "20 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "21 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "22 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "23 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "24 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "25 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "26 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "27 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "28 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "29 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "30 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "31 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "32 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "33 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "34 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "35 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "36 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "37 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "38 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "39 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "40 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "41 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "42 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "43 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "44 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "45 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "46 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "47 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "48 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "49 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "50 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "51 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "52 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "53 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "54 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "55 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "56 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "57 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "58 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "59 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "60 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "61 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "62 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "63 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "65 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "66 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "67 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "68 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "69 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "70 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "71 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "72 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "73 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "74 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "75 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "76 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "77 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "78 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "79 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "80 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "81 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "82 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "83 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "84 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "85 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "86 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "87 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "88 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "89 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "90 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "91 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "92 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "93 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "94 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "95 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "96 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "97 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "98 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "99 Train Loss 0.023829479 Test MSE 3.8403070165747226 Test RE 0.9989541499810756\n",
      "Training time: 6.82\n",
      "Training time: 6.82\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 0.042458203 Test MSE 3.8766849391116374 Test RE 1.0036743745505428\n",
      "1 Train Loss 0.024773147 Test MSE 3.854330549553927 Test RE 1.0007764132487527\n",
      "2 Train Loss 0.023966443 Test MSE 3.8456817568226445 Test RE 0.9996529537179769\n",
      "3 Train Loss 0.023902677 Test MSE 3.8434452369588596 Test RE 0.9993622290841839\n",
      "4 Train Loss 0.023895888 Test MSE 3.8430198338159642 Test RE 0.9993069214667633\n",
      "5 Train Loss 0.023890011 Test MSE 3.8426184875449407 Test RE 0.9992547387308124\n",
      "6 Train Loss 0.023883954 Test MSE 3.8422073187107593 Test RE 0.999201276046617\n",
      "7 Train Loss 0.023875946 Test MSE 3.8417274229756244 Test RE 0.9991388734528767\n",
      "8 Train Loss 0.023826562 Test MSE 3.839462534622109 Test RE 0.9988443091364034\n",
      "9 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "10 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "11 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "12 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "13 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "14 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "15 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "16 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "17 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "18 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "19 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "20 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "21 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "22 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "23 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "24 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "25 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "26 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "27 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "28 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "29 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "30 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "31 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "32 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "33 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "34 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "35 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "36 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "37 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "38 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "39 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "40 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "41 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "42 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "43 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "44 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "45 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "46 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "47 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "48 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "49 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "50 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "51 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "52 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "53 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "54 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "55 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "56 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "58 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "59 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "60 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "61 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "62 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "63 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "64 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "65 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "66 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "67 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "68 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "69 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "70 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "71 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "72 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "73 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "74 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "75 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "76 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "77 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "78 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "79 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "80 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "81 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "82 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "83 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "84 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "85 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "86 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "87 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "88 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "89 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "90 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "91 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "92 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "93 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "94 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "95 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "96 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "97 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "98 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "99 Train Loss 0.023823004 Test MSE 3.8394553640134608 Test RE 0.9988433764114555\n",
      "Training time: 6.91\n",
      "Training time: 6.91\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 0.03648468 Test MSE 3.8576703579251768 Test RE 1.001209909761164\n",
      "1 Train Loss 0.024254194 Test MSE 3.851053258786889 Test RE 1.0003508487110855\n",
      "2 Train Loss 0.023876183 Test MSE 3.8427046470001476 Test RE 0.9992659413471506\n",
      "3 Train Loss 0.023860294 Test MSE 3.84205072027273 Test RE 0.9991809134086177\n",
      "4 Train Loss 0.023856409 Test MSE 3.841760063965515 Test RE 0.9991431180034399\n",
      "5 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "6 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "7 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "8 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "9 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "10 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "11 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "12 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "13 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "14 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "15 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "16 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "17 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "18 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "19 Train Loss 0.023853594 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "20 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "21 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "22 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "23 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "24 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "25 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "26 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "27 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "28 Train Loss 0.023853594 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "29 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "30 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "31 Train Loss 0.023853594 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "32 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "33 Train Loss 0.023853594 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "34 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "35 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "36 Train Loss 0.023853594 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "37 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "38 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "39 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "40 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "41 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "42 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "43 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "44 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "45 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "46 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "47 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "48 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "49 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "50 Train Loss 0.023853594 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "51 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "52 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "53 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "55 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "56 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "57 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "58 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "59 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "60 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "61 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "62 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "63 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "64 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "65 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "66 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "67 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "68 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "69 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "70 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "71 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "72 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "73 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "74 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "75 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "76 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "77 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "78 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "79 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "80 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "81 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "82 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "83 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "84 Train Loss 0.023853594 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "85 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "86 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "87 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "88 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "89 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "90 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "91 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "92 Train Loss 0.023853594 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "93 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "94 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "95 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "96 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "97 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "98 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "99 Train Loss 0.023853593 Test MSE 3.8414905235042256 Test RE 0.9991080671131795\n",
      "Training time: 6.42\n",
      "Training time: 6.42\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    " \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9989821195084605\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
