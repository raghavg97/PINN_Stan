{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"1D_SODE_tanh\"\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "\n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.3680706 Test MSE 386.0759172229266 Test RE 1.001610661044255\n",
      "1 Train Loss 3.7148273 Test MSE 386.52098520649474 Test RE 1.0021878227063223\n",
      "2 Train Loss 3.4524229 Test MSE 385.40664717340957 Test RE 1.0007421287761424\n",
      "3 Train Loss 2.4228966 Test MSE 385.034690202943 Test RE 1.0002591027732213\n",
      "4 Train Loss 2.4148123 Test MSE 384.6595593365403 Test RE 0.999771718763325\n",
      "5 Train Loss 2.4120722 Test MSE 384.3808619726394 Test RE 0.9994094708694783\n",
      "6 Train Loss 2.3860297 Test MSE 383.95883784864395 Test RE 0.9988606782761752\n",
      "7 Train Loss 2.3811405 Test MSE 383.8559830611567 Test RE 0.9987268820538755\n",
      "8 Train Loss 2.381041 Test MSE 383.8145817467679 Test RE 0.9986730210715737\n",
      "9 Train Loss 2.3810372 Test MSE 383.81155760414913 Test RE 0.9986690867036028\n",
      "10 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "11 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "12 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "13 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "14 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "15 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "16 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "17 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "18 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "19 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "20 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "21 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "22 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "23 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "24 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "25 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "26 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "27 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "28 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "29 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "30 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "31 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "32 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "33 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "34 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "35 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "36 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "37 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "38 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "39 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "40 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "41 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "42 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "43 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "44 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "45 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "46 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "47 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "48 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "49 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "50 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "51 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "52 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "53 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "54 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "55 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "56 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "57 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "58 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "59 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "60 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "61 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "62 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "63 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "64 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "65 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "66 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "67 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "68 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "69 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "70 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "71 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "72 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "73 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "74 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "75 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "76 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "77 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "78 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "79 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "80 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "81 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "82 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "83 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "84 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "85 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "86 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "87 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "88 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "89 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "90 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "91 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "92 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "93 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "94 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "95 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "96 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "98 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "99 Train Loss 2.3810344 Test MSE 383.80891734084855 Test RE 0.9986656517446741\n",
      "Training time: 22.36\n",
      "Training time: 22.36\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 4.345121 Test MSE 386.070539269312 Test RE 1.001603684910555\n",
      "1 Train Loss 3.858025 Test MSE 388.31167356342587 Test RE 1.0045066260910136\n",
      "2 Train Loss 3.4203756 Test MSE 385.2438032040551 Test RE 1.0005306871215118\n",
      "3 Train Loss 2.4108782 Test MSE 384.2322276656471 Test RE 0.9992162238692278\n",
      "4 Train Loss 2.3819292 Test MSE 383.78736946066834 Test RE 0.9986376177036799\n",
      "5 Train Loss 2.381567 Test MSE 383.71608776277094 Test RE 0.9985448737831337\n",
      "6 Train Loss 2.3812723 Test MSE 383.6659424602736 Test RE 0.9984796250584101\n",
      "7 Train Loss 2.381268 Test MSE 383.6653456056457 Test RE 0.9984788484095719\n",
      "8 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "9 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "10 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "11 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "12 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "13 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "14 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "15 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "16 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "17 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "18 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "19 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "20 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "21 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "22 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "23 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "24 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "25 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "26 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "27 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "28 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "29 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "30 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "31 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "32 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "33 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "34 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "35 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "36 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "37 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "38 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "39 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "40 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "41 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "42 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "43 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "44 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "45 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "46 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "47 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "48 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "49 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "50 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "51 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "52 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "53 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "54 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "55 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "56 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "57 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "58 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "59 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "60 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "61 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "62 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "63 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "64 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "65 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "66 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "67 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "68 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "69 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "70 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "71 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "72 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "73 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "74 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "75 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "76 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "77 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "78 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "79 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "80 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "81 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "82 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "83 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "84 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "85 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "86 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "87 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "88 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "89 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "90 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "91 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "92 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "94 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "95 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "96 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "97 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "98 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "99 Train Loss 2.3812656 Test MSE 383.66502013975037 Test RE 0.9984784249013179\n",
      "Training time: 8.33\n",
      "Training time: 8.33\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 4.3090944 Test MSE 386.66578997307965 Test RE 1.0023755330634447\n",
      "1 Train Loss 3.7856784 Test MSE 387.8033364316049 Test RE 1.0038489131291277\n",
      "2 Train Loss 3.2338872 Test MSE 387.0561947178464 Test RE 1.0028814395142693\n",
      "3 Train Loss 2.3875766 Test MSE 383.4820650811066 Test RE 0.9982403285773733\n",
      "4 Train Loss 2.3829987 Test MSE 383.57945149794847 Test RE 0.9983670735888992\n",
      "5 Train Loss 2.3809652 Test MSE 383.76014650240626 Test RE 0.9986021991953394\n",
      "6 Train Loss 2.380961 Test MSE 383.7620475554957 Test RE 0.9986046726067425\n",
      "7 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "8 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "9 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "10 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "11 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "12 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "13 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "14 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "15 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "16 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "17 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "18 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "19 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "20 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "21 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "22 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "23 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "24 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "25 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "26 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "27 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "28 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "29 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "30 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "31 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "32 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "33 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "34 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "35 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "36 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "37 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "38 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "39 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "40 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "41 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "42 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "43 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "44 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "45 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "46 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "47 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "48 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "49 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "50 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "51 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "52 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "53 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "54 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "55 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "56 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "57 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "58 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "59 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "60 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "61 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "62 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "63 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "64 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "65 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "66 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "67 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "68 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "69 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "70 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "71 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "72 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "73 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "74 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "75 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "76 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "77 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "78 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "79 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "80 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "81 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "82 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "83 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "84 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "85 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "86 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "87 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "89 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "90 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "91 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "92 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "93 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "94 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "95 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "96 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "97 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "98 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "99 Train Loss 2.3809586 Test MSE 383.76340881050766 Test RE 0.9986064436969516\n",
      "Training time: 9.62\n",
      "Training time: 9.62\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 4.330005 Test MSE 386.3354839592029 Test RE 1.0019473061380277\n",
      "1 Train Loss 3.6555629 Test MSE 385.90980316636427 Test RE 1.0013951600205193\n",
      "2 Train Loss 3.385395 Test MSE 385.11072393991657 Test RE 1.0003578596972298\n",
      "3 Train Loss 2.3974962 Test MSE 383.47048793316526 Test RE 0.9982252602544424\n",
      "4 Train Loss 2.3815103 Test MSE 383.4952182737168 Test RE 0.9982574479369501\n",
      "5 Train Loss 2.3810687 Test MSE 383.5490785169518 Test RE 0.9983275459443222\n",
      "6 Train Loss 2.3806384 Test MSE 383.62820940355925 Test RE 0.9984305242515161\n",
      "7 Train Loss 2.380632 Test MSE 383.62960027505125 Test RE 0.9984323341904887\n",
      "8 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "9 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "10 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "11 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "12 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "13 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "14 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "15 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "16 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "17 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "18 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "19 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "20 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "21 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "22 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "23 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "24 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "25 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "26 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "27 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "28 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "29 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "30 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "31 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "32 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "33 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "34 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "35 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "36 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "37 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "38 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "39 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "40 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "41 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "42 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "43 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "44 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "45 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "46 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "47 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "48 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "49 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "50 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "51 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "52 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "53 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "54 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "55 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "56 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "57 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "58 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "59 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "60 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "61 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "62 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "63 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "64 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "65 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "66 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "67 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "68 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "69 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "70 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "71 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "72 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "73 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "74 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "75 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "76 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "77 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "78 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "79 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "80 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "81 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "82 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "83 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "84 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "85 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "87 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "88 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "89 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "90 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "91 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "92 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "93 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "94 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "95 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "96 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "97 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "98 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "99 Train Loss 2.3806286 Test MSE 383.6304211318412 Test RE 0.9984334023686505\n",
      "Training time: 10.40\n",
      "Training time: 10.40\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 4.270591 Test MSE 386.8452371310591 Test RE 1.0026081015771862\n",
      "1 Train Loss 3.3855014 Test MSE 385.0787834210357 Test RE 1.0003163747298447\n",
      "2 Train Loss 2.4766 Test MSE 382.98951520410344 Test RE 0.9975990454789615\n",
      "3 Train Loss 2.384137 Test MSE 383.58409473454014 Test RE 0.9983731161968666\n",
      "4 Train Loss 2.383265 Test MSE 383.63932532294626 Test RE 0.9984449892889581\n",
      "5 Train Loss 2.3812487 Test MSE 383.82227016721714 Test RE 0.9986830235310831\n",
      "6 Train Loss 2.3811412 Test MSE 383.8274505975975 Test RE 0.998689763096268\n",
      "7 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "8 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "9 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "10 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "11 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "12 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "13 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "14 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "15 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "16 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "17 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "18 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "19 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "20 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "21 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "22 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "23 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "24 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "25 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "26 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "27 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "28 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "29 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "30 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "31 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "32 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "33 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "34 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "35 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "36 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "37 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "38 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "39 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "40 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "41 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "42 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "43 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "44 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "45 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "46 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "47 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "48 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "49 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "50 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "51 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "52 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "53 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "54 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "55 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "56 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "57 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "58 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "59 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "60 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "61 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "62 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "63 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "64 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "65 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "66 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "67 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "68 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "69 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "70 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "71 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "72 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "73 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "74 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "75 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "76 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "77 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "78 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "79 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "80 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "81 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "82 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "83 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "85 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "86 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "87 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "88 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "89 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "90 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "91 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "92 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "93 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "94 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "95 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "96 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "97 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "98 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "99 Train Loss 2.3811367 Test MSE 383.8271025537772 Test RE 0.9986893103044228\n",
      "Training time: 10.03\n",
      "Training time: 10.03\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 4.353035 Test MSE 386.8636024851913 Test RE 1.0026319005444968\n",
      "1 Train Loss 3.4396896 Test MSE 385.1846478254655 Test RE 1.000453866880216\n",
      "2 Train Loss 2.4512026 Test MSE 382.43839881481125 Test RE 0.9968810217594841\n",
      "3 Train Loss 2.3851016 Test MSE 383.4379775623997 Test RE 0.9981829449235398\n",
      "4 Train Loss 2.3813856 Test MSE 383.7368086637262 Test RE 0.9985718344329062\n",
      "5 Train Loss 2.3809881 Test MSE 383.7520579697706 Test RE 0.9985916753445523\n",
      "6 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "7 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "8 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "9 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "10 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "11 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "12 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "13 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "14 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "15 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "16 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "17 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "18 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "19 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "20 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "21 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "22 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "23 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "24 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "25 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "26 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "27 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "28 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "29 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "30 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "31 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "32 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "33 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "34 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "35 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "36 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "37 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "38 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "39 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "40 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "41 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "42 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "43 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "44 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "45 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "46 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "47 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "48 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "49 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "50 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "51 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "52 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "53 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "54 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "55 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "56 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "57 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "58 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "59 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "60 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "61 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "62 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "63 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "64 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "65 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "66 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "67 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "68 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "69 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "70 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "71 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "72 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "73 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "74 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "75 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "76 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "77 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "78 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "79 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "81 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "82 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "83 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "84 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "85 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "86 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "87 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "88 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "89 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "90 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "91 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "92 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "93 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "94 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "95 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "96 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "97 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "98 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "99 Train Loss 2.3809838 Test MSE 383.75170459289797 Test RE 0.9985912155694483\n",
      "Training time: 8.78\n",
      "Training time: 8.78\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 4.385085 Test MSE 386.5686138551745 Test RE 1.0022495675832868\n",
      "1 Train Loss 3.9431558 Test MSE 386.0418485848856 Test RE 1.0015664673217597\n",
      "2 Train Loss 2.8628995 Test MSE 386.19916950017017 Test RE 1.0017705271978157\n",
      "3 Train Loss 2.3837035 Test MSE 383.8626308137338 Test RE 0.9987355301676888\n",
      "4 Train Loss 2.3817487 Test MSE 383.8918425125833 Test RE 0.9987735310098803\n",
      "5 Train Loss 2.381105 Test MSE 383.7782068571635 Test RE 0.9986256968122207\n",
      "6 Train Loss 2.3810987 Test MSE 383.77336727827833 Test RE 0.9986194002808042\n",
      "7 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "8 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "9 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "10 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "11 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "12 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "13 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "14 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "15 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "16 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "17 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "18 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "19 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "20 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "21 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "22 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "23 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "24 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "25 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "26 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "27 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "28 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "29 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "30 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "31 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "32 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "33 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "34 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "35 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "36 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "37 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "38 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "39 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "40 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "41 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "42 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "43 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "44 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "45 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "46 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "47 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "48 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "49 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "50 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "51 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "52 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "53 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "54 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "55 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "56 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "57 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "58 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "59 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "60 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "61 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "62 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "63 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "64 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "65 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "66 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "67 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "68 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "69 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "70 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "71 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "72 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "73 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "74 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "75 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "77 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "78 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "79 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "80 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "81 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "82 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "83 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "84 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "85 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "86 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "87 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "88 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "89 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "90 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "91 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "92 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "93 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "94 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "95 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "96 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "97 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "98 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "99 Train Loss 2.381093 Test MSE 383.76977958913403 Test RE 0.9986147324942141\n",
      "Training time: 22.78\n",
      "Training time: 22.78\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 4.3561306 Test MSE 386.2064340395711 Test RE 1.0017799489778425\n",
      "1 Train Loss 3.4823275 Test MSE 385.5147041601303 Test RE 1.0008824086612111\n",
      "2 Train Loss 2.3881905 Test MSE 383.45958898231237 Test RE 0.9982110744361071\n",
      "3 Train Loss 2.3834858 Test MSE 383.64608610784126 Test RE 0.9984537869298894\n",
      "4 Train Loss 2.381325 Test MSE 383.8406089190714 Test RE 0.9987068814241934\n",
      "5 Train Loss 2.3812878 Test MSE 383.84273740192424 Test RE 0.998709650447944\n",
      "6 Train Loss 2.381284 Test MSE 383.84257334501564 Test RE 0.9987094370203874\n",
      "7 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "8 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "9 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "10 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "11 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "12 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "13 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "14 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "15 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "16 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "17 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "18 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "19 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "20 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "21 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "22 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "23 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "24 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "25 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "26 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "27 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "28 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "29 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "30 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "31 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "32 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "33 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "34 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "35 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "36 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "37 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "38 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "39 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "40 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "41 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "42 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "43 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "44 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "45 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "46 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "47 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "48 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "49 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "50 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "51 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "52 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "53 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "54 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "55 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "56 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "57 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "58 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "59 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "60 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "61 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "62 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "63 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "64 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "65 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "66 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "67 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "68 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "69 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "70 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "71 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "72 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "74 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "75 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "76 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "77 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "78 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "79 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "80 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "81 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "82 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "83 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "84 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "85 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "86 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "87 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "88 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "89 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "90 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "91 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "92 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "93 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "94 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "95 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "96 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "97 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "98 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "99 Train Loss 2.3812811 Test MSE 383.8425347910859 Test RE 0.9987093868641812\n",
      "Training time: 10.47\n",
      "Training time: 10.47\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 4.3613544 Test MSE 386.5026908554886 Test RE 1.0021641052453634\n",
      "1 Train Loss 3.7241147 Test MSE 386.8045770656991 Test RE 1.0025554097274711\n",
      "2 Train Loss 2.6364617 Test MSE 383.00215806714886 Test RE 0.9976155112091178\n",
      "3 Train Loss 2.3838205 Test MSE 383.95428458669335 Test RE 0.9988547556519825\n",
      "4 Train Loss 2.3819346 Test MSE 383.96784915034556 Test RE 0.9988723995614156\n",
      "5 Train Loss 2.3813221 Test MSE 383.844165778414 Test RE 0.9987115086725745\n",
      "6 Train Loss 2.3813164 Test MSE 383.8394140234326 Test RE 0.9987053269359528\n",
      "7 Train Loss 2.3813114 Test MSE 383.83486140290637 Test RE 0.9986994042249349\n",
      "8 Train Loss 2.3813064 Test MSE 383.83030112964326 Test RE 0.9986934715229105\n",
      "9 Train Loss 2.3813004 Test MSE 383.8253488302544 Test RE 0.9986870287733317\n",
      "10 Train Loss 2.3812926 Test MSE 383.81975958972794 Test RE 0.9986797573380912\n",
      "11 Train Loss 2.3812833 Test MSE 383.8131953968134 Test RE 0.9986712174512665\n",
      "12 Train Loss 2.380972 Test MSE 383.7219942037286 Test RE 0.9985525589230496\n",
      "13 Train Loss 2.380937 Test MSE 383.73088301427606 Test RE 0.9985641244486778\n",
      "14 Train Loss 2.3809326 Test MSE 383.73310578023984 Test RE 0.9985670165421118\n",
      "15 Train Loss 2.38093 Test MSE 383.7354657783661 Test RE 0.9985700871823945\n",
      "16 Train Loss 2.38093 Test MSE 383.7354657783661 Test RE 0.9985700871823945\n",
      "17 Train Loss 2.38093 Test MSE 383.7354657783661 Test RE 0.9985700871823945\n",
      "18 Train Loss 2.38093 Test MSE 383.7354657783661 Test RE 0.9985700871823945\n",
      "19 Train Loss 2.3809288 Test MSE 383.73668168518253 Test RE 0.9985716692191385\n",
      "20 Train Loss 2.3809288 Test MSE 383.73668168518253 Test RE 0.9985716692191385\n",
      "21 Train Loss 2.3809288 Test MSE 383.73668168518253 Test RE 0.9985716692191385\n",
      "22 Train Loss 2.3809288 Test MSE 383.73668168518253 Test RE 0.9985716692191385\n",
      "23 Train Loss 2.3809288 Test MSE 383.73668168518253 Test RE 0.9985716692191385\n",
      "24 Train Loss 2.3809288 Test MSE 383.73668168518253 Test RE 0.9985716692191385\n",
      "25 Train Loss 2.3809252 Test MSE 383.7395488943097 Test RE 0.9985753997830809\n",
      "26 Train Loss 2.3809252 Test MSE 383.7395488943097 Test RE 0.9985753997830809\n",
      "27 Train Loss 2.3809192 Test MSE 383.7451499677924 Test RE 0.9985826873741033\n",
      "28 Train Loss 2.380914 Test MSE 383.7395593284737 Test RE 0.998575413359085\n",
      "29 Train Loss 2.3809094 Test MSE 383.72256626579804 Test RE 0.9985533032559767\n",
      "30 Train Loss 2.3806672 Test MSE 383.5961839638953 Test RE 0.9983888486876733\n",
      "31 Train Loss 2.3799963 Test MSE 383.5135875914078 Test RE 0.9982813557821357\n",
      "32 Train Loss 2.3795018 Test MSE 383.3371129587484 Test RE 0.9980516486578962\n",
      "33 Train Loss 2.3793848 Test MSE 383.27698005992556 Test RE 0.9979733649611382\n",
      "34 Train Loss 2.379376 Test MSE 383.2726122736796 Test RE 0.997967678542461\n",
      "35 Train Loss 2.3793561 Test MSE 383.2661855547703 Test RE 0.9979593115414918\n",
      "36 Train Loss 2.3793488 Test MSE 383.26600830985 Test RE 0.9979590807838019\n",
      "37 Train Loss 2.3793395 Test MSE 383.2637237415995 Test RE 0.9979561064670361\n",
      "38 Train Loss 2.3793328 Test MSE 383.2653172038999 Test RE 0.9979581810225988\n",
      "39 Train Loss 2.379325 Test MSE 383.26488193716926 Test RE 0.9979576143418966\n",
      "40 Train Loss 2.3793182 Test MSE 383.26795463990305 Test RE 0.9979616147357344\n",
      "41 Train Loss 2.3793104 Test MSE 383.2690398881314 Test RE 0.9979630276314573\n",
      "42 Train Loss 2.3793042 Test MSE 383.2730879770896 Test RE 0.9979682978620594\n",
      "43 Train Loss 2.3792946 Test MSE 383.2758157223904 Test RE 0.9979718491138649\n",
      "44 Train Loss 2.3792875 Test MSE 383.2816615475653 Test RE 0.9979794597515246\n",
      "45 Train Loss 2.379113 Test MSE 383.3063061073699 Test RE 0.9980115436882296\n",
      "46 Train Loss 2.37908 Test MSE 383.2869936529055 Test RE 0.9979864015304218\n",
      "47 Train Loss 2.379074 Test MSE 383.28273334095843 Test RE 0.9979808551053219\n",
      "48 Train Loss 2.3790703 Test MSE 383.2806156908409 Test RE 0.9979780981624561\n",
      "49 Train Loss 2.3790643 Test MSE 383.2780019239945 Test RE 0.9979746953207695\n",
      "50 Train Loss 2.3790557 Test MSE 383.2776032097453 Test RE 0.9979741762369284\n",
      "51 Train Loss 2.379046 Test MSE 383.2775297182018 Test RE 0.9979740805586713\n",
      "52 Train Loss 2.378993 Test MSE 383.28935971446475 Test RE 0.9979894818509861\n",
      "53 Train Loss 2.378986 Test MSE 383.2939825858998 Test RE 0.9979955002324519\n",
      "54 Train Loss 2.3789785 Test MSE 383.29433041912625 Test RE 0.9979959530649145\n",
      "55 Train Loss 2.3789728 Test MSE 383.29820840059415 Test RE 0.9980010016655781\n",
      "56 Train Loss 2.3789682 Test MSE 383.30011270755915 Test RE 0.9980034808030439\n",
      "57 Train Loss 2.3789618 Test MSE 383.3026561720129 Test RE 0.9980067920232638\n",
      "58 Train Loss 2.3789546 Test MSE 383.30460142559895 Test RE 0.9980093244526599\n",
      "59 Train Loss 2.3789468 Test MSE 383.30857217318436 Test RE 0.9980144937525295\n",
      "60 Train Loss 2.3789396 Test MSE 383.3111567523518 Test RE 0.9980178584608981\n",
      "61 Train Loss 2.3789334 Test MSE 383.3131980612179 Test RE 0.9980205159102769\n",
      "62 Train Loss 2.3784895 Test MSE 383.2794119685376 Test RE 0.9979765310476726\n",
      "63 Train Loss 2.376922 Test MSE 382.674363854346 Test RE 0.9971885128449255\n",
      "64 Train Loss 2.3756084 Test MSE 382.2259573676092 Test RE 0.9966041036124842\n",
      "65 Train Loss 2.3751183 Test MSE 382.3611201344205 Test RE 0.9967802976399694\n",
      "66 Train Loss 2.3751116 Test MSE 382.36458189321513 Test RE 0.9967848098732615\n",
      "67 Train Loss 2.3751068 Test MSE 382.3672910569314 Test RE 0.9967883411214269\n",
      "68 Train Loss 2.3751035 Test MSE 382.36920944510797 Test RE 0.9967908416289419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 Train Loss 2.3750992 Test MSE 382.3708464302168 Test RE 0.9967929753389498\n",
      "70 Train Loss 2.3750954 Test MSE 382.37185325889084 Test RE 0.996794287676303\n",
      "71 Train Loss 2.375093 Test MSE 382.37278742298156 Test RE 0.9967955052984327\n",
      "72 Train Loss 2.3750865 Test MSE 382.37265347578744 Test RE 0.9967953307070692\n",
      "73 Train Loss 2.3750768 Test MSE 382.3725479498041 Test RE 0.9967951931608641\n",
      "74 Train Loss 2.3750672 Test MSE 382.37018178469725 Test RE 0.9967921090147007\n",
      "75 Train Loss 2.373781 Test MSE 382.30208434882815 Test RE 0.9967033442359421\n",
      "76 Train Loss 2.3718007 Test MSE 381.86089212527594 Test RE 0.9961280600238388\n",
      "77 Train Loss 2.3689778 Test MSE 381.49301959588456 Test RE 0.9956481255041857\n",
      "78 Train Loss 2.3627687 Test MSE 379.99622768784496 Test RE 0.9936929878790763\n",
      "79 Train Loss 2.3621237 Test MSE 379.9301936956425 Test RE 0.9936066444351724\n",
      "80 Train Loss 2.3507397 Test MSE 377.3100647327349 Test RE 0.9901745908892255\n",
      "81 Train Loss 2.3323355 Test MSE 374.6859008380585 Test RE 0.9867252870938118\n",
      "82 Train Loss 2.3291125 Test MSE 373.20062582564884 Test RE 0.9847676291146623\n",
      "83 Train Loss 2.3099859 Test MSE 369.44458585046283 Test RE 0.9797995501906075\n",
      "84 Train Loss 2.2921188 Test MSE 365.55995959145633 Test RE 0.9746347523131695\n",
      "85 Train Loss 2.2507527 Test MSE 356.3755764642702 Test RE 0.9623134346778467\n",
      "86 Train Loss 2.1898475 Test MSE 345.35196243022796 Test RE 0.9473131078480269\n",
      "87 Train Loss 2.124916 Test MSE 335.4799100034245 Test RE 0.9336752388893452\n",
      "88 Train Loss 2.1005487 Test MSE 326.3447939318056 Test RE 0.9208755176593079\n",
      "89 Train Loss 2.0564196 Test MSE 312.7823179167124 Test RE 0.9015372575668286\n",
      "90 Train Loss 2.0193534 Test MSE 307.2657047953984 Test RE 0.8935515804465023\n",
      "91 Train Loss 1.9517612 Test MSE 300.4283447976539 Test RE 0.883553873278068\n",
      "92 Train Loss 1.9321802 Test MSE 300.5841582741569 Test RE 0.8837829657680439\n",
      "93 Train Loss 1.9092045 Test MSE 290.80979907213344 Test RE 0.8692948379400758\n",
      "94 Train Loss 1.8222921 Test MSE 274.0377003516884 Test RE 0.8438548284470279\n",
      "95 Train Loss 1.7980855 Test MSE 268.16774816903234 Test RE 0.8347681204460746\n",
      "96 Train Loss 1.6277134 Test MSE 253.64994227353168 Test RE 0.8118577978970801\n",
      "97 Train Loss 1.5495532 Test MSE 240.41251527279542 Test RE 0.7903894207161225\n",
      "98 Train Loss 1.484583 Test MSE 228.42404185633336 Test RE 0.7704305373654783\n",
      "99 Train Loss 1.4592941 Test MSE 221.80374140412536 Test RE 0.7591839469947546\n",
      "Training time: 53.84\n",
      "Training time: 53.84\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 4.378791 Test MSE 386.9713442510771 Test RE 1.0027715076444799\n",
      "1 Train Loss 4.2235928 Test MSE 384.6256399487598 Test RE 0.9997276377163119\n",
      "2 Train Loss 3.9530745 Test MSE 386.8268998024407 Test RE 1.0025843383634396\n",
      "3 Train Loss 2.4233193 Test MSE 384.37676325283843 Test RE 0.9994041424170077\n",
      "4 Train Loss 2.3833735 Test MSE 384.036785199222 Test RE 0.9989620623109801\n",
      "5 Train Loss 2.3822894 Test MSE 383.932421726199 Test RE 0.9988263171972874\n",
      "6 Train Loss 2.381123 Test MSE 383.8210637972825 Test RE 0.9986814540779608\n",
      "7 Train Loss 2.3811195 Test MSE 383.8209765884892 Test RE 0.998681340621695\n",
      "8 Train Loss 2.3811195 Test MSE 383.8209765884892 Test RE 0.998681340621695\n",
      "9 Train Loss 2.3811195 Test MSE 383.8209765884892 Test RE 0.998681340621695\n",
      "10 Train Loss 2.3811195 Test MSE 383.8209765884892 Test RE 0.998681340621695\n",
      "11 Train Loss 2.3811176 Test MSE 383.8207892720669 Test RE 0.9986810969280852\n",
      "12 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "13 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "14 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "15 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "16 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "17 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "18 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "19 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "20 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "21 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "22 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "23 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "24 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "25 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "26 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "27 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "28 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "29 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "30 Train Loss 2.3811154 Test MSE 383.82353197871146 Test RE 0.9986846651094652\n",
      "31 Train Loss 2.381113 Test MSE 383.82239821414373 Test RE 0.9986831901163633\n",
      "32 Train Loss 2.3811104 Test MSE 383.81844264773366 Test RE 0.9986780440281239\n",
      "33 Train Loss 2.3811104 Test MSE 383.81844264773366 Test RE 0.9986780440281239\n",
      "34 Train Loss 2.3811104 Test MSE 383.81844264773366 Test RE 0.9986780440281239\n",
      "35 Train Loss 2.38111 Test MSE 383.8184416292538 Test RE 0.9986780427031051\n",
      "36 Train Loss 2.38111 Test MSE 383.8184416292538 Test RE 0.9986780427031051\n",
      "37 Train Loss 2.38111 Test MSE 383.8184416292538 Test RE 0.9986780427031051\n",
      "38 Train Loss 2.38111 Test MSE 383.8184416292538 Test RE 0.9986780427031051\n",
      "39 Train Loss 2.38111 Test MSE 383.8184416292538 Test RE 0.9986780427031051\n",
      "40 Train Loss 2.38111 Test MSE 383.8184416292538 Test RE 0.9986780427031051\n",
      "41 Train Loss 2.38111 Test MSE 383.8184416292538 Test RE 0.9986780427031051\n",
      "42 Train Loss 2.38111 Test MSE 383.8184416292538 Test RE 0.9986780427031051\n",
      "43 Train Loss 2.38111 Test MSE 383.8184416292538 Test RE 0.9986780427031051\n",
      "44 Train Loss 2.38111 Test MSE 383.8184416292538 Test RE 0.9986780427031051\n",
      "45 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "46 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "47 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "48 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "49 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "50 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "51 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "52 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "53 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "54 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "55 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "56 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "57 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "58 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "59 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "60 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "61 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "62 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "63 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "64 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "66 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "67 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "68 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "69 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "70 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "71 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "72 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "73 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "74 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "75 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "76 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "77 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "78 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "79 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "80 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "81 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "82 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "83 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "84 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "85 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "86 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "87 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "88 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "89 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "90 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "91 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "92 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "93 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "94 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "95 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "96 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "97 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "98 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "99 Train Loss 2.3811097 Test MSE 383.8184431749472 Test RE 0.9986780447140167\n",
      "Training time: 29.46\n",
      "Training time: 29.46\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    " \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746650559652632\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
