{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 0.005\n",
    "label = \"1D_SODE_rowdy\"\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.3443027 Test MSE 386.51022543182546 Test RE 1.002173873411708\n",
      "1 Train Loss 2.606071 Test MSE 382.9875848378914 Test RE 0.997596531397191\n",
      "2 Train Loss 2.397064 Test MSE 383.704570772472 Test RE 0.9985298883312785\n",
      "3 Train Loss 2.3842182 Test MSE 383.97660525184676 Test RE 0.9988837887679266\n",
      "4 Train Loss 2.381233 Test MSE 383.7029607920882 Test RE 0.9985277934706204\n",
      "5 Train Loss 2.3805058 Test MSE 383.67669506736087 Test RE 0.9984936166363461\n",
      "6 Train Loss 2.3804994 Test MSE 383.6767376576848 Test RE 0.998493672055617\n",
      "7 Train Loss 2.3804932 Test MSE 383.6770232834544 Test RE 0.9984940437167609\n",
      "8 Train Loss 2.3804886 Test MSE 383.6767435190533 Test RE 0.9984936796825312\n",
      "9 Train Loss 2.380485 Test MSE 383.6764170623608 Test RE 0.9984932548913252\n",
      "10 Train Loss 2.3804796 Test MSE 383.67554693859995 Test RE 0.9984921226700574\n",
      "11 Train Loss 2.380473 Test MSE 383.6742814172371 Test RE 0.9984904759477631\n",
      "12 Train Loss 2.3804655 Test MSE 383.6720950152532 Test RE 0.9984876309503607\n",
      "13 Train Loss 2.379966 Test MSE 383.41588237928454 Test RE 0.9981541849232602\n",
      "14 Train Loss 2.378471 Test MSE 383.1418639153674 Test RE 0.9977974423170671\n",
      "15 Train Loss 2.3766835 Test MSE 382.9472855986551 Test RE 0.9975440447886367\n",
      "16 Train Loss 2.3731992 Test MSE 381.8880795514367 Test RE 0.9961635201568487\n",
      "17 Train Loss 2.36951 Test MSE 381.2880669445821 Test RE 0.9953806394333075\n",
      "18 Train Loss 2.3679817 Test MSE 381.0881088358937 Test RE 0.9951196025048126\n",
      "19 Train Loss 2.3606052 Test MSE 379.66593498617004 Test RE 0.9932610350482006\n",
      "20 Train Loss 2.3557737 Test MSE 378.7329864850983 Test RE 0.9920399202768526\n",
      "21 Train Loss 2.3510678 Test MSE 377.53822758601456 Test RE 0.9904739294667048\n",
      "22 Train Loss 2.3417504 Test MSE 375.0491069579438 Test RE 0.9872034180471047\n",
      "23 Train Loss 2.3361998 Test MSE 372.29696374997053 Test RE 0.9835746561571322\n",
      "24 Train Loss 2.288048 Test MSE 363.74484371226123 Test RE 0.9722120629074942\n",
      "25 Train Loss 2.2497284 Test MSE 360.3188450858116 Test RE 0.9676227507325288\n",
      "26 Train Loss 2.1740386 Test MSE 344.9374698076033 Test RE 0.9467444529332394\n",
      "27 Train Loss 2.123357 Test MSE 337.8913521785669 Test RE 0.9370248766540902\n",
      "28 Train Loss 2.0634673 Test MSE 327.8509963318374 Test RE 0.9229981625037496\n",
      "29 Train Loss 2.0089025 Test MSE 319.37966812285146 Test RE 0.9109954652806141\n",
      "30 Train Loss 1.9645958 Test MSE 311.64162834149397 Test RE 0.8998918425233808\n",
      "31 Train Loss 1.9200627 Test MSE 299.72496201339334 Test RE 0.8825189496744684\n",
      "32 Train Loss 1.8509753 Test MSE 288.2669337605313 Test RE 0.8654858996967505\n",
      "33 Train Loss 1.7916859 Test MSE 284.7197485734428 Test RE 0.8601444239174939\n",
      "34 Train Loss 1.761525 Test MSE 279.94470767735515 Test RE 0.8529011762241907\n",
      "35 Train Loss 1.7261763 Test MSE 272.79468875121324 Test RE 0.8419388265625545\n",
      "36 Train Loss 1.6750481 Test MSE 265.97577224718634 Test RE 0.8313494652839333\n",
      "37 Train Loss 1.6533415 Test MSE 261.80412589556227 Test RE 0.8248041278892138\n",
      "38 Train Loss 1.6368053 Test MSE 257.79937028741716 Test RE 0.8184714004628318\n",
      "39 Train Loss 1.6322846 Test MSE 256.5817983025747 Test RE 0.8165363156461389\n",
      "40 Train Loss 1.618969 Test MSE 254.49114087453287 Test RE 0.813202896497218\n",
      "41 Train Loss 1.5809202 Test MSE 242.00580503101438 Test RE 0.7930041759755472\n",
      "42 Train Loss 1.5289985 Test MSE 233.7626128375628 Test RE 0.7793815289468289\n",
      "43 Train Loss 1.4811063 Test MSE 230.92115358265056 Test RE 0.7746302306521684\n",
      "44 Train Loss 1.4602888 Test MSE 229.25088730490708 Test RE 0.7718236734474826\n",
      "45 Train Loss 1.4476397 Test MSE 225.91430615802815 Test RE 0.7661864185656665\n",
      "46 Train Loss 1.4375659 Test MSE 223.39105191659192 Test RE 0.7618956063344267\n",
      "47 Train Loss 1.4138929 Test MSE 222.31573520269754 Test RE 0.7600596613074386\n",
      "48 Train Loss 1.390628 Test MSE 218.22969713462658 Test RE 0.7530425347074409\n",
      "49 Train Loss 1.3589581 Test MSE 213.31157921596696 Test RE 0.7445087358463658\n",
      "50 Train Loss 1.3519932 Test MSE 210.15478417048297 Test RE 0.7389792146369716\n",
      "51 Train Loss 1.3439827 Test MSE 208.96873246195196 Test RE 0.736890973589553\n",
      "52 Train Loss 1.3200523 Test MSE 206.69780582490006 Test RE 0.732876026954522\n",
      "53 Train Loss 1.2929789 Test MSE 200.35130063370653 Test RE 0.7215370983832021\n",
      "54 Train Loss 1.2442615 Test MSE 195.8920940764341 Test RE 0.7134623121785867\n",
      "55 Train Loss 1.2141656 Test MSE 189.74192530209297 Test RE 0.7021731747994577\n",
      "56 Train Loss 1.1973649 Test MSE 187.12564557712528 Test RE 0.697315370751244\n",
      "57 Train Loss 1.15213 Test MSE 181.38162979060857 Test RE 0.6865295458117829\n",
      "58 Train Loss 1.1435499 Test MSE 179.3365836909822 Test RE 0.6826483245997667\n",
      "59 Train Loss 1.1422224 Test MSE 178.72198419040078 Test RE 0.6814775779800806\n",
      "60 Train Loss 1.1403081 Test MSE 177.88943278935835 Test RE 0.6798884410695367\n",
      "61 Train Loss 1.1272941 Test MSE 176.27208488812778 Test RE 0.6767906545815107\n",
      "62 Train Loss 1.115646 Test MSE 175.13403734808333 Test RE 0.6746023695147453\n",
      "63 Train Loss 1.1014078 Test MSE 172.76169225431644 Test RE 0.6700177486746342\n",
      "64 Train Loss 1.0886837 Test MSE 168.20063728177337 Test RE 0.6611140714722579\n",
      "65 Train Loss 1.0653864 Test MSE 166.47470114264152 Test RE 0.6577134217906055\n",
      "66 Train Loss 1.0382229 Test MSE 160.5277370309799 Test RE 0.6458588638391396\n",
      "67 Train Loss 1.0213228 Test MSE 159.08118762487422 Test RE 0.6429422930680331\n",
      "68 Train Loss 1.0155168 Test MSE 157.51507674748106 Test RE 0.6397696696469302\n",
      "69 Train Loss 1.0096498 Test MSE 155.16792192331877 Test RE 0.634985129013406\n",
      "70 Train Loss 1.0029478 Test MSE 152.6028455871649 Test RE 0.6297147971111392\n",
      "71 Train Loss 0.9966534 Test MSE 150.8776040862552 Test RE 0.6261450793811149\n",
      "72 Train Loss 0.9851419 Test MSE 147.85164610039183 Test RE 0.6198343846219728\n",
      "73 Train Loss 0.9714578 Test MSE 146.27028158685164 Test RE 0.6165107181125781\n",
      "74 Train Loss 0.9593853 Test MSE 145.06985866771294 Test RE 0.6139756909186737\n",
      "75 Train Loss 0.95250183 Test MSE 143.12834414823251 Test RE 0.6098533390662898\n",
      "76 Train Loss 0.94793457 Test MSE 142.96286672010876 Test RE 0.6095006970083151\n",
      "77 Train Loss 0.9451012 Test MSE 141.6556785147114 Test RE 0.6067077978627555\n",
      "78 Train Loss 0.94004387 Test MSE 139.14628428809795 Test RE 0.6013099486919211\n",
      "79 Train Loss 0.92225075 Test MSE 135.00572040638042 Test RE 0.5922958201807967\n",
      "80 Train Loss 0.9051958 Test MSE 128.5590571216812 Test RE 0.5779814787618257\n",
      "81 Train Loss 0.8985432 Test MSE 125.87846990211962 Test RE 0.5719239850755168\n",
      "82 Train Loss 0.8887345 Test MSE 122.64751910321553 Test RE 0.5645364220393345\n",
      "83 Train Loss 0.86944973 Test MSE 119.56593260533118 Test RE 0.5573991605964024\n",
      "84 Train Loss 0.85885555 Test MSE 119.2716044656766 Test RE 0.5567126801671163\n",
      "85 Train Loss 0.8291572 Test MSE 114.60684519793409 Test RE 0.5457174755227814\n",
      "86 Train Loss 0.764188 Test MSE 110.87070083985765 Test RE 0.5367486724199183\n",
      "87 Train Loss 0.7460937 Test MSE 108.47861153830512 Test RE 0.5309267918611936\n",
      "88 Train Loss 0.74206924 Test MSE 107.62727020181117 Test RE 0.5288393285340642\n",
      "89 Train Loss 0.73982024 Test MSE 107.4160717661121 Test RE 0.5283201994822235\n",
      "90 Train Loss 0.72560465 Test MSE 106.83512063861764 Test RE 0.5268895739917012\n",
      "91 Train Loss 0.7135415 Test MSE 105.2603327446566 Test RE 0.522991886945951\n",
      "92 Train Loss 0.68567985 Test MSE 101.42414303269463 Test RE 0.513373274284788\n",
      "93 Train Loss 0.66644776 Test MSE 99.73485806671333 Test RE 0.5090800399388398\n",
      "94 Train Loss 0.65450495 Test MSE 98.7431993723186 Test RE 0.5065428387078125\n",
      "95 Train Loss 0.64046127 Test MSE 97.42776597897819 Test RE 0.5031575047026383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 0.6299419 Test MSE 95.83965249626657 Test RE 0.4990398164790996\n",
      "97 Train Loss 0.6262821 Test MSE 94.75974162551083 Test RE 0.4962202880201349\n",
      "98 Train Loss 0.61982447 Test MSE 95.12998026155407 Test RE 0.4971887415784638\n",
      "99 Train Loss 0.61541224 Test MSE 94.72437972118604 Test RE 0.4961276910406399\n",
      "Training time: 71.86\n",
      "Training time: 71.86\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 4.261027 Test MSE 386.50028383478946 Test RE 1.0021609846545396\n",
      "1 Train Loss 2.6893754 Test MSE 384.941441211801 Test RE 1.000137972388645\n",
      "2 Train Loss 2.460285 Test MSE 384.7030586675187 Test RE 0.9998282468937311\n",
      "3 Train Loss 2.387203 Test MSE 384.39140253688197 Test RE 0.9994231737727145\n",
      "4 Train Loss 2.3841858 Test MSE 384.08259236725627 Test RE 0.9990216376698561\n",
      "5 Train Loss 2.3816967 Test MSE 383.6662201033718 Test RE 0.9984799863374499\n",
      "6 Train Loss 2.380737 Test MSE 383.672183041435 Test RE 0.9984877454922441\n",
      "7 Train Loss 2.3805645 Test MSE 383.7067278281757 Test RE 0.9985326950238852\n",
      "8 Train Loss 2.3805563 Test MSE 383.7090251695729 Test RE 0.9985356842431206\n",
      "9 Train Loss 2.380548 Test MSE 383.71207506603554 Test RE 0.9985396526465198\n",
      "10 Train Loss 2.3805408 Test MSE 383.7136870312572 Test RE 0.9985417500648791\n",
      "11 Train Loss 2.3802884 Test MSE 383.7026177873527 Test RE 0.9985273471620523\n",
      "12 Train Loss 2.3795497 Test MSE 383.3858987931804 Test RE 0.9981151557274791\n",
      "13 Train Loss 2.3795416 Test MSE 383.3809398933088 Test RE 0.9981087006529206\n",
      "14 Train Loss 2.3795352 Test MSE 383.37636550065713 Test RE 0.9981027460601775\n",
      "15 Train Loss 2.379529 Test MSE 383.3727855015475 Test RE 0.9980980858677329\n",
      "16 Train Loss 2.3795252 Test MSE 383.3697479404897 Test RE 0.9980941317661407\n",
      "17 Train Loss 2.3795204 Test MSE 383.36629135171677 Test RE 0.9980896321821681\n",
      "18 Train Loss 2.3795154 Test MSE 383.36271222092444 Test RE 0.9980849730588252\n",
      "19 Train Loss 2.3795106 Test MSE 383.3583244958484 Test RE 0.9980792613204958\n",
      "20 Train Loss 2.3795035 Test MSE 383.35383941101696 Test RE 0.9980734228099395\n",
      "21 Train Loss 2.3794954 Test MSE 383.34778257924074 Test RE 0.9980655382055887\n",
      "22 Train Loss 2.3792746 Test MSE 383.20048042833383 Test RE 0.9978737654435791\n",
      "23 Train Loss 2.378982 Test MSE 383.0618146864856 Test RE 0.9976932027445652\n",
      "24 Train Loss 2.3783016 Test MSE 382.9324545170858 Test RE 0.9975247277691651\n",
      "25 Train Loss 2.3762908 Test MSE 382.5297262131903 Test RE 0.9970000436932592\n",
      "26 Train Loss 2.3750684 Test MSE 382.4643905132253 Test RE 0.9969148967452673\n",
      "27 Train Loss 2.372833 Test MSE 382.3500115300726 Test RE 0.9967658179795255\n",
      "28 Train Loss 2.3715386 Test MSE 381.7029255615022 Test RE 0.9959220017355861\n",
      "29 Train Loss 2.3709207 Test MSE 381.4096028876273 Test RE 0.9955392660683092\n",
      "30 Train Loss 2.3676834 Test MSE 380.1793469113813 Test RE 0.9939323881130322\n",
      "31 Train Loss 2.3624682 Test MSE 378.8186752312315 Test RE 0.9921521389800653\n",
      "32 Train Loss 2.3584695 Test MSE 378.70903957555544 Test RE 0.9920085569329992\n",
      "33 Train Loss 2.3567533 Test MSE 378.45446807650643 Test RE 0.9916750825071329\n",
      "34 Train Loss 2.3448646 Test MSE 376.2551521202682 Test RE 0.9887894187306213\n",
      "35 Train Loss 2.3357158 Test MSE 372.83191443277445 Test RE 0.98428104797676\n",
      "36 Train Loss 2.302634 Test MSE 366.2026820700041 Test RE 0.9754911707315961\n",
      "37 Train Loss 2.2362227 Test MSE 352.4385886833022 Test RE 0.9569831901574004\n",
      "38 Train Loss 2.1981666 Test MSE 348.36907689710193 Test RE 0.9514421371585146\n",
      "39 Train Loss 2.127843 Test MSE 335.6036846299167 Test RE 0.9338474617901554\n",
      "40 Train Loss 2.0614412 Test MSE 326.7880149137176 Test RE 0.921500643096258\n",
      "41 Train Loss 2.0392554 Test MSE 323.7664502235372 Test RE 0.9172305354688142\n",
      "42 Train Loss 2.0243683 Test MSE 321.12171545831865 Test RE 0.9134765860361312\n",
      "43 Train Loss 1.9866114 Test MSE 313.88353535733984 Test RE 0.9031228910858113\n",
      "44 Train Loss 1.9689652 Test MSE 310.0983777277993 Test RE 0.8976609432041602\n",
      "45 Train Loss 1.9230726 Test MSE 304.5307918106582 Test RE 0.8895660265186603\n",
      "46 Train Loss 1.8809966 Test MSE 294.18435208856596 Test RE 0.8743239334634658\n",
      "47 Train Loss 1.7975333 Test MSE 281.5448183056739 Test RE 0.8553352133929651\n",
      "48 Train Loss 1.7149086 Test MSE 264.9247322409581 Test RE 0.8297052432055703\n",
      "49 Train Loss 1.6868056 Test MSE 262.3754560101604 Test RE 0.8257036145253159\n",
      "50 Train Loss 1.6593372 Test MSE 256.057974577668 Test RE 0.8157023912924144\n",
      "51 Train Loss 1.6265881 Test MSE 256.63113261235645 Test RE 0.8166148117039831\n",
      "52 Train Loss 1.5665048 Test MSE 245.63454003617713 Test RE 0.7989273709711817\n",
      "53 Train Loss 1.5215079 Test MSE 239.47766381742557 Test RE 0.788851197123563\n",
      "54 Train Loss 1.5021118 Test MSE 236.0109834745259 Test RE 0.7831206750179396\n",
      "55 Train Loss 1.4865236 Test MSE 233.1591457521319 Test RE 0.7783748773038464\n",
      "56 Train Loss 1.462257 Test MSE 229.77066039473107 Test RE 0.7726981434352492\n",
      "57 Train Loss 1.4476861 Test MSE 225.6457600826171 Test RE 0.7657308973070756\n",
      "58 Train Loss 1.4069397 Test MSE 212.4417802943918 Test RE 0.7429892815748849\n",
      "59 Train Loss 1.354269 Test MSE 204.6971878703577 Test RE 0.7293206673855023\n",
      "60 Train Loss 1.318652 Test MSE 203.52791542484556 Test RE 0.7272346693662427\n",
      "61 Train Loss 1.3021365 Test MSE 201.80425549478642 Test RE 0.7241486786535407\n",
      "62 Train Loss 1.2730399 Test MSE 196.7690802438601 Test RE 0.7150575727023971\n",
      "63 Train Loss 1.2285836 Test MSE 190.80720944364668 Test RE 0.7041415509874033\n",
      "64 Train Loss 1.1870697 Test MSE 183.05399825624102 Test RE 0.6896872415702407\n",
      "65 Train Loss 1.1628106 Test MSE 175.97845130757005 Test RE 0.6762267215779921\n",
      "66 Train Loss 1.1392815 Test MSE 172.42916236987276 Test RE 0.6693726165386988\n",
      "67 Train Loss 1.1110871 Test MSE 170.5042835312393 Test RE 0.665625926450466\n",
      "68 Train Loss 1.1014408 Test MSE 171.77128980084137 Test RE 0.668094460597798\n",
      "69 Train Loss 1.0529101 Test MSE 161.7595423611645 Test RE 0.648332118718641\n",
      "70 Train Loss 1.0165775 Test MSE 155.22150764777152 Test RE 0.6350947625014098\n",
      "71 Train Loss 1.0039166 Test MSE 155.92206064475687 Test RE 0.6365263188299906\n",
      "72 Train Loss 0.9756289 Test MSE 150.48553854757006 Test RE 0.6253310102640935\n",
      "73 Train Loss 0.9619097 Test MSE 149.27470353060662 Test RE 0.6228101636139529\n",
      "74 Train Loss 0.9449605 Test MSE 146.3457462393322 Test RE 0.61666973458298\n",
      "75 Train Loss 0.92354006 Test MSE 141.962738678403 Test RE 0.607365007481357\n",
      "76 Train Loss 0.9092739 Test MSE 142.48765107964076 Test RE 0.608486848553855\n",
      "77 Train Loss 0.8976561 Test MSE 139.28145481923218 Test RE 0.6016019423153243\n",
      "78 Train Loss 0.88493013 Test MSE 135.8936011626087 Test RE 0.5942402794592276\n",
      "79 Train Loss 0.8707524 Test MSE 132.5820888258006 Test RE 0.5869552764068453\n",
      "80 Train Loss 0.86126643 Test MSE 129.9676499248143 Test RE 0.5811392593758095\n",
      "81 Train Loss 0.84915406 Test MSE 128.42581400599542 Test RE 0.5776818809753715\n",
      "82 Train Loss 0.84671396 Test MSE 128.80494532911712 Test RE 0.5785339522398327\n",
      "83 Train Loss 0.8431103 Test MSE 128.3415629608948 Test RE 0.5774923618841936\n",
      "84 Train Loss 0.8140773 Test MSE 125.44337881712212 Test RE 0.5709347196993309\n",
      "85 Train Loss 0.79271084 Test MSE 123.58100075296117 Test RE 0.5666807192111959\n",
      "86 Train Loss 0.78867817 Test MSE 122.44529648405954 Test RE 0.5640708230143505\n",
      "87 Train Loss 0.78609174 Test MSE 121.81777374660125 Test RE 0.5626235565338691\n",
      "88 Train Loss 0.7833531 Test MSE 121.11384141352949 Test RE 0.5609956220025683\n",
      "89 Train Loss 0.7673351 Test MSE 116.562612932356 Test RE 0.5503541170438911\n",
      "90 Train Loss 0.7653445 Test MSE 115.92876205986735 Test RE 0.5488557036111443\n",
      "91 Train Loss 0.7653378 Test MSE 115.93934124186643 Test RE 0.5488807461968433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Train Loss 0.76271737 Test MSE 116.26969971904617 Test RE 0.5496621825077997\n",
      "93 Train Loss 0.75797546 Test MSE 115.35355302937425 Test RE 0.5474923691794491\n",
      "94 Train Loss 0.7527384 Test MSE 112.89664226972558 Test RE 0.5416304789932735\n",
      "95 Train Loss 0.7197054 Test MSE 108.96684434708496 Test RE 0.5321202292798511\n",
      "96 Train Loss 0.71320635 Test MSE 107.21386239355725 Test RE 0.5278226872440558\n",
      "97 Train Loss 0.6984404 Test MSE 105.70704075704329 Test RE 0.5241004589899563\n",
      "98 Train Loss 0.69488394 Test MSE 106.44568808417412 Test RE 0.5259283953105578\n",
      "99 Train Loss 0.6882765 Test MSE 105.72541016683128 Test RE 0.524145995208801\n",
      "Training time: 67.26\n",
      "Training time: 67.26\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 3.1366425 Test MSE 384.18639336196225 Test RE 0.9991566248267284\n",
      "1 Train Loss 2.429103 Test MSE 384.5511728195281 Test RE 0.9996308547106928\n",
      "2 Train Loss 2.3892243 Test MSE 383.407099418623 Test RE 0.9981427524297863\n",
      "3 Train Loss 2.3834016 Test MSE 383.3578556518395 Test RE 0.9980786509990349\n",
      "4 Train Loss 2.3805285 Test MSE 383.56247717830956 Test RE 0.998344983264139\n",
      "5 Train Loss 2.3800094 Test MSE 383.6117938916673 Test RE 0.9984091625227955\n",
      "6 Train Loss 2.3789678 Test MSE 383.3383120410422 Test RE 0.9980532096144069\n",
      "7 Train Loss 2.3781948 Test MSE 383.00362927931724 Test RE 0.9976174272592486\n",
      "8 Train Loss 2.3781857 Test MSE 382.98925119614483 Test RE 0.9975987016390884\n",
      "9 Train Loss 2.37818 Test MSE 382.9750567814411 Test RE 0.9975802148788979\n",
      "10 Train Loss 2.3781724 Test MSE 382.96052945016964 Test RE 0.9975612941744946\n",
      "11 Train Loss 2.3781638 Test MSE 382.94448997329744 Test RE 0.9975404036027322\n",
      "12 Train Loss 2.3777063 Test MSE 382.695989509407 Test RE 0.9972166889552002\n",
      "13 Train Loss 2.3770514 Test MSE 382.68820546975286 Test RE 0.9972065472056977\n",
      "14 Train Loss 2.3721144 Test MSE 381.8433065098245 Test RE 0.9961051227094255\n",
      "15 Train Loss 2.363501 Test MSE 379.66052422227096 Test RE 0.9932539573522537\n",
      "16 Train Loss 2.34759 Test MSE 375.3542326336629 Test RE 0.9876049119632594\n",
      "17 Train Loss 2.3310642 Test MSE 373.86322199654774 Test RE 0.9856414404797714\n",
      "18 Train Loss 2.3172214 Test MSE 370.69702133959146 Test RE 0.9814589295971666\n",
      "19 Train Loss 2.2521915 Test MSE 359.8447645709901 Test RE 0.9669859784842574\n",
      "20 Train Loss 2.193582 Test MSE 347.3271386698197 Test RE 0.950018235952014\n",
      "21 Train Loss 2.1653724 Test MSE 342.6718362474466 Test RE 0.9436301061456527\n",
      "22 Train Loss 2.1264043 Test MSE 335.94362465046254 Test RE 0.9343202989371975\n",
      "23 Train Loss 2.0524473 Test MSE 326.70688774757133 Test RE 0.9213862518506014\n",
      "24 Train Loss 2.02932 Test MSE 323.2028312991143 Test RE 0.916431821348293\n",
      "25 Train Loss 2.0035586 Test MSE 317.92577018680754 Test RE 0.9089195578902127\n",
      "26 Train Loss 1.949945 Test MSE 308.19040928703157 Test RE 0.8948951251747254\n",
      "27 Train Loss 1.9137769 Test MSE 304.36599482047853 Test RE 0.8893252993969439\n",
      "28 Train Loss 1.8299365 Test MSE 287.38487231945356 Test RE 0.8641607448971905\n",
      "29 Train Loss 1.7965776 Test MSE 275.10505195416266 Test RE 0.8454965997801487\n",
      "30 Train Loss 1.6988415 Test MSE 258.77050431294924 Test RE 0.8200115482975767\n",
      "31 Train Loss 1.6259217 Test MSE 253.9831196436143 Test RE 0.8123908236029985\n",
      "32 Train Loss 1.5572461 Test MSE 245.7285284268942 Test RE 0.7990802051700145\n",
      "33 Train Loss 1.5202653 Test MSE 238.9350606253056 Test RE 0.7879570095343486\n",
      "34 Train Loss 1.4966344 Test MSE 234.58031200599305 Test RE 0.7807434731776552\n",
      "35 Train Loss 1.4830709 Test MSE 234.31695108732688 Test RE 0.7803050837405497\n",
      "36 Train Loss 1.4588503 Test MSE 226.9986524631599 Test RE 0.7680229929658835\n",
      "37 Train Loss 1.4274151 Test MSE 223.9955882457506 Test RE 0.762925823027956\n",
      "38 Train Loss 1.4201018 Test MSE 222.19681144756876 Test RE 0.7598563440743965\n",
      "39 Train Loss 1.3488882 Test MSE 212.5389270463436 Test RE 0.7431591416316182\n",
      "40 Train Loss 1.3341296 Test MSE 211.29626656999105 Test RE 0.7409834265163956\n",
      "41 Train Loss 1.2892793 Test MSE 199.54589312389464 Test RE 0.7200853568514599\n",
      "42 Train Loss 1.2532514 Test MSE 190.2550301759949 Test RE 0.703121950962765\n",
      "43 Train Loss 1.2278622 Test MSE 189.17643458652438 Test RE 0.7011260453831606\n",
      "44 Train Loss 1.1579176 Test MSE 178.67173538212228 Test RE 0.681381770394923\n",
      "45 Train Loss 1.1217562 Test MSE 173.0173964556296 Test RE 0.6705134112343948\n",
      "46 Train Loss 1.0981766 Test MSE 169.75932007458638 Test RE 0.6641702157788052\n",
      "47 Train Loss 1.0450906 Test MSE 158.88972564382445 Test RE 0.6425552703528006\n",
      "48 Train Loss 1.0338182 Test MSE 153.47233488805355 Test RE 0.631506220367806\n",
      "49 Train Loss 1.0220534 Test MSE 149.90217213991392 Test RE 0.624117766336862\n",
      "50 Train Loss 1.0139589 Test MSE 149.01559717611232 Test RE 0.6222694016661034\n",
      "51 Train Loss 1.0011573 Test MSE 149.65548058432077 Test RE 0.623604004671912\n",
      "52 Train Loss 0.956806 Test MSE 145.2504741315727 Test RE 0.6143577792809014\n",
      "53 Train Loss 0.9221064 Test MSE 140.91601172635384 Test RE 0.605121737357132\n",
      "54 Train Loss 0.8853837 Test MSE 136.48276196068352 Test RE 0.5955270377410339\n",
      "55 Train Loss 0.8533866 Test MSE 132.6973415719034 Test RE 0.5872103391867971\n",
      "56 Train Loss 0.83956593 Test MSE 129.39026562991216 Test RE 0.5798469602013375\n",
      "57 Train Loss 0.829208 Test MSE 127.00432100580824 Test RE 0.5744759222634199\n",
      "58 Train Loss 0.8202136 Test MSE 126.79084348331182 Test RE 0.5739929100577938\n",
      "59 Train Loss 0.8047718 Test MSE 123.68849670661128 Test RE 0.5669271270041175\n",
      "60 Train Loss 0.78560877 Test MSE 119.455793489566 Test RE 0.5571423750988105\n",
      "61 Train Loss 0.7755759 Test MSE 117.66054676393873 Test RE 0.5529400069294309\n",
      "62 Train Loss 0.7662487 Test MSE 118.12694771023246 Test RE 0.5540348372999967\n",
      "63 Train Loss 0.74370396 Test MSE 114.18671508762864 Test RE 0.5447163012577796\n",
      "64 Train Loss 0.7352341 Test MSE 110.03537883737337 Test RE 0.534722863646724\n",
      "65 Train Loss 0.69843215 Test MSE 102.39040162331816 Test RE 0.5158129077744974\n",
      "66 Train Loss 0.6586816 Test MSE 100.47321745382033 Test RE 0.5109609816103614\n",
      "67 Train Loss 0.6484455 Test MSE 97.90889614995221 Test RE 0.5043983528157192\n",
      "68 Train Loss 0.64516795 Test MSE 97.16425849898704 Test RE 0.5024766128872842\n",
      "69 Train Loss 0.6293531 Test MSE 94.72377777849528 Test RE 0.4961261146729053\n",
      "70 Train Loss 0.61143416 Test MSE 92.44633719546813 Test RE 0.4901256563407509\n",
      "71 Train Loss 0.5960492 Test MSE 90.59961429896255 Test RE 0.48520554670478305\n",
      "72 Train Loss 0.5913515 Test MSE 89.27133475561476 Test RE 0.48163561821865536\n",
      "73 Train Loss 0.5829604 Test MSE 88.62619043015177 Test RE 0.4798921251421998\n",
      "74 Train Loss 0.5777283 Test MSE 87.6921182099931 Test RE 0.47735652446666993\n",
      "75 Train Loss 0.5731354 Test MSE 84.89252560988447 Test RE 0.4696748547051866\n",
      "76 Train Loss 0.55976105 Test MSE 80.56950967101817 Test RE 0.45755988571080497\n",
      "77 Train Loss 0.54168874 Test MSE 78.70535370995219 Test RE 0.45223557189604674\n",
      "78 Train Loss 0.5398526 Test MSE 79.28827861079394 Test RE 0.4539072056784616\n",
      "79 Train Loss 0.5244669 Test MSE 76.22485237839524 Test RE 0.44505212448301296\n",
      "80 Train Loss 0.4986833 Test MSE 73.38193263036794 Test RE 0.4366738198826019\n",
      "81 Train Loss 0.4919277 Test MSE 73.10180049357152 Test RE 0.4358395318430303\n",
      "82 Train Loss 0.4803645 Test MSE 70.5434715864934 Test RE 0.42814511843862174\n",
      "83 Train Loss 0.4641953 Test MSE 70.21257483280456 Test RE 0.42713979256188156\n",
      "84 Train Loss 0.46003902 Test MSE 69.26932839381618 Test RE 0.42426096064265595\n",
      "85 Train Loss 0.45163324 Test MSE 66.58192749526475 Test RE 0.4159496510323023\n",
      "86 Train Loss 0.43948448 Test MSE 65.55422385047244 Test RE 0.41272703963425406\n",
      "87 Train Loss 0.43364128 Test MSE 64.2458034129975 Test RE 0.40858739530978616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 Train Loss 0.43163052 Test MSE 63.208817330550545 Test RE 0.4052764934059438\n",
      "89 Train Loss 0.42912462 Test MSE 63.28816656903959 Test RE 0.40553079570350736\n",
      "90 Train Loss 0.41980347 Test MSE 61.92024807919887 Test RE 0.4011242576379726\n",
      "91 Train Loss 0.41091177 Test MSE 60.084840398506934 Test RE 0.3951345800724092\n",
      "92 Train Loss 0.40019822 Test MSE 60.145999271356985 Test RE 0.3953356277827722\n",
      "93 Train Loss 0.38437173 Test MSE 56.72758589483219 Test RE 0.3839367935605864\n",
      "94 Train Loss 0.36896503 Test MSE 55.21668576492888 Test RE 0.37878934167795764\n",
      "95 Train Loss 0.3656722 Test MSE 54.72849328189296 Test RE 0.3771111110309978\n",
      "96 Train Loss 0.3503491 Test MSE 51.366378103836524 Test RE 0.3653440633295721\n",
      "97 Train Loss 0.33371413 Test MSE 48.638449663007684 Test RE 0.3555105099253795\n",
      "98 Train Loss 0.32987252 Test MSE 48.30443990432066 Test RE 0.3542877268907029\n",
      "99 Train Loss 0.32890576 Test MSE 48.032539419595714 Test RE 0.3532891960419192\n",
      "Training time: 72.40\n",
      "Training time: 72.40\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 4.3035097 Test MSE 386.64165515263664 Test RE 1.0023442495472257\n",
      "1 Train Loss 3.956913 Test MSE 386.1263661513327 Test RE 1.001676099642826\n",
      "2 Train Loss 2.4526808 Test MSE 382.79663530135906 Test RE 0.9973478100907718\n",
      "3 Train Loss 2.3818352 Test MSE 383.7290811231248 Test RE 0.9985617799592825\n",
      "4 Train Loss 2.3807929 Test MSE 383.6944819167318 Test RE 0.9985167609267805\n",
      "5 Train Loss 2.3801842 Test MSE 383.46583329432156 Test RE 0.998219201910558\n",
      "6 Train Loss 2.3785462 Test MSE 383.1011389412039 Test RE 0.9977444118879414\n",
      "7 Train Loss 2.3783352 Test MSE 383.1146384132334 Test RE 0.997761990671663\n",
      "8 Train Loss 2.3770866 Test MSE 382.9834724379141 Test RE 0.9975911754442455\n",
      "9 Train Loss 2.375597 Test MSE 382.15578593571473 Test RE 0.9965126180061045\n",
      "10 Train Loss 2.371144 Test MSE 381.1576636529974 Test RE 0.9952104111687378\n",
      "11 Train Loss 2.365923 Test MSE 378.9389422057177 Test RE 0.9923096202166592\n",
      "12 Train Loss 2.3420126 Test MSE 373.7834916996664 Test RE 0.9855363356308747\n",
      "13 Train Loss 2.287783 Test MSE 366.4077108203304 Test RE 0.9757642104428025\n",
      "14 Train Loss 2.2525277 Test MSE 361.3343992286402 Test RE 0.9689854075399256\n",
      "15 Train Loss 2.231958 Test MSE 354.5785658856951 Test RE 0.9598841535811355\n",
      "16 Train Loss 2.157594 Test MSE 342.9174216703109 Test RE 0.9439681851295908\n",
      "17 Train Loss 2.0713391 Test MSE 330.89498596585383 Test RE 0.9272731310461741\n",
      "18 Train Loss 1.9184061 Test MSE 301.67495843232905 Test RE 0.885385108719956\n",
      "19 Train Loss 1.8557884 Test MSE 292.911141350471 Test RE 0.8724298733459908\n",
      "20 Train Loss 1.8387145 Test MSE 287.7322466152797 Test RE 0.8646828610585899\n",
      "21 Train Loss 1.7529283 Test MSE 268.1077043441504 Test RE 0.8346746612639698\n",
      "22 Train Loss 1.6071161 Test MSE 253.5762567360913 Test RE 0.8117398666197115\n",
      "23 Train Loss 1.5614873 Test MSE 243.96327641546645 Test RE 0.7962048361636437\n",
      "24 Train Loss 1.5030918 Test MSE 234.20036222079673 Test RE 0.7801109317506334\n",
      "25 Train Loss 1.4469614 Test MSE 225.5782512761656 Test RE 0.7656163428702288\n",
      "26 Train Loss 1.4000651 Test MSE 218.03450834548298 Test RE 0.7527056915925774\n",
      "27 Train Loss 1.3552778 Test MSE 207.177830281903 Test RE 0.7337265304289796\n",
      "28 Train Loss 1.2671298 Test MSE 195.70726602564727 Test RE 0.7131256498728595\n",
      "29 Train Loss 1.2215711 Test MSE 187.0545369436517 Test RE 0.6971828665867185\n",
      "30 Train Loss 1.186096 Test MSE 181.29247903661948 Test RE 0.6863608072542322\n",
      "31 Train Loss 1.1351767 Test MSE 172.0819930449026 Test RE 0.668698418724337\n",
      "32 Train Loss 1.0921471 Test MSE 166.9427566638304 Test RE 0.658637376952964\n",
      "33 Train Loss 1.0420244 Test MSE 160.91881554013906 Test RE 0.6466451076318439\n",
      "34 Train Loss 0.9693355 Test MSE 150.11776179070085 Test RE 0.6245664088928845\n",
      "35 Train Loss 0.89524347 Test MSE 131.9057901877356 Test RE 0.585456338766278\n",
      "36 Train Loss 0.849921 Test MSE 124.49527833932552 Test RE 0.5687730665238591\n",
      "37 Train Loss 0.79882526 Test MSE 119.45295199074458 Test RE 0.5571357486775608\n",
      "38 Train Loss 0.7784819 Test MSE 116.52446074323619 Test RE 0.5502640412800484\n",
      "39 Train Loss 0.74633497 Test MSE 107.96686259084403 Test RE 0.5296729851455215\n",
      "40 Train Loss 0.6771907 Test MSE 96.67277907489465 Test RE 0.5012041799790701\n",
      "41 Train Loss 0.6550797 Test MSE 93.7030879414565 Test RE 0.49344588780716475\n",
      "42 Train Loss 0.6020565 Test MSE 88.5117661113204 Test RE 0.4795822333112834\n",
      "43 Train Loss 0.53012246 Test MSE 82.47850884360844 Test RE 0.462948821443681\n",
      "44 Train Loss 0.4904182 Test MSE 76.59812659708865 Test RE 0.44614050704048186\n",
      "45 Train Loss 0.4527546 Test MSE 67.24913675944404 Test RE 0.4180285459429339\n",
      "46 Train Loss 0.42223525 Test MSE 60.79494858472299 Test RE 0.39746265594955316\n",
      "47 Train Loss 0.40400082 Test MSE 58.089025427043126 Test RE 0.3885166431206644\n",
      "48 Train Loss 0.37845317 Test MSE 53.463500479958604 Test RE 0.3727273638596834\n",
      "49 Train Loss 0.35285807 Test MSE 48.47425289173849 Test RE 0.3549099251307086\n",
      "50 Train Loss 0.30598384 Test MSE 40.87592463847408 Test RE 0.3259090177998392\n",
      "51 Train Loss 0.29110542 Test MSE 37.24747233775374 Test RE 0.31110786155707354\n",
      "52 Train Loss 0.28641567 Test MSE 36.195487890550034 Test RE 0.3066830685803555\n",
      "53 Train Loss 0.2809611 Test MSE 36.32354312319102 Test RE 0.30722509329215686\n",
      "54 Train Loss 0.2617653 Test MSE 37.1946634638138 Test RE 0.3108872414421055\n",
      "55 Train Loss 0.2516838 Test MSE 36.20785837715699 Test RE 0.3067354714476291\n",
      "56 Train Loss 0.24874356 Test MSE 34.87498924484777 Test RE 0.30103682418260813\n",
      "57 Train Loss 0.24128407 Test MSE 31.831702725232077 Test RE 0.2876024078343663\n",
      "58 Train Loss 0.21734342 Test MSE 29.607655971043396 Test RE 0.27737326291503805\n",
      "59 Train Loss 0.18299909 Test MSE 26.86328352259818 Test RE 0.26420566807532975\n",
      "60 Train Loss 0.15200761 Test MSE 21.436747213391133 Test RE 0.23601631531287767\n",
      "61 Train Loss 0.13177207 Test MSE 15.828171344411608 Test RE 0.20280467126437957\n",
      "62 Train Loss 0.10784124 Test MSE 12.923444390773527 Test RE 0.18325326524543678\n",
      "63 Train Loss 0.09956553 Test MSE 11.095930989706346 Test RE 0.16980264699803246\n",
      "64 Train Loss 0.082029685 Test MSE 7.682943826193988 Test RE 0.1412948756203919\n",
      "65 Train Loss 0.06672192 Test MSE 4.365495359517322 Test RE 0.10650729330131517\n",
      "66 Train Loss 0.057662196 Test MSE 3.422569783449936 Test RE 0.09430587271737914\n",
      "67 Train Loss 0.04782252 Test MSE 3.0745943596211265 Test RE 0.0893833228397777\n",
      "68 Train Loss 0.03684926 Test MSE 2.1514679309899254 Test RE 0.07477045458348498\n",
      "69 Train Loss 0.035118163 Test MSE 1.8897034663822185 Test RE 0.07007440395518427\n",
      "70 Train Loss 0.029217813 Test MSE 1.8800066638995554 Test RE 0.06989438322663119\n",
      "71 Train Loss 0.025982287 Test MSE 2.2027648641401547 Test RE 0.07565657082834075\n",
      "72 Train Loss 0.023676738 Test MSE 2.19480411459952 Test RE 0.07551973639662653\n",
      "73 Train Loss 0.022407603 Test MSE 1.8232407476513408 Test RE 0.06883108122932434\n",
      "74 Train Loss 0.019686058 Test MSE 1.268072615835386 Test RE 0.05740300719214355\n",
      "75 Train Loss 0.018478202 Test MSE 1.1820147829502468 Test RE 0.055420959009786476\n",
      "76 Train Loss 0.0176358 Test MSE 0.9968458518351184 Test RE 0.050895171896950496\n",
      "77 Train Loss 0.01719509 Test MSE 0.8925529941315327 Test RE 0.04815923594337878\n",
      "78 Train Loss 0.016082019 Test MSE 0.876137035319466 Test RE 0.047714304958091154\n",
      "79 Train Loss 0.014325278 Test MSE 0.5179913897233573 Test RE 0.036687984416566725\n",
      "80 Train Loss 0.0126229385 Test MSE 0.32400572902051117 Test RE 0.0290160925117289\n",
      "81 Train Loss 0.009851056 Test MSE 0.305429376449441 Test RE 0.028172019822151055\n",
      "82 Train Loss 0.008352988 Test MSE 0.14672906585628723 Test RE 0.01952633153412105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 Train Loss 0.0070774467 Test MSE 0.051880645637080836 Test RE 0.011610883528620879\n",
      "84 Train Loss 0.0066052023 Test MSE 0.025491316642505084 Test RE 0.008138768889921712\n",
      "85 Train Loss 0.0064064744 Test MSE 0.011911965102576829 Test RE 0.005563579378631222\n",
      "86 Train Loss 0.006085333 Test MSE 0.013215231947910895 Test RE 0.005860031864846258\n",
      "87 Train Loss 0.005727466 Test MSE 0.026527902523946524 Test RE 0.00830259854587768\n",
      "88 Train Loss 0.005550961 Test MSE 0.026825667445441365 Test RE 0.008349065164232249\n",
      "89 Train Loss 0.0055436534 Test MSE 0.02689223418700306 Test RE 0.008359417669187243\n",
      "90 Train Loss 0.005539143 Test MSE 0.027010615599824307 Test RE 0.008377796822564619\n",
      "91 Train Loss 0.005532927 Test MSE 0.027283842581331416 Test RE 0.008420063177132033\n",
      "92 Train Loss 0.00552989 Test MSE 0.02751936545270284 Test RE 0.008456327425785852\n",
      "93 Train Loss 0.0055264193 Test MSE 0.027899964684839774 Test RE 0.008514603114775018\n",
      "94 Train Loss 0.0055249454 Test MSE 0.028431924859207584 Test RE 0.008595392515402919\n",
      "95 Train Loss 0.0055211294 Test MSE 0.02911908662503881 Test RE 0.008698641985414337\n",
      "96 Train Loss 0.0055168853 Test MSE 0.030039259251643434 Test RE 0.008835012983347214\n",
      "97 Train Loss 0.0055131577 Test MSE 0.03140729693957155 Test RE 0.009033953749726204\n",
      "98 Train Loss 0.0055091893 Test MSE 0.03270562528112724 Test RE 0.009218787638979079\n",
      "99 Train Loss 0.0055047744 Test MSE 0.03434342498695376 Test RE 0.00944679268413301\n",
      "Training time: 67.42\n",
      "Training time: 67.42\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 4.318307 Test MSE 386.81149271903314 Test RE 1.0025643719981927\n",
      "1 Train Loss 2.437343 Test MSE 383.8260622143148 Test RE 0.9986879568609099\n",
      "2 Train Loss 2.3967328 Test MSE 384.0428299430537 Test RE 0.9989699241177599\n",
      "3 Train Loss 2.3850458 Test MSE 384.1479363014812 Test RE 0.99910661578289\n",
      "4 Train Loss 2.3816164 Test MSE 383.84663831308296 Test RE 0.9987147252707064\n",
      "5 Train Loss 2.38126 Test MSE 383.7841121646208 Test RE 0.9986333798554261\n",
      "6 Train Loss 2.3811557 Test MSE 383.7682613371076 Test RE 0.9986127571558454\n",
      "7 Train Loss 2.3811464 Test MSE 383.7667187093853 Test RE 0.9986107500992096\n",
      "8 Train Loss 2.3811383 Test MSE 383.76544994226964 Test RE 0.998609099349592\n",
      "9 Train Loss 2.3811312 Test MSE 383.7636596107695 Test RE 0.998606770005702\n",
      "10 Train Loss 2.380519 Test MSE 383.64201612246075 Test RE 0.9984484907686091\n",
      "11 Train Loss 2.3793695 Test MSE 383.53942359273094 Test RE 0.9983149806195667\n",
      "12 Train Loss 2.3781214 Test MSE 383.0730391490667 Test RE 0.997707819821442\n",
      "13 Train Loss 2.378114 Test MSE 383.0646520449862 Test RE 0.9976968977200746\n",
      "14 Train Loss 2.3768008 Test MSE 382.82068133640576 Test RE 0.9973791346657207\n",
      "15 Train Loss 2.3763282 Test MSE 382.78669735980094 Test RE 0.9973348637275254\n",
      "16 Train Loss 2.3718748 Test MSE 381.5929896150177 Test RE 0.9957785714588022\n",
      "17 Train Loss 2.3656378 Test MSE 380.40286430462567 Test RE 0.9942245246194718\n",
      "18 Train Loss 2.3619738 Test MSE 379.0143136702945 Test RE 0.992408301168101\n",
      "19 Train Loss 2.3564548 Test MSE 378.1190037968352 Test RE 0.9912354717368931\n",
      "20 Train Loss 2.3525858 Test MSE 377.09210307215886 Test RE 0.9898885512422515\n",
      "21 Train Loss 2.3401022 Test MSE 373.5747553576465 Test RE 0.9852611148379583\n",
      "22 Train Loss 2.3308573 Test MSE 372.36469873914785 Test RE 0.9836641269226283\n",
      "23 Train Loss 2.3127503 Test MSE 370.518876795584 Test RE 0.9812230731507924\n",
      "24 Train Loss 2.2957876 Test MSE 367.12489536718914 Test RE 0.9767186948723486\n",
      "25 Train Loss 2.279353 Test MSE 362.750011306102 Test RE 0.9708816656567658\n",
      "26 Train Loss 2.233797 Test MSE 356.0547645961958 Test RE 0.9618801964024398\n",
      "27 Train Loss 2.2069087 Test MSE 349.93295596623216 Test RE 0.9535753263323217\n",
      "28 Train Loss 2.178727 Test MSE 343.5852415329222 Test RE 0.9448869108073605\n",
      "29 Train Loss 2.1562338 Test MSE 342.3183483433542 Test RE 0.9431432731758086\n",
      "30 Train Loss 2.1249917 Test MSE 339.3560824550023 Test RE 0.9390536419768679\n",
      "31 Train Loss 2.098073 Test MSE 333.7824284040367 Test RE 0.9313101099835432\n",
      "32 Train Loss 2.0396216 Test MSE 324.0224901787713 Test RE 0.9175931444622064\n",
      "33 Train Loss 1.987531 Test MSE 313.7128214750568 Test RE 0.9028772640197612\n",
      "34 Train Loss 1.8627324 Test MSE 295.2700659600335 Test RE 0.8759358332651648\n",
      "35 Train Loss 1.7650766 Test MSE 279.4749313632641 Test RE 0.852185247339518\n",
      "36 Train Loss 1.6829553 Test MSE 264.265346083089 Test RE 0.8286720497456672\n",
      "37 Train Loss 1.6380035 Test MSE 257.11234421664665 Test RE 0.817380074435767\n",
      "38 Train Loss 1.5109951 Test MSE 236.42343257785654 Test RE 0.7838046610390978\n",
      "39 Train Loss 1.4682198 Test MSE 229.71560765808994 Test RE 0.7726055691794538\n",
      "40 Train Loss 1.3557436 Test MSE 203.28314527190278 Test RE 0.7267972382354266\n",
      "41 Train Loss 1.2123822 Test MSE 179.792964771195 Test RE 0.683516384579957\n",
      "42 Train Loss 1.075266 Test MSE 164.31097655052693 Test RE 0.6534251871574357\n",
      "43 Train Loss 1.0150955 Test MSE 151.32547183530178 Test RE 0.6270737208080466\n",
      "44 Train Loss 0.99497813 Test MSE 143.23598294845223 Test RE 0.6100826142143599\n",
      "45 Train Loss 0.92184 Test MSE 129.26838580089617 Test RE 0.579573800940691\n",
      "46 Train Loss 0.8595512 Test MSE 124.68890932233236 Test RE 0.5692152089883717\n",
      "47 Train Loss 0.7885257 Test MSE 118.9771431804828 Test RE 0.5560250410759426\n",
      "48 Train Loss 0.7513441 Test MSE 114.58852525975844 Test RE 0.5456738572295029\n",
      "49 Train Loss 0.7175748 Test MSE 109.62234735648984 Test RE 0.5337183457254439\n",
      "50 Train Loss 0.65368026 Test MSE 96.0204328843566 Test RE 0.499510259067234\n",
      "51 Train Loss 0.6072318 Test MSE 85.64723201304554 Test RE 0.4717579726583145\n",
      "52 Train Loss 0.5574552 Test MSE 82.27772902157461 Test RE 0.46238499323899523\n",
      "53 Train Loss 0.52487934 Test MSE 79.83648676930267 Test RE 0.4554736856097335\n",
      "54 Train Loss 0.50822896 Test MSE 74.89133297451635 Test RE 0.4411419542918266\n",
      "55 Train Loss 0.49868035 Test MSE 72.57411384998065 Test RE 0.4342636250685262\n",
      "56 Train Loss 0.4814426 Test MSE 72.46470521852935 Test RE 0.4339361659610758\n",
      "57 Train Loss 0.4696521 Test MSE 70.6410269700105 Test RE 0.42844105957925976\n",
      "58 Train Loss 0.4615343 Test MSE 68.25511160688222 Test RE 0.42114356883864146\n",
      "59 Train Loss 0.44423252 Test MSE 65.96386636397771 Test RE 0.41401457861603863\n",
      "60 Train Loss 0.435959 Test MSE 65.95891105670148 Test RE 0.4139990276174564\n",
      "61 Train Loss 0.4299132 Test MSE 64.63428340854264 Test RE 0.4098208517816377\n",
      "62 Train Loss 0.41555244 Test MSE 61.16532502035811 Test RE 0.39867153329315846\n",
      "63 Train Loss 0.40872145 Test MSE 60.04696526771679 Test RE 0.39501002175617017\n",
      "64 Train Loss 0.4002412 Test MSE 57.776469642845015 Test RE 0.38747000039506135\n",
      "65 Train Loss 0.39323604 Test MSE 54.76141277587863 Test RE 0.37722451119926215\n",
      "66 Train Loss 0.37396163 Test MSE 47.77548965560958 Test RE 0.35234260098589415\n",
      "67 Train Loss 0.35114172 Test MSE 47.52915836986604 Test RE 0.3514330846996471\n",
      "68 Train Loss 0.34384233 Test MSE 48.21936101179276 Test RE 0.3539755848629639\n",
      "69 Train Loss 0.34006536 Test MSE 48.09732809858512 Test RE 0.3535273827720777\n",
      "70 Train Loss 0.33327246 Test MSE 45.12551443492141 Test RE 0.34243146629531024\n",
      "71 Train Loss 0.3263399 Test MSE 41.95271822578379 Test RE 0.33017382053298294\n",
      "72 Train Loss 0.30389774 Test MSE 39.47958867458373 Test RE 0.3202940656070556\n",
      "73 Train Loss 0.28347877 Test MSE 36.933969220512175 Test RE 0.30979583452372084\n",
      "74 Train Loss 0.26408285 Test MSE 34.65179123893047 Test RE 0.3000719684206621\n",
      "75 Train Loss 0.25802168 Test MSE 35.257290460096925 Test RE 0.3026823166489801\n",
      "76 Train Loss 0.24196737 Test MSE 32.08525453517896 Test RE 0.2887455680661535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 Train Loss 0.22656849 Test MSE 31.131351804179857 Test RE 0.28442094311575544\n",
      "78 Train Loss 0.2112812 Test MSE 30.97181826335397 Test RE 0.28369124527244893\n",
      "79 Train Loss 0.1953136 Test MSE 28.288429900713382 Test RE 0.2711234013248126\n",
      "80 Train Loss 0.17792903 Test MSE 25.319935748309817 Test RE 0.25650384652859637\n",
      "81 Train Loss 0.15434282 Test MSE 22.35548352535267 Test RE 0.2410208514522464\n",
      "82 Train Loss 0.15089473 Test MSE 21.25515113982913 Test RE 0.235014512405765\n",
      "83 Train Loss 0.14758319 Test MSE 21.164711101109027 Test RE 0.23451398951499988\n",
      "84 Train Loss 0.14458203 Test MSE 19.941597884145086 Test RE 0.22763684609247384\n",
      "85 Train Loss 0.13154912 Test MSE 17.867303221131266 Test RE 0.2154726170203847\n",
      "86 Train Loss 0.122033775 Test MSE 17.42912169196671 Test RE 0.21281406808467504\n",
      "87 Train Loss 0.11363382 Test MSE 15.31764405322722 Test RE 0.19950719800373237\n",
      "88 Train Loss 0.10944723 Test MSE 14.246456361114205 Test RE 0.1924048470643374\n",
      "89 Train Loss 0.10367135 Test MSE 12.602861526007175 Test RE 0.18096607406163354\n",
      "90 Train Loss 0.09664357 Test MSE 11.78682532179797 Test RE 0.1750092510472353\n",
      "91 Train Loss 0.09425175 Test MSE 12.339161844959172 Test RE 0.179062817145593\n",
      "92 Train Loss 0.092126876 Test MSE 12.293420783994732 Test RE 0.17873061760187903\n",
      "93 Train Loss 0.080651976 Test MSE 10.27669981248907 Test RE 0.16341405916961949\n",
      "94 Train Loss 0.075232156 Test MSE 9.67372218305617 Test RE 0.15854749618691877\n",
      "95 Train Loss 0.07295056 Test MSE 9.297707229848143 Test RE 0.1554356078668371\n",
      "96 Train Loss 0.07268094 Test MSE 9.155160136318141 Test RE 0.15423948095735307\n",
      "97 Train Loss 0.07267139 Test MSE 9.138278888890456 Test RE 0.15409721385445616\n",
      "98 Train Loss 0.07196822 Test MSE 8.924439844836625 Test RE 0.15228357572259843\n",
      "99 Train Loss 0.071595475 Test MSE 8.923733087721832 Test RE 0.1522775456729028\n",
      "Training time: 70.76\n",
      "Training time: 70.76\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 4.377245 Test MSE 387.22714793026284 Test RE 1.0031028891118046\n",
      "1 Train Loss 3.330648 Test MSE 384.6854300383764 Test RE 0.9998053385714589\n",
      "2 Train Loss 2.4256516 Test MSE 383.6089174569545 Test RE 0.998405419332151\n",
      "3 Train Loss 2.3887112 Test MSE 383.9991900560949 Test RE 0.9989131645981315\n",
      "4 Train Loss 2.3814838 Test MSE 383.538969058176 Test RE 0.9983143890652674\n",
      "5 Train Loss 2.3808534 Test MSE 383.4099369728263 Test RE 0.9981464459958764\n",
      "6 Train Loss 2.3791945 Test MSE 383.4503791064679 Test RE 0.9981990869209579\n",
      "7 Train Loss 2.3778772 Test MSE 383.23266449171314 Test RE 0.9979156690421545\n",
      "8 Train Loss 2.3745177 Test MSE 382.1799138102163 Test RE 0.9965440755330816\n",
      "9 Train Loss 2.373961 Test MSE 382.143299104326 Test RE 0.9964963374902228\n",
      "10 Train Loss 2.3681407 Test MSE 380.4623548596532 Test RE 0.9943022641188931\n",
      "11 Train Loss 2.3445284 Test MSE 376.158140652811 Test RE 0.9886619386205263\n",
      "12 Train Loss 2.3324037 Test MSE 372.65166130476166 Test RE 0.9840430839233248\n",
      "13 Train Loss 2.2635722 Test MSE 358.55113764942007 Test RE 0.9652462764048018\n",
      "14 Train Loss 2.2237182 Test MSE 352.6138268027766 Test RE 0.957221074287292\n",
      "15 Train Loss 2.149138 Test MSE 339.2541837156945 Test RE 0.9389126461742376\n",
      "16 Train Loss 2.0468447 Test MSE 320.0293181171369 Test RE 0.9119215222657769\n",
      "17 Train Loss 1.8803916 Test MSE 294.7110101157225 Test RE 0.8751062045327274\n",
      "18 Train Loss 1.75751 Test MSE 275.0260556028031 Test RE 0.8453751989889797\n",
      "19 Train Loss 1.5885453 Test MSE 241.85954953451642 Test RE 0.7927645148897485\n",
      "20 Train Loss 1.5386134 Test MSE 238.85483577169418 Test RE 0.7878247161768362\n",
      "21 Train Loss 1.4662138 Test MSE 217.8000307312508 Test RE 0.7523008471659577\n",
      "22 Train Loss 1.319579 Test MSE 207.0809266049253 Test RE 0.7335549167129501\n",
      "23 Train Loss 1.265872 Test MSE 196.29316311898435 Test RE 0.7141923093053673\n",
      "24 Train Loss 1.1769886 Test MSE 181.82626275709458 Test RE 0.6873704987396637\n",
      "25 Train Loss 1.1367599 Test MSE 179.10175543568337 Test RE 0.6822012389490999\n",
      "26 Train Loss 1.0900911 Test MSE 167.92042636507614 Test RE 0.6605631562040002\n",
      "27 Train Loss 1.051872 Test MSE 157.29826935315702 Test RE 0.6393292211495691\n",
      "28 Train Loss 1.0270245 Test MSE 157.36260140931705 Test RE 0.6394599446441858\n",
      "29 Train Loss 0.9821308 Test MSE 150.57496785450414 Test RE 0.6255167909494233\n",
      "30 Train Loss 0.9517648 Test MSE 142.45411120756435 Test RE 0.6084152291116431\n",
      "31 Train Loss 0.92628866 Test MSE 140.22495546588934 Test RE 0.6036361463864578\n",
      "32 Train Loss 0.8930946 Test MSE 133.76810767206928 Test RE 0.5895747482859856\n",
      "33 Train Loss 0.844566 Test MSE 122.06128179180861 Test RE 0.5631856049145781\n",
      "34 Train Loss 0.7917417 Test MSE 113.59854180781237 Test RE 0.5433115791033367\n",
      "35 Train Loss 0.75095713 Test MSE 109.45359643740453 Test RE 0.533307388691637\n",
      "36 Train Loss 0.7199652 Test MSE 100.45285604594658 Test RE 0.5109092045678257\n",
      "37 Train Loss 0.64831275 Test MSE 90.88810849389478 Test RE 0.48597744694289985\n",
      "38 Train Loss 0.61293554 Test MSE 86.64045250773532 Test RE 0.47448549266078616\n",
      "39 Train Loss 0.5840798 Test MSE 79.4731032681831 Test RE 0.45443593711508157\n",
      "40 Train Loss 0.5395017 Test MSE 76.1682034919794 Test RE 0.4448867165325805\n",
      "41 Train Loss 0.4578607 Test MSE 67.47669093242733 Test RE 0.418735200417976\n",
      "42 Train Loss 0.42089528 Test MSE 60.644882327540344 Test RE 0.39697180442801305\n",
      "43 Train Loss 0.3886368 Test MSE 57.549841483756936 Test RE 0.3867093283344447\n",
      "44 Train Loss 0.36983797 Test MSE 54.181704505190616 Test RE 0.3752225358788131\n",
      "45 Train Loss 0.3563111 Test MSE 51.27121834373858 Test RE 0.3650054939066941\n",
      "46 Train Loss 0.34458447 Test MSE 47.548821408052774 Test RE 0.35150577194942506\n",
      "47 Train Loss 0.33310807 Test MSE 46.2102995406916 Test RE 0.3465229271907165\n",
      "48 Train Loss 0.32282642 Test MSE 44.615069476811065 Test RE 0.3404892221811147\n",
      "49 Train Loss 0.3169803 Test MSE 44.09374014088528 Test RE 0.3384940597985937\n",
      "50 Train Loss 0.30865026 Test MSE 44.88800482300923 Test RE 0.3415291157567781\n",
      "51 Train Loss 0.28603616 Test MSE 41.92446314517376 Test RE 0.33006261606734566\n",
      "52 Train Loss 0.2666072 Test MSE 37.505844441520225 Test RE 0.3121850175974353\n",
      "53 Train Loss 0.25524056 Test MSE 34.38367667857315 Test RE 0.2989088266114673\n",
      "54 Train Loss 0.24314553 Test MSE 32.44693500569244 Test RE 0.29036844709638315\n",
      "55 Train Loss 0.23579694 Test MSE 30.94638523409971 Test RE 0.28357474243278424\n",
      "56 Train Loss 0.2318373 Test MSE 28.59700302894194 Test RE 0.2725981117538468\n",
      "57 Train Loss 0.18816665 Test MSE 23.5612139898516 Test RE 0.2474351613607456\n",
      "58 Train Loss 0.17480373 Test MSE 23.319476357497493 Test RE 0.24616254854288022\n",
      "59 Train Loss 0.16991976 Test MSE 22.146881557558654 Test RE 0.23989371723986594\n",
      "60 Train Loss 0.16221756 Test MSE 19.450672606630455 Test RE 0.224817386308818\n",
      "61 Train Loss 0.14801148 Test MSE 18.266243294874872 Test RE 0.21786486706386737\n",
      "62 Train Loss 0.14407551 Test MSE 17.376446009313042 Test RE 0.2124922329006885\n",
      "63 Train Loss 0.13481583 Test MSE 16.163281642722914 Test RE 0.20494029300501515\n",
      "64 Train Loss 0.13034265 Test MSE 16.627352167069724 Test RE 0.20786153501884444\n",
      "65 Train Loss 0.12573133 Test MSE 16.38368439858528 Test RE 0.20633284634526752\n",
      "66 Train Loss 0.11798631 Test MSE 14.549799385845299 Test RE 0.19444245025010048\n",
      "67 Train Loss 0.10620431 Test MSE 13.586955925997126 Test RE 0.18789865260193428\n",
      "68 Train Loss 0.09718431 Test MSE 12.789682803712369 Test RE 0.18230243493491996\n",
      "69 Train Loss 0.089975014 Test MSE 11.180488181051583 Test RE 0.17044841455904083\n",
      "70 Train Loss 0.085863516 Test MSE 10.217116612383387 Test RE 0.16293964194620983\n",
      "71 Train Loss 0.0833876 Test MSE 9.646760503094743 Test RE 0.15832639775954221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 Train Loss 0.078963995 Test MSE 8.84708779024765 Test RE 0.15162218501897\n",
      "73 Train Loss 0.0750621 Test MSE 8.14869373443177 Test RE 0.14551460347191414\n",
      "74 Train Loss 0.07409494 Test MSE 8.078881516205442 Test RE 0.14488992978704482\n",
      "75 Train Loss 0.07175862 Test MSE 8.701342185571812 Test RE 0.15036809802948387\n",
      "76 Train Loss 0.068701334 Test MSE 8.517458345761913 Test RE 0.14877076345306597\n",
      "77 Train Loss 0.06574774 Test MSE 7.599160296639768 Test RE 0.14052234395250696\n",
      "78 Train Loss 0.06341919 Test MSE 7.701580084319071 Test RE 0.14146613893730767\n",
      "79 Train Loss 0.06185522 Test MSE 7.677725087160151 Test RE 0.14124687928195556\n",
      "80 Train Loss 0.05716316 Test MSE 6.428139285564819 Test RE 0.12924246178913942\n",
      "81 Train Loss 0.05011646 Test MSE 5.28489737017486 Test RE 0.1171873845388925\n",
      "82 Train Loss 0.04863572 Test MSE 5.115783147015538 Test RE 0.11529716958063571\n",
      "83 Train Loss 0.04370399 Test MSE 4.373943455135161 Test RE 0.10661029981370475\n",
      "84 Train Loss 0.03839846 Test MSE 4.230762566902501 Test RE 0.10485083878837592\n",
      "85 Train Loss 0.033941798 Test MSE 3.8057415709799773 Test RE 0.09944483456586013\n",
      "86 Train Loss 0.027522068 Test MSE 2.2554687723909224 Test RE 0.0765563099016587\n",
      "87 Train Loss 0.024473242 Test MSE 1.8391270898196819 Test RE 0.06913030186264982\n",
      "88 Train Loss 0.022789903 Test MSE 1.8487849005660524 Test RE 0.06931157621975807\n",
      "89 Train Loss 0.018940134 Test MSE 1.2745145753685838 Test RE 0.05754862952969569\n",
      "90 Train Loss 0.015413016 Test MSE 0.7102820911966993 Test RE 0.042961359383855466\n",
      "91 Train Loss 0.013612216 Test MSE 0.3814657628641786 Test RE 0.031484033458311814\n",
      "92 Train Loss 0.012325458 Test MSE 0.2952461978235548 Test RE 0.027698403605556157\n",
      "93 Train Loss 0.011667301 Test MSE 0.3088647658748452 Test RE 0.028330012539985443\n",
      "94 Train Loss 0.011562615 Test MSE 0.30970550345982983 Test RE 0.028368543836863422\n",
      "95 Train Loss 0.010285864 Test MSE 0.39078108562267183 Test RE 0.03186613194609285\n",
      "96 Train Loss 0.007890453 Test MSE 0.38532309869856646 Test RE 0.031642814450896034\n",
      "97 Train Loss 0.007526604 Test MSE 0.30267967993163736 Test RE 0.02804492064974671\n",
      "98 Train Loss 0.0072048474 Test MSE 0.34864959099940246 Test RE 0.030099353155133093\n",
      "99 Train Loss 0.0059118113 Test MSE 0.32272531982385133 Test RE 0.028958702709701002\n",
      "Training time: 74.59\n",
      "Training time: 74.59\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 5.102653 Test MSE 382.70214251025095 Test RE 0.9972247055671245\n",
      "1 Train Loss 4.9096165 Test MSE 383.28751507504984 Test RE 0.9979870803586122\n",
      "2 Train Loss 4.8929825 Test MSE 383.3428148916148 Test RE 0.9980590713703649\n",
      "3 Train Loss 4.891949 Test MSE 383.3547489519423 Test RE 0.9980746068180186\n",
      "4 Train Loss 4.891914 Test MSE 383.3561449543727 Test RE 0.9980764240816787\n",
      "5 Train Loss 4.8919067 Test MSE 383.35554102485474 Test RE 0.9980756379092659\n",
      "6 Train Loss 4.891903 Test MSE 383.35594613578616 Test RE 0.998076165267294\n",
      "7 Train Loss 4.8918996 Test MSE 383.35532629798377 Test RE 0.9980753583858654\n",
      "8 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "9 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "10 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "11 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "12 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "13 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "14 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "15 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "16 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "17 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "18 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "19 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "20 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "21 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "22 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "23 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "24 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "25 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "26 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "27 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "28 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "29 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "30 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "31 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "32 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "33 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "34 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "35 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "36 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "37 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "38 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "39 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "40 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "41 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "42 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "43 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "44 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "45 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "46 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "47 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "48 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "49 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "50 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "51 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "52 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "53 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "54 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "55 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "56 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "57 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "58 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "59 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "60 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "61 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "62 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "63 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "64 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "65 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "66 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "68 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "69 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "70 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "71 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "72 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "73 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "74 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "75 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "76 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "77 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "78 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "79 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "80 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "81 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "82 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "83 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "84 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "85 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "86 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "87 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "88 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "89 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "90 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "91 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "92 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "93 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "94 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "95 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "96 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "97 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "98 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "99 Train Loss 4.891897 Test MSE 383.35558080631847 Test RE 0.9980756896952772\n",
      "Training time: 12.77\n",
      "Training time: 12.77\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 4.301084 Test MSE 386.62016731532935 Test RE 1.0023163962247357\n",
      "1 Train Loss 2.4957032 Test MSE 382.7576278304586 Test RE 0.9972969932820087\n",
      "2 Train Loss 2.4000793 Test MSE 384.1252530230824 Test RE 0.9990771175792854\n",
      "3 Train Loss 2.383057 Test MSE 383.9967174572305 Test RE 0.9989099485554988\n",
      "4 Train Loss 2.3823946 Test MSE 383.9252569996536 Test RE 0.9988169973920158\n",
      "5 Train Loss 2.3809807 Test MSE 383.699215044645 Test RE 0.9985229195936378\n",
      "6 Train Loss 2.3802588 Test MSE 383.64655109675704 Test RE 0.9984543920055311\n",
      "7 Train Loss 2.3775828 Test MSE 383.0621168935376 Test RE 0.9976935962970553\n",
      "8 Train Loss 2.3762517 Test MSE 382.6766365794449 Test RE 0.9971914740205501\n",
      "9 Train Loss 2.3754025 Test MSE 382.4479864012608 Test RE 0.9968935173968269\n",
      "10 Train Loss 2.371643 Test MSE 381.76547057467326 Test RE 0.996003593202509\n",
      "11 Train Loss 2.3638573 Test MSE 379.4390639807614 Test RE 0.9929642270092294\n",
      "12 Train Loss 2.3540337 Test MSE 376.82242548944356 Test RE 0.9895345283204748\n",
      "13 Train Loss 2.3305945 Test MSE 373.0632333013045 Test RE 0.9845863430235948\n",
      "14 Train Loss 2.3032687 Test MSE 368.52557472920523 Test RE 0.9785801422044311\n",
      "15 Train Loss 2.2910566 Test MSE 366.7654475412923 Test RE 0.9762404306734626\n",
      "16 Train Loss 2.2775078 Test MSE 364.5632092779009 Test RE 0.9733051064038193\n",
      "17 Train Loss 2.2629068 Test MSE 361.0564433401763 Test RE 0.9686126406281919\n",
      "18 Train Loss 2.2303722 Test MSE 355.38679259072126 Test RE 0.9609775112912463\n",
      "19 Train Loss 2.2106926 Test MSE 350.80015962094876 Test RE 0.9547561701121924\n",
      "20 Train Loss 2.1954236 Test MSE 350.53583121419257 Test RE 0.9543963972779819\n",
      "21 Train Loss 2.1863627 Test MSE 349.32705334842467 Test RE 0.952749419405533\n",
      "22 Train Loss 2.171977 Test MSE 345.61149202200164 Test RE 0.9476689905974003\n",
      "23 Train Loss 2.1313367 Test MSE 336.97312708220073 Test RE 0.935750820533391\n",
      "24 Train Loss 2.101784 Test MSE 328.5136437505065 Test RE 0.9239304665353901\n",
      "25 Train Loss 2.0521472 Test MSE 323.57348431804957 Test RE 0.9169571584699824\n",
      "26 Train Loss 2.020012 Test MSE 321.03833127524524 Test RE 0.9133579792267761\n",
      "27 Train Loss 2.0055287 Test MSE 315.6942847616956 Test RE 0.9057241385809383\n",
      "28 Train Loss 1.9722933 Test MSE 308.9701221490403 Test RE 0.8960264395288826\n",
      "29 Train Loss 1.9543641 Test MSE 309.40906593364997 Test RE 0.8966626914161149\n",
      "30 Train Loss 1.9511669 Test MSE 309.46594639553314 Test RE 0.8967451069783049\n",
      "31 Train Loss 1.9250833 Test MSE 306.2532525882509 Test RE 0.8920782224970736\n",
      "32 Train Loss 1.8901585 Test MSE 301.32179954134716 Test RE 0.8848667143704462\n",
      "33 Train Loss 1.8596067 Test MSE 294.16301476303886 Test RE 0.8742922253312326\n",
      "34 Train Loss 1.8288805 Test MSE 286.82573211396686 Test RE 0.8633196737137727\n",
      "35 Train Loss 1.8141313 Test MSE 282.0099898894568 Test RE 0.8560415191054945\n",
      "36 Train Loss 1.7582823 Test MSE 276.35890332584387 Test RE 0.8474211770799741\n",
      "37 Train Loss 1.7492926 Test MSE 277.9370282650078 Test RE 0.8498372974247684\n",
      "38 Train Loss 1.7413269 Test MSE 275.63266736498514 Test RE 0.8463069872096629\n",
      "39 Train Loss 1.7307781 Test MSE 271.428970173244 Test RE 0.8398286420326426\n",
      "40 Train Loss 1.718811 Test MSE 267.992838544683 Test RE 0.8344958416292895\n",
      "41 Train Loss 1.6806549 Test MSE 265.0398590761421 Test RE 0.8298855037699404\n",
      "42 Train Loss 1.6572769 Test MSE 259.9725331298803 Test RE 0.8219138816195454\n",
      "43 Train Loss 1.619033 Test MSE 254.82216219097833 Test RE 0.813731598626122\n",
      "44 Train Loss 1.5743588 Test MSE 248.72128906773492 Test RE 0.8039315310283358\n",
      "45 Train Loss 1.5474515 Test MSE 242.9345320831938 Test RE 0.7945243444603197\n",
      "46 Train Loss 1.5259941 Test MSE 238.1154255055664 Test RE 0.7866043573681226\n",
      "47 Train Loss 1.4923192 Test MSE 234.74674392218884 Test RE 0.7810203881317063\n",
      "48 Train Loss 1.4375525 Test MSE 227.3703813547649 Test RE 0.7686515860158832\n",
      "49 Train Loss 1.4146105 Test MSE 224.03449104822812 Test RE 0.762992071350666\n",
      "50 Train Loss 1.3802339 Test MSE 215.08840136892567 Test RE 0.7476030738952885\n",
      "51 Train Loss 1.3328619 Test MSE 205.52891017237545 Test RE 0.7308008473170561\n",
      "52 Train Loss 1.3156507 Test MSE 203.65638525173918 Test RE 0.7274641537917644\n",
      "53 Train Loss 1.263027 Test MSE 197.0238239947008 Test RE 0.7155202915645416\n",
      "54 Train Loss 1.2356229 Test MSE 193.35058723726732 Test RE 0.7088189671660414\n",
      "55 Train Loss 1.1859521 Test MSE 186.32777202377773 Test RE 0.6958271625846654\n",
      "56 Train Loss 1.1285214 Test MSE 176.38564309351526 Test RE 0.6770086209097189\n",
      "57 Train Loss 1.0888122 Test MSE 169.87175136524343 Test RE 0.6643901187408411\n",
      "58 Train Loss 1.0770739 Test MSE 166.14187039058538 Test RE 0.6570556138010354\n",
      "59 Train Loss 1.0368066 Test MSE 162.03951410718935 Test RE 0.6488929394269505\n",
      "60 Train Loss 1.0177665 Test MSE 159.17951639423143 Test RE 0.6431409650773088\n",
      "61 Train Loss 0.976802 Test MSE 150.46190762567363 Test RE 0.625281910102811\n",
      "62 Train Loss 0.95251894 Test MSE 148.18662996418396 Test RE 0.6205361591624468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 Train Loss 0.9460063 Test MSE 148.1954582824688 Test RE 0.620554643316958\n",
      "64 Train Loss 0.9329438 Test MSE 143.32255155128811 Test RE 0.6102669464643733\n",
      "65 Train Loss 0.8874643 Test MSE 137.02729277518586 Test RE 0.596713854342702\n",
      "66 Train Loss 0.8579332 Test MSE 134.68563178115213 Test RE 0.5915932586080369\n",
      "67 Train Loss 0.8408239 Test MSE 131.82845208647083 Test RE 0.5852846832222746\n",
      "68 Train Loss 0.82544124 Test MSE 127.72529123030009 Test RE 0.5761041893018751\n",
      "69 Train Loss 0.802184 Test MSE 123.52344732460257 Test RE 0.5665487482119704\n",
      "70 Train Loss 0.77420694 Test MSE 116.97569789092587 Test RE 0.5513284515172053\n",
      "71 Train Loss 0.71854913 Test MSE 109.64064863113303 Test RE 0.5337628955760001\n",
      "72 Train Loss 0.7067124 Test MSE 106.04811764624637 Test RE 0.5249453156258186\n",
      "73 Train Loss 0.6708326 Test MSE 101.56073738175043 Test RE 0.51371885420263\n",
      "74 Train Loss 0.6578477 Test MSE 101.70641062348511 Test RE 0.5140871474876848\n",
      "75 Train Loss 0.65173626 Test MSE 101.32549178386692 Test RE 0.5131235446140361\n",
      "76 Train Loss 0.6408973 Test MSE 100.30612323402642 Test RE 0.5105359222961164\n",
      "77 Train Loss 0.6324445 Test MSE 98.10032125924073 Test RE 0.5048911954756389\n",
      "78 Train Loss 0.62542397 Test MSE 95.73031889880674 Test RE 0.4987550836630722\n",
      "79 Train Loss 0.6029386 Test MSE 88.79412681205899 Test RE 0.4803465799998377\n",
      "80 Train Loss 0.5762757 Test MSE 82.39861476103215 Test RE 0.46272454585235007\n",
      "81 Train Loss 0.5711584 Test MSE 82.29187135192507 Test RE 0.462424730118144\n",
      "82 Train Loss 0.5676062 Test MSE 82.54146054919075 Test RE 0.463125460543429\n",
      "83 Train Loss 0.55282676 Test MSE 81.37414502086085 Test RE 0.4598389997967444\n",
      "84 Train Loss 0.5326342 Test MSE 81.02716064740984 Test RE 0.4588575615219413\n",
      "85 Train Loss 0.522048 Test MSE 80.07021160823753 Test RE 0.4561399080266266\n",
      "86 Train Loss 0.506401 Test MSE 76.74179563333796 Test RE 0.4465587062345102\n",
      "87 Train Loss 0.49555683 Test MSE 74.40805526905936 Test RE 0.4397162945036611\n",
      "88 Train Loss 0.4922685 Test MSE 74.06259888908002 Test RE 0.4386943653583911\n",
      "89 Train Loss 0.48676544 Test MSE 75.00787411895426 Test RE 0.44148505943504185\n",
      "90 Train Loss 0.48266876 Test MSE 75.16254084409105 Test RE 0.44193999756895963\n",
      "91 Train Loss 0.47364464 Test MSE 73.94087333976992 Test RE 0.4383337091502109\n",
      "92 Train Loss 0.457488 Test MSE 71.40398027949799 Test RE 0.4307485192829149\n",
      "93 Train Loss 0.45051187 Test MSE 70.41567746335872 Test RE 0.42775713617095873\n",
      "94 Train Loss 0.44385403 Test MSE 68.7971612424256 Test RE 0.4228125228493758\n",
      "95 Train Loss 0.4400574 Test MSE 67.00219334188189 Test RE 0.4172603253865461\n",
      "96 Train Loss 0.43840906 Test MSE 66.57423606941856 Test RE 0.4159256254564805\n",
      "97 Train Loss 0.43706694 Test MSE 66.74585498449787 Test RE 0.4164613789677307\n",
      "98 Train Loss 0.43255812 Test MSE 66.27201048523729 Test RE 0.41498046738558825\n",
      "99 Train Loss 0.4166438 Test MSE 62.210143804125565 Test RE 0.40206214488042874\n",
      "Training time: 75.76\n",
      "Training time: 75.76\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 4.382981 Test MSE 386.18989064140146 Test RE 1.0017584928075223\n",
      "1 Train Loss 2.4224455 Test MSE 384.3192044704554 Test RE 0.9993293113596721\n",
      "2 Train Loss 2.3829412 Test MSE 383.63974265084454 Test RE 0.9984455323495991\n",
      "3 Train Loss 2.3813515 Test MSE 383.66630560241276 Test RE 0.9984800975918058\n",
      "4 Train Loss 2.3805113 Test MSE 383.7202010512242 Test RE 0.9985502257767345\n",
      "5 Train Loss 2.3799276 Test MSE 383.45010970983867 Test RE 0.9981987362738712\n",
      "6 Train Loss 2.3783228 Test MSE 382.8703600941178 Test RE 0.9974438476650634\n",
      "7 Train Loss 2.37639 Test MSE 382.74289668231324 Test RE 0.9972778016708647\n",
      "8 Train Loss 2.3731124 Test MSE 381.94771924659403 Test RE 0.9962413028534981\n",
      "9 Train Loss 2.3714552 Test MSE 381.50633290332354 Test RE 0.9956654983698359\n",
      "10 Train Loss 2.3661304 Test MSE 380.6443263734096 Test RE 0.9945400183357833\n",
      "11 Train Loss 2.3558993 Test MSE 377.88612856396657 Test RE 0.9909301845900256\n",
      "12 Train Loss 2.336852 Test MSE 374.98314277367274 Test RE 0.9871165988405289\n",
      "13 Train Loss 2.3086593 Test MSE 369.1858296049452 Test RE 0.9794563679235861\n",
      "14 Train Loss 2.2622466 Test MSE 362.7450855108931 Test RE 0.9708750738161827\n",
      "15 Train Loss 2.254152 Test MSE 360.3283937794055 Test RE 0.9676355719769277\n",
      "16 Train Loss 2.219078 Test MSE 353.69513570355014 Test RE 0.9586876352857946\n",
      "17 Train Loss 2.171444 Test MSE 343.26542480925923 Test RE 0.9444470476326892\n",
      "18 Train Loss 2.1170387 Test MSE 339.0577726707764 Test RE 0.9386408153141749\n",
      "19 Train Loss 2.0876715 Test MSE 333.39978104228044 Test RE 0.9307761310824191\n",
      "20 Train Loss 2.0467408 Test MSE 326.11455236205427 Test RE 0.9205506140015229\n",
      "21 Train Loss 1.99858 Test MSE 316.8270385361174 Test RE 0.9073476139026126\n",
      "22 Train Loss 1.9641743 Test MSE 313.3611913495304 Test RE 0.9023711198065355\n",
      "23 Train Loss 1.9364514 Test MSE 305.70876471236977 Test RE 0.891284856455141\n",
      "24 Train Loss 1.8769399 Test MSE 296.69380472046697 Test RE 0.878045095644645\n",
      "25 Train Loss 1.8538429 Test MSE 295.24500919225727 Test RE 0.8758986662967805\n",
      "26 Train Loss 1.8301227 Test MSE 289.7564082848933 Test RE 0.8677190005441633\n",
      "27 Train Loss 1.788931 Test MSE 282.9730676778018 Test RE 0.8575019849237135\n",
      "28 Train Loss 1.7369905 Test MSE 277.68417999876 Test RE 0.8494506472874707\n",
      "29 Train Loss 1.677618 Test MSE 266.56853513435954 Test RE 0.832275336927573\n",
      "30 Train Loss 1.661028 Test MSE 262.2560942315106 Test RE 0.8255157755688118\n",
      "31 Train Loss 1.6329333 Test MSE 259.70363512316305 Test RE 0.8214887055805937\n",
      "32 Train Loss 1.618365 Test MSE 257.28834213586276 Test RE 0.8176597820923085\n",
      "33 Train Loss 1.5845245 Test MSE 249.77259379229858 Test RE 0.8056287837962947\n",
      "34 Train Loss 1.5578343 Test MSE 242.5231167581164 Test RE 0.7938512865782523\n",
      "35 Train Loss 1.5191957 Test MSE 233.83507704856152 Test RE 0.7795023200577327\n",
      "36 Train Loss 1.4573492 Test MSE 230.6753651637583 Test RE 0.7742178694913255\n",
      "37 Train Loss 1.4327848 Test MSE 225.8474397834681 Test RE 0.766073021808255\n",
      "38 Train Loss 1.4179561 Test MSE 223.77775975811585 Test RE 0.7625547724147804\n",
      "39 Train Loss 1.3966787 Test MSE 221.1235401233508 Test RE 0.7580189657259018\n",
      "40 Train Loss 1.3753897 Test MSE 217.08576476560384 Test RE 0.751066264779943\n",
      "41 Train Loss 1.3485395 Test MSE 213.2254980069337 Test RE 0.7443584986222389\n",
      "42 Train Loss 1.3299694 Test MSE 209.22801134910907 Test RE 0.7373479822562444\n",
      "43 Train Loss 1.3210263 Test MSE 207.84575289903046 Test RE 0.7349083127456681\n",
      "44 Train Loss 1.2921478 Test MSE 203.56296945858278 Test RE 0.7272972932353549\n",
      "45 Train Loss 1.2479866 Test MSE 195.79128366926562 Test RE 0.7132787068054692\n",
      "46 Train Loss 1.2091953 Test MSE 189.22403157497115 Test RE 0.7012142418584335\n",
      "47 Train Loss 1.1683564 Test MSE 182.2325067456782 Test RE 0.6881379465501597\n",
      "48 Train Loss 1.1263126 Test MSE 175.58290258187557 Test RE 0.6754663128746479\n",
      "49 Train Loss 1.1034967 Test MSE 169.97573149693793 Test RE 0.6645934274224591\n",
      "50 Train Loss 1.083788 Test MSE 165.98031742260397 Test RE 0.6567360823632576\n",
      "51 Train Loss 1.0701839 Test MSE 164.18868820523366 Test RE 0.653181986247568\n",
      "52 Train Loss 1.0538504 Test MSE 164.3846345355495 Test RE 0.6535716307818742\n",
      "53 Train Loss 1.0309632 Test MSE 163.14263175559853 Test RE 0.6510979297908497\n",
      "54 Train Loss 1.011651 Test MSE 157.79021041423562 Test RE 0.6403281729366274\n",
      "55 Train Loss 0.99775773 Test MSE 154.26724545925254 Test RE 0.6331395526312591\n",
      "56 Train Loss 0.97631216 Test MSE 153.12766816582564 Test RE 0.6307967064321254\n",
      "57 Train Loss 0.94352067 Test MSE 147.99208453996215 Test RE 0.6201286928521298\n",
      "58 Train Loss 0.9237281 Test MSE 144.22752559070324 Test RE 0.6121906028528998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 Train Loss 0.8942568 Test MSE 136.65719051337405 Test RE 0.5959074658244912\n",
      "60 Train Loss 0.8710714 Test MSE 131.26566266440895 Test RE 0.5840340262963751\n",
      "61 Train Loss 0.8552012 Test MSE 129.21068190338937 Test RE 0.5794444290140257\n",
      "62 Train Loss 0.8331138 Test MSE 122.64728600931993 Test RE 0.5645358855830847\n",
      "63 Train Loss 0.7670526 Test MSE 114.58516999896887 Test RE 0.5456658682467684\n",
      "64 Train Loss 0.74540496 Test MSE 111.73626858741052 Test RE 0.5388397978929981\n",
      "65 Train Loss 0.72681165 Test MSE 108.04785169531456 Test RE 0.5298716095104564\n",
      "66 Train Loss 0.6880718 Test MSE 101.69575663467482 Test RE 0.5140602208557989\n",
      "67 Train Loss 0.641101 Test MSE 96.07095604704806 Test RE 0.4996416556797945\n",
      "68 Train Loss 0.6310514 Test MSE 93.77629949614798 Test RE 0.4936386183122288\n",
      "69 Train Loss 0.6138974 Test MSE 91.87452878775531 Test RE 0.4886075178450635\n",
      "70 Train Loss 0.5900309 Test MSE 89.64029546057229 Test RE 0.48262989808324575\n",
      "71 Train Loss 0.5811281 Test MSE 87.92341095805713 Test RE 0.4779856368541131\n",
      "72 Train Loss 0.5710881 Test MSE 84.41461285937643 Test RE 0.46835094306709874\n",
      "73 Train Loss 0.56265014 Test MSE 81.99785952447715 Test RE 0.46159791701437813\n",
      "74 Train Loss 0.53544647 Test MSE 78.51996463399395 Test RE 0.4517026414173446\n",
      "75 Train Loss 0.5133412 Test MSE 75.5083885264771 Test RE 0.4429555866123676\n",
      "76 Train Loss 0.49751952 Test MSE 74.18168051009378 Test RE 0.4390469012881\n",
      "77 Train Loss 0.49419522 Test MSE 74.15659984938954 Test RE 0.4389726746246279\n",
      "78 Train Loss 0.4908583 Test MSE 72.5822272846377 Test RE 0.4342878986758127\n",
      "79 Train Loss 0.4830593 Test MSE 69.81581579973195 Test RE 0.42593123651681136\n",
      "80 Train Loss 0.47146693 Test MSE 68.41206708376717 Test RE 0.4216275093917877\n",
      "81 Train Loss 0.46840099 Test MSE 67.60309849715208 Test RE 0.4191272360207823\n",
      "82 Train Loss 0.45313254 Test MSE 66.99512560443456 Test RE 0.41723831741893164\n",
      "83 Train Loss 0.4461199 Test MSE 66.85409889253938 Test RE 0.41679893659839345\n",
      "84 Train Loss 0.4425918 Test MSE 65.69663264951137 Test RE 0.41317509677125663\n",
      "85 Train Loss 0.4387609 Test MSE 64.73956570113161 Test RE 0.4101544930045444\n",
      "86 Train Loss 0.43685132 Test MSE 64.67182435214004 Test RE 0.4099398507658277\n",
      "87 Train Loss 0.43470308 Test MSE 64.06756306195786 Test RE 0.4080202194101793\n",
      "88 Train Loss 0.43261516 Test MSE 62.96827309439255 Test RE 0.4045046086169993\n",
      "89 Train Loss 0.42198494 Test MSE 58.96609800387516 Test RE 0.391438715379156\n",
      "90 Train Loss 0.40832275 Test MSE 54.63761802198487 Test RE 0.3767978892872603\n",
      "91 Train Loss 0.38633695 Test MSE 52.01772289683955 Test RE 0.3676531158294479\n",
      "92 Train Loss 0.36634344 Test MSE 49.48113936435068 Test RE 0.35857699895366124\n",
      "93 Train Loss 0.3524345 Test MSE 47.6618327892142 Test RE 0.35192324367711725\n",
      "94 Train Loss 0.34799135 Test MSE 45.84103506541392 Test RE 0.3451356254144536\n",
      "95 Train Loss 0.33853632 Test MSE 45.125793250610315 Test RE 0.34243252417929126\n",
      "96 Train Loss 0.3323081 Test MSE 44.689285565909614 Test RE 0.3407723022991326\n",
      "97 Train Loss 0.29428995 Test MSE 39.80618444212945 Test RE 0.3216161568403036\n",
      "98 Train Loss 0.27851596 Test MSE 39.4563865539506 Test RE 0.3201999334999268\n",
      "99 Train Loss 0.27225846 Test MSE 38.833224179552396 Test RE 0.3176612989825199\n",
      "Training time: 75.64\n",
      "Training time: 75.64\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 4.375328 Test MSE 386.92312907579804 Test RE 1.0027090349177035\n",
      "1 Train Loss 4.2929745 Test MSE 386.2468492910441 Test RE 1.0018323641188558\n",
      "2 Train Loss 2.9689958 Test MSE 384.3283835826704 Test RE 0.999341245321091\n",
      "3 Train Loss 2.444877 Test MSE 383.1571453045303 Test RE 0.9978173404023772\n",
      "4 Train Loss 2.4091403 Test MSE 383.8242767116839 Test RE 0.9986856339831486\n",
      "5 Train Loss 2.38422 Test MSE 383.8687260426917 Test RE 0.9987434594333572\n",
      "6 Train Loss 2.3804796 Test MSE 383.51952218094726 Test RE 0.9982890795856262\n",
      "7 Train Loss 2.3801746 Test MSE 383.4775420327007 Test RE 0.9982344415971837\n",
      "8 Train Loss 2.3784108 Test MSE 383.2046285578349 Test RE 0.997879166400277\n",
      "9 Train Loss 2.377279 Test MSE 382.660672589678 Test RE 0.9971706740565697\n",
      "10 Train Loss 2.3761356 Test MSE 381.97791369585104 Test RE 0.996280680445601\n",
      "11 Train Loss 2.3618815 Test MSE 379.86168573721494 Test RE 0.9935170581998016\n",
      "12 Train Loss 2.324165 Test MSE 370.4636634536541 Test RE 0.981149961324527\n",
      "13 Train Loss 2.257501 Test MSE 354.5136688469393 Test RE 0.9597963077611867\n",
      "14 Train Loss 2.2165508 Test MSE 352.81642519975435 Test RE 0.9574960260788394\n",
      "15 Train Loss 2.1627975 Test MSE 341.22195262821964 Test RE 0.9416316867219907\n",
      "16 Train Loss 2.0814962 Test MSE 326.4371480053688 Test RE 0.9210058102069194\n",
      "17 Train Loss 1.9915043 Test MSE 313.9512210666342 Test RE 0.9032202603510027\n",
      "18 Train Loss 1.9155347 Test MSE 302.314462855377 Test RE 0.8863230519046215\n",
      "19 Train Loss 1.8846524 Test MSE 298.9625316706163 Test RE 0.8813957738383741\n",
      "20 Train Loss 1.8533674 Test MSE 292.2622791017437 Test RE 0.8714630261033249\n",
      "21 Train Loss 1.8083333 Test MSE 280.0958858741247 Test RE 0.8531314407285224\n",
      "22 Train Loss 1.7524927 Test MSE 278.179861550776 Test RE 0.8502084673408896\n",
      "23 Train Loss 1.7006332 Test MSE 264.11322028176073 Test RE 0.8284335005964375\n",
      "24 Train Loss 1.632163 Test MSE 253.89340778730994 Test RE 0.8122473346856457\n",
      "25 Train Loss 1.6127175 Test MSE 249.8244567945363 Test RE 0.8057124201914606\n",
      "26 Train Loss 1.5897287 Test MSE 248.0474753703579 Test RE 0.8028418224258479\n",
      "27 Train Loss 1.519238 Test MSE 240.42165310452015 Test RE 0.7904044415166976\n",
      "28 Train Loss 1.4661485 Test MSE 230.01175249769986 Test RE 0.7731034227211946\n",
      "29 Train Loss 1.4272752 Test MSE 223.27145106218853 Test RE 0.7616916242040916\n",
      "30 Train Loss 1.3942924 Test MSE 217.5247896058004 Test RE 0.7518253431199349\n",
      "31 Train Loss 1.3261698 Test MSE 201.99665737461964 Test RE 0.7244938011424402\n",
      "32 Train Loss 1.2197526 Test MSE 186.103825414573 Test RE 0.6954088809124729\n",
      "33 Train Loss 1.1731393 Test MSE 178.70249994120672 Test RE 0.6814404296658343\n",
      "34 Train Loss 1.1037431 Test MSE 162.8622403476467 Test RE 0.6505381718272422\n",
      "35 Train Loss 0.95969427 Test MSE 139.128469114465 Test RE 0.6012714540094951\n",
      "36 Train Loss 0.9268357 Test MSE 135.38232300698135 Test RE 0.5931213585499968\n",
      "37 Train Loss 0.8942175 Test MSE 127.96140021869724 Test RE 0.5766364275607603\n",
      "38 Train Loss 0.85358524 Test MSE 125.06124403976406 Test RE 0.5700644449107526\n",
      "39 Train Loss 0.8371428 Test MSE 124.34843609822364 Test RE 0.5684375335005977\n",
      "40 Train Loss 0.7959921 Test MSE 120.51699164101083 Test RE 0.5596116198263704\n",
      "41 Train Loss 0.7616658 Test MSE 118.70454624626974 Test RE 0.5553877016858486\n",
      "42 Train Loss 0.738358 Test MSE 112.42827356057093 Test RE 0.5405057935258423\n",
      "43 Train Loss 0.728748 Test MSE 110.31013948506359 Test RE 0.5353900545091184\n",
      "44 Train Loss 0.72041357 Test MSE 109.3945423803444 Test RE 0.5331635002532537\n",
      "45 Train Loss 0.7023305 Test MSE 104.2587773072365 Test RE 0.5204977976532252\n",
      "46 Train Loss 0.69610596 Test MSE 102.23821573490697 Test RE 0.5154294312225489\n",
      "47 Train Loss 0.68680274 Test MSE 99.1045992668569 Test RE 0.5074689649197417\n",
      "48 Train Loss 0.6661456 Test MSE 95.69015959671594 Test RE 0.49865045768157473\n",
      "49 Train Loss 0.66122806 Test MSE 95.12708865098188 Test RE 0.49718118514282283\n",
      "50 Train Loss 0.6484672 Test MSE 92.74447206360867 Test RE 0.49091533569222146\n",
      "51 Train Loss 0.60443884 Test MSE 86.49012987885217 Test RE 0.4740736938454782\n",
      "52 Train Loss 0.56089973 Test MSE 83.46156652574948 Test RE 0.4656995823897043\n",
      "53 Train Loss 0.54076296 Test MSE 82.30727263233132 Test RE 0.46246800048995795\n",
      "54 Train Loss 0.5318403 Test MSE 79.39592039983512 Test RE 0.4542152134502431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 Train Loss 0.5283619 Test MSE 78.38638924950683 Test RE 0.45131826760458504\n",
      "56 Train Loss 0.5233457 Test MSE 78.28571853219982 Test RE 0.4510283631243374\n",
      "57 Train Loss 0.4917271 Test MSE 72.79066200715567 Test RE 0.43491102493563294\n",
      "58 Train Loss 0.45171708 Test MSE 66.69782486904882 Test RE 0.41631150982694065\n",
      "59 Train Loss 0.4225987 Test MSE 59.03042614171138 Test RE 0.3916521741347705\n",
      "60 Train Loss 0.3867645 Test MSE 55.85105954032241 Test RE 0.38095904622458016\n",
      "61 Train Loss 0.37220165 Test MSE 53.17504530548977 Test RE 0.3717205035789415\n",
      "62 Train Loss 0.34980065 Test MSE 46.91277208102604 Test RE 0.34914685196464296\n",
      "63 Train Loss 0.29532388 Test MSE 40.61727178858897 Test RE 0.32487624516216285\n",
      "64 Train Loss 0.28907293 Test MSE 40.91478007146245 Test RE 0.32606388069524306\n",
      "65 Train Loss 0.28216413 Test MSE 41.13267140060421 Test RE 0.32693095310622244\n",
      "66 Train Loss 0.26944044 Test MSE 39.24546983574917 Test RE 0.3193429617777276\n",
      "67 Train Loss 0.26527268 Test MSE 37.88868540681288 Test RE 0.31377428680300506\n",
      "68 Train Loss 0.26285407 Test MSE 36.98018180802464 Test RE 0.30998958558739603\n",
      "69 Train Loss 0.25500795 Test MSE 35.10297756213909 Test RE 0.30201920549892547\n",
      "70 Train Loss 0.20521776 Test MSE 27.302760766193128 Test RE 0.26635807306498155\n",
      "71 Train Loss 0.18759443 Test MSE 21.95417713945072 Test RE 0.23884775553242618\n",
      "72 Train Loss 0.1851295 Test MSE 19.742281177975354 Test RE 0.22649637154583385\n",
      "73 Train Loss 0.18064103 Test MSE 16.11941763631585 Test RE 0.20466202001086858\n",
      "74 Train Loss 0.17433655 Test MSE 15.305838140120864 Test RE 0.19943029914599497\n",
      "75 Train Loss 0.17199615 Test MSE 15.828097707439605 Test RE 0.20280419951249665\n",
      "76 Train Loss 0.16834305 Test MSE 17.494630225451964 Test RE 0.21321363102980032\n",
      "77 Train Loss 0.16352701 Test MSE 18.531146471604803 Test RE 0.21943895524507231\n",
      "78 Train Loss 0.14917353 Test MSE 16.469360494455728 Test RE 0.20687163669498532\n",
      "79 Train Loss 0.13964853 Test MSE 15.528866032946285 Test RE 0.20087803627625403\n",
      "80 Train Loss 0.1299164 Test MSE 13.065260448011564 Test RE 0.18425599127724338\n",
      "81 Train Loss 0.111382715 Test MSE 11.66900710650735 Test RE 0.1741323795187989\n",
      "82 Train Loss 0.09873515 Test MSE 11.441459110065882 Test RE 0.17242621262671837\n",
      "83 Train Loss 0.086235076 Test MSE 9.621572373365302 Test RE 0.15811956395113216\n",
      "84 Train Loss 0.073959015 Test MSE 7.574491274690862 Test RE 0.14029407092122062\n",
      "85 Train Loss 0.069510095 Test MSE 6.607907682454545 Test RE 0.13103718837531267\n",
      "86 Train Loss 0.06453494 Test MSE 5.774764063652758 Test RE 0.12249819942010556\n",
      "87 Train Loss 0.059448555 Test MSE 5.13352298545096 Test RE 0.11549690274151064\n",
      "88 Train Loss 0.051147364 Test MSE 3.1990605475961145 Test RE 0.09117458862723632\n",
      "89 Train Loss 0.046116482 Test MSE 2.4921206799802738 Test RE 0.08047243023597384\n",
      "90 Train Loss 0.044621598 Test MSE 2.06121299773799 Test RE 0.07318532718249415\n",
      "91 Train Loss 0.040155523 Test MSE 1.5655728301923466 Test RE 0.06378215966803798\n",
      "92 Train Loss 0.03474338 Test MSE 1.8423391798761042 Test RE 0.06919064458871331\n",
      "93 Train Loss 0.031663083 Test MSE 1.4771705178963186 Test RE 0.06195521923737163\n",
      "94 Train Loss 0.027046166 Test MSE 1.146632119210514 Test RE 0.054585165827833036\n",
      "95 Train Loss 0.026189901 Test MSE 1.24065291460355 Test RE 0.05677899911150649\n",
      "96 Train Loss 0.025018 Test MSE 1.370454187782865 Test RE 0.05967533197633916\n",
      "97 Train Loss 0.024473488 Test MSE 1.2414837348517542 Test RE 0.05679800734768889\n",
      "98 Train Loss 0.024112565 Test MSE 1.2897073346574672 Test RE 0.05789061554272144\n",
      "99 Train Loss 0.023937318 Test MSE 1.346003811628812 Test RE 0.05914060015355926\n",
      "Training time: 75.53\n",
      "Training time: 75.53\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 5.0\n",
    "rowdy_terms = 2\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    alpha_val = []\n",
    "    omega_val = []\n",
    "\n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "\n",
    "\n",
    "\n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)  \n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"alpha\": alpha_full, \"omega\": omega_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lrnr_tune' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32749/370070687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlrnr_tune\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lrnr_tune' is not defined"
     ]
    }
   ],
   "source": [
    "lrnr_tune[tune_reps,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tune_reps in range(75):\n",
    "    label = \"1D_SODE_rowdy_tune\"+str(tune_reps)+\".mat\" #WRONGLY SAVED AS STAN - DOESN'T MATTER\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33411856570698817\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
