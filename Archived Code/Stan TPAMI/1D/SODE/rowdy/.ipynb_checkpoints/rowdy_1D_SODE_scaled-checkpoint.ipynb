{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y/50\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 0.005\n",
    "label = \"1D_SODE_rowdy_scaled\"\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0/50.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.omega = Parameter(0.1*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.348187 Test MSE 386.4776439216414 Test RE 1.0021316325804648\n",
      "1 Train Loss 2.7387257 Test MSE 382.6372457491159 Test RE 0.997140149733563\n",
      "2 Train Loss 2.4777768 Test MSE 383.29975242574693 Test RE 0.9980030117677173\n",
      "3 Train Loss 2.3898745 Test MSE 383.8494510078944 Test RE 0.9987183843812723\n",
      "4 Train Loss 2.384634 Test MSE 383.9848259324957 Test RE 0.9988944814254613\n",
      "5 Train Loss 2.3812375 Test MSE 383.8290936625558 Test RE 0.998691900658857\n",
      "6 Train Loss 2.3802814 Test MSE 383.53125948186164 Test RE 0.9983043553786447\n",
      "7 Train Loss 2.3782456 Test MSE 383.1747468130406 Test RE 0.9978402590519768\n",
      "8 Train Loss 2.377997 Test MSE 383.0950856435775 Test RE 0.9977365292857714\n",
      "9 Train Loss 2.3771982 Test MSE 382.64640427686777 Test RE 0.9971520830740698\n",
      "10 Train Loss 2.3752205 Test MSE 382.39183180163565 Test RE 0.9968203280830342\n",
      "11 Train Loss 2.3733752 Test MSE 381.8165601542552 Test RE 0.9960702358227115\n",
      "12 Train Loss 2.365906 Test MSE 379.9057806347067 Test RE 0.9935747209797057\n",
      "13 Train Loss 2.3476512 Test MSE 376.39458481303956 Test RE 0.9889726146282651\n",
      "14 Train Loss 2.30512 Test MSE 369.40059740974567 Test RE 0.979741217846473\n",
      "15 Train Loss 2.2803946 Test MSE 364.4191446156199 Test RE 0.9731127766195551\n",
      "16 Train Loss 2.2288885 Test MSE 356.58605788033566 Test RE 0.9625975719888843\n",
      "17 Train Loss 2.107835 Test MSE 333.53032839155475 Test RE 0.930958342456192\n",
      "18 Train Loss 2.0478437 Test MSE 325.56803768080596 Test RE 0.9197789443925751\n",
      "19 Train Loss 1.9811296 Test MSE 316.5848210421788 Test RE 0.9070007093464306\n",
      "20 Train Loss 1.9325423 Test MSE 305.7156231190959 Test RE 0.8912948541399899\n",
      "21 Train Loss 1.8898042 Test MSE 301.4788782580918 Test RE 0.8850973243339405\n",
      "22 Train Loss 1.7823792 Test MSE 279.5811070477839 Test RE 0.8523471093702958\n",
      "23 Train Loss 1.726494 Test MSE 262.24859676104984 Test RE 0.8255039754152322\n",
      "24 Train Loss 1.6316156 Test MSE 250.31255992390544 Test RE 0.806499130296963\n",
      "25 Train Loss 1.5208452 Test MSE 237.46608917314518 Test RE 0.7855310974593996\n",
      "26 Train Loss 1.460049 Test MSE 229.1208074917703 Test RE 0.771604671186892\n",
      "27 Train Loss 1.4013898 Test MSE 220.15090447172918 Test RE 0.7563500145018938\n",
      "28 Train Loss 1.376901 Test MSE 214.274912127356 Test RE 0.7461879739101804\n",
      "29 Train Loss 1.3511101 Test MSE 211.85708399924022 Test RE 0.7419661250100723\n",
      "30 Train Loss 1.3346591 Test MSE 208.10822153186865 Test RE 0.7353721892505011\n",
      "31 Train Loss 1.3227108 Test MSE 204.51026628146602 Test RE 0.7289875975581679\n",
      "32 Train Loss 1.3106539 Test MSE 199.6991809172831 Test RE 0.7203618824760734\n",
      "33 Train Loss 1.2748749 Test MSE 195.36743638186465 Test RE 0.7125062387151734\n",
      "34 Train Loss 1.2714012 Test MSE 195.22033850820327 Test RE 0.7122379547822405\n",
      "35 Train Loss 1.2505305 Test MSE 193.09633071333013 Test RE 0.7083527644606212\n",
      "36 Train Loss 1.2154666 Test MSE 187.2345741460796 Test RE 0.697518299950034\n",
      "37 Train Loss 1.1897624 Test MSE 182.2510119099499 Test RE 0.6881728848350707\n",
      "38 Train Loss 1.1650997 Test MSE 179.97061478052998 Test RE 0.6838539859766911\n",
      "39 Train Loss 1.1478621 Test MSE 174.87224310349507 Test RE 0.674097975655058\n",
      "40 Train Loss 1.1293576 Test MSE 168.3261460500637 Test RE 0.661360682130625\n",
      "41 Train Loss 1.1023664 Test MSE 160.36360076713132 Test RE 0.6455285907826882\n",
      "42 Train Loss 1.0576589 Test MSE 155.77094343777264 Test RE 0.6362177884114774\n",
      "43 Train Loss 1.0284688 Test MSE 153.28887693929724 Test RE 0.6311286621607086\n",
      "44 Train Loss 1.0152968 Test MSE 152.92043186965122 Test RE 0.630369715563561\n",
      "45 Train Loss 0.99946994 Test MSE 153.30789490850643 Test RE 0.6311678118154054\n",
      "46 Train Loss 0.9725795 Test MSE 152.0241961704868 Test RE 0.6285197663798301\n",
      "47 Train Loss 0.9449045 Test MSE 144.28396028705964 Test RE 0.6123103629737929\n",
      "48 Train Loss 0.934472 Test MSE 142.15434527394606 Test RE 0.607774748449723\n",
      "49 Train Loss 0.919829 Test MSE 138.9602485851601 Test RE 0.6009078447661343\n",
      "50 Train Loss 0.91403496 Test MSE 136.00322333131723 Test RE 0.594479910964303\n",
      "51 Train Loss 0.8799054 Test MSE 125.79767650225301 Test RE 0.571740414763292\n",
      "52 Train Loss 0.78474003 Test MSE 119.18398338223584 Test RE 0.5565081523143122\n",
      "53 Train Loss 0.774781 Test MSE 118.25383752583646 Test RE 0.5543323244979794\n",
      "54 Train Loss 0.7633814 Test MSE 118.66933143031929 Test RE 0.5553053150922574\n",
      "55 Train Loss 0.7590689 Test MSE 117.46466934294943 Test RE 0.5524795570158695\n",
      "56 Train Loss 0.7537875 Test MSE 116.1282466116387 Test RE 0.5493277226858613\n",
      "57 Train Loss 0.74493814 Test MSE 112.22747256296645 Test RE 0.5400228962798733\n",
      "58 Train Loss 0.74267447 Test MSE 110.3768097740705 Test RE 0.5355518221312954\n",
      "59 Train Loss 0.73800915 Test MSE 110.04728164553465 Test RE 0.5347517840340595\n",
      "60 Train Loss 0.73187876 Test MSE 110.47061287700171 Test RE 0.535779341634929\n",
      "61 Train Loss 0.72747713 Test MSE 111.02003762828058 Test RE 0.537110036373317\n",
      "62 Train Loss 0.71763897 Test MSE 108.85414077442138 Test RE 0.5318449741487284\n",
      "63 Train Loss 0.7071714 Test MSE 104.59607355865694 Test RE 0.5213390706663987\n",
      "64 Train Loss 0.6973795 Test MSE 100.77207312836671 Test RE 0.5117203392171981\n",
      "65 Train Loss 0.67350996 Test MSE 94.89619274835735 Test RE 0.4965774304987748\n",
      "66 Train Loss 0.618944 Test MSE 90.80551256814196 Test RE 0.48575657712099735\n",
      "67 Train Loss 0.61548835 Test MSE 90.42133483196105 Test RE 0.4847279244598808\n",
      "68 Train Loss 0.6140338 Test MSE 89.9449935958446 Test RE 0.4834494607338109\n",
      "69 Train Loss 0.6139107 Test MSE 89.96160301045757 Test RE 0.48349409602450816\n",
      "70 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "71 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "72 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "73 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "74 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "75 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "76 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "77 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "78 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "79 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "80 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "81 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "82 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "83 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "84 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "85 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "86 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "87 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "88 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "89 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "90 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "91 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "92 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "93 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "94 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "95 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "97 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "98 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "99 Train Loss 0.6138576 Test MSE 89.96190331281873 Test RE 0.4834949030037783\n",
      "Training time: 61.23\n",
      "Training time: 61.23\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 4.259908 Test MSE 386.5245294149284 Test RE 1.002192417481932\n",
      "1 Train Loss 2.7738912 Test MSE 385.81060546908157 Test RE 1.0012664479775648\n",
      "2 Train Loss 2.467772 Test MSE 384.89666241906906 Test RE 1.0000797995490036\n",
      "3 Train Loss 2.3862863 Test MSE 384.3672977399781 Test RE 0.9993918368721534\n",
      "4 Train Loss 2.3832011 Test MSE 383.87659736075204 Test RE 0.9987536991150462\n",
      "5 Train Loss 2.3810942 Test MSE 383.6831353044066 Test RE 0.9985019967499644\n",
      "6 Train Loss 2.3806977 Test MSE 383.64490864214247 Test RE 0.9984522547286883\n",
      "7 Train Loss 2.3802507 Test MSE 383.59716727174555 Test RE 0.9983901283184469\n",
      "8 Train Loss 2.3798864 Test MSE 383.5988690760229 Test RE 0.9983923429629377\n",
      "9 Train Loss 2.3795385 Test MSE 383.4814437769951 Test RE 0.9982395199201659\n",
      "10 Train Loss 2.3794696 Test MSE 383.44432273521727 Test RE 0.9981912039086396\n",
      "11 Train Loss 2.3794618 Test MSE 383.44217787429915 Test RE 0.9981884121287524\n",
      "12 Train Loss 2.3794425 Test MSE 383.4289702535853 Test RE 0.9981712207394912\n",
      "13 Train Loss 2.3781967 Test MSE 383.15015464319515 Test RE 0.9978082378252711\n",
      "14 Train Loss 2.3774211 Test MSE 382.7880289658026 Test RE 0.9973365984481206\n",
      "15 Train Loss 2.3727388 Test MSE 381.8197580131413 Test RE 0.9960744070474801\n",
      "16 Train Loss 2.3653471 Test MSE 380.8689385485195 Test RE 0.9948334062006392\n",
      "17 Train Loss 2.3605034 Test MSE 379.37554220870885 Test RE 0.9928811076188868\n",
      "18 Train Loss 2.3512204 Test MSE 376.8570693428722 Test RE 0.9895800146012144\n",
      "19 Train Loss 2.335447 Test MSE 373.2080816850628 Test RE 0.9847774659849929\n",
      "20 Train Loss 2.3288574 Test MSE 371.40674321744393 Test RE 0.9823980115836747\n",
      "21 Train Loss 2.3145862 Test MSE 368.3255484868935 Test RE 0.978314532040337\n",
      "22 Train Loss 2.2796085 Test MSE 364.1802123113578 Test RE 0.9727937124250937\n",
      "23 Train Loss 2.244701 Test MSE 358.73549259467217 Test RE 0.9654943931138653\n",
      "24 Train Loss 2.2116306 Test MSE 353.88457145760697 Test RE 0.9589443329134487\n",
      "25 Train Loss 2.1590297 Test MSE 339.78102018492564 Test RE 0.9396413940514207\n",
      "26 Train Loss 2.121659 Test MSE 331.2419954276723 Test RE 0.9277592191516466\n",
      "27 Train Loss 2.0469925 Test MSE 323.1527924471777 Test RE 0.9163608767790651\n",
      "28 Train Loss 1.9988276 Test MSE 315.00962321907184 Test RE 0.9047414615116178\n",
      "29 Train Loss 1.9629399 Test MSE 310.22647259419557 Test RE 0.8978463261257946\n",
      "30 Train Loss 1.9294442 Test MSE 304.6523272108862 Test RE 0.8897435175660723\n",
      "31 Train Loss 1.8968114 Test MSE 297.66091092107223 Test RE 0.8794749738278195\n",
      "32 Train Loss 1.8938866 Test MSE 296.60543176532127 Test RE 0.8779143190365618\n",
      "33 Train Loss 1.8721973 Test MSE 290.4003023702147 Test RE 0.8686825842052096\n",
      "34 Train Loss 1.8506793 Test MSE 286.8833699823388 Test RE 0.86340641175812\n",
      "35 Train Loss 1.8448302 Test MSE 286.4049385391521 Test RE 0.8626861658587233\n",
      "36 Train Loss 1.7912664 Test MSE 279.7671606128038 Test RE 0.8526306690245083\n",
      "37 Train Loss 1.7547929 Test MSE 278.17945176604695 Test RE 0.8502078411227065\n",
      "38 Train Loss 1.736857 Test MSE 273.14350215784106 Test RE 0.8424769340529118\n",
      "39 Train Loss 1.7226301 Test MSE 271.2982960714608 Test RE 0.8396264582623157\n",
      "40 Train Loss 1.713374 Test MSE 271.68528791956464 Test RE 0.8402250849565401\n",
      "41 Train Loss 1.6992853 Test MSE 267.8931092200763 Test RE 0.8343405549224421\n",
      "42 Train Loss 1.6913873 Test MSE 266.67651783748084 Test RE 0.83244389065627\n",
      "43 Train Loss 1.663394 Test MSE 260.43682327140584 Test RE 0.8226474904360719\n",
      "44 Train Loss 1.6308148 Test MSE 257.8481047023558 Test RE 0.8185487587587121\n",
      "45 Train Loss 1.6230869 Test MSE 253.6275876988467 Test RE 0.8118220219463724\n",
      "46 Train Loss 1.579653 Test MSE 245.3977955237602 Test RE 0.7985422718989903\n",
      "47 Train Loss 1.5503433 Test MSE 243.09855405725352 Test RE 0.7947925184839838\n",
      "48 Train Loss 1.5215833 Test MSE 240.15855851621873 Test RE 0.7899718514190921\n",
      "49 Train Loss 1.4713563 Test MSE 232.11687420072624 Test RE 0.7766331770860576\n",
      "50 Train Loss 1.4559425 Test MSE 228.9336014432715 Test RE 0.7712893821018012\n",
      "51 Train Loss 1.4377363 Test MSE 227.99210449831816 Test RE 0.7697017717041013\n",
      "52 Train Loss 1.4311786 Test MSE 226.4586631559788 Test RE 0.7671089541998213\n",
      "53 Train Loss 1.4180497 Test MSE 222.0074855998482 Test RE 0.759532552065005\n",
      "54 Train Loss 1.3900605 Test MSE 216.14540318294488 Test RE 0.7494377831197683\n",
      "55 Train Loss 1.3718195 Test MSE 215.19141121011123 Test RE 0.7477820729840153\n",
      "56 Train Loss 1.362543 Test MSE 211.30478245364353 Test RE 0.7409983583113889\n",
      "57 Train Loss 1.3445283 Test MSE 209.47517319490805 Test RE 0.7377833697012962\n",
      "58 Train Loss 1.3300555 Test MSE 209.2382262593377 Test RE 0.737365981403368\n",
      "59 Train Loss 1.3173966 Test MSE 206.73011256604732 Test RE 0.7329332987550253\n",
      "60 Train Loss 1.3116673 Test MSE 205.19220066085845 Test RE 0.7302019815435309\n",
      "61 Train Loss 1.3079853 Test MSE 204.0850631966772 Test RE 0.7282293738653042\n",
      "62 Train Loss 1.3011814 Test MSE 202.52101606693577 Test RE 0.7254335404592658\n",
      "63 Train Loss 1.2942895 Test MSE 201.79311048175055 Test RE 0.7241286821527835\n",
      "64 Train Loss 1.2862912 Test MSE 201.64201256760975 Test RE 0.723857526158978\n",
      "65 Train Loss 1.2840973 Test MSE 201.9013958106498 Test RE 0.7243229454648521\n",
      "66 Train Loss 1.2829285 Test MSE 201.91793367402113 Test RE 0.7243526097189649\n",
      "67 Train Loss 1.2775749 Test MSE 200.32860207842808 Test RE 0.7214962243944514\n",
      "68 Train Loss 1.2730927 Test MSE 198.43099735707312 Test RE 0.7180709214271186\n",
      "69 Train Loss 1.266868 Test MSE 198.50916020692094 Test RE 0.7182123331630145\n",
      "70 Train Loss 1.2545054 Test MSE 196.6939061990135 Test RE 0.7149209686561683\n",
      "71 Train Loss 1.2467561 Test MSE 194.41278348059976 Test RE 0.7107632944128788\n",
      "72 Train Loss 1.2293236 Test MSE 192.1760543905355 Test RE 0.7066627819812202\n",
      "73 Train Loss 1.220906 Test MSE 192.46297063943723 Test RE 0.7071901042192654\n",
      "74 Train Loss 1.2176667 Test MSE 192.34294066875637 Test RE 0.7069695494656734\n",
      "75 Train Loss 1.2115037 Test MSE 192.2937270417071 Test RE 0.7068790996642789\n",
      "76 Train Loss 1.2084926 Test MSE 191.27987970472242 Test RE 0.7050131661267005\n",
      "77 Train Loss 1.2066108 Test MSE 191.37803937243925 Test RE 0.7051940397818898\n",
      "78 Train Loss 1.20001 Test MSE 190.76334389613814 Test RE 0.7040606071641268\n",
      "79 Train Loss 1.1935546 Test MSE 189.14970238699618 Test RE 0.7010765061698064\n",
      "80 Train Loss 1.1897027 Test MSE 189.14530381250358 Test RE 0.7010683545440668\n",
      "81 Train Loss 1.186757 Test MSE 188.17985753710593 Test RE 0.6992768489512716\n",
      "82 Train Loss 1.1852942 Test MSE 187.66910145484997 Test RE 0.6983272186668096\n",
      "83 Train Loss 1.1832283 Test MSE 187.48653087328182 Test RE 0.6979874583436798\n",
      "84 Train Loss 1.1820157 Test MSE 187.06161135143324 Test RE 0.6971960501997557\n",
      "85 Train Loss 1.1808767 Test MSE 186.87143289286104 Test RE 0.6968415536609788\n",
      "86 Train Loss 1.1799973 Test MSE 186.90650165344422 Test RE 0.6969069361082704\n",
      "87 Train Loss 1.1785026 Test MSE 186.59274441726402 Test RE 0.6963217466298693\n",
      "88 Train Loss 1.1764319 Test MSE 186.3031227247918 Test RE 0.6957811355784058\n",
      "89 Train Loss 1.1696447 Test MSE 184.9104071883756 Test RE 0.6931755888231699\n",
      "90 Train Loss 1.1602559 Test MSE 182.6363837596514 Test RE 0.6889000753438712\n",
      "91 Train Loss 1.1506019 Test MSE 181.06272102157655 Test RE 0.6859257453784222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Train Loss 1.1429898 Test MSE 178.61354839273017 Test RE 0.6812708105325972\n",
      "93 Train Loss 1.1330713 Test MSE 176.4369027390585 Test RE 0.6771069869116476\n",
      "94 Train Loss 1.1187229 Test MSE 175.19160332311614 Test RE 0.6747132301821217\n",
      "95 Train Loss 1.1088328 Test MSE 174.21525272114297 Test RE 0.6728304999528721\n",
      "96 Train Loss 1.0921457 Test MSE 170.1413713166882 Test RE 0.664917168726172\n",
      "97 Train Loss 1.0841321 Test MSE 167.4156216452609 Test RE 0.6595695117940327\n",
      "98 Train Loss 1.068005 Test MSE 163.74987068079636 Test RE 0.6523085415694303\n",
      "99 Train Loss 1.0533954 Test MSE 159.37908725253942 Test RE 0.6435440068512174\n",
      "Training time: 70.94\n",
      "Training time: 70.94\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 4.3062477 Test MSE 386.47520290483595 Test RE 1.0021284678126479\n",
      "1 Train Loss 3.9415154 Test MSE 382.76105840022586 Test RE 0.997301462545322\n",
      "2 Train Loss 2.4036756 Test MSE 382.68161117550756 Test RE 0.9971979554832064\n",
      "3 Train Loss 2.3801434 Test MSE 383.2765316334389 Test RE 0.9979727811564174\n",
      "4 Train Loss 2.3787966 Test MSE 383.4215532393682 Test RE 0.9981615664273331\n",
      "5 Train Loss 2.3777895 Test MSE 382.98820950707284 Test RE 0.9975973449582292\n",
      "6 Train Loss 2.364427 Test MSE 380.65191745222035 Test RE 0.9945499351971379\n",
      "7 Train Loss 2.3452055 Test MSE 376.0544064108333 Test RE 0.9885256061085415\n",
      "8 Train Loss 2.335407 Test MSE 374.73408848517664 Test RE 0.9867887354930779\n",
      "9 Train Loss 2.2998824 Test MSE 364.83865587035586 Test RE 0.9736727284616606\n",
      "10 Train Loss 2.2440524 Test MSE 357.1783416152771 Test RE 0.9633966698846526\n",
      "11 Train Loss 2.1217093 Test MSE 336.2407315767578 Test RE 0.9347333618195793\n",
      "12 Train Loss 2.0471225 Test MSE 325.78171348397325 Test RE 0.9200807281140794\n",
      "13 Train Loss 1.9734536 Test MSE 310.9828079363303 Test RE 0.8989401391548126\n",
      "14 Train Loss 1.8929799 Test MSE 297.1857914618146 Test RE 0.8787727947126637\n",
      "15 Train Loss 1.8644521 Test MSE 289.35547653364574 Test RE 0.867118467619679\n",
      "16 Train Loss 1.8157145 Test MSE 280.95102800185845 Test RE 0.8544327676785325\n",
      "17 Train Loss 1.7355803 Test MSE 274.68955176243713 Test RE 0.8448578678783326\n",
      "18 Train Loss 1.7077456 Test MSE 269.7618462426625 Test RE 0.8372455446684539\n",
      "19 Train Loss 1.5542028 Test MSE 243.58792113490227 Test RE 0.7955920908010445\n",
      "20 Train Loss 1.4714522 Test MSE 227.58876941343107 Test RE 0.7690206403267513\n",
      "21 Train Loss 1.4437137 Test MSE 224.78787231709117 Test RE 0.7642738862393993\n",
      "22 Train Loss 1.3814793 Test MSE 207.6979526044124 Test RE 0.7346469675401257\n",
      "23 Train Loss 1.3463366 Test MSE 196.0703258796054 Test RE 0.7137868091005596\n",
      "24 Train Loss 1.304523 Test MSE 192.51205255502836 Test RE 0.7072802722973558\n",
      "25 Train Loss 1.1939151 Test MSE 176.62210492087658 Test RE 0.6774622663308653\n",
      "26 Train Loss 1.091184 Test MSE 157.84245320793826 Test RE 0.6404341673581433\n",
      "27 Train Loss 0.98392403 Test MSE 142.99553350549576 Test RE 0.6095703279959758\n",
      "28 Train Loss 0.9419554 Test MSE 139.93121501394413 Test RE 0.6030035724311088\n",
      "29 Train Loss 0.89692414 Test MSE 135.0605149739685 Test RE 0.5924160050916556\n",
      "30 Train Loss 0.79093575 Test MSE 119.47175570614131 Test RE 0.5571795977814717\n",
      "31 Train Loss 0.6844371 Test MSE 101.83083951220875 Test RE 0.514401521672361\n",
      "32 Train Loss 0.65369797 Test MSE 97.89834188375367 Test RE 0.5043711658175588\n",
      "33 Train Loss 0.63566047 Test MSE 92.59029467405901 Test RE 0.49050711984291867\n",
      "34 Train Loss 0.62010443 Test MSE 90.53054189970497 Test RE 0.4850205530836833\n",
      "35 Train Loss 0.6149014 Test MSE 88.32244076846129 Test RE 0.47906904913175086\n",
      "36 Train Loss 0.6093866 Test MSE 86.67566861695099 Test RE 0.4745819131828509\n",
      "37 Train Loss 0.5875069 Test MSE 86.51664884286014 Test RE 0.47414636676035576\n",
      "38 Train Loss 0.57060796 Test MSE 85.29180511439493 Test RE 0.4707780822991787\n",
      "39 Train Loss 0.54982907 Test MSE 80.22356354878784 Test RE 0.4565765028496926\n",
      "40 Train Loss 0.5165669 Test MSE 72.99707043246408 Test RE 0.4355272149038953\n",
      "41 Train Loss 0.45899254 Test MSE 65.52454919059517 Test RE 0.41263361376364477\n",
      "42 Train Loss 0.37321243 Test MSE 50.64754138367256 Test RE 0.3627786885714095\n",
      "43 Train Loss 0.34788036 Test MSE 48.86982948048983 Test RE 0.3563551129231812\n",
      "44 Train Loss 0.33680212 Test MSE 47.182066600366454 Test RE 0.35014752602738536\n",
      "45 Train Loss 0.30843973 Test MSE 42.90751270719307 Test RE 0.3339098668605299\n",
      "46 Train Loss 0.2804671 Test MSE 39.13707593824191 Test RE 0.31890165272275334\n",
      "47 Train Loss 0.26290336 Test MSE 34.74856299129243 Test RE 0.3004906804634928\n",
      "48 Train Loss 0.24216786 Test MSE 31.52131469682885 Test RE 0.28619678048792824\n",
      "49 Train Loss 0.21917751 Test MSE 26.314824629378794 Test RE 0.26149465881588113\n",
      "50 Train Loss 0.18902965 Test MSE 20.845167527628313 Test RE 0.23273691720483558\n",
      "51 Train Loss 0.16941263 Test MSE 19.700185587744578 Test RE 0.2262547686053687\n",
      "52 Train Loss 0.16424666 Test MSE 17.39711487574287 Test RE 0.21261857255369254\n",
      "53 Train Loss 0.14990216 Test MSE 14.201107913583987 Test RE 0.19209837732223015\n",
      "54 Train Loss 0.12208641 Test MSE 12.856365551826723 Test RE 0.1827770605936302\n",
      "55 Train Loss 0.12012062 Test MSE 12.752426037088549 Test RE 0.18203671478671504\n",
      "56 Train Loss 0.109756306 Test MSE 11.108460938605141 Test RE 0.16989849376310465\n",
      "57 Train Loss 0.08496983 Test MSE 8.459267322601583 Test RE 0.14826169375069526\n",
      "58 Train Loss 0.06977686 Test MSE 9.071508405791436 Test RE 0.15353321217579785\n",
      "59 Train Loss 0.067649595 Test MSE 8.779719431118913 Test RE 0.15104379935513296\n",
      "60 Train Loss 0.06649101 Test MSE 7.8662267717823795 Test RE 0.1429702951341892\n",
      "61 Train Loss 0.0640178 Test MSE 7.12189772923857 Test RE 0.13603806595846518\n",
      "62 Train Loss 0.061453402 Test MSE 6.812615134107039 Test RE 0.13305141879653434\n",
      "63 Train Loss 0.060413383 Test MSE 6.124318105118177 Test RE 0.12615121937296794\n",
      "64 Train Loss 0.059705604 Test MSE 5.877495956645499 Test RE 0.12358300525279213\n",
      "65 Train Loss 0.052823536 Test MSE 4.536459233323577 Test RE 0.10857281267603515\n",
      "66 Train Loss 0.042939916 Test MSE 4.352273707728855 Test RE 0.106345883187201\n",
      "67 Train Loss 0.041380588 Test MSE 4.063655883701895 Test RE 0.10275927809596404\n",
      "68 Train Loss 0.041123193 Test MSE 3.8115267478206225 Test RE 0.09952038981256925\n",
      "69 Train Loss 0.040627442 Test MSE 3.624314511702681 Test RE 0.0970455267149048\n",
      "70 Train Loss 0.03839595 Test MSE 3.3179561856868505 Test RE 0.09285342032193405\n",
      "71 Train Loss 0.037089266 Test MSE 2.8133425949296953 Test RE 0.08550153139659372\n",
      "72 Train Loss 0.036062866 Test MSE 2.2785803892703846 Test RE 0.07694754356164216\n",
      "73 Train Loss 0.03524607 Test MSE 2.1878901679032823 Test RE 0.07540069358982227\n",
      "74 Train Loss 0.034617633 Test MSE 1.9807495130694242 Test RE 0.07174264124853684\n",
      "75 Train Loss 0.033411376 Test MSE 1.4810092684308496 Test RE 0.06203566909566684\n",
      "76 Train Loss 0.032223295 Test MSE 1.3142022002366749 Test RE 0.05843777574737172\n",
      "77 Train Loss 0.030327331 Test MSE 1.0170411124998704 Test RE 0.05140813362264449\n",
      "78 Train Loss 0.02759132 Test MSE 0.7092304363065138 Test RE 0.04292954296866861\n",
      "79 Train Loss 0.023516148 Test MSE 0.6524678534463676 Test RE 0.04117580913501357\n",
      "80 Train Loss 0.020387456 Test MSE 0.4338545057092885 Test RE 0.03357643931604478\n",
      "81 Train Loss 0.01686735 Test MSE 0.14803977313255717 Test RE 0.019613350432898455\n",
      "82 Train Loss 0.013535316 Test MSE 0.14918244980338652 Test RE 0.019688899848434215\n",
      "83 Train Loss 0.011650627 Test MSE 0.26836038360270165 Test RE 0.026407165332619144\n",
      "84 Train Loss 0.009861498 Test MSE 0.17482556729430623 Test RE 0.02131400462159918\n",
      "85 Train Loss 0.0091878865 Test MSE 0.0488120301475052 Test RE 0.011262272155056418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 Train Loss 0.008724052 Test MSE 0.04574474865967571 Test RE 0.010902678491864399\n",
      "87 Train Loss 0.007376134 Test MSE 0.08773389813794495 Test RE 0.015098934306917496\n",
      "88 Train Loss 0.0050328877 Test MSE 0.030214187125943908 Test RE 0.008860700144750124\n",
      "89 Train Loss 0.0037370664 Test MSE 0.027005700580499163 Test RE 0.008377034549922025\n",
      "90 Train Loss 0.0031992944 Test MSE 0.06180689322408213 Test RE 0.012673046824542901\n",
      "91 Train Loss 0.0030094306 Test MSE 0.08022415485850458 Test RE 0.014438269961508574\n",
      "92 Train Loss 0.0028955485 Test MSE 0.10340884560529943 Test RE 0.016392357896477455\n",
      "93 Train Loss 0.002885974 Test MSE 0.10507313631524191 Test RE 0.016523742952253992\n",
      "94 Train Loss 0.0028776277 Test MSE 0.10654834402003427 Test RE 0.016639333819127564\n",
      "95 Train Loss 0.002538916 Test MSE 0.09630567777164548 Test RE 0.01581934612820778\n",
      "96 Train Loss 0.0022339062 Test MSE 0.05331709690657942 Test RE 0.01177052487969715\n",
      "97 Train Loss 0.002214602 Test MSE 0.050327631628442476 Test RE 0.011435780983594015\n",
      "98 Train Loss 0.0022072145 Test MSE 0.049462928844324364 Test RE 0.01133711356016342\n",
      "99 Train Loss 0.0021872837 Test MSE 0.04816457149275446 Test RE 0.011187329588096253\n",
      "Training time: 69.09\n",
      "Training time: 69.09\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 4.305668 Test MSE 386.6295352520724 Test RE 1.0023285393829466\n",
      "1 Train Loss 3.7148168 Test MSE 384.11942385826956 Test RE 0.999069536969539\n",
      "2 Train Loss 2.4151442 Test MSE 383.38688540621393 Test RE 0.9981164400115271\n",
      "3 Train Loss 2.383531 Test MSE 383.83740490803865 Test RE 0.9987027131907267\n",
      "4 Train Loss 2.3820508 Test MSE 383.70475648705224 Test RE 0.9985301299775222\n",
      "5 Train Loss 2.379191 Test MSE 383.4158604647792 Test RE 0.9981541563980262\n",
      "6 Train Loss 2.3786867 Test MSE 383.2110465657148 Test RE 0.9978875227314123\n",
      "7 Train Loss 2.3784575 Test MSE 383.2045182623242 Test RE 0.9978790227934516\n",
      "8 Train Loss 2.3777373 Test MSE 383.0672762617388 Test RE 0.9977003151176536\n",
      "9 Train Loss 2.3728707 Test MSE 381.5681129613121 Test RE 0.9957461127356304\n",
      "10 Train Loss 2.3719401 Test MSE 381.08991954392974 Test RE 0.9951219666154717\n",
      "11 Train Loss 2.36378 Test MSE 379.7971558971169 Test RE 0.9934326666668972\n",
      "12 Train Loss 2.3545687 Test MSE 377.9637986057104 Test RE 0.9910320163702218\n",
      "13 Train Loss 2.2730465 Test MSE 364.1965591000682 Test RE 0.9728155448507656\n",
      "14 Train Loss 2.2044053 Test MSE 349.5789303795823 Test RE 0.9530928403498411\n",
      "15 Train Loss 2.0831182 Test MSE 316.22397026995674 Test RE 0.9064836516709781\n",
      "16 Train Loss 1.9473478 Test MSE 304.3406791897358 Test RE 0.8892883138278608\n",
      "17 Train Loss 1.7611469 Test MSE 272.38956247612384 Test RE 0.8413134143253232\n",
      "18 Train Loss 1.5481976 Test MSE 234.35472685425884 Test RE 0.7803679802465608\n",
      "19 Train Loss 1.4949658 Test MSE 223.0118037305541 Test RE 0.7612486013088688\n",
      "20 Train Loss 1.3779076 Test MSE 209.71226458410064 Test RE 0.7382007762455258\n",
      "21 Train Loss 1.2581954 Test MSE 187.48755184432545 Test RE 0.6979893588109033\n",
      "22 Train Loss 1.2107458 Test MSE 177.25432728335127 Test RE 0.6786736781073396\n",
      "23 Train Loss 1.0709934 Test MSE 153.5003845528142 Test RE 0.6315639269521891\n",
      "24 Train Loss 0.97793853 Test MSE 149.09540900773672 Test RE 0.6224360211792201\n",
      "25 Train Loss 0.93166864 Test MSE 137.55591737839467 Test RE 0.5978637492989518\n",
      "26 Train Loss 0.79756737 Test MSE 120.74707083603887 Test RE 0.5601455428701709\n",
      "27 Train Loss 0.7909405 Test MSE 119.23506705249164 Test RE 0.5566274025381994\n",
      "28 Train Loss 0.74213016 Test MSE 114.55509539305984 Test RE 0.5455942544293126\n",
      "29 Train Loss 0.6510468 Test MSE 99.39472460840744 Test RE 0.50821122114639\n",
      "30 Train Loss 0.5860835 Test MSE 88.63011544200643 Test RE 0.4799027515801454\n",
      "31 Train Loss 0.5503557 Test MSE 85.22312374638895 Test RE 0.470588496708358\n",
      "32 Train Loss 0.5279398 Test MSE 80.47679224334796 Test RE 0.45729653554353394\n",
      "33 Train Loss 0.50755316 Test MSE 75.0833872546529 Test RE 0.44170723300809756\n",
      "34 Train Loss 0.4969756 Test MSE 73.57683587221119 Test RE 0.4372533406330645\n",
      "35 Train Loss 0.4618312 Test MSE 67.69220009714762 Test RE 0.41940335213836977\n",
      "36 Train Loss 0.43766493 Test MSE 61.80594262455955 Test RE 0.40075384673175274\n",
      "37 Train Loss 0.41693392 Test MSE 58.637533388457925 Test RE 0.39034662537297565\n",
      "38 Train Loss 0.4033356 Test MSE 56.7588658508647 Test RE 0.3840426315823901\n",
      "39 Train Loss 0.39388427 Test MSE 54.62046609861957 Test RE 0.37673874216446723\n",
      "40 Train Loss 0.37891534 Test MSE 53.21287451309591 Test RE 0.3718527027333616\n",
      "41 Train Loss 0.3698243 Test MSE 52.32439421033204 Test RE 0.36873527559961994\n",
      "42 Train Loss 0.36073568 Test MSE 49.83583880518622 Test RE 0.35985991144087554\n",
      "43 Train Loss 0.344224 Test MSE 44.67839855103732 Test RE 0.3407307910167641\n",
      "44 Train Loss 0.29733145 Test MSE 40.357598990192 Test RE 0.3238360867892167\n",
      "45 Train Loss 0.29439697 Test MSE 39.06699419132822 Test RE 0.318616000328255\n",
      "46 Train Loss 0.2839322 Test MSE 35.846184910209786 Test RE 0.30519966522221026\n",
      "47 Train Loss 0.27167675 Test MSE 35.51107595987752 Test RE 0.3037697321066798\n",
      "48 Train Loss 0.2691158 Test MSE 35.43338551095899 Test RE 0.3034372593743505\n",
      "49 Train Loss 0.2524625 Test MSE 31.410696956227238 Test RE 0.2856941640026776\n",
      "50 Train Loss 0.21723694 Test MSE 27.854574319640875 Test RE 0.2690362776178043\n",
      "51 Train Loss 0.20488977 Test MSE 26.263066462036612 Test RE 0.2612373676076206\n",
      "52 Train Loss 0.19635922 Test MSE 23.49178884477913 Test RE 0.2470703479233695\n",
      "53 Train Loss 0.18879482 Test MSE 21.86353853247313 Test RE 0.2383541998314251\n",
      "54 Train Loss 0.17579204 Test MSE 22.346497284732095 Test RE 0.24097240497017805\n",
      "55 Train Loss 0.17098068 Test MSE 22.14059779385536 Test RE 0.23985968214623535\n",
      "56 Train Loss 0.16684718 Test MSE 22.042456905450944 Test RE 0.2393274881531671\n",
      "57 Train Loss 0.16588823 Test MSE 22.48665408107989 Test RE 0.24172691087644915\n",
      "58 Train Loss 0.16578512 Test MSE 22.579561678176546 Test RE 0.24222576492081155\n",
      "59 Train Loss 0.16569671 Test MSE 22.60446496085759 Test RE 0.24235930503215347\n",
      "60 Train Loss 0.16360606 Test MSE 22.00532230723494 Test RE 0.2391258074513029\n",
      "61 Train Loss 0.15478075 Test MSE 20.475652342050477 Test RE 0.2306648694904208\n",
      "62 Train Loss 0.15264413 Test MSE 20.030694330922298 Test RE 0.22814480515386776\n",
      "63 Train Loss 0.15127555 Test MSE 19.12189037251461 Test RE 0.22290920067091033\n",
      "64 Train Loss 0.15122026 Test MSE 19.027571508199923 Test RE 0.22235877044911068\n",
      "65 Train Loss 0.15118214 Test MSE 18.990055440971435 Test RE 0.2221394533764018\n",
      "66 Train Loss 0.15118098 Test MSE 18.923673944371465 Test RE 0.22175085897643085\n",
      "67 Train Loss 0.15114483 Test MSE 18.858414014074842 Test RE 0.22136816522174693\n",
      "68 Train Loss 0.15046075 Test MSE 18.34677222365212 Test RE 0.21834458065114057\n",
      "69 Train Loss 0.14896499 Test MSE 18.288888319747148 Test RE 0.21799987093807005\n",
      "70 Train Loss 0.14542143 Test MSE 17.536435482476048 Test RE 0.21346822721095593\n",
      "71 Train Loss 0.14321557 Test MSE 16.73176350935482 Test RE 0.20851314620444925\n",
      "72 Train Loss 0.14132997 Test MSE 16.30460025351861 Test RE 0.20583425898905242\n",
      "73 Train Loss 0.14035642 Test MSE 16.172876016851912 Test RE 0.20500110931029097\n",
      "74 Train Loss 0.13982257 Test MSE 15.667389452007173 Test RE 0.20177200149723232\n",
      "75 Train Loss 0.13966092 Test MSE 15.429966826115312 Test RE 0.20023734522584774\n",
      "76 Train Loss 0.13961846 Test MSE 15.377887147458246 Test RE 0.19989913610760632\n",
      "77 Train Loss 0.13961846 Test MSE 15.377887147458246 Test RE 0.19989913610760632\n",
      "78 Train Loss 0.1396173 Test MSE 15.377892046115855 Test RE 0.19989916794674448\n",
      "79 Train Loss 0.13961563 Test MSE 15.359114177676322 Test RE 0.1997770827199164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 Train Loss 0.13961285 Test MSE 15.35911539679956 Test RE 0.1997770906485273\n",
      "81 Train Loss 0.13960853 Test MSE 15.346877992038747 Test RE 0.19969748839877005\n",
      "82 Train Loss 0.13960664 Test MSE 15.337886510858093 Test RE 0.19963898010521539\n",
      "83 Train Loss 0.13958019 Test MSE 15.329456640368562 Test RE 0.19958411067947604\n",
      "84 Train Loss 0.13955934 Test MSE 15.329340991323349 Test RE 0.19958335782319697\n",
      "85 Train Loss 0.13955827 Test MSE 15.329337162364736 Test RE 0.19958333289725708\n",
      "86 Train Loss 0.139557 Test MSE 15.320045488072438 Test RE 0.19952283633424228\n",
      "87 Train Loss 0.13955611 Test MSE 15.320076835595113 Test RE 0.19952304046364394\n",
      "88 Train Loss 0.13955611 Test MSE 15.320076835595113 Test RE 0.19952304046364394\n",
      "89 Train Loss 0.13955581 Test MSE 15.320076863193162 Test RE 0.19952304064335735\n",
      "90 Train Loss 0.13955581 Test MSE 15.320076863193162 Test RE 0.19952304064335735\n",
      "91 Train Loss 0.13955583 Test MSE 15.320076863193162 Test RE 0.19952304064335735\n",
      "92 Train Loss 0.13955581 Test MSE 15.320076863193162 Test RE 0.19952304064335735\n",
      "93 Train Loss 0.13954172 Test MSE 15.322009467883595 Test RE 0.19953562501247374\n",
      "94 Train Loss 0.13954172 Test MSE 15.322009467883595 Test RE 0.19953562501247374\n",
      "95 Train Loss 0.13954163 Test MSE 15.32199905032006 Test RE 0.19953555717948454\n",
      "96 Train Loss 0.13954163 Test MSE 15.32199905032006 Test RE 0.19953555717948454\n",
      "97 Train Loss 0.1395415 Test MSE 15.321999003523507 Test RE 0.19953555687477312\n",
      "98 Train Loss 0.1395415 Test MSE 15.321999003523507 Test RE 0.19953555687477312\n",
      "99 Train Loss 0.1395415 Test MSE 15.321999003523507 Test RE 0.19953555687477312\n",
      "Training time: 59.86\n",
      "Training time: 59.86\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 4.329498 Test MSE 386.79217226308754 Test RE 1.002539333648731\n",
      "1 Train Loss 2.4188359 Test MSE 383.48723457362706 Test RE 0.9982470568953196\n",
      "2 Train Loss 2.3977609 Test MSE 384.23294193427756 Test RE 0.9992171526154661\n",
      "3 Train Loss 2.3841438 Test MSE 384.05971577858713 Test RE 0.9989918855438168\n",
      "4 Train Loss 2.381576 Test MSE 383.8572168370684 Test RE 0.9987284870883595\n",
      "5 Train Loss 2.3815713 Test MSE 383.85529969961834 Test RE 0.9987259930592104\n",
      "6 Train Loss 2.381567 Test MSE 383.8535472808752 Test RE 0.9987237133093302\n",
      "7 Train Loss 2.3815627 Test MSE 383.85180307658703 Test RE 0.9987214442405998\n",
      "8 Train Loss 2.3815582 Test MSE 383.84990496419164 Test RE 0.9987189749440103\n",
      "9 Train Loss 2.381553 Test MSE 383.8476963234076 Test RE 0.9987161016664555\n",
      "10 Train Loss 2.3815458 Test MSE 383.845003235115 Test RE 0.9987125981475018\n",
      "11 Train Loss 2.3815377 Test MSE 383.84157700064384 Test RE 0.9987081408390341\n",
      "12 Train Loss 2.3811767 Test MSE 383.7397195833116 Test RE 0.9985756218683727\n",
      "13 Train Loss 2.3803246 Test MSE 383.68575866563305 Test RE 0.9985054102788694\n",
      "14 Train Loss 2.3790386 Test MSE 383.4233586538155 Test RE 0.9981639164428397\n",
      "15 Train Loss 2.3772814 Test MSE 383.03445489440776 Test RE 0.9976575725081847\n",
      "16 Train Loss 2.3767893 Test MSE 382.7118442810882 Test RE 0.9972373456639213\n",
      "17 Train Loss 2.3756676 Test MSE 382.5910909742046 Test RE 0.997080009002871\n",
      "18 Train Loss 2.3739476 Test MSE 382.0972325159336 Test RE 0.996436272888477\n",
      "19 Train Loss 2.3715324 Test MSE 381.5514384667807 Test RE 0.9957243554871276\n",
      "20 Train Loss 2.3686311 Test MSE 380.2790336962139 Test RE 0.9940626890232515\n",
      "21 Train Loss 2.3670762 Test MSE 378.1064817651558 Test RE 0.9912190584058791\n",
      "22 Train Loss 2.3430667 Test MSE 371.8229191131005 Test RE 0.9829482653293474\n",
      "23 Train Loss 2.3332655 Test MSE 371.7919555607355 Test RE 0.9829073369704731\n",
      "24 Train Loss 2.305289 Test MSE 368.2969187705777 Test RE 0.9782765094107172\n",
      "25 Train Loss 2.2845426 Test MSE 365.64480589597423 Test RE 0.97474785189787\n",
      "26 Train Loss 2.2200346 Test MSE 349.75395585287356 Test RE 0.9533314054292941\n",
      "27 Train Loss 2.058149 Test MSE 325.3314237493185 Test RE 0.9194446485716534\n",
      "28 Train Loss 1.9659292 Test MSE 305.16439772801255 Test RE 0.8904909599638706\n",
      "29 Train Loss 1.8761784 Test MSE 294.19207427658216 Test RE 0.8743354086650644\n",
      "30 Train Loss 1.7006611 Test MSE 263.8942948870885 Test RE 0.828090082099263\n",
      "31 Train Loss 1.6242831 Test MSE 247.38092180166166 Test RE 0.8017623978717678\n",
      "32 Train Loss 1.5666234 Test MSE 233.76067094845257 Test RE 0.7793782917404409\n",
      "33 Train Loss 1.5308859 Test MSE 226.1712734680338 Test RE 0.76662204593075\n",
      "34 Train Loss 1.46385 Test MSE 212.48344614270508 Test RE 0.7430621386204969\n",
      "35 Train Loss 1.335161 Test MSE 201.4254151883279 Test RE 0.7234686494380567\n",
      "36 Train Loss 1.2997212 Test MSE 200.04711428832218 Test RE 0.7209891480991857\n",
      "37 Train Loss 1.2619804 Test MSE 192.89826193429127 Test RE 0.7079893744261996\n",
      "38 Train Loss 1.2402416 Test MSE 183.88693492739537 Test RE 0.6912545763352448\n",
      "39 Train Loss 1.1985633 Test MSE 177.45554434233006 Test RE 0.67905878009625\n",
      "40 Train Loss 1.1528045 Test MSE 172.72506284568243 Test RE 0.6699467154073333\n",
      "41 Train Loss 1.122211 Test MSE 161.46173442542576 Test RE 0.6477350367908731\n",
      "42 Train Loss 1.1047035 Test MSE 158.95977739778542 Test RE 0.6426969005411206\n",
      "43 Train Loss 1.0473185 Test MSE 152.72406456261228 Test RE 0.6299648521790352\n",
      "44 Train Loss 0.94449806 Test MSE 136.777056098099 Test RE 0.5961687515293089\n",
      "45 Train Loss 0.8214707 Test MSE 119.18149968592708 Test RE 0.5565023536977298\n",
      "46 Train Loss 0.78594136 Test MSE 109.3393813712279 Test RE 0.5330290623535617\n",
      "47 Train Loss 0.70567787 Test MSE 98.32734917560504 Test RE 0.505475078148487\n",
      "48 Train Loss 0.6686963 Test MSE 92.85280256659694 Test RE 0.49120195970078284\n",
      "49 Train Loss 0.6408646 Test MSE 94.22064284848595 Test RE 0.4948067483247259\n",
      "50 Train Loss 0.60706276 Test MSE 89.98881757659892 Test RE 0.48356722215077896\n",
      "51 Train Loss 0.5115987 Test MSE 71.34503507332583 Test RE 0.43057068744197646\n",
      "52 Train Loss 0.47187912 Test MSE 63.58543580556111 Test RE 0.4064820841339194\n",
      "53 Train Loss 0.45088834 Test MSE 58.739516645941755 Test RE 0.3906859261937869\n",
      "54 Train Loss 0.38723284 Test MSE 51.648016849584785 Test RE 0.36634427388627955\n",
      "55 Train Loss 0.3426325 Test MSE 46.38554611639824 Test RE 0.3471793770637131\n",
      "56 Train Loss 0.31273386 Test MSE 38.69064178914101 Test RE 0.31707759057501994\n",
      "57 Train Loss 0.3015394 Test MSE 32.69558907530238 Test RE 0.2914789294357357\n",
      "58 Train Loss 0.29056853 Test MSE 30.915501773533308 Test RE 0.28343320804369976\n",
      "59 Train Loss 0.28132832 Test MSE 31.498687526443117 Test RE 0.28609404070784344\n",
      "60 Train Loss 0.26742932 Test MSE 30.932947480080283 Test RE 0.28351316785205266\n",
      "61 Train Loss 0.25978017 Test MSE 32.08790809550731 Test RE 0.28875750794249605\n",
      "62 Train Loss 0.25460672 Test MSE 34.05696152739068 Test RE 0.29748531481148527\n",
      "63 Train Loss 0.25235116 Test MSE 34.11583947692227 Test RE 0.29774235127252546\n",
      "64 Train Loss 0.24838397 Test MSE 31.62676761928219 Test RE 0.28667510900432364\n",
      "65 Train Loss 0.24453044 Test MSE 31.384477652294073 Test RE 0.2855749010339064\n",
      "66 Train Loss 0.23591787 Test MSE 29.15987946804109 Test RE 0.2752678208389963\n",
      "67 Train Loss 0.23160252 Test MSE 27.68523246656248 Test RE 0.26821722799441994\n",
      "68 Train Loss 0.21599391 Test MSE 25.446335460425562 Test RE 0.2571432962069677\n",
      "69 Train Loss 0.20062552 Test MSE 22.882018584295455 Test RE 0.24384269495868222\n",
      "70 Train Loss 0.19741549 Test MSE 22.610536638102484 Test RE 0.24239185233372157\n",
      "71 Train Loss 0.19665124 Test MSE 23.258338310686554 Test RE 0.24583964732421337\n",
      "72 Train Loss 0.19546998 Test MSE 23.232773798274565 Test RE 0.2457045022728984\n",
      "73 Train Loss 0.18641527 Test MSE 21.21182831678719 Test RE 0.23477488377625688\n",
      "74 Train Loss 0.17089245 Test MSE 19.8791336524095 Test RE 0.22728004637314173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 Train Loss 0.1567012 Test MSE 17.71796454194456 Test RE 0.21457024477786904\n",
      "76 Train Loss 0.1470113 Test MSE 16.049089428625038 Test RE 0.20421506692422817\n",
      "77 Train Loss 0.13872059 Test MSE 14.57025725645525 Test RE 0.1945791009723012\n",
      "78 Train Loss 0.13254218 Test MSE 12.422159651202355 Test RE 0.17966402950306556\n",
      "79 Train Loss 0.129855 Test MSE 11.868316823773082 Test RE 0.17561319657591215\n",
      "80 Train Loss 0.123261206 Test MSE 12.021237255256706 Test RE 0.1767409425936915\n",
      "81 Train Loss 0.1208106 Test MSE 11.826176385067107 Test RE 0.17530114735914282\n",
      "82 Train Loss 0.118648574 Test MSE 10.872679654468216 Test RE 0.1680857432039336\n",
      "83 Train Loss 0.11485492 Test MSE 10.45058062356116 Test RE 0.16479073567541133\n",
      "84 Train Loss 0.11019272 Test MSE 10.582798805546613 Test RE 0.16582990515494847\n",
      "85 Train Loss 0.102876395 Test MSE 8.80847400678432 Test RE 0.15129093994342616\n",
      "86 Train Loss 0.09830857 Test MSE 6.987856787592682 Test RE 0.1347518016311201\n",
      "87 Train Loss 0.094223484 Test MSE 6.164417894201946 Test RE 0.1265635415022113\n",
      "88 Train Loss 0.08567186 Test MSE 5.129267320671576 Test RE 0.11544901963951042\n",
      "89 Train Loss 0.07111992 Test MSE 3.2937357948304595 Test RE 0.09251389423042136\n",
      "90 Train Loss 0.059832767 Test MSE 2.8929174900520853 Test RE 0.08670229754583196\n",
      "91 Train Loss 0.059108384 Test MSE 3.1076043759347387 Test RE 0.08986186850918136\n",
      "92 Train Loss 0.057559498 Test MSE 3.0836345283297555 Test RE 0.0895146323930317\n",
      "93 Train Loss 0.05053501 Test MSE 3.478088066313938 Test RE 0.09506767431060567\n",
      "94 Train Loss 0.04506609 Test MSE 3.9967380903298726 Test RE 0.1019096775101257\n",
      "95 Train Loss 0.03808547 Test MSE 2.9251411741187185 Test RE 0.08718384089936332\n",
      "96 Train Loss 0.036939282 Test MSE 2.4997728458610022 Test RE 0.08059588260586834\n",
      "97 Train Loss 0.03104579 Test MSE 2.2920137809187158 Test RE 0.0771740327104877\n",
      "98 Train Loss 0.023250487 Test MSE 1.7986745135069668 Test RE 0.0683657957209067\n",
      "99 Train Loss 0.020440519 Test MSE 1.181580352818313 Test RE 0.055410773541921685\n",
      "Training time: 71.68\n",
      "Training time: 71.68\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 4.381975 Test MSE 387.1856129425112 Test RE 1.0030490899609195\n",
      "1 Train Loss 3.4630024 Test MSE 382.444773690378 Test RE 0.9968893302426353\n",
      "2 Train Loss 2.4065123 Test MSE 383.81683394315115 Test RE 0.998675951138035\n",
      "3 Train Loss 2.3835833 Test MSE 383.7803847403943 Test RE 0.9986285303330338\n",
      "4 Train Loss 2.3809657 Test MSE 383.64364731701323 Test RE 0.9984506134010881\n",
      "5 Train Loss 2.3800385 Test MSE 383.56336267827254 Test RE 0.9983461356630153\n",
      "6 Train Loss 2.378931 Test MSE 383.1536055521681 Test RE 0.9978127312824786\n",
      "7 Train Loss 2.378068 Test MSE 383.0435364098749 Test RE 0.9976693993671751\n",
      "8 Train Loss 2.3771894 Test MSE 383.0828269769263 Test RE 0.9977205658633439\n",
      "9 Train Loss 2.3760235 Test MSE 382.5926590654498 Test RE 0.9970820523212699\n",
      "10 Train Loss 2.3702493 Test MSE 381.30149236308995 Test RE 0.9953981633019854\n",
      "11 Train Loss 2.3683012 Test MSE 380.9302277091837 Test RE 0.9949134469253873\n",
      "12 Train Loss 2.3598201 Test MSE 379.16136146148267 Test RE 0.9926007969284388\n",
      "13 Train Loss 2.3414702 Test MSE 376.7332042356942 Test RE 0.9894173740166051\n",
      "14 Train Loss 2.3281538 Test MSE 373.4702144624901 Test RE 0.9851232478055819\n",
      "15 Train Loss 2.3120863 Test MSE 370.4659417676073 Test RE 0.981152978306259\n",
      "16 Train Loss 2.2863545 Test MSE 365.90380571267485 Test RE 0.9750930158658945\n",
      "17 Train Loss 2.2552822 Test MSE 359.191415649656 Test RE 0.9661077299449807\n",
      "18 Train Loss 2.1953902 Test MSE 350.10559991729417 Test RE 0.953810526688148\n",
      "19 Train Loss 2.1134093 Test MSE 335.4302741315735 Test RE 0.9336061654553636\n",
      "20 Train Loss 2.069949 Test MSE 328.0142347658223 Test RE 0.9232279163039697\n",
      "21 Train Loss 2.0275462 Test MSE 324.3528108878151 Test RE 0.9180607397970031\n",
      "22 Train Loss 1.9856325 Test MSE 314.10404225205855 Test RE 0.9034400626458255\n",
      "23 Train Loss 1.940087 Test MSE 302.76392807426913 Test RE 0.8869816764066439\n",
      "24 Train Loss 1.8827567 Test MSE 297.54006427132794 Test RE 0.8792964277210485\n",
      "25 Train Loss 1.7974688 Test MSE 281.0161996516975 Test RE 0.8545318624649868\n",
      "26 Train Loss 1.7432712 Test MSE 269.1563335383343 Test RE 0.836305367943337\n",
      "27 Train Loss 1.6598798 Test MSE 260.31330506146975 Test RE 0.8224523874645251\n",
      "28 Train Loss 1.6274436 Test MSE 256.262708344686 Test RE 0.8160284277201919\n",
      "29 Train Loss 1.6034641 Test MSE 249.96879736233154 Test RE 0.8059451439935744\n",
      "30 Train Loss 1.5255413 Test MSE 233.7530727693685 Test RE 0.7793656251444461\n",
      "31 Train Loss 1.4580576 Test MSE 221.49077718400812 Test RE 0.7586481551234573\n",
      "32 Train Loss 1.4136152 Test MSE 211.5547233168988 Test RE 0.7414364719975101\n",
      "33 Train Loss 1.3773572 Test MSE 203.41550448342647 Test RE 0.7270338113548892\n",
      "34 Train Loss 1.2792758 Test MSE 192.87762044001215 Test RE 0.7079514934483054\n",
      "35 Train Loss 1.1992549 Test MSE 183.9037479095718 Test RE 0.6912861766912837\n",
      "36 Train Loss 1.1129551 Test MSE 171.74203381786 Test RE 0.668037563451254\n",
      "37 Train Loss 1.045226 Test MSE 161.67586923072676 Test RE 0.6481644161084786\n",
      "38 Train Loss 1.0305616 Test MSE 156.7989542814455 Test RE 0.6383136967828699\n",
      "39 Train Loss 1.0208722 Test MSE 154.7374661921522 Test RE 0.6341037522017795\n",
      "40 Train Loss 0.9698303 Test MSE 147.28886210230675 Test RE 0.6186535879725741\n",
      "41 Train Loss 0.90782356 Test MSE 138.52021256287998 Test RE 0.5999556632478253\n",
      "42 Train Loss 0.8828094 Test MSE 135.67027439634506 Test RE 0.5937517931459563\n",
      "43 Train Loss 0.8359357 Test MSE 129.11657440056487 Test RE 0.5792333783486663\n",
      "44 Train Loss 0.7900212 Test MSE 118.39850691326333 Test RE 0.5546713004131338\n",
      "45 Train Loss 0.7635095 Test MSE 115.9863259710924 Test RE 0.5489919526291196\n",
      "46 Train Loss 0.74785453 Test MSE 113.10786544177198 Test RE 0.5421369220738119\n",
      "47 Train Loss 0.71188694 Test MSE 104.5551400144001 Test RE 0.5212370479829693\n",
      "48 Train Loss 0.67767715 Test MSE 96.12689281825482 Test RE 0.4997870912782414\n",
      "49 Train Loss 0.6516322 Test MSE 90.50793903672361 Test RE 0.4849600014884047\n",
      "50 Train Loss 0.61381483 Test MSE 87.29025467354536 Test RE 0.47626148613463376\n",
      "51 Train Loss 0.5800931 Test MSE 84.77229446102162 Test RE 0.46934214251858114\n",
      "52 Train Loss 0.5681863 Test MSE 81.37193954097488 Test RE 0.459832768256392\n",
      "53 Train Loss 0.52010095 Test MSE 75.73277503128656 Test RE 0.44361325873440316\n",
      "54 Train Loss 0.4689466 Test MSE 69.37422288810086 Test RE 0.42458206815956423\n",
      "55 Train Loss 0.44352147 Test MSE 61.43610262889981 Test RE 0.3995530140464119\n",
      "56 Train Loss 0.41243386 Test MSE 54.898665408072205 Test RE 0.37769694838754037\n",
      "57 Train Loss 0.354182 Test MSE 43.28766350149188 Test RE 0.3353857876901995\n",
      "58 Train Loss 0.32640418 Test MSE 36.78495914031266 Test RE 0.30917026735432207\n",
      "59 Train Loss 0.30401862 Test MSE 33.33999194850598 Test RE 0.29433731770510224\n",
      "60 Train Loss 0.29611027 Test MSE 32.59292220954022 Test RE 0.29102093559238734\n",
      "61 Train Loss 0.2743209 Test MSE 31.91111911210653 Test RE 0.28796095157970353\n",
      "62 Train Loss 0.26340032 Test MSE 31.088246258312353 Test RE 0.2842239653716873\n",
      "63 Train Loss 0.25654054 Test MSE 30.785926666204567 Test RE 0.2828386123053767\n",
      "64 Train Loss 0.23955691 Test MSE 28.780683700754057 Test RE 0.27347216941280517\n",
      "65 Train Loss 0.2142696 Test MSE 27.202845389363187 Test RE 0.26587025316717916\n",
      "66 Train Loss 0.20712344 Test MSE 27.666008727213676 Test RE 0.2681240910944272\n",
      "67 Train Loss 0.19757767 Test MSE 26.082871473599734 Test RE 0.26033962994941046\n",
      "68 Train Loss 0.18868943 Test MSE 25.184970898828468 Test RE 0.25581930175234696\n",
      "69 Train Loss 0.17898245 Test MSE 23.683866328246104 Test RE 0.2480783605484889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 Train Loss 0.17086022 Test MSE 20.865571380576785 Test RE 0.2328507941585199\n",
      "71 Train Loss 0.16864602 Test MSE 19.311619880282567 Test RE 0.22401233591927047\n",
      "72 Train Loss 0.16702777 Test MSE 18.215247303905638 Test RE 0.21756053519085605\n",
      "73 Train Loss 0.15889168 Test MSE 16.169369473479478 Test RE 0.2049788843130482\n",
      "74 Train Loss 0.13460281 Test MSE 15.618418528086506 Test RE 0.2014564194219474\n",
      "75 Train Loss 0.12549949 Test MSE 15.95375936823648 Test RE 0.20360765457855187\n",
      "76 Train Loss 0.12075368 Test MSE 16.0858402404388 Test RE 0.20444874927657436\n",
      "77 Train Loss 0.11394195 Test MSE 14.669123695061126 Test RE 0.19523814284505378\n",
      "78 Train Loss 0.10998588 Test MSE 13.830244964111262 Test RE 0.18957345218323066\n",
      "79 Train Loss 0.10805252 Test MSE 12.852551373729218 Test RE 0.18274994577752468\n",
      "80 Train Loss 0.10298775 Test MSE 12.115874081313326 Test RE 0.17743527260735456\n",
      "81 Train Loss 0.10124384 Test MSE 12.595355485229815 Test RE 0.18091217594336542\n",
      "82 Train Loss 0.09915114 Test MSE 12.140975738596907 Test RE 0.17761898262502188\n",
      "83 Train Loss 0.09123091 Test MSE 10.667607282470648 Test RE 0.16649304343155713\n",
      "84 Train Loss 0.08870085 Test MSE 10.111421680722591 Test RE 0.16209465473021997\n",
      "85 Train Loss 0.07896785 Test MSE 8.134726051244488 Test RE 0.14538983662754124\n",
      "86 Train Loss 0.07740961 Test MSE 7.620727884391605 Test RE 0.14072161469388034\n",
      "87 Train Loss 0.07649562 Test MSE 7.129531844396351 Test RE 0.13611095749272883\n",
      "88 Train Loss 0.07539736 Test MSE 6.40653785408555 Test RE 0.12902512272325223\n",
      "89 Train Loss 0.07405913 Test MSE 5.942606647484724 Test RE 0.12426564396301011\n",
      "90 Train Loss 0.070190996 Test MSE 5.691479662992169 Test RE 0.12161164881795344\n",
      "91 Train Loss 0.058789104 Test MSE 4.8957619021696015 Test RE 0.11279055320062238\n",
      "92 Train Loss 0.05262521 Test MSE 4.2339411583606035 Test RE 0.10489021885289865\n",
      "93 Train Loss 0.050390538 Test MSE 4.3657035300565505 Test RE 0.1065098326942754\n",
      "94 Train Loss 0.047783386 Test MSE 4.699404694437753 Test RE 0.11050552843984084\n",
      "95 Train Loss 0.046463087 Test MSE 4.57146032009204 Test RE 0.10899085506664469\n",
      "96 Train Loss 0.045990836 Test MSE 4.539158889368305 Test RE 0.10860511382109728\n",
      "97 Train Loss 0.04598066 Test MSE 4.541989099087119 Test RE 0.10863896671743851\n",
      "98 Train Loss 0.04597181 Test MSE 4.5441240831611065 Test RE 0.10866449685508085\n",
      "99 Train Loss 0.045961738 Test MSE 4.546445273330271 Test RE 0.10869224684108873\n",
      "Training time: 96.09\n",
      "Training time: 96.09\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 5.070549 Test MSE 382.95028571806756 Test RE 0.997547952304768\n",
      "1 Train Loss 4.904154 Test MSE 383.3094628351056 Test RE 0.9980156532530658\n",
      "2 Train Loss 4.8923707 Test MSE 383.3594097587447 Test RE 0.9980806740687704\n",
      "3 Train Loss 4.889838 Test MSE 383.32631251858226 Test RE 0.9980375886142985\n",
      "4 Train Loss 4.873123 Test MSE 384.52404041188623 Test RE 0.9995955890917226\n",
      "5 Train Loss 4.329788 Test MSE 386.66786660271725 Test RE 1.0023782247418904\n",
      "6 Train Loss 3.7686694 Test MSE 382.4255277189742 Test RE 0.9968642464315252\n",
      "7 Train Loss 2.6184928 Test MSE 382.7832642491829 Test RE 0.9973303913038343\n",
      "8 Train Loss 2.4112475 Test MSE 382.6703736720437 Test RE 0.9971833139412307\n",
      "9 Train Loss 2.3836675 Test MSE 382.6196296834574 Test RE 0.9971171960238511\n",
      "10 Train Loss 2.3799706 Test MSE 382.92309952397795 Test RE 0.9975125429910957\n",
      "11 Train Loss 2.3795674 Test MSE 383.0563427603803 Test RE 0.9976860768411623\n",
      "12 Train Loss 2.376208 Test MSE 382.5014007877393 Test RE 0.9969631302571506\n",
      "13 Train Loss 2.3735325 Test MSE 382.32542668657965 Test RE 0.9967337717851658\n",
      "14 Train Loss 2.3705058 Test MSE 381.91381041738475 Test RE 0.9961970793586484\n",
      "15 Train Loss 2.3631704 Test MSE 379.79232838921706 Test RE 0.9934263530081302\n",
      "16 Train Loss 2.3598285 Test MSE 379.0779509877986 Test RE 0.9924916114141368\n",
      "17 Train Loss 2.3549867 Test MSE 377.292128373369 Test RE 0.9901510554206149\n",
      "18 Train Loss 2.343166 Test MSE 375.13030840177925 Test RE 0.9873102813923618\n",
      "19 Train Loss 2.3345282 Test MSE 375.2255796271081 Test RE 0.987435646214572\n",
      "20 Train Loss 2.3106995 Test MSE 369.2634055397074 Test RE 0.9795592676642392\n",
      "21 Train Loss 2.2978368 Test MSE 366.9741856506173 Test RE 0.9765181962005991\n",
      "22 Train Loss 2.2627625 Test MSE 361.2320230578513 Test RE 0.9688481274423354\n",
      "23 Train Loss 2.1901767 Test MSE 341.8822779125523 Test RE 0.9425423589133158\n",
      "24 Train Loss 2.153246 Test MSE 332.0373169573821 Test RE 0.9288723396384735\n",
      "25 Train Loss 2.1288505 Test MSE 333.7883337929571 Test RE 0.9313183484700712\n",
      "26 Train Loss 2.1136944 Test MSE 332.959836668435 Test RE 0.9301618161934035\n",
      "27 Train Loss 2.0652866 Test MSE 327.4808260506263 Test RE 0.922476945563811\n",
      "28 Train Loss 1.9937938 Test MSE 308.28918822466284 Test RE 0.8950385263078465\n",
      "29 Train Loss 1.9358996 Test MSE 303.1592746999624 Test RE 0.8875605941393406\n",
      "30 Train Loss 1.9142904 Test MSE 293.6741795887143 Test RE 0.8735654811896704\n",
      "31 Train Loss 1.8318417 Test MSE 285.1277697721263 Test RE 0.8607605236344461\n",
      "32 Train Loss 1.7785051 Test MSE 278.44784115288144 Test RE 0.8506178853355716\n",
      "33 Train Loss 1.7434067 Test MSE 266.55135542573055 Test RE 0.8322485174116092\n",
      "34 Train Loss 1.6747934 Test MSE 251.7788107261995 Test RE 0.8088577880689795\n",
      "35 Train Loss 1.6206988 Test MSE 249.70421414983215 Test RE 0.8055184987200872\n",
      "36 Train Loss 1.5732896 Test MSE 243.2284613184391 Test RE 0.7950048511524187\n",
      "37 Train Loss 1.5523677 Test MSE 232.18961343916547 Test RE 0.7767548556175888\n",
      "38 Train Loss 1.5073583 Test MSE 225.30311818939376 Test RE 0.7651492972752836\n",
      "39 Train Loss 1.4750754 Test MSE 223.9369011898098 Test RE 0.7628258728372205\n",
      "40 Train Loss 1.4527987 Test MSE 225.08546713591855 Test RE 0.7647796268702457\n",
      "41 Train Loss 1.430395 Test MSE 221.48408997339192 Test RE 0.7586367025511322\n",
      "42 Train Loss 1.3966432 Test MSE 215.40370978444003 Test RE 0.7481508468390654\n",
      "43 Train Loss 1.380856 Test MSE 209.61476080931465 Test RE 0.7380291464874282\n",
      "44 Train Loss 1.3488901 Test MSE 203.11006766380135 Test RE 0.7264877705860958\n",
      "45 Train Loss 1.2833469 Test MSE 190.1704427529817 Test RE 0.7029656295083715\n",
      "46 Train Loss 1.22469 Test MSE 169.98473720132074 Test RE 0.6646110330318452\n",
      "47 Train Loss 1.1515753 Test MSE 165.12570712314738 Test RE 0.6550431775983329\n",
      "48 Train Loss 1.1021218 Test MSE 152.19281387448902 Test RE 0.6288682312691798\n",
      "49 Train Loss 0.9699375 Test MSE 145.06722184919803 Test RE 0.613970111021371\n",
      "50 Train Loss 0.9514105 Test MSE 145.76635080818406 Test RE 0.6154477996146925\n",
      "51 Train Loss 0.9437581 Test MSE 144.61485605082405 Test RE 0.6130120864108464\n",
      "52 Train Loss 0.92661476 Test MSE 140.61343333874203 Test RE 0.604471721966741\n",
      "53 Train Loss 0.9096074 Test MSE 137.97298607696953 Test RE 0.5987694229314435\n",
      "54 Train Loss 0.89557713 Test MSE 133.4035955970746 Test RE 0.5887709179804266\n",
      "55 Train Loss 0.87184435 Test MSE 127.17999365828035 Test RE 0.5748730931402489\n",
      "56 Train Loss 0.8319807 Test MSE 124.73592479037015 Test RE 0.5693225136290131\n",
      "57 Train Loss 0.8021674 Test MSE 119.20983401848095 Test RE 0.5565685014879791\n",
      "58 Train Loss 0.75883615 Test MSE 112.26304556040962 Test RE 0.5401084756421869\n",
      "59 Train Loss 0.726248 Test MSE 107.46348438469813 Test RE 0.5284367848320418\n",
      "60 Train Loss 0.70744807 Test MSE 104.10262217094557 Test RE 0.5201078599260154\n",
      "61 Train Loss 0.69509697 Test MSE 104.45554597896847 Test RE 0.5209887365698154\n",
      "62 Train Loss 0.68333316 Test MSE 104.71096865418987 Test RE 0.5216253283623272\n",
      "63 Train Loss 0.67549235 Test MSE 104.68537870275863 Test RE 0.5215615853627655\n",
      "64 Train Loss 0.67170286 Test MSE 104.60666702531579 Test RE 0.5213654705495185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Train Loss 0.65895426 Test MSE 102.64825195794458 Test RE 0.516461986686085\n",
      "66 Train Loss 0.64080405 Test MSE 99.65606564981961 Test RE 0.5088789087952913\n",
      "67 Train Loss 0.63067526 Test MSE 95.97665045895265 Test RE 0.49939636526590014\n",
      "68 Train Loss 0.61526805 Test MSE 93.05789055750276 Test RE 0.4917441299671682\n",
      "69 Train Loss 0.6089588 Test MSE 92.3053890008784 Test RE 0.4897518790763222\n",
      "70 Train Loss 0.589701 Test MSE 91.81775769931919 Test RE 0.48845653439305603\n",
      "71 Train Loss 0.57367265 Test MSE 86.56319465191802 Test RE 0.47427389457728\n",
      "72 Train Loss 0.5571678 Test MSE 84.03004156930318 Test RE 0.4672828819368125\n",
      "73 Train Loss 0.5517255 Test MSE 81.79849119706736 Test RE 0.46103641448150856\n",
      "74 Train Loss 0.54370743 Test MSE 80.68066931174474 Test RE 0.45787541860592385\n",
      "75 Train Loss 0.53778493 Test MSE 80.94314792709412 Test RE 0.4586196174212803\n",
      "76 Train Loss 0.5333705 Test MSE 80.78424102358618 Test RE 0.4581692171799109\n",
      "77 Train Loss 0.5241557 Test MSE 80.40452934630002 Test RE 0.4570911782407006\n",
      "78 Train Loss 0.51990277 Test MSE 79.43089578320573 Test RE 0.454315247570195\n",
      "79 Train Loss 0.5159314 Test MSE 78.5392494615212 Test RE 0.45175810802876465\n",
      "80 Train Loss 0.4924115 Test MSE 71.03247402369432 Test RE 0.4296264917650559\n",
      "81 Train Loss 0.4668214 Test MSE 66.51846077275908 Test RE 0.4157513595737455\n",
      "82 Train Loss 0.452289 Test MSE 64.70587848807656 Test RE 0.4100477672370603\n",
      "83 Train Loss 0.4469339 Test MSE 64.0802247928356 Test RE 0.408060536122232\n",
      "84 Train Loss 0.43581682 Test MSE 62.16467228189799 Test RE 0.4019151775318182\n",
      "85 Train Loss 0.42658433 Test MSE 60.73086405293897 Test RE 0.3972531161382537\n",
      "86 Train Loss 0.4004665 Test MSE 56.21597544374858 Test RE 0.38220156228277646\n",
      "87 Train Loss 0.37169772 Test MSE 54.87055517167019 Test RE 0.3776002382956431\n",
      "88 Train Loss 0.34175846 Test MSE 51.20814692490163 Test RE 0.3647809186045193\n",
      "89 Train Loss 0.33159277 Test MSE 49.86632579455187 Test RE 0.359969966455396\n",
      "90 Train Loss 0.32700577 Test MSE 50.433831525631625 Test RE 0.3620124979755389\n",
      "91 Train Loss 0.32272565 Test MSE 49.369861919025794 Test RE 0.358173572601019\n",
      "92 Train Loss 0.32151854 Test MSE 48.84656935330184 Test RE 0.35627029728265713\n",
      "93 Train Loss 0.3150261 Test MSE 47.49761110845527 Test RE 0.35131643428885034\n",
      "94 Train Loss 0.30780527 Test MSE 46.93056286252743 Test RE 0.34921304935912434\n",
      "95 Train Loss 0.29864785 Test MSE 44.5998756086465 Test RE 0.34043123965479694\n",
      "96 Train Loss 0.2917841 Test MSE 42.36868011347689 Test RE 0.33180662261766225\n",
      "97 Train Loss 0.28987905 Test MSE 41.60663415943756 Test RE 0.3288091349982764\n",
      "98 Train Loss 0.28889704 Test MSE 40.82882679044264 Test RE 0.3257212050764513\n",
      "99 Train Loss 0.28460276 Test MSE 40.045067659334364 Test RE 0.32257974808365614\n",
      "Training time: 100.22\n",
      "Training time: 100.22\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 4.3006063 Test MSE 386.6247029974392 Test RE 1.002322275606617\n",
      "1 Train Loss 2.464503 Test MSE 383.64755465169424 Test RE 0.9984556978991671\n",
      "2 Train Loss 2.3901007 Test MSE 384.0890355469323 Test RE 0.9990300171812482\n",
      "3 Train Loss 2.3829887 Test MSE 383.92165686660957 Test RE 0.9988123143418105\n",
      "4 Train Loss 2.3812032 Test MSE 383.79854288154064 Test RE 0.9986521545515245\n",
      "5 Train Loss 2.3800678 Test MSE 383.6272516145491 Test RE 0.998429277880216\n",
      "6 Train Loss 2.3788202 Test MSE 383.2848840052188 Test RE 0.9979836550211019\n",
      "7 Train Loss 2.3771632 Test MSE 382.59643958422873 Test RE 0.9970869785502783\n",
      "8 Train Loss 2.3751235 Test MSE 382.466539703207 Test RE 0.9969176977334794\n",
      "9 Train Loss 2.3729775 Test MSE 382.1662684246771 Test RE 0.996526285025252\n",
      "10 Train Loss 2.371719 Test MSE 381.32121775308883 Test RE 0.9954239098089382\n",
      "11 Train Loss 2.3647954 Test MSE 379.6329795064546 Test RE 0.9932179259604053\n",
      "12 Train Loss 2.347142 Test MSE 374.18789415370526 Test RE 0.9860693254223796\n",
      "13 Train Loss 2.3215606 Test MSE 372.5934051403984 Test RE 0.9839661638096994\n",
      "14 Train Loss 2.288336 Test MSE 366.30767495445065 Test RE 0.9756310007993788\n",
      "15 Train Loss 2.2803133 Test MSE 364.6759806431644 Test RE 0.9734556323702644\n",
      "16 Train Loss 2.2245343 Test MSE 351.79411000812615 Test RE 0.9561078071966936\n",
      "17 Train Loss 2.1566122 Test MSE 342.003899534507 Test RE 0.9427099945948091\n",
      "18 Train Loss 2.1128082 Test MSE 338.07745274201085 Test RE 0.9372828839277852\n",
      "19 Train Loss 2.088128 Test MSE 332.7907921699573 Test RE 0.9299256635360998\n",
      "20 Train Loss 2.0597587 Test MSE 327.4493520232201 Test RE 0.9224326150870278\n",
      "21 Train Loss 2.019913 Test MSE 321.5646640337165 Test RE 0.9141063841733267\n",
      "22 Train Loss 1.9858836 Test MSE 315.51784706164295 Test RE 0.905471004099318\n",
      "23 Train Loss 1.9592793 Test MSE 308.3279233566438 Test RE 0.8950947532983199\n",
      "24 Train Loss 1.9091996 Test MSE 300.95621357226105 Test RE 0.8843297584692487\n",
      "25 Train Loss 1.8142388 Test MSE 287.53769463349767 Test RE 0.8643904812288032\n",
      "26 Train Loss 1.7724038 Test MSE 276.4184187879719 Test RE 0.8475124206536537\n",
      "27 Train Loss 1.6960855 Test MSE 257.1949491418464 Test RE 0.8175113676216655\n",
      "28 Train Loss 1.630934 Test MSE 246.47566833067202 Test RE 0.8002940886071871\n",
      "29 Train Loss 1.6052018 Test MSE 241.78162810809482 Test RE 0.7926367996140552\n",
      "30 Train Loss 1.5749147 Test MSE 239.96175383703735 Test RE 0.7896481027634203\n",
      "31 Train Loss 1.531912 Test MSE 239.29374155458927 Test RE 0.7885482144510165\n",
      "32 Train Loss 1.5172836 Test MSE 237.40409165169538 Test RE 0.7854285477430264\n",
      "33 Train Loss 1.467108 Test MSE 219.92875675297054 Test RE 0.7559683130538309\n",
      "34 Train Loss 1.324815 Test MSE 199.00240409933886 Test RE 0.7191040654692417\n",
      "35 Train Loss 1.2669169 Test MSE 194.14652274822328 Test RE 0.7102764097716874\n",
      "36 Train Loss 1.2127559 Test MSE 186.42642747609156 Test RE 0.6960113489204454\n",
      "37 Train Loss 1.1630023 Test MSE 173.0136704960946 Test RE 0.6705061913844268\n",
      "38 Train Loss 1.1091332 Test MSE 162.6092601529665 Test RE 0.6500327224603688\n",
      "39 Train Loss 1.0655501 Test MSE 161.7523733120561 Test RE 0.648317751787859\n",
      "40 Train Loss 0.97995734 Test MSE 151.6855538981792 Test RE 0.6278193449024584\n",
      "41 Train Loss 0.96964175 Test MSE 146.66382272572736 Test RE 0.6173395240478529\n",
      "42 Train Loss 0.952005 Test MSE 143.66120263693884 Test RE 0.6109875086652389\n",
      "43 Train Loss 0.9277389 Test MSE 142.7700871215361 Test RE 0.6090896149564542\n",
      "44 Train Loss 0.86969644 Test MSE 131.41881168730634 Test RE 0.584374626286411\n",
      "45 Train Loss 0.7711475 Test MSE 114.8320415878244 Test RE 0.5462533653928665\n",
      "46 Train Loss 0.72375244 Test MSE 103.73253687960536 Test RE 0.5191825439242753\n",
      "47 Train Loss 0.68544334 Test MSE 95.09014417896532 Test RE 0.4970846307312915\n",
      "48 Train Loss 0.66243196 Test MSE 90.33889907281093 Test RE 0.48450691452817407\n",
      "49 Train Loss 0.64476424 Test MSE 88.43712180727543 Test RE 0.4793799684929353\n",
      "50 Train Loss 0.6311748 Test MSE 86.56489328605775 Test RE 0.474278547905291\n",
      "51 Train Loss 0.575447 Test MSE 78.05519609450239 Test RE 0.45036381783779755\n",
      "52 Train Loss 0.52347976 Test MSE 72.47277961108217 Test RE 0.4339603409945603\n",
      "53 Train Loss 0.48586777 Test MSE 69.64988333425248 Test RE 0.4254247763151976\n",
      "54 Train Loss 0.4714501 Test MSE 65.16805309270948 Test RE 0.41150958541011834\n",
      "55 Train Loss 0.45138144 Test MSE 59.12408219158452 Test RE 0.39196274327999353\n",
      "56 Train Loss 0.43369526 Test MSE 55.943241113946584 Test RE 0.38127330101560175\n",
      "57 Train Loss 0.42821202 Test MSE 56.10658931866405 Test RE 0.38182953364801286\n",
      "58 Train Loss 0.41585988 Test MSE 55.53746232811418 Test RE 0.37988802028864804\n",
      "59 Train Loss 0.36856517 Test MSE 51.725937376736674 Test RE 0.3666205185726329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 Train Loss 0.33785763 Test MSE 47.59659899489183 Test RE 0.35168232607254035\n",
      "61 Train Loss 0.3212449 Test MSE 46.43084982723806 Test RE 0.3473488767988473\n",
      "62 Train Loss 0.3157735 Test MSE 46.002061733366276 Test RE 0.34574127621896744\n",
      "63 Train Loss 0.30567145 Test MSE 45.340333104416715 Test RE 0.3432455659772405\n",
      "64 Train Loss 0.2838052 Test MSE 41.69814741709872 Test RE 0.3291705421153361\n",
      "65 Train Loss 0.27521843 Test MSE 38.064156716906105 Test RE 0.3145000283368022\n",
      "66 Train Loss 0.23019496 Test MSE 30.433407872648036 Test RE 0.28121460732161746\n",
      "67 Train Loss 0.2186069 Test MSE 27.13815829273269 Test RE 0.2655539514913974\n",
      "68 Train Loss 0.20925114 Test MSE 24.61071803818556 Test RE 0.2528859638130151\n",
      "69 Train Loss 0.20477434 Test MSE 24.263503970872843 Test RE 0.25109573843968575\n",
      "70 Train Loss 0.20160356 Test MSE 24.341159580046657 Test RE 0.2514972347731324\n",
      "71 Train Loss 0.19155297 Test MSE 24.549289147896815 Test RE 0.25257016218068074\n",
      "72 Train Loss 0.18847822 Test MSE 23.603713830900244 Test RE 0.24765822323524633\n",
      "73 Train Loss 0.18525621 Test MSE 22.855161081584612 Test RE 0.243699549181351\n",
      "74 Train Loss 0.17739478 Test MSE 22.116495646408293 Test RE 0.23972909155277144\n",
      "75 Train Loss 0.17455128 Test MSE 21.584983672015063 Test RE 0.23683094328908277\n",
      "76 Train Loss 0.16988643 Test MSE 20.160055362181883 Test RE 0.22888031512127086\n",
      "77 Train Loss 0.16550288 Test MSE 17.804081654967376 Test RE 0.21509106564773003\n",
      "78 Train Loss 0.14847931 Test MSE 14.876175715719024 Test RE 0.1966111902331434\n",
      "79 Train Loss 0.13525565 Test MSE 16.295175603369227 Test RE 0.2057747605571362\n",
      "80 Train Loss 0.12493413 Test MSE 16.049991676799007 Test RE 0.2042208071278248\n",
      "81 Train Loss 0.11521244 Test MSE 13.846934475113464 Test RE 0.18968780063844393\n",
      "82 Train Loss 0.106909715 Test MSE 13.13121250525306 Test RE 0.18472045827868863\n",
      "83 Train Loss 0.10525739 Test MSE 13.215973826085508 Test RE 0.18531567995800674\n",
      "84 Train Loss 0.104338035 Test MSE 12.638177799683696 Test RE 0.18121945208736004\n",
      "85 Train Loss 0.101676024 Test MSE 12.24407042029302 Test RE 0.17837151125289633\n",
      "86 Train Loss 0.10068612 Test MSE 12.557402628931188 Test RE 0.18063940420767488\n",
      "87 Train Loss 0.10006623 Test MSE 12.287044705312 Test RE 0.17868426157515246\n",
      "88 Train Loss 0.09826701 Test MSE 11.609300793086698 Test RE 0.17368632032603198\n",
      "89 Train Loss 0.09610075 Test MSE 11.230190908945497 Test RE 0.17082685763925853\n",
      "90 Train Loss 0.09385979 Test MSE 10.623286866351243 Test RE 0.16614682140124942\n",
      "91 Train Loss 0.09190126 Test MSE 9.860849874375111 Test RE 0.16007361615618287\n",
      "92 Train Loss 0.09080346 Test MSE 9.04876927219415 Test RE 0.15334066411054165\n",
      "93 Train Loss 0.089738235 Test MSE 8.209437620451833 Test RE 0.14605596084940164\n",
      "94 Train Loss 0.08697214 Test MSE 7.4232853910057175 Test RE 0.13888670071636555\n",
      "95 Train Loss 0.085437916 Test MSE 7.328188100612408 Test RE 0.13799421703906856\n",
      "96 Train Loss 0.0792083 Test MSE 6.638078261852695 Test RE 0.13133599441525545\n",
      "97 Train Loss 0.074392185 Test MSE 5.014368847428834 Test RE 0.11414863456430098\n",
      "98 Train Loss 0.07430902 Test MSE 4.97087752328552 Test RE 0.11365253156509071\n",
      "99 Train Loss 0.07345398 Test MSE 4.726395091268883 Test RE 0.11082241089510075\n",
      "Training time: 128.08\n",
      "Training time: 128.08\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 4.4094076 Test MSE 385.87969111390186 Test RE 1.0013560904612717\n",
      "1 Train Loss 2.4059353 Test MSE 383.495043449719 Test RE 0.9982572203990704\n",
      "2 Train Loss 2.3855805 Test MSE 383.5995487912249 Test RE 0.9983932275095396\n",
      "3 Train Loss 2.381185 Test MSE 383.743891851072 Test RE 0.9985810504353624\n",
      "4 Train Loss 2.3802927 Test MSE 383.59256625627575 Test RE 0.9983841407583294\n",
      "5 Train Loss 2.377793 Test MSE 382.8360964467228 Test RE 0.9973992152874934\n",
      "6 Train Loss 2.3758836 Test MSE 382.7594572120413 Test RE 0.9972993765585775\n",
      "7 Train Loss 2.3659086 Test MSE 379.9698151593763 Test RE 0.9936584527988488\n",
      "8 Train Loss 2.354735 Test MSE 378.25366636479487 Test RE 0.9914119643626524\n",
      "9 Train Loss 2.3305554 Test MSE 374.0587989952895 Test RE 0.9858992133489626\n",
      "10 Train Loss 2.3020675 Test MSE 368.0652048560023 Test RE 0.9779687197807226\n",
      "11 Train Loss 2.2696369 Test MSE 364.2227545038487 Test RE 0.9728505298570068\n",
      "12 Train Loss 2.2030911 Test MSE 351.7436342804075 Test RE 0.9560392131423842\n",
      "13 Train Loss 2.1541374 Test MSE 340.9893515211177 Test RE 0.9413106905059965\n",
      "14 Train Loss 2.0681024 Test MSE 328.59837430469844 Test RE 0.9240496093697285\n",
      "15 Train Loss 1.9866052 Test MSE 312.5984154756201 Test RE 0.9012721861895095\n",
      "16 Train Loss 1.9398085 Test MSE 302.75561169695726 Test RE 0.8869694944320738\n",
      "17 Train Loss 1.8777611 Test MSE 288.88679680401737 Test RE 0.8664159311533747\n",
      "18 Train Loss 1.7998416 Test MSE 284.42658068659415 Test RE 0.8597014766507728\n",
      "19 Train Loss 1.7447007 Test MSE 271.1871626395589 Test RE 0.8394544702163835\n",
      "20 Train Loss 1.6902491 Test MSE 263.59465308447267 Test RE 0.8276198163804079\n",
      "21 Train Loss 1.660809 Test MSE 255.52228456354737 Test RE 0.8148486932095539\n",
      "22 Train Loss 1.5731901 Test MSE 239.14252291208552 Test RE 0.7882990185584011\n",
      "23 Train Loss 1.5287588 Test MSE 233.8342785210363 Test RE 0.779500989088492\n",
      "24 Train Loss 1.4850086 Test MSE 227.66843551594388 Test RE 0.7691552241133566\n",
      "25 Train Loss 1.3543642 Test MSE 210.23503743151275 Test RE 0.7391203007209097\n",
      "26 Train Loss 1.3425963 Test MSE 208.86912615482598 Test RE 0.7367153307161417\n",
      "27 Train Loss 1.31581 Test MSE 202.4143378699253 Test RE 0.7252424537824755\n",
      "28 Train Loss 1.2011591 Test MSE 183.34460136724135 Test RE 0.6902344729279027\n",
      "29 Train Loss 1.1668227 Test MSE 178.72175556948068 Test RE 0.6814771421073437\n",
      "30 Train Loss 1.138643 Test MSE 177.47779778650627 Test RE 0.6791013567398473\n",
      "31 Train Loss 1.1222701 Test MSE 174.96915113543025 Test RE 0.6742847304516643\n",
      "32 Train Loss 1.0800941 Test MSE 163.71855429983353 Test RE 0.6522461631397919\n",
      "33 Train Loss 0.9801296 Test MSE 152.54723967421398 Test RE 0.6296000579120211\n",
      "34 Train Loss 0.95128286 Test MSE 142.58245687132015 Test RE 0.6086892467417041\n",
      "35 Train Loss 0.84386283 Test MSE 120.5033024074724 Test RE 0.5595798364587246\n",
      "36 Train Loss 0.77760625 Test MSE 109.4519963179158 Test RE 0.533303490424733\n",
      "37 Train Loss 0.7316152 Test MSE 100.1431661932124 Test RE 0.5101210461266461\n",
      "38 Train Loss 0.6796902 Test MSE 89.03193930744256 Test RE 0.48098939298514615\n",
      "39 Train Loss 0.609652 Test MSE 73.30630886762884 Test RE 0.4364487547439829\n",
      "40 Train Loss 0.43466437 Test MSE 52.360807437321206 Test RE 0.36886355712419366\n",
      "41 Train Loss 0.40657008 Test MSE 50.112720568151644 Test RE 0.3608581953787628\n",
      "42 Train Loss 0.3877743 Test MSE 49.098339135373735 Test RE 0.35718727886936635\n",
      "43 Train Loss 0.37457347 Test MSE 48.26144605834026 Test RE 0.3541300231382841\n",
      "44 Train Loss 0.35855484 Test MSE 45.87900119723872 Test RE 0.3452785187047324\n",
      "45 Train Loss 0.3417612 Test MSE 44.5697713195983 Test RE 0.340316327109953\n",
      "46 Train Loss 0.33368468 Test MSE 43.05677619835248 Test RE 0.3344901531453612\n",
      "47 Train Loss 0.32139456 Test MSE 40.833940113091465 Test RE 0.32574160078216663\n",
      "48 Train Loss 0.26474583 Test MSE 30.168386582820155 Test RE 0.27998748842429927\n",
      "49 Train Loss 0.23043089 Test MSE 25.553338594436095 Test RE 0.2576833793487061\n",
      "50 Train Loss 0.17936471 Test MSE 20.83696202856174 Test RE 0.23269110537513177\n",
      "51 Train Loss 0.1633572 Test MSE 17.721439148295737 Test RE 0.21459128304627328\n",
      "52 Train Loss 0.15244392 Test MSE 15.34236715107332 Test RE 0.19966813813567807\n",
      "53 Train Loss 0.14507684 Test MSE 15.268693193952947 Test RE 0.19918815861313127\n",
      "54 Train Loss 0.14267145 Test MSE 15.204491808001592 Test RE 0.198768947000047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 Train Loss 0.1367847 Test MSE 14.513783388913101 Test RE 0.19420164356819647\n",
      "56 Train Loss 0.128524 Test MSE 14.581600802046971 Test RE 0.19465483015913224\n",
      "57 Train Loss 0.12759364 Test MSE 14.609768386583127 Test RE 0.19484274886322991\n",
      "58 Train Loss 0.11660773 Test MSE 12.62634896936488 Test RE 0.18113462514464734\n",
      "59 Train Loss 0.079747185 Test MSE 8.208916002097435 Test RE 0.1460513206606522\n",
      "60 Train Loss 0.06078467 Test MSE 5.674994530665972 Test RE 0.12143539957861121\n",
      "61 Train Loss 0.051804848 Test MSE 4.14803971940233 Test RE 0.10382071982883749\n",
      "62 Train Loss 0.047657423 Test MSE 3.7750840207887255 Test RE 0.09904348049906067\n",
      "63 Train Loss 0.043558314 Test MSE 3.788713266683165 Test RE 0.09922210854501541\n",
      "64 Train Loss 0.03870446 Test MSE 3.250079693472793 Test RE 0.09189874649817725\n",
      "65 Train Loss 0.035934154 Test MSE 2.833644631091156 Test RE 0.08580948088058661\n",
      "66 Train Loss 0.03540463 Test MSE 2.80460311768896 Test RE 0.08536862545314422\n",
      "67 Train Loss 0.03540025 Test MSE 2.8056104274321227 Test RE 0.08538395470377479\n",
      "68 Train Loss 0.035395935 Test MSE 2.806621521423029 Test RE 0.0853993387759906\n",
      "69 Train Loss 0.0353888 Test MSE 2.8077095377027654 Test RE 0.08541589014690927\n",
      "70 Train Loss 0.03538706 Test MSE 2.809117548401636 Test RE 0.08543730465071213\n",
      "71 Train Loss 0.03537845 Test MSE 2.810511022152754 Test RE 0.08545849277805576\n",
      "72 Train Loss 0.035372674 Test MSE 2.812452253259536 Test RE 0.08548800094348756\n",
      "73 Train Loss 0.035366282 Test MSE 2.813732505348597 Test RE 0.08550745616077991\n",
      "74 Train Loss 0.035347126 Test MSE 2.8172168488535245 Test RE 0.08556038321931031\n",
      "75 Train Loss 0.03438258 Test MSE 2.6648193416826613 Test RE 0.08321401339805602\n",
      "76 Train Loss 0.031676687 Test MSE 2.163178526358014 Test RE 0.07497366892398766\n",
      "77 Train Loss 0.027991388 Test MSE 1.6662661394099043 Test RE 0.06580134442633895\n",
      "78 Train Loss 0.024235584 Test MSE 1.3530042685876034 Test RE 0.05929419344501712\n",
      "79 Train Loss 0.021888964 Test MSE 0.9979916279884928 Test RE 0.05092441299130359\n",
      "80 Train Loss 0.01927209 Test MSE 0.6002584998549787 Test RE 0.03949405637791653\n",
      "81 Train Loss 0.018279731 Test MSE 0.4061717941804981 Test RE 0.03248758757502148\n",
      "82 Train Loss 0.017941518 Test MSE 0.3253341087784273 Test RE 0.029075512701833003\n",
      "83 Train Loss 0.01791923 Test MSE 0.317150465248678 Test RE 0.028707492427878054\n",
      "84 Train Loss 0.01735605 Test MSE 0.22385240919682672 Test RE 0.024118120958951435\n",
      "85 Train Loss 0.016798444 Test MSE 0.23490146969551617 Test RE 0.024706171371984723\n",
      "86 Train Loss 0.016751233 Test MSE 0.2519663874486266 Test RE 0.02558785536491713\n",
      "87 Train Loss 0.016733807 Test MSE 0.25788778404198126 Test RE 0.025886776118506108\n",
      "88 Train Loss 0.016727272 Test MSE 0.26227226976129453 Test RE 0.0261059059996708\n",
      "89 Train Loss 0.016718695 Test MSE 0.26457860223615615 Test RE 0.026220437958280433\n",
      "90 Train Loss 0.016697181 Test MSE 0.2691075337982014 Test RE 0.026443900272322327\n",
      "91 Train Loss 0.016112434 Test MSE 0.3097020778603822 Test RE 0.028368386946631022\n",
      "92 Train Loss 0.015209358 Test MSE 0.3119018729904636 Test RE 0.028468958143316594\n",
      "93 Train Loss 0.01520469 Test MSE 0.31038931921216656 Test RE 0.02839984479985087\n",
      "94 Train Loss 0.0151951015 Test MSE 0.3089601830856537 Test RE 0.028334388179970665\n",
      "95 Train Loss 0.015194934 Test MSE 0.3079201641230505 Test RE 0.028286658499214946\n",
      "96 Train Loss 0.015190807 Test MSE 0.30663445170928527 Test RE 0.028227541631244628\n",
      "97 Train Loss 0.015190807 Test MSE 0.30663445170928527 Test RE 0.028227541631244628\n",
      "98 Train Loss 0.015190807 Test MSE 0.30663445170928527 Test RE 0.028227541631244628\n",
      "99 Train Loss 0.015190807 Test MSE 0.30663445170928527 Test RE 0.028227541631244628\n",
      "Training time: 109.43\n",
      "Training time: 109.43\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 4.3728566 Test MSE 386.9996878477933 Test RE 1.0028082308168924\n",
      "1 Train Loss 4.2728696 Test MSE 385.09122010819937 Test RE 1.0003325279456374\n",
      "2 Train Loss 2.9364073 Test MSE 384.3596265068796 Test RE 0.9993818638515095\n",
      "3 Train Loss 2.3966637 Test MSE 383.7751722198646 Test RE 0.9986217486036932\n",
      "4 Train Loss 2.3822525 Test MSE 383.92263012248077 Test RE 0.9988135803544406\n",
      "5 Train Loss 2.3818417 Test MSE 383.8889176464977 Test RE 0.998769726182493\n",
      "6 Train Loss 2.3818378 Test MSE 383.88783864843725 Test RE 0.9987683225585472\n",
      "7 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "8 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "9 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "10 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "11 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "12 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "13 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "14 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "15 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "16 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "17 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "18 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "19 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "20 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "21 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "22 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "23 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "24 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "25 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "26 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "27 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "28 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "29 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "30 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "31 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "32 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "33 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "34 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "35 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "36 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "37 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "38 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "39 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "40 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "41 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "42 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "43 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "44 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "45 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "46 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "47 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "48 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "49 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "51 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "52 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "53 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "54 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "55 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "56 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "57 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "58 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "59 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "60 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "61 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "62 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "63 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "64 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "65 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "66 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "67 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "68 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "69 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "70 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "71 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "72 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "73 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "74 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "75 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "76 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "77 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "78 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "79 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "80 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "81 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "82 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "83 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "84 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "85 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "86 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "87 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "88 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "89 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "90 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "91 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "92 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "93 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "94 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "95 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "96 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "97 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "98 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "99 Train Loss 2.3818362 Test MSE 383.887080988261 Test RE 0.9987673369485844\n",
      "Training time: 18.24\n",
      "Training time: 18.24\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 8.0\n",
    "rowdy_terms = 2\n",
    "\n",
    "for reps in range(max_reps):\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    alpha_val = []\n",
    "    omega_val = []\n",
    "\n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "\n",
    "\n",
    "\n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)  \n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"alpha\": alpha_full, \"omega\": omega_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lrnr_tune' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9e773a12e949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlrnr_tune\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lrnr_tune' is not defined"
     ]
    }
   ],
   "source": [
    "lrnr_tune[tune_reps,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tune_reps in range(75):\n",
    "    label = \"1D_SODE_rowdy_tune\"+str(tune_reps)+\".mat\" #WRONGLY SAVED AS STAN - DOESN'T MATTER\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
