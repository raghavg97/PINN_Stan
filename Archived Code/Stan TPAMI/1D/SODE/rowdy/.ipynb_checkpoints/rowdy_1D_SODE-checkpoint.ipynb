{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 0.005\n",
    "label = \"1D_SODE_rowdy\"\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.3443027 Test MSE 386.51022543182546 Test RE 1.002173873411708\n",
      "1 Train Loss 2.606071 Test MSE 382.9875848378914 Test RE 0.997596531397191\n",
      "2 Train Loss 2.397064 Test MSE 383.704570772472 Test RE 0.9985298883312785\n",
      "3 Train Loss 2.3842182 Test MSE 383.97660525184676 Test RE 0.9988837887679266\n",
      "4 Train Loss 2.381233 Test MSE 383.7029607920882 Test RE 0.9985277934706204\n",
      "5 Train Loss 2.3805058 Test MSE 383.67669506736087 Test RE 0.9984936166363461\n",
      "6 Train Loss 2.3804994 Test MSE 383.6767376576848 Test RE 0.998493672055617\n",
      "7 Train Loss 2.3804932 Test MSE 383.6770232834544 Test RE 0.9984940437167609\n",
      "8 Train Loss 2.3804886 Test MSE 383.6767435190533 Test RE 0.9984936796825312\n",
      "9 Train Loss 2.380485 Test MSE 383.6764170623608 Test RE 0.9984932548913252\n",
      "10 Train Loss 2.3804796 Test MSE 383.67554693859995 Test RE 0.9984921226700574\n",
      "11 Train Loss 2.380473 Test MSE 383.6742814172371 Test RE 0.9984904759477631\n",
      "12 Train Loss 2.3804655 Test MSE 383.6720950152532 Test RE 0.9984876309503607\n",
      "13 Train Loss 2.379966 Test MSE 383.41588237928454 Test RE 0.9981541849232602\n",
      "14 Train Loss 2.378471 Test MSE 383.1418639153674 Test RE 0.9977974423170671\n",
      "15 Train Loss 2.3766835 Test MSE 382.9472855986551 Test RE 0.9975440447886367\n",
      "16 Train Loss 2.3731992 Test MSE 381.8880795514367 Test RE 0.9961635201568487\n",
      "17 Train Loss 2.36951 Test MSE 381.2880669445821 Test RE 0.9953806394333075\n",
      "18 Train Loss 2.3679817 Test MSE 381.0881088358937 Test RE 0.9951196025048126\n",
      "19 Train Loss 2.3606052 Test MSE 379.66593498617004 Test RE 0.9932610350482006\n",
      "20 Train Loss 2.3557737 Test MSE 378.7329864850983 Test RE 0.9920399202768526\n",
      "21 Train Loss 2.3510678 Test MSE 377.53822758601456 Test RE 0.9904739294667048\n",
      "22 Train Loss 2.3417504 Test MSE 375.0491069579438 Test RE 0.9872034180471047\n",
      "23 Train Loss 2.3361998 Test MSE 372.29696374997053 Test RE 0.9835746561571322\n",
      "24 Train Loss 2.288048 Test MSE 363.74484371226123 Test RE 0.9722120629074942\n",
      "25 Train Loss 2.2497284 Test MSE 360.3188450858116 Test RE 0.9676227507325288\n",
      "26 Train Loss 2.1740386 Test MSE 344.9374698076033 Test RE 0.9467444529332394\n",
      "27 Train Loss 2.123357 Test MSE 337.8913521785669 Test RE 0.9370248766540902\n",
      "28 Train Loss 2.0634673 Test MSE 327.8509963318374 Test RE 0.9229981625037496\n",
      "29 Train Loss 2.0089025 Test MSE 319.37966812285146 Test RE 0.9109954652806141\n",
      "30 Train Loss 1.9645958 Test MSE 311.64162834149397 Test RE 0.8998918425233808\n",
      "31 Train Loss 1.9200627 Test MSE 299.72496201339334 Test RE 0.8825189496744684\n",
      "32 Train Loss 1.8509753 Test MSE 288.2669337605313 Test RE 0.8654858996967505\n",
      "33 Train Loss 1.7916859 Test MSE 284.7197485734428 Test RE 0.8601444239174939\n",
      "34 Train Loss 1.761525 Test MSE 279.94470767735515 Test RE 0.8529011762241907\n",
      "35 Train Loss 1.7261763 Test MSE 272.79468875121324 Test RE 0.8419388265625545\n",
      "36 Train Loss 1.6750481 Test MSE 265.97577224718634 Test RE 0.8313494652839333\n",
      "37 Train Loss 1.6533415 Test MSE 261.80412589556227 Test RE 0.8248041278892138\n",
      "38 Train Loss 1.6368053 Test MSE 257.79937028741716 Test RE 0.8184714004628318\n",
      "39 Train Loss 1.6322846 Test MSE 256.5817983025747 Test RE 0.8165363156461389\n",
      "40 Train Loss 1.618969 Test MSE 254.49114087453287 Test RE 0.813202896497218\n",
      "41 Train Loss 1.5809202 Test MSE 242.00580503101438 Test RE 0.7930041759755472\n",
      "42 Train Loss 1.5289985 Test MSE 233.7626128375628 Test RE 0.7793815289468289\n",
      "43 Train Loss 1.4811063 Test MSE 230.92115358265056 Test RE 0.7746302306521684\n",
      "44 Train Loss 1.4602888 Test MSE 229.25088730490708 Test RE 0.7718236734474826\n",
      "45 Train Loss 1.4476397 Test MSE 225.91430615802815 Test RE 0.7661864185656665\n",
      "46 Train Loss 1.4375659 Test MSE 223.39105191659192 Test RE 0.7618956063344267\n",
      "47 Train Loss 1.4138929 Test MSE 222.31573520269754 Test RE 0.7600596613074386\n",
      "48 Train Loss 1.390628 Test MSE 218.22969713462658 Test RE 0.7530425347074409\n",
      "49 Train Loss 1.3589581 Test MSE 213.31157921596696 Test RE 0.7445087358463658\n",
      "50 Train Loss 1.3519932 Test MSE 210.15478417048297 Test RE 0.7389792146369716\n",
      "51 Train Loss 1.3439827 Test MSE 208.96873246195196 Test RE 0.736890973589553\n",
      "52 Train Loss 1.3200523 Test MSE 206.69780582490006 Test RE 0.732876026954522\n",
      "53 Train Loss 1.2929789 Test MSE 200.35130063370653 Test RE 0.7215370983832021\n",
      "54 Train Loss 1.2442615 Test MSE 195.8920940764341 Test RE 0.7134623121785867\n",
      "55 Train Loss 1.2141656 Test MSE 189.74192530209297 Test RE 0.7021731747994577\n",
      "56 Train Loss 1.1973649 Test MSE 187.12564557712528 Test RE 0.697315370751244\n",
      "57 Train Loss 1.15213 Test MSE 181.38162979060857 Test RE 0.6865295458117829\n",
      "58 Train Loss 1.1435499 Test MSE 179.3365836909822 Test RE 0.6826483245997667\n",
      "59 Train Loss 1.1422224 Test MSE 178.72198419040078 Test RE 0.6814775779800806\n",
      "60 Train Loss 1.1403081 Test MSE 177.88943278935835 Test RE 0.6798884410695367\n",
      "61 Train Loss 1.1272941 Test MSE 176.27208488812778 Test RE 0.6767906545815107\n",
      "62 Train Loss 1.115646 Test MSE 175.13403734808333 Test RE 0.6746023695147453\n",
      "63 Train Loss 1.1014078 Test MSE 172.76169225431644 Test RE 0.6700177486746342\n",
      "64 Train Loss 1.0886837 Test MSE 168.20063728177337 Test RE 0.6611140714722579\n",
      "65 Train Loss 1.0653864 Test MSE 166.47470114264152 Test RE 0.6577134217906055\n",
      "66 Train Loss 1.0382229 Test MSE 160.5277370309799 Test RE 0.6458588638391396\n",
      "67 Train Loss 1.0213228 Test MSE 159.08118762487422 Test RE 0.6429422930680331\n",
      "68 Train Loss 1.0155168 Test MSE 157.51507674748106 Test RE 0.6397696696469302\n",
      "69 Train Loss 1.0096498 Test MSE 155.16792192331877 Test RE 0.634985129013406\n",
      "70 Train Loss 1.0029478 Test MSE 152.6028455871649 Test RE 0.6297147971111392\n",
      "71 Train Loss 0.9966534 Test MSE 150.8776040862552 Test RE 0.6261450793811149\n",
      "72 Train Loss 0.9851419 Test MSE 147.85164610039183 Test RE 0.6198343846219728\n",
      "73 Train Loss 0.9714578 Test MSE 146.27028158685164 Test RE 0.6165107181125781\n",
      "74 Train Loss 0.9593853 Test MSE 145.06985866771294 Test RE 0.6139756909186737\n",
      "75 Train Loss 0.95250183 Test MSE 143.12834414823251 Test RE 0.6098533390662898\n",
      "76 Train Loss 0.94793457 Test MSE 142.96286672010876 Test RE 0.6095006970083151\n",
      "77 Train Loss 0.9451012 Test MSE 141.6556785147114 Test RE 0.6067077978627555\n",
      "78 Train Loss 0.94004387 Test MSE 139.14628428809795 Test RE 0.6013099486919211\n",
      "79 Train Loss 0.92225075 Test MSE 135.00572040638042 Test RE 0.5922958201807967\n",
      "80 Train Loss 0.9051958 Test MSE 128.5590571216812 Test RE 0.5779814787618257\n",
      "81 Train Loss 0.8985432 Test MSE 125.87846990211962 Test RE 0.5719239850755168\n",
      "82 Train Loss 0.8887345 Test MSE 122.64751910321553 Test RE 0.5645364220393345\n",
      "83 Train Loss 0.86944973 Test MSE 119.56593260533118 Test RE 0.5573991605964024\n",
      "84 Train Loss 0.85885555 Test MSE 119.2716044656766 Test RE 0.5567126801671163\n",
      "85 Train Loss 0.8291572 Test MSE 114.60684519793409 Test RE 0.5457174755227814\n",
      "86 Train Loss 0.764188 Test MSE 110.87070083985765 Test RE 0.5367486724199183\n",
      "87 Train Loss 0.7460937 Test MSE 108.47861153830512 Test RE 0.5309267918611936\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 5.0\n",
    "rowdy_terms = 2\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    alpha_val = []\n",
    "    omega_val = []\n",
    "\n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "\n",
    "\n",
    "\n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)  \n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"alpha\": alpha_full, \"omega\": omega_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_tune[tune_reps,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tune_reps in range(75):\n",
    "    label = \"1D_SODE_rowdy_tune\"+str(tune_reps)+\".mat\" #WRONGLY SAVED AS STAN - DOESN'T MATTER\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
