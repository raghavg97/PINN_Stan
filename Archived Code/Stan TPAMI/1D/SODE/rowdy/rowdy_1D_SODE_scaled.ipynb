{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y/10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 0.005\n",
    "label = \"1D_SODE_rowdy_scaled\"\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0/10.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.omega = Parameter(0.1*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 0.054414906 Test MSE 3.926339730206851 Test RE 1.0100817393674\n",
      "1 Train Loss 0.04388325 Test MSE 3.878048952493898 Test RE 1.0038509306671681\n",
      "2 Train Loss 0.043418463 Test MSE 3.8663409398576674 Test RE 1.0023344485737864\n",
      "3 Train Loss 0.042138517 Test MSE 3.84793099100453 Test RE 0.9999452458350868\n",
      "4 Train Loss 0.032773968 Test MSE 3.8356318435638905 Test RE 0.9983459035931654\n",
      "5 Train Loss 0.027022138 Test MSE 3.8236423517019555 Test RE 0.9967843579376826\n",
      "6 Train Loss 0.024288991 Test MSE 3.836714253571377 Test RE 0.9984867595739358\n",
      "7 Train Loss 0.023954486 Test MSE 3.8330744148874003 Test RE 0.9980130217788021\n",
      "8 Train Loss 0.023886196 Test MSE 3.8323839903806913 Test RE 0.9979231352275642\n",
      "9 Train Loss 0.023848213 Test MSE 3.833985098638541 Test RE 0.9981315715598027\n",
      "10 Train Loss 0.023843715 Test MSE 3.8348051770969183 Test RE 0.9982383145977789\n",
      "11 Train Loss 0.023828555 Test MSE 3.835886345133286 Test RE 0.9983790241317212\n",
      "12 Train Loss 0.023806496 Test MSE 3.833765045275897 Test RE 0.9981029270352755\n",
      "13 Train Loss 0.023800641 Test MSE 3.8319661976339154 Test RE 0.997868738745854\n",
      "14 Train Loss 0.023775216 Test MSE 3.8273551017400163 Test RE 0.9972681784533258\n",
      "15 Train Loss 0.023769643 Test MSE 3.8288402588599393 Test RE 0.9974616483879996\n",
      "16 Train Loss 0.023769299 Test MSE 3.829121552633008 Test RE 0.9974982880201648\n",
      "17 Train Loss 0.023768878 Test MSE 3.829360986026414 Test RE 0.9975294741111909\n",
      "18 Train Loss 0.023763642 Test MSE 3.8288095129566124 Test RE 0.9974576435302739\n",
      "19 Train Loss 0.02372822 Test MSE 3.8214114832884643 Test RE 0.996493533250414\n",
      "20 Train Loss 0.023715375 Test MSE 3.815341803946473 Test RE 0.9957018362730177\n",
      "21 Train Loss 0.02370781 Test MSE 3.8113308199356175 Test RE 0.9951783190449629\n",
      "22 Train Loss 0.023669101 Test MSE 3.8005199204547098 Test RE 0.9937658973112101\n",
      "23 Train Loss 0.023598539 Test MSE 3.7921256691703906 Test RE 0.992667819659724\n",
      "24 Train Loss 0.02355176 Test MSE 3.783477550966493 Test RE 0.9915352611112498\n",
      "25 Train Loss 0.023275107 Test MSE 3.721521691852634 Test RE 0.9833833705330612\n",
      "26 Train Loss 0.023189029 Test MSE 3.6655789463812534 Test RE 0.975964163905456\n",
      "27 Train Loss 0.023014475 Test MSE 3.6675487076786397 Test RE 0.9762263541725837\n",
      "28 Train Loss 0.022799864 Test MSE 3.649331583411641 Test RE 0.9737988232701028\n",
      "29 Train Loss 0.02237365 Test MSE 3.5670381160393476 Test RE 0.962756495886435\n",
      "30 Train Loss 0.02204038 Test MSE 3.489291306491676 Test RE 0.952206620552975\n",
      "31 Train Loss 0.021145904 Test MSE 3.384634497701226 Test RE 0.9378177986292406\n",
      "32 Train Loss 0.020730639 Test MSE 3.2498805879124775 Test RE 0.9189593151617941\n",
      "33 Train Loss 0.020203823 Test MSE 3.1451345407018247 Test RE 0.9040286551816443\n",
      "34 Train Loss 0.019643525 Test MSE 3.036929017362124 Test RE 0.8883414016726126\n",
      "35 Train Loss 0.018902414 Test MSE 2.9228308645597703 Test RE 0.8714940470705442\n",
      "36 Train Loss 0.01764761 Test MSE 2.645362445092902 Test RE 0.8290966768425708\n",
      "37 Train Loss 0.01733484 Test MSE 2.5528909191746796 Test RE 0.8144767880821463\n",
      "38 Train Loss 0.015680047 Test MSE 2.3109639787354546 Test RE 0.774924105375901\n",
      "39 Train Loss 0.014834401 Test MSE 2.1534037372871357 Test RE 0.7480408477877871\n",
      "40 Train Loss 0.014261797 Test MSE 1.9839907356009838 Test RE 0.7180131570841478\n",
      "41 Train Loss 0.013409989 Test MSE 1.6911003922577428 Test RE 0.6628988684165072\n",
      "42 Train Loss 0.010231325 Test MSE 1.320745828732136 Test RE 0.5858308073693366\n",
      "43 Train Loss 0.00812971 Test MSE 1.113437009708288 Test RE 0.5378924010444287\n",
      "44 Train Loss 0.0072007356 Test MSE 1.0064528394097092 Test RE 0.5113983203053519\n",
      "45 Train Loss 0.0065747914 Test MSE 0.8834056633521676 Test RE 0.47911820398128896\n",
      "46 Train Loss 0.0065416396 Test MSE 0.8627478207781836 Test RE 0.47348313886175114\n",
      "47 Train Loss 0.006401382 Test MSE 0.7954262290870906 Test RE 0.45463465413903875\n",
      "48 Train Loss 0.0057388586 Test MSE 0.7122087115016505 Test RE 0.43019585669126337\n",
      "49 Train Loss 0.005582041 Test MSE 0.6652745708164879 Test RE 0.415779472830991\n",
      "50 Train Loss 0.0053617572 Test MSE 0.5637639232011223 Test RE 0.38274649549059175\n",
      "51 Train Loss 0.005041698 Test MSE 0.43895363484609395 Test RE 0.3377317632039202\n",
      "52 Train Loss 0.004879647 Test MSE 0.3904538987956459 Test RE 0.31852788974459795\n",
      "53 Train Loss 0.004775006 Test MSE 0.36843267027479354 Test RE 0.30941520342955936\n",
      "54 Train Loss 0.0043536327 Test MSE 0.36454911843688775 Test RE 0.3077801512273547\n",
      "55 Train Loss 0.00395729 Test MSE 0.34026907815874163 Test RE 0.29735402724639687\n",
      "56 Train Loss 0.003811133 Test MSE 0.3125674505007818 Test RE 0.28499317371805827\n",
      "57 Train Loss 0.0037246628 Test MSE 0.2852274448347355 Test RE 0.27224395055816014\n",
      "58 Train Loss 0.0034263222 Test MSE 0.2309327902413344 Test RE 0.24496575929463682\n",
      "59 Train Loss 0.0020059065 Test MSE 0.20414562835316302 Test RE 0.2303205158914417\n",
      "60 Train Loss 0.0018352007 Test MSE 0.2193100123384028 Test RE 0.23872165254383204\n",
      "61 Train Loss 0.0017213342 Test MSE 0.22057044338537374 Test RE 0.2394066670831973\n",
      "62 Train Loss 0.0015279839 Test MSE 0.15317595551948165 Test RE 0.19950688214700169\n",
      "63 Train Loss 0.001058057 Test MSE 0.04068449909973006 Test RE 0.10281987445124624\n",
      "64 Train Loss 0.00095575664 Test MSE 0.04895873012039122 Test RE 0.11279183309053493\n",
      "65 Train Loss 0.00085438666 Test MSE 0.04947511948037227 Test RE 0.11338510546895922\n",
      "66 Train Loss 0.000629922 Test MSE 0.01598998571324872 Test RE 0.0644594536823069\n",
      "67 Train Loss 0.0003952766 Test MSE 0.000804960787414693 Test RE 0.014462718889182104\n",
      "68 Train Loss 0.00034956503 Test MSE 0.0014790135245853408 Test RE 0.019604178816215845\n",
      "69 Train Loss 0.00030119516 Test MSE 0.006038955703805998 Test RE 0.03961352635307939\n",
      "70 Train Loss 0.000244434 Test MSE 0.009840290775198593 Test RE 0.05056692542510409\n",
      "71 Train Loss 0.00016917387 Test MSE 0.004811852510793983 Test RE 0.0353605275849269\n",
      "72 Train Loss 0.00014844995 Test MSE 0.0018673865935936017 Test RE 0.02202823499376102\n",
      "73 Train Loss 0.00014529398 Test MSE 0.0016564254529811014 Test RE 0.020746676265687786\n",
      "74 Train Loss 0.00014325859 Test MSE 0.001319407170027756 Test RE 0.018516205940775642\n",
      "75 Train Loss 0.00014073357 Test MSE 0.0005943302975318666 Test RE 0.012427292419348708\n",
      "76 Train Loss 0.00013896053 Test MSE 0.00035628062819363616 Test RE 0.009621852327342416\n",
      "77 Train Loss 0.0001384695 Test MSE 0.0004202926493557595 Test RE 0.010450533934208781\n",
      "78 Train Loss 0.00013814107 Test MSE 0.0004946909601970871 Test RE 0.011337820309541754\n",
      "79 Train Loss 0.00013808852 Test MSE 0.0005036984031397344 Test RE 0.011440575448432706\n",
      "80 Train Loss 0.00013808852 Test MSE 0.0005036984031397344 Test RE 0.011440575448432706\n",
      "81 Train Loss 0.00013808852 Test MSE 0.0005036984031397344 Test RE 0.011440575448432706\n",
      "82 Train Loss 0.00013808852 Test MSE 0.0005036984031397344 Test RE 0.011440575448432706\n",
      "83 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "84 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "85 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "86 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "87 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "88 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "89 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "90 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "92 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "93 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "94 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "95 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "96 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "97 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "98 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "99 Train Loss 0.00013808659 Test MSE 0.0005037080773810342 Test RE 0.011440685314133468\n",
      "Training time: 88.43\n",
      "Training time: 88.43\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 0.05026998 Test MSE 3.9061994072456456 Test RE 1.0074877804875444\n",
      "1 Train Loss 0.043251455 Test MSE 3.869506402764176 Test RE 1.0027446818421357\n",
      "2 Train Loss 0.04269087 Test MSE 3.8535127719835067 Test RE 1.0006702396981562\n",
      "3 Train Loss 0.030800657 Test MSE 3.8220658026218164 Test RE 0.9965788416605319\n",
      "4 Train Loss 0.02527191 Test MSE 3.852215718743774 Test RE 1.000501817807743\n",
      "5 Train Loss 0.024002843 Test MSE 3.848053454230655 Test RE 0.9999611577039246\n",
      "6 Train Loss 0.023976017 Test MSE 3.8472029395263587 Test RE 0.9998506435598231\n",
      "7 Train Loss 0.02395555 Test MSE 3.8455279112655 Test RE 0.9996329580821605\n",
      "8 Train Loss 0.023927314 Test MSE 3.842915485539777 Test RE 0.9992933544456446\n",
      "9 Train Loss 0.023838522 Test MSE 3.83844025144243 Test RE 0.9987113257168463\n",
      "10 Train Loss 0.023813512 Test MSE 3.8371062772021816 Test RE 0.9985377694199461\n",
      "11 Train Loss 0.023804449 Test MSE 3.8361146798532046 Test RE 0.9984087384118101\n",
      "12 Train Loss 0.023800185 Test MSE 3.835222591228478 Test RE 0.9982926416654919\n",
      "13 Train Loss 0.023797877 Test MSE 3.8345642158755266 Test RE 0.9982069517917058\n",
      "14 Train Loss 0.02379359 Test MSE 3.8333905499297227 Test RE 0.9980541767825397\n",
      "15 Train Loss 0.023787282 Test MSE 3.8322430768489766 Test RE 0.9979047886622967\n",
      "16 Train Loss 0.023780461 Test MSE 3.8304602198389 Test RE 0.9976726363061602\n",
      "17 Train Loss 0.023771854 Test MSE 3.8266061110611855 Test RE 0.9971705939469462\n",
      "18 Train Loss 0.023767205 Test MSE 3.8247138132027745 Test RE 0.9969240076690777\n",
      "19 Train Loss 0.023756696 Test MSE 3.8227038679261627 Test RE 0.9966620238816143\n",
      "20 Train Loss 0.02374643 Test MSE 3.8200907415515712 Test RE 0.9963213162119445\n",
      "21 Train Loss 0.023730928 Test MSE 3.812801233886635 Test RE 0.9953702707598062\n",
      "22 Train Loss 0.023725342 Test MSE 3.8107353532561463 Test RE 0.9951005747208539\n",
      "23 Train Loss 0.023723805 Test MSE 3.8096098369077076 Test RE 0.9949536103461507\n",
      "24 Train Loss 0.02370949 Test MSE 3.8002651011376942 Test RE 0.9937325814758302\n",
      "25 Train Loss 0.023684302 Test MSE 3.7924167513175626 Test RE 0.9927059173338136\n",
      "26 Train Loss 0.023621092 Test MSE 3.7957707065278545 Test RE 0.9931447872668414\n",
      "27 Train Loss 0.023538742 Test MSE 3.7912607596465655 Test RE 0.9925546091694429\n",
      "28 Train Loss 0.023302956 Test MSE 3.722165605212301 Test RE 0.983468441419977\n",
      "29 Train Loss 0.023006322 Test MSE 3.6568049597578938 Test RE 0.9747954226266102\n",
      "30 Train Loss 0.022755772 Test MSE 3.596483795381917 Test RE 0.9667220765068835\n",
      "31 Train Loss 0.022299528 Test MSE 3.4967204525521396 Test RE 0.9532197662092586\n",
      "32 Train Loss 0.022111068 Test MSE 3.398371499726121 Test RE 0.9397190024092106\n",
      "33 Train Loss 0.021641107 Test MSE 3.3026618584186536 Test RE 0.9263916629824186\n",
      "34 Train Loss 0.021051595 Test MSE 3.28383280422493 Test RE 0.9237471277239332\n",
      "35 Train Loss 0.019929465 Test MSE 3.1308506851138413 Test RE 0.9019734633725973\n",
      "36 Train Loss 0.019464448 Test MSE 2.9893381492644284 Test RE 0.8813534417363443\n",
      "37 Train Loss 0.018795075 Test MSE 2.8801672126667164 Test RE 0.8651102020840205\n",
      "38 Train Loss 0.018260498 Test MSE 2.7916849561054478 Test RE 0.8517179216141942\n",
      "39 Train Loss 0.017764429 Test MSE 2.6735819992763044 Test RE 0.8335071637185436\n",
      "40 Train Loss 0.01691858 Test MSE 2.64771223160454 Test RE 0.8294648244575719\n",
      "41 Train Loss 0.016517745 Test MSE 2.548506084875621 Test RE 0.8137770165839374\n",
      "42 Train Loss 0.016032139 Test MSE 2.35303306280567 Test RE 0.7819457027850509\n",
      "43 Train Loss 0.014101353 Test MSE 2.0563204578532375 Test RE 0.730984184396406\n",
      "44 Train Loss 0.012485738 Test MSE 1.8010160750659008 Test RE 0.6841028143856179\n",
      "45 Train Loss 0.011304976 Test MSE 1.41132557197164 Test RE 0.6055865038080526\n",
      "46 Train Loss 0.009756502 Test MSE 1.280928869904734 Test RE 0.5769326129533396\n",
      "47 Train Loss 0.008352998 Test MSE 1.1809778522998593 Test RE 0.553966444661302\n",
      "48 Train Loss 0.006610983 Test MSE 0.8455123222875431 Test RE 0.46872978708515267\n",
      "49 Train Loss 0.003754379 Test MSE 0.49146778575354877 Test RE 0.35736343234752155\n",
      "50 Train Loss 0.00330014 Test MSE 0.42294213661040564 Test RE 0.33151490561872876\n",
      "51 Train Loss 0.0026462693 Test MSE 0.2534747285698305 Test RE 0.25664329109391465\n",
      "52 Train Loss 0.0020816175 Test MSE 0.15281495609028237 Test RE 0.19927164824547178\n",
      "53 Train Loss 0.0014877473 Test MSE 0.1075942467541382 Test RE 0.16720802118699174\n",
      "54 Train Loss 0.0012179033 Test MSE 0.12440590990412542 Test RE 0.17979726805969337\n",
      "55 Train Loss 0.0011399988 Test MSE 0.1070442732236257 Test RE 0.16678012748275597\n",
      "56 Train Loss 0.0009636693 Test MSE 0.05610625877419634 Test RE 0.12074474474743252\n",
      "57 Train Loss 0.00081770914 Test MSE 0.03059258956315396 Test RE 0.08916013196573516\n",
      "58 Train Loss 0.000768156 Test MSE 0.03156826749657186 Test RE 0.09057074842263213\n",
      "59 Train Loss 0.00064908364 Test MSE 0.04012652954912364 Test RE 0.10211237622615241\n",
      "60 Train Loss 0.00047978706 Test MSE 0.02196500911883231 Test RE 0.07554892281144172\n",
      "61 Train Loss 0.00043261395 Test MSE 0.013042430649590849 Test RE 0.058215931459123405\n",
      "62 Train Loss 0.00041994653 Test MSE 0.014092277120664359 Test RE 0.06051362504680143\n",
      "63 Train Loss 0.00035117174 Test MSE 0.004659423390617652 Test RE 0.034795947771491156\n",
      "64 Train Loss 0.0002656031 Test MSE 6.174426856179375e-06 Test RE 0.001266662483522617\n",
      "65 Train Loss 0.00022197957 Test MSE 0.00015177890639560344 Test RE 0.006280125059768798\n",
      "66 Train Loss 0.00021727291 Test MSE 8.620597236909079e-07 Test RE 0.000473294284512368\n",
      "67 Train Loss 0.00020810527 Test MSE 0.0002908180500862719 Test RE 0.008693071699060997\n",
      "68 Train Loss 0.00019125192 Test MSE 0.00012337843790474951 Test RE 0.005662161027405129\n",
      "69 Train Loss 0.00018471877 Test MSE 4.25882004854313e-05 Test RE 0.0033266508940586253\n",
      "70 Train Loss 0.00015394624 Test MSE 0.0005602710511466301 Test RE 0.012065954247364803\n",
      "71 Train Loss 0.00014984737 Test MSE 0.0004007115391007468 Test RE 0.01020418930526631\n",
      "72 Train Loss 0.00014945833 Test MSE 0.00027042493520032886 Test RE 0.008382739101389966\n",
      "73 Train Loss 0.00014945834 Test MSE 0.00027042493520032886 Test RE 0.008382739101389966\n",
      "74 Train Loss 0.0001494576 Test MSE 0.00027017138592907194 Test RE 0.008378808369516664\n",
      "75 Train Loss 0.0001494576 Test MSE 0.00027017138592907194 Test RE 0.008378808369516664\n",
      "76 Train Loss 0.0001494576 Test MSE 0.00027017138592907194 Test RE 0.008378808369516664\n",
      "77 Train Loss 0.0001494576 Test MSE 0.00027017138592907194 Test RE 0.008378808369516664\n",
      "78 Train Loss 0.0001494576 Test MSE 0.00027017138592907194 Test RE 0.008378808369516664\n",
      "79 Train Loss 0.000149285 Test MSE 0.0002178748717074704 Test RE 0.007524300897875429\n",
      "80 Train Loss 0.00014880068 Test MSE 0.00017042987063289018 Test RE 0.006654806615962441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 Train Loss 0.0001450754 Test MSE 0.0003277435090112609 Test RE 0.009228468456959916\n",
      "82 Train Loss 0.00013908376 Test MSE 0.00122645680357518 Test RE 0.017852075335976807\n",
      "83 Train Loss 0.00012793017 Test MSE 0.0026823083068836656 Test RE 0.02640079043820448\n",
      "84 Train Loss 0.00011906495 Test MSE 0.002240543312768054 Test RE 0.02412899616784861\n",
      "85 Train Loss 9.9791e-05 Test MSE 0.0006342456288733528 Test RE 0.012837821236155027\n",
      "86 Train Loss 7.899649e-05 Test MSE 2.8244001427909795e-05 Test RE 0.0027091041085513454\n",
      "87 Train Loss 6.145091e-05 Test MSE 7.648329587301814e-05 Test RE 0.004458059697970892\n",
      "88 Train Loss 5.1790154e-05 Test MSE 0.00010584673691833029 Test RE 0.005244466547050337\n",
      "89 Train Loss 4.632508e-05 Test MSE 3.224753458140547e-05 Test RE 0.0028947485236357415\n",
      "90 Train Loss 4.0196872e-05 Test MSE 1.263850598721586e-06 Test RE 0.0005730736655522068\n",
      "91 Train Loss 3.4289747e-05 Test MSE 8.399503173053942e-06 Test RE 0.0014773703603809456\n",
      "92 Train Loss 2.4963512e-05 Test MSE 3.953201396327487e-05 Test RE 0.0032050665569119167\n",
      "93 Train Loss 2.0807956e-05 Test MSE 0.00011041758295155291 Test RE 0.005356507293806727\n",
      "94 Train Loss 1.7728278e-05 Test MSE 8.720994055213472e-05 Test RE 0.004760423360965779\n",
      "95 Train Loss 1.4583328e-05 Test MSE 3.7436961449051936e-05 Test RE 0.0031189820735159296\n",
      "96 Train Loss 1.3095954e-05 Test MSE 3.426741050974265e-05 Test RE 0.002984030282795912\n",
      "97 Train Loss 1.2101753e-05 Test MSE 4.729261197019513e-05 Test RE 0.0035055747566276963\n",
      "98 Train Loss 1.0593703e-05 Test MSE 4.334750871059671e-05 Test RE 0.0033561754309857517\n",
      "99 Train Loss 9.52153e-06 Test MSE 2.702545770850587e-05 Test RE 0.002650019755334143\n",
      "Training time: 110.68\n",
      "Training time: 110.68\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 0.04673175 Test MSE 3.8387765418624515 Test RE 0.9987550739157653\n",
      "1 Train Loss 0.04167893 Test MSE 3.8642163457189973 Test RE 1.002059014182341\n",
      "2 Train Loss 0.03371387 Test MSE 3.8117652841425014 Test RE 0.9952350390016493\n",
      "3 Train Loss 0.028566757 Test MSE 3.81723739415417 Test RE 0.9959491546483517\n",
      "4 Train Loss 0.02414562 Test MSE 3.819420951904145 Test RE 0.9962339681616034\n",
      "5 Train Loss 0.023954893 Test MSE 3.8218293122798817 Test RE 0.9965480095230167\n",
      "6 Train Loss 0.02386389 Test MSE 3.825624819788727 Test RE 0.9970427290043586\n",
      "7 Train Loss 0.023859585 Test MSE 3.8268372358265714 Test RE 0.9972007077518318\n",
      "8 Train Loss 0.023855107 Test MSE 3.8280391020051536 Test RE 0.9973572871467344\n",
      "9 Train Loss 0.023852272 Test MSE 3.8275770024700906 Test RE 0.9972970876247458\n",
      "10 Train Loss 0.023839641 Test MSE 3.8265159353593283 Test RE 0.9971588444899426\n",
      "11 Train Loss 0.02381706 Test MSE 3.827470415064728 Test RE 0.9972832015489175\n",
      "12 Train Loss 0.023802029 Test MSE 3.829417721264481 Test RE 0.9975368637073632\n",
      "13 Train Loss 0.023795357 Test MSE 3.830924153621628 Test RE 0.9977330520264001\n",
      "14 Train Loss 0.023794299 Test MSE 3.831423155857248 Test RE 0.9977980304483013\n",
      "15 Train Loss 0.023792833 Test MSE 3.832205884945749 Test RE 0.9978999463196226\n",
      "16 Train Loss 0.023792235 Test MSE 3.8326023073913675 Test RE 0.9979515588528686\n",
      "17 Train Loss 0.023787541 Test MSE 3.8329045647223166 Test RE 0.9979909096912127\n",
      "18 Train Loss 0.02378041 Test MSE 3.8305319729302005 Test RE 0.9976819805828807\n",
      "19 Train Loss 0.02377754 Test MSE 3.829855148254974 Test RE 0.9975938354351591\n",
      "20 Train Loss 0.023776818 Test MSE 3.8298192834566165 Test RE 0.9975891644248823\n",
      "21 Train Loss 0.023776766 Test MSE 3.829814752966972 Test RE 0.9975885743750157\n",
      "22 Train Loss 0.023776766 Test MSE 3.829814752966972 Test RE 0.9975885743750157\n",
      "23 Train Loss 0.023776766 Test MSE 3.829814752966972 Test RE 0.9975885743750157\n",
      "24 Train Loss 0.023776764 Test MSE 3.829814753806103 Test RE 0.9975885744843037\n",
      "25 Train Loss 0.023776596 Test MSE 3.829763678580006 Test RE 0.997581922435165\n",
      "26 Train Loss 0.023776554 Test MSE 3.829761547221513 Test RE 0.9975816448455825\n",
      "27 Train Loss 0.023776554 Test MSE 3.829761547221513 Test RE 0.9975816448455825\n",
      "28 Train Loss 0.02377655 Test MSE 3.8297607032308814 Test RE 0.9975815349236462\n",
      "29 Train Loss 0.02377655 Test MSE 3.8297607032308814 Test RE 0.9975815349236462\n",
      "30 Train Loss 0.02377655 Test MSE 3.8297607032308814 Test RE 0.9975815349236462\n",
      "31 Train Loss 0.02377655 Test MSE 3.8297607032308814 Test RE 0.9975815349236462\n",
      "32 Train Loss 0.02377655 Test MSE 3.8297607032308814 Test RE 0.9975815349236462\n",
      "33 Train Loss 0.02377655 Test MSE 3.8297607032308814 Test RE 0.9975815349236462\n",
      "34 Train Loss 0.02377655 Test MSE 3.8297607032308814 Test RE 0.9975815349236462\n",
      "35 Train Loss 0.02377655 Test MSE 3.8297607032308814 Test RE 0.9975815349236462\n",
      "36 Train Loss 0.02377655 Test MSE 3.8297607032308814 Test RE 0.9975815349236462\n",
      "37 Train Loss 0.023776533 Test MSE 3.8297422565203316 Test RE 0.9975791324080052\n",
      "38 Train Loss 0.0237765 Test MSE 3.829728713076329 Test RE 0.997577368494384\n",
      "39 Train Loss 0.02377623 Test MSE 3.829570471580318 Test RE 0.9975567587100258\n",
      "40 Train Loss 0.023775894 Test MSE 3.829400350603003 Test RE 0.997534601236136\n",
      "41 Train Loss 0.02377379 Test MSE 3.827816255871765 Test RE 0.9973282565590299\n",
      "42 Train Loss 0.023772223 Test MSE 3.8264005352126262 Test RE 0.9971438082080505\n",
      "43 Train Loss 0.023770157 Test MSE 3.8241041332510934 Test RE 0.9968445469781252\n",
      "44 Train Loss 0.023764884 Test MSE 3.822501407697855 Test RE 0.9966356306401161\n",
      "45 Train Loss 0.023756338 Test MSE 3.820517305057544 Test RE 0.9963769408657916\n",
      "46 Train Loss 0.023740102 Test MSE 3.8165543766602292 Test RE 0.9958600481745512\n",
      "47 Train Loss 0.023717016 Test MSE 3.8110067146526716 Test RE 0.9951360045066505\n",
      "48 Train Loss 0.023704015 Test MSE 3.805140022630372 Test RE 0.9943697496227782\n",
      "49 Train Loss 0.023693632 Test MSE 3.797574390634797 Test RE 0.9933807217919698\n",
      "50 Train Loss 0.023648558 Test MSE 3.7825746680192505 Test RE 0.9914169448687411\n",
      "51 Train Loss 0.023532217 Test MSE 3.7693904779689134 Test RE 0.9896876413847163\n",
      "52 Train Loss 0.023450458 Test MSE 3.7560706950170357 Test RE 0.9879374790100236\n",
      "53 Train Loss 0.023394234 Test MSE 3.736550701982026 Test RE 0.9853670197948221\n",
      "54 Train Loss 0.023370378 Test MSE 3.728998048291928 Test RE 0.9843706596572517\n",
      "55 Train Loss 0.02333838 Test MSE 3.729384119005081 Test RE 0.9844216152814804\n",
      "56 Train Loss 0.023312626 Test MSE 3.734576998324264 Test RE 0.9851067423978996\n",
      "57 Train Loss 0.023303274 Test MSE 3.733540076126419 Test RE 0.9849699732318987\n",
      "58 Train Loss 0.023262212 Test MSE 3.7089714823595465 Test RE 0.981723822239524\n",
      "59 Train Loss 0.023259202 Test MSE 3.7061262391932313 Test RE 0.9813471977620069\n",
      "60 Train Loss 0.023258375 Test MSE 3.705017845820614 Test RE 0.9812004407442525\n",
      "61 Train Loss 0.023258375 Test MSE 3.705017845820614 Test RE 0.9812004407442525\n",
      "62 Train Loss 0.023257613 Test MSE 3.7041757476294173 Test RE 0.9810889279114153\n",
      "63 Train Loss 0.023257613 Test MSE 3.7041757476294173 Test RE 0.9810889279114153\n",
      "64 Train Loss 0.023257613 Test MSE 3.7041757476294173 Test RE 0.9810889279114153\n",
      "65 Train Loss 0.023257613 Test MSE 3.7041757476294173 Test RE 0.9810889279114153\n",
      "66 Train Loss 0.023257613 Test MSE 3.7041757476294173 Test RE 0.9810889279114153\n",
      "67 Train Loss 0.023257613 Test MSE 3.7041757476294173 Test RE 0.9810889279114153\n",
      "68 Train Loss 0.023255594 Test MSE 3.7026549421249437 Test RE 0.9808875067693682\n",
      "69 Train Loss 0.023243805 Test MSE 3.6974819661975626 Test RE 0.9802020687477091\n",
      "70 Train Loss 0.023115098 Test MSE 3.6836408475380464 Test RE 0.9783657090548354\n",
      "71 Train Loss 0.023006849 Test MSE 3.665179684098748 Test RE 0.9759110104613761\n",
      "72 Train Loss 0.0229826 Test MSE 3.661661339937699 Test RE 0.9754424911109726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 Train Loss 0.022955414 Test MSE 3.6573876067174593 Test RE 0.9748730777071511\n",
      "74 Train Loss 0.022922779 Test MSE 3.6436875849168855 Test RE 0.9730455009609834\n",
      "75 Train Loss 0.022911377 Test MSE 3.635869042372995 Test RE 0.9720009707956467\n",
      "76 Train Loss 0.022837462 Test MSE 3.605494604656701 Test RE 0.9679323556798443\n",
      "77 Train Loss 0.022659462 Test MSE 3.5836877377241065 Test RE 0.9650007766381855\n",
      "78 Train Loss 0.022582117 Test MSE 3.5777524074044442 Test RE 0.9642013247500607\n",
      "79 Train Loss 0.022441221 Test MSE 3.540675634616858 Test RE 0.9591922336139025\n",
      "80 Train Loss 0.02234691 Test MSE 3.486422880298605 Test RE 0.9518151519514056\n",
      "81 Train Loss 0.02226748 Test MSE 3.467941681267236 Test RE 0.949289058660087\n",
      "82 Train Loss 0.022226542 Test MSE 3.4774588558159887 Test RE 0.95059074717503\n",
      "83 Train Loss 0.022171997 Test MSE 3.4728958478844323 Test RE 0.9499668753864455\n",
      "84 Train Loss 0.022147717 Test MSE 3.4790876941437254 Test RE 0.9508133489996975\n",
      "85 Train Loss 0.022140384 Test MSE 3.4838071811530624 Test RE 0.9514580338808906\n",
      "86 Train Loss 0.022129629 Test MSE 3.4897461117448163 Test RE 0.9522686753406243\n",
      "87 Train Loss 0.022076376 Test MSE 3.4857563686207387 Test RE 0.9517241666839054\n",
      "88 Train Loss 0.021832675 Test MSE 3.4096592660374108 Test RE 0.9412783573792296\n",
      "89 Train Loss 0.021520762 Test MSE 3.341649380283696 Test RE 0.9318435908507684\n",
      "90 Train Loss 0.021340534 Test MSE 3.3343794160112763 Test RE 0.9308293973456822\n",
      "91 Train Loss 0.021256685 Test MSE 3.332788600769996 Test RE 0.9306073238999905\n",
      "92 Train Loss 0.021176994 Test MSE 3.3190103620469085 Test RE 0.9286816977936043\n",
      "93 Train Loss 0.021081774 Test MSE 3.2887500193645334 Test RE 0.9244384790360383\n",
      "94 Train Loss 0.021066656 Test MSE 3.278960848231035 Test RE 0.9230616290850582\n",
      "95 Train Loss 0.020988036 Test MSE 3.2420109809618833 Test RE 0.9178460078080367\n",
      "96 Train Loss 0.020668888 Test MSE 3.1839711661161694 Test RE 0.9095930756621298\n",
      "97 Train Loss 0.020066282 Test MSE 3.080856758051448 Test RE 0.894743054500751\n",
      "98 Train Loss 0.019805191 Test MSE 3.019366043244551 Test RE 0.885768977453866\n",
      "99 Train Loss 0.019766932 Test MSE 2.998303570841802 Test RE 0.8826741002018306\n",
      "Training time: 100.32\n",
      "Training time: 100.32\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 0.04388652 Test MSE 3.8712482633199357 Test RE 1.0029703494994977\n",
      "1 Train Loss 0.042837813 Test MSE 3.8608045183362467 Test RE 1.0016165431789332\n",
      "2 Train Loss 0.041111887 Test MSE 3.8415165159932445 Test RE 0.9991114472154308\n",
      "3 Train Loss 0.03831479 Test MSE 3.8374186274282267 Test RE 0.9985784103520041\n",
      "4 Train Loss 0.03510817 Test MSE 3.7858537750214962 Test RE 0.9918465805366736\n",
      "5 Train Loss 0.03502576 Test MSE 3.7873162946413625 Test RE 0.9920381430086701\n",
      "6 Train Loss 0.03499182 Test MSE 3.7888014321941874 Test RE 0.9922326301572841\n",
      "7 Train Loss 0.034725834 Test MSE 3.791449011766761 Test RE 0.9925792511292039\n",
      "8 Train Loss 0.034447778 Test MSE 3.7918135008436 Test RE 0.9926269605404658\n",
      "9 Train Loss 0.033398245 Test MSE 3.805415943216393 Test RE 0.9944058011358395\n",
      "10 Train Loss 0.028405197 Test MSE 3.9114930929032012 Test RE 1.0081702236182895\n",
      "11 Train Loss 0.028091244 Test MSE 3.9214388484482168 Test RE 1.0094511473628398\n",
      "12 Train Loss 0.028026884 Test MSE 3.9182448192566954 Test RE 1.0090399624178676\n",
      "13 Train Loss 0.02800271 Test MSE 3.913633231855649 Test RE 1.0084459916261128\n",
      "14 Train Loss 0.027742684 Test MSE 3.8989935937508324 Test RE 1.0065580890630983\n",
      "15 Train Loss 0.02701056 Test MSE 3.8836559189915816 Test RE 1.0045763631723454\n",
      "16 Train Loss 0.026155584 Test MSE 3.840337859478059 Test RE 0.9989581614551958\n",
      "17 Train Loss 0.024638044 Test MSE 3.7818836198835895 Test RE 0.9913263784971771\n",
      "18 Train Loss 0.023762146 Test MSE 3.7635389163108033 Test RE 0.9889191527401564\n",
      "19 Train Loss 0.023458257 Test MSE 3.7372520767941753 Test RE 0.9854594953471475\n",
      "20 Train Loss 0.023355652 Test MSE 3.7200195418378286 Test RE 0.9831848847307869\n",
      "21 Train Loss 0.023338158 Test MSE 3.711780913117851 Test RE 0.9820955645166256\n",
      "22 Train Loss 0.023336153 Test MSE 3.7098951891915397 Test RE 0.9818460621317217\n",
      "23 Train Loss 0.023335995 Test MSE 3.7097976515898767 Test RE 0.981833155090595\n",
      "24 Train Loss 0.023334239 Test MSE 3.708320434660771 Test RE 0.981637655887373\n",
      "25 Train Loss 0.023330256 Test MSE 3.7055410088162137 Test RE 0.9812697129668881\n",
      "26 Train Loss 0.023313856 Test MSE 3.6975688739314627 Test RE 0.9802135882954407\n",
      "27 Train Loss 0.023194421 Test MSE 3.6980403112457254 Test RE 0.9802760745598644\n",
      "28 Train Loss 0.023158293 Test MSE 3.698795238560892 Test RE 0.9803761274741639\n",
      "29 Train Loss 0.023158273 Test MSE 3.698794803475401 Test RE 0.9803760698138433\n",
      "30 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "31 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "32 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "33 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "34 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "35 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "36 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "37 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "38 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "39 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "40 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "41 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "42 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "43 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "44 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "45 Train Loss 0.023157798 Test MSE 3.6985693958468824 Test RE 0.9803461968925895\n",
      "46 Train Loss 0.023157764 Test MSE 3.6985669688181075 Test RE 0.9803458752373\n",
      "47 Train Loss 0.023157764 Test MSE 3.6985669688181075 Test RE 0.9803458752373\n",
      "48 Train Loss 0.023157764 Test MSE 3.6985669688181075 Test RE 0.9803458752373\n",
      "49 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "50 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "51 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "52 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "53 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "54 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "55 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "56 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "57 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "58 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "59 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "60 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "61 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "62 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "63 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "64 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "65 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "67 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "68 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "69 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "70 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "71 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "72 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "73 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "74 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "75 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "76 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "77 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "78 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "79 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "80 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "81 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "82 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "83 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "84 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "85 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "86 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "87 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "88 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "89 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "90 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "91 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "92 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "93 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "94 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "95 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "96 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "97 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "98 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "99 Train Loss 0.02315776 Test MSE 3.6985671649766187 Test RE 0.9803459012342857\n",
      "Training time: 63.08\n",
      "Training time: 63.08\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 0.06037813 Test MSE 3.817727562466644 Test RE 0.9960130971013954\n",
      "1 Train Loss 0.043339856 Test MSE 3.870395432025153 Test RE 1.002859866838901\n",
      "2 Train Loss 0.04223677 Test MSE 3.8852737679689504 Test RE 1.00478558402629\n",
      "3 Train Loss 0.032730628 Test MSE 3.8226060633034478 Test RE 0.9966492739040991\n",
      "4 Train Loss 0.028325893 Test MSE 3.800611393347172 Test RE 0.9937778564767972\n",
      "5 Train Loss 0.023963485 Test MSE 3.8377799230466896 Test RE 0.9986254176696581\n",
      "6 Train Loss 0.023835842 Test MSE 3.8391625669938954 Test RE 0.9988052897674091\n",
      "7 Train Loss 0.023828495 Test MSE 3.8384924740452293 Test RE 0.9987181195086032\n",
      "8 Train Loss 0.02382297 Test MSE 3.837269384457739 Test RE 0.9985589920565647\n",
      "9 Train Loss 0.023821287 Test MSE 3.8368962189830387 Test RE 0.9985104371125816\n",
      "10 Train Loss 0.023821095 Test MSE 3.836944891019491 Test RE 0.9985167702760942\n",
      "11 Train Loss 0.02382107 Test MSE 3.8369521946246556 Test RE 0.9985177206114487\n",
      "12 Train Loss 0.023821067 Test MSE 3.836952193150961 Test RE 0.998517720419694\n",
      "13 Train Loss 0.023821067 Test MSE 3.836952193150961 Test RE 0.998517720419694\n",
      "14 Train Loss 0.023821067 Test MSE 3.836952193150961 Test RE 0.998517720419694\n",
      "15 Train Loss 0.023821067 Test MSE 3.836952193150961 Test RE 0.998517720419694\n",
      "16 Train Loss 0.023821056 Test MSE 3.8369522631724515 Test RE 0.9985177295307925\n",
      "17 Train Loss 0.023821056 Test MSE 3.8369522631724515 Test RE 0.9985177295307925\n",
      "18 Train Loss 0.023821056 Test MSE 3.8369522631724515 Test RE 0.9985177295307925\n",
      "19 Train Loss 0.023821056 Test MSE 3.8369522631724515 Test RE 0.9985177295307925\n",
      "20 Train Loss 0.023821056 Test MSE 3.8369522631724515 Test RE 0.9985177295307925\n",
      "21 Train Loss 0.023821052 Test MSE 3.836954984641002 Test RE 0.998518083644413\n",
      "22 Train Loss 0.023821052 Test MSE 3.836954984641002 Test RE 0.998518083644413\n",
      "23 Train Loss 0.023821052 Test MSE 3.8369580991403693 Test RE 0.998518488898491\n",
      "24 Train Loss 0.023821052 Test MSE 3.8369580991403693 Test RE 0.998518488898491\n",
      "25 Train Loss 0.023821052 Test MSE 3.8369580991403693 Test RE 0.998518488898491\n",
      "26 Train Loss 0.023821041 Test MSE 3.8369578796471764 Test RE 0.9985184603383646\n",
      "27 Train Loss 0.023821041 Test MSE 3.8369578796471764 Test RE 0.9985184603383646\n",
      "28 Train Loss 0.023821041 Test MSE 3.8369578796471764 Test RE 0.9985184603383646\n",
      "29 Train Loss 0.023821041 Test MSE 3.8369578796471764 Test RE 0.9985184603383646\n",
      "30 Train Loss 0.023821041 Test MSE 3.8369578796471764 Test RE 0.9985184603383646\n",
      "31 Train Loss 0.023821041 Test MSE 3.8369578796471764 Test RE 0.9985184603383646\n",
      "32 Train Loss 0.023821041 Test MSE 3.8369578796471764 Test RE 0.9985184603383646\n",
      "33 Train Loss 0.023821041 Test MSE 3.8369578796471764 Test RE 0.9985184603383646\n",
      "34 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "35 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "36 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "37 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "38 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "39 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "40 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "41 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "42 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "43 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "44 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "45 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "46 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "47 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "48 Train Loss 0.02382104 Test MSE 3.8369577355632947 Test RE 0.9985184415903852\n",
      "49 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "50 Train Loss 0.023821034 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "51 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "52 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "53 Train Loss 0.023821034 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "54 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "55 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "56 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "57 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "58 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "59 Train Loss 0.023821034 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "61 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "62 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "63 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "64 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "65 Train Loss 0.023821034 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "66 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "67 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "68 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "69 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "70 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "71 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "72 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "73 Train Loss 0.023821034 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "74 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "75 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "76 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "77 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "78 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "79 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "80 Train Loss 0.023821034 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "81 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "82 Train Loss 0.023821034 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "83 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "84 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "85 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "86 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "87 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "88 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "89 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "90 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "91 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "92 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "93 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "94 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "95 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "96 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "97 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "98 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "99 Train Loss 0.023821035 Test MSE 3.8369577069929024 Test RE 0.9985184378728482\n",
      "Training time: 50.24\n",
      "Training time: 50.24\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 0.04585912 Test MSE 3.8845352416776144 Test RE 1.00469008292624\n",
      "1 Train Loss 0.044071708 Test MSE 3.8713115378959717 Test RE 1.0029785461146468\n",
      "2 Train Loss 0.04367238 Test MSE 3.870722095451431 Test RE 1.002902186898165\n",
      "3 Train Loss 0.041456364 Test MSE 3.8404587116911593 Test RE 0.9989738795194979\n",
      "4 Train Loss 0.026650168 Test MSE 3.8072752609295617 Test RE 0.9946487036935071\n",
      "5 Train Loss 0.023920687 Test MSE 3.820420556829624 Test RE 0.9963643249939137\n",
      "6 Train Loss 0.02381713 Test MSE 3.8247925971690857 Test RE 0.996934275263833\n",
      "7 Train Loss 0.023808166 Test MSE 3.827682504065216 Test RE 0.997310832052156\n",
      "8 Train Loss 0.023795132 Test MSE 3.8289937225438497 Test RE 0.9974816378097253\n",
      "9 Train Loss 0.023784535 Test MSE 3.8289438852303808 Test RE 0.9974751462912708\n",
      "10 Train Loss 0.023779688 Test MSE 3.829190404048 Test RE 0.9975072559856514\n",
      "11 Train Loss 0.02377175 Test MSE 3.829095228556834 Test RE 0.9974948592616776\n",
      "12 Train Loss 0.023753041 Test MSE 3.825304612961844 Test RE 0.9970010016285186\n",
      "13 Train Loss 0.023735302 Test MSE 3.8146103702124043 Test RE 0.9956063894090473\n",
      "14 Train Loss 0.023714565 Test MSE 3.8097258196741546 Test RE 0.9949687558070845\n",
      "15 Train Loss 0.023684263 Test MSE 3.80821329677692 Test RE 0.9947712268384916\n",
      "16 Train Loss 0.02365982 Test MSE 3.802889137014199 Test RE 0.9940756022788892\n",
      "17 Train Loss 0.023607858 Test MSE 3.790496188088193 Test RE 0.9924545214497559\n",
      "18 Train Loss 0.023552919 Test MSE 3.7839615056001077 Test RE 0.9915986740340893\n",
      "19 Train Loss 0.023460234 Test MSE 3.760949972490904 Test RE 0.9885789547724616\n",
      "20 Train Loss 0.023322174 Test MSE 3.7275494914493223 Test RE 0.9841794480583014\n",
      "21 Train Loss 0.023243818 Test MSE 3.7051757966832963 Test RE 0.9812213555968642\n",
      "22 Train Loss 0.023068156 Test MSE 3.677737466524939 Test RE 0.9775814331634307\n",
      "23 Train Loss 0.022317812 Test MSE 3.504095960656079 Test RE 0.9542245329364125\n",
      "24 Train Loss 0.021477176 Test MSE 3.3107090054735147 Test RE 0.9275195826355992\n",
      "25 Train Loss 0.020430816 Test MSE 3.13081916016443 Test RE 0.9019689223161494\n",
      "26 Train Loss 0.019904325 Test MSE 3.0323493820066343 Test RE 0.8876713473926847\n",
      "27 Train Loss 0.019411603 Test MSE 2.9363433754726613 Test RE 0.8735062219577174\n",
      "28 Train Loss 0.018893614 Test MSE 2.8483338855932736 Test RE 0.8603160612015708\n",
      "29 Train Loss 0.018192597 Test MSE 2.7003752128301635 Test RE 0.8376732344360822\n",
      "30 Train Loss 0.017462382 Test MSE 2.523524384565552 Test RE 0.8097786743086476\n",
      "31 Train Loss 0.016834963 Test MSE 2.46131549144286 Test RE 0.7997352246349316\n",
      "32 Train Loss 0.01661979 Test MSE 2.4933941465702665 Test RE 0.804929882017005\n",
      "33 Train Loss 0.016583558 Test MSE 2.5066238023862386 Test RE 0.80706248851797\n",
      "34 Train Loss 0.016473651 Test MSE 2.5086399181289822 Test RE 0.8073869896131873\n",
      "35 Train Loss 0.01567603 Test MSE 2.35287106876562 Test RE 0.7819187858824109\n",
      "36 Train Loss 0.01504987 Test MSE 2.174701272588297 Test RE 0.7517308731517562\n",
      "37 Train Loss 0.014643256 Test MSE 2.0218532989363664 Test RE 0.7248320744788826\n",
      "38 Train Loss 0.014122453 Test MSE 1.9570078738587295 Test RE 0.7131138462169982\n",
      "39 Train Loss 0.013359953 Test MSE 2.023763371842349 Test RE 0.7251743731206953\n",
      "40 Train Loss 0.012860384 Test MSE 1.9340278049102104 Test RE 0.7089146303663166\n",
      "41 Train Loss 0.012330084 Test MSE 1.7606258478141057 Test RE 0.6763883507557915\n",
      "42 Train Loss 0.01182288 Test MSE 1.6994349421800743 Test RE 0.6645304016615338\n",
      "43 Train Loss 0.011071601 Test MSE 1.6477274908340542 Test RE 0.6543427220430482\n",
      "44 Train Loss 0.01030691 Test MSE 1.466533907267962 Test RE 0.6173175683956067\n",
      "45 Train Loss 0.009754478 Test MSE 1.295199074531034 Test RE 0.5801373746174155\n",
      "46 Train Loss 0.008686602 Test MSE 1.0686555383733458 Test RE 0.5269646138907557\n",
      "47 Train Loss 0.008516235 Test MSE 1.0592488811245266 Test RE 0.5246402297308103\n",
      "48 Train Loss 0.0080089215 Test MSE 1.067843696351476 Test RE 0.5267644121963161\n",
      "49 Train Loss 0.0064864783 Test MSE 0.9143165904601321 Test RE 0.4874284593608716\n",
      "50 Train Loss 0.0060050255 Test MSE 0.7916859320719147 Test RE 0.4535644905583578\n",
      "51 Train Loss 0.005670099 Test MSE 0.7265952515003388 Test RE 0.4345190888439502\n",
      "52 Train Loss 0.0053162538 Test MSE 0.6107927760451506 Test RE 0.398391008922161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 Train Loss 0.0049341186 Test MSE 0.5099972716670432 Test RE 0.3640378232940272\n",
      "54 Train Loss 0.004378705 Test MSE 0.5085128694579685 Test RE 0.363507651506885\n",
      "55 Train Loss 0.004215044 Test MSE 0.5401672223966215 Test RE 0.37465082757400964\n",
      "56 Train Loss 0.0038749224 Test MSE 0.5318575742687415 Test RE 0.3717579432709376\n",
      "57 Train Loss 0.0036902092 Test MSE 0.49350152760386273 Test RE 0.3581020714445177\n",
      "58 Train Loss 0.0034679489 Test MSE 0.4385888668600807 Test RE 0.3375914074473453\n",
      "59 Train Loss 0.0030818228 Test MSE 0.3783248685728377 Test RE 0.31354149694421163\n",
      "60 Train Loss 0.00275786 Test MSE 0.3170097406093194 Test RE 0.2870112273872836\n",
      "61 Train Loss 0.0026068764 Test MSE 0.28871672367914514 Test RE 0.273904112465382\n",
      "62 Train Loss 0.0025532139 Test MSE 0.26832152112055196 Test RE 0.26405253192805184\n",
      "63 Train Loss 0.0024820988 Test MSE 0.2409277998043416 Test RE 0.2502107932863735\n",
      "64 Train Loss 0.0023093289 Test MSE 0.2041400878027963 Test RE 0.2303173903993907\n",
      "65 Train Loss 0.0019831078 Test MSE 0.20890994098544688 Test RE 0.2329926042702599\n",
      "66 Train Loss 0.0017756484 Test MSE 0.18338491984794833 Test RE 0.2182953036095224\n",
      "67 Train Loss 0.0015909493 Test MSE 0.14945425072662505 Test RE 0.1970682764716661\n",
      "68 Train Loss 0.0015303834 Test MSE 0.1328757064254113 Test RE 0.18581697073406997\n",
      "69 Train Loss 0.001431453 Test MSE 0.11509117675214851 Test RE 0.172935278563035\n",
      "70 Train Loss 0.0013124825 Test MSE 0.1092541538034184 Test RE 0.1684928830046896\n",
      "71 Train Loss 0.0012275332 Test MSE 0.10785866878542164 Test RE 0.16741335908687469\n",
      "72 Train Loss 0.0011350934 Test MSE 0.0913888057123511 Test RE 0.15410228680352772\n",
      "73 Train Loss 0.0010121018 Test MSE 0.05875079052389775 Test RE 0.12355759315502166\n",
      "74 Train Loss 0.00055316143 Test MSE 0.032168386163438784 Test RE 0.09142757889689251\n",
      "75 Train Loss 0.0003274557 Test MSE 0.029996490016256663 Test RE 0.08828721194621908\n",
      "76 Train Loss 0.00026719988 Test MSE 0.02508028544175614 Test RE 0.08072886005517964\n",
      "77 Train Loss 0.00017013619 Test MSE 0.0063591104369934695 Test RE 0.040650020242217394\n",
      "78 Train Loss 0.00014713421 Test MSE 0.0013088415891884713 Test RE 0.018441919661753105\n",
      "79 Train Loss 0.00013694855 Test MSE 0.00032408000766598485 Test RE 0.00917674582188924\n",
      "80 Train Loss 0.00011564863 Test MSE 8.204670990293385e-05 Test RE 0.004617353954536618\n",
      "81 Train Loss 0.00010327829 Test MSE 0.00019896695641186132 Test RE 0.007190400167168346\n",
      "82 Train Loss 9.5453164e-05 Test MSE 6.950925404042813e-05 Test RE 0.004249950741202478\n",
      "83 Train Loss 8.4527586e-05 Test MSE 2.6431481203354397e-05 Test RE 0.0026207363545867435\n",
      "84 Train Loss 7.4104195e-05 Test MSE 5.469323416802615e-05 Test RE 0.0037698961357054994\n",
      "85 Train Loss 6.960577e-05 Test MSE 1.9074587883362393e-05 Test RE 0.0022263332080800307\n",
      "86 Train Loss 6.467435e-05 Test MSE 1.1382628001803497e-06 Test RE 0.0005438559120365777\n",
      "87 Train Loss 6.3061496e-05 Test MSE 2.2373913239927735e-05 Test RE 0.0024112017899292153\n",
      "88 Train Loss 6.178008e-05 Test MSE 0.00010767677608940726 Test RE 0.005289609410695466\n",
      "89 Train Loss 5.4361408e-05 Test MSE 4.5809480947431296e-05 Test RE 0.003450168200149762\n",
      "90 Train Loss 4.5028548e-05 Test MSE 2.324431170069727e-05 Test RE 0.0024576550580854843\n",
      "91 Train Loss 4.1623352e-05 Test MSE 0.00014444126364427713 Test RE 0.006126440530022336\n",
      "92 Train Loss 3.9454622e-05 Test MSE 0.00024470137189528714 Test RE 0.007974083602693952\n",
      "93 Train Loss 3.8553655e-05 Test MSE 0.0003068596239908402 Test RE 0.008929609286020644\n",
      "94 Train Loss 3.830496e-05 Test MSE 0.0003258052783478678 Test RE 0.009201140037464615\n",
      "95 Train Loss 3.8143546e-05 Test MSE 0.0003297633134355664 Test RE 0.009256861195579672\n",
      "96 Train Loss 3.8143546e-05 Test MSE 0.0003297633134355664 Test RE 0.009256861195579672\n",
      "97 Train Loss 3.8143546e-05 Test MSE 0.0003297633134355664 Test RE 0.009256861195579672\n",
      "98 Train Loss 3.814351e-05 Test MSE 0.0003297633134355664 Test RE 0.009256861195579672\n",
      "99 Train Loss 3.814351e-05 Test MSE 0.0003297633134355664 Test RE 0.009256861195579672\n",
      "Training time: 116.24\n",
      "Training time: 116.24\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 0.22234185 Test MSE 3.601273873452458 Test RE 0.9673656402812357\n",
      "1 Train Loss 0.08715759 Test MSE 3.76592437337668 Test RE 0.9892325081410451\n",
      "2 Train Loss 0.055973087 Test MSE 3.8135431129721784 Test RE 0.9954671035637717\n",
      "3 Train Loss 0.049522076 Test MSE 3.831287238997263 Test RE 0.9977803322233292\n",
      "4 Train Loss 0.048944373 Test MSE 3.833236580531793 Test RE 0.9980341329933955\n",
      "5 Train Loss 0.04889883 Test MSE 3.8334711610070284 Test RE 0.9980646705997489\n",
      "6 Train Loss 0.048355557 Test MSE 3.8585239195063314 Test RE 1.0013206692308643\n",
      "7 Train Loss 0.038139094 Test MSE 3.8425717466554308 Test RE 0.9992486613388569\n",
      "8 Train Loss 0.027999556 Test MSE 3.813785191578227 Test RE 0.9954986985212023\n",
      "9 Train Loss 0.024629928 Test MSE 3.838277243570553 Test RE 0.9986901192468234\n",
      "10 Train Loss 0.024349015 Test MSE 3.838306661385088 Test RE 0.9986939463836543\n",
      "11 Train Loss 0.02387598 Test MSE 3.829075711865849 Test RE 0.997492317169803\n",
      "12 Train Loss 0.023857305 Test MSE 3.8292230617477285 Test RE 0.9975115096554289\n",
      "13 Train Loss 0.02377927 Test MSE 3.8253583867240066 Test RE 0.9970080092150707\n",
      "14 Train Loss 0.02374856 Test MSE 3.821634981796839 Test RE 0.9965226732112559\n",
      "15 Train Loss 0.02373841 Test MSE 3.821001960770265 Test RE 0.9964401370803536\n",
      "16 Train Loss 0.023732444 Test MSE 3.820127060081136 Test RE 0.9963260523349352\n",
      "17 Train Loss 0.023709025 Test MSE 3.8156060940618635 Test RE 0.9957363219901159\n",
      "18 Train Loss 0.023632629 Test MSE 3.7997906900819918 Test RE 0.9936705525877696\n",
      "19 Train Loss 0.023496544 Test MSE 3.7809461404928406 Test RE 0.9912035024725289\n",
      "20 Train Loss 0.02316607 Test MSE 3.707962770262098 Test RE 0.9815903156811729\n",
      "21 Train Loss 0.022654453 Test MSE 3.550159773682327 Test RE 0.9604760321994598\n",
      "22 Train Loss 0.02239707 Test MSE 3.5134546383051135 Test RE 0.9554979462717789\n",
      "23 Train Loss 0.022134548 Test MSE 3.4308858147037196 Test RE 0.9442037363726669\n",
      "24 Train Loss 0.021056 Test MSE 3.2609808943358733 Test RE 0.9205273780709782\n",
      "25 Train Loss 0.020404855 Test MSE 3.1751571604654583 Test RE 0.9083332158606958\n",
      "26 Train Loss 0.019586742 Test MSE 3.053918898020914 Test RE 0.8908228169834808\n",
      "27 Train Loss 0.018699024 Test MSE 2.8889715807519285 Test RE 0.8664314685369104\n",
      "28 Train Loss 0.017483953 Test MSE 2.586343863737804 Test RE 0.8197958495174676\n",
      "29 Train Loss 0.016643632 Test MSE 2.4611495132572188 Test RE 0.7997082592093467\n",
      "30 Train Loss 0.015641155 Test MSE 2.367650202632094 Test RE 0.7843706823064506\n",
      "31 Train Loss 0.013872519 Test MSE 2.0205686518573933 Test RE 0.7246017656419336\n",
      "32 Train Loss 0.012977208 Test MSE 1.9312672475462807 Test RE 0.7084085108747324\n",
      "33 Train Loss 0.012565108 Test MSE 1.8980472052424333 Test RE 0.7022893575378553\n",
      "34 Train Loss 0.012074806 Test MSE 1.820519631904078 Test RE 0.6877969819749821\n",
      "35 Train Loss 0.011606272 Test MSE 1.68816754403126 Test RE 0.6623237915458441\n",
      "36 Train Loss 0.009777364 Test MSE 1.4324172945147167 Test RE 0.6100948520834706\n",
      "37 Train Loss 0.00917941 Test MSE 1.3618912640865155 Test RE 0.5948860701049871\n",
      "38 Train Loss 0.009000184 Test MSE 1.2768887814736614 Test RE 0.5760220629478656\n",
      "39 Train Loss 0.008568754 Test MSE 1.1128858540930293 Test RE 0.5377592551586494\n",
      "40 Train Loss 0.008221522 Test MSE 1.039207050729014 Test RE 0.5196532225604498\n",
      "41 Train Loss 0.0070402753 Test MSE 0.9573454251770778 Test RE 0.49876608607165\n",
      "42 Train Loss 0.006395442 Test MSE 0.9096158806440081 Test RE 0.4861738543726488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 Train Loss 0.0059292167 Test MSE 0.7660224455506913 Test RE 0.44615249926047224\n",
      "44 Train Loss 0.0054232404 Test MSE 0.6215141745904902 Test RE 0.40187232683955815\n",
      "45 Train Loss 0.005056534 Test MSE 0.577744087727566 Test RE 0.387463089861588\n",
      "46 Train Loss 0.004603532 Test MSE 0.5183238043366495 Test RE 0.36699754560264175\n",
      "47 Train Loss 0.0044585806 Test MSE 0.481209284900807 Test RE 0.3536141065230207\n",
      "48 Train Loss 0.0043001324 Test MSE 0.41253799612468955 Test RE 0.3274119752371264\n",
      "49 Train Loss 0.004152066 Test MSE 0.3684372414297013 Test RE 0.3094171228853416\n",
      "50 Train Loss 0.0038072367 Test MSE 0.3490935930179595 Test RE 0.3011851268066008\n",
      "51 Train Loss 0.003574609 Test MSE 0.3688552630171094 Test RE 0.30959260239124675\n",
      "52 Train Loss 0.003505102 Test MSE 0.4033288593123423 Test RE 0.3237369207580184\n",
      "53 Train Loss 0.003318795 Test MSE 0.4209936587694035 Test RE 0.33075038598804246\n",
      "54 Train Loss 0.0030976315 Test MSE 0.37240055364306457 Test RE 0.31107688504396697\n",
      "55 Train Loss 0.0030077551 Test MSE 0.3350872678176761 Test RE 0.2950812032736039\n",
      "56 Train Loss 0.0028308956 Test MSE 0.2992964792761759 Test RE 0.2788774422351794\n",
      "57 Train Loss 0.0026262081 Test MSE 0.2778860930372332 Test RE 0.2687175238349854\n",
      "58 Train Loss 0.002382461 Test MSE 0.25452475226650373 Test RE 0.25717431647911215\n",
      "59 Train Loss 0.0022316466 Test MSE 0.24041547436530158 Test RE 0.24994461899091056\n",
      "60 Train Loss 0.0020992092 Test MSE 0.2353326255195317 Test RE 0.2472883475975322\n",
      "61 Train Loss 0.0019224633 Test MSE 0.20697216032676802 Test RE 0.23190950499483873\n",
      "62 Train Loss 0.0017152161 Test MSE 0.14287186821614295 Test RE 0.19267969241994257\n",
      "63 Train Loss 0.001360363 Test MSE 0.097667526296843 Test RE 0.15930803345420594\n",
      "64 Train Loss 0.0011983342 Test MSE 0.09288033230831301 Test RE 0.15535472366500278\n",
      "65 Train Loss 0.001129075 Test MSE 0.07428602713112113 Test RE 0.13893643429685235\n",
      "66 Train Loss 0.0010458514 Test MSE 0.052318170397004114 Test RE 0.11659739739873574\n",
      "67 Train Loss 0.00100276 Test MSE 0.04704008320956007 Test RE 0.11055964185268019\n",
      "68 Train Loss 0.0009815703 Test MSE 0.05273205079554681 Test RE 0.11705768025601156\n",
      "69 Train Loss 0.00096643285 Test MSE 0.061838697576998405 Test RE 0.12676307028988\n",
      "70 Train Loss 0.0009406449 Test MSE 0.06861767052549118 Test RE 0.13353052842915292\n",
      "71 Train Loss 0.00086669263 Test MSE 0.061193479725780445 Test RE 0.12610002056782202\n",
      "72 Train Loss 0.00071477285 Test MSE 0.040590147520786664 Test RE 0.1027005802632855\n",
      "73 Train Loss 0.00069026893 Test MSE 0.03364063351845745 Test RE 0.09349635185904265\n",
      "74 Train Loss 0.0006696509 Test MSE 0.025271312304641868 Test RE 0.08103571715891449\n",
      "75 Train Loss 0.0006660683 Test MSE 0.024090289474889764 Test RE 0.07911951052801179\n",
      "76 Train Loss 0.00066188315 Test MSE 0.02260020025731784 Test RE 0.07663351149540229\n",
      "77 Train Loss 0.0006475441 Test MSE 0.02324562815190876 Test RE 0.07772007762673336\n",
      "78 Train Loss 0.0006024893 Test MSE 0.02990842824900966 Test RE 0.08815752273203195\n",
      "79 Train Loss 0.0004886503 Test MSE 0.03458307971791112 Test RE 0.09479696110094696\n",
      "80 Train Loss 0.00046265876 Test MSE 0.03733320700098937 Test RE 0.0984941035284307\n",
      "81 Train Loss 0.00043550465 Test MSE 0.03913537597793797 Test RE 0.1008433670290694\n",
      "82 Train Loss 0.00040323415 Test MSE 0.033714820973497955 Test RE 0.09359938851094672\n",
      "83 Train Loss 0.0003882912 Test MSE 0.025901485564141975 Test RE 0.0820398616430731\n",
      "84 Train Loss 0.00038482604 Test MSE 0.02282306354826413 Test RE 0.0770104306970898\n",
      "85 Train Loss 0.00038095674 Test MSE 0.02142473028420966 Test RE 0.07461398992660259\n",
      "86 Train Loss 0.00036663178 Test MSE 0.021045834433534936 Test RE 0.07395127349766897\n",
      "87 Train Loss 0.00025412388 Test MSE 0.01633280862362358 Test RE 0.0651467896643943\n",
      "88 Train Loss 0.00016354729 Test MSE 0.007911225322689347 Test RE 0.04534030732503079\n",
      "89 Train Loss 8.630449e-05 Test MSE 0.0004674932130575697 Test RE 0.011021741922546398\n",
      "90 Train Loss 5.485942e-05 Test MSE 4.78743443026429e-06 Test RE 0.0011153572688395769\n",
      "91 Train Loss 4.1668314e-05 Test MSE 5.747965208344029e-07 Test RE 0.0003864734356172577\n",
      "92 Train Loss 3.7317845e-05 Test MSE 0.00017486316666436046 Test RE 0.006740804814875117\n",
      "93 Train Loss 2.825282e-05 Test MSE 0.0007017260693333815 Test RE 0.013503501220743128\n",
      "94 Train Loss 1.8042578e-05 Test MSE 0.0004186805202403371 Test RE 0.010430471964492462\n",
      "95 Train Loss 1.0798631e-05 Test MSE 5.538189854841651e-05 Test RE 0.0037935560257170986\n",
      "96 Train Loss 8.536865e-06 Test MSE 1.6933302221900106e-07 Test RE 0.00020976518641480175\n",
      "97 Train Loss 7.5023754e-06 Test MSE 4.985385462529572e-09 Test RE 3.599249508359021e-05\n",
      "98 Train Loss 7.2722046e-06 Test MSE 9.454648497117795e-07 Test RE 0.0004956616020110354\n",
      "99 Train Loss 7.236347e-06 Test MSE 1.711291456984608e-06 Test RE 0.0006668445001959928\n",
      "Training time: 120.67\n",
      "Training time: 120.67\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 0.044612784 Test MSE 3.882377777378163 Test RE 1.0044110425903134\n",
      "1 Train Loss 0.042832643 Test MSE 3.862874269268168 Test RE 1.0018849870997564\n",
      "2 Train Loss 0.04080109 Test MSE 3.8393222542077026 Test RE 0.9988260618462905\n",
      "3 Train Loss 0.03497523 Test MSE 3.807301141338038 Test RE 0.9946520843094635\n",
      "4 Train Loss 0.034812186 Test MSE 3.806162074269659 Test RE 0.9945032833419082\n",
      "5 Train Loss 0.025915135 Test MSE 3.834249911809566 Test RE 0.9981660414150324\n",
      "6 Train Loss 0.02427077 Test MSE 3.8423646454478546 Test RE 0.9992217329681509\n",
      "7 Train Loss 0.024198249 Test MSE 3.841888167472945 Test RE 0.9991597760805635\n",
      "8 Train Loss 0.023884457 Test MSE 3.8347857619450254 Test RE 0.998235787615033\n",
      "9 Train Loss 0.023831544 Test MSE 3.834187188974858 Test RE 0.9981578770985284\n",
      "10 Train Loss 0.023829829 Test MSE 3.834237723118296 Test RE 0.9981644548795087\n",
      "11 Train Loss 0.023824532 Test MSE 3.8347960941703154 Test RE 0.9982371324085552\n",
      "12 Train Loss 0.02381064 Test MSE 3.836088826683053 Test RE 0.9984053740605536\n",
      "13 Train Loss 0.02380528 Test MSE 3.8365662820403417 Test RE 0.9984675049423226\n",
      "14 Train Loss 0.02380363 Test MSE 3.8366269860409608 Test RE 0.9984754040280974\n",
      "15 Train Loss 0.023798741 Test MSE 3.8357111534778907 Test RE 0.9983562250110821\n",
      "16 Train Loss 0.023794953 Test MSE 3.8339976029438474 Test RE 0.9981331992306738\n",
      "17 Train Loss 0.023792155 Test MSE 3.831636282019593 Test RE 0.9978257817432055\n",
      "18 Train Loss 0.023775658 Test MSE 3.8285173572171294 Test RE 0.9974195875061089\n",
      "19 Train Loss 0.023767188 Test MSE 3.827899338435883 Test RE 0.9973390799807499\n",
      "20 Train Loss 0.023754813 Test MSE 3.8233562194423096 Test RE 0.9967470613667094\n",
      "21 Train Loss 0.023749106 Test MSE 3.8212590103941237 Test RE 0.9964736531917668\n",
      "22 Train Loss 0.023738299 Test MSE 3.8189902617222575 Test RE 0.9961777973060193\n",
      "23 Train Loss 0.023719026 Test MSE 3.8174048559456937 Test RE 0.9959710004995954\n",
      "24 Train Loss 0.023712108 Test MSE 3.8173396751429336 Test RE 0.9959624975415776\n",
      "25 Train Loss 0.02370334 Test MSE 3.812749004113947 Test RE 0.9953634531816223\n",
      "26 Train Loss 0.023695119 Test MSE 3.807252153085711 Test RE 0.9946456852327666\n",
      "27 Train Loss 0.02366973 Test MSE 3.8050412303090617 Test RE 0.9943568411972208\n",
      "28 Train Loss 0.023650492 Test MSE 3.804255907405142 Test RE 0.9942542231898185\n",
      "29 Train Loss 0.023600409 Test MSE 3.797085319747249 Test RE 0.993316753428965\n",
      "30 Train Loss 0.023498088 Test MSE 3.7688482390265587 Test RE 0.9896164539494987\n",
      "31 Train Loss 0.023462776 Test MSE 3.750739508597476 Test RE 0.9872361145369455\n",
      "32 Train Loss 0.023420624 Test MSE 3.7374090162821294 Test RE 0.9854801864701722\n",
      "33 Train Loss 0.023399191 Test MSE 3.7382453034605474 Test RE 0.9855904364245132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 Train Loss 0.023398886 Test MSE 3.738275459108865 Test RE 0.985594411693153\n",
      "35 Train Loss 0.023396768 Test MSE 3.738718007921166 Test RE 0.9856527488506124\n",
      "36 Train Loss 0.023394344 Test MSE 3.7387594925287204 Test RE 0.9856582172095469\n",
      "37 Train Loss 0.02338966 Test MSE 3.7378614653661186 Test RE 0.9855398355629039\n",
      "38 Train Loss 0.023375968 Test MSE 3.7290517798155003 Test RE 0.9843777515816893\n",
      "39 Train Loss 0.0233604 Test MSE 3.714986784867023 Test RE 0.9825195918174119\n",
      "40 Train Loss 0.023315052 Test MSE 3.7009964738472134 Test RE 0.9806678059002225\n",
      "41 Train Loss 0.023257017 Test MSE 3.703418184047438 Test RE 0.9809885985401878\n",
      "42 Train Loss 0.023156106 Test MSE 3.6965191434253892 Test RE 0.9800744383321335\n",
      "43 Train Loss 0.023119627 Test MSE 3.6833759821755847 Test RE 0.9783305346367528\n",
      "44 Train Loss 0.022943785 Test MSE 3.618322265511679 Test RE 0.9696526860698154\n",
      "45 Train Loss 0.022592144 Test MSE 3.555127474021522 Test RE 0.9611477891702198\n",
      "46 Train Loss 0.02206273 Test MSE 3.4276801852424392 Test RE 0.9437625274809036\n",
      "47 Train Loss 0.021691246 Test MSE 3.3228072670969158 Test RE 0.9292127459016576\n",
      "48 Train Loss 0.021446016 Test MSE 3.266701165963569 Test RE 0.9213343989698752\n",
      "49 Train Loss 0.021071598 Test MSE 3.2373121718433735 Test RE 0.9171806265010451\n",
      "50 Train Loss 0.020830853 Test MSE 3.240588417581555 Test RE 0.9176446146927593\n",
      "51 Train Loss 0.020734077 Test MSE 3.2427853208595225 Test RE 0.9179556129812786\n",
      "52 Train Loss 0.020432355 Test MSE 3.1771855854581674 Test RE 0.908623310418714\n",
      "53 Train Loss 0.019428877 Test MSE 2.953574007037818 Test RE 0.8760653654874304\n",
      "54 Train Loss 0.018858751 Test MSE 2.762527859466967 Test RE 0.8472584633995629\n",
      "55 Train Loss 0.018560322 Test MSE 2.6498925134163547 Test RE 0.8298062692155068\n",
      "56 Train Loss 0.018339194 Test MSE 2.549616239739493 Test RE 0.8139542420093163\n",
      "57 Train Loss 0.018091356 Test MSE 2.417439773042481 Test RE 0.7925750815778057\n",
      "58 Train Loss 0.018060563 Test MSE 2.3841482332444732 Test RE 0.7870987261080012\n",
      "59 Train Loss 0.017873818 Test MSE 2.2425893715387946 Test RE 0.7633741726253837\n",
      "60 Train Loss 0.016758664 Test MSE 2.0046158097484383 Test RE 0.7217356506521454\n",
      "61 Train Loss 0.01619457 Test MSE 1.8438774792278787 Test RE 0.6921952464414273\n",
      "62 Train Loss 0.016116343 Test MSE 1.8175567123435379 Test RE 0.6872370547587958\n",
      "63 Train Loss 0.01599959 Test MSE 1.779823477190842 Test RE 0.6800659770224583\n",
      "64 Train Loss 0.015850091 Test MSE 1.732382970122548 Test RE 0.6709413148492974\n",
      "65 Train Loss 0.015614321 Test MSE 1.6433225245098046 Test RE 0.6534674903544676\n",
      "66 Train Loss 0.014611928 Test MSE 1.563400977769691 Test RE 0.6373790317888408\n",
      "67 Train Loss 0.013788808 Test MSE 1.6140208131119353 Test RE 0.647615370723133\n",
      "68 Train Loss 0.01346067 Test MSE 1.6908030046821378 Test RE 0.662840578986764\n",
      "69 Train Loss 0.013207709 Test MSE 1.7488663636396566 Test RE 0.674125716673119\n",
      "70 Train Loss 0.013038771 Test MSE 1.7413036467672094 Test RE 0.6726665584389653\n",
      "71 Train Loss 0.012948659 Test MSE 1.7032229106704735 Test RE 0.6652705944829101\n",
      "72 Train Loss 0.012829102 Test MSE 1.650841099501062 Test RE 0.654960665731704\n",
      "73 Train Loss 0.012144025 Test MSE 1.6388374725297246 Test RE 0.6525751402213031\n",
      "74 Train Loss 0.011580794 Test MSE 1.6578772523922538 Test RE 0.6563549560056219\n",
      "75 Train Loss 0.010857409 Test MSE 1.5000680069008658 Test RE 0.6243355388530063\n",
      "76 Train Loss 0.00955847 Test MSE 1.3471106426859363 Test RE 0.5916491108089154\n",
      "77 Train Loss 0.008958163 Test MSE 1.25163393541039 Test RE 0.5702972105175979\n",
      "78 Train Loss 0.008798257 Test MSE 1.1559013900640958 Test RE 0.5480535251510953\n",
      "79 Train Loss 0.00870391 Test MSE 1.135815863151507 Test RE 0.5432710307867956\n",
      "80 Train Loss 0.008677691 Test MSE 1.1418755505518206 Test RE 0.5447183047446872\n",
      "81 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "82 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "83 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "84 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "85 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "86 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "87 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "88 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "89 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "90 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "91 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "92 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "93 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "94 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "95 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "96 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "97 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "98 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "99 Train Loss 0.008673724 Test MSE 1.1430736516691555 Test RE 0.5450039998488035\n",
      "Training time: 101.82\n",
      "Training time: 101.82\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 0.10381869 Test MSE 4.03032249552193 Test RE 1.0233695294564404\n",
      "1 Train Loss 0.049361557 Test MSE 3.901237057188246 Test RE 1.0068476319250512\n",
      "2 Train Loss 0.043875724 Test MSE 3.8765331566862242 Test RE 1.0036547261106457\n",
      "3 Train Loss 0.04370038 Test MSE 3.870483474315531 Test RE 1.002871273112374\n",
      "4 Train Loss 0.043678824 Test MSE 3.868468711292112 Test RE 1.0026102190364414\n",
      "5 Train Loss 0.04325841 Test MSE 3.858966533465532 Test RE 1.0013780986787453\n",
      "6 Train Loss 0.028056785 Test MSE 3.8343388377344354 Test RE 0.9981776163417843\n",
      "7 Train Loss 0.023948459 Test MSE 3.834032591851125 Test RE 0.9981377536819146\n",
      "8 Train Loss 0.02394013 Test MSE 3.833782405518937 Test RE 0.9981051868620479\n",
      "9 Train Loss 0.023901748 Test MSE 3.834817966510835 Test RE 0.9982399792028579\n",
      "10 Train Loss 0.023839312 Test MSE 3.83721597813335 Test RE 0.9985520431632678\n",
      "11 Train Loss 0.023835815 Test MSE 3.837574812927121 Test RE 0.9985987315490672\n",
      "12 Train Loss 0.023832178 Test MSE 3.837647226941582 Test RE 0.9986081531506495\n",
      "13 Train Loss 0.023812799 Test MSE 3.835295372231658 Test RE 0.9983021139182101\n",
      "14 Train Loss 0.023808168 Test MSE 3.835035249803253 Test RE 0.9982682592671075\n",
      "15 Train Loss 0.023805495 Test MSE 3.8357913860866444 Test RE 0.9983666664002178\n",
      "16 Train Loss 0.023801375 Test MSE 3.8360125689124276 Test RE 0.9983954503400888\n",
      "17 Train Loss 0.023778046 Test MSE 3.830137254882588 Test RE 0.9976305760740942\n",
      "18 Train Loss 0.023734147 Test MSE 3.8219984440776478 Test RE 0.9965700599703216\n",
      "19 Train Loss 0.023679968 Test MSE 3.8080444766718573 Test RE 0.994749177227405\n",
      "20 Train Loss 0.02366458 Test MSE 3.805262811291413 Test RE 0.9943857932301663\n",
      "21 Train Loss 0.023624025 Test MSE 3.7977590285700327 Test RE 0.9934048705665751\n",
      "22 Train Loss 0.023446832 Test MSE 3.7544232480185595 Test RE 0.9877207960355696\n",
      "23 Train Loss 0.02322182 Test MSE 3.7159078032138906 Test RE 0.9826413772426087\n",
      "24 Train Loss 0.022915665 Test MSE 3.671975082818709 Test RE 0.9768152817363834\n",
      "25 Train Loss 0.022798851 Test MSE 3.657874411497289 Test RE 0.9749379542166181\n",
      "26 Train Loss 0.022703834 Test MSE 3.640017740200085 Test RE 0.972555362138579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 Train Loss 0.022387372 Test MSE 3.5828233996372414 Test RE 0.9648843969080166\n",
      "28 Train Loss 0.022341736 Test MSE 3.574980488445489 Test RE 0.963827737438094\n",
      "29 Train Loss 0.02220075 Test MSE 3.5335884583619084 Test RE 0.9582317715000486\n",
      "30 Train Loss 0.021907806 Test MSE 3.503289207091226 Test RE 0.9541146803001241\n",
      "31 Train Loss 0.020817373 Test MSE 3.270825634569768 Test RE 0.9219158442740453\n",
      "32 Train Loss 0.02052193 Test MSE 3.239400288652434 Test RE 0.9174763767177022\n",
      "33 Train Loss 0.020020934 Test MSE 3.1245914566975 Test RE 0.9010713950469104\n",
      "34 Train Loss 0.01903764 Test MSE 2.9619290590256333 Test RE 0.8773035945913333\n",
      "35 Train Loss 0.018506583 Test MSE 2.902289984878842 Test RE 0.8684263333514858\n",
      "36 Train Loss 0.017922174 Test MSE 2.7576157451556265 Test RE 0.8465048633738753\n",
      "37 Train Loss 0.01742142 Test MSE 2.631442608083112 Test RE 0.8269124557956857\n",
      "38 Train Loss 0.017111335 Test MSE 2.5522336202287805 Test RE 0.8143719286863826\n",
      "39 Train Loss 0.016866794 Test MSE 2.501077815269161 Test RE 0.8061691680477487\n",
      "40 Train Loss 0.01586989 Test MSE 2.417353226033331 Test RE 0.792560893919946\n",
      "41 Train Loss 0.014929258 Test MSE 2.272571664624326 Test RE 0.7684601942400902\n",
      "42 Train Loss 0.013546425 Test MSE 2.02323893750674 Test RE 0.7250804068529425\n",
      "43 Train Loss 0.01185309 Test MSE 1.7878179860724555 Test RE 0.6815916064084176\n",
      "44 Train Loss 0.011241164 Test MSE 1.7392235866942058 Test RE 0.6722646742099231\n",
      "45 Train Loss 0.010784883 Test MSE 1.6439424050232023 Test RE 0.6535907265315168\n",
      "46 Train Loss 0.010526262 Test MSE 1.5738701310029268 Test RE 0.6395095424935475\n",
      "47 Train Loss 0.010295087 Test MSE 1.532413209535787 Test RE 0.6310307546201585\n",
      "48 Train Loss 0.009875924 Test MSE 1.4771109626709045 Test RE 0.6195397029752568\n",
      "49 Train Loss 0.009792003 Test MSE 1.4826416560086844 Test RE 0.6206984793873549\n",
      "50 Train Loss 0.009356917 Test MSE 1.416734958710559 Test RE 0.6067459523132152\n",
      "51 Train Loss 0.008926064 Test MSE 1.3445573836176563 Test RE 0.591088150971217\n",
      "52 Train Loss 0.008692037 Test MSE 1.2748517548696687 Test RE 0.575562414218489\n",
      "53 Train Loss 0.008400333 Test MSE 1.1770641020274173 Test RE 0.5530477629278406\n",
      "54 Train Loss 0.008076016 Test MSE 1.150079277444953 Test RE 0.546671548512492\n",
      "55 Train Loss 0.007753946 Test MSE 1.1139402485060184 Test RE 0.5380139426063176\n",
      "56 Train Loss 0.0073423814 Test MSE 1.037384951202833 Test RE 0.5191974542418407\n",
      "57 Train Loss 0.007202634 Test MSE 1.0039746613200655 Test RE 0.5107683269456061\n",
      "58 Train Loss 0.007034306 Test MSE 0.9845953729065277 Test RE 0.505814736221769\n",
      "59 Train Loss 0.0064672916 Test MSE 0.8598536633893349 Test RE 0.47268830296178077\n",
      "60 Train Loss 0.004717368 Test MSE 0.6282509580686211 Test RE 0.4040444655197335\n",
      "61 Train Loss 0.0042754575 Test MSE 0.6053413382457031 Test RE 0.39660916770347066\n",
      "62 Train Loss 0.003320269 Test MSE 0.465577895475272 Test RE 0.347823370308515\n",
      "63 Train Loss 0.0027740279 Test MSE 0.31307887399338086 Test RE 0.28522623162723776\n",
      "64 Train Loss 0.001986194 Test MSE 0.20247085754122462 Test RE 0.22937381802654797\n",
      "65 Train Loss 0.0016514119 Test MSE 0.16776890523355686 Test RE 0.208794145861016\n",
      "66 Train Loss 0.001516391 Test MSE 0.12574873772865533 Test RE 0.18076502254443416\n",
      "67 Train Loss 0.0012840992 Test MSE 0.09043164098349425 Test RE 0.15329316404740462\n",
      "68 Train Loss 0.00092634046 Test MSE 0.0644572706827203 Test RE 0.12941914865992607\n",
      "69 Train Loss 0.0008966394 Test MSE 0.06295274965110954 Test RE 0.12789982036657777\n",
      "70 Train Loss 0.0008812996 Test MSE 0.06572697936807965 Test RE 0.13068761111286845\n",
      "71 Train Loss 0.0008749779 Test MSE 0.06634743523390058 Test RE 0.13130300096489222\n",
      "72 Train Loss 0.00083463854 Test MSE 0.05623507815279832 Test RE 0.12088327960482836\n",
      "73 Train Loss 0.00075327035 Test MSE 0.031050678540406215 Test RE 0.08982518701750153\n",
      "74 Train Loss 0.0005890176 Test MSE 0.019358033542302634 Test RE 0.0709239968018253\n",
      "75 Train Loss 0.0004293649 Test MSE 0.013515245726526303 Test RE 0.05926176140456137\n",
      "76 Train Loss 0.00026274406 Test MSE 0.007393024893752875 Test RE 0.043830221641328944\n",
      "77 Train Loss 0.00013336068 Test MSE 0.001874789299670231 Test RE 0.0220718540439858\n",
      "78 Train Loss 0.000105798325 Test MSE 0.0006557338131386848 Test RE 0.013053481918400951\n",
      "79 Train Loss 0.000100424855 Test MSE 0.0006659873817266765 Test RE 0.013155143290171355\n",
      "80 Train Loss 9.730687e-05 Test MSE 0.0007589780513498345 Test RE 0.01404355925193373\n",
      "81 Train Loss 8.756456e-05 Test MSE 0.0003432596459142668 Test RE 0.009444390976865386\n",
      "82 Train Loss 8.175882e-05 Test MSE 6.71736424730489e-05 Test RE 0.004177938393706242\n",
      "83 Train Loss 7.851331e-05 Test MSE 4.8119342742624244e-05 Test RE 0.0035360828008422123\n",
      "84 Train Loss 6.725356e-05 Test MSE 0.00022725294599330048 Test RE 0.007684530585408346\n",
      "85 Train Loss 5.3913143e-05 Test MSE 0.00022101263920170212 Test RE 0.007578288558332994\n",
      "86 Train Loss 4.4627817e-05 Test MSE 4.490093143630027e-06 Test RE 0.0010801653896661545\n",
      "87 Train Loss 3.4449684e-05 Test MSE 2.259706873751931e-09 Test RE 2.4231965157859618e-05\n",
      "88 Train Loss 2.7386792e-05 Test MSE 4.601550848580257e-05 Test RE 0.003457918039288263\n",
      "89 Train Loss 2.4988936e-05 Test MSE 1.9837640533227306e-05 Test RE 0.002270427250595081\n",
      "90 Train Loss 2.48316e-05 Test MSE 1.6910374731015975e-05 Test RE 0.002096231285219161\n",
      "91 Train Loss 2.4544863e-05 Test MSE 1.9461352613352995e-05 Test RE 0.002248790993853491\n",
      "92 Train Loss 2.4250008e-05 Test MSE 2.7421009566263584e-05 Test RE 0.002669342509363904\n",
      "93 Train Loss 2.4060433e-05 Test MSE 2.9715084746402176e-05 Test RE 0.00277876019519114\n",
      "94 Train Loss 2.377223e-05 Test MSE 3.213367645614589e-05 Test RE 0.00288963368175847\n",
      "95 Train Loss 2.2367869e-05 Test MSE 4.0221419829036255e-05 Test RE 0.0032328926299121596\n",
      "96 Train Loss 2.0882944e-05 Test MSE 4.9881597366885605e-05 Test RE 0.0036002508267330134\n",
      "97 Train Loss 2.0024981e-05 Test MSE 5.0640831673504326e-05 Test RE 0.0036275465758775103\n",
      "98 Train Loss 1.9403413e-05 Test MSE 2.0172728378933874e-05 Test RE 0.002289522432066437\n",
      "99 Train Loss 1.9060097e-05 Test MSE 2.828652288165391e-06 Test RE 0.0008573385756457238\n",
      "Training time: 114.94\n",
      "Training time: 114.94\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 0.068456024 Test MSE 3.7993352162731613 Test RE 0.9936109960869193\n",
      "1 Train Loss 0.04423514 Test MSE 3.871198819751527 Test RE 1.0029639445122611\n",
      "2 Train Loss 0.044185475 Test MSE 3.87531989586425 Test RE 1.0034976540309142\n",
      "3 Train Loss 0.043780494 Test MSE 3.869656444765957 Test RE 1.0027641226112192\n",
      "4 Train Loss 0.04375347 Test MSE 3.8690638166683122 Test RE 1.0026873342764944\n",
      "5 Train Loss 0.043639567 Test MSE 3.867619137876708 Test RE 1.0025001189137166\n",
      "6 Train Loss 0.043509774 Test MSE 3.871712586477518 Test RE 1.0030304965586043\n",
      "7 Train Loss 0.043091416 Test MSE 3.8662516390901773 Test RE 1.0023228730617213\n",
      "8 Train Loss 0.042360086 Test MSE 3.8637412043929578 Test RE 1.0019974060523988\n",
      "9 Train Loss 0.040648673 Test MSE 3.851668487131812 Test RE 1.0004307514776012\n",
      "10 Train Loss 0.036224056 Test MSE 3.833770724319213 Test RE 0.9981036662912797\n",
      "11 Train Loss 0.029380055 Test MSE 3.8084400822832634 Test RE 0.9948008465472871\n",
      "12 Train Loss 0.028824452 Test MSE 3.801127762998028 Test RE 0.9938453639437357\n",
      "13 Train Loss 0.026767015 Test MSE 3.828966053103943 Test RE 0.997478033754705\n",
      "14 Train Loss 0.02422231 Test MSE 3.829386783305422 Test RE 0.9975328341369128\n",
      "15 Train Loss 0.0238759 Test MSE 3.835082577432664 Test RE 0.9982744189920363\n",
      "16 Train Loss 0.023828896 Test MSE 3.8371308955567973 Test RE 0.9985409726564087\n",
      "17 Train Loss 0.023828397 Test MSE 3.8371847920367927 Test RE 0.9985479854031775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 Train Loss 0.023828397 Test MSE 3.8371847920367927 Test RE 0.9985479854031775\n",
      "19 Train Loss 0.023828397 Test MSE 3.8371847920367927 Test RE 0.9985479854031775\n",
      "20 Train Loss 0.023828397 Test MSE 3.8371847920367927 Test RE 0.9985479854031775\n",
      "21 Train Loss 0.023828397 Test MSE 3.8371847920367927 Test RE 0.9985479854031775\n",
      "22 Train Loss 0.023828397 Test MSE 3.837187541830998 Test RE 0.99854834319165\n",
      "23 Train Loss 0.023828322 Test MSE 3.837197149182228 Test RE 0.9985495932476538\n",
      "24 Train Loss 0.023828322 Test MSE 3.837197149182228 Test RE 0.9985495932476538\n",
      "25 Train Loss 0.023828322 Test MSE 3.837197149182228 Test RE 0.9985495932476538\n",
      "26 Train Loss 0.023828322 Test MSE 3.837197149182228 Test RE 0.9985495932476538\n",
      "27 Train Loss 0.023828322 Test MSE 3.837197149182228 Test RE 0.9985495932476538\n",
      "28 Train Loss 0.023828322 Test MSE 3.837197149182228 Test RE 0.9985495932476538\n",
      "29 Train Loss 0.023828322 Test MSE 3.837197149182228 Test RE 0.9985495932476538\n",
      "30 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "31 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "32 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "33 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "34 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "35 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "36 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "37 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "38 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "39 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "40 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "41 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "42 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "43 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "44 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "45 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "46 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "47 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "48 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "49 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "50 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "51 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "52 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "53 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "54 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "55 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "56 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "57 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "58 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "59 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "60 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "61 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "62 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "63 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "64 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "65 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "66 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "67 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "68 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "69 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "70 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "71 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "72 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "73 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "74 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "75 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "76 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "77 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "78 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "79 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "80 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "81 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "82 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "83 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "84 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "85 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "86 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "87 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "88 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "89 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "90 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "91 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "92 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "93 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "94 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "95 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "96 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "97 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "98 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "99 Train Loss 0.0238283 Test MSE 3.8371969952486156 Test RE 0.9985495732186661\n",
      "Training time: 51.97\n",
      "Training time: 51.97\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 8.0\n",
    "rowdy_terms = 2\n",
    "\n",
    "for reps in range(max_reps):\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    alpha_val = []\n",
    "    omega_val = []\n",
    "\n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "\n",
    "\n",
    "\n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.05, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-9, \n",
    "                              tolerance_change = 1e-9, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)  \n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"alpha\": alpha_full, \"omega\": omega_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_tune[tune_reps,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tune_reps in range(75):\n",
    "    label = \"1D_SODE_rowdy_tune\"+str(tune_reps)+\".mat\" #WRONGLY SAVED AS STAN - DOESN'T MATTER\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44299637617173226\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
