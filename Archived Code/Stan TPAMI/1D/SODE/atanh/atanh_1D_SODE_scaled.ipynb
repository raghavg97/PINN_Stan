{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y/50\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"1D_SODE_atanh_scaled\"\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0/50.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(3.0)\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 28932.719 Test MSE 1.2513083475211464 Test RE 2.8511151495293943\n",
      "1 Train Loss 6420.2207 Test MSE 1.388448494353748 Test RE 3.0032914091182286\n",
      "2 Train Loss 2729.6077 Test MSE 1.597508223969185 Test RE 3.22147034738217\n",
      "3 Train Loss 1014.9164 Test MSE 1.6163467122108437 Test RE 3.240409143452295\n",
      "4 Train Loss 466.13693 Test MSE 1.3640224568122574 Test RE 2.976756750017905\n",
      "5 Train Loss 149.36345 Test MSE 1.0874531436185506 Test RE 2.657895264597951\n",
      "6 Train Loss 77.64294 Test MSE 1.0596255277494777 Test RE 2.623667484782725\n",
      "7 Train Loss 45.26196 Test MSE 1.1129839772444508 Test RE 2.688914808771205\n",
      "8 Train Loss 36.41607 Test MSE 1.1249328127012495 Test RE 2.703310176056638\n",
      "9 Train Loss 32.098766 Test MSE 1.1123854933384165 Test RE 2.6881917576460412\n",
      "10 Train Loss 29.12054 Test MSE 1.1235689393377875 Test RE 2.7016709266259133\n",
      "11 Train Loss 25.551699 Test MSE 1.1033760067454377 Test RE 2.677283458889316\n",
      "12 Train Loss 22.340683 Test MSE 1.0821353880136353 Test RE 2.651388612279599\n",
      "13 Train Loss 16.274336 Test MSE 1.0865626831890665 Test RE 2.656806833673682\n",
      "14 Train Loss 11.671859 Test MSE 0.9797273286935824 Test RE 2.522813800957389\n",
      "15 Train Loss 7.7533884 Test MSE 0.8285002212861938 Test RE 2.3199514756760813\n",
      "16 Train Loss 5.765421 Test MSE 0.8891601737720893 Test RE 2.4033808028142007\n",
      "17 Train Loss 3.8714447 Test MSE 0.7850281337269597 Test RE 2.258266528003992\n",
      "18 Train Loss 2.8265505 Test MSE 0.8382438392713497 Test RE 2.333553552850032\n",
      "19 Train Loss 1.7048906 Test MSE 0.6889698008130807 Test RE 2.1155957210127347\n",
      "20 Train Loss 1.0938692 Test MSE 0.6333479632324515 Test RE 2.02840081321511\n",
      "21 Train Loss 0.6280372 Test MSE 0.5222241638408045 Test RE 1.8418788817321654\n",
      "22 Train Loss 0.25079268 Test MSE 0.4350939821837554 Test RE 1.681218363930786\n",
      "23 Train Loss 0.12621166 Test MSE 0.425281363535286 Test RE 1.6621521003233324\n",
      "24 Train Loss 0.05730682 Test MSE 0.4373397604258825 Test RE 1.6855516622818272\n",
      "25 Train Loss 0.03987715 Test MSE 0.42567427182010775 Test RE 1.6629197363125618\n",
      "26 Train Loss 0.031197004 Test MSE 0.43374276086984914 Test RE 1.6786057507234686\n",
      "27 Train Loss 0.022935102 Test MSE 0.45227738442557874 Test RE 1.7140955318168167\n",
      "28 Train Loss 0.019227972 Test MSE 0.455144534325636 Test RE 1.7195200840773035\n",
      "29 Train Loss 0.015908435 Test MSE 0.4706548590131364 Test RE 1.7485733708648232\n",
      "30 Train Loss 0.013227756 Test MSE 0.467223554676948 Test RE 1.7421877330148934\n",
      "31 Train Loss 0.011781841 Test MSE 0.4748939709943516 Test RE 1.7564302774297897\n",
      "32 Train Loss 0.011683976 Test MSE 0.4746263851111617 Test RE 1.7559353646693396\n",
      "33 Train Loss 0.010012331 Test MSE 0.4830720979494034 Test RE 1.7714894229620692\n",
      "34 Train Loss 0.008910833 Test MSE 0.4856325052289798 Test RE 1.7761778951038745\n",
      "35 Train Loss 0.008851415 Test MSE 0.4851768990978861 Test RE 1.7753445206694969\n",
      "36 Train Loss 0.008791161 Test MSE 0.48432946970585666 Test RE 1.773793399160199\n",
      "37 Train Loss 0.008737157 Test MSE 0.4832534708578003 Test RE 1.7718219510021298\n",
      "38 Train Loss 0.0086946525 Test MSE 0.4821158899602733 Test RE 1.7697352837533655\n",
      "39 Train Loss 0.008631284 Test MSE 0.48012899999902836 Test RE 1.7660848133703493\n",
      "40 Train Loss 0.00776444 Test MSE 0.4761193897035391 Test RE 1.758694968025584\n",
      "41 Train Loss 0.0068657245 Test MSE 0.47017686745272 Test RE 1.747685229996975\n",
      "42 Train Loss 0.006020953 Test MSE 0.4676661879222154 Test RE 1.7430127852279822\n",
      "43 Train Loss 0.0055528115 Test MSE 0.4616022677217721 Test RE 1.7316756642602462\n",
      "44 Train Loss 0.004947656 Test MSE 0.4619369795773008 Test RE 1.732303377101855\n",
      "45 Train Loss 0.0049256664 Test MSE 0.46201613028773586 Test RE 1.732451781728605\n",
      "46 Train Loss 0.004909641 Test MSE 0.4619096109527349 Test RE 1.7322520589944927\n",
      "47 Train Loss 0.004900695 Test MSE 0.4619368677668443 Test RE 1.7323031674524314\n",
      "48 Train Loss 0.0048876507 Test MSE 0.46198532472421194 Test RE 1.7323940239583016\n",
      "49 Train Loss 0.004867847 Test MSE 0.4619714391486913 Test RE 1.7323679890741615\n",
      "50 Train Loss 0.0048455447 Test MSE 0.4622705832159989 Test RE 1.7329287853664739\n",
      "51 Train Loss 0.004826122 Test MSE 0.4622321495678521 Test RE 1.7328567451350254\n",
      "52 Train Loss 0.004807898 Test MSE 0.4623892030512232 Test RE 1.733151108124335\n",
      "53 Train Loss 0.004786511 Test MSE 0.462476495845713 Test RE 1.7333146980857\n",
      "54 Train Loss 0.0047580656 Test MSE 0.4624058888401131 Test RE 1.7331823791103627\n",
      "55 Train Loss 0.004721411 Test MSE 0.46224314660409604 Test RE 1.7328773583432455\n",
      "56 Train Loss 0.004356159 Test MSE 0.4565237545068076 Test RE 1.722123436071421\n",
      "57 Train Loss 0.004347034 Test MSE 0.4561817005211159 Test RE 1.7214781579670524\n",
      "58 Train Loss 0.0043388065 Test MSE 0.45587206101515165 Test RE 1.7208938204891433\n",
      "59 Train Loss 0.0043314416 Test MSE 0.45559930568444706 Test RE 1.720378924716786\n",
      "60 Train Loss 0.0043259677 Test MSE 0.4554284741404689 Test RE 1.720056357782249\n",
      "61 Train Loss 0.0043207267 Test MSE 0.4552893679292845 Test RE 1.7197936504527254\n",
      "62 Train Loss 0.0043159835 Test MSE 0.45513212055544505 Test RE 1.7194966345232585\n",
      "63 Train Loss 0.004310359 Test MSE 0.45494683539689773 Test RE 1.719146593706379\n",
      "64 Train Loss 0.004302337 Test MSE 0.45461425263295835 Test RE 1.718518099326855\n",
      "65 Train Loss 0.0042948835 Test MSE 0.45415716462729444 Test RE 1.717653947408416\n",
      "66 Train Loss 0.0042830124 Test MSE 0.45383434514319326 Test RE 1.7170433760313142\n",
      "67 Train Loss 0.0041019158 Test MSE 0.44534248136804444 Test RE 1.7009033995768197\n",
      "68 Train Loss 0.004095001 Test MSE 0.4449974563625188 Test RE 1.700244392252765\n",
      "69 Train Loss 0.004090986 Test MSE 0.4446386075747897 Test RE 1.6995587100278353\n",
      "70 Train Loss 0.004086862 Test MSE 0.44431914388227695 Test RE 1.6989480513366384\n",
      "71 Train Loss 0.0040828167 Test MSE 0.4439352298198823 Test RE 1.6982139044826554\n",
      "72 Train Loss 0.004077049 Test MSE 0.4434838512362092 Test RE 1.697350341194556\n",
      "73 Train Loss 0.004069352 Test MSE 0.4428931235548258 Test RE 1.6962195154101456\n",
      "74 Train Loss 0.0040625287 Test MSE 0.4422591712422975 Test RE 1.6950051057660478\n",
      "75 Train Loss 0.0037944925 Test MSE 0.41902191844723286 Test RE 1.6498746787660588\n",
      "76 Train Loss 0.0037835378 Test MSE 0.4185903469196151 Test RE 1.6490248158594913\n",
      "77 Train Loss 0.0037539145 Test MSE 0.4181904696453816 Test RE 1.6482369750614227\n",
      "78 Train Loss 0.003720331 Test MSE 0.42139348566338847 Test RE 1.6545370457796482\n",
      "79 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "80 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "81 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "82 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "83 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "84 Train Loss 0.0036896986 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "85 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "86 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "87 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "88 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "89 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "90 Train Loss 0.0036896986 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "91 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "92 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "93 Train Loss 0.0036896986 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "94 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "95 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "97 Train Loss 0.0036896986 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "98 Train Loss 0.0036896986 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "99 Train Loss 0.0036896989 Test MSE 0.42424938605625095 Test RE 1.660134206377826\n",
      "Training time: 26.26\n",
      "Training time: 26.26\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 16610.041 Test MSE 2.9727432194793666 Test RE 4.394518382285581\n",
      "1 Train Loss 5757.7275 Test MSE 3.2608967701744187 Test RE 4.60257752241031\n",
      "2 Train Loss 2400.831 Test MSE 2.9829619369248217 Test RE 4.402064916492047\n",
      "3 Train Loss 1028.6343 Test MSE 2.6776146478289204 Test RE 4.170677647557073\n",
      "4 Train Loss 601.45636 Test MSE 2.522668850066871 Test RE 4.048206978016869\n",
      "5 Train Loss 389.2341 Test MSE 2.123759475726808 Test RE 3.7143707584365995\n",
      "6 Train Loss 253.01056 Test MSE 1.58823402292322 Test RE 3.2121057472453516\n",
      "7 Train Loss 183.29362 Test MSE 1.5347226088697337 Test RE 3.157530340591927\n",
      "8 Train Loss 146.35881 Test MSE 1.4925685900089263 Test RE 3.113864683273201\n",
      "9 Train Loss 120.363335 Test MSE 1.4004672647995124 Test RE 3.016262035194269\n",
      "10 Train Loss 103.729675 Test MSE 1.317262711809206 Test RE 2.925289053398744\n",
      "11 Train Loss 98.32251 Test MSE 1.3128336659854511 Test RE 2.920367048008197\n",
      "12 Train Loss 88.71778 Test MSE 1.336143916483535 Test RE 2.9461795196539606\n",
      "13 Train Loss 83.28091 Test MSE 1.3648234854138992 Test RE 2.977630678931367\n",
      "14 Train Loss 71.937 Test MSE 1.4008291205039936 Test RE 3.0166516841197697\n",
      "15 Train Loss 65.821335 Test MSE 1.3848869984155145 Test RE 2.9994370788365\n",
      "16 Train Loss 62.72326 Test MSE 1.3610827200227689 Test RE 2.9735472715583886\n",
      "17 Train Loss 58.01935 Test MSE 1.3079982207534813 Test RE 2.9149839205795964\n",
      "18 Train Loss 48.44981 Test MSE 1.156276118090369 Test RE 2.740711769121833\n",
      "19 Train Loss 43.03893 Test MSE 1.0741647741735703 Test RE 2.641605985341247\n",
      "20 Train Loss 38.82416 Test MSE 1.0003905490898455 Test RE 2.5492790500791855\n",
      "21 Train Loss 33.028557 Test MSE 0.8296156460023194 Test RE 2.3215126465474265\n",
      "22 Train Loss 29.17431 Test MSE 0.7362794058390469 Test RE 2.187025835669251\n",
      "23 Train Loss 24.461037 Test MSE 0.7065427390040171 Test RE 2.1424061465775126\n",
      "24 Train Loss 20.069557 Test MSE 0.574573330669879 Test RE 1.9319919781613628\n",
      "25 Train Loss 15.143578 Test MSE 0.4995031609868935 Test RE 1.8013649462515395\n",
      "26 Train Loss 10.679927 Test MSE 0.3587911327849918 Test RE 1.5266990575932595\n",
      "27 Train Loss 7.609883 Test MSE 0.2385674301052233 Test RE 1.2449105947739632\n",
      "28 Train Loss 4.682775 Test MSE 0.1773169208182602 Test RE 1.0732667557176576\n",
      "29 Train Loss 3.6579282 Test MSE 0.15846315465440589 Test RE 1.014604386577105\n",
      "30 Train Loss 2.334953 Test MSE 0.15348771670530156 Test RE 0.9985490426617415\n",
      "31 Train Loss 1.4276419 Test MSE 0.17644052751312514 Test RE 1.070611146440417\n",
      "32 Train Loss 0.9719932 Test MSE 0.1585203853876317 Test RE 1.01478758787461\n",
      "33 Train Loss 0.80985504 Test MSE 0.1532180050098687 Test RE 0.9976713216984561\n",
      "34 Train Loss 0.7551282 Test MSE 0.1569603705996731 Test RE 1.009781929631153\n",
      "35 Train Loss 0.6908737 Test MSE 0.15821147776497047 Test RE 1.0137983507917712\n",
      "36 Train Loss 0.6511488 Test MSE 0.15524446666761466 Test RE 1.0042472513100347\n",
      "37 Train Loss 0.6024626 Test MSE 0.15803431000754428 Test RE 1.0132305579419352\n",
      "38 Train Loss 0.5363274 Test MSE 0.1557553541352154 Test RE 1.0058983115361873\n",
      "39 Train Loss 0.40143934 Test MSE 0.15484828587070157 Test RE 1.0029650232081282\n",
      "40 Train Loss 0.39022982 Test MSE 0.1528756815747834 Test RE 0.9965561875831532\n",
      "41 Train Loss 0.35467762 Test MSE 0.14999069681092245 Test RE 0.9871081740485511\n",
      "42 Train Loss 0.30927634 Test MSE 0.15411552973193804 Test RE 1.0005891484148395\n",
      "43 Train Loss 0.18208413 Test MSE 0.16044284039986878 Test RE 1.0209224589338763\n",
      "44 Train Loss 0.10879774 Test MSE 0.17291185015967372 Test RE 1.0598513705371466\n",
      "45 Train Loss 0.06229236 Test MSE 0.17130992055449826 Test RE 1.0549304876962817\n",
      "46 Train Loss 0.059089646 Test MSE 0.16936717826306003 Test RE 1.0489317057085992\n",
      "47 Train Loss 0.048951443 Test MSE 0.1715312890885411 Test RE 1.0556118639456553\n",
      "48 Train Loss 0.03838148 Test MSE 0.17088949411896898 Test RE 1.0536351945858882\n",
      "49 Train Loss 0.034744706 Test MSE 0.17208343617782648 Test RE 1.0573094688955\n",
      "50 Train Loss 0.03409878 Test MSE 0.17226386413504308 Test RE 1.0578636134818815\n",
      "51 Train Loss 0.033632018 Test MSE 0.17227581996049018 Test RE 1.0579003229024304\n",
      "52 Train Loss 0.028967801 Test MSE 0.17225993926295213 Test RE 1.05785156219346\n",
      "53 Train Loss 0.02749323 Test MSE 0.17296084875251977 Test RE 1.060001526663738\n",
      "54 Train Loss 0.027203122 Test MSE 0.17310856091927174 Test RE 1.0604540617549345\n",
      "55 Train Loss 0.027063766 Test MSE 0.1733069581867299 Test RE 1.0610615734225546\n",
      "56 Train Loss 0.026985265 Test MSE 0.1734263951801079 Test RE 1.0614271334050032\n",
      "57 Train Loss 0.026928993 Test MSE 0.1735523430623763 Test RE 1.0618124848873958\n",
      "58 Train Loss 0.026913246 Test MSE 0.17363648017077868 Test RE 1.0620698337816357\n",
      "59 Train Loss 0.026871564 Test MSE 0.1737060394992462 Test RE 1.0622825467639696\n",
      "60 Train Loss 0.026812473 Test MSE 0.17376972939313112 Test RE 1.062477273620837\n",
      "61 Train Loss 0.026725143 Test MSE 0.1738017707071496 Test RE 1.062575223933945\n",
      "62 Train Loss 0.02662329 Test MSE 0.17381752736493244 Test RE 1.0626233887329988\n",
      "63 Train Loss 0.026168458 Test MSE 0.17362179322610694 Test RE 1.0620249155399915\n",
      "64 Train Loss 0.02597041 Test MSE 0.17351001252709625 Test RE 1.0616829855354575\n",
      "65 Train Loss 0.025401955 Test MSE 0.17315461528833925 Test RE 1.0605951157115394\n",
      "66 Train Loss 0.024702298 Test MSE 0.17335272534293567 Test RE 1.0612016675239118\n",
      "67 Train Loss 0.024614917 Test MSE 0.17338756520849455 Test RE 1.0613083005921449\n",
      "68 Train Loss 0.024565676 Test MSE 0.17352638347317356 Test RE 1.0617330700945502\n",
      "69 Train Loss 0.024537282 Test MSE 0.17359464157551824 Test RE 1.0619418705018762\n",
      "70 Train Loss 0.0244941 Test MSE 0.17364937317982443 Test RE 1.0621092639226377\n",
      "71 Train Loss 0.024418041 Test MSE 0.17375943568708155 Test RE 1.0624458038396791\n",
      "72 Train Loss 0.02432237 Test MSE 0.17372416944679703 Test RE 1.062337981291212\n",
      "73 Train Loss 0.024240552 Test MSE 0.17377044173874878 Test RE 1.0624794513598965\n",
      "74 Train Loss 0.023336614 Test MSE 0.17323294685541796 Test RE 1.0608349842352138\n",
      "75 Train Loss 0.023280296 Test MSE 0.17318104526319034 Test RE 1.060676056183736\n",
      "76 Train Loss 0.023190368 Test MSE 0.1731341087615486 Test RE 1.0605323112460392\n",
      "77 Train Loss 0.019709203 Test MSE 0.17687075714394634 Test RE 1.0719156318709604\n",
      "78 Train Loss 0.017399311 Test MSE 0.17746028722634943 Test RE 1.0737005533193191\n",
      "79 Train Loss 0.01730598 Test MSE 0.17745708745256994 Test RE 1.0736908733666428\n",
      "80 Train Loss 0.01725509 Test MSE 0.17732770920022542 Test RE 1.0732994052683678\n",
      "81 Train Loss 0.017206894 Test MSE 0.17744367651621304 Test RE 1.0736503016649606\n",
      "82 Train Loss 0.017177489 Test MSE 0.1774996310753217 Test RE 1.0738195691620478\n",
      "83 Train Loss 0.017143572 Test MSE 0.17756179246878978 Test RE 1.074007581602699\n",
      "84 Train Loss 0.01711449 Test MSE 0.1775949751585617 Test RE 1.0741079320075555\n",
      "85 Train Loss 0.01709753 Test MSE 0.1775861385930728 Test RE 1.0740812095646948\n",
      "86 Train Loss 0.017076287 Test MSE 0.17754808760711038 Test RE 1.0739661329070824\n",
      "87 Train Loss 0.017067492 Test MSE 0.17741914345946164 Test RE 1.0735760785796693\n",
      "88 Train Loss 0.01704641 Test MSE 0.17736636464602262 Test RE 1.0734163824920389\n",
      "89 Train Loss 0.01702265 Test MSE 0.1772866840966285 Test RE 1.0731752431548456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 Train Loss 0.016998293 Test MSE 0.17716173647974467 Test RE 1.0727970017653148\n",
      "91 Train Loss 0.016978102 Test MSE 0.17704476636156305 Test RE 1.0724427889591164\n",
      "92 Train Loss 0.01695951 Test MSE 0.17693995611827545 Test RE 1.072125299680951\n",
      "93 Train Loss 0.01693431 Test MSE 0.1768573568642182 Test RE 1.0718750252675309\n",
      "94 Train Loss 0.016901338 Test MSE 0.1767856850396395 Test RE 1.0716578134282368\n",
      "95 Train Loss 0.016862672 Test MSE 0.17675392940452106 Test RE 1.0715615593031882\n",
      "96 Train Loss 0.016790839 Test MSE 0.1767369119106614 Test RE 1.0715099742231506\n",
      "97 Train Loss 0.016684042 Test MSE 0.1768167579200396 Test RE 1.0717519896958438\n",
      "98 Train Loss 0.016570464 Test MSE 0.17692646070681742 Test RE 1.07208441279404\n",
      "99 Train Loss 0.0147988405 Test MSE 0.17755787549880112 Test RE 1.0739957353642515\n",
      "Training time: 31.85\n",
      "Training time: 31.85\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 16772.469 Test MSE 4.65893110669983 Test RE 5.501431769595187\n",
      "1 Train Loss 6208.5176 Test MSE 4.514898803041931 Test RE 5.415724918935168\n",
      "2 Train Loss 3186.594 Test MSE 4.511503004954261 Test RE 5.413687866889839\n",
      "3 Train Loss 1957.2572 Test MSE 4.444380252790008 Test RE 5.373264155762213\n",
      "4 Train Loss 1327.1993 Test MSE 4.599080629666261 Test RE 5.4659807597690735\n",
      "5 Train Loss 1068.427 Test MSE 4.559638207546033 Test RE 5.44249174455923\n",
      "6 Train Loss 736.4165 Test MSE 4.573316237547272 Test RE 5.4506488413775145\n",
      "7 Train Loss 579.29565 Test MSE 4.315658297511042 Test RE 5.294879883435555\n",
      "8 Train Loss 412.2495 Test MSE 4.355945824704788 Test RE 5.319536850263249\n",
      "9 Train Loss 262.10187 Test MSE 4.407115085491302 Test RE 5.350689907129317\n",
      "10 Train Loss 232.28282 Test MSE 4.443853460445282 Test RE 5.3729456998532905\n",
      "11 Train Loss 188.12924 Test MSE 3.7145147244868286 Test RE 4.9122858292372085\n",
      "12 Train Loss 160.4738 Test MSE 3.145877730349746 Test RE 4.520677295235408\n",
      "13 Train Loss 137.70016 Test MSE 2.5719562347682228 Test RE 4.0875621966520015\n",
      "14 Train Loss 105.66597 Test MSE 1.9225465153125407 Test RE 3.5340363723966024\n",
      "15 Train Loss 79.29053 Test MSE 1.335203756367966 Test RE 2.9451428169969676\n",
      "16 Train Loss 51.50069 Test MSE 0.8875074949326326 Test RE 2.4011461863054224\n",
      "17 Train Loss 39.029625 Test MSE 0.6382239936261551 Test RE 2.03619398627606\n",
      "18 Train Loss 27.371336 Test MSE 0.49696480203454946 Test RE 1.7967820575814017\n",
      "19 Train Loss 18.065746 Test MSE 0.1678206305365483 Test RE 1.0441316516781916\n",
      "20 Train Loss 8.256706 Test MSE 0.09492947292424248 Test RE 0.7852955232020042\n",
      "21 Train Loss 2.0092435 Test MSE 0.057101063946712625 Test RE 0.6090524395086939\n",
      "22 Train Loss 0.30575275 Test MSE 0.09843953397681704 Test RE 0.7996820750704561\n",
      "23 Train Loss 0.093554355 Test MSE 0.08833661373050065 Test RE 0.7575354488159352\n",
      "24 Train Loss 0.03687336 Test MSE 0.0955739743807576 Test RE 0.7879568039611248\n",
      "25 Train Loss 0.013374683 Test MSE 0.10460326617320073 Test RE 0.8243377898613086\n",
      "26 Train Loss 0.0037419908 Test MSE 0.10311098628560712 Test RE 0.8184366279812445\n",
      "27 Train Loss 0.002886062 Test MSE 0.10522626009996415 Test RE 0.8267889324758531\n",
      "28 Train Loss 0.0025544874 Test MSE 0.1054281884497007 Test RE 0.8275818529235064\n",
      "29 Train Loss 0.0015933897 Test MSE 0.10727101736142894 Test RE 0.8347833658939641\n",
      "30 Train Loss 0.0012789181 Test MSE 0.10768300491953679 Test RE 0.8363848738519745\n",
      "31 Train Loss 0.0012724417 Test MSE 0.10768986958139122 Test RE 0.8364115326912359\n",
      "32 Train Loss 0.0012680442 Test MSE 0.10767898144968939 Test RE 0.8363692483558348\n",
      "33 Train Loss 0.0012627395 Test MSE 0.10767311257244311 Test RE 0.8363464555355918\n",
      "34 Train Loss 0.0012560633 Test MSE 0.10767035488851263 Test RE 0.8363357453689898\n",
      "35 Train Loss 0.0012451856 Test MSE 0.10768107676111505 Test RE 0.836377385716968\n",
      "36 Train Loss 0.0007456738 Test MSE 0.11052616097059946 Test RE 0.8473544793299709\n",
      "37 Train Loss 0.00074057933 Test MSE 0.11068418657243725 Test RE 0.8479600186526173\n",
      "38 Train Loss 0.0007352839 Test MSE 0.11080985483433549 Test RE 0.8484412591768973\n",
      "39 Train Loss 0.00072936463 Test MSE 0.11094499872526958 Test RE 0.848958481738377\n",
      "40 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "41 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "42 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "43 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "44 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "45 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "46 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "47 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "48 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "49 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "50 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "51 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "52 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "53 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "54 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "55 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "56 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "57 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "58 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "59 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "60 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "61 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "62 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "63 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "64 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "65 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "66 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "67 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "68 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "69 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "70 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "71 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "72 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "73 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "74 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "75 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "76 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "77 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "78 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "79 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "80 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "81 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "82 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "84 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "85 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "86 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "87 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "88 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "89 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "90 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "91 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "92 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "93 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "94 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "95 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "96 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "97 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "98 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "99 Train Loss 0.0007250265 Test MSE 0.11107988656552997 Test RE 0.8494744103143022\n",
      "Training time: 21.10\n",
      "Training time: 21.10\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 489160.12 Test MSE 1.0709715957415218 Test RE 2.6376767011274325\n",
      "1 Train Loss 8779.797 Test MSE 1.0259065920021102 Test RE 2.5815854046913422\n",
      "2 Train Loss 2173.5212 Test MSE 0.954505411330379 Test RE 2.490128645594158\n",
      "3 Train Loss 838.2487 Test MSE 0.8443315656403823 Test RE 2.3420119126488266\n",
      "4 Train Loss 490.92303 Test MSE 0.8357869043673593 Test RE 2.330131161722948\n",
      "5 Train Loss 281.40414 Test MSE 0.9171942959840926 Test RE 2.440974595234723\n",
      "6 Train Loss 194.42378 Test MSE 0.926023101006789 Test RE 2.4526947266137875\n",
      "7 Train Loss 167.21983 Test MSE 0.9213279113761388 Test RE 2.446468909404303\n",
      "8 Train Loss 137.11084 Test MSE 0.8616705398951887 Test RE 2.3659371809796164\n",
      "9 Train Loss 120.30972 Test MSE 0.8392528980213647 Test RE 2.3349576695973138\n",
      "10 Train Loss 92.17092 Test MSE 0.7756011260550852 Test RE 2.244666382404656\n",
      "11 Train Loss 80.826706 Test MSE 0.8393295323574722 Test RE 2.335064272652624\n",
      "12 Train Loss 74.84848 Test MSE 0.8170928280553186 Test RE 2.3039247284151143\n",
      "13 Train Loss 68.945366 Test MSE 0.8549714163121395 Test RE 2.3567221554247206\n",
      "14 Train Loss 64.40269 Test MSE 0.8240302736581359 Test RE 2.3136846779126565\n",
      "15 Train Loss 62.446835 Test MSE 0.8600354077933335 Test RE 2.3636912779909736\n",
      "16 Train Loss 56.283283 Test MSE 0.8970413733903225 Test RE 2.4140086608096154\n",
      "17 Train Loss 50.893085 Test MSE 0.8102500688638804 Test RE 2.294257315485522\n",
      "18 Train Loss 46.197033 Test MSE 0.761555704993433 Test RE 2.2242491066146854\n",
      "19 Train Loss 39.960285 Test MSE 0.7193792717642405 Test RE 2.1617802599001035\n",
      "20 Train Loss 36.249485 Test MSE 0.7325357870176445 Test RE 2.1814587745962863\n",
      "21 Train Loss 35.256523 Test MSE 0.7277341001386202 Test RE 2.1742974155673886\n",
      "22 Train Loss 35.17598 Test MSE 0.7263055219349356 Test RE 2.172162240249035\n",
      "23 Train Loss 31.899403 Test MSE 0.7022924703632762 Test RE 2.1359525120801015\n",
      "24 Train Loss 29.631807 Test MSE 0.7012802475496991 Test RE 2.1344126696600654\n",
      "25 Train Loss 26.123959 Test MSE 0.6096631093528315 Test RE 1.990112122771966\n",
      "26 Train Loss 24.51564 Test MSE 0.5669734051155413 Test RE 1.919172141698548\n",
      "27 Train Loss 22.187 Test MSE 0.48270685626102044 Test RE 1.7708196014830015\n",
      "28 Train Loss 20.833668 Test MSE 0.4610353553733797 Test RE 1.7306119672311469\n",
      "29 Train Loss 18.765083 Test MSE 0.45081708088704914 Test RE 1.7113260770375724\n",
      "30 Train Loss 15.6623125 Test MSE 0.37328173408126836 Test RE 1.5572235262963616\n",
      "31 Train Loss 14.042918 Test MSE 0.3577373675843307 Test RE 1.5244554607300014\n",
      "32 Train Loss 13.256708 Test MSE 0.35487824081982267 Test RE 1.5183513246236153\n",
      "33 Train Loss 11.946114 Test MSE 0.28550225927473205 Test RE 1.3618753572702225\n",
      "34 Train Loss 8.491449 Test MSE 0.2204759446533542 Test RE 1.1967768861632937\n",
      "35 Train Loss 6.9620175 Test MSE 0.21270946478938743 Test RE 1.1755090967423851\n",
      "36 Train Loss 6.3943315 Test MSE 0.20936473634355499 Test RE 1.1662303897972843\n",
      "37 Train Loss 5.988746 Test MSE 0.21705637610216721 Test RE 1.1874596490184606\n",
      "38 Train Loss 5.630732 Test MSE 0.23077001784869325 Test RE 1.2243970616445816\n",
      "39 Train Loss 5.2196436 Test MSE 0.21488027094073847 Test RE 1.1814921985953215\n",
      "40 Train Loss 4.7830086 Test MSE 0.20645116976064845 Test RE 1.15808719821225\n",
      "41 Train Loss 4.412617 Test MSE 0.1900329002341241 Test RE 1.1110842340070661\n",
      "42 Train Loss 3.8755536 Test MSE 0.17705583558220667 Test RE 1.0724763141532148\n",
      "43 Train Loss 3.2642312 Test MSE 0.16372291432855107 Test RE 1.0313054675130902\n",
      "44 Train Loss 3.0115542 Test MSE 0.1497046448096404 Test RE 0.9861664522295986\n",
      "45 Train Loss 2.799934 Test MSE 0.1392129637262354 Test RE 0.9509822841290266\n",
      "46 Train Loss 2.6347256 Test MSE 0.12716266254293385 Test RE 0.9088922351687032\n",
      "47 Train Loss 2.605808 Test MSE 0.1273532132237801 Test RE 0.9095729585779526\n",
      "48 Train Loss 2.3758583 Test MSE 0.12589912920545782 Test RE 0.9043654242153387\n",
      "49 Train Loss 2.3276448 Test MSE 0.12709454341096216 Test RE 0.9086487625857982\n",
      "50 Train Loss 2.3213205 Test MSE 0.12716428285852105 Test RE 0.9088980257346407\n",
      "51 Train Loss 2.3163185 Test MSE 0.12703945527183239 Test RE 0.9084518178808701\n",
      "52 Train Loss 2.3096192 Test MSE 0.12673035811178537 Test RE 0.907345976838842\n",
      "53 Train Loss 2.0690892 Test MSE 0.1221883257152253 Test RE 0.8909379202218629\n",
      "54 Train Loss 1.5188901 Test MSE 0.12390422156532059 Test RE 0.8971718502228906\n",
      "55 Train Loss 1.2862036 Test MSE 0.12411741174250271 Test RE 0.8979433575200296\n",
      "56 Train Loss 1.2115451 Test MSE 0.12398034722429623 Test RE 0.897447415132142\n",
      "57 Train Loss 1.0535946 Test MSE 0.12405290938954984 Test RE 0.8977100019193288\n",
      "58 Train Loss 1.0079074 Test MSE 0.12240736316686605 Test RE 0.8917361200162829\n",
      "59 Train Loss 0.99061275 Test MSE 0.12159248033672611 Test RE 0.8887629579180485\n",
      "60 Train Loss 0.9879508 Test MSE 0.12154468662501616 Test RE 0.8885882700780933\n",
      "61 Train Loss 0.985203 Test MSE 0.12153702808540755 Test RE 0.8885602746310898\n",
      "62 Train Loss 0.9828664 Test MSE 0.12149215079898067 Test RE 0.8883962099986562\n",
      "63 Train Loss 0.98040056 Test MSE 0.12139682012358634 Test RE 0.8880475947577372\n",
      "64 Train Loss 0.9028939 Test MSE 0.11558450987758688 Test RE 0.866527609278816\n",
      "65 Train Loss 0.8082133 Test MSE 0.11431257679564479 Test RE 0.8617466308965978\n",
      "66 Train Loss 0.7199627 Test MSE 0.12002874594207956 Test RE 0.8830295174237167\n",
      "67 Train Loss 0.7032807 Test MSE 0.1189211820998585 Test RE 0.8789460032010037\n",
      "68 Train Loss 0.70197165 Test MSE 0.11850356897277131 Test RE 0.8774013574184614\n",
      "69 Train Loss 0.6886008 Test MSE 0.12290384423334821 Test RE 0.893542719191072\n",
      "70 Train Loss 0.64905936 Test MSE 0.12483623655321457 Test RE 0.9005398188134475\n",
      "71 Train Loss 0.6458944 Test MSE 0.12479396615942764 Test RE 0.9003873414690293\n",
      "72 Train Loss 0.6301126 Test MSE 0.12687670248899793 Test RE 0.9078697135029444\n",
      "73 Train Loss 0.5296521 Test MSE 0.1446775079875008 Test RE 0.9694671475343616\n",
      "74 Train Loss 0.45110932 Test MSE 0.14709074964511115 Test RE 0.9775191350996765\n",
      "75 Train Loss 0.392297 Test MSE 0.14239090416079977 Test RE 0.9617755021049706\n",
      "76 Train Loss 0.255262 Test MSE 0.12446279933935686 Test RE 0.8991918650190983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 Train Loss 0.23286995 Test MSE 0.12068297153917987 Test RE 0.8854327561101624\n",
      "78 Train Loss 0.23040602 Test MSE 0.12074114176599174 Test RE 0.8856461234913511\n",
      "79 Train Loss 0.22825912 Test MSE 0.12112099103299367 Test RE 0.8870381422136981\n",
      "80 Train Loss 0.22040558 Test MSE 0.12297097653692676 Test RE 0.8937867204767648\n",
      "81 Train Loss 0.21775728 Test MSE 0.12302276637170902 Test RE 0.8939749120126711\n",
      "82 Train Loss 0.2169437 Test MSE 0.12267371507059847 Test RE 0.8927057779768063\n",
      "83 Train Loss 0.21601675 Test MSE 0.12212388092842756 Test RE 0.8907029391930602\n",
      "84 Train Loss 0.21530592 Test MSE 0.12161298347471027 Test RE 0.8888378871464068\n",
      "85 Train Loss 0.2145296 Test MSE 0.12104641806162734 Test RE 0.8867650299547077\n",
      "86 Train Loss 0.21400307 Test MSE 0.12069236187936834 Test RE 0.8854672031947679\n",
      "87 Train Loss 0.21301195 Test MSE 0.12024272407668139 Test RE 0.8838162659290296\n",
      "88 Train Loss 0.21174538 Test MSE 0.11992325528898634 Test RE 0.8826413944122776\n",
      "89 Train Loss 0.21061012 Test MSE 0.11985858665049269 Test RE 0.882403380049728\n",
      "90 Train Loss 0.20875746 Test MSE 0.12016656584822448 Test RE 0.8835363298954104\n",
      "91 Train Loss 0.207559 Test MSE 0.12073885748096909 Test RE 0.8856377457432435\n",
      "92 Train Loss 0.2066065 Test MSE 0.12090726916747897 Test RE 0.8862551930793882\n",
      "93 Train Loss 0.19427873 Test MSE 0.12064692270352274 Test RE 0.8853005037987871\n",
      "94 Train Loss 0.19206476 Test MSE 0.12094645632648965 Test RE 0.8863988031774678\n",
      "95 Train Loss 0.19143891 Test MSE 0.12122641063306572 Test RE 0.8874240822023343\n",
      "96 Train Loss 0.19119777 Test MSE 0.12130175773300093 Test RE 0.8876998242790328\n",
      "97 Train Loss 0.19113904 Test MSE 0.12137370152163872 Test RE 0.8879630316020863\n",
      "98 Train Loss 0.19101653 Test MSE 0.12146003107413277 Test RE 0.8882787664935218\n",
      "99 Train Loss 0.19088754 Test MSE 0.12150966149299502 Test RE 0.88846023007566\n",
      "Training time: 38.01\n",
      "Training time: 38.01\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 69038.305 Test MSE 4.432110733414855 Test RE 5.365842093635357\n",
      "1 Train Loss 23905.883 Test MSE 4.699391280857883 Test RE 5.525268536547578\n",
      "2 Train Loss 11131.873 Test MSE 4.689563966415329 Test RE 5.519488323229266\n",
      "3 Train Loss 7147.664 Test MSE 4.382154691983345 Test RE 5.335516152488721\n",
      "4 Train Loss 3719.119 Test MSE 3.8960035472745496 Test RE 5.030860310894361\n",
      "5 Train Loss 2142.6255 Test MSE 3.8985844745772846 Test RE 5.032526394473438\n",
      "6 Train Loss 1265.3351 Test MSE 2.489359132647481 Test RE 4.021391583444267\n",
      "7 Train Loss 816.65234 Test MSE 1.134227267261705 Test RE 2.7144548896935565\n",
      "8 Train Loss 616.0031 Test MSE 0.5934208306008311 Test RE 1.963423480582448\n",
      "9 Train Loss 426.03506 Test MSE 0.5466053783976575 Test RE 1.8843845603603862\n",
      "10 Train Loss 331.4288 Test MSE 0.5092660901274746 Test RE 1.818883848658448\n",
      "11 Train Loss 283.73383 Test MSE 0.48543680425430763 Test RE 1.7758199755168877\n",
      "12 Train Loss 188.88036 Test MSE 0.45684956391250364 Test RE 1.7227378443421393\n",
      "13 Train Loss 140.41484 Test MSE 0.45246491061474864 Test RE 1.7144508497055402\n",
      "14 Train Loss 119.54684 Test MSE 0.45665280738965497 Test RE 1.72236682898108\n",
      "15 Train Loss 92.98646 Test MSE 0.45863602471577786 Test RE 1.7261028479289389\n",
      "16 Train Loss 81.76523 Test MSE 0.47034478098657007 Test RE 1.7479972761941482\n",
      "17 Train Loss 67.51987 Test MSE 0.5088358229078683 Test RE 1.8181153197077995\n",
      "18 Train Loss 59.594627 Test MSE 0.49827169919552333 Test RE 1.7991430573758855\n",
      "19 Train Loss 53.5027 Test MSE 0.5109708314319851 Test RE 1.8219256139944764\n",
      "20 Train Loss 49.382904 Test MSE 0.5277570745155967 Test RE 1.8516104310046395\n",
      "21 Train Loss 42.46737 Test MSE 0.5746928483626674 Test RE 1.9321929057060439\n",
      "22 Train Loss 40.473114 Test MSE 0.5737449701075995 Test RE 1.9305988024691434\n",
      "23 Train Loss 37.138554 Test MSE 0.610661706985615 Test RE 1.9917413078964383\n",
      "24 Train Loss 35.965057 Test MSE 0.6135654780673111 Test RE 1.9964711785101772\n",
      "25 Train Loss 34.113556 Test MSE 0.5856234800456016 Test RE 1.9504814637712529\n",
      "26 Train Loss 32.26077 Test MSE 0.5661369392351324 Test RE 1.917755924881482\n",
      "27 Train Loss 30.09763 Test MSE 0.5832645722032956 Test RE 1.9465492028745535\n",
      "28 Train Loss 29.296776 Test MSE 0.5844385023163577 Test RE 1.9485071171599977\n",
      "29 Train Loss 27.302385 Test MSE 0.563880006317915 Test RE 1.9139294930715913\n",
      "30 Train Loss 26.39188 Test MSE 0.533107978581933 Test RE 1.8609734534258555\n",
      "31 Train Loss 25.632475 Test MSE 0.527790628883395 Test RE 1.8516692920156181\n",
      "32 Train Loss 25.035595 Test MSE 0.5270875682739411 Test RE 1.8504355929910428\n",
      "33 Train Loss 24.575127 Test MSE 0.527421693089842 Test RE 1.8510220026671207\n",
      "34 Train Loss 24.386942 Test MSE 0.5230802725743154 Test RE 1.8433880065323283\n",
      "35 Train Loss 23.95002 Test MSE 0.5013757963034149 Test RE 1.804738442327728\n",
      "36 Train Loss 23.015837 Test MSE 0.478722298085379 Test RE 1.763495741215986\n",
      "37 Train Loss 22.827513 Test MSE 0.4819916037539434 Test RE 1.7695071561737978\n",
      "38 Train Loss 22.050934 Test MSE 0.4616946744941188 Test RE 1.7318489850630958\n",
      "39 Train Loss 21.414879 Test MSE 0.4468469575253493 Test RE 1.7037740116949383\n",
      "40 Train Loss 20.92234 Test MSE 0.42273767378068333 Test RE 1.6571738192033034\n",
      "41 Train Loss 20.678633 Test MSE 0.4145364178765205 Test RE 1.641020218820382\n",
      "42 Train Loss 20.499258 Test MSE 0.41464667346065426 Test RE 1.6412384380236367\n",
      "43 Train Loss 20.232124 Test MSE 0.40312113676222155 Test RE 1.6182677223924606\n",
      "44 Train Loss 19.70958 Test MSE 0.3704210891345872 Test RE 1.551245158426353\n",
      "45 Train Loss 18.906858 Test MSE 0.34099037918680003 Test RE 1.4883451266970795\n",
      "46 Train Loss 18.03012 Test MSE 0.3256856453197801 Test RE 1.454560854448446\n",
      "47 Train Loss 16.966648 Test MSE 0.3008219100976151 Test RE 1.3979360966900034\n",
      "48 Train Loss 16.717962 Test MSE 0.29229591454566123 Test RE 1.3779833163522153\n",
      "49 Train Loss 16.110748 Test MSE 0.2989974549426498 Test RE 1.3936904774493442\n",
      "50 Train Loss 15.912204 Test MSE 0.2996680530851172 Test RE 1.3952525021051274\n",
      "51 Train Loss 13.809919 Test MSE 0.28100698858730494 Test RE 1.3511113655823914\n",
      "52 Train Loss 13.104368 Test MSE 0.28147560021422974 Test RE 1.3522374634723244\n",
      "53 Train Loss 12.603231 Test MSE 0.2691379777682286 Test RE 1.3222698010559464\n",
      "54 Train Loss 12.133259 Test MSE 0.2666053717714067 Test RE 1.31603377331085\n",
      "55 Train Loss 11.5600395 Test MSE 0.2576705756181196 Test RE 1.2937936065541067\n",
      "56 Train Loss 11.113897 Test MSE 0.2487499845675298 Test RE 1.2712006846456052\n",
      "57 Train Loss 10.837404 Test MSE 0.23272385785668345 Test RE 1.2295693836222061\n",
      "58 Train Loss 10.448317 Test MSE 0.22390427458079812 Test RE 1.2060457407881204\n",
      "59 Train Loss 9.692932 Test MSE 0.20179469224200297 Test RE 1.1449524646720843\n",
      "60 Train Loss 9.213491 Test MSE 0.19223271555133103 Test RE 1.117496669225792\n",
      "61 Train Loss 8.2025175 Test MSE 0.19197869342850574 Test RE 1.116758078201903\n",
      "62 Train Loss 6.8203135 Test MSE 0.18369112661207468 Test RE 1.092387383822517\n",
      "63 Train Loss 5.160736 Test MSE 0.15151093135358495 Test RE 0.9920979923855775\n",
      "64 Train Loss 4.457243 Test MSE 0.22339500649506092 Test RE 1.20467339036529\n",
      "65 Train Loss 3.984087 Test MSE 0.21799122653501776 Test RE 1.1900140647682236\n",
      "66 Train Loss 3.6453981 Test MSE 0.20137273538963565 Test RE 1.1437547786703575\n",
      "67 Train Loss 3.34779 Test MSE 0.2134542196866649 Test RE 1.1775651904464024\n",
      "68 Train Loss 2.821463 Test MSE 0.22698291108530416 Test RE 1.2143088708435514\n",
      "69 Train Loss 2.1957688 Test MSE 0.21818774668233407 Test RE 1.1905503456541666\n",
      "70 Train Loss 1.8667195 Test MSE 0.1932704281831576 Test RE 1.1205088506345007\n",
      "71 Train Loss 1.0914128 Test MSE 0.1514833801860148 Test RE 0.9920077853605761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 Train Loss 0.6015011 Test MSE 0.14033819934443376 Test RE 0.9548178662690137\n",
      "73 Train Loss 0.41047293 Test MSE 0.13667762079274787 Test RE 0.9422828612007418\n",
      "74 Train Loss 0.27955735 Test MSE 0.16300504738623423 Test RE 1.0290420292465015\n",
      "75 Train Loss 0.1379478 Test MSE 0.17683205423461923 Test RE 1.0717983470129846\n",
      "76 Train Loss 0.053995747 Test MSE 0.19437319913503656 Test RE 1.1237010280584854\n",
      "77 Train Loss 0.040280335 Test MSE 0.19764145563266486 Test RE 1.1331087905439667\n",
      "78 Train Loss 0.02913329 Test MSE 0.19632919569685922 Test RE 1.1293408319141627\n",
      "79 Train Loss 0.025015425 Test MSE 0.19059012397585826 Test RE 1.1127120293056005\n",
      "80 Train Loss 0.022777498 Test MSE 0.1896273654713434 Test RE 1.109898060648579\n",
      "81 Train Loss 0.020668076 Test MSE 0.1823887015990146 Test RE 1.0885078181765995\n",
      "82 Train Loss 0.020081416 Test MSE 0.18253795837124082 Test RE 1.0889531142314808\n",
      "83 Train Loss 0.016859675 Test MSE 0.18083855670008667 Test RE 1.083872264044176\n",
      "84 Train Loss 0.013778336 Test MSE 0.17018998445180167 Test RE 1.0514765370768728\n",
      "85 Train Loss 0.012229862 Test MSE 0.1684050499213901 Test RE 1.0459481162224338\n",
      "86 Train Loss 0.01114066 Test MSE 0.1636640806794925 Test RE 1.0311201516210182\n",
      "87 Train Loss 0.01110052 Test MSE 0.16327831472244736 Test RE 1.0299042289835953\n",
      "88 Train Loss 0.011081522 Test MSE 0.16304388286596733 Test RE 1.0291646050854344\n",
      "89 Train Loss 0.0101863025 Test MSE 0.15539540026236529 Test RE 1.0047353132274406\n",
      "90 Train Loss 0.009042157 Test MSE 0.1535177848540441 Test RE 0.9986468454419366\n",
      "91 Train Loss 0.009005112 Test MSE 0.15368398555032772 Test RE 0.9991872743685531\n",
      "92 Train Loss 0.008976085 Test MSE 0.15384919208228282 Test RE 0.9997241810782727\n",
      "93 Train Loss 0.008946055 Test MSE 0.15397934406531003 Test RE 1.0001469606064135\n",
      "94 Train Loss 0.008526738 Test MSE 0.15485494464988392 Test RE 1.002986587705222\n",
      "95 Train Loss 0.008141352 Test MSE 0.1463867449192592 Test RE 0.9751770316459706\n",
      "96 Train Loss 0.008028756 Test MSE 0.14307686860203134 Test RE 0.9640893827996391\n",
      "97 Train Loss 0.0061929477 Test MSE 0.12985440217738467 Test RE 0.9184614347649523\n",
      "98 Train Loss 0.005094244 Test MSE 0.1254490596160307 Test RE 0.9027474948693921\n",
      "99 Train Loss 0.0046888227 Test MSE 0.12428896699026128 Test RE 0.898563712467865\n",
      "Training time: 49.08\n",
      "Training time: 49.08\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 135903.88 Test MSE 7.967316671154329 Test RE 7.194301368525492\n",
      "1 Train Loss 52524.105 Test MSE 8.066065026084233 Test RE 7.238747805509088\n",
      "2 Train Loss 16382.387 Test MSE 7.979294502049351 Test RE 7.199707188697718\n",
      "3 Train Loss 5991.0786 Test MSE 7.664557681277684 Test RE 7.056285356007918\n",
      "4 Train Loss 2730.0667 Test MSE 7.831763159952774 Test RE 7.132837985590328\n",
      "5 Train Loss 1574.9934 Test MSE 8.092017705697485 Test RE 7.250383840558788\n",
      "6 Train Loss 1185.2909 Test MSE 8.3246673758901 Test RE 7.353871413089323\n",
      "7 Train Loss 689.77075 Test MSE 8.811604279987476 Test RE 7.565890986799962\n",
      "8 Train Loss 569.5062 Test MSE 8.970065244894899 Test RE 7.633617377114109\n",
      "9 Train Loss 487.16144 Test MSE 9.101819868732242 Test RE 7.689475278050689\n",
      "10 Train Loss 374.7076 Test MSE 9.213946522165271 Test RE 7.736694185701057\n",
      "11 Train Loss 339.16998 Test MSE 9.191988566008 Test RE 7.727469944792453\n",
      "12 Train Loss 308.58716 Test MSE 9.06559033217668 Test RE 7.6741561490813615\n",
      "13 Train Loss 288.2295 Test MSE 8.954692030282681 Test RE 7.6270731904686055\n",
      "14 Train Loss 280.2999 Test MSE 8.94347213014879 Test RE 7.622293471545249\n",
      "15 Train Loss 269.9446 Test MSE 8.817484524484101 Test RE 7.568415037338564\n",
      "16 Train Loss 262.16544 Test MSE 8.813644482996303 Test RE 7.566766823916237\n",
      "17 Train Loss 244.46535 Test MSE 8.543248598238488 Test RE 7.449791340465132\n",
      "18 Train Loss 237.84753 Test MSE 8.411848740542023 Test RE 7.392278396232563\n",
      "19 Train Loss 220.48102 Test MSE 8.005151821539757 Test RE 7.211363266399419\n",
      "20 Train Loss 214.24399 Test MSE 7.714603044277616 Test RE 7.079284712757138\n",
      "21 Train Loss 209.98126 Test MSE 7.472320079386793 Test RE 6.967232770377278\n",
      "22 Train Loss 197.97714 Test MSE 6.992314287761263 Test RE 6.739738667448616\n",
      "23 Train Loss 186.82892 Test MSE 6.763821667425574 Test RE 6.6287045316400555\n",
      "24 Train Loss 183.00894 Test MSE 6.749572585947276 Test RE 6.621718632752174\n",
      "25 Train Loss 182.26459 Test MSE 6.750344143428032 Test RE 6.622097093053455\n",
      "26 Train Loss 174.42513 Test MSE 6.604190651157208 Test RE 6.550016407828414\n",
      "27 Train Loss 172.37366 Test MSE 6.6445244173054006 Test RE 6.5699874159405915\n",
      "28 Train Loss 172.20442 Test MSE 6.645763145489497 Test RE 6.570599803584684\n",
      "29 Train Loss 170.61856 Test MSE 6.612339526123489 Test RE 6.554056176881186\n",
      "30 Train Loss 170.56375 Test MSE 6.610016699059104 Test RE 6.552904899316279\n",
      "31 Train Loss 170.52267 Test MSE 6.607770645694989 Test RE 6.551791481206428\n",
      "32 Train Loss 170.48787 Test MSE 6.605430181970309 Test RE 6.550631060453549\n",
      "33 Train Loss 170.44315 Test MSE 6.602142379447985 Test RE 6.549000594201607\n",
      "34 Train Loss 170.37177 Test MSE 6.596232959056977 Test RE 6.546069010834745\n",
      "35 Train Loss 167.69424 Test MSE 6.387528569001987 Test RE 6.4416780400173055\n",
      "36 Train Loss 167.64612 Test MSE 6.389373019362358 Test RE 6.442608016115924\n",
      "37 Train Loss 167.60254 Test MSE 6.390934335290598 Test RE 6.443395130286208\n",
      "38 Train Loss 167.55788 Test MSE 6.3937376342101295 Test RE 6.44480813043185\n",
      "39 Train Loss 167.50902 Test MSE 6.395817070034444 Test RE 6.445856067976605\n",
      "40 Train Loss 167.45247 Test MSE 6.399209611278906 Test RE 6.447565383084466\n",
      "41 Train Loss 166.34013 Test MSE 6.416771691067269 Test RE 6.456406715192297\n",
      "42 Train Loss 164.11456 Test MSE 6.302283999836688 Test RE 6.398550056268142\n",
      "43 Train Loss 160.75627 Test MSE 6.019065938816732 Test RE 6.253125389774601\n",
      "44 Train Loss 160.567 Test MSE 6.014021132336524 Test RE 6.2505043501886215\n",
      "45 Train Loss 160.52948 Test MSE 6.017893575738437 Test RE 6.252516384123998\n",
      "46 Train Loss 160.5102 Test MSE 6.0170991989594755 Test RE 6.252103696720742\n",
      "47 Train Loss 160.47891 Test MSE 6.019655924609758 Test RE 6.253431846356\n",
      "48 Train Loss 160.43126 Test MSE 6.021157857491956 Test RE 6.254211928245687\n",
      "49 Train Loss 154.0794 Test MSE 5.458340060973329 Test RE 5.954741065588033\n",
      "50 Train Loss 139.06313 Test MSE 4.732075164461526 Test RE 5.544449140773766\n",
      "51 Train Loss 127.57442 Test MSE 4.361626690386047 Test RE 5.323004493015422\n",
      "52 Train Loss 125.10815 Test MSE 4.07566689428382 Test RE 5.145551481967795\n",
      "53 Train Loss 118.84217 Test MSE 3.3818404136329216 Test RE 4.687153124752228\n",
      "54 Train Loss 113.53114 Test MSE 2.9925471918263487 Test RE 4.409131897728431\n",
      "55 Train Loss 103.713356 Test MSE 2.5873858658443214 Test RE 4.099804875409067\n",
      "56 Train Loss 94.39011 Test MSE 2.3916120561382637 Test RE 3.941649051900108\n",
      "57 Train Loss 91.576935 Test MSE 2.2423891103567786 Test RE 3.816700437781661\n",
      "58 Train Loss 89.09966 Test MSE 2.107142611548113 Test RE 3.699811105956058\n",
      "59 Train Loss 84.06089 Test MSE 2.0516356683642987 Test RE 3.6507551562608995\n",
      "60 Train Loss 78.26422 Test MSE 2.025954900180754 Test RE 3.6278345582258176\n",
      "61 Train Loss 73.6896 Test MSE 2.067209599012615 Test RE 3.664585369078752\n",
      "62 Train Loss 71.94561 Test MSE 2.0502420190854505 Test RE 3.6495149905009394\n",
      "63 Train Loss 70.97186 Test MSE 2.1644764312200793 Test RE 3.749807880640842\n",
      "64 Train Loss 70.543396 Test MSE 2.1801099165001725 Test RE 3.763325489772371\n",
      "65 Train Loss 68.96622 Test MSE 2.0605267269863723 Test RE 3.6586571410237427\n",
      "66 Train Loss 68.669235 Test MSE 1.9948085427346494 Test RE 3.5998399851521823\n",
      "67 Train Loss 68.30699 Test MSE 1.9616092385809194 Test RE 3.56975849679579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 Train Loss 67.783035 Test MSE 2.0096194034094395 Test RE 3.6131791422223154\n",
      "69 Train Loss 66.715744 Test MSE 2.001656360827698 Test RE 3.606013492392786\n",
      "70 Train Loss 66.19269 Test MSE 2.0421346464811823 Test RE 3.6422921148961236\n",
      "71 Train Loss 66.16351 Test MSE 2.0423063496120846 Test RE 3.642445234039277\n",
      "72 Train Loss 65.678764 Test MSE 2.019702115247578 Test RE 3.6222318671167706\n",
      "73 Train Loss 65.5342 Test MSE 2.0254350693761567 Test RE 3.6273691033571462\n",
      "74 Train Loss 64.39476 Test MSE 2.1081290926185288 Test RE 3.700677057441199\n",
      "75 Train Loss 63.98143 Test MSE 2.1376571097876984 Test RE 3.7265041452337075\n",
      "76 Train Loss 63.815907 Test MSE 2.176505224671433 Test RE 3.7602129768011223\n",
      "77 Train Loss 63.314762 Test MSE 2.0865943821032222 Test RE 3.681727180693285\n",
      "78 Train Loss 62.226837 Test MSE 2.066104990894959 Test RE 3.6636061573811762\n",
      "79 Train Loss 61.840424 Test MSE 1.9939936834195404 Test RE 3.5991046607575043\n",
      "80 Train Loss 61.46263 Test MSE 1.9761788774938591 Test RE 3.5829909677317127\n",
      "81 Train Loss 61.03769 Test MSE 1.907272677605398 Test RE 3.519970149711648\n",
      "82 Train Loss 60.860817 Test MSE 1.907771129526196 Test RE 3.520430079034821\n",
      "83 Train Loss 60.85654 Test MSE 1.9067826894594433 Test RE 3.519517971463663\n",
      "84 Train Loss 60.84266 Test MSE 1.9027789916729783 Test RE 3.51582103972169\n",
      "85 Train Loss 60.83889 Test MSE 1.900413080256568 Test RE 3.51363457758796\n",
      "86 Train Loss 60.813866 Test MSE 1.8909510722855933 Test RE 3.5048766069069544\n",
      "87 Train Loss 60.80184 Test MSE 1.8914241821157036 Test RE 3.5053150338696577\n",
      "88 Train Loss 60.596603 Test MSE 1.8408455820667646 Test RE 3.4581296110953774\n",
      "89 Train Loss 59.985695 Test MSE 1.7164003713397435 Test RE 3.3391958100243233\n",
      "90 Train Loss 59.80836 Test MSE 1.732179323147173 Test RE 3.3545093903854593\n",
      "91 Train Loss 59.777454 Test MSE 1.7385239657951959 Test RE 3.360647239286438\n",
      "92 Train Loss 59.450073 Test MSE 1.7620550800365506 Test RE 3.3833141633649824\n",
      "93 Train Loss 58.646576 Test MSE 1.678951774554662 Test RE 3.3025674621170316\n",
      "94 Train Loss 57.7455 Test MSE 1.6534023565971225 Test RE 3.277342750069555\n",
      "95 Train Loss 56.721294 Test MSE 1.5832853515375498 Test RE 3.207097651162495\n",
      "96 Train Loss 56.03774 Test MSE 1.5886831975006848 Test RE 3.21255992913061\n",
      "97 Train Loss 56.007263 Test MSE 1.5936209285050358 Test RE 3.2175484788575\n",
      "98 Train Loss 55.72748 Test MSE 1.5939461888473139 Test RE 3.2178768152598805\n",
      "99 Train Loss 55.138466 Test MSE 1.5579362029068067 Test RE 3.1813204822253693\n",
      "Training time: 40.50\n",
      "Training time: 40.50\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 1092.3993 Test MSE 2.2865846677270465 Test RE 3.8541288557803184\n",
      "1 Train Loss 278.154 Test MSE 2.2739971551484848 Test RE 3.8435058407938256\n",
      "2 Train Loss 166.48344 Test MSE 2.1636421746631544 Test RE 3.74908516468496\n",
      "3 Train Loss 130.90688 Test MSE 2.0971271625943646 Test RE 3.691007856041699\n",
      "4 Train Loss 101.256546 Test MSE 1.8317223839238326 Test RE 3.4495497528935037\n",
      "5 Train Loss 87.256874 Test MSE 1.5734442040134897 Test RE 3.197115016656764\n",
      "6 Train Loss 64.60957 Test MSE 1.215864997829702 Test RE 2.8104461282441884\n",
      "7 Train Loss 50.30916 Test MSE 0.9786373016943315 Test RE 2.521409991762497\n",
      "8 Train Loss 36.694542 Test MSE 0.6157552072851022 Test RE 2.000030568541752\n",
      "9 Train Loss 28.57083 Test MSE 0.5251250486531515 Test RE 1.8469874915599622\n",
      "10 Train Loss 23.410715 Test MSE 0.41564727887782965 Test RE 1.6432175236304214\n",
      "11 Train Loss 17.089384 Test MSE 0.22557499366845143 Test RE 1.210536987795813\n",
      "12 Train Loss 13.733585 Test MSE 0.19060636089799854 Test RE 1.1127594258690436\n",
      "13 Train Loss 6.1713533 Test MSE 0.11212218806738047 Test RE 0.8534505627234583\n",
      "14 Train Loss 4.3462405 Test MSE 0.09545971434624072 Test RE 0.7874856563661766\n",
      "15 Train Loss 3.0015736 Test MSE 0.11464813494288537 Test RE 0.8630105084774093\n",
      "16 Train Loss 1.6360729 Test MSE 0.10096682706179495 Test RE 0.8098823626187577\n",
      "17 Train Loss 0.9596435 Test MSE 0.12034698966023873 Test RE 0.8841993728958921\n",
      "18 Train Loss 0.8146337 Test MSE 0.11069093608448906 Test RE 0.8479858725225032\n",
      "19 Train Loss 0.6184088 Test MSE 0.09681209677035277 Test RE 0.7930442123509249\n",
      "20 Train Loss 0.40647167 Test MSE 0.09986876163249729 Test RE 0.8054663826594993\n",
      "21 Train Loss 0.19287774 Test MSE 0.11538718434123059 Test RE 0.8657876266229502\n",
      "22 Train Loss 0.07807537 Test MSE 0.1256104324575432 Test RE 0.9033279380788817\n",
      "23 Train Loss 0.058714278 Test MSE 0.12596082569046868 Test RE 0.9045869878360737\n",
      "24 Train Loss 0.04612308 Test MSE 0.12805094072659123 Test RE 0.9120611847747782\n",
      "25 Train Loss 0.041137684 Test MSE 0.12941087396769954 Test RE 0.9168915534590477\n",
      "26 Train Loss 0.037710454 Test MSE 0.1324396924456309 Test RE 0.9275592668743158\n",
      "27 Train Loss 0.037607092 Test MSE 0.13220839268360793 Test RE 0.9267489433659456\n",
      "28 Train Loss 0.037488952 Test MSE 0.131980037534854 Test RE 0.9259482402094451\n",
      "29 Train Loss 0.0338258 Test MSE 0.12867137978087612 Test RE 0.9142680978029204\n",
      "30 Train Loss 0.033230964 Test MSE 0.12904069286449674 Test RE 0.9155792255618226\n",
      "31 Train Loss 0.03315768 Test MSE 0.12914705836793325 Test RE 0.9159564940926086\n",
      "32 Train Loss 0.03259293 Test MSE 0.12929180085259576 Test RE 0.9164696327362252\n",
      "33 Train Loss 0.03219392 Test MSE 0.12906574319526312 Test RE 0.9156680907443357\n",
      "34 Train Loss 0.032084525 Test MSE 0.12899892467455149 Test RE 0.91543103516844\n",
      "35 Train Loss 0.027316604 Test MSE 0.12539261565102883 Test RE 0.9025443830205434\n",
      "36 Train Loss 0.022472069 Test MSE 0.12353815548047892 Test RE 0.8958455551302719\n",
      "37 Train Loss 0.019873349 Test MSE 0.12296950124897638 Test RE 0.8937813590623443\n",
      "38 Train Loss 0.011528971 Test MSE 0.12610809079258017 Test RE 0.9051156251360434\n",
      "39 Train Loss 0.010391275 Test MSE 0.1270567584210014 Test RE 0.9085136826852898\n",
      "40 Train Loss 0.008086266 Test MSE 0.1257383057227336 Test RE 0.9037876216719387\n",
      "41 Train Loss 0.0066882744 Test MSE 0.1284810292083296 Test RE 0.9135915842403207\n",
      "42 Train Loss 0.0061173965 Test MSE 0.12868589164805624 Test RE 0.914319653025046\n",
      "43 Train Loss 0.0060589407 Test MSE 0.12878086009391088 Test RE 0.9146569685657707\n",
      "44 Train Loss 0.005094043 Test MSE 0.1288429115474522 Test RE 0.9148773000623465\n",
      "45 Train Loss 0.005067437 Test MSE 0.1288889287776911 Test RE 0.9150406631858126\n",
      "46 Train Loss 0.0050419625 Test MSE 0.12895394107450564 Test RE 0.9152714099070978\n",
      "47 Train Loss 0.0050240145 Test MSE 0.12898770447555652 Test RE 0.9153912226592906\n",
      "48 Train Loss 0.0050079534 Test MSE 0.12901020864855908 Test RE 0.9154710722235899\n",
      "49 Train Loss 0.004988077 Test MSE 0.12898737884054182 Test RE 0.9153900671862801\n",
      "50 Train Loss 0.0049652644 Test MSE 0.12893031855024775 Test RE 0.915187573728229\n",
      "51 Train Loss 0.004937174 Test MSE 0.12883848400871326 Test RE 0.914861580573248\n",
      "52 Train Loss 0.0048996955 Test MSE 0.12868997417535608 Test RE 0.9143341561896583\n",
      "53 Train Loss 0.0042962176 Test MSE 0.127858427410784 Test RE 0.9113753250485614\n",
      "54 Train Loss 0.00427814 Test MSE 0.12800918870518058 Test RE 0.9119124802728499\n",
      "55 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "56 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "57 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "58 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "59 Train Loss 0.004270916 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "60 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "61 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "62 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "63 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "64 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Train Loss 0.004270916 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "66 Train Loss 0.004270916 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "67 Train Loss 0.004270916 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "68 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "69 Train Loss 0.004270916 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "70 Train Loss 0.004270916 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "71 Train Loss 0.004270916 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "72 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "73 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "74 Train Loss 0.004270916 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "75 Train Loss 0.004270916 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "76 Train Loss 0.004270916 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "77 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "78 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "79 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "80 Train Loss 0.004270916 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "81 Train Loss 0.004270916 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "82 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "83 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "84 Train Loss 0.004270916 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "85 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "86 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "87 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "88 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "89 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "90 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "91 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "92 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "93 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "94 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "95 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "96 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "97 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "98 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "99 Train Loss 0.0042709154 Test MSE 0.12811280189066487 Test RE 0.9122814656789345\n",
      "Training time: 23.89\n",
      "Training time: 23.89\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 74590.586 Test MSE 0.9661593976066613 Test RE 2.505284076902924\n",
      "1 Train Loss 11970.694 Test MSE 0.9991518806251632 Test RE 2.5477003218349132\n",
      "2 Train Loss 2850.591 Test MSE 0.7348942961915703 Test RE 2.1849677198308406\n",
      "3 Train Loss 874.1116 Test MSE 0.5916625325061784 Test RE 1.9605125237621184\n",
      "4 Train Loss 392.54193 Test MSE 0.562573864214351 Test RE 1.9117115449411257\n",
      "5 Train Loss 307.23224 Test MSE 0.4923858849806811 Test RE 1.7884853382532027\n",
      "6 Train Loss 225.34995 Test MSE 0.5408715684119227 Test RE 1.8744750461389166\n",
      "7 Train Loss 184.31828 Test MSE 0.5952456296922097 Test RE 1.966439976685761\n",
      "8 Train Loss 121.70409 Test MSE 0.6869011925245034 Test RE 2.112417331530849\n",
      "9 Train Loss 100.79102 Test MSE 0.6524654104096725 Test RE 2.0587866023804477\n",
      "10 Train Loss 83.09214 Test MSE 0.5980101699848842 Test RE 1.9710011232482196\n",
      "11 Train Loss 63.96891 Test MSE 0.5243721312910871 Test RE 1.8456629234618451\n",
      "12 Train Loss 50.060085 Test MSE 0.4969178734697713 Test RE 1.7966972201909748\n",
      "13 Train Loss 37.4385 Test MSE 0.436487406942029 Test RE 1.6839083329393547\n",
      "14 Train Loss 31.195063 Test MSE 0.40379491890770475 Test RE 1.6196195550903016\n",
      "15 Train Loss 26.295366 Test MSE 0.37712102663417135 Test RE 1.5652112494638775\n",
      "16 Train Loss 20.914162 Test MSE 0.3442041591036415 Test RE 1.4953423860212944\n",
      "17 Train Loss 16.843435 Test MSE 0.3331504400702827 Test RE 1.4711358622090451\n",
      "18 Train Loss 14.762498 Test MSE 0.3046855621691183 Test RE 1.4068847576967642\n",
      "19 Train Loss 11.738699 Test MSE 0.2648384032457157 Test RE 1.3116654141265665\n",
      "20 Train Loss 7.761659 Test MSE 0.20385702430498293 Test RE 1.1507882717573157\n",
      "21 Train Loss 5.9961696 Test MSE 0.19561181124164206 Test RE 1.1272756449229249\n",
      "22 Train Loss 4.5456176 Test MSE 0.17468876134311248 Test RE 1.0652831791886637\n",
      "23 Train Loss 3.7587373 Test MSE 0.1668317203009837 Test RE 1.0410507488914613\n",
      "24 Train Loss 3.205517 Test MSE 0.16676078602432778 Test RE 1.0408294059929546\n",
      "25 Train Loss 2.9456918 Test MSE 0.16647338132572595 Test RE 1.0399321079281483\n",
      "26 Train Loss 2.4595604 Test MSE 0.16043323841301427 Test RE 1.0208919090175388\n",
      "27 Train Loss 2.289325 Test MSE 0.16008600236994347 Test RE 1.019786519355393\n",
      "28 Train Loss 1.6752516 Test MSE 0.15582566907605572 Test RE 1.0061253396849947\n",
      "29 Train Loss 1.1082814 Test MSE 0.14163108899606314 Test RE 0.9592059942521537\n",
      "30 Train Loss 0.9703457 Test MSE 0.14123321896600177 Test RE 0.9578577460572067\n",
      "31 Train Loss 0.8359051 Test MSE 0.14180165245701054 Test RE 0.959783396672199\n",
      "32 Train Loss 0.75365555 Test MSE 0.13499443150939125 Test RE 0.9364627651964381\n",
      "33 Train Loss 0.618647 Test MSE 0.12569303983792626 Test RE 0.903624924915467\n",
      "34 Train Loss 0.4239836 Test MSE 0.12171642474134735 Test RE 0.8892158195492909\n",
      "35 Train Loss 0.38000363 Test MSE 0.12087253032257056 Test RE 0.8861278653608751\n",
      "36 Train Loss 0.35001075 Test MSE 0.11833715650791607 Test RE 0.8767850813862579\n",
      "37 Train Loss 0.33543712 Test MSE 0.11483216079044588 Test RE 0.8637028553744445\n",
      "38 Train Loss 0.2529884 Test MSE 0.11977656137943578 Test RE 0.8821013918238283\n",
      "39 Train Loss 0.17636283 Test MSE 0.12733564214601117 Test RE 0.9095102089696184\n",
      "40 Train Loss 0.15909146 Test MSE 0.12798735314141102 Test RE 0.9118347008082188\n",
      "41 Train Loss 0.09790773 Test MSE 0.1336470810792692 Test RE 0.93177772892114\n",
      "42 Train Loss 0.05020929 Test MSE 0.13927822322723754 Test RE 0.951205156181513\n",
      "43 Train Loss 0.0386714 Test MSE 0.14000642090697804 Test RE 0.9536885392745239\n",
      "44 Train Loss 0.036344294 Test MSE 0.14097527863220663 Test RE 0.9569826578151837\n",
      "45 Train Loss 0.036049195 Test MSE 0.14095979344281212 Test RE 0.9569300973066566\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5aff6a09d458>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-33fd3811f7f0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mx_coll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolloc_pts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mf_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_bc2_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbc2_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a9ac8e8143dc>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_coll, f_hat)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_add_grad\u001b[0;34m(self, step_size, update)\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mnumel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;31m# view as to avoid deprecated pointwise semantics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    alpha_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.5, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"alpha\": alpha_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
