{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"1D_SODE_atanh\"\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(3.0)\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 144917150000000.0 Test MSE 401.8892164631241 Test RE 1.0219173183892591\n",
      "1 Train Loss 144917150000000.0 Test MSE 401.8892164631241 Test RE 1.0219173183892591\n",
      "2 Train Loss 144868980000000.0 Test MSE 401.8894321868952 Test RE 1.0219175926586581\n",
      "3 Train Loss 142135430000000.0 Test MSE 401.8894314268233 Test RE 1.0219175916923093\n",
      "4 Train Loss 130813550000000.0 Test MSE 401.8896013958727 Test RE 1.0219178077894864\n",
      "5 Train Loss 118731955000000.0 Test MSE 401.88964893314625 Test RE 1.0219178682279564\n",
      "6 Train Loss 100908780000000.0 Test MSE 401.8897182226798 Test RE 1.0219179563220506\n",
      "7 Train Loss 95558065000000.0 Test MSE 401.88972191815844 Test RE 1.021917961020449\n",
      "8 Train Loss 92864520000000.0 Test MSE 401.8890350519006 Test RE 1.0219170877444888\n",
      "9 Train Loss 89044220000000.0 Test MSE 401.8848918336366 Test RE 1.0219118200759465\n",
      "10 Train Loss 86863900000000.0 Test MSE 401.88225071262605 Test RE 1.0219084621527352\n",
      "11 Train Loss 84592220000000.0 Test MSE 401.87879578482557 Test RE 1.0219040695383097\n",
      "12 Train Loss 80877520000000.0 Test MSE 401.87095492949555 Test RE 1.0218941005611737\n",
      "13 Train Loss 74069980000000.0 Test MSE 401.85823294377724 Test RE 1.021877925437189\n",
      "14 Train Loss 69290630000000.0 Test MSE 401.8339970745136 Test RE 1.0218471104989932\n",
      "15 Train Loss 68079246000000.0 Test MSE 401.83564587660044 Test RE 1.021849206914344\n",
      "16 Train Loss 66942946000000.0 Test MSE 401.83342428699916 Test RE 1.0218463822113426\n",
      "17 Train Loss 65736735000000.0 Test MSE 401.83221676690164 Test RE 1.0218448468724493\n",
      "18 Train Loss 64855474000000.0 Test MSE 401.831302864905 Test RE 1.0218436848618664\n",
      "19 Train Loss 63758120000000.0 Test MSE 401.8310625597121 Test RE 1.0218433793177502\n",
      "20 Train Loss 62142870000000.0 Test MSE 401.8299005199877 Test RE 1.021841901801988\n",
      "21 Train Loss 59882600000000.0 Test MSE 401.82453025248446 Test RE 1.021835073561125\n",
      "22 Train Loss 58393104000000.0 Test MSE 401.82412930230834 Test RE 1.0218345637551973\n",
      "23 Train Loss 56218820000000.0 Test MSE 401.82474209136336 Test RE 1.0218353429129834\n",
      "24 Train Loss 55557654000000.0 Test MSE 401.8214925392555 Test RE 1.021831211119246\n",
      "25 Train Loss 55115113000000.0 Test MSE 401.8175559913976 Test RE 1.0218262057905219\n",
      "26 Train Loss 54224583000000.0 Test MSE 401.8090605887904 Test RE 1.0218154037850256\n",
      "27 Train Loss 53361252000000.0 Test MSE 401.7970854824547 Test RE 1.021800177100902\n",
      "28 Train Loss 52916115000000.0 Test MSE 401.7898699397829 Test RE 1.0217910022261365\n",
      "29 Train Loss 52507490000000.0 Test MSE 401.78416223877304 Test RE 1.0217837445789384\n",
      "30 Train Loss 51837927000000.0 Test MSE 401.77341196258635 Test RE 1.0217700748876382\n",
      "31 Train Loss 51134100000000.0 Test MSE 401.7624947957848 Test RE 1.0217561927966567\n",
      "32 Train Loss 50389496000000.0 Test MSE 401.74940622323544 Test RE 1.0217395493329802\n",
      "33 Train Loss 49643780000000.0 Test MSE 401.73661709174814 Test RE 1.0217232863774721\n",
      "34 Train Loss 48821420000000.0 Test MSE 401.7213785176849 Test RE 1.0217039083161286\n",
      "35 Train Loss 47931740000000.0 Test MSE 401.7034116045336 Test RE 1.0216810603030213\n",
      "36 Train Loss 47345540000000.0 Test MSE 401.69124435119903 Test RE 1.0216655872623879\n",
      "37 Train Loss 47005896000000.0 Test MSE 401.6834753613813 Test RE 1.0216557073508452\n",
      "38 Train Loss 46728630000000.0 Test MSE 401.6792138875677 Test RE 1.0216502879461924\n",
      "39 Train Loss 46519437000000.0 Test MSE 401.675995164903 Test RE 1.0216461946107507\n",
      "40 Train Loss 46234237000000.0 Test MSE 401.6737422146575 Test RE 1.0216433294640899\n",
      "41 Train Loss 45730073000000.0 Test MSE 401.66949334343343 Test RE 1.021637926020988\n",
      "42 Train Loss 45237580000000.0 Test MSE 401.663282599098 Test RE 1.0216300275415233\n",
      "43 Train Loss 44536690000000.0 Test MSE 401.6577220145118 Test RE 1.0216229558472814\n",
      "44 Train Loss 44048925000000.0 Test MSE 401.6542270502269 Test RE 1.0216185110883302\n",
      "45 Train Loss 43683050000000.0 Test MSE 401.64944145651185 Test RE 1.0216124249259553\n",
      "46 Train Loss 43392277000000.0 Test MSE 401.6444499300228 Test RE 1.0216060768263469\n",
      "47 Train Loss 43053704000000.0 Test MSE 401.63803466755763 Test RE 1.021597917996702\n",
      "48 Train Loss 42637940000000.0 Test MSE 401.6314919791867 Test RE 1.0215895970416855\n",
      "49 Train Loss 42182490000000.0 Test MSE 401.6238833815304 Test RE 1.0215799203838762\n",
      "50 Train Loss 41537765000000.0 Test MSE 401.61577779403103 Test RE 1.0215696115507185\n",
      "51 Train Loss 40876726000000.0 Test MSE 401.6071096993063 Test RE 1.0215585871955624\n",
      "52 Train Loss 39867514000000.0 Test MSE 401.5944032009734 Test RE 1.0215424264568134\n",
      "53 Train Loss 39069470000000.0 Test MSE 401.5820171495303 Test RE 1.0215266730318322\n",
      "54 Train Loss 38405006000000.0 Test MSE 401.58842970825003 Test RE 1.0215348289916917\n",
      "55 Train Loss 37943130000000.0 Test MSE 401.59297315543307 Test RE 1.021540607639768\n",
      "56 Train Loss 37472290000000.0 Test MSE 401.60031392955176 Test RE 1.0215499440389173\n",
      "57 Train Loss 37282585000000.0 Test MSE 401.60271262272795 Test RE 1.0215529948099649\n",
      "58 Train Loss 37122803000000.0 Test MSE 401.6051560688097 Test RE 1.0215561024905\n",
      "59 Train Loss 36941744000000.0 Test MSE 401.6078207528108 Test RE 1.0215594915402249\n",
      "60 Train Loss 36413366000000.0 Test MSE 401.6130386854982 Test RE 1.021566127879302\n",
      "61 Train Loss 36094120000000.0 Test MSE 401.61641625842043 Test RE 1.0215704235650802\n",
      "62 Train Loss 35246473000000.0 Test MSE 401.6330918091187 Test RE 1.0215916317028375\n",
      "63 Train Loss 34982005000000.0 Test MSE 401.63426406300846 Test RE 1.021593122570889\n",
      "64 Train Loss 34274602000000.0 Test MSE 401.6395361148468 Test RE 1.0215998275195293\n",
      "65 Train Loss 33909828000000.0 Test MSE 401.6424271949597 Test RE 1.0216035043508225\n",
      "66 Train Loss 33710466000000.0 Test MSE 401.64550947129044 Test RE 1.0216074243279545\n",
      "67 Train Loss 33533911000000.0 Test MSE 401.6477416503458 Test RE 1.0216102631590545\n",
      "68 Train Loss 33408883000000.0 Test MSE 401.6506677721118 Test RE 1.0216139845176888\n",
      "69 Train Loss 33211753000000.0 Test MSE 401.65163271827043 Test RE 1.0216152117058612\n",
      "70 Train Loss 33143992000000.0 Test MSE 401.6522545489253 Test RE 1.0216160025297476\n",
      "71 Train Loss 32966270000000.0 Test MSE 401.6507206288781 Test RE 1.0216140517392998\n",
      "72 Train Loss 32511210000000.0 Test MSE 401.6520533774459 Test RE 1.0216157466865077\n",
      "73 Train Loss 32485694000000.0 Test MSE 401.6524002934787 Test RE 1.0216161878828147\n",
      "74 Train Loss 32481626000000.0 Test MSE 401.65266958816653 Test RE 1.0216165303627378\n",
      "75 Train Loss 32421370000000.0 Test MSE 401.65363131271994 Test RE 1.0216177534507287\n",
      "76 Train Loss 32380495000000.0 Test MSE 401.65493326455004 Test RE 1.0216194092256579\n",
      "77 Train Loss 32332710000000.0 Test MSE 401.6567477364887 Test RE 1.0216217168005224\n",
      "78 Train Loss 32281155000000.0 Test MSE 401.6574270795135 Test RE 1.0216225807612276\n",
      "79 Train Loss 32174140000000.0 Test MSE 401.6595339690994 Test RE 1.0216252602126807\n",
      "80 Train Loss 32113288000000.0 Test MSE 401.66079479388014 Test RE 1.021626863671985\n",
      "81 Train Loss 32036335000000.0 Test MSE 401.6610804721128 Test RE 1.0216272269841489\n",
      "82 Train Loss 31962884000000.0 Test MSE 401.6606732987215 Test RE 1.0216267091601072\n",
      "83 Train Loss 31865927000000.0 Test MSE 401.6596900035407 Test RE 1.0216254586502846\n",
      "84 Train Loss 31816537000000.0 Test MSE 401.658711114058 Test RE 1.0216242137419072\n",
      "85 Train Loss 31781445000000.0 Test MSE 401.6570616831325 Test RE 1.0216221160651286\n",
      "86 Train Loss 31772589000000.0 Test MSE 401.65631733367496 Test RE 1.0216211694314294\n",
      "87 Train Loss 31758548000000.0 Test MSE 401.6559929938165 Test RE 1.0216207569487694\n",
      "88 Train Loss 31702221000000.0 Test MSE 401.6560076054178 Test RE 1.021620775531232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 Train Loss 31668646000000.0 Test MSE 401.6557933817892 Test RE 1.0216205030899708\n",
      "90 Train Loss 31616938000000.0 Test MSE 401.65662299079565 Test RE 1.021621558153967\n",
      "91 Train Loss 31520618000000.0 Test MSE 401.66040145848274 Test RE 1.021626363446282\n",
      "92 Train Loss 31401870000000.0 Test MSE 401.6660335105817 Test RE 1.0216335260053915\n",
      "93 Train Loss 31238620000000.0 Test MSE 401.67548152796667 Test RE 1.021645541403445\n",
      "94 Train Loss 31012790000000.0 Test MSE 401.6892186724123 Test RE 1.0216630111931486\n",
      "95 Train Loss 30901426000000.0 Test MSE 401.69678238834683 Test RE 1.0216726299880547\n",
      "96 Train Loss 30787764000000.0 Test MSE 401.70326378851166 Test RE 1.021680872327466\n",
      "97 Train Loss 30628820000000.0 Test MSE 401.70881171564196 Test RE 1.0216879275246353\n",
      "98 Train Loss 30586916000000.0 Test MSE 401.71412227577997 Test RE 1.021694680820918\n",
      "99 Train Loss 30570503000000.0 Test MSE 401.7143261377148 Test RE 1.0216949400657593\n",
      "Training time: 11.16\n",
      "Training time: 11.16\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 34542264000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "1 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "2 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "3 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "4 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "5 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "6 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "7 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "8 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "9 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "10 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "11 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "12 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "13 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "14 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "15 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "16 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "17 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "18 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "19 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "20 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "21 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "22 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "23 Train Loss 34542272000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "24 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "25 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "26 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "27 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "28 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "29 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "30 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "31 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "32 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "33 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "34 Train Loss 34542264000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "35 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "36 Train Loss 34542264000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "37 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "38 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "39 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "40 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "41 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "42 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "43 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "44 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "45 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "46 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "47 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "48 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "49 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "50 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "51 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "52 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "53 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "54 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "55 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "56 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "57 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "58 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "59 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "60 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "61 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "62 Train Loss 34542272000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "63 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "64 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "65 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "66 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "67 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "68 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "69 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "70 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "71 Train Loss 34542264000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "72 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "73 Train Loss 34542272000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "74 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "75 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "76 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "78 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "79 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "80 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "81 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "82 Train Loss 34542264000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "83 Train Loss 34542262000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "84 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "85 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "86 Train Loss 34542272000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "87 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "88 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "89 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "90 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "91 Train Loss 34542264000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "92 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "93 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "94 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "95 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "96 Train Loss 34542266000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "97 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "98 Train Loss 34542268000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "99 Train Loss 34542264000000.0 Test MSE 411.7874155673466 Test RE 1.0344252610667966\n",
      "Training time: 9.80\n",
      "Training time: 9.80\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 150764530000000.0 Test MSE 353.6407956328396 Test RE 0.9586139883089215\n",
      "1 Train Loss 150764530000000.0 Test MSE 353.6407956328396 Test RE 0.9586139883089215\n",
      "2 Train Loss 108449990000000.0 Test MSE 353.6198195374191 Test RE 0.9585855579398551\n",
      "3 Train Loss 97477300000000.0 Test MSE 353.6265314470279 Test RE 0.9585946551520756\n",
      "4 Train Loss 96526440000000.0 Test MSE 353.6269064097199 Test RE 0.9585951633678134\n",
      "5 Train Loss 60471483000000.0 Test MSE 353.6188533417208 Test RE 0.958584248366941\n",
      "6 Train Loss 57174294000000.0 Test MSE 353.6165208616951 Test RE 0.9585810869372289\n",
      "7 Train Loss 49010010000000.0 Test MSE 353.614494578152 Test RE 0.9585783405159658\n",
      "8 Train Loss 42355843000000.0 Test MSE 353.6151798994913 Test RE 0.9585792694002225\n",
      "9 Train Loss 38061830000000.0 Test MSE 353.6167478302508 Test RE 0.9585813945695328\n",
      "10 Train Loss 34018940000000.0 Test MSE 353.6164058466106 Test RE 0.9585809310461899\n",
      "11 Train Loss 26920962000000.0 Test MSE 353.61523250714873 Test RE 0.9585793407045822\n",
      "12 Train Loss 25494062000000.0 Test MSE 353.61351217484685 Test RE 0.9585770089653671\n",
      "13 Train Loss 24450987000000.0 Test MSE 353.61387901661556 Test RE 0.9585775061833496\n",
      "14 Train Loss 23282910000000.0 Test MSE 353.6120571641937 Test RE 0.9585750368388128\n",
      "15 Train Loss 22794824000000.0 Test MSE 353.61152431873677 Test RE 0.9585743146171922\n",
      "16 Train Loss 22391002000000.0 Test MSE 353.6104190870028 Test RE 0.9585728165783743\n",
      "17 Train Loss 21760900000000.0 Test MSE 353.6072877479076 Test RE 0.9585685723267642\n",
      "18 Train Loss 20944886000000.0 Test MSE 353.6022781616512 Test RE 0.9585617822391164\n",
      "19 Train Loss 19703210000000.0 Test MSE 353.5942936442409 Test RE 0.9585509597742762\n",
      "20 Train Loss 18012818000000.0 Test MSE 353.5811274024917 Test RE 0.9585331135711883\n",
      "21 Train Loss 16269418000000.0 Test MSE 353.5616703660757 Test RE 0.9585067398922338\n",
      "22 Train Loss 15474616000000.0 Test MSE 353.56074405864587 Test RE 0.9585054842803128\n",
      "23 Train Loss 14534199000000.0 Test MSE 353.5626267200481 Test RE 0.9585080362305891\n",
      "24 Train Loss 14271650000000.0 Test MSE 353.561496381353 Test RE 0.9585065040556545\n",
      "25 Train Loss 14075253000000.0 Test MSE 353.56108661897247 Test RE 0.9585059486218406\n",
      "26 Train Loss 13797031000000.0 Test MSE 353.5605195244567 Test RE 0.9585051799234383\n",
      "27 Train Loss 13545757000000.0 Test MSE 353.560510960564 Test RE 0.9585051683150504\n",
      "28 Train Loss 13290307000000.0 Test MSE 353.5624765798125 Test RE 0.9585078327155299\n",
      "29 Train Loss 12535084000000.0 Test MSE 353.5719978176632 Test RE 0.9585207386659936\n",
      "30 Train Loss 11680465000000.0 Test MSE 353.5889901245344 Test RE 0.9585437711501116\n",
      "31 Train Loss 10961919000000.0 Test MSE 353.6015007087464 Test RE 0.9585607284604815\n",
      "32 Train Loss 10218577000000.0 Test MSE 353.61140223474837 Test RE 0.958574149143816\n",
      "33 Train Loss 9384315000000.0 Test MSE 353.62568546388155 Test RE 0.9585935085251467\n",
      "34 Train Loss 8660005000000.0 Test MSE 353.639375858077 Test RE 0.9586120640154995\n",
      "35 Train Loss 8278858300000.0 Test MSE 353.6488946666201 Test RE 0.9586249652701752\n",
      "36 Train Loss 8123613000000.0 Test MSE 353.6552686489582 Test RE 0.9586336041082401\n",
      "37 Train Loss 8063614000000.0 Test MSE 353.6575904132392 Test RE 0.9586367508415208\n",
      "38 Train Loss 8027204000000.0 Test MSE 353.66027734083536 Test RE 0.9586403924750077\n",
      "39 Train Loss 8007612000000.0 Test MSE 353.66119972532283 Test RE 0.9586416425934222\n",
      "40 Train Loss 7980274700000.0 Test MSE 353.6624499429722 Test RE 0.9586433370253289\n",
      "41 Train Loss 7956796000000.0 Test MSE 353.66285721729514 Test RE 0.958643889007457\n",
      "42 Train Loss 7909815600000.0 Test MSE 353.6636312016315 Test RE 0.9586449379937066\n",
      "43 Train Loss 7840488000000.0 Test MSE 353.66361910329215 Test RE 0.9586449215967534\n",
      "44 Train Loss 7756295000000.0 Test MSE 353.6629586421394 Test RE 0.9586440264693054\n",
      "45 Train Loss 7662770000000.0 Test MSE 353.6627010313464 Test RE 0.9586436773274364\n",
      "46 Train Loss 7522275000000.0 Test MSE 353.6642472706684 Test RE 0.9586457729554532\n",
      "47 Train Loss 7320854600000.0 Test MSE 353.6684419972709 Test RE 0.9586514580721045\n",
      "48 Train Loss 7258552400000.0 Test MSE 353.67368503027785 Test RE 0.9586585639119423\n",
      "49 Train Loss 7200747000000.0 Test MSE 353.67579923998875 Test RE 0.9586614292682043\n",
      "50 Train Loss 6937934000000.0 Test MSE 353.69539301216923 Test RE 0.9586879840021352\n",
      "51 Train Loss 6778597000000.0 Test MSE 353.71050909294405 Test RE 0.9587084697795296\n",
      "52 Train Loss 6639501400000.0 Test MSE 353.72559846624915 Test RE 0.9587289189255664\n",
      "53 Train Loss 6483168700000.0 Test MSE 353.74622191678196 Test RE 0.9587568671576555\n",
      "54 Train Loss 6261130700000.0 Test MSE 353.77830380120207 Test RE 0.9588003418688101\n",
      "55 Train Loss 6072612400000.0 Test MSE 353.8109119406082 Test RE 0.9588445276971694\n",
      "56 Train Loss 5908637700000.0 Test MSE 353.83291549144883 Test RE 0.9588743425722762\n",
      "57 Train Loss 5669343000000.0 Test MSE 353.8520708426177 Test RE 0.9589002973737822\n",
      "58 Train Loss 5423726000000.0 Test MSE 353.8586347129735 Test RE 0.9589091910168699\n",
      "59 Train Loss 5301773000000.0 Test MSE 353.86417818826567 Test RE 0.9589167020227741\n",
      "60 Train Loss 5235377000000.0 Test MSE 353.86070705824574 Test RE 0.9589119989009006\n",
      "61 Train Loss 5199504600000.0 Test MSE 353.8587605293071 Test RE 0.9589093614895051\n",
      "62 Train Loss 5163160400000.0 Test MSE 353.8553662766957 Test RE 0.958904762495821\n",
      "63 Train Loss 5126390000000.0 Test MSE 353.85454165960897 Test RE 0.9589036451894468\n",
      "64 Train Loss 5081872700000.0 Test MSE 353.8526129471452 Test RE 0.9589010318954032\n",
      "65 Train Loss 5036290600000.0 Test MSE 353.8494946368481 Test RE 0.958896806749905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 Train Loss 4995331000000.0 Test MSE 353.84676553829115 Test RE 0.9588931089504492\n",
      "67 Train Loss 4961866400000.0 Test MSE 353.8453177797713 Test RE 0.9588911473003461\n",
      "68 Train Loss 4934829000000.0 Test MSE 353.8447167100372 Test RE 0.9588903328757055\n",
      "69 Train Loss 4909980000000.0 Test MSE 353.84606307665274 Test RE 0.9588921571458487\n",
      "70 Train Loss 4890582000000.0 Test MSE 353.84608388521883 Test RE 0.9588921853405533\n",
      "71 Train Loss 4863263500000.0 Test MSE 353.84732476144904 Test RE 0.9588938666725377\n",
      "72 Train Loss 4827801700000.0 Test MSE 353.8473792807969 Test RE 0.9588939405437565\n",
      "73 Train Loss 4772226700000.0 Test MSE 353.84743919681705 Test RE 0.9588940217272128\n",
      "74 Train Loss 4685208000000.0 Test MSE 353.84475526809035 Test RE 0.9588903851202942\n",
      "75 Train Loss 4571787000000.0 Test MSE 353.83827157282064 Test RE 0.9588815999377502\n",
      "76 Train Loss 4452365400000.0 Test MSE 353.82949213163266 Test RE 0.9588697039707089\n",
      "77 Train Loss 4317425700000.0 Test MSE 353.82007444723064 Test RE 0.9588569430328204\n",
      "78 Train Loss 4166305400000.0 Test MSE 353.80631429747694 Test RE 0.9588382977603167\n",
      "79 Train Loss 4084336000000.0 Test MSE 353.79409383274367 Test RE 0.9588217384871904\n",
      "80 Train Loss 4031521500000.0 Test MSE 353.79072938389874 Test RE 0.9588171794591909\n",
      "81 Train Loss 3963992700000.0 Test MSE 353.7858046175555 Test RE 0.9588105060692246\n",
      "82 Train Loss 3882902600000.0 Test MSE 353.7838737797766 Test RE 0.9588078896413609\n",
      "83 Train Loss 3823379200000.0 Test MSE 353.782171145067 Test RE 0.9588055824395949\n",
      "84 Train Loss 3762251000000.0 Test MSE 353.78483497727757 Test RE 0.958809192135892\n",
      "85 Train Loss 3680859300000.0 Test MSE 353.7874485084215 Test RE 0.9588127336571909\n",
      "86 Train Loss 3582231800000.0 Test MSE 353.7947301808046 Test RE 0.958822600774168\n",
      "87 Train Loss 3530701700000.0 Test MSE 353.7967394945917 Test RE 0.9588253235008957\n",
      "88 Train Loss 3506722300000.0 Test MSE 353.79530949564673 Test RE 0.958823385777291\n",
      "89 Train Loss 3485994300000.0 Test MSE 353.79540383035294 Test RE 0.9588235136058886\n",
      "90 Train Loss 3471827300000.0 Test MSE 353.7955626955315 Test RE 0.9588237288767061\n",
      "91 Train Loss 3461755700000.0 Test MSE 353.7952280029559 Test RE 0.958823275350301\n",
      "92 Train Loss 3454901200000.0 Test MSE 353.79536088892803 Test RE 0.9588234554179518\n",
      "93 Train Loss 3440873300000.0 Test MSE 353.795264334672 Test RE 0.9588233245817369\n",
      "94 Train Loss 3426847000000.0 Test MSE 353.794468566433 Test RE 0.9588222462722531\n",
      "95 Train Loss 3398471300000.0 Test MSE 353.7980818017757 Test RE 0.9588271423939991\n",
      "96 Train Loss 3359617600000.0 Test MSE 353.8003286522344 Test RE 0.9588301869805798\n",
      "97 Train Loss 3285808300000.0 Test MSE 353.80490277506965 Test RE 0.9588363850992968\n",
      "98 Train Loss 3210598700000.0 Test MSE 353.81091092161654 Test RE 0.9588445263164113\n",
      "99 Train Loss 3164787400000.0 Test MSE 353.8160828737445 Test RE 0.9588515344098765\n",
      "Training time: 10.98\n",
      "Training time: 10.98\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "1 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "2 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "3 Train Loss 69697670000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "4 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "5 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "6 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "7 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "8 Train Loss 69697670000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "9 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "10 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "11 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "12 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "13 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "14 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "15 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "16 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "17 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "18 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "19 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "20 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "21 Train Loss 69697656000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "22 Train Loss 69697656000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "23 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "24 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "25 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "26 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "27 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "28 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "29 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "30 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "31 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "32 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "33 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "34 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "35 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "36 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "37 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "38 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "39 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "40 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "41 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "42 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "43 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "44 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "45 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "46 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "47 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "48 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "49 Train Loss 69697656000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "50 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "51 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "52 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "53 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "54 Train Loss 69697656000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "55 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "57 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "58 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "59 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "60 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "61 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "62 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "63 Train Loss 69697656000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "64 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "65 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "66 Train Loss 69697670000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "67 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "68 Train Loss 69697656000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "69 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "70 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "71 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "72 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "73 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "74 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "75 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "76 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "77 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "78 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "79 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "80 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "81 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "82 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "83 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "84 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "85 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "86 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "87 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "88 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "89 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "90 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "91 Train Loss 69697670000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "92 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "93 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "94 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "95 Train Loss 69697660000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "96 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "97 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "98 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "99 Train Loss 69697664000000.0 Test MSE 385.4212190359124 Test RE 1.0007610471562114\n",
      "Training time: 9.93\n",
      "Training time: 9.93\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "1 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "2 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "3 Train Loss 39277832000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "4 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "5 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "6 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "7 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "8 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "9 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "10 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "11 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "12 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "13 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "14 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "15 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "16 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "17 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "18 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "19 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "20 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "21 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "22 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "23 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "24 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "25 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "26 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "27 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "28 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "29 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "30 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "31 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "32 Train Loss 39277832000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "33 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "34 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "35 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "36 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "37 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "38 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "39 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "40 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "41 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "42 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "43 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "45 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "46 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "47 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "48 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "49 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "50 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "51 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "52 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "53 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "54 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "55 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "56 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "57 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "58 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "59 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "60 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "61 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "62 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "63 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "64 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "65 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "66 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "67 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "68 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "69 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "70 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "71 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "72 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "73 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "74 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "75 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "76 Train Loss 39277832000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "77 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "78 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "79 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "80 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "81 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "82 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "83 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "84 Train Loss 39277832000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "85 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "86 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "87 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "88 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "89 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "90 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "91 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "92 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "93 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "94 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "95 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "96 Train Loss 39277837000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "97 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "98 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "99 Train Loss 39277840000000.0 Test MSE 406.0199576812837 Test RE 1.0271556829815256\n",
      "Training time: 9.84\n",
      "Training time: 9.84\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 54754980000000.0 Test MSE 417.4027759079756 Test RE 1.0414543757117316\n",
      "1 Train Loss 54754978000000.0 Test MSE 417.4027759079756 Test RE 1.0414543757117316\n",
      "2 Train Loss 54754980000000.0 Test MSE 417.4027759079756 Test RE 1.0414543757117316\n",
      "3 Train Loss 54355873000000.0 Test MSE 417.40697898042794 Test RE 1.0414596192048637\n",
      "4 Train Loss 51789685000000.0 Test MSE 417.43002520730727 Test RE 1.0414883697823418\n",
      "5 Train Loss 48098047000000.0 Test MSE 417.47265762136425 Test RE 1.0415415523843499\n",
      "6 Train Loss 45779580000000.0 Test MSE 417.4864820429277 Test RE 1.0415587973344482\n",
      "7 Train Loss 44215600000000.0 Test MSE 417.4845812774307 Test RE 1.0415564262860868\n",
      "8 Train Loss 40573633000000.0 Test MSE 417.5079347180082 Test RE 1.041585557404994\n",
      "9 Train Loss 32912320000000.0 Test MSE 417.609542430411 Test RE 1.0417122935441985\n",
      "10 Train Loss 27444371000000.0 Test MSE 417.54377680961727 Test RE 1.041630265302339\n",
      "11 Train Loss 26671223000000.0 Test MSE 417.5506810109739 Test RE 1.0416388770884097\n",
      "12 Train Loss 25287526000000.0 Test MSE 417.5540415555763 Test RE 1.0416430687554603\n",
      "13 Train Loss 24272907000000.0 Test MSE 417.5508514815402 Test RE 1.0416390897197847\n",
      "14 Train Loss 23873557000000.0 Test MSE 417.5520003550498 Test RE 1.0416405227317513\n",
      "15 Train Loss 23213707000000.0 Test MSE 417.554636320713 Test RE 1.041643810614836\n",
      "16 Train Loss 22789872000000.0 Test MSE 417.5579444630955 Test RE 1.0416479369002518\n",
      "17 Train Loss 22538713000000.0 Test MSE 417.55911848033446 Test RE 1.0416494012620146\n",
      "18 Train Loss 22323317000000.0 Test MSE 417.5582897331123 Test RE 1.041648367558716\n",
      "19 Train Loss 22081792000000.0 Test MSE 417.5568278010809 Test RE 1.0416465440762415\n",
      "20 Train Loss 21781790000000.0 Test MSE 417.5492812435776 Test RE 1.0416371311286647\n",
      "21 Train Loss 21534765000000.0 Test MSE 417.5391034857395 Test RE 1.041624436106118\n",
      "22 Train Loss 21267776000000.0 Test MSE 417.520951095161 Test RE 1.0416017937009499\n",
      "23 Train Loss 20857817000000.0 Test MSE 417.48840430670725 Test RE 1.0415611951946566\n",
      "24 Train Loss 20615988000000.0 Test MSE 417.46932023122247 Test RE 1.04153738919259\n",
      "25 Train Loss 20265460000000.0 Test MSE 417.44833837487766 Test RE 1.0415112152183466\n",
      "26 Train Loss 19969654000000.0 Test MSE 417.4271549246781 Test RE 1.0414847890970358\n",
      "27 Train Loss 19798631000000.0 Test MSE 417.40949473050773 Test RE 1.0414627576863749\n",
      "28 Train Loss 19563756000000.0 Test MSE 417.38050048319246 Test RE 1.0414265858293086\n",
      "29 Train Loss 18991947000000.0 Test MSE 417.3172324072394 Test RE 1.0413476511960764\n",
      "30 Train Loss 18467604000000.0 Test MSE 417.30529269097894 Test RE 1.041332754274201\n",
      "31 Train Loss 17815392000000.0 Test MSE 417.24491919269445 Test RE 1.0412574243222406\n",
      "32 Train Loss 17666744000000.0 Test MSE 417.24847761829676 Test RE 1.041261864435238\n",
      "33 Train Loss 17460184000000.0 Test MSE 417.2618195252446 Test RE 1.0412785119589498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 Train Loss 17073182000000.0 Test MSE 417.2507471049949 Test RE 1.041264696233152\n",
      "35 Train Loss 15909960000000.0 Test MSE 417.1713673422361 Test RE 1.0411656439526111\n",
      "36 Train Loss 15612939000000.0 Test MSE 417.19597361513627 Test RE 1.041196349353566\n",
      "37 Train Loss 14985256000000.0 Test MSE 417.2416119522744 Test RE 1.041253297614752\n",
      "38 Train Loss 14754924000000.0 Test MSE 417.15543897762234 Test RE 1.0411457669600799\n",
      "39 Train Loss 14117945000000.0 Test MSE 417.2055283938563 Test RE 1.0412082722200262\n",
      "40 Train Loss 13579806000000.0 Test MSE 417.26356901139957 Test RE 1.0412806948814526\n",
      "41 Train Loss 13261665000000.0 Test MSE 417.30108043608993 Test RE 1.0413274986853898\n",
      "42 Train Loss 13088411000000.0 Test MSE 417.32442679339636 Test RE 1.0413566273708612\n",
      "43 Train Loss 12905564000000.0 Test MSE 417.3437281313966 Test RE 1.0413807085683136\n",
      "44 Train Loss 12598852000000.0 Test MSE 417.37243702085294 Test RE 1.0414165260102666\n",
      "45 Train Loss 12242300000000.0 Test MSE 417.40197381753006 Test RE 1.0414533750703203\n",
      "46 Train Loss 11954730000000.0 Test MSE 417.4233131251416 Test RE 1.0414799964225068\n",
      "47 Train Loss 11703909000000.0 Test MSE 417.4345467472447 Test RE 1.0414940103906138\n",
      "48 Train Loss 11439650000000.0 Test MSE 417.44071430483217 Test RE 1.04150170435199\n",
      "49 Train Loss 11174029000000.0 Test MSE 417.44289929631174 Test RE 1.0415044300915912\n",
      "50 Train Loss 10856478000000.0 Test MSE 417.45260801496227 Test RE 1.041516541466236\n",
      "51 Train Loss 10577234000000.0 Test MSE 417.4631949164952 Test RE 1.041529748190723\n",
      "52 Train Loss 10355464000000.0 Test MSE 417.4810274995741 Test RE 1.0415519932265114\n",
      "53 Train Loss 9591124000000.0 Test MSE 417.54130680081306 Test RE 1.041627184380201\n",
      "54 Train Loss 9586186000000.0 Test MSE 417.54327786176987 Test RE 1.0416296429492315\n",
      "55 Train Loss 9491523000000.0 Test MSE 417.5442487421839 Test RE 1.0416308539581076\n",
      "56 Train Loss 9251662000000.0 Test MSE 417.5545189040111 Test RE 1.0416436641592846\n",
      "57 Train Loss 9136483000000.0 Test MSE 417.5526266181161 Test RE 1.041641303880857\n",
      "58 Train Loss 9012313000000.0 Test MSE 417.5539651618466 Test RE 1.0416429734683896\n",
      "59 Train Loss 8845181000000.0 Test MSE 417.5462036773217 Test RE 1.0416332924043075\n",
      "60 Train Loss 8710546000000.0 Test MSE 417.54173022491625 Test RE 1.041627712531473\n",
      "61 Train Loss 8591820500000.0 Test MSE 417.53279348375276 Test RE 1.041616565373726\n",
      "62 Train Loss 8434606000000.0 Test MSE 417.5293543074341 Test RE 1.0416122755185846\n",
      "63 Train Loss 8289133300000.0 Test MSE 417.5223412761417 Test RE 1.0416035277621916\n",
      "64 Train Loss 8132554000000.0 Test MSE 417.5234406671496 Test RE 1.041604899100543\n",
      "65 Train Loss 7966681000000.0 Test MSE 417.523790689997 Test RE 1.0416053357052946\n",
      "66 Train Loss 7748337000000.0 Test MSE 417.5303992999784 Test RE 1.041613578991466\n",
      "67 Train Loss 7514219300000.0 Test MSE 417.5423279425798 Test RE 1.0416284580845694\n",
      "68 Train Loss 7208506500000.0 Test MSE 417.55780365873454 Test RE 1.0416477612736277\n",
      "69 Train Loss 6864762300000.0 Test MSE 417.58106405497927 Test RE 1.0416767737864412\n",
      "70 Train Loss 6462317300000.0 Test MSE 417.6078304226423 Test RE 1.041710158270473\n",
      "71 Train Loss 6187207600000.0 Test MSE 417.6328026543779 Test RE 1.0417413040449224\n",
      "72 Train Loss 6023210000000.0 Test MSE 417.64557360587366 Test RE 1.0417572318239126\n",
      "73 Train Loss 5917769000000.0 Test MSE 417.6512591377723 Test RE 1.0417643226734141\n",
      "74 Train Loss 5827747000000.0 Test MSE 417.6565241632857 Test RE 1.0417708890350996\n",
      "75 Train Loss 5771818400000.0 Test MSE 417.6642378967343 Test RE 1.0417805092677357\n",
      "76 Train Loss 5740245000000.0 Test MSE 417.6709729305259 Test RE 1.0417889088361103\n",
      "77 Train Loss 5712473000000.0 Test MSE 417.67896041370574 Test RE 1.0417988703035599\n",
      "78 Train Loss 5687732000000.0 Test MSE 417.68943972963564 Test RE 1.0418119392776468\n",
      "79 Train Loss 5644640300000.0 Test MSE 417.7040464111864 Test RE 1.0418301553022202\n",
      "80 Train Loss 5578846400000.0 Test MSE 417.7221548427403 Test RE 1.0418527379247184\n",
      "81 Train Loss 5474093000000.0 Test MSE 417.751644453613 Test RE 1.0418895127153829\n",
      "82 Train Loss 5358428400000.0 Test MSE 417.7810601987588 Test RE 1.0419261940992715\n",
      "83 Train Loss 5245590600000.0 Test MSE 417.80701736292974 Test RE 1.0419585615660782\n",
      "84 Train Loss 5160397400000.0 Test MSE 417.8257161470042 Test RE 1.0419818775222016\n",
      "85 Train Loss 5090610500000.0 Test MSE 417.84585684514354 Test RE 1.0420069908514606\n",
      "86 Train Loss 5057313500000.0 Test MSE 417.8474128904621 Test RE 1.0420089310508953\n",
      "87 Train Loss 5006686000000.0 Test MSE 417.85050371526614 Test RE 1.0420127849231402\n",
      "88 Train Loss 4978773000000.0 Test MSE 417.8561737807433 Test RE 1.0420198547490982\n",
      "89 Train Loss 4952691300000.0 Test MSE 417.8638122340688 Test RE 1.0420293788199981\n",
      "90 Train Loss 4918832000000.0 Test MSE 417.87311827898907 Test RE 1.0420409820241212\n",
      "91 Train Loss 4885422000000.0 Test MSE 417.88348411421435 Test RE 1.042053906471351\n",
      "92 Train Loss 4839396000000.0 Test MSE 417.8975756483703 Test RE 1.0420714759793406\n",
      "93 Train Loss 4778899300000.0 Test MSE 417.9164495362183 Test RE 1.0420950077240168\n",
      "94 Train Loss 4721961600000.0 Test MSE 417.93379497651006 Test RE 1.0421166333494303\n",
      "95 Train Loss 4674607000000.0 Test MSE 417.9516090232877 Test RE 1.0421388427482152\n",
      "96 Train Loss 4624832500000.0 Test MSE 417.9635379736012 Test RE 1.042153714725621\n",
      "97 Train Loss 4573963500000.0 Test MSE 417.973256097268 Test RE 1.0421658302798373\n",
      "98 Train Loss 4535453000000.0 Test MSE 417.9767705339411 Test RE 1.0421702116823088\n",
      "99 Train Loss 4503857000000.0 Test MSE 417.97524470793485 Test RE 1.0421683094571428\n",
      "Training time: 11.26\n",
      "Training time: 11.26\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "1 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "2 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "3 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "4 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "5 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "6 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "7 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "8 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "9 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "10 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "11 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "12 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "13 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "14 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "15 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "16 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "17 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "18 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "19 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "20 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "21 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "22 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "24 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "25 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "26 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "27 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "28 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "29 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "30 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "31 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "32 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "33 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "34 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "35 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "36 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "37 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "38 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "39 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "40 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "41 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "42 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "43 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "44 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "45 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "46 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "47 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "48 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "49 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "50 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "51 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "52 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "53 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "54 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "55 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "56 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "57 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "58 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "59 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "60 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "61 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "62 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "63 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "64 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "65 Train Loss 11719158000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "66 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "67 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "68 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "69 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "70 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "71 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "72 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "73 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "74 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "75 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "76 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "77 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "78 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "79 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "80 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "81 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "82 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "83 Train Loss 11719160000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "84 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "85 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "86 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "87 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "88 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "89 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "90 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "91 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "92 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "93 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "94 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "95 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "96 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "97 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "98 Train Loss 11719159000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "99 Train Loss 11719158000000.0 Test MSE 379.8510431036025 Test RE 0.9935031403548371\n",
      "Training time: 9.85\n",
      "Training time: 9.85\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 5687826400000.0 Test MSE 361.67011030203037 Test RE 0.9694354388810381\n",
      "1 Train Loss 5687827000000.0 Test MSE 361.67011030203037 Test RE 0.9694354388810381\n",
      "2 Train Loss 5687826400000.0 Test MSE 361.67011030203037 Test RE 0.9694354388810381\n",
      "3 Train Loss 5403315300000.0 Test MSE 361.66806008225774 Test RE 0.9694326911304426\n",
      "4 Train Loss 3040609000000.0 Test MSE 361.6665594371762 Test RE 0.9694306799272575\n",
      "5 Train Loss 2732629200000.0 Test MSE 361.6651906557673 Test RE 0.9694288454475235\n",
      "6 Train Loss 2627011000000.0 Test MSE 361.66723876088435 Test RE 0.9694315903749029\n",
      "7 Train Loss 2550948400000.0 Test MSE 361.66941802899464 Test RE 0.9694345110820354\n",
      "8 Train Loss 2478072100000.0 Test MSE 361.6742071019696 Test RE 0.9694409294810172\n",
      "9 Train Loss 2381995300000.0 Test MSE 361.68118294361625 Test RE 0.9694502785494168\n",
      "10 Train Loss 2237505200000.0 Test MSE 361.68708790796643 Test RE 0.9694581923506302\n",
      "11 Train Loss 2032603400000.0 Test MSE 361.67361155798534 Test RE 0.9694401313249144\n",
      "12 Train Loss 1845168400000.0 Test MSE 361.66524991093894 Test RE 0.9694289248630599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Train Loss 1733916400000.0 Test MSE 361.6468032595839 Test RE 0.9694042018001909\n",
      "14 Train Loss 1600820600000.0 Test MSE 361.6195390025859 Test RE 0.9693676598161212\n",
      "15 Train Loss 1409099200000.0 Test MSE 361.5594359103538 Test RE 0.9692870994361565\n",
      "16 Train Loss 1244479900000.0 Test MSE 361.4657384125141 Test RE 0.9691614967667284\n",
      "17 Train Loss 1122136200000.0 Test MSE 361.3662079818533 Test RE 0.9690280571457488\n",
      "18 Train Loss 1077654200000.0 Test MSE 361.30620543393957 Test RE 0.9689476033499025\n",
      "19 Train Loss 1027381900000.0 Test MSE 361.301210586261 Test RE 0.9689409057588548\n",
      "20 Train Loss 973754000000.0 Test MSE 361.28058373143153 Test RE 0.9689132467196939\n",
      "21 Train Loss 961601340000.0 Test MSE 361.26959109432806 Test RE 0.968898506109203\n",
      "22 Train Loss 945906400000.0 Test MSE 361.26196165512965 Test RE 0.9688882752574344\n",
      "23 Train Loss 915822800000.0 Test MSE 361.2477837402601 Test RE 0.9688692628070212\n",
      "24 Train Loss 879933260000.0 Test MSE 361.24461521028337 Test RE 0.9688650137871342\n",
      "25 Train Loss 828905100000.0 Test MSE 361.25673581449536 Test RE 0.9688812674977376\n",
      "26 Train Loss 773977300000.0 Test MSE 361.2697007739303 Test RE 0.9688986531855099\n",
      "27 Train Loss 745885340000.0 Test MSE 361.2897812456867 Test RE 0.9689255799823749\n",
      "28 Train Loss 737159100000.0 Test MSE 361.29671429058016 Test RE 0.9689348766365654\n",
      "29 Train Loss 732079600000.0 Test MSE 361.3039484698142 Test RE 0.9689445769925326\n",
      "30 Train Loss 724827400000.0 Test MSE 361.3174244222232 Test RE 0.968962646722063\n",
      "31 Train Loss 718840500000.0 Test MSE 361.324854966768 Test RE 0.9689726100985926\n",
      "32 Train Loss 714014060000.0 Test MSE 361.33429751110714 Test RE 0.9689852711526722\n",
      "33 Train Loss 709112370000.0 Test MSE 361.3438556981154 Test RE 0.9689980870979694\n",
      "34 Train Loss 703182500000.0 Test MSE 361.35278785685 Test RE 0.9690100634898519\n",
      "35 Train Loss 696894100000.0 Test MSE 361.3641092350541 Test RE 0.9690252431755415\n",
      "36 Train Loss 689706600000.0 Test MSE 361.37367813999384 Test RE 0.969038072962474\n",
      "37 Train Loss 681269700000.0 Test MSE 361.38206687477964 Test RE 0.9690493202623025\n",
      "38 Train Loss 672157000000.0 Test MSE 361.3846281296916 Test RE 0.9690527542704636\n",
      "39 Train Loss 665274740000.0 Test MSE 361.3804972958476 Test RE 0.9690472158398808\n",
      "40 Train Loss 660418130000.0 Test MSE 361.3768255764025 Test RE 0.969042292942035\n",
      "41 Train Loss 655771700000.0 Test MSE 361.37289908470154 Test RE 0.969037028427839\n",
      "42 Train Loss 649592300000.0 Test MSE 361.36443439772034 Test RE 0.9690256791496086\n",
      "43 Train Loss 642716600000.0 Test MSE 361.3561709056273 Test RE 0.9690145995011238\n",
      "44 Train Loss 634332450000.0 Test MSE 361.34930318375837 Test RE 0.9690053911979146\n",
      "45 Train Loss 625106940000.0 Test MSE 361.3409954895584 Test RE 0.9689942520499104\n",
      "46 Train Loss 617047500000.0 Test MSE 361.3328293799187 Test RE 0.968983302616953\n",
      "47 Train Loss 607783400000.0 Test MSE 361.3255808950622 Test RE 0.9689735834668384\n",
      "48 Train Loss 597979100000.0 Test MSE 361.32297532309894 Test RE 0.9689700897549518\n",
      "49 Train Loss 587453400000.0 Test MSE 361.3301735182928 Test RE 0.9689797415091745\n",
      "50 Train Loss 576573900000.0 Test MSE 361.3519556859833 Test RE 0.9690089477071102\n",
      "51 Train Loss 565681700000.0 Test MSE 361.3805063329785 Test RE 0.9690472279564818\n",
      "52 Train Loss 555285940000.0 Test MSE 361.41360086495683 Test RE 0.9690915986816797\n",
      "53 Train Loss 547526870000.0 Test MSE 361.44587355949574 Test RE 0.969134865590998\n",
      "54 Train Loss 540612070000.0 Test MSE 361.4775284887293 Test RE 0.9691773024068253\n",
      "55 Train Loss 533091000000.0 Test MSE 361.51606465179395 Test RE 0.9692289617442266\n",
      "56 Train Loss 522565160000.0 Test MSE 361.5830957068167 Test RE 0.9693188131159081\n",
      "57 Train Loss 506088000000.0 Test MSE 361.74496064342077 Test RE 0.9695357496580911\n",
      "58 Train Loss 496997630000.0 Test MSE 361.838983097077 Test RE 0.9696617392605269\n",
      "59 Train Loss 490516100000.0 Test MSE 361.88808142869823 Test RE 0.9697275242656622\n",
      "60 Train Loss 486044200000.0 Test MSE 361.93751780336356 Test RE 0.9697937576932862\n",
      "61 Train Loss 483483900000.0 Test MSE 361.9632222500425 Test RE 0.9698281939815029\n",
      "62 Train Loss 481521400000.0 Test MSE 361.9857875680095 Test RE 0.9698584237661309\n",
      "63 Train Loss 479528580000.0 Test MSE 362.0095670756132 Test RE 0.9698902791287006\n",
      "64 Train Loss 476248000000.0 Test MSE 362.04407887700245 Test RE 0.9699365097618037\n",
      "65 Train Loss 470325070000.0 Test MSE 362.1189932226252 Test RE 0.9700368544501021\n",
      "66 Train Loss 447659150000.0 Test MSE 362.5218702599177 Test RE 0.9705763137764808\n",
      "67 Train Loss 435881740000.0 Test MSE 362.51659496187875 Test RE 0.9705692519984597\n",
      "68 Train Loss 424970850000.0 Test MSE 362.51425984686534 Test RE 0.9705661260802079\n",
      "69 Train Loss 413754980000.0 Test MSE 362.52875506934566 Test RE 0.9705855300497294\n",
      "70 Train Loss 409811400000.0 Test MSE 362.5386230185086 Test RE 0.9705987395170614\n",
      "71 Train Loss 405846100000.0 Test MSE 362.5488343981913 Test RE 0.9706124085192452\n",
      "72 Train Loss 402152550000.0 Test MSE 362.5559476010372 Test RE 0.9706219301731951\n",
      "73 Train Loss 398764500000.0 Test MSE 362.5631147630493 Test RE 0.9706315239618118\n",
      "74 Train Loss 395655200000.0 Test MSE 362.56987569791113 Test RE 0.9706405738977378\n",
      "75 Train Loss 391336000000.0 Test MSE 362.5779591312014 Test RE 0.9706513939701191\n",
      "76 Train Loss 387011200000.0 Test MSE 362.5926068622417 Test RE 0.970671000370069\n",
      "77 Train Loss 383312130000.0 Test MSE 362.59731117663847 Test RE 0.9706772971431749\n",
      "78 Train Loss 373430580000.0 Test MSE 362.584993591729 Test RE 0.9706608098427986\n",
      "79 Train Loss 361506800000.0 Test MSE 362.54279192767643 Test RE 0.9706043200622699\n",
      "80 Train Loss 350532500000.0 Test MSE 362.5089538785671 Test RE 0.9705590231707355\n",
      "81 Train Loss 341539300000.0 Test MSE 362.49838058335706 Test RE 0.9705448689246264\n",
      "82 Train Loss 336187100000.0 Test MSE 362.4795702223146 Test RE 0.9705196873828457\n",
      "83 Train Loss 331583750000.0 Test MSE 362.4627572159583 Test RE 0.9704971791586157\n",
      "84 Train Loss 321790540000.0 Test MSE 362.4269060161852 Test RE 0.9704491820224199\n",
      "85 Train Loss 314736180000.0 Test MSE 362.3935442084588 Test RE 0.9704045155206011\n",
      "86 Train Loss 306036340000.0 Test MSE 362.34634915591175 Test RE 0.9703413248484356\n",
      "87 Train Loss 302065840000.0 Test MSE 362.32643702506823 Test RE 0.9703146627488703\n",
      "88 Train Loss 297611430000.0 Test MSE 362.3135534837631 Test RE 0.9702974114543345\n",
      "89 Train Loss 295763500000.0 Test MSE 362.3077627077611 Test RE 0.97028965740075\n",
      "90 Train Loss 293750600000.0 Test MSE 362.31446791487434 Test RE 0.9702986359042232\n",
      "91 Train Loss 292755280000.0 Test MSE 362.31448807836904 Test RE 0.9702986629037124\n",
      "92 Train Loss 292172700000.0 Test MSE 362.31019889348636 Test RE 0.9702929195470282\n",
      "93 Train Loss 291866280000.0 Test MSE 362.3080488686578 Test RE 0.970290040581753\n",
      "94 Train Loss 291691660000.0 Test MSE 362.30629023201635 Test RE 0.9702876856938416\n",
      "95 Train Loss 291245130000.0 Test MSE 362.29841376102723 Test RE 0.9702771387001097\n",
      "96 Train Loss 289787150000.0 Test MSE 362.28364812022903 Test RE 0.9702573664500292\n",
      "97 Train Loss 288380940000.0 Test MSE 362.261427300223 Test RE 0.9702276104218203\n",
      "98 Train Loss 287617680000.0 Test MSE 362.25990355653687 Test RE 0.9702255699344282\n",
      "99 Train Loss 285227550000.0 Test MSE 362.25995613435555 Test RE 0.9702256403429136\n",
      "Training time: 11.39\n",
      "Training time: 11.39\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 27149963000000.0 Test MSE 416.0662438072297 Test RE 1.0397856598750626\n",
      "1 Train Loss 27149963000000.0 Test MSE 416.0662438072297 Test RE 1.0397856598750626\n",
      "2 Train Loss 27149963000000.0 Test MSE 416.0662438072297 Test RE 1.0397856598750626\n",
      "3 Train Loss 27148930000000.0 Test MSE 416.06619895061226 Test RE 1.0397856038247704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 23741370000000.0 Test MSE 416.10572699407464 Test RE 1.0398349946602974\n",
      "5 Train Loss 22333207000000.0 Test MSE 416.12927699335125 Test RE 1.0398644195940456\n",
      "6 Train Loss 18309622000000.0 Test MSE 416.24451215267464 Test RE 1.0400083900547128\n",
      "7 Train Loss 16122758000000.0 Test MSE 416.1922676100467 Test RE 1.0399431201639002\n",
      "8 Train Loss 14701883000000.0 Test MSE 416.16108608325544 Test RE 1.0399041626625543\n",
      "9 Train Loss 13832048000000.0 Test MSE 416.1183342954778 Test RE 1.0398507471638547\n",
      "10 Train Loss 12664333000000.0 Test MSE 416.07748986045766 Test RE 1.0397997122117209\n",
      "11 Train Loss 11645814000000.0 Test MSE 416.03697153726347 Test RE 1.0397490822598867\n",
      "12 Train Loss 11077837000000.0 Test MSE 416.0351584229156 Test RE 1.039746816612646\n",
      "13 Train Loss 10897045000000.0 Test MSE 416.04421928913246 Test RE 1.0397581389195707\n",
      "14 Train Loss 10652045000000.0 Test MSE 416.06531478073106 Test RE 1.0397844990154825\n",
      "15 Train Loss 10342738000000.0 Test MSE 416.10323324901395 Test RE 1.039831878760781\n",
      "16 Train Loss 10133694000000.0 Test MSE 416.13469629542675 Test RE 1.0398711907122675\n",
      "17 Train Loss 9869122000000.0 Test MSE 416.15432349443887 Test RE 1.0398957134429307\n",
      "18 Train Loss 9532856000000.0 Test MSE 416.1597864814396 Test RE 1.0399025389374652\n",
      "19 Train Loss 8920852000000.0 Test MSE 416.1303958260601 Test RE 1.0398658175172486\n",
      "20 Train Loss 8681885300000.0 Test MSE 416.1319892546505 Test RE 1.0398678084200452\n",
      "21 Train Loss 8193416000000.0 Test MSE 416.1290115705987 Test RE 1.0398640879618635\n",
      "22 Train Loss 7855114600000.0 Test MSE 416.1402005582264 Test RE 1.039878067939083\n",
      "23 Train Loss 7416207000000.0 Test MSE 416.1453751974375 Test RE 1.0398845332806794\n",
      "24 Train Loss 6976822000000.0 Test MSE 416.14584775181606 Test RE 1.0398851237015747\n",
      "25 Train Loss 6665265000000.0 Test MSE 416.13836013832537 Test RE 1.0398757684563555\n",
      "26 Train Loss 6393731000000.0 Test MSE 416.1062236373346 Test RE 1.0398356152079777\n",
      "27 Train Loss 6198735000000.0 Test MSE 416.0659998430676 Test RE 1.0397853550312084\n",
      "28 Train Loss 6054381400000.0 Test MSE 416.02469545154304 Test RE 1.039733742105172\n",
      "29 Train Loss 5909389500000.0 Test MSE 415.9773822085472 Test RE 1.0396746175213953\n",
      "30 Train Loss 5772290300000.0 Test MSE 415.9219681399895 Test RE 1.0396053655357966\n",
      "31 Train Loss 5641486000000.0 Test MSE 415.86319556328203 Test RE 1.0395319113191468\n",
      "32 Train Loss 5510671600000.0 Test MSE 415.8121168775844 Test RE 1.0394680687443711\n",
      "33 Train Loss 5365165000000.0 Test MSE 415.7530572129825 Test RE 1.0393942459611183\n",
      "34 Train Loss 5193494700000.0 Test MSE 415.6864684826302 Test RE 1.039311005785835\n",
      "35 Train Loss 5042771300000.0 Test MSE 415.6332489514742 Test RE 1.0392444731715682\n",
      "36 Train Loss 4927759300000.0 Test MSE 415.6033329845984 Test RE 1.039207071733192\n",
      "37 Train Loss 4895688600000.0 Test MSE 415.601774600439 Test RE 1.0392051233785609\n",
      "38 Train Loss 4871354000000.0 Test MSE 415.6123633610018 Test RE 1.0392183618015067\n",
      "39 Train Loss 4841920300000.0 Test MSE 415.626626005406 Test RE 1.0392361931704688\n",
      "40 Train Loss 4815150000000.0 Test MSE 415.65968713180916 Test RE 1.0392775255030986\n",
      "41 Train Loss 4803040000000.0 Test MSE 415.656197716997 Test RE 1.0392731631868641\n",
      "42 Train Loss 4739092700000.0 Test MSE 415.6436126582319 Test RE 1.0392574297358688\n",
      "43 Train Loss 4710039300000.0 Test MSE 415.6225214567473 Test RE 1.0392310616343887\n",
      "44 Train Loss 4691640500000.0 Test MSE 415.6070223543751 Test RE 1.0392116843172945\n",
      "45 Train Loss 4673227000000.0 Test MSE 415.59262637053683 Test RE 1.0391936858194815\n",
      "46 Train Loss 4654436500000.0 Test MSE 415.5807701111168 Test RE 1.039178862363745\n",
      "47 Train Loss 4632494500000.0 Test MSE 415.56486403768736 Test RE 1.039158975238733\n",
      "48 Train Loss 4597865000000.0 Test MSE 415.5524287733211 Test RE 1.0391434273491424\n",
      "49 Train Loss 4563610000000.0 Test MSE 415.5414987576275 Test RE 1.039129761289377\n",
      "50 Train Loss 4524386000000.0 Test MSE 415.54621429390676 Test RE 1.0391356572590957\n",
      "51 Train Loss 4484494300000.0 Test MSE 415.55185552323695 Test RE 1.0391427106053317\n",
      "52 Train Loss 4332998400000.0 Test MSE 415.58519453597904 Test RE 1.0391843940881271\n",
      "53 Train Loss 4318281900000.0 Test MSE 415.5764339543449 Test RE 1.0391734409697113\n",
      "54 Train Loss 4252982800000.0 Test MSE 415.52887391268155 Test RE 1.039113975920408\n",
      "55 Train Loss 4232319000000.0 Test MSE 415.520396305131 Test RE 1.0391033758803472\n",
      "56 Train Loss 4224655000000.0 Test MSE 415.5215673441713 Test RE 1.0391048401042138\n",
      "57 Train Loss 4212216800000.0 Test MSE 415.52573425427903 Test RE 1.0391100502376298\n",
      "58 Train Loss 4207966400000.0 Test MSE 415.529866404458 Test RE 1.0391152168828597\n",
      "59 Train Loss 4199254000000.0 Test MSE 415.53571478520973 Test RE 1.0391225293777504\n",
      "60 Train Loss 4192117000000.0 Test MSE 415.5414339509067 Test RE 1.0391296802594499\n",
      "61 Train Loss 4183243600000.0 Test MSE 415.547232119307 Test RE 1.0391369298708804\n",
      "62 Train Loss 4169371500000.0 Test MSE 415.55192970522614 Test RE 1.0391428033562953\n",
      "63 Train Loss 4152924600000.0 Test MSE 415.55384412074136 Test RE 1.0391451969786945\n",
      "64 Train Loss 4132670000000.0 Test MSE 415.5729836261228 Test RE 1.03916912708591\n",
      "65 Train Loss 4118943000000.0 Test MSE 415.57574637409874 Test RE 1.039172581301829\n",
      "66 Train Loss 4099517000000.0 Test MSE 415.5747971369911 Test RE 1.039171394488417\n",
      "67 Train Loss 4084246500000.0 Test MSE 415.57185597162294 Test RE 1.03916771719573\n",
      "68 Train Loss 4064742200000.0 Test MSE 415.56884406555037 Test RE 1.0391639514435698\n",
      "69 Train Loss 4036618600000.0 Test MSE 415.5656939201062 Test RE 1.0391600128376832\n",
      "70 Train Loss 4000549200000.0 Test MSE 415.57338102791545 Test RE 1.0391696239511918\n",
      "71 Train Loss 3969553800000.0 Test MSE 415.5737761060094 Test RE 1.0391701179109534\n",
      "72 Train Loss 3941046200000.0 Test MSE 415.5843170641277 Test RE 1.0391832970139923\n",
      "73 Train Loss 3911433000000.0 Test MSE 415.5911820236163 Test RE 1.0391918800156619\n",
      "74 Train Loss 3887293300000.0 Test MSE 415.59749292781106 Test RE 1.0391997702402158\n",
      "75 Train Loss 3866616700000.0 Test MSE 415.60206412241786 Test RE 1.039205485350874\n",
      "76 Train Loss 3842511000000.0 Test MSE 415.60089983556287 Test RE 1.039204029710686\n",
      "77 Train Loss 3818441400000.0 Test MSE 415.59460135906966 Test RE 1.039196155056234\n",
      "78 Train Loss 3783661200000.0 Test MSE 415.5809089551999 Test RE 1.039179035956744\n",
      "79 Train Loss 3736704600000.0 Test MSE 415.5604802492429 Test RE 1.0391534941868972\n",
      "80 Train Loss 3693380000000.0 Test MSE 415.5323145392803 Test RE 1.0391182779027832\n",
      "81 Train Loss 3642938400000.0 Test MSE 415.48941203692794 Test RE 1.039064633548818\n",
      "82 Train Loss 3591874300000.0 Test MSE 415.4244991757511 Test RE 1.038983462656158\n",
      "83 Train Loss 3555216300000.0 Test MSE 415.35671725858646 Test RE 1.0388986973570058\n",
      "84 Train Loss 3540493500000.0 Test MSE 415.33117542892086 Test RE 1.0388667539931242\n",
      "85 Train Loss 3534373500000.0 Test MSE 415.31992255764686 Test RE 1.0388526805094453\n",
      "86 Train Loss 3527528700000.0 Test MSE 415.3075147952259 Test RE 1.0388371624319066\n",
      "87 Train Loss 3516898500000.0 Test MSE 415.2970334962663 Test RE 1.0388240535533269\n",
      "88 Train Loss 3509378000000.0 Test MSE 415.27874743802624 Test RE 1.0388011829273396\n",
      "89 Train Loss 3496549300000.0 Test MSE 415.25584449787476 Test RE 1.038772537192988\n",
      "90 Train Loss 3483417200000.0 Test MSE 415.21892575832237 Test RE 1.0387263596066985\n",
      "91 Train Loss 3468366200000.0 Test MSE 415.17314085722717 Test RE 1.038669089463433\n",
      "92 Train Loss 3453961600000.0 Test MSE 415.1217505444099 Test RE 1.0386048040171407\n",
      "93 Train Loss 3437221000000.0 Test MSE 415.07018877872935 Test RE 1.038540300096919\n",
      "94 Train Loss 3423597500000.0 Test MSE 415.02689993517816 Test RE 1.0384861425326952\n",
      "95 Train Loss 3413834200000.0 Test MSE 414.9897597573872 Test RE 1.0384396751556457\n",
      "96 Train Loss 3401933700000.0 Test MSE 414.9585595231819 Test RE 1.038400637842906\n",
      "97 Train Loss 3391163000000.0 Test MSE 414.93947140142734 Test RE 1.038376754318517\n",
      "98 Train Loss 3378394200000.0 Test MSE 414.9243426697882 Test RE 1.0383578244895508\n",
      "99 Train Loss 3370306500000.0 Test MSE 414.9244144695162 Test RE 1.038357914329792\n",
      "Training time: 10.91\n",
      "Training time: 10.91\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 137037510000000.0 Test MSE 410.9164748460812 Test RE 1.0333307643409877\n",
      "1 Train Loss 137037500000000.0 Test MSE 410.9164748460812 Test RE 1.0333307643409877\n",
      "2 Train Loss 125898050000000.0 Test MSE 410.9052500636103 Test RE 1.0333166507765874\n",
      "3 Train Loss 125623690000000.0 Test MSE 410.90447865972243 Test RE 1.033315680839046\n",
      "4 Train Loss 81447335000000.0 Test MSE 410.91290753340354 Test RE 1.033326278974542\n",
      "5 Train Loss 71854660000000.0 Test MSE 410.91460760154115 Test RE 1.0333284165604313\n",
      "6 Train Loss 70115390000000.0 Test MSE 410.9118192462522 Test RE 1.033324910611032\n",
      "7 Train Loss 68576317000000.0 Test MSE 410.91031726134787 Test RE 1.033323022079524\n",
      "8 Train Loss 64057197000000.0 Test MSE 410.9080018753773 Test RE 1.0333201108055814\n",
      "9 Train Loss 57131034000000.0 Test MSE 410.9068308032373 Test RE 1.0333186383430712\n",
      "10 Train Loss 48625627000000.0 Test MSE 410.91163011010855 Test RE 1.033324672799533\n",
      "11 Train Loss 38164760000000.0 Test MSE 410.92790815910234 Test RE 1.0333451399046685\n",
      "12 Train Loss 29098737000000.0 Test MSE 410.9541781533782 Test RE 1.0333781694659798\n",
      "13 Train Loss 26384270000000.0 Test MSE 410.97123246949326 Test RE 1.0333996114856352\n",
      "14 Train Loss 25642765000000.0 Test MSE 410.97971018910823 Test RE 1.0334102701720391\n",
      "15 Train Loss 25237991000000.0 Test MSE 410.98457168908163 Test RE 1.0334163822853861\n",
      "16 Train Loss 24978764000000.0 Test MSE 410.9897106448324 Test RE 1.0334228431902786\n",
      "17 Train Loss 24855530000000.0 Test MSE 410.9929077536365 Test RE 1.0334268627055596\n",
      "18 Train Loss 24713066000000.0 Test MSE 410.9978043705206 Test RE 1.0334330188706138\n",
      "19 Train Loss 24453377000000.0 Test MSE 411.00928825411626 Test RE 1.0334474565895826\n",
      "20 Train Loss 24185785000000.0 Test MSE 411.0240296642328 Test RE 1.033465989426465\n",
      "21 Train Loss 23942687000000.0 Test MSE 411.0366292217864 Test RE 1.0334818292721755\n",
      "22 Train Loss 23577118000000.0 Test MSE 411.0545758195114 Test RE 1.0335043908626493\n",
      "23 Train Loss 22898731000000.0 Test MSE 411.08528661992773 Test RE 1.0335429978459183\n",
      "24 Train Loss 22133994000000.0 Test MSE 411.127364056605 Test RE 1.0335958916476076\n",
      "25 Train Loss 21530143000000.0 Test MSE 411.17102577444234 Test RE 1.0336507741220236\n",
      "26 Train Loss 21368404000000.0 Test MSE 411.1715979116852 Test RE 1.0336514932752001\n",
      "27 Train Loss 21291323000000.0 Test MSE 411.1744815790237 Test RE 1.0336551179198004\n",
      "28 Train Loss 21184035000000.0 Test MSE 411.1731913373409 Test RE 1.0336534961436143\n",
      "29 Train Loss 21089328000000.0 Test MSE 411.17213488362535 Test RE 1.03365216822657\n",
      "30 Train Loss 20983090000000.0 Test MSE 411.16995397019264 Test RE 1.0336494269064553\n",
      "31 Train Loss 20845859000000.0 Test MSE 411.17087223090226 Test RE 1.033650581123972\n",
      "32 Train Loss 20619322000000.0 Test MSE 411.17071830213104 Test RE 1.033650387641664\n",
      "33 Train Loss 20331767000000.0 Test MSE 411.17514913670743 Test RE 1.0336559570090036\n",
      "34 Train Loss 20010966000000.0 Test MSE 411.1759481161062 Test RE 1.0336569612883424\n",
      "35 Train Loss 19722210000000.0 Test MSE 411.1791729013306 Test RE 1.033661014681027\n",
      "36 Train Loss 19623097000000.0 Test MSE 411.17991451276134 Test RE 1.033661946847013\n",
      "37 Train Loss 19558629000000.0 Test MSE 411.18025103938146 Test RE 1.0336623698427354\n",
      "38 Train Loss 19545125000000.0 Test MSE 411.1806873226862 Test RE 1.0336629182269463\n",
      "39 Train Loss 19530290000000.0 Test MSE 411.18043053548087 Test RE 1.0336625954595335\n",
      "40 Train Loss 19513106000000.0 Test MSE 411.1802016326672 Test RE 1.0336623077411842\n",
      "41 Train Loss 19486038000000.0 Test MSE 411.17998769574274 Test RE 1.033662038834069\n",
      "42 Train Loss 19452014000000.0 Test MSE 411.17852681893396 Test RE 1.0336602025892527\n",
      "43 Train Loss 19396603000000.0 Test MSE 411.17633100372257 Test RE 1.0336574425596323\n",
      "44 Train Loss 19334779000000.0 Test MSE 411.17363827161074 Test RE 1.0336540579203242\n",
      "45 Train Loss 19275853000000.0 Test MSE 411.1706477582229 Test RE 1.0336502989707812\n",
      "46 Train Loss 19203386000000.0 Test MSE 411.16686636831133 Test RE 1.033645545903143\n",
      "47 Train Loss 19110094000000.0 Test MSE 411.16093887934204 Test RE 1.0336380952241673\n",
      "48 Train Loss 18961070000000.0 Test MSE 411.15331128430415 Test RE 1.033628507482937\n",
      "49 Train Loss 18791880000000.0 Test MSE 411.14466967665226 Test RE 1.0336176450397692\n",
      "50 Train Loss 18636308000000.0 Test MSE 411.13763017990175 Test RE 1.033608796354944\n",
      "51 Train Loss 18527132000000.0 Test MSE 411.13171280200527 Test RE 1.0336013581208792\n",
      "52 Train Loss 18418140000000.0 Test MSE 411.12715199483034 Test RE 1.0335956250803264\n",
      "53 Train Loss 18282755000000.0 Test MSE 411.12252336348 Test RE 1.0335898067506977\n",
      "54 Train Loss 18108600000000.0 Test MSE 411.1142768561286 Test RE 1.033579440560331\n",
      "55 Train Loss 17791076000000.0 Test MSE 411.0988523592778 Test RE 1.0335600510702774\n",
      "56 Train Loss 17351442000000.0 Test MSE 411.0680284861623 Test RE 1.0335213025784133\n",
      "57 Train Loss 17133289000000.0 Test MSE 411.0578789582003 Test RE 1.0335085433544127\n",
      "58 Train Loss 16768505000000.0 Test MSE 411.0532874518614 Test RE 1.0335027712058629\n",
      "59 Train Loss 16144780000000.0 Test MSE 411.03737141727595 Test RE 1.0334827623340388\n",
      "60 Train Loss 15480789000000.0 Test MSE 411.02023745331894 Test RE 1.0334612219070973\n",
      "61 Train Loss 15327805000000.0 Test MSE 411.01735830083527 Test RE 1.0334576022584967\n",
      "62 Train Loss 14952764000000.0 Test MSE 411.01366072633164 Test RE 1.033452953677363\n",
      "63 Train Loss 14797953000000.0 Test MSE 411.0062500175734 Test RE 1.0334436368904658\n",
      "64 Train Loss 14676602000000.0 Test MSE 411.0047080479565 Test RE 1.033441698306584\n",
      "65 Train Loss 14496600000000.0 Test MSE 410.99949940400774 Test RE 1.0334351499060461\n",
      "66 Train Loss 14314349000000.0 Test MSE 410.998999953614 Test RE 1.0334345219858783\n",
      "67 Train Loss 14109098000000.0 Test MSE 410.99851848737717 Test RE 1.033433916675432\n",
      "68 Train Loss 13855320000000.0 Test MSE 410.99903381039394 Test RE 1.0334345645513887\n",
      "69 Train Loss 13586177000000.0 Test MSE 411.00232616693654 Test RE 1.0334387037682191\n",
      "70 Train Loss 13215190000000.0 Test MSE 411.0074445104284 Test RE 1.033445138619629\n",
      "71 Train Loss 12768708000000.0 Test MSE 411.01527391408325 Test RE 1.0334549817754168\n",
      "72 Train Loss 12351854000000.0 Test MSE 411.02375467014195 Test RE 1.033465643708128\n",
      "73 Train Loss 12019558000000.0 Test MSE 411.0333815237897 Test RE 1.0334777463714584\n",
      "74 Train Loss 11674380000000.0 Test MSE 411.0450434226444 Test RE 1.0334924072579\n",
      "75 Train Loss 11352536000000.0 Test MSE 411.0577030831678 Test RE 1.033508322256147\n",
      "76 Train Loss 11178677000000.0 Test MSE 411.0620849376955 Test RE 1.0335138308149434\n",
      "77 Train Loss 11066650000000.0 Test MSE 411.0643717142177 Test RE 1.0335167055776175\n",
      "78 Train Loss 10999055000000.0 Test MSE 411.0654309017436 Test RE 1.0335180371054415\n",
      "79 Train Loss 10922166000000.0 Test MSE 411.0621199516305 Test RE 1.0335138748318775\n",
      "80 Train Loss 10809269000000.0 Test MSE 411.05766660049864 Test RE 1.0335082763925822\n",
      "81 Train Loss 10718714000000.0 Test MSE 411.05098621574797 Test RE 1.0334998782266536\n",
      "82 Train Loss 10615033000000.0 Test MSE 411.04372774970346 Test RE 1.0334907532553708\n",
      "83 Train Loss 10523479000000.0 Test MSE 411.0364771021222 Test RE 1.0334816380326215\n",
      "84 Train Loss 10459033000000.0 Test MSE 411.03014000963356 Test RE 1.033473671228781\n",
      "85 Train Loss 10399201000000.0 Test MSE 411.0262060412839 Test RE 1.033468725530102\n",
      "86 Train Loss 10349654000000.0 Test MSE 411.02414007682455 Test RE 1.0334661282354423\n",
      "87 Train Loss 10306804000000.0 Test MSE 411.0240886827471 Test RE 1.0334660636236173\n",
      "88 Train Loss 10262560000000.0 Test MSE 411.0255492035626 Test RE 1.0334678997658273\n",
      "89 Train Loss 10209079000000.0 Test MSE 411.0297022118954 Test RE 1.0334731208402423\n",
      "90 Train Loss 10137976000000.0 Test MSE 411.03678128538473 Test RE 1.0334820204412105\n",
      "91 Train Loss 10066788000000.0 Test MSE 411.04529185632464 Test RE 1.0334927195768144\n",
      "92 Train Loss 9990197000000.0 Test MSE 411.05357650755565 Test RE 1.0335031345891732\n",
      "93 Train Loss 9859767000000.0 Test MSE 411.06759925911996 Test RE 1.0335207629896146\n",
      "94 Train Loss 9740039000000.0 Test MSE 411.08326537242027 Test RE 1.0335404569513107\n",
      "95 Train Loss 9645793000000.0 Test MSE 411.09886773498425 Test RE 1.0335600703986165\n",
      "96 Train Loss 9559327000000.0 Test MSE 411.10970031438285 Test RE 1.0335736876190107\n",
      "97 Train Loss 9443192000000.0 Test MSE 411.11969878194685 Test RE 1.0335862561501716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 Train Loss 9328318000000.0 Test MSE 411.12946062171267 Test RE 1.0335985270819146\n",
      "99 Train Loss 9238113000000.0 Test MSE 411.13798931972303 Test RE 1.0336092477974421\n",
      "Training time: 11.01\n",
      "Training time: 11.01\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    alpha_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.5, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"alpha\": alpha_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0120752717962298\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
