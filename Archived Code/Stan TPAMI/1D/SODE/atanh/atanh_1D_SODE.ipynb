{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"1D_SODE_atanh\"\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(3.0)\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 26800.29 Test MSE 395.81081640344325 Test RE 1.014159846467565\n",
      "1 Train Loss 4031.3628 Test MSE 395.0983107052206 Test RE 1.0132466322231892\n",
      "2 Train Loss 1835.0089 Test MSE 396.88273673120943 Test RE 1.0155321732141342\n",
      "3 Train Loss 970.9157 Test MSE 398.6388093802865 Test RE 1.0177763876657318\n",
      "4 Train Loss 499.218 Test MSE 393.35874286350366 Test RE 1.0110135732214571\n",
      "5 Train Loss 338.32303 Test MSE 397.33490657196836 Test RE 1.0161105081516035\n",
      "6 Train Loss 217.47844 Test MSE 397.1046762795203 Test RE 1.0158160798070828\n",
      "7 Train Loss 190.09976 Test MSE 398.13238532976806 Test RE 1.017129699197415\n",
      "8 Train Loss 148.13367 Test MSE 399.7850368980827 Test RE 1.0192385708241554\n",
      "9 Train Loss 127.31828 Test MSE 400.6506982669998 Test RE 1.020341461463256\n",
      "10 Train Loss 106.41651 Test MSE 398.2237721221494 Test RE 1.0172464278166498\n",
      "11 Train Loss 95.33894 Test MSE 396.5965391525021 Test RE 1.0151659500987087\n",
      "12 Train Loss 89.561806 Test MSE 397.04303714850556 Test RE 1.0157372385668284\n",
      "13 Train Loss 80.70825 Test MSE 395.53612955535726 Test RE 1.0138078794306848\n",
      "14 Train Loss 74.371956 Test MSE 395.6908486745797 Test RE 1.0140061421359485\n",
      "15 Train Loss 66.41207 Test MSE 394.90584930567974 Test RE 1.012999814388261\n",
      "16 Train Loss 59.362785 Test MSE 394.7743349677696 Test RE 1.012831122162287\n",
      "17 Train Loss 56.513 Test MSE 394.4168158305248 Test RE 1.0123723935956304\n",
      "18 Train Loss 52.089527 Test MSE 393.6078087221816 Test RE 1.0113335980346791\n",
      "19 Train Loss 47.84711 Test MSE 393.4359213584547 Test RE 1.011112750728471\n",
      "20 Train Loss 42.6177 Test MSE 393.3159145992114 Test RE 1.0109585329611679\n",
      "21 Train Loss 37.688797 Test MSE 393.0889193244106 Test RE 1.010666762501952\n",
      "22 Train Loss 35.874237 Test MSE 392.97608931857656 Test RE 1.0105217040747594\n",
      "23 Train Loss 33.297836 Test MSE 392.57043927014473 Test RE 1.0100000132790432\n",
      "24 Train Loss 31.020523 Test MSE 391.5721036708634 Test RE 1.0087149434655585\n",
      "25 Train Loss 27.93827 Test MSE 390.73553610151333 Test RE 1.0076368413813672\n",
      "26 Train Loss 25.601562 Test MSE 390.6959262884465 Test RE 1.0075857667864154\n",
      "27 Train Loss 24.684792 Test MSE 390.3564257018622 Test RE 1.007147893902363\n",
      "28 Train Loss 23.84039 Test MSE 389.6115522714175 Test RE 1.006186521322467\n",
      "29 Train Loss 22.444248 Test MSE 388.15882770817007 Test RE 1.0043089114763812\n",
      "30 Train Loss 21.582607 Test MSE 387.5151886326852 Test RE 1.0034759011165058\n",
      "31 Train Loss 20.045797 Test MSE 386.7451299244307 Test RE 1.002478366503718\n",
      "32 Train Loss 19.406927 Test MSE 386.9470310578542 Test RE 1.0027400053630415\n",
      "33 Train Loss 18.981808 Test MSE 386.4189269738324 Test RE 1.0020555035380598\n",
      "34 Train Loss 18.50989 Test MSE 386.3237548374743 Test RE 1.0019320964930352\n",
      "35 Train Loss 17.923927 Test MSE 386.71683928160985 Test RE 1.002441699880303\n",
      "36 Train Loss 17.131527 Test MSE 386.29534289880905 Test RE 1.0018952525812528\n",
      "37 Train Loss 16.429066 Test MSE 387.59207578941624 Test RE 1.0035754463530056\n",
      "38 Train Loss 15.461238 Test MSE 388.2210766983259 Test RE 1.0043894387049441\n",
      "39 Train Loss 14.160818 Test MSE 389.04942268344143 Test RE 1.0054603988904207\n",
      "40 Train Loss 12.884884 Test MSE 388.2940319352406 Test RE 1.00448380765045\n",
      "41 Train Loss 11.589476 Test MSE 387.4782135579878 Test RE 1.0034280262426956\n",
      "42 Train Loss 9.694242 Test MSE 386.040781993544 Test RE 1.001565083711584\n",
      "43 Train Loss 8.607488 Test MSE 383.4434770948959 Test RE 0.9981901032127293\n",
      "44 Train Loss 7.5240307 Test MSE 380.33009831496804 Test RE 0.9941294291321754\n",
      "45 Train Loss 6.1822023 Test MSE 381.54928857921027 Test RE 0.9957215502317738\n",
      "46 Train Loss 4.699019 Test MSE 383.06362433230674 Test RE 0.9976955593736173\n",
      "47 Train Loss 4.0720816 Test MSE 384.2107201587145 Test RE 0.9991882577733889\n",
      "48 Train Loss 3.1635783 Test MSE 386.2516420273539 Test RE 1.0018385797080076\n",
      "49 Train Loss 2.940139 Test MSE 387.074499080355 Test RE 1.0029051529810027\n",
      "50 Train Loss 2.773202 Test MSE 391.44459804949804 Test RE 1.008550698750018\n",
      "51 Train Loss 2.6940782 Test MSE 390.41558519011704 Test RE 1.0072242088965386\n",
      "52 Train Loss 2.6268308 Test MSE 390.3320669452465 Test RE 1.0071164697326285\n",
      "53 Train Loss 2.57457 Test MSE 390.9011067061388 Test RE 1.0078503072114442\n",
      "54 Train Loss 2.5329297 Test MSE 390.60699229807926 Test RE 1.00747108204462\n",
      "55 Train Loss 2.4801934 Test MSE 389.3739550052306 Test RE 1.0058796725854156\n",
      "56 Train Loss 2.4390454 Test MSE 386.12465625802207 Test RE 1.001673881766363\n",
      "57 Train Loss 2.4122314 Test MSE 383.4841422919425 Test RE 0.9982430321625055\n",
      "58 Train Loss 2.4045007 Test MSE 382.3101651585345 Test RE 0.996713877957609\n",
      "59 Train Loss 2.3929017 Test MSE 380.9575022592391 Test RE 0.9949490641184088\n",
      "60 Train Loss 2.3913045 Test MSE 380.9675870771766 Test RE 0.9949622333219358\n",
      "61 Train Loss 2.3818085 Test MSE 380.7745967046031 Test RE 0.9947101876586116\n",
      "62 Train Loss 2.3759458 Test MSE 380.71514623366073 Test RE 0.9946325324026806\n",
      "63 Train Loss 2.374582 Test MSE 380.50798587612275 Test RE 0.9943618884970877\n",
      "64 Train Loss 2.3745067 Test MSE 380.5012023572217 Test RE 0.9943530249475978\n",
      "65 Train Loss 2.3744378 Test MSE 380.4957381518203 Test RE 0.9943458851952884\n",
      "66 Train Loss 2.366097 Test MSE 379.9238021619365 Test RE 0.9935982867195007\n",
      "67 Train Loss 2.362947 Test MSE 379.70856604868027 Test RE 0.9933167979976634\n",
      "68 Train Loss 2.3612225 Test MSE 379.4871194535813 Test RE 0.9930271038440249\n",
      "69 Train Loss 2.35625 Test MSE 378.7147517186978 Test RE 0.9920160382351553\n",
      "70 Train Loss 2.3538444 Test MSE 378.6546789817479 Test RE 0.9919373570091975\n",
      "71 Train Loss 2.352083 Test MSE 378.4867259473295 Test RE 0.9917173447180172\n",
      "72 Train Loss 2.3488579 Test MSE 378.0975250103505 Test RE 0.9912073181170049\n",
      "73 Train Loss 2.3467255 Test MSE 377.8966686258384 Test RE 0.9909440040872733\n",
      "74 Train Loss 2.34566 Test MSE 377.7156568100608 Test RE 0.9907066454740101\n",
      "75 Train Loss 2.3428426 Test MSE 377.20663694400565 Test RE 0.9900388688436704\n",
      "76 Train Loss 2.3417265 Test MSE 376.9297928899424 Test RE 0.9896754915115384\n",
      "77 Train Loss 2.3406653 Test MSE 376.732452440254 Test RE 0.9894163867929768\n",
      "78 Train Loss 2.340632 Test MSE 376.7323949566772 Test RE 0.989416311308114\n",
      "79 Train Loss 2.3405957 Test MSE 376.73163277918417 Test RE 0.9894153104501637\n",
      "80 Train Loss 2.3382707 Test MSE 376.2883080561988 Test RE 0.9888329842688196\n",
      "81 Train Loss 2.335737 Test MSE 375.8444029846451 Test RE 0.9882495519647474\n",
      "82 Train Loss 2.3342762 Test MSE 375.1903950278849 Test RE 0.987389349608157\n",
      "83 Train Loss 2.3313806 Test MSE 374.57620524163804 Test RE 0.9865808363124826\n",
      "84 Train Loss 2.326906 Test MSE 374.0738186854441 Test RE 0.9859190066971\n",
      "85 Train Loss 2.3256733 Test MSE 373.74308966323485 Test RE 0.9854830711718411\n",
      "86 Train Loss 2.324905 Test MSE 373.4358365834983 Test RE 0.9850779065362119\n",
      "87 Train Loss 2.3229005 Test MSE 373.6026842051226 Test RE 0.9852979437359515\n",
      "88 Train Loss 2.3213665 Test MSE 373.33056026957115 Test RE 0.9849390437507378\n",
      "89 Train Loss 2.3197894 Test MSE 373.1366532964707 Test RE 0.9846832230720712\n",
      "90 Train Loss 2.3157725 Test MSE 372.8720068356943 Test RE 0.9843339687836996\n",
      "91 Train Loss 2.3146107 Test MSE 372.6582297987293 Test RE 0.9840517564360884\n",
      "92 Train Loss 2.314586 Test MSE 372.6550795826317 Test RE 0.9840475971528799\n",
      "93 Train Loss 2.3144143 Test MSE 372.55286621095075 Test RE 0.9839126335815714\n",
      "94 Train Loss 2.3131466 Test MSE 372.1267840709734 Test RE 0.9833498308589301\n",
      "95 Train Loss 2.3123772 Test MSE 372.162619400514 Test RE 0.9833971773795679\n",
      "96 Train Loss 2.3119876 Test MSE 372.22109750112895 Test RE 0.9834744351927245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 Train Loss 2.3114698 Test MSE 372.2683000151958 Test RE 0.9835367919405313\n",
      "98 Train Loss 2.3114629 Test MSE 372.26531333473247 Test RE 0.9835328465119073\n",
      "99 Train Loss 2.3109725 Test MSE 372.19219370334497 Test RE 0.9834362499605448\n",
      "Training time: 84.55\n",
      "Training time: 84.55\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 16571.096 Test MSE 427.22343674802954 Test RE 1.0536348264389706\n",
      "1 Train Loss 5713.0674 Test MSE 431.1425635503714 Test RE 1.0584565441191178\n",
      "2 Train Loss 2395.6453 Test MSE 427.59898742722805 Test RE 1.0540978234568579\n",
      "3 Train Loss 980.2572 Test MSE 422.9867160508532 Test RE 1.048397427887828\n",
      "4 Train Loss 556.4724 Test MSE 419.65788008839013 Test RE 1.044263921562595\n",
      "5 Train Loss 341.82352 Test MSE 413.1413436848329 Test RE 1.036124424354661\n",
      "6 Train Loss 235.93451 Test MSE 408.1963561573251 Test RE 1.029904947238205\n",
      "7 Train Loss 179.6638 Test MSE 408.67113967978963 Test RE 1.0305037274417783\n",
      "8 Train Loss 138.72401 Test MSE 408.6849585698946 Test RE 1.0305211501271598\n",
      "9 Train Loss 116.31642 Test MSE 407.958400610751 Test RE 1.0296047151108239\n",
      "10 Train Loss 98.92263 Test MSE 407.27329618029364 Test RE 1.0287398191466706\n",
      "11 Train Loss 91.93301 Test MSE 407.8649720234706 Test RE 1.029486810905428\n",
      "12 Train Loss 81.96779 Test MSE 408.51596214022516 Test RE 1.0303080612931987\n",
      "13 Train Loss 76.85094 Test MSE 408.8166186430016 Test RE 1.0306871307455907\n",
      "14 Train Loss 68.936134 Test MSE 409.3361368858265 Test RE 1.0313418139662716\n",
      "15 Train Loss 62.457104 Test MSE 409.65888135704876 Test RE 1.031748318856424\n",
      "16 Train Loss 60.173172 Test MSE 409.2870601304616 Test RE 1.0312799865064757\n",
      "17 Train Loss 53.36467 Test MSE 407.9775902975127 Test RE 1.0296289302754138\n",
      "18 Train Loss 47.857964 Test MSE 406.11805088910353 Test RE 1.0272797543591214\n",
      "19 Train Loss 41.890316 Test MSE 405.3139601015866 Test RE 1.0262622725438748\n",
      "20 Train Loss 34.531364 Test MSE 403.2595152162932 Test RE 1.0236580224719896\n",
      "21 Train Loss 28.819878 Test MSE 401.1853537485757 Test RE 1.0210220409314745\n",
      "22 Train Loss 23.776434 Test MSE 400.4173494830031 Test RE 1.0200442822513167\n",
      "23 Train Loss 20.40153 Test MSE 397.57145817347964 Test RE 1.016412931623572\n",
      "24 Train Loss 15.579458 Test MSE 394.11025889556845 Test RE 1.01197888843415\n",
      "25 Train Loss 12.07869 Test MSE 393.2042896390951 Test RE 1.0108150653190193\n",
      "26 Train Loss 8.335461 Test MSE 389.00213035726085 Test RE 1.0053992858241791\n",
      "27 Train Loss 6.80501 Test MSE 387.1731120807809 Test RE 1.0030328973664966\n",
      "28 Train Loss 5.9909506 Test MSE 386.32301194245343 Test RE 1.001931133142071\n",
      "29 Train Loss 4.892858 Test MSE 384.3943330819718 Test RE 0.999426983495139\n",
      "30 Train Loss 3.795971 Test MSE 382.31315972028534 Test RE 0.996717781483702\n",
      "31 Train Loss 3.2916527 Test MSE 382.14509578927806 Test RE 0.9964986800510813\n",
      "32 Train Loss 3.0622134 Test MSE 381.2982651895671 Test RE 0.9953939509799009\n",
      "33 Train Loss 2.9035425 Test MSE 381.68580340509124 Test RE 0.9958996643072098\n",
      "34 Train Loss 2.8141415 Test MSE 381.3889279659727 Test RE 0.9955122832981962\n",
      "35 Train Loss 2.678252 Test MSE 381.4574191948258 Test RE 0.9956016681671838\n",
      "36 Train Loss 2.6225183 Test MSE 381.67582986308463 Test RE 0.9958866526731192\n",
      "37 Train Loss 2.5763292 Test MSE 381.7228646189368 Test RE 0.995948013439115\n",
      "38 Train Loss 2.5365477 Test MSE 382.0267670584055 Test RE 0.9963443884551704\n",
      "39 Train Loss 2.518105 Test MSE 382.1981300696554 Test RE 0.9965678249354655\n",
      "40 Train Loss 2.4833012 Test MSE 381.9863796807455 Test RE 0.9962917209405544\n",
      "41 Train Loss 2.4596865 Test MSE 381.7640839765595 Test RE 0.9960017844245234\n",
      "42 Train Loss 2.4433382 Test MSE 381.7818073098068 Test RE 0.9960249037636749\n",
      "43 Train Loss 2.4337113 Test MSE 381.73362798141164 Test RE 0.9959620546125735\n",
      "44 Train Loss 2.4200706 Test MSE 381.73770418574514 Test RE 0.9959673721084762\n",
      "45 Train Loss 2.3977394 Test MSE 381.50699566939676 Test RE 0.9956663632218307\n",
      "46 Train Loss 2.3954523 Test MSE 381.4354357543945 Test RE 0.9955729794273569\n",
      "47 Train Loss 2.3952157 Test MSE 381.4270828583119 Test RE 0.9955620785482205\n",
      "48 Train Loss 2.3950367 Test MSE 381.4244739528032 Test RE 0.9955586737928307\n",
      "49 Train Loss 2.394878 Test MSE 381.42530552672065 Test RE 0.9955597590406736\n",
      "50 Train Loss 2.3947375 Test MSE 381.4256860494694 Test RE 0.9955602556425456\n",
      "51 Train Loss 2.3945365 Test MSE 381.42308720155285 Test RE 0.9955568640066413\n",
      "52 Train Loss 2.3943243 Test MSE 381.4223124041886 Test RE 0.9955558528522939\n",
      "53 Train Loss 2.393935 Test MSE 381.4170872548015 Test RE 0.9955490337102287\n",
      "54 Train Loss 2.3928623 Test MSE 381.4000392245644 Test RE 0.9955267846549954\n",
      "55 Train Loss 2.3904254 Test MSE 381.313272901272 Test RE 0.9954135398946979\n",
      "56 Train Loss 2.3903604 Test MSE 381.3080886626796 Test RE 0.9954067731766265\n",
      "57 Train Loss 2.389987 Test MSE 381.28377041275394 Test RE 0.9953750312107882\n",
      "58 Train Loss 2.3857946 Test MSE 381.15523114153433 Test RE 0.9952072354952216\n",
      "59 Train Loss 2.3846416 Test MSE 381.1482837002893 Test RE 0.9951981654697606\n",
      "60 Train Loss 2.383373 Test MSE 381.07840617437716 Test RE 0.9951069343450016\n",
      "61 Train Loss 2.3833635 Test MSE 381.0716163819905 Test RE 0.9950980692407689\n",
      "62 Train Loss 2.3833292 Test MSE 381.0634300389985 Test RE 0.9950873806227954\n",
      "63 Train Loss 2.382496 Test MSE 380.9564955735777 Test RE 0.9949477495339573\n",
      "64 Train Loss 2.3773725 Test MSE 380.7309026150072 Test RE 0.994653114256974\n",
      "65 Train Loss 2.3770564 Test MSE 380.7001493333054 Test RE 0.9946129422287362\n",
      "66 Train Loss 2.374207 Test MSE 380.5547330751213 Test RE 0.9944229676428715\n",
      "67 Train Loss 2.371932 Test MSE 380.39407305528033 Test RE 0.9942130361068697\n",
      "68 Train Loss 2.3718503 Test MSE 380.3887039051468 Test RE 0.9942060195702435\n",
      "69 Train Loss 2.3718052 Test MSE 380.3844382320398 Test RE 0.9942004450490675\n",
      "70 Train Loss 2.3717885 Test MSE 380.3852604909277 Test RE 0.9942015196063158\n",
      "71 Train Loss 2.3717172 Test MSE 380.38293418571374 Test RE 0.9941984795046674\n",
      "72 Train Loss 2.3713014 Test MSE 380.36814507648813 Test RE 0.994179152332624\n",
      "73 Train Loss 2.3680434 Test MSE 380.2943229968412 Test RE 0.9940826722055796\n",
      "74 Train Loss 2.3664718 Test MSE 380.28074683150885 Test RE 0.9940649281185067\n",
      "75 Train Loss 2.366441 Test MSE 380.2732137015561 Test RE 0.9940550821593922\n",
      "76 Train Loss 2.3664067 Test MSE 380.26689908450766 Test RE 0.9940468287471067\n",
      "77 Train Loss 2.3653407 Test MSE 380.1773170031039 Test RE 0.9939297346360725\n",
      "78 Train Loss 2.3647223 Test MSE 380.1482319079893 Test RE 0.9938917140960478\n",
      "79 Train Loss 2.3645341 Test MSE 380.1307800731086 Test RE 0.9938689000570712\n",
      "80 Train Loss 2.3644197 Test MSE 380.1084042646682 Test RE 0.993839648351397\n",
      "81 Train Loss 2.3643775 Test MSE 380.10367922368727 Test RE 0.9938334712350696\n",
      "82 Train Loss 2.364314 Test MSE 380.1004912730391 Test RE 0.993829303558097\n",
      "83 Train Loss 2.3639002 Test MSE 380.0677102733261 Test RE 0.9937864472328432\n",
      "84 Train Loss 2.3632114 Test MSE 379.8922845489349 Test RE 0.9935570725397082\n",
      "85 Train Loss 2.36318 Test MSE 379.8885227241749 Test RE 0.9935521532546885\n",
      "86 Train Loss 2.3631582 Test MSE 379.88407211408855 Test RE 0.9935463332233881\n",
      "87 Train Loss 2.3631208 Test MSE 379.88467225377457 Test RE 0.9935471180237953\n",
      "88 Train Loss 2.363088 Test MSE 379.88514131003444 Test RE 0.9935477314064681\n",
      "89 Train Loss 2.3630722 Test MSE 379.88328330274123 Test RE 0.993545301696817\n",
      "90 Train Loss 2.3630383 Test MSE 379.8849361631594 Test RE 0.9935474631369077\n",
      "91 Train Loss 2.3625875 Test MSE 379.7388967422245 Test RE 0.9933564697202192\n",
      "92 Train Loss 2.3621936 Test MSE 379.59123378098576 Test RE 0.9931633156591265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 Train Loss 2.3620045 Test MSE 379.56990849387296 Test RE 0.9931354175038954\n",
      "94 Train Loss 2.3619478 Test MSE 379.557418956586 Test RE 0.9931190780844803\n",
      "95 Train Loss 2.3619423 Test MSE 379.5539482196906 Test RE 0.9931145374501341\n",
      "96 Train Loss 2.3619263 Test MSE 379.5507188744391 Test RE 0.9931103126007386\n",
      "97 Train Loss 2.361916 Test MSE 379.5505531422906 Test RE 0.9931100957781698\n",
      "98 Train Loss 2.3619072 Test MSE 379.5497893354568 Test RE 0.9931090965112037\n",
      "99 Train Loss 2.3619008 Test MSE 379.55153234679057 Test RE 0.9931113768423645\n",
      "Training time: 86.35\n",
      "Training time: 86.35\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 16794.39 Test MSE 327.8302981262067 Test RE 0.9229690262383166\n",
      "1 Train Loss 6223.0723 Test MSE 332.5340709045888 Test RE 0.9295669129982395\n",
      "2 Train Loss 3217.5383 Test MSE 332.1329252470134 Test RE 0.9290060618123517\n",
      "3 Train Loss 1954.7529 Test MSE 331.6213408589301 Test RE 0.9282903120096607\n",
      "4 Train Loss 1351.368 Test MSE 333.13739245949125 Test RE 0.9304097944581136\n",
      "5 Train Loss 1025.7677 Test MSE 334.7139085298809 Test RE 0.9326086991258333\n",
      "6 Train Loss 717.3068 Test MSE 335.7918670921733 Test RE 0.9341092424185712\n",
      "7 Train Loss 504.9768 Test MSE 344.78344890599544 Test RE 0.9465330599587384\n",
      "8 Train Loss 348.32678 Test MSE 349.89572705605235 Test RE 0.953524600166304\n",
      "9 Train Loss 243.08401 Test MSE 358.06792575633085 Test RE 0.9645956359756939\n",
      "10 Train Loss 213.96985 Test MSE 357.3339311391618 Test RE 0.9636064783913441\n",
      "11 Train Loss 183.99588 Test MSE 357.08912204394545 Test RE 0.963276338951661\n",
      "12 Train Loss 156.93736 Test MSE 360.714521390801 Test RE 0.968153891899163\n",
      "13 Train Loss 130.8441 Test MSE 364.542738013426 Test RE 0.9732777790906714\n",
      "14 Train Loss 103.19836 Test MSE 367.07357790507683 Test RE 0.976650428627077\n",
      "15 Train Loss 79.384705 Test MSE 364.19650759789545 Test RE 0.9728154760663208\n",
      "16 Train Loss 61.74598 Test MSE 361.96495400219857 Test RE 0.9698305139686646\n",
      "17 Train Loss 52.56234 Test MSE 363.82020860308626 Test RE 0.9723127747971231\n",
      "18 Train Loss 43.38192 Test MSE 365.35929821956694 Test RE 0.9743672197418785\n",
      "19 Train Loss 35.558315 Test MSE 368.44602430802973 Test RE 0.9784745176979266\n",
      "20 Train Loss 25.203781 Test MSE 376.4623349417054 Test RE 0.9890616169808054\n",
      "21 Train Loss 16.597063 Test MSE 387.4410014343661 Test RE 1.0033798421345452\n",
      "22 Train Loss 7.0241914 Test MSE 387.5515043568884 Test RE 1.0035229200481437\n",
      "23 Train Loss 3.8974013 Test MSE 382.60237610168934 Test RE 0.9970947141179478\n",
      "24 Train Loss 3.2859986 Test MSE 380.3297723848099 Test RE 0.9941290031642688\n",
      "25 Train Loss 2.5088322 Test MSE 380.3691965956464 Test RE 0.9941805265245963\n",
      "26 Train Loss 2.4297476 Test MSE 381.4012319940325 Test RE 0.9955283413315854\n",
      "27 Train Loss 2.3972433 Test MSE 381.6514747846559 Test RE 0.9958548779541323\n",
      "28 Train Loss 2.389678 Test MSE 381.64541014523707 Test RE 0.9958469655980431\n",
      "29 Train Loss 2.376765 Test MSE 380.8880942604165 Test RE 0.9948584233398413\n",
      "30 Train Loss 2.3661876 Test MSE 379.77292249136167 Test RE 0.9934009725894642\n",
      "31 Train Loss 2.3630295 Test MSE 379.12952559126904 Test RE 0.9925591247319728\n",
      "32 Train Loss 2.3593235 Test MSE 378.185783348495 Test RE 0.9913229988610855\n",
      "33 Train Loss 2.3580143 Test MSE 377.94200615196826 Test RE 0.9910034457362481\n",
      "34 Train Loss 2.3549998 Test MSE 377.747440919787 Test RE 0.9907483277100467\n",
      "35 Train Loss 2.3549857 Test MSE 377.745959772238 Test RE 0.9907463853462652\n",
      "36 Train Loss 2.3531668 Test MSE 377.42819977937575 Test RE 0.9903295896259874\n",
      "37 Train Loss 2.3507023 Test MSE 377.9074090776484 Test RE 0.990958086125543\n",
      "38 Train Loss 2.3487494 Test MSE 377.1909411236525 Test RE 0.9900182705397682\n",
      "39 Train Loss 2.3471398 Test MSE 377.0163996750403 Test RE 0.9897891833598172\n",
      "40 Train Loss 2.3463624 Test MSE 376.8873621647523 Test RE 0.9896197864022089\n",
      "41 Train Loss 2.3463488 Test MSE 376.8857199085624 Test RE 0.9896176303057813\n",
      "42 Train Loss 2.3463452 Test MSE 376.88597957274504 Test RE 0.9896179712157644\n",
      "43 Train Loss 2.3463318 Test MSE 376.8832397707707 Test RE 0.9896143741567786\n",
      "44 Train Loss 2.3436089 Test MSE 376.79987516247223 Test RE 0.9895049193329187\n",
      "45 Train Loss 2.3399124 Test MSE 376.28679615599395 Test RE 0.988830997735806\n",
      "46 Train Loss 2.334708 Test MSE 375.88800343722033 Test RE 0.9883068720656001\n",
      "47 Train Loss 2.3255107 Test MSE 374.02188832284554 Test RE 0.985850569792415\n",
      "48 Train Loss 2.3199213 Test MSE 372.880446533314 Test RE 0.9843451085768363\n",
      "49 Train Loss 2.307209 Test MSE 370.8229859772406 Test RE 0.9816256676752454\n",
      "50 Train Loss 2.269932 Test MSE 361.610497486023 Test RE 0.9693555412666625\n",
      "51 Train Loss 2.2441213 Test MSE 359.22830782396227 Test RE 0.9661573426279159\n",
      "52 Train Loss 2.238988 Test MSE 358.7737038754326 Test RE 0.9655458123290233\n",
      "53 Train Loss 2.2345934 Test MSE 358.4968570452354 Test RE 0.9651732099315854\n",
      "54 Train Loss 2.232603 Test MSE 358.33230836473854 Test RE 0.9649516791100207\n",
      "55 Train Loss 2.2236857 Test MSE 357.45284822949435 Test RE 0.9637668042612112\n",
      "56 Train Loss 2.2192523 Test MSE 356.8643404034017 Test RE 0.9629731080379565\n",
      "57 Train Loss 2.217657 Test MSE 356.74302555425334 Test RE 0.9628094143675776\n",
      "58 Train Loss 2.2151878 Test MSE 356.2802614922379 Test RE 0.9621847375701306\n",
      "59 Train Loss 2.2119017 Test MSE 355.6534353684464 Test RE 0.9613379490920109\n",
      "60 Train Loss 2.207302 Test MSE 355.260945309185 Test RE 0.96080734864292\n",
      "61 Train Loss 2.2049983 Test MSE 354.9401807244579 Test RE 0.9603734949187366\n",
      "62 Train Loss 2.2032523 Test MSE 354.31396927455376 Test RE 0.9595259403127387\n",
      "63 Train Loss 2.2004902 Test MSE 353.767650917727 Test RE 0.958785906182535\n",
      "64 Train Loss 2.1927695 Test MSE 352.9442098012449 Test RE 0.9576694054381596\n",
      "65 Train Loss 2.1887739 Test MSE 352.0106299811403 Test RE 0.956401991475562\n",
      "66 Train Loss 2.187839 Test MSE 351.83783676075 Test RE 0.9561672257448272\n",
      "67 Train Loss 2.1869357 Test MSE 351.6054533979571 Test RE 0.9558514068736339\n",
      "68 Train Loss 2.183247 Test MSE 350.98764116696583 Test RE 0.9550112658529664\n",
      "69 Train Loss 2.1820946 Test MSE 350.97782759076944 Test RE 0.9549979147541837\n",
      "70 Train Loss 2.1799767 Test MSE 350.7345487032999 Test RE 0.9546668808743666\n",
      "71 Train Loss 2.17855 Test MSE 350.415766993765 Test RE 0.9542329351394055\n",
      "72 Train Loss 2.1753368 Test MSE 349.6557068483117 Test RE 0.9531974963787873\n",
      "73 Train Loss 2.1734476 Test MSE 349.42651549410084 Test RE 0.9528850455419634\n",
      "74 Train Loss 2.172442 Test MSE 349.3604158668508 Test RE 0.9527949145380427\n",
      "75 Train Loss 2.1711311 Test MSE 349.0077778380562 Test RE 0.9523139262319261\n",
      "76 Train Loss 2.1708312 Test MSE 348.98958929245316 Test RE 0.9522891109812995\n",
      "77 Train Loss 2.169352 Test MSE 348.7098215321458 Test RE 0.9519073328205206\n",
      "78 Train Loss 2.1672912 Test MSE 348.3217651206427 Test RE 0.9513775275976478\n",
      "79 Train Loss 2.1654506 Test MSE 348.37909572682497 Test RE 0.9514558184363042\n",
      "80 Train Loss 2.163062 Test MSE 347.8599161082166 Test RE 0.9507465901099065\n",
      "81 Train Loss 2.1587431 Test MSE 347.2218489016396 Test RE 0.9498742293794246\n",
      "82 Train Loss 2.1577237 Test MSE 346.87735923757225 Test RE 0.949402912471707\n",
      "83 Train Loss 2.155179 Test MSE 346.7546950300333 Test RE 0.9492350317349264\n",
      "84 Train Loss 2.1534352 Test MSE 346.5005572439589 Test RE 0.948887119060824\n",
      "85 Train Loss 2.1517355 Test MSE 346.37490361111406 Test RE 0.9487150530676884\n",
      "86 Train Loss 2.1480641 Test MSE 345.62085268469343 Test RE 0.9476818240099241\n",
      "87 Train Loss 2.1434295 Test MSE 344.82013502344506 Test RE 0.9465834157682512\n",
      "88 Train Loss 2.1396222 Test MSE 344.51306730196256 Test RE 0.9461618482284652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 Train Loss 2.1373932 Test MSE 343.9549212310219 Test RE 0.94539509852561\n",
      "90 Train Loss 2.1333184 Test MSE 343.45740765153374 Test RE 0.9447111177524227\n",
      "91 Train Loss 2.1314952 Test MSE 342.9177301923662 Test RE 0.9439686097725525\n",
      "92 Train Loss 2.1292102 Test MSE 342.45457578855553 Test RE 0.9433309190019827\n",
      "93 Train Loss 2.1270747 Test MSE 341.5715308992499 Test RE 0.9421139090831311\n",
      "94 Train Loss 2.1215765 Test MSE 339.56819021272474 Test RE 0.9393470645187629\n",
      "95 Train Loss 2.108943 Test MSE 337.9657482746063 Test RE 0.9371280269056257\n",
      "96 Train Loss 2.0988476 Test MSE 336.9748925335501 Test RE 0.935753271797679\n",
      "97 Train Loss 2.096106 Test MSE 336.84408682836624 Test RE 0.9355716354948072\n",
      "98 Train Loss 2.0960383 Test MSE 336.8480703817919 Test RE 0.9355771675624913\n",
      "99 Train Loss 2.0947406 Test MSE 336.69741105891313 Test RE 0.9353679201312877\n",
      "Training time: 116.61\n",
      "Training time: 116.61\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 489185.9 Test MSE 375.1597672062194 Test RE 0.9873490471343784\n",
      "1 Train Loss 8783.086 Test MSE 373.6868567768282 Test RE 0.9854089311492404\n",
      "2 Train Loss 2175.2485 Test MSE 373.1383967747345 Test RE 0.984685523531937\n",
      "3 Train Loss 834.11237 Test MSE 372.55771262329034 Test RE 0.9839190332515961\n",
      "4 Train Loss 486.29214 Test MSE 373.11022972895046 Test RE 0.9846483574215831\n",
      "5 Train Loss 273.41074 Test MSE 375.0021306572543 Test RE 0.9871415906543656\n",
      "6 Train Loss 190.28958 Test MSE 376.77140073925665 Test RE 0.9894675306340031\n",
      "7 Train Loss 163.48074 Test MSE 377.80662448225405 Test RE 0.9908259373940205\n",
      "8 Train Loss 136.53694 Test MSE 378.4483610910431 Test RE 0.9916670813203908\n",
      "9 Train Loss 120.00026 Test MSE 379.6171504325827 Test RE 0.9931972192717676\n",
      "10 Train Loss 96.7804 Test MSE 381.36733156452993 Test RE 0.9954840971269427\n",
      "11 Train Loss 84.11571 Test MSE 384.1333698091972 Test RE 0.9990876730587464\n",
      "12 Train Loss 77.81663 Test MSE 384.73558022028607 Test RE 0.9998705071243004\n",
      "13 Train Loss 72.38763 Test MSE 385.4091374696625 Test RE 1.0007453619071303\n",
      "14 Train Loss 72.138885 Test MSE 385.3070133053505 Test RE 1.0006127663808577\n",
      "15 Train Loss 72.03553 Test MSE 385.3000560862037 Test RE 1.000603732657773\n",
      "16 Train Loss 69.96458 Test MSE 386.5595303240781 Test RE 1.0022377921593792\n",
      "17 Train Loss 66.78907 Test MSE 387.62949602812404 Test RE 1.0036238904889103\n",
      "18 Train Loss 59.030056 Test MSE 387.7926155870276 Test RE 1.0038350373040037\n",
      "19 Train Loss 55.24719 Test MSE 387.1214511487585 Test RE 1.0029659772455664\n",
      "20 Train Loss 52.94475 Test MSE 387.80662032026754 Test RE 1.0038531633776124\n",
      "21 Train Loss 46.65413 Test MSE 390.28686443759284 Test RE 1.0070581533504401\n",
      "22 Train Loss 44.604443 Test MSE 390.69777052017986 Test RE 1.0075881448755215\n",
      "23 Train Loss 42.512077 Test MSE 389.8759897781102 Test RE 1.006527923318026\n",
      "24 Train Loss 39.216675 Test MSE 389.46701781446325 Test RE 1.0059998711685552\n",
      "25 Train Loss 32.18765 Test MSE 390.08051942499173 Test RE 1.0067919018829388\n",
      "26 Train Loss 28.832619 Test MSE 388.6555582476624 Test RE 1.0049513178463971\n",
      "27 Train Loss 27.141987 Test MSE 388.868765242796 Test RE 1.005226925990194\n",
      "28 Train Loss 23.993467 Test MSE 389.0434852627849 Test RE 1.0054527265180038\n",
      "29 Train Loss 19.317223 Test MSE 386.9227456093863 Test RE 1.0027085380420944\n",
      "30 Train Loss 17.07607 Test MSE 386.5764722118152 Test RE 1.0022597546422414\n",
      "31 Train Loss 15.89355 Test MSE 386.09479152346086 Test RE 1.0016351438839586\n",
      "32 Train Loss 13.732775 Test MSE 385.4219927959703 Test RE 1.0007620517046107\n",
      "33 Train Loss 12.74802 Test MSE 385.1312915741416 Test RE 1.0003845724301768\n",
      "34 Train Loss 12.154291 Test MSE 385.28963561543344 Test RE 1.0005902018623798\n",
      "35 Train Loss 11.2216425 Test MSE 385.1572783552888 Test RE 1.0004183223967418\n",
      "36 Train Loss 10.623558 Test MSE 385.30938255106184 Test RE 1.0006158427508423\n",
      "37 Train Loss 9.997012 Test MSE 384.6926062041428 Test RE 0.9998146640291229\n",
      "38 Train Loss 9.766499 Test MSE 384.81979668872816 Test RE 0.9999799341695371\n",
      "39 Train Loss 9.201595 Test MSE 384.8836758790322 Test RE 1.0000629278959365\n",
      "40 Train Loss 8.7996235 Test MSE 384.7002687575329 Test RE 0.9998246214535317\n",
      "41 Train Loss 8.350068 Test MSE 383.82999202422764 Test RE 0.9986930693899605\n",
      "42 Train Loss 8.329545 Test MSE 383.83900164780783 Test RE 0.9987047904592014\n",
      "43 Train Loss 8.323143 Test MSE 383.8336209232513 Test RE 0.9986977904224665\n",
      "44 Train Loss 8.316728 Test MSE 383.83165282039425 Test RE 0.9986952300129728\n",
      "45 Train Loss 8.309076 Test MSE 383.83139134847085 Test RE 0.9986948898497721\n",
      "46 Train Loss 8.300819 Test MSE 383.84364506609296 Test RE 0.998710831260215\n",
      "47 Train Loss 8.29159 Test MSE 383.85164341141115 Test RE 0.99872123652886\n",
      "48 Train Loss 8.27866 Test MSE 383.8685474640733 Test RE 0.9987432271218456\n",
      "49 Train Loss 8.065377 Test MSE 383.72542630856543 Test RE 0.9985570245641332\n",
      "50 Train Loss 7.747029 Test MSE 383.5442865687009 Test RE 0.9983213095209439\n",
      "51 Train Loss 7.4749293 Test MSE 384.0965796948731 Test RE 0.9990398284392391\n",
      "52 Train Loss 7.1462293 Test MSE 384.385453082453 Test RE 0.9994154394092555\n",
      "53 Train Loss 6.6863036 Test MSE 385.59577233079966 Test RE 1.0009876386821044\n",
      "54 Train Loss 6.4709682 Test MSE 385.97042591196333 Test RE 1.0014738117411854\n",
      "55 Train Loss 6.297939 Test MSE 384.90063533937797 Test RE 1.000084960969563\n",
      "56 Train Loss 6.0269785 Test MSE 384.91325636749093 Test RE 1.0001013574051356\n",
      "57 Train Loss 5.993025 Test MSE 384.83552574542546 Test RE 1.0000003704627007\n",
      "58 Train Loss 5.732858 Test MSE 384.47868918207786 Test RE 0.999536640597942\n",
      "59 Train Loss 5.5418177 Test MSE 383.573179683009 Test RE 0.998358911526127\n",
      "60 Train Loss 5.4168444 Test MSE 383.3749115149214 Test RE 0.9981008533658668\n",
      "61 Train Loss 5.053212 Test MSE 383.72158457402105 Test RE 0.9985520259370493\n",
      "62 Train Loss 4.81211 Test MSE 384.71112561998234 Test RE 0.9998387296862069\n",
      "63 Train Loss 4.6263247 Test MSE 383.9913140723719 Test RE 0.9989029204825576\n",
      "64 Train Loss 4.6079693 Test MSE 384.08527037445054 Test RE 0.9990251204916796\n",
      "65 Train Loss 4.6010776 Test MSE 384.14989403608007 Test RE 0.9991091616550825\n",
      "66 Train Loss 4.593808 Test MSE 384.1983290058008 Test RE 0.9991721452683637\n",
      "67 Train Loss 4.527121 Test MSE 384.1054507881404 Test RE 0.9990513652828252\n",
      "68 Train Loss 4.5044966 Test MSE 383.98530717571697 Test RE 0.9988951073760393\n",
      "69 Train Loss 4.445171 Test MSE 384.18314059868317 Test RE 0.9991523950740423\n",
      "70 Train Loss 4.411413 Test MSE 384.34467897281655 Test RE 0.9993624309576\n",
      "71 Train Loss 4.230753 Test MSE 385.0273880565464 Test RE 1.0002496178187232\n",
      "72 Train Loss 4.0663195 Test MSE 386.09559847752297 Test RE 1.0016361906128233\n",
      "73 Train Loss 4.0592017 Test MSE 386.08670562249745 Test RE 1.0016246553133306\n",
      "74 Train Loss 4.0373144 Test MSE 386.0403680128929 Test RE 1.0015645466845442\n",
      "75 Train Loss 4.019145 Test MSE 385.95905793682766 Test RE 1.0014590634438336\n",
      "76 Train Loss 3.9760177 Test MSE 385.91152143253055 Test RE 1.0013973893776036\n",
      "77 Train Loss 3.969741 Test MSE 385.91649355043927 Test RE 1.001403840402747\n",
      "78 Train Loss 3.9549835 Test MSE 385.9640892224813 Test RE 1.0014655908333254\n",
      "79 Train Loss 3.8757963 Test MSE 386.00156624216606 Test RE 1.0015142106863997\n",
      "80 Train Loss 3.8676834 Test MSE 385.9895697115166 Test RE 1.001498647551004\n",
      "81 Train Loss 3.7115867 Test MSE 385.27573219023134 Test RE 1.0005721482278558\n",
      "82 Train Loss 3.512229 Test MSE 385.70023106146596 Test RE 1.0011232143614817\n",
      "83 Train Loss 3.2637906 Test MSE 385.29681137727425 Test RE 1.0005995194807307\n",
      "84 Train Loss 3.1809323 Test MSE 384.75117899508325 Test RE 0.9998907763658037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 Train Loss 3.1476762 Test MSE 384.58102779103274 Test RE 0.9996696575717601\n",
      "86 Train Loss 3.1086066 Test MSE 384.534998591796 Test RE 0.9996098322457415\n",
      "87 Train Loss 3.0827692 Test MSE 384.33738616626675 Test RE 0.9993529496354252\n",
      "88 Train Loss 3.0788765 Test MSE 384.3032148156494 Test RE 0.9993085225248373\n",
      "89 Train Loss 3.046501 Test MSE 384.3275283396746 Test RE 0.999340133407279\n",
      "90 Train Loss 3.0152085 Test MSE 384.25283507424433 Test RE 0.999243018837786\n",
      "91 Train Loss 2.9383929 Test MSE 384.60607870563166 Test RE 0.9997022153809273\n",
      "92 Train Loss 2.8489468 Test MSE 385.2471616307638 Test RE 1.0005350482580584\n",
      "93 Train Loss 2.7853136 Test MSE 384.458786720112 Test RE 0.9995107698566218\n",
      "94 Train Loss 2.7386038 Test MSE 383.7158349253998 Test RE 0.9985445448035699\n",
      "95 Train Loss 2.73252 Test MSE 383.61301492407176 Test RE 0.9984107514845917\n",
      "96 Train Loss 2.7316573 Test MSE 383.6042646418583 Test RE 0.9983993644547977\n",
      "97 Train Loss 2.7309546 Test MSE 383.6066020319195 Test RE 0.9984024061900078\n",
      "98 Train Loss 2.7303078 Test MSE 383.58460787835725 Test RE 0.99837378398892\n",
      "99 Train Loss 2.7293344 Test MSE 383.5616388624753 Test RE 0.9983438922699532\n",
      "Training time: 95.53\n",
      "Training time: 95.53\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 69906.74 Test MSE 315.31999301559523 Test RE 0.9051870594704953\n",
      "1 Train Loss 23277.076 Test MSE 313.4357547026513 Test RE 0.9024784716680455\n",
      "2 Train Loss 10649.685 Test MSE 312.7586908294771 Test RE 0.901503206566371\n",
      "3 Train Loss 6141.632 Test MSE 314.8956770061696 Test RE 0.9045778138323799\n",
      "4 Train Loss 3865.9963 Test MSE 315.97009611683205 Test RE 0.906119702383504\n",
      "5 Train Loss 1891.9681 Test MSE 322.70615248693116 Test RE 0.9157273917621891\n",
      "6 Train Loss 1358.2998 Test MSE 326.69699833465734 Test RE 0.9213723065706034\n",
      "7 Train Loss 735.9754 Test MSE 358.19602244383657 Test RE 0.9647681597456131\n",
      "8 Train Loss 497.3043 Test MSE 364.7343308887674 Test RE 0.9735335084935804\n",
      "9 Train Loss 302.41922 Test MSE 368.5070204505992 Test RE 0.9785555074458351\n",
      "10 Train Loss 245.75102 Test MSE 368.631598833759 Test RE 0.9787208999009217\n",
      "11 Train Loss 195.032 Test MSE 371.88013127750264 Test RE 0.9830238852437717\n",
      "12 Train Loss 146.19836 Test MSE 373.1233730530755 Test RE 0.9846657000692919\n",
      "13 Train Loss 113.47471 Test MSE 375.5459236250696 Test RE 0.9878570615115014\n",
      "14 Train Loss 83.05573 Test MSE 376.50118928306665 Test RE 0.9891126557469891\n",
      "15 Train Loss 69.508255 Test MSE 377.2695055972562 Test RE 0.9901213698007058\n",
      "16 Train Loss 58.759792 Test MSE 378.4385566813486 Test RE 0.9916542357457793\n",
      "17 Train Loss 51.244698 Test MSE 380.903926578237 Test RE 0.9948790996898068\n",
      "18 Train Loss 42.860996 Test MSE 384.48055097913544 Test RE 0.999539060669816\n",
      "19 Train Loss 36.51213 Test MSE 387.99350762751203 Test RE 1.0040950169301124\n",
      "20 Train Loss 30.347507 Test MSE 386.2440139494158 Test RE 1.0018286870115538\n",
      "21 Train Loss 27.7898 Test MSE 386.91731769346734 Test RE 1.0027015048077013\n",
      "22 Train Loss 25.081696 Test MSE 387.1286324631424 Test RE 1.0029752799857978\n",
      "23 Train Loss 24.424408 Test MSE 387.0145929017754 Test RE 1.0028275419008137\n",
      "24 Train Loss 22.38945 Test MSE 385.27001866652085 Test RE 1.0005647291077167\n",
      "25 Train Loss 19.865345 Test MSE 384.0290877508007 Test RE 0.9989520509035096\n",
      "26 Train Loss 19.42918 Test MSE 383.3689060586863 Test RE 0.9980930358557911\n",
      "27 Train Loss 18.660986 Test MSE 382.6639203524631 Test RE 0.9971749056998581\n",
      "28 Train Loss 18.014435 Test MSE 382.5509419466424 Test RE 0.9970276909484329\n",
      "29 Train Loss 17.81248 Test MSE 383.0679678842012 Test RE 0.9977012157840377\n",
      "30 Train Loss 17.804699 Test MSE 383.063141182838 Test RE 0.997694930188019\n",
      "31 Train Loss 17.79802 Test MSE 383.04807593396873 Test RE 0.9976753111375449\n",
      "32 Train Loss 17.78917 Test MSE 383.0204975846989 Test RE 0.9976393956282767\n",
      "33 Train Loss 17.168924 Test MSE 382.631666253358 Test RE 0.9971328797153161\n",
      "34 Train Loss 17.078968 Test MSE 382.8239832643543 Test RE 0.9973834359836733\n",
      "35 Train Loss 17.01534 Test MSE 383.1810503632894 Test RE 0.9978484666783889\n",
      "36 Train Loss 16.962503 Test MSE 383.4240713159936 Test RE 0.9981648440771691\n",
      "37 Train Loss 16.92622 Test MSE 383.5306624438222 Test RE 0.9983035783547488\n",
      "38 Train Loss 16.57508 Test MSE 382.96604714688516 Test RE 0.9975684805820052\n",
      "39 Train Loss 16.54977 Test MSE 382.8295382863244 Test RE 0.997390672294671\n",
      "40 Train Loss 16.546146 Test MSE 382.8153774160304 Test RE 0.997372225376208\n",
      "41 Train Loss 16.543266 Test MSE 382.80230181656464 Test RE 0.997355191902804\n",
      "42 Train Loss 16.540628 Test MSE 382.7920177534593 Test RE 0.9973417947359473\n",
      "43 Train Loss 16.536732 Test MSE 382.779087609347 Test RE 0.9973249502336601\n",
      "44 Train Loss 16.531002 Test MSE 382.7649098784852 Test RE 0.9973064801341728\n",
      "45 Train Loss 16.522196 Test MSE 382.7454095674126 Test RE 0.9972810754617513\n",
      "46 Train Loss 15.7954445 Test MSE 383.2171152610601 Test RE 0.9978954241886419\n",
      "47 Train Loss 15.398939 Test MSE 383.44765917804807 Test RE 0.9981955466520888\n",
      "48 Train Loss 15.235855 Test MSE 383.23181640757707 Test RE 0.9979145648605507\n",
      "49 Train Loss 14.994546 Test MSE 382.7877397184633 Test RE 0.9973362216377322\n",
      "50 Train Loss 14.169159 Test MSE 382.0632653586101 Test RE 0.9963919819894617\n",
      "51 Train Loss 13.934734 Test MSE 381.5153331455827 Test RE 0.9956772428378439\n",
      "52 Train Loss 13.876019 Test MSE 381.39245321256936 Test RE 0.9955168841372896\n",
      "53 Train Loss 13.868234 Test MSE 381.4225267439662 Test RE 0.9955561325779194\n",
      "54 Train Loss 13.311037 Test MSE 380.82366864323734 Test RE 0.9947742817252204\n",
      "55 Train Loss 12.616842 Test MSE 379.74381825777454 Test RE 0.9933629067793598\n",
      "56 Train Loss 12.376022 Test MSE 379.1416988309814 Test RE 0.9925750593431696\n",
      "57 Train Loss 12.206534 Test MSE 379.12701527670737 Test RE 0.9925558387313053\n",
      "58 Train Loss 11.661261 Test MSE 379.886244125416 Test RE 0.9935491735514829\n",
      "59 Train Loss 10.949593 Test MSE 379.4643368232499 Test RE 0.9929972950470691\n",
      "60 Train Loss 10.462369 Test MSE 379.61650571082845 Test RE 0.9931963758745124\n",
      "61 Train Loss 10.0809765 Test MSE 380.211258432699 Test RE 0.9939741016217911\n",
      "62 Train Loss 9.656395 Test MSE 380.8358585795949 Test RE 0.9947902026607427\n",
      "63 Train Loss 9.086291 Test MSE 379.7301910299562 Test RE 0.9933450830473709\n",
      "64 Train Loss 8.521061 Test MSE 378.19161418813695 Test RE 0.9913306409029263\n",
      "65 Train Loss 7.3270264 Test MSE 378.97702812764487 Test RE 0.9923594858717134\n",
      "66 Train Loss 6.530181 Test MSE 377.9753236230219 Test RE 0.9910471257194766\n",
      "67 Train Loss 5.876376 Test MSE 377.2679928198953 Test RE 0.9901193847016293\n",
      "68 Train Loss 5.56704 Test MSE 377.1615914366959 Test RE 0.9899797525261969\n",
      "69 Train Loss 5.1518736 Test MSE 375.75493164087607 Test RE 0.9881319164784836\n",
      "70 Train Loss 4.717168 Test MSE 375.5722388092735 Test RE 0.9878916713734797\n",
      "71 Train Loss 4.0627737 Test MSE 375.6114828260657 Test RE 0.9879432830484599\n",
      "72 Train Loss 3.7788496 Test MSE 376.24024987409297 Test RE 0.9887698371658821\n",
      "73 Train Loss 3.6084146 Test MSE 375.86894162266543 Test RE 0.9882818125252724\n",
      "74 Train Loss 3.3612776 Test MSE 375.50036794877394 Test RE 0.9877971435913063\n",
      "75 Train Loss 3.1822343 Test MSE 376.21187778904465 Test RE 0.9887325551488502\n",
      "76 Train Loss 2.9536726 Test MSE 376.81847293943724 Test RE 0.9895293386147965\n",
      "77 Train Loss 2.8558674 Test MSE 376.921793087144 Test RE 0.9896649892231495\n",
      "78 Train Loss 2.7640512 Test MSE 376.73719000814583 Test RE 0.9894226079355131\n",
      "79 Train Loss 2.641532 Test MSE 377.12205086614017 Test RE 0.9899278578063437\n",
      "80 Train Loss 2.589547 Test MSE 377.00036623128995 Test RE 0.9897681366653803\n",
      "81 Train Loss 2.5884967 Test MSE 377.00159995891926 Test RE 0.9897697561642022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 Train Loss 2.57586 Test MSE 377.3292284497844 Test RE 0.9901997362358872\n",
      "83 Train Loss 2.5545228 Test MSE 377.2949518754782 Test RE 0.9901547603592454\n",
      "84 Train Loss 2.5197961 Test MSE 377.51248750368495 Test RE 0.9904401642567284\n",
      "85 Train Loss 2.504856 Test MSE 377.4530304446533 Test RE 0.990362165541267\n",
      "86 Train Loss 2.4878037 Test MSE 377.5333575605442 Test RE 0.9904675411747816\n",
      "87 Train Loss 2.4654973 Test MSE 377.1530860162097 Test RE 0.9899685898816818\n",
      "88 Train Loss 2.4601452 Test MSE 376.92538687581475 Test RE 0.989669707228865\n",
      "89 Train Loss 2.4497092 Test MSE 376.68549743703596 Test RE 0.9893547256631496\n",
      "90 Train Loss 2.443061 Test MSE 376.80996252377554 Test RE 0.9895181643304458\n",
      "91 Train Loss 2.440001 Test MSE 376.82073282358147 Test RE 0.9895323058503096\n",
      "92 Train Loss 2.433293 Test MSE 376.8612770605963 Test RE 0.9895855390587383\n",
      "93 Train Loss 2.4031758 Test MSE 376.93408594637447 Test RE 0.9896811274691052\n",
      "94 Train Loss 2.3971481 Test MSE 376.9153402358357 Test RE 0.9896565177199483\n",
      "95 Train Loss 2.3923717 Test MSE 376.80756838313476 Test RE 0.9895150207704945\n",
      "96 Train Loss 2.3921063 Test MSE 376.793298166508 Test RE 0.9894962834512623\n",
      "97 Train Loss 2.392053 Test MSE 376.7883145212649 Test RE 0.9894897396581849\n",
      "98 Train Loss 2.3919642 Test MSE 376.78315272780696 Test RE 0.9894829619013376\n",
      "99 Train Loss 2.3919487 Test MSE 376.77897648413904 Test RE 0.9894774781988749\n",
      "Training time: 86.43\n",
      "Training time: 86.43\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 138934.9 Test MSE 467.62620524916986 Test RE 1.1023309540363662\n",
      "1 Train Loss 47397.17 Test MSE 469.0809211634499 Test RE 1.1040442168688878\n",
      "2 Train Loss 18042.387 Test MSE 468.3755190442867 Test RE 1.1032137757842826\n",
      "3 Train Loss 8266.633 Test MSE 467.63353543545907 Test RE 1.1023395936928801\n",
      "4 Train Loss 5026.9644 Test MSE 467.78212823612927 Test RE 1.10251471663118\n",
      "5 Train Loss 3467.632 Test MSE 469.5860098819928 Test RE 1.104638453622928\n",
      "6 Train Loss 2350.0588 Test MSE 472.4062737009833 Test RE 1.1079506351799002\n",
      "7 Train Loss 1396.2812 Test MSE 473.60873812669246 Test RE 1.1093598295363158\n",
      "8 Train Loss 999.5064 Test MSE 474.5729560378567 Test RE 1.1104885257085402\n",
      "9 Train Loss 773.21576 Test MSE 474.27541624238177 Test RE 1.1101403533882532\n",
      "10 Train Loss 557.8147 Test MSE 473.60139710915513 Test RE 1.109351231868171\n",
      "11 Train Loss 488.0041 Test MSE 473.1919344035868 Test RE 1.1088715709312873\n",
      "12 Train Loss 445.98352 Test MSE 473.02846198487555 Test RE 1.1086800148596603\n",
      "13 Train Loss 410.29575 Test MSE 472.48463433532015 Test RE 1.1080425223111705\n",
      "14 Train Loss 362.4077 Test MSE 471.29831762159114 Test RE 1.1066506087840475\n",
      "15 Train Loss 331.75488 Test MSE 470.27674581364846 Test RE 1.1054505871549305\n",
      "16 Train Loss 311.85703 Test MSE 468.689282944045 Test RE 1.1035832343605818\n",
      "17 Train Loss 289.0239 Test MSE 467.2666167487363 Test RE 1.1019070451949422\n",
      "18 Train Loss 265.73126 Test MSE 462.38828824538706 Test RE 1.0961399218624879\n",
      "19 Train Loss 256.15384 Test MSE 461.19177231654214 Test RE 1.0947207699516284\n",
      "20 Train Loss 246.54633 Test MSE 460.6515468929549 Test RE 1.0940794214776286\n",
      "21 Train Loss 238.66496 Test MSE 457.2332423197102 Test RE 1.0900125073057294\n",
      "22 Train Loss 213.85931 Test MSE 453.8364178533679 Test RE 1.0859560617432467\n",
      "23 Train Loss 207.16635 Test MSE 451.60296580386887 Test RE 1.0832806239509314\n",
      "24 Train Loss 196.36006 Test MSE 445.54344704801423 Test RE 1.0759884581791874\n",
      "25 Train Loss 188.24283 Test MSE 447.37814582179993 Test RE 1.0782015830662084\n",
      "26 Train Loss 177.40952 Test MSE 442.635823213232 Test RE 1.072471752114373\n",
      "27 Train Loss 165.62122 Test MSE 439.3356766158749 Test RE 1.0684662745699969\n",
      "28 Train Loss 150.3031 Test MSE 435.85866057523 Test RE 1.0642298167543687\n",
      "29 Train Loss 138.10118 Test MSE 429.81148203654897 Test RE 1.0568213761138232\n",
      "30 Train Loss 133.27557 Test MSE 421.27910495245146 Test RE 1.0462790805482978\n",
      "31 Train Loss 123.091446 Test MSE 413.767776337974 Test RE 1.0369096475361326\n",
      "32 Train Loss 117.11237 Test MSE 408.55176272277197 Test RE 1.0303532061885348\n",
      "33 Train Loss 108.003265 Test MSE 405.8357725304341 Test RE 1.0269226793062394\n",
      "34 Train Loss 101.70574 Test MSE 409.2782434447985 Test RE 1.031268878751889\n",
      "35 Train Loss 99.62911 Test MSE 410.81171949997645 Test RE 1.0331990419250718\n",
      "36 Train Loss 99.5565 Test MSE 410.7986551069839 Test RE 1.0331826131999418\n",
      "37 Train Loss 97.50893 Test MSE 411.03255103813996 Test RE 1.033476702309212\n",
      "38 Train Loss 96.69232 Test MSE 410.39916149930826 Test RE 1.032680116123817\n",
      "39 Train Loss 95.534035 Test MSE 407.97098355893104 Test RE 1.0296205933999578\n",
      "40 Train Loss 94.254745 Test MSE 407.5239663768404 Test RE 1.0290563568964959\n",
      "41 Train Loss 93.19 Test MSE 407.37081335574794 Test RE 1.028862972073683\n",
      "42 Train Loss 90.44879 Test MSE 406.51465577995674 Test RE 1.0277812400138433\n",
      "43 Train Loss 87.534035 Test MSE 404.88310383929587 Test RE 1.0257166595844776\n",
      "44 Train Loss 86.54195 Test MSE 405.09926590185574 Test RE 1.0259904317436597\n",
      "45 Train Loss 85.36714 Test MSE 404.44675412775473 Test RE 1.0251637940559328\n",
      "46 Train Loss 83.957245 Test MSE 403.5352377266352 Test RE 1.0240079179118458\n",
      "47 Train Loss 80.18015 Test MSE 403.0592687469645 Test RE 1.02340383211741\n",
      "48 Train Loss 73.82333 Test MSE 401.7984015590418 Test RE 1.0218018505403537\n",
      "49 Train Loss 71.50105 Test MSE 401.1917788487592 Test RE 1.0210302168812888\n",
      "50 Train Loss 67.772316 Test MSE 397.36222408894383 Test RE 1.0161454372989243\n",
      "51 Train Loss 65.97748 Test MSE 394.7868945831227 Test RE 1.01284723347831\n",
      "52 Train Loss 65.09515 Test MSE 394.026043219241 Test RE 1.0118707600133692\n",
      "53 Train Loss 60.05343 Test MSE 395.8525227565266 Test RE 1.014213275773619\n",
      "54 Train Loss 55.756596 Test MSE 396.30131161592016 Test RE 1.0147880336154078\n",
      "55 Train Loss 48.56906 Test MSE 398.28539153852415 Test RE 1.0173251269187333\n",
      "56 Train Loss 45.664906 Test MSE 397.67563277781403 Test RE 1.016546086906189\n",
      "57 Train Loss 38.299797 Test MSE 398.4888244190881 Test RE 1.0175849041603702\n",
      "58 Train Loss 31.839588 Test MSE 396.19134258172113 Test RE 1.0146472278706684\n",
      "59 Train Loss 29.950993 Test MSE 397.26646176832406 Test RE 1.016022986921557\n",
      "60 Train Loss 27.506653 Test MSE 395.8298505355374 Test RE 1.0141842311225753\n",
      "61 Train Loss 23.960455 Test MSE 396.7254842101197 Test RE 1.015330966659413\n",
      "62 Train Loss 21.48321 Test MSE 398.106434048277 Test RE 1.0170965491068167\n",
      "63 Train Loss 19.896935 Test MSE 395.53046004613236 Test RE 1.013800613578981\n",
      "64 Train Loss 13.0679655 Test MSE 393.16329927026607 Test RE 1.0107623767251663\n",
      "65 Train Loss 7.554303 Test MSE 388.7095038997117 Test RE 1.005021059384769\n",
      "66 Train Loss 4.3070893 Test MSE 384.93124674246656 Test RE 1.000124728889369\n",
      "67 Train Loss 3.0246403 Test MSE 383.9545266174709 Test RE 0.9988550704727368\n",
      "68 Train Loss 2.8170328 Test MSE 382.3120073272103 Test RE 0.9967162792969402\n",
      "69 Train Loss 2.580778 Test MSE 382.8711180290006 Test RE 0.9974448349406405\n",
      "70 Train Loss 2.4588377 Test MSE 382.9220318603807 Test RE 0.997511152361262\n",
      "71 Train Loss 2.4210668 Test MSE 382.7755067258139 Test RE 0.9973202852546168\n",
      "72 Train Loss 2.38933 Test MSE 382.76461022345313 Test RE 0.9973060897536078\n",
      "73 Train Loss 2.3831816 Test MSE 382.7260914255221 Test RE 0.9972559074780978\n",
      "74 Train Loss 2.3807786 Test MSE 382.63632084814765 Test RE 0.9971389446019958\n",
      "75 Train Loss 2.377996 Test MSE 382.44761536745204 Test RE 0.9968930338261169\n",
      "76 Train Loss 2.375469 Test MSE 382.17929017958056 Test RE 0.99654326246619\n",
      "77 Train Loss 2.3739927 Test MSE 381.9983616176172 Test RE 0.996307346381055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 Train Loss 2.3702657 Test MSE 381.29171959780894 Test RE 0.9953854071823038\n",
      "79 Train Loss 2.367306 Test MSE 381.0791389536222 Test RE 0.9951078910947748\n",
      "80 Train Loss 2.364019 Test MSE 380.6928167324832 Test RE 0.9946033636472609\n",
      "81 Train Loss 2.362665 Test MSE 380.5187821246051 Test RE 0.9943759950366082\n",
      "82 Train Loss 2.3616104 Test MSE 380.43810094878455 Test RE 0.9942705709664299\n",
      "83 Train Loss 2.3609283 Test MSE 380.1996794201779 Test RE 0.9939589661859769\n",
      "84 Train Loss 2.3605313 Test MSE 380.2029972690285 Test RE 0.9939633031154872\n",
      "85 Train Loss 2.3592849 Test MSE 379.9209380887363 Test RE 0.993594541569063\n",
      "86 Train Loss 2.358741 Test MSE 379.81744427959086 Test RE 0.9934592004000109\n",
      "87 Train Loss 2.3572807 Test MSE 379.77485992922647 Test RE 0.9934035065381616\n",
      "88 Train Loss 2.3564093 Test MSE 379.584813421575 Test RE 0.9931549165025658\n",
      "89 Train Loss 2.3535357 Test MSE 378.8905782054395 Test RE 0.9922462939273713\n",
      "90 Train Loss 2.3526354 Test MSE 378.6985154318796 Test RE 0.9919947731147883\n",
      "91 Train Loss 2.3507876 Test MSE 378.0127655155131 Test RE 0.9910962106183083\n",
      "92 Train Loss 2.3488936 Test MSE 377.8847814614013 Test RE 0.9909284183360108\n",
      "93 Train Loss 2.3470263 Test MSE 377.53958385036844 Test RE 0.9904757085491647\n",
      "94 Train Loss 2.345682 Test MSE 377.29819951317666 Test RE 0.9901590218223147\n",
      "95 Train Loss 2.3445866 Test MSE 377.1452178462474 Test RE 0.9899582634624684\n",
      "96 Train Loss 2.342755 Test MSE 376.75330321649113 Test RE 0.9894437667201778\n",
      "97 Train Loss 2.3400755 Test MSE 376.61758357011735 Test RE 0.9892655346345307\n",
      "98 Train Loss 2.3379767 Test MSE 376.39956904542066 Test RE 0.9889791626143557\n",
      "99 Train Loss 2.3366709 Test MSE 376.42636776785235 Test RE 0.9890143684282329\n",
      "Training time: 92.29\n",
      "Training time: 92.29\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 1087.3411 Test MSE 349.7215470808467 Test RE 0.9532872357845681\n",
      "1 Train Loss 262.3622 Test MSE 347.41114969571254 Test RE 0.9501331235782773\n",
      "2 Train Loss 160.3269 Test MSE 349.8270021513823 Test RE 0.9534309521158789\n",
      "3 Train Loss 118.747154 Test MSE 351.0236039182608 Test RE 0.9550601905854897\n",
      "4 Train Loss 90.25877 Test MSE 351.92372791108215 Test RE 0.9562839290705688\n",
      "5 Train Loss 71.295525 Test MSE 354.57696321874624 Test RE 0.9598819842786255\n",
      "6 Train Loss 52.41022 Test MSE 357.38294466751654 Test RE 0.9636725624235132\n",
      "7 Train Loss 34.584373 Test MSE 360.50772490068107 Test RE 0.9678763323359177\n",
      "8 Train Loss 26.750511 Test MSE 361.8338347503284 Test RE 0.969654840926111\n",
      "9 Train Loss 22.891895 Test MSE 362.3673689062292 Test RE 0.970369469242092\n",
      "10 Train Loss 18.102547 Test MSE 364.46147393339083 Test RE 0.973169291218856\n",
      "11 Train Loss 13.719758 Test MSE 364.4278070340005 Test RE 0.9731243422302457\n",
      "12 Train Loss 8.195822 Test MSE 368.4062102411197 Test RE 0.978421649570951\n",
      "13 Train Loss 6.3699293 Test MSE 371.3785752033865 Test RE 0.9823607576481981\n",
      "14 Train Loss 4.607973 Test MSE 373.38053754060013 Test RE 0.9850049677916258\n",
      "15 Train Loss 3.4499507 Test MSE 373.37154791491514 Test RE 0.9849931100776229\n",
      "16 Train Loss 2.81214 Test MSE 373.5663517303911 Test RE 0.9852500329679925\n",
      "17 Train Loss 2.5942152 Test MSE 374.39608601715406 Test RE 0.9863436034873517\n",
      "18 Train Loss 2.4811416 Test MSE 375.0046083367304 Test RE 0.9871448517243964\n",
      "19 Train Loss 2.443837 Test MSE 375.1765946968922 Test RE 0.9873711902611597\n",
      "20 Train Loss 2.419441 Test MSE 375.3342608822573 Test RE 0.9875786374997649\n",
      "21 Train Loss 2.3941152 Test MSE 375.27637790482106 Test RE 0.9875024837855483\n",
      "22 Train Loss 2.3830512 Test MSE 375.2618158021879 Test RE 0.987483324236921\n",
      "23 Train Loss 2.3772786 Test MSE 375.26869750652224 Test RE 0.9874923786315217\n",
      "24 Train Loss 2.3649445 Test MSE 374.6501932359664 Test RE 0.9866782684665719\n",
      "25 Train Loss 2.3639262 Test MSE 374.62352864142565 Test RE 0.9866431559208093\n",
      "26 Train Loss 2.3564706 Test MSE 374.22294816092455 Test RE 0.9861155119403026\n",
      "27 Train Loss 2.3475444 Test MSE 373.99356711820155 Test RE 0.9858132444310299\n",
      "28 Train Loss 2.3452907 Test MSE 373.81649776866726 Test RE 0.9855798474003329\n",
      "29 Train Loss 2.34513 Test MSE 373.8197269248243 Test RE 0.9855841042808859\n",
      "30 Train Loss 2.3450336 Test MSE 373.83483686390747 Test RE 0.9856040229264539\n",
      "31 Train Loss 2.34496 Test MSE 373.8471013176108 Test RE 0.9856201902209061\n",
      "32 Train Loss 2.3448865 Test MSE 373.86023404725586 Test RE 0.9856375018033314\n",
      "33 Train Loss 2.3447967 Test MSE 373.87582757294615 Test RE 0.9856580568156664\n",
      "34 Train Loss 2.3352437 Test MSE 373.4665910732542 Test RE 0.9851184689858907\n",
      "35 Train Loss 2.32733 Test MSE 372.98925941542467 Test RE 0.9844887224567025\n",
      "36 Train Loss 2.3241117 Test MSE 372.69475394313815 Test RE 0.9840999785937775\n",
      "37 Train Loss 2.323673 Test MSE 372.66976092047787 Test RE 0.9840669810198027\n",
      "38 Train Loss 2.3236022 Test MSE 372.68399402360257 Test RE 0.9840857727149716\n",
      "39 Train Loss 2.3234975 Test MSE 372.70095895772647 Test RE 0.984108170725868\n",
      "40 Train Loss 2.323397 Test MSE 372.7198146018873 Test RE 0.9841330643548581\n",
      "41 Train Loss 2.3229349 Test MSE 372.78201237386077 Test RE 0.9842151747338823\n",
      "42 Train Loss 2.3208315 Test MSE 372.59848811306614 Test RE 0.9839728754905693\n",
      "43 Train Loss 2.3206625 Test MSE 372.58072274886325 Test RE 0.9839494174715097\n",
      "44 Train Loss 2.3205023 Test MSE 372.5635752742614 Test RE 0.9839267748054377\n",
      "45 Train Loss 2.3203502 Test MSE 372.541245901322 Test RE 0.9838972888350365\n",
      "46 Train Loss 2.31836 Test MSE 372.28919440548555 Test RE 0.9835643931534777\n",
      "47 Train Loss 2.3181193 Test MSE 372.2589981279306 Test RE 0.983524504021428\n",
      "48 Train Loss 2.3180594 Test MSE 372.2548321071583 Test RE 0.9835190006017686\n",
      "49 Train Loss 2.3179908 Test MSE 372.2517221160807 Test RE 0.9835148922041852\n",
      "50 Train Loss 2.3179178 Test MSE 372.24774419045383 Test RE 0.9835096372123158\n",
      "51 Train Loss 2.3174455 Test MSE 372.1704639966026 Test RE 0.9834075415491512\n",
      "52 Train Loss 2.3173084 Test MSE 372.16077805455313 Test RE 0.9833947446034562\n",
      "53 Train Loss 2.3166873 Test MSE 372.12790166229746 Test RE 0.9833513074825718\n",
      "54 Train Loss 2.316646 Test MSE 372.12538297457667 Test RE 0.9833479796495298\n",
      "55 Train Loss 2.3165803 Test MSE 372.1221866392574 Test RE 0.9833437564538385\n",
      "56 Train Loss 2.3165164 Test MSE 372.11891257560177 Test RE 0.9833394305398628\n",
      "57 Train Loss 2.316405 Test MSE 372.1110861284999 Test RE 0.9833290896323696\n",
      "58 Train Loss 2.3152373 Test MSE 371.9515121943106 Test RE 0.9831182244856702\n",
      "59 Train Loss 2.31513 Test MSE 371.94554238554656 Test RE 0.9831103349474508\n",
      "60 Train Loss 2.3151083 Test MSE 371.9420100891127 Test RE 0.9831056667299578\n",
      "61 Train Loss 2.3150876 Test MSE 371.9428717278161 Test RE 0.9831068054577409\n",
      "62 Train Loss 2.3150692 Test MSE 371.9414395231309 Test RE 0.9831049126784528\n",
      "63 Train Loss 2.3132565 Test MSE 371.66957712650503 Test RE 0.982745557567957\n",
      "64 Train Loss 2.3122983 Test MSE 371.52295249286595 Test RE 0.982551690581731\n",
      "65 Train Loss 2.3122647 Test MSE 371.51224160663463 Test RE 0.982537527156355\n",
      "66 Train Loss 2.3122232 Test MSE 371.50218624328204 Test RE 0.982524230368808\n",
      "67 Train Loss 2.311352 Test MSE 371.4704299766526 Test RE 0.9824822360422728\n",
      "68 Train Loss 2.3103228 Test MSE 371.29884428788006 Test RE 0.9822553009362326\n",
      "69 Train Loss 2.3085327 Test MSE 370.9938315357914 Test RE 0.9818517689147188\n",
      "70 Train Loss 2.3072708 Test MSE 370.59204362383025 Test RE 0.9813199500451261\n",
      "71 Train Loss 2.3071945 Test MSE 370.5926798544686 Test RE 0.98132079240745\n",
      "72 Train Loss 2.3049245 Test MSE 370.378384105338 Test RE 0.9810370263010055\n",
      "73 Train Loss 2.3046846 Test MSE 370.35715846252964 Test RE 0.9810089152654574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 Train Loss 2.3039927 Test MSE 370.3160245967111 Test RE 0.980954435679098\n",
      "75 Train Loss 2.3032212 Test MSE 370.27216109787946 Test RE 0.9808963375073778\n",
      "76 Train Loss 2.3022826 Test MSE 370.1621461140291 Test RE 0.9807506050934423\n",
      "77 Train Loss 2.3012762 Test MSE 369.9840624304949 Test RE 0.9805146589090099\n",
      "78 Train Loss 2.3010697 Test MSE 369.9624007955917 Test RE 0.9804859551571393\n",
      "79 Train Loss 2.3004963 Test MSE 369.92113729476915 Test RE 0.9804312747205399\n",
      "80 Train Loss 2.2997842 Test MSE 369.8372158210466 Test RE 0.9803200565496603\n",
      "81 Train Loss 2.2993944 Test MSE 369.69408663048483 Test RE 0.9801303433620145\n",
      "82 Train Loss 2.2992065 Test MSE 369.67704123225724 Test RE 0.9801077477820483\n",
      "83 Train Loss 2.2986922 Test MSE 369.57058787646963 Test RE 0.9799666201768128\n",
      "84 Train Loss 2.2982295 Test MSE 369.42956327547006 Test RE 0.9797796293927169\n",
      "85 Train Loss 2.2981858 Test MSE 369.4156874057247 Test RE 0.9797612288320535\n",
      "86 Train Loss 2.2981791 Test MSE 369.4061906311874 Test RE 0.9797486351176986\n",
      "87 Train Loss 2.2978792 Test MSE 369.36917812217325 Test RE 0.979699551122232\n",
      "88 Train Loss 2.2970736 Test MSE 369.1738688614608 Test RE 0.9794405017657901\n",
      "89 Train Loss 2.2968073 Test MSE 369.146039573906 Test RE 0.9794035846823189\n",
      "90 Train Loss 2.2967715 Test MSE 369.1548886762932 Test RE 0.9794153236551518\n",
      "91 Train Loss 2.2947943 Test MSE 368.84625235226486 Test RE 0.9790058121428539\n",
      "92 Train Loss 2.2939591 Test MSE 368.79200178235857 Test RE 0.9789338125278467\n",
      "93 Train Loss 2.293817 Test MSE 368.7801261385066 Test RE 0.9789180508480287\n",
      "94 Train Loss 2.2933028 Test MSE 368.5416672244173 Test RE 0.9786015079188086\n",
      "95 Train Loss 2.291596 Test MSE 368.4065641343409 Test RE 0.9784221195096985\n",
      "96 Train Loss 2.2914124 Test MSE 368.38539382850576 Test RE 0.9783940068278089\n",
      "97 Train Loss 2.2903817 Test MSE 368.16207072184574 Test RE 0.9780974001797396\n",
      "98 Train Loss 2.2903395 Test MSE 368.1463334198541 Test RE 0.9780764952850165\n",
      "99 Train Loss 2.2903266 Test MSE 368.14064361020957 Test RE 0.9780689370261486\n",
      "Training time: 59.26\n",
      "Training time: 59.26\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 74587.945 Test MSE 396.80166586329125 Test RE 1.015428447010066\n",
      "1 Train Loss 12729.121 Test MSE 398.5624900361029 Test RE 1.0176789564288247\n",
      "2 Train Loss 3348.732 Test MSE 399.15654021947296 Test RE 1.0184370900777704\n",
      "3 Train Loss 866.9795 Test MSE 391.84837048486025 Test RE 1.009070721271121\n",
      "4 Train Loss 391.7349 Test MSE 389.23911797152175 Test RE 1.0057054935301835\n",
      "5 Train Loss 274.91678 Test MSE 386.5106531610836 Test RE 1.0021744279364533\n",
      "6 Train Loss 168.16556 Test MSE 385.7878804254292 Test RE 1.001236959209647\n",
      "7 Train Loss 167.57405 Test MSE 385.8023984232057 Test RE 1.0012557983437065\n",
      "8 Train Loss 127.22278 Test MSE 387.23312266015773 Test RE 1.003110627781127\n",
      "9 Train Loss 96.52463 Test MSE 389.15772635362794 Test RE 1.005600339303612\n",
      "10 Train Loss 86.00256 Test MSE 389.9893399931671 Test RE 1.0066742286411194\n",
      "11 Train Loss 70.2821 Test MSE 390.4440642661901 Test RE 1.0072609444840352\n",
      "12 Train Loss 39.235847 Test MSE 390.04276019582414 Test RE 1.006743172704904\n",
      "13 Train Loss 29.2672 Test MSE 388.26143062162936 Test RE 1.0044416383453116\n",
      "14 Train Loss 21.765934 Test MSE 385.70380711875674 Test RE 1.0011278553564604\n",
      "15 Train Loss 17.79221 Test MSE 385.3484852179106 Test RE 1.0006666146202567\n",
      "16 Train Loss 17.719925 Test MSE 385.34535059270206 Test RE 1.0006625446403623\n",
      "17 Train Loss 16.807985 Test MSE 385.3530186324085 Test RE 1.0006725007510842\n",
      "18 Train Loss 13.918834 Test MSE 384.8347848559446 Test RE 0.999999407856519\n",
      "19 Train Loss 11.093304 Test MSE 384.4369550159453 Test RE 0.9994823905686705\n",
      "20 Train Loss 9.467723 Test MSE 383.9804598616054 Test RE 0.9988888024816327\n",
      "21 Train Loss 7.852974 Test MSE 383.57852542605616 Test RE 0.9983658684145355\n",
      "22 Train Loss 7.163667 Test MSE 383.46204988765953 Test RE 0.9982142775103032\n",
      "23 Train Loss 6.6010766 Test MSE 383.2108142123619 Test RE 0.997887220205495\n",
      "24 Train Loss 5.3958464 Test MSE 383.41885066393326 Test RE 0.9981580486129845\n",
      "25 Train Loss 4.20204 Test MSE 383.18128453226853 Test RE 0.9978487715801069\n",
      "26 Train Loss 3.839235 Test MSE 383.106563720844 Test RE 0.9977514759812852\n",
      "27 Train Loss 3.606664 Test MSE 382.87204486771867 Test RE 0.997446042226708\n",
      "28 Train Loss 3.5042439 Test MSE 382.7450525342964 Test RE 0.9972806103190686\n",
      "29 Train Loss 3.0889263 Test MSE 382.19591815460905 Test RE 0.9965649411869818\n",
      "30 Train Loss 2.8568628 Test MSE 382.05949654591336 Test RE 0.9963870675886881\n",
      "31 Train Loss 2.7844834 Test MSE 382.003942446027 Test RE 0.9963146241610065\n",
      "32 Train Loss 2.7752852 Test MSE 382.0007296294189 Test RE 0.996310434436051\n",
      "33 Train Loss 2.7353716 Test MSE 381.9479263183646 Test RE 0.9962415729080263\n",
      "34 Train Loss 2.7331638 Test MSE 381.943927417702 Test RE 0.9962363576924969\n",
      "35 Train Loss 2.6804836 Test MSE 381.95949195082517 Test RE 0.996256656216322\n",
      "36 Train Loss 2.6349201 Test MSE 381.98008258848546 Test RE 0.9962835089111807\n",
      "37 Train Loss 2.5965447 Test MSE 382.1139502036984 Test RE 0.9964580709111602\n",
      "38 Train Loss 2.5739803 Test MSE 382.34817766955695 Test RE 0.9967634275890277\n",
      "39 Train Loss 2.5544333 Test MSE 382.6266642597114 Test RE 0.9971263621305609\n",
      "40 Train Loss 2.5436141 Test MSE 382.6091083900578 Test RE 0.9971034865407502\n",
      "41 Train Loss 2.5166802 Test MSE 382.6576362598561 Test RE 0.9971667178817153\n",
      "42 Train Loss 2.4937875 Test MSE 382.85549837282196 Test RE 0.9974244887934036\n",
      "43 Train Loss 2.4600654 Test MSE 383.130413105279 Test RE 0.9977825318159391\n",
      "44 Train Loss 2.4507356 Test MSE 383.18324297025504 Test RE 0.9978513215773547\n",
      "45 Train Loss 2.4426763 Test MSE 383.15867193891995 Test RE 0.9978193282300447\n",
      "46 Train Loss 2.439394 Test MSE 383.11341454461115 Test RE 0.997760396983448\n",
      "47 Train Loss 2.4355738 Test MSE 383.0659491343878 Test RE 0.9976985868618026\n",
      "48 Train Loss 2.427083 Test MSE 382.8050579904308 Test RE 0.9973587823715052\n",
      "49 Train Loss 2.4264371 Test MSE 382.78814702302765 Test RE 0.997336752244443\n",
      "50 Train Loss 2.411192 Test MSE 382.8657673241641 Test RE 0.9974378651650453\n",
      "51 Train Loss 2.3979993 Test MSE 382.859458632901 Test RE 0.9974296474631715\n",
      "52 Train Loss 2.3977423 Test MSE 382.8526750191928 Test RE 0.9974208110517307\n",
      "53 Train Loss 2.397637 Test MSE 382.8508878024508 Test RE 0.9974184829900912\n",
      "54 Train Loss 2.3976023 Test MSE 382.84956895472567 Test RE 0.9974167650311261\n",
      "55 Train Loss 2.397438 Test MSE 382.8486906975858 Test RE 0.9974156209931424\n",
      "56 Train Loss 2.3969398 Test MSE 382.84147036858104 Test RE 0.9974062155765008\n",
      "57 Train Loss 2.3968437 Test MSE 382.83830459024443 Test RE 0.9974020917109296\n",
      "58 Train Loss 2.3967512 Test MSE 382.83459996965007 Test RE 0.9973972659068966\n",
      "59 Train Loss 2.3966813 Test MSE 382.8292489626634 Test RE 0.9973902954052902\n",
      "60 Train Loss 2.3965693 Test MSE 382.821843399339 Test RE 0.9973806484507635\n",
      "61 Train Loss 2.3939443 Test MSE 382.7649679632989 Test RE 0.9973065558051102\n",
      "62 Train Loss 2.3912685 Test MSE 382.89806118735464 Test RE 0.9974799300941959\n",
      "63 Train Loss 2.391253 Test MSE 382.9095264061028 Test RE 0.9974948638861706\n",
      "64 Train Loss 2.3911796 Test MSE 382.9129868490143 Test RE 0.9974993711731331\n",
      "65 Train Loss 2.3911304 Test MSE 382.91597005419936 Test RE 0.9975032568333256\n",
      "66 Train Loss 2.389004 Test MSE 383.0018782635779 Test RE 0.9976151468032944\n",
      "67 Train Loss 2.3862393 Test MSE 383.0272970006141 Test RE 0.9976482506831925\n",
      "68 Train Loss 2.3849611 Test MSE 383.08026043985046 Test RE 0.9977172236473371\n",
      "69 Train Loss 2.3849354 Test MSE 383.08064375913966 Test RE 0.9977177228170943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 Train Loss 2.3849049 Test MSE 383.0803181510032 Test RE 0.9977172988005293\n",
      "71 Train Loss 2.3848722 Test MSE 383.0776973039245 Test RE 0.9977138858492282\n",
      "72 Train Loss 2.3839784 Test MSE 383.02824396318283 Test RE 0.997649483930762\n",
      "73 Train Loss 2.3839784 Test MSE 383.02824396318283 Test RE 0.997649483930762\n",
      "74 Train Loss 2.3839784 Test MSE 383.02824396318283 Test RE 0.997649483930762\n",
      "75 Train Loss 2.3839784 Test MSE 383.02824396318283 Test RE 0.997649483930762\n",
      "76 Train Loss 2.3839767 Test MSE 383.0268331179008 Test RE 0.9976476465592551\n",
      "77 Train Loss 2.3839319 Test MSE 383.02539377714135 Test RE 0.9976457720740415\n",
      "78 Train Loss 2.3839312 Test MSE 383.02292683981307 Test RE 0.9976425593189625\n",
      "79 Train Loss 2.3838937 Test MSE 383.02055211301786 Test RE 0.9976394666422274\n",
      "80 Train Loss 2.3838928 Test MSE 383.02055171531424 Test RE 0.9976394661242853\n",
      "81 Train Loss 2.3838758 Test MSE 383.0177139263145 Test RE 0.9976357703754101\n",
      "82 Train Loss 2.3838613 Test MSE 383.01492923006634 Test RE 0.9976321437577168\n",
      "83 Train Loss 2.3838599 Test MSE 383.01218624798787 Test RE 0.9976285714531122\n",
      "84 Train Loss 2.3838215 Test MSE 383.0116815936822 Test RE 0.9976279142185149\n",
      "85 Train Loss 2.3837955 Test MSE 383.01123858562954 Test RE 0.9976273372683218\n",
      "86 Train Loss 2.3837955 Test MSE 383.01123858562954 Test RE 0.9976273372683218\n",
      "87 Train Loss 2.383787 Test MSE 383.0114248567994 Test RE 0.9976275798580553\n",
      "88 Train Loss 2.383787 Test MSE 383.0114248567994 Test RE 0.9976275798580553\n",
      "89 Train Loss 2.383779 Test MSE 383.0177414629981 Test RE 0.9976358062374329\n",
      "90 Train Loss 2.3837712 Test MSE 383.01775703055534 Test RE 0.9976358265116272\n",
      "91 Train Loss 2.3837645 Test MSE 383.01815661749407 Test RE 0.9976363469080509\n",
      "92 Train Loss 2.3837636 Test MSE 383.0204939988738 Test RE 0.9976393909583436\n",
      "93 Train Loss 2.3837473 Test MSE 383.0235330786884 Test RE 0.9976433488401915\n",
      "94 Train Loss 2.3837278 Test MSE 383.0272145882554 Test RE 0.9976481433559203\n",
      "95 Train Loss 2.3836956 Test MSE 383.02723358501885 Test RE 0.9976481680957883\n",
      "96 Train Loss 2.3836942 Test MSE 383.0272335992112 Test RE 0.9976481681142713\n",
      "97 Train Loss 2.3836942 Test MSE 383.0272335992112 Test RE 0.9976481681142713\n",
      "98 Train Loss 2.3836942 Test MSE 383.0272335992112 Test RE 0.9976481681142713\n",
      "99 Train Loss 2.3836942 Test MSE 383.0272335992112 Test RE 0.9976481681142713\n",
      "Training time: 51.45\n",
      "Training time: 51.45\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 43106.74 Test MSE 418.49187049636413 Test RE 1.0428121810289608\n",
      "1 Train Loss 13349.481 Test MSE 418.02593460595745 Test RE 1.0422315019600905\n",
      "2 Train Loss 5099.58 Test MSE 418.4596872839487 Test RE 1.042772082650047\n",
      "3 Train Loss 2820.8054 Test MSE 418.6888626061706 Test RE 1.043057587950079\n",
      "4 Train Loss 2033.6976 Test MSE 419.0648385014401 Test RE 1.0435258073674174\n",
      "5 Train Loss 1600.7242 Test MSE 419.0130062666926 Test RE 1.0434612708777982\n",
      "6 Train Loss 1599.2782 Test MSE 419.0078367498808 Test RE 1.0434548340761487\n",
      "7 Train Loss 1597.9451 Test MSE 419.003753698742 Test RE 1.043449750054498\n",
      "8 Train Loss 1596.0833 Test MSE 418.99951101083303 Test RE 1.0434444672344685\n",
      "9 Train Loss 1593.0009 Test MSE 418.9948158957534 Test RE 1.043438621039606\n",
      "10 Train Loss 1587.5869 Test MSE 418.98965652979 Test RE 1.043432196737831\n",
      "11 Train Loss 1175.4302 Test MSE 419.0673232634641 Test RE 1.0435289010527087\n",
      "12 Train Loss 1009.6155 Test MSE 418.8279050479975 Test RE 1.0432307681421589\n",
      "13 Train Loss 848.8444 Test MSE 419.2093406341403 Test RE 1.0437057063878838\n",
      "14 Train Loss 772.81946 Test MSE 419.2403704192597 Test RE 1.0437443331089267\n",
      "15 Train Loss 618.00366 Test MSE 419.76893883481773 Test RE 1.044402090027516\n",
      "16 Train Loss 535.46204 Test MSE 420.5555893112623 Test RE 1.0453802410700705\n",
      "17 Train Loss 491.5858 Test MSE 421.0514182272221 Test RE 1.0459963035824378\n",
      "18 Train Loss 425.9047 Test MSE 421.48410914496634 Test RE 1.0465336214307333\n",
      "19 Train Loss 361.8661 Test MSE 422.5792823311074 Test RE 1.047892382033258\n",
      "20 Train Loss 354.41016 Test MSE 422.713863283244 Test RE 1.0480592325319114\n",
      "21 Train Loss 353.96725 Test MSE 422.72389970431044 Test RE 1.0480716744012681\n",
      "22 Train Loss 353.51733 Test MSE 422.72976578554824 Test RE 1.048078946349058\n",
      "23 Train Loss 318.47968 Test MSE 422.7308819724703 Test RE 1.0480803300359138\n",
      "24 Train Loss 284.51282 Test MSE 422.7411520629355 Test RE 1.0480930613206245\n",
      "25 Train Loss 233.21584 Test MSE 423.44426308099537 Test RE 1.0489643032296867\n",
      "26 Train Loss 226.761 Test MSE 423.50488118214633 Test RE 1.0490393827079965\n",
      "27 Train Loss 198.68808 Test MSE 423.6540029892151 Test RE 1.049224056943312\n",
      "28 Train Loss 182.92992 Test MSE 424.4202848511275 Test RE 1.050172517395002\n",
      "29 Train Loss 170.85483 Test MSE 424.00271763795934 Test RE 1.0496557825222714\n",
      "30 Train Loss 165.44885 Test MSE 424.13895136734806 Test RE 1.049824398230648\n",
      "31 Train Loss 154.47203 Test MSE 423.82509574100425 Test RE 1.0494359002273168\n",
      "32 Train Loss 145.10992 Test MSE 423.6969293895441 Test RE 1.0492772114877609\n",
      "33 Train Loss 142.77135 Test MSE 423.6267284237139 Test RE 1.04919028222306\n",
      "34 Train Loss 138.56955 Test MSE 423.64596995236906 Test RE 1.0492141095624263\n",
      "35 Train Loss 127.24017 Test MSE 423.9599436237166 Test RE 1.0496028357816907\n",
      "36 Train Loss 124.66514 Test MSE 424.08355420495104 Test RE 1.0497558367467201\n",
      "37 Train Loss 124.56942 Test MSE 424.0921867476956 Test RE 1.0497665209809073\n",
      "38 Train Loss 124.5108 Test MSE 424.0928460924439 Test RE 1.0497673370272707\n",
      "39 Train Loss 124.47303 Test MSE 424.09341725127507 Test RE 1.0497680439287063\n",
      "40 Train Loss 124.43328 Test MSE 424.0901889749902 Test RE 1.0497640484083888\n",
      "41 Train Loss 124.39838 Test MSE 424.08719568369617 Test RE 1.0497603437068737\n",
      "42 Train Loss 124.36986 Test MSE 424.08383881822925 Test RE 1.0497561890056049\n",
      "43 Train Loss 124.34364 Test MSE 424.08182677130446 Test RE 1.0497536987418603\n",
      "44 Train Loss 124.30949 Test MSE 424.0801310980556 Test RE 1.0497516000418787\n",
      "45 Train Loss 124.267136 Test MSE 424.080671731434 Test RE 1.0497522691731718\n",
      "46 Train Loss 124.202415 Test MSE 424.0825393042351 Test RE 1.0497545806279942\n",
      "47 Train Loss 118.185524 Test MSE 424.96066274787773 Test RE 1.0508408519648267\n",
      "48 Train Loss 115.24796 Test MSE 425.20196906371797 Test RE 1.0511391602201807\n",
      "49 Train Loss 114.78056 Test MSE 425.2991481191597 Test RE 1.0512592712290143\n",
      "50 Train Loss 113.915695 Test MSE 425.7871804197854 Test RE 1.051862260191138\n",
      "51 Train Loss 112.43058 Test MSE 426.37843885650415 Test RE 1.0525923275931326\n",
      "52 Train Loss 109.58512 Test MSE 426.4582442295955 Test RE 1.0526908299861193\n",
      "53 Train Loss 103.99153 Test MSE 426.4354006932293 Test RE 1.0526626355454942\n",
      "54 Train Loss 101.90655 Test MSE 426.14886558802436 Test RE 1.0523089178624485\n",
      "55 Train Loss 101.11803 Test MSE 426.05440376279256 Test RE 1.0521922819421838\n",
      "56 Train Loss 101.079445 Test MSE 426.0543264765193 Test RE 1.0521921865083341\n",
      "57 Train Loss 101.00746 Test MSE 426.07000838226105 Test RE 1.0522115505023542\n",
      "58 Train Loss 97.98096 Test MSE 426.55774062231745 Test RE 1.0528136237569512\n",
      "59 Train Loss 92.53682 Test MSE 426.3260862208719 Test RE 1.0525277046323813\n",
      "60 Train Loss 90.74497 Test MSE 426.57726447648076 Test RE 1.0528377174991286\n",
      "61 Train Loss 86.403076 Test MSE 427.90298297581046 Test RE 1.0544724549719948\n",
      "62 Train Loss 83.86716 Test MSE 428.24633762775096 Test RE 1.05489543101601\n",
      "63 Train Loss 82.42715 Test MSE 427.6318041509437 Test RE 1.054138271837677\n",
      "64 Train Loss 81.61288 Test MSE 427.28939020756684 Test RE 1.0537161517762244\n",
      "65 Train Loss 79.33092 Test MSE 427.82667652949493 Test RE 1.0543784305827393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 Train Loss 77.597565 Test MSE 428.0347546112539 Test RE 1.054634803529735\n",
      "67 Train Loss 74.86167 Test MSE 426.90010884749165 Test RE 1.0532360491398183\n",
      "68 Train Loss 72.51281 Test MSE 425.6123999762089 Test RE 1.0516463497184025\n",
      "69 Train Loss 69.91926 Test MSE 424.006930901882 Test RE 1.0496609976608418\n",
      "70 Train Loss 67.353134 Test MSE 423.3113490567537 Test RE 1.048799661715529\n",
      "71 Train Loss 65.10994 Test MSE 422.96612103470676 Test RE 1.0483719045983662\n",
      "72 Train Loss 63.52965 Test MSE 423.1790903799303 Test RE 1.048635806325973\n",
      "73 Train Loss 62.194435 Test MSE 423.8564383961034 Test RE 1.0494747033795069\n",
      "74 Train Loss 60.905678 Test MSE 423.0025640473013 Test RE 1.0484170677990334\n",
      "75 Train Loss 60.677925 Test MSE 422.90682992940276 Test RE 1.0482984219901728\n",
      "76 Train Loss 60.60974 Test MSE 422.81487125947865 Test RE 1.0481844425475026\n",
      "77 Train Loss 60.581985 Test MSE 422.7898843549317 Test RE 1.0481534700423318\n",
      "78 Train Loss 60.079468 Test MSE 422.95609588886344 Test RE 1.0483594802657812\n",
      "79 Train Loss 59.15511 Test MSE 422.9605935088187 Test RE 1.048365054260375\n",
      "80 Train Loss 58.32742 Test MSE 422.7833764746412 Test RE 1.0481454030523076\n",
      "81 Train Loss 58.121773 Test MSE 422.7470848961722 Test RE 1.0481004158678489\n",
      "82 Train Loss 58.090614 Test MSE 422.7308880151972 Test RE 1.0480803375268064\n",
      "83 Train Loss 58.076645 Test MSE 422.7136760794406 Test RE 1.0480590004592099\n",
      "84 Train Loss 58.05094 Test MSE 422.696675532026 Test RE 1.0480379250163185\n",
      "85 Train Loss 56.641273 Test MSE 422.19963405112884 Test RE 1.0474215592188205\n",
      "86 Train Loss 55.42459 Test MSE 422.36696315034004 Test RE 1.0476290993607729\n",
      "87 Train Loss 54.55528 Test MSE 421.9700725681198 Test RE 1.0471367646329743\n",
      "88 Train Loss 54.107285 Test MSE 421.6368323665156 Test RE 1.046723208068553\n",
      "89 Train Loss 53.051914 Test MSE 421.382502097228 Test RE 1.046407470054387\n",
      "90 Train Loss 51.93911 Test MSE 420.7557269758874 Test RE 1.045628953820508\n",
      "91 Train Loss 50.916122 Test MSE 420.53896734197014 Test RE 1.0453595821483908\n",
      "92 Train Loss 49.332615 Test MSE 420.6487191438331 Test RE 1.045495981652464\n",
      "93 Train Loss 48.68771 Test MSE 420.6259290398905 Test RE 1.0454676595825934\n",
      "94 Train Loss 47.879326 Test MSE 421.09706988174605 Test RE 1.0460530070698841\n",
      "95 Train Loss 45.433804 Test MSE 419.89486508013323 Test RE 1.044558733075196\n",
      "96 Train Loss 42.49357 Test MSE 420.24577037033123 Test RE 1.0449951097356232\n",
      "97 Train Loss 40.63724 Test MSE 421.17745760714934 Test RE 1.046152848430802\n",
      "98 Train Loss 38.055866 Test MSE 420.0308238964298 Test RE 1.044727829540244\n",
      "99 Train Loss 36.624077 Test MSE 420.6999699879104 Test RE 1.0455596700902356\n",
      "Training time: 49.85\n",
      "Training time: 49.85\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 8707.565 Test MSE 393.1557309561763 Test RE 1.0107526481925617\n",
      "1 Train Loss 3294.1924 Test MSE 386.337491593701 Test RE 1.0019499094988984\n",
      "2 Train Loss 1642.6129 Test MSE 382.12146972871045 Test RE 0.9964678753877063\n",
      "3 Train Loss 810.4977 Test MSE 382.103424800641 Test RE 0.9964443470009943\n",
      "4 Train Loss 369.12595 Test MSE 374.8652616987127 Test RE 0.9869614298489521\n",
      "5 Train Loss 212.4864 Test MSE 372.15496113885706 Test RE 0.983387059287035\n",
      "6 Train Loss 118.813965 Test MSE 364.0725116108678 Test RE 0.9726498573940747\n",
      "7 Train Loss 81.03399 Test MSE 361.3052925851365 Test RE 0.9689463793144371\n",
      "8 Train Loss 69.05059 Test MSE 361.8291074933622 Test RE 0.9696485067719595\n",
      "9 Train Loss 53.827015 Test MSE 362.80972438878075 Test RE 0.9709615718590274\n",
      "10 Train Loss 37.554363 Test MSE 362.8605895372224 Test RE 0.9710296328428694\n",
      "11 Train Loss 24.450756 Test MSE 365.64277929705014 Test RE 0.9747451506073646\n",
      "12 Train Loss 19.878616 Test MSE 369.4185961391928 Test RE 0.9797650860838765\n",
      "13 Train Loss 13.09186 Test MSE 371.0449994456661 Test RE 0.9819194756613077\n",
      "14 Train Loss 9.522687 Test MSE 374.09984452854064 Test RE 0.9859533033062198\n",
      "15 Train Loss 7.306964 Test MSE 375.04316324890175 Test RE 0.9871955955073078\n",
      "16 Train Loss 6.017761 Test MSE 374.9425076667505 Test RE 0.9870631128691654\n",
      "17 Train Loss 4.94048 Test MSE 376.2454480866415 Test RE 0.98877666766584\n",
      "18 Train Loss 3.9316056 Test MSE 377.11500570177895 Test RE 0.9899186111486371\n",
      "19 Train Loss 3.4208713 Test MSE 377.56866523196595 Test RE 0.990513855340396\n",
      "20 Train Loss 2.720139 Test MSE 375.3169000617239 Test RE 0.9875557973602689\n",
      "21 Train Loss 2.511044 Test MSE 376.2035295937687 Test RE 0.9887215850297181\n",
      "22 Train Loss 2.4323297 Test MSE 375.9870290988454 Test RE 0.9884370455430286\n",
      "23 Train Loss 2.3886023 Test MSE 376.3454596392969 Test RE 0.9889080745966038\n",
      "24 Train Loss 2.3744385 Test MSE 376.5836454822613 Test RE 0.9892209608566677\n",
      "25 Train Loss 2.365399 Test MSE 376.72265953839576 Test RE 0.9894035271090325\n",
      "26 Train Loss 2.355894 Test MSE 376.88776428596316 Test RE 0.9896203143411113\n",
      "27 Train Loss 2.350203 Test MSE 376.8175220641136 Test RE 0.9895280901096257\n",
      "28 Train Loss 2.3450735 Test MSE 376.43985564169503 Test RE 0.9890320871412128\n",
      "29 Train Loss 2.3449032 Test MSE 376.40731432218286 Test RE 0.9889893378092174\n",
      "30 Train Loss 2.3448634 Test MSE 376.4049819921263 Test RE 0.9889862737705503\n",
      "31 Train Loss 2.3448524 Test MSE 376.4056390708525 Test RE 0.9889871369917991\n",
      "32 Train Loss 2.344846 Test MSE 376.40707587536343 Test RE 0.9889890245562724\n",
      "33 Train Loss 2.3448384 Test MSE 376.40877927182737 Test RE 0.9889912623443076\n",
      "34 Train Loss 2.34482 Test MSE 376.4087107115371 Test RE 0.9889911722752989\n",
      "35 Train Loss 2.3448164 Test MSE 376.40871114332623 Test RE 0.9889911728425487\n",
      "36 Train Loss 2.3448124 Test MSE 376.4109201787722 Test RE 0.988994074891952\n",
      "37 Train Loss 2.344795 Test MSE 376.4071058652911 Test RE 0.9889890639547197\n",
      "38 Train Loss 2.344762 Test MSE 376.40929411688086 Test RE 0.9889919387061689\n",
      "39 Train Loss 2.3436866 Test MSE 376.38758493182945 Test RE 0.988963418530224\n",
      "40 Train Loss 2.3415375 Test MSE 376.1735005902722 Test RE 0.9886821237885267\n",
      "41 Train Loss 2.337736 Test MSE 375.5973131060572 Test RE 0.987924648086042\n",
      "42 Train Loss 2.3355763 Test MSE 375.1345900673654 Test RE 0.9873159158616353\n",
      "43 Train Loss 2.3331807 Test MSE 374.7359773391014 Test RE 0.9867912224531444\n",
      "44 Train Loss 2.3297424 Test MSE 374.7891061227819 Test RE 0.9868611719141873\n",
      "45 Train Loss 2.3267899 Test MSE 374.4852859662957 Test RE 0.9864610947790029\n",
      "46 Train Loss 2.3264334 Test MSE 374.39136085282604 Test RE 0.986337379263208\n",
      "47 Train Loss 2.3261902 Test MSE 374.321026785871 Test RE 0.9862447270481945\n",
      "48 Train Loss 2.3261764 Test MSE 374.31688477391174 Test RE 0.9862392704368067\n",
      "49 Train Loss 2.326159 Test MSE 374.3108487992524 Test RE 0.986231318699275\n",
      "50 Train Loss 2.3254375 Test MSE 373.9909102341794 Test RE 0.9858097427717235\n",
      "51 Train Loss 2.3234675 Test MSE 372.7599211517598 Test RE 0.9841860117949184\n",
      "52 Train Loss 2.3057642 Test MSE 370.6950283811174 Test RE 0.9814562913111714\n",
      "53 Train Loss 2.304435 Test MSE 370.6782979521116 Test RE 0.9814341432260576\n",
      "54 Train Loss 2.303886 Test MSE 370.6599171472693 Test RE 0.9814098097633368\n",
      "55 Train Loss 2.302477 Test MSE 370.5733806261542 Test RE 0.9812952401210384\n",
      "56 Train Loss 2.3022816 Test MSE 370.6554810649719 Test RE 0.9814039369571491\n",
      "57 Train Loss 2.3022614 Test MSE 370.6605964563646 Test RE 0.9814107090786842\n",
      "58 Train Loss 2.3022337 Test MSE 370.66325002401646 Test RE 0.981414222043206\n",
      "59 Train Loss 2.3011389 Test MSE 370.28149929614875 Test RE 0.9809087064452731\n",
      "60 Train Loss 2.300365 Test MSE 369.93316083191786 Test RE 0.980447208057241\n",
      "61 Train Loss 2.2984004 Test MSE 369.15253640476504 Test RE 0.9794122032109686\n",
      "62 Train Loss 2.29628 Test MSE 369.0368737848892 Test RE 0.9792587568102388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 Train Loss 2.2896721 Test MSE 367.4664222156477 Test RE 0.9771728974049864\n",
      "64 Train Loss 2.2850595 Test MSE 367.1029780719762 Test RE 0.9766895394653544\n",
      "65 Train Loss 2.281189 Test MSE 366.8684142831053 Test RE 0.9763774572703658\n",
      "66 Train Loss 2.2780566 Test MSE 366.479361552769 Test RE 0.9758596107314944\n",
      "67 Train Loss 2.2756093 Test MSE 365.7842232466457 Test RE 0.974933665848025\n",
      "68 Train Loss 2.2727418 Test MSE 365.46684824107865 Test RE 0.9745106203685372\n",
      "69 Train Loss 2.2687185 Test MSE 364.9033375759926 Test RE 0.9737590351354302\n",
      "70 Train Loss 2.2639506 Test MSE 364.1933545076694 Test RE 0.9728112649034675\n",
      "71 Train Loss 2.2618182 Test MSE 364.07792303660995 Test RE 0.9726570859031635\n",
      "72 Train Loss 2.2591245 Test MSE 363.4112057161562 Test RE 0.9717660891651492\n",
      "73 Train Loss 2.2562559 Test MSE 362.6705708137574 Test RE 0.9707753506488903\n",
      "74 Train Loss 2.2528772 Test MSE 361.2490678692347 Test RE 0.9688709848272566\n",
      "75 Train Loss 2.2494118 Test MSE 360.89852131730294 Test RE 0.9684007873288101\n",
      "76 Train Loss 2.2434402 Test MSE 359.42651957706437 Test RE 0.9664238546583136\n",
      "77 Train Loss 2.2356825 Test MSE 358.71978992467257 Test RE 0.9654732619377424\n",
      "78 Train Loss 2.2323573 Test MSE 358.86267227007977 Test RE 0.965665522518304\n",
      "79 Train Loss 2.2310655 Test MSE 358.1905340205408 Test RE 0.9647607684347184\n",
      "80 Train Loss 2.2272544 Test MSE 357.3115564458623 Test RE 0.9635763095002149\n",
      "81 Train Loss 2.2248878 Test MSE 356.0187420496966 Test RE 0.9618315378055435\n",
      "82 Train Loss 2.214655 Test MSE 354.6640384135117 Test RE 0.9599998384945336\n",
      "83 Train Loss 2.2078266 Test MSE 354.3617796926079 Test RE 0.9595906763841912\n",
      "84 Train Loss 2.2059882 Test MSE 354.14336320679433 Test RE 0.9592949013579074\n",
      "85 Train Loss 2.2037103 Test MSE 353.75470545240927 Test RE 0.9587683635338313\n",
      "86 Train Loss 2.2004726 Test MSE 353.22663138825357 Test RE 0.9580524864201563\n",
      "87 Train Loss 2.1949663 Test MSE 352.60800842870856 Test RE 0.9572131768470501\n",
      "88 Train Loss 2.19245 Test MSE 352.20184953428645 Test RE 0.9566617250122368\n",
      "89 Train Loss 2.1885777 Test MSE 351.65253061220415 Test RE 0.955915395237421\n",
      "90 Train Loss 2.1870844 Test MSE 351.4936793141025 Test RE 0.9556994639691847\n",
      "91 Train Loss 2.1863847 Test MSE 351.2614479318132 Test RE 0.955383697154339\n",
      "92 Train Loss 2.185495 Test MSE 350.9530448540252 Test RE 0.9549641976959253\n",
      "93 Train Loss 2.182502 Test MSE 350.59642270364856 Test RE 0.9544788792888738\n",
      "94 Train Loss 2.1802258 Test MSE 350.56828404484855 Test RE 0.954440575568848\n",
      "95 Train Loss 2.1786358 Test MSE 349.9676044256779 Test RE 0.9536225340868953\n",
      "96 Train Loss 2.1760483 Test MSE 349.3641502458238 Test RE 0.9528000068261561\n",
      "97 Train Loss 2.1739213 Test MSE 349.15648530139185 Test RE 0.9525167886253899\n",
      "98 Train Loss 2.1729028 Test MSE 349.06006986676175 Test RE 0.9523852664213683\n",
      "99 Train Loss 2.1714962 Test MSE 348.72179136858983 Test RE 0.9519236702974179\n",
      "Training time: 57.77\n",
      "Training time: 57.77\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    alpha_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.5, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"alpha\": alpha_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9861951731359333\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
