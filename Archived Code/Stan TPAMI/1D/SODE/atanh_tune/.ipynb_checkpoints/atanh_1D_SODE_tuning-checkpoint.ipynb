{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.5,0.25,0.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.5381145 Test MSE 383.21158026227056 Test RE 0.9978882176082347\n",
      "1 Train Loss 3.5556834 Test MSE 381.8524086850851 Test RE 0.996116994947617\n",
      "2 Train Loss 2.3911674 Test MSE 380.9360460972038 Test RE 0.9949210451283584\n",
      "3 Train Loss 2.3821518 Test MSE 381.45662908104964 Test RE 0.9956006370704462\n",
      "4 Train Loss 2.3702505 Test MSE 381.55345995600675 Test RE 0.9957269931962117\n",
      "5 Train Loss 2.340914 Test MSE 375.5920615594917 Test RE 0.9879177415531054\n",
      "6 Train Loss 2.3032722 Test MSE 367.30736758469016 Test RE 0.9769613941392613\n",
      "7 Train Loss 2.292344 Test MSE 364.570796204425 Test RE 0.973315234079516\n",
      "8 Train Loss 2.2046983 Test MSE 344.84173080579683 Test RE 0.9466130571483348\n",
      "9 Train Loss 2.1254497 Test MSE 338.88124644675736 Test RE 0.9383964376527025\n",
      "10 Train Loss 2.0314167 Test MSE 321.81560035594424 Test RE 0.9144629808193827\n",
      "11 Train Loss 1.9353342 Test MSE 296.6648188655893 Test RE 0.8780022037659513\n",
      "12 Train Loss 1.7825527 Test MSE 268.5533672508603 Test RE 0.835368093558723\n",
      "13 Train Loss 1.5700336 Test MSE 241.08866567039803 Test RE 0.7915001093336501\n",
      "14 Train Loss 1.3491262 Test MSE 189.28784674893453 Test RE 0.7013324729739684\n",
      "15 Train Loss 1.0769838 Test MSE 152.11723785533522 Test RE 0.6287120699563047\n",
      "16 Train Loss 0.7002226 Test MSE 105.13973054640091 Test RE 0.5226921916726971\n",
      "17 Train Loss 0.5942718 Test MSE 85.95536181080296 Test RE 0.4726058236541298\n",
      "18 Train Loss 0.50744957 Test MSE 72.59709292545755 Test RE 0.4343323698750725\n",
      "19 Train Loss 0.44573465 Test MSE 56.6481678272449 Test RE 0.38366794554622907\n",
      "20 Train Loss 0.43476093 Test MSE 55.0556455704477 Test RE 0.37823656627650787\n",
      "21 Train Loss 0.38391846 Test MSE 48.557479207027114 Test RE 0.35521447007905443\n",
      "22 Train Loss 0.2677391 Test MSE 33.83357692886201 Test RE 0.2965080848522678\n",
      "23 Train Loss 0.24777934 Test MSE 27.395718385494202 Test RE 0.26681112199441964\n",
      "24 Train Loss 0.24000698 Test MSE 24.10637122570733 Test RE 0.25028135792858663\n",
      "25 Train Loss 0.20963377 Test MSE 20.52038293911502 Test RE 0.2309166843854735\n",
      "26 Train Loss 0.17698175 Test MSE 16.911009637562923 Test RE 0.20962706367561879\n",
      "27 Train Loss 0.13307777 Test MSE 13.148533247368986 Test RE 0.18484224600547994\n",
      "28 Train Loss 0.10488255 Test MSE 13.417508798879382 Test RE 0.18672330548717828\n",
      "29 Train Loss 0.094105184 Test MSE 12.218343601457532 Test RE 0.17818401867649764\n",
      "30 Train Loss 0.090159774 Test MSE 10.764337349755635 Test RE 0.16724618980826586\n",
      "31 Train Loss 0.05869241 Test MSE 4.034748520079653 Test RE 0.10239312978775458\n",
      "32 Train Loss 0.0350262 Test MSE 1.2049465720428558 Test RE 0.055955976328206196\n",
      "33 Train Loss 0.02912874 Test MSE 0.6539322958552574 Test RE 0.0412219921029594\n",
      "34 Train Loss 0.028433757 Test MSE 0.7075876681609 Test RE 0.04287979597025771\n",
      "35 Train Loss 0.028057862 Test MSE 0.5966109614714736 Test RE 0.03937387849013868\n",
      "36 Train Loss 0.023903677 Test MSE 0.015223143217431597 Test RE 0.006289480125705343\n",
      "37 Train Loss 0.013283735 Test MSE 0.028299590009725484 Test RE 0.008575365791171392\n",
      "38 Train Loss 0.010437886 Test MSE 0.15756041539295476 Test RE 0.02023420478628632\n",
      "39 Train Loss 0.009117392 Test MSE 0.12155913202601157 Test RE 0.017772821443818834\n",
      "40 Train Loss 0.008518423 Test MSE 0.08707898472714583 Test RE 0.015042473703820981\n",
      "41 Train Loss 0.008413162 Test MSE 0.08322502616449752 Test RE 0.014705830389692687\n",
      "42 Train Loss 0.008239063 Test MSE 0.0793482019853749 Test RE 0.014359229196997664\n",
      "43 Train Loss 0.007937762 Test MSE 0.06455479624186675 Test RE 0.012951701895508353\n",
      "44 Train Loss 0.0067492086 Test MSE 0.019242680934254054 Test RE 0.007071236651724359\n",
      "45 Train Loss 0.006028674 Test MSE 0.0015111948728737808 Test RE 0.001981631139915421\n",
      "46 Train Loss 0.005219214 Test MSE 0.004560667766111395 Test RE 0.003442522603967184\n",
      "47 Train Loss 0.0045730453 Test MSE 0.00025488093707300136 Test RE 0.0008138254370520738\n",
      "48 Train Loss 0.004070472 Test MSE 0.006794002653827894 Test RE 0.0042017038498213454\n",
      "49 Train Loss 0.004041062 Test MSE 0.0071224398751142445 Test RE 0.0043020651045405185\n",
      "50 Train Loss 0.0040324377 Test MSE 0.007249540966274819 Test RE 0.004340280890406498\n",
      "51 Train Loss 0.0040131593 Test MSE 0.0071987146406239525 Test RE 0.004325039335915182\n",
      "52 Train Loss 0.003875623 Test MSE 0.003465334683453971 Test RE 0.0030007870379250207\n",
      "53 Train Loss 0.0037048175 Test MSE 0.00010517059608972338 Test RE 0.000522768908602323\n",
      "54 Train Loss 0.0036828658 Test MSE 4.5745573580738546e-05 Test RE 0.0003447760749622961\n",
      "55 Train Loss 0.0036783835 Test MSE 3.707849867522996e-05 Test RE 0.00031040138690044807\n",
      "56 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "57 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "58 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "59 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "60 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "61 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "62 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "63 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "64 Train Loss 0.0036745453 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "65 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "66 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "67 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "68 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "69 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "70 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "71 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "72 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "73 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "74 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "75 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "76 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "77 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "78 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "79 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "80 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "81 Train Loss 0.0036745458 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "82 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "83 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "84 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "85 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "86 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "87 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "88 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "89 Train Loss 0.0036745458 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "90 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "91 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "93 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "94 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "95 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "96 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "97 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "98 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "99 Train Loss 0.0036745456 Test MSE 3.177721073552541e-05 Test RE 0.0002873561322819831\n",
      "Training time: 37.32\n",
      "Training time: 37.32\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 3.8715718 Test MSE 385.00996808157356 Test RE 1.0002269901811343\n",
      "1 Train Loss 2.6707232 Test MSE 381.67475359877716 Test RE 0.9958852485522263\n",
      "2 Train Loss 2.3899648 Test MSE 384.01707689563204 Test RE 0.9989364292193684\n",
      "3 Train Loss 2.3844354 Test MSE 384.0788467163293 Test RE 0.9990167663277009\n",
      "4 Train Loss 2.3833241 Test MSE 384.00050799562763 Test RE 0.9989148788022939\n",
      "5 Train Loss 2.3798003 Test MSE 383.34793055276793 Test RE 0.9980657308338893\n",
      "6 Train Loss 2.3794963 Test MSE 383.2396340903838 Test RE 0.9979247432165964\n",
      "7 Train Loss 2.3758688 Test MSE 381.6322921328072 Test RE 0.9958298506992608\n",
      "8 Train Loss 2.3479755 Test MSE 375.9027923416805 Test RE 0.9883263138035846\n",
      "9 Train Loss 2.3204534 Test MSE 369.7265073015969 Test RE 0.9801733191466182\n",
      "10 Train Loss 2.279922 Test MSE 359.05762611334137 Test RE 0.9659277880840712\n",
      "11 Train Loss 2.2350872 Test MSE 356.90350844219074 Test RE 0.9630259526751144\n",
      "12 Train Loss 2.1135685 Test MSE 329.1207273915511 Test RE 0.9247837707868083\n",
      "13 Train Loss 1.9002006 Test MSE 285.82295577509205 Test RE 0.8618092190511446\n",
      "14 Train Loss 1.6457206 Test MSE 253.01486813043738 Test RE 0.8108408194948397\n",
      "15 Train Loss 1.5551263 Test MSE 227.09291342545112 Test RE 0.7681824368033434\n",
      "16 Train Loss 1.2130361 Test MSE 175.55212775703498 Test RE 0.6754071150025865\n",
      "17 Train Loss 1.0781503 Test MSE 162.117439207196 Test RE 0.6490489475763586\n",
      "18 Train Loss 1.0343301 Test MSE 158.59075002344392 Test RE 0.6419504520886453\n",
      "19 Train Loss 0.9569984 Test MSE 138.08537030380734 Test RE 0.599013233488573\n",
      "20 Train Loss 0.805764 Test MSE 121.1521666146115 Test RE 0.5610843755634904\n",
      "21 Train Loss 0.6397002 Test MSE 84.78597742196808 Test RE 0.4693800188738666\n",
      "22 Train Loss 0.62854546 Test MSE 85.90539626666478 Test RE 0.4724684416495262\n",
      "23 Train Loss 0.5312525 Test MSE 74.00164110361382 Test RE 0.43851379282789893\n",
      "24 Train Loss 0.4280206 Test MSE 58.01541010118714 Test RE 0.388270384501663\n",
      "25 Train Loss 0.35658306 Test MSE 38.25166765987864 Test RE 0.3152737187763219\n",
      "26 Train Loss 0.27706173 Test MSE 29.54950312660842 Test RE 0.2771007325207347\n",
      "27 Train Loss 0.24689868 Test MSE 21.761645145234425 Test RE 0.23779813523364343\n",
      "28 Train Loss 0.20356667 Test MSE 11.3176487183868 Test RE 0.17149074519516427\n",
      "29 Train Loss 0.14093885 Test MSE 7.152576791857589 Test RE 0.13633075728734625\n",
      "30 Train Loss 0.10405001 Test MSE 4.352542659431382 Test RE 0.10634916899439117\n",
      "31 Train Loss 0.06295362 Test MSE 1.2952282817808114 Test RE 0.05801439157438012\n",
      "32 Train Loss 0.03176084 Test MSE 0.020638398862497288 Test RE 0.007323194742063243\n",
      "33 Train Loss 0.027086096 Test MSE 0.0015576103418819443 Test RE 0.0020118333056226697\n",
      "34 Train Loss 0.024694458 Test MSE 0.0020741182386700393 Test RE 0.002321556928459467\n",
      "35 Train Loss 0.020256523 Test MSE 0.16644274713302046 Test RE 0.020796728402377555\n",
      "36 Train Loss 0.017109733 Test MSE 0.3303679196652308 Test RE 0.029299588223573272\n",
      "37 Train Loss 0.013932897 Test MSE 0.21834560124827998 Test RE 0.023819618754563402\n",
      "38 Train Loss 0.010807143 Test MSE 0.04159333536531533 Test RE 0.010396195945985526\n",
      "39 Train Loss 0.009523045 Test MSE 4.455383162013242e-05 Test RE 0.00034025546719408857\n",
      "40 Train Loss 0.007881299 Test MSE 0.0102039142969639 Test RE 0.005149273784514669\n",
      "41 Train Loss 0.007005489 Test MSE 0.00010362805570022315 Test RE 0.0005189210132990148\n",
      "42 Train Loss 0.0065892385 Test MSE 0.005023059632470399 Test RE 0.00361282353636408\n",
      "43 Train Loss 0.0065449504 Test MSE 0.0035805680888780058 Test RE 0.003050271879026393\n",
      "44 Train Loss 0.0065374807 Test MSE 0.0030061655176932267 Test RE 0.0027949177189187935\n",
      "45 Train Loss 0.0065299305 Test MSE 0.0024380071824239642 Test RE 0.002516981786466953\n",
      "46 Train Loss 0.006519947 Test MSE 0.0019032865754692778 Test RE 0.002223897029373603\n",
      "47 Train Loss 0.006189523 Test MSE 0.0009288176921580425 Test RE 0.0015535592536991337\n",
      "48 Train Loss 0.0043369904 Test MSE 3.326860937268522e-07 Test RE 2.9402205347009075e-05\n",
      "49 Train Loss 0.003608652 Test MSE 0.00917004412590176 Test RE 0.004881443818233348\n",
      "50 Train Loss 0.003059139 Test MSE 0.020095086174488887 Test RE 0.007226159094967181\n",
      "51 Train Loss 0.0026520214 Test MSE 0.004933437081506099 Test RE 0.0035804480719199725\n",
      "52 Train Loss 0.0024073082 Test MSE 0.003424084249034787 Test RE 0.0029828732775554006\n",
      "53 Train Loss 0.0023723473 Test MSE 0.004386613477466697 Test RE 0.0033761930135382547\n",
      "54 Train Loss 0.0023013959 Test MSE 0.011958243781524745 Test RE 0.0055743763172474235\n",
      "55 Train Loss 0.0022952943 Test MSE 0.012490075402860821 Test RE 0.005696985481029796\n",
      "56 Train Loss 0.0022225985 Test MSE 0.019145088794749975 Test RE 0.007053282440335337\n",
      "57 Train Loss 0.0022129118 Test MSE 0.019440545297554568 Test RE 0.007107498943088738\n",
      "58 Train Loss 0.0021959743 Test MSE 0.01827384663592981 Test RE 0.006890925750120087\n",
      "59 Train Loss 0.0021935524 Test MSE 0.017934076563888263 Test RE 0.006826562838879009\n",
      "60 Train Loss 0.0021887333 Test MSE 0.01702985236782523 Test RE 0.006652241757304985\n",
      "61 Train Loss 0.0021887333 Test MSE 0.01702985236782523 Test RE 0.006652241757304985\n",
      "62 Train Loss 0.0021879515 Test MSE 0.01687933393863861 Test RE 0.006622778575373608\n",
      "63 Train Loss 0.0021828602 Test MSE 0.01555011362406248 Test RE 0.006356665606622185\n",
      "64 Train Loss 0.002180128 Test MSE 0.014789114564169916 Test RE 0.006199171755124798\n",
      "65 Train Loss 0.0021789547 Test MSE 0.014566236985674448 Test RE 0.00615228248723825\n",
      "66 Train Loss 0.0021789547 Test MSE 0.014566236985674448 Test RE 0.00615228248723825\n",
      "67 Train Loss 0.0021789547 Test MSE 0.014566236985674448 Test RE 0.00615228248723825\n",
      "68 Train Loss 0.0021788504 Test MSE 0.014566332173404209 Test RE 0.006152302589233247\n",
      "69 Train Loss 0.0021756627 Test MSE 0.01385691727569077 Test RE 0.00600061681532192\n",
      "70 Train Loss 0.0021727823 Test MSE 0.01327588923852497 Test RE 0.005873465102785435\n",
      "71 Train Loss 0.0021713953 Test MSE 0.013119428669084053 Test RE 0.005838752198551833\n",
      "72 Train Loss 0.0021692314 Test MSE 0.013016453325088417 Test RE 0.005815792662996101\n",
      "73 Train Loss 0.0021610318 Test MSE 0.01240434858233198 Test RE 0.005677400917178171\n",
      "74 Train Loss 0.0021537468 Test MSE 0.012105454755230272 Test RE 0.005608582815120263\n",
      "75 Train Loss 0.0021443274 Test MSE 0.011984873644073017 Test RE 0.005580579666443002\n",
      "76 Train Loss 0.0021375578 Test MSE 0.012085073135203182 Test RE 0.005603859318040622\n",
      "77 Train Loss 0.0021321203 Test MSE 0.012457090291169017 Test RE 0.005689457907042354\n",
      "78 Train Loss 0.002130582 Test MSE 0.01258813145663137 Test RE 0.005719304473501689\n",
      "79 Train Loss 0.0021230509 Test MSE 0.013592641989773106 Test RE 0.00594312030582328\n",
      "80 Train Loss 0.0021219486 Test MSE 0.013759882127372572 Test RE 0.005979569793264244\n",
      "81 Train Loss 0.0021156278 Test MSE 0.015008329937123638 Test RE 0.006244947141621352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 Train Loss 0.0021151747 Test MSE 0.015241962893968149 Test RE 0.006293366623383254\n",
      "83 Train Loss 0.0021118089 Test MSE 0.016197362005483388 Test RE 0.00648760988484637\n",
      "84 Train Loss 0.0021101297 Test MSE 0.016422260622358534 Test RE 0.006532494497701262\n",
      "85 Train Loss 0.0020966444 Test MSE 0.02111766532050515 Test RE 0.00740773664012248\n",
      "86 Train Loss 0.0020935407 Test MSE 0.021785136002244614 Test RE 0.00752389489932145\n",
      "87 Train Loss 0.0020641575 Test MSE 0.030413752394873986 Test RE 0.008889914528534144\n",
      "88 Train Loss 0.0017808839 Test MSE 0.045043976207407646 Test RE 0.0108188460952481\n",
      "89 Train Loss 0.001399251 Test MSE 0.011305534907696637 Test RE 0.005420110494572929\n",
      "90 Train Loss 0.0009459902 Test MSE 0.0006958143994999097 Test RE 0.0013446501000930157\n",
      "91 Train Loss 0.0007300763 Test MSE 0.002653228442877915 Test RE 0.002625729023676897\n",
      "92 Train Loss 0.00072229997 Test MSE 0.00249754450234307 Test RE 0.002547529373688065\n",
      "93 Train Loss 0.0006974945 Test MSE 0.002165614739453602 Test RE 0.0023722102704600706\n",
      "94 Train Loss 0.00066435477 Test MSE 0.0015287894036392013 Test RE 0.0019931336182387053\n",
      "95 Train Loss 0.00065563753 Test MSE 0.0013648246522609251 Test RE 0.001883219800266121\n",
      "96 Train Loss 0.0006473637 Test MSE 0.0012245377044337261 Test RE 0.0017838102844284552\n",
      "97 Train Loss 0.00063829473 Test MSE 0.0010767337550087477 Test RE 0.0016726949533556726\n",
      "98 Train Loss 0.00062905415 Test MSE 0.0009646781409267681 Test RE 0.001583265693195834\n",
      "99 Train Loss 0.00062288647 Test MSE 0.0008807506248150756 Test RE 0.001512826287912957\n",
      "Training time: 45.20\n",
      "Training time: 45.20\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 3.8136115 Test MSE 385.6369128819573 Test RE 1.0010410366803841\n",
      "1 Train Loss 2.4210725 Test MSE 383.4570099014431 Test RE 0.9982077175355517\n",
      "2 Train Loss 2.3878238 Test MSE 382.96506011372446 Test RE 0.9975671950453209\n",
      "3 Train Loss 2.3825078 Test MSE 382.9502283273505 Test RE 0.9975478775561694\n",
      "4 Train Loss 2.3701441 Test MSE 381.2979791407229 Test RE 0.9953935776090144\n",
      "5 Train Loss 2.365741 Test MSE 380.38781757144403 Test RE 0.9942048612829352\n",
      "6 Train Loss 2.3577967 Test MSE 376.74947977949756 Test RE 0.9894387460800579\n",
      "7 Train Loss 2.3207502 Test MSE 371.0186554730545 Test RE 0.9818846171953862\n",
      "8 Train Loss 2.238808 Test MSE 356.57420412918543 Test RE 0.962581572363194\n",
      "9 Train Loss 2.1512976 Test MSE 342.1134872013878 Test RE 0.9428610178938281\n",
      "10 Train Loss 2.097694 Test MSE 330.79719196700074 Test RE 0.9271360959557975\n",
      "11 Train Loss 1.9828191 Test MSE 308.1462764034159 Test RE 0.8948310483650399\n",
      "12 Train Loss 1.8571296 Test MSE 286.4466586619648 Test RE 0.8627489965877723\n",
      "13 Train Loss 1.7968326 Test MSE 278.79269838764924 Test RE 0.8511444668327436\n",
      "14 Train Loss 1.7152302 Test MSE 267.2391495181789 Test RE 0.8333215692278406\n",
      "15 Train Loss 1.6517683 Test MSE 262.2134233792766 Test RE 0.8254486143179901\n",
      "16 Train Loss 1.5957224 Test MSE 247.42153673859116 Test RE 0.8018282117440073\n",
      "17 Train Loss 1.5544168 Test MSE 225.92245034809986 Test RE 0.7662002289163534\n",
      "18 Train Loss 1.2987779 Test MSE 191.92701638861027 Test RE 0.70620475681481\n",
      "19 Train Loss 1.141659 Test MSE 167.0547723152871 Test RE 0.6588583069429611\n",
      "20 Train Loss 1.0281134 Test MSE 144.47176250190475 Test RE 0.6127087296640387\n",
      "21 Train Loss 0.90587366 Test MSE 129.67884923960227 Test RE 0.5804932264758468\n",
      "22 Train Loss 0.80417764 Test MSE 117.37085973349366 Test RE 0.5522589023986348\n",
      "23 Train Loss 0.7512208 Test MSE 104.14635848120035 Test RE 0.5202171041003487\n",
      "24 Train Loss 0.6717939 Test MSE 96.01733734961881 Test RE 0.4995022073235858\n",
      "25 Train Loss 0.6512652 Test MSE 89.45399157298952 Test RE 0.4821281003506462\n",
      "26 Train Loss 0.5636041 Test MSE 65.3337428448538 Test RE 0.4120323848050792\n",
      "27 Train Loss 0.4247045 Test MSE 51.87200368618207 Test RE 0.3671377944034388\n",
      "28 Train Loss 0.3212765 Test MSE 36.414561055767706 Test RE 0.3076097680131258\n",
      "29 Train Loss 0.28855437 Test MSE 32.36700198833641 Test RE 0.2900105652467527\n",
      "30 Train Loss 0.2527326 Test MSE 31.13656304813766 Test RE 0.2844447474932202\n",
      "31 Train Loss 0.22851494 Test MSE 28.991398010891896 Test RE 0.27447144051689487\n",
      "32 Train Loss 0.15512128 Test MSE 21.574526101365272 Test RE 0.23677356598435337\n",
      "33 Train Loss 0.14527002 Test MSE 19.72324182292761 Test RE 0.2263871292298503\n",
      "34 Train Loss 0.13605167 Test MSE 17.245582446317506 Test RE 0.21169057173411807\n",
      "35 Train Loss 0.12958772 Test MSE 15.997729241822876 Test RE 0.20388804124431814\n",
      "36 Train Loss 0.11809281 Test MSE 13.687736744302006 Test RE 0.1885942313000585\n",
      "37 Train Loss 0.099341676 Test MSE 9.174599691230918 Test RE 0.15440314586720819\n",
      "38 Train Loss 0.09244574 Test MSE 7.153867659459455 Test RE 0.1363430589391354\n",
      "39 Train Loss 0.07320244 Test MSE 4.1758548525622725 Test RE 0.10416822882252631\n",
      "40 Train Loss 0.03149984 Test MSE 2.092069412201102 Test RE 0.07373108543206539\n",
      "41 Train Loss 0.028634898 Test MSE 1.9647757890819022 Test RE 0.07145277194033925\n",
      "42 Train Loss 0.02622674 Test MSE 1.555810420146868 Test RE 0.06358298614145612\n",
      "43 Train Loss 0.018268693 Test MSE 0.5708859587536737 Test RE 0.038515652813225276\n",
      "44 Train Loss 0.012053985 Test MSE 0.1990237708617121 Test RE 0.02274128797625268\n",
      "45 Train Loss 0.009841785 Test MSE 0.07629039709554057 Test RE 0.014079833536401489\n",
      "46 Train Loss 0.009217473 Test MSE 0.04322215835747253 Test RE 0.010597802154691754\n",
      "47 Train Loss 0.0074968976 Test MSE 0.03729894015504422 Test RE 0.009844889100362438\n",
      "48 Train Loss 0.0063791266 Test MSE 0.055590101410402985 Test RE 0.01201880573866943\n",
      "49 Train Loss 0.0052765487 Test MSE 0.027455334324077083 Test RE 0.00844648374816797\n",
      "50 Train Loss 0.0048671905 Test MSE 0.012588798517173793 Test RE 0.005719456007977512\n",
      "51 Train Loss 0.0048608007 Test MSE 0.01263528477711457 Test RE 0.005730006304830192\n",
      "52 Train Loss 0.0048594247 Test MSE 0.01267523715466622 Test RE 0.005739058205896441\n",
      "53 Train Loss 0.0048578475 Test MSE 0.012736910841472581 Test RE 0.005753003482326433\n",
      "54 Train Loss 0.0048489794 Test MSE 0.013355380940774497 Test RE 0.005891023057526168\n",
      "55 Train Loss 0.004847544 Test MSE 0.013410269988329566 Test RE 0.005903116351300776\n",
      "56 Train Loss 0.0048471703 Test MSE 0.013473107821308344 Test RE 0.005916930596587995\n",
      "57 Train Loss 0.004845412 Test MSE 0.013676755572319801 Test RE 0.005961480465764482\n",
      "58 Train Loss 0.0048431475 Test MSE 0.013844351320282158 Test RE 0.005997895409679015\n",
      "59 Train Loss 0.004834802 Test MSE 0.014795591236124036 Test RE 0.006200529023907179\n",
      "60 Train Loss 0.004815078 Test MSE 0.01708074726844975 Test RE 0.006662174685575126\n",
      "61 Train Loss 0.0046219947 Test MSE 0.04114014807505393 Test RE 0.010339404061493515\n",
      "62 Train Loss 0.0037043504 Test MSE 0.14028000607159305 Test RE 0.01909239762387622\n",
      "63 Train Loss 0.0023440155 Test MSE 0.10379583782185789 Test RE 0.016423002232836305\n",
      "64 Train Loss 0.0019827494 Test MSE 0.05001763772787093 Test RE 0.011400507138772142\n",
      "65 Train Loss 0.0019503711 Test MSE 0.045289240409827666 Test RE 0.010848260388147084\n",
      "66 Train Loss 0.001872744 Test MSE 0.033753735434916006 Test RE 0.009365339027139104\n",
      "67 Train Loss 0.001863716 Test MSE 0.03235523334255521 Test RE 0.009169271888028569\n",
      "68 Train Loss 0.0018608386 Test MSE 0.03206570360671266 Test RE 0.009128154233159165\n",
      "69 Train Loss 0.0018315435 Test MSE 0.028290828864820358 Test RE 0.008574038283635448\n",
      "70 Train Loss 0.0018220238 Test MSE 0.026908771427284844 Test RE 0.008361987564485411\n",
      "71 Train Loss 0.0017530654 Test MSE 0.014315740363101091 Test RE 0.006099152454736858\n",
      "72 Train Loss 0.0017373967 Test MSE 0.010699317279038978 Test RE 0.005272791708465841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 Train Loss 0.0017348675 Test MSE 0.010318758711716579 Test RE 0.005178170082079465\n",
      "74 Train Loss 0.0017275182 Test MSE 0.009050265733401642 Test RE 0.004849458510616591\n",
      "75 Train Loss 0.0017209906 Test MSE 0.007953697096407426 Test RE 0.00454618501694931\n",
      "76 Train Loss 0.0017151776 Test MSE 0.007052058059132141 Test RE 0.004280756473535283\n",
      "77 Train Loss 0.0017076759 Test MSE 0.006186679190343591 Test RE 0.004009510735878244\n",
      "78 Train Loss 0.0016996835 Test MSE 0.005309905758855689 Test RE 0.0037145481230884277\n",
      "79 Train Loss 0.0016932914 Test MSE 0.004808568573443891 Test RE 0.003534845930393575\n",
      "80 Train Loss 0.0014077824 Test MSE 0.00013529166807864295 Test RE 0.0005929227418570655\n",
      "81 Train Loss 0.00048756608 Test MSE 0.0013564104260112408 Test RE 0.0018774057436172758\n",
      "82 Train Loss 0.0004055222 Test MSE 0.0001654723089264442 Test RE 0.0006557302908758978\n",
      "83 Train Loss 0.0004011307 Test MSE 0.00012667928919588543 Test RE 0.00057374034662862\n",
      "84 Train Loss 0.00039225936 Test MSE 5.8961208132982786e-05 Test RE 0.00039142248465905437\n",
      "85 Train Loss 0.00038849935 Test MSE 4.1424149823388655e-05 Test RE 0.0003280872735153036\n",
      "86 Train Loss 0.00038012135 Test MSE 4.878324667462308e-06 Test RE 0.00011258951097655023\n",
      "87 Train Loss 0.00037382016 Test MSE 4.1450494511330255e-08 Test RE 1.0378329157410087e-05\n",
      "88 Train Loss 0.00036853284 Test MSE 1.5235732271947103e-05 Test RE 0.00019897304614918345\n",
      "89 Train Loss 0.00036464352 Test MSE 2.6135839359191895e-05 Test RE 0.00026060383870433047\n",
      "90 Train Loss 0.00036055467 Test MSE 5.152494975964047e-05 Test RE 0.00036590755031358757\n",
      "91 Train Loss 0.00035684707 Test MSE 7.257789916101675e-05 Test RE 0.0004342749500532303\n",
      "92 Train Loss 0.00035586036 Test MSE 8.333925008862774e-05 Test RE 0.00046535820610311176\n",
      "93 Train Loss 0.00035185833 Test MSE 0.0001099188447829683 Test RE 0.0005344396368741342\n",
      "94 Train Loss 0.00035098533 Test MSE 0.0001156347597384277 Test RE 0.0005481592962407553\n",
      "95 Train Loss 0.00034322886 Test MSE 0.00018852605770881676 Test RE 0.000699919793753355\n",
      "96 Train Loss 0.00034211163 Test MSE 0.00019672052707456127 Test RE 0.0007149693463069936\n",
      "97 Train Loss 0.0003402508 Test MSE 0.0002057039110582542 Test RE 0.0007311119071662299\n",
      "98 Train Loss 0.00033399524 Test MSE 0.00023301323184445443 Test RE 0.0007781312808228726\n",
      "99 Train Loss 0.00033135482 Test MSE 0.00022954012112493086 Test RE 0.0007723104046978996\n",
      "Training time: 45.89\n",
      "Training time: 45.89\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 4.623303 Test MSE 382.06787805417275 Test RE 0.9963979967511758\n",
      "1 Train Loss 2.8492115 Test MSE 378.74016347517085 Test RE 0.9920493198105389\n",
      "2 Train Loss 2.3886578 Test MSE 381.34750979844574 Test RE 0.9954582263880437\n",
      "3 Train Loss 2.3622234 Test MSE 379.74658400699906 Test RE 0.9933665242018884\n",
      "4 Train Loss 2.347236 Test MSE 375.20610948895546 Test RE 0.9874100272817251\n",
      "5 Train Loss 2.3343453 Test MSE 373.2074480615479 Test RE 0.9847766300191406\n",
      "6 Train Loss 2.3103595 Test MSE 367.4933505593715 Test RE 0.9772087009015953\n",
      "7 Train Loss 2.2855592 Test MSE 366.8252099488025 Test RE 0.9763199639315494\n",
      "8 Train Loss 2.2444637 Test MSE 357.6567064095666 Test RE 0.9640415869959718\n",
      "9 Train Loss 2.211029 Test MSE 349.69505880785664 Test RE 0.9532511336185717\n",
      "10 Train Loss 2.1075478 Test MSE 333.05503363606454 Test RE 0.9302947785603329\n",
      "11 Train Loss 1.9692762 Test MSE 311.5379314840683 Test RE 0.8997421132968\n",
      "12 Train Loss 1.8889757 Test MSE 293.257743688571 Test RE 0.8729458947439205\n",
      "13 Train Loss 1.7461363 Test MSE 274.9379632092213 Test RE 0.845239798928564\n",
      "14 Train Loss 1.6791643 Test MSE 258.96993095090306 Test RE 0.8203274665499362\n",
      "15 Train Loss 1.4952896 Test MSE 219.72073486048902 Test RE 0.7556107082433596\n",
      "16 Train Loss 1.3475004 Test MSE 196.93069364680792 Test RE 0.7153511634643532\n",
      "17 Train Loss 1.1335258 Test MSE 168.86852425719107 Test RE 0.6624253388650361\n",
      "18 Train Loss 1.1028308 Test MSE 161.39520735755562 Test RE 0.647601579929989\n",
      "19 Train Loss 1.0127285 Test MSE 149.7869538820048 Test RE 0.6238778645931851\n",
      "20 Train Loss 0.8408086 Test MSE 111.04613427849335 Test RE 0.5371731598843369\n",
      "21 Train Loss 0.6537404 Test MSE 83.02407190305786 Test RE 0.46447741049871805\n",
      "22 Train Loss 0.49909452 Test MSE 65.07068356790285 Test RE 0.4112020461408319\n",
      "23 Train Loss 0.4689749 Test MSE 63.559332368997545 Test RE 0.4063986399586563\n",
      "24 Train Loss 0.45296764 Test MSE 59.057070392734516 Test RE 0.3917405531475941\n",
      "25 Train Loss 0.38792226 Test MSE 41.95640775494056 Test RE 0.33018833877169346\n",
      "26 Train Loss 0.35171318 Test MSE 39.12499774883941 Test RE 0.3188524404133161\n",
      "27 Train Loss 0.27260417 Test MSE 34.32865957511379 Test RE 0.29866958981191044\n",
      "28 Train Loss 0.19822803 Test MSE 20.392811086434165 Test RE 0.2301977797524633\n",
      "29 Train Loss 0.1594066 Test MSE 10.844937883073378 Test RE 0.1678711698588946\n",
      "30 Train Loss 0.13991591 Test MSE 7.054605663882854 Test RE 0.13539385500794127\n",
      "31 Train Loss 0.12211974 Test MSE 6.618784703575475 Test RE 0.131144991653283\n",
      "32 Train Loss 0.10878289 Test MSE 5.387455136378032 Test RE 0.11831897967021643\n",
      "33 Train Loss 0.081717655 Test MSE 2.529214797381798 Test RE 0.08106911640422668\n",
      "34 Train Loss 0.03606086 Test MSE 2.1486078748456974 Test RE 0.07472073996494483\n",
      "35 Train Loss 0.027262826 Test MSE 2.6800360986660245 Test RE 0.08345126117285992\n",
      "36 Train Loss 0.023912933 Test MSE 1.9625827784455796 Test RE 0.07141288432595512\n",
      "37 Train Loss 0.018218137 Test MSE 0.5950343876952909 Test RE 0.03932182037171307\n",
      "38 Train Loss 0.01552957 Test MSE 0.3224974537211858 Test RE 0.028948477494146124\n",
      "39 Train Loss 0.014050535 Test MSE 0.2649211792347765 Test RE 0.02623740760944757\n",
      "40 Train Loss 0.013082093 Test MSE 0.3591001811749878 Test RE 0.0305471286988309\n",
      "41 Train Loss 0.010184838 Test MSE 0.23303663501003644 Test RE 0.024607907340182814\n",
      "42 Train Loss 0.008385675 Test MSE 0.04074695311532146 Test RE 0.010289876262348906\n",
      "43 Train Loss 0.007993784 Test MSE 0.012348076438143632 Test RE 0.00566450855634096\n",
      "44 Train Loss 0.007815315 Test MSE 0.01799035081899871 Test RE 0.0068372647797157425\n",
      "45 Train Loss 0.0075955335 Test MSE 0.04688592514018263 Test RE 0.011037833214379896\n",
      "46 Train Loss 0.006868604 Test MSE 0.054230844184276136 Test RE 0.011870957905998077\n",
      "47 Train Loss 0.005858274 Test MSE 0.01323028816969991 Test RE 0.005863369105684253\n",
      "48 Train Loss 0.005581201 Test MSE 0.01602483461314315 Test RE 0.0064529657573994675\n",
      "49 Train Loss 0.0055323616 Test MSE 0.01733924720730671 Test RE 0.00671239803737574\n",
      "50 Train Loss 0.0048833373 Test MSE 0.004393015555017448 Test RE 0.0033786558210026118\n",
      "51 Train Loss 0.0038217744 Test MSE 5.940053201765671e-05 Test RE 0.00039287803582667674\n",
      "52 Train Loss 0.003038316 Test MSE 0.0004418095241873777 Test RE 0.001071470255761677\n",
      "53 Train Loss 0.0027733527 Test MSE 0.0010115693986599151 Test RE 0.0016212889545482471\n",
      "54 Train Loss 0.0027679745 Test MSE 0.001081992446672679 Test RE 0.0016767746402441006\n",
      "55 Train Loss 0.002762825 Test MSE 0.00126954320053723 Test RE 0.0018162947366968608\n",
      "56 Train Loss 0.0027591246 Test MSE 0.0014618340772183992 Test RE 0.0019489990316548039\n",
      "57 Train Loss 0.002757902 Test MSE 0.001506772762465451 Test RE 0.0019787296571598463\n",
      "58 Train Loss 0.002754428 Test MSE 0.0017496617853493485 Test RE 0.0021322574267063042\n",
      "59 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "60 Train Loss 0.0027492058 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "61 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "62 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "63 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "64 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Train Loss 0.0027492058 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "66 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "67 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "68 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "69 Train Loss 0.0027492058 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "70 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "71 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "72 Train Loss 0.0027492063 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "73 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "74 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "75 Train Loss 0.0027492058 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "76 Train Loss 0.0027492058 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "77 Train Loss 0.0027492058 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "78 Train Loss 0.0027492063 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "79 Train Loss 0.0027492058 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "80 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "81 Train Loss 0.0027492063 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "82 Train Loss 0.0027492058 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "83 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "84 Train Loss 0.0027492058 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "85 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "86 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "87 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "88 Train Loss 0.0027492058 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "89 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "90 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "91 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "92 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "93 Train Loss 0.0027492063 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "94 Train Loss 0.0027492063 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "95 Train Loss 0.0027492063 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "96 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "97 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "98 Train Loss 0.002749206 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "99 Train Loss 0.0027492058 Test MSE 0.0022870868771376228 Test RE 0.002437832786160767\n",
      "Training time: 38.49\n",
      "Training time: 38.49\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 3.770248 Test MSE 383.1239353696976 Test RE 0.997774096830739\n",
      "1 Train Loss 2.4887984 Test MSE 385.20345781862954 Test RE 1.0004782945161097\n",
      "2 Train Loss 2.3821566 Test MSE 382.294828283158 Test RE 0.9966938855126809\n",
      "3 Train Loss 2.3794088 Test MSE 382.0104698421778 Test RE 0.9963231362622437\n",
      "4 Train Loss 2.3687794 Test MSE 380.76769029017714 Test RE 0.9947011666892501\n",
      "5 Train Loss 2.3551264 Test MSE 377.5768666126338 Test RE 0.9905246130353783\n",
      "6 Train Loss 2.3475454 Test MSE 376.9813893877766 Test RE 0.9897432256664858\n",
      "7 Train Loss 2.3313708 Test MSE 374.51971268520157 Test RE 0.9865064367986012\n",
      "8 Train Loss 2.32174 Test MSE 371.24036282264717 Test RE 0.9821779427755304\n",
      "9 Train Loss 2.3158753 Test MSE 369.6275121958165 Test RE 0.9800420885538371\n",
      "10 Train Loss 2.2547374 Test MSE 356.0306926457948 Test RE 0.9618476807250127\n",
      "11 Train Loss 2.2240849 Test MSE 351.34762793550937 Test RE 0.9555008889520039\n",
      "12 Train Loss 2.167844 Test MSE 340.888829954495 Test RE 0.9411719339738756\n",
      "13 Train Loss 2.1322978 Test MSE 340.51615569574614 Test RE 0.9406573285683362\n",
      "14 Train Loss 2.0493522 Test MSE 328.0630230169666 Test RE 0.9232965733863504\n",
      "15 Train Loss 1.9678113 Test MSE 306.4505879520056 Test RE 0.8923655830890167\n",
      "16 Train Loss 1.7932495 Test MSE 271.52374474123394 Test RE 0.8399752502777447\n",
      "17 Train Loss 1.6751145 Test MSE 259.9384416178927 Test RE 0.8218599889923487\n",
      "18 Train Loss 1.6145978 Test MSE 243.6721889287217 Test RE 0.7957296940793573\n",
      "19 Train Loss 1.5210196 Test MSE 234.39607440863637 Test RE 0.7804368179543772\n",
      "20 Train Loss 1.4101051 Test MSE 217.92705968943358 Test RE 0.7525201998962834\n",
      "21 Train Loss 1.3496741 Test MSE 210.12901891030012 Test RE 0.738933913323875\n",
      "22 Train Loss 1.2437615 Test MSE 184.99983238503887 Test RE 0.693343183150669\n",
      "23 Train Loss 1.0978271 Test MSE 157.91822063158793 Test RE 0.6405878592964316\n",
      "24 Train Loss 0.84356666 Test MSE 121.58801904173107 Test RE 0.5620927373722377\n",
      "25 Train Loss 0.741064 Test MSE 113.9411424636845 Test RE 0.5441302461743325\n",
      "26 Train Loss 0.6798764 Test MSE 101.43024680813836 Test RE 0.5133887216326701\n",
      "27 Train Loss 0.6454534 Test MSE 91.78371252784153 Test RE 0.48836596842466695\n",
      "28 Train Loss 0.52379334 Test MSE 73.65812409923005 Test RE 0.4374948143022089\n",
      "29 Train Loss 0.4685092 Test MSE 60.57771753774715 Test RE 0.3967519184861424\n",
      "30 Train Loss 0.37953728 Test MSE 51.107336683606654 Test RE 0.3644216811495296\n",
      "31 Train Loss 0.32358754 Test MSE 43.84301578692862 Test RE 0.33753032103641967\n",
      "32 Train Loss 0.28560027 Test MSE 37.76574893124562 Test RE 0.3132648253943114\n",
      "33 Train Loss 0.2576967 Test MSE 31.67026844373032 Test RE 0.2868721939314826\n",
      "34 Train Loss 0.18634032 Test MSE 22.708230741452 Test RE 0.2429149432082414\n",
      "35 Train Loss 0.13699003 Test MSE 13.432791628483258 Test RE 0.18682961614331303\n",
      "36 Train Loss 0.09422067 Test MSE 6.503293455800552 Test RE 0.12999578123777783\n",
      "37 Train Loss 0.025641283 Test MSE 1.7042611921088187 Test RE 0.06654733374325608\n",
      "38 Train Loss 0.020302853 Test MSE 1.215706072540445 Test RE 0.056205248920515184\n",
      "39 Train Loss 0.01172824 Test MSE 0.14908232328081594 Test RE 0.019682291457263513\n",
      "40 Train Loss 0.0057981396 Test MSE 0.14724849765154624 Test RE 0.0195608633320671\n",
      "41 Train Loss 0.0032597643 Test MSE 0.08298317998482573 Test RE 0.014684447780428258\n",
      "42 Train Loss 0.0019475068 Test MSE 0.0002968088331362879 Test RE 0.000878215288529033\n",
      "43 Train Loss 0.0013006091 Test MSE 0.0006359679061125348 Test RE 0.0012855239801360476\n",
      "44 Train Loss 0.0012828194 Test MSE 0.00039706872501438033 Test RE 0.001015770096087921\n",
      "45 Train Loss 0.0012760344 Test MSE 0.00030656976983909945 Test RE 0.000892539091445387\n",
      "46 Train Loss 0.001251834 Test MSE 9.515538117667323e-05 Test RE 0.0004972551150081209\n",
      "47 Train Loss 0.0010299789 Test MSE 7.324914443298514e-07 Test RE 4.362785496655341e-05\n",
      "48 Train Loss 0.00096921594 Test MSE 4.2324173921158843e-05 Test RE 0.0003316323036457807\n",
      "49 Train Loss 0.00096197653 Test MSE 6.227191373295941e-05 Test RE 0.0004022617038808932\n",
      "50 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "51 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "52 Train Loss 0.0009585166 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "53 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "54 Train Loss 0.0009585166 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "55 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "57 Train Loss 0.0009585166 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "58 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "59 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "60 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "61 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "62 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "63 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "64 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "65 Train Loss 0.0009585166 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "66 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "67 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "68 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "69 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "70 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "71 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "72 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "73 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "74 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "75 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "76 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "77 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "78 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "79 Train Loss 0.0009585166 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "80 Train Loss 0.0009585166 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "81 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "82 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "83 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "84 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "85 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "86 Train Loss 0.0009585166 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "87 Train Loss 0.0009585166 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "88 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "89 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "90 Train Loss 0.0009585166 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "91 Train Loss 0.0009585166 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "92 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "93 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "94 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "95 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "96 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "97 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "98 Train Loss 0.0009585166 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "99 Train Loss 0.00095851655 Test MSE 7.572407867696396e-05 Test RE 0.0004435877880657743\n",
      "Training time: 34.40\n",
      "Training time: 34.40\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 4.416309 Test MSE 376.58813579171874 Test RE 0.9892268584773534\n",
      "1 Train Loss 3.0944161 Test MSE 382.73120194495726 Test RE 0.9972625656061923\n",
      "2 Train Loss 2.4122791 Test MSE 385.479749852273 Test RE 1.000837033031542\n",
      "3 Train Loss 2.3963618 Test MSE 385.3485310121061 Test RE 1.0006666740790624\n",
      "4 Train Loss 2.3888137 Test MSE 384.1226144520278 Test RE 0.9990736862239266\n",
      "5 Train Loss 2.3786497 Test MSE 382.73835769486175 Test RE 0.997271888243225\n",
      "6 Train Loss 2.3711908 Test MSE 381.7244481964595 Test RE 0.9959500792824251\n",
      "7 Train Loss 2.3606308 Test MSE 379.80838123871706 Test RE 0.9934473475807329\n",
      "8 Train Loss 2.3232162 Test MSE 372.40146769385626 Test RE 0.983712691420072\n",
      "9 Train Loss 2.2772236 Test MSE 363.9132659180313 Test RE 0.9724371151016914\n",
      "10 Train Loss 2.239468 Test MSE 354.73950991354684 Test RE 0.9601019756806867\n",
      "11 Train Loss 2.1970453 Test MSE 341.05809171388546 Test RE 0.941405565342026\n",
      "12 Train Loss 2.0489368 Test MSE 323.99989721325056 Test RE 0.917561153609266\n",
      "13 Train Loss 2.0209014 Test MSE 322.56301838634613 Test RE 0.9155242869363032\n",
      "14 Train Loss 1.9373282 Test MSE 306.5236852375024 Test RE 0.892472004181102\n",
      "15 Train Loss 1.9063224 Test MSE 299.30363630388194 Test RE 0.8818984496627181\n",
      "16 Train Loss 1.8253771 Test MSE 284.8485795051489 Test RE 0.8603390024032574\n",
      "17 Train Loss 1.7328584 Test MSE 270.27058479393855 Test RE 0.838034645245482\n",
      "18 Train Loss 1.6131085 Test MSE 247.78516766047338 Test RE 0.8024172115653516\n",
      "19 Train Loss 1.4827667 Test MSE 224.02994395377146 Test RE 0.7629843283138548\n",
      "20 Train Loss 1.404967 Test MSE 210.83008311836716 Test RE 0.7401655584098356\n",
      "21 Train Loss 1.3614709 Test MSE 210.30760136643065 Test RE 0.7392478457078668\n",
      "22 Train Loss 1.320883 Test MSE 203.48723025331583 Test RE 0.7271619787331346\n",
      "23 Train Loss 1.2681246 Test MSE 195.0616445024694 Test RE 0.711948407918705\n",
      "24 Train Loss 1.1970716 Test MSE 180.57415182620792 Test RE 0.6849996890325225\n",
      "25 Train Loss 1.0758103 Test MSE 166.37255926237336 Test RE 0.6575116181742002\n",
      "26 Train Loss 1.0043209 Test MSE 150.24557678150356 Test RE 0.624832240077235\n",
      "27 Train Loss 0.9392825 Test MSE 137.7380507212904 Test RE 0.5982594243976171\n",
      "28 Train Loss 0.8699427 Test MSE 123.41361831185874 Test RE 0.5662968230574453\n",
      "29 Train Loss 0.7693919 Test MSE 102.53459708081895 Test RE 0.5161759872675239\n",
      "30 Train Loss 0.73844296 Test MSE 97.92479080298915 Test RE 0.5044392934849024\n",
      "31 Train Loss 0.68143916 Test MSE 85.12100276614906 Test RE 0.47030646431216516\n",
      "32 Train Loss 0.43976995 Test MSE 56.801818295519915 Test RE 0.3841879168170678\n",
      "33 Train Loss 0.3628187 Test MSE 44.199030215553194 Test RE 0.338897958562113\n",
      "34 Train Loss 0.32289225 Test MSE 40.42171970723169 Test RE 0.32409324233588865\n",
      "35 Train Loss 0.30097362 Test MSE 40.70137853388609 Test RE 0.3252124340768279\n",
      "36 Train Loss 0.27818912 Test MSE 36.692626383425846 Test RE 0.3087820043283488\n",
      "37 Train Loss 0.23257554 Test MSE 29.46021866896014 Test RE 0.27668178291091444\n",
      "38 Train Loss 0.22675735 Test MSE 28.85633096816918 Test RE 0.27383133124951214\n",
      "39 Train Loss 0.21448416 Test MSE 27.70452078167916 Test RE 0.2683106452638167\n",
      "40 Train Loss 0.17141828 Test MSE 22.781551339048903 Test RE 0.24330679044953238\n",
      "41 Train Loss 0.14136201 Test MSE 17.026750658570535 Test RE 0.21034319708699611\n",
      "42 Train Loss 0.11855523 Test MSE 11.167729950433522 Test RE 0.17035113611852357\n",
      "43 Train Loss 0.09149343 Test MSE 5.75233450565687 Test RE 0.12226007249604565\n",
      "44 Train Loss 0.05422737 Test MSE 2.148933683820733 Test RE 0.07472640497370853\n",
      "45 Train Loss 0.041219223 Test MSE 1.196334636931772 Test RE 0.05575565484798992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 Train Loss 0.031010913 Test MSE 0.50317982228174 Test RE 0.036159647757829996\n",
      "47 Train Loss 0.019519001 Test MSE 0.23661170702691078 Test RE 0.024795946857813972\n",
      "48 Train Loss 0.008444232 Test MSE 0.20031551788929997 Test RE 0.022814968822215714\n",
      "49 Train Loss 0.0074210307 Test MSE 0.21554217624999733 Test RE 0.02366621002107211\n",
      "50 Train Loss 0.0068432493 Test MSE 0.2461306413059627 Test RE 0.025289801709201414\n",
      "51 Train Loss 0.0064890874 Test MSE 0.26508868473177616 Test RE 0.026245701048888147\n",
      "52 Train Loss 0.0063210926 Test MSE 0.23721828256365285 Test RE 0.02482770984157497\n",
      "53 Train Loss 0.006291479 Test MSE 0.22456676313598006 Test RE 0.024156572971909263\n",
      "54 Train Loss 0.0061557326 Test MSE 0.15232394287252027 Test RE 0.01989512485305008\n",
      "55 Train Loss 0.005726125 Test MSE 0.07036878613478996 Test RE 0.013522363689171488\n",
      "56 Train Loss 0.004227804 Test MSE 0.013628889290572791 Test RE 0.005951039246269923\n",
      "57 Train Loss 0.0036531962 Test MSE 0.02284193400858213 Test RE 0.007704226082390317\n",
      "58 Train Loss 0.003498489 Test MSE 0.021535379191169577 Test RE 0.0074806415320061844\n",
      "59 Train Loss 0.0034952054 Test MSE 0.02118839834552276 Test RE 0.0074201322716794214\n",
      "60 Train Loss 0.003385155 Test MSE 0.008796029625517697 Test RE 0.004780858877050807\n",
      "61 Train Loss 0.0032288863 Test MSE 0.002810552467573098 Test RE 0.002702454751596742\n",
      "62 Train Loss 0.003000038 Test MSE 0.0007404693631888596 Test RE 0.0013871266949328225\n",
      "63 Train Loss 0.002991093 Test MSE 0.0006896800728222078 Test RE 0.0013387097348225498\n",
      "64 Train Loss 0.0028045806 Test MSE 0.00010317013214782505 Test RE 0.0005177732100025653\n",
      "65 Train Loss 0.002642522 Test MSE 0.0006796884711737489 Test RE 0.0013289772124165497\n",
      "66 Train Loss 0.002607781 Test MSE 0.0010044305214607404 Test RE 0.0016155579213849051\n",
      "67 Train Loss 0.0025979122 Test MSE 0.0010228907269338396 Test RE 0.0016303363184345155\n",
      "68 Train Loss 0.002517579 Test MSE 0.00014135521084926238 Test RE 0.000606064008472044\n",
      "69 Train Loss 0.0024992237 Test MSE 1.196779191755459e-06 Test RE 5.576601321358787e-05\n",
      "70 Train Loss 0.0024798224 Test MSE 0.00018701914869658092 Test RE 0.0006971169145735845\n",
      "71 Train Loss 0.0024762396 Test MSE 0.00030406816193387457 Test RE 0.0008888900747767114\n",
      "72 Train Loss 0.0024759762 Test MSE 0.0003194011298335378 Test RE 0.0009110260733540429\n",
      "73 Train Loss 0.002473799 Test MSE 0.00036372675810435797 Test RE 0.0009721878931288184\n",
      "74 Train Loss 0.0024735813 Test MSE 0.0003782769632921418 Test RE 0.0009914424947962362\n",
      "75 Train Loss 0.0024719436 Test MSE 0.0003865219722747476 Test RE 0.0010021891023614896\n",
      "76 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "77 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "78 Train Loss 0.0024631792 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "79 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "80 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "81 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "82 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "83 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "84 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "85 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "86 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "87 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "88 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "89 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "90 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "91 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "92 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "93 Train Loss 0.0024631792 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "94 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "95 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "96 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "97 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "98 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "99 Train Loss 0.0024631794 Test MSE 0.0004622994394325286 Test RE 0.0010960346040893474\n",
      "Training time: 46.71\n",
      "Training time: 46.71\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 4.137315 Test MSE 385.40992126078294 Test RE 1.0007463794944857\n",
      "1 Train Loss 2.524317 Test MSE 381.0863974705327 Test RE 0.9951173680935431\n",
      "2 Train Loss 2.381302 Test MSE 382.7772608488488 Test RE 0.9973225704328094\n",
      "3 Train Loss 2.375904 Test MSE 382.7135362970106 Test RE 0.9972395501156875\n",
      "4 Train Loss 2.3717322 Test MSE 381.00641191792573 Test RE 0.995012930897573\n",
      "5 Train Loss 2.3359916 Test MSE 373.62542881472075 Test RE 0.9853279353245389\n",
      "6 Train Loss 2.3038082 Test MSE 365.0235675342696 Test RE 0.9739194411308025\n",
      "7 Train Loss 2.2275782 Test MSE 356.5244708193703 Test RE 0.962514441824338\n",
      "8 Train Loss 2.174364 Test MSE 344.89622191153205 Test RE 0.9466878450148012\n",
      "9 Train Loss 2.129599 Test MSE 340.10455654124485 Test RE 0.9400886465476108\n",
      "10 Train Loss 2.0092344 Test MSE 312.1271006593851 Test RE 0.9005924911728985\n",
      "11 Train Loss 1.9386706 Test MSE 305.04180491484783 Test RE 0.8903120748140779\n",
      "12 Train Loss 1.8566326 Test MSE 286.8198192310194 Test RE 0.8633107750448008\n",
      "13 Train Loss 1.6377095 Test MSE 249.70422828980713 Test RE 0.8055185215270936\n",
      "14 Train Loss 1.5037146 Test MSE 231.6795238738227 Test RE 0.7759011731729427\n",
      "15 Train Loss 1.239629 Test MSE 157.33464291627664 Test RE 0.639403135941133\n",
      "16 Train Loss 1.0563304 Test MSE 145.83592291883946 Test RE 0.6155946541332423\n",
      "17 Train Loss 0.9527472 Test MSE 128.6912876214379 Test RE 0.5782786462241808\n",
      "18 Train Loss 0.7547063 Test MSE 98.63291180721622 Test RE 0.5062598775288883\n",
      "19 Train Loss 0.6759436 Test MSE 77.5610640044207 Test RE 0.4489360299150528\n",
      "20 Train Loss 0.57130396 Test MSE 59.35343631462338 Test RE 0.392722258297179\n",
      "21 Train Loss 0.4506266 Test MSE 39.35983447527222 Test RE 0.31980791957841553\n",
      "22 Train Loss 0.27717373 Test MSE 17.3691168538359 Test RE 0.21244741496530886\n",
      "23 Train Loss 0.15378498 Test MSE 9.373633362237099 Test RE 0.15606896987607558\n",
      "24 Train Loss 0.09761286 Test MSE 3.3800328321895416 Test RE 0.09371800648294733\n",
      "25 Train Loss 0.077448815 Test MSE 0.6913408190993404 Test RE 0.04238465792706285\n",
      "26 Train Loss 0.061277762 Test MSE 0.005197065250597576 Test RE 0.0036748673510774888\n",
      "27 Train Loss 0.055874433 Test MSE 0.10390135628086465 Test RE 0.016431347893536435\n",
      "28 Train Loss 0.044842843 Test MSE 0.02483191285694691 Test RE 0.008032813247214258\n",
      "29 Train Loss 0.038435586 Test MSE 0.16670255880682747 Test RE 0.020812953578859338\n",
      "30 Train Loss 0.024588631 Test MSE 0.9612215136201864 Test RE 0.04997747651342586\n",
      "31 Train Loss 0.021841519 Test MSE 0.6013519182252238 Test RE 0.039530010786814805\n",
      "32 Train Loss 0.019690746 Test MSE 0.17153766460034625 Test RE 0.021112629627352917\n",
      "33 Train Loss 0.016485794 Test MSE 0.12243870184906477 Test RE 0.017837005273324963\n",
      "34 Train Loss 0.015296131 Test MSE 0.18361940361932919 Test RE 0.021843481986359665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 Train Loss 0.015132345 Test MSE 0.23898916411741805 Test RE 0.024920209421064234\n",
      "36 Train Loss 0.015124432 Test MSE 0.24247635566524686 Test RE 0.02510136163275527\n",
      "37 Train Loss 0.01509163 Test MSE 0.2604350163377034 Test RE 0.026014307566743333\n",
      "38 Train Loss 0.015082947 Test MSE 0.26442727056374715 Test RE 0.026212938201590916\n",
      "39 Train Loss 0.015081329 Test MSE 0.26517637257469034 Test RE 0.026250041555960255\n",
      "40 Train Loss 0.015077793 Test MSE 0.26686116681109484 Test RE 0.026333299150484775\n",
      "41 Train Loss 0.015058844 Test MSE 0.2715173740417086 Test RE 0.026562038075480722\n",
      "42 Train Loss 0.01505522 Test MSE 0.27214577330325546 Test RE 0.026592757874902995\n",
      "43 Train Loss 0.015047659 Test MSE 0.2729172539104213 Test RE 0.02663042386128971\n",
      "44 Train Loss 0.01504368 Test MSE 0.27335799541843303 Test RE 0.026651918285280716\n",
      "45 Train Loss 0.0150083 Test MSE 0.27463103343474543 Test RE 0.026713905670749983\n",
      "46 Train Loss 0.014934859 Test MSE 0.26595852789761276 Test RE 0.026288726171751986\n",
      "47 Train Loss 0.013847068 Test MSE 0.13147267016819403 Test RE 0.01848333454486819\n",
      "48 Train Loss 0.010361498 Test MSE 0.1541918297874675 Test RE 0.020016736107073194\n",
      "49 Train Loss 0.00974797 Test MSE 0.20295913476406482 Test RE 0.022965022958058783\n",
      "50 Train Loss 0.008978506 Test MSE 0.1834840968604752 Test RE 0.021835432413785796\n",
      "51 Train Loss 0.007837655 Test MSE 0.06976674110517948 Test RE 0.013464393671062513\n",
      "52 Train Loss 0.007642989 Test MSE 0.03280666178655703 Test RE 0.0092330163202154\n",
      "53 Train Loss 0.0074679866 Test MSE 0.018714515999571017 Test RE 0.006973517296085155\n",
      "54 Train Loss 0.00738632 Test MSE 0.020189736727448483 Test RE 0.007243157192252194\n",
      "55 Train Loss 0.007277241 Test MSE 0.028338881042157264 Test RE 0.008581316727326961\n",
      "56 Train Loss 0.0072519216 Test MSE 0.02917717022292128 Test RE 0.00870731321756837\n",
      "57 Train Loss 0.0072423667 Test MSE 0.028804954840835676 Test RE 0.008651595018015695\n",
      "58 Train Loss 0.007240432 Test MSE 0.02733796098272838 Test RE 0.00842840977671789\n",
      "59 Train Loss 0.007238772 Test MSE 0.027277052970857 Test RE 0.008419015441534455\n",
      "60 Train Loss 0.0072315624 Test MSE 0.026599634085645567 Test RE 0.008313816098879224\n",
      "61 Train Loss 0.0072261738 Test MSE 0.025866131175664185 Test RE 0.008198385212266804\n",
      "62 Train Loss 0.007217174 Test MSE 0.024508627139746948 Test RE 0.007980352498556425\n",
      "63 Train Loss 0.00721098 Test MSE 0.023013601162386293 Test RE 0.007733122211497411\n",
      "64 Train Loss 0.0072088097 Test MSE 0.022681607027840344 Test RE 0.007677140582582599\n",
      "65 Train Loss 0.0072039454 Test MSE 0.021706291408312726 Test RE 0.007510267347158454\n",
      "66 Train Loss 0.007181564 Test MSE 0.017456597394506824 Test RE 0.006735074133932304\n",
      "67 Train Loss 0.0070522153 Test MSE 0.008723395930426573 Test RE 0.00476107885709652\n",
      "68 Train Loss 0.00681005 Test MSE 0.013425691017268404 Test RE 0.005906509495987193\n",
      "69 Train Loss 0.006202382 Test MSE 0.012107016044112429 Test RE 0.0056089444841300245\n",
      "70 Train Loss 0.005489165 Test MSE 0.0023281985405770615 Test RE 0.0024596458996947166\n",
      "71 Train Loss 0.0038151608 Test MSE 0.0001103506460184778 Test RE 0.0005354883446786329\n",
      "72 Train Loss 0.0028531498 Test MSE 0.0103334475194426 Test RE 0.00518185434768353\n",
      "73 Train Loss 0.002332478 Test MSE 0.02010049566883586 Test RE 0.007227131652041363\n",
      "74 Train Loss 0.0016998985 Test MSE 0.0035359498077990824 Test RE 0.0030312072314776914\n",
      "75 Train Loss 0.0014722239 Test MSE 7.969778997789216e-05 Test RE 0.0004550778753601779\n",
      "76 Train Loss 0.0013950525 Test MSE 0.0003183222053103353 Test RE 0.0009094860665126862\n",
      "77 Train Loss 0.0013418588 Test MSE 0.0014666609753259533 Test RE 0.0019522141252192073\n",
      "78 Train Loss 0.0012707367 Test MSE 0.005078408718667611 Test RE 0.0036326738519628205\n",
      "79 Train Loss 0.0012656287 Test MSE 0.005290312687603863 Test RE 0.0037076886164551714\n",
      "80 Train Loss 0.0012494605 Test MSE 0.005806721403390872 Test RE 0.0038844369573691347\n",
      "81 Train Loss 0.0012342148 Test MSE 0.0057901017168577 Test RE 0.0038788740605099005\n",
      "82 Train Loss 0.0012305712 Test MSE 0.005648707442026149 Test RE 0.00383122028688017\n",
      "83 Train Loss 0.0012231768 Test MSE 0.005215639841215759 Test RE 0.003681428580503018\n",
      "84 Train Loss 0.0012198718 Test MSE 0.004908745639864425 Test RE 0.0035714769106852365\n",
      "85 Train Loss 0.001215108 Test MSE 0.004455936844276598 Test RE 0.0034027660876123013\n",
      "86 Train Loss 0.001210444 Test MSE 0.003957530070658168 Test RE 0.0032068208178474404\n",
      "87 Train Loss 0.001209263 Test MSE 0.003845401346901065 Test RE 0.0031610649517985587\n",
      "88 Train Loss 0.0012014349 Test MSE 0.002935054179808315 Test RE 0.0027616627607662957\n",
      "89 Train Loss 0.0012002494 Test MSE 0.0029209084350610443 Test RE 0.002754999687773249\n",
      "90 Train Loss 0.0011958919 Test MSE 0.0025939024333923133 Test RE 0.002596207503770348\n",
      "91 Train Loss 0.0011958918 Test MSE 0.0025939024333923133 Test RE 0.002596207503770348\n",
      "92 Train Loss 0.0011958919 Test MSE 0.0025939024333923133 Test RE 0.002596207503770348\n",
      "93 Train Loss 0.0011958919 Test MSE 0.0025939024333923133 Test RE 0.002596207503770348\n",
      "94 Train Loss 0.0011958919 Test MSE 0.0025939024333923133 Test RE 0.002596207503770348\n",
      "95 Train Loss 0.0011958919 Test MSE 0.0025939024333923133 Test RE 0.002596207503770348\n",
      "96 Train Loss 0.0011958919 Test MSE 0.0025939024333923133 Test RE 0.002596207503770348\n",
      "97 Train Loss 0.0011958919 Test MSE 0.0025939024333923133 Test RE 0.002596207503770348\n",
      "98 Train Loss 0.0011958919 Test MSE 0.0025939024333923133 Test RE 0.002596207503770348\n",
      "99 Train Loss 0.0011958919 Test MSE 0.0025939024333923133 Test RE 0.002596207503770348\n",
      "Training time: 44.35\n",
      "Training time: 44.35\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 4.2939425 Test MSE 383.26796832668236 Test RE 0.9979616325547044\n",
      "1 Train Loss 2.702107 Test MSE 381.0380541453537 Test RE 0.9950542474880655\n",
      "2 Train Loss 2.4269361 Test MSE 383.2838264660019 Test RE 0.9979822782285057\n",
      "3 Train Loss 2.3686855 Test MSE 380.8898173638188 Test RE 0.9948606736622078\n",
      "4 Train Loss 2.3613527 Test MSE 379.0359265210651 Test RE 0.9924365962297634\n",
      "5 Train Loss 2.2908907 Test MSE 364.80804133194806 Test RE 0.9736318759286078\n",
      "6 Train Loss 2.2682571 Test MSE 363.16724201830186 Test RE 0.9714398534212413\n",
      "7 Train Loss 2.2034943 Test MSE 350.0039926216707 Test RE 0.9536721096779999\n",
      "8 Train Loss 2.159219 Test MSE 339.99653032261637 Test RE 0.9399393361589852\n",
      "9 Train Loss 2.0131156 Test MSE 316.6437331992603 Test RE 0.9070850957085284\n",
      "10 Train Loss 1.9649491 Test MSE 310.18223978520405 Test RE 0.8977823153396518\n",
      "11 Train Loss 1.8240312 Test MSE 280.00905344658497 Test RE 0.8529991909883916\n",
      "12 Train Loss 1.6930414 Test MSE 262.32762589322806 Test RE 0.8256283496718877\n",
      "13 Train Loss 1.568339 Test MSE 231.19832136373265 Test RE 0.7750949740336813\n",
      "14 Train Loss 1.3102919 Test MSE 200.68216090781644 Test RE 0.722132626048391\n",
      "15 Train Loss 1.2638516 Test MSE 187.17383892066357 Test RE 0.6974051601428243\n",
      "16 Train Loss 1.0833048 Test MSE 166.1944846620037 Test RE 0.6571596446686987\n",
      "17 Train Loss 1.0086726 Test MSE 154.09444890357292 Test RE 0.6327848597504929\n",
      "18 Train Loss 0.83776546 Test MSE 115.54899083559621 Test RE 0.5479559674891702\n",
      "19 Train Loss 0.6064602 Test MSE 80.26598922554203 Test RE 0.4566972155551998\n",
      "20 Train Loss 0.5440756 Test MSE 62.26640295981376 Test RE 0.4022439043472987\n",
      "21 Train Loss 0.37251735 Test MSE 41.229762812320324 Test RE 0.32731657695877714\n",
      "22 Train Loss 0.2761088 Test MSE 31.734491888599564 Test RE 0.2871629175427519\n",
      "23 Train Loss 0.18392994 Test MSE 15.838058775283374 Test RE 0.20286800467385865\n",
      "24 Train Loss 0.090929374 Test MSE 2.7694043575879346 Test RE 0.0848312307994004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 Train Loss 0.059730724 Test MSE 0.10036330472231861 Test RE 0.01614916452601225\n",
      "26 Train Loss 0.04257008 Test MSE 0.0030954827714465578 Test RE 0.00283613420794047\n",
      "27 Train Loss 0.037649658 Test MSE 0.19818998592929568 Test RE 0.02269360210455604\n",
      "28 Train Loss 0.029111575 Test MSE 0.2604737096968901 Test RE 0.026016239994218256\n",
      "29 Train Loss 0.019442352 Test MSE 0.08122169546959172 Test RE 0.014527758373055923\n",
      "30 Train Loss 0.01223373 Test MSE 0.1156231495840448 Test RE 0.01733344873068329\n",
      "31 Train Loss 0.0068172556 Test MSE 0.011006113784850217 Test RE 0.005347854486367721\n",
      "32 Train Loss 0.006187152 Test MSE 0.0036791175871765305 Test RE 0.003091963914464836\n",
      "33 Train Loss 0.005919063 Test MSE 0.003146063057409916 Test RE 0.0028592115798133193\n",
      "34 Train Loss 0.0058999397 Test MSE 0.003256158980224313 Test RE 0.002908810190344968\n",
      "35 Train Loss 0.0058994265 Test MSE 0.0032561952449403657 Test RE 0.002908826388399569\n",
      "36 Train Loss 0.0058897743 Test MSE 0.003225104052306969 Test RE 0.002894905877426645\n",
      "37 Train Loss 0.005883757 Test MSE 0.003259830639172746 Test RE 0.0029104497217648764\n",
      "38 Train Loss 0.0058794473 Test MSE 0.003246853612401926 Test RE 0.0029046508549393796\n",
      "39 Train Loss 0.0056378464 Test MSE 0.002553190502485931 Test RE 0.0025757528714651663\n",
      "40 Train Loss 0.003487497 Test MSE 0.003283360278332641 Test RE 0.0029209347295585907\n",
      "41 Train Loss 0.0026973546 Test MSE 0.016084463081224062 Test RE 0.0064649603643331405\n",
      "42 Train Loss 0.0026642398 Test MSE 0.020068330078657874 Test RE 0.007221346769094924\n",
      "43 Train Loss 0.0026560582 Test MSE 0.02186654663020707 Test RE 0.007537940114064759\n",
      "44 Train Loss 0.0026546987 Test MSE 0.022332021993404547 Test RE 0.007617748072818582\n",
      "45 Train Loss 0.0026518656 Test MSE 0.022814936469426526 Test RE 0.0076996718130264855\n",
      "46 Train Loss 0.002631482 Test MSE 0.027867479570927157 Test RE 0.008509644714006535\n",
      "47 Train Loss 0.0026269061 Test MSE 0.028594287045243087 Test RE 0.008619899825891637\n",
      "48 Train Loss 0.0025971567 Test MSE 0.03504845488659125 Test RE 0.009543265820489895\n",
      "49 Train Loss 0.0024297035 Test MSE 0.047755255306866716 Test RE 0.011139691612357092\n",
      "50 Train Loss 0.002059749 Test MSE 0.024631871915023414 Test RE 0.008000392449869213\n",
      "51 Train Loss 0.0013481479 Test MSE 2.5993092717116777e-05 Test RE 0.00025989119167582576\n",
      "52 Train Loss 0.00076507044 Test MSE 0.0018535697031308394 Test RE 0.0021946589661033756\n",
      "53 Train Loss 0.0007343558 Test MSE 0.003049918600678831 Test RE 0.002815183489836124\n",
      "54 Train Loss 0.00069443276 Test MSE 0.005031082427186984 Test RE 0.0036157075731163613\n",
      "55 Train Loss 0.0006845077 Test MSE 0.004956450802692222 Test RE 0.0035887894737274347\n",
      "56 Train Loss 0.0005852588 Test MSE 0.002246849889471722 Test RE 0.002416293089106375\n",
      "57 Train Loss 0.0005776556 Test MSE 0.001883342633584979 Test RE 0.00221221458557689\n",
      "58 Train Loss 0.00054204656 Test MSE 0.0005274992040330572 Test RE 0.00117077512574468\n",
      "59 Train Loss 0.0005353001 Test MSE 0.00035056817736440896 Test RE 0.0009544404303472853\n",
      "60 Train Loss 0.00052883965 Test MSE 0.00023580343024409277 Test RE 0.0007827762525372851\n",
      "61 Train Loss 0.0005204459 Test MSE 0.00011563307869009678 Test RE 0.0005481553117744013\n",
      "62 Train Loss 0.00049849786 Test MSE 4.455596558740761e-08 Test RE 1.0760080301774523e-05\n",
      "63 Train Loss 0.0004802333 Test MSE 2.4506504922772845e-05 Test RE 0.00025234997798568627\n",
      "64 Train Loss 0.00047351982 Test MSE 2.189607332576851e-05 Test RE 0.00023853147950196965\n",
      "65 Train Loss 0.000430742 Test MSE 5.608168981545464e-06 Test RE 0.00012071830476057686\n",
      "66 Train Loss 0.00042678305 Test MSE 1.047050540514372e-05 Test RE 0.00016494775354862108\n",
      "67 Train Loss 0.00041225695 Test MSE 3.9431634026853645e-05 Test RE 0.00032009948080206414\n",
      "68 Train Loss 0.0004082304 Test MSE 4.2417283994957925e-05 Test RE 0.00033199688659636673\n",
      "69 Train Loss 0.000405408 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "70 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "71 Train Loss 0.000405408 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "72 Train Loss 0.00040540795 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "73 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "74 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "75 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "76 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "77 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "78 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "79 Train Loss 0.000405408 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "80 Train Loss 0.000405408 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "81 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "82 Train Loss 0.000405408 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "83 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "84 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "85 Train Loss 0.00040540795 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "86 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "87 Train Loss 0.00040540795 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "88 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "89 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "90 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "91 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "92 Train Loss 0.000405408 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "93 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "94 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "95 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "96 Train Loss 0.000405408 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "97 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "98 Train Loss 0.000405408 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "99 Train Loss 0.00040540798 Test MSE 4.2047891187683687e-05 Test RE 0.0003305481207003217\n",
      "Training time: 36.54\n",
      "Training time: 36.54\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 4.6160994 Test MSE 386.55956649150437 Test RE 1.0022378390452507\n",
      "1 Train Loss 3.1788125 Test MSE 384.470844312841 Test RE 0.9995264433172317\n",
      "2 Train Loss 2.4217758 Test MSE 382.8694235975951 Test RE 0.9974426277961945\n",
      "3 Train Loss 2.383907 Test MSE 383.76484339874537 Test RE 0.9986083101955371\n",
      "4 Train Loss 2.3777308 Test MSE 382.94039049992784 Test RE 0.9975350641849168\n",
      "5 Train Loss 2.377418 Test MSE 382.88957908051185 Test RE 0.9974688817512362\n",
      "6 Train Loss 2.3720732 Test MSE 380.23376510516863 Test RE 0.9940035204225204\n",
      "7 Train Loss 2.3548112 Test MSE 377.26096428213265 Test RE 0.9901101616506022\n",
      "8 Train Loss 2.3384545 Test MSE 370.4963509269781 Test RE 0.9811932457393535\n",
      "9 Train Loss 2.2404613 Test MSE 357.3275026096998 Test RE 0.9635978105879853\n",
      "10 Train Loss 2.1212156 Test MSE 336.63268300090624 Test RE 0.9352780063732444\n",
      "11 Train Loss 2.0709984 Test MSE 325.38235008390535 Test RE 0.9195166092095061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 Train Loss 1.9793038 Test MSE 313.99278119885554 Test RE 0.9032800414709854\n",
      "13 Train Loss 1.9388372 Test MSE 306.02588278537354 Test RE 0.8917470108141856\n",
      "14 Train Loss 1.8461099 Test MSE 283.457900750235 Test RE 0.8582362728155465\n",
      "15 Train Loss 1.6910188 Test MSE 253.35786250724223 Test RE 0.8113902331570441\n",
      "16 Train Loss 1.4013679 Test MSE 213.48754761337386 Test RE 0.7448157585640367\n",
      "17 Train Loss 1.2820225 Test MSE 196.82997023478742 Test RE 0.7151682010624042\n",
      "18 Train Loss 1.1604869 Test MSE 161.53355647738064 Test RE 0.6478790848134396\n",
      "19 Train Loss 0.91647977 Test MSE 131.23289731726828 Test RE 0.5839611311023166\n",
      "20 Train Loss 0.8010816 Test MSE 117.72870260829451 Test RE 0.5531001312794002\n",
      "21 Train Loss 0.76986116 Test MSE 109.1428755222531 Test RE 0.5325498643240275\n",
      "22 Train Loss 0.60234874 Test MSE 79.30505279688 Test RE 0.45395521732287103\n",
      "23 Train Loss 0.48117417 Test MSE 59.4963292307657 Test RE 0.3931947119355633\n",
      "24 Train Loss 0.42722505 Test MSE 45.232981219836965 Test RE 0.3428389754989907\n",
      "25 Train Loss 0.3764287 Test MSE 32.71313296357271 Test RE 0.291557120227254\n",
      "26 Train Loss 0.35452354 Test MSE 30.05926944659083 Test RE 0.27948068126452796\n",
      "27 Train Loss 0.3034138 Test MSE 28.176487286170364 Test RE 0.27058642653532916\n",
      "28 Train Loss 0.2567721 Test MSE 27.98689092737243 Test RE 0.26967451736202425\n",
      "29 Train Loss 0.21430776 Test MSE 28.364895752461948 Test RE 0.27148958797683126\n",
      "30 Train Loss 0.19664598 Test MSE 25.91376524025176 Test RE 0.2594943118877121\n",
      "31 Train Loss 0.14460781 Test MSE 16.92429838688015 Test RE 0.20970941045808633\n",
      "32 Train Loss 0.09137004 Test MSE 8.093684472475742 Test RE 0.14502261014069523\n",
      "33 Train Loss 0.054388385 Test MSE 3.5607153090843418 Test RE 0.09619028431908876\n",
      "34 Train Loss 0.045479402 Test MSE 2.3219894083207877 Test RE 0.07767704571444364\n",
      "35 Train Loss 0.031486772 Test MSE 0.9450948716323334 Test RE 0.04955646116937725\n",
      "36 Train Loss 0.01999713 Test MSE 0.5880654373944033 Test RE 0.03909087666047668\n",
      "37 Train Loss 0.01758389 Test MSE 0.5427976158850527 Test RE 0.03755619179258791\n",
      "38 Train Loss 0.014861822 Test MSE 0.7855084068950983 Test RE 0.0451791443196029\n",
      "39 Train Loss 0.013609571 Test MSE 1.0438686755655135 Test RE 0.052081743610185326\n",
      "40 Train Loss 0.01112714 Test MSE 0.9465083689636637 Test RE 0.0495935059974226\n",
      "41 Train Loss 0.0078679845 Test MSE 0.31194272530129546 Test RE 0.02847082248711782\n",
      "42 Train Loss 0.006776765 Test MSE 0.1405774095644125 Test RE 0.019112625522232224\n",
      "43 Train Loss 0.006180272 Test MSE 0.14608600450535347 Test RE 0.01948349606351009\n",
      "44 Train Loss 0.00550303 Test MSE 0.18528913315759352 Test RE 0.021942573274556922\n",
      "45 Train Loss 0.0051082624 Test MSE 0.1273009751316395 Test RE 0.018187727877451283\n",
      "46 Train Loss 0.0048788544 Test MSE 0.053423714795081324 Test RE 0.011782287727171962\n",
      "47 Train Loss 0.0048697097 Test MSE 0.05153744408104357 Test RE 0.011572415566914082\n",
      "48 Train Loss 0.0046408353 Test MSE 0.022365398638931527 Test RE 0.007623438555034379\n",
      "49 Train Loss 0.004434736 Test MSE 0.032012782432597565 Test RE 0.009120618578778586\n",
      "50 Train Loss 0.0043594358 Test MSE 0.034558308019102604 Test RE 0.009476300370828543\n",
      "51 Train Loss 0.0039852527 Test MSE 0.04324481831690367 Test RE 0.010600579830646877\n",
      "52 Train Loss 0.002964636 Test MSE 0.024187095284245976 Test RE 0.007927832034868135\n",
      "53 Train Loss 0.0026393551 Test MSE 0.003945941684328308 Test RE 0.003202122290989188\n",
      "54 Train Loss 0.0023079528 Test MSE 0.0012915635902890385 Test RE 0.0018319789505660284\n",
      "55 Train Loss 0.0019480732 Test MSE 0.010259492934673814 Test RE 0.005163278261984391\n",
      "56 Train Loss 0.0016515215 Test MSE 0.022952547680476196 Test RE 0.007722857681742797\n",
      "57 Train Loss 0.0012718848 Test MSE 0.009955613503169845 Test RE 0.005086237043672445\n",
      "58 Train Loss 0.0011079484 Test MSE 0.00012835136113258742 Test RE 0.0005775144056445708\n",
      "59 Train Loss 0.0011046275 Test MSE 6.346419798991701e-05 Test RE 0.000406094381252804\n",
      "60 Train Loss 0.0010975299 Test MSE 5.378502492821003e-07 Test RE 3.738464574528918e-05\n",
      "61 Train Loss 0.0010919182 Test MSE 4.1681779490547795e-05 Test RE 0.00032910593051481304\n",
      "62 Train Loss 0.0010903579 Test MSE 6.048052832869944e-05 Test RE 0.0003964335218439155\n",
      "63 Train Loss 0.0010868516 Test MSE 0.00011811768825393939 Test RE 0.0005540131226870527\n",
      "64 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "65 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "66 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "67 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "68 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "69 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "70 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "71 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "72 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "73 Train Loss 0.0010844679 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "74 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "75 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "76 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "77 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "78 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "79 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "80 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "81 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "82 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "83 Train Loss 0.0010844679 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "84 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "85 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "86 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "87 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "88 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "89 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "90 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "91 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "92 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "93 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "94 Train Loss 0.0010844679 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "95 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "96 Train Loss 0.0010844679 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "97 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "98 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "99 Train Loss 0.0010844677 Test MSE 0.00015186186573824727 Test RE 0.00062818411212661\n",
      "Training time: 41.84\n",
      "Training time: 41.84\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 3.0820105 Test MSE 381.8867048072554 Test RE 0.9961617271299008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 2.3988862 Test MSE 383.2586006649557 Test RE 0.9979494366175445\n",
      "2 Train Loss 2.379083 Test MSE 382.9189518944452 Test RE 0.9975071407005427\n",
      "3 Train Loss 2.3748853 Test MSE 381.05642498745544 Test RE 0.9950782342840536\n",
      "4 Train Loss 2.363305 Test MSE 379.1142061831994 Test RE 0.9925390714641023\n",
      "5 Train Loss 2.3431747 Test MSE 374.04687510959434 Test RE 0.9858834994515155\n",
      "6 Train Loss 2.280586 Test MSE 364.517420142188 Test RE 0.9732439809282387\n",
      "7 Train Loss 2.2202034 Test MSE 348.8107525642334 Test RE 0.9520450835124492\n",
      "8 Train Loss 2.0821009 Test MSE 321.1163710093001 Test RE 0.9134689844802071\n",
      "9 Train Loss 1.988815 Test MSE 316.52629768233675 Test RE 0.906916872126998\n",
      "10 Train Loss 1.8049664 Test MSE 279.79965670446944 Test RE 0.8526801858437603\n",
      "11 Train Loss 1.777154 Test MSE 271.2507714414319 Test RE 0.8395529143348016\n",
      "12 Train Loss 1.6368129 Test MSE 246.01401719917075 Test RE 0.7995442583396406\n",
      "13 Train Loss 1.4798735 Test MSE 221.986845740726 Test RE 0.7594972446761366\n",
      "14 Train Loss 1.377922 Test MSE 204.66767479918394 Test RE 0.7292680890650797\n",
      "15 Train Loss 1.2626302 Test MSE 187.44951911532783 Test RE 0.697918560013144\n",
      "16 Train Loss 1.1281432 Test MSE 170.41645537099305 Test RE 0.6654544696704424\n",
      "17 Train Loss 1.0122161 Test MSE 155.84142975551782 Test RE 0.6363617163363473\n",
      "18 Train Loss 0.9596407 Test MSE 144.99227609935065 Test RE 0.6138114935391146\n",
      "19 Train Loss 0.9101365 Test MSE 124.88358825538658 Test RE 0.5696593984109444\n",
      "20 Train Loss 0.83335996 Test MSE 111.97431227009609 Test RE 0.5394134664463158\n",
      "21 Train Loss 0.82403165 Test MSE 113.56505461837354 Test RE 0.5432314930454016\n",
      "22 Train Loss 0.7477202 Test MSE 108.23523829733804 Test RE 0.530330886702471\n",
      "23 Train Loss 0.60192543 Test MSE 85.61612700789308 Test RE 0.4716722993307534\n",
      "24 Train Loss 0.4751183 Test MSE 56.86929254848022 Test RE 0.38441603538510344\n",
      "25 Train Loss 0.43678296 Test MSE 51.02315758165526 Test RE 0.36412143724932716\n",
      "26 Train Loss 0.23084387 Test MSE 24.617516627309598 Test RE 0.2529208906494066\n",
      "27 Train Loss 0.2048434 Test MSE 22.284326539730497 Test RE 0.24063696384207092\n",
      "28 Train Loss 0.20165637 Test MSE 21.513862149451803 Test RE 0.23644044785704116\n",
      "29 Train Loss 0.2014706 Test MSE 21.464496654997298 Test RE 0.2361690251147446\n",
      "30 Train Loss 0.20098238 Test MSE 21.62924911220528 Test RE 0.2370736596351534\n",
      "31 Train Loss 0.19971149 Test MSE 22.19369625538725 Test RE 0.24014713041803873\n",
      "32 Train Loss 0.1969616 Test MSE 21.64124442682389 Test RE 0.23713938958604724\n",
      "33 Train Loss 0.18589005 Test MSE 16.06272198291416 Test RE 0.20430178156474788\n",
      "34 Train Loss 0.17824067 Test MSE 14.325234041003563 Test RE 0.19293607782624267\n",
      "35 Train Loss 0.1743449 Test MSE 13.752024904713437 Test RE 0.18903660440387154\n",
      "36 Train Loss 0.15976682 Test MSE 13.666239639076446 Test RE 0.18844607594054302\n",
      "37 Train Loss 0.11616853 Test MSE 14.567598807729139 Test RE 0.19456134898098562\n",
      "38 Train Loss 0.09775518 Test MSE 10.76998705026852 Test RE 0.16729007392782821\n",
      "39 Train Loss 0.070151836 Test MSE 5.46284487029657 Test RE 0.1191439561042793\n",
      "40 Train Loss 0.056532964 Test MSE 4.569134121289193 Test RE 0.10896312140607113\n",
      "41 Train Loss 0.053176004 Test MSE 3.6141644758920615 Test RE 0.0969095415266217\n",
      "42 Train Loss 0.0416321 Test MSE 2.786648380027878 Test RE 0.08509492677829204\n",
      "43 Train Loss 0.037998993 Test MSE 2.4122530606996677 Test RE 0.07917243744836268\n",
      "44 Train Loss 0.033586185 Test MSE 1.1284425968211802 Test RE 0.054150480963202685\n",
      "45 Train Loss 0.031366214 Test MSE 0.6221663605371962 Test RE 0.04020831239325449\n",
      "46 Train Loss 0.028553776 Test MSE 0.5920164534296356 Test RE 0.03922197612341931\n",
      "47 Train Loss 0.026162285 Test MSE 0.3945575014179047 Test RE 0.03201973511555856\n",
      "48 Train Loss 0.019880958 Test MSE 0.02749632786397349 Test RE 0.008452787115556413\n",
      "49 Train Loss 0.018503182 Test MSE 0.02144076724168418 Test RE 0.007464190994084371\n",
      "50 Train Loss 0.017473621 Test MSE 0.0719118337563448 Test RE 0.013669819002098094\n",
      "51 Train Loss 0.0155131575 Test MSE 0.06233727668013417 Test RE 0.012727306273645095\n",
      "52 Train Loss 0.014486835 Test MSE 0.005852276072100512 Test RE 0.003899644207942889\n",
      "53 Train Loss 0.013745445 Test MSE 0.008574621985369573 Test RE 0.004720305139692421\n",
      "54 Train Loss 0.012802827 Test MSE 0.08721844920649618 Test RE 0.015054514791884317\n",
      "55 Train Loss 0.012350476 Test MSE 0.19170586954616567 Test RE 0.022319285500303144\n",
      "56 Train Loss 0.011984907 Test MSE 0.22827997882141993 Test RE 0.024355468852035437\n",
      "57 Train Loss 0.011956929 Test MSE 0.23585343886889887 Test RE 0.024756183261543524\n",
      "58 Train Loss 0.011944273 Test MSE 0.24021501099840528 Test RE 0.02498403927780592\n",
      "59 Train Loss 0.0119442735 Test MSE 0.24021501099840528 Test RE 0.02498403927780592\n",
      "60 Train Loss 0.011943427 Test MSE 0.2402132296541057 Test RE 0.0249839466415084\n",
      "61 Train Loss 0.011943427 Test MSE 0.2402132296541057 Test RE 0.0249839466415084\n",
      "62 Train Loss 0.011943427 Test MSE 0.2402132296541057 Test RE 0.0249839466415084\n",
      "63 Train Loss 0.011943428 Test MSE 0.2402132296541057 Test RE 0.0249839466415084\n",
      "64 Train Loss 0.011943427 Test MSE 0.2402132296541057 Test RE 0.0249839466415084\n",
      "65 Train Loss 0.011943427 Test MSE 0.2402132296541057 Test RE 0.0249839466415084\n",
      "66 Train Loss 0.011943427 Test MSE 0.2402132296541057 Test RE 0.0249839466415084\n",
      "67 Train Loss 0.011943428 Test MSE 0.2402132296541057 Test RE 0.0249839466415084\n",
      "68 Train Loss 0.011943427 Test MSE 0.2402132296541057 Test RE 0.0249839466415084\n",
      "69 Train Loss 0.011943427 Test MSE 0.2402132296541057 Test RE 0.0249839466415084\n",
      "70 Train Loss 0.011943428 Test MSE 0.2402132296541057 Test RE 0.0249839466415084\n",
      "71 Train Loss 0.011943428 Test MSE 0.2402132296541057 Test RE 0.0249839466415084\n",
      "72 Train Loss 0.011943427 Test MSE 0.2402132296541057 Test RE 0.0249839466415084\n",
      "73 Train Loss 0.011943428 Test MSE 0.2402132296541057 Test RE 0.0249839466415084\n",
      "74 Train Loss 0.011943423 Test MSE 0.24021137599029369 Test RE 0.024983850243971963\n",
      "75 Train Loss 0.011943423 Test MSE 0.24021137599029369 Test RE 0.024983850243971963\n",
      "76 Train Loss 0.011943423 Test MSE 0.24021137599029369 Test RE 0.024983850243971963\n",
      "77 Train Loss 0.011943423 Test MSE 0.24021137599029369 Test RE 0.024983850243971963\n",
      "78 Train Loss 0.011939421 Test MSE 0.2435167561653109 Test RE 0.025155155569606034\n",
      "79 Train Loss 0.011938582 Test MSE 0.2440327652840746 Test RE 0.02518179320405037\n",
      "80 Train Loss 0.01193351 Test MSE 0.24805717154138038 Test RE 0.025388583802139125\n",
      "81 Train Loss 0.011931245 Test MSE 0.24942518961546503 Test RE 0.025458495683124336\n",
      "82 Train Loss 0.011930998 Test MSE 0.2494248797635331 Test RE 0.025458479870033173\n",
      "83 Train Loss 0.011930413 Test MSE 0.2502585138157598 Test RE 0.025500988364521276\n",
      "84 Train Loss 0.011922321 Test MSE 0.2557873851061273 Test RE 0.025781141566301066\n",
      "85 Train Loss 0.011921899 Test MSE 0.25595939267944123 Test RE 0.025789808542447574\n",
      "86 Train Loss 0.011903181 Test MSE 0.26770582443939606 Test RE 0.026374940745850218\n",
      "87 Train Loss 0.0118877925 Test MSE 0.27701882899390723 Test RE 0.026829787137841142\n",
      "88 Train Loss 0.011871318 Test MSE 0.285783172372722 Test RE 0.027250903696735464\n",
      "89 Train Loss 0.011727481 Test MSE 0.2920367851839558 Test RE 0.02754744737035533\n",
      "90 Train Loss 0.011481467 Test MSE 0.20340206979361294 Test RE 0.02299006856482352\n",
      "91 Train Loss 0.011349253 Test MSE 0.15576064703248244 Test RE 0.02011830805339964\n",
      "92 Train Loss 0.010572334 Test MSE 0.061193594040552614 Test RE 0.012610013835070068\n",
      "93 Train Loss 0.009329687 Test MSE 0.05885222259096867 Test RE 0.012366420701726819\n",
      "94 Train Loss 0.008627735 Test MSE 0.06566454284830307 Test RE 0.013062552383598802\n",
      "95 Train Loss 0.0076598017 Test MSE 0.05124880597362425 Test RE 0.011539964112188635\n",
      "96 Train Loss 0.0067813266 Test MSE 0.01957179369319571 Test RE 0.00713145091136909\n",
      "97 Train Loss 0.005922675 Test MSE 0.007931134682536594 Test RE 0.004539732309946375\n",
      "98 Train Loss 0.0048952424 Test MSE 0.010029303299265999 Test RE 0.005105026079964165\n",
      "99 Train Loss 0.0042378623 Test MSE 0.01637977294254641 Test RE 0.0065240385885758905\n",
      "Training time: 68.95\n",
      "Training time: 68.95\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 4.063854 Test MSE 380.91411672340735 Test RE 0.9948924073691745\n",
      "1 Train Loss 2.3884227 Test MSE 380.9466836955339 Test RE 0.9949349365632479\n",
      "2 Train Loss 2.3684883 Test MSE 381.350355721982 Test RE 0.9954619408382992\n",
      "3 Train Loss 2.3249598 Test MSE 372.4359661056394 Test RE 0.983758254800637\n",
      "4 Train Loss 2.2866821 Test MSE 360.9000303792605 Test RE 0.9684028119634011\n",
      "5 Train Loss 2.1637175 Test MSE 342.31826402828824 Test RE 0.9431431570248738\n"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    max_reps = 10\n",
    "    max_iter = 100\n",
    "    label = \"1D_SODE_Stan_tune\"+str(tune_reps)\n",
    "\n",
    "    N_f = 1000\n",
    "\n",
    "    train_loss_full = []\n",
    "    test_mse_full = []\n",
    "    test_re_full = []\n",
    "    beta_full = []\n",
    "    elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "    time_threshold = np.empty((max_reps,1))\n",
    "    time_threshold[:] = np.nan\n",
    "    epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "    for reps in range(max_reps):\n",
    "\n",
    "        train_loss = []\n",
    "        test_mse_loss = []\n",
    "        test_re_loss =[]\n",
    "        beta_val = []\n",
    "\n",
    "        'Generate Training data'\n",
    "        torch.manual_seed(reps*36)\n",
    "         #Total number of collocation points \n",
    "\n",
    "\n",
    "        layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "        PINN = Sequentialmodel(layers)\n",
    "        PINN.to(device)\n",
    "\n",
    "        'Neural Network Summary'\n",
    "        print(PINN)\n",
    "\n",
    "        params = list(PINN.parameters())\n",
    "\n",
    "        optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lr_tune[tune_reps], \n",
    "                                  max_iter = 10, \n",
    "                                  max_eval = 15, \n",
    "                                  tolerance_grad = 1e-5, \n",
    "                                  tolerance_change = 1e-5, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "        train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "        torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "        train_loss_full.append(train_loss)\n",
    "        test_mse_full.append(test_mse_loss)\n",
    "        test_re_full.append(test_re_loss)\n",
    "        beta_full.append(beta_val)    \n",
    "\n",
    "        print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "    mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "    savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
