{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.5,0.25,0.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.539222 Test MSE 383.8337102563185 Test RE 0.9986979066404409\n",
      "1 Train Loss 3.5665226 Test MSE 381.60592609825386 Test RE 0.9957954503899815\n",
      "2 Train Loss 2.4127002 Test MSE 380.3217129419256 Test RE 0.994118469978402\n",
      "3 Train Loss 2.38287 Test MSE 381.0137035961305 Test RE 0.9950224521013666\n",
      "4 Train Loss 2.36286 Test MSE 380.7015402255296 Test RE 0.9946147591417926\n",
      "5 Train Loss 2.3323689 Test MSE 372.68015931860367 Test RE 0.9840807098622602\n",
      "6 Train Loss 2.2985966 Test MSE 366.26984456027225 Test RE 0.9755806203940034\n",
      "7 Train Loss 2.1998947 Test MSE 346.05404781868185 Test RE 0.948275541943829\n",
      "8 Train Loss 2.0057042 Test MSE 315.6344790940695 Test RE 0.9056383435490931\n",
      "9 Train Loss 1.9267085 Test MSE 293.09028234308795 Test RE 0.8726966164683545\n",
      "10 Train Loss 1.6810855 Test MSE 259.3631064519026 Test RE 0.820949952594439\n",
      "11 Train Loss 1.2926201 Test MSE 194.54587130843663 Test RE 0.711006533972344\n",
      "12 Train Loss 0.9627595 Test MSE 145.82305305927812 Test RE 0.6155674907588657\n",
      "13 Train Loss 0.9043975 Test MSE 133.6637771899697 Test RE 0.5893447883312195\n",
      "14 Train Loss 0.8266692 Test MSE 126.23959391131646 Test RE 0.5727437740587942\n",
      "15 Train Loss 0.59692454 Test MSE 81.99557183148511 Test RE 0.461591477811626\n",
      "16 Train Loss 0.32598636 Test MSE 43.195936523509175 Test RE 0.33503025638142797\n",
      "17 Train Loss 0.22994469 Test MSE 25.797103298544172 Test RE 0.25890954045893166\n",
      "18 Train Loss 0.21482533 Test MSE 20.691772932220342 Test RE 0.23187900838761366\n",
      "19 Train Loss 0.20534536 Test MSE 16.71589260624133 Test RE 0.20841423023935904\n",
      "20 Train Loss 0.1792168 Test MSE 14.04209075306253 Test RE 0.19101983706668482\n",
      "21 Train Loss 0.1547452 Test MSE 13.57425270724463 Test RE 0.18781079348626528\n",
      "22 Train Loss 0.11277719 Test MSE 6.530391154037824 Test RE 0.13026633066424778\n",
      "23 Train Loss 0.07573929 Test MSE 2.066448118601744 Test RE 0.07327820721690718\n",
      "24 Train Loss 0.06378903 Test MSE 0.2564195960883011 Test RE 0.025812982585190943\n",
      "25 Train Loss 0.03488907 Test MSE 0.11398831400470698 Test RE 0.01721047067168282\n",
      "26 Train Loss 0.0264089 Test MSE 0.45768484073385646 Test RE 0.03448624002650334\n",
      "27 Train Loss 0.021617327 Test MSE 0.8067064305542089 Test RE 0.04578469674354991\n",
      "28 Train Loss 0.01968679 Test MSE 0.8860895172849339 Test RE 0.04798454509059804\n",
      "29 Train Loss 0.013041717 Test MSE 0.2729857644470707 Test RE 0.026633766174974774\n",
      "30 Train Loss 0.011798021 Test MSE 0.10852520683096968 Test RE 0.01679298475153687\n",
      "31 Train Loss 0.011707241 Test MSE 0.0864429630967986 Test RE 0.01498743819414884\n",
      "32 Train Loss 0.0117007345 Test MSE 0.08405806056906268 Test RE 0.014779245567069969\n",
      "33 Train Loss 0.011695559 Test MSE 0.08110136170163304 Test RE 0.01451699260488444\n",
      "34 Train Loss 0.011691846 Test MSE 0.07946047541994752 Test RE 0.014369384374195398\n",
      "35 Train Loss 0.011689074 Test MSE 0.07787242841739389 Test RE 0.014225070964179451\n",
      "36 Train Loss 0.011685951 Test MSE 0.07665024573363327 Test RE 0.014113000544206184\n",
      "37 Train Loss 0.011681266 Test MSE 0.0756631303634416 Test RE 0.014021831215543101\n",
      "38 Train Loss 0.0116783865 Test MSE 0.07469637301928127 Test RE 0.013931963879580017\n",
      "39 Train Loss 0.011675355 Test MSE 0.0740816087221402 Test RE 0.01387451417262531\n",
      "40 Train Loss 0.01167167 Test MSE 0.07358271231486029 Test RE 0.013827716872894346\n",
      "41 Train Loss 0.011669161 Test MSE 0.07336055239332488 Test RE 0.01380682686616563\n",
      "42 Train Loss 0.011665224 Test MSE 0.07282612595745074 Test RE 0.013756444061020189\n",
      "43 Train Loss 0.011662157 Test MSE 0.07256533580628122 Test RE 0.013731791077013483\n",
      "44 Train Loss 0.011655626 Test MSE 0.07164493293826077 Test RE 0.013644427649101322\n",
      "45 Train Loss 0.011434252 Test MSE 0.04396209405459424 Test RE 0.010688130998388137\n",
      "46 Train Loss 0.009121326 Test MSE 0.09646445701516186 Test RE 0.015832381441529157\n",
      "47 Train Loss 0.0060105035 Test MSE 0.17174846695320958 Test RE 0.021125598282135425\n",
      "48 Train Loss 0.0035813858 Test MSE 0.0012703848228776662 Test RE 0.0018168966779954787\n",
      "49 Train Loss 0.0021537084 Test MSE 0.0016053313023865476 Test RE 0.002042419365185784\n",
      "50 Train Loss 0.0019761021 Test MSE 0.007504268050384311 Test RE 0.004415874801264798\n",
      "51 Train Loss 0.0017545673 Test MSE 0.015478377269016335 Test RE 0.006341986255650327\n",
      "52 Train Loss 0.0017446518 Test MSE 0.014290695633509026 Test RE 0.006093815025914381\n",
      "53 Train Loss 0.0017361973 Test MSE 0.013149768493657187 Test RE 0.005845499612000048\n",
      "54 Train Loss 0.0017288935 Test MSE 0.01209597944828445 Test RE 0.005606387381407967\n",
      "55 Train Loss 0.0017224498 Test MSE 0.01112486990549179 Test RE 0.005376628787945807\n",
      "56 Train Loss 0.0017155022 Test MSE 0.010304567647209406 Test RE 0.005174608169737247\n",
      "57 Train Loss 0.0017080479 Test MSE 0.009556613701804552 Test RE 0.004983272062228216\n",
      "58 Train Loss 0.0017005659 Test MSE 0.008954681061296809 Test RE 0.0048237816781064825\n",
      "59 Train Loss 0.001680875 Test MSE 0.008162303864105613 Test RE 0.004605417021786482\n",
      "60 Train Loss 0.0016624281 Test MSE 0.007889779170074863 Test RE 0.004527881021197031\n",
      "61 Train Loss 0.0016532262 Test MSE 0.008157061384454007 Test RE 0.004603937801982946\n",
      "62 Train Loss 0.001645737 Test MSE 0.008371962959587502 Test RE 0.004664189980041615\n",
      "63 Train Loss 0.0016387677 Test MSE 0.008905612629102498 Test RE 0.004810547228691697\n",
      "64 Train Loss 0.0016333989 Test MSE 0.00909240229562849 Test RE 0.004860734545239234\n",
      "65 Train Loss 0.0016295938 Test MSE 0.00937337239524433 Test RE 0.004935265467189447\n",
      "66 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "67 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "68 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "69 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "70 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "71 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "72 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "73 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "74 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "75 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "76 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "77 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "78 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "79 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "80 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "81 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "82 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "83 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "84 Train Loss 0.0016268042 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "85 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "86 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "87 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "88 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "89 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "90 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "91 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "93 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "94 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "95 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "96 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "97 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "98 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "99 Train Loss 0.0016268044 Test MSE 0.009813420251337803 Test RE 0.005049783759757697\n",
      "Training time: 28.14\n",
      "Training time: 28.14\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 3.9200773 Test MSE 384.88307610634837 Test RE 1.0000621486856227\n",
      "1 Train Loss 2.6503117 Test MSE 381.89040623036834 Test RE 0.996166554749009\n",
      "2 Train Loss 2.386058 Test MSE 383.7736433353764 Test RE 0.998619759445806\n",
      "3 Train Loss 2.3813949 Test MSE 383.7634389737318 Test RE 0.9986064829414297\n",
      "4 Train Loss 2.3790898 Test MSE 382.8650115847891 Test RE 0.997436880742429\n",
      "5 Train Loss 2.3685143 Test MSE 380.75039873011445 Test RE 0.9946785805682588\n",
      "6 Train Loss 2.3362226 Test MSE 372.0196156543643 Test RE 0.9832082237006832\n",
      "7 Train Loss 2.3122933 Test MSE 367.4170779108699 Test RE 0.9771072865992514\n",
      "8 Train Loss 2.2476885 Test MSE 348.55634432683837 Test RE 0.951697828890693\n",
      "9 Train Loss 2.1516166 Test MSE 343.2274501725663 Test RE 0.9443948052236262\n",
      "10 Train Loss 2.0176117 Test MSE 314.62824428482907 Test RE 0.904193615018828\n",
      "11 Train Loss 1.9150697 Test MSE 299.73490739491865 Test RE 0.8825335912892193\n",
      "12 Train Loss 1.7757602 Test MSE 267.2934995734184 Test RE 0.8334063037640325\n",
      "13 Train Loss 1.5096735 Test MSE 236.57944282090443 Test RE 0.7840632254891452\n",
      "14 Train Loss 1.2668052 Test MSE 179.13476124203564 Test RE 0.6822640958559392\n",
      "15 Train Loss 1.136042 Test MSE 148.36042878131747 Test RE 0.6208999465258346\n",
      "16 Train Loss 0.9812847 Test MSE 139.3996683649819 Test RE 0.6018571895597699\n",
      "17 Train Loss 0.8110618 Test MSE 117.52140703687694 Test RE 0.552612970032166\n",
      "18 Train Loss 0.7398005 Test MSE 106.36216883785714 Test RE 0.5257220282726599\n",
      "19 Train Loss 0.6234881 Test MSE 89.10338448424993 Test RE 0.4811823432970747\n",
      "20 Train Loss 0.59134036 Test MSE 81.33799303904578 Test RE 0.4597368424225861\n",
      "21 Train Loss 0.55890304 Test MSE 74.56355174641425 Test RE 0.4401755099073457\n",
      "22 Train Loss 0.5346898 Test MSE 74.28282188468253 Test RE 0.43934610373997407\n",
      "23 Train Loss 0.5054335 Test MSE 74.69271766311141 Test RE 0.4405566014094719\n",
      "24 Train Loss 0.4495501 Test MSE 66.20974840567024 Test RE 0.4147854859892955\n",
      "25 Train Loss 0.36308485 Test MSE 52.10830063663095 Test RE 0.36797307124357176\n",
      "26 Train Loss 0.3104709 Test MSE 40.56792372460163 Test RE 0.32467883054612623\n",
      "27 Train Loss 0.23594841 Test MSE 28.886427636573345 Test RE 0.2739740947732435\n",
      "28 Train Loss 0.1909082 Test MSE 23.67555660995583 Test RE 0.24803483635961207\n",
      "29 Train Loss 0.16798525 Test MSE 19.469603916944983 Test RE 0.22492676691182079\n",
      "30 Train Loss 0.12683006 Test MSE 10.627713045971001 Test RE 0.1661814302327509\n",
      "31 Train Loss 0.11332757 Test MSE 6.938588751703616 Test RE 0.13427592610423797\n",
      "32 Train Loss 0.08436499 Test MSE 3.597353215317169 Test RE 0.0966838917942006\n",
      "33 Train Loss 0.061583664 Test MSE 1.0965245856342336 Test RE 0.05337916419122948\n",
      "34 Train Loss 0.051669363 Test MSE 0.06848214845679657 Test RE 0.013339859974846331\n",
      "35 Train Loss 0.04617839 Test MSE 0.13396945141769734 Test RE 0.018658016467021792\n",
      "36 Train Loss 0.044018723 Test MSE 0.1567999942958599 Test RE 0.020185318377307724\n",
      "37 Train Loss 0.031450912 Test MSE 0.014525603564870659 Test RE 0.00614369540786778\n",
      "38 Train Loss 0.026307214 Test MSE 0.00528581535147377 Test RE 0.0037061123138532512\n",
      "39 Train Loss 0.025878465 Test MSE 0.028173622313762414 Test RE 0.008556259086333453\n",
      "40 Train Loss 0.024190392 Test MSE 0.16559615317582527 Test RE 0.02074377076293326\n",
      "41 Train Loss 0.013792755 Test MSE 0.08735372304890232 Test RE 0.015066184874632474\n",
      "42 Train Loss 0.010425638 Test MSE 4.316018750393663e-05 Test RE 0.00033489159192327356\n",
      "43 Train Loss 0.009236121 Test MSE 0.01796187547032237 Test RE 0.00683185158343064\n",
      "44 Train Loss 0.008169514 Test MSE 0.02528317803025248 Test RE 0.008105473941485605\n",
      "45 Train Loss 0.008081146 Test MSE 0.021136685289828164 Test RE 0.0074110718386627\n",
      "46 Train Loss 0.0080753565 Test MSE 0.02064105150582783 Test RE 0.007323665350264551\n",
      "47 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "48 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "49 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "50 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "51 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "52 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "53 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "54 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "55 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "56 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "57 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "58 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "59 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "60 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "61 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "62 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "63 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "64 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "65 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "66 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "67 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "68 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "69 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "70 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "71 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "72 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "73 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "74 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "75 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "76 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "77 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "78 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "79 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "80 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "81 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "82 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "84 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "85 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "86 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "87 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "88 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "89 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "90 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "91 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "92 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "93 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "94 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "95 Train Loss 0.008072698 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "96 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "97 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "98 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "99 Train Loss 0.008072697 Test MSE 0.020121137722163406 Test RE 0.007230841624136789\n",
      "Training time: 30.19\n",
      "Training time: 30.19\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 3.676158 Test MSE 385.3957381927743 Test RE 1.0007279656138865\n",
      "1 Train Loss 2.4262211 Test MSE 383.4261166217336 Test RE 0.998167506337725\n",
      "2 Train Loss 2.3908823 Test MSE 382.6680012030711 Test RE 0.9971802227819696\n",
      "3 Train Loss 2.3801436 Test MSE 383.1949879953703 Test RE 0.9978666141289514\n",
      "4 Train Loss 2.3686612 Test MSE 381.1283009066218 Test RE 0.9951720770664904\n",
      "5 Train Loss 2.3572412 Test MSE 377.3937479707241 Test RE 0.9902843897387368\n",
      "6 Train Loss 2.298946 Test MSE 366.26954087074455 Test RE 0.9755802159467309\n",
      "7 Train Loss 2.2036572 Test MSE 351.27640897421884 Test RE 0.9554040429450533\n",
      "8 Train Loss 2.0994716 Test MSE 328.89967213728124 Test RE 0.9244731512597087\n",
      "9 Train Loss 2.0086083 Test MSE 315.6496882304402 Test RE 0.9056601627926453\n",
      "10 Train Loss 1.8070997 Test MSE 278.5024226229219 Test RE 0.8507012504991279\n",
      "11 Train Loss 1.636416 Test MSE 258.736768881014 Test RE 0.8199580948610767\n",
      "12 Train Loss 1.5020901 Test MSE 222.46590298576052 Test RE 0.7603163170200301\n",
      "13 Train Loss 1.3433039 Test MSE 203.63841152598607 Test RE 0.7274320518528012\n",
      "14 Train Loss 1.1876109 Test MSE 172.34893193274857 Test RE 0.6692168705343755\n",
      "15 Train Loss 1.0029559 Test MSE 156.69678470727055 Test RE 0.6381057015637304\n",
      "16 Train Loss 0.80673164 Test MSE 117.08398521638809 Test RE 0.5515835817453323\n",
      "17 Train Loss 0.6850418 Test MSE 96.03854781359786 Test RE 0.49955737490496027\n",
      "18 Train Loss 0.5738833 Test MSE 81.56992703953804 Test RE 0.460391841991618\n",
      "19 Train Loss 0.46209273 Test MSE 57.135776990702695 Test RE 0.3853156523955364\n",
      "20 Train Loss 0.2902337 Test MSE 32.53956619569136 Test RE 0.2907826311368706\n",
      "21 Train Loss 0.23868981 Test MSE 26.171436170118785 Test RE 0.2607812485228612\n",
      "22 Train Loss 0.20199594 Test MSE 19.141026431432504 Test RE 0.22302070996358223\n",
      "23 Train Loss 0.17562056 Test MSE 16.835601887397015 Test RE 0.20915916829965253\n",
      "24 Train Loss 0.1237566 Test MSE 11.060656720682092 Test RE 0.1695325285120209\n",
      "25 Train Loss 0.095716655 Test MSE 6.9466795527967085 Test RE 0.13435419009505922\n",
      "26 Train Loss 0.057412483 Test MSE 1.9569488021924193 Test RE 0.07131030835773548\n",
      "27 Train Loss 0.019060815 Test MSE 0.6874314244757503 Test RE 0.042264649631807226\n",
      "28 Train Loss 0.013987965 Test MSE 0.10094597113856285 Test RE 0.01619597425568746\n",
      "29 Train Loss 0.01139035 Test MSE 0.00016892404118260893 Test RE 0.0006625342188241716\n",
      "30 Train Loss 0.010701243 Test MSE 0.00028782308860329244 Test RE 0.0008648193478655882\n",
      "31 Train Loss 0.008326522 Test MSE 0.00013336807889779146 Test RE 0.0005886925370658968\n",
      "32 Train Loss 0.0080271205 Test MSE 0.00587980361250224 Test RE 0.00390880488957932\n",
      "33 Train Loss 0.0068035196 Test MSE 0.04121360912367377 Test RE 0.010348631115072901\n",
      "34 Train Loss 0.005526499 Test MSE 0.0004776310394721541 Test RE 0.0011140606986386878\n",
      "35 Train Loss 0.0030473112 Test MSE 0.04787055031464247 Test RE 0.011153130725586923\n",
      "36 Train Loss 0.002246978 Test MSE 0.004317710385610598 Test RE 0.0033495721477432992\n",
      "37 Train Loss 0.0021368216 Test MSE 0.0016441297154813873 Test RE 0.0020669530974466994\n",
      "38 Train Loss 0.002131251 Test MSE 0.001622941507206026 Test RE 0.0020535913025759005\n",
      "39 Train Loss 0.0021257398 Test MSE 0.0014503538920677482 Test RE 0.0019413309342839605\n",
      "40 Train Loss 0.0021217512 Test MSE 0.0014239889931181296 Test RE 0.0019236050064826836\n",
      "41 Train Loss 0.0021175926 Test MSE 0.0014445043261086558 Test RE 0.001937412091870899\n",
      "42 Train Loss 0.0021138296 Test MSE 0.0014342241126528033 Test RE 0.0019305057183908116\n",
      "43 Train Loss 0.0021103153 Test MSE 0.0014485733188187746 Test RE 0.0019401388998687337\n",
      "44 Train Loss 0.0021061257 Test MSE 0.0014319914343323332 Test RE 0.0019290025094531203\n",
      "45 Train Loss 0.002102534 Test MSE 0.001433508328879485 Test RE 0.0019300239257942044\n",
      "46 Train Loss 0.0020989042 Test MSE 0.0013936674177742908 Test RE 0.001903014756657336\n",
      "47 Train Loss 0.002095342 Test MSE 0.0013628979243840485 Test RE 0.0018818900567123024\n",
      "48 Train Loss 0.0020910348 Test MSE 0.0012847656015039439 Test RE 0.0018271513900208211\n",
      "49 Train Loss 0.0020866056 Test MSE 0.0012152036208609223 Test RE 0.001776998699543899\n",
      "50 Train Loss 0.0020816908 Test MSE 0.001120370603910929 Test RE 0.0017062530284034023\n",
      "51 Train Loss 0.0020766174 Test MSE 0.0010184030664096406 Test RE 0.0016267560541499287\n",
      "52 Train Loss 0.0020697778 Test MSE 0.0009269383339432013 Test RE 0.0015519867315643967\n",
      "53 Train Loss 0.0020602879 Test MSE 0.0007774465856892318 Test RE 0.0014213396220339788\n",
      "54 Train Loss 0.0016697468 Test MSE 0.0011063403128050507 Test RE 0.0016955357502390057\n",
      "55 Train Loss 0.0012717155 Test MSE 0.00034894556437037495 Test RE 0.0009522290436075646\n",
      "56 Train Loss 0.0012250626 Test MSE 0.00021816526483124283 Test RE 0.00075293135862182\n",
      "57 Train Loss 0.0012195405 Test MSE 0.0003671154009201919 Test RE 0.0009767060650235505\n",
      "58 Train Loss 0.0012149512 Test MSE 0.00048732906741604404 Test RE 0.0011253140484653167\n",
      "59 Train Loss 0.0012106918 Test MSE 0.0005622898327453741 Test RE 0.0012087672863289585\n",
      "60 Train Loss 0.0012067585 Test MSE 0.0006132212456068622 Test RE 0.001262324987301881\n",
      "61 Train Loss 0.0012026401 Test MSE 0.0005999145185764519 Test RE 0.0012485538229487405\n",
      "62 Train Loss 0.0011982213 Test MSE 0.0005554777410144568 Test RE 0.0012014229199270943\n",
      "63 Train Loss 0.0011954214 Test MSE 0.00047995391955847345 Test RE 0.0011167664384830198\n",
      "64 Train Loss 0.001192069 Test MSE 0.0003459990133592453 Test RE 0.000948200134782353\n",
      "65 Train Loss 0.0011888444 Test MSE 0.00022549346052500827 Test RE 0.0007654724387352196\n",
      "66 Train Loss 0.0011843195 Test MSE 9.744756328812425e-05 Test RE 0.0005032086228751337\n",
      "67 Train Loss 0.0011811979 Test MSE 2.246982990740663e-05 Test RE 0.00024163646575086408\n",
      "68 Train Loss 0.0011782921 Test MSE 3.517817183314194e-08 Test RE 9.560909679676664e-06\n",
      "69 Train Loss 0.0011755738 Test MSE 2.617997240763314e-05 Test RE 0.00026082377406813373\n",
      "70 Train Loss 0.0011726213 Test MSE 9.130464295601194e-05 Test RE 0.00048708977594253165\n",
      "71 Train Loss 0.0011706316 Test MSE 0.00017680298908827615 Test RE 0.0006778090826007921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "73 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "74 Train Loss 0.0011683096 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "75 Train Loss 0.0011683096 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "76 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "77 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "78 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "79 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "80 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "81 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "82 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "83 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "84 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "85 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "86 Train Loss 0.0011683096 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "87 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "88 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "89 Train Loss 0.0011683096 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "90 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "91 Train Loss 0.0011683096 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "92 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "93 Train Loss 0.0011683096 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "94 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "95 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "96 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "97 Train Loss 0.0011683096 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "98 Train Loss 0.0011683096 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "99 Train Loss 0.0011683097 Test MSE 0.0002836364377047753 Test RE 0.0008585065118351516\n",
      "Training time: 28.94\n",
      "Training time: 28.94\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 4.6535034 Test MSE 382.8176228210283 Test RE 0.9973751504171384\n",
      "1 Train Loss 3.507279 Test MSE 380.72735235102147 Test RE 0.9946484767435256\n",
      "2 Train Loss 2.417998 Test MSE 381.0431964579873 Test RE 0.9950609618604344\n",
      "3 Train Loss 2.3832045 Test MSE 381.24670176057276 Test RE 0.9953266445359267\n",
      "4 Train Loss 2.3384087 Test MSE 374.39377780928015 Test RE 0.986340563004737\n",
      "5 Train Loss 2.293934 Test MSE 365.0114012681461 Test RE 0.9739032105872281\n",
      "6 Train Loss 2.2292273 Test MSE 352.80426221521134 Test RE 0.9574795215896681\n",
      "7 Train Loss 2.19748 Test MSE 350.4707532035859 Test RE 0.9543077999131219\n",
      "8 Train Loss 2.13034 Test MSE 339.3779078529377 Test RE 0.9390838387096739\n",
      "9 Train Loss 2.0584407 Test MSE 324.70469527007805 Test RE 0.9185585983862855\n",
      "10 Train Loss 1.9246863 Test MSE 299.2708912189918 Test RE 0.8818502066312636\n",
      "11 Train Loss 1.6955874 Test MSE 257.83349852594694 Test RE 0.8185255744950493\n",
      "12 Train Loss 1.6026372 Test MSE 247.5265514331935 Test RE 0.8019983562128411\n",
      "13 Train Loss 1.3434682 Test MSE 206.8648303224836 Test RE 0.7331720715384215\n",
      "14 Train Loss 1.2301123 Test MSE 183.17575274185592 Test RE 0.689916568866119\n",
      "15 Train Loss 0.91180986 Test MSE 128.66016122829066 Test RE 0.578208708245674\n",
      "16 Train Loss 0.61324686 Test MSE 86.25639569769855 Test RE 0.4734326831860885\n",
      "17 Train Loss 0.5269269 Test MSE 78.25529190442131 Test RE 0.4509407059752112\n",
      "18 Train Loss 0.4329471 Test MSE 55.30868468451597 Test RE 0.37910476903703827\n",
      "19 Train Loss 0.32881916 Test MSE 43.89252337375845 Test RE 0.3377208370936756\n",
      "20 Train Loss 0.29237482 Test MSE 39.1166335778051 Test RE 0.3188183563360007\n",
      "21 Train Loss 0.2614818 Test MSE 30.281307379994228 Test RE 0.2805109980317952\n",
      "22 Train Loss 0.17786103 Test MSE 13.026649026004243 Test RE 0.1839835263736299\n",
      "23 Train Loss 0.12935227 Test MSE 9.008548029582746 Test RE 0.15299948948542003\n",
      "24 Train Loss 0.10590484 Test MSE 7.455567710080164 Test RE 0.1391883677397002\n",
      "25 Train Loss 0.09311977 Test MSE 6.3803961815721335 Test RE 0.12876161219659502\n",
      "26 Train Loss 0.06444222 Test MSE 2.941593633907177 Test RE 0.08742867991284493\n",
      "27 Train Loss 0.033562675 Test MSE 1.4804299429575507 Test RE 0.06202353468187293\n",
      "28 Train Loss 0.024202053 Test MSE 0.8130581815220799 Test RE 0.04596459043293683\n",
      "29 Train Loss 0.01909526 Test MSE 0.16708656151090054 Test RE 0.020836911319036356\n",
      "30 Train Loss 0.0149302045 Test MSE 2.5974338260080934e-06 Test RE 8.215515673397209e-05\n",
      "31 Train Loss 0.014178024 Test MSE 0.0031243738019051757 Test RE 0.0028493386970589027\n",
      "32 Train Loss 0.012041057 Test MSE 0.04265088074990642 Test RE 0.01052753234586312\n",
      "33 Train Loss 0.010889177 Test MSE 0.11027176208216367 Test RE 0.016927574729554907\n",
      "34 Train Loss 0.008545666 Test MSE 0.0024065441417114143 Test RE 0.002500687933758425\n",
      "35 Train Loss 0.0080904225 Test MSE 0.0005577488545022109 Test RE 0.0012038764696319724\n",
      "36 Train Loss 0.0076881014 Test MSE 0.0010060954258184426 Test RE 0.001616896309497968\n",
      "37 Train Loss 0.0054091224 Test MSE 9.595463275217686e-05 Test RE 0.0004993390794918624\n",
      "38 Train Loss 0.0013588674 Test MSE 0.000217190625772215 Test RE 0.0007512476402289124\n",
      "39 Train Loss 0.000803168 Test MSE 0.001073185466896234 Test RE 0.0016699365641952258\n",
      "40 Train Loss 0.0006071757 Test MSE 0.004847189291478997 Test RE 0.0035490128560160994\n",
      "41 Train Loss 0.0006018771 Test MSE 0.004626571123039395 Test RE 0.003467306262140313\n",
      "42 Train Loss 0.0005965369 Test MSE 0.00429753250793593 Test RE 0.003341736234766\n",
      "43 Train Loss 0.0005905805 Test MSE 0.003869237524121273 Test RE 0.003170846934896351\n",
      "44 Train Loss 0.0005841517 Test MSE 0.003311667867114298 Test RE 0.0029334991695447493\n",
      "45 Train Loss 0.00057635107 Test MSE 0.002653836087004668 Test RE 0.0026260296795596024\n",
      "46 Train Loss 0.00056769006 Test MSE 0.0019952979262695867 Test RE 0.0022770179704498946\n",
      "47 Train Loss 0.00054774067 Test MSE 0.0007960250571398169 Test RE 0.0014382220798237783\n",
      "48 Train Loss 0.00037144558 Test MSE 0.001098500975827547 Test RE 0.0016895179337821252\n",
      "49 Train Loss 0.00031581084 Test MSE 6.066914639628977e-05 Test RE 0.00039705121086363725\n",
      "50 Train Loss 0.0003084975 Test MSE 4.001480419265226e-06 Test RE 0.00010197012004161448\n",
      "51 Train Loss 0.00030364902 Test MSE 8.84642776762162e-06 Test RE 0.00015161652914996373\n",
      "52 Train Loss 0.0002993466 Test MSE 4.800555583637824e-05 Test RE 0.00035318994720176587\n",
      "53 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "54 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "55 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "56 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "57 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "58 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "59 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "60 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "61 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "63 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "64 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "65 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "66 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "67 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "68 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "69 Train Loss 0.00029676282 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "70 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "71 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "72 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "73 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "74 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "75 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "76 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "77 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "78 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "79 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "80 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "81 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "82 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "83 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "84 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "85 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "86 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "87 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "88 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "89 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "90 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "91 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "92 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "93 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "94 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "95 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "96 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "97 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "98 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "99 Train Loss 0.0002967628 Test MSE 0.00010020776247345763 Test RE 0.0005102855436715048\n",
      "Training time: 30.75\n",
      "Training time: 30.75\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 3.0016615 Test MSE 386.6259007817617 Test RE 1.0023238282298308\n",
      "1 Train Loss 2.3869839 Test MSE 382.4433944258593 Test RE 0.9968875326297296\n",
      "2 Train Loss 2.365529 Test MSE 379.63256745189733 Test RE 0.9932173869396854\n",
      "3 Train Loss 2.3438282 Test MSE 375.9915074299172 Test RE 0.9884429320960344\n",
      "4 Train Loss 2.3266006 Test MSE 371.89902368596466 Test RE 0.9830488549199391\n",
      "5 Train Loss 2.2948225 Test MSE 366.46057841220085 Test RE 0.9758346025795559\n",
      "6 Train Loss 2.2679365 Test MSE 360.4530124708467 Test RE 0.9678028847069368\n",
      "7 Train Loss 2.200415 Test MSE 347.00768056356947 Test RE 0.9495812403796704\n",
      "8 Train Loss 2.1087165 Test MSE 334.474388904659 Test RE 0.9322749543115767\n",
      "9 Train Loss 2.0583804 Test MSE 323.4553953279189 Test RE 0.9167898202432923\n",
      "10 Train Loss 1.98094 Test MSE 311.01552111604576 Test RE 0.8989874189662026\n",
      "11 Train Loss 1.9029678 Test MSE 296.1363698410367 Test RE 0.8772198625773902\n",
      "12 Train Loss 1.7286384 Test MSE 271.39314665960427 Test RE 0.8397732194196051\n",
      "13 Train Loss 1.6105511 Test MSE 253.81225728831723 Test RE 0.812117517324725\n",
      "14 Train Loss 1.5055928 Test MSE 232.01332017243666 Test RE 0.7764599185367198\n",
      "15 Train Loss 1.3688384 Test MSE 203.0949222531131 Test RE 0.7264606838914395\n",
      "16 Train Loss 1.312128 Test MSE 198.82292260717398 Test RE 0.7187797101285792\n",
      "17 Train Loss 1.010894 Test MSE 151.84982505500702 Test RE 0.6281592081923436\n",
      "18 Train Loss 0.9222398 Test MSE 130.24603048230009 Test RE 0.5817613039186468\n",
      "19 Train Loss 0.7984632 Test MSE 106.3603283392808 Test RE 0.5257174796871948\n",
      "20 Train Loss 0.6658246 Test MSE 86.81162558395368 Test RE 0.4749539751250524\n",
      "21 Train Loss 0.5405065 Test MSE 70.83053037719569 Test RE 0.4290153481030629\n",
      "22 Train Loss 0.32169953 Test MSE 39.88864105917185 Test RE 0.3219490907982771\n",
      "23 Train Loss 0.2761056 Test MSE 30.043948647223225 Test RE 0.2794094484424226\n",
      "24 Train Loss 0.22827348 Test MSE 23.993720898427533 Test RE 0.2496958842350148\n",
      "25 Train Loss 0.1835202 Test MSE 16.74868126055048 Test RE 0.20861853504408612\n",
      "26 Train Loss 0.1321896 Test MSE 8.644429439336179 Test RE 0.14987553603474377\n",
      "27 Train Loss 0.09506941 Test MSE 3.8936091816248988 Test RE 0.10058628332402234\n",
      "28 Train Loss 0.052512188 Test MSE 3.0453185399212868 Test RE 0.08895675778884257\n",
      "29 Train Loss 0.027364332 Test MSE 1.76253308209831 Test RE 0.06767546074137601\n",
      "30 Train Loss 0.015839644 Test MSE 0.6285937024343504 Test RE 0.04041546643820177\n",
      "31 Train Loss 0.010918634 Test MSE 0.6937029208277556 Test RE 0.042457003938989456\n",
      "32 Train Loss 0.009401626 Test MSE 0.4244985598987693 Test RE 0.0332124331410337\n",
      "33 Train Loss 0.0074277064 Test MSE 0.11108492214479236 Test RE 0.016989873293755866\n",
      "34 Train Loss 0.0023119906 Test MSE 0.02825980105945847 Test RE 0.008569335229952662\n",
      "35 Train Loss 0.0018991425 Test MSE 0.03480307352504236 Test RE 0.009509799969738851\n",
      "36 Train Loss 0.0018900181 Test MSE 0.0336167826753247 Test RE 0.009346320203420088\n",
      "37 Train Loss 0.001880443 Test MSE 0.03199394971083778 Test RE 0.009117935410863904\n",
      "38 Train Loss 0.0017998423 Test MSE 0.014323686514044635 Test RE 0.00610084492964232\n",
      "39 Train Loss 0.001791186 Test MSE 0.012578820831120415 Test RE 0.005717188982715296\n",
      "40 Train Loss 0.001783154 Test MSE 0.011122824343173717 Test RE 0.005376134457002794\n",
      "41 Train Loss 0.0017757172 Test MSE 0.009990617316238683 Test RE 0.0050951707708898385\n",
      "42 Train Loss 0.0017688656 Test MSE 0.009073133393000473 Test RE 0.004855581303411849\n",
      "43 Train Loss 0.0017625976 Test MSE 0.008445819815911852 Test RE 0.004684718379054911\n",
      "44 Train Loss 0.0017563523 Test MSE 0.007930665986912822 Test RE 0.004539598168730263\n",
      "45 Train Loss 0.0017510871 Test MSE 0.007498305459788082 Test RE 0.004414120114087613\n",
      "46 Train Loss 0.0017450897 Test MSE 0.007062265517126704 Test RE 0.004283853430566008\n",
      "47 Train Loss 0.001738534 Test MSE 0.006677170859323669 Test RE 0.004165420280740677\n",
      "48 Train Loss 0.0017310069 Test MSE 0.005781060176500753 Test RE 0.003875844347038291\n",
      "49 Train Loss 0.0017241323 Test MSE 0.005236693495428892 Test RE 0.0036888513959870187\n",
      "50 Train Loss 0.0017158856 Test MSE 0.004799684767418304 Test RE 0.0035315791158855043\n",
      "51 Train Loss 0.0017081441 Test MSE 0.004313939488133297 Test RE 0.0033481091440845778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 Train Loss 0.001699432 Test MSE 0.003542214686939573 Test RE 0.0030338913395534972\n",
      "53 Train Loss 0.0016896997 Test MSE 0.0032169334541465285 Test RE 0.002891236521000049\n",
      "54 Train Loss 0.0016825882 Test MSE 0.0026359734045727653 Test RE 0.0026171769975358323\n",
      "55 Train Loss 0.0016763167 Test MSE 0.0022877192043914193 Test RE 0.0024381697653969034\n",
      "56 Train Loss 0.001670696 Test MSE 0.001964810260337413 Test RE 0.0022595548659016087\n",
      "57 Train Loss 0.0016657886 Test MSE 0.0016844791987935088 Test RE 0.0020921624768271046\n",
      "58 Train Loss 0.0016616588 Test MSE 0.0014606078311996992 Test RE 0.0019481814102338961\n",
      "59 Train Loss 0.0016577903 Test MSE 0.0012748943297782797 Test RE 0.0018201185560651626\n",
      "60 Train Loss 0.0016543239 Test MSE 0.0011171440814131102 Test RE 0.0017037943628986508\n",
      "61 Train Loss 0.0016509368 Test MSE 0.0009718765518300327 Test RE 0.0015891618643933444\n",
      "62 Train Loss 0.0016477337 Test MSE 0.0008398940766438769 Test RE 0.0014773208993142924\n",
      "63 Train Loss 0.0016439391 Test MSE 0.0007085968235909456 Test RE 0.0013569448063913017\n",
      "64 Train Loss 0.0016396008 Test MSE 0.0005878863699877143 Test RE 0.0012359738382384541\n",
      "65 Train Loss 0.0016336696 Test MSE 0.00044264761057509244 Test RE 0.0010724860319442063\n",
      "66 Train Loss 0.0016258874 Test MSE 0.00031892747311122705 Test RE 0.0009103503185103485\n",
      "67 Train Loss 0.0011813644 Test MSE 0.0011062674352520891 Test RE 0.001695479904614479\n",
      "68 Train Loss 0.00097211264 Test MSE 3.2225502740607023e-06 Test RE 9.1508710017373e-05\n",
      "69 Train Loss 0.00096584734 Test MSE 6.904360177268215e-07 Test RE 4.235691310750987e-05\n",
      "70 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "71 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "72 Train Loss 0.0009615078 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "73 Train Loss 0.0009615078 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "74 Train Loss 0.0009615078 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "75 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "76 Train Loss 0.0009615078 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "77 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "78 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "79 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "80 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "81 Train Loss 0.0009615079 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "82 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "83 Train Loss 0.0009615078 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "84 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "85 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "86 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "87 Train Loss 0.0009615078 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "88 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "89 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "90 Train Loss 0.0009615078 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "91 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "92 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "93 Train Loss 0.0009615078 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "94 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "95 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "96 Train Loss 0.0009615079 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "97 Train Loss 0.0009615079 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "98 Train Loss 0.00096150785 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "99 Train Loss 0.0009615078 Test MSE 5.275684049468902e-06 Test RE 0.00011708519183265489\n",
      "Training time: 30.74\n",
      "Training time: 30.74\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 4.5275993 Test MSE 376.437552868361 Test RE 0.9890290620627498\n",
      "1 Train Loss 2.9101045 Test MSE 383.78916148277233 Test RE 0.9986399491751381\n",
      "2 Train Loss 2.417865 Test MSE 386.20147736854256 Test RE 1.001773520408631\n",
      "3 Train Loss 2.3906286 Test MSE 383.9385784322307 Test RE 0.9988343257099981\n",
      "4 Train Loss 2.3583524 Test MSE 379.73562929429534 Test RE 0.993352196063307\n",
      "5 Train Loss 2.3158653 Test MSE 370.353964782622 Test RE 0.9810046855170312\n",
      "6 Train Loss 2.2704358 Test MSE 361.99591036028636 Test RE 0.9698719845288225\n",
      "7 Train Loss 2.1807628 Test MSE 346.40488553816976 Test RE 0.948756112176935\n",
      "8 Train Loss 2.1112015 Test MSE 331.6929012756435 Test RE 0.9283904642768681\n",
      "9 Train Loss 1.8965017 Test MSE 297.9672518170427 Test RE 0.8799274179653158\n",
      "10 Train Loss 1.837358 Test MSE 292.37526893824605 Test RE 0.8716314654758138\n",
      "11 Train Loss 1.6866465 Test MSE 259.9950155190588 Test RE 0.8219494203513608\n",
      "12 Train Loss 1.5155609 Test MSE 234.93210760518755 Test RE 0.7813286868649858\n",
      "13 Train Loss 1.3440953 Test MSE 201.81406311453452 Test RE 0.7241662751323048\n",
      "14 Train Loss 1.272655 Test MSE 187.9359197449387 Test RE 0.6988234652323009\n",
      "15 Train Loss 0.88849807 Test MSE 124.95720959946266 Test RE 0.5698272864096913\n",
      "16 Train Loss 0.6114154 Test MSE 82.6165916733406 Test RE 0.46333618629271234\n",
      "17 Train Loss 0.38034764 Test MSE 45.277506229222396 Test RE 0.3430076704815137\n",
      "18 Train Loss 0.2826567 Test MSE 26.206865863706184 Test RE 0.2609577056881087\n",
      "19 Train Loss 0.1801784 Test MSE 16.556502212089857 Test RE 0.20741820820836399\n",
      "20 Train Loss 0.14796275 Test MSE 12.772985242150341 Test RE 0.1821833936595783\n",
      "21 Train Loss 0.078348905 Test MSE 3.993547846950888 Test RE 0.10186899663856532\n",
      "22 Train Loss 0.04259397 Test MSE 0.643964578686596 Test RE 0.0409066177301455\n",
      "23 Train Loss 0.025698217 Test MSE 0.011928985513497924 Test RE 0.00556755271991122\n",
      "24 Train Loss 0.020231219 Test MSE 0.02053808014960828 Test RE 0.007305374842390048\n",
      "25 Train Loss 0.0127229085 Test MSE 0.12821560642269852 Test RE 0.018252948477664524\n",
      "26 Train Loss 0.007995748 Test MSE 0.00011238240712798943 Test RE 0.0005403955294404769\n",
      "27 Train Loss 0.00603002 Test MSE 0.011888000548868292 Test RE 0.005557980142020668\n",
      "28 Train Loss 0.004800629 Test MSE 0.006381621982043162 Test RE 0.0040721908152632795\n",
      "29 Train Loss 0.0040106634 Test MSE 0.011966184475340732 Test RE 0.005576226800918607\n",
      "30 Train Loss 0.0036570665 Test MSE 0.00022132284452554447 Test RE 0.0007583604998967208\n",
      "31 Train Loss 0.0036480336 Test MSE 0.00010999392536798439 Test RE 0.0005346221314724918\n",
      "32 Train Loss 0.0036398282 Test MSE 3.936766920978853e-05 Test RE 0.00031983974752566126\n",
      "33 Train Loss 0.0036312107 Test MSE 8.256156249021292e-06 Test RE 0.00014647096210213505\n",
      "34 Train Loss 0.003621774 Test MSE 1.2758768793922942e-06 Test RE 5.7579377624911245e-05\n",
      "35 Train Loss 0.0033941043 Test MSE 0.0004987302605693987 Test RE 0.0011384014559007252\n",
      "36 Train Loss 0.0030073945 Test MSE 0.00512417211613205 Test RE 0.0036490048194299817\n",
      "37 Train Loss 0.0019495923 Test MSE 6.952117287026936e-06 Test RE 0.00013440676481490837\n",
      "38 Train Loss 0.0015306604 Test MSE 0.0012828853663364283 Test RE 0.0018258138961784688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Train Loss 0.0011764545 Test MSE 8.531683631419713e-05 Test RE 0.0004708471585472278\n",
      "40 Train Loss 0.0010748565 Test MSE 0.0005585075665315432 Test RE 0.0012046950146971077\n",
      "41 Train Loss 0.0010654134 Test MSE 0.0006843785254051432 Test RE 0.0013335545004323167\n",
      "42 Train Loss 0.0010579512 Test MSE 0.0007566743115070996 Test RE 0.0014022229719120424\n",
      "43 Train Loss 0.0010494319 Test MSE 0.0006967826324223465 Test RE 0.0013455853221134445\n",
      "44 Train Loss 0.0010420653 Test MSE 0.0006533225341043995 Test RE 0.0013029459536414282\n",
      "45 Train Loss 0.0010352127 Test MSE 0.0006241595667025521 Test RE 0.0012735335727337433\n",
      "46 Train Loss 0.0010304648 Test MSE 0.0005741114577783043 Test RE 0.0012214078016819058\n",
      "47 Train Loss 0.0010257656 Test MSE 0.0005015195592702783 Test RE 0.00114158044318295\n",
      "48 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "49 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "50 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "51 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "52 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "53 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "54 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "55 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "56 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "57 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "58 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "59 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "60 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "61 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "62 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "63 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "64 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "65 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "66 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "67 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "68 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "69 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "70 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "71 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "72 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "73 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "74 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "75 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "76 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "77 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "78 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "79 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "80 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "81 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "82 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "83 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "84 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "85 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "86 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "87 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "88 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "89 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "90 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "91 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "92 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "93 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "94 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "95 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "96 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "97 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "98 Train Loss 0.0010231987 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "99 Train Loss 0.0010231988 Test MSE 0.0004413160311583185 Test RE 0.0010708716823830285\n",
      "Training time: 29.17\n",
      "Training time: 29.17\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 3.3116913 Test MSE 387.03823999324857 Test RE 1.0028581784618413\n",
      "1 Train Loss 2.45448 Test MSE 381.67900353818294 Test RE 0.9958907931162034\n",
      "2 Train Loss 2.3766768 Test MSE 382.6826441312117 Test RE 0.9971993013286706\n",
      "3 Train Loss 2.368923 Test MSE 380.6075432492921 Test RE 0.994491964062786\n",
      "4 Train Loss 2.319558 Test MSE 369.54733629853143 Test RE 0.9799357923330977\n",
      "5 Train Loss 2.2615404 Test MSE 358.2958499504568 Test RE 0.9649025885003222\n",
      "6 Train Loss 2.189945 Test MSE 351.64094738768 Test RE 0.9558996514664143\n",
      "7 Train Loss 2.132112 Test MSE 338.74783557936075 Test RE 0.9382117053786662\n",
      "8 Train Loss 2.0628152 Test MSE 329.5174199231707 Test RE 0.9253409285526804\n",
      "9 Train Loss 1.9812491 Test MSE 309.4528362096921 Test RE 0.8967261119650297\n",
      "10 Train Loss 1.862166 Test MSE 290.6989902928195 Test RE 0.8691292061804784\n",
      "11 Train Loss 1.598791 Test MSE 249.03733893316746 Test RE 0.804442146321502\n",
      "12 Train Loss 1.5284005 Test MSE 231.27921804502867 Test RE 0.7752305656909667\n",
      "13 Train Loss 1.4866186 Test MSE 230.8601834578841 Test RE 0.7745279610701524\n",
      "14 Train Loss 1.4194529 Test MSE 220.18492408685262 Test RE 0.7564084511058605\n",
      "15 Train Loss 1.2642965 Test MSE 186.49885191244815 Test RE 0.6961465318328897\n",
      "16 Train Loss 1.1235405 Test MSE 164.93423802286677 Test RE 0.6546632945730774\n",
      "17 Train Loss 1.016374 Test MSE 150.906572195984 Test RE 0.6262051856140288\n",
      "18 Train Loss 0.96363246 Test MSE 142.26009734367182 Test RE 0.6080007756184783\n",
      "19 Train Loss 0.83389485 Test MSE 119.59886206266665 Test RE 0.5574759115064726\n",
      "20 Train Loss 0.64890945 Test MSE 89.01221251600153 Test RE 0.4809361036647216\n",
      "21 Train Loss 0.48889768 Test MSE 56.13840702457874 Test RE 0.3819377849073515\n",
      "22 Train Loss 0.24202496 Test MSE 20.611347432556116 Test RE 0.23142793196569808\n",
      "23 Train Loss 0.17236903 Test MSE 12.942259648346214 Test RE 0.18338661604665712\n",
      "24 Train Loss 0.15884173 Test MSE 7.568994309258337 Test RE 0.1402431545231082\n",
      "25 Train Loss 0.09856069 Test MSE 2.352254715780225 Test RE 0.07818163643025793\n",
      "26 Train Loss 0.06499033 Test MSE 1.3169093839399835 Test RE 0.0584979340724019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 Train Loss 0.033036713 Test MSE 0.3832390039372755 Test RE 0.0315571252756306\n",
      "28 Train Loss 0.02038151 Test MSE 0.07133879410271286 Test RE 0.013615245118280446\n",
      "29 Train Loss 0.010702049 Test MSE 0.11672686729955956 Test RE 0.017415983220081397\n",
      "30 Train Loss 0.00983985 Test MSE 0.17805922675018368 Test RE 0.021510218603956355\n",
      "31 Train Loss 0.008465944 Test MSE 0.03617329513289584 Test RE 0.009695196560595556\n",
      "32 Train Loss 0.0059551504 Test MSE 0.016596902777447884 Test RE 0.006567137470482118\n",
      "33 Train Loss 0.0042876187 Test MSE 0.0003021414423444374 Test RE 0.0008860693855505024\n",
      "34 Train Loss 0.0037858945 Test MSE 0.017725777356827635 Test RE 0.006786802756655862\n",
      "35 Train Loss 0.0033020682 Test MSE 0.00023604890461503215 Test RE 0.0007831835865738392\n",
      "36 Train Loss 0.0030429333 Test MSE 0.0007819688884567433 Test RE 0.0014254674991148212\n",
      "37 Train Loss 0.0027188722 Test MSE 0.0021125902886300297 Test RE 0.0023429888511170083\n",
      "38 Train Loss 0.0026563045 Test MSE 0.0006739995413350977 Test RE 0.0013234038173399975\n",
      "39 Train Loss 0.0025752236 Test MSE 6.055342980423263e-05 Test RE 0.0003966723746264019\n",
      "40 Train Loss 0.0025658829 Test MSE 0.00011563439855105781 Test RE 0.0005481584401470965\n",
      "41 Train Loss 0.0025571513 Test MSE 0.00017340233702455393 Test RE 0.000671258898147612\n",
      "42 Train Loss 0.0025487079 Test MSE 0.00019899750375736692 Test RE 0.0007190952116125664\n",
      "43 Train Loss 0.0025418059 Test MSE 0.00020061703563595292 Test RE 0.0007220154434860176\n",
      "44 Train Loss 0.002535178 Test MSE 0.0001784932073313193 Test RE 0.0006810412684088777\n",
      "45 Train Loss 0.0025292013 Test MSE 0.00012689652271486736 Test RE 0.00057423206965922\n",
      "46 Train Loss 0.0025251457 Test MSE 9.842981689278291e-05 Test RE 0.0005057383893779915\n",
      "47 Train Loss 0.0025215144 Test MSE 5.920580925098835e-05 Test RE 0.0003922335542486093\n",
      "48 Train Loss 0.0025170376 Test MSE 2.452631260898877e-05 Test RE 0.0002524519398848962\n",
      "49 Train Loss 0.00251162 Test MSE 8.557255345311935e-07 Test RE 4.7155225737634204e-05\n",
      "50 Train Loss 0.0025052135 Test MSE 1.13004914717369e-05 Test RE 0.00017136070824809964\n",
      "51 Train Loss 0.0022775508 Test MSE 0.002386761070098396 Test RE 0.0024903882310153246\n",
      "52 Train Loss 0.0020282057 Test MSE 5.572590906196045e-05 Test RE 0.00038053198229885915\n",
      "53 Train Loss 0.0011468867 Test MSE 0.01250190272200288 Test RE 0.005699682186999592\n",
      "54 Train Loss 0.00071329926 Test MSE 1.9509374850913702e-05 Test RE 0.00022515638082647063\n",
      "55 Train Loss 0.00043236147 Test MSE 6.039466773258274e-05 Test RE 0.0003961520253981548\n",
      "56 Train Loss 0.00038933527 Test MSE 0.00024176306511174782 Test RE 0.0007926063713376051\n",
      "57 Train Loss 0.00038233414 Test MSE 0.0002867127167745609 Test RE 0.0008631495739495192\n",
      "58 Train Loss 0.00037746227 Test MSE 0.00028595041560701916 Test RE 0.000862001355148931\n",
      "59 Train Loss 0.00037446772 Test MSE 0.00026807592351300353 Test RE 0.0008346251896520202\n",
      "60 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "61 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "62 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "63 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "64 Train Loss 0.00037198156 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "65 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "66 Train Loss 0.00037198156 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "67 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "68 Train Loss 0.00037198156 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "69 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "70 Train Loss 0.00037198156 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "71 Train Loss 0.00037198156 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "72 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "73 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "74 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "75 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "76 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "77 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "78 Train Loss 0.00037198156 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "79 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "80 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "81 Train Loss 0.00037198156 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "82 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "83 Train Loss 0.00037198156 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "84 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "85 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "86 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "87 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "88 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "89 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "90 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "91 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "92 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "93 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "94 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "95 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "96 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "97 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "98 Train Loss 0.00037198156 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "99 Train Loss 0.0003719816 Test MSE 0.00023778736668462436 Test RE 0.0007860623063009975\n",
      "Training time: 33.29\n",
      "Training time: 33.29\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 4.325357 Test MSE 383.410643919823 Test RE 0.9981473662071684\n",
      "1 Train Loss 2.5120623 Test MSE 384.2834047128461 Test RE 0.9992827659621498\n",
      "2 Train Loss 2.3939257 Test MSE 383.3049755486993 Test RE 0.9980098115042765\n",
      "3 Train Loss 2.3697937 Test MSE 381.66334850940666 Test RE 0.9958703690704072\n",
      "4 Train Loss 2.362521 Test MSE 378.9905168912324 Test RE 0.9923771460231203\n",
      "5 Train Loss 2.3192854 Test MSE 369.63531641026367 Test RE 0.9800524346698629\n",
      "6 Train Loss 2.251377 Test MSE 358.5874831811014 Test RE 0.9652951975996115\n",
      "7 Train Loss 2.101353 Test MSE 328.1108953669471 Test RE 0.923363936608105\n",
      "8 Train Loss 2.0506537 Test MSE 320.94498826826975 Test RE 0.9132251885754581\n",
      "9 Train Loss 1.8636837 Test MSE 289.13079036616466 Test RE 0.8667817410371637\n",
      "10 Train Loss 1.7555761 Test MSE 273.2392260930625 Test RE 0.8426245453679799\n",
      "11 Train Loss 1.4795203 Test MSE 230.6585352874814 Test RE 0.774189625841167\n",
      "12 Train Loss 1.2545573 Test MSE 188.057642696274 Test RE 0.6990497367342507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Train Loss 1.1454612 Test MSE 167.1921211166972 Test RE 0.6591291007811737\n",
      "14 Train Loss 1.0113294 Test MSE 151.56864297237388 Test RE 0.6275773538644549\n",
      "15 Train Loss 0.908794 Test MSE 133.52914803455084 Test RE 0.58904791291048\n",
      "16 Train Loss 0.8004043 Test MSE 102.06707828280865 Test RE 0.5149978596058987\n",
      "17 Train Loss 0.6165535 Test MSE 69.55125924338276 Test RE 0.4251234693128285\n",
      "18 Train Loss 0.47557086 Test MSE 57.16911590007281 Test RE 0.3854280524617998\n",
      "19 Train Loss 0.35445285 Test MSE 39.95688070761546 Test RE 0.3222243609508064\n",
      "20 Train Loss 0.21782956 Test MSE 22.229511150815828 Test RE 0.24034082003998972\n",
      "21 Train Loss 0.13624588 Test MSE 8.604549962771461 Test RE 0.14952942485915202\n",
      "22 Train Loss 0.06695484 Test MSE 4.2883917459732945 Test RE 0.105562534353467\n",
      "23 Train Loss 0.044596396 Test MSE 1.3038216259671496 Test RE 0.05820652506609801\n",
      "24 Train Loss 0.032436747 Test MSE 0.04555454116436019 Test RE 0.010879988113002096\n",
      "25 Train Loss 0.029035434 Test MSE 0.0073279990164190146 Test RE 0.0043637039998359346\n",
      "26 Train Loss 0.023101475 Test MSE 0.03195416922485941 Test RE 0.009112265140028521\n",
      "27 Train Loss 0.012233591 Test MSE 0.25372684070074697 Test RE 0.025677089120339672\n",
      "28 Train Loss 0.010269556 Test MSE 0.04672524476384963 Test RE 0.011018903382869372\n",
      "29 Train Loss 0.008681533 Test MSE 0.005595376445918442 Test RE 0.0038130915951995205\n",
      "30 Train Loss 0.008069792 Test MSE 0.0020693522133481256 Test RE 0.0023188880925106604\n",
      "31 Train Loss 0.0059511014 Test MSE 0.01900637898526073 Test RE 0.007027684804748583\n",
      "32 Train Loss 0.004397056 Test MSE 0.00153600146507429 Test RE 0.001997829388924667\n",
      "33 Train Loss 0.0036883627 Test MSE 0.002332069153085297 Test RE 0.002461689622004752\n",
      "34 Train Loss 0.0036694584 Test MSE 0.002035769138201195 Test RE 0.0022999947551566916\n",
      "35 Train Loss 0.0036628807 Test MSE 0.001999845106822723 Test RE 0.0022796110968563146\n",
      "36 Train Loss 0.0036557373 Test MSE 0.00196169729664018 Test RE 0.002257764183951219\n",
      "37 Train Loss 0.0036472976 Test MSE 0.0020481372013682196 Test RE 0.0023069708421955414\n",
      "38 Train Loss 0.0036376824 Test MSE 0.0021853288905639106 Test RE 0.0023829832305762256\n",
      "39 Train Loss 0.00345518 Test MSE 0.011130607200785674 Test RE 0.005378015021277146\n",
      "40 Train Loss 0.0032436298 Test MSE 0.013967147729319862 Test RE 0.006024436704782406\n",
      "41 Train Loss 0.0032225852 Test MSE 0.01342939767324095 Test RE 0.005907324794349156\n",
      "42 Train Loss 0.003058138 Test MSE 0.01372246551338206 Test RE 0.005971434274438396\n",
      "43 Train Loss 0.002441186 Test MSE 0.0014048193496392061 Test RE 0.0019106134151303936\n",
      "44 Train Loss 0.0022557317 Test MSE 0.0008423946261033663 Test RE 0.0014795184195831015\n",
      "45 Train Loss 0.0022488993 Test MSE 0.0008323667435732834 Test RE 0.0014706859469302278\n",
      "46 Train Loss 0.0022436015 Test MSE 0.0008263252020814345 Test RE 0.001465338909805087\n",
      "47 Train Loss 0.0022397817 Test MSE 0.000820122111830988 Test RE 0.0014598285172813922\n",
      "48 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "49 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "50 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "51 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "52 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "53 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "54 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "55 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "56 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "57 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "58 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "59 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "60 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "61 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "62 Train Loss 0.0022372142 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "63 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "64 Train Loss 0.0022372142 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "65 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "66 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "67 Train Loss 0.0022372142 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "68 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "69 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "70 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "71 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "72 Train Loss 0.0022372142 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "73 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "74 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "75 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "76 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "77 Train Loss 0.0022372142 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "78 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "79 Train Loss 0.0022372142 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "80 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "81 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "82 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "83 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "84 Train Loss 0.0022372142 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "85 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "86 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "87 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "88 Train Loss 0.0022372142 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "89 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "90 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "91 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "92 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "93 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "94 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "95 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "96 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "97 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "98 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "99 Train Loss 0.0022372145 Test MSE 0.0008182810682469604 Test RE 0.0014581890553676995\n",
      "Training time: 27.81\n",
      "Training time: 27.81\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 4.5134106 Test MSE 386.13077311366885 Test RE 1.001681815823636\n",
      "1 Train Loss 2.9902544 Test MSE 383.1364470701348 Test RE 0.9977903888808191\n",
      "2 Train Loss 2.3957307 Test MSE 383.46006804107225 Test RE 0.9982116979719683\n",
      "3 Train Loss 2.3792596 Test MSE 383.33978294029157 Test RE 0.9980551244168965\n",
      "4 Train Loss 2.3766868 Test MSE 382.60102481606083 Test RE 0.9970929533330815\n",
      "5 Train Loss 2.3622336 Test MSE 377.63482576488315 Test RE 0.9906006343292518\n",
      "6 Train Loss 2.3453226 Test MSE 374.7263979197447 Test RE 0.9867786096431648\n",
      "7 Train Loss 2.2620313 Test MSE 357.30329393358755 Test RE 0.9635651685127941\n",
      "8 Train Loss 2.142282 Test MSE 336.5399768942706 Test RE 0.9351492132445057\n",
      "9 Train Loss 2.0145466 Test MSE 320.15181891848783 Test RE 0.912096038197248\n",
      "10 Train Loss 1.9018601 Test MSE 297.377765916376 Test RE 0.8790565813156962\n",
      "11 Train Loss 1.7330425 Test MSE 258.44984449493654 Test RE 0.8195033252619653\n",
      "12 Train Loss 1.577172 Test MSE 231.41989288355336 Test RE 0.7754662955966991\n",
      "13 Train Loss 1.2508267 Test MSE 164.90833900615732 Test RE 0.6546118928706129\n",
      "14 Train Loss 0.93710494 Test MSE 121.6696643378525 Test RE 0.5622814258913852\n",
      "15 Train Loss 0.78923756 Test MSE 103.15227483994137 Test RE 0.5177283984105038\n",
      "16 Train Loss 0.66887635 Test MSE 96.31410216515573 Test RE 0.5002735280161893\n",
      "17 Train Loss 0.5188033 Test MSE 65.66785421865089 Test RE 0.41308459111728224\n",
      "18 Train Loss 0.3621712 Test MSE 44.90272460733728 Test RE 0.34158510869829184\n",
      "19 Train Loss 0.27944475 Test MSE 37.26265570599157 Test RE 0.3111712643015109\n",
      "20 Train Loss 0.16303068 Test MSE 19.95162795508207 Test RE 0.22769408640745054\n",
      "21 Train Loss 0.09766354 Test MSE 10.704373203988883 Test RE 0.16677970588707416\n",
      "22 Train Loss 0.061551586 Test MSE 5.769036712751299 Test RE 0.12243743813107998\n",
      "23 Train Loss 0.050306506 Test MSE 3.8881752561983927 Test RE 0.10051606965188144\n",
      "24 Train Loss 0.033751816 Test MSE 1.8257734326912323 Test RE 0.06887887166837385\n",
      "25 Train Loss 0.019978411 Test MSE 0.9818491498668024 Test RE 0.050510883709013354\n",
      "26 Train Loss 0.013751969 Test MSE 0.12599578622619179 Test RE 0.018094250271515115\n",
      "27 Train Loss 0.010353723 Test MSE 8.31150151136836e-05 Test RE 0.0004647317313528603\n",
      "28 Train Loss 0.007116828 Test MSE 0.02146531318324371 Test RE 0.00746846237081214\n",
      "29 Train Loss 0.0049615907 Test MSE 0.012772655860235814 Test RE 0.005761070475319634\n",
      "30 Train Loss 0.0046696262 Test MSE 0.03141379684727434 Test RE 0.00903488851383364\n",
      "31 Train Loss 0.004436217 Test MSE 0.09196804544070687 Test RE 0.01545898802913245\n",
      "32 Train Loss 0.0038067093 Test MSE 0.08507733284736377 Test RE 0.014868580747713644\n",
      "33 Train Loss 0.0026840083 Test MSE 0.04497296374699853 Test RE 0.01081031470013287\n",
      "34 Train Loss 0.002510261 Test MSE 0.03830656292170494 Test RE 0.009976981702479742\n",
      "35 Train Loss 0.0025026454 Test MSE 0.0382737550821053 Test RE 0.009972708370381722\n",
      "36 Train Loss 0.002495372 Test MSE 0.03831631638915313 Test RE 0.0099782517716983\n",
      "37 Train Loss 0.002487929 Test MSE 0.03849742770588391 Test RE 0.010001806278079683\n",
      "38 Train Loss 0.0024820294 Test MSE 0.03866511812723581 Test RE 0.010023565974052266\n",
      "39 Train Loss 0.0024766065 Test MSE 0.03895198140595254 Test RE 0.010060680556532739\n",
      "40 Train Loss 0.0024715257 Test MSE 0.03922791853198576 Test RE 0.010096252766125517\n",
      "41 Train Loss 0.0024641105 Test MSE 0.039494464827693775 Test RE 0.010130495761041537\n",
      "42 Train Loss 0.002459066 Test MSE 0.03988454712063027 Test RE 0.010180401706547397\n",
      "43 Train Loss 0.0024503358 Test MSE 0.0400115828270121 Test RE 0.010196601544009263\n",
      "44 Train Loss 0.0024430025 Test MSE 0.040568351139542304 Test RE 0.010267300212264827\n",
      "45 Train Loss 0.002435229 Test MSE 0.04070028414176721 Test RE 0.010283981888561037\n",
      "46 Train Loss 0.0020262944 Test MSE 0.046933275417205246 Test RE 0.01104340538310955\n",
      "47 Train Loss 0.00091680157 Test MSE 0.016820227539346155 Test RE 0.006611172907705564\n",
      "48 Train Loss 0.0004951026 Test MSE 0.0008936888442844774 Test RE 0.0015238974801086444\n",
      "49 Train Loss 0.00047113627 Test MSE 0.0005538225345359175 Test RE 0.0011996315910679932\n",
      "50 Train Loss 0.0004663584 Test MSE 0.0004948716300364563 Test RE 0.0011339890506235181\n",
      "51 Train Loss 0.00046324314 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "52 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "53 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "54 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "55 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "56 Train Loss 0.00046324314 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "57 Train Loss 0.00046324314 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "58 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "59 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "60 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "61 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "62 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "63 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "64 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "65 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "66 Train Loss 0.00046324314 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "67 Train Loss 0.00046324314 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "68 Train Loss 0.00046324314 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "69 Train Loss 0.00046324308 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "70 Train Loss 0.00046324314 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "71 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "72 Train Loss 0.00046324314 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "73 Train Loss 0.00046324314 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "74 Train Loss 0.00046324314 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "75 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "76 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "77 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "78 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "79 Train Loss 0.00046324308 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "80 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "81 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "82 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "83 Train Loss 0.00046324314 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "84 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "85 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "86 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "87 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "88 Train Loss 0.00046324314 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "89 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "90 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "91 Train Loss 0.00046324314 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "92 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "93 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "94 Train Loss 0.00046324308 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "95 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "96 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "98 Train Loss 0.0004632431 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "99 Train Loss 0.00046324314 Test MSE 0.0004536837683119849 Test RE 0.0010857734137339583\n",
      "Training time: 27.16\n",
      "Training time: 27.16\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 3.0932853 Test MSE 381.860613044024 Test RE 0.9961276960159988\n",
      "1 Train Loss 2.4086845 Test MSE 384.13636410309476 Test RE 0.9990915669619164\n",
      "2 Train Loss 2.3791702 Test MSE 382.9381162418836 Test RE 0.9975321020326229\n",
      "3 Train Loss 2.3786314 Test MSE 382.7453632698695 Test RE 0.9972810151453323\n",
      "4 Train Loss 2.3724198 Test MSE 380.08313109232154 Test RE 0.9938066079108792\n",
      "5 Train Loss 2.354477 Test MSE 376.8677015948314 Test RE 0.9895939739915187\n",
      "6 Train Loss 2.3251743 Test MSE 367.84660440194455 Test RE 0.9776782601618902\n",
      "7 Train Loss 2.2418063 Test MSE 355.8178761833359 Test RE 0.9615601667220139\n",
      "8 Train Loss 2.1942058 Test MSE 351.6744379407953 Test RE 0.9559451706900349\n",
      "9 Train Loss 2.1476235 Test MSE 338.97487495935843 Test RE 0.938526062109286\n",
      "10 Train Loss 2.047821 Test MSE 327.3947788148916 Test RE 0.9223557448709537\n",
      "11 Train Loss 1.9988261 Test MSE 313.77240494104404 Test RE 0.9029630016736229\n",
      "12 Train Loss 1.9578217 Test MSE 307.16184959691856 Test RE 0.8934005583536224\n",
      "13 Train Loss 1.8733889 Test MSE 289.85328369815113 Test RE 0.8678640423956262\n",
      "14 Train Loss 1.7550633 Test MSE 269.7691510095227 Test RE 0.8372568803005901\n",
      "15 Train Loss 1.6245216 Test MSE 247.73318547132786 Test RE 0.8023330386676767\n",
      "16 Train Loss 1.4902574 Test MSE 232.09999678274843 Test RE 0.7766049417389627\n",
      "17 Train Loss 1.2948015 Test MSE 184.09480573455934 Test RE 0.691645172495576\n",
      "18 Train Loss 0.8866793 Test MSE 127.63428811852692 Test RE 0.5758989182265676\n",
      "19 Train Loss 0.7568979 Test MSE 92.51216913663397 Test RE 0.4903001369214595\n",
      "20 Train Loss 0.63988537 Test MSE 83.7486648526025 Test RE 0.4664998716381193\n",
      "21 Train Loss 0.57509404 Test MSE 80.62188938039512 Test RE 0.45770859556318755\n",
      "22 Train Loss 0.52293825 Test MSE 73.80160226317432 Test RE 0.437920703607799\n",
      "23 Train Loss 0.49144793 Test MSE 65.3868017643103 Test RE 0.41219966097296246\n",
      "24 Train Loss 0.46162862 Test MSE 60.063868097162164 Test RE 0.39506561421779834\n",
      "25 Train Loss 0.33753985 Test MSE 47.83294260969108 Test RE 0.35255439411641765\n",
      "26 Train Loss 0.30235934 Test MSE 40.397753873768835 Test RE 0.3239971514675619\n",
      "27 Train Loss 0.2375345 Test MSE 25.80163798491589 Test RE 0.258932295377577\n",
      "28 Train Loss 0.20781699 Test MSE 16.8833335394278 Test RE 0.2094554584620863\n",
      "29 Train Loss 0.194634 Test MSE 11.153219626217368 Test RE 0.1702404308242262\n",
      "30 Train Loss 0.18195699 Test MSE 7.321304033889471 Test RE 0.1379293862386961\n",
      "31 Train Loss 0.1672114 Test MSE 7.540621377664249 Test RE 0.13998005183939824\n",
      "32 Train Loss 0.13790977 Test MSE 12.501318201052195 Test RE 0.18023556293518478\n",
      "33 Train Loss 0.0989321 Test MSE 11.014523193109465 Test RE 0.16917860260000964\n",
      "34 Train Loss 0.051855747 Test MSE 3.1065338669293117 Test RE 0.08984638934680252\n",
      "35 Train Loss 0.024027787 Test MSE 0.5961593830298342 Test RE 0.03935897450610696\n",
      "36 Train Loss 0.013751953 Test MSE 0.43554370433362677 Test RE 0.03364174021167408\n",
      "37 Train Loss 0.011155008 Test MSE 0.17048904518124686 Test RE 0.021047999355942645\n",
      "38 Train Loss 0.009153347 Test MSE 4.073370237292977e-05 Test RE 0.0003253415455280606\n",
      "39 Train Loss 0.008199596 Test MSE 0.003358614400133658 Test RE 0.0029542187851018227\n",
      "40 Train Loss 0.00794875 Test MSE 0.0028527217346100892 Test RE 0.0027226529601558805\n",
      "41 Train Loss 0.007942731 Test MSE 0.0025753867089583654 Test RE 0.0025869248195385906\n",
      "42 Train Loss 0.007938294 Test MSE 0.002303735424485682 Test RE 0.0024466896384724574\n",
      "43 Train Loss 0.007935086 Test MSE 0.002067601457467366 Test RE 0.002317906948268429\n",
      "44 Train Loss 0.007933831 Test MSE 0.0018403813174237382 Test RE 0.002186837388648326\n",
      "45 Train Loss 0.007931725 Test MSE 0.0016081015670902656 Test RE 0.0020441808717984915\n",
      "46 Train Loss 0.007926863 Test MSE 0.0013704265080723014 Test RE 0.0018870806343009394\n",
      "47 Train Loss 0.007924485 Test MSE 0.001130548452630214 Test RE 0.001713985614192525\n",
      "48 Train Loss 0.007919863 Test MSE 0.0008729743280255857 Test RE 0.0015061329810092245\n",
      "49 Train Loss 0.007912394 Test MSE 0.0005838834723188627 Test RE 0.0012317587997188499\n",
      "50 Train Loss 0.007904061 Test MSE 0.00030765660956183407 Test RE 0.0008941197898883848\n",
      "51 Train Loss 0.0069262 Test MSE 0.03729222051512051 Test RE 0.009844002250905632\n",
      "52 Train Loss 0.004387613 Test MSE 0.00021771320298637938 Test RE 0.0007521508768072572\n",
      "53 Train Loss 0.0029768012 Test MSE 0.0015663773305645428 Test RE 0.002017487149376691\n",
      "54 Train Loss 0.0021763537 Test MSE 0.003841331879629478 Test RE 0.0031593918812451898\n",
      "55 Train Loss 0.0019533262 Test MSE 0.00022727438535138404 Test RE 0.0007684893061495659\n",
      "56 Train Loss 0.0019489505 Test MSE 0.00021299868815297454 Test RE 0.0007439625029187311\n",
      "57 Train Loss 0.0019421913 Test MSE 0.00044684833154297706 Test RE 0.001077562955736565\n",
      "58 Train Loss 0.0019390862 Test MSE 0.000711027609132222 Test RE 0.001359270259869469\n",
      "59 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "60 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "61 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "62 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "63 Train Loss 0.0019361972 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "64 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "65 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "66 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "67 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "68 Train Loss 0.0019361972 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "69 Train Loss 0.0019361972 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "70 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "71 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "72 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "73 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "74 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "75 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "76 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "77 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "78 Train Loss 0.0019361972 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "79 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "80 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "81 Train Loss 0.0019361972 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "82 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "83 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "84 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "85 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "86 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "87 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "88 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "90 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "91 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "92 Train Loss 0.0019361972 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "93 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "94 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "95 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "96 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "97 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "98 Train Loss 0.0019361972 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "99 Train Loss 0.0019361973 Test MSE 0.0007355321033917763 Test RE 0.001382494457861302\n",
      "Training time: 30.77\n",
      "Training time: 30.77\n"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    max_reps = 10\n",
    "    max_iter = 100\n",
    "    label = \"1D_SODE_Stan_tune\"+str(tune_reps)\n",
    "\n",
    "    N_f = 1000\n",
    "\n",
    "    train_loss_full = []\n",
    "    test_mse_full = []\n",
    "    test_re_full = []\n",
    "    beta_full = []\n",
    "    elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "    time_threshold = np.empty((max_reps,1))\n",
    "    time_threshold[:] = np.nan\n",
    "    epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "    for reps in range(max_reps):\n",
    "\n",
    "        train_loss = []\n",
    "        test_mse_loss = []\n",
    "        test_re_loss =[]\n",
    "        beta_val = []\n",
    "\n",
    "        'Generate Training data'\n",
    "        torch.manual_seed(reps*36)\n",
    "         #Total number of collocation points \n",
    "\n",
    "\n",
    "        layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "        PINN = Sequentialmodel(layers)\n",
    "        PINN.to(device)\n",
    "\n",
    "        'Neural Network Summary'\n",
    "        print(PINN)\n",
    "\n",
    "        params = list(PINN.parameters())\n",
    "\n",
    "        optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lr_tune[tune_reps], \n",
    "                                  max_iter = 10, \n",
    "                                  max_eval = 15, \n",
    "                                  tolerance_grad = 1e-5, \n",
    "                                  tolerance_change = 1e-5, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "        train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "        torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "        train_loss_full.append(train_loss)\n",
    "        test_mse_full.append(test_mse_loss)\n",
    "        test_re_full.append(test_re_loss)\n",
    "        beta_full.append(beta_val)    \n",
    "\n",
    "        print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "    mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "    savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0019549893546880782\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
