{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 0.005\n",
    "label = \"1D_SODE_rowdy\"\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.omega = Parameter(0.1*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat,i):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        #loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss = PINN.loss_PDE(x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        grads_layer1 = PINN.linears[0].weight.grad\n",
    "        if(i%20==0):\n",
    "            #print(i)\n",
    "            plt.hist(grads_layer1.flatten().cpu().detach().numpy(),color = 'b')\n",
    "            plt.savefig(label + 'grad_PDE_hist_' + str(i)+'.eps', format='eps',pad_inches=0, bbox_inches='tight')\n",
    "        return loss\n",
    "    \n",
    "    if(i%20==0):\n",
    "        plt.figure()\n",
    "        plt.xlabel(\"Gradients\",fontsize=14)\n",
    "        plt.title(\"Gradients of Weights Histogram\", fontsize=14, math_fontfamily='cm')\n",
    "        \n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat,i)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.8075366 Test MSE 385.6346542492583 Test RE 1.0010381051828363\n",
      "Training time: 1.65\n",
      "Training time: 1.65\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 1\n",
    "\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 8.0\n",
    "rowdy_terms = 2\n",
    "\n",
    "for reps in range(max_reps):\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    alpha_val = []\n",
    "    omega_val = []\n",
    "\n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "\n",
    "\n",
    "\n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)  \n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"alpha\": alpha_full, \"omega\": omega_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lrnr_tune' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9e773a12e949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlrnr_tune\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lrnr_tune' is not defined"
     ]
    }
   ],
   "source": [
    "lrnr_tune[tune_reps,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tune_reps in range(75):\n",
    "    label = \"1D_SODE_rowdy_tune\"+str(tune_reps)+\".mat\" #WRONGLY SAVED AS STAN - DOESN'T MATTER\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
