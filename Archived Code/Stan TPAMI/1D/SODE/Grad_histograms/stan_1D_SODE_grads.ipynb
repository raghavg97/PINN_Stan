{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"1D_SODE_Stan\"\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat,i):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        #loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss = PINN.loss_PDE(x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        grads_layer1 = PINN.linears[0].weight.grad\n",
    "        if(i%20==0):\n",
    "            #print(i)\n",
    "            plt.hist(grads_layer1.flatten().cpu().detach().numpy(),color = 'b')\n",
    "            plt.savefig(label + 'grad_PDE_hist_' + str(i)+'.eps', format='eps',pad_inches=0, bbox_inches='tight')\n",
    "            \n",
    "        return loss\n",
    "\n",
    "    if(i%20==0):\n",
    "        plt.figure()\n",
    "        plt.xlabel(\"Gradients\",fontsize=14)\n",
    "        plt.title(\"Gradients of Weights Histogram\", fontsize=14, math_fontfamily='cm')\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat,i)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.830705 Test MSE 384.5250024250919 Test RE 0.99959683949931\n",
      "Training time: 1.54\n",
      "Training time: 1.54\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 1\n",
    "\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    beta_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    #torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "# mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "# savemat(label+'.mat', mdic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
