{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"1D_SODE_atanh\"\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(3.0)\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat,i):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        #loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss = PINN.loss_PDE(x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        grads_layer1 = PINN.linears[0].weight.grad\n",
    "        if(i%20==0):\n",
    "            #print(i)\n",
    "            plt.hist(grads_layer1.flatten().cpu().detach().numpy(),color = 'b')\n",
    "            plt.savefig(label + 'grad_PDE_hist_' + str(i)+'.eps', format='eps',pad_inches=0, bbox_inches='tight')\n",
    "        return loss\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    if(i%20==0):\n",
    "        plt.figure()\n",
    "        plt.xlabel(\"Gradients\",fontsize=14)\n",
    "        plt.title(\"Gradients of Weights Histogram\", fontsize=14, math_fontfamily='cm')\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat,i)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 22192.037 Test MSE 394.10805262708 Test RE 1.0119760558508732\n",
      "Training time: 2.29\n",
      "Training time: 2.29\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 1\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    alpha_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.5, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "#     torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "# mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"alpha\": alpha_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "# savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3f67c32123a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_re_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
