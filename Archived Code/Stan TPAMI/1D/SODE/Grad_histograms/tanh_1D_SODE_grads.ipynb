{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2*x) + np.exp(-3*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"1D_SODE_tanh\"\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(-1.0,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - 6*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat,i):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        #loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss = PINN.loss_PDE(x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        grads_layer1 = PINN.linears[0].weight.grad\n",
    "        if(i%20==0):\n",
    "            #print(i)\n",
    "            plt.hist(grads_layer1.flatten().cpu().detach().numpy(),color = 'b')\n",
    "            plt.savefig(label + 'grad_PDE_hist_' + str(i)+'.eps', format='eps',pad_inches=0, bbox_inches='tight')\n",
    "        return loss\n",
    "\n",
    "    if(i%20==0):\n",
    "        plt.figure()\n",
    "        plt.xlabel(\"Gradients\",fontsize=14)\n",
    "        plt.title(\"Gradients of Weights Histogram\", fontsize=14, math_fontfamily='cm')\n",
    "        \n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat,i)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.8189425 Test MSE 385.33439620210913 Test RE 1.0006483213906814\n",
      "Training time: 1.36\n",
      "Training time: 1.36\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 1\n",
    "\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "#     torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "# mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "# savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grads_layer1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-72735f2341f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_layer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grads_layer1' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(grads_layer1.flatten().cpu().detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
