{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvP0Nx4vNOlZ",
    "outputId": "4c900037-3d26-473d-c9aa-392edcfda7eb"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDzAYhTsNbP6",
    "outputId": "a1b70a95-2d46-4fe5-8a51-d369c604c433"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/stan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1wXUvTNETmrW",
    "outputId": "2d3abfaa-8287-4fe9-d053-d79e6506dc03"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dmSz5jcVVt4p"
   },
   "outputs": [],
   "source": [
    "# lr_tune = np.array([0.05,0.1,0.25,0.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "def true_1D_2(x): #True function for 1D_1 dy/dx = cos(0.01*x) BC1: y(0)=0; x \\in [-100,100]\n",
    "    y = 100*np.sin(0.01*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "label = \"1D_FODE_stan_\"  \n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(-600,600,5000).reshape(-1,1)\n",
    "ysol = true_1D_2(x)\n",
    "\n",
    "bc1_x = np.array(0).reshape(-1,1) \n",
    "bc1_y = np.array(0).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "\n",
    " \n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "\n",
    "y_true = true_1D_2(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "\n",
    "  #Collocation Points\n",
    "  # Latin Hypercube sampling for collocation points \n",
    "  # N_f sets of tuples(x,y)\n",
    "  x01 = np.array([[0.0, 1.0]])\n",
    "  sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "  x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "  x_coll_train = np.vstack((x_coll_train, bc1_x)) # append training points to collocation points \n",
    "\n",
    "  return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.iter = 0\n",
    "        \n",
    "              \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        f = dy_dx - torch.cos(0.01*g)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + 100*loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "       \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "      \n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(seed):\n",
    "    x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_coll_train,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "  print(rep) \n",
    "  torch.manual_seed(rep*11)\n",
    "  start_time = time.time() \n",
    "  thresh_flag = 0\n",
    "\n",
    "  x_coll = torch.from_numpy(colloc_pts(N_f,123)).float().to(device)\n",
    "  f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "  for i in range(max_iter):\n",
    "    train_step(i)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_coll,f_hat).cpu().detach().numpy()\n",
    "    if(thresh_flag == 0):\n",
    "        if(loss_np < loss_thresh):\n",
    "            time_threshold[rep] = time.time() - start_time\n",
    "            epoch_threshold[rep] = i+1            \n",
    "            thresh_flag = 1       \n",
    "    data_update(loss_np)\n",
    "    print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "  elapsed_time[rep] = time.time() - start_time  \n",
    "  print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 47.76649 Test MSE 5217.26593432726 Test RE 0.9994812510638998\n",
      "1 Train Loss 47.685417 Test MSE 5144.388256860855 Test RE 0.9924760463856304\n",
      "2 Train Loss 44.87665 Test MSE 4553.222004963004 Test RE 0.9337112203041019\n",
      "3 Train Loss 38.11796 Test MSE 5771.452907500114 Test RE 1.051225150616455\n",
      "4 Train Loss 34.60018 Test MSE 6786.685964223334 Test RE 1.1399401272177705\n",
      "5 Train Loss 31.437141 Test MSE 6505.292784905372 Test RE 1.1160575468331464\n",
      "6 Train Loss 30.019697 Test MSE 6502.78188876433 Test RE 1.1158421395379667\n",
      "7 Train Loss 28.65719 Test MSE 6968.417310034795 Test RE 1.1551017455051116\n",
      "8 Train Loss 27.43075 Test MSE 7170.098459283032 Test RE 1.1716980965015888\n",
      "9 Train Loss 24.61216 Test MSE 5025.330911696408 Test RE 0.9809243086209821\n",
      "10 Train Loss 19.942749 Test MSE 5383.235625119077 Test RE 1.015254350943715\n",
      "11 Train Loss 14.949922 Test MSE 6322.139086895185 Test RE 1.1002343194671476\n",
      "12 Train Loss 10.744022 Test MSE 7811.104189253615 Test RE 1.2229519110803375\n",
      "13 Train Loss 9.315624 Test MSE 7059.81059428433 Test RE 1.162651856717042\n",
      "14 Train Loss 8.70531 Test MSE 6747.843812664129 Test RE 1.1366733440702208\n",
      "15 Train Loss 8.109988 Test MSE 6428.665923030958 Test RE 1.1094649673073396\n",
      "16 Train Loss 7.3627667 Test MSE 6017.275507566169 Test RE 1.073379046087934\n",
      "17 Train Loss 7.157718 Test MSE 5672.780482984828 Test RE 1.0422002038462161\n",
      "18 Train Loss 6.939869 Test MSE 5370.101124860725 Test RE 1.014015040304403\n",
      "19 Train Loss 6.672483 Test MSE 5054.675978438358 Test RE 0.9837841590108677\n",
      "20 Train Loss 6.4065647 Test MSE 4992.310921083627 Test RE 0.9776963128868706\n",
      "21 Train Loss 6.211353 Test MSE 4900.492099091676 Test RE 0.9686636691923983\n",
      "22 Train Loss 6.026559 Test MSE 4738.535728148058 Test RE 0.9525225028193677\n",
      "23 Train Loss 5.9824724 Test MSE 4708.239482024102 Test RE 0.9494726017314918\n",
      "24 Train Loss 5.850388 Test MSE 4481.767059017685 Test RE 0.9263557568530301\n",
      "25 Train Loss 5.7044992 Test MSE 4597.379392385891 Test RE 0.9382278863679134\n",
      "26 Train Loss 5.640488 Test MSE 4578.647404926021 Test RE 0.9363145341909063\n",
      "27 Train Loss 5.4532237 Test MSE 4587.449019169154 Test RE 0.9372140490389558\n",
      "28 Train Loss 5.321449 Test MSE 4689.598187356316 Test RE 0.947591117935366\n",
      "29 Train Loss 5.0115094 Test MSE 3844.4570634702072 Test RE 0.857967174294389\n",
      "30 Train Loss 4.8552403 Test MSE 3667.65958543309 Test RE 0.8380070543652052\n",
      "31 Train Loss 4.6804805 Test MSE 3416.1968073794187 Test RE 0.8087692038456106\n",
      "32 Train Loss 4.535508 Test MSE 3673.0615195014916 Test RE 0.8386239591139718\n",
      "33 Train Loss 4.0852265 Test MSE 3178.537119458853 Test RE 0.7801296935193271\n",
      "34 Train Loss 3.6260695 Test MSE 2400.1416724344244 Test RE 0.6779093179714291\n",
      "35 Train Loss 2.5988064 Test MSE 1271.9799700862907 Test RE 0.4935069815562879\n",
      "36 Train Loss 2.2331274 Test MSE 1068.6381518833255 Test RE 0.4523436543042131\n",
      "37 Train Loss 2.0505667 Test MSE 986.9626524818325 Test RE 0.4347138971254461\n",
      "38 Train Loss 1.6600446 Test MSE 745.0775216914478 Test RE 0.37770601002855264\n",
      "39 Train Loss 1.3244919 Test MSE 585.1007740433213 Test RE 0.3347098560375591\n",
      "40 Train Loss 0.8927589 Test MSE 506.4242645587424 Test RE 0.3113941272512475\n",
      "41 Train Loss 0.6534695 Test MSE 308.24651083581114 Test RE 0.24294180247266522\n",
      "42 Train Loss 0.53783894 Test MSE 219.3539017846562 Test RE 0.20493958225657924\n",
      "43 Train Loss 0.4227611 Test MSE 193.43653461733444 Test RE 0.19245199449895442\n",
      "44 Train Loss 0.35364512 Test MSE 141.97342912380634 Test RE 0.16487571834356332\n",
      "45 Train Loss 0.29225218 Test MSE 121.08909377409425 Test RE 0.15226696070098442\n",
      "46 Train Loss 0.2148602 Test MSE 95.17058352227622 Test RE 0.13499091177939646\n",
      "47 Train Loss 0.16310987 Test MSE 59.3188925623012 Test RE 0.10657361120483126\n",
      "48 Train Loss 0.10582093 Test MSE 48.75569321330208 Test RE 0.09661973288671573\n",
      "49 Train Loss 0.089783475 Test MSE 38.337441390204155 Test RE 0.08567709391140521\n",
      "50 Train Loss 0.068003975 Test MSE 21.910375340587578 Test RE 0.06477062285380143\n",
      "51 Train Loss 0.056489434 Test MSE 17.369267112105643 Test RE 0.05766919798814665\n",
      "52 Train Loss 0.05152236 Test MSE 12.71665349813413 Test RE 0.04934459440764283\n",
      "53 Train Loss 0.044264976 Test MSE 8.154897823692917 Test RE 0.03951504006829397\n",
      "54 Train Loss 0.042053044 Test MSE 5.775430187479204 Test RE 0.03325411037588997\n",
      "55 Train Loss 0.039302353 Test MSE 3.838511680186048 Test RE 0.027110317131516154\n",
      "56 Train Loss 0.035124574 Test MSE 3.657474219585175 Test RE 0.026463287892015287\n",
      "57 Train Loss 0.031297207 Test MSE 2.8294830374597755 Test RE 0.02327591229282363\n",
      "58 Train Loss 0.030052377 Test MSE 2.247570139326621 Test RE 0.02074482575956053\n",
      "59 Train Loss 0.026162155 Test MSE 1.8166285216756224 Test RE 0.018650317472050155\n",
      "60 Train Loss 0.024476005 Test MSE 1.5211667244452265 Test RE 0.017066386350173393\n",
      "61 Train Loss 0.02329632 Test MSE 1.5704852428646943 Test RE 0.017340838542767\n",
      "62 Train Loss 0.019446803 Test MSE 1.2777673513387209 Test RE 0.01564152374295934\n",
      "63 Train Loss 0.018660363 Test MSE 0.9558520735804552 Test RE 0.01352846455112031\n",
      "64 Train Loss 0.018006641 Test MSE 0.7794759931959065 Test RE 0.012216717650724794\n",
      "65 Train Loss 0.01782007 Test MSE 0.8624424935993841 Test RE 0.012850447094850219\n",
      "66 Train Loss 0.01618138 Test MSE 0.2717544533077215 Test RE 0.007213424648895974\n",
      "67 Train Loss 0.015664496 Test MSE 0.33867055940873286 Test RE 0.00805270681058006\n",
      "68 Train Loss 0.015312268 Test MSE 0.44839380188607647 Test RE 0.009265800922145073\n",
      "69 Train Loss 0.014754223 Test MSE 0.4262255113996842 Test RE 0.009033850203269769\n",
      "70 Train Loss 0.014435282 Test MSE 0.3053525915386355 Test RE 0.0076463464168250745\n",
      "71 Train Loss 0.013507774 Test MSE 0.24946704099275402 Test RE 0.006911300122364053\n",
      "72 Train Loss 0.012860075 Test MSE 0.3969495983252659 Test RE 0.008718079962994355\n",
      "73 Train Loss 0.0123524945 Test MSE 0.24069163538521726 Test RE 0.006788653830812259\n",
      "74 Train Loss 0.011893118 Test MSE 0.3050026588231399 Test RE 0.007641963821317433\n",
      "75 Train Loss 0.011184006 Test MSE 0.12062233186511309 Test RE 0.00480581473505124\n",
      "76 Train Loss 0.009822369 Test MSE 0.04657816235585461 Test RE 0.0029863749900646826\n",
      "77 Train Loss 0.009637766 Test MSE 0.037977090844409894 Test RE 0.0026965843605722684\n",
      "78 Train Loss 0.008805528 Test MSE 0.1119076198470164 Test RE 0.004628955375897873\n",
      "79 Train Loss 0.008125192 Test MSE 0.011876890907838092 Test RE 0.001508011135144016\n",
      "80 Train Loss 0.008054157 Test MSE 0.012463415434210045 Test RE 0.0015447980064770754\n",
      "81 Train Loss 0.007877892 Test MSE 0.01252091517776675 Test RE 0.0015483573549008984\n",
      "82 Train Loss 0.007710896 Test MSE 0.008946365014512644 Test RE 0.001308809598806813\n",
      "83 Train Loss 0.0076082214 Test MSE 0.007759248806710434 Test RE 0.001218885761207302\n",
      "84 Train Loss 0.0074891946 Test MSE 0.0052036870666265275 Test RE 0.0009981797393014485\n",
      "85 Train Loss 0.0074354406 Test MSE 0.00749341380928932 Test RE 0.0011978240340388866\n",
      "86 Train Loss 0.007390916 Test MSE 0.0059135676206367935 Test RE 0.0010640889867798483\n",
      "87 Train Loss 0.007166539 Test MSE 0.015341526906788826 Test RE 0.0017139078097263007\n",
      "88 Train Loss 0.006881041 Test MSE 0.008363679178166349 Test RE 0.0012654699807877177\n",
      "89 Train Loss 0.0067537557 Test MSE 0.00466887699973162 Test RE 0.0009454953145885721\n",
      "90 Train Loss 0.006648175 Test MSE 0.00929068033477152 Test RE 0.0013337576500934232\n",
      "91 Train Loss 0.0065455246 Test MSE 0.0051588783823142475 Test RE 0.0009938728101191539\n",
      "92 Train Loss 0.0064233555 Test MSE 0.00575263564984358 Test RE 0.001049510043102274\n",
      "93 Train Loss 0.006389375 Test MSE 0.010953660127950075 Test RE 0.0014482141796488869\n",
      "94 Train Loss 0.0063406946 Test MSE 0.01341638595081183 Test RE 0.0016027690086557996\n",
      "95 Train Loss 0.006296677 Test MSE 0.005473795497797498 Test RE 0.00102375833046698\n",
      "96 Train Loss 0.0062603485 Test MSE 0.005814473640574471 Test RE 0.0010551358221171172\n",
      "97 Train Loss 0.006232159 Test MSE 0.013106896736179777 Test RE 0.0015841748111111876\n",
      "98 Train Loss 0.0061882758 Test MSE 0.038541801569009555 Test RE 0.0027165591781469624\n",
      "99 Train Loss 0.006126463 Test MSE 0.0385830383465329 Test RE 0.0027180120448285564\n",
      "Training time: 37.95\n",
      "Training time: 37.95\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 47.759174 Test MSE 5202.7452757575375 Test RE 0.9980894072838895\n",
      "1 Train Loss 46.896435 Test MSE 4789.907279904605 Test RE 0.9576718415223731\n",
      "2 Train Loss 39.228954 Test MSE 3990.750236155926 Test RE 0.8741388829103823\n",
      "3 Train Loss 31.471287 Test MSE 3318.018554685954 Test RE 0.7970628538544731\n",
      "4 Train Loss 21.369017 Test MSE 3787.6936672867164 Test RE 0.8516096786401017\n",
      "5 Train Loss 15.54148 Test MSE 3450.094924671978 Test RE 0.8127719121502452\n",
      "6 Train Loss 11.6816025 Test MSE 2676.1782139486427 Test RE 0.7158312933532993\n",
      "7 Train Loss 8.302334 Test MSE 1892.7352575059451 Test RE 0.6020022310201264\n",
      "8 Train Loss 6.3707175 Test MSE 1519.0721461165606 Test RE 0.5393148328914822\n",
      "9 Train Loss 4.81764 Test MSE 554.6771650603592 Test RE 0.32589170641420423\n",
      "10 Train Loss 2.9695542 Test MSE 265.53945792610153 Test RE 0.22548502095919937\n",
      "11 Train Loss 2.1733494 Test MSE 179.34098681695926 Test RE 0.18530747663750366\n",
      "12 Train Loss 1.3592538 Test MSE 104.3870329379478 Test RE 0.14137624466339677\n",
      "13 Train Loss 0.9839228 Test MSE 67.48003317490083 Test RE 0.11366867782736155\n",
      "14 Train Loss 0.6959068 Test MSE 35.138572705562716 Test RE 0.08202480812218771\n",
      "15 Train Loss 0.5651905 Test MSE 19.743768839806126 Test RE 0.06148486071023464\n",
      "16 Train Loss 0.4768508 Test MSE 5.9003467322264935 Test RE 0.03361181244275773\n",
      "17 Train Loss 0.37161523 Test MSE 3.8659598663475894 Test RE 0.02720707383838251\n",
      "18 Train Loss 0.2721777 Test MSE 2.566077766828914 Test RE 0.022166037906882402\n",
      "19 Train Loss 0.20416155 Test MSE 3.842140672991671 Test RE 0.02712312937643596\n",
      "20 Train Loss 0.16849364 Test MSE 4.725351264725445 Test RE 0.030079472346775413\n",
      "21 Train Loss 0.15161964 Test MSE 1.9230285870449304 Test RE 0.019188721287412527\n",
      "22 Train Loss 0.12891383 Test MSE 1.6226286696614336 Test RE 0.017626364113192506\n",
      "23 Train Loss 0.116749786 Test MSE 1.2988885154224277 Test RE 0.015770269064329035\n",
      "24 Train Loss 0.10159888 Test MSE 0.5026802449067527 Test RE 0.009810679171861477\n",
      "25 Train Loss 0.08980382 Test MSE 0.33194411716831135 Test RE 0.007972337078789632\n",
      "26 Train Loss 0.08061841 Test MSE 0.8419891364768478 Test RE 0.012697154634919421\n",
      "27 Train Loss 0.07040737 Test MSE 0.45260733496601313 Test RE 0.009309234248626123\n",
      "28 Train Loss 0.0575101 Test MSE 0.3912136054789357 Test RE 0.008654861844605692\n",
      "29 Train Loss 0.048750684 Test MSE 0.8009184791235401 Test RE 0.012383611581560329\n",
      "30 Train Loss 0.040158544 Test MSE 0.11087300765422742 Test RE 0.004607507799748234\n",
      "31 Train Loss 0.03145453 Test MSE 0.06518492213899418 Test RE 0.0035328625433986606\n",
      "32 Train Loss 0.0274329 Test MSE 0.20017732711734215 Test RE 0.006190997201221023\n",
      "33 Train Loss 0.025057597 Test MSE 0.26729870525512706 Test RE 0.007154043758627219\n",
      "34 Train Loss 0.02287921 Test MSE 0.354497657624458 Test RE 0.008238722018193442\n",
      "35 Train Loss 0.01966742 Test MSE 0.5056230913818153 Test RE 0.009839354647696504\n",
      "36 Train Loss 0.017908718 Test MSE 0.7201138862285051 Test RE 0.011742315835626535\n",
      "37 Train Loss 0.01612887 Test MSE 0.5961460208041132 Test RE 0.010683892004190695\n",
      "38 Train Loss 0.015006379 Test MSE 0.6715070551911195 Test RE 0.011339096586956867\n",
      "39 Train Loss 0.01392411 Test MSE 0.7403140862608786 Test RE 0.011905870958728759\n",
      "40 Train Loss 0.012622241 Test MSE 0.42945948542612294 Test RE 0.009068057480799446\n",
      "41 Train Loss 0.011571901 Test MSE 0.20572645794963265 Test RE 0.006276221164598658\n",
      "42 Train Loss 0.010647319 Test MSE 0.07179985037696411 Test RE 0.003707788339278395\n",
      "43 Train Loss 0.010199122 Test MSE 0.03175972920387443 Test RE 0.002465991413595177\n",
      "44 Train Loss 0.009389443 Test MSE 0.007118473796167558 Test RE 0.0011674723670485503\n",
      "45 Train Loss 0.009166179 Test MSE 0.014648326645435054 Test RE 0.0016747391489917206\n",
      "46 Train Loss 0.008861518 Test MSE 0.06847829437730674 Test RE 0.0036210092312205185\n",
      "47 Train Loss 0.0086610615 Test MSE 0.14127136029356271 Test RE 0.005200920654026611\n",
      "48 Train Loss 0.008487734 Test MSE 0.17760292859307455 Test RE 0.005831472421659728\n",
      "49 Train Loss 0.008083894 Test MSE 0.157079935456555 Test RE 0.005484202996863768\n",
      "50 Train Loss 0.0078363735 Test MSE 0.1271994378875132 Test RE 0.00493509776733067\n",
      "51 Train Loss 0.0075433603 Test MSE 0.14071184250277374 Test RE 0.005190611081426594\n",
      "52 Train Loss 0.007142204 Test MSE 0.09497821763757018 Test RE 0.004264471068353529\n",
      "53 Train Loss 0.0068266923 Test MSE 0.03403458970868777 Test RE 0.002552780210139874\n",
      "54 Train Loss 0.006377097 Test MSE 0.005170863632916696 Test RE 0.0009950266369176967\n",
      "55 Train Loss 0.0057848096 Test MSE 0.013536945343492679 Test RE 0.0016099541295497666\n",
      "56 Train Loss 0.0056352564 Test MSE 0.02364948390734134 Test RE 0.00212796263825027\n",
      "57 Train Loss 0.005216579 Test MSE 0.08677435391661743 Test RE 0.004076137814175974\n",
      "58 Train Loss 0.004545816 Test MSE 0.13275915134686286 Test RE 0.005041797498535167\n",
      "59 Train Loss 0.0042684027 Test MSE 0.14328714754061542 Test RE 0.00523789494009262\n",
      "60 Train Loss 0.0037499573 Test MSE 0.30976562365420995 Test RE 0.007701401668869534\n",
      "61 Train Loss 0.003198768 Test MSE 0.1638167125249709 Test RE 0.005600570491041414\n",
      "62 Train Loss 0.003038831 Test MSE 0.1174348141236042 Test RE 0.004741891330163532\n",
      "63 Train Loss 0.0029548034 Test MSE 0.11813844035323758 Test RE 0.004756075949352187\n",
      "64 Train Loss 0.0028114244 Test MSE 0.05578748944836841 Test RE 0.0032682974382086285\n",
      "65 Train Loss 0.0027131392 Test MSE 0.05327837121479143 Test RE 0.0031939538334755245\n",
      "66 Train Loss 0.0026251175 Test MSE 0.055539000497523194 Test RE 0.003261010480288534\n",
      "67 Train Loss 0.0024433031 Test MSE 0.0815679218013031 Test RE 0.003951962940990642\n",
      "68 Train Loss 0.002359352 Test MSE 0.1402689919268791 Test RE 0.005182436657311571\n",
      "69 Train Loss 0.0023049654 Test MSE 0.1206559956964555 Test RE 0.004806485302632104\n",
      "70 Train Loss 0.002198641 Test MSE 0.10939256417461242 Test RE 0.004576643307247706\n",
      "71 Train Loss 0.00212224 Test MSE 0.07731234401125435 Test RE 0.003847490755764206\n",
      "72 Train Loss 0.002016483 Test MSE 0.034028210035864564 Test RE 0.0025525409437651947\n",
      "73 Train Loss 0.0018790909 Test MSE 0.02618179989980879 Test RE 0.002238993920515251\n",
      "74 Train Loss 0.0017417055 Test MSE 0.02860717694606301 Test RE 0.0023404031060349326\n",
      "75 Train Loss 0.0016397937 Test MSE 0.004734227901821614 Test RE 0.0009520894329052923\n",
      "76 Train Loss 0.0015870001 Test MSE 0.0011970720601608825 Test RE 0.00047875498812177236\n",
      "77 Train Loss 0.0015157412 Test MSE 0.0019056383591231519 Test RE 0.0006040507220685172\n",
      "78 Train Loss 0.001478738 Test MSE 0.0008141665464518919 Test RE 0.00039482967962963856\n",
      "79 Train Loss 0.001422422 Test MSE 0.004654953363136107 Test RE 0.0009440844225397988\n",
      "80 Train Loss 0.0013116153 Test MSE 0.002593346739681819 Test RE 0.0007046662287573641\n",
      "81 Train Loss 0.0012028201 Test MSE 0.00107642849866888 Test RE 0.0004539894475589078\n",
      "82 Train Loss 0.0011581011 Test MSE 0.0021299270625135635 Test RE 0.0006386097288930385\n",
      "83 Train Loss 0.001147489 Test MSE 0.0032065485960643763 Test RE 0.000783559675580896\n",
      "84 Train Loss 0.0011183023 Test MSE 0.002500280917088928 Test RE 0.000691906750546903\n",
      "85 Train Loss 0.0010753184 Test MSE 0.0006661894429693163 Test RE 0.00035715113759590515\n",
      "86 Train Loss 0.0010652877 Test MSE 0.00099782777744759 Test RE 0.00043710015399030374\n",
      "87 Train Loss 0.0010248284 Test MSE 0.0032292415300718092 Test RE 0.0007863274363893135\n",
      "88 Train Loss 0.0010150917 Test MSE 0.004443206203134827 Test RE 0.0009223619929886129\n",
      "89 Train Loss 0.0009959453 Test MSE 0.005770254096698759 Test RE 0.0010511159679259343\n",
      "90 Train Loss 0.0009954777 Test MSE 0.0060940321402460215 Test RE 0.0010802033874588832\n",
      "91 Train Loss 0.0009894562 Test MSE 0.008118351453492998 Test RE 0.001246772139564536\n",
      "92 Train Loss 0.0009887759 Test MSE 0.008177366366646502 Test RE 0.0012512955282077568\n",
      "93 Train Loss 0.0009881665 Test MSE 0.008283352369459632 Test RE 0.0012593783784756655\n",
      "94 Train Loss 0.0009874147 Test MSE 0.0088886818353913 Test RE 0.0013045833915119732\n",
      "95 Train Loss 0.0009824658 Test MSE 0.008880673558543694 Test RE 0.0013039955754898316\n",
      "96 Train Loss 0.000966819 Test MSE 0.009996904424246703 Test RE 0.0013835215707525893\n",
      "97 Train Loss 0.00091976416 Test MSE 0.00768308716805925 Test RE 0.0012128889653016403\n",
      "98 Train Loss 0.00088937423 Test MSE 0.003917241423704823 Test RE 0.000866050733540055\n",
      "99 Train Loss 0.00086623675 Test MSE 0.006474544304175626 Test RE 0.0011134167954678046\n",
      "Training time: 44.58\n",
      "Training time: 44.58\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 47.765366 Test MSE 5214.271402938366 Test RE 0.999194375945157\n",
      "1 Train Loss 47.7296 Test MSE 5144.572576522607 Test RE 0.9924938260711317\n",
      "2 Train Loss 47.112526 Test MSE 4972.658114885057 Test RE 0.9757700082382141\n",
      "3 Train Loss 42.04747 Test MSE 4340.346584911818 Test RE 0.911623201446737\n",
      "4 Train Loss 26.890057 Test MSE 8034.2506692675415 Test RE 1.24029745766668\n",
      "5 Train Loss 22.647627 Test MSE 7376.646798185943 Test RE 1.1884547736556126\n",
      "6 Train Loss 18.406599 Test MSE 6244.911688099881 Test RE 1.093493775605317\n",
      "7 Train Loss 15.461616 Test MSE 4444.975739129467 Test RE 0.9225456430602658\n",
      "8 Train Loss 12.672059 Test MSE 5857.775930320066 Test RE 1.0590575051860696\n",
      "9 Train Loss 10.970298 Test MSE 5348.343167307999 Test RE 1.0119587205950087\n",
      "10 Train Loss 8.962322 Test MSE 4378.254995451929 Test RE 0.9155955866224404\n",
      "11 Train Loss 7.5705748 Test MSE 3901.0534326063716 Test RE 0.8642594046346006\n",
      "12 Train Loss 6.805947 Test MSE 3324.3639708869323 Test RE 0.7978246458206163\n",
      "13 Train Loss 5.763788 Test MSE 3588.034567449342 Test RE 0.828860558033085\n",
      "14 Train Loss 4.9214516 Test MSE 2899.102272808811 Test RE 0.7450491659592436\n",
      "15 Train Loss 4.122333 Test MSE 3063.7004614273087 Test RE 0.7659074883274006\n",
      "16 Train Loss 3.517944 Test MSE 2580.8596860143834 Test RE 0.702967685601206\n",
      "17 Train Loss 2.9184444 Test MSE 2287.2089866423526 Test RE 0.6617684969962762\n",
      "18 Train Loss 2.4356477 Test MSE 1350.9949237505787 Test RE 0.5086042949370501\n",
      "19 Train Loss 1.6513418 Test MSE 737.4543607057564 Test RE 0.37576881800306444\n",
      "20 Train Loss 1.4630479 Test MSE 772.2680143935833 Test RE 0.3845361630399827\n",
      "21 Train Loss 1.2865791 Test MSE 792.4224496091166 Test RE 0.389521604131911\n",
      "22 Train Loss 1.0632035 Test MSE 725.392594732099 Test RE 0.3726831207809841\n",
      "23 Train Loss 0.98138237 Test MSE 591.9582808413329 Test RE 0.3366655780770789\n",
      "24 Train Loss 0.7472005 Test MSE 442.38520621818884 Test RE 0.29104052312787404\n",
      "25 Train Loss 0.59393966 Test MSE 354.19531945507254 Test RE 0.260420143342217\n",
      "26 Train Loss 0.39064655 Test MSE 194.29938672008444 Test RE 0.1928807471108432\n",
      "27 Train Loss 0.32418585 Test MSE 188.41626572336128 Test RE 0.18993821870386046\n",
      "28 Train Loss 0.2570633 Test MSE 156.18087956171033 Test RE 0.17292870803330465\n",
      "29 Train Loss 0.20052357 Test MSE 89.47831146190933 Test RE 0.13089168383728325\n",
      "30 Train Loss 0.13916375 Test MSE 19.420290374571916 Test RE 0.060979101971957335\n",
      "31 Train Loss 0.12006493 Test MSE 10.873176954065562 Test RE 0.045627995774602345\n",
      "32 Train Loss 0.08720595 Test MSE 4.589444791826355 Test RE 0.029643756665225444\n",
      "33 Train Loss 0.06136944 Test MSE 5.707177520800423 Test RE 0.03305703178645951\n",
      "34 Train Loss 0.051951364 Test MSE 1.1647574908944474 Test RE 0.014933820288354145\n",
      "35 Train Loss 0.048669424 Test MSE 1.3526110332585835 Test RE 0.016093096945594708\n",
      "36 Train Loss 0.045913998 Test MSE 1.2475817569625447 Test RE 0.015455664166447456\n",
      "37 Train Loss 0.04243804 Test MSE 2.1221552521577998 Test RE 0.020157735441938898\n",
      "38 Train Loss 0.036787007 Test MSE 1.6967011021114744 Test RE 0.01802419327215906\n",
      "39 Train Loss 0.029848067 Test MSE 0.3204103697670366 Test RE 0.007832608980877459\n",
      "40 Train Loss 0.028512092 Test MSE 0.22922395954895605 Test RE 0.006624958616438888\n",
      "41 Train Loss 0.027891297 Test MSE 0.06965584215621186 Test RE 0.003652009834182814\n",
      "42 Train Loss 0.026335292 Test MSE 0.08868235060427129 Test RE 0.004120707258464842\n",
      "43 Train Loss 0.024895748 Test MSE 0.08275664724540403 Test RE 0.00398065563424669\n",
      "44 Train Loss 0.022535842 Test MSE 0.19313858688303773 Test RE 0.006081177632611279\n",
      "45 Train Loss 0.021262085 Test MSE 0.0549348684431552 Test RE 0.003243225970952224\n",
      "46 Train Loss 0.017068556 Test MSE 0.23632981058029828 Test RE 0.006726860447894561\n",
      "47 Train Loss 0.016227815 Test MSE 0.8595783766891969 Test RE 0.012829091589488435\n",
      "48 Train Loss 0.015784668 Test MSE 0.7357895141517975 Test RE 0.011869432686913801\n",
      "49 Train Loss 0.0147462515 Test MSE 0.9037899793263731 Test RE 0.013154881054910897\n",
      "50 Train Loss 0.014430987 Test MSE 0.8455979352674096 Test RE 0.012724335795031136\n",
      "51 Train Loss 0.013841623 Test MSE 0.616336376492212 Test RE 0.01086330730204371\n",
      "52 Train Loss 0.012942501 Test MSE 0.280738633773533 Test RE 0.007331692734213582\n",
      "53 Train Loss 0.012735292 Test MSE 0.18759393728116228 Test RE 0.005993252344944089\n",
      "54 Train Loss 0.012561918 Test MSE 0.31385833394845586 Test RE 0.007752111260526365\n",
      "55 Train Loss 0.012359 Test MSE 0.4329503608695441 Test RE 0.009104837889975277\n",
      "56 Train Loss 0.011980051 Test MSE 0.6593719001245748 Test RE 0.011236172095042856\n",
      "57 Train Loss 0.011041761 Test MSE 0.8984437737629561 Test RE 0.013115915690829693\n",
      "58 Train Loss 0.010780175 Test MSE 0.858229075292782 Test RE 0.012819018564090958\n",
      "59 Train Loss 0.010683756 Test MSE 0.7956699005967565 Test RE 0.012342968748743162\n",
      "60 Train Loss 0.010494802 Test MSE 0.8104327526522389 Test RE 0.012456948150017787\n",
      "61 Train Loss 0.009810609 Test MSE 0.3384700563553434 Test RE 0.008050322736586688\n",
      "62 Train Loss 0.009240162 Test MSE 0.23829834500685174 Test RE 0.006754818400621602\n",
      "63 Train Loss 0.008973918 Test MSE 0.08336164207864262 Test RE 0.003995179486675167\n",
      "64 Train Loss 0.008973464 Test MSE 0.08135388557034828 Test RE 0.0039467745111840405\n",
      "65 Train Loss 0.008893107 Test MSE 0.049459169201390235 Test RE 0.003077347732205034\n",
      "66 Train Loss 0.008339937 Test MSE 0.0518365850171493 Test RE 0.003150441037774458\n",
      "67 Train Loss 0.0079603875 Test MSE 0.10871502073458753 Test RE 0.004562448142048355\n",
      "68 Train Loss 0.0078092283 Test MSE 0.09146919448645062 Test RE 0.004184953067698947\n",
      "69 Train Loss 0.007600152 Test MSE 0.12293421474828321 Test RE 0.004851650974982862\n",
      "70 Train Loss 0.007196754 Test MSE 0.04387300696346763 Test RE 0.002898356915382111\n",
      "71 Train Loss 0.0070609557 Test MSE 0.023861093534859237 Test RE 0.0021374616737798503\n",
      "72 Train Loss 0.00703762 Test MSE 0.02447891221762283 Test RE 0.0021649567370394945\n",
      "73 Train Loss 0.007025535 Test MSE 0.01641344948399071 Test RE 0.001772772867245927\n",
      "74 Train Loss 0.007022646 Test MSE 0.016770996285455583 Test RE 0.0017919776802251119\n",
      "75 Train Loss 0.006991857 Test MSE 0.04773893366027479 Test RE 0.0030233576243535623\n",
      "76 Train Loss 0.006981744 Test MSE 0.03922251070769051 Test RE 0.0027404435453764944\n",
      "77 Train Loss 0.0069811502 Test MSE 0.039949587749747 Test RE 0.0027657270398925527\n",
      "78 Train Loss 0.0069807973 Test MSE 0.040763741719522545 Test RE 0.002793767013732899\n",
      "79 Train Loss 0.006979868 Test MSE 0.041423809769433974 Test RE 0.0028162952586487542\n",
      "80 Train Loss 0.006979394 Test MSE 0.04157250916837677 Test RE 0.002821345569831952\n",
      "81 Train Loss 0.006979107 Test MSE 0.04143509842929773 Test RE 0.002816678975570863\n",
      "82 Train Loss 0.006918505 Test MSE 0.028971172416745357 Test RE 0.0023552455941819265\n",
      "83 Train Loss 0.006762117 Test MSE 0.05372575680785167 Test RE 0.003207335827890848\n",
      "84 Train Loss 0.0066280677 Test MSE 0.020027288445700082 Test RE 0.0019582324413853565\n",
      "85 Train Loss 0.0064999154 Test MSE 0.03270400043751249 Test RE 0.0025023819838427114\n",
      "86 Train Loss 0.005924748 Test MSE 0.024285847708702988 Test RE 0.002156402359427287\n",
      "87 Train Loss 0.005454567 Test MSE 0.016748895225226074 Test RE 0.0017907965439810025\n",
      "88 Train Loss 0.0052492507 Test MSE 0.019180504653819555 Test RE 0.001916386838344489\n",
      "89 Train Loss 0.0052345004 Test MSE 0.017086233073065536 Test RE 0.001808740769524918\n",
      "90 Train Loss 0.0051599923 Test MSE 0.044541967167630335 Test RE 0.002920369885563497\n",
      "91 Train Loss 0.00502118 Test MSE 0.09872753875957051 Test RE 0.0043478276474266005\n",
      "92 Train Loss 0.004907364 Test MSE 0.15138911870011415 Test RE 0.005383943517178345\n",
      "93 Train Loss 0.0048398445 Test MSE 0.22414091279488257 Test RE 0.006551092517714145\n",
      "94 Train Loss 0.0046996744 Test MSE 0.14261793797269873 Test RE 0.005225649069502492\n",
      "95 Train Loss 0.0044279047 Test MSE 0.11525960527139643 Test RE 0.004697769767169479\n",
      "96 Train Loss 0.0040495563 Test MSE 0.054280654361735944 Test RE 0.0032238564973604677\n",
      "97 Train Loss 0.0038906545 Test MSE 0.09049692287942966 Test RE 0.004162651671372818\n",
      "98 Train Loss 0.003746786 Test MSE 0.08669517110872357 Test RE 0.004074277623420929\n",
      "99 Train Loss 0.0037023625 Test MSE 0.11584661822255703 Test RE 0.004709717359070207\n",
      "Training time: 59.15\n",
      "Training time: 59.15\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 47.75888 Test MSE 5214.8552169107625 Test RE 0.9992503115941854\n",
      "1 Train Loss 46.406605 Test MSE 5081.085167214857 Test RE 0.9863508016956518\n",
      "2 Train Loss 42.910957 Test MSE 4728.221126358184 Test RE 0.9514852369546669\n",
      "3 Train Loss 33.89817 Test MSE 3419.3448921430177 Test RE 0.8091417654775451\n",
      "4 Train Loss 25.184317 Test MSE 2376.5904352190623 Test RE 0.6745751477330248\n",
      "5 Train Loss 17.702358 Test MSE 1612.5683907135185 Test RE 0.555663968093857\n",
      "6 Train Loss 12.115677 Test MSE 2240.0233379926453 Test RE 0.6549067041472626\n",
      "7 Train Loss 10.390801 Test MSE 2286.389365588531 Test RE 0.6616499140427816\n",
      "8 Train Loss 9.112518 Test MSE 2149.5544393776854 Test RE 0.6415453905023845\n",
      "9 Train Loss 6.358685 Test MSE 1261.8753665728555 Test RE 0.49154286435460504\n",
      "10 Train Loss 5.344886 Test MSE 1165.662489634531 Test RE 0.4724322922333199\n",
      "11 Train Loss 4.1843905 Test MSE 651.3544534747911 Test RE 0.35315215286468943\n",
      "12 Train Loss 2.5001404 Test MSE 135.2710490379584 Test RE 0.16093688595237318\n",
      "13 Train Loss 1.6512463 Test MSE 67.75215142741399 Test RE 0.11389763593559175\n",
      "14 Train Loss 1.4467189 Test MSE 68.43419057054692 Test RE 0.11446948583779537\n",
      "15 Train Loss 0.9159448 Test MSE 103.69041666663762 Test RE 0.14090372500470333\n",
      "16 Train Loss 0.63043255 Test MSE 83.19787800786577 Test RE 0.1262145111323188\n",
      "17 Train Loss 0.5158886 Test MSE 66.65635224274422 Test RE 0.11297281133182521\n",
      "18 Train Loss 0.41812357 Test MSE 43.46311363546879 Test RE 0.0912249391497179\n",
      "19 Train Loss 0.34645078 Test MSE 27.13676345780353 Test RE 0.07208288980712002\n",
      "20 Train Loss 0.30661052 Test MSE 13.817647334121858 Test RE 0.05143635874991616\n",
      "21 Train Loss 0.24867451 Test MSE 6.937090372875576 Test RE 0.03644532612800765\n",
      "22 Train Loss 0.21702631 Test MSE 5.018373217970166 Test RE 0.030998069173290614\n",
      "23 Train Loss 0.18837634 Test MSE 5.169199211951754 Test RE 0.03146044051487642\n",
      "24 Train Loss 0.1740524 Test MSE 5.1877751543034085 Test RE 0.031516917661309436\n",
      "25 Train Loss 0.15361948 Test MSE 3.0895988687681397 Test RE 0.02432227605673739\n",
      "26 Train Loss 0.13933405 Test MSE 0.7869590400336357 Test RE 0.012275218433268236\n",
      "27 Train Loss 0.12456915 Test MSE 0.2326594452377466 Test RE 0.006674419644006215\n",
      "28 Train Loss 0.11935816 Test MSE 0.18734284893736677 Test RE 0.00598924011590382\n",
      "29 Train Loss 0.111595646 Test MSE 0.14926702721103688 Test RE 0.005346075729625774\n",
      "30 Train Loss 0.10082082 Test MSE 0.7297908298874349 Test RE 0.011820949599916612\n",
      "31 Train Loss 0.09724879 Test MSE 1.4763762705329744 Test RE 0.01681325085803635\n",
      "32 Train Loss 0.089755975 Test MSE 1.8784499454462167 Test RE 0.01896500572722705\n",
      "33 Train Loss 0.08635748 Test MSE 2.6510627996052927 Test RE 0.02253010275696369\n",
      "34 Train Loss 0.08375157 Test MSE 3.124206717980304 Test RE 0.024458118545625747\n",
      "35 Train Loss 0.08139687 Test MSE 2.084363233452964 Test RE 0.019977441456612066\n",
      "36 Train Loss 0.07008273 Test MSE 1.996705095056971 Test RE 0.019552852663461563\n",
      "37 Train Loss 0.05921909 Test MSE 1.4341808032780943 Test RE 0.016571244198012713\n",
      "38 Train Loss 0.053363465 Test MSE 1.1253027836630973 Test RE 0.014678709014712896\n",
      "39 Train Loss 0.04949529 Test MSE 1.017336125119027 Test RE 0.013956785304767821\n",
      "40 Train Loss 0.047418546 Test MSE 1.030541583272427 Test RE 0.014047075769465091\n",
      "41 Train Loss 0.046036486 Test MSE 0.6820377709260105 Test RE 0.01142766176926828\n",
      "42 Train Loss 0.042802025 Test MSE 1.1209928122825372 Test RE 0.014650571916906953\n",
      "43 Train Loss 0.040984914 Test MSE 0.5210370436617027 Test RE 0.009988205410789977\n",
      "44 Train Loss 0.038046803 Test MSE 0.32405841353083725 Test RE 0.00787707200555884\n",
      "45 Train Loss 0.036620386 Test MSE 0.22714593099306057 Test RE 0.00659486098554815\n",
      "46 Train Loss 0.033093832 Test MSE 0.05921839661471793 Test RE 0.003367297491815871\n",
      "47 Train Loss 0.031538088 Test MSE 0.08350460735536584 Test RE 0.003998603886952864\n",
      "48 Train Loss 0.029978875 Test MSE 0.05008469122048541 Test RE 0.0030967465682683513\n",
      "49 Train Loss 0.028050803 Test MSE 0.0735431234420082 Test RE 0.0037525302344635953\n",
      "50 Train Loss 0.027345004 Test MSE 0.09923772545099441 Test RE 0.004359047138465189\n",
      "51 Train Loss 0.026500281 Test MSE 0.07479272569312495 Test RE 0.0037842763620521288\n",
      "52 Train Loss 0.02421056 Test MSE 0.2277391341476788 Test RE 0.006603466778080912\n",
      "53 Train Loss 0.023105009 Test MSE 0.14163624417264187 Test RE 0.005207632942652741\n",
      "54 Train Loss 0.022369074 Test MSE 0.2558177919898981 Test RE 0.006998718692169204\n",
      "55 Train Loss 0.020967783 Test MSE 0.1736992391338228 Test RE 0.005767028832905026\n",
      "56 Train Loss 0.019409304 Test MSE 0.028811104880070708 Test RE 0.00234873014265829\n",
      "57 Train Loss 0.018670835 Test MSE 0.03134054135463972 Test RE 0.002449663389316088\n",
      "58 Train Loss 0.01654638 Test MSE 0.03834445043380759 Test RE 0.002709595257591579\n",
      "59 Train Loss 0.015022587 Test MSE 0.02787956262800018 Test RE 0.00231044770366682\n",
      "60 Train Loss 0.013876797 Test MSE 0.05709699793150794 Test RE 0.0033064335714427947\n",
      "61 Train Loss 0.012847484 Test MSE 0.19022032598870708 Test RE 0.0060350604629264655\n",
      "62 Train Loss 0.012514349 Test MSE 0.2685696879148684 Test RE 0.007171032023616077\n",
      "63 Train Loss 0.011745483 Test MSE 0.18698447369083607 Test RE 0.005983508851330525\n",
      "64 Train Loss 0.010462382 Test MSE 0.4473155219618563 Test RE 0.009254653197895356\n",
      "65 Train Loss 0.010058622 Test MSE 0.5569572880372855 Test RE 0.010326760628455967\n",
      "66 Train Loss 0.009683052 Test MSE 0.6912628474912679 Test RE 0.01150468607422595\n",
      "67 Train Loss 0.009165153 Test MSE 0.6774592895398579 Test RE 0.011389240553868488\n",
      "68 Train Loss 0.008776092 Test MSE 0.384383305195414 Test RE 0.008578975411686772\n",
      "69 Train Loss 0.008247822 Test MSE 0.22554193334485081 Test RE 0.006571534830955782\n",
      "70 Train Loss 0.0074368003 Test MSE 0.14267938898772956 Test RE 0.005226774758390029\n",
      "71 Train Loss 0.0067924396 Test MSE 0.21907080571443963 Test RE 0.0064765752734747455\n",
      "72 Train Loss 0.0061352057 Test MSE 0.0765417811483803 Test RE 0.0038282690010722708\n",
      "73 Train Loss 0.005833741 Test MSE 0.12710938773331504 Test RE 0.0049333505701577565\n",
      "74 Train Loss 0.005577862 Test MSE 0.0709196348572411 Test RE 0.003684990818607465\n",
      "75 Train Loss 0.0053707515 Test MSE 0.07809621217645035 Test RE 0.0038669463774210775\n",
      "76 Train Loss 0.0052283974 Test MSE 0.10531261616786897 Test RE 0.0044904861939565645\n",
      "77 Train Loss 0.005041028 Test MSE 0.03008400065508373 Test RE 0.002400053702587423\n",
      "78 Train Loss 0.00486457 Test MSE 0.027672493459864306 Test RE 0.002301851547611751\n",
      "79 Train Loss 0.004552367 Test MSE 0.024410813887142148 Test RE 0.002161943273544468\n",
      "80 Train Loss 0.004287114 Test MSE 0.009285552366054073 Test RE 0.0013333895171215996\n",
      "81 Train Loss 0.0041521555 Test MSE 0.029797009937091702 Test RE 0.0023885784381039933\n",
      "82 Train Loss 0.0040641855 Test MSE 0.019911490689088134 Test RE 0.0019525629855512401\n",
      "83 Train Loss 0.003940378 Test MSE 0.05293418606842074 Test RE 0.0031836204409185502\n",
      "84 Train Loss 0.0037205871 Test MSE 0.02848397105882493 Test RE 0.0023353578234937577\n",
      "85 Train Loss 0.003582376 Test MSE 0.01507619267654652 Test RE 0.0016990220072189838\n",
      "86 Train Loss 0.0035407713 Test MSE 0.020715432423550895 Test RE 0.0019915910520237686\n",
      "87 Train Loss 0.0034653714 Test MSE 0.0331887804769883 Test RE 0.0025208604924525846\n",
      "88 Train Loss 0.003390023 Test MSE 0.05428149016585703 Test RE 0.003223881317457108\n",
      "89 Train Loss 0.0032749868 Test MSE 0.0320533784217417 Test RE 0.002477365413426241\n",
      "90 Train Loss 0.0032542993 Test MSE 0.02385526121613097 Test RE 0.002137200430102827\n",
      "91 Train Loss 0.0031561228 Test MSE 0.031576203725537114 Test RE 0.002458856151977871\n",
      "92 Train Loss 0.0028508694 Test MSE 0.009979639792376186 Test RE 0.00138232638515974\n",
      "93 Train Loss 0.0025940682 Test MSE 0.004235633143972023 Test RE 0.0009005593376985357\n",
      "94 Train Loss 0.0025166678 Test MSE 0.0030543804846790527 Test RE 0.0007647416307160782\n",
      "95 Train Loss 0.0024752137 Test MSE 0.0024203104628395386 Test RE 0.0006807516517908816\n",
      "96 Train Loss 0.0024007175 Test MSE 0.012028222278904653 Test RE 0.0015175880120878466\n",
      "97 Train Loss 0.0022839515 Test MSE 0.019065813603616352 Test RE 0.0019106486689172586\n",
      "98 Train Loss 0.0022425249 Test MSE 0.012362665785319261 Test RE 0.0015385415487245166\n",
      "99 Train Loss 0.0022162858 Test MSE 0.009813793959516178 Test RE 0.0013707922251787372\n",
      "Training time: 93.82\n",
      "Training time: 93.82\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 47.66047 Test MSE 5126.034958851813 Test RE 0.9907040685445702\n",
      "1 Train Loss 45.02302 Test MSE 4614.314025817683 Test RE 0.9399542982361809\n",
      "2 Train Loss 39.08954 Test MSE 5686.130714358755 Test RE 1.0434258318693603\n",
      "3 Train Loss 32.803078 Test MSE 7973.35393345683 Test RE 1.235588011981487\n",
      "4 Train Loss 27.692516 Test MSE 8130.463920962987 Test RE 1.2477018762521763\n",
      "5 Train Loss 22.184168 Test MSE 7169.365097668627 Test RE 1.1716381740064155\n",
      "6 Train Loss 15.3747425 Test MSE 6682.0858397560105 Test RE 1.1311213241406293\n",
      "7 Train Loss 12.480404 Test MSE 7308.361546949804 Test RE 1.1829412511272073\n",
      "8 Train Loss 11.7613945 Test MSE 8098.847388635823 Test RE 1.2452735750068342\n",
      "9 Train Loss 10.573557 Test MSE 8416.14682020493 Test RE 1.2694330943579022\n",
      "10 Train Loss 9.590937 Test MSE 6920.076750821245 Test RE 1.1510882487525753\n",
      "11 Train Loss 8.679106 Test MSE 6312.045391835672 Test RE 1.0993556717709858\n",
      "12 Train Loss 8.240045 Test MSE 6448.620867578662 Test RE 1.111185554312268\n",
      "13 Train Loss 8.099379 Test MSE 6442.8547958592235 Test RE 1.1106886567100247\n",
      "14 Train Loss 7.8605585 Test MSE 6334.553486594121 Test RE 1.1013140212720782\n",
      "15 Train Loss 7.7453713 Test MSE 6112.067808369796 Test RE 1.0818006711584665\n",
      "16 Train Loss 7.60472 Test MSE 5757.069055056251 Test RE 1.0499143801276556\n",
      "17 Train Loss 7.384554 Test MSE 5799.119715594287 Test RE 1.0537417849002881\n",
      "18 Train Loss 7.204649 Test MSE 5889.676658100107 Test RE 1.0619373382115664\n",
      "19 Train Loss 7.115125 Test MSE 5940.478482634164 Test RE 1.0665074124822278\n",
      "20 Train Loss 6.9461303 Test MSE 5724.724669880681 Test RE 1.0469609096936652\n",
      "21 Train Loss 6.8757133 Test MSE 5381.224469890984 Test RE 1.0150646857517343\n",
      "22 Train Loss 6.790459 Test MSE 5371.419786485782 Test RE 1.0141395315024222\n",
      "23 Train Loss 6.745149 Test MSE 5428.535039810936 Test RE 1.0195170365048076\n",
      "24 Train Loss 6.6335707 Test MSE 5316.304291914985 Test RE 1.0089231337060884\n",
      "25 Train Loss 6.463556 Test MSE 5037.430766636784 Test RE 0.9821045200487988\n",
      "26 Train Loss 6.2498484 Test MSE 5227.379039269283 Test RE 1.000449475168346\n",
      "27 Train Loss 5.9753976 Test MSE 4495.406260462314 Test RE 0.9277642587326771\n",
      "28 Train Loss 5.691294 Test MSE 4147.79640401556 Test RE 0.8911727125319686\n",
      "29 Train Loss 5.4571447 Test MSE 3906.0578164188655 Test RE 0.8648135754056304\n",
      "30 Train Loss 4.9008937 Test MSE 3652.804789076391 Test RE 0.8363082796935773\n",
      "31 Train Loss 4.3467493 Test MSE 3195.2048627930335 Test RE 0.7821724570833731\n",
      "32 Train Loss 4.002331 Test MSE 2579.326166355867 Test RE 0.7027588065685985\n",
      "33 Train Loss 2.9841814 Test MSE 1047.7292067311078 Test RE 0.44789652187926066\n",
      "34 Train Loss 2.1305263 Test MSE 1002.613072735101 Test RE 0.43814700376110965\n",
      "35 Train Loss 1.6477667 Test MSE 792.3398464292617 Test RE 0.3895013014753588\n",
      "36 Train Loss 1.1259836 Test MSE 452.6028459117063 Test RE 0.2943823750931738\n",
      "37 Train Loss 0.96813506 Test MSE 398.92149402578946 Test RE 0.27637380840540166\n",
      "38 Train Loss 0.77157074 Test MSE 236.69107905973695 Test RE 0.21288453314387826\n",
      "39 Train Loss 0.59985334 Test MSE 103.37278118080236 Test RE 0.1406877438536187\n",
      "40 Train Loss 0.38992473 Test MSE 13.960499091253682 Test RE 0.05170155877319881\n",
      "41 Train Loss 0.28044307 Test MSE 22.307440905261977 Test RE 0.06535488281278852\n",
      "42 Train Loss 0.17864652 Test MSE 8.44700684454326 Test RE 0.04021652926340415\n",
      "43 Train Loss 0.14518741 Test MSE 9.861899790181498 Test RE 0.04345436993685818\n",
      "44 Train Loss 0.12207341 Test MSE 9.464088723960211 Test RE 0.0425689135518658\n",
      "45 Train Loss 0.11004574 Test MSE 11.32720445315623 Test RE 0.04657088965254131\n",
      "46 Train Loss 0.084205285 Test MSE 9.775057604789238 Test RE 0.04326262104021547\n",
      "47 Train Loss 0.06555129 Test MSE 6.8586670240374 Test RE 0.0362387345892187\n",
      "48 Train Loss 0.059258018 Test MSE 3.513855773286826 Test RE 0.025938516304181685\n",
      "49 Train Loss 0.055775106 Test MSE 2.9696817019341064 Test RE 0.023845592401941673\n",
      "50 Train Loss 0.0465817 Test MSE 4.387626936225311 Test RE 0.028984646743546765\n",
      "51 Train Loss 0.03809542 Test MSE 4.566884747969321 Test RE 0.02957080793636987\n",
      "52 Train Loss 0.03389022 Test MSE 4.696016213129422 Test RE 0.029985960081477133\n",
      "53 Train Loss 0.02369207 Test MSE 0.27099021102153736 Test RE 0.007203274520006966\n",
      "54 Train Loss 0.017322376 Test MSE 0.1612867961380902 Test RE 0.005557155792058238\n",
      "55 Train Loss 0.01640637 Test MSE 0.3240047386226142 Test RE 0.007876419625296114\n",
      "56 Train Loss 0.015909957 Test MSE 0.11597770092300082 Test RE 0.0047123811741152585\n",
      "57 Train Loss 0.015190638 Test MSE 0.057030360444274106 Test RE 0.0033045035509730513\n",
      "58 Train Loss 0.0147462655 Test MSE 0.12180563402747195 Test RE 0.0048293296688056275\n",
      "59 Train Loss 0.014062317 Test MSE 0.2735120437182148 Test RE 0.007236713710960667\n",
      "60 Train Loss 0.012219082 Test MSE 0.5007695341180983 Test RE 0.009792015998137312\n",
      "61 Train Loss 0.011679162 Test MSE 0.4600165532206873 Test RE 0.009385121394094737\n",
      "62 Train Loss 0.011555803 Test MSE 0.301195989512584 Test RE 0.0075941252747351515\n",
      "63 Train Loss 0.010398164 Test MSE 0.060728843213065965 Test RE 0.0034099708697391145\n",
      "64 Train Loss 0.009658051 Test MSE 0.1523235158780986 Test RE 0.0054005332262215\n",
      "65 Train Loss 0.009174886 Test MSE 0.1533695332663021 Test RE 0.005419044441201799\n",
      "66 Train Loss 0.0090132775 Test MSE 0.2649423616274433 Test RE 0.007122441096742731\n",
      "67 Train Loss 0.008877468 Test MSE 0.17535988226087618 Test RE 0.005794530955417134\n",
      "68 Train Loss 0.0086859865 Test MSE 0.26344522111884267 Test RE 0.007102288785476062\n",
      "69 Train Loss 0.008356678 Test MSE 0.1516254782560082 Test RE 0.005388144777477748\n",
      "70 Train Loss 0.0077915234 Test MSE 0.11155770205077409 Test RE 0.004621712696491518\n",
      "71 Train Loss 0.007418119 Test MSE 0.20122594309221895 Test RE 0.0062071915897224566\n",
      "72 Train Loss 0.007196707 Test MSE 0.19064405097723125 Test RE 0.006041778418594437\n",
      "73 Train Loss 0.006368435 Test MSE 0.10542635922663171 Test RE 0.004492910517916076\n",
      "74 Train Loss 0.005769733 Test MSE 0.18688384697211496 Test RE 0.005981898605629278\n",
      "75 Train Loss 0.0056535336 Test MSE 0.14138801936791878 Test RE 0.005203067619209061\n",
      "76 Train Loss 0.0055966256 Test MSE 0.13193016177628514 Test RE 0.005026031569672496\n",
      "77 Train Loss 0.0055200243 Test MSE 0.14833242120678408 Test RE 0.005329312750351363\n",
      "78 Train Loss 0.005435678 Test MSE 0.16905222403448475 Test RE 0.005689362540884191\n",
      "79 Train Loss 0.0052813664 Test MSE 0.23657932671825418 Test RE 0.0067304106082427245\n",
      "80 Train Loss 0.0050037424 Test MSE 0.10782128084239496 Test RE 0.0045436556282659555\n",
      "81 Train Loss 0.0045110746 Test MSE 0.0443454157452953 Test RE 0.0029139193679799543\n",
      "82 Train Loss 0.004304094 Test MSE 0.04016512863034894 Test RE 0.002773177996992214\n",
      "83 Train Loss 0.004234814 Test MSE 0.02698725369728822 Test RE 0.002273173112742253\n",
      "84 Train Loss 0.0041691563 Test MSE 0.026203999757886738 Test RE 0.0022399429541089975\n",
      "85 Train Loss 0.004138332 Test MSE 0.024634833377703377 Test RE 0.00217184075893823\n",
      "86 Train Loss 0.0040967325 Test MSE 0.04182403257398148 Test RE 0.002829867599551936\n",
      "87 Train Loss 0.0040883347 Test MSE 0.04563934780924393 Test RE 0.0029561255801245283\n",
      "88 Train Loss 0.0039819716 Test MSE 0.03915447338960845 Test RE 0.002738065658884306\n",
      "89 Train Loss 0.003395824 Test MSE 0.10005725335658901 Test RE 0.004377009136834316\n",
      "90 Train Loss 0.0030214305 Test MSE 0.014603129306055293 Test RE 0.0016721534532344146\n",
      "91 Train Loss 0.0029328363 Test MSE 0.025660182450671393 Test RE 0.002216578082790846\n",
      "92 Train Loss 0.0028642274 Test MSE 0.025076545664464243 Test RE 0.0021912252332292096\n",
      "93 Train Loss 0.0028395925 Test MSE 0.023944944734481617 Test RE 0.002141214048804644\n",
      "94 Train Loss 0.002823823 Test MSE 0.023683239596225373 Test RE 0.0021294807523811587\n",
      "95 Train Loss 0.002821349 Test MSE 0.02432059388783499 Test RE 0.002157944408998087\n",
      "96 Train Loss 0.0028020649 Test MSE 0.02252739084573643 Test RE 0.0020768666458986355\n",
      "97 Train Loss 0.0027992958 Test MSE 0.022954273632501507 Test RE 0.002096452089716348\n",
      "98 Train Loss 0.0027983442 Test MSE 0.022892292519771613 Test RE 0.0020936197572653465\n",
      "99 Train Loss 0.0027977366 Test MSE 0.022753141232908582 Test RE 0.0020872470021655208\n",
      "Training time: 72.51\n",
      "Training time: 72.51\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 47.063236 Test MSE 5098.159176018952 Test RE 0.988006632855306\n",
      "1 Train Loss 43.92039 Test MSE 4408.895313515137 Test RE 0.9187938046610282\n",
      "2 Train Loss 37.20308 Test MSE 5885.285711963687 Test RE 1.0615414099344285\n",
      "3 Train Loss 30.889751 Test MSE 7130.3037520444705 Test RE 1.1684420561754771\n",
      "4 Train Loss 29.569378 Test MSE 6354.744675840454 Test RE 1.1030678266703966\n",
      "5 Train Loss 22.834969 Test MSE 8633.260656297647 Test RE 1.2857028048992614\n",
      "6 Train Loss 17.585085 Test MSE 6597.534825524515 Test RE 1.123942284450801\n",
      "7 Train Loss 13.226805 Test MSE 8112.872856479454 Test RE 1.2463513820416268\n",
      "8 Train Loss 10.308947 Test MSE 9087.971869501862 Test RE 1.3191271279049817\n",
      "9 Train Loss 8.175771 Test MSE 5871.789874067242 Test RE 1.0603235749763151\n",
      "10 Train Loss 7.3998513 Test MSE 5747.252072917494 Test RE 1.0490188386419421\n",
      "11 Train Loss 7.0065484 Test MSE 5535.847583206158 Test RE 1.029544746467366\n",
      "12 Train Loss 6.7711396 Test MSE 4863.370997866807 Test RE 0.9649878938320571\n",
      "13 Train Loss 6.1999054 Test MSE 4957.10703698209 Test RE 0.9742430424442827\n",
      "14 Train Loss 5.882464 Test MSE 4377.767369971791 Test RE 0.9155445982526489\n",
      "15 Train Loss 5.677082 Test MSE 4256.368356616405 Test RE 0.9027609556256715\n",
      "16 Train Loss 5.107077 Test MSE 3797.7313476308286 Test RE 0.8527373476963124\n",
      "17 Train Loss 4.566533 Test MSE 3167.2078994431085 Test RE 0.7787381492021513\n",
      "18 Train Loss 3.7386758 Test MSE 2834.30652677457 Test RE 0.736676087880065\n",
      "19 Train Loss 2.967483 Test MSE 1742.5112767634967 Test RE 0.5776183257085585\n",
      "20 Train Loss 2.7932222 Test MSE 1453.6046123944263 Test RE 0.5275654072147229\n",
      "21 Train Loss 1.8389499 Test MSE 874.6892052251087 Test RE 0.4092418583257119\n",
      "22 Train Loss 1.3960638 Test MSE 779.3287281984864 Test RE 0.38629003736234746\n",
      "23 Train Loss 1.1122205 Test MSE 697.5021418833118 Test RE 0.3654482928490037\n",
      "24 Train Loss 1.016821 Test MSE 738.6543347079624 Test RE 0.3760744162583644\n",
      "25 Train Loss 0.89868855 Test MSE 806.6137784707513 Test RE 0.39299405696562917\n",
      "26 Train Loss 0.7190144 Test MSE 630.697288719581 Test RE 0.3475070715950622\n",
      "27 Train Loss 0.6298683 Test MSE 445.85822589713905 Test RE 0.29218072103228837\n",
      "28 Train Loss 0.60069764 Test MSE 389.6982361331228 Test RE 0.2731601765348269\n",
      "29 Train Loss 0.5419881 Test MSE 319.6689283400087 Test RE 0.24740209746346187\n",
      "30 Train Loss 0.4287288 Test MSE 201.66669685435505 Test RE 0.19650348506070608\n",
      "31 Train Loss 0.26426798 Test MSE 36.864420326816564 Test RE 0.084015007269372\n",
      "32 Train Loss 0.24170384 Test MSE 26.279431392750563 Test RE 0.07093509370375871\n",
      "33 Train Loss 0.21908148 Test MSE 27.693067217189157 Test RE 0.0728179912884218\n",
      "34 Train Loss 0.16663995 Test MSE 16.803273570199003 Test RE 0.056721814302013014\n",
      "35 Train Loss 0.14609097 Test MSE 15.792773111488877 Test RE 0.054989828182754025\n",
      "36 Train Loss 0.12881623 Test MSE 25.59387912910985 Test RE 0.07000373659072368\n",
      "37 Train Loss 0.12020461 Test MSE 22.136758728749808 Test RE 0.06510437596914878\n",
      "38 Train Loss 0.100173235 Test MSE 20.95035841032525 Test RE 0.06333574643423287\n",
      "39 Train Loss 0.08299585 Test MSE 13.778925524877815 Test RE 0.05136423698614014\n",
      "40 Train Loss 0.053798273 Test MSE 6.351178474438448 Test RE 0.034872278400768494\n",
      "41 Train Loss 0.035638016 Test MSE 1.7554818898980782 Test RE 0.018333751538332856\n",
      "42 Train Loss 0.03146403 Test MSE 1.4771529408703843 Test RE 0.01681767271056216\n",
      "43 Train Loss 0.029034032 Test MSE 0.9669167403436222 Test RE 0.01360654005010747\n",
      "44 Train Loss 0.027496751 Test MSE 1.0496762562989672 Test RE 0.014176886142330536\n",
      "45 Train Loss 0.025190916 Test MSE 1.0681601558840645 Test RE 0.014301162837925005\n",
      "46 Train Loss 0.023297435 Test MSE 0.8767590542469177 Test RE 0.012956666920977293\n",
      "47 Train Loss 0.021909742 Test MSE 0.4743236753603045 Test RE 0.009529948768546286\n",
      "48 Train Loss 0.02031298 Test MSE 0.34315509546682776 Test RE 0.008105846788121356\n",
      "49 Train Loss 0.018898083 Test MSE 0.35585640307586636 Test RE 0.008254495916440495\n",
      "50 Train Loss 0.017610934 Test MSE 0.4487073357883377 Test RE 0.009269039855243102\n",
      "51 Train Loss 0.016585538 Test MSE 0.2659513340380804 Test RE 0.007135990303495459\n",
      "52 Train Loss 0.016110418 Test MSE 0.20591760615474328 Test RE 0.006279136224438072\n",
      "53 Train Loss 0.015919734 Test MSE 0.12345542231789941 Test RE 0.004861924935805284\n",
      "54 Train Loss 0.0153698 Test MSE 0.2249349114271949 Test RE 0.006562685582238715\n",
      "55 Train Loss 0.014331805 Test MSE 0.10825193405124539 Test RE 0.004552720582235974\n",
      "56 Train Loss 0.013840779 Test MSE 0.12518262040379982 Test RE 0.004895817087799695\n",
      "57 Train Loss 0.013536432 Test MSE 0.10190930693434229 Test RE 0.004417332481098111\n",
      "58 Train Loss 0.013099979 Test MSE 0.08335433552243206 Test RE 0.003995004396295284\n",
      "59 Train Loss 0.012541901 Test MSE 0.14315434360862425 Test RE 0.0052354670381133395\n",
      "60 Train Loss 0.011498823 Test MSE 0.14404593806619068 Test RE 0.005251745509458887\n",
      "61 Train Loss 0.010262784 Test MSE 0.09950266492568056 Test RE 0.004364862033302812\n",
      "62 Train Loss 0.0096363565 Test MSE 0.22017850539244738 Test RE 0.006492928558153408\n",
      "63 Train Loss 0.009474044 Test MSE 0.3136279151256385 Test RE 0.0077492651351914865\n",
      "64 Train Loss 0.009174917 Test MSE 0.2928051042737215 Test RE 0.0074875974253815454\n",
      "65 Train Loss 0.008706886 Test MSE 0.15258399333878517 Test RE 0.005405148785103696\n",
      "66 Train Loss 0.008550238 Test MSE 0.1408198044798792 Test RE 0.005192601962846488\n",
      "67 Train Loss 0.008506786 Test MSE 0.13330882674189343 Test RE 0.005052224234859922\n",
      "68 Train Loss 0.0084374845 Test MSE 0.13052446690570674 Test RE 0.004999184081111475\n",
      "69 Train Loss 0.008218427 Test MSE 0.15971752753733148 Test RE 0.0055300550421585925\n",
      "70 Train Loss 0.007911016 Test MSE 0.14314708480119837 Test RE 0.005235334301209176\n",
      "71 Train Loss 0.006891677 Test MSE 0.11494698251853447 Test RE 0.004691394476708696\n",
      "72 Train Loss 0.0063980836 Test MSE 0.11446847084421742 Test RE 0.004681619412775488\n",
      "73 Train Loss 0.0061261556 Test MSE 0.16162366909947481 Test RE 0.00556295626386349\n",
      "74 Train Loss 0.0060128 Test MSE 0.22408066351492334 Test RE 0.00655021198874329\n",
      "75 Train Loss 0.005952057 Test MSE 0.27440353503622167 Test RE 0.007248497870246682\n",
      "76 Train Loss 0.0059196437 Test MSE 0.2695463620219797 Test RE 0.007184059192610647\n",
      "77 Train Loss 0.0059196437 Test MSE 0.2695463620219797 Test RE 0.007184059192610647\n",
      "78 Train Loss 0.0059195356 Test MSE 0.26949067581748976 Test RE 0.007183317068602057\n",
      "79 Train Loss 0.005875257 Test MSE 0.2734694926024795 Test RE 0.007236150770082782\n",
      "80 Train Loss 0.005833674 Test MSE 0.27413185811898083 Test RE 0.007244908745169071\n",
      "81 Train Loss 0.0055746594 Test MSE 0.09910689114318609 Test RE 0.004356172722457548\n",
      "82 Train Loss 0.0052908952 Test MSE 0.12470690214230544 Test RE 0.004886505705390614\n",
      "83 Train Loss 0.0051565254 Test MSE 0.047894723029700194 Test RE 0.0030282867597682768\n",
      "84 Train Loss 0.005060807 Test MSE 0.06038250969651169 Test RE 0.003400233521421001\n",
      "85 Train Loss 0.004976513 Test MSE 0.1040442351991379 Test RE 0.00446336265913182\n",
      "86 Train Loss 0.0048868624 Test MSE 0.0848859746238541 Test RE 0.004031541496910412\n",
      "87 Train Loss 0.0048515447 Test MSE 0.12264099652311206 Test RE 0.0048458615298593525\n",
      "88 Train Loss 0.0048433444 Test MSE 0.11862841291829661 Test RE 0.0047659285234914325\n",
      "89 Train Loss 0.0048427945 Test MSE 0.11838485310108494 Test RE 0.0047610334689381275\n",
      "90 Train Loss 0.004842265 Test MSE 0.11845348480705145 Test RE 0.004762413335095644\n",
      "91 Train Loss 0.0048417947 Test MSE 0.12010553688059886 Test RE 0.004795508654755085\n",
      "92 Train Loss 0.00484137 Test MSE 0.1239858207845501 Test RE 0.004872357825886042\n",
      "93 Train Loss 0.00484137 Test MSE 0.1239858207845501 Test RE 0.004872357825886042\n",
      "94 Train Loss 0.00484137 Test MSE 0.1239858207845501 Test RE 0.004872357825886042\n",
      "95 Train Loss 0.00484137 Test MSE 0.1239858207845501 Test RE 0.004872357825886042\n",
      "96 Train Loss 0.00484137 Test MSE 0.1239858207845501 Test RE 0.004872357825886042\n",
      "97 Train Loss 0.00484137 Test MSE 0.1239858207845501 Test RE 0.004872357825886042\n",
      "98 Train Loss 0.00484137 Test MSE 0.1239858207845501 Test RE 0.004872357825886042\n",
      "99 Train Loss 0.00484137 Test MSE 0.1239858207845501 Test RE 0.004872357825886042\n",
      "Training time: 68.12\n",
      "Training time: 68.12\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 47.771988 Test MSE 5227.981812457502 Test RE 1.0005071548135276\n",
      "1 Train Loss 47.735573 Test MSE 5177.2272779448795 Test RE 0.9956387251299458\n",
      "2 Train Loss 47.10605 Test MSE 5095.575646822153 Test RE 0.9877562613627251\n",
      "3 Train Loss 41.848835 Test MSE 4480.73459389532 Test RE 0.9262490483732247\n",
      "4 Train Loss 29.595226 Test MSE 7596.664129082659 Test RE 1.206048095976291\n",
      "5 Train Loss 24.520573 Test MSE 6180.233614444331 Test RE 1.0878164215830464\n",
      "6 Train Loss 21.548073 Test MSE 6789.089712584235 Test RE 1.14014198469585\n",
      "7 Train Loss 17.605772 Test MSE 5608.968191565404 Test RE 1.0363218458251953\n",
      "8 Train Loss 13.908928 Test MSE 4176.3451270515625 Test RE 0.8942343641048759\n",
      "9 Train Loss 7.6422434 Test MSE 1610.0329322722346 Test RE 0.5552269580622624\n",
      "10 Train Loss 3.0240946 Test MSE 680.2314096226451 Test RE 0.3608955320596121\n",
      "11 Train Loss 1.7186756 Test MSE 487.0637471306227 Test RE 0.30538385105090726\n",
      "12 Train Loss 0.86696863 Test MSE 277.4535931141101 Test RE 0.2304880116510101\n",
      "13 Train Loss 0.42361805 Test MSE 115.82113078298325 Test RE 0.1489179554635191\n",
      "14 Train Loss 0.29428208 Test MSE 77.91702225966941 Test RE 0.12214321246268446\n",
      "15 Train Loss 0.20721668 Test MSE 48.539490859425335 Test RE 0.09640526948965306\n",
      "16 Train Loss 0.16241048 Test MSE 43.570577244922376 Test RE 0.09133764746869374\n",
      "17 Train Loss 0.14447682 Test MSE 39.55937041473205 Test RE 0.08703177686342836\n",
      "18 Train Loss 0.11976039 Test MSE 35.103991025194034 Test RE 0.08198443576608275\n",
      "19 Train Loss 0.1105101 Test MSE 26.776974190504884 Test RE 0.07160344450445072\n",
      "20 Train Loss 0.09256508 Test MSE 18.489432894401027 Test RE 0.05949972521464194\n",
      "21 Train Loss 0.08728398 Test MSE 21.05803934018178 Test RE 0.06349830477658774\n",
      "22 Train Loss 0.0755168 Test MSE 12.958287035801009 Test RE 0.0498111951918586\n",
      "23 Train Loss 0.06892055 Test MSE 9.773047186481287 Test RE 0.04325817193917622\n",
      "24 Train Loss 0.059879806 Test MSE 6.987857491238865 Test RE 0.036578440397212475\n",
      "25 Train Loss 0.0518282 Test MSE 3.4004218376157835 Test RE 0.025516409479713252\n",
      "26 Train Loss 0.04875205 Test MSE 2.349819507766759 Test RE 0.0212114529077572\n",
      "27 Train Loss 0.04325311 Test MSE 0.8562978027927031 Test RE 0.01280458712491384\n",
      "28 Train Loss 0.03952952 Test MSE 0.5542150280236079 Test RE 0.010301306606046171\n",
      "29 Train Loss 0.03770808 Test MSE 0.8171416202006737 Test RE 0.012508402001132929\n",
      "30 Train Loss 0.03219264 Test MSE 0.0789736346072063 Test RE 0.0038886085585832396\n",
      "31 Train Loss 0.027458245 Test MSE 0.46794984523984995 Test RE 0.00946570179985338\n",
      "32 Train Loss 0.025136009 Test MSE 0.12447708894921271 Test RE 0.004882001137868176\n",
      "33 Train Loss 0.022714831 Test MSE 0.2985702570356707 Test RE 0.0075609512111551\n",
      "34 Train Loss 0.02060718 Test MSE 0.3814637913116182 Test RE 0.008546333283528714\n",
      "35 Train Loss 0.019482853 Test MSE 0.2497984062738318 Test RE 0.006915888714309528\n",
      "36 Train Loss 0.017594403 Test MSE 0.2641797272771043 Test RE 0.007112182766021408\n",
      "37 Train Loss 0.016030438 Test MSE 0.20459056309933746 Test RE 0.006258870466746017\n",
      "38 Train Loss 0.015299653 Test MSE 0.3303504510621251 Test RE 0.007953176429542274\n",
      "39 Train Loss 0.014604414 Test MSE 0.3703831222984892 Test RE 0.00842129263110114\n",
      "40 Train Loss 0.012163246 Test MSE 0.22198394779383698 Test RE 0.006519494901375615\n",
      "41 Train Loss 0.008983337 Test MSE 0.08979457078453322 Test RE 0.004146466911718462\n",
      "42 Train Loss 0.008645361 Test MSE 0.13506113289891822 Test RE 0.00508532084527343\n",
      "43 Train Loss 0.00832473 Test MSE 0.059565615861615004 Test RE 0.003377154915551871\n",
      "44 Train Loss 0.008007194 Test MSE 0.020654900538392935 Test RE 0.0019886791418459314\n",
      "45 Train Loss 0.007427286 Test MSE 0.013203822774702528 Test RE 0.0015900215406462529\n",
      "46 Train Loss 0.0070498264 Test MSE 0.013439352082601783 Test RE 0.001604140229837346\n",
      "47 Train Loss 0.0068875365 Test MSE 0.005603273814802408 Test RE 0.0010357956612076851\n",
      "48 Train Loss 0.0065682055 Test MSE 0.046151750246434704 Test RE 0.00297267378018607\n",
      "49 Train Loss 0.0063950242 Test MSE 0.07008205275545247 Test RE 0.0036631657651811545\n",
      "50 Train Loss 0.006208051 Test MSE 0.0741146863668807 Test RE 0.0037670839814019\n",
      "51 Train Loss 0.0057211183 Test MSE 0.030117055755911736 Test RE 0.002401371882269391\n",
      "52 Train Loss 0.0056535914 Test MSE 0.02501180657727283 Test RE 0.0021883949072565587\n",
      "53 Train Loss 0.0050350325 Test MSE 0.020693507431439804 Test RE 0.0019905368336725618\n",
      "54 Train Loss 0.0046849446 Test MSE 0.030807678457170515 Test RE 0.002428749091249065\n",
      "55 Train Loss 0.004423726 Test MSE 0.14298200307189926 Test RE 0.005232314654119313\n",
      "56 Train Loss 0.0043575065 Test MSE 0.12188680318546333 Test RE 0.004830938491515228\n",
      "57 Train Loss 0.004082859 Test MSE 0.059482134834107435 Test RE 0.0033747875496811654\n",
      "58 Train Loss 0.0036552565 Test MSE 0.033195864238030656 Test RE 0.0025211295023460324\n",
      "59 Train Loss 0.003470067 Test MSE 0.025056986552495233 Test RE 0.0021903705146276414\n",
      "60 Train Loss 0.0034060196 Test MSE 0.03465134894415443 Test RE 0.0025758065169322756\n",
      "61 Train Loss 0.0033366727 Test MSE 0.05445284413803092 Test RE 0.003228965827716614\n",
      "62 Train Loss 0.0030810705 Test MSE 0.039909925838897166 Test RE 0.0027643537934459303\n",
      "63 Train Loss 0.0028623748 Test MSE 0.039147450313364536 Test RE 0.0027378200866087334\n",
      "64 Train Loss 0.002784473 Test MSE 0.04354354891655781 Test RE 0.002887454010793431\n",
      "65 Train Loss 0.0027489117 Test MSE 0.023715648068342296 Test RE 0.002130937259709619\n",
      "66 Train Loss 0.0027094227 Test MSE 0.008761021676337369 Test RE 0.0012951812305689744\n",
      "67 Train Loss 0.0026365987 Test MSE 0.012830776931616987 Test RE 0.0015673992766447895\n",
      "68 Train Loss 0.002569866 Test MSE 0.00887607548960896 Test RE 0.0013036579525711209\n",
      "69 Train Loss 0.002477212 Test MSE 0.004074097421800451 Test RE 0.0008832199486750056\n",
      "70 Train Loss 0.0023906096 Test MSE 0.0027994845135764545 Test RE 0.0007321367370485792\n",
      "71 Train Loss 0.0023754109 Test MSE 0.0025614110958516677 Test RE 0.0007003139992117842\n",
      "72 Train Loss 0.0023446803 Test MSE 0.0016998204331498009 Test RE 0.000570498737173477\n",
      "73 Train Loss 0.002340976 Test MSE 0.0020349808865052104 Test RE 0.0006242137513975147\n",
      "74 Train Loss 0.0023407857 Test MSE 0.0021100881117451305 Test RE 0.0006356286439594622\n",
      "75 Train Loss 0.0023342613 Test MSE 0.0019685138328627465 Test RE 0.0006139350102439502\n",
      "76 Train Loss 0.0023335072 Test MSE 0.002011414751226374 Test RE 0.0006205888666511501\n",
      "77 Train Loss 0.002332651 Test MSE 0.0021189073245266874 Test RE 0.0006369555788562852\n",
      "78 Train Loss 0.0023316552 Test MSE 0.0022346991702769407 Test RE 0.0006541279382644982\n",
      "79 Train Loss 0.0023298324 Test MSE 0.0025012232197064222 Test RE 0.0006920371207223809\n",
      "80 Train Loss 0.0023148246 Test MSE 0.0024944674878626806 Test RE 0.0006911019026350931\n",
      "81 Train Loss 0.0022975197 Test MSE 0.009391420573095674 Test RE 0.0013409692207438056\n",
      "82 Train Loss 0.0022725246 Test MSE 0.005764077362837538 Test RE 0.0010505532369033876\n",
      "83 Train Loss 0.0022694764 Test MSE 0.006589502282749838 Test RE 0.0011232578724441591\n",
      "84 Train Loss 0.0022634154 Test MSE 0.008189971072029876 Test RE 0.0012522595389648466\n",
      "85 Train Loss 0.0022600652 Test MSE 0.006052383247438039 Test RE 0.00107650580216246\n",
      "86 Train Loss 0.0022357504 Test MSE 0.0034926257431512325 Test RE 0.0008177662635809641\n",
      "87 Train Loss 0.002209317 Test MSE 0.0026497411350797553 Test RE 0.0007122867876594176\n",
      "88 Train Loss 0.0021661937 Test MSE 0.007774323843805625 Test RE 0.0012200692412289175\n",
      "89 Train Loss 0.0021589058 Test MSE 0.0076742947593934985 Test RE 0.0012121947607485442\n",
      "90 Train Loss 0.0021335392 Test MSE 0.0035651334213684634 Test RE 0.0008262111629370137\n",
      "91 Train Loss 0.0020937787 Test MSE 0.009255650496254895 Test RE 0.001331240857109817\n",
      "92 Train Loss 0.0020819434 Test MSE 0.0057297726363689655 Test RE 0.0010474224042932665\n",
      "93 Train Loss 0.0020809846 Test MSE 0.004880439088812176 Test RE 0.0009666797322256492\n",
      "94 Train Loss 0.0020790577 Test MSE 0.005227733140931122 Test RE 0.0010004833597231002\n",
      "95 Train Loss 0.0020770123 Test MSE 0.005208204321271816 Test RE 0.0009986128988931581\n",
      "96 Train Loss 0.0020761683 Test MSE 0.005091253452818225 Test RE 0.0009873372527760141\n",
      "97 Train Loss 0.0020716623 Test MSE 0.0037211968568846843 Test RE 0.0008441011411690314\n",
      "98 Train Loss 0.0020681182 Test MSE 0.004196909460625955 Test RE 0.0008964332666715249\n",
      "99 Train Loss 0.0020674847 Test MSE 0.004520096327849536 Test RE 0.0009303085444388636\n",
      "Training time: 66.16\n",
      "Training time: 66.16\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 47.766613 Test MSE 5216.727716357095 Test RE 0.9994296960299782\n",
      "1 Train Loss 47.542492 Test MSE 5129.0389196561055 Test RE 0.990994312401441\n",
      "2 Train Loss 46.289444 Test MSE 5021.108886346307 Test RE 0.9805121608836257\n",
      "3 Train Loss 41.186165 Test MSE 4450.070126126408 Test RE 0.9230741564834417\n",
      "4 Train Loss 37.00463 Test MSE 4179.797744857219 Test RE 0.8946039230796353\n",
      "5 Train Loss 28.733023 Test MSE 3836.1215576855125 Test RE 0.8570365525014266\n",
      "6 Train Loss 22.578228 Test MSE 3959.0982231390653 Test RE 0.870665433819317\n",
      "7 Train Loss 17.32283 Test MSE 6175.77101057997 Test RE 1.0874236071106862\n",
      "8 Train Loss 15.763401 Test MSE 4763.537812651788 Test RE 0.9550321089565461\n",
      "9 Train Loss 14.023062 Test MSE 5694.562889095097 Test RE 1.0441992127453152\n",
      "10 Train Loss 11.739322 Test MSE 4453.644519345861 Test RE 0.9234447987019839\n",
      "11 Train Loss 10.056978 Test MSE 3427.651389336489 Test RE 0.8101239794851195\n",
      "12 Train Loss 9.013697 Test MSE 2394.287639775843 Test RE 0.6770820905375255\n",
      "13 Train Loss 8.105254 Test MSE 2068.7205481734754 Test RE 0.6293671612310394\n",
      "14 Train Loss 7.564231 Test MSE 1880.8256453890644 Test RE 0.600105260232673\n",
      "15 Train Loss 7.0192385 Test MSE 1734.5292909580276 Test RE 0.5762938483908974\n",
      "16 Train Loss 5.980861 Test MSE 1597.4158118886996 Test RE 0.5530471442948263\n",
      "17 Train Loss 5.0093627 Test MSE 1698.7194949867517 Test RE 0.5703139571956586\n",
      "18 Train Loss 4.222083 Test MSE 1399.5554583318021 Test RE 0.5176643063869488\n",
      "19 Train Loss 3.2602715 Test MSE 1350.5394082252708 Test RE 0.5085185445465517\n",
      "20 Train Loss 2.839602 Test MSE 1404.636185497911 Test RE 0.5186030789184846\n",
      "21 Train Loss 2.3840845 Test MSE 1306.080198346886 Test RE 0.5000783894765308\n",
      "22 Train Loss 2.132832 Test MSE 1361.139700589908 Test RE 0.5105103076932294\n",
      "23 Train Loss 1.7524824 Test MSE 1202.2509492356035 Test RE 0.4797894885371943\n",
      "24 Train Loss 1.518957 Test MSE 1170.834633343654 Test RE 0.47347924331733043\n",
      "25 Train Loss 1.2365366 Test MSE 1185.2719831617596 Test RE 0.47638949276431447\n",
      "26 Train Loss 1.1047271 Test MSE 1067.1977842380725 Test RE 0.45203870502992655\n",
      "27 Train Loss 0.9745663 Test MSE 1047.582071243033 Test RE 0.4478650711061153\n",
      "28 Train Loss 0.8656342 Test MSE 969.1042939347103 Test RE 0.43076303046398445\n",
      "29 Train Loss 0.8236387 Test MSE 938.2426384893017 Test RE 0.4238485946175334\n",
      "30 Train Loss 0.779726 Test MSE 862.9335652667962 Test RE 0.40648249315364693\n",
      "31 Train Loss 0.7485741 Test MSE 857.8112821128202 Test RE 0.40527427879931294\n",
      "32 Train Loss 0.7103395 Test MSE 780.774415417998 Test RE 0.38664816340124825\n",
      "33 Train Loss 0.6283862 Test MSE 610.9828180915398 Test RE 0.34203272725407124\n",
      "34 Train Loss 0.59166825 Test MSE 597.2958588939841 Test RE 0.3381799974734137\n",
      "35 Train Loss 0.5207689 Test MSE 521.6522181205036 Test RE 0.31604119398968833\n",
      "36 Train Loss 0.50655615 Test MSE 481.03366713041163 Test RE 0.30348756514613057\n",
      "37 Train Loss 0.4572426 Test MSE 427.8238475520373 Test RE 0.2862105638490439\n",
      "38 Train Loss 0.4166611 Test MSE 352.55865759089846 Test RE 0.2598177734031658\n",
      "39 Train Loss 0.3617536 Test MSE 288.3004810724666 Test RE 0.2349502157786226\n",
      "40 Train Loss 0.32802224 Test MSE 267.96696856323393 Test RE 0.22651334645779697\n",
      "41 Train Loss 0.28535616 Test MSE 232.83707132250402 Test RE 0.2111442352721677\n",
      "42 Train Loss 0.22869886 Test MSE 166.54276329333618 Test RE 0.17857310404038723\n",
      "43 Train Loss 0.2014285 Test MSE 152.49348924737254 Test RE 0.1708751133067482\n",
      "44 Train Loss 0.1830296 Test MSE 123.56558973579926 Test RE 0.1538161503500855\n",
      "45 Train Loss 0.1690877 Test MSE 113.01128030986669 Test RE 0.1471004706378225\n",
      "46 Train Loss 0.14662796 Test MSE 104.81935554161674 Test RE 0.14166869954195407\n",
      "47 Train Loss 0.13165478 Test MSE 89.73741079831385 Test RE 0.13108105616942103\n",
      "48 Train Loss 0.11275226 Test MSE 76.47770250563684 Test RE 0.12100981054800326\n",
      "49 Train Loss 0.099501334 Test MSE 73.01948818278863 Test RE 0.11824221507776611\n",
      "50 Train Loss 0.07761085 Test MSE 43.9015583125628 Test RE 0.09168391138114575\n",
      "51 Train Loss 0.049399782 Test MSE 16.99660929086936 Test RE 0.057047196993399225\n",
      "52 Train Loss 0.042426668 Test MSE 11.581721853305563 Test RE 0.04709119702374919\n",
      "53 Train Loss 0.038021516 Test MSE 7.791985704713034 Test RE 0.038625777836114296\n",
      "54 Train Loss 0.030815676 Test MSE 3.0001189172501808 Test RE 0.023967481420808616\n",
      "55 Train Loss 0.024971912 Test MSE 0.5143110975605754 Test RE 0.009923528296217506\n",
      "56 Train Loss 0.018898228 Test MSE 0.07266714463478562 Test RE 0.0037301149229861065\n",
      "57 Train Loss 0.017054832 Test MSE 0.038063849983654335 Test RE 0.0026996627937526407\n",
      "58 Train Loss 0.015933746 Test MSE 0.04336034758584266 Test RE 0.002881373397181583\n",
      "59 Train Loss 0.014578941 Test MSE 0.030279579454015014 Test RE 0.002407842546787478\n",
      "60 Train Loss 0.013692282 Test MSE 0.02565966298866083 Test RE 0.00221655564659155\n",
      "61 Train Loss 0.013295324 Test MSE 0.05190736418572733 Test RE 0.003152591155605471\n",
      "62 Train Loss 0.011974446 Test MSE 0.08673067715126351 Test RE 0.004075111849043059\n",
      "63 Train Loss 0.0110897515 Test MSE 0.09036113102172147 Test RE 0.0041595274413571024\n",
      "64 Train Loss 0.007998768 Test MSE 0.08957246416719479 Test RE 0.0041413355995821985\n",
      "65 Train Loss 0.0074493177 Test MSE 0.18715771949795393 Test RE 0.00598628014511218\n",
      "66 Train Loss 0.00706956 Test MSE 0.13740177003274318 Test RE 0.0051291963953482385\n",
      "67 Train Loss 0.0069591366 Test MSE 0.1917710563857082 Test RE 0.006059610297989708\n",
      "68 Train Loss 0.006769322 Test MSE 0.23446917085816585 Test RE 0.006700327619645337\n",
      "69 Train Loss 0.0055768793 Test MSE 0.04098992637513954 Test RE 0.0028015071409287066\n",
      "70 Train Loss 0.0054868944 Test MSE 0.08326590938854317 Test RE 0.003992884791294993\n",
      "71 Train Loss 0.005351598 Test MSE 0.031230116836235625 Test RE 0.002445344038407128\n",
      "72 Train Loss 0.0051981853 Test MSE 0.025257375265104003 Test RE 0.0021991116188581432\n",
      "73 Train Loss 0.0050389995 Test MSE 0.09413470557784873 Test RE 0.004245492216355242\n",
      "74 Train Loss 0.0047697886 Test MSE 0.06348736689548042 Test RE 0.0034865574228566763\n",
      "75 Train Loss 0.004416199 Test MSE 0.077730885388762 Test RE 0.003857891168128667\n",
      "76 Train Loss 0.0041825245 Test MSE 0.05305754938175121 Test RE 0.0031873280015682283\n",
      "77 Train Loss 0.0040878463 Test MSE 0.05553329194396504 Test RE 0.003260842885164487\n",
      "78 Train Loss 0.0035012322 Test MSE 0.08454103065655275 Test RE 0.00402334184098961\n",
      "79 Train Loss 0.002707517 Test MSE 0.023449414410768886 Test RE 0.002118942469729606\n",
      "80 Train Loss 0.0025980077 Test MSE 0.01941563627682142 Test RE 0.0019280974419711649\n",
      "81 Train Loss 0.0025554167 Test MSE 0.019195780083429495 Test RE 0.0019171497954901286\n",
      "82 Train Loss 0.002520286 Test MSE 0.008934144880641095 Test RE 0.0013079154203420916\n",
      "83 Train Loss 0.0023190328 Test MSE 0.0032023031050916763 Test RE 0.0007830407847442897\n",
      "84 Train Loss 0.0021988477 Test MSE 0.003608330019730535 Test RE 0.0008312014466711011\n",
      "85 Train Loss 0.0021055082 Test MSE 0.0070319744181572925 Test RE 0.0011603574788212948\n",
      "86 Train Loss 0.001989809 Test MSE 0.003475921043370395 Test RE 0.0008158082935346599\n",
      "87 Train Loss 0.0019675626 Test MSE 0.0027571402547801158 Test RE 0.0007265785863261956\n",
      "88 Train Loss 0.0019264823 Test MSE 0.003078708882798758 Test RE 0.0007677812058901481\n",
      "89 Train Loss 0.0017783223 Test MSE 0.004812616765162681 Test RE 0.0009599393716145674\n",
      "90 Train Loss 0.0016648747 Test MSE 0.005527488363316227 Test RE 0.001028767138149927\n",
      "91 Train Loss 0.001599964 Test MSE 0.006481134598648185 Test RE 0.001113983312424535\n",
      "92 Train Loss 0.001599706 Test MSE 0.005464402204645161 Test RE 0.0010228795443267948\n",
      "93 Train Loss 0.0015925048 Test MSE 0.005745755542749006 Test RE 0.001048882252450076\n",
      "94 Train Loss 0.0015354878 Test MSE 0.005081814934744254 Test RE 0.0009864216311464874\n",
      "95 Train Loss 0.0014853975 Test MSE 0.0024085129647439764 Test RE 0.0006790905060224292\n",
      "96 Train Loss 0.0014010671 Test MSE 0.005290825578036028 Test RE 0.0010065025670931138\n",
      "97 Train Loss 0.0013511837 Test MSE 0.011421004499821445 Test RE 0.0014787859511170753\n",
      "98 Train Loss 0.0013378019 Test MSE 0.005177914775054643 Test RE 0.0009957048296282205\n",
      "99 Train Loss 0.0013371266 Test MSE 0.004952252599170136 Test RE 0.0009737658931052071\n",
      "Training time: 73.20\n",
      "Training time: 73.20\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 47.765633 Test MSE 5215.532961430903 Test RE 0.999315242871476\n",
      "1 Train Loss 47.74811 Test MSE 5167.055451055945 Test RE 0.9946601661904801\n",
      "2 Train Loss 47.54435 Test MSE 5026.477211408919 Test RE 0.9810361787801828\n",
      "3 Train Loss 45.68439 Test MSE 4891.440929580769 Test RE 0.9677686987457484\n",
      "4 Train Loss 39.467472 Test MSE 3901.531910425294 Test RE 0.8643124052256771\n",
      "5 Train Loss 36.898476 Test MSE 5152.469607470012 Test RE 0.9932552837887945\n",
      "6 Train Loss 32.0058 Test MSE 7016.248119354105 Test RE 1.159059241563019\n",
      "7 Train Loss 28.438032 Test MSE 6338.632904334439 Test RE 1.1016685842950757\n",
      "8 Train Loss 22.620468 Test MSE 5149.555813491428 Test RE 0.9929743941600588\n",
      "9 Train Loss 13.236404 Test MSE 3661.6090675805876 Test RE 0.8373155412361136\n",
      "10 Train Loss 10.247668 Test MSE 2194.6997764635034 Test RE 0.6482473116559471\n",
      "11 Train Loss 6.2677326 Test MSE 1065.5258760035122 Test RE 0.4516844766576507\n",
      "12 Train Loss 4.8750696 Test MSE 1088.206402211489 Test RE 0.45646638728792965\n",
      "13 Train Loss 3.5562687 Test MSE 746.9711411298445 Test RE 0.3781856766115519\n",
      "14 Train Loss 2.8191676 Test MSE 1209.8821825561256 Test RE 0.4813098008236517\n",
      "15 Train Loss 1.1629658 Test MSE 662.2487101068567 Test RE 0.3560932367362362\n",
      "16 Train Loss 0.78680664 Test MSE 458.8966726655043 Test RE 0.2964221268579849\n",
      "17 Train Loss 0.4981404 Test MSE 297.96703371635346 Test RE 0.2388566150671061\n",
      "18 Train Loss 0.25838014 Test MSE 203.2306294726225 Test RE 0.1972639593924415\n",
      "19 Train Loss 0.19517088 Test MSE 157.72858397698974 Test RE 0.17378343086212805\n",
      "20 Train Loss 0.17792964 Test MSE 146.1477852065546 Test RE 0.16728202757629243\n",
      "21 Train Loss 0.15685621 Test MSE 119.60491855445355 Test RE 0.15133092424447322\n",
      "22 Train Loss 0.14927779 Test MSE 108.05604817081513 Test RE 0.1438393477023827\n",
      "23 Train Loss 0.11568845 Test MSE 60.93281419360251 Test RE 0.10801368505420045\n",
      "24 Train Loss 0.09521469 Test MSE 34.04867951615472 Test RE 0.08074270624307099\n",
      "25 Train Loss 0.08304526 Test MSE 38.765885130843174 Test RE 0.08615451004108954\n",
      "26 Train Loss 0.07472901 Test MSE 23.650748083217522 Test RE 0.06729388564097628\n",
      "27 Train Loss 0.05263125 Test MSE 12.367500424509696 Test RE 0.048662468057393785\n",
      "28 Train Loss 0.036140822 Test MSE 4.046528252040817 Test RE 0.027835206858111486\n",
      "29 Train Loss 0.029809978 Test MSE 2.330709779996717 Test RE 0.021125026574379823\n",
      "30 Train Loss 0.023211045 Test MSE 2.0505388888867713 Test RE 0.019814684870238485\n",
      "31 Train Loss 0.01756209 Test MSE 0.2813535809876958 Test RE 0.007339718237652097\n",
      "32 Train Loss 0.015930597 Test MSE 0.23257003516445493 Test RE 0.006673137044683629\n",
      "33 Train Loss 0.01054364 Test MSE 0.46395990363790324 Test RE 0.009425261089663106\n",
      "34 Train Loss 0.008192967 Test MSE 0.03678530988582741 Test RE 0.0026539355635160967\n",
      "35 Train Loss 0.0070301387 Test MSE 0.11053140359787542 Test RE 0.004600404368517141\n",
      "36 Train Loss 0.0056936955 Test MSE 0.05235135325209483 Test RE 0.0031660452737700025\n",
      "37 Train Loss 0.004478175 Test MSE 0.025992759860910142 Test RE 0.0022308961907251804\n",
      "38 Train Loss 0.004368294 Test MSE 0.015809820736926546 Test RE 0.0017398693498194102\n",
      "39 Train Loss 0.0042169206 Test MSE 0.020033878533887935 Test RE 0.0019585545984033108\n",
      "40 Train Loss 0.0041052033 Test MSE 0.007406317811969797 Test RE 0.0011908425299701443\n",
      "41 Train Loss 0.0039998717 Test MSE 0.006485664026250243 Test RE 0.0011143725055071457\n",
      "42 Train Loss 0.003870923 Test MSE 0.02571556635595244 Test RE 0.0022189688799074715\n",
      "43 Train Loss 0.003757406 Test MSE 0.055950592533156415 Test RE 0.0032730716298859123\n",
      "44 Train Loss 0.0036826157 Test MSE 0.06459297565429087 Test RE 0.0035167849376558286\n",
      "45 Train Loss 0.003476327 Test MSE 0.07649506739912672 Test RE 0.0038271006189845615\n",
      "46 Train Loss 0.0033343371 Test MSE 0.08136468693538491 Test RE 0.003947036509347936\n",
      "47 Train Loss 0.002873982 Test MSE 0.0076110189993237026 Test RE 0.0012071870493854538\n",
      "48 Train Loss 0.0028317692 Test MSE 0.004303403007354734 Test RE 0.0009077351946132142\n",
      "49 Train Loss 0.0028311566 Test MSE 0.004431620272201293 Test RE 0.0009211586504776621\n",
      "50 Train Loss 0.0028305496 Test MSE 0.004604496572423008 Test RE 0.0009389538384643782\n",
      "51 Train Loss 0.0028298877 Test MSE 0.004949037621234073 Test RE 0.0009734497597838694\n",
      "52 Train Loss 0.0028293205 Test MSE 0.005213921545883422 Test RE 0.0009991608543811026\n",
      "53 Train Loss 0.0028289868 Test MSE 0.005456494257009298 Test RE 0.0010221391333726999\n",
      "54 Train Loss 0.0028281359 Test MSE 0.005811271507705698 Test RE 0.0010548452411798424\n",
      "55 Train Loss 0.0028273882 Test MSE 0.006250647204783564 Test RE 0.0010939958093189788\n",
      "56 Train Loss 0.0025940554 Test MSE 0.01096168623445038 Test RE 0.001448744659536484\n",
      "57 Train Loss 0.00207735 Test MSE 0.05166155255178944 Test RE 0.003145117618598926\n",
      "58 Train Loss 0.0016140828 Test MSE 0.07013913625574368 Test RE 0.00366465732937353\n",
      "59 Train Loss 0.0014950153 Test MSE 0.04971695992595956 Test RE 0.003085357173894894\n",
      "60 Train Loss 0.0013234175 Test MSE 0.019551204894977168 Test RE 0.0019348171498331345\n",
      "61 Train Loss 0.0012257217 Test MSE 0.007002128357739035 Test RE 0.001157892387005363\n",
      "62 Train Loss 0.001164526 Test MSE 0.007343507823386524 Test RE 0.0011857822514350102\n",
      "63 Train Loss 0.0011450856 Test MSE 0.002965553550771389 Test RE 0.0007535395475479453\n",
      "64 Train Loss 0.0011443311 Test MSE 0.002752493165315747 Test RE 0.0007259660132753979\n",
      "65 Train Loss 0.0011435017 Test MSE 0.0025750475131927093 Test RE 0.0007021756873745122\n",
      "66 Train Loss 0.0011426435 Test MSE 0.0025557772410989927 Test RE 0.0006995434006783688\n",
      "67 Train Loss 0.0011236256 Test MSE 0.007613003122744703 Test RE 0.0012073443904731932\n",
      "68 Train Loss 0.0010932623 Test MSE 0.006694064298870444 Test RE 0.0011321347071017191\n",
      "69 Train Loss 0.0010791381 Test MSE 0.0020611796214261564 Test RE 0.0006282190253407738\n",
      "70 Train Loss 0.0010622896 Test MSE 0.0023249788719475594 Test RE 0.0006672101892769267\n",
      "71 Train Loss 0.0010398809 Test MSE 0.0038144097502569837 Test RE 0.0008546077691118526\n",
      "72 Train Loss 0.00097475585 Test MSE 0.002686821297802499 Test RE 0.0007172533011768505\n",
      "73 Train Loss 0.000955997 Test MSE 0.0011107377744744235 Test RE 0.0004611677570259084\n",
      "74 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "75 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "76 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "77 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "78 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "79 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "80 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "81 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "82 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "83 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "84 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "85 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "86 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "87 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "88 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "89 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "90 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "91 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "92 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "93 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "94 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "95 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "96 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "97 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "98 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "99 Train Loss 0.00095421774 Test MSE 0.0011524972827367946 Test RE 0.0004697568478716165\n",
      "Training time: 50.62\n",
      "Training time: 50.62\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 47.74126 Test MSE 5172.446109740336 Test RE 0.9951788828721416\n",
      "1 Train Loss 44.307835 Test MSE 4482.531013575215 Test RE 0.9264347060194549\n",
      "2 Train Loss 40.09563 Test MSE 6043.371292484516 Test RE 1.0757040489168868\n",
      "3 Train Loss 32.31735 Test MSE 7173.730671989675 Test RE 1.1719948370480733\n",
      "4 Train Loss 24.22145 Test MSE 5727.0907061399275 Test RE 1.047177242505982\n",
      "5 Train Loss 14.426139 Test MSE 7542.790130050571 Test RE 1.2017639628442056\n",
      "6 Train Loss 10.652579 Test MSE 5681.545017871977 Test RE 1.0430050009469025\n",
      "7 Train Loss 8.153557 Test MSE 6148.086705138168 Test RE 1.0849835570574653\n",
      "8 Train Loss 6.505632 Test MSE 5280.4650688431275 Test RE 1.0055166161365985\n",
      "9 Train Loss 5.315767 Test MSE 3926.6521527549417 Test RE 0.867090403912965\n",
      "10 Train Loss 4.0313 Test MSE 2823.1652776115566 Test RE 0.7352267786262351\n",
      "11 Train Loss 3.200482 Test MSE 2250.304085580894 Test RE 0.6564078543285216\n",
      "12 Train Loss 1.9633813 Test MSE 864.3413865433239 Test RE 0.4068139332066939\n",
      "13 Train Loss 1.0012811 Test MSE 647.0263372401212 Test RE 0.3519768855860606\n",
      "14 Train Loss 0.54361284 Test MSE 116.86262573350623 Test RE 0.14958601226498394\n",
      "15 Train Loss 0.3164086 Test MSE 62.20462251410491 Test RE 0.1091351109114926\n",
      "16 Train Loss 0.2042496 Test MSE 46.51493605805106 Test RE 0.09437335156792746\n",
      "17 Train Loss 0.14541873 Test MSE 29.811242712257737 Test RE 0.07555151976979818\n",
      "18 Train Loss 0.11736226 Test MSE 21.320222715425842 Test RE 0.06389237518477532\n",
      "19 Train Loss 0.096977316 Test MSE 15.296025027508714 Test RE 0.05411808961182507\n",
      "20 Train Loss 0.08024045 Test MSE 10.006690713894198 Test RE 0.04377220284317308\n",
      "21 Train Loss 0.06656104 Test MSE 4.23322779072243 Test RE 0.028470099434303092\n",
      "22 Train Loss 0.047708932 Test MSE 3.1026698382809785 Test RE 0.024373671109175803\n",
      "23 Train Loss 0.037087753 Test MSE 0.6791835788293309 Test RE 0.011403725457728515\n",
      "24 Train Loss 0.029885963 Test MSE 0.9585387872043509 Test RE 0.013547464146192805\n",
      "25 Train Loss 0.022724777 Test MSE 0.2142188016884178 Test RE 0.006404451740771075\n",
      "26 Train Loss 0.019255968 Test MSE 0.1116484082835937 Test RE 0.004623591244361721\n",
      "27 Train Loss 0.01612128 Test MSE 0.21321000422691044 Test RE 0.006389354048946997\n",
      "28 Train Loss 0.012594208 Test MSE 0.033832386404087186 Test RE 0.0025451857370401556\n",
      "29 Train Loss 0.011069194 Test MSE 0.031132937436009964 Test RE 0.00244153646026912\n",
      "30 Train Loss 0.008628934 Test MSE 0.019389432008733706 Test RE 0.0019267958765578448\n",
      "31 Train Loss 0.00762993 Test MSE 0.05503162741417701 Test RE 0.003246080925884722\n",
      "32 Train Loss 0.0067162095 Test MSE 0.02286770269824771 Test RE 0.0020924950213226895\n",
      "33 Train Loss 0.005676515 Test MSE 0.005878030665416787 Test RE 0.0010608869041309067\n",
      "34 Train Loss 0.005397031 Test MSE 0.019908285526567517 Test RE 0.001952405826710948\n",
      "35 Train Loss 0.004995668 Test MSE 0.10720335512674169 Test RE 0.00453061703392007\n",
      "36 Train Loss 0.0040958845 Test MSE 0.1305222826636043 Test RE 0.004999142251888611\n",
      "37 Train Loss 0.0037878342 Test MSE 0.08460973808408316 Test RE 0.004024976416079973\n",
      "38 Train Loss 0.0031260862 Test MSE 0.12230773925757236 Test RE 0.004839273124790214\n",
      "39 Train Loss 0.0028784797 Test MSE 0.08586050866292544 Test RE 0.0040546175252929965\n",
      "40 Train Loss 0.0026404362 Test MSE 0.03996072169104831 Test RE 0.002766112416801547\n",
      "41 Train Loss 0.0024765993 Test MSE 0.02722762774919561 Test RE 0.0022832741869599077\n",
      "42 Train Loss 0.0023023437 Test MSE 0.004944995894949179 Test RE 0.0009730521854078165\n",
      "43 Train Loss 0.0019722844 Test MSE 0.02193419387621674 Test RE 0.002049339937299114\n",
      "44 Train Loss 0.0017812488 Test MSE 0.011020365378251625 Test RE 0.0014526171308600865\n",
      "45 Train Loss 0.0017463637 Test MSE 0.018325049694941437 Test RE 0.0018731637526235587\n",
      "46 Train Loss 0.0016860542 Test MSE 0.006879590263844967 Test RE 0.0011477160402026663\n",
      "47 Train Loss 0.001602133 Test MSE 0.004488038488949043 Test RE 0.0009270036647206163\n",
      "48 Train Loss 0.0014807378 Test MSE 0.010788897529408644 Test RE 0.0014372810473889204\n",
      "49 Train Loss 0.0014142854 Test MSE 0.0083551299818889 Test RE 0.001264823045562585\n",
      "50 Train Loss 0.0013658651 Test MSE 0.0016105739973501009 Test RE 0.000555320244564781\n",
      "51 Train Loss 0.0013007311 Test MSE 0.0029770566104663893 Test RE 0.0007549995820651194\n",
      "52 Train Loss 0.0012934606 Test MSE 0.002457045333107105 Test RE 0.0006858983361936707\n",
      "53 Train Loss 0.0012210836 Test MSE 0.007012348140059011 Test RE 0.0011587370654302465\n",
      "54 Train Loss 0.0010919712 Test MSE 0.0020485507962780007 Test RE 0.000626291522785964\n",
      "55 Train Loss 0.0010851588 Test MSE 0.0009196696437257947 Test RE 0.0004196324749844165\n",
      "56 Train Loss 0.001084569 Test MSE 0.0009442573761965372 Test RE 0.000425204995051743\n",
      "57 Train Loss 0.0010836406 Test MSE 0.001005763460242979 Test RE 0.0004388348315346199\n",
      "58 Train Loss 0.0010248531 Test MSE 0.005594068002540487 Test RE 0.001034944439188224\n",
      "59 Train Loss 0.000980451 Test MSE 0.0034861512178321298 Test RE 0.0008170079366357831\n",
      "60 Train Loss 0.0009665549 Test MSE 0.00424390358855375 Test RE 0.0009014381194126734\n",
      "61 Train Loss 0.00096158526 Test MSE 0.004727639847353148 Test RE 0.0009514267482147453\n",
      "62 Train Loss 0.0009392308 Test MSE 0.013038082322929617 Test RE 0.0015800106856592208\n",
      "63 Train Loss 0.000926445 Test MSE 0.012043499578198558 Test RE 0.001518551466560681\n",
      "64 Train Loss 0.0009062331 Test MSE 0.007979526610478349 Test RE 0.001236066192823002\n",
      "65 Train Loss 0.00087214395 Test MSE 0.006281008055596895 Test RE 0.0010966494871950398\n",
      "66 Train Loss 0.00084690773 Test MSE 0.00405072951610064 Test RE 0.000880683352415212\n",
      "67 Train Loss 0.00083964877 Test MSE 0.0033477022556372233 Test RE 0.0008006202622958915\n",
      "68 Train Loss 0.0008221185 Test MSE 0.0022338704401995875 Test RE 0.0006540066365313733\n",
      "69 Train Loss 0.00080232084 Test MSE 0.0012180930298043003 Test RE 0.00048294024012785153\n",
      "70 Train Loss 0.00077326666 Test MSE 0.002085634298362117 Test RE 0.0006319347601487311\n",
      "71 Train Loss 0.00075523235 Test MSE 0.0014183210961325296 Test RE 0.0005211232455399354\n",
      "72 Train Loss 0.00074686273 Test MSE 0.004098631030911367 Test RE 0.0008858752668179317\n",
      "73 Train Loss 0.00074638927 Test MSE 0.0036839139783829493 Test RE 0.0008398619480950183\n",
      "74 Train Loss 0.00074064295 Test MSE 0.0033048671679056256 Test RE 0.0007954816556929757\n",
      "75 Train Loss 0.000740142 Test MSE 0.0029214308171092917 Test RE 0.0007479128031549863\n",
      "76 Train Loss 0.0007396842 Test MSE 0.00252439380032434 Test RE 0.0006952351434255793\n",
      "77 Train Loss 0.00073073176 Test MSE 0.0014544424143883464 Test RE 0.000527717419556102\n",
      "78 Train Loss 0.00072772003 Test MSE 0.0014935584417063422 Test RE 0.0005347666005756107\n",
      "79 Train Loss 0.00072704564 Test MSE 0.0015088914952153265 Test RE 0.0005375045810493781\n",
      "80 Train Loss 0.00072637957 Test MSE 0.0015400492896580705 Test RE 0.0005430258137218279\n",
      "81 Train Loss 0.00072540034 Test MSE 0.0015607363067966836 Test RE 0.0005466607984317989\n",
      "82 Train Loss 0.00070860167 Test MSE 0.005010826564123824 Test RE 0.000979507690656094\n",
      "83 Train Loss 0.0006815176 Test MSE 0.00604929579665071 Test RE 0.0010762311927626476\n",
      "84 Train Loss 0.00066304544 Test MSE 0.004562442276428071 Test RE 0.0009346561244816661\n",
      "85 Train Loss 0.0006622576 Test MSE 0.004408798976844932 Test RE 0.0009187837665434301\n",
      "86 Train Loss 0.00066145544 Test MSE 0.0044388254862141605 Test RE 0.0009219071859145001\n",
      "87 Train Loss 0.00066060806 Test MSE 0.0044820511159867635 Test RE 0.0009263851128677214\n",
      "88 Train Loss 0.00065861177 Test MSE 0.004891761317871824 Test RE 0.0009678003925438454\n",
      "89 Train Loss 0.0006577188 Test MSE 0.005092938705802416 Test RE 0.0009875006482342361\n",
      "90 Train Loss 0.0006570862 Test MSE 0.0051505939482672045 Test RE 0.0009930744794236222\n",
      "91 Train Loss 0.0006564901 Test MSE 0.005209502484807619 Test RE 0.0009987373450555074\n",
      "92 Train Loss 0.0006564901 Test MSE 0.005209502484807619 Test RE 0.0009987373450555074\n",
      "93 Train Loss 0.0006564901 Test MSE 0.005209502484807619 Test RE 0.0009987373450555074\n",
      "94 Train Loss 0.0006564901 Test MSE 0.005209502484807619 Test RE 0.0009987373450555074\n",
      "95 Train Loss 0.0006564901 Test MSE 0.005209502484807619 Test RE 0.0009987373450555074\n",
      "96 Train Loss 0.0006564901 Test MSE 0.005209502484807619 Test RE 0.0009987373450555074\n",
      "97 Train Loss 0.0006564901 Test MSE 0.005209502484807619 Test RE 0.0009987373450555074\n",
      "98 Train Loss 0.0006564901 Test MSE 0.005209502484807619 Test RE 0.0009987373450555074\n",
      "99 Train Loss 0.0006564901 Test MSE 0.005209502484807619 Test RE 0.0009987373450555074\n",
      "Training time: 45.09\n",
      "Training time: 45.09\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []   \n",
    "  beta_val = []\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "  N_f = 10000 #Total number of collocation points\n",
    "\n",
    "  layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "  PINN = Sequentialmodel(layers)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.5, \n",
    "                            max_iter = 10, \n",
    "                            max_eval = 15, \n",
    "                            tolerance_grad = 1e-6, \n",
    "                            tolerance_change = 1e-6, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "  train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "  beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "  print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1D_FODE_stan_'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Ky6HsA0AWWTD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF7H51LTWXDq",
    "outputId": "aefc6a9f-3ad4-417a-b9f7-438009e620af"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8a00fd8dc024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1D_FODE_stan_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"1D_FODE_stan_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
