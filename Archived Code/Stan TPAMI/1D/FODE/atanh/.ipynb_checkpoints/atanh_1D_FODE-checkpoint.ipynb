{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "84a34ebd-2e54-4cae-ca1c-79397867998c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvP0Nx4vNOlZ",
    "outputId": "94a6280c-bfd4-43c8-a396-40f22c70c38f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDzAYhTsNbP6",
    "outputId": "150eeb9e-6cdc-4ff0-fd50-61a1c228e3a0"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1wXUvTNETmrW",
    "outputId": "30d0ca6b-cde8-4b85-ccae-4eac06a2c482"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dmSz5jcVVt4p"
   },
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1]).reshape(-1,1)\n",
    "n_value = np.array([1.0,3.0,5.0,8.0,10.0]).reshape(-1,1)\n",
    "\n",
    "LR_tune,N_value = np.meshgrid(lr_tune,n_value)\n",
    "\n",
    "LR_tune = LR_tune.flatten('F').reshape(-1,1)\n",
    "N_value = N_value.flatten('F').reshape(-1,1)\n",
    "\n",
    "lrn_tune = np.hstack((LR_tune,N_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "def true_1D_2(x): #True function for 1D_1 dy/dx = cos(0.01*x) BC1: y(0)=0; x \\in [-100,100]\n",
    "    y = 100*np.sin(0.01*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "loss_thresh = 0.005\n",
    "label = \"1D_FODE_atanh_\" \n",
    "\n",
    "x = np.linspace(-600,600,5000).reshape(-1,1)\n",
    "ysol = true_1D_2(x)\n",
    "\n",
    "bc1_x = np.array(0).reshape(-1,1) \n",
    "bc1_y = np.array(0).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "\n",
    " \n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "\n",
    "y_true = true_1D_2(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "\n",
    "  #Collocation Points\n",
    "  # Latin Hypercube sampling for collocation points \n",
    "  # N_f sets of tuples(x,y)\n",
    "  x01 = np.array([[0.0, 1.0]])\n",
    "  sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "  x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "  x_coll_train = np.vstack((x_coll_train, bc1_x)) # append training points to collocation points \n",
    "\n",
    "  return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "        \n",
    "              \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        f = dy_dx - torch.cos(0.01*g)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + 100*loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "       \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "      \n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(seed):\n",
    "    x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_coll_train,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "  print(rep) \n",
    "  torch.manual_seed(rep*11)\n",
    "  start_time = time.time() \n",
    "  thresh_flag = 0\n",
    "\n",
    "  x_coll = torch.from_numpy(colloc_pts(N_f,123)).float().to(device)\n",
    "  f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "  for i in range(max_iter):\n",
    "    \n",
    "    train_step(i)\n",
    "\n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_coll,f_hat).cpu().detach().numpy()\n",
    "    if(thresh_flag == 0):\n",
    "        if(loss_np < loss_thresh):\n",
    "            time_threshold[rep] = time.time() - start_time\n",
    "            epoch_threshold[rep] = i+1            \n",
    "            thresh_flag = 1       \n",
    "    data_update(loss_np)\n",
    "    print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "  elapsed_time[rep] = time.time() - start_time  \n",
    "  print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "509236d6-c6b5-4579-8ffe-6c945b3ae573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D_FODE_atanh_\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 46.616234 Test MSE 5116.2267136472765 Test RE 0.9897557994474948\n",
      "1 Train Loss 45.90228 Test MSE 5076.272652499002 Test RE 0.9858835833647457\n",
      "2 Train Loss 45.35167 Test MSE 5033.188077874833 Test RE 0.9816908526735876\n",
      "3 Train Loss 44.87046 Test MSE 4990.457856197118 Test RE 0.9775148435344435\n",
      "4 Train Loss 44.348557 Test MSE 4942.293500183131 Test RE 0.9727862670384069\n",
      "5 Train Loss 44.08552 Test MSE 4913.514417951393 Test RE 0.9699498541218059\n",
      "6 Train Loss 43.75864 Test MSE 4891.352855165998 Test RE 0.9677599859710575\n",
      "7 Train Loss 43.333294 Test MSE 4865.253652881334 Test RE 0.9651746535418383\n",
      "8 Train Loss 43.053352 Test MSE 4839.283552486415 Test RE 0.962595217466633\n",
      "9 Train Loss 42.53149 Test MSE 4807.338741960317 Test RE 0.9594128418259984\n",
      "10 Train Loss 42.170685 Test MSE 4770.576136555253 Test RE 0.955737398226234\n",
      "11 Train Loss 41.54839 Test MSE 4715.303958567858 Test RE 0.9501846526752287\n",
      "12 Train Loss 40.743916 Test MSE 4652.054758474261 Test RE 0.9437904396111494\n",
      "13 Train Loss 40.296524 Test MSE 4611.280515692225 Test RE 0.9396452783418092\n",
      "14 Train Loss 39.397995 Test MSE 4528.989887938356 Test RE 0.9312233135680061\n",
      "15 Train Loss 38.836098 Test MSE 4482.937835598621 Test RE 0.9264767453817523\n",
      "16 Train Loss 38.090855 Test MSE 4411.644334082295 Test RE 0.9190802017392106\n",
      "17 Train Loss 36.987423 Test MSE 4308.270034190239 Test RE 0.9082483609492386\n",
      "18 Train Loss 35.345776 Test MSE 4197.434339518134 Test RE 0.8964893203217297\n",
      "19 Train Loss 33.7828 Test MSE 4056.842897017926 Test RE 0.8813476676871714\n",
      "20 Train Loss 33.17895 Test MSE 3988.603682838188 Test RE 0.8739037594380148\n",
      "21 Train Loss 32.58887 Test MSE 3964.4593609849767 Test RE 0.8712547319404496\n",
      "22 Train Loss 30.681156 Test MSE 3703.932514265047 Test RE 0.8421407781823186\n",
      "23 Train Loss 29.8134 Test MSE 3650.1756172632777 Test RE 0.8360072515683974\n",
      "24 Train Loss 29.162403 Test MSE 3549.656358092262 Test RE 0.8244158260845585\n",
      "25 Train Loss 27.67065 Test MSE 3422.889218916926 Test RE 0.809561015381666\n",
      "26 Train Loss 26.547373 Test MSE 3306.5512574090862 Test RE 0.7956843100836306\n",
      "27 Train Loss 25.750217 Test MSE 3174.120819448792 Test RE 0.7795875440638933\n",
      "28 Train Loss 23.802965 Test MSE 3045.4913366745263 Test RE 0.7636280080420861\n",
      "29 Train Loss 22.484192 Test MSE 2916.8691815192524 Test RE 0.7473286649863966\n",
      "30 Train Loss 21.954885 Test MSE 2872.572035599145 Test RE 0.741632287437444\n",
      "31 Train Loss 20.972801 Test MSE 2764.9403695466763 Test RE 0.7276056275983104\n",
      "32 Train Loss 19.972513 Test MSE 2629.5775719927224 Test RE 0.7095714909282412\n",
      "33 Train Loss 18.859062 Test MSE 2528.8421022552775 Test RE 0.6958474200510205\n",
      "34 Train Loss 17.60681 Test MSE 2445.5736845580755 Test RE 0.684295274543865\n",
      "35 Train Loss 16.705965 Test MSE 2312.5000329726786 Test RE 0.6654172244620506\n",
      "36 Train Loss 16.131136 Test MSE 2314.5128011270017 Test RE 0.6657067464669544\n",
      "37 Train Loss 15.650999 Test MSE 2241.944366202225 Test RE 0.6551874656496957\n",
      "38 Train Loss 15.374503 Test MSE 2180.5580751190687 Test RE 0.6461554229807458\n",
      "39 Train Loss 15.068881 Test MSE 2107.0876791209616 Test RE 0.6351765681996236\n",
      "40 Train Loss 14.990271 Test MSE 2094.5438847422524 Test RE 0.6332830974803955\n",
      "41 Train Loss 14.674105 Test MSE 2058.476672147906 Test RE 0.6278069794434304\n",
      "42 Train Loss 14.354523 Test MSE 2039.8333133006856 Test RE 0.6249575294014679\n",
      "43 Train Loss 14.063616 Test MSE 2000.4962141025178 Test RE 0.6189022072907883\n",
      "44 Train Loss 13.462882 Test MSE 1877.6024261932255 Test RE 0.5995908318452583\n",
      "45 Train Loss 12.987062 Test MSE 1778.8666579118064 Test RE 0.5836128722486542\n",
      "46 Train Loss 12.199615 Test MSE 1605.679978711126 Test RE 0.554475882441106\n",
      "47 Train Loss 11.603661 Test MSE 1477.2537160141626 Test RE 0.5318396478872369\n",
      "48 Train Loss 10.866163 Test MSE 1441.2347823697637 Test RE 0.5253158831007443\n",
      "49 Train Loss 10.341535 Test MSE 1358.2195206696813 Test RE 0.5099623909782197\n",
      "50 Train Loss 10.131675 Test MSE 1353.681077365312 Test RE 0.5091096672462029\n",
      "51 Train Loss 9.533449 Test MSE 1293.5297099039822 Test RE 0.4976698932595568\n",
      "52 Train Loss 9.296695 Test MSE 1233.5269021704255 Test RE 0.4859901615544883\n",
      "53 Train Loss 9.161892 Test MSE 1169.7847810085113 Test RE 0.47326691838476187\n",
      "54 Train Loss 8.939076 Test MSE 1148.0216647680468 Test RE 0.4688438316918508\n",
      "55 Train Loss 8.614937 Test MSE 1112.4284431349006 Test RE 0.46151859834349807\n",
      "56 Train Loss 8.379485 Test MSE 1115.02832244538 Test RE 0.46205759588858\n",
      "57 Train Loss 8.167069 Test MSE 1075.0226908548302 Test RE 0.4536928972408221\n",
      "58 Train Loss 8.021972 Test MSE 1051.7105264094082 Test RE 0.44874670740406764\n",
      "59 Train Loss 7.8906865 Test MSE 1002.9015210382793 Test RE 0.43821002591519437\n",
      "60 Train Loss 7.6884294 Test MSE 980.2453506740987 Test RE 0.4332320325366722\n",
      "61 Train Loss 7.4773116 Test MSE 895.7589681557231 Test RE 0.41414149521648547\n",
      "62 Train Loss 7.3170075 Test MSE 891.1799087096714 Test RE 0.41308160726263143\n",
      "63 Train Loss 7.1093063 Test MSE 896.3994165952124 Test RE 0.41428951990345286\n",
      "64 Train Loss 6.9715295 Test MSE 893.8802309364837 Test RE 0.41370696345709607\n",
      "65 Train Loss 6.890897 Test MSE 863.1430841430538 Test RE 0.4065318368051817\n",
      "66 Train Loss 6.7472944 Test MSE 866.9273227561299 Test RE 0.40742203160015855\n",
      "67 Train Loss 6.566784 Test MSE 850.6387289783925 Test RE 0.4035763801614955\n",
      "68 Train Loss 6.470581 Test MSE 842.9112418896476 Test RE 0.40173908649747675\n",
      "69 Train Loss 6.372906 Test MSE 832.5061413608556 Test RE 0.39925180446502034\n",
      "70 Train Loss 6.185464 Test MSE 847.969899318026 Test RE 0.4029427840641694\n",
      "71 Train Loss 6.0246687 Test MSE 819.5573020751016 Test RE 0.39613464527370945\n",
      "72 Train Loss 5.860207 Test MSE 825.3378673437223 Test RE 0.39752921435998373\n",
      "73 Train Loss 5.724901 Test MSE 755.1465500060262 Test RE 0.3802496176033905\n",
      "74 Train Loss 5.4940743 Test MSE 712.9872356020167 Test RE 0.36948264345963927\n",
      "75 Train Loss 5.3087583 Test MSE 629.6092289392528 Test RE 0.3472071878862899\n",
      "76 Train Loss 5.132872 Test MSE 627.1722083685689 Test RE 0.3465345711223384\n",
      "77 Train Loss 4.919541 Test MSE 606.854284951313 Test RE 0.3408751767053537\n",
      "78 Train Loss 4.693685 Test MSE 622.4151201865377 Test RE 0.34521784076214973\n",
      "79 Train Loss 4.4615827 Test MSE 575.3577358339081 Test RE 0.3319113800127804\n",
      "80 Train Loss 4.2949533 Test MSE 546.0080473804246 Test RE 0.3233349764165909\n",
      "81 Train Loss 4.1713176 Test MSE 525.2362543697433 Test RE 0.31712502354\n",
      "82 Train Loss 4.001874 Test MSE 496.0774904389493 Test RE 0.30819665821784037\n",
      "83 Train Loss 3.8554044 Test MSE 454.55747846916614 Test RE 0.29501735725254646\n",
      "84 Train Loss 3.7389228 Test MSE 461.75075421141366 Test RE 0.2973424882540892\n",
      "85 Train Loss 3.6753142 Test MSE 436.1478850434344 Test RE 0.28898150605110207\n",
      "86 Train Loss 3.5560515 Test MSE 426.3669938969397 Test RE 0.2857228368501218\n",
      "87 Train Loss 3.4808195 Test MSE 411.4541056573416 Test RE 0.2806815491260949\n",
      "88 Train Loss 3.1247408 Test MSE 353.1375140916718 Test RE 0.2600309798033875\n",
      "89 Train Loss 3.0034595 Test MSE 346.92608240848926 Test RE 0.257733956473155\n",
      "90 Train Loss 2.9188943 Test MSE 338.1394092985289 Test RE 0.2544491822664213\n",
      "91 Train Loss 2.8367884 Test MSE 319.24389718708574 Test RE 0.24723757009521696\n",
      "92 Train Loss 2.7193565 Test MSE 309.0615683947772 Test RE 0.24326278068982676\n",
      "93 Train Loss 2.661398 Test MSE 307.5363634047357 Test RE 0.24266179285009745\n",
      "94 Train Loss 2.6064894 Test MSE 310.4898681983084 Test RE 0.24382424111169956\n",
      "95 Train Loss 2.5447874 Test MSE 299.49574317162353 Test RE 0.23946855394735267\n",
      "96 Train Loss 2.5253537 Test MSE 295.4144253558149 Test RE 0.23783130228221974\n",
      "97 Train Loss 2.5015717 Test MSE 293.81181885524194 Test RE 0.2371853143166421\n",
      "98 Train Loss 2.4595642 Test MSE 276.356261507187 Test RE 0.23003176895087515\n",
      "99 Train Loss 2.4219894 Test MSE 264.1513578325449 Test RE 0.22489489040393812\n",
      "Training time: 80.34\n",
      "Training time: 80.34\n",
      "1D_FODE_atanh_\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 47.36216 Test MSE 5196.281389560635 Test RE 0.9974692018834227\n",
      "1 Train Loss 46.944633 Test MSE 5169.031791208652 Test RE 0.9948503711250414\n",
      "2 Train Loss 46.566666 Test MSE 5132.982945500125 Test RE 0.9913752566833924\n",
      "3 Train Loss 46.012825 Test MSE 5070.549824933049 Test RE 0.985327699826231\n",
      "4 Train Loss 45.55559 Test MSE 5009.200306911537 Test RE 0.9793487287869329\n",
      "5 Train Loss 44.84753 Test MSE 4944.066250364599 Test RE 0.9729607156439302\n",
      "6 Train Loss 44.245552 Test MSE 4887.70729236905 Test RE 0.967399279279298\n",
      "7 Train Loss 43.640293 Test MSE 4828.5008716544635 Test RE 0.9615222130595006\n",
      "8 Train Loss 43.099285 Test MSE 4779.839310058582 Test RE 0.9566648404935771\n",
      "9 Train Loss 42.668644 Test MSE 4734.550358910991 Test RE 0.9521218566464164\n",
      "10 Train Loss 42.003277 Test MSE 4675.295439560747 Test RE 0.946144991218161\n",
      "11 Train Loss 41.281425 Test MSE 4587.19722152338 Test RE 0.9371883276054873\n",
      "12 Train Loss 40.80221 Test MSE 4578.246005686217 Test RE 0.9362734910457887\n",
      "13 Train Loss 40.553577 Test MSE 4554.839919294204 Test RE 0.9338770952152203\n",
      "14 Train Loss 39.426838 Test MSE 4474.663325046838 Test RE 0.9256213150082606\n",
      "15 Train Loss 38.33505 Test MSE 4328.302232591036 Test RE 0.9103574570368937\n",
      "16 Train Loss 37.370964 Test MSE 4210.072618090566 Test RE 0.897837949636748\n",
      "17 Train Loss 36.351246 Test MSE 4059.9152266627766 Test RE 0.881681335783693\n",
      "18 Train Loss 35.489216 Test MSE 3915.946105655438 Test RE 0.8659075327798232\n",
      "19 Train Loss 34.463383 Test MSE 3743.057663277722 Test RE 0.8465769185485359\n",
      "20 Train Loss 33.07019 Test MSE 3571.621773054974 Test RE 0.8269626511854685\n",
      "21 Train Loss 32.039745 Test MSE 3512.3391509878397 Test RE 0.8200708729442792\n",
      "22 Train Loss 31.100744 Test MSE 3467.082302793948 Test RE 0.8147703948662478\n",
      "23 Train Loss 30.53463 Test MSE 3398.364681669966 Test RE 0.8066556043415858\n",
      "24 Train Loss 29.540253 Test MSE 3297.536686010722 Test RE 0.7945989422991077\n",
      "25 Train Loss 28.202232 Test MSE 3125.032448657882 Test RE 0.7735358217177121\n",
      "26 Train Loss 27.283705 Test MSE 2986.364440314899 Test RE 0.7561789219803499\n",
      "27 Train Loss 25.507462 Test MSE 2728.9867961927243 Test RE 0.722859481491236\n",
      "28 Train Loss 24.167324 Test MSE 2536.952671666037 Test RE 0.6969623969144587\n",
      "29 Train Loss 22.800303 Test MSE 2351.966483835864 Test RE 0.671071398033893\n",
      "30 Train Loss 21.968784 Test MSE 2304.6303174670124 Test RE 0.6642840121369086\n",
      "31 Train Loss 20.739515 Test MSE 2110.4926935642807 Test RE 0.6356895777815805\n",
      "32 Train Loss 20.096031 Test MSE 2044.7834621737018 Test RE 0.6257153752022787\n",
      "33 Train Loss 19.089396 Test MSE 1938.8126174014844 Test RE 0.6092858375612595\n",
      "34 Train Loss 17.320194 Test MSE 1740.5528140458587 Test RE 0.5772936328328657\n",
      "35 Train Loss 14.889403 Test MSE 1660.3872706943873 Test RE 0.5638425704765946\n",
      "36 Train Loss 13.594318 Test MSE 1583.238659522468 Test RE 0.5505875130225723\n",
      "37 Train Loss 12.602843 Test MSE 1521.8972456497402 Test RE 0.5398160962304281\n",
      "38 Train Loss 11.992843 Test MSE 1428.0477712014213 Test RE 0.5229070926062421\n",
      "39 Train Loss 11.662233 Test MSE 1398.7285202734438 Test RE 0.5175113508303875\n",
      "40 Train Loss 11.261561 Test MSE 1377.926027236704 Test RE 0.5136486090797311\n",
      "41 Train Loss 10.873656 Test MSE 1387.3709509151581 Test RE 0.5154059918472191\n",
      "42 Train Loss 10.41678 Test MSE 1292.989911409396 Test RE 0.4975660419566302\n",
      "43 Train Loss 10.111119 Test MSE 1253.6515877044778 Test RE 0.4899385270448568\n",
      "44 Train Loss 9.901982 Test MSE 1267.4205850257433 Test RE 0.49262170490796414\n",
      "45 Train Loss 9.718792 Test MSE 1225.4384403348238 Test RE 0.4843941777092202\n",
      "46 Train Loss 9.454862 Test MSE 1132.6422014055013 Test RE 0.465692811570613\n",
      "47 Train Loss 9.18987 Test MSE 1109.5942264669982 Test RE 0.4609303007657368\n",
      "48 Train Loss 8.832831 Test MSE 1110.0834294728174 Test RE 0.4610318981042532\n",
      "49 Train Loss 8.321325 Test MSE 1140.018809928218 Test RE 0.46720681948026904\n",
      "50 Train Loss 7.9047637 Test MSE 1098.426418018142 Test RE 0.45860485643621984\n",
      "51 Train Loss 7.4839354 Test MSE 1042.0513474690206 Test RE 0.44668125167762057\n",
      "52 Train Loss 6.9780607 Test MSE 997.7076352211112 Test RE 0.43707383894491036\n",
      "53 Train Loss 6.7758203 Test MSE 990.4807275927119 Test RE 0.4354879870547827\n",
      "54 Train Loss 6.551451 Test MSE 993.5259728632604 Test RE 0.43615692988740307\n",
      "55 Train Loss 6.150613 Test MSE 940.0769500764172 Test RE 0.4242627149788237\n",
      "56 Train Loss 5.8378806 Test MSE 901.5076040985523 Test RE 0.415468270216819\n",
      "57 Train Loss 5.418134 Test MSE 856.2502069582014 Test RE 0.4049053446766702\n",
      "58 Train Loss 5.0361943 Test MSE 860.0699940336075 Test RE 0.4058074940122018\n",
      "59 Train Loss 4.957785 Test MSE 835.9969582606265 Test RE 0.4000879886808956\n",
      "60 Train Loss 4.7272463 Test MSE 789.8684445403865 Test RE 0.38889337669633445\n",
      "61 Train Loss 4.577221 Test MSE 788.9885296654932 Test RE 0.3886767023733432\n",
      "62 Train Loss 4.512705 Test MSE 797.5574510294092 Test RE 0.3907816416714149\n",
      "63 Train Loss 4.3479266 Test MSE 785.5518911103791 Test RE 0.38782928887706625\n",
      "64 Train Loss 4.225861 Test MSE 764.8922547146531 Test RE 0.38269544783730897\n",
      "65 Train Loss 4.071821 Test MSE 734.1543675057209 Test RE 0.3749271219177409\n",
      "66 Train Loss 3.949054 Test MSE 712.0174446032644 Test RE 0.3692312764981373\n",
      "67 Train Loss 3.8473754 Test MSE 713.0892771428298 Test RE 0.36950908238271796\n",
      "68 Train Loss 3.695903 Test MSE 706.4809328586953 Test RE 0.367792939017121\n",
      "69 Train Loss 3.5903738 Test MSE 704.1001731999328 Test RE 0.3671727060518005\n",
      "70 Train Loss 3.5214472 Test MSE 702.8215708543361 Test RE 0.3668391731072231\n",
      "71 Train Loss 3.449365 Test MSE 695.5670146221378 Test RE 0.3649409968181723\n",
      "72 Train Loss 3.3687358 Test MSE 673.1883316954005 Test RE 0.3590223246062481\n",
      "73 Train Loss 3.3035595 Test MSE 667.3770867850843 Test RE 0.35746934998052937\n",
      "74 Train Loss 3.1909616 Test MSE 676.6562564866597 Test RE 0.3599458871488438\n",
      "75 Train Loss 3.1202312 Test MSE 684.143691864142 Test RE 0.3619318711951352\n",
      "76 Train Loss 2.964718 Test MSE 641.1280628613697 Test RE 0.35036890679654326\n",
      "77 Train Loss 2.8025255 Test MSE 640.7841051320335 Test RE 0.35027490977140585\n",
      "78 Train Loss 2.735436 Test MSE 614.4048804524735 Test RE 0.3429892377756875\n",
      "79 Train Loss 2.6762395 Test MSE 597.962098786261 Test RE 0.3383685524481515\n",
      "80 Train Loss 2.6174593 Test MSE 623.5060938361561 Test RE 0.3455202584435363\n",
      "81 Train Loss 2.565756 Test MSE 614.589213183987 Test RE 0.34304068544781785\n",
      "82 Train Loss 2.5243928 Test MSE 615.1530324246827 Test RE 0.34319800077007917\n",
      "83 Train Loss 2.4718056 Test MSE 601.1576107554106 Test RE 0.33927146924091406\n",
      "84 Train Loss 2.4239242 Test MSE 600.3896326563041 Test RE 0.3390546905454333\n",
      "85 Train Loss 2.3160279 Test MSE 614.8376930278273 Test RE 0.3431100245122801\n",
      "86 Train Loss 2.252236 Test MSE 611.7592634301304 Test RE 0.3422499881900748\n",
      "87 Train Loss 2.2089221 Test MSE 599.3431070524869 Test RE 0.33875906237821196\n",
      "88 Train Loss 2.1750748 Test MSE 606.8151959663027 Test RE 0.34086419822207087\n",
      "89 Train Loss 2.1475391 Test MSE 597.3377151469713 Test RE 0.3381918464585908\n",
      "90 Train Loss 2.112986 Test MSE 594.303344607894 Test RE 0.3373317752642614\n",
      "91 Train Loss 2.085692 Test MSE 594.4914178534437 Test RE 0.3373851470514732\n",
      "92 Train Loss 2.051478 Test MSE 608.9215184146932 Test RE 0.34145527435046374\n",
      "93 Train Loss 1.9559666 Test MSE 616.453521952405 Test RE 0.3435605851501491\n",
      "94 Train Loss 1.9403288 Test MSE 611.297919476651 Test RE 0.342120913935299\n",
      "95 Train Loss 1.9080321 Test MSE 601.3489750474143 Test RE 0.3393254644639464\n",
      "96 Train Loss 1.8885583 Test MSE 597.9191557141116 Test RE 0.33835640214123336\n",
      "97 Train Loss 1.8708835 Test MSE 583.6278953558879 Test RE 0.3342883067280807\n",
      "98 Train Loss 1.845223 Test MSE 597.1183815728101 Test RE 0.3381297512359356\n",
      "99 Train Loss 1.8314222 Test MSE 597.1097260328452 Test RE 0.3381273005441871\n",
      "Training time: 86.79\n",
      "Training time: 86.79\n",
      "1D_FODE_atanh_\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 46.449707 Test MSE 5148.9394648802445 Test RE 0.9929149679954422\n",
      "1 Train Loss 45.944553 Test MSE 5131.153060037187 Test RE 0.9911985305089123\n",
      "2 Train Loss 45.66151 Test MSE 5099.048487131955 Test RE 0.9880928018953565\n",
      "3 Train Loss 45.279663 Test MSE 5052.512987221855 Test RE 0.9835736465866713\n",
      "4 Train Loss 45.03035 Test MSE 5048.197367128502 Test RE 0.9831534955644954\n",
      "5 Train Loss 44.5018 Test MSE 4986.682561270195 Test RE 0.977145027261972\n",
      "6 Train Loss 44.04344 Test MSE 4942.802981721471 Test RE 0.9728364060954716\n",
      "7 Train Loss 43.615356 Test MSE 4896.628602836339 Test RE 0.9682817517954714\n",
      "8 Train Loss 43.07919 Test MSE 4855.111832584472 Test RE 0.964168155719458\n",
      "9 Train Loss 42.396103 Test MSE 4793.703252587437 Test RE 0.9580512409538043\n",
      "10 Train Loss 41.642174 Test MSE 4716.627596384015 Test RE 0.9503180069721584\n",
      "11 Train Loss 40.97618 Test MSE 4640.124617522926 Test RE 0.9425794928814102\n",
      "12 Train Loss 40.198242 Test MSE 4594.690410380278 Test RE 0.9379534640611451\n",
      "13 Train Loss 39.31961 Test MSE 4541.933008593449 Test RE 0.9325530070482388\n",
      "14 Train Loss 38.711735 Test MSE 4471.708188715795 Test RE 0.9253156172672368\n",
      "15 Train Loss 37.98906 Test MSE 4394.843764997237 Test RE 0.9173284966933675\n",
      "16 Train Loss 37.132206 Test MSE 4343.170625645375 Test RE 0.9119197264330426\n",
      "17 Train Loss 36.1134 Test MSE 4247.803247064329 Test RE 0.9018521832094195\n",
      "18 Train Loss 35.064438 Test MSE 4143.083010490718 Test RE 0.8906662217030926\n",
      "19 Train Loss 34.595383 Test MSE 4145.531596670415 Test RE 0.8909293772884004\n",
      "20 Train Loss 33.780754 Test MSE 4060.8788012191712 Test RE 0.8817859580789728\n",
      "21 Train Loss 33.22721 Test MSE 3981.5570311180963 Test RE 0.8731314568735066\n",
      "22 Train Loss 32.602615 Test MSE 3937.232304335195 Test RE 0.8682577821509002\n",
      "23 Train Loss 31.92366 Test MSE 3840.015006663218 Test RE 0.8574713643022149\n",
      "24 Train Loss 30.69002 Test MSE 3725.286565728405 Test RE 0.8445648602434525\n",
      "25 Train Loss 29.479559 Test MSE 3643.945637730579 Test RE 0.8352935143614291\n",
      "26 Train Loss 28.324276 Test MSE 3520.7266486158787 Test RE 0.8210494574026503\n",
      "27 Train Loss 27.283195 Test MSE 3400.427336931601 Test RE 0.8069003691311325\n",
      "28 Train Loss 25.931362 Test MSE 3299.4016383604294 Test RE 0.7948236069261914\n",
      "29 Train Loss 25.109762 Test MSE 3214.6495161947673 Test RE 0.7845488310781067\n",
      "30 Train Loss 24.097889 Test MSE 3144.9563462790275 Test RE 0.7759977740167235\n",
      "31 Train Loss 22.54803 Test MSE 2980.190439165464 Test RE 0.7553968565103639\n",
      "32 Train Loss 21.44129 Test MSE 2806.1562309866695 Test RE 0.7330086301792765\n",
      "33 Train Loss 20.403997 Test MSE 2672.3895888714737 Test RE 0.7153244181207274\n",
      "34 Train Loss 19.375738 Test MSE 2608.265054589708 Test RE 0.7066901304578544\n",
      "35 Train Loss 18.230904 Test MSE 2448.5180923022435 Test RE 0.6847070875732619\n",
      "36 Train Loss 17.10953 Test MSE 2248.922987014103 Test RE 0.6562063919809026\n",
      "37 Train Loss 16.277052 Test MSE 2159.6272648294284 Test RE 0.6430467765239998\n",
      "38 Train Loss 15.406251 Test MSE 2090.600475467737 Test RE 0.6326866738428975\n",
      "39 Train Loss 14.231177 Test MSE 1933.955617646271 Test RE 0.6085221854090626\n",
      "40 Train Loss 13.43597 Test MSE 1819.2666342152227 Test RE 0.5902029038561971\n",
      "41 Train Loss 12.855203 Test MSE 1745.6011626735624 Test RE 0.5781302260455196\n",
      "42 Train Loss 12.363589 Test MSE 1660.6537829119632 Test RE 0.5638878204333011\n",
      "43 Train Loss 11.939746 Test MSE 1538.6564142066368 Test RE 0.5427801922247496\n",
      "44 Train Loss 11.607709 Test MSE 1494.1643141479458 Test RE 0.5348750554861595\n",
      "45 Train Loss 11.129178 Test MSE 1407.2742899100392 Test RE 0.5190898552462504\n",
      "46 Train Loss 10.787881 Test MSE 1369.6364181338956 Test RE 0.5121012221399308\n",
      "47 Train Loss 10.418508 Test MSE 1332.899237664349 Test RE 0.5051866023559116\n",
      "48 Train Loss 10.037561 Test MSE 1241.0196420513782 Test RE 0.487463937646221\n",
      "49 Train Loss 9.550156 Test MSE 1179.7073196844824 Test RE 0.4752698906736897\n",
      "50 Train Loss 9.206717 Test MSE 1157.863834991873 Test RE 0.4708492785524832\n",
      "51 Train Loss 8.974616 Test MSE 1174.6939798734513 Test RE 0.47425895083524255\n",
      "52 Train Loss 8.560081 Test MSE 1076.4188412172787 Test RE 0.45398741101353723\n",
      "53 Train Loss 8.2116995 Test MSE 1028.9325196780408 Test RE 0.44386061584651654\n",
      "54 Train Loss 7.7943525 Test MSE 975.762182187331 Test RE 0.43224020022757614\n",
      "55 Train Loss 7.5430713 Test MSE 932.8052278398225 Test RE 0.42261864220200845\n",
      "56 Train Loss 7.2026024 Test MSE 832.6336659059609 Test RE 0.3992823822905645\n",
      "57 Train Loss 6.901825 Test MSE 797.7698552058204 Test RE 0.3908336743663861\n",
      "58 Train Loss 6.6662893 Test MSE 753.4610823258056 Test RE 0.37982502686815833\n",
      "59 Train Loss 6.354737 Test MSE 744.5581759734033 Test RE 0.3775743497731894\n",
      "60 Train Loss 6.0888863 Test MSE 705.4387077490892 Test RE 0.36752154845612345\n",
      "61 Train Loss 5.8683023 Test MSE 700.6840876541911 Test RE 0.36628091643962696\n",
      "62 Train Loss 5.654358 Test MSE 608.9871465753955 Test RE 0.341473674486748\n",
      "63 Train Loss 5.5075636 Test MSE 600.4456725389598 Test RE 0.33907051372148805\n",
      "64 Train Loss 5.276605 Test MSE 544.6370869383309 Test RE 0.3229287936967968\n",
      "65 Train Loss 5.047938 Test MSE 498.2627849255583 Test RE 0.30887473813777166\n",
      "66 Train Loss 4.927905 Test MSE 458.71526511647284 Test RE 0.2963635313940488\n",
      "67 Train Loss 4.7807465 Test MSE 458.2462251272673 Test RE 0.29621197561414503\n",
      "68 Train Loss 4.614857 Test MSE 450.8472479806516 Test RE 0.2938108815230341\n",
      "69 Train Loss 4.501048 Test MSE 418.1897103433775 Test RE 0.2829696359244115\n",
      "70 Train Loss 4.3839307 Test MSE 416.49131712926084 Test RE 0.28239443924982743\n",
      "71 Train Loss 4.302923 Test MSE 413.9084224384795 Test RE 0.28151743481504843\n",
      "72 Train Loss 4.245991 Test MSE 391.6139004721616 Test RE 0.27383074885494885\n",
      "73 Train Loss 4.204966 Test MSE 390.3762720407269 Test RE 0.27339770893592735\n",
      "74 Train Loss 4.099607 Test MSE 376.44709845259393 Test RE 0.268475797872264\n",
      "75 Train Loss 3.967461 Test MSE 375.5557914929443 Test RE 0.2681577768816958\n",
      "76 Train Loss 3.8773603 Test MSE 367.7805117295494 Test RE 0.26536737048186604\n",
      "77 Train Loss 3.7987223 Test MSE 346.60435872128767 Test RE 0.2576144232991075\n",
      "78 Train Loss 3.7687366 Test MSE 343.6595859663623 Test RE 0.2565177346544038\n",
      "79 Train Loss 3.6802256 Test MSE 328.2883600806406 Test RE 0.25071533992811335\n",
      "80 Train Loss 3.625084 Test MSE 322.1381011628693 Test RE 0.24835574573034555\n",
      "81 Train Loss 3.4844081 Test MSE 304.24763167378416 Test RE 0.2413608173183333\n",
      "82 Train Loss 3.4349308 Test MSE 295.5734649246394 Test RE 0.2378953132022564\n",
      "83 Train Loss 3.3655913 Test MSE 273.6710680351649 Test RE 0.22891149842774441\n",
      "84 Train Loss 3.2227147 Test MSE 245.7964349540656 Test RE 0.21694065869162119\n",
      "85 Train Loss 3.1017785 Test MSE 232.87213101941393 Test RE 0.21116013131098615\n",
      "86 Train Loss 3.0517557 Test MSE 240.7701538319543 Test RE 0.21471109648823486\n",
      "87 Train Loss 2.9999206 Test MSE 238.66824389771583 Test RE 0.21377183415494247\n",
      "88 Train Loss 2.9249616 Test MSE 243.5608644306277 Test RE 0.21595184461772562\n",
      "89 Train Loss 2.860636 Test MSE 236.41468958152961 Test RE 0.2127602018166858\n",
      "90 Train Loss 2.8194907 Test MSE 227.1314273077664 Test RE 0.20854115747814408\n",
      "91 Train Loss 2.7193408 Test MSE 205.55442181426397 Test RE 0.19838853778375085\n",
      "92 Train Loss 2.6702456 Test MSE 206.72715669056169 Test RE 0.19895365883046842\n",
      "93 Train Loss 2.624453 Test MSE 200.6982158565203 Test RE 0.19603107456887944\n",
      "94 Train Loss 2.5863395 Test MSE 197.89062129004517 Test RE 0.19465509277455395\n",
      "95 Train Loss 2.5388217 Test MSE 198.3439885897019 Test RE 0.19487794256323354\n",
      "96 Train Loss 2.4971669 Test MSE 201.31927825790225 Test RE 0.1963341502281392\n",
      "97 Train Loss 2.4631066 Test MSE 198.81249437136998 Test RE 0.19510796614890236\n",
      "98 Train Loss 2.43097 Test MSE 191.2440003268701 Test RE 0.1913581987804534\n",
      "99 Train Loss 2.3959913 Test MSE 187.48293050571974 Test RE 0.1894671975018496\n",
      "Training time: 72.92\n",
      "Training time: 72.92\n",
      "1D_FODE_atanh_\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 46.741367 Test MSE 5165.594739898592 Test RE 0.9945195625222764\n",
      "1 Train Loss 46.288143 Test MSE 5139.871293148813 Test RE 0.9920402353085622\n",
      "2 Train Loss 45.610485 Test MSE 5070.357501309003 Test RE 0.9853090131356741\n",
      "3 Train Loss 45.28961 Test MSE 5048.613336220776 Test RE 0.983194000423276\n",
      "4 Train Loss 44.86469 Test MSE 5003.117617399794 Test RE 0.9787539348678745\n",
      "5 Train Loss 44.052727 Test MSE 4914.714683194145 Test RE 0.9700683157675584\n",
      "6 Train Loss 43.342392 Test MSE 4851.065122781384 Test RE 0.9637662574507372\n",
      "7 Train Loss 42.475773 Test MSE 4767.748022635281 Test RE 0.955454064004905\n",
      "8 Train Loss 41.659706 Test MSE 4684.2010930339675 Test RE 0.9470456862443121\n",
      "9 Train Loss 41.009182 Test MSE 4633.696905374953 Test RE 0.9419264145932821\n",
      "10 Train Loss 40.13487 Test MSE 4561.092047199179 Test RE 0.9345178111260062\n",
      "11 Train Loss 38.721714 Test MSE 4373.940313877797 Test RE 0.9151443249695449\n",
      "12 Train Loss 37.16749 Test MSE 4194.108957336761 Test RE 0.8961341318832611\n",
      "13 Train Loss 35.45643 Test MSE 3951.223004763364 Test RE 0.8697990630999878\n",
      "14 Train Loss 34.32523 Test MSE 3839.925389054112 Test RE 0.8574613584842998\n",
      "15 Train Loss 33.71864 Test MSE 3778.153263079209 Test RE 0.8505364895398234\n",
      "16 Train Loss 32.970688 Test MSE 3711.5684025267265 Test RE 0.8430083941626573\n",
      "17 Train Loss 31.046972 Test MSE 3443.8864802758517 Test RE 0.8120402915036791\n",
      "18 Train Loss 29.352386 Test MSE 3293.958377311776 Test RE 0.7941676973500109\n",
      "19 Train Loss 28.096813 Test MSE 3137.053794196701 Test RE 0.7750222088098832\n",
      "20 Train Loss 27.117813 Test MSE 3034.938801795066 Test RE 0.7623038861218411\n",
      "21 Train Loss 26.547606 Test MSE 2983.523875725915 Test RE 0.7558192059893091\n",
      "22 Train Loss 25.59045 Test MSE 2793.0916917210902 Test RE 0.7313003161317964\n",
      "23 Train Loss 25.086245 Test MSE 2770.885389581872 Test RE 0.7283874360228053\n",
      "24 Train Loss 23.920658 Test MSE 2602.1887004371074 Test RE 0.7058664788585303\n",
      "25 Train Loss 23.151888 Test MSE 2481.276829324586 Test RE 0.6892722190216035\n",
      "26 Train Loss 22.531847 Test MSE 2444.8342434243614 Test RE 0.6841918153165484\n",
      "27 Train Loss 21.882692 Test MSE 2462.845890258652 Test RE 0.6867074883615996\n",
      "28 Train Loss 21.141748 Test MSE 2321.7306948022656 Test RE 0.6667439539949371\n",
      "29 Train Loss 20.545912 Test MSE 2202.729254632118 Test RE 0.6494320603805994\n",
      "30 Train Loss 19.82392 Test MSE 2151.5708330917105 Test RE 0.6418462214195475\n",
      "31 Train Loss 18.678864 Test MSE 2060.396611860777 Test RE 0.6280996887661833\n",
      "32 Train Loss 18.06846 Test MSE 1880.3444716313013 Test RE 0.6000284925178362\n",
      "33 Train Loss 17.218418 Test MSE 1706.9559763211068 Test RE 0.5716949091602049\n",
      "34 Train Loss 16.913074 Test MSE 1689.5660785675996 Test RE 0.5687753357550998\n",
      "35 Train Loss 16.600805 Test MSE 1583.7379015225483 Test RE 0.550674314448187\n",
      "36 Train Loss 16.04458 Test MSE 1505.7526111667096 Test RE 0.5369452158103214\n",
      "37 Train Loss 15.86009 Test MSE 1495.483111546906 Test RE 0.5351110523720274\n",
      "38 Train Loss 15.633013 Test MSE 1475.5373834707505 Test RE 0.5315306017918838\n",
      "39 Train Loss 15.424669 Test MSE 1480.061879565942 Test RE 0.5323449042335228\n",
      "40 Train Loss 15.13187 Test MSE 1389.1084646696422 Test RE 0.5157286325911119\n",
      "41 Train Loss 14.940227 Test MSE 1321.4719611974674 Test RE 0.5030163957583219\n",
      "42 Train Loss 14.877911 Test MSE 1356.53558804484 Test RE 0.509646164982072\n",
      "43 Train Loss 14.738165 Test MSE 1350.542048955725 Test RE 0.5085190417033703\n",
      "44 Train Loss 14.620721 Test MSE 1312.7650333399724 Test RE 0.501356517319295\n",
      "45 Train Loss 14.527063 Test MSE 1316.5189657796889 Test RE 0.5020728355107528\n",
      "46 Train Loss 14.295312 Test MSE 1269.7865229240952 Test RE 0.49308128752658154\n",
      "47 Train Loss 13.936279 Test MSE 1200.7844633473533 Test RE 0.47949677959528264\n",
      "48 Train Loss 13.810858 Test MSE 1191.0849050015945 Test RE 0.47755624097297916\n",
      "49 Train Loss 13.66312 Test MSE 1205.7847988734243 Test RE 0.4804941084065242\n",
      "50 Train Loss 13.523992 Test MSE 1156.6398397872508 Test RE 0.47060034183359395\n",
      "51 Train Loss 13.417665 Test MSE 1137.6025755662572 Test RE 0.4667114417051904\n",
      "52 Train Loss 13.242744 Test MSE 1111.3465258204553 Test RE 0.4612941135992993\n",
      "53 Train Loss 13.168629 Test MSE 1103.5282200755876 Test RE 0.45966865127495904\n",
      "54 Train Loss 13.067071 Test MSE 1111.3448945595765 Test RE 0.46129377504993613\n",
      "55 Train Loss 12.922746 Test MSE 1096.7189926624894 Test RE 0.458248283580537\n",
      "56 Train Loss 12.790766 Test MSE 1063.5916711884745 Test RE 0.4512743284177839\n",
      "57 Train Loss 12.734135 Test MSE 1041.4923929765753 Test RE 0.4465614360931277\n",
      "58 Train Loss 12.526594 Test MSE 1044.4632053485998 Test RE 0.4471978812279254\n",
      "59 Train Loss 12.422923 Test MSE 1002.1302178222172 Test RE 0.4380414860309751\n",
      "60 Train Loss 12.238316 Test MSE 981.174695714413 Test RE 0.4334373518767811\n",
      "61 Train Loss 12.14173 Test MSE 901.899075886471 Test RE 0.41555846715634764\n",
      "62 Train Loss 12.01859 Test MSE 848.2400131333385 Test RE 0.4030069560039223\n",
      "63 Train Loss 11.823861 Test MSE 844.0249595968946 Test RE 0.40200440283039457\n",
      "64 Train Loss 11.725164 Test MSE 846.8274151059659 Test RE 0.4026712467517274\n",
      "65 Train Loss 11.633386 Test MSE 862.0411368010143 Test RE 0.406272250789843\n",
      "66 Train Loss 11.559969 Test MSE 850.2293190576652 Test RE 0.403479248409834\n",
      "67 Train Loss 11.32485 Test MSE 865.4811788498273 Test RE 0.40708207414979614\n",
      "68 Train Loss 11.10921 Test MSE 824.0409898079993 Test RE 0.3972167669302814\n",
      "69 Train Loss 11.022747 Test MSE 800.9134852972503 Test RE 0.3916029617134215\n",
      "70 Train Loss 10.896136 Test MSE 804.888339329882 Test RE 0.3925735023203421\n",
      "71 Train Loss 10.812011 Test MSE 781.1244766966518 Test RE 0.38673483080481946\n",
      "72 Train Loss 10.769382 Test MSE 768.4347651015083 Test RE 0.38358062906378165\n",
      "73 Train Loss 10.725198 Test MSE 780.3140389074496 Test RE 0.38653415481178305\n",
      "74 Train Loss 10.653808 Test MSE 752.2542181256015 Test RE 0.37952071059633863\n",
      "75 Train Loss 10.522037 Test MSE 705.749289592066 Test RE 0.36760244347770327\n",
      "76 Train Loss 10.275142 Test MSE 730.7471685137858 Test RE 0.37405609466808337\n",
      "77 Train Loss 10.0266905 Test MSE 694.5847115881185 Test RE 0.36468321482476035\n",
      "78 Train Loss 9.868265 Test MSE 654.591911442907 Test RE 0.35402870947400994\n",
      "79 Train Loss 9.849721 Test MSE 644.8207346184125 Test RE 0.35137645882320345\n",
      "80 Train Loss 9.8254795 Test MSE 638.4360473633641 Test RE 0.3496325557001763\n",
      "81 Train Loss 9.801029 Test MSE 635.9215427813234 Test RE 0.3489433558814222\n",
      "82 Train Loss 9.711642 Test MSE 592.8857529732127 Test RE 0.3369292163610609\n",
      "83 Train Loss 9.623414 Test MSE 568.8121772677345 Test RE 0.3300179842762109\n",
      "84 Train Loss 9.503111 Test MSE 554.5806254611464 Test RE 0.3258633450305348\n",
      "85 Train Loss 9.478467 Test MSE 549.5740854147155 Test RE 0.3243891260023148\n",
      "86 Train Loss 9.404474 Test MSE 541.2473816284778 Test RE 0.3219223051696224\n",
      "87 Train Loss 9.330549 Test MSE 529.0834318664084 Test RE 0.318284321190132\n",
      "88 Train Loss 9.268056 Test MSE 530.9782451346025 Test RE 0.31885374967756924\n",
      "89 Train Loss 9.228527 Test MSE 533.87345058763 Test RE 0.3197218569467088\n",
      "90 Train Loss 9.063978 Test MSE 514.5711263832328 Test RE 0.3138888373382933\n",
      "91 Train Loss 9.005996 Test MSE 511.9038170764772 Test RE 0.3130742498458913\n",
      "92 Train Loss 8.939361 Test MSE 501.06718215181763 Test RE 0.3097427460177598\n",
      "93 Train Loss 8.888215 Test MSE 501.2231220693547 Test RE 0.30979094065367957\n",
      "94 Train Loss 8.845982 Test MSE 507.21037056039944 Test RE 0.31163571704923276\n",
      "95 Train Loss 8.814059 Test MSE 504.2840763056541 Test RE 0.3107354427246299\n",
      "96 Train Loss 8.685276 Test MSE 486.103768231064 Test RE 0.30508275427501197\n",
      "97 Train Loss 8.63154 Test MSE 471.5674501031626 Test RE 0.30048657588295474\n",
      "98 Train Loss 8.601152 Test MSE 480.15269495363367 Test RE 0.30320953198022577\n",
      "99 Train Loss 8.554615 Test MSE 468.84382429588493 Test RE 0.29961756113026095\n",
      "Training time: 74.15\n",
      "Training time: 74.15\n",
      "1D_FODE_atanh_\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 47.619743 Test MSE 5200.528616103551 Test RE 0.9978767637555986\n",
      "1 Train Loss 46.557552 Test MSE 5103.344779065352 Test RE 0.9885089816314737\n",
      "2 Train Loss 45.86291 Test MSE 5083.419594795083 Test RE 0.9865773576409499\n",
      "3 Train Loss 45.149086 Test MSE 5030.361588191779 Test RE 0.981415169678399\n",
      "4 Train Loss 44.766582 Test MSE 5006.443592700395 Test RE 0.9790792091093035\n",
      "5 Train Loss 44.26205 Test MSE 4958.719600353777 Test RE 0.9744014918069067\n",
      "6 Train Loss 43.938747 Test MSE 4940.293687072964 Test RE 0.9725894366030938\n",
      "7 Train Loss 43.26683 Test MSE 4857.344775573941 Test RE 0.9643898483511789\n",
      "8 Train Loss 42.74663 Test MSE 4813.599072787177 Test RE 0.9600373336690814\n",
      "9 Train Loss 42.246754 Test MSE 4741.758419885835 Test RE 0.9528463544085226\n",
      "10 Train Loss 41.486553 Test MSE 4671.946377589343 Test RE 0.9458060537206794\n",
      "11 Train Loss 40.56273 Test MSE 4595.0682911624535 Test RE 0.9379920332866917\n",
      "12 Train Loss 39.650925 Test MSE 4510.510122419067 Test RE 0.9293215231921113\n",
      "13 Train Loss 38.795666 Test MSE 4402.204398933467 Test RE 0.9180963618120818\n",
      "14 Train Loss 37.88511 Test MSE 4330.906865092508 Test RE 0.9106313277003297\n",
      "15 Train Loss 36.837727 Test MSE 4214.941346973209 Test RE 0.8983569509557822\n",
      "16 Train Loss 35.75511 Test MSE 4131.020889877859 Test RE 0.8893687393375448\n",
      "17 Train Loss 34.887093 Test MSE 3972.0867781592665 Test RE 0.8720924539862835\n",
      "18 Train Loss 33.756756 Test MSE 3873.402187303081 Test RE 0.8611909581440326\n",
      "19 Train Loss 33.21957 Test MSE 3840.418512472415 Test RE 0.8575164143329282\n",
      "20 Train Loss 32.5039 Test MSE 3774.133870579746 Test RE 0.8500839470589593\n",
      "21 Train Loss 31.573124 Test MSE 3729.262796421386 Test RE 0.8450154684098565\n",
      "22 Train Loss 30.152538 Test MSE 3617.612027853115 Test RE 0.8322698444179418\n",
      "23 Train Loss 28.408983 Test MSE 3395.445210164164 Test RE 0.8063090385260061\n",
      "24 Train Loss 27.095266 Test MSE 3167.7736722181194 Test RE 0.7788077008677395\n",
      "25 Train Loss 24.927387 Test MSE 3070.0789597609273 Test RE 0.766704367717858\n",
      "26 Train Loss 23.712868 Test MSE 3037.8460712501924 Test RE 0.762668916927058\n",
      "27 Train Loss 21.675701 Test MSE 2822.9086674490864 Test RE 0.7351933638361061\n",
      "28 Train Loss 20.249935 Test MSE 2685.5254034664817 Test RE 0.7170803092859551\n",
      "29 Train Loss 18.792105 Test MSE 2441.8433163706122 Test RE 0.6837731787436997\n",
      "30 Train Loss 17.727968 Test MSE 2327.9924766606046 Test RE 0.6676424634481978\n",
      "31 Train Loss 16.792631 Test MSE 2240.120268659552 Test RE 0.654920873610544\n",
      "32 Train Loss 15.808627 Test MSE 2154.3950887580318 Test RE 0.6422673424252352\n",
      "33 Train Loss 14.781659 Test MSE 1990.5741328393071 Test RE 0.6173654807549468\n",
      "34 Train Loss 13.9507265 Test MSE 1828.7404712796547 Test RE 0.5917376501964849\n",
      "35 Train Loss 13.006298 Test MSE 1844.6211885177306 Test RE 0.5943014106851539\n",
      "36 Train Loss 12.219034 Test MSE 1674.8386918991375 Test RE 0.5662909972166567\n",
      "37 Train Loss 10.914015 Test MSE 1398.077106898034 Test RE 0.5173908295604892\n",
      "38 Train Loss 9.9696 Test MSE 1329.5397327734436 Test RE 0.5045495520916037\n",
      "39 Train Loss 9.417009 Test MSE 1157.9380038235356 Test RE 0.47086435881520067\n",
      "40 Train Loss 8.81263 Test MSE 1116.9015548237564 Test RE 0.46244555824457617\n",
      "41 Train Loss 8.152749 Test MSE 1051.0239437629548 Test RE 0.4486002070150624\n",
      "42 Train Loss 7.3329954 Test MSE 980.0249692879814 Test RE 0.43318332960597916\n",
      "43 Train Loss 6.557084 Test MSE 821.7928734547692 Test RE 0.396674561258758\n",
      "44 Train Loss 6.268649 Test MSE 772.4265744558709 Test RE 0.3845756369968805\n",
      "45 Train Loss 5.9848857 Test MSE 762.4995431450137 Test RE 0.3820964111479018\n",
      "46 Train Loss 5.638633 Test MSE 723.6918356309237 Test RE 0.3722459670390748\n",
      "47 Train Loss 5.414785 Test MSE 651.6378167687772 Test RE 0.3532289616402875\n",
      "48 Train Loss 5.1156616 Test MSE 591.741835389662 Test RE 0.3366040227295451\n",
      "49 Train Loss 4.6317735 Test MSE 531.2442259327881 Test RE 0.31893360074258487\n",
      "50 Train Loss 4.467421 Test MSE 502.2384882196002 Test RE 0.3101045655439293\n",
      "51 Train Loss 4.282114 Test MSE 463.99858929612685 Test RE 0.29806535163731135\n",
      "52 Train Loss 4.159062 Test MSE 431.84153989549503 Test RE 0.28755132466589733\n",
      "53 Train Loss 3.9546595 Test MSE 427.11422063787376 Test RE 0.2859730981123926\n",
      "54 Train Loss 3.8124573 Test MSE 390.8813977444327 Test RE 0.27357453265107223\n",
      "55 Train Loss 3.669445 Test MSE 394.2875988143162 Test RE 0.27476393255272713\n",
      "56 Train Loss 3.5932817 Test MSE 378.36530462516237 Test RE 0.2691589450644463\n",
      "57 Train Loss 3.4731925 Test MSE 362.7678945920514 Test RE 0.26355277110079306\n",
      "58 Train Loss 3.401997 Test MSE 343.7548107275405 Test RE 0.25655327148960605\n",
      "59 Train Loss 3.3105133 Test MSE 345.3778618297412 Test RE 0.25715822115590653\n",
      "60 Train Loss 3.2356267 Test MSE 330.3025195275927 Test RE 0.25148327528950754\n",
      "61 Train Loss 3.1628447 Test MSE 306.8435938486731 Test RE 0.24238832359326276\n",
      "62 Train Loss 3.0807734 Test MSE 301.7572145720306 Test RE 0.2403709587994404\n",
      "63 Train Loss 2.8909442 Test MSE 285.05088964715645 Test RE 0.2336223375719674\n",
      "64 Train Loss 2.7022839 Test MSE 245.33436813156987 Test RE 0.21673665199928624\n",
      "65 Train Loss 2.5832534 Test MSE 252.96033657427375 Test RE 0.22007939317174702\n",
      "66 Train Loss 2.53403 Test MSE 237.40988549575025 Test RE 0.2132075422970644\n",
      "67 Train Loss 2.4325671 Test MSE 230.58233707802333 Test RE 0.21011941500965\n",
      "68 Train Loss 2.3467467 Test MSE 209.70231787863116 Test RE 0.20038018816676853\n",
      "69 Train Loss 2.2901847 Test MSE 197.64410032737055 Test RE 0.19453380982895419\n",
      "70 Train Loss 2.22352 Test MSE 196.63873181387348 Test RE 0.19403840542177944\n",
      "71 Train Loss 2.149577 Test MSE 183.42684411852895 Test RE 0.18740648352890293\n",
      "72 Train Loss 2.1097286 Test MSE 173.2854575050278 Test RE 0.18215211711902934\n",
      "73 Train Loss 2.0628424 Test MSE 160.8431484720253 Test RE 0.17549083789418896\n",
      "74 Train Loss 2.0168355 Test MSE 154.920182533571 Test RE 0.17222935084919463\n",
      "75 Train Loss 1.9436969 Test MSE 138.335132000517 Test RE 0.1627494048503951\n",
      "76 Train Loss 1.9090784 Test MSE 137.8875082309186 Test RE 0.16248587985548021\n",
      "77 Train Loss 1.8733375 Test MSE 145.64019209429858 Test RE 0.16699127715362438\n",
      "78 Train Loss 1.8427854 Test MSE 142.7561829904428 Test RE 0.1653296050680959\n",
      "79 Train Loss 1.8127459 Test MSE 136.7995102688303 Test RE 0.16184356490213056\n",
      "80 Train Loss 1.7968878 Test MSE 129.43323465560056 Test RE 0.15742585730291664\n",
      "81 Train Loss 1.7831451 Test MSE 125.19920386791382 Test RE 0.1548295844919526\n",
      "82 Train Loss 1.6905053 Test MSE 117.70889291141937 Test RE 0.15012665295777425\n",
      "83 Train Loss 1.6518071 Test MSE 116.59056086428356 Test RE 0.14941178713232126\n",
      "84 Train Loss 1.6406485 Test MSE 117.5953316064246 Test RE 0.1500542170858908\n",
      "85 Train Loss 1.6044981 Test MSE 116.49695839783838 Test RE 0.1493517989146896\n",
      "86 Train Loss 1.5419339 Test MSE 108.22416528951847 Test RE 0.14395119918446458\n",
      "87 Train Loss 1.5280321 Test MSE 101.47444583336522 Test RE 0.13938996492374364\n",
      "88 Train Loss 1.4460526 Test MSE 87.7196534143777 Test RE 0.12959899037244588\n",
      "89 Train Loss 1.4065248 Test MSE 86.25443562758532 Test RE 0.12851205952568917\n",
      "90 Train Loss 1.3929752 Test MSE 85.1676665888393 Test RE 0.12769989466063406\n",
      "91 Train Loss 1.3712335 Test MSE 90.3591132859055 Test RE 0.1315343384616789\n",
      "92 Train Loss 1.3227458 Test MSE 79.79580284402799 Test RE 0.12360703499876512\n",
      "93 Train Loss 1.2691776 Test MSE 71.33295878204657 Test RE 0.11686871901464195\n",
      "94 Train Loss 1.2398969 Test MSE 71.40263679958976 Test RE 0.11692578375161024\n",
      "95 Train Loss 1.2277918 Test MSE 70.70983473485384 Test RE 0.11635715013071254\n",
      "96 Train Loss 1.1972971 Test MSE 75.76180239357734 Test RE 0.1204420984930474\n",
      "97 Train Loss 1.1824516 Test MSE 74.03534703116935 Test RE 0.11906187645388795\n",
      "98 Train Loss 1.1703125 Test MSE 70.76218336672359 Test RE 0.11640021352277215\n",
      "99 Train Loss 1.1633784 Test MSE 66.40926166875782 Test RE 0.11276322564928468\n",
      "Training time: 73.09\n",
      "Training time: 73.09\n",
      "1D_FODE_atanh_\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 46.898186 Test MSE 5152.626273609004 Test RE 0.9932703841483648\n",
      "1 Train Loss 46.5903 Test MSE 5137.318494015404 Test RE 0.9917938484016003\n",
      "2 Train Loss 46.222336 Test MSE 5108.905171021615 Test RE 0.9890473541356315\n",
      "3 Train Loss 45.864872 Test MSE 5075.299294514063 Test RE 0.9857890589243692\n",
      "4 Train Loss 45.596626 Test MSE 5052.156924543557 Test RE 0.9835389885820196\n",
      "5 Train Loss 45.195072 Test MSE 5008.5054804919855 Test RE 0.9792808036763443\n",
      "6 Train Loss 44.335934 Test MSE 4917.996587272381 Test RE 0.9703921534726351\n",
      "7 Train Loss 43.78684 Test MSE 4877.903994603727 Test RE 0.9664286336598654\n",
      "8 Train Loss 43.471546 Test MSE 4847.8873244062315 Test RE 0.9634505374491644\n",
      "9 Train Loss 42.89833 Test MSE 4798.211831432471 Test RE 0.9585016687518859\n",
      "10 Train Loss 42.345856 Test MSE 4727.703212157153 Test RE 0.9514331242046011\n",
      "11 Train Loss 41.903522 Test MSE 4680.987371392328 Test RE 0.9467207574718337\n",
      "12 Train Loss 41.380547 Test MSE 4635.693821824657 Test RE 0.9421293568424826\n",
      "13 Train Loss 40.84916 Test MSE 4592.2044075383865 Test RE 0.9376996852143594\n",
      "14 Train Loss 40.284912 Test MSE 4518.750492337471 Test RE 0.9301700368278931\n",
      "15 Train Loss 39.76343 Test MSE 4467.413729677723 Test RE 0.9248711915174768\n",
      "16 Train Loss 38.60122 Test MSE 4347.403832243719 Test RE 0.912364033684887\n",
      "17 Train Loss 37.487236 Test MSE 4186.639736902125 Test RE 0.895335820949177\n",
      "18 Train Loss 36.720467 Test MSE 4084.8871811271338 Test RE 0.884388726498202\n",
      "19 Train Loss 35.693317 Test MSE 3950.003396759686 Test RE 0.8696648140624635\n",
      "20 Train Loss 34.57537 Test MSE 3784.6923240833953 Test RE 0.8512722068591033\n",
      "21 Train Loss 33.051548 Test MSE 3617.446139676885 Test RE 0.8322507620359246\n",
      "22 Train Loss 31.732006 Test MSE 3452.285048731798 Test RE 0.8130298454753602\n",
      "23 Train Loss 30.14158 Test MSE 3269.7576437632724 Test RE 0.7912449414666555\n",
      "24 Train Loss 28.479925 Test MSE 3084.6608346594458 Test RE 0.7685230087739677\n",
      "25 Train Loss 27.783314 Test MSE 2988.178807651928 Test RE 0.7564085955583617\n",
      "26 Train Loss 27.198887 Test MSE 2898.4268761560934 Test RE 0.7449623747776315\n",
      "27 Train Loss 26.342566 Test MSE 2812.646619306224 Test RE 0.7338558322793489\n",
      "28 Train Loss 25.804861 Test MSE 2695.7000963574455 Test RE 0.71843743186058\n",
      "29 Train Loss 25.401178 Test MSE 2697.0418624356107 Test RE 0.7186162082484897\n",
      "30 Train Loss 24.406727 Test MSE 2531.6644610601556 Test RE 0.6962356181569195\n",
      "31 Train Loss 23.611574 Test MSE 2430.626051507596 Test RE 0.6822008227865209\n",
      "32 Train Loss 22.071932 Test MSE 2183.485948825805 Test RE 0.6465890795895886\n",
      "33 Train Loss 21.205564 Test MSE 2066.037719103182 Test RE 0.6289589300897251\n",
      "34 Train Loss 20.483707 Test MSE 1912.97308405841 Test RE 0.605212089021751\n",
      "35 Train Loss 20.037035 Test MSE 1938.9225649681143 Test RE 0.6093031132239318\n",
      "36 Train Loss 18.841942 Test MSE 1725.5052401434552 Test RE 0.5747927821707539\n",
      "37 Train Loss 18.427288 Test MSE 1695.3908466966539 Test RE 0.56975491685244\n",
      "38 Train Loss 17.976807 Test MSE 1562.1068064175463 Test RE 0.5469007601780818\n",
      "39 Train Loss 16.650442 Test MSE 1474.0600325778187 Test RE 0.5312644432139925\n",
      "40 Train Loss 16.044065 Test MSE 1218.755623058512 Test RE 0.48307157223200925\n",
      "41 Train Loss 15.284291 Test MSE 1152.3865746400625 Test RE 0.4697342850698182\n",
      "42 Train Loss 14.456583 Test MSE 1068.5092582378174 Test RE 0.45231637379737444\n",
      "43 Train Loss 13.94496 Test MSE 1068.3683144014433 Test RE 0.45228654096863086\n",
      "44 Train Loss 13.4333 Test MSE 1053.226158907346 Test RE 0.4490699380909967\n",
      "45 Train Loss 13.002059 Test MSE 988.2823558044092 Test RE 0.4350044358426675\n",
      "46 Train Loss 12.733673 Test MSE 909.6985909855567 Test RE 0.4173514490336434\n",
      "47 Train Loss 12.371673 Test MSE 905.5736013105759 Test RE 0.41640414271180926\n",
      "48 Train Loss 11.857864 Test MSE 823.151645239583 Test RE 0.39700236137007755\n",
      "49 Train Loss 11.393759 Test MSE 728.8599951715498 Test RE 0.3735727776830294\n",
      "50 Train Loss 11.1102915 Test MSE 711.3876946936633 Test RE 0.36906795549959476\n",
      "51 Train Loss 10.734563 Test MSE 615.6363452105527 Test RE 0.34333279602051076\n",
      "52 Train Loss 10.155416 Test MSE 563.8891563184878 Test RE 0.32858674195292426\n",
      "53 Train Loss 9.925017 Test MSE 570.1067066942195 Test RE 0.33039330605596234\n",
      "54 Train Loss 9.72131 Test MSE 534.9414734198757 Test RE 0.3200415016435727\n",
      "55 Train Loss 9.528414 Test MSE 516.1930433733786 Test RE 0.31438313353359926\n",
      "56 Train Loss 9.405854 Test MSE 515.2463265378019 Test RE 0.3140947061697106\n",
      "57 Train Loss 9.274925 Test MSE 480.38347261755655 Test RE 0.3032823896172412\n",
      "58 Train Loss 9.163093 Test MSE 476.82876375989747 Test RE 0.30215820191266823\n",
      "59 Train Loss 8.94157 Test MSE 407.01896502378753 Test RE 0.2791646910126837\n",
      "60 Train Loss 8.688828 Test MSE 417.1937818230566 Test RE 0.28263248568772165\n",
      "61 Train Loss 8.520569 Test MSE 399.3734168010544 Test RE 0.2765303107083702\n",
      "62 Train Loss 8.297804 Test MSE 395.1723447161515 Test RE 0.2750720325859018\n",
      "63 Train Loss 8.112568 Test MSE 361.269735380467 Test RE 0.26300799781958045\n",
      "64 Train Loss 7.981785 Test MSE 357.1850697409177 Test RE 0.2615169323707729\n",
      "65 Train Loss 7.8633604 Test MSE 353.2653989723656 Test RE 0.2600780592262771\n",
      "66 Train Loss 7.7328186 Test MSE 333.7188357573305 Test RE 0.2527804744071834\n",
      "67 Train Loss 7.501478 Test MSE 301.5378985451092 Test RE 0.24028359256049975\n",
      "68 Train Loss 7.394291 Test MSE 290.76212708178446 Test RE 0.23595114181718055\n",
      "69 Train Loss 7.331597 Test MSE 284.956242371166 Test RE 0.23358354878995768\n",
      "70 Train Loss 7.199324 Test MSE 269.2612954929814 Test RE 0.22705973683552055\n",
      "71 Train Loss 7.100427 Test MSE 273.73158887748997 Test RE 0.22893680828413077\n",
      "72 Train Loss 7.050401 Test MSE 275.3337169961569 Test RE 0.2296058048223829\n",
      "73 Train Loss 6.938316 Test MSE 261.2741413511896 Test RE 0.2236667252779237\n",
      "74 Train Loss 6.874097 Test MSE 244.94706770085102 Test RE 0.21656550729866805\n",
      "75 Train Loss 6.7736506 Test MSE 242.20912886079057 Test RE 0.21535175655572839\n",
      "76 Train Loss 6.7304006 Test MSE 239.1504658236994 Test RE 0.21398768492373282\n",
      "77 Train Loss 6.6535873 Test MSE 233.21207163566888 Test RE 0.211314198110694\n",
      "78 Train Loss 6.590344 Test MSE 234.21524330702013 Test RE 0.2117681989259122\n",
      "79 Train Loss 6.5734816 Test MSE 230.04632471888578 Test RE 0.20987505081856378\n",
      "80 Train Loss 6.5146985 Test MSE 225.60995419350797 Test RE 0.2078415121049156\n",
      "81 Train Loss 6.4894123 Test MSE 228.4059259846165 Test RE 0.20912543070167872\n",
      "82 Train Loss 6.4440727 Test MSE 217.72242874379234 Test RE 0.20417602751080036\n",
      "83 Train Loss 6.390143 Test MSE 206.3495208247285 Test RE 0.19877185789685145\n",
      "84 Train Loss 6.3095093 Test MSE 203.18896539361458 Test RE 0.19724373792659547\n",
      "85 Train Loss 6.2717724 Test MSE 200.3570368728959 Test RE 0.1958643811811721\n",
      "86 Train Loss 6.2492433 Test MSE 198.68408507908669 Test RE 0.1950449476684391\n",
      "87 Train Loss 6.2338448 Test MSE 197.99906578199585 Test RE 0.19470842117716905\n",
      "88 Train Loss 6.18787 Test MSE 192.0551436738955 Test RE 0.1917635832398015\n",
      "89 Train Loss 6.1693707 Test MSE 191.88542391856063 Test RE 0.1916788334610341\n",
      "90 Train Loss 6.139602 Test MSE 187.93618712564557 Test RE 0.18969608612644775\n",
      "91 Train Loss 6.109253 Test MSE 188.36494556736017 Test RE 0.18991234959212092\n",
      "92 Train Loss 6.074798 Test MSE 184.33736302354853 Test RE 0.1878710445178553\n",
      "93 Train Loss 6.0496907 Test MSE 183.25101998321134 Test RE 0.18731664258148104\n",
      "94 Train Loss 5.992542 Test MSE 178.85957785048893 Test RE 0.1850585970336713\n",
      "95 Train Loss 5.966747 Test MSE 174.91786174157053 Test RE 0.18300807150146992\n",
      "96 Train Loss 5.9531927 Test MSE 174.7723353965579 Test RE 0.18293192708388314\n",
      "97 Train Loss 5.923904 Test MSE 170.25853825158126 Test RE 0.18055420814492806\n",
      "98 Train Loss 5.829347 Test MSE 166.24953640505038 Test RE 0.17841583060757168\n",
      "99 Train Loss 5.766083 Test MSE 167.52699185519515 Test RE 0.17909998926828752\n",
      "Training time: 71.40\n",
      "Training time: 71.40\n",
      "1D_FODE_atanh_\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 46.50362 Test MSE 5061.306828947064 Test RE 0.9844292238731502\n",
      "1 Train Loss 45.86676 Test MSE 5012.312660311347 Test RE 0.9796529296415745\n",
      "2 Train Loss 45.19118 Test MSE 4963.7299911875325 Test RE 0.9748936450306831\n",
      "3 Train Loss 44.59384 Test MSE 4911.537866994249 Test RE 0.9697547444720795\n",
      "4 Train Loss 44.064323 Test MSE 4861.421278172569 Test RE 0.9647944431876069\n",
      "5 Train Loss 43.62787 Test MSE 4819.505966052118 Test RE 0.9606261965274969\n",
      "6 Train Loss 43.169563 Test MSE 4767.376556998556 Test RE 0.9554168425259227\n",
      "7 Train Loss 42.728313 Test MSE 4730.966968189626 Test RE 0.9517614770770114\n",
      "8 Train Loss 41.93542 Test MSE 4659.61435439273 Test RE 0.9445569587746098\n",
      "9 Train Loss 41.281673 Test MSE 4595.666167872339 Test RE 0.938053053636608\n",
      "10 Train Loss 40.497257 Test MSE 4511.062390940282 Test RE 0.9293784146855575\n",
      "11 Train Loss 39.605404 Test MSE 4427.790757152389 Test RE 0.9207605620692214\n",
      "12 Train Loss 38.019432 Test MSE 4284.006838821866 Test RE 0.9056872259950923\n",
      "13 Train Loss 37.181118 Test MSE 4175.667971780486 Test RE 0.8941618653000262\n",
      "14 Train Loss 36.173832 Test MSE 4079.8024984227673 Test RE 0.8838381315737618\n",
      "15 Train Loss 35.297905 Test MSE 3934.212208940255 Test RE 0.8679247151421081\n",
      "16 Train Loss 34.280304 Test MSE 3864.2042245055795 Test RE 0.8601678381428259\n",
      "17 Train Loss 32.95995 Test MSE 3746.4747856192103 Test RE 0.8469632600405528\n",
      "18 Train Loss 31.763832 Test MSE 3637.394576124759 Test RE 0.8345423339875836\n",
      "19 Train Loss 30.0236 Test MSE 3416.1594561646125 Test RE 0.808764782467608\n",
      "20 Train Loss 28.552273 Test MSE 3341.6910909035046 Test RE 0.7999011388084751\n",
      "21 Train Loss 27.31369 Test MSE 3173.2831698382815 Test RE 0.7794846708072489\n",
      "22 Train Loss 26.079206 Test MSE 3005.862207145448 Test RE 0.7586434258152566\n",
      "23 Train Loss 24.337124 Test MSE 2820.022443291528 Test RE 0.7348174261498087\n",
      "24 Train Loss 23.502645 Test MSE 2744.2165293244616 Test RE 0.7248737162653169\n",
      "25 Train Loss 22.617146 Test MSE 2607.713165662795 Test RE 0.7066153613905248\n",
      "26 Train Loss 21.380857 Test MSE 2434.392583144482 Test RE 0.6827291920734206\n",
      "27 Train Loss 19.988024 Test MSE 2330.0568328120876 Test RE 0.6679384150960475\n",
      "28 Train Loss 19.07834 Test MSE 2221.6152085615076 Test RE 0.6522101971074747\n",
      "29 Train Loss 17.930706 Test MSE 2082.5475547160013 Test RE 0.6314669545337862\n",
      "30 Train Loss 17.396925 Test MSE 2031.9286710895217 Test RE 0.6237454546712988\n",
      "31 Train Loss 16.72953 Test MSE 1991.6873493191638 Test RE 0.6175380855718436\n",
      "32 Train Loss 15.788588 Test MSE 1847.2806774843116 Test RE 0.5947296744333244\n",
      "33 Train Loss 14.809661 Test MSE 1694.1717581536825 Test RE 0.5695500358955616\n",
      "34 Train Loss 14.046762 Test MSE 1652.4112830580502 Test RE 0.5624866774108677\n",
      "35 Train Loss 12.895041 Test MSE 1549.8583362244758 Test RE 0.5447524177989489\n",
      "36 Train Loss 12.493037 Test MSE 1496.9793648575678 Test RE 0.535378678769516\n",
      "37 Train Loss 11.7974 Test MSE 1471.590928952173 Test RE 0.5308193131988314\n",
      "38 Train Loss 11.544571 Test MSE 1457.6760803414497 Test RE 0.5283037316689666\n",
      "39 Train Loss 10.942223 Test MSE 1392.131684019886 Test RE 0.5162895367299367\n",
      "40 Train Loss 10.492801 Test MSE 1328.399828135295 Test RE 0.5043332134258275\n",
      "41 Train Loss 9.98948 Test MSE 1232.8221111850262 Test RE 0.4858513034459961\n",
      "42 Train Loss 9.432055 Test MSE 1134.6417301987876 Test RE 0.46610368959737175\n",
      "43 Train Loss 8.825279 Test MSE 1051.0501745619056 Test RE 0.44860580492198493\n",
      "44 Train Loss 8.276447 Test MSE 1028.4311973820688 Test RE 0.4437524725380877\n",
      "45 Train Loss 7.4963255 Test MSE 920.5033526384714 Test RE 0.41982263679142495\n",
      "46 Train Loss 7.053345 Test MSE 864.956436351576 Test RE 0.40695864821164107\n",
      "47 Train Loss 6.6163807 Test MSE 783.8007748293635 Test RE 0.3873967820714538\n",
      "48 Train Loss 6.1439385 Test MSE 738.4592875137819 Test RE 0.37602476035086485\n",
      "49 Train Loss 5.7498903 Test MSE 673.2114823722378 Test RE 0.3590284978700779\n",
      "50 Train Loss 5.4500065 Test MSE 609.2537875469777 Test RE 0.3415484222942538\n",
      "51 Train Loss 5.1283927 Test MSE 567.4270432880558 Test RE 0.3296159203189359\n",
      "52 Train Loss 4.8975163 Test MSE 548.8807319682269 Test RE 0.3241844335786883\n",
      "53 Train Loss 4.604653 Test MSE 474.99783250200414 Test RE 0.30157752910507535\n",
      "54 Train Loss 4.306506 Test MSE 408.4461356202574 Test RE 0.2796536940233342\n",
      "55 Train Loss 4.114235 Test MSE 377.25279028908903 Test RE 0.26876294730757816\n",
      "56 Train Loss 3.940457 Test MSE 376.4925185667829 Test RE 0.2684919938180512\n",
      "57 Train Loss 3.7885764 Test MSE 362.68546970669246 Test RE 0.26352282834489865\n",
      "58 Train Loss 3.6657004 Test MSE 331.43239638924393 Test RE 0.25191303669851284\n",
      "59 Train Loss 3.585064 Test MSE 300.45687180320334 Test RE 0.23985249216615653\n",
      "60 Train Loss 3.4214804 Test MSE 297.32111434064063 Test RE 0.23859758336157055\n",
      "61 Train Loss 3.2542756 Test MSE 278.5841530492286 Test RE 0.23095712721828932\n",
      "62 Train Loss 3.0547411 Test MSE 240.8487672341146 Test RE 0.2147461460820828\n",
      "63 Train Loss 2.8985345 Test MSE 236.2377719835647 Test RE 0.2126805788698566\n",
      "64 Train Loss 2.8210852 Test MSE 234.69380826656445 Test RE 0.21198443831799332\n",
      "65 Train Loss 2.7248838 Test MSE 226.10690781364562 Test RE 0.2080702936243131\n",
      "66 Train Loss 2.6285684 Test MSE 224.3896743543567 Test RE 0.2072786630715906\n",
      "67 Train Loss 2.5421538 Test MSE 206.0778759530102 Test RE 0.19864098010848086\n",
      "68 Train Loss 2.4716547 Test MSE 209.64411574355833 Test RE 0.200352378831533\n",
      "69 Train Loss 2.366154 Test MSE 192.556392229938 Test RE 0.19201366397677255\n",
      "70 Train Loss 2.2894926 Test MSE 176.73029378936758 Test RE 0.18395375795915603\n",
      "71 Train Loss 2.207303 Test MSE 169.71826164692618 Test RE 0.1802675067374505\n",
      "72 Train Loss 2.1184657 Test MSE 166.1088859789345 Test RE 0.17834034296125514\n",
      "73 Train Loss 2.0309885 Test MSE 148.23656107825346 Test RE 0.16847320207057206\n",
      "74 Train Loss 1.9705358 Test MSE 133.54920401550507 Test RE 0.15990933477604333\n",
      "75 Train Loss 1.9126544 Test MSE 127.59469196558396 Test RE 0.1563037755331266\n",
      "76 Train Loss 1.8484325 Test MSE 122.3767325177928 Test RE 0.15307440898448382\n",
      "77 Train Loss 1.762146 Test MSE 120.15222895139665 Test RE 0.1516767731188346\n",
      "78 Train Loss 1.700882 Test MSE 107.87896478628936 Test RE 0.14372143665864826\n",
      "79 Train Loss 1.6337314 Test MSE 100.08632095009736 Test RE 0.13843328583734527\n",
      "80 Train Loss 1.5801587 Test MSE 88.84246248275049 Test RE 0.13042578474397318\n",
      "81 Train Loss 1.4876578 Test MSE 75.56649071504266 Test RE 0.12028675022958024\n",
      "82 Train Loss 1.4308469 Test MSE 70.69616285627093 Test RE 0.11634590065078312\n",
      "83 Train Loss 1.4041559 Test MSE 71.03515359181705 Test RE 0.11662450853115504\n",
      "84 Train Loss 1.3192245 Test MSE 68.28398791868489 Test RE 0.1143437952663095\n",
      "85 Train Loss 1.2733449 Test MSE 68.15260416351282 Test RE 0.11423373908081623\n",
      "86 Train Loss 1.2286288 Test MSE 64.19993023888878 Test RE 0.11087163263437369\n",
      "87 Train Loss 1.2031648 Test MSE 64.87398259873132 Test RE 0.11145214843230582\n",
      "88 Train Loss 1.1591811 Test MSE 64.98712778351455 Test RE 0.11154929662556477\n",
      "89 Train Loss 1.1351793 Test MSE 62.683900758898396 Test RE 0.10955473983730415\n",
      "90 Train Loss 1.111727 Test MSE 62.66853544560725 Test RE 0.10954131177962724\n",
      "91 Train Loss 1.087122 Test MSE 59.299309226562755 Test RE 0.10655601782941886\n",
      "92 Train Loss 1.059117 Test MSE 56.91517943602719 Test RE 0.10439200030066588\n",
      "93 Train Loss 1.0273612 Test MSE 52.92500643175219 Test RE 0.10066618828668233\n",
      "94 Train Loss 1.0028938 Test MSE 50.24221368814864 Test RE 0.09808160134971934\n",
      "95 Train Loss 0.9750931 Test MSE 48.20044247237754 Test RE 0.09606798402592776\n",
      "96 Train Loss 0.94977045 Test MSE 46.445459133337984 Test RE 0.09430284496290986\n",
      "97 Train Loss 0.93107635 Test MSE 46.67214766410726 Test RE 0.09453269899864694\n",
      "98 Train Loss 0.91681963 Test MSE 45.913842312168036 Test RE 0.09376159440137159\n",
      "99 Train Loss 0.89882547 Test MSE 42.660029389297314 Test RE 0.09037821088243876\n",
      "Training time: 48.24\n",
      "Training time: 48.24\n",
      "1D_FODE_atanh_\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 47.237423 Test MSE 5182.501776305309 Test RE 0.9961457685416024\n",
      "1 Train Loss 46.828697 Test MSE 5145.504693340886 Test RE 0.9925837342484384\n",
      "2 Train Loss 46.230568 Test MSE 5094.446586779996 Test RE 0.9876468234907568\n",
      "3 Train Loss 45.798695 Test MSE 5052.878447412778 Test RE 0.9836092180452424\n",
      "4 Train Loss 45.429688 Test MSE 5017.871619767668 Test RE 0.9801960264292158\n",
      "5 Train Loss 45.11052 Test MSE 4985.854204481647 Test RE 0.9770638652545925\n",
      "6 Train Loss 44.431892 Test MSE 4914.643588424705 Test RE 0.9700612993854172\n",
      "7 Train Loss 44.040043 Test MSE 4894.356809570472 Test RE 0.968057108337392\n",
      "8 Train Loss 43.361103 Test MSE 4819.301538920018 Test RE 0.9606058230553791\n",
      "9 Train Loss 42.640507 Test MSE 4736.388785113229 Test RE 0.952306693193566\n",
      "10 Train Loss 41.72162 Test MSE 4616.99951715267 Test RE 0.940227781135753\n",
      "11 Train Loss 40.699444 Test MSE 4475.829739968003 Test RE 0.9257419484598757\n",
      "12 Train Loss 38.89175 Test MSE 4284.563259325992 Test RE 0.9057460408615781\n",
      "13 Train Loss 37.66261 Test MSE 4180.118532738364 Test RE 0.8946382516056169\n",
      "14 Train Loss 35.873608 Test MSE 3981.1450937359914 Test RE 0.8730862880127593\n",
      "15 Train Loss 31.757507 Test MSE 3577.0971612643493 Test RE 0.8275962859603176\n",
      "16 Train Loss 29.368925 Test MSE 3327.6467548095443 Test RE 0.7982184714787767\n",
      "17 Train Loss 27.164848 Test MSE 2975.1664264814763 Test RE 0.7547598629569574\n",
      "18 Train Loss 25.022663 Test MSE 2847.828274226561 Test RE 0.7384312430929346\n",
      "19 Train Loss 23.530123 Test MSE 2644.705580509563 Test RE 0.7116096527070376\n",
      "20 Train Loss 22.125113 Test MSE 2499.6199668667587 Test RE 0.6918152915943763\n",
      "21 Train Loss 20.571117 Test MSE 2347.1136343029098 Test RE 0.670378724449656\n",
      "22 Train Loss 19.61007 Test MSE 2327.6243854398576 Test RE 0.6675896791185738\n",
      "23 Train Loss 18.271297 Test MSE 2162.5488207662634 Test RE 0.6434815881717305\n",
      "24 Train Loss 17.77608 Test MSE 2112.7102876381787 Test RE 0.6360234645866198\n",
      "25 Train Loss 16.763176 Test MSE 2092.058620481743 Test RE 0.6329072774736986\n",
      "26 Train Loss 16.403017 Test MSE 2046.0404600784625 Test RE 0.6259076699101777\n",
      "27 Train Loss 15.755549 Test MSE 2001.433389968062 Test RE 0.6190471594013846\n",
      "28 Train Loss 15.331558 Test MSE 1984.42286309101 Test RE 0.616410851659492\n",
      "29 Train Loss 15.048425 Test MSE 1965.1837422310452 Test RE 0.6134155004072654\n",
      "30 Train Loss 14.527346 Test MSE 1918.5473523784026 Test RE 0.6060932202196676\n",
      "31 Train Loss 13.901677 Test MSE 1854.0059031751732 Test RE 0.5958112798488653\n",
      "32 Train Loss 13.396004 Test MSE 1815.607121241894 Test RE 0.5896089990400638\n",
      "33 Train Loss 13.085744 Test MSE 1775.0735768988893 Test RE 0.5829903207440245\n",
      "34 Train Loss 12.999008 Test MSE 1757.207253866715 Test RE 0.5800489679002578\n",
      "35 Train Loss 12.539409 Test MSE 1740.2219280742593 Test RE 0.577238757323701\n",
      "36 Train Loss 12.224643 Test MSE 1672.0325441975554 Test RE 0.5658163955824046\n",
      "37 Train Loss 11.869288 Test MSE 1633.9336657010563 Test RE 0.5593329127757254\n",
      "38 Train Loss 11.37058 Test MSE 1548.1492414859265 Test RE 0.5444519741121012\n",
      "39 Train Loss 10.912012 Test MSE 1479.4324775147247 Test RE 0.5322317013302643\n",
      "40 Train Loss 10.657136 Test MSE 1452.1730606659196 Test RE 0.5273055624121505\n",
      "41 Train Loss 10.443597 Test MSE 1496.6659385630765 Test RE 0.5353226290524782\n",
      "42 Train Loss 10.056895 Test MSE 1403.6447763848025 Test RE 0.5184200284678433\n",
      "43 Train Loss 9.768777 Test MSE 1365.808201695933 Test RE 0.5113850444407325\n",
      "44 Train Loss 9.441524 Test MSE 1298.5892467830959 Test RE 0.4986422411480358\n",
      "45 Train Loss 9.132627 Test MSE 1242.565476354786 Test RE 0.4877674396787594\n",
      "46 Train Loss 8.961961 Test MSE 1194.9136161609308 Test RE 0.4783231711482561\n",
      "47 Train Loss 8.6251335 Test MSE 1143.603688961731 Test RE 0.46794082723745006\n",
      "48 Train Loss 8.446437 Test MSE 1098.950873404647 Test RE 0.4587143262562723\n",
      "49 Train Loss 8.221143 Test MSE 1036.844675050248 Test RE 0.44556391931528566\n",
      "50 Train Loss 8.027515 Test MSE 988.0409513419764 Test RE 0.43495130405047727\n",
      "51 Train Loss 7.818522 Test MSE 987.2961077086906 Test RE 0.43478732714786966\n",
      "52 Train Loss 7.532486 Test MSE 961.8439495687112 Test RE 0.42914639970401314\n",
      "53 Train Loss 7.2270913 Test MSE 939.1843580819966 Test RE 0.4240612509195002\n",
      "54 Train Loss 7.0688586 Test MSE 947.625767389207 Test RE 0.42596272370141985\n",
      "55 Train Loss 6.9283295 Test MSE 913.0487185406973 Test RE 0.41811922844202687\n",
      "56 Train Loss 6.595846 Test MSE 839.1639668160442 Test RE 0.40084509928756173\n",
      "57 Train Loss 6.5140395 Test MSE 815.1845665383909 Test RE 0.3950764466938748\n",
      "58 Train Loss 6.3853755 Test MSE 760.8067096887182 Test RE 0.3816720272913305\n",
      "59 Train Loss 6.296348 Test MSE 750.8909438071881 Test RE 0.37917666103908787\n",
      "60 Train Loss 6.2103815 Test MSE 735.832667432376 Test RE 0.3753554248631573\n",
      "61 Train Loss 6.0973306 Test MSE 708.6571763634927 Test RE 0.3683589780364834\n",
      "62 Train Loss 5.9999676 Test MSE 677.9255107395852 Test RE 0.36028331756609017\n",
      "63 Train Loss 5.8846946 Test MSE 659.6474335267659 Test RE 0.3553931912814938\n",
      "64 Train Loss 5.6040683 Test MSE 628.2087207165825 Test RE 0.3468208075615728\n",
      "65 Train Loss 5.500793 Test MSE 634.3749296459278 Test RE 0.348518768194332\n",
      "66 Train Loss 5.40044 Test MSE 634.994113926615 Test RE 0.3486888133056044\n",
      "67 Train Loss 5.3017254 Test MSE 619.1780990385813 Test RE 0.3443189758939318\n",
      "68 Train Loss 5.2290306 Test MSE 600.4764956886253 Test RE 0.3390792164963999\n",
      "69 Train Loss 5.1309633 Test MSE 601.3173584861837 Test RE 0.339316544148301\n",
      "70 Train Loss 5.065054 Test MSE 594.0207093867026 Test RE 0.3372515526136543\n",
      "71 Train Loss 4.986479 Test MSE 569.3996006355196 Test RE 0.3301883482895966\n",
      "72 Train Loss 4.8944697 Test MSE 553.2666219048174 Test RE 0.32547707159488315\n",
      "73 Train Loss 4.8384204 Test MSE 533.0666123586642 Test RE 0.3194801691882483\n",
      "74 Train Loss 4.734205 Test MSE 518.7681628133454 Test RE 0.31516633563618673\n",
      "75 Train Loss 4.676326 Test MSE 512.2386963917244 Test RE 0.31317663719485533\n",
      "76 Train Loss 4.6243625 Test MSE 528.8160738431362 Test RE 0.3182038928353892\n",
      "77 Train Loss 4.504347 Test MSE 515.5150819389927 Test RE 0.3141766122789846\n",
      "78 Train Loss 4.424134 Test MSE 515.8045497050057 Test RE 0.314264806826853\n",
      "79 Train Loss 4.3797326 Test MSE 507.07328371447204 Test RE 0.31159360035857975\n",
      "80 Train Loss 4.318153 Test MSE 481.616343554209 Test RE 0.3036713168702913\n",
      "81 Train Loss 4.2533746 Test MSE 476.1629859811129 Test RE 0.30194718225273626\n",
      "82 Train Loss 4.205496 Test MSE 465.05413537828446 Test RE 0.2984041920908542\n",
      "83 Train Loss 4.137739 Test MSE 449.4143331618551 Test RE 0.29334360461452386\n",
      "84 Train Loss 4.047586 Test MSE 425.7633239290176 Test RE 0.28552049544755925\n",
      "85 Train Loss 4.0105433 Test MSE 419.3461184257818 Test RE 0.28336060975684396\n",
      "86 Train Loss 3.911002 Test MSE 418.9007791254163 Test RE 0.2832101074255267\n",
      "87 Train Loss 3.8417408 Test MSE 423.34188913611695 Test RE 0.28470742040023284\n",
      "88 Train Loss 3.7933261 Test MSE 410.14002971007045 Test RE 0.28023297924841667\n",
      "89 Train Loss 3.758946 Test MSE 404.3677218282667 Test RE 0.2782539930299602\n",
      "90 Train Loss 3.700611 Test MSE 399.410250694292 Test RE 0.2765430624998884\n",
      "91 Train Loss 3.6341994 Test MSE 402.97573529481934 Test RE 0.27777465245\n",
      "92 Train Loss 3.5792987 Test MSE 409.86517999233837 Test RE 0.2801390663677019\n",
      "93 Train Loss 3.5452225 Test MSE 407.2949975343264 Test RE 0.2792593370570406\n",
      "94 Train Loss 3.4844878 Test MSE 408.03383058542966 Test RE 0.2795125104881186\n",
      "95 Train Loss 3.3862848 Test MSE 394.3000608738503 Test RE 0.2747682746845109\n",
      "96 Train Loss 3.3549418 Test MSE 390.25443566221304 Test RE 0.2733550419137013\n",
      "97 Train Loss 3.317721 Test MSE 379.37079091108444 Test RE 0.26951634578689854\n",
      "98 Train Loss 3.2911115 Test MSE 376.10839082214665 Test RE 0.26835499037299165\n",
      "99 Train Loss 3.2556896 Test MSE 367.8053300034352 Test RE 0.26537632398642585\n",
      "Training time: 32.17\n",
      "Training time: 32.17\n",
      "1D_FODE_atanh_\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 47.181805 Test MSE 5197.254738814226 Test RE 0.9975626187299009\n",
      "1 Train Loss 46.80962 Test MSE 5172.92050615968 Test RE 0.9952245187728468\n",
      "2 Train Loss 46.500687 Test MSE 5141.834503150163 Test RE 0.9922296755944033\n",
      "3 Train Loss 46.12976 Test MSE 5117.502739034418 Test RE 0.989879218019437\n",
      "4 Train Loss 45.818226 Test MSE 5093.416264679183 Test RE 0.9875469455365434\n",
      "5 Train Loss 45.457523 Test MSE 5068.684786685572 Test RE 0.9851464726493687\n",
      "6 Train Loss 45.07747 Test MSE 5036.391891187236 Test RE 0.9820032445245207\n",
      "7 Train Loss 44.728394 Test MSE 5004.910348650293 Test RE 0.9789292741009059\n",
      "8 Train Loss 44.27703 Test MSE 4968.636587658873 Test RE 0.9753753622295516\n",
      "9 Train Loss 43.954212 Test MSE 4936.872023791489 Test RE 0.9722525689796672\n",
      "10 Train Loss 43.501644 Test MSE 4891.873797383343 Test RE 0.9678115191179539\n",
      "11 Train Loss 43.0785 Test MSE 4843.180568585142 Test RE 0.9629827225707955\n",
      "12 Train Loss 42.56944 Test MSE 4783.112875191549 Test RE 0.9569923796056807\n",
      "13 Train Loss 41.828342 Test MSE 4723.989363374084 Test RE 0.9510593514870745\n",
      "14 Train Loss 41.181736 Test MSE 4679.073969576307 Test RE 0.9465272467631235\n",
      "15 Train Loss 40.562317 Test MSE 4633.241296325686 Test RE 0.9418801059161525\n",
      "16 Train Loss 39.814034 Test MSE 4571.064550648672 Test RE 0.9355388816792622\n",
      "17 Train Loss 39.08556 Test MSE 4489.589359850414 Test RE 0.9271638169603553\n",
      "18 Train Loss 38.43598 Test MSE 4458.162363521283 Test RE 0.9239130581458548\n",
      "19 Train Loss 37.410957 Test MSE 4375.363422306138 Test RE 0.9152931888762786\n",
      "20 Train Loss 36.906673 Test MSE 4346.9296847768055 Test RE 0.9123142790602876\n",
      "21 Train Loss 36.511242 Test MSE 4316.412324860329 Test RE 0.9091062147162975\n",
      "22 Train Loss 36.00422 Test MSE 4260.087164991421 Test RE 0.9031552426965067\n",
      "23 Train Loss 35.285126 Test MSE 4182.709338544734 Test RE 0.8949154536665131\n",
      "24 Train Loss 34.137833 Test MSE 4053.462123974196 Test RE 0.880980355279213\n",
      "25 Train Loss 32.998325 Test MSE 3949.9085305402214 Test RE 0.8696543707412576\n",
      "26 Train Loss 32.277557 Test MSE 3905.0203538918927 Test RE 0.8646987190334238\n",
      "27 Train Loss 30.88508 Test MSE 3807.062535156993 Test RE 0.8537843109574735\n",
      "28 Train Loss 29.2385 Test MSE 3641.7047437318383 Test RE 0.835036637301364\n",
      "29 Train Loss 28.416615 Test MSE 3607.722790954487 Test RE 0.8311315042502622\n",
      "30 Train Loss 27.603668 Test MSE 3526.140534671898 Test RE 0.8216804863277865\n",
      "31 Train Loss 26.53875 Test MSE 3429.1419331261322 Test RE 0.8103001050419743\n",
      "32 Train Loss 25.43803 Test MSE 3252.3373254384965 Test RE 0.7891343650471838\n",
      "33 Train Loss 24.514439 Test MSE 3205.6694994259014 Test RE 0.7834522591681787\n",
      "34 Train Loss 23.35439 Test MSE 3044.4166330185385 Test RE 0.763493260289644\n",
      "35 Train Loss 22.57534 Test MSE 2922.1831128973186 Test RE 0.7480090942310877\n",
      "36 Train Loss 21.489386 Test MSE 2784.412463171015 Test RE 0.7301632140049151\n",
      "37 Train Loss 21.119532 Test MSE 2792.7002102118627 Test RE 0.7312490645769912\n",
      "38 Train Loss 20.454367 Test MSE 2689.525745718798 Test RE 0.7176141897699903\n",
      "39 Train Loss 20.014626 Test MSE 2733.767643257119 Test RE 0.7234923845874416\n",
      "40 Train Loss 19.400404 Test MSE 2683.8600870165883 Test RE 0.7168579411007914\n",
      "41 Train Loss 18.663044 Test MSE 2579.878668736119 Test RE 0.7028340694663436\n",
      "42 Train Loss 17.862904 Test MSE 2511.9537134326997 Test RE 0.6935199856865921\n",
      "43 Train Loss 17.278683 Test MSE 2481.524591025117 Test RE 0.6893066309405446\n",
      "44 Train Loss 16.913273 Test MSE 2405.667338149804 Test RE 0.6786892191792434\n",
      "45 Train Loss 15.841631 Test MSE 2252.7856057086847 Test RE 0.6567696810915116\n",
      "46 Train Loss 15.50443 Test MSE 2232.358565196631 Test RE 0.6537852844725459\n",
      "47 Train Loss 15.038634 Test MSE 2169.4136362949566 Test RE 0.6445021159546224\n",
      "48 Train Loss 14.610246 Test MSE 2062.844114623709 Test RE 0.6284726313980044\n",
      "49 Train Loss 14.24492 Test MSE 1980.4868233881834 Test RE 0.6157992325568992\n",
      "50 Train Loss 13.810321 Test MSE 1994.1580231512162 Test RE 0.617920992637104\n",
      "51 Train Loss 13.341137 Test MSE 1936.4848516675822 Test RE 0.6089199691328254\n",
      "52 Train Loss 12.948973 Test MSE 1829.9006096353546 Test RE 0.591925317258232\n",
      "53 Train Loss 12.603676 Test MSE 1742.1875923660732 Test RE 0.5775646747722404\n",
      "54 Train Loss 12.346189 Test MSE 1740.1895894886527 Test RE 0.5772333938779841\n",
      "55 Train Loss 11.953652 Test MSE 1651.232264071613 Test RE 0.5622859704746335\n",
      "56 Train Loss 11.566348 Test MSE 1602.0557273691784 Test RE 0.553849762904235\n",
      "57 Train Loss 11.35561 Test MSE 1574.2265444711536 Test RE 0.5490182488850475\n",
      "58 Train Loss 11.023376 Test MSE 1513.3638659733135 Test RE 0.5383005763552534\n",
      "59 Train Loss 10.745505 Test MSE 1444.483651468824 Test RE 0.525907640193749\n",
      "60 Train Loss 10.556979 Test MSE 1428.2528269425486 Test RE 0.5229446338057372\n",
      "61 Train Loss 10.379666 Test MSE 1438.1201120898556 Test RE 0.524747942769128\n",
      "62 Train Loss 10.100725 Test MSE 1370.7560551239171 Test RE 0.5123104931309449\n",
      "63 Train Loss 9.861554 Test MSE 1288.4609681847173 Test RE 0.4966938676724163\n",
      "64 Train Loss 9.391574 Test MSE 1221.3639441274547 Test RE 0.4835882190091051\n",
      "65 Train Loss 9.115453 Test MSE 1190.284133269441 Test RE 0.477395682379348\n",
      "66 Train Loss 8.635363 Test MSE 1124.068648583059 Test RE 0.4639269290008613\n",
      "67 Train Loss 8.34691 Test MSE 1080.3643710189165 Test RE 0.45481867780172064\n",
      "68 Train Loss 8.093031 Test MSE 1046.7595682599415 Test RE 0.44768921725108923\n",
      "69 Train Loss 7.81405 Test MSE 996.8689657971818 Test RE 0.4368900989811561\n",
      "70 Train Loss 7.4229474 Test MSE 932.6584214563117 Test RE 0.42258538469341106\n",
      "71 Train Loss 7.244963 Test MSE 926.0252246716395 Test RE 0.4210799601197637\n",
      "72 Train Loss 7.020042 Test MSE 921.3249495598811 Test RE 0.4200099517280912\n",
      "73 Train Loss 6.893326 Test MSE 918.3278657189229 Test RE 0.41932624592137907\n",
      "74 Train Loss 6.753137 Test MSE 881.955100679211 Test RE 0.4109380945130372\n",
      "75 Train Loss 6.527927 Test MSE 839.5945483056139 Test RE 0.40094792445182936\n",
      "76 Train Loss 6.344604 Test MSE 829.2878477926932 Test RE 0.39847934546801095\n",
      "77 Train Loss 6.111896 Test MSE 759.9001440183315 Test RE 0.3814445622271927\n",
      "78 Train Loss 5.9802146 Test MSE 752.9038221121883 Test RE 0.37968454149255554\n",
      "79 Train Loss 5.898667 Test MSE 737.1742927717319 Test RE 0.37569745711889735\n",
      "80 Train Loss 5.747794 Test MSE 707.965337152524 Test RE 0.36817912561778937\n",
      "81 Train Loss 5.6645055 Test MSE 711.1774082149624 Test RE 0.369013403152417\n",
      "82 Train Loss 5.5128536 Test MSE 691.1434185249043 Test RE 0.3637786886389244\n",
      "83 Train Loss 5.3786373 Test MSE 691.8470821510685 Test RE 0.3639638258360132\n",
      "84 Train Loss 5.1999216 Test MSE 654.5046703017905 Test RE 0.3540051169904988\n",
      "85 Train Loss 5.0548387 Test MSE 624.5632098649211 Test RE 0.34581303851526046\n",
      "86 Train Loss 4.9721894 Test MSE 611.558218004497 Test RE 0.34219374592881985\n",
      "87 Train Loss 4.8881207 Test MSE 597.0574855993132 Test RE 0.33811250903907086\n",
      "88 Train Loss 4.7766623 Test MSE 591.5379161131184 Test RE 0.33654601942540396\n",
      "89 Train Loss 4.7205334 Test MSE 588.7746341320853 Test RE 0.3357590367771343\n",
      "90 Train Loss 4.627597 Test MSE 585.8623204831798 Test RE 0.33492760845152725\n",
      "91 Train Loss 4.5405498 Test MSE 572.7799559359466 Test RE 0.33116701271352533\n",
      "92 Train Loss 4.450545 Test MSE 559.913669756747 Test RE 0.32742640512639426\n",
      "93 Train Loss 4.3383207 Test MSE 527.9324640006533 Test RE 0.31793793491903577\n",
      "94 Train Loss 4.258158 Test MSE 504.01099843383935 Test RE 0.3106512972319641\n",
      "95 Train Loss 4.124954 Test MSE 468.09581739590277 Test RE 0.2993784564933664\n",
      "96 Train Loss 3.9762115 Test MSE 444.99415641761254 Test RE 0.291897461841318\n",
      "97 Train Loss 3.8623488 Test MSE 434.9269336891338 Test RE 0.28857673550303753\n",
      "98 Train Loss 3.7871358 Test MSE 439.3235637405703 Test RE 0.29003166356297083\n",
      "99 Train Loss 3.7100947 Test MSE 436.18692498029435 Test RE 0.2889944392415255\n",
      "Training time: 32.55\n",
      "Training time: 32.55\n",
      "1D_FODE_atanh_\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 46.650436 Test MSE 5145.369605769898 Test RE 0.9925707047582601\n",
      "1 Train Loss 45.90367 Test MSE 5066.343512553019 Test RE 0.984918922065718\n",
      "2 Train Loss 45.610233 Test MSE 5043.745316135629 Test RE 0.9827198739523277\n",
      "3 Train Loss 45.320274 Test MSE 5000.676552036579 Test RE 0.9785151343826968\n",
      "4 Train Loss 44.949017 Test MSE 4966.004138119063 Test RE 0.9751169445973813\n",
      "5 Train Loss 44.668407 Test MSE 4934.300151715602 Test RE 0.9719992876542825\n",
      "6 Train Loss 44.188572 Test MSE 4886.778636066351 Test RE 0.9673073727819376\n",
      "7 Train Loss 43.80153 Test MSE 4845.82499689503 Test RE 0.9632455861191362\n",
      "8 Train Loss 43.46673 Test MSE 4806.516074083115 Test RE 0.9593307473514877\n",
      "9 Train Loss 43.1972 Test MSE 4787.608561524921 Test RE 0.9574420164006672\n",
      "10 Train Loss 42.79371 Test MSE 4741.7792459756365 Test RE 0.9528484468856375\n",
      "11 Train Loss 42.194897 Test MSE 4662.329652499886 Test RE 0.9448321296359917\n",
      "12 Train Loss 41.717747 Test MSE 4625.0923968528805 Test RE 0.941051456642014\n",
      "13 Train Loss 40.71292 Test MSE 4501.258384171955 Test RE 0.9283679444791574\n",
      "14 Train Loss 39.782093 Test MSE 4422.372171850156 Test RE 0.9201969912877834\n",
      "15 Train Loss 38.216362 Test MSE 4242.770318870727 Test RE 0.9013177537306454\n",
      "16 Train Loss 36.59006 Test MSE 3992.888101304132 Test RE 0.8743729918784581\n",
      "17 Train Loss 35.244076 Test MSE 3905.611166761334 Test RE 0.8647641291674631\n",
      "18 Train Loss 33.582554 Test MSE 3696.23850407207 Test RE 0.8412656531541388\n",
      "19 Train Loss 32.923103 Test MSE 3603.7961236707347 Test RE 0.8306790768350772\n",
      "20 Train Loss 32.23539 Test MSE 3497.73268824358 Test RE 0.8183639173489976\n",
      "21 Train Loss 31.9108 Test MSE 3487.3335659107497 Test RE 0.8171464713424788\n",
      "22 Train Loss 31.598951 Test MSE 3512.10986365664 Test RE 0.8200441051803228\n",
      "23 Train Loss 31.15511 Test MSE 3438.3565936381115 Test RE 0.8113880785403035\n",
      "24 Train Loss 29.791737 Test MSE 3299.874597304139 Test RE 0.7948805726284662\n",
      "25 Train Loss 28.33062 Test MSE 3250.6823361785077 Test RE 0.7889335594488032\n",
      "26 Train Loss 27.460533 Test MSE 3148.1653549127504 Test RE 0.7763935742093581\n",
      "27 Train Loss 26.553364 Test MSE 3061.5366808819585 Test RE 0.7656369742118837\n",
      "28 Train Loss 25.749784 Test MSE 2977.8720115533083 Test RE 0.7551029703126113\n",
      "29 Train Loss 23.991266 Test MSE 2747.0486206340724 Test RE 0.7252476625523543\n",
      "30 Train Loss 22.652786 Test MSE 2639.4122321146933 Test RE 0.7108971566677608\n",
      "31 Train Loss 21.206347 Test MSE 2488.898467770517 Test RE 0.690330012254134\n",
      "32 Train Loss 20.453936 Test MSE 2505.617412859519 Test RE 0.6926447454895651\n",
      "33 Train Loss 19.47964 Test MSE 2426.021951377861 Test RE 0.6815544029699305\n",
      "34 Train Loss 18.27941 Test MSE 2216.1056543057944 Test RE 0.651400961984423\n",
      "35 Train Loss 17.731544 Test MSE 2176.1023722376517 Test RE 0.645494915728634\n",
      "36 Train Loss 16.864382 Test MSE 2078.88897644458 Test RE 0.6309120363938161\n",
      "37 Train Loss 16.61238 Test MSE 2060.9872228266286 Test RE 0.6281897044411949\n",
      "38 Train Loss 16.201378 Test MSE 1964.3941408604294 Test RE 0.6132922543280328\n",
      "39 Train Loss 15.717542 Test MSE 1953.4388856465757 Test RE 0.6115797245605791\n",
      "40 Train Loss 14.889272 Test MSE 1849.9036980406984 Test RE 0.5951517637244825\n",
      "41 Train Loss 14.004504 Test MSE 1826.3943741777848 Test RE 0.5913579573015264\n",
      "42 Train Loss 13.477776 Test MSE 1790.5196468091813 Test RE 0.5855213157247864\n",
      "43 Train Loss 12.993509 Test MSE 1712.1025393548368 Test RE 0.5725561057854796\n",
      "44 Train Loss 12.590155 Test MSE 1704.6148572710044 Test RE 0.5713027300156338\n",
      "45 Train Loss 12.01207 Test MSE 1643.183893073978 Test RE 0.5609139605634904\n",
      "46 Train Loss 11.146177 Test MSE 1476.2826055898697 Test RE 0.5316648099674077\n",
      "47 Train Loss 10.570189 Test MSE 1425.8670483029787 Test RE 0.5225076833005784\n",
      "48 Train Loss 9.768299 Test MSE 1293.0486962426846 Test RE 0.497577352564751\n",
      "49 Train Loss 9.399737 Test MSE 1260.0168436281792 Test RE 0.49118075239942155\n",
      "50 Train Loss 9.1953335 Test MSE 1215.3499951063545 Test RE 0.48239616490845205\n",
      "51 Train Loss 8.829327 Test MSE 1196.7878539460662 Test RE 0.4786981522701317\n",
      "52 Train Loss 8.570545 Test MSE 1164.487609453005 Test RE 0.47219414814236316\n",
      "53 Train Loss 8.390604 Test MSE 1117.7501337305098 Test RE 0.4626211990770876\n",
      "54 Train Loss 8.161209 Test MSE 1050.2207354284312 Test RE 0.44842876073451726\n",
      "55 Train Loss 7.718118 Test MSE 956.088039759466 Test RE 0.4278604143140275\n",
      "56 Train Loss 7.2243123 Test MSE 884.4740835699827 Test RE 0.4115245234326475\n",
      "57 Train Loss 7.0172286 Test MSE 889.4765157678764 Test RE 0.41268663827718527\n",
      "58 Train Loss 6.8835545 Test MSE 865.8345656033902 Test RE 0.40716517401056496\n",
      "59 Train Loss 6.6785245 Test MSE 786.1670797605996 Test RE 0.3879811193839859\n",
      "60 Train Loss 6.549897 Test MSE 740.7604296759441 Test RE 0.37661017745144654\n",
      "61 Train Loss 6.48689 Test MSE 723.4032829811229 Test RE 0.37217174810777004\n",
      "62 Train Loss 6.3736033 Test MSE 646.3697259981778 Test RE 0.3517982447561977\n",
      "63 Train Loss 6.2802033 Test MSE 632.5674716761533 Test RE 0.3480219150428146\n",
      "64 Train Loss 6.128499 Test MSE 627.7657974694398 Test RE 0.3466985217037083\n",
      "65 Train Loss 5.8535376 Test MSE 629.1472720145982 Test RE 0.34707978807274453\n",
      "66 Train Loss 5.6105895 Test MSE 591.8830983763077 Test RE 0.33664419806382906\n",
      "67 Train Loss 5.556552 Test MSE 581.4042678435194 Test RE 0.33365087827420037\n",
      "68 Train Loss 5.508726 Test MSE 579.0804548729724 Test RE 0.33298342657628754\n",
      "69 Train Loss 5.431107 Test MSE 578.778177331586 Test RE 0.3328965072637347\n",
      "70 Train Loss 5.3730135 Test MSE 572.5536459220413 Test RE 0.33110158286134367\n",
      "71 Train Loss 5.2826385 Test MSE 585.9905895589209 Test RE 0.33496427107861754\n",
      "72 Train Loss 5.087203 Test MSE 550.844704625666 Test RE 0.3247639044929161\n",
      "73 Train Loss 4.904552 Test MSE 517.459073308141 Test RE 0.31476842999785865\n",
      "74 Train Loss 4.819334 Test MSE 491.76111047155484 Test RE 0.3068529162686454\n",
      "75 Train Loss 4.7777762 Test MSE 498.2215200797987 Test RE 0.3088619477661889\n",
      "76 Train Loss 4.7338643 Test MSE 509.40153706466856 Test RE 0.31230813020720516\n",
      "77 Train Loss 4.6477156 Test MSE 491.49860439100394 Test RE 0.30677100504470745\n",
      "78 Train Loss 4.592778 Test MSE 502.51551968234384 Test RE 0.3101900795776054\n",
      "79 Train Loss 4.4982443 Test MSE 471.540017480267 Test RE 0.3004778356117533\n",
      "80 Train Loss 4.41317 Test MSE 478.5651621443472 Test RE 0.3027078648841634\n",
      "81 Train Loss 4.334384 Test MSE 465.19154278326573 Test RE 0.29844827289162684\n",
      "82 Train Loss 4.287028 Test MSE 450.7146478941122 Test RE 0.2937676715294877\n",
      "83 Train Loss 4.241717 Test MSE 432.0311873425205 Test RE 0.2876144582272525\n",
      "84 Train Loss 4.121115 Test MSE 426.351734630009 Test RE 0.2857177239308432\n",
      "85 Train Loss 4.051862 Test MSE 424.20652687158065 Test RE 0.2849980167472476\n",
      "86 Train Loss 3.9065812 Test MSE 403.9137965086723 Test RE 0.278097771362978\n",
      "87 Train Loss 3.7620056 Test MSE 396.16195743055295 Test RE 0.27541624261684844\n",
      "88 Train Loss 3.6952982 Test MSE 403.36137977288763 Test RE 0.2779075346994247\n",
      "89 Train Loss 3.6243563 Test MSE 400.85184791056577 Test RE 0.2770416784347949\n",
      "90 Train Loss 3.503757 Test MSE 390.7820742287398 Test RE 0.27353977260506035\n",
      "91 Train Loss 3.416467 Test MSE 379.3169311362571 Test RE 0.26949721331110194\n",
      "92 Train Loss 3.3476365 Test MSE 369.21111473681646 Test RE 0.26588298629530077\n",
      "93 Train Loss 3.253263 Test MSE 350.6284899195507 Test RE 0.2591055796648549\n",
      "94 Train Loss 3.1802332 Test MSE 352.37955938431855 Test RE 0.25975177189195764\n",
      "95 Train Loss 3.1123948 Test MSE 351.04567462773446 Test RE 0.25925967831129165\n",
      "96 Train Loss 3.082356 Test MSE 346.6314499173817 Test RE 0.25762449089680745\n",
      "97 Train Loss 2.9851923 Test MSE 311.7818636373364 Test RE 0.24433100930034637\n",
      "98 Train Loss 2.8381822 Test MSE 304.5238918668073 Test RE 0.24147037159210868\n",
      "99 Train Loss 2.7611792 Test MSE 296.6436671149976 Test RE 0.23832560562283642\n",
      "Training time: 33.30\n",
      "Training time: 33.30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "n_val = 3.0\n",
    "\n",
    "for reps in range(max_reps):  \n",
    "  print(label) \n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []   \n",
    "  alpha_val = []\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "  N_f = 10000 #Total number of collocation points\n",
    "\n",
    "  layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "  PINN = Sequentialmodel(layers,n_val)\n",
    "\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.5, \n",
    "                            max_iter = 10, \n",
    "                            max_eval = 15, \n",
    "                            tolerance_grad = 1e-6, \n",
    "                            tolerance_change = 1e-6, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "  train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "  alpha_full.append(alpha_val)\n",
    "\n",
    "\n",
    "  print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"alpha\": alpha_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "77a1e198-62ae-4129-82a3-1a1f3e433466"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1D_FODE_atanh_'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "d2yA4xTDHldi"
   },
   "outputs": [],
   "source": [
    "#3,4,8,9,13,14,18,19,23,24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Ky6HsA0AWWTD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF7H51LTWXDq",
    "outputId": "1986cfc6-aa7b-43ff-e3e8-c586ef7bafd1"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1D_FODE_atanh_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_atanh_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7eff156fcc4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1D_FODE_atanh_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_atanh_tune0.mat'"
     ]
    }
   ],
   "source": [
    "s = set([3,4,8,9,13,14,18,19,23,24])\n",
    "for tune_reps in range(25):\n",
    "  if tune_reps not in s:\n",
    "    label = \"1D_FODE_atanh_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2eNXAFRRtWs",
    "outputId": "737b4c47-e8bf-4e68-c774-00d25a78ecb3"
   },
   "outputs": [],
   "source": [
    "lrn_tune[5]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "atanh_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
