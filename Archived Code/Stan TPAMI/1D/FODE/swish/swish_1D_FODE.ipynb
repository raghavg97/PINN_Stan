{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "12aa433d-9505-4460-d56f-b6a376e9ccb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvP0Nx4vNOlZ",
    "outputId": "4778640e-2987-401e-f7cb-585a0f18a01a"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDzAYhTsNbP6",
    "outputId": "63653bcd-0776-4d8a-c31d-d2238f724ffe"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/swish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1wXUvTNETmrW",
    "outputId": "d48278bc-c428-4c93-93f7-f711d3b81f62"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "def true_1D_2(x): #True function for 1D_1 dy/dx = cos(0.01*x) BC1: y(0)=0; x \\in [-100,100]\n",
    "    y = 100*np.sin(0.01*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "label = \"1D_FODE_swish_\"\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(-600,600,5000).reshape(-1,1)\n",
    "ysol = true_1D_2(x)\n",
    "\n",
    "bc1_x = np.array(0).reshape(-1,1) \n",
    "bc1_y = np.array(0).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "\n",
    " \n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "\n",
    "y_true = true_1D_2(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "\n",
    "  #Collocation Points\n",
    "  # Latin Hypercube sampling for collocation points \n",
    "  # N_f sets of tuples(x,y)\n",
    "  x01 = np.array([[0.0, 1.0]])\n",
    "  sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "  x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "  x_coll_train = np.vstack((x_coll_train, bc1_x)) # append training points to collocation points \n",
    "\n",
    "  return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "              \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        f = dy_dx - torch.cos(0.01*g)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + 100*loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "       \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "      \n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(seed):\n",
    "    x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_coll_train,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "  print(rep) \n",
    "  torch.manual_seed(rep*11)\n",
    "  start_time = time.time() \n",
    "  thresh_flag = 0\n",
    "  x_coll = torch.from_numpy(colloc_pts(N_f,123)).float().to(device)\n",
    "  f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "  for i in range(max_iter):\n",
    "    \n",
    "    train_step(i)\n",
    "\n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_coll,f_hat).cpu().detach().numpy()\n",
    "    if(thresh_flag == 0):\n",
    "        if(loss_np < loss_thresh):\n",
    "            time_threshold[rep] = time.time() - start_time\n",
    "            epoch_threshold[rep] = i+1            \n",
    "            thresh_flag = 1       \n",
    "    data_update(loss_np)\n",
    "    print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "  elapsed_time[rep] = time.time() - start_time  \n",
    "  print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "2bf8b190-81d6-4793-b63a-bc5565d05793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "1 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "2 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "3 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "4 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "5 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "6 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "7 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "8 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "9 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "10 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "11 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "12 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "13 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "14 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "15 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "16 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "17 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "18 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "19 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "20 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "21 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "22 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "23 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "24 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "25 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "26 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "27 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "28 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "29 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "30 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "31 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "32 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "33 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "34 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "35 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "36 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "37 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "38 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "39 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "40 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "41 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "42 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "43 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "44 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "45 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "46 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "47 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "48 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "49 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "50 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "51 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "52 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "53 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "54 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "55 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "56 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "57 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "58 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "59 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "60 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "61 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "62 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "63 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "64 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "65 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "66 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "67 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "68 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "69 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "70 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "71 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "72 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "73 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "74 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "75 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "76 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "77 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "78 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "79 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "80 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "81 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "82 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "83 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "84 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "85 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "86 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "87 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "88 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "89 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "90 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "91 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "92 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "93 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "94 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "95 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "96 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "97 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "98 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "99 Train Loss 47.7695 Test MSE 5222.675787360136 Test RE 0.9999993045627861\n",
      "Training time: 3.83\n",
      "Training time: 3.83\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "2 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "3 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "4 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "5 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "6 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "7 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "8 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "9 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "10 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "11 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "12 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "13 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "14 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "15 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "16 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "17 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "18 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "19 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "20 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "21 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "22 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "23 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "24 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "25 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "26 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "27 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "28 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "29 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "30 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "31 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "32 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "33 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "34 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "35 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "36 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "37 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "38 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "39 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "40 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "41 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "42 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "43 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "44 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "45 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "46 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "47 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "48 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "49 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "50 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "51 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "52 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "53 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "54 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "55 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "56 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "57 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "58 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "59 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "60 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "61 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "62 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "63 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "64 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "65 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "66 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "67 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "68 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "69 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "70 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "71 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "72 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "73 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "74 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "75 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "76 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "77 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "78 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "79 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "80 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "81 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "82 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "83 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "84 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "85 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "86 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "87 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "88 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "89 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "90 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "91 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "92 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "93 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "94 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "95 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "96 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "97 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "98 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "99 Train Loss 47.7695 Test MSE 5222.677250724914 Test RE 0.9999994446599045\n",
      "Training time: 3.59\n",
      "Training time: 3.59\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "1 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "2 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "3 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "4 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "5 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "7 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "8 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "9 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "10 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "11 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "12 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "13 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "14 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "15 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "16 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "17 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "18 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "19 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "20 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "21 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "22 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "23 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "24 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "25 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "26 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "27 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "28 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "29 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "30 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "31 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "32 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "33 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "34 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "35 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "36 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "37 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "38 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "39 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "40 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "41 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "42 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "43 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "44 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "45 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "46 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "47 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "48 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "49 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "50 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "51 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "52 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "53 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "54 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "55 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "56 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "57 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "58 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "59 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "60 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "61 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "62 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "63 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "64 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "65 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "66 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "67 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "68 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "69 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "70 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "71 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "72 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "73 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "74 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "75 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "76 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "77 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "78 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "79 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "80 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "81 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "82 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "83 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "84 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "85 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "86 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "87 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "88 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "89 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "90 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "91 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "92 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "93 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "94 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "95 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "96 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "97 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "98 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "99 Train Loss 47.769505 Test MSE 5222.686905193035 Test RE 1.0000003689423973\n",
      "Training time: 3.75\n",
      "Training time: 3.75\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "1 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "2 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "3 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "4 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "5 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "7 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "8 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "9 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "10 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "11 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "12 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "13 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "14 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "15 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "16 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "17 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "18 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "19 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "20 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "21 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "22 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "23 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "24 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "25 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "26 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "27 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "28 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "29 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "30 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "31 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "32 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "33 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "34 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "35 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "36 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "37 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "38 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "39 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "40 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "41 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "42 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "43 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "44 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "45 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "46 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "47 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "48 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "49 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "50 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "51 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "52 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "53 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "54 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "55 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "56 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "57 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "58 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "59 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "60 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "61 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "62 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "63 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "64 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "65 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "66 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "67 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "68 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "69 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "70 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "71 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "72 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "73 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "74 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "75 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "76 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "77 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "78 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "79 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "80 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "81 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "82 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "83 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "84 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "85 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "86 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "87 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "88 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "89 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "90 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "91 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "92 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "93 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "94 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "95 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "96 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "97 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "98 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "99 Train Loss 47.7695 Test MSE 5222.676831227249 Test RE 0.9999994044987551\n",
      "Training time: 3.95\n",
      "Training time: 3.95\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "1 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "2 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "3 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "4 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "5 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "6 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "7 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "8 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "9 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "11 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "12 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "13 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "14 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "15 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "16 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "17 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "18 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "19 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "20 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "21 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "22 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "23 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "24 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "25 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "26 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "27 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "28 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "29 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "30 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "31 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "32 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "33 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "34 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "35 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "36 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "37 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "38 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "39 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "40 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "41 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "42 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "43 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "44 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "45 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "46 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "47 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "48 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "49 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "50 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "51 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "52 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "53 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "54 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "55 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "56 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "57 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "58 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "59 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "60 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "61 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "62 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "63 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "64 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "65 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "66 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "67 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "68 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "69 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "70 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "71 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "72 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "73 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "74 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "75 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "76 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "77 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "78 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "79 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "80 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "81 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "82 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "83 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "84 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "85 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "86 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "87 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "88 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "89 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "90 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "91 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "92 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "93 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "94 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "95 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "96 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "97 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "98 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "99 Train Loss 47.769505 Test MSE 5222.691690389984 Test RE 1.000000827058852\n",
      "Training time: 3.89\n",
      "Training time: 3.89\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "1 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "2 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "3 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "4 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "5 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "6 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "7 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "9 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "10 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "11 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "12 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "13 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "14 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "15 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "16 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "17 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "18 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "19 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "20 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "21 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "22 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "23 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "24 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "25 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "26 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "27 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "28 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "29 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "30 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "31 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "32 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "33 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "34 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "35 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "36 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "37 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "38 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "39 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "40 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "41 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "42 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "43 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "44 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "45 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "46 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "47 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "48 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "49 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "50 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "51 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "52 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "53 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "54 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "55 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "56 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "57 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "58 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "59 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "60 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "61 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "62 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "63 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "64 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "65 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "66 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "67 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "68 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "69 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "70 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "71 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "72 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "73 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "74 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "75 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "76 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "77 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "78 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "79 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "80 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "81 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "82 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "83 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "84 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "85 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "86 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "87 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "88 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "89 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "90 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "91 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "92 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "93 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "94 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "95 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "96 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "97 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "98 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "99 Train Loss 47.7695 Test MSE 5222.682019008245 Test RE 0.9999999011575363\n",
      "Training time: 3.82\n",
      "Training time: 3.82\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "1 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "2 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "3 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "4 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "5 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "7 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "8 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "9 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "10 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "11 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "12 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "13 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "14 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "15 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "16 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "17 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "18 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "19 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "20 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "21 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "22 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "23 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "24 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "25 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "26 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "27 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "28 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "29 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "30 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "31 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "32 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "33 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "34 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "35 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "36 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "37 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "38 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "39 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "40 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "41 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "42 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "43 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "44 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "45 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "46 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "47 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "48 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "49 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "50 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "51 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "52 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "53 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "54 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "55 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "56 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "57 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "58 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "59 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "60 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "61 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "62 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "63 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "64 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "65 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "66 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "67 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "68 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "69 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "70 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "71 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "72 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "73 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "74 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "75 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "76 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "77 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "78 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "79 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "80 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "81 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "82 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "83 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "84 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "85 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "86 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "87 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "88 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "89 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "90 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "91 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "92 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "93 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "94 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "95 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "96 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "97 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "98 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "99 Train Loss 47.769512 Test MSE 5222.697564556521 Test RE 1.0000013894288033\n",
      "Training time: 40.66\n",
      "Training time: 40.66\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "1 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "2 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "3 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "5 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "6 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "7 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "8 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "9 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "10 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "11 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "12 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "13 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "14 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "15 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "16 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "17 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "18 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "19 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "20 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "21 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "22 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "23 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "24 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "25 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "26 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "27 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "28 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "29 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "30 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "31 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "32 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "33 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "34 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "35 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "36 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "37 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "38 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "39 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "40 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "41 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "42 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "43 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "44 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "45 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "46 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "47 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "48 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "49 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "50 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "51 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "52 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "53 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "54 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "55 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "56 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "57 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "58 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "59 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "60 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "61 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "62 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "63 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "64 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "65 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "66 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "67 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "68 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "69 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "70 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "71 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "72 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "73 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "74 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "75 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "76 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "77 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "78 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "79 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "80 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "81 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "82 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "83 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "84 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "85 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "86 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "87 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "88 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "89 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "90 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "91 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "92 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "93 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "94 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "95 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "96 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "97 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "98 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "99 Train Loss 47.769497 Test MSE 5222.677957914704 Test RE 0.9999995123636262\n",
      "Training time: 3.94\n",
      "Training time: 3.94\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "1 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "2 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "3 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "5 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "6 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "7 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "8 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "9 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "10 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "11 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "12 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "13 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "14 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "15 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "16 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "17 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "18 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "19 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "20 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "21 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "22 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "23 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "24 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "25 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "26 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "27 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "28 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "29 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "30 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "31 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "32 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "33 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "34 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "35 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "36 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "37 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "38 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "39 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "40 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "41 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "42 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "43 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "44 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "45 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "46 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "47 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "48 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "49 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "50 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "51 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "52 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "53 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "54 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "55 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "56 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "57 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "58 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "59 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "60 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "61 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "62 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "63 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "64 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "65 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "66 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "67 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "68 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "69 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "70 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "71 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "72 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "73 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "74 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "75 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "76 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "77 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "78 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "79 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "80 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "81 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "82 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "83 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "84 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "85 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "86 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "87 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "88 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "89 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "90 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "91 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "92 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "93 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "94 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "95 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "96 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "97 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "98 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "99 Train Loss 47.769497 Test MSE 5222.670463086977 Test RE 0.9999987948364473\n",
      "Training time: 3.85\n",
      "Training time: 3.85\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "1 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "3 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "4 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "5 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "6 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "7 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "8 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "9 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "10 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "11 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "12 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "13 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "14 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "15 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "16 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "17 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "18 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "19 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "20 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "21 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "22 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "23 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "24 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "25 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "26 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "27 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "28 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "29 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "30 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "31 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "32 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "33 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "34 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "35 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "36 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "37 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "38 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "39 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "40 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "41 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "42 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "43 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "44 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "45 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "46 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "47 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "48 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "49 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "50 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "51 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "52 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "53 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "54 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "55 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "56 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "57 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "58 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "59 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "60 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "61 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "62 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "63 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "64 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "65 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "66 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "67 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "68 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "69 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "70 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "71 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "72 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "73 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "74 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "75 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "76 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "77 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "78 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "79 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "80 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "81 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "82 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "83 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "84 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "85 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "86 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "87 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "88 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "89 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "90 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "91 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "92 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "93 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "94 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "95 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "96 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "97 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "98 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "99 Train Loss 47.7695 Test MSE 5222.685989605434 Test RE 1.0000002812875182\n",
      "Training time: 3.85\n",
      "Training time: 3.85\n"
     ]
    }
   ],
   "source": [
    " \n",
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []   \n",
    "  beta_val = []\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "  N_f = 10000 #Total number of collocation points\n",
    "\n",
    "  layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "  PINN = Sequentialmodel(layers)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                            max_iter = 10, \n",
    "                            max_eval = 15, \n",
    "                            tolerance_grad = 1e-6, \n",
    "                            tolerance_change = 1e-6, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "  train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "  beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "  print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "7ef8bb86-89cf-4917-9bf1-6cda3bafbc51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1D_FODE_swish_'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ky6HsA0AWWTD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF7H51LTWXDq",
    "outputId": "bb023909-b810-4d9b-857a-cc29fc84d142"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1D_FODE_swish_tune4.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_swish_tune4.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e4cbf5bf2ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1D_FODE_swish_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_swish_tune4.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(4,5):\n",
    "    label = \"1D_FODE_swish_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(i,' ',np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "swish_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
