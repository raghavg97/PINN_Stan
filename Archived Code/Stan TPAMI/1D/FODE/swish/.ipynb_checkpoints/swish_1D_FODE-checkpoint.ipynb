{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "12aa433d-9505-4460-d56f-b6a376e9ccb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvP0Nx4vNOlZ",
    "outputId": "4778640e-2987-401e-f7cb-585a0f18a01a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDzAYhTsNbP6",
    "outputId": "63653bcd-0776-4d8a-c31d-d2238f724ffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/swish\n"
     ]
    }
   ],
   "source": [
    "%cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/swish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1wXUvTNETmrW",
    "outputId": "d48278bc-c428-4c93-93f7-f711d3b81f62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting smt\n",
      "  Downloading smt-1.2.0.tar.gz (252 kB)\n",
      "\u001b[K     |████████████████████████████████| 252 kB 7.6 MB/s \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting numpydoc\n",
      "  Downloading numpydoc-1.4.0-py3-none-any.whl (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 771 kB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from smt) (21.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from smt) (3.2.2)\n",
      "Collecting pyDOE2\n",
      "  Downloading pyDOE2-1.3.0.tar.gz (19 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from smt) (1.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from smt) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->smt) (1.21.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->smt) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->smt) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->smt) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->smt) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->smt) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->smt) (1.15.0)\n",
      "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.7/dist-packages (from numpydoc->smt) (2.11.3)\n",
      "Collecting sphinx>=3.0\n",
      "  Downloading Sphinx-5.1.1-py3-none-any.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 60.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10->numpydoc->smt) (2.0.1)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.0\n",
      "  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 11.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc->smt) (2.6.1)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc->smt) (2.2.0)\n",
      "Collecting sphinxcontrib-jsmath\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc->smt) (2.10.3)\n",
      "Requirement already satisfied: docutils<0.20,>=0.14 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc->smt) (0.17.1)\n",
      "Collecting sphinxcontrib-devhelp\n",
      "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 2.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc->smt) (0.7.12)\n",
      "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc->smt) (2.23.0)\n",
      "Collecting sphinxcontrib-qthelp\n",
      "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 12.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc->smt) (1.1.5)\n",
      "Collecting sphinxcontrib-applehelp\n",
      "  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 76.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc->smt) (1.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc->smt) (4.12.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel>=1.3->sphinx>=3.0->numpydoc->smt) (2022.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->sphinx>=3.0->numpydoc->smt) (3.8.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx>=3.0->numpydoc->smt) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx>=3.0->numpydoc->smt) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx>=3.0->numpydoc->smt) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx>=3.0->numpydoc->smt) (2022.6.15)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->smt) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->smt) (1.1.0)\n",
      "Building wheels for collected packages: smt, pyDOE2\n",
      "  Building wheel for smt (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for smt: filename=smt-1.2.0-cp37-cp37m-linux_x86_64.whl size=527844 sha256=efe3119c76c9f9ee9349815681ecfa72db69b465eb6dffe7ffa20b12f9081ec6\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/69/1e/ae0014b43757c2db9bdd61fdae3a4f532f9d64c451a1bc678e\n",
      "  Building wheel for pyDOE2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyDOE2: filename=pyDOE2-1.3.0-py3-none-any.whl size=25539 sha256=d05b22e805da296c452a9e707b93155be7b31c74ee6384ec1966d5dd06fd00ad\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/91/2d/d08e80806bf7756193541f6c03c0492af288fcd6158d3d0998\n",
      "Successfully built smt pyDOE2\n",
      "Installing collected packages: sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, sphinx, pyDOE2, numpydoc, smt\n",
      "  Attempting uninstall: sphinx\n",
      "    Found existing installation: Sphinx 1.8.6\n",
      "    Uninstalling Sphinx-1.8.6:\n",
      "      Successfully uninstalled Sphinx-1.8.6\n",
      "Successfully installed numpydoc-1.4.0 pyDOE2-1.3.0 smt-1.2.0 sphinx-5.1.1 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "sphinxcontrib"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmSz5jcVVt4p"
   },
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "def true_1D_2(x): #True function for 1D_1 dy/dx = cos(0.01*x) BC1: y(0)=0; x \\in [-100,100]\n",
    "    y = 100*np.sin(0.01*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(-100,100,5000).reshape(-1,1)\n",
    "ysol = true_1D_2(x)\n",
    "\n",
    "bc1_x = np.array(0).reshape(-1,1) \n",
    "bc1_y = np.array(0).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "\n",
    " \n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "\n",
    "y_true = true_1D_2(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "\n",
    "  #Collocation Points\n",
    "  # Latin Hypercube sampling for collocation points \n",
    "  # N_f sets of tuples(x,y)\n",
    "  x01 = np.array([[0.0, 1.0]])\n",
    "  sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "  x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "  x_coll_train = np.vstack((x_coll_train, bc1_x)) # append training points to collocation points \n",
    "\n",
    "  return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "              \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        f = dy_dx - torch.cos(0.01*g)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + 100*loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "       \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "      \n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(seed):\n",
    "    x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_coll_train,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "  print(rep) \n",
    "  torch.manual_seed(rep*11)\n",
    "  start_time = time.time() \n",
    "  thresh_flag = 0\n",
    "  x_coll = torch.from_numpy(colloc_pts(N_f,123)).float().to(device)\n",
    "  f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "  for i in range(max_iter):\n",
    "    \n",
    "    train_step(i)\n",
    "\n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_coll,f_hat).cpu().detach().numpy()\n",
    "    if(thresh_flag == 0):\n",
    "        if(loss_np < loss_thresh):\n",
    "            time_threshold[rep] = time.time() - start_time\n",
    "            epoch_threshold[rep] = i+1            \n",
    "            thresh_flag = 1       \n",
    "    data_update(loss_np)\n",
    "    print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "  elapsed_time[rep] = time.time() - start_time  \n",
    "  print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "2bf8b190-81d6-4793-b63a-bc5565d05793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 72.73553 Test MSE 2727.640296714531 Test RE 1.0000023732988126\n",
      "1 Train Loss 72.60072 Test MSE 2713.43958897323 Test RE 0.9973958579174454\n",
      "2 Train Loss 71.39455 Test MSE 2562.6708258558137 Test RE 0.9692903614770091\n",
      "3 Train Loss 31.30379 Test MSE 746.0929605291298 Test RE 0.5230027265655118\n",
      "4 Train Loss 8.392497 Test MSE 210.39821848905558 Test RE 0.27773365584617415\n",
      "5 Train Loss 2.732834 Test MSE 37.15351912602167 Test RE 0.11670983623511627\n",
      "6 Train Loss 1.2197943 Test MSE 3.2968536822455556 Test RE 0.034766207535870494\n",
      "7 Train Loss 0.8648533 Test MSE 4.9941134157389575 Test RE 0.04278944783011193\n",
      "8 Train Loss 0.74099 Test MSE 8.79431055165557 Test RE 0.056781698667636576\n",
      "9 Train Loss 0.46792305 Test MSE 3.6717053759920293 Test RE 0.03668946630817514\n",
      "10 Train Loss 0.3599821 Test MSE 0.9378107762971466 Test RE 0.01854236352510393\n",
      "11 Train Loss 0.16270551 Test MSE 0.49266798096261655 Test RE 0.013439547628721382\n",
      "12 Train Loss 0.12255559 Test MSE 0.21584590976165682 Test RE 0.008895684947979359\n",
      "13 Train Loss 0.10179747 Test MSE 0.24274628703233148 Test RE 0.00943373732920775\n",
      "14 Train Loss 0.06430448 Test MSE 0.4051427316498225 Test RE 0.012187412418174653\n",
      "15 Train Loss 0.034326557 Test MSE 0.02827340713940654 Test RE 0.0032195602393057642\n",
      "16 Train Loss 0.029325519 Test MSE 0.10288319505581217 Test RE 0.0061415742005217445\n",
      "17 Train Loss 0.020721631 Test MSE 0.17044914908166375 Test RE 0.007905055083847801\n",
      "18 Train Loss 0.013190552 Test MSE 0.041434768566464365 Test RE 0.003897534251913322\n",
      "19 Train Loss 0.012436671 Test MSE 0.014441784379258008 Test RE 0.0023010068420822477\n",
      "20 Train Loss 0.012169579 Test MSE 0.00954378090037782 Test RE 0.0018705430682679225\n",
      "21 Train Loss 0.011456782 Test MSE 0.007510046176326381 Test RE 0.0016593147946974214\n",
      "22 Train Loss 0.009296738 Test MSE 0.014342365664614613 Test RE 0.002293072981389542\n",
      "23 Train Loss 0.006211666 Test MSE 0.021882061036998617 Test RE 0.002832380490747435\n",
      "24 Train Loss 0.0046240287 Test MSE 0.006411283142195827 Test RE 0.0015331334539625183\n",
      "25 Train Loss 0.0042031445 Test MSE 0.003942957705664674 Test RE 0.001202315763309899\n",
      "26 Train Loss 0.00397519 Test MSE 0.0040668182215670935 Test RE 0.0012210539761485753\n",
      "27 Train Loss 0.0037564128 Test MSE 0.003473570194841909 Test RE 0.001128484300654744\n",
      "28 Train Loss 0.003493337 Test MSE 0.002659488250394665 Test RE 0.0009874304592308266\n",
      "29 Train Loss 0.0032393858 Test MSE 0.0033590471591113343 Test RE 0.0011097254148949435\n",
      "30 Train Loss 0.0031751967 Test MSE 0.0025182481317274916 Test RE 0.0009608525388103266\n",
      "31 Train Loss 0.003070572 Test MSE 0.001897836750788727 Test RE 0.0008341360823323213\n",
      "32 Train Loss 0.0021457088 Test MSE 0.011781202025368302 Test RE 0.0020782715068940866\n",
      "33 Train Loss 0.00095704646 Test MSE 0.0014912279155525321 Test RE 0.0007394000818051056\n",
      "34 Train Loss 0.00081735326 Test MSE 0.0007674628832888405 Test RE 0.0005304398850184384\n",
      "35 Train Loss 0.0007357201 Test MSE 0.0005010714710033152 Test RE 0.00042860508576657226\n",
      "36 Train Loss 0.0006394272 Test MSE 0.00037427849081573405 Test RE 0.0003704289465857357\n",
      "37 Train Loss 0.00045414685 Test MSE 0.0007410834865125346 Test RE 0.0005212439770983376\n",
      "38 Train Loss 0.00035052796 Test MSE 0.0002793391089301311 Test RE 0.00032001722393604464\n",
      "39 Train Loss 0.00030970128 Test MSE 0.0001547933676772553 Test RE 0.00023822298172309324\n",
      "40 Train Loss 0.00026550968 Test MSE 0.00012432263600779797 Test RE 0.0002134924822418193\n",
      "41 Train Loss 0.00023553058 Test MSE 0.00011326178434558372 Test RE 0.00020377419278260088\n",
      "42 Train Loss 0.00021729307 Test MSE 0.00014008251208576028 Test RE 0.00022662062994372603\n",
      "43 Train Loss 0.0002049892 Test MSE 0.00011560691030097699 Test RE 0.00020587299329220666\n",
      "44 Train Loss 0.00019935772 Test MSE 0.00010663143286976098 Test RE 0.00019771977352820883\n",
      "45 Train Loss 0.00019382374 Test MSE 0.00010138128913609683 Test RE 0.00019279083593463965\n",
      "46 Train Loss 0.00018772775 Test MSE 0.00010029295954161813 Test RE 0.00019175323756667783\n",
      "47 Train Loss 0.00018343647 Test MSE 9.821625468476729e-05 Test RE 0.00018975759449366237\n",
      "48 Train Loss 0.00017351694 Test MSE 8.956282274036457e-05 Test RE 0.0001812054969860621\n",
      "49 Train Loss 0.00017264337 Test MSE 8.945993949518657e-05 Test RE 0.00018110138924351052\n",
      "50 Train Loss 0.00016999745 Test MSE 9.395427278871894e-05 Test RE 0.0001855947767179603\n",
      "51 Train Loss 0.000169425 Test MSE 9.364035402543832e-05 Test RE 0.00018528446390137304\n",
      "52 Train Loss 0.00016882988 Test MSE 9.303353871528091e-05 Test RE 0.0001846831409293955\n",
      "53 Train Loss 0.00016414664 Test MSE 9.617353185727914e-05 Test RE 0.00018777391638942096\n",
      "54 Train Loss 0.00016198322 Test MSE 9.673275172624634e-05 Test RE 0.00018831904923038894\n",
      "55 Train Loss 0.00016113468 Test MSE 9.93110635174181e-05 Test RE 0.00019081227000087813\n",
      "56 Train Loss 0.00016062116 Test MSE 9.998015836517944e-05 Test RE 0.00019145397688243048\n",
      "57 Train Loss 0.0001598901 Test MSE 0.00010198541424966472 Test RE 0.00019336439735090103\n",
      "58 Train Loss 0.00015928269 Test MSE 0.00010261781714805016 Test RE 0.00019396298893977588\n",
      "59 Train Loss 0.00015695562 Test MSE 0.00011006992760441446 Test RE 0.00020088236938785198\n",
      "60 Train Loss 0.00015656486 Test MSE 0.00010969969890957127 Test RE 0.0002005442431448522\n",
      "61 Train Loss 0.00015604064 Test MSE 0.00011137785832571068 Test RE 0.0002020723597589528\n",
      "62 Train Loss 0.0001556067 Test MSE 0.00011102639483561791 Test RE 0.00020175327850307227\n",
      "63 Train Loss 0.0001536529 Test MSE 0.00011048971554570802 Test RE 0.00020126507040314165\n",
      "64 Train Loss 0.0001528729 Test MSE 0.00010970371174985905 Test RE 0.00020054791108864328\n",
      "65 Train Loss 0.00015042287 Test MSE 0.00010559001470047857 Test RE 0.0001967518873113315\n",
      "66 Train Loss 0.0001497846 Test MSE 0.00010068669608678369 Test RE 0.00019212926745844374\n",
      "67 Train Loss 0.00014929411 Test MSE 9.86290765517183e-05 Test RE 0.00019015597020957542\n",
      "68 Train Loss 0.00014879131 Test MSE 9.692381271132492e-05 Test RE 0.00018850493598187817\n",
      "69 Train Loss 0.00014828354 Test MSE 9.493578393432773e-05 Test RE 0.00018656168356766525\n",
      "70 Train Loss 0.00014815769 Test MSE 9.73674972546136e-05 Test RE 0.00018893589937135738\n",
      "71 Train Loss 0.00014717483 Test MSE 9.34737968937643e-05 Test RE 0.00018511960877998405\n",
      "72 Train Loss 0.00014666464 Test MSE 8.982950932340514e-05 Test RE 0.00018147507957084623\n",
      "73 Train Loss 0.00014502332 Test MSE 8.098825952388379e-05 Test RE 0.00017231318690612304\n",
      "74 Train Loss 0.00014452948 Test MSE 7.699281281230737e-05 Test RE 0.00016800901083096955\n",
      "75 Train Loss 0.00014398883 Test MSE 7.481068919266295e-05 Test RE 0.00016561104960165197\n",
      "76 Train Loss 0.00014315303 Test MSE 7.067091547648734e-05 Test RE 0.00016096366070426563\n",
      "77 Train Loss 0.00014289921 Test MSE 6.978106434733544e-05 Test RE 0.00015994706543071753\n",
      "78 Train Loss 0.00014241668 Test MSE 6.822162550013755e-05 Test RE 0.0001581497512981272\n",
      "79 Train Loss 0.00014218532 Test MSE 6.98503209262015e-05 Test RE 0.0001600264181855535\n",
      "80 Train Loss 0.00014136158 Test MSE 6.40771368887248e-05 Test RE 0.00015327066119339106\n",
      "81 Train Loss 0.00014091552 Test MSE 6.346781366255527e-05 Test RE 0.00015254017868527946\n",
      "82 Train Loss 0.00014070563 Test MSE 6.30632271141838e-05 Test RE 0.0001520532045017181\n",
      "83 Train Loss 0.00013996485 Test MSE 6.246737539452994e-05 Test RE 0.00015133316371539418\n",
      "84 Train Loss 0.00013980278 Test MSE 6.240441146450972e-05 Test RE 0.00015125687642984014\n",
      "85 Train Loss 0.00013921953 Test MSE 6.255515845214703e-05 Test RE 0.0001514394577926966\n",
      "86 Train Loss 0.00013877085 Test MSE 6.290356524199159e-05 Test RE 0.00015186060030167057\n",
      "87 Train Loss 0.0001385918 Test MSE 6.315320320663178e-05 Test RE 0.00015216163755659473\n",
      "88 Train Loss 0.00013843762 Test MSE 6.339012575585275e-05 Test RE 0.00015244679154049958\n",
      "89 Train Loss 0.00013809683 Test MSE 6.392189685085914e-05 Test RE 0.00015308488376016187\n",
      "90 Train Loss 0.00013795505 Test MSE 6.408937688204078e-05 Test RE 0.00015328529934966986\n",
      "91 Train Loss 0.00013778353 Test MSE 6.439724283800277e-05 Test RE 0.00015365302634294916\n",
      "92 Train Loss 0.00013762526 Test MSE 6.457564139651885e-05 Test RE 0.0001538657102954719\n",
      "93 Train Loss 0.00013735995 Test MSE 6.49397771216606e-05 Test RE 0.00015429891729572257\n",
      "94 Train Loss 0.000137215 Test MSE 6.492854660357424e-05 Test RE 0.0001542855746898219\n",
      "95 Train Loss 0.00013683792 Test MSE 6.535361951730633e-05 Test RE 0.0001547897876546165\n",
      "96 Train Loss 0.00013683792 Test MSE 6.535361951730633e-05 Test RE 0.0001547897876546165\n",
      "97 Train Loss 0.00013683792 Test MSE 6.535361951730633e-05 Test RE 0.0001547897876546165\n",
      "98 Train Loss 0.00013683792 Test MSE 6.535361951730633e-05 Test RE 0.0001547897876546165\n",
      "99 Train Loss 0.00013683792 Test MSE 6.535361951730633e-05 Test RE 0.0001547897876546165\n",
      "Training time: 17.63\n",
      "Training time: 17.63\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 72.73545 Test MSE 2727.637653174712 Test RE 1.0000018887138882\n",
      "1 Train Loss 70.02215 Test MSE 2655.458486329798 Test RE 0.9866820783625563\n",
      "2 Train Loss 67.49544 Test MSE 2547.483201061742 Test RE 0.9664138518776431\n",
      "3 Train Loss 51.18904 Test MSE 1651.047246911636 Test RE 0.7780136783840131\n",
      "4 Train Loss 13.187706 Test MSE 219.7878088498319 Test RE 0.2838633223562513\n",
      "5 Train Loss 3.150274 Test MSE 22.26507475353183 Test RE 0.09034821165090265\n",
      "6 Train Loss 1.7908477 Test MSE 5.59167754407117 Test RE 0.045277093615723796\n",
      "7 Train Loss 0.66835195 Test MSE 1.7972411338354977 Test RE 0.025669100875860076\n",
      "8 Train Loss 0.32623628 Test MSE 0.8345715528024539 Test RE 0.017491992014857433\n",
      "9 Train Loss 0.23536463 Test MSE 0.7449265104013463 Test RE 0.016525864855444338\n",
      "10 Train Loss 0.13917054 Test MSE 0.3763491252573688 Test RE 0.011746350002803788\n",
      "11 Train Loss 0.088861115 Test MSE 0.18247618393252127 Test RE 0.008179195273293208\n",
      "12 Train Loss 0.050873 Test MSE 0.11861595643379429 Test RE 0.006594456978307818\n",
      "13 Train Loss 0.039139375 Test MSE 0.08231218525523468 Test RE 0.005493378805793706\n",
      "14 Train Loss 0.023359615 Test MSE 0.029472972005013417 Test RE 0.0032871494339315192\n",
      "15 Train Loss 0.020462973 Test MSE 0.027335232532053715 Test RE 0.0031656935262348433\n",
      "16 Train Loss 0.014019073 Test MSE 0.057723828688606565 Test RE 0.004600288263799214\n",
      "17 Train Loss 0.009971432 Test MSE 0.009664785276422585 Test RE 0.0018823639055940684\n",
      "18 Train Loss 0.009180286 Test MSE 0.010834740642572891 Test RE 0.00199304333485323\n",
      "19 Train Loss 0.008646385 Test MSE 0.007463431518223823 Test RE 0.0016541571173128043\n",
      "20 Train Loss 0.008079643 Test MSE 0.006698242278777653 Test RE 0.0015670682366000328\n",
      "21 Train Loss 0.00612685 Test MSE 0.012226473429675595 Test RE 0.0021171814781668352\n",
      "22 Train Loss 0.0047816103 Test MSE 0.005207317176254856 Test RE 0.0013817023536913547\n",
      "23 Train Loss 0.004440358 Test MSE 0.003956113160185523 Test RE 0.0012043198222356718\n",
      "24 Train Loss 0.0037328966 Test MSE 0.004621654578645924 Test RE 0.0013016859685645581\n",
      "25 Train Loss 0.0028972793 Test MSE 0.0018374448470209032 Test RE 0.0008207570802206259\n",
      "26 Train Loss 0.0023430598 Test MSE 0.0014660050532999452 Test RE 0.0007331202497149148\n",
      "27 Train Loss 0.002068819 Test MSE 0.001334106959995563 Test RE 0.0006993632489780672\n",
      "28 Train Loss 0.0017839142 Test MSE 0.0010855844655024793 Test RE 0.0006308691517625716\n",
      "29 Train Loss 0.0015408639 Test MSE 0.0009204816816446648 Test RE 0.0005809182987449594\n",
      "30 Train Loss 0.0014156861 Test MSE 0.000982824328895751 Test RE 0.0006002683304574258\n",
      "31 Train Loss 0.0011867767 Test MSE 0.0007934462553527793 Test RE 0.000539344481192752\n",
      "32 Train Loss 0.0010486969 Test MSE 0.0005152852790211863 Test RE 0.0004346416588036545\n",
      "33 Train Loss 0.0010138238 Test MSE 0.00044326938853174284 Test RE 0.00040312650385653155\n",
      "34 Train Loss 0.0009935723 Test MSE 0.0005208083734384651 Test RE 0.00043696480738934\n",
      "35 Train Loss 0.00087279617 Test MSE 0.0005465396162335688 Test RE 0.0004476290934380211\n",
      "36 Train Loss 0.0007720847 Test MSE 0.0003222929766402814 Test RE 0.00034374223985890164\n",
      "37 Train Loss 0.0006838151 Test MSE 0.00025983737228282416 Test RE 0.00030864432038982346\n",
      "38 Train Loss 0.00061026064 Test MSE 0.00020771514662355311 Test RE 0.00027595709535464943\n",
      "39 Train Loss 0.0005873925 Test MSE 0.00017162986649817756 Test RE 0.00025084411356913474\n",
      "40 Train Loss 0.00055730715 Test MSE 0.00016522508909394337 Test RE 0.00024611919114881037\n",
      "41 Train Loss 0.00052658736 Test MSE 0.00018425598188964617 Test RE 0.0002599071816216456\n",
      "42 Train Loss 0.0004926751 Test MSE 0.0001754575398023753 Test RE 0.0002536258406698463\n",
      "43 Train Loss 0.0004692284 Test MSE 0.00014696139554956102 Test RE 0.00023211815798018274\n",
      "44 Train Loss 0.00045754673 Test MSE 0.00013693659273415434 Test RE 0.0002240615007590581\n",
      "45 Train Loss 0.0004356561 Test MSE 0.00019031864719773486 Test RE 0.000264148502410846\n",
      "46 Train Loss 0.0004191238 Test MSE 0.000230417137867429 Test RE 0.00029064634817944766\n",
      "47 Train Loss 0.00041689115 Test MSE 0.0002537494860778335 Test RE 0.0003050071831639607\n",
      "48 Train Loss 0.00038450703 Test MSE 0.000307805426746909 Test RE 0.00033592754645767953\n",
      "49 Train Loss 0.0003364316 Test MSE 0.00028486559751991075 Test RE 0.000323167354961879\n",
      "50 Train Loss 0.0003071153 Test MSE 0.00023827732161696904 Test RE 0.00029556216331501074\n",
      "51 Train Loss 0.00030143943 Test MSE 0.00022008611803632405 Test RE 0.0002840558951923575\n",
      "52 Train Loss 0.0003006947 Test MSE 0.00021897886275317645 Test RE 0.00028334045029519136\n",
      "53 Train Loss 0.00030002007 Test MSE 0.0002177045609303326 Test RE 0.0002825148270777828\n",
      "54 Train Loss 0.00029924532 Test MSE 0.00022147543618960305 Test RE 0.00028495105197172665\n",
      "55 Train Loss 0.00029857206 Test MSE 0.00021982749433664686 Test RE 0.0002838889487677438\n",
      "56 Train Loss 0.0002976245 Test MSE 0.0002347956805712753 Test RE 0.0002933948817539085\n",
      "57 Train Loss 0.00029689062 Test MSE 0.00023398075897360427 Test RE 0.00029288528624968644\n",
      "58 Train Loss 0.00029607848 Test MSE 0.00023001916157332146 Test RE 0.0002903952376343896\n",
      "59 Train Loss 0.00029508478 Test MSE 0.0002275757239885878 Test RE 0.0002888487206255424\n",
      "60 Train Loss 0.00027434266 Test MSE 0.00014046244648072058 Test RE 0.00022692774418844842\n",
      "61 Train Loss 0.0002483563 Test MSE 0.00012782424918882828 Test RE 0.00021647816952763294\n",
      "62 Train Loss 0.00023287938 Test MSE 0.00016785130901430444 Test RE 0.0002480674875146099\n",
      "63 Train Loss 0.00023212914 Test MSE 0.0001686141751238961 Test RE 0.00024863056845221695\n",
      "64 Train Loss 0.00023143645 Test MSE 0.00017043844546808004 Test RE 0.00024997194188669254\n",
      "65 Train Loss 0.00023079733 Test MSE 0.0001707891070792459 Test RE 0.00025022895703409397\n",
      "66 Train Loss 0.00023020775 Test MSE 0.00017259372406959271 Test RE 0.00025154748618750537\n",
      "67 Train Loss 0.00022957483 Test MSE 0.0001708138696283968 Test RE 0.00025024709660508487\n",
      "68 Train Loss 0.00022891306 Test MSE 0.000170685021866886 Test RE 0.00025015269601252134\n",
      "69 Train Loss 0.00022840351 Test MSE 0.00016954662122045102 Test RE 0.00024931709131238787\n",
      "70 Train Loss 0.00022775869 Test MSE 0.00016886752224137228 Test RE 0.0002488172852409604\n",
      "71 Train Loss 0.00022707478 Test MSE 0.0001662258423130175 Test RE 0.0002468634265986126\n",
      "72 Train Loss 0.00022638497 Test MSE 0.0001633737023008099 Test RE 0.0002447363942790865\n",
      "73 Train Loss 0.00022375131 Test MSE 0.0001503619803190281 Test RE 0.00023478832645081648\n",
      "74 Train Loss 0.00022315815 Test MSE 0.00014660669558466774 Test RE 0.0002318378733493694\n",
      "75 Train Loss 0.00022245545 Test MSE 0.00014133285108323902 Test RE 0.00022762976067248022\n",
      "76 Train Loss 0.00022171532 Test MSE 0.000135831971607328 Test RE 0.00022315595675244027\n",
      "77 Train Loss 0.00022095154 Test MSE 0.00012986163668995496 Test RE 0.0002181965692714745\n",
      "78 Train Loss 0.00022021936 Test MSE 0.00012438332563087455 Test RE 0.00021354458537374335\n",
      "79 Train Loss 0.00021770758 Test MSE 0.00010307039258644926 Test RE 0.00019439023594620391\n",
      "80 Train Loss 0.00021614689 Test MSE 9.102013659491576e-05 Test RE 0.0001826737832310643\n",
      "81 Train Loss 0.00021573891 Test MSE 8.876699628134728e-05 Test RE 0.00018039863369963556\n",
      "82 Train Loss 0.00021521936 Test MSE 8.619950122203112e-05 Test RE 0.00017777056724951995\n",
      "83 Train Loss 0.00021337923 Test MSE 7.90753172801716e-05 Test RE 0.00017026600781497306\n",
      "84 Train Loss 0.00021286531 Test MSE 7.843092682943111e-05 Test RE 0.00016957083368689016\n",
      "85 Train Loss 0.00021041521 Test MSE 8.070272714259042e-05 Test RE 0.0001720091648338366\n",
      "86 Train Loss 0.00021007967 Test MSE 8.12105643912267e-05 Test RE 0.0001725495162815035\n",
      "87 Train Loss 0.00020952402 Test MSE 8.192040090811728e-05 Test RE 0.00017330197667659816\n",
      "88 Train Loss 0.0002061162 Test MSE 9.143013961307125e-05 Test RE 0.00018308475079798089\n",
      "89 Train Loss 0.00020243294 Test MSE 0.00010986443351809579 Test RE 0.00020069476399572436\n",
      "90 Train Loss 0.0001958538 Test MSE 0.00012768710924749142 Test RE 0.00021636201093179835\n",
      "91 Train Loss 0.0001949024 Test MSE 0.00012636772538351956 Test RE 0.00021524127993280103\n",
      "92 Train Loss 0.0001907686 Test MSE 0.00010204717094231564 Test RE 0.00019342293385098735\n",
      "93 Train Loss 0.00019052578 Test MSE 0.00010163261826700132 Test RE 0.00019302965692860645\n",
      "94 Train Loss 0.00019031229 Test MSE 0.00010037860264122755 Test RE 0.00019183509195265213\n",
      "95 Train Loss 0.00018993417 Test MSE 0.00010054483185507357 Test RE 0.00019199386784984188\n",
      "96 Train Loss 0.0001895769 Test MSE 0.00010040439050056337 Test RE 0.0001918597321575769\n",
      "97 Train Loss 0.00018921241 Test MSE 0.00010049239351264017 Test RE 0.00019194379489708942\n",
      "98 Train Loss 0.00018834439 Test MSE 9.991489311758169e-05 Test RE 0.00019139147782636632\n",
      "99 Train Loss 0.00018803522 Test MSE 9.98037456560346e-05 Test RE 0.00019128499422018847\n",
      "Training time: 20.09\n",
      "Training time: 20.09\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 72.73497 Test MSE 2727.6203797099433 Test RE 0.9999987223247014\n",
      "1 Train Loss 72.73497 Test MSE 2727.6203199327106 Test RE 0.9999987113669528\n",
      "2 Train Loss 72.73496 Test MSE 2727.620260147561 Test RE 0.9999987004077531\n",
      "3 Train Loss 72.73496 Test MSE 2727.620200361059 Test RE 0.999998689448305\n",
      "4 Train Loss 72.73496 Test MSE 2727.6201405718775 Test RE 0.9999986784883659\n",
      "5 Train Loss 72.73496 Test MSE 2727.620080776074 Test RE 0.9999986675272124\n",
      "6 Train Loss 72.73496 Test MSE 2727.619959155149 Test RE 0.9999986452329117\n",
      "7 Train Loss 72.734955 Test MSE 2727.619899350539 Test RE 0.9999986342701439\n",
      "8 Train Loss 72.734955 Test MSE 2727.6198395418146 Test RE 0.9999986233066217\n",
      "9 Train Loss 72.734955 Test MSE 2727.6197797274467 Test RE 0.9999986123420647\n",
      "10 Train Loss 72.734955 Test MSE 2727.6197199150815 Test RE 0.9999986013778747\n",
      "11 Train Loss 72.73494 Test MSE 2727.619536390676 Test RE 0.9999985677360606\n",
      "12 Train Loss 72.73494 Test MSE 2727.619414691278 Test RE 0.9999985454273729\n",
      "13 Train Loss 72.73494 Test MSE 2727.619354849834 Test RE 0.9999985344578518\n",
      "14 Train Loss 72.73494 Test MSE 2727.619294995198 Test RE 0.9999985234859123\n",
      "15 Train Loss 72.73493 Test MSE 2727.619173254275 Test RE 0.9999985011696119\n",
      "16 Train Loss 72.73493 Test MSE 2727.619113390375 Test RE 0.9999984901959736\n",
      "17 Train Loss 72.73493 Test MSE 2727.619053521254 Test RE 0.9999984792213785\n",
      "18 Train Loss 72.73493 Test MSE 2727.61899364314 Test RE 0.9999984682451348\n",
      "19 Train Loss 72.734924 Test MSE 2727.6188718497247 Test RE 0.9999984459192105\n",
      "20 Train Loss 72.734924 Test MSE 2727.6188119586127 Test RE 0.9999984349405838\n",
      "21 Train Loss 72.734924 Test MSE 2727.618752058138 Test RE 0.9999984239602405\n",
      "22 Train Loss 72.734924 Test MSE 2727.6186921542294 Test RE 0.9999984129792678\n",
      "23 Train Loss 72.734924 Test MSE 2727.6185702869498 Test RE 0.9999983906398022\n",
      "24 Train Loss 72.73492 Test MSE 2727.618510360422 Test RE 0.9999983796546829\n",
      "25 Train Loss 72.73492 Test MSE 2727.6184504254506 Test RE 0.9999983686680156\n",
      "26 Train Loss 72.73492 Test MSE 2727.6183904832264 Test RE 0.9999983576800182\n",
      "27 Train Loss 72.73492 Test MSE 2727.6183305352283 Test RE 0.9999983466909631\n",
      "28 Train Loss 72.73492 Test MSE 2727.6182705789342 Test RE 0.9999983357003864\n",
      "29 Train Loss 72.73492 Test MSE 2727.6182106140204 Test RE 0.9999983247082299\n",
      "30 Train Loss 72.7349 Test MSE 2727.618150641037 Test RE 0.999998313714594\n",
      "31 Train Loss 72.7349 Test MSE 2727.618090661306 Test RE 0.9999983027197209\n",
      "32 Train Loss 72.7349 Test MSE 2727.6180306705182 Test RE 0.999998291722821\n",
      "33 Train Loss 72.7349 Test MSE 2727.617970671438 Test RE 0.9999982807244009\n",
      "34 Train Loss 72.7349 Test MSE 2727.6179106667064 Test RE 0.9999982697249448\n",
      "35 Train Loss 72.7349 Test MSE 2727.6178506519823 Test RE 0.9999982587236568\n",
      "36 Train Loss 72.73489 Test MSE 2727.6177285466165 Test RE 0.9999982363405442\n",
      "37 Train Loss 72.73489 Test MSE 2727.6176685067326 Test RE 0.9999982253346437\n",
      "38 Train Loss 72.73489 Test MSE 2727.6175463477202 Test RE 0.9999982029416963\n",
      "39 Train Loss 72.734886 Test MSE 2727.6174862766525 Test RE 0.9999981919300793\n",
      "40 Train Loss 72.734886 Test MSE 2727.6174261965325 Test RE 0.9999981809168027\n",
      "41 Train Loss 72.734886 Test MSE 2727.617366101326 Test RE 0.9999981699007605\n",
      "42 Train Loss 72.734886 Test MSE 2727.6173060029946 Test RE 0.9999981588841454\n",
      "43 Train Loss 72.734886 Test MSE 2727.6172458879305 Test RE 0.9999981478644628\n",
      "44 Train Loss 72.73487 Test MSE 2727.617061383436 Test RE 0.9999981140429732\n",
      "45 Train Loss 72.73487 Test MSE 2727.617001226076 Test RE 0.9999981030155369\n",
      "46 Train Loss 72.73487 Test MSE 2727.616941058426 Test RE 0.9999980919862145\n",
      "47 Train Loss 72.73487 Test MSE 2727.616880880607 Test RE 0.9999980809550274\n",
      "48 Train Loss 72.73487 Test MSE 2727.6168206908137 Test RE 0.9999980699216454\n",
      "49 Train Loss 72.73486 Test MSE 2727.6166982161767 Test RE 0.9999980474708375\n",
      "50 Train Loss 72.73486 Test MSE 2727.6166379895612 Test RE 0.9999980364307054\n",
      "51 Train Loss 72.73486 Test MSE 2727.616577747786 Test RE 0.9999980253877943\n",
      "52 Train Loss 72.73486 Test MSE 2727.616517501087 Test RE 0.9999980143439801\n",
      "53 Train Loss 72.734856 Test MSE 2727.6163948954127 Test RE 0.9999979918691506\n",
      "54 Train Loss 72.734856 Test MSE 2727.6163346069625 Test RE 0.9999979808176828\n",
      "55 Train Loss 72.734856 Test MSE 2727.6162743052005 Test RE 0.9999979697637746\n",
      "56 Train Loss 72.734856 Test MSE 2727.616213990294 Test RE 0.9999979587074568\n",
      "57 Train Loss 72.734856 Test MSE 2727.61615366079 Test RE 0.9999979476484632\n",
      "58 Train Loss 72.73485 Test MSE 2727.6160933195883 Test RE 0.999997936587325\n",
      "59 Train Loss 72.73485 Test MSE 2727.6160329623076 Test RE 0.9999979255232393\n",
      "60 Train Loss 72.73485 Test MSE 2727.6159725921048 Test RE 0.9999979144567848\n",
      "61 Train Loss 72.73485 Test MSE 2727.615912208289 Test RE 0.9999979033878346\n",
      "62 Train Loss 72.73485 Test MSE 2727.6158518092698 Test RE 0.9999978923160976\n",
      "63 Train Loss 72.73483 Test MSE 2727.6157288860254 Test RE 0.9999978697830514\n",
      "64 Train Loss 72.73483 Test MSE 2727.6156684437497 Test RE 0.9999978587033845\n",
      "65 Train Loss 72.73483 Test MSE 2727.615607981343 Test RE 0.9999978476200274\n",
      "66 Train Loss 72.73483 Test MSE 2727.61554750283 Test RE 0.9999978365337177\n",
      "67 Train Loss 72.73483 Test MSE 2727.615487013751 Test RE 0.9999978254454711\n",
      "68 Train Loss 72.73483 Test MSE 2727.6154265077503 Test RE 0.999997814354122\n",
      "69 Train Loss 72.734825 Test MSE 2727.6153659882248 Test RE 0.9999978032602939\n",
      "70 Train Loss 72.734825 Test MSE 2727.6152428077476 Test RE 0.9999977806800927\n",
      "71 Train Loss 72.734825 Test MSE 2727.6151822379657 Test RE 0.9999977695770516\n",
      "72 Train Loss 72.734825 Test MSE 2727.615121652857 Test RE 0.999997758471201\n",
      "73 Train Loss 72.73482 Test MSE 2727.6150610505706 Test RE 0.9999977473622013\n",
      "74 Train Loss 72.73482 Test MSE 2727.615000432974 Test RE 0.9999977362503949\n",
      "75 Train Loss 72.73482 Test MSE 2727.614939799253 Test RE 0.9999977251356328\n",
      "76 Train Loss 72.73482 Test MSE 2727.614879147335 Test RE 0.9999977140175347\n",
      "77 Train Loss 72.73482 Test MSE 2727.6147556829133 Test RE 0.9999976913852816\n",
      "78 Train Loss 72.73481 Test MSE 2727.61463215402 Test RE 0.9999976687412095\n",
      "79 Train Loss 72.73481 Test MSE 2727.6145714145914 Test RE 0.9999976576070695\n",
      "80 Train Loss 72.73481 Test MSE 2727.6145106547788 Test RE 0.9999976464691929\n",
      "81 Train Loss 72.73481 Test MSE 2727.614449875753 Test RE 0.9999976353277938\n",
      "82 Train Loss 72.734795 Test MSE 2727.614327647654 Test RE 0.9999976129221689\n",
      "83 Train Loss 72.734795 Test MSE 2727.6142668148495 Test RE 0.9999976017709115\n",
      "84 Train Loss 72.734795 Test MSE 2727.614205962491 Test RE 0.9999975906160693\n",
      "85 Train Loss 72.734795 Test MSE 2727.6141450902687 Test RE 0.9999975794575859\n",
      "86 Train Loss 72.734795 Test MSE 2727.6140841985302 Test RE 0.9999975682955247\n",
      "87 Train Loss 72.734795 Test MSE 2727.6140232875787 Test RE 0.9999975571299418\n",
      "88 Train Loss 72.73479 Test MSE 2727.613962357126 Test RE 0.9999975459607836\n",
      "89 Train Loss 72.73479 Test MSE 2727.6139014038927 Test RE 0.9999975347874496\n",
      "90 Train Loss 72.73479 Test MSE 2727.613840433487 Test RE 0.9999975236109676\n",
      "91 Train Loss 72.73479 Test MSE 2727.6137163006233 Test RE 0.9999975008561779\n",
      "92 Train Loss 72.73478 Test MSE 2727.613655266441 Test RE 0.9999974896680046\n",
      "93 Train Loss 72.73478 Test MSE 2727.613594214961 Test RE 0.9999974784766603\n",
      "94 Train Loss 72.73478 Test MSE 2727.6135331403034 Test RE 0.9999974672810672\n",
      "95 Train Loss 72.73478 Test MSE 2727.613472043758 Test RE 0.9999974560814615\n",
      "96 Train Loss 72.73478 Test MSE 2727.613410927903 Test RE 0.9999974448783163\n",
      "97 Train Loss 72.734764 Test MSE 2727.6132864864358 Test RE 0.9999974220669551\n",
      "98 Train Loss 72.734764 Test MSE 2727.6131619525972 Test RE 0.9999973992386605\n",
      "99 Train Loss 72.734764 Test MSE 2727.6131007246668 Test RE 0.9999973880149698\n",
      "Training time: 9.56\n",
      "Training time: 9.56\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 72.73547 Test MSE 2727.638307525903 Test RE 1.0000020086624428\n",
      "1 Train Loss 71.06295 Test MSE 2623.969656453827 Test RE 0.9808145189445578\n",
      "2 Train Loss 26.41084 Test MSE 824.1934393865706 Test RE 0.5496953439211186\n",
      "3 Train Loss 17.889109 Test MSE 439.9329015659822 Test RE 0.40160647232154845\n",
      "4 Train Loss 12.316589 Test MSE 219.72254011418292 Test RE 0.2838211708467242\n",
      "5 Train Loss 1.6481171 Test MSE 7.88326743701468 Test RE 0.05376016733346974\n",
      "6 Train Loss 1.2011012 Test MSE 4.966505131182869 Test RE 0.042671010347131014\n",
      "7 Train Loss 1.1433893 Test MSE 5.874830231506647 Test RE 0.04640931329189163\n",
      "8 Train Loss 0.8596691 Test MSE 8.61928897690349 Test RE 0.05621383343622598\n",
      "9 Train Loss 0.6522488 Test MSE 6.791573596111879 Test RE 0.04989909710608544\n",
      "10 Train Loss 0.41005415 Test MSE 2.3663239291770224 Test RE 0.029454020145896893\n",
      "11 Train Loss 0.2343129 Test MSE 1.2315141091264874 Test RE 0.021248446203307056\n",
      "12 Train Loss 0.16715084 Test MSE 0.5661727964484977 Test RE 0.01440727948322489\n",
      "13 Train Loss 0.087381 Test MSE 0.11650089999510892 Test RE 0.006535399220479947\n",
      "14 Train Loss 0.06341464 Test MSE 0.1684392879887427 Test RE 0.007858310417831348\n",
      "15 Train Loss 0.053252134 Test MSE 0.357009296165734 Test RE 0.011440558975286652\n",
      "16 Train Loss 0.036719386 Test MSE 0.23873550868496776 Test RE 0.009355478206175627\n",
      "17 Train Loss 0.0286499 Test MSE 0.09631908353816546 Test RE 0.005942424226467773\n",
      "18 Train Loss 0.023435084 Test MSE 0.04677185339871518 Test RE 0.004140947950260658\n",
      "19 Train Loss 0.0115205655 Test MSE 0.008450061381454111 Test RE 0.0017601002864845873\n",
      "20 Train Loss 0.007968197 Test MSE 0.011792228149904934 Test RE 0.0020792438152062594\n",
      "21 Train Loss 0.0068985745 Test MSE 0.006881229680096268 Test RE 0.001588329158419043\n",
      "22 Train Loss 0.0060062795 Test MSE 0.0047803341331867965 Test RE 0.0013238433818637655\n",
      "23 Train Loss 0.0036345155 Test MSE 0.004387292432430207 Test RE 0.0012682526349464293\n",
      "24 Train Loss 0.0032894218 Test MSE 0.002607234472674827 Test RE 0.0009776817909218386\n",
      "25 Train Loss 0.001603436 Test MSE 0.004443724189581245 Test RE 0.0012763830538631215\n",
      "26 Train Loss 0.0008478245 Test MSE 0.0005729345020193052 Test RE 0.0004583106717220432\n",
      "27 Train Loss 0.0008061492 Test MSE 0.0004901219487313113 Test RE 0.00042389623339284893\n",
      "28 Train Loss 0.0008054132 Test MSE 0.0004926622755640128 Test RE 0.00042499335142701176\n",
      "29 Train Loss 0.00073626713 Test MSE 0.0004610089797753988 Test RE 0.0004111139127298056\n",
      "30 Train Loss 0.00062190933 Test MSE 0.0005264704090455961 Test RE 0.00043933364622518787\n",
      "31 Train Loss 0.0005580499 Test MSE 0.00087763667742969 Test RE 0.0005672374083173121\n",
      "32 Train Loss 0.00049701834 Test MSE 0.0013808012556725566 Test RE 0.0007114969921326922\n",
      "33 Train Loss 0.00034692313 Test MSE 0.0007320771298997541 Test RE 0.0005180669669489726\n",
      "34 Train Loss 0.00028175267 Test MSE 0.0002465323342401628 Test RE 0.00030063838215044713\n",
      "35 Train Loss 0.00026562274 Test MSE 0.0001477018522589854 Test RE 0.00023270218039242278\n",
      "36 Train Loss 0.00026489337 Test MSE 0.0001444538901374678 Test RE 0.00023012939870398945\n",
      "37 Train Loss 0.00026438618 Test MSE 0.0001412374044321311 Test RE 0.0002275528848167494\n",
      "38 Train Loss 0.0002637519 Test MSE 0.00014487143193082612 Test RE 0.0002304617514922235\n",
      "39 Train Loss 0.0002617456 Test MSE 0.00013531360185303697 Test RE 0.00022272973944963714\n",
      "40 Train Loss 0.00026029974 Test MSE 0.00014021806461728505 Test RE 0.0002267302495237079\n",
      "41 Train Loss 0.00024395571 Test MSE 0.00012658692676436158 Test RE 0.0002154278811510761\n",
      "42 Train Loss 0.0002009621 Test MSE 9.671976099499054e-05 Test RE 0.00018830640364626312\n",
      "43 Train Loss 0.00018404757 Test MSE 9.810929349926141e-05 Test RE 0.0001896542397775405\n",
      "44 Train Loss 0.00018180122 Test MSE 0.00010144575211397065 Test RE 0.00019285211892171243\n",
      "45 Train Loss 0.0001809416 Test MSE 0.00010194376015426281 Test RE 0.00019332490522498162\n",
      "46 Train Loss 0.00017635371 Test MSE 0.00010576146883777467 Test RE 0.0001969115626473457\n",
      "47 Train Loss 0.0001753621 Test MSE 0.00010616437635292369 Test RE 0.00019728628196821293\n",
      "48 Train Loss 0.00017097524 Test MSE 0.00010120415043110105 Test RE 0.00019262233516969575\n",
      "49 Train Loss 0.00016847964 Test MSE 9.696968313815344e-05 Test RE 0.00018854953688448762\n",
      "50 Train Loss 0.00016709708 Test MSE 9.481090280954599e-05 Test RE 0.00018643893902035164\n",
      "51 Train Loss 0.00016687997 Test MSE 9.378740938625332e-05 Test RE 0.0001854298947093876\n",
      "52 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "53 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "54 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "55 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "56 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "57 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "58 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "59 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "60 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "61 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "62 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "63 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "64 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "65 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "66 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "67 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "68 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "69 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "70 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "71 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "72 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "73 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "74 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "75 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "76 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "77 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "78 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "79 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "80 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "81 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "82 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "83 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "84 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "85 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "86 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "87 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "88 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "89 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "90 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "91 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "92 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "93 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "94 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "95 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "96 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "97 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "98 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "99 Train Loss 0.00016674252 Test MSE 9.379421736834574e-05 Test RE 0.00018543662471985685\n",
      "Training time: 13.77\n",
      "Training time: 13.77\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 72.734726 Test MSE 2727.611731755056 Test RE 0.9999971370691512\n",
      "1 Train Loss 72.73472 Test MSE 2727.611431224594 Test RE 0.9999970819789037\n",
      "2 Train Loss 72.73471 Test MSE 2727.611143745991 Test RE 0.9999970292811899\n",
      "3 Train Loss 72.73471 Test MSE 2727.6110029531615 Test RE 0.9999970034724504\n",
      "4 Train Loss 72.7347 Test MSE 2727.6107013581186 Test RE 0.9999969481870474\n",
      "5 Train Loss 72.73469 Test MSE 2727.6103993020315 Test RE 0.9999968928171272\n",
      "6 Train Loss 72.73468 Test MSE 2727.610096779177 Test RE 0.9999968373616407\n",
      "7 Train Loss 72.73468 Test MSE 2727.6099553131585 Test RE 0.9999968114294935\n",
      "8 Train Loss 72.73467 Test MSE 2727.6096520806195 Test RE 0.9999967558439102\n",
      "9 Train Loss 72.73466 Test MSE 2727.6091862712688 Test RE 0.9999966704563515\n",
      "10 Train Loss 72.73465 Test MSE 2727.6088817862706 Test RE 0.9999966146411717\n",
      "11 Train Loss 72.73464 Test MSE 2727.6085767973086 Test RE 0.9999965587336074\n",
      "12 Train Loss 72.73463 Test MSE 2727.608434287955 Test RE 0.9999965326101993\n",
      "13 Train Loss 72.73463 Test MSE 2727.6081285261857 Test RE 0.999996476560967\n",
      "14 Train Loss 72.73462 Test MSE 2727.6078222345964 Test RE 0.99999642041461\n",
      "15 Train Loss 72.73461 Test MSE 2727.6075153908487 Test RE 0.9999963641670339\n",
      "16 Train Loss 72.734604 Test MSE 2727.607208006122 Test RE 0.9999963078202874\n",
      "17 Train Loss 72.73459 Test MSE 2727.6067354422316 Test RE 0.9999962211945143\n",
      "18 Train Loss 72.73458 Test MSE 2727.606426615546 Test RE 0.9999961645834342\n",
      "19 Train Loss 72.73457 Test MSE 2727.6061172094696 Test RE 0.9999961078661425\n",
      "20 Train Loss 72.734566 Test MSE 2727.605807211594 Test RE 0.9999960510403646\n",
      "21 Train Loss 72.73455 Test MSE 2727.605309003601 Test RE 0.9999959597137429\n",
      "22 Train Loss 72.73454 Test MSE 2727.6051640072824 Test RE 0.9999959331344328\n",
      "23 Train Loss 72.734535 Test MSE 2727.6048521226294 Test RE 0.9999958759627796\n",
      "24 Train Loss 72.734535 Test MSE 2727.6047067437926 Test RE 0.9999958493133476\n",
      "25 Train Loss 72.73452 Test MSE 2727.60440889793 Test RE 0.9999957947151441\n",
      "26 Train Loss 72.73451 Test MSE 2727.604263146623 Test RE 0.9999957679974324\n",
      "27 Train Loss 72.73451 Test MSE 2727.603949414034 Test RE 0.9999957104870242\n",
      "28 Train Loss 72.734505 Test MSE 2727.6038032664424 Test RE 0.999995683696667\n",
      "29 Train Loss 72.734505 Test MSE 2727.6036569918006 Test RE 0.9999956568830198\n",
      "30 Train Loss 72.73449 Test MSE 2727.6033419697033 Test RE 0.9999955991362247\n",
      "31 Train Loss 72.73448 Test MSE 2727.6031952906696 Test RE 0.9999955722484462\n",
      "32 Train Loss 72.73448 Test MSE 2727.6028792672164 Test RE 0.9999955143180875\n",
      "33 Train Loss 72.734474 Test MSE 2727.6025625506995 Test RE 0.9999954562606798\n",
      "34 Train Loss 72.73447 Test MSE 2727.602245117084 Test RE 0.9999953980718169\n",
      "35 Train Loss 72.73446 Test MSE 2727.6020974528356 Test RE 0.9999953710034327\n",
      "36 Train Loss 72.73446 Test MSE 2727.601949650099 Test RE 0.9999953439096612\n",
      "37 Train Loss 72.73444 Test MSE 2727.601630827398 Test RE 0.9999952854661585\n",
      "38 Train Loss 72.734436 Test MSE 2727.601311267213 Test RE 0.9999952268874635\n",
      "39 Train Loss 72.73443 Test MSE 2727.6009909633603 Test RE 0.9999951681724429\n",
      "40 Train Loss 72.73443 Test MSE 2727.600842112493 Test RE 0.9999951408865327\n",
      "41 Train Loss 72.73441 Test MSE 2727.6005368578894 Test RE 0.9999950849301915\n",
      "42 Train Loss 72.734406 Test MSE 2727.6003875727356 Test RE 0.9999950575646699\n",
      "43 Train Loss 72.7344 Test MSE 2727.6000650495866 Test RE 0.9999949984428195\n",
      "44 Train Loss 72.73438 Test MSE 2727.5995441265018 Test RE 0.9999949029521883\n",
      "45 Train Loss 72.73438 Test MSE 2727.599393858228 Test RE 0.9999948754064456\n",
      "46 Train Loss 72.734375 Test MSE 2727.5990688661495 Test RE 0.9999948158320038\n",
      "47 Train Loss 72.73437 Test MSE 2727.5989181118907 Test RE 0.9999947881971728\n",
      "48 Train Loss 72.73437 Test MSE 2727.5987672011033 Test RE 0.9999947605336476\n",
      "49 Train Loss 72.73436 Test MSE 2727.5984406150333 Test RE 0.9999947006670036\n",
      "50 Train Loss 72.73435 Test MSE 2727.598113180487 Test RE 0.9999946406448212\n",
      "51 Train Loss 72.73433 Test MSE 2727.597583266913 Test RE 0.9999945435061047\n",
      "52 Train Loss 72.73433 Test MSE 2727.5974309529865 Test RE 0.9999945155853626\n",
      "53 Train Loss 72.73432 Test MSE 2727.597100837256 Test RE 0.9999944550716802\n",
      "54 Train Loss 72.73432 Test MSE 2727.596948002425 Test RE 0.9999944270554482\n",
      "55 Train Loss 72.73431 Test MSE 2727.596634191031 Test RE 0.9999943695305169\n",
      "56 Train Loss 72.7343 Test MSE 2727.596301883103 Test RE 0.9999943086149728\n",
      "57 Train Loss 72.73429 Test MSE 2727.595968661268 Test RE 0.9999942475318959\n",
      "58 Train Loss 72.734276 Test MSE 2727.595652540412 Test RE 0.9999941895836053\n",
      "59 Train Loss 72.734276 Test MSE 2727.5954980954016 Test RE 0.9999941612722033\n",
      "60 Train Loss 72.73427 Test MSE 2727.5951625871376 Test RE 0.9999940997699909\n",
      "61 Train Loss 72.734245 Test MSE 2727.5946178210907 Test RE 0.999993999908606\n",
      "62 Train Loss 72.734245 Test MSE 2727.5944621879758 Test RE 0.9999939713794064\n",
      "63 Train Loss 72.734245 Test MSE 2727.5943063777245 Test RE 0.9999939428177348\n",
      "64 Train Loss 72.734245 Test MSE 2727.5941503805384 Test RE 0.9999939142217954\n",
      "65 Train Loss 72.73423 Test MSE 2727.593829778199 Test RE 0.9999938554519822\n",
      "66 Train Loss 72.73422 Test MSE 2727.5934893408075 Test RE 0.9999937930461894\n",
      "67 Train Loss 72.734215 Test MSE 2727.593332372191 Test RE 0.9999937642721721\n",
      "68 Train Loss 72.734215 Test MSE 2727.593175217269 Test RE 0.9999937354640022\n",
      "69 Train Loss 72.7342 Test MSE 2727.592852120468 Test RE 0.9999936762369169\n",
      "70 Train Loss 72.73419 Test MSE 2727.592508675443 Test RE 0.9999936132797818\n",
      "71 Train Loss 72.734184 Test MSE 2727.5921641483083 Test RE 0.9999935501242804\n",
      "72 Train Loss 72.73416 Test MSE 2727.591602665603 Test RE 0.9999934471984984\n",
      "73 Train Loss 72.73416 Test MSE 2727.59144337571 Test RE 0.9999934179989562\n",
      "74 Train Loss 72.73415 Test MSE 2727.591095437771 Test RE 0.9999933542182057\n",
      "75 Train Loss 72.73413 Test MSE 2727.5905568426265 Test RE 0.9999932554879392\n",
      "76 Train Loss 72.73413 Test MSE 2727.590396220266 Test RE 0.9999932260441358\n",
      "77 Train Loss 72.73412 Test MSE 2727.5902353936985 Test RE 0.9999931965628983\n",
      "78 Train Loss 72.73411 Test MSE 2727.5896622639198 Test RE 0.9999930915020461\n",
      "79 Train Loss 72.73411 Test MSE 2727.589500477951 Test RE 0.999993061844936\n",
      "80 Train Loss 72.73411 Test MSE 2727.589338475333 Test RE 0.9999930321481111\n",
      "81 Train Loss 72.73409 Test MSE 2727.5890049541595 Test RE 0.9999929710100841\n",
      "82 Train Loss 72.734085 Test MSE 2727.5886488013102 Test RE 0.9999929057234233\n",
      "83 Train Loss 72.73408 Test MSE 2727.58829139819 Test RE 0.99999284020757\n",
      "84 Train Loss 72.734055 Test MSE 2727.5877595143284 Test RE 0.9999927427075034\n",
      "85 Train Loss 72.734055 Test MSE 2727.587595167089 Test RE 0.9999927125808744\n",
      "86 Train Loss 72.734055 Test MSE 2727.587430591989 Test RE 0.9999926824124751\n",
      "87 Train Loss 72.73404 Test MSE 2727.587068833512 Test RE 0.9999926160982228\n",
      "88 Train Loss 72.73403 Test MSE 2727.586903532944 Test RE 0.9999925857968345\n",
      "89 Train Loss 72.734024 Test MSE 2727.586539842439 Test RE 0.9999925191284139\n",
      "90 Train Loss 72.73402 Test MSE 2727.586373797155 Test RE 0.9999924886905082\n",
      "91 Train Loss 72.73402 Test MSE 2727.5862075153523 Test RE 0.9999924582092449\n",
      "92 Train Loss 72.734 Test MSE 2727.585864753287 Test RE 0.9999923953772273\n",
      "93 Train Loss 72.734 Test MSE 2727.5856977496946 Test RE 0.9999923647636492\n",
      "94 Train Loss 72.734 Test MSE 2727.5855305085165 Test RE 0.9999923341065183\n",
      "95 Train Loss 72.73398 Test MSE 2727.5849597898923 Test RE 0.9999922294875662\n",
      "96 Train Loss 72.73398 Test MSE 2727.5847914819897 Test RE 0.9999921986348889\n",
      "97 Train Loss 72.73397 Test MSE 2727.5844197611495 Test RE 0.9999921304943944\n",
      "98 Train Loss 72.733955 Test MSE 2727.584071429455 Test RE 0.9999920666413818\n",
      "99 Train Loss 72.73395 Test MSE 2727.5836968712406 Test RE 0.9999919979807568\n",
      "Training time: 14.32\n",
      "Training time: 14.32\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 72.73521 Test MSE 2727.6288080111494 Test RE 1.0000002673131445\n",
      "1 Train Loss 72.73518 Test MSE 2727.628085272034 Test RE 1.0000001348282268\n",
      "2 Train Loss 72.73517 Test MSE 2727.62766603151 Test RE 1.0000000579774642\n",
      "3 Train Loss 72.73517 Test MSE 2727.6274563213906 Test RE 1.000000019535609\n",
      "4 Train Loss 72.735146 Test MSE 2727.6268815268563 Test RE 0.999999914170308\n",
      "5 Train Loss 72.73514 Test MSE 2727.6263058331365 Test RE 0.9999998086401665\n",
      "6 Train Loss 72.73512 Test MSE 2727.625729133332 Test RE 0.999999702925589\n",
      "7 Train Loss 72.7351 Test MSE 2727.625001944904 Test RE 0.9999995696249951\n",
      "8 Train Loss 72.73508 Test MSE 2727.6244226961117 Test RE 0.9999994634431384\n",
      "9 Train Loss 72.73508 Test MSE 2727.624211871123 Test RE 0.9999994247968941\n",
      "10 Train Loss 72.73506 Test MSE 2727.6236308965363 Test RE 0.9999993182986673\n",
      "11 Train Loss 72.73505 Test MSE 2727.6231907959973 Test RE 0.9999992376240001\n",
      "12 Train Loss 72.73503 Test MSE 2727.6226877729573 Test RE 0.9999991454150249\n",
      "13 Train Loss 72.73503 Test MSE 2727.622476181439 Test RE 0.999999106628256\n",
      "14 Train Loss 72.73501 Test MSE 2727.6217421061347 Test RE 0.9999989720651512\n",
      "15 Train Loss 72.73499 Test MSE 2727.621154793565 Test RE 0.9999988644050622\n",
      "16 Train Loss 72.73497 Test MSE 2727.6204173105107 Test RE 0.9999987292172514\n",
      "17 Train Loss 72.734955 Test MSE 2727.6198260184196 Test RE 0.9999986208276515\n",
      "18 Train Loss 72.73494 Test MSE 2727.619316356229 Test RE 0.999998527401598\n",
      "19 Train Loss 72.734924 Test MSE 2727.6187214377396 Test RE 0.9999984183472218\n",
      "20 Train Loss 72.7349 Test MSE 2727.6181244351233 Test RE 0.9999983089107929\n",
      "21 Train Loss 72.7349 Test MSE 2727.6179102413157 Test RE 0.9999982696469665\n",
      "22 Train Loss 72.734886 Test MSE 2727.617310254807 Test RE 0.9999981596635441\n",
      "23 Train Loss 72.73487 Test MSE 2727.6168621370666 Test RE 0.9999980775191517\n",
      "24 Train Loss 72.734856 Test MSE 2727.616258027233 Test RE 0.9999979667798624\n",
      "25 Train Loss 72.73483 Test MSE 2727.615651424971 Test RE 0.9999978555836742\n",
      "26 Train Loss 72.734825 Test MSE 2727.615130993215 Test RE 0.9999977601833812\n",
      "27 Train Loss 72.73481 Test MSE 2727.6145194514033 Test RE 0.9999976480817013\n",
      "28 Train Loss 72.734795 Test MSE 2727.613995579126 Test RE 0.9999975520507066\n",
      "29 Train Loss 72.73478 Test MSE 2727.6133787110284 Test RE 0.9999974389726421\n",
      "30 Train Loss 72.73476 Test MSE 2727.6129238775907 Test RE 0.9999973555971362\n",
      "31 Train Loss 72.73476 Test MSE 2727.6127057796084 Test RE 0.999997315617598\n",
      "32 Train Loss 72.73474 Test MSE 2727.6120823825345 Test RE 0.9999972013426803\n",
      "33 Train Loss 72.734726 Test MSE 2727.611625049908 Test RE 0.9999971175090279\n",
      "34 Train Loss 72.73471 Test MSE 2727.6109957799936 Test RE 0.9999970021575366\n",
      "35 Train Loss 72.73468 Test MSE 2727.6101223611754 Test RE 0.9999968420510787\n",
      "36 Train Loss 72.73468 Test MSE 2727.6099017413803 Test RE 0.9999968016092464\n",
      "37 Train Loss 72.73465 Test MSE 2727.6091199867446 Test RE 0.9999966583057279\n",
      "38 Train Loss 72.73464 Test MSE 2727.6086564274283 Test RE 0.9999965733306149\n",
      "39 Train Loss 72.73463 Test MSE 2727.6081918293244 Test RE 0.9999964881650744\n",
      "40 Train Loss 72.73461 Test MSE 2727.6075412954474 Test RE 0.9999963689156101\n",
      "41 Train Loss 72.73459 Test MSE 2727.60688623997 Test RE 0.9999962488372761\n",
      "42 Train Loss 72.73458 Test MSE 2727.6066623421825 Test RE 0.9999962077945304\n",
      "43 Train Loss 72.73457 Test MSE 2727.6061077469044 Test RE 0.9999961061315575\n",
      "44 Train Loss 72.73454 Test MSE 2727.6051950496794 Test RE 0.9999959388248222\n",
      "45 Train Loss 72.73454 Test MSE 2727.604969296789 Test RE 0.9999958974420041\n",
      "46 Train Loss 72.73452 Test MSE 2727.604294625404 Test RE 0.9999957737678162\n",
      "47 Train Loss 72.7345 Test MSE 2727.60347900869 Test RE 0.9999956242568827\n",
      "48 Train Loss 72.734474 Test MSE 2727.6027915714308 Test RE 0.9999954982425447\n",
      "49 Train Loss 72.73447 Test MSE 2727.602563007743 Test RE 0.9999954563444604\n",
      "50 Train Loss 72.73447 Test MSE 2727.602334164223 Test RE 0.9999954143950781\n",
      "51 Train Loss 72.73444 Test MSE 2727.6016361208917 Test RE 0.9999952864365105\n",
      "52 Train Loss 72.73443 Test MSE 2727.600931262613 Test RE 0.999995157228676\n",
      "53 Train Loss 72.734406 Test MSE 2727.600219220039 Test RE 0.9999950267038685\n",
      "54 Train Loss 72.73438 Test MSE 2727.5993726452816 Test RE 0.9999948715178911\n",
      "55 Train Loss 72.73436 Test MSE 2727.5986435532814 Test RE 0.9999947378677083\n",
      "56 Train Loss 72.73436 Test MSE 2727.5984096571574 Test RE 0.9999946949921006\n",
      "57 Train Loss 72.73433 Test MSE 2727.5974093180726 Test RE 0.9999945116194556\n",
      "58 Train Loss 72.7343 Test MSE 2727.5965374723905 Test RE 0.9999943518009716\n",
      "59 Train Loss 72.7343 Test MSE 2727.5963006561747 Test RE 0.9999943083900638\n",
      "60 Train Loss 72.73427 Test MSE 2727.595268528262 Test RE 0.9999941191901196\n",
      "61 Train Loss 72.734245 Test MSE 2727.5942439027417 Test RE 0.9999939313654077\n",
      "62 Train Loss 72.734215 Test MSE 2727.5931748685293 Test RE 0.9999937354000745\n",
      "63 Train Loss 72.734184 Test MSE 2727.592085052746 Test RE 0.9999935356252189\n",
      "64 Train Loss 72.73414 Test MSE 2727.590924589643 Test RE 0.9999933228999124\n",
      "65 Train Loss 72.73414 Test MSE 2727.590679265754 Test RE 0.9999932779294122\n",
      "66 Train Loss 72.73411 Test MSE 2727.589535715578 Test RE 0.9999930683043726\n",
      "67 Train Loss 72.73411 Test MSE 2727.5892881255595 Test RE 0.9999930229184548\n",
      "68 Train Loss 72.734085 Test MSE 2727.5885621485604 Test RE 0.9999928898390383\n",
      "69 Train Loss 72.73408 Test MSE 2727.588312940644 Test RE 0.9999928441565343\n",
      "70 Train Loss 72.73403 Test MSE 2727.586993898621 Test RE 0.9999926023618442\n",
      "71 Train Loss 72.73403 Test MSE 2727.5867420098402 Test RE 0.9999925561878955\n",
      "72 Train Loss 72.734 Test MSE 2727.585624252373 Test RE 0.999992351290789\n",
      "73 Train Loss 72.73397 Test MSE 2727.584465237225 Test RE 0.9999921388306564\n",
      "74 Train Loss 72.733925 Test MSE 2727.583102720842 Test RE 0.999991889066468\n",
      "75 Train Loss 72.73391 Test MSE 2727.582253249836 Test RE 0.9999917333490891\n",
      "76 Train Loss 72.73388 Test MSE 2727.581376905673 Test RE 0.9999915727055398\n",
      "77 Train Loss 72.733864 Test MSE 2727.5808201873942 Test RE 0.9999914706529265\n",
      "78 Train Loss 72.73384 Test MSE 2727.5799206389925 Test RE 0.9999913057557406\n",
      "79 Train Loss 72.73381 Test MSE 2727.5788163050934 Test RE 0.9999911033190673\n",
      "80 Train Loss 72.249855 Test MSE 2693.0913644830935 Test RE 0.9936490586525552\n",
      "81 Train Loss 64.385185 Test MSE 1926.015784134078 Test RE 0.8403058804730696\n",
      "82 Train Loss 31.482765 Test MSE 989.3564252943829 Test RE 0.6022597937539609\n",
      "83 Train Loss 29.127869 Test MSE 971.3470326998442 Test RE 0.596753109538447\n",
      "84 Train Loss 28.766733 Test MSE 970.1581899052737 Test RE 0.5963878112371553\n",
      "85 Train Loss 26.699682 Test MSE 849.9503502454655 Test RE 0.5582185449478286\n",
      "86 Train Loss 9.110929 Test MSE 256.61425173918053 Test RE 0.3067240768047128\n",
      "87 Train Loss 3.7580101 Test MSE 29.157883736751707 Test RE 0.10339165370804447\n",
      "88 Train Loss 0.9154936 Test MSE 5.351284317770272 Test RE 0.044293142618841624\n",
      "89 Train Loss 0.19317341 Test MSE 0.34863930912700614 Test RE 0.01130565316218625\n",
      "90 Train Loss 0.10793838 Test MSE 0.3930524773182403 Test RE 0.012004186959063188\n",
      "91 Train Loss 0.03699961 Test MSE 0.08023052188706196 Test RE 0.005423470601950396\n",
      "92 Train Loss 0.027141653 Test MSE 0.03147387660773962 Test RE 0.0033968987381472037\n",
      "93 Train Loss 0.020699749 Test MSE 0.02259076950831521 Test RE 0.0028778820741070022\n",
      "94 Train Loss 0.010379065 Test MSE 0.010495504916977112 Test RE 0.001961594115389516\n",
      "95 Train Loss 0.0077714548 Test MSE 0.00705439693003002 Test RE 0.0016081902619925939\n",
      "96 Train Loss 0.00582914 Test MSE 0.0035199003896541924 Test RE 0.0011359851875458289\n",
      "97 Train Loss 0.004134863 Test MSE 0.002682627687604652 Test RE 0.0009917168290651851\n",
      "98 Train Loss 0.0036604106 Test MSE 0.002248244301949395 Test RE 0.0009078816378753091\n",
      "99 Train Loss 0.0030222435 Test MSE 0.002096255884176359 Test RE 0.0008766568469403454\n",
      "Training time: 21.72\n",
      "Training time: 21.72\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 72.734436 Test MSE 2727.6012280661957 Test RE 0.9999952116358537\n",
      "1 Train Loss 57.631348 Test MSE 1676.2659758402876 Test RE 0.7839330002167848\n",
      "2 Train Loss 4.9761357 Test MSE 67.4135722957926 Test RE 0.15721035646958684\n",
      "3 Train Loss 3.4277875 Test MSE 47.6764427769623 Test RE 0.13220850795720915\n",
      "4 Train Loss 2.3916674 Test MSE 27.331926534506213 Test RE 0.10010196531808987\n",
      "5 Train Loss 1.1897014 Test MSE 4.750073823728282 Test RE 0.0417308913808093\n",
      "6 Train Loss 0.74891907 Test MSE 3.935292839739907 Test RE 0.03798358999956235\n",
      "7 Train Loss 0.25096327 Test MSE 0.6239351177438792 Test RE 0.01512436664256209\n",
      "8 Train Loss 0.11435953 Test MSE 1.2358084786989008 Test RE 0.021285461317886726\n",
      "9 Train Loss 0.0668791 Test MSE 0.24951587122430074 Test RE 0.009564374425712554\n",
      "10 Train Loss 0.060435694 Test MSE 0.07354657722286538 Test RE 0.005192645981497306\n",
      "11 Train Loss 0.05676033 Test MSE 0.05740686412864292 Test RE 0.004587640666111176\n",
      "12 Train Loss 0.045767482 Test MSE 0.056640159534302646 Test RE 0.004556902282567787\n",
      "13 Train Loss 0.03045447 Test MSE 0.09875160862709247 Test RE 0.006016993900011633\n",
      "14 Train Loss 0.011384588 Test MSE 0.026336138156041976 Test RE 0.00310730245150984\n",
      "15 Train Loss 0.00871448 Test MSE 0.008567337137164312 Test RE 0.001772272140016416\n",
      "16 Train Loss 0.008146453 Test MSE 0.006841378305510432 Test RE 0.0015837232222047231\n",
      "17 Train Loss 0.0064732637 Test MSE 0.006329515402524814 Test RE 0.0015233255006886991\n",
      "18 Train Loss 0.004970468 Test MSE 0.003996885896029467 Test RE 0.0012105099312697326\n",
      "19 Train Loss 0.0046292306 Test MSE 0.0033723214619544328 Test RE 0.001111915962671016\n",
      "20 Train Loss 0.0045175217 Test MSE 0.0034344670737116744 Test RE 0.0011221144637785937\n",
      "21 Train Loss 0.0042431382 Test MSE 0.003186274943534593 Test RE 0.0010808093820278324\n",
      "22 Train Loss 0.0038726681 Test MSE 0.002362684232985726 Test RE 0.0009307013060010564\n",
      "23 Train Loss 0.0036859903 Test MSE 0.0027635257147289533 Test RE 0.0010065590010134146\n",
      "24 Train Loss 0.0034510694 Test MSE 0.002999240595277764 Test RE 0.0010486079225970287\n",
      "25 Train Loss 0.0028198664 Test MSE 0.0017342072446420438 Test RE 0.0007973664895234043\n",
      "26 Train Loss 0.0025369686 Test MSE 0.001245558656446522 Test RE 0.0006757554764042828\n",
      "27 Train Loss 0.002181844 Test MSE 0.0010259147044902957 Test RE 0.0006132860823311817\n",
      "28 Train Loss 0.0017341338 Test MSE 0.0009131542411744048 Test RE 0.0005786014958064224\n",
      "29 Train Loss 0.0015882737 Test MSE 0.0007916171837353011 Test RE 0.0005387224675037725\n",
      "30 Train Loss 0.0015705086 Test MSE 0.0008303777927406219 Test RE 0.0005517538151613992\n",
      "31 Train Loss 0.0015618748 Test MSE 0.0008750991351572167 Test RE 0.0005664167778188574\n",
      "32 Train Loss 0.0015007597 Test MSE 0.0011942295236308698 Test RE 0.0006616851432214542\n",
      "33 Train Loss 0.0009927282 Test MSE 0.002082789204931937 Test RE 0.0008738364188253167\n",
      "34 Train Loss 0.0005901147 Test MSE 0.00047948753778818124 Test RE 0.00041927227385469824\n",
      "35 Train Loss 0.00051282847 Test MSE 0.000306175664672517 Test RE 0.00033503703486913107\n",
      "36 Train Loss 0.00046509903 Test MSE 0.00023003399434926814 Test RE 0.00029040460054578456\n",
      "37 Train Loss 0.00038793392 Test MSE 0.0002623904056728716 Test RE 0.0003101569071400952\n",
      "38 Train Loss 0.0002855998 Test MSE 0.0003359196081188702 Test RE 0.0003509337678431956\n",
      "39 Train Loss 0.0002033517 Test MSE 0.0002018050815991569 Test RE 0.00027200289772714233\n",
      "40 Train Loss 0.00015605654 Test MSE 0.00017060363922991018 Test RE 0.00025009305248584125\n",
      "41 Train Loss 0.00013123125 Test MSE 0.00012624753586205244 Test RE 0.0002151388965887834\n",
      "42 Train Loss 0.00013094924 Test MSE 9.509327999657456e-05 Test RE 0.00018671636999414878\n",
      "43 Train Loss 0.00010154419 Test MSE 9.194501916894313e-05 Test RE 0.0001835995386718408\n",
      "44 Train Loss 8.4136074e-05 Test MSE 5.0061094060004544e-05 Test RE 0.00013547452910084933\n",
      "45 Train Loss 8.0983235e-05 Test MSE 3.91539935063934e-05 Test RE 0.0001198106747438217\n",
      "46 Train Loss 8.027871e-05 Test MSE 3.711055666503013e-05 Test RE 0.00011664233802406388\n",
      "47 Train Loss 7.9660786e-05 Test MSE 3.3993971352705686e-05 Test RE 0.00011163707109224765\n",
      "48 Train Loss 7.9167665e-05 Test MSE 3.395973752400233e-05 Test RE 0.0001115808445484818\n",
      "49 Train Loss 7.868702e-05 Test MSE 3.0219499166644793e-05 Test RE 0.00010525703035599743\n",
      "50 Train Loss 7.834743e-05 Test MSE 3.094676520386378e-05 Test RE 0.00010651606443931143\n",
      "51 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "52 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "53 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "54 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "55 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "56 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "57 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "58 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "59 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "60 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "61 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "62 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "63 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "64 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "65 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "66 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "67 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "68 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "69 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "70 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "71 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "72 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "73 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "74 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "75 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "76 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "77 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "78 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "79 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "80 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "81 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "82 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "83 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "84 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "85 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "86 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "87 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "88 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "89 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "90 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "91 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "92 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "93 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "94 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "95 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "96 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "97 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "98 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "99 Train Loss 7.738002e-05 Test MSE 2.8015648885101963e-05 Test RE 0.00010134628292224199\n",
      "Training time: 14.62\n",
      "Training time: 14.62\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 72.73542 Test MSE 2727.6364653651467 Test RE 1.0000016709775008\n",
      "1 Train Loss 71.50841 Test MSE 2663.4490693503058 Test RE 0.9881654837551426\n",
      "2 Train Loss 51.93664 Test MSE 1900.9344215780177 Test RE 0.834816547992992\n",
      "3 Train Loss 8.676319 Test MSE 143.60115042127956 Test RE 0.22944914375389394\n",
      "4 Train Loss 2.1553252 Test MSE 18.234843060327364 Test RE 0.0817633160539478\n",
      "5 Train Loss 1.0110639 Test MSE 6.158568500894338 Test RE 0.04751682017591439\n",
      "6 Train Loss 0.3499448 Test MSE 2.5852323701967888 Test RE 0.030786284212018726\n",
      "7 Train Loss 0.22959149 Test MSE 2.7114905519904466 Test RE 0.03152909677439853\n",
      "8 Train Loss 0.11539064 Test MSE 0.2977144351626833 Test RE 0.0104473808807473\n",
      "9 Train Loss 0.087331615 Test MSE 0.37049297017695887 Test RE 0.011654602554067892\n",
      "10 Train Loss 0.054570895 Test MSE 0.19765178781332873 Test RE 0.008512514315686032\n",
      "11 Train Loss 0.035588905 Test MSE 0.05670568421543115 Test RE 0.004559537367504492\n",
      "12 Train Loss 0.027575653 Test MSE 0.0643741439823007 Test RE 0.004858063814428995\n",
      "13 Train Loss 0.017941725 Test MSE 0.04160569877805438 Test RE 0.0039055651969170805\n",
      "14 Train Loss 0.0137093235 Test MSE 0.011980272966326344 Test RE 0.002095756580532062\n",
      "15 Train Loss 0.0114575 Test MSE 0.009731454824984059 Test RE 0.0018888452017119589\n",
      "16 Train Loss 0.010962751 Test MSE 0.009323280990103959 Test RE 0.0018488082416943706\n",
      "17 Train Loss 0.009801905 Test MSE 0.011186381324539425 Test RE 0.0020251271289287286\n",
      "18 Train Loss 0.0068314704 Test MSE 0.005144915124807162 Test RE 0.0013733985644218184\n",
      "19 Train Loss 0.0052568973 Test MSE 0.00387052523038206 Test RE 0.0011912212528950728\n",
      "20 Train Loss 0.0041196407 Test MSE 0.0038607499749825133 Test RE 0.001189716049748936\n",
      "21 Train Loss 0.003170489 Test MSE 0.0030407153833502745 Test RE 0.0010558333298817644\n",
      "22 Train Loss 0.0027133112 Test MSE 0.003298155833840381 Test RE 0.0010996211074444458\n",
      "23 Train Loss 0.0024468577 Test MSE 0.0025857683255075706 Test RE 0.000973648697930136\n",
      "24 Train Loss 0.0022859178 Test MSE 0.0019398219056775837 Test RE 0.0008433122548465614\n",
      "25 Train Loss 0.0019261694 Test MSE 0.001805649959668407 Test RE 0.0008136249597069666\n",
      "26 Train Loss 0.0013243752 Test MSE 0.0017863437164234732 Test RE 0.0008092635785368066\n",
      "27 Train Loss 0.0007670418 Test MSE 0.0009647052221052814 Test RE 0.0005947093910583937\n",
      "28 Train Loss 0.00053079164 Test MSE 0.0004783071069838787 Test RE 0.0004187558611734924\n",
      "29 Train Loss 0.00043663598 Test MSE 0.00026714228701543307 Test RE 0.0003129527712177768\n",
      "30 Train Loss 0.0003928229 Test MSE 0.00023338153877556063 Test RE 0.00029251000904971606\n",
      "31 Train Loss 0.00034382383 Test MSE 0.00022576762230239932 Test RE 0.00028769897285607334\n",
      "32 Train Loss 0.0003056829 Test MSE 0.00013394365632016244 Test RE 0.00022159938803995098\n",
      "33 Train Loss 0.0002898127 Test MSE 9.336291857600236e-05 Test RE 0.0001850097820585054\n",
      "34 Train Loss 0.0002879196 Test MSE 9.305112764403705e-05 Test RE 0.00018470059820949547\n",
      "35 Train Loss 0.0002871189 Test MSE 8.980970719241386e-05 Test RE 0.00018145507617001348\n",
      "36 Train Loss 0.0002852154 Test MSE 9.145212353598447e-05 Test RE 0.0001831067603844869\n",
      "37 Train Loss 0.00028430243 Test MSE 9.701222834410065e-05 Test RE 0.00018859089516738895\n",
      "38 Train Loss 0.0002837633 Test MSE 9.40660373598104e-05 Test RE 0.00018570513229260505\n",
      "39 Train Loss 0.00028322905 Test MSE 9.537252686431569e-05 Test RE 0.00018699032068541816\n",
      "40 Train Loss 0.00027059412 Test MSE 0.00013376995962811658 Test RE 0.00022145565757700027\n",
      "41 Train Loss 0.00025237672 Test MSE 0.00012624101567591716 Test RE 0.000215133340980299\n",
      "42 Train Loss 0.00022193309 Test MSE 0.00016446304509415037 Test RE 0.0002455509650678493\n",
      "43 Train Loss 0.00020253002 Test MSE 0.00010767633912581676 Test RE 0.0001986861628961407\n",
      "44 Train Loss 0.0002020555 Test MSE 0.00010551386444516084 Test RE 0.0001966809269614954\n",
      "45 Train Loss 0.00018686894 Test MSE 9.593762441203258e-05 Test RE 0.0001875434763609041\n",
      "46 Train Loss 0.00016539135 Test MSE 0.00013777651263295715 Test RE 0.00022474760668867932\n",
      "47 Train Loss 0.00014094276 Test MSE 8.604445640453631e-05 Test RE 0.00017761061962876955\n",
      "48 Train Loss 0.00013511423 Test MSE 8.58403485312109e-05 Test RE 0.00017739983760238671\n",
      "49 Train Loss 0.000118258766 Test MSE 6.402744373017969e-05 Test RE 0.0001532112173660048\n",
      "50 Train Loss 0.00010716398 Test MSE 9.141549674561139e-05 Test RE 0.00018307008936851033\n",
      "51 Train Loss 0.00010617565 Test MSE 7.861789207160914e-05 Test RE 0.00016977282658485685\n",
      "52 Train Loss 0.00010528704 Test MSE 6.692592187943199e-05 Test RE 0.00015664071716934216\n",
      "53 Train Loss 0.000103315644 Test MSE 5.3134386590301366e-05 Test RE 0.00013957104089490436\n",
      "54 Train Loss 9.7198266e-05 Test MSE 4.4106047108887315e-05 Test RE 0.00012716176587983686\n",
      "55 Train Loss 9.67993e-05 Test MSE 4.066651714341911e-05 Test RE 0.00012210289791632912\n",
      "56 Train Loss 9.654129e-05 Test MSE 4.069993808213649e-05 Test RE 0.0001221530614866373\n",
      "57 Train Loss 9.619168e-05 Test MSE 3.8317044111149154e-05 Test RE 0.00011852323078499724\n",
      "58 Train Loss 9.5874675e-05 Test MSE 3.885072951543287e-05 Test RE 0.00011934578100095726\n",
      "59 Train Loss 9.5553354e-05 Test MSE 3.683820986551527e-05 Test RE 0.0001162135427115132\n",
      "60 Train Loss 9.531731e-05 Test MSE 3.7124095284614053e-05 Test RE 0.00011666361272837794\n",
      "61 Train Loss 9.522905e-05 Test MSE 3.686923229879423e-05 Test RE 0.00011626246566668904\n",
      "62 Train Loss 9.497442e-05 Test MSE 3.650650953081177e-05 Test RE 0.0001156891518939041\n",
      "63 Train Loss 9.472891e-05 Test MSE 3.6517587184671555e-05 Test RE 0.00011570670310861505\n",
      "64 Train Loss 9.44756e-05 Test MSE 3.642256541120599e-05 Test RE 0.00011555606586023076\n",
      "65 Train Loss 9.4211995e-05 Test MSE 3.657045715714803e-05 Test RE 0.00011579043259477345\n",
      "66 Train Loss 9.392605e-05 Test MSE 3.659514531517977e-05 Test RE 0.00011582951017850156\n",
      "67 Train Loss 9.359813e-05 Test MSE 3.688981426823159e-05 Test RE 0.00011629491245908248\n",
      "68 Train Loss 9.3218725e-05 Test MSE 3.6985708808289195e-05 Test RE 0.00011644596782992716\n",
      "69 Train Loss 9.279931e-05 Test MSE 3.744332185156071e-05 Test RE 0.00011716412871803998\n",
      "70 Train Loss 9.2270886e-05 Test MSE 3.742420614047186e-05 Test RE 0.00011713421735536212\n",
      "71 Train Loss 9.1505055e-05 Test MSE 3.9185264099325964e-05 Test RE 0.00011985850898434454\n",
      "72 Train Loss 9.10182e-05 Test MSE 3.960722983747626e-05 Test RE 0.0001205021279112993\n",
      "73 Train Loss 8.928548e-05 Test MSE 3.932880953976932e-05 Test RE 0.0001200778441727841\n",
      "74 Train Loss 8.872239e-05 Test MSE 3.931496907203089e-05 Test RE 0.00012005671360985673\n",
      "75 Train Loss 8.830338e-05 Test MSE 3.9119345709903754e-05 Test RE 0.00011975765212383137\n",
      "76 Train Loss 8.784707e-05 Test MSE 3.855272403361917e-05 Test RE 0.00011888717749290755\n",
      "77 Train Loss 8.7470704e-05 Test MSE 3.812508347879609e-05 Test RE 0.00011822596931909241\n",
      "78 Train Loss 8.680555e-05 Test MSE 3.684761408161001e-05 Test RE 0.00011622837551055275\n",
      "79 Train Loss 8.660089e-05 Test MSE 3.63694756643481e-05 Test RE 0.0001154718175461501\n",
      "80 Train Loss 8.640018e-05 Test MSE 3.5754223063650914e-05 Test RE 0.00011449094879838222\n",
      "81 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "82 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "83 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "84 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "85 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "86 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "87 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "88 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "89 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "90 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "91 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "92 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "93 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "94 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "95 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "96 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "97 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "98 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "99 Train Loss 8.56056e-05 Test MSE 3.262324522243523e-05 Test RE 0.00010936316292335184\n",
      "Training time: 15.38\n",
      "Training time: 15.38\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 72.735794 Test MSE 2727.649896416328 Test RE 1.0000041330096046\n",
      "1 Train Loss 72.71431 Test MSE 2725.2357004453065 Test RE 0.9995614918221021\n",
      "2 Train Loss 60.528484 Test MSE 2232.60097578345 Test RE 0.9047175956399576\n",
      "3 Train Loss 40.082867 Test MSE 1187.8554308761834 Test RE 0.6599169381633275\n",
      "4 Train Loss 29.725342 Test MSE 776.9040877913922 Test RE 0.5336926049072171\n",
      "5 Train Loss 25.084927 Test MSE 696.212002318728 Test RE 0.5052173247070606\n",
      "6 Train Loss 21.227316 Test MSE 670.2730933538436 Test RE 0.4957165000543983\n",
      "7 Train Loss 12.713159 Test MSE 336.5241546119217 Test RE 0.35124940945266364\n",
      "8 Train Loss 1.1975882 Test MSE 6.499661519108059 Test RE 0.04881495036827208\n",
      "9 Train Loss 0.38479337 Test MSE 2.029449132384014 Test RE 0.027276997690906617\n",
      "10 Train Loss 0.12852372 Test MSE 0.2885829931491016 Test RE 0.010285913044828999\n",
      "11 Train Loss 0.06645175 Test MSE 0.17173193841060433 Test RE 0.007934745793932071\n",
      "12 Train Loss 0.041516867 Test MSE 0.0626175823966164 Test RE 0.004791324992148191\n",
      "13 Train Loss 0.028611168 Test MSE 0.042609564891764706 Test RE 0.003952401281998017\n",
      "14 Train Loss 0.01882002 Test MSE 0.028078762094177104 Test RE 0.0032084587520319935\n",
      "15 Train Loss 0.014508861 Test MSE 0.012897203637659286 Test RE 0.002174479212783021\n",
      "16 Train Loss 0.010760931 Test MSE 0.014622628553999619 Test RE 0.0023153689548443897\n",
      "17 Train Loss 0.0088416645 Test MSE 0.009233674062627713 Test RE 0.0018399022563128414\n",
      "18 Train Loss 0.0062403413 Test MSE 0.007159889335584391 Test RE 0.0016201701881472968\n",
      "19 Train Loss 0.0046530804 Test MSE 0.007848503431944895 Test RE 0.0016962931489257656\n",
      "20 Train Loss 0.004154186 Test MSE 0.0061596948025548414 Test RE 0.0015027511847321815\n",
      "21 Train Loss 0.0036977706 Test MSE 0.004480581648045203 Test RE 0.0012816654573641053\n",
      "22 Train Loss 0.0034782926 Test MSE 0.0024943099820247 Test RE 0.0009562747622581419\n",
      "23 Train Loss 0.0033099286 Test MSE 0.0015343918133727485 Test RE 0.0007500247901562512\n",
      "24 Train Loss 0.0029310477 Test MSE 0.0017007991147968723 Test RE 0.0007896488229574192\n",
      "25 Train Loss 0.0021120587 Test MSE 0.0014687910516550816 Test RE 0.0007338165305179766\n",
      "26 Train Loss 0.0016046116 Test MSE 0.0009121238712246635 Test RE 0.0005782749672544266\n",
      "27 Train Loss 0.0014537796 Test MSE 0.0009206704990982306 Test RE 0.0005809778772741368\n",
      "28 Train Loss 0.0014249196 Test MSE 0.0007216845196807615 Test RE 0.0005143765682076019\n",
      "29 Train Loss 0.0014060601 Test MSE 0.0007176489046745728 Test RE 0.0005129363711025477\n",
      "30 Train Loss 0.0013676034 Test MSE 0.0008434404333047427 Test RE 0.0005560766895958143\n",
      "31 Train Loss 0.0012818758 Test MSE 0.0007014522164868467 Test RE 0.0005071150828367059\n",
      "32 Train Loss 0.0011686354 Test MSE 0.0006413582472190448 Test RE 0.0004849062966917198\n",
      "33 Train Loss 0.0011223803 Test MSE 0.000643706752531154 Test RE 0.0004857932926981766\n",
      "34 Train Loss 0.0010825946 Test MSE 0.0005431052759687334 Test RE 0.0004462204734353483\n",
      "35 Train Loss 0.0010286889 Test MSE 0.0004875159977369294 Test RE 0.00042276781517502893\n",
      "36 Train Loss 0.0009863205 Test MSE 0.00046828891790794814 Test RE 0.00041434721291202977\n",
      "37 Train Loss 0.0009608058 Test MSE 0.00040170599476486524 Test RE 0.00038376170786692235\n",
      "38 Train Loss 0.0009485403 Test MSE 0.0003610699947065153 Test RE 0.0003638339166274299\n",
      "39 Train Loss 0.00094834587 Test MSE 0.00036075369616305543 Test RE 0.00036367452183795563\n",
      "40 Train Loss 0.00094803656 Test MSE 0.0003620438117050437 Test RE 0.0003643242219487901\n",
      "41 Train Loss 0.0009478567 Test MSE 0.00036079269133528326 Test RE 0.00036369417675424846\n",
      "42 Train Loss 0.00094761717 Test MSE 0.00036145540386839294 Test RE 0.00036402804453554405\n",
      "43 Train Loss 0.0009467574 Test MSE 0.0003650374675387252 Test RE 0.0003658273770431022\n",
      "44 Train Loss 0.0009464613 Test MSE 0.00036656382554208644 Test RE 0.0003665914096548442\n",
      "45 Train Loss 0.0009461689 Test MSE 0.00036675064739065004 Test RE 0.000366684815708597\n",
      "46 Train Loss 0.00092963147 Test MSE 0.0003822777395297327 Test RE 0.00037436650624708\n",
      "47 Train Loss 0.0009057238 Test MSE 0.00037239492951768744 Test RE 0.00036949567672290657\n",
      "48 Train Loss 0.0009047292 Test MSE 0.0003787647078536434 Test RE 0.00037264237125451976\n",
      "49 Train Loss 0.00088118296 Test MSE 0.0004399468154743002 Test RE 0.000401612823143951\n",
      "50 Train Loss 0.00081652694 Test MSE 0.00045201182704152033 Test RE 0.00040708245127893524\n",
      "51 Train Loss 0.000688527 Test MSE 0.00046349988542209396 Test RE 0.00041222307379250626\n",
      "52 Train Loss 0.0006008816 Test MSE 0.0003449282928718336 Test RE 0.0003556083018865312\n",
      "53 Train Loss 0.0005079615 Test MSE 0.00031217226395363097 Test RE 0.0003383020572518235\n",
      "54 Train Loss 0.00042437422 Test MSE 0.00040977780875337677 Test RE 0.0003875981537159835\n",
      "55 Train Loss 0.0004048051 Test MSE 0.00022460610497728007 Test RE 0.00028695794930102533\n",
      "56 Train Loss 0.00039393845 Test MSE 0.0001930977381788405 Test RE 0.00026607010142205547\n",
      "57 Train Loss 0.0003920637 Test MSE 0.00019079425085249946 Test RE 0.00026447834818794127\n",
      "58 Train Loss 0.00038026628 Test MSE 0.00018626507001911813 Test RE 0.00026132032628767103\n",
      "59 Train Loss 0.0003602985 Test MSE 0.00014757705479046248 Test RE 0.0002326038512939844\n",
      "60 Train Loss 0.00034218314 Test MSE 0.0001423429960567596 Test RE 0.00022844177875261542\n",
      "61 Train Loss 0.00030972683 Test MSE 0.00013755176017260975 Test RE 0.00022456421841193533\n",
      "62 Train Loss 0.0003082249 Test MSE 0.00012931376216584076 Test RE 0.00021773580696330698\n",
      "63 Train Loss 0.00030093626 Test MSE 0.00011994833606331137 Test RE 0.00020970297689972914\n",
      "64 Train Loss 0.0003001881 Test MSE 0.0001194832194519319 Test RE 0.00020929600554142055\n",
      "65 Train Loss 0.0002997762 Test MSE 0.00011936523436204734 Test RE 0.00020919264413553454\n",
      "66 Train Loss 0.00029923327 Test MSE 0.0001178262066227646 Test RE 0.00020783966309649736\n",
      "67 Train Loss 0.00029840262 Test MSE 0.00011896246786440469 Test RE 0.00020883941321381676\n",
      "68 Train Loss 0.00029791862 Test MSE 0.00011807830556422676 Test RE 0.0002080618893907232\n",
      "69 Train Loss 0.00029744845 Test MSE 0.00011775181384942164 Test RE 0.00020777404013062976\n",
      "70 Train Loss 0.00029696157 Test MSE 0.0001169764474320146 Test RE 0.00020708884010212653\n",
      "71 Train Loss 0.00029646762 Test MSE 0.00011632529984120858 Test RE 0.00020651165734397907\n",
      "72 Train Loss 0.00029598715 Test MSE 0.00011552643458628183 Test RE 0.00020580132516915842\n",
      "73 Train Loss 0.00029558636 Test MSE 0.00011478989897974725 Test RE 0.00020514423583642587\n",
      "74 Train Loss 0.00029510687 Test MSE 0.00011409015401184473 Test RE 0.00020451801314170904\n",
      "75 Train Loss 0.00029470032 Test MSE 0.00011312679880521813 Test RE 0.00020365272740854312\n",
      "76 Train Loss 0.00029427215 Test MSE 0.00011276729370456012 Test RE 0.00020332887640946287\n",
      "77 Train Loss 0.00029388233 Test MSE 0.00011212920197953613 Test RE 0.00020275279390047973\n",
      "78 Train Loss 0.00029352753 Test MSE 0.00011191795043715342 Test RE 0.00020256171061212847\n",
      "79 Train Loss 0.00029284085 Test MSE 0.0001121782827351765 Test RE 0.00020279716312561792\n",
      "80 Train Loss 0.00029255284 Test MSE 0.00011329729021474261 Test RE 0.00020380613035504643\n",
      "81 Train Loss 0.0002921327 Test MSE 0.00011512401404694872 Test RE 0.00020544257209094863\n",
      "82 Train Loss 0.0002918677 Test MSE 0.00011326003812689952 Test RE 0.0002037726219276281\n",
      "83 Train Loss 0.00029100032 Test MSE 0.0001146589225549631 Test RE 0.0002050271666184206\n",
      "84 Train Loss 0.00029065323 Test MSE 0.00011525153510490749 Test RE 0.00020555632335095497\n",
      "85 Train Loss 0.00029008422 Test MSE 0.00011628512553624262 Test RE 0.00020647599365449657\n",
      "86 Train Loss 0.0002893784 Test MSE 0.0001177368356648984 Test RE 0.00020776082514573112\n",
      "87 Train Loss 0.0002884877 Test MSE 0.00012014061000300469 Test RE 0.000209870983700238\n",
      "88 Train Loss 0.00028819637 Test MSE 0.00012112379944945188 Test RE 0.00021072798989903014\n",
      "89 Train Loss 0.00028504248 Test MSE 0.00013626974589788494 Test RE 0.00022351527331461487\n",
      "90 Train Loss 0.0002742533 Test MSE 0.00013429374602476915 Test RE 0.00022188879718372165\n",
      "91 Train Loss 0.00027004408 Test MSE 0.00011545338131244827 Test RE 0.00020573624552685347\n",
      "92 Train Loss 0.0002694749 Test MSE 0.00011492535088419795 Test RE 0.00020526523509378276\n",
      "93 Train Loss 0.0002679734 Test MSE 0.00011003248439985219 Test RE 0.00020084819874925346\n",
      "94 Train Loss 0.0002658378 Test MSE 0.00010695085250526564 Test RE 0.00019801569167636067\n",
      "95 Train Loss 0.000254473 Test MSE 0.00012106942023637984 Test RE 0.00021068068082858127\n",
      "96 Train Loss 0.0002203537 Test MSE 7.901005054081416e-05 Test RE 0.00017019572671134628\n",
      "97 Train Loss 0.00018257699 Test MSE 7.393563644165533e-05 Test RE 0.00016463963310599445\n",
      "98 Train Loss 0.00016830883 Test MSE 7.913125629626047e-05 Test RE 0.00017032622147853638\n",
      "99 Train Loss 0.00016769645 Test MSE 7.646993642920996e-05 Test RE 0.00016743754449310194\n",
      "Training time: 20.76\n",
      "Training time: 20.76\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 72.73501 Test MSE 2727.6220218345316 Test RE 0.9999990233420748\n",
      "1 Train Loss 64.12764 Test MSE 2332.0783055277707 Test RE 0.9246535603135823\n",
      "2 Train Loss 31.6137 Test MSE 817.8780052399751 Test RE 0.5475852563787831\n",
      "3 Train Loss 9.212115 Test MSE 51.00096422777908 Test RE 0.13674034624875522\n",
      "4 Train Loss 1.9380846 Test MSE 10.650890160650079 Test RE 0.062488548195129584\n",
      "5 Train Loss 0.50971115 Test MSE 3.5862513486618335 Test RE 0.03626000369950553\n",
      "6 Train Loss 0.2956575 Test MSE 1.0964507431576678 Test RE 0.020049430676408908\n",
      "7 Train Loss 0.20684946 Test MSE 0.6391131403001051 Test RE 0.015307221097478651\n",
      "8 Train Loss 0.10152786 Test MSE 0.18664435828492595 Test RE 0.008272083607788333\n",
      "9 Train Loss 0.07568825 Test MSE 0.12222440238667719 Test RE 0.006694011328833669\n",
      "10 Train Loss 0.057583854 Test MSE 0.1187744030867194 Test RE 0.006598859931195905\n",
      "11 Train Loss 0.02989635 Test MSE 0.039435251678710107 Test RE 0.0038023298609243927\n",
      "12 Train Loss 0.025488976 Test MSE 0.03299077758819767 Test RE 0.003477793230120984\n",
      "13 Train Loss 0.018964907 Test MSE 0.03699517824430355 Test RE 0.0036828161913094737\n",
      "14 Train Loss 0.01342389 Test MSE 0.01576354293757096 Test RE 0.002403999642825541\n",
      "15 Train Loss 0.009072329 Test MSE 0.012883255341132813 Test RE 0.0021733030475198814\n",
      "16 Train Loss 0.006166875 Test MSE 0.006241121640769382 Test RE 0.001512651231736809\n",
      "17 Train Loss 0.0059322156 Test MSE 0.005109711471721009 Test RE 0.0013686918165940244\n",
      "18 Train Loss 0.005841704 Test MSE 0.004776619211152385 Test RE 0.001323328885318612\n",
      "19 Train Loss 0.0056755897 Test MSE 0.0053128959824345215 Test RE 0.00139563913319316\n",
      "20 Train Loss 0.0051319674 Test MSE 0.0056942102046362185 Test RE 0.0014448548810674785\n",
      "21 Train Loss 0.003984156 Test MSE 0.0035998457251379427 Test RE 0.0011488132245673977\n",
      "22 Train Loss 0.003527865 Test MSE 0.0026877450275571394 Test RE 0.000992662270405134\n",
      "23 Train Loss 0.0034445822 Test MSE 0.002364146923480647 Test RE 0.0009309893506977133\n",
      "24 Train Loss 0.0032691155 Test MSE 0.0023820159421347326 Test RE 0.0009345010929839602\n",
      "25 Train Loss 0.0026779147 Test MSE 0.0034266937107265377 Test RE 0.0011208438817646212\n",
      "26 Train Loss 0.001930174 Test MSE 0.002744507632240477 Test RE 0.0010030895434687435\n",
      "27 Train Loss 0.0016659923 Test MSE 0.001839315196530891 Test RE 0.0008211747015048105\n",
      "28 Train Loss 0.0010835989 Test MSE 0.001411807986358998 Test RE 0.0007194411902876536\n",
      "29 Train Loss 0.00071807473 Test MSE 0.0007122781681259218 Test RE 0.000511013411526004\n",
      "30 Train Loss 0.0005510214 Test MSE 0.0005551221485874587 Test RE 0.00045113005296202626\n",
      "31 Train Loss 0.00038567933 Test MSE 0.00045366011763216105 Test RE 0.00040782400221230905\n",
      "32 Train Loss 0.0002645731 Test MSE 0.00037448001372009473 Test RE 0.0003705286582620216\n",
      "33 Train Loss 0.00019193633 Test MSE 0.00018892905428519259 Test RE 0.00026318240854522444\n",
      "34 Train Loss 0.00014463346 Test MSE 0.0001202838662441206 Test RE 0.00020999607200748244\n",
      "35 Train Loss 0.00010791744 Test MSE 9.55407095379884e-05 Test RE 0.00018715512013561277\n",
      "36 Train Loss 7.8005345e-05 Test MSE 7.463603707928827e-05 Test RE 0.0001654176198829299\n",
      "37 Train Loss 6.5674794e-05 Test MSE 4.666232095458832e-05 Test RE 0.00013079485177976193\n",
      "38 Train Loss 5.2319338e-05 Test MSE 5.530534103650492e-05 Test RE 0.00014239378004525275\n",
      "39 Train Loss 4.5015586e-05 Test MSE 5.785674668908817e-05 Test RE 0.0001456412797729917\n",
      "40 Train Loss 4.183335e-05 Test MSE 4.754138016834234e-05 Test RE 0.00013202110838166455\n",
      "41 Train Loss 3.72413e-05 Test MSE 3.84117032215496e-05 Test RE 0.00011866954143214739\n",
      "42 Train Loss 3.6635854e-05 Test MSE 3.858951464603737e-05 Test RE 0.00011894389059389018\n",
      "43 Train Loss 3.6066198e-05 Test MSE 3.158377018448307e-05 Test RE 0.00010760673821829831\n",
      "44 Train Loss 3.5661335e-05 Test MSE 2.9926550566130417e-05 Test RE 0.00010474560571980613\n",
      "45 Train Loss 3.5279918e-05 Test MSE 2.6996855005770686e-05 Test RE 9.948648085616137e-05\n",
      "46 Train Loss 3.4823122e-05 Test MSE 2.53019407517267e-05 Test RE 9.631288652763568e-05\n",
      "47 Train Loss 3.4674144e-05 Test MSE 2.3932744567163388e-05 Test RE 9.36706932876453e-05\n",
      "48 Train Loss 3.4518293e-05 Test MSE 2.3359316795459563e-05 Test RE 9.254171636152744e-05\n",
      "49 Train Loss 3.4368524e-05 Test MSE 2.2269775304479874e-05 Test RE 9.035774820005633e-05\n",
      "50 Train Loss 3.4152414e-05 Test MSE 2.1602630383546477e-05 Test RE 8.899401473576867e-05\n",
      "51 Train Loss 3.400067e-05 Test MSE 2.0862446306150824e-05 Test RE 8.745609821845585e-05\n",
      "52 Train Loss 3.3844884e-05 Test MSE 2.0427660297280807e-05 Test RE 8.653998096612657e-05\n",
      "53 Train Loss 3.367508e-05 Test MSE 1.9835381390555455e-05 Test RE 8.5276184293006e-05\n",
      "54 Train Loss 3.3455824e-05 Test MSE 1.941379820660558e-05 Test RE 8.436508284835968e-05\n",
      "55 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "56 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "57 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "58 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "59 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "60 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "61 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "62 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "63 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "64 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "65 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "66 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "67 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "68 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "69 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "70 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "71 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "72 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "73 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "74 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "75 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "76 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "77 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "78 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "79 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "80 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "81 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "82 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "83 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "84 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "85 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "86 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "87 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "88 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "89 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "90 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "91 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "92 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "93 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "94 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "95 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "96 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "97 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "98 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "99 Train Loss 3.284638e-05 Test MSE 1.911108464233704e-05 Test RE 8.370475889129784e-05\n",
      "Training time: 13.75\n",
      "Training time: 13.75\n"
     ]
    }
   ],
   "source": [
    "#for tune_reps in range(5):\n",
    "for tune_reps in range(4,5):\n",
    "  label = \"1D_FODE_swish_tune\"+str(tune_reps)  \n",
    "  max_reps = 10\n",
    "  max_iter = 100\n",
    "\n",
    "  train_loss_full = []\n",
    "  test_mse_full = []\n",
    "  test_re_full = []\n",
    "  beta_full = []\n",
    "  elapsed_time= np.zeros((max_reps,1))\n",
    "  time_threshold = np.empty((max_reps,1))\n",
    "  time_threshold[:] = np.nan\n",
    "  epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "  for reps in range(max_reps):   \n",
    "      train_loss = []\n",
    "      test_mse_loss = []\n",
    "      test_re_loss = []   \n",
    "      beta_val = []\n",
    "      \n",
    "      torch.manual_seed(reps*36)\n",
    "      N_f = 10000 #Total number of collocation points\n",
    "    \n",
    "      layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "      PINN = Sequentialmodel(layers)\n",
    "    \n",
    "      PINN.to(device)\n",
    "\n",
    "      'Neural Network Summary'\n",
    "      print(PINN)\n",
    "\n",
    "      params = list(PINN.parameters())\n",
    "      \n",
    "      optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.025, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "    \n",
    "\n",
    "      \n",
    "      train_model(max_iter,reps)\n",
    "\n",
    "      \n",
    "      torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "      train_loss_full.append(train_loss)\n",
    "      test_mse_full.append(test_mse_loss)\n",
    "      test_re_full.append(test_re_loss)\n",
    "      beta_full.append(beta_val)\n",
    "      \n",
    "      \n",
    "      print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "  mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "  savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "7ef8bb86-89cf-4917-9bf1-6cda3bafbc51"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1D_FODE_swish_tune4.mat'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ky6HsA0AWWTD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF7H51LTWXDq",
    "outputId": "bb023909-b810-4d9b-857a-cc29fc84d142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20018594059984918\n"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(4,5):\n",
    "    label = \"1D_FODE_swish_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(i,' ',np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "swish_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
