{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "3df10486-4078-44cd-95da-12a75fb13c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvP0Nx4vNOlZ",
    "outputId": "515a82ba-2a23-4124-c9e1-230f67f43912"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDzAYhTsNbP6",
    "outputId": "d35a8c58-7c75-4550-d489-9565724f04e6"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1wXUvTNETmrW",
    "outputId": "7b44eee8-32ab-4621-ca04-81e30b53601d"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "def true_1D_2(x): #True function for 1D_1 dy/dx = cos(0.01*x) BC1: y(0)=0; x \\in [-100,100]\n",
    "    y = 100*np.sin(0.01*x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "loss_thresh = 0.005\n",
    "label = \"1D_FODE_tanh_\" \n",
    "\n",
    "x = np.linspace(-600,600,5000).reshape(-1,1)\n",
    "ysol = true_1D_2(x)\n",
    "\n",
    "bc1_x = np.array(0).reshape(-1,1) \n",
    "bc1_y = np.array(0).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "\n",
    " \n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "\n",
    "y_true = true_1D_2(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "\n",
    "  #Collocation Points\n",
    "  # Latin Hypercube sampling for collocation points \n",
    "  # N_f sets of tuples(x,y)\n",
    "  x01 = np.array([[0.0, 1.0]])\n",
    "  sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "  x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "  x_coll_train = np.vstack((x_coll_train, bc1_x)) # append training points to collocation points \n",
    "\n",
    "  return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "             \n",
    "      \n",
    "              \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        f = dy_dx - torch.cos(0.01*g)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + 100*loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(seed):\n",
    "    x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_coll_train,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "   \n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fAcpqTqePPt9"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    x_coll_np_array = colloc_pts(N_f,123)\n",
    "    x_coll = torch.from_numpy(x_coll_np_array).float().to(device)\n",
    "\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(i)        \n",
    "    \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "3645d237-1d2a-45c9-8d9f-de486f1ca919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 47.768238 Test MSE 5220.0045811306 Test RE 0.9997435404560636\n",
      "1 Train Loss 47.76683 Test MSE 5215.485107216673 Test RE 0.9993106583394861\n",
      "2 Train Loss 47.749943 Test MSE 5174.956820242627 Test RE 0.9954203839808021\n",
      "3 Train Loss 47.748604 Test MSE 5177.407936872666 Test RE 0.9956560963448615\n",
      "4 Train Loss 47.726677 Test MSE 5129.951429002209 Test RE 0.9910824625762618\n",
      "5 Train Loss 47.689312 Test MSE 5052.146933828339 Test RE 0.9835380161000731\n",
      "6 Train Loss 47.611176 Test MSE 4977.759345948416 Test RE 0.9762703796928455\n",
      "7 Train Loss 47.42509 Test MSE 4759.141742506148 Test RE 0.9545913276104164\n",
      "8 Train Loss 47.19505 Test MSE 4656.25354308514 Test RE 0.9442162599440046\n",
      "9 Train Loss 47.147163 Test MSE 4614.400936822851 Test RE 0.9399631502548936\n",
      "10 Train Loss 47.11147 Test MSE 4594.9320684103595 Test RE 0.9379781295974399\n",
      "11 Train Loss 47.072968 Test MSE 4565.8603625553515 Test RE 0.9350061713213\n",
      "12 Train Loss 47.001945 Test MSE 4543.726011325697 Test RE 0.9327370592017726\n",
      "13 Train Loss 46.83106 Test MSE 4539.636071750976 Test RE 0.9323171728518984\n",
      "14 Train Loss 46.53701 Test MSE 4519.407874616005 Test RE 0.9302376943652642\n",
      "15 Train Loss 46.012806 Test MSE 4484.844733944661 Test RE 0.926673771216601\n",
      "16 Train Loss 45.196274 Test MSE 4400.269145255301 Test RE 0.9178945373950093\n",
      "17 Train Loss 44.260212 Test MSE 4296.4967134037215 Test RE 0.9070065150867075\n",
      "18 Train Loss 43.510643 Test MSE 4228.163261924577 Test RE 0.8997648826132079\n",
      "19 Train Loss 42.34856 Test MSE 4163.408006231978 Test RE 0.8928482488795131\n",
      "20 Train Loss 40.844093 Test MSE 4189.237519697283 Test RE 0.8956135529072606\n",
      "21 Train Loss 40.668472 Test MSE 4275.351390975167 Test RE 0.9047718338852689\n",
      "22 Train Loss 40.644276 Test MSE 4305.959146571882 Test RE 0.9080047432791137\n",
      "23 Train Loss 40.584545 Test MSE 4219.042585568859 Test RE 0.8987939060740114\n",
      "24 Train Loss 40.4001 Test MSE 4155.4971196750685 Test RE 0.891999595613393\n",
      "25 Train Loss 39.39905 Test MSE 3884.601545567106 Test RE 0.8624350613971105\n",
      "26 Train Loss 34.6337 Test MSE 6113.0384817808035 Test RE 1.0818865695395414\n",
      "27 Train Loss 31.447853 Test MSE 6691.494507537935 Test RE 1.1319173780547012\n",
      "28 Train Loss 30.61373 Test MSE 7053.319629809399 Test RE 1.1621172483689322\n",
      "29 Train Loss 29.949873 Test MSE 7076.657753161195 Test RE 1.1640382755619167\n",
      "30 Train Loss 28.876999 Test MSE 6487.517385169393 Test RE 1.114531717014748\n",
      "31 Train Loss 28.861336 Test MSE 6447.974075150447 Test RE 1.1111298273250578\n",
      "32 Train Loss 28.861336 Test MSE 6447.974075150447 Test RE 1.1111298273250578\n",
      "33 Train Loss 28.861336 Test MSE 6447.974075150447 Test RE 1.1111298273250578\n",
      "34 Train Loss 28.861336 Test MSE 6447.974075150447 Test RE 1.1111298273250578\n",
      "35 Train Loss 28.861336 Test MSE 6447.974075150447 Test RE 1.1111298273250578\n",
      "36 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "37 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "38 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "39 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "40 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "41 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "42 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "43 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "44 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "45 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "46 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "47 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "48 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "49 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "50 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "51 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "52 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "53 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "54 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "55 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "56 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "57 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "58 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "59 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "60 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "61 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "62 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "63 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "64 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "65 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "66 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "67 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "68 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "69 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "70 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "71 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "72 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "73 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "74 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "75 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "76 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "77 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "78 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "79 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "80 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "81 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "82 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "83 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "84 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "85 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "86 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "87 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "88 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "89 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "90 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "91 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "92 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "93 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "94 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "95 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "96 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "97 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "98 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "99 Train Loss 28.861221 Test MSE 6447.9030267502 Test RE 1.111123705695749\n",
      "Training time: 21.40\n",
      "Training time: 21.40\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 47.76833 Test MSE 5220.246779464616 Test RE 0.9997667332912018\n",
      "1 Train Loss 47.76683 Test MSE 5215.403358712495 Test RE 0.9993028266162653\n",
      "2 Train Loss 47.74634 Test MSE 5169.998287146971 Test RE 0.9949433744127183\n",
      "3 Train Loss 47.745472 Test MSE 5172.130720942068 Test RE 0.9951485419995706\n",
      "4 Train Loss 47.73545 Test MSE 5162.088103227977 Test RE 0.994181943042029\n",
      "5 Train Loss 47.320904 Test MSE 4937.630695207722 Test RE 0.9723272713310455\n",
      "6 Train Loss 45.09703 Test MSE 5032.301825908832 Test RE 0.981604420005369\n",
      "7 Train Loss 39.49209 Test MSE 3591.2789506052623 Test RE 0.8292352107240546\n",
      "8 Train Loss 37.12409 Test MSE 3384.402134727949 Test RE 0.8049967830551102\n",
      "9 Train Loss 34.81187 Test MSE 2953.7969472592463 Test RE 0.752044402850827\n",
      "10 Train Loss 34.250458 Test MSE 2713.4698363884495 Test RE 0.7208014701928259\n",
      "11 Train Loss 30.888367 Test MSE 2432.6465318995424 Test RE 0.6824843067592082\n",
      "12 Train Loss 25.50415 Test MSE 1805.3650509191475 Test RE 0.5879436174441085\n",
      "13 Train Loss 22.681288 Test MSE 1489.208111729003 Test RE 0.5339872177168826\n",
      "14 Train Loss 19.136766 Test MSE 1012.282215347087 Test RE 0.44025466661843515\n",
      "15 Train Loss 18.868792 Test MSE 930.339672449641 Test RE 0.42205974786452577\n",
      "16 Train Loss 18.777885 Test MSE 964.000541811404 Test RE 0.42962723425376964\n",
      "17 Train Loss 18.472532 Test MSE 1017.0572626959625 Test RE 0.4412918099939361\n",
      "18 Train Loss 16.54806 Test MSE 840.7784046437664 Test RE 0.4012304998551407\n",
      "19 Train Loss 15.491176 Test MSE 663.8790283563154 Test RE 0.3565312809613511\n",
      "20 Train Loss 15.242416 Test MSE 634.524871857559 Test RE 0.34855995408053836\n",
      "21 Train Loss 14.99356 Test MSE 640.6911151833444 Test RE 0.3502494930769082\n",
      "22 Train Loss 14.7611475 Test MSE 667.141329793163 Test RE 0.3574062047628966\n",
      "23 Train Loss 14.596642 Test MSE 701.4024116835386 Test RE 0.3664686194215364\n",
      "24 Train Loss 14.509479 Test MSE 725.1385152211334 Test RE 0.3726178461704093\n",
      "25 Train Loss 14.422575 Test MSE 731.7709642630928 Test RE 0.3743180341027539\n",
      "26 Train Loss 14.298174 Test MSE 709.8174051820397 Test RE 0.3686603973475154\n",
      "27 Train Loss 14.239183 Test MSE 700.1225556290096 Test RE 0.36613411726657735\n",
      "28 Train Loss 14.201435 Test MSE 696.742816103989 Test RE 0.36524931864032645\n",
      "29 Train Loss 14.039855 Test MSE 710.8911422958205 Test RE 0.3689391273161776\n",
      "30 Train Loss 13.901506 Test MSE 706.6179558651322 Test RE 0.3678286042761143\n",
      "31 Train Loss 13.748828 Test MSE 674.1893897626363 Test RE 0.3592891657451451\n",
      "32 Train Loss 13.592473 Test MSE 576.4853891341601 Test RE 0.3322364801455933\n",
      "33 Train Loss 12.328708 Test MSE 428.3758358548294 Test RE 0.28639514210267164\n",
      "34 Train Loss 10.39299 Test MSE 686.8481488011264 Test RE 0.3626465337269537\n",
      "35 Train Loss 9.837425 Test MSE 618.5959651009312 Test RE 0.34415707829477515\n",
      "36 Train Loss 9.27075 Test MSE 606.021048180561 Test RE 0.34064107825558376\n",
      "37 Train Loss 9.166477 Test MSE 594.6351258407539 Test RE 0.33742592309064795\n",
      "38 Train Loss 9.09017 Test MSE 565.6837990569994 Test RE 0.3291092093165113\n",
      "39 Train Loss 9.065787 Test MSE 576.7103471595872 Test RE 0.3323012970291857\n",
      "40 Train Loss 9.037477 Test MSE 563.2578280014327 Test RE 0.3284027481582301\n",
      "41 Train Loss 8.885834 Test MSE 600.4247909144514 Test RE 0.339064617763675\n",
      "42 Train Loss 8.504222 Test MSE 661.1019466577437 Test RE 0.35578479397583473\n",
      "43 Train Loss 8.45256 Test MSE 752.5031328136897 Test RE 0.3795834955323516\n",
      "44 Train Loss 8.365433 Test MSE 716.0819846270314 Test RE 0.370283652141999\n",
      "45 Train Loss 8.242164 Test MSE 710.3483183763793 Test RE 0.36879824271635997\n",
      "46 Train Loss 8.150846 Test MSE 736.0186537629139 Test RE 0.3754028585872049\n",
      "47 Train Loss 8.096612 Test MSE 768.6786535160198 Test RE 0.38364149529082714\n",
      "48 Train Loss 8.0462265 Test MSE 783.6542205910208 Test RE 0.38736056285840886\n",
      "49 Train Loss 7.9759254 Test MSE 786.7782635789105 Test RE 0.38813190267094916\n",
      "50 Train Loss 7.9048862 Test MSE 808.5136190218677 Test RE 0.3934565998487541\n",
      "51 Train Loss 7.8546247 Test MSE 821.7922722332735 Test RE 0.3966744161557008\n",
      "52 Train Loss 7.7316885 Test MSE 794.5933184077301 Test RE 0.3900547931848599\n",
      "53 Train Loss 7.6278863 Test MSE 696.1619627605645 Test RE 0.3650970382590174\n",
      "54 Train Loss 7.5740876 Test MSE 665.6220827688117 Test RE 0.35699902132873423\n",
      "55 Train Loss 7.556916 Test MSE 627.5162268040675 Test RE 0.3466295991991161\n",
      "56 Train Loss 7.5137463 Test MSE 579.9959862762704 Test RE 0.33324654751317423\n",
      "57 Train Loss 7.415029 Test MSE 462.9957026541941 Test RE 0.2977430581370461\n",
      "58 Train Loss 7.2461743 Test MSE 439.86911177843467 Test RE 0.29021168704399625\n",
      "59 Train Loss 7.2098827 Test MSE 460.11533789733693 Test RE 0.29681546140233184\n",
      "60 Train Loss 6.9019837 Test MSE 405.85363799639225 Test RE 0.2787647694190289\n",
      "61 Train Loss 6.8087673 Test MSE 444.32625290441075 Test RE 0.29167832126687837\n",
      "62 Train Loss 6.7670245 Test MSE 429.8033582138103 Test RE 0.2868719377100768\n",
      "63 Train Loss 6.755929 Test MSE 423.9633443308238 Test RE 0.28491631541425844\n",
      "64 Train Loss 6.5661616 Test MSE 503.16665853317875 Test RE 0.3103909802643516\n",
      "65 Train Loss 6.28125 Test MSE 466.0675544110604 Test RE 0.29872914772099257\n",
      "66 Train Loss 5.9888744 Test MSE 464.5062459870402 Test RE 0.2982283623749981\n",
      "67 Train Loss 5.9620404 Test MSE 451.60909705870887 Test RE 0.2940590199647011\n",
      "68 Train Loss 5.9013476 Test MSE 461.2456622181229 Test RE 0.29717981780718367\n",
      "69 Train Loss 5.8751717 Test MSE 457.26998327141666 Test RE 0.2958962842994876\n",
      "70 Train Loss 5.7046647 Test MSE 449.19653254763057 Test RE 0.2932725141360254\n",
      "71 Train Loss 5.623982 Test MSE 438.162839279819 Test RE 0.2896482678956049\n",
      "72 Train Loss 5.554071 Test MSE 501.9094822275866 Test RE 0.31000297737724775\n",
      "73 Train Loss 5.5362225 Test MSE 500.25792765308944 Test RE 0.3094925180946932\n",
      "74 Train Loss 5.535263 Test MSE 501.40285664938176 Test RE 0.30984647994491027\n",
      "75 Train Loss 5.535263 Test MSE 501.40285664938176 Test RE 0.30984647994491027\n",
      "76 Train Loss 5.5348554 Test MSE 502.0096758265888 Test RE 0.31003391798024166\n",
      "77 Train Loss 5.5346956 Test MSE 501.996940704168 Test RE 0.31002998544155663\n",
      "78 Train Loss 5.5335245 Test MSE 503.4193998021297 Test RE 0.31046892537474025\n",
      "79 Train Loss 5.533327 Test MSE 504.0629011315937 Test RE 0.3106672921461212\n",
      "80 Train Loss 5.533327 Test MSE 504.0629011315937 Test RE 0.3106672921461212\n",
      "81 Train Loss 5.533327 Test MSE 504.0629011315937 Test RE 0.3106672921461212\n",
      "82 Train Loss 5.533327 Test MSE 504.0629011315937 Test RE 0.3106672921461212\n",
      "83 Train Loss 5.5332303 Test MSE 504.17970921259 Test RE 0.31070328601521646\n",
      "84 Train Loss 5.530014 Test MSE 506.9912611083923 Test RE 0.3115683981308368\n",
      "85 Train Loss 5.530014 Test MSE 506.9912611083923 Test RE 0.3115683981308368\n",
      "86 Train Loss 5.529721 Test MSE 506.9285569598005 Test RE 0.31154913030835485\n",
      "87 Train Loss 5.529721 Test MSE 506.9285569598005 Test RE 0.31154913030835485\n",
      "88 Train Loss 5.529721 Test MSE 506.9285569598005 Test RE 0.31154913030835485\n",
      "89 Train Loss 5.529721 Test MSE 506.928857434222 Test RE 0.3115492226414159\n",
      "90 Train Loss 5.519842 Test MSE 504.06004951811303 Test RE 0.31066641338248946\n",
      "91 Train Loss 5.465199 Test MSE 512.2454721637829 Test RE 0.3131787085011676\n",
      "92 Train Loss 5.3678975 Test MSE 569.9619047814609 Test RE 0.3303513449455815\n",
      "93 Train Loss 5.34777 Test MSE 603.480193408691 Test RE 0.33992622797846533\n",
      "94 Train Loss 5.327325 Test MSE 585.0977682826859 Test RE 0.3347089963061706\n",
      "95 Train Loss 5.296159 Test MSE 585.0514135330807 Test RE 0.33469573727505564\n",
      "96 Train Loss 5.2567267 Test MSE 604.3251398377845 Test RE 0.34016411398558627\n",
      "97 Train Loss 5.142667 Test MSE 633.1165328707203 Test RE 0.3481729217654089\n",
      "98 Train Loss 4.979185 Test MSE 653.9318635761229 Test RE 0.35385017470601965\n",
      "99 Train Loss 4.886111 Test MSE 775.2444808495654 Test RE 0.38527648780596074\n",
      "Training time: 42.18\n",
      "Training time: 42.18\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 47.76986 Test MSE 5223.414372220588 Test RE 1.0000700114443049\n",
      "1 Train Loss 47.749126 Test MSE 5172.878094242078 Test RE 0.9952204389240698\n",
      "2 Train Loss 47.74749 Test MSE 5173.7957546073485 Test RE 0.9953087102727373\n",
      "3 Train Loss 47.73213 Test MSE 5139.70777402239 Test RE 0.9920244548703359\n",
      "4 Train Loss 47.716015 Test MSE 5102.915274790106 Test RE 0.9884673836417674\n",
      "5 Train Loss 47.53399 Test MSE 4933.081081836458 Test RE 0.9718792089993209\n",
      "6 Train Loss 47.374886 Test MSE 4816.201787099489 Test RE 0.960296844816548\n",
      "7 Train Loss 47.25087 Test MSE 4739.830773552604 Test RE 0.9526526564880398\n",
      "8 Train Loss 47.08384 Test MSE 4661.3479504694715 Test RE 0.9447326522802131\n",
      "9 Train Loss 46.953 Test MSE 4605.147642990859 Test RE 0.9390202196192485\n",
      "10 Train Loss 46.585182 Test MSE 4560.609716291024 Test RE 0.9344683976591216\n",
      "11 Train Loss 44.498814 Test MSE 4478.251681154761 Test RE 0.9259923812799068\n",
      "12 Train Loss 42.410885 Test MSE 4417.653240630643 Test RE 0.9197059080925345\n",
      "13 Train Loss 40.864193 Test MSE 4602.477086325784 Test RE 0.9387479080056905\n",
      "14 Train Loss 40.196873 Test MSE 5017.173723435953 Test RE 0.9801278601776293\n",
      "15 Train Loss 39.26086 Test MSE 5671.690841494181 Test RE 1.0421001048625076\n",
      "16 Train Loss 37.772537 Test MSE 5410.147165676652 Test RE 1.017788885834932\n",
      "17 Train Loss 27.362692 Test MSE 4585.438384365037 Test RE 0.9370086405754106\n",
      "18 Train Loss 21.516733 Test MSE 3129.5044605613984 Test RE 0.774089099926537\n",
      "19 Train Loss 16.961082 Test MSE 2353.4379496933516 Test RE 0.6712812871328134\n",
      "20 Train Loss 15.843013 Test MSE 2302.249051797064 Test RE 0.6639407367921096\n",
      "21 Train Loss 11.058365 Test MSE 4331.444434056744 Test RE 0.9106878414935515\n",
      "22 Train Loss 9.914853 Test MSE 3653.8372442788987 Test RE 0.8364264614579675\n",
      "23 Train Loss 9.041231 Test MSE 2820.6979513505757 Test RE 0.734905429947902\n",
      "24 Train Loss 8.596731 Test MSE 2603.966374675685 Test RE 0.7061075425495218\n",
      "25 Train Loss 7.864435 Test MSE 2434.534989853701 Test RE 0.6827491608731079\n",
      "26 Train Loss 7.239247 Test MSE 2477.668773245923 Test RE 0.6887708969767734\n",
      "27 Train Loss 6.6401267 Test MSE 2679.3568331633173 Test RE 0.7162562799735449\n",
      "28 Train Loss 5.8083444 Test MSE 2312.191010570947 Test RE 0.6653727626899433\n",
      "29 Train Loss 5.3759565 Test MSE 2141.1304281421503 Test RE 0.6402870620860721\n",
      "30 Train Loss 5.0812125 Test MSE 1878.4043612184084 Test RE 0.5997188625591761\n",
      "31 Train Loss 5.0020895 Test MSE 1722.1152129977863 Test RE 0.5742278690382014\n",
      "32 Train Loss 4.9733653 Test MSE 1749.4784788357363 Test RE 0.5787719391541462\n",
      "33 Train Loss 4.857356 Test MSE 1855.6717819733612 Test RE 0.5960788967206714\n",
      "34 Train Loss 4.7304616 Test MSE 1704.7417609909373 Test RE 0.5713239955501989\n",
      "35 Train Loss 4.5154047 Test MSE 1437.2904164633776 Test RE 0.5245965493341307\n",
      "36 Train Loss 4.3508835 Test MSE 1193.0679371996807 Test RE 0.47795361632299477\n",
      "37 Train Loss 4.3264 Test MSE 1090.8418381186589 Test RE 0.45701879188312594\n",
      "38 Train Loss 4.189759 Test MSE 909.9954900819488 Test RE 0.41741954914983403\n",
      "39 Train Loss 4.118834 Test MSE 949.2392884415127 Test RE 0.42632521252798755\n",
      "40 Train Loss 4.0813346 Test MSE 991.4696397466605 Test RE 0.43570533198164185\n",
      "41 Train Loss 4.0190344 Test MSE 972.2263118565386 Test RE 0.4314563348510622\n",
      "42 Train Loss 3.9254112 Test MSE 1029.9089933770067 Test RE 0.4440711813732271\n",
      "43 Train Loss 3.8801131 Test MSE 1068.430746525544 Test RE 0.4522997558855104\n",
      "44 Train Loss 3.866562 Test MSE 1097.3369774603902 Test RE 0.45837737343520574\n",
      "45 Train Loss 3.8162036 Test MSE 1139.5372417398808 Test RE 0.46710813000907736\n",
      "46 Train Loss 3.778903 Test MSE 1083.7725328651534 Test RE 0.4555355077582591\n",
      "47 Train Loss 3.7707937 Test MSE 1103.427061461186 Test RE 0.45964758225801794\n",
      "48 Train Loss 3.7551255 Test MSE 1101.5404882562234 Test RE 0.4592544752671232\n",
      "49 Train Loss 3.7246704 Test MSE 1028.5177758325558 Test RE 0.44377115078949625\n",
      "50 Train Loss 3.682619 Test MSE 952.6821116304837 Test RE 0.4270976383463758\n",
      "51 Train Loss 3.0648017 Test MSE 1140.6466047873205 Test RE 0.467335444406181\n",
      "52 Train Loss 2.9289575 Test MSE 1161.0571414103151 Test RE 0.47149811597311125\n",
      "53 Train Loss 2.8749063 Test MSE 1185.2850318244305 Test RE 0.4763921150437452\n",
      "54 Train Loss 2.8303516 Test MSE 1230.2125500881796 Test RE 0.4853368211492721\n",
      "55 Train Loss 2.5882723 Test MSE 1058.4239201121845 Test RE 0.45017667359564206\n",
      "56 Train Loss 2.355774 Test MSE 1088.3537238546223 Test RE 0.45649728450661725\n",
      "57 Train Loss 2.3111024 Test MSE 1119.5865151874018 Test RE 0.46300106945938657\n",
      "58 Train Loss 2.2848477 Test MSE 1130.6458946144357 Test RE 0.46528223366630994\n",
      "59 Train Loss 2.2444487 Test MSE 1136.4889009978108 Test RE 0.46648293838385424\n",
      "60 Train Loss 2.109244 Test MSE 1116.424647728604 Test RE 0.4623468176194683\n",
      "61 Train Loss 2.0749521 Test MSE 1093.711427373512 Test RE 0.4576195182285152\n",
      "62 Train Loss 2.0435853 Test MSE 1064.870893796533 Test RE 0.4515456293615468\n",
      "63 Train Loss 2.030922 Test MSE 1042.0246624792376 Test RE 0.44667553230449303\n",
      "64 Train Loss 2.0070126 Test MSE 1018.9011073694704 Test RE 0.4416916424987269\n",
      "65 Train Loss 1.9672426 Test MSE 1047.1401812395889 Test RE 0.44777060214855463\n",
      "66 Train Loss 1.9290314 Test MSE 1042.398222734572 Test RE 0.44675559052190533\n",
      "67 Train Loss 1.9017985 Test MSE 1044.6973837961916 Test RE 0.44724801139798914\n",
      "68 Train Loss 1.8840988 Test MSE 1044.3532746259127 Test RE 0.4471743466124224\n",
      "69 Train Loss 1.874794 Test MSE 1073.751752035728 Test RE 0.4534246301400843\n",
      "70 Train Loss 1.8712976 Test MSE 1089.0066190310886 Test RE 0.456634188613838\n",
      "71 Train Loss 1.856122 Test MSE 1080.048037232061 Test RE 0.4547520868201032\n",
      "72 Train Loss 1.8443543 Test MSE 1105.714233406137 Test RE 0.4601237119805869\n",
      "73 Train Loss 1.8196052 Test MSE 1154.567611068124 Test RE 0.4701785905292985\n",
      "74 Train Loss 1.772023 Test MSE 1122.9172599038877 Test RE 0.4636892668836595\n",
      "75 Train Loss 1.731652 Test MSE 1089.3691779431929 Test RE 0.45671019504690386\n",
      "76 Train Loss 1.6852574 Test MSE 1090.7046592828913 Test RE 0.45699005477627547\n",
      "77 Train Loss 1.6553116 Test MSE 1082.1677604680472 Test RE 0.45519812071366633\n",
      "78 Train Loss 1.6512111 Test MSE 1071.9931290473567 Test RE 0.45305316171320775\n",
      "79 Train Loss 1.6484373 Test MSE 1059.611485791083 Test RE 0.450429154893454\n",
      "80 Train Loss 1.6431918 Test MSE 1062.6144194428553 Test RE 0.45106696031503624\n",
      "81 Train Loss 1.6256658 Test MSE 1069.3157201206704 Test RE 0.4524870354498436\n",
      "82 Train Loss 1.5811868 Test MSE 1073.4830767105466 Test RE 0.45336789839021524\n",
      "83 Train Loss 1.5507648 Test MSE 1087.185867364127 Test RE 0.4562522968713874\n",
      "84 Train Loss 1.5385772 Test MSE 1066.3665996926904 Test RE 0.45186263608646016\n",
      "85 Train Loss 1.5311002 Test MSE 1069.289652095056 Test RE 0.45248151999949654\n",
      "86 Train Loss 1.5190449 Test MSE 1098.0941696078924 Test RE 0.4585354925462602\n",
      "87 Train Loss 1.5067333 Test MSE 1104.0925653148422 Test RE 0.45978617371475117\n",
      "88 Train Loss 1.4938176 Test MSE 1149.8588821386195 Test RE 0.4692188349320212\n",
      "89 Train Loss 1.4846501 Test MSE 1161.9548591144448 Test RE 0.471680359553225\n",
      "90 Train Loss 1.4779655 Test MSE 1153.600663438106 Test RE 0.4699816625629777\n",
      "91 Train Loss 1.4774157 Test MSE 1152.9917312507878 Test RE 0.46985760544900995\n",
      "92 Train Loss 1.47703 Test MSE 1158.909750358882 Test RE 0.47106189307493557\n",
      "93 Train Loss 1.4769887 Test MSE 1159.6809174987736 Test RE 0.47121859511364\n",
      "94 Train Loss 1.4769887 Test MSE 1159.6809174987736 Test RE 0.47121859511364\n",
      "95 Train Loss 1.4769887 Test MSE 1159.6809174987736 Test RE 0.47121859511364\n",
      "96 Train Loss 1.4769887 Test MSE 1159.6809174987736 Test RE 0.47121859511364\n",
      "97 Train Loss 1.475735 Test MSE 1164.0870783888704 Test RE 0.4721129344442519\n",
      "98 Train Loss 1.4663056 Test MSE 1162.8212226547694 Test RE 0.4718561712548441\n",
      "99 Train Loss 1.456817 Test MSE 1189.822989906221 Test RE 0.47730319640431157\n",
      "Training time: 68.77\n",
      "Training time: 68.77\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 47.768715 Test MSE 5221.043838829687 Test RE 0.9998430556429755\n",
      "1 Train Loss 47.743134 Test MSE 5164.947947156842 Test RE 0.9944572978457233\n",
      "2 Train Loss 47.74143 Test MSE 5163.230824017293 Test RE 0.9942919769552846\n",
      "3 Train Loss 47.720394 Test MSE 5126.454117811679 Test RE 0.9907445729501702\n",
      "4 Train Loss 47.36402 Test MSE 4996.093078703556 Test RE 0.9780665924550836\n",
      "5 Train Loss 45.78136 Test MSE 4709.86349938025 Test RE 0.9496363388402131\n",
      "6 Train Loss 44.16574 Test MSE 4562.935694415078 Test RE 0.9347066636060455\n",
      "7 Train Loss 43.278908 Test MSE 4443.833509434002 Test RE 0.9224271017232776\n",
      "8 Train Loss 41.14781 Test MSE 4647.806940145601 Test RE 0.9433594509270246\n",
      "9 Train Loss 35.73835 Test MSE 4810.049962617007 Test RE 0.9596833462992305\n",
      "10 Train Loss 33.18985 Test MSE 5604.415137707833 Test RE 1.0359011457118938\n",
      "11 Train Loss 23.761715 Test MSE 3857.932682287492 Test RE 0.8594695353092996\n",
      "12 Train Loss 14.251568 Test MSE 1675.505427599507 Test RE 0.566403703251464\n",
      "13 Train Loss 13.20132 Test MSE 1296.4286280985882 Test RE 0.49822724300339893\n",
      "14 Train Loss 12.984649 Test MSE 1227.8874740535603 Test RE 0.48487796601065436\n",
      "15 Train Loss 12.407527 Test MSE 1146.4255466638342 Test RE 0.4685177967238512\n",
      "16 Train Loss 11.987143 Test MSE 1149.0897554741603 Test RE 0.4690618812867813\n",
      "17 Train Loss 11.686062 Test MSE 1144.651537912682 Test RE 0.46815515805948527\n",
      "18 Train Loss 11.427096 Test MSE 1137.1194905773975 Test RE 0.46661233625431514\n",
      "19 Train Loss 11.203027 Test MSE 1174.2785971988942 Test RE 0.4741750922418346\n",
      "20 Train Loss 10.95072 Test MSE 1228.7489392642262 Test RE 0.48504802730285806\n",
      "21 Train Loss 10.229216 Test MSE 1154.517742115348 Test RE 0.47016843626596366\n",
      "22 Train Loss 9.859428 Test MSE 1006.086898536089 Test RE 0.43890538717975786\n",
      "23 Train Loss 9.257697 Test MSE 868.8272096043027 Test RE 0.4078682236360788\n",
      "24 Train Loss 8.959707 Test MSE 816.6383329325881 Test RE 0.39542857125896763\n",
      "25 Train Loss 8.357288 Test MSE 737.729386633088 Test RE 0.3758388810059203\n",
      "26 Train Loss 8.065607 Test MSE 663.3938631648015 Test RE 0.35640097997702513\n",
      "27 Train Loss 7.5249467 Test MSE 556.8678978072329 Test RE 0.32653463722811255\n",
      "28 Train Loss 7.3666234 Test MSE 548.6474191830696 Test RE 0.3241155257011695\n",
      "29 Train Loss 7.2261863 Test MSE 535.5067580353563 Test RE 0.3202105545001076\n",
      "30 Train Loss 7.0181684 Test MSE 600.206805880014 Test RE 0.339003063241702\n",
      "31 Train Loss 6.8580933 Test MSE 618.0030496931001 Test RE 0.34399210392895585\n",
      "32 Train Loss 6.6345315 Test MSE 553.6120897300897 Test RE 0.3255786720751389\n",
      "33 Train Loss 6.5165634 Test MSE 557.8710939645007 Test RE 0.32682863055618755\n",
      "34 Train Loss 6.3625584 Test MSE 529.8505015876162 Test RE 0.31851496330021345\n",
      "35 Train Loss 6.0633397 Test MSE 528.570217801215 Test RE 0.31812991489815806\n",
      "36 Train Loss 5.924265 Test MSE 558.8446405883425 Test RE 0.3271136822627678\n",
      "37 Train Loss 5.796882 Test MSE 594.142991386213 Test RE 0.33728626324865935\n",
      "38 Train Loss 5.679658 Test MSE 632.8727268090917 Test RE 0.3481058765705568\n",
      "39 Train Loss 5.6165776 Test MSE 648.0783085955284 Test RE 0.3522629011630945\n",
      "40 Train Loss 5.58569 Test MSE 661.2070159227126 Test RE 0.3558130653807295\n",
      "41 Train Loss 5.5490847 Test MSE 645.940053771199 Test RE 0.3516812969336255\n",
      "42 Train Loss 5.4975257 Test MSE 652.928513089338 Test RE 0.35357860815961367\n",
      "43 Train Loss 5.446078 Test MSE 663.6990598144348 Test RE 0.3564829523042145\n",
      "44 Train Loss 5.402498 Test MSE 674.0517242502953 Test RE 0.3592524815040116\n",
      "45 Train Loss 5.36024 Test MSE 683.0434226106606 Test RE 0.3616407169003727\n",
      "46 Train Loss 5.300119 Test MSE 694.4579282067451 Test RE 0.3646499302732906\n",
      "47 Train Loss 5.252525 Test MSE 688.2101444263403 Test RE 0.3630059132912762\n",
      "48 Train Loss 5.233634 Test MSE 686.3827880623937 Test RE 0.3625236608225964\n",
      "49 Train Loss 5.2146473 Test MSE 697.9312297579357 Test RE 0.3655606834158862\n",
      "50 Train Loss 5.2027645 Test MSE 719.5278461074481 Test RE 0.3711735047613023\n",
      "51 Train Loss 5.189824 Test MSE 737.0905964064573 Test RE 0.3756761287814271\n",
      "52 Train Loss 5.179868 Test MSE 744.0297860453696 Test RE 0.377440349571187\n",
      "53 Train Loss 5.1735215 Test MSE 745.1255134147387 Test RE 0.37771817417919884\n",
      "54 Train Loss 5.164313 Test MSE 742.2577486941146 Test RE 0.3769906114533203\n",
      "55 Train Loss 5.1577544 Test MSE 735.6217885714511 Test RE 0.37530163532656485\n",
      "56 Train Loss 5.133648 Test MSE 733.7598059539798 Test RE 0.37482635856679886\n",
      "57 Train Loss 5.1174774 Test MSE 727.2170374452858 Test RE 0.373151496123727\n",
      "58 Train Loss 5.0834284 Test MSE 749.4930334793885 Test RE 0.37882354591530243\n",
      "59 Train Loss 5.07041 Test MSE 762.4607284163483 Test RE 0.38208668579304617\n",
      "60 Train Loss 5.057334 Test MSE 755.009690697528 Test RE 0.3802151586908523\n",
      "61 Train Loss 5.0444283 Test MSE 758.2381842810767 Test RE 0.38102720969028425\n",
      "62 Train Loss 5.037984 Test MSE 759.7005929522265 Test RE 0.3813944749443667\n",
      "63 Train Loss 5.023293 Test MSE 771.2610965292791 Test RE 0.3842853934767134\n",
      "64 Train Loss 5.003379 Test MSE 758.4988865340252 Test RE 0.3810927076545523\n",
      "65 Train Loss 4.9544144 Test MSE 766.5031455980047 Test RE 0.3830982211448728\n",
      "66 Train Loss 4.9281955 Test MSE 775.6187424806096 Test RE 0.38536947576748604\n",
      "67 Train Loss 4.9063225 Test MSE 768.0759222785583 Test RE 0.3834910565749296\n",
      "68 Train Loss 4.890374 Test MSE 755.4878280702679 Test RE 0.3803355321816262\n",
      "69 Train Loss 4.886763 Test MSE 754.0458185655184 Test RE 0.379972383108087\n",
      "70 Train Loss 4.8837814 Test MSE 754.0600232843099 Test RE 0.37997596205203105\n",
      "71 Train Loss 4.882194 Test MSE 758.0363961578093 Test RE 0.3809765053843979\n",
      "72 Train Loss 4.8814845 Test MSE 758.3276303554878 Test RE 0.38104968309231546\n",
      "73 Train Loss 4.879284 Test MSE 760.2938310528311 Test RE 0.3815433583222181\n",
      "74 Train Loss 4.8775973 Test MSE 761.6666781763012 Test RE 0.3818876755057437\n",
      "75 Train Loss 4.8725443 Test MSE 768.2061690967051 Test RE 0.3835235705303395\n",
      "76 Train Loss 4.8607793 Test MSE 757.0294063989564 Test RE 0.3807233731242698\n",
      "77 Train Loss 4.8510723 Test MSE 752.3556615593983 Test RE 0.3795462994101249\n",
      "78 Train Loss 4.807148 Test MSE 766.188738577748 Test RE 0.38301964278226003\n",
      "79 Train Loss 4.7624683 Test MSE 759.840327319105 Test RE 0.3814295489363928\n",
      "80 Train Loss 4.755613 Test MSE 760.9363738761417 Test RE 0.38170455006237153\n",
      "81 Train Loss 4.742447 Test MSE 762.437039687743 Test RE 0.38208075026235644\n",
      "82 Train Loss 4.729192 Test MSE 763.1915874702 Test RE 0.38226976710856153\n",
      "83 Train Loss 4.721772 Test MSE 762.7345267408695 Test RE 0.38215528296824525\n",
      "84 Train Loss 4.7112966 Test MSE 767.5638995853731 Test RE 0.3833632118833752\n",
      "85 Train Loss 4.7015224 Test MSE 769.5051710916363 Test RE 0.3838476941013232\n",
      "86 Train Loss 4.695543 Test MSE 778.943532894899 Test RE 0.38619456064392654\n",
      "87 Train Loss 4.6878304 Test MSE 778.0251534954598 Test RE 0.3859668305737533\n",
      "88 Train Loss 4.6820946 Test MSE 766.9536130814922 Test RE 0.3832107764194835\n",
      "89 Train Loss 4.6795564 Test MSE 756.5196118850723 Test RE 0.38059515923181153\n",
      "90 Train Loss 4.6769686 Test MSE 749.0419445257664 Test RE 0.3787095296210102\n",
      "91 Train Loss 4.6748724 Test MSE 743.5625265159797 Test RE 0.37732181244970964\n",
      "92 Train Loss 4.6734653 Test MSE 743.1064508753299 Test RE 0.377206076598718\n",
      "93 Train Loss 4.6728215 Test MSE 745.1432877500531 Test RE 0.3777226792252109\n",
      "94 Train Loss 4.6706624 Test MSE 743.4892790322567 Test RE 0.3773032272245218\n",
      "95 Train Loss 4.666827 Test MSE 745.1257116632095 Test RE 0.3777182244271397\n",
      "96 Train Loss 4.6631646 Test MSE 740.9889136515907 Test RE 0.37666825476822097\n",
      "97 Train Loss 4.657728 Test MSE 734.7587807539019 Test RE 0.3750814248190463\n",
      "98 Train Loss 4.6531844 Test MSE 725.5595044526989 Test RE 0.37272599470758727\n",
      "99 Train Loss 4.6496196 Test MSE 724.5965743889737 Test RE 0.3724785799712867\n",
      "Training time: 85.38\n",
      "Training time: 85.38\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 47.770046 Test MSE 5223.777237876034 Test RE 1.0001047477998886\n",
      "1 Train Loss 47.75167 Test MSE 5184.9343322450695 Test RE 0.9963795259169129\n",
      "2 Train Loss 47.74747 Test MSE 5169.320956164997 Test RE 0.9948781975974441\n",
      "3 Train Loss 47.720303 Test MSE 5107.116251177536 Test RE 0.9888741779567803\n",
      "4 Train Loss 47.675224 Test MSE 5041.676962192721 Test RE 0.9825183549593651\n",
      "5 Train Loss 47.543373 Test MSE 4774.745057925337 Test RE 0.9561549079626939\n",
      "6 Train Loss 47.478046 Test MSE 4716.798159766254 Test RE 0.950335189586729\n",
      "7 Train Loss 47.421055 Test MSE 4784.9206953069415 Test RE 0.9571732144172671\n",
      "8 Train Loss 46.043327 Test MSE 4844.739039419056 Test RE 0.9631376475990987\n",
      "9 Train Loss 44.891804 Test MSE 4452.242083434956 Test RE 0.9232993926069077\n",
      "10 Train Loss 40.512493 Test MSE 4682.353928472449 Test RE 0.9468589391674554\n",
      "11 Train Loss 35.58157 Test MSE 4399.0527433530115 Test RE 0.9177676581294515\n",
      "12 Train Loss 34.7577 Test MSE 4282.309447173213 Test RE 0.9055077843767687\n",
      "13 Train Loss 32.85485 Test MSE 4250.452984659881 Test RE 0.9021334225876727\n",
      "14 Train Loss 31.656115 Test MSE 4368.884396745483 Test RE 0.9146152560534133\n",
      "15 Train Loss 30.17976 Test MSE 4417.500493403681 Test RE 0.9196900078244737\n",
      "16 Train Loss 28.87459 Test MSE 4528.099956859109 Test RE 0.9311318179692787\n",
      "17 Train Loss 27.937164 Test MSE 4636.93027198965 Test RE 0.9422549926563921\n",
      "18 Train Loss 26.500393 Test MSE 4990.911409551337 Test RE 0.9775592628117756\n",
      "19 Train Loss 25.235666 Test MSE 5719.219519298839 Test RE 1.0464573864578248\n",
      "20 Train Loss 24.90303 Test MSE 6023.277889474522 Test RE 1.0739142737868157\n",
      "21 Train Loss 23.850737 Test MSE 7055.091836115162 Test RE 1.16226323510089\n",
      "22 Train Loss 14.810964 Test MSE 1213.8966959608886 Test RE 0.48210765721776155\n",
      "23 Train Loss 13.73105 Test MSE 763.5668937091597 Test RE 0.38236374783195287\n",
      "24 Train Loss 12.736686 Test MSE 673.4793035712296 Test RE 0.35909990624909494\n",
      "25 Train Loss 12.228172 Test MSE 712.9465305300331 Test RE 0.36947209626362154\n",
      "26 Train Loss 11.880885 Test MSE 643.0783438706081 Test RE 0.35090140490302546\n",
      "27 Train Loss 11.783964 Test MSE 587.4064296746207 Test RE 0.33536868860528934\n",
      "28 Train Loss 10.513829 Test MSE 1259.8447743800316 Test RE 0.4911472131698347\n",
      "29 Train Loss 9.671461 Test MSE 1466.6274786153629 Test RE 0.5299233710564867\n",
      "30 Train Loss 9.317682 Test MSE 1506.6763092369324 Test RE 0.5371098840352945\n",
      "31 Train Loss 9.245513 Test MSE 1438.210364118039 Test RE 0.5247644083003132\n",
      "32 Train Loss 8.809707 Test MSE 1238.0882976366706 Test RE 0.48688789137075716\n",
      "33 Train Loss 8.506209 Test MSE 1162.4552756926385 Test RE 0.47178191739852254\n",
      "34 Train Loss 7.8503456 Test MSE 687.5763438172854 Test RE 0.36283872136147394\n",
      "35 Train Loss 7.674722 Test MSE 621.0344642843783 Test RE 0.34483474301611444\n",
      "36 Train Loss 7.626928 Test MSE 676.194519255872 Test RE 0.35982305611148707\n",
      "37 Train Loss 7.2646317 Test MSE 614.1176861626855 Test RE 0.34290906583411457\n",
      "38 Train Loss 7.0243115 Test MSE 598.5012870229466 Test RE 0.3385210731798897\n",
      "39 Train Loss 6.811865 Test MSE 511.8881148134538 Test RE 0.31306944815097126\n",
      "40 Train Loss 6.8080063 Test MSE 509.7342969951827 Test RE 0.312410119166795\n",
      "41 Train Loss 6.7965703 Test MSE 510.38613269869506 Test RE 0.31260980654326126\n",
      "42 Train Loss 6.6611075 Test MSE 579.9611539506708 Test RE 0.33323654061070673\n",
      "43 Train Loss 6.5641007 Test MSE 562.0475449912669 Test RE 0.32804973575063034\n",
      "44 Train Loss 6.2935905 Test MSE 627.5929823967986 Test RE 0.34665079781173924\n",
      "45 Train Loss 5.8809924 Test MSE 675.2231163868073 Test RE 0.35956450715712857\n",
      "46 Train Loss 5.638452 Test MSE 647.5980747015087 Test RE 0.35213236143270193\n",
      "47 Train Loss 5.62169 Test MSE 684.4733374023424 Test RE 0.36201905672316176\n",
      "48 Train Loss 5.593192 Test MSE 668.2768410515084 Test RE 0.35771023796247686\n",
      "49 Train Loss 5.561335 Test MSE 643.4259826028738 Test RE 0.35099623816796316\n",
      "50 Train Loss 5.411452 Test MSE 684.3061900493046 Test RE 0.3619748517714349\n",
      "51 Train Loss 5.261351 Test MSE 704.74739911255 Test RE 0.36734142430520733\n",
      "52 Train Loss 5.0099134 Test MSE 760.0390607619582 Test RE 0.3814794264230848\n",
      "53 Train Loss 4.7522144 Test MSE 737.7292621031548 Test RE 0.3758388492848086\n",
      "54 Train Loss 4.5961957 Test MSE 719.7986842067057 Test RE 0.3712433550588948\n",
      "55 Train Loss 4.5329204 Test MSE 778.8064822365947 Test RE 0.3861605847876264\n",
      "56 Train Loss 4.5235386 Test MSE 772.2062570572633 Test RE 0.384520787287331\n",
      "57 Train Loss 4.5107236 Test MSE 758.5722487157449 Test RE 0.3811111368948445\n",
      "58 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "59 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "60 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "61 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "62 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "63 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "64 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "65 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "66 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "67 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "68 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "69 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "70 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "71 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "72 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "73 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "74 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "75 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "76 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "77 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "78 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "79 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "80 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "81 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "82 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "83 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "84 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "85 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "86 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "87 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "88 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "89 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "90 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "91 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "92 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "93 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "94 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "95 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "96 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "97 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "98 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "99 Train Loss 4.510475 Test MSE 759.6680097649494 Test RE 0.3813862959453181\n",
      "Training time: 44.73\n",
      "Training time: 44.73\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 47.75011 Test MSE 5179.706530058572 Test RE 0.995877090555358\n",
      "1 Train Loss 47.731953 Test MSE 5127.2764005706385 Test RE 0.9908240274323943\n",
      "2 Train Loss 47.70203 Test MSE 5076.085420727557 Test RE 0.9858654016747049\n",
      "3 Train Loss 47.57816 Test MSE 4879.338052336487 Test RE 0.9665706836684679\n",
      "4 Train Loss 47.49621 Test MSE 4769.664598588262 Test RE 0.9556460850892353\n",
      "5 Train Loss 47.12491 Test MSE 4758.507697402498 Test RE 0.9545277369305885\n",
      "6 Train Loss 46.760616 Test MSE 4909.5187491123315 Test RE 0.9695553924079181\n",
      "7 Train Loss 46.378582 Test MSE 5303.801386254313 Test RE 1.0077360406155358\n",
      "8 Train Loss 44.463696 Test MSE 5068.146490214029 Test RE 0.9850941597740579\n",
      "9 Train Loss 39.483383 Test MSE 4238.979809227507 Test RE 0.9009150431441624\n",
      "10 Train Loss 32.64055 Test MSE 3309.9336824622346 Test RE 0.7960911773015125\n",
      "11 Train Loss 31.083952 Test MSE 3026.8683233693296 Test RE 0.7612896560410621\n",
      "12 Train Loss 30.02726 Test MSE 3076.8176312568517 Test RE 0.7675453455807418\n",
      "13 Train Loss 29.292418 Test MSE 3164.938610841763 Test RE 0.7784591182197879\n",
      "14 Train Loss 26.606174 Test MSE 3283.661590652767 Test RE 0.7929254570176526\n",
      "15 Train Loss 20.98048 Test MSE 2384.851519118553 Test RE 0.6757465501505413\n",
      "16 Train Loss 18.783815 Test MSE 1976.8762115520346 Test RE 0.6152376468225124\n",
      "17 Train Loss 16.973984 Test MSE 1597.7052375338126 Test RE 0.5530972435788208\n",
      "18 Train Loss 16.359808 Test MSE 1899.3557953972463 Test RE 0.6030541740720337\n",
      "19 Train Loss 16.064512 Test MSE 1919.7047891774732 Test RE 0.6062760170695681\n",
      "20 Train Loss 15.584814 Test MSE 1608.4601952149108 Test RE 0.5549557091491544\n",
      "21 Train Loss 15.136657 Test MSE 1533.1653581722885 Test RE 0.5418108074179102\n",
      "22 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "23 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "24 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "25 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "26 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "27 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "28 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "29 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "30 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "31 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "32 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "33 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "34 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "35 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "36 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "37 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "38 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "39 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "40 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "41 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "42 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "43 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "44 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "45 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "46 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "47 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "48 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "49 Train Loss 14.63501 Test MSE 1241.934508930985 Test RE 0.48764358123790946\n",
      "50 Train Loss 14.634953 Test MSE 1241.9115607524254 Test RE 0.4876390759344535\n",
      "51 Train Loss 13.600334 Test MSE 844.346022282961 Test RE 0.4020808557480443\n",
      "52 Train Loss 13.226161 Test MSE 737.0889523390681 Test RE 0.3756757098117512\n",
      "53 Train Loss 13.184877 Test MSE 708.8045690719065 Test RE 0.36839728330343074\n",
      "54 Train Loss 13.163035 Test MSE 681.7410535375523 Test RE 0.3612957795254368\n",
      "55 Train Loss 13.130938 Test MSE 688.0879469287671 Test RE 0.36297368448457024\n",
      "56 Train Loss 13.114 Test MSE 709.0714934394556 Test RE 0.3684666430113928\n",
      "57 Train Loss 13.101501 Test MSE 700.2166690868436 Test RE 0.36615872509388203\n",
      "58 Train Loss 13.087245 Test MSE 687.4459569287889 Test RE 0.3628043167056171\n",
      "59 Train Loss 13.08493 Test MSE 686.9047476062999 Test RE 0.36266147512100216\n",
      "60 Train Loss 13.084598 Test MSE 685.9720185694382 Test RE 0.36241516746499314\n",
      "61 Train Loss 13.084598 Test MSE 685.9719262501607 Test RE 0.36241514307776845\n",
      "62 Train Loss 13.084566 Test MSE 685.9678088348724 Test RE 0.36241405541234584\n",
      "63 Train Loss 13.084566 Test MSE 685.9678088348724 Test RE 0.36241405541234584\n",
      "64 Train Loss 13.084566 Test MSE 685.9678088348724 Test RE 0.36241405541234584\n",
      "65 Train Loss 13.084566 Test MSE 685.9678088348724 Test RE 0.36241405541234584\n",
      "66 Train Loss 13.084566 Test MSE 685.9678088348724 Test RE 0.36241405541234584\n",
      "67 Train Loss 13.084566 Test MSE 685.9678088348724 Test RE 0.36241405541234584\n",
      "68 Train Loss 13.084566 Test MSE 685.9678088348724 Test RE 0.36241405541234584\n",
      "69 Train Loss 13.084566 Test MSE 685.9678088348724 Test RE 0.36241405541234584\n",
      "70 Train Loss 13.084566 Test MSE 685.9678088348724 Test RE 0.36241405541234584\n",
      "71 Train Loss 13.084566 Test MSE 685.9678088348724 Test RE 0.36241405541234584\n",
      "72 Train Loss 13.084515 Test MSE 685.9682738724586 Test RE 0.3624141782578546\n",
      "73 Train Loss 13.084515 Test MSE 685.9682738724586 Test RE 0.3624141782578546\n",
      "74 Train Loss 13.084515 Test MSE 685.9682738724586 Test RE 0.3624141782578546\n",
      "75 Train Loss 13.084395 Test MSE 685.9652450723189 Test RE 0.36241337816157676\n",
      "76 Train Loss 13.084395 Test MSE 685.9652450723189 Test RE 0.36241337816157676\n",
      "77 Train Loss 13.084395 Test MSE 685.9652450723189 Test RE 0.36241337816157676\n",
      "78 Train Loss 13.084395 Test MSE 685.9652450723189 Test RE 0.36241337816157676\n",
      "79 Train Loss 13.084395 Test MSE 685.9652450723189 Test RE 0.36241337816157676\n",
      "80 Train Loss 13.084395 Test MSE 685.9652450723189 Test RE 0.36241337816157676\n",
      "81 Train Loss 13.084395 Test MSE 685.9652450723189 Test RE 0.36241337816157676\n",
      "82 Train Loss 13.084395 Test MSE 685.9652450723189 Test RE 0.36241337816157676\n",
      "83 Train Loss 13.084395 Test MSE 685.9652450723189 Test RE 0.36241337816157676\n",
      "84 Train Loss 13.084395 Test MSE 685.9652450723189 Test RE 0.36241337816157676\n",
      "85 Train Loss 13.084395 Test MSE 685.9652450723189 Test RE 0.36241337816157676\n",
      "86 Train Loss 13.084395 Test MSE 685.9652450723189 Test RE 0.36241337816157676\n",
      "87 Train Loss 13.084286 Test MSE 685.8128872388905 Test RE 0.3623731286126119\n",
      "88 Train Loss 13.084286 Test MSE 685.8128872388905 Test RE 0.3623731286126119\n",
      "89 Train Loss 13.084286 Test MSE 685.8128872388905 Test RE 0.3623731286126119\n",
      "90 Train Loss 13.084066 Test MSE 686.9672563290962 Test RE 0.36267797594588747\n",
      "91 Train Loss 13.084066 Test MSE 686.9672563290962 Test RE 0.36267797594588747\n",
      "92 Train Loss 13.084066 Test MSE 686.9672563290962 Test RE 0.36267797594588747\n",
      "93 Train Loss 13.084066 Test MSE 686.9672563290962 Test RE 0.36267797594588747\n",
      "94 Train Loss 13.08398 Test MSE 687.0990951434243 Test RE 0.36271277581445066\n",
      "95 Train Loss 13.08398 Test MSE 687.0990951434243 Test RE 0.36271277581445066\n",
      "96 Train Loss 13.083699 Test MSE 686.6323267843007 Test RE 0.36258955369745516\n",
      "97 Train Loss 13.083245 Test MSE 686.6189196361926 Test RE 0.36258601372752525\n",
      "98 Train Loss 13.083245 Test MSE 686.6189196361926 Test RE 0.36258601372752525\n",
      "99 Train Loss 13.083245 Test MSE 686.6189196361926 Test RE 0.36258601372752525\n",
      "Training time: 32.71\n",
      "Training time: 32.71\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 47.771984 Test MSE 5227.58558683834 Test RE 1.000469240175276\n",
      "1 Train Loss 47.771347 Test MSE 5226.127185446533 Test RE 1.0003296740709156\n",
      "2 Train Loss 47.76545 Test MSE 5214.164662883707 Test RE 0.9991841487628828\n",
      "3 Train Loss 47.756462 Test MSE 5190.605182057624 Test RE 0.9969242555285143\n",
      "4 Train Loss 47.741104 Test MSE 5155.810820645249 Test RE 0.9935772788759486\n",
      "5 Train Loss 47.71278 Test MSE 5081.400992166431 Test RE 0.9863814555169698\n",
      "6 Train Loss 47.6619 Test MSE 5016.245645161774 Test RE 0.9800372038147908\n",
      "7 Train Loss 47.2341 Test MSE 4802.566804995854 Test RE 0.9589365497910671\n",
      "8 Train Loss 45.791756 Test MSE 4781.2672367997675 Test RE 0.9568077266045586\n",
      "9 Train Loss 38.93052 Test MSE 3489.666396036463 Test RE 0.8174197381780395\n",
      "10 Train Loss 36.10155 Test MSE 3155.4872480754625 Test RE 0.7772959044077303\n",
      "11 Train Loss 33.2497 Test MSE 2589.51977967251 Test RE 0.7041461045003778\n",
      "12 Train Loss 29.505274 Test MSE 1986.6335872715947 Test RE 0.6167541089063078\n",
      "13 Train Loss 26.860283 Test MSE 1698.3285031166397 Test RE 0.5702483192389431\n",
      "14 Train Loss 24.528496 Test MSE 1417.2835589589831 Test RE 0.5209326033581368\n",
      "15 Train Loss 23.288141 Test MSE 1283.7749258427486 Test RE 0.4957898245255513\n",
      "16 Train Loss 22.811083 Test MSE 1218.1159591253556 Test RE 0.48294478552768316\n",
      "17 Train Loss 21.628643 Test MSE 938.773224623276 Test RE 0.42396842311008376\n",
      "18 Train Loss 20.01544 Test MSE 797.155673080934 Test RE 0.3906831990923821\n",
      "19 Train Loss 16.915678 Test MSE 878.9825140088374 Test RE 0.41024498659159425\n",
      "20 Train Loss 15.484927 Test MSE 1168.0386786806569 Test RE 0.4729135708771833\n",
      "21 Train Loss 15.411634 Test MSE 1184.4561842813332 Test RE 0.4762255198887538\n",
      "22 Train Loss 15.357718 Test MSE 1202.5904790482946 Test RE 0.479857232852756\n",
      "23 Train Loss 15.346052 Test MSE 1213.7979322426718 Test RE 0.48208804446426273\n",
      "24 Train Loss 15.323456 Test MSE 1216.502822879514 Test RE 0.4826249006157066\n",
      "25 Train Loss 15.282187 Test MSE 1232.6152153645594 Test RE 0.4858105332404211\n",
      "26 Train Loss 15.239538 Test MSE 1205.6748507209231 Test RE 0.48047220124535\n",
      "27 Train Loss 15.14114 Test MSE 1219.3042296653914 Test RE 0.48318028411584996\n",
      "28 Train Loss 15.050866 Test MSE 1160.667891217077 Test RE 0.47141907330105626\n",
      "29 Train Loss 14.970978 Test MSE 1130.4939943799461 Test RE 0.4652509777024806\n",
      "30 Train Loss 14.795349 Test MSE 1073.2022676531278 Test RE 0.4533085969714479\n",
      "31 Train Loss 14.786325 Test MSE 1087.4866610323909 Test RE 0.4563154085779525\n",
      "32 Train Loss 14.775383 Test MSE 1099.3561522874645 Test RE 0.458798902417632\n",
      "33 Train Loss 14.769856 Test MSE 1095.3084756768267 Test RE 0.4579535066354862\n",
      "34 Train Loss 14.768934 Test MSE 1100.0871391531293 Test RE 0.4589514099738519\n",
      "35 Train Loss 14.767558 Test MSE 1106.5248991651852 Test RE 0.4602923532911614\n",
      "36 Train Loss 14.705564 Test MSE 1088.0625768450711 Test RE 0.45643622131267736\n",
      "37 Train Loss 14.535123 Test MSE 973.5334124800207 Test RE 0.43174627116172043\n",
      "38 Train Loss 14.530127 Test MSE 955.2729770358421 Test RE 0.4276780004487046\n",
      "39 Train Loss 14.504994 Test MSE 948.6572919861871 Test RE 0.42619449848732616\n",
      "40 Train Loss 14.377689 Test MSE 952.4101567921202 Test RE 0.4270366738556135\n",
      "41 Train Loss 14.33962 Test MSE 988.977745385458 Test RE 0.4351574509994197\n",
      "42 Train Loss 14.280596 Test MSE 938.2900338508475 Test RE 0.4238592998460493\n",
      "43 Train Loss 14.231099 Test MSE 940.5297178220231 Test RE 0.42436487116362376\n",
      "44 Train Loss 14.18762 Test MSE 956.5894759595275 Test RE 0.4279725988453592\n",
      "45 Train Loss 14.145727 Test MSE 916.3282088111438 Test RE 0.4188694561320729\n",
      "46 Train Loss 14.095336 Test MSE 943.5495660384678 Test RE 0.4250455995077866\n",
      "47 Train Loss 13.728211 Test MSE 789.1620676263789 Test RE 0.3887194447272121\n",
      "48 Train Loss 13.638564 Test MSE 764.51877292129 Test RE 0.3826020051118805\n",
      "49 Train Loss 13.59345 Test MSE 703.5288706988874 Test RE 0.36702371499812225\n",
      "50 Train Loss 13.423891 Test MSE 621.3935003981334 Test RE 0.3449344075630428\n",
      "51 Train Loss 12.610405 Test MSE 633.9065037838493 Test RE 0.34839007036511976\n",
      "52 Train Loss 12.083905 Test MSE 618.5373754654179 Test RE 0.34414077967953144\n",
      "53 Train Loss 11.476819 Test MSE 618.8202734543363 Test RE 0.3442194698355078\n",
      "54 Train Loss 10.508005 Test MSE 473.18635261412476 Test RE 0.3010019228193084\n",
      "55 Train Loss 6.903248 Test MSE 661.1563130259838 Test RE 0.35579942283116534\n",
      "56 Train Loss 6.797167 Test MSE 697.7856511568011 Test RE 0.36552255602878636\n",
      "57 Train Loss 6.7574725 Test MSE 681.3237570276559 Test RE 0.361185187281164\n",
      "58 Train Loss 6.7272015 Test MSE 686.4073701863746 Test RE 0.3625301524785228\n",
      "59 Train Loss 6.638941 Test MSE 670.8027637697155 Test RE 0.3583856288880768\n",
      "60 Train Loss 6.6143384 Test MSE 651.6741983833605 Test RE 0.3532388220721445\n",
      "61 Train Loss 6.577913 Test MSE 626.115467708507 Test RE 0.34624250496356607\n",
      "62 Train Loss 6.561783 Test MSE 618.7856876535726 Test RE 0.3442098505059045\n",
      "63 Train Loss 6.5451107 Test MSE 614.1391971138999 Test RE 0.34291507138953203\n",
      "64 Train Loss 6.5224657 Test MSE 590.3760931028417 Test RE 0.3362153566910919\n",
      "65 Train Loss 6.489898 Test MSE 569.8222579516913 Test RE 0.33031087264056214\n",
      "66 Train Loss 6.48098 Test MSE 549.5252232702574 Test RE 0.32437470510704175\n",
      "67 Train Loss 6.480946 Test MSE 548.1444535598168 Test RE 0.3239669272204137\n",
      "68 Train Loss 6.480945 Test MSE 548.1444344046813 Test RE 0.3239669215598342\n",
      "69 Train Loss 6.480945 Test MSE 548.1444344046813 Test RE 0.3239669215598342\n",
      "70 Train Loss 6.480945 Test MSE 548.1444344046813 Test RE 0.3239669215598342\n",
      "71 Train Loss 6.480945 Test MSE 548.1444344046813 Test RE 0.3239669215598342\n",
      "72 Train Loss 6.480945 Test MSE 548.1444344046813 Test RE 0.3239669215598342\n",
      "73 Train Loss 6.480945 Test MSE 548.1444344046813 Test RE 0.3239669215598342\n",
      "74 Train Loss 6.480945 Test MSE 548.1444344046813 Test RE 0.3239669215598342\n",
      "75 Train Loss 6.480945 Test MSE 548.1444344046813 Test RE 0.3239669215598342\n",
      "76 Train Loss 6.480945 Test MSE 548.1444344046813 Test RE 0.3239669215598342\n",
      "77 Train Loss 6.480945 Test MSE 548.1444344046813 Test RE 0.3239669215598342\n",
      "78 Train Loss 6.4809413 Test MSE 548.1419281041664 Test RE 0.32396618091618307\n",
      "79 Train Loss 6.4809413 Test MSE 548.1419281041664 Test RE 0.32396618091618307\n",
      "80 Train Loss 6.4809413 Test MSE 548.1419281041664 Test RE 0.32396618091618307\n",
      "81 Train Loss 6.4809413 Test MSE 548.1419281041664 Test RE 0.32396618091618307\n",
      "82 Train Loss 6.4809413 Test MSE 548.1419281041664 Test RE 0.32396618091618307\n",
      "83 Train Loss 6.4809413 Test MSE 548.1419281041664 Test RE 0.32396618091618307\n",
      "84 Train Loss 6.48094 Test MSE 548.142124398126 Test RE 0.32396623892360377\n",
      "85 Train Loss 6.480936 Test MSE 547.3404170963911 Test RE 0.32372923730903946\n",
      "86 Train Loss 6.480936 Test MSE 547.3404170963911 Test RE 0.32372923730903946\n",
      "87 Train Loss 6.480936 Test MSE 547.3404170963911 Test RE 0.32372923730903946\n",
      "88 Train Loss 6.480936 Test MSE 547.3404170963911 Test RE 0.32372923730903946\n",
      "89 Train Loss 6.480936 Test MSE 547.3404170963911 Test RE 0.32372923730903946\n",
      "90 Train Loss 6.480936 Test MSE 547.3404170963911 Test RE 0.32372923730903946\n",
      "91 Train Loss 6.480936 Test MSE 547.3404170963911 Test RE 0.32372923730903946\n",
      "92 Train Loss 6.480897 Test MSE 547.325610022004 Test RE 0.32372485839313253\n",
      "93 Train Loss 6.480897 Test MSE 547.325610022004 Test RE 0.32372485839313253\n",
      "94 Train Loss 6.480897 Test MSE 547.325610022004 Test RE 0.32372485839313253\n",
      "95 Train Loss 6.480897 Test MSE 547.325610022004 Test RE 0.32372485839313253\n",
      "96 Train Loss 6.480897 Test MSE 547.325610022004 Test RE 0.32372485839313253\n",
      "97 Train Loss 6.480897 Test MSE 547.325610022004 Test RE 0.32372485839313253\n",
      "98 Train Loss 6.480897 Test MSE 547.325610022004 Test RE 0.32372485839313253\n",
      "99 Train Loss 6.4752445 Test MSE 550.9043655962763 Test RE 0.32478149130548745\n",
      "Training time: 50.37\n",
      "Training time: 50.37\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 47.76869 Test MSE 5221.031392931627 Test RE 0.9998418639318398\n",
      "1 Train Loss 47.75109 Test MSE 5181.3665632642505 Test RE 0.9960366610439519\n",
      "2 Train Loss 47.70971 Test MSE 5078.335452534758 Test RE 0.986083875416609\n",
      "3 Train Loss 47.18677 Test MSE 4822.618018010473 Test RE 0.9609362943066958\n",
      "4 Train Loss 44.538017 Test MSE 4538.495665130863 Test RE 0.9322000613212892\n",
      "5 Train Loss 44.161808 Test MSE 4502.4953993159515 Test RE 0.928495500622295\n",
      "6 Train Loss 40.594315 Test MSE 3861.237711087328 Test RE 0.8598376033515814\n",
      "7 Train Loss 34.202084 Test MSE 4467.473206075826 Test RE 0.9248773480800233\n",
      "8 Train Loss 30.040089 Test MSE 4747.219161893507 Test RE 0.9533948588307345\n",
      "9 Train Loss 28.254715 Test MSE 5112.279887506996 Test RE 0.9893739606479761\n",
      "10 Train Loss 27.01249 Test MSE 5122.20619912383 Test RE 0.9903340089919782\n",
      "11 Train Loss 26.623402 Test MSE 5133.765956602048 Test RE 0.9914508684903169\n",
      "12 Train Loss 25.611406 Test MSE 5809.3420270039605 Test RE 1.054670109756035\n",
      "13 Train Loss 24.234552 Test MSE 5981.07898452334 Test RE 1.070145755994849\n",
      "14 Train Loss 20.414042 Test MSE 5624.897157352697 Test RE 1.0377923330250223\n",
      "15 Train Loss 17.995472 Test MSE 5338.20032259089 Test RE 1.0109987026222484\n",
      "16 Train Loss 13.990035 Test MSE 3395.723155005144 Test RE 0.8063420393311311\n",
      "17 Train Loss 12.011932 Test MSE 3087.1594664532986 Test RE 0.7688342046272088\n",
      "18 Train Loss 9.850984 Test MSE 1977.4981551631272 Test RE 0.615334418949789\n",
      "19 Train Loss 7.1867123 Test MSE 601.1516033314065 Test RE 0.33926977405098085\n",
      "20 Train Loss 6.9022155 Test MSE 558.1892879693069 Test RE 0.32692182420544663\n",
      "21 Train Loss 6.6120625 Test MSE 581.992205032081 Test RE 0.33381953561191524\n",
      "22 Train Loss 6.2406225 Test MSE 456.25902162201265 Test RE 0.29556901011909337\n",
      "23 Train Loss 6.058621 Test MSE 397.8138662413149 Test RE 0.2759898580561215\n",
      "24 Train Loss 5.9038053 Test MSE 466.53045015135064 Test RE 0.29887745898066503\n",
      "25 Train Loss 5.637033 Test MSE 512.256605186183 Test RE 0.313182111758807\n",
      "26 Train Loss 5.3884215 Test MSE 433.458291797203 Test RE 0.2880890969643147\n",
      "27 Train Loss 5.3197646 Test MSE 402.06226462622146 Test RE 0.2774596422233468\n",
      "28 Train Loss 5.292876 Test MSE 423.7542663952744 Test RE 0.2848460533717316\n",
      "29 Train Loss 5.2179337 Test MSE 427.2098541660711 Test RE 0.2860051118973601\n",
      "30 Train Loss 5.090407 Test MSE 363.3021722050033 Test RE 0.2637467774337362\n",
      "31 Train Loss 5.0092616 Test MSE 352.3540776250235 Test RE 0.2597423799570182\n",
      "32 Train Loss 4.9748135 Test MSE 358.246718790141 Test RE 0.2619052929543259\n",
      "33 Train Loss 4.932986 Test MSE 375.3607196921311 Test RE 0.26808812436118196\n",
      "34 Train Loss 4.8831797 Test MSE 371.8707882319334 Test RE 0.2668389338957961\n",
      "35 Train Loss 4.8395915 Test MSE 363.52950810711445 Test RE 0.2638292841447626\n",
      "36 Train Loss 4.803593 Test MSE 384.4560923032464 Test RE 0.2713167076339276\n",
      "37 Train Loss 4.7930374 Test MSE 380.416799748443 Test RE 0.2698876479894354\n",
      "38 Train Loss 4.786065 Test MSE 374.43765850257563 Test RE 0.26775828954822933\n",
      "39 Train Loss 4.7508526 Test MSE 370.60760906371416 Test RE 0.26638534609289505\n",
      "40 Train Loss 4.6993246 Test MSE 377.6034136843159 Test RE 0.26888781409981394\n",
      "41 Train Loss 4.6884747 Test MSE 382.4161434288551 Test RE 0.2705959382566406\n",
      "42 Train Loss 4.6614323 Test MSE 385.16187108292746 Test RE 0.271565633032431\n",
      "43 Train Loss 4.578229 Test MSE 395.07865153737094 Test RE 0.2750394216235279\n",
      "44 Train Loss 4.4902153 Test MSE 398.3791613816944 Test RE 0.2761858798057204\n",
      "45 Train Loss 4.454058 Test MSE 401.58468210636846 Test RE 0.2772948055052367\n",
      "46 Train Loss 4.38544 Test MSE 384.9188441281887 Test RE 0.27147994414710674\n",
      "47 Train Loss 4.335386 Test MSE 381.9921096620732 Test RE 0.27044587445594737\n",
      "48 Train Loss 4.2632513 Test MSE 423.11089418364577 Test RE 0.2846297350206129\n",
      "49 Train Loss 4.2401342 Test MSE 420.4410724748096 Test RE 0.28373030982442793\n",
      "50 Train Loss 4.1893406 Test MSE 428.1115714829196 Test RE 0.28630679012066007\n",
      "51 Train Loss 4.0497465 Test MSE 450.5734822885666 Test RE 0.2937216633299795\n",
      "52 Train Loss 3.9761508 Test MSE 424.548908264931 Test RE 0.28511300594565164\n",
      "53 Train Loss 3.9410877 Test MSE 433.4786973870337 Test RE 0.28809587796132674\n",
      "54 Train Loss 3.8342776 Test MSE 456.4935154861839 Test RE 0.2956449540599008\n",
      "55 Train Loss 3.8084638 Test MSE 436.4234908926149 Test RE 0.2890727966637323\n",
      "56 Train Loss 3.7627892 Test MSE 450.5224122878938 Test RE 0.293705016999015\n",
      "57 Train Loss 3.7211812 Test MSE 442.93496946795534 Test RE 0.29122130867671747\n",
      "58 Train Loss 3.6526625 Test MSE 451.96518386427323 Test RE 0.29417492762158964\n",
      "59 Train Loss 3.5909426 Test MSE 444.2913474913586 Test RE 0.2916668641988158\n",
      "60 Train Loss 3.5523605 Test MSE 438.21128139600336 Test RE 0.2896642788235034\n",
      "61 Train Loss 3.4854393 Test MSE 472.43012080980213 Test RE 0.3007613006570389\n",
      "62 Train Loss 3.4051692 Test MSE 504.64438811359594 Test RE 0.3108464333967183\n",
      "63 Train Loss 3.364383 Test MSE 490.9647291901327 Test RE 0.30660434950469856\n",
      "64 Train Loss 3.339666 Test MSE 500.3341479016525 Test RE 0.309516094630799\n",
      "65 Train Loss 3.3242638 Test MSE 520.4435628359058 Test RE 0.31567485185113464\n",
      "66 Train Loss 3.3095357 Test MSE 532.771128359555 Test RE 0.319391611434534\n",
      "67 Train Loss 3.2876 Test MSE 574.994058947615 Test RE 0.3318064647832398\n",
      "68 Train Loss 3.2390258 Test MSE 619.8744506351276 Test RE 0.34451253869437026\n",
      "69 Train Loss 3.2147336 Test MSE 612.1295268985622 Test RE 0.34235354485570046\n",
      "70 Train Loss 3.184844 Test MSE 617.9571977231775 Test RE 0.34397934265845653\n",
      "71 Train Loss 3.172249 Test MSE 621.2758985752448 Test RE 0.34490176573902936\n",
      "72 Train Loss 3.1653228 Test MSE 619.9404521117827 Test RE 0.34453087928819515\n",
      "73 Train Loss 3.1529078 Test MSE 619.7154735543054 Test RE 0.34446835788505836\n",
      "74 Train Loss 3.1166072 Test MSE 651.785201844995 Test RE 0.35326890540406486\n",
      "75 Train Loss 3.1066525 Test MSE 665.2287202099751 Test RE 0.3568935179163771\n",
      "76 Train Loss 3.0879097 Test MSE 703.7655081095544 Test RE 0.36708543544991706\n",
      "77 Train Loss 3.0615416 Test MSE 711.1523134799102 Test RE 0.36900689255803687\n",
      "78 Train Loss 3.0383174 Test MSE 689.0288007615252 Test RE 0.3632217548981173\n",
      "79 Train Loss 3.025973 Test MSE 689.7653212581698 Test RE 0.363415831557456\n",
      "80 Train Loss 3.0142365 Test MSE 699.5313003386341 Test RE 0.36597948401301345\n",
      "81 Train Loss 3.0019991 Test MSE 708.756063785807 Test RE 0.3683846779096989\n",
      "82 Train Loss 2.9840734 Test MSE 712.9434970657891 Test RE 0.369471310242835\n",
      "83 Train Loss 2.9735494 Test MSE 711.2194404204531 Test RE 0.3690243077578235\n",
      "84 Train Loss 2.9604495 Test MSE 687.9685887923097 Test RE 0.3629422017782558\n",
      "85 Train Loss 2.9306047 Test MSE 661.1192513616684 Test RE 0.3557894503758524\n",
      "86 Train Loss 2.9088767 Test MSE 631.429468033167 Test RE 0.3477087243468042\n",
      "87 Train Loss 2.8619466 Test MSE 563.7614212100156 Test RE 0.32854952325501413\n",
      "88 Train Loss 2.8252194 Test MSE 554.9658985415352 Test RE 0.3259765157508339\n",
      "89 Train Loss 2.802707 Test MSE 557.8891572343625 Test RE 0.32683392169425596\n",
      "90 Train Loss 2.7913127 Test MSE 562.6825734731791 Test RE 0.32823500665697714\n",
      "91 Train Loss 2.7744122 Test MSE 585.1965843125483 Test RE 0.33473725929024617\n",
      "92 Train Loss 2.7391372 Test MSE 584.5849293562884 Test RE 0.3345622777222209\n",
      "93 Train Loss 2.7031887 Test MSE 591.8660842460633 Test RE 0.33663935948218177\n",
      "94 Train Loss 2.687899 Test MSE 599.5547255240684 Test RE 0.33881886230528646\n",
      "95 Train Loss 2.6734264 Test MSE 616.7222652865659 Test RE 0.3436354647260253\n",
      "96 Train Loss 2.666289 Test MSE 627.0812999893308 Test RE 0.3465094551833216\n",
      "97 Train Loss 2.6604528 Test MSE 619.4894071067556 Test RE 0.3444055227257052\n",
      "98 Train Loss 2.65878 Test MSE 621.8680571902207 Test RE 0.34506609523369997\n",
      "99 Train Loss 2.6309462 Test MSE 625.8599276218187 Test RE 0.34617184078580354\n",
      "Training time: 65.26\n",
      "Training time: 65.26\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 47.767467 Test MSE 5218.289043795572 Test RE 0.9995792457432063\n",
      "1 Train Loss 47.766663 Test MSE 5216.33249022413 Test RE 0.9993918362606806\n",
      "2 Train Loss 47.747005 Test MSE 5158.200718315839 Test RE 0.99380753101274\n",
      "3 Train Loss 47.74459 Test MSE 5165.172852814558 Test RE 0.9944789492389414\n",
      "4 Train Loss 47.740337 Test MSE 5163.230642010044 Test RE 0.9942919594305646\n",
      "5 Train Loss 47.681576 Test MSE 5047.218339712777 Test RE 0.9830581564931334\n",
      "6 Train Loss 47.381878 Test MSE 4781.069252124668 Test RE 0.9567879164560587\n",
      "7 Train Loss 46.850075 Test MSE 4766.767900612809 Test RE 0.9553558510018285\n",
      "8 Train Loss 45.469128 Test MSE 4682.011958122483 Test RE 0.9468243621576856\n",
      "9 Train Loss 44.557312 Test MSE 4510.282043583845 Test RE 0.9292980268198218\n",
      "10 Train Loss 39.7893 Test MSE 4236.105512733144 Test RE 0.9006095526515957\n",
      "11 Train Loss 33.873573 Test MSE 2845.718831575284 Test RE 0.7381577071267779\n",
      "12 Train Loss 29.52197 Test MSE 2361.4629235268803 Test RE 0.6724248121385549\n",
      "13 Train Loss 27.62345 Test MSE 2157.2211197732927 Test RE 0.6426884519515162\n",
      "14 Train Loss 20.380768 Test MSE 1094.647441006994 Test RE 0.4578152949626885\n",
      "15 Train Loss 18.840288 Test MSE 832.1734135150275 Test RE 0.39917201197931995\n",
      "16 Train Loss 17.188715 Test MSE 708.9292180237667 Test RE 0.3684296746836141\n",
      "17 Train Loss 14.281705 Test MSE 598.1238044060386 Test RE 0.33841430149972895\n",
      "18 Train Loss 13.228987 Test MSE 627.4501991216368 Test RE 0.3466113624187737\n",
      "19 Train Loss 13.181516 Test MSE 602.8227552439998 Test RE 0.3397410177788503\n",
      "20 Train Loss 12.48356 Test MSE 579.186347503784 Test RE 0.33301387042821184\n",
      "21 Train Loss 12.136172 Test MSE 594.5537414821426 Test RE 0.3374028315082514\n",
      "22 Train Loss 12.028487 Test MSE 573.0254458743748 Test RE 0.33123797317540205\n",
      "23 Train Loss 11.785509 Test MSE 643.7506363212009 Test RE 0.35108477816037426\n",
      "24 Train Loss 11.520156 Test MSE 623.1550532472496 Test RE 0.34542297895169866\n",
      "25 Train Loss 11.44222 Test MSE 589.9657112916982 Test RE 0.33609848148685906\n",
      "26 Train Loss 11.246924 Test MSE 578.8069453507039 Test RE 0.3329047804281875\n",
      "27 Train Loss 10.942809 Test MSE 528.3065636656426 Test RE 0.3180505624043943\n",
      "28 Train Loss 10.519712 Test MSE 490.2935354142945 Test RE 0.3063946997063772\n",
      "29 Train Loss 10.435237 Test MSE 501.0268829788098 Test RE 0.3097302899759919\n",
      "30 Train Loss 10.047144 Test MSE 503.26265627686604 Test RE 0.310420588160844\n",
      "31 Train Loss 9.736235 Test MSE 521.8339430576913 Test RE 0.31609623790893265\n",
      "32 Train Loss 9.664168 Test MSE 539.3893734994313 Test RE 0.3213692785075661\n",
      "33 Train Loss 8.3727 Test MSE 777.5040567182772 Test RE 0.3858375547020997\n",
      "34 Train Loss 6.062158 Test MSE 971.1826307085753 Test RE 0.4312246893216955\n",
      "35 Train Loss 5.57493 Test MSE 742.8404770209431 Test RE 0.3771385654548731\n",
      "36 Train Loss 5.506714 Test MSE 745.1850795175778 Test RE 0.3777332714678892\n",
      "37 Train Loss 5.3981175 Test MSE 808.0590679590523 Test RE 0.39334598250380465\n",
      "38 Train Loss 5.250253 Test MSE 791.9784892660788 Test RE 0.389412472712501\n",
      "39 Train Loss 5.2108784 Test MSE 784.6248940207305 Test RE 0.38760039072825153\n",
      "40 Train Loss 5.1772604 Test MSE 781.9653597228387 Test RE 0.38694293546179537\n",
      "41 Train Loss 5.1380796 Test MSE 750.9263414139057 Test RE 0.37918559828107895\n",
      "42 Train Loss 5.0777845 Test MSE 727.9367011366814 Test RE 0.3733360883180794\n",
      "43 Train Loss 4.962826 Test MSE 745.4926802308756 Test RE 0.37781122460929606\n",
      "44 Train Loss 4.904591 Test MSE 695.6217608347483 Test RE 0.3649553582987138\n",
      "45 Train Loss 4.893704 Test MSE 697.8539744677696 Test RE 0.36554045056411266\n",
      "46 Train Loss 4.8580523 Test MSE 713.5646978907297 Test RE 0.36963223878070106\n",
      "47 Train Loss 4.8039002 Test MSE 711.9900251181336 Test RE 0.36922416696097404\n",
      "48 Train Loss 4.673528 Test MSE 708.6979569068617 Test RE 0.36836957671781584\n",
      "49 Train Loss 4.5484567 Test MSE 726.8250979758157 Test RE 0.3730509260715735\n",
      "50 Train Loss 4.4841475 Test MSE 732.508135958659 Test RE 0.3745065269612036\n",
      "51 Train Loss 4.453277 Test MSE 703.6391785165918 Test RE 0.36705248709256066\n",
      "52 Train Loss 4.397931 Test MSE 700.4894550481508 Test RE 0.366230041043771\n",
      "53 Train Loss 4.3148284 Test MSE 713.4088495582128 Test RE 0.36959187123308557\n",
      "54 Train Loss 4.1908393 Test MSE 730.7906902483621 Test RE 0.3740672334917308\n",
      "55 Train Loss 4.0602646 Test MSE 817.1371912295241 Test RE 0.3955493301674533\n",
      "56 Train Loss 3.9942596 Test MSE 832.0977140068611 Test RE 0.39915385602065545\n",
      "57 Train Loss 3.9740195 Test MSE 824.862008375992 Test RE 0.3974145975928911\n",
      "58 Train Loss 3.947112 Test MSE 823.3218258039791 Test RE 0.3970433979121806\n",
      "59 Train Loss 3.9321728 Test MSE 839.2516379004821 Test RE 0.40086603775019297\n",
      "60 Train Loss 3.8434458 Test MSE 940.1442610695101 Test RE 0.42427790364689066\n",
      "61 Train Loss 3.7397397 Test MSE 872.1717960754866 Test RE 0.4086525224068955\n",
      "62 Train Loss 3.6900423 Test MSE 870.5614746559751 Test RE 0.4082750933588725\n",
      "63 Train Loss 3.634031 Test MSE 850.8470180423284 Test RE 0.4036257873883295\n",
      "64 Train Loss 3.5672746 Test MSE 837.4614761218285 Test RE 0.4004382768071367\n",
      "65 Train Loss 3.5390904 Test MSE 842.9618340175048 Test RE 0.40175114264838013\n",
      "66 Train Loss 3.485767 Test MSE 896.5123054010555 Test RE 0.41431560602975986\n",
      "67 Train Loss 3.4448707 Test MSE 860.603964782729 Test RE 0.4059334463828558\n",
      "68 Train Loss 3.429355 Test MSE 834.6526492514005 Test RE 0.39976618231965755\n",
      "69 Train Loss 3.4004145 Test MSE 827.3822584113916 Test RE 0.39802125680381345\n",
      "70 Train Loss 3.1890454 Test MSE 854.0810813981615 Test RE 0.40439214919300054\n",
      "71 Train Loss 2.954903 Test MSE 894.0889354324634 Test RE 0.41375525709926847\n",
      "72 Train Loss 2.9247377 Test MSE 908.7725363114132 Test RE 0.4171389672985544\n",
      "73 Train Loss 2.9094913 Test MSE 935.2825617890451 Test RE 0.42317946308346566\n",
      "74 Train Loss 2.8999588 Test MSE 934.9553510074679 Test RE 0.423105431447704\n",
      "75 Train Loss 2.8924992 Test MSE 942.7775199590005 Test RE 0.4248716701483823\n",
      "76 Train Loss 2.8862271 Test MSE 936.2779517053084 Test RE 0.4234045910673824\n",
      "77 Train Loss 2.8814282 Test MSE 925.680248476911 Test RE 0.42100151944137193\n",
      "78 Train Loss 2.8454907 Test MSE 943.0079829521345 Test RE 0.4249235971467817\n",
      "79 Train Loss 2.7787187 Test MSE 968.548883514753 Test RE 0.4306395739038426\n",
      "80 Train Loss 2.763823 Test MSE 945.1378692830008 Test RE 0.4254031946271777\n",
      "81 Train Loss 2.6993716 Test MSE 1031.126370166111 Test RE 0.44433355518278794\n",
      "82 Train Loss 2.6736436 Test MSE 1050.1527920107865 Test RE 0.4484142550814024\n",
      "83 Train Loss 2.6406653 Test MSE 1049.832891896001 Test RE 0.448345951355135\n",
      "84 Train Loss 2.6319652 Test MSE 1071.45584517648 Test RE 0.4529396121680501\n",
      "85 Train Loss 2.604759 Test MSE 1055.8326809041605 Test RE 0.44962527341105796\n",
      "86 Train Loss 2.5558312 Test MSE 1018.9862618572475 Test RE 0.44171009926527594\n",
      "87 Train Loss 2.5331569 Test MSE 1045.890120359621 Test RE 0.4475032512841211\n",
      "88 Train Loss 2.4930134 Test MSE 1039.8818258290667 Test RE 0.44621602049129633\n",
      "89 Train Loss 2.4816504 Test MSE 1030.3737382499623 Test RE 0.44417136329668133\n",
      "90 Train Loss 2.4739358 Test MSE 1021.3811661452861 Test RE 0.44222886611055806\n",
      "91 Train Loss 2.4543736 Test MSE 1015.0427569411058 Test RE 0.4408545555897399\n",
      "92 Train Loss 2.424786 Test MSE 1037.0419573754095 Test RE 0.44560630642842347\n",
      "93 Train Loss 2.3400307 Test MSE 1091.3971735865616 Test RE 0.4571351086773906\n",
      "94 Train Loss 2.3286414 Test MSE 1088.6700155443825 Test RE 0.4565636121194368\n",
      "95 Train Loss 2.3144684 Test MSE 1119.1726748191454 Test RE 0.46291549043527697\n",
      "96 Train Loss 2.293931 Test MSE 1148.0813547147957 Test RE 0.4688560200069544\n",
      "97 Train Loss 2.2714639 Test MSE 1172.9502562368966 Test RE 0.4739068235250812\n",
      "98 Train Loss 2.2675147 Test MSE 1162.5851255582181 Test RE 0.47180826641563217\n",
      "99 Train Loss 2.2458022 Test MSE 1157.1307961889563 Test RE 0.47070020858986944\n",
      "Training time: 66.25\n",
      "Training time: 66.25\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 47.770065 Test MSE 5223.797661906716 Test RE 1.0001067029129378\n",
      "1 Train Loss 47.748383 Test MSE 5177.246390263638 Test RE 0.9956405628846065\n",
      "2 Train Loss 47.700153 Test MSE 5067.736903803895 Test RE 0.9850543533740684\n",
      "3 Train Loss 47.497715 Test MSE 4858.461411834619 Test RE 0.9645006919129846\n",
      "4 Train Loss 46.063244 Test MSE 4991.506745467653 Test RE 0.9776175646666522\n",
      "5 Train Loss 37.97675 Test MSE 4543.129478206785 Test RE 0.9326758289681283\n",
      "6 Train Loss 34.745377 Test MSE 4297.908440096958 Test RE 0.9071555132391553\n",
      "7 Train Loss 32.70767 Test MSE 4457.632074008714 Test RE 0.9238581077048926\n",
      "8 Train Loss 31.650553 Test MSE 4585.611131915883 Test RE 0.9370262904062169\n",
      "9 Train Loss 30.22256 Test MSE 4893.29248752734 Test RE 0.9679518462387207\n",
      "10 Train Loss 28.689322 Test MSE 5267.418083209872 Test RE 1.0042736314477345\n",
      "11 Train Loss 28.43971 Test MSE 5405.078831682399 Test RE 1.017312031642028\n",
      "12 Train Loss 27.473633 Test MSE 5509.997162846097 Test RE 1.027138131538264\n",
      "13 Train Loss 27.44064 Test MSE 5541.842695556303 Test RE 1.0301020745153078\n",
      "14 Train Loss 27.193054 Test MSE 5799.260540313773 Test RE 1.0537545792550929\n",
      "15 Train Loss 27.063557 Test MSE 5901.452412038618 Test RE 1.062998420942723\n",
      "16 Train Loss 25.932892 Test MSE 6157.974625998586 Test RE 1.0858556919177385\n",
      "17 Train Loss 23.183594 Test MSE 6822.505875707709 Test RE 1.1429444520205336\n",
      "18 Train Loss 20.559244 Test MSE 6780.659094907469 Test RE 1.139433856840562\n",
      "19 Train Loss 20.405586 Test MSE 6838.867894911741 Test RE 1.1443141599057751\n",
      "20 Train Loss 20.373219 Test MSE 6918.269736563955 Test RE 1.1509379492141907\n",
      "21 Train Loss 20.01209 Test MSE 7291.646791615328 Test RE 1.1815877403754322\n",
      "22 Train Loss 19.699028 Test MSE 7686.425693066235 Test RE 1.2131524544608798\n",
      "23 Train Loss 19.347872 Test MSE 8646.641300891943 Test RE 1.2866987710941848\n",
      "24 Train Loss 19.197716 Test MSE 9027.003116962282 Test RE 1.3146948475643119\n",
      "25 Train Loss 19.120443 Test MSE 9388.897029878395 Test RE 1.340789044533415\n",
      "26 Train Loss 19.116999 Test MSE 9474.226490697692 Test RE 1.3468680351754445\n",
      "27 Train Loss 19.112684 Test MSE 9522.609220621067 Test RE 1.350302730782926\n",
      "28 Train Loss 19.01309 Test MSE 9661.552248666037 Test RE 1.360118093730592\n",
      "29 Train Loss 18.586708 Test MSE 8273.999416299766 Test RE 1.2586671788727402\n",
      "30 Train Loss 18.274101 Test MSE 5288.650075477159 Test RE 1.0062956169667745\n",
      "31 Train Loss 14.812373 Test MSE 1196.385114949431 Test RE 0.4786176005520788\n",
      "32 Train Loss 14.001662 Test MSE 1075.9213210226585 Test RE 0.4538824825221957\n",
      "33 Train Loss 13.46605 Test MSE 1141.1267760611343 Test RE 0.46743379977912025\n",
      "34 Train Loss 10.1252365 Test MSE 772.5118838106993 Test RE 0.3845968733154878\n",
      "35 Train Loss 7.8039465 Test MSE 441.93794088311614 Test RE 0.2908933603679202\n",
      "36 Train Loss 7.0669723 Test MSE 555.955331403305 Test RE 0.3262669734595737\n",
      "37 Train Loss 6.555941 Test MSE 495.80622662790995 Test RE 0.3081123830454787\n",
      "38 Train Loss 6.485987 Test MSE 527.5723924605554 Test RE 0.31782949308563035\n",
      "39 Train Loss 6.422828 Test MSE 435.77883406963946 Test RE 0.2888592178548969\n",
      "40 Train Loss 6.323515 Test MSE 318.78218884193257 Test RE 0.2470587210187754\n",
      "41 Train Loss 6.2739983 Test MSE 318.10266281338323 Test RE 0.24679526151736728\n",
      "42 Train Loss 6.2576733 Test MSE 295.2557111495689 Test RE 0.23776740513554107\n",
      "43 Train Loss 6.253178 Test MSE 286.7313357965204 Test RE 0.23430995663826254\n",
      "44 Train Loss 6.2527905 Test MSE 285.4934448432653 Test RE 0.2338036221855348\n",
      "45 Train Loss 6.241014 Test MSE 282.1114570468116 Test RE 0.23241466435156147\n",
      "46 Train Loss 6.2190194 Test MSE 283.15475094525044 Test RE 0.23284402131157858\n",
      "47 Train Loss 6.214115 Test MSE 292.07775236502846 Test RE 0.23648434904109544\n",
      "48 Train Loss 6.1968846 Test MSE 338.39862763879404 Test RE 0.2545466942051\n",
      "49 Train Loss 6.1861706 Test MSE 348.8089934562057 Test RE 0.2584324243197631\n",
      "50 Train Loss 6.1774874 Test MSE 330.68208252212384 Test RE 0.25162772822700913\n",
      "51 Train Loss 6.170683 Test MSE 326.2101256385626 Test RE 0.24992050136575775\n",
      "52 Train Loss 6.16384 Test MSE 337.5730301108676 Test RE 0.2542359933931918\n",
      "53 Train Loss 6.1567082 Test MSE 332.7740050041586 Test RE 0.2524223824620167\n",
      "54 Train Loss 6.1478724 Test MSE 327.29195668548164 Test RE 0.25033457184464075\n",
      "55 Train Loss 6.1384115 Test MSE 326.8994848010782 Test RE 0.25018443257189793\n",
      "56 Train Loss 6.125722 Test MSE 327.33962363611346 Test RE 0.25035280060186343\n",
      "57 Train Loss 6.1068497 Test MSE 330.42810988162415 Test RE 0.2515310812704036\n",
      "58 Train Loss 6.084893 Test MSE 337.5306777193696 Test RE 0.25422004448864033\n",
      "59 Train Loss 6.0615287 Test MSE 343.69780828589023 Test RE 0.2565319993957507\n",
      "60 Train Loss 6.0018363 Test MSE 325.63358913524104 Test RE 0.24969955173610683\n",
      "61 Train Loss 5.9179387 Test MSE 321.0115446731734 Test RE 0.2479211001281505\n",
      "62 Train Loss 5.8324943 Test MSE 293.271118057945 Test RE 0.23696696818917204\n",
      "63 Train Loss 5.7601595 Test MSE 297.9774709543288 Test RE 0.23886079838475607\n",
      "64 Train Loss 5.730144 Test MSE 324.1507794145393 Test RE 0.24913038534918303\n",
      "65 Train Loss 5.7050095 Test MSE 329.7286837309854 Test RE 0.2512647289127788\n",
      "66 Train Loss 5.6638165 Test MSE 357.9365720729663 Test RE 0.2617918981193079\n",
      "67 Train Loss 5.601243 Test MSE 400.3512527060299 Test RE 0.27686863562342234\n",
      "68 Train Loss 5.567822 Test MSE 434.7769039192778 Test RE 0.28852695836553727\n",
      "69 Train Loss 5.5302916 Test MSE 450.5571929738897 Test RE 0.2937163539098369\n",
      "70 Train Loss 5.514811 Test MSE 448.2729438415139 Test RE 0.2929708615904755\n",
      "71 Train Loss 5.5066214 Test MSE 461.7968813756371 Test RE 0.2973573395815044\n",
      "72 Train Loss 5.504865 Test MSE 468.74016461505005 Test RE 0.2995844371174693\n",
      "73 Train Loss 5.5046597 Test MSE 472.5066911199686 Test RE 0.30078567299487474\n",
      "74 Train Loss 5.5045567 Test MSE 473.8621889260142 Test RE 0.3012168016326159\n",
      "75 Train Loss 5.5045567 Test MSE 473.8621889260142 Test RE 0.3012168016326159\n",
      "76 Train Loss 5.5045524 Test MSE 473.9630916339593 Test RE 0.3012488699996086\n",
      "77 Train Loss 5.5045524 Test MSE 473.9630916339593 Test RE 0.3012488699996086\n",
      "78 Train Loss 5.5045524 Test MSE 473.9630916339593 Test RE 0.3012488699996086\n",
      "79 Train Loss 5.5045524 Test MSE 473.9630916339593 Test RE 0.3012488699996086\n",
      "80 Train Loss 5.5045395 Test MSE 474.0084349400999 Test RE 0.3012632796586043\n",
      "81 Train Loss 5.504375 Test MSE 473.69332312409796 Test RE 0.3011631259539648\n",
      "82 Train Loss 5.5043015 Test MSE 473.71229674706814 Test RE 0.30116915738623007\n",
      "83 Train Loss 5.5043015 Test MSE 473.71229674706814 Test RE 0.30116915738623007\n",
      "84 Train Loss 5.5043015 Test MSE 473.71229674706814 Test RE 0.30116915738623007\n",
      "85 Train Loss 5.5043015 Test MSE 473.71229674706814 Test RE 0.30116915738623007\n",
      "86 Train Loss 5.5043015 Test MSE 473.71229674706814 Test RE 0.30116915738623007\n",
      "87 Train Loss 5.5043015 Test MSE 473.71229674706814 Test RE 0.30116915738623007\n",
      "88 Train Loss 5.5043015 Test MSE 473.71229674706814 Test RE 0.30116915738623007\n",
      "89 Train Loss 5.5043015 Test MSE 473.71229674706814 Test RE 0.30116915738623007\n",
      "90 Train Loss 5.504216 Test MSE 473.7973075900761 Test RE 0.30119617958471934\n",
      "91 Train Loss 5.504216 Test MSE 473.7973075900761 Test RE 0.30119617958471934\n",
      "92 Train Loss 5.504216 Test MSE 473.7973075900761 Test RE 0.30119617958471934\n",
      "93 Train Loss 5.504216 Test MSE 473.7973075900761 Test RE 0.30119617958471934\n",
      "94 Train Loss 5.504216 Test MSE 473.7973075900761 Test RE 0.30119617958471934\n",
      "95 Train Loss 5.5027666 Test MSE 482.4213260030075 Test RE 0.303924991838172\n",
      "96 Train Loss 5.4887075 Test MSE 496.02217986814844 Test RE 0.3081794764180265\n",
      "97 Train Loss 5.459278 Test MSE 512.2403951951485 Test RE 0.31317715650851347\n",
      "98 Train Loss 5.4543676 Test MSE 508.44606354985683 Test RE 0.3120150979103262\n",
      "99 Train Loss 5.4432793 Test MSE 523.0089434203162 Test RE 0.3164519107802774\n",
      "Training time: 57.76\n",
      "Training time: 57.76\n"
     ]
    }
   ],
   "source": [
    " \n",
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []   \n",
    "\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "  N_f = 10000 #Total number of collocation points\n",
    "\n",
    "  layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "  PINN = Sequentialmodel(layers)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                            max_iter = 10, \n",
    "                            max_eval = 15, \n",
    "                            tolerance_grad = 1e-6, \n",
    "                            tolerance_change = 1e-6, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "  train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "\n",
    "\n",
    "  print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ky6HsA0AWWTD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF7H51LTWXDq",
    "outputId": "e199619a-d416-48f4-91f7-2c23d1e79435"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1D_FODE_tanh_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_tanh_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d8822cfe9f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1D_FODE_tanh_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_tanh_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"1D_FODE_tanh_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tanh_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
