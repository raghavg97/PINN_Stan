{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "84a34ebd-2e54-4cae-ca1c-79397867998c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvP0Nx4vNOlZ",
    "outputId": "94a6280c-bfd4-43c8-a396-40f22c70c38f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDzAYhTsNbP6",
    "outputId": "150eeb9e-6cdc-4ff0-fd50-61a1c228e3a0"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1wXUvTNETmrW",
    "outputId": "30d0ca6b-cde8-4b85-ccae-4eac06a2c482"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dmSz5jcVVt4p"
   },
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1]).reshape(-1,1)\n",
    "n_value = np.array([1.0,3.0,5.0,8.0,10.0]).reshape(-1,1)\n",
    "r_value = np.array([2,6,8]).reshape(-1,1)\n",
    "\n",
    "LR_tune,N_value,R_value = np.meshgrid(lr_tune,n_value,r_value)\n",
    "\n",
    "LR_tune = LR_tune.flatten('F').reshape(-1,1)\n",
    "N_value = N_value.flatten('F').reshape(-1,1)\n",
    "R_value = R_value.flatten('F').reshape(-1,1)\n",
    "\n",
    "\n",
    "lrnr_tune = np.hstack((LR_tune,N_value,R_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "def true_1D_2(x): #True function for 1D_1 dy/dx = cos(0.01*x) BC1: y(0)=0; x \\in [-100,100]\n",
    "    y = 100*np.sin(0.01*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(-600,600,5000).reshape(-1,1)\n",
    "ysol = true_1D_2(x)\n",
    "\n",
    "bc1_x = np.array(0).reshape(-1,1) \n",
    "bc1_y = np.array(0).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "\n",
    " \n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "\n",
    "y_true = true_1D_2(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "\n",
    "  #Collocation Points\n",
    "  # Latin Hypercube sampling for collocation points \n",
    "  # N_f sets of tuples(x,y)\n",
    "  x01 = np.array([[0.0, 1.0]])\n",
    "  sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "  x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "  x_coll_train = np.vstack((x_coll_train, bc1_x)) # append training points to collocation points \n",
    "\n",
    "  return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.omega = Parameter(0.1*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "\n",
    "                      \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        f = dy_dx - torch.cos(0.01*g)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + 100*loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "       \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "      \n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(seed):\n",
    "    x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_coll_train,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "  print(rep) \n",
    "  torch.manual_seed(rep*11)\n",
    "  start_time = time.time() \n",
    "  thresh_flag = 0\n",
    "\n",
    "  x_coll = torch.from_numpy(colloc_pts(N_f,123)).float().to(device)\n",
    "  f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "  for i in range(max_iter):\n",
    "    \n",
    "    train_step(i)\n",
    "\n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_coll,f_hat).cpu().detach().numpy()\n",
    "    if(thresh_flag == 0):\n",
    "        if(loss_np < loss_thresh):\n",
    "            time_threshold[rep] = time.time() - start_time\n",
    "            epoch_threshold[rep] = i+1            \n",
    "            thresh_flag = 1       \n",
    "    data_update(loss_np)\n",
    "    print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "  elapsed_time[rep] = time.time() - start_time  \n",
    "  print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "509236d6-c6b5-4579-8ffe-6c945b3ae573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D_FODE_rowdy_tune74\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 47.761093 Test MSE 5222.048794609697 Test RE 0.9999392767955084\n",
      "1 Train Loss 47.612225 Test MSE 5207.9922506727235 Test RE 0.9985925676447551\n",
      "2 Train Loss 47.612225 Test MSE 5207.9922506727235 Test RE 0.9985925676447551\n",
      "3 Train Loss 47.277916 Test MSE 5181.086738463788 Test RE 0.9960097647107444\n",
      "4 Train Loss 46.640625 Test MSE 5121.919404102788 Test RE 0.9903062839426865\n",
      "5 Train Loss 44.751892 Test MSE 4951.658608339754 Test RE 0.9737074928777364\n",
      "6 Train Loss 41.28275 Test MSE 4593.991556131111 Test RE 0.9378821298000303\n",
      "7 Train Loss 36.986584 Test MSE 4369.543924308972 Test RE 0.9146842886888615\n",
      "8 Train Loss 34.517006 Test MSE 4039.7761479501537 Test RE 0.879491841245162\n",
      "9 Train Loss 30.054152 Test MSE 3583.3612111969815 Test RE 0.8283205935019748\n",
      "10 Train Loss 26.069212 Test MSE 3197.0978695430313 Test RE 0.7824041227456124\n",
      "11 Train Loss 20.624496 Test MSE 2594.899413031354 Test RE 0.7048771440083458\n",
      "12 Train Loss 18.52576 Test MSE 2447.7853393753185 Test RE 0.6846046258707226\n",
      "13 Train Loss 15.310453 Test MSE 2091.6534612886207 Test RE 0.6328459884123246\n",
      "14 Train Loss 11.942498 Test MSE 1834.7161568372205 Test RE 0.5927036577467484\n",
      "15 Train Loss 8.309744 Test MSE 976.86008899393 Test RE 0.4324833055920685\n",
      "16 Train Loss 6.9735045 Test MSE 895.6299409143102 Test RE 0.41411166718184333\n",
      "17 Train Loss 5.598375 Test MSE 822.499865355198 Test RE 0.3968451549811834\n",
      "18 Train Loss 5.0287056 Test MSE 740.402822201801 Test RE 0.3765192608290858\n",
      "19 Train Loss 4.0909066 Test MSE 579.6354877685828 Test RE 0.333142966144268\n",
      "20 Train Loss 3.2201648 Test MSE 522.189988545354 Test RE 0.3162040552040835\n",
      "21 Train Loss 2.2174156 Test MSE 399.60581620082814 Test RE 0.2766107568886394\n",
      "22 Train Loss 1.5855974 Test MSE 440.86271568011006 Test RE 0.29053927632186505\n",
      "23 Train Loss 1.3662813 Test MSE 373.6724401438036 Test RE 0.2674845479201559\n",
      "24 Train Loss 1.2783514 Test MSE 348.2491334678261 Test RE 0.258224941025304\n",
      "25 Train Loss 1.0485172 Test MSE 303.6397098235573 Test RE 0.2411195633728725\n",
      "26 Train Loss 0.92601967 Test MSE 287.8578754634741 Test RE 0.2347697959858895\n",
      "27 Train Loss 0.84894437 Test MSE 303.00642091624013 Test RE 0.240867985528824\n",
      "28 Train Loss 0.80188364 Test MSE 312.65674997652275 Test RE 0.2446735759130058\n",
      "29 Train Loss 0.5648397 Test MSE 310.8151423129992 Test RE 0.24395192475234811\n",
      "30 Train Loss 0.46597484 Test MSE 269.34189181269954 Test RE 0.22709371648695292\n",
      "31 Train Loss 0.36369643 Test MSE 147.27498829949255 Test RE 0.16792589170606181\n",
      "32 Train Loss 0.33066452 Test MSE 156.05255987196608 Test RE 0.17285765350342433\n",
      "33 Train Loss 0.30805665 Test MSE 146.88417687268836 Test RE 0.16770293819092497\n",
      "34 Train Loss 0.27189106 Test MSE 142.22689615432446 Test RE 0.16502283010705973\n",
      "35 Train Loss 0.25966963 Test MSE 129.69663667082617 Test RE 0.15758595997363367\n",
      "36 Train Loss 0.22698173 Test MSE 131.0516307955923 Test RE 0.158407003779878\n",
      "37 Train Loss 0.17032278 Test MSE 118.80420353085373 Test RE 0.1508235185672787\n",
      "38 Train Loss 0.14538133 Test MSE 106.4117667139262 Test RE 0.14274075564828123\n",
      "39 Train Loss 0.13657336 Test MSE 96.52960786178828 Test RE 0.1359513221851593\n",
      "40 Train Loss 0.08037308 Test MSE 67.53716378510873 Test RE 0.11371678529490847\n",
      "41 Train Loss 0.057839718 Test MSE 41.88397751840791 Test RE 0.08955237817776245\n",
      "42 Train Loss 0.04732035 Test MSE 26.763546293794324 Test RE 0.07158548869996838\n",
      "43 Train Loss 0.037303485 Test MSE 12.21073597229433 Test RE 0.04835307354532958\n",
      "44 Train Loss 0.033523373 Test MSE 6.959378098439422 Test RE 0.03650382558498882\n",
      "45 Train Loss 0.03056869 Test MSE 8.036718666194814 Test RE 0.03922767310140583\n",
      "46 Train Loss 0.029203614 Test MSE 6.905408254810626 Test RE 0.03636200686597244\n",
      "47 Train Loss 0.028137717 Test MSE 7.379660748694553 Test RE 0.037589916700617586\n",
      "48 Train Loss 0.020286204 Test MSE 2.667499948802113 Test RE 0.02259984052423006\n",
      "49 Train Loss 0.018377196 Test MSE 1.2248689424517403 Test RE 0.015314329114749453\n",
      "50 Train Loss 0.017101493 Test MSE 1.3272931165662298 Test RE 0.015941771713701612\n",
      "51 Train Loss 0.01674619 Test MSE 1.1240970134659627 Test RE 0.014670842734697664\n",
      "52 Train Loss 0.015433171 Test MSE 1.0576816258595714 Test RE 0.014230843563858675\n",
      "53 Train Loss 0.015131361 Test MSE 0.4288209909455587 Test RE 0.009061314052759032\n",
      "54 Train Loss 0.014094277 Test MSE 0.3002443925681333 Test RE 0.007582119365673789\n",
      "55 Train Loss 0.01232376 Test MSE 0.13341683934517493 Test RE 0.005054270586222438\n",
      "56 Train Loss 0.010167985 Test MSE 0.34903493931906143 Test RE 0.008174997269186326\n",
      "57 Train Loss 0.009400396 Test MSE 0.20777101192626757 Test RE 0.006307331281735468\n",
      "58 Train Loss 0.00912214 Test MSE 0.10849379462077072 Test RE 0.004557803674956985\n",
      "59 Train Loss 0.009115891 Test MSE 0.11184580569797309 Test RE 0.004627676756703982\n",
      "60 Train Loss 0.009056324 Test MSE 0.09156221756887226 Test RE 0.00418708055066721\n",
      "61 Train Loss 0.0086166635 Test MSE 0.1448531087530541 Test RE 0.005266439200973923\n",
      "62 Train Loss 0.0076409737 Test MSE 0.20635110807066823 Test RE 0.00628574223188156\n",
      "63 Train Loss 0.0074186907 Test MSE 0.09774601930171388 Test RE 0.004326161266211401\n",
      "64 Train Loss 0.0066413498 Test MSE 0.029192942191017056 Test RE 0.0023642429262557027\n",
      "65 Train Loss 0.004948544 Test MSE 0.06505057648288345 Test RE 0.0035292200634919994\n",
      "66 Train Loss 0.004850634 Test MSE 0.10312830144680316 Test RE 0.00444367304526401\n",
      "67 Train Loss 0.004820367 Test MSE 0.0987270055589496 Test RE 0.004347815906692656\n",
      "68 Train Loss 0.00472089 Test MSE 0.049134487121226866 Test RE 0.003067230246743628\n",
      "69 Train Loss 0.004264122 Test MSE 0.029208950648568873 Test RE 0.0023648910742749317\n",
      "70 Train Loss 0.0040802015 Test MSE 0.0558511025316766 Test RE 0.003270160286361427\n",
      "71 Train Loss 0.0038319824 Test MSE 0.03379596677285736 Test RE 0.002543815457035192\n",
      "72 Train Loss 0.0036647243 Test MSE 0.032413693901428106 Test RE 0.0024912506668071863\n",
      "73 Train Loss 0.0035207001 Test MSE 0.025928239167041095 Test RE 0.002228125642208272\n",
      "74 Train Loss 0.0033660152 Test MSE 0.03760687717128902 Test RE 0.002683408557964313\n",
      "75 Train Loss 0.0033615606 Test MSE 0.04123634990390356 Test RE 0.002809915582587124\n",
      "76 Train Loss 0.003360875 Test MSE 0.04280180889711065 Test RE 0.002862755294156045\n",
      "77 Train Loss 0.0033600086 Test MSE 0.04678927281837137 Test RE 0.002993135049077993\n",
      "78 Train Loss 0.0033344815 Test MSE 0.04455276402859762 Test RE 0.002920723809279889\n",
      "79 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "80 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "81 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "82 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "83 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "84 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "85 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "86 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "87 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "88 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "89 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "90 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "91 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "92 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "94 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "95 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "96 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "97 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "98 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "99 Train Loss 0.0033146318 Test MSE 0.041898090870957846 Test RE 0.0028323719310325663\n",
      "Training time: 77.92\n",
      "Training time: 77.92\n",
      "1D_FODE_rowdy_tune74\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "1 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "2 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "3 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "4 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "5 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "6 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "7 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "8 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "9 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "10 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "11 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "12 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "13 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "14 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "15 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "16 Train Loss 47.58819 Test MSE 5209.683556902406 Test RE 0.9987547019808878\n",
      "17 Train Loss 47.242935 Test MSE 5184.823203536295 Test RE 0.9963688481574301\n",
      "18 Train Loss 46.898975 Test MSE 5159.106720286706 Test RE 0.9938948048604945\n",
      "19 Train Loss 46.243965 Test MSE 5106.013180022824 Test RE 0.9887673801638914\n",
      "20 Train Loss 44.87471 Test MSE 4970.115842616505 Test RE 0.9755205450620507\n",
      "21 Train Loss 43.930748 Test MSE 4904.176667118221 Test RE 0.9690277588063987\n",
      "22 Train Loss 42.690662 Test MSE 4773.698694991478 Test RE 0.9560501337955407\n",
      "23 Train Loss 41.30385 Test MSE 4644.3747709042045 Test RE 0.943011075092402\n",
      "24 Train Loss 40.315178 Test MSE 4583.598071525839 Test RE 0.9368205928973287\n",
      "25 Train Loss 38.266014 Test MSE 4313.489369594271 Test RE 0.9087983517726432\n",
      "26 Train Loss 34.73705 Test MSE 3999.258996372543 Test RE 0.8750702714137905\n",
      "27 Train Loss 31.021374 Test MSE 3618.196291227453 Test RE 0.8323370496837247\n",
      "28 Train Loss 28.115158 Test MSE 3361.5150227954387 Test RE 0.8022702593147022\n",
      "29 Train Loss 25.536844 Test MSE 3171.2211403242327 Test RE 0.7792313713837817\n",
      "30 Train Loss 23.565891 Test MSE 2968.482233452397 Test RE 0.7539115411055495\n",
      "31 Train Loss 21.279366 Test MSE 2742.2176355046695 Test RE 0.7246096683073521\n",
      "32 Train Loss 20.117897 Test MSE 2567.1732402483417 Test RE 0.7011012690658704\n",
      "33 Train Loss 18.269321 Test MSE 2279.366136428366 Test RE 0.6606329190634329\n",
      "34 Train Loss 16.326704 Test MSE 1984.4053315599497 Test RE 0.61640812878977\n",
      "35 Train Loss 14.7239275 Test MSE 1811.8080315561065 Test RE 0.5889918086701326\n",
      "36 Train Loss 12.391374 Test MSE 1632.663524850986 Test RE 0.5591154713600548\n",
      "37 Train Loss 11.7247925 Test MSE 1665.813545470889 Test RE 0.5647631596441015\n",
      "38 Train Loss 11.277025 Test MSE 1636.9599241250296 Test RE 0.5598506519382034\n",
      "39 Train Loss 9.583945 Test MSE 1441.446408639149 Test RE 0.5253544495304697\n",
      "40 Train Loss 8.678685 Test MSE 1290.101468604822 Test RE 0.4970099685553134\n",
      "41 Train Loss 8.052117 Test MSE 1216.7885577133234 Test RE 0.48268157728196764\n",
      "42 Train Loss 7.363018 Test MSE 1116.2250207866996 Test RE 0.4623054798508427\n",
      "43 Train Loss 7.0618877 Test MSE 1095.6879320403318 Test RE 0.45803282599086736\n",
      "44 Train Loss 5.0560913 Test MSE 843.4509479602012 Test RE 0.4018676803025611\n",
      "45 Train Loss 4.44028 Test MSE 775.3670183738305 Test RE 0.38530693559547413\n",
      "46 Train Loss 4.298386 Test MSE 733.5926073008845 Test RE 0.3747836511083727\n",
      "47 Train Loss 4.1471653 Test MSE 685.2462516962486 Test RE 0.36222339686462135\n",
      "48 Train Loss 3.444093 Test MSE 548.6423362964853 Test RE 0.32411402433051195\n",
      "49 Train Loss 3.1718767 Test MSE 556.0450195901096 Test RE 0.32629328952449693\n",
      "50 Train Loss 2.9797535 Test MSE 574.014416660134 Test RE 0.33152368776292873\n",
      "51 Train Loss 2.4600217 Test MSE 430.4367994942456 Test RE 0.2870832548394309\n",
      "52 Train Loss 1.814724 Test MSE 337.80923720784347 Test RE 0.2543249250306232\n",
      "53 Train Loss 1.4192581 Test MSE 320.1993972827157 Test RE 0.24760728588959627\n",
      "54 Train Loss 1.1478921 Test MSE 305.1803593259945 Test RE 0.24173050241894076\n",
      "55 Train Loss 0.8520789 Test MSE 278.63677707500153 Test RE 0.23097893987305518\n",
      "56 Train Loss 0.7534363 Test MSE 226.36843562194454 Test RE 0.20819059167777082\n",
      "57 Train Loss 0.64564574 Test MSE 190.41621662533166 Test RE 0.19094361066641266\n",
      "58 Train Loss 0.41628405 Test MSE 144.34222576416934 Test RE 0.16624548665190564\n",
      "59 Train Loss 0.30652824 Test MSE 116.60057797488996 Test RE 0.14941820550032833\n",
      "60 Train Loss 0.2897852 Test MSE 123.00620778911768 Test RE 0.15346759224042789\n",
      "61 Train Loss 0.25188258 Test MSE 126.16365677144375 Test RE 0.15542479341847856\n",
      "62 Train Loss 0.23987071 Test MSE 124.07624842803092 Test RE 0.15413366013727464\n",
      "63 Train Loss 0.20822956 Test MSE 117.33100623640796 Test RE 0.14988547973365512\n",
      "64 Train Loss 0.19022724 Test MSE 112.28631044838818 Test RE 0.1466278851212555\n",
      "65 Train Loss 0.1616879 Test MSE 92.55896074363119 Test RE 0.13312585177508973\n",
      "66 Train Loss 0.13891312 Test MSE 72.56539912918292 Test RE 0.11787398303221919\n",
      "67 Train Loss 0.12977685 Test MSE 77.99517528695574 Test RE 0.12220445369756165\n",
      "68 Train Loss 0.115099005 Test MSE 70.55945771589708 Test RE 0.11623335720858329\n",
      "69 Train Loss 0.100983016 Test MSE 59.16351369788408 Test RE 0.10643394116067258\n",
      "70 Train Loss 0.08559917 Test MSE 50.18244570336916 Test RE 0.09802324520197238\n",
      "71 Train Loss 0.07632724 Test MSE 43.70878211108898 Test RE 0.09148239323265517\n",
      "72 Train Loss 0.07362852 Test MSE 40.6240396395685 Test RE 0.08819515307174261\n",
      "73 Train Loss 0.06775898 Test MSE 38.56625470330244 Test RE 0.0859323912752633\n",
      "74 Train Loss 0.0542628 Test MSE 34.530777055292575 Test RE 0.08131231773753188\n",
      "75 Train Loss 0.050104763 Test MSE 31.24701491429992 Test RE 0.07734948598950876\n",
      "76 Train Loss 0.04826118 Test MSE 31.412793598256656 Test RE 0.07755440049522316\n",
      "77 Train Loss 0.045522198 Test MSE 26.88573216651785 Test RE 0.07174871025115288\n",
      "78 Train Loss 0.032760933 Test MSE 16.3621462152073 Test RE 0.055972318832481625\n",
      "79 Train Loss 0.026759109 Test MSE 13.342743224515333 Test RE 0.05054471229261096\n",
      "80 Train Loss 0.023388194 Test MSE 10.539304254429057 Test RE 0.04492200540346927\n",
      "81 Train Loss 0.022295052 Test MSE 8.483136202512812 Test RE 0.04030244413480313\n",
      "82 Train Loss 0.02155059 Test MSE 8.179085635201378 Test RE 0.03957359841487326\n",
      "83 Train Loss 0.020610048 Test MSE 5.505374763845597 Test RE 0.03246733249170785\n",
      "84 Train Loss 0.019248309 Test MSE 4.513545344088462 Test RE 0.029397613080729173\n",
      "85 Train Loss 0.018326262 Test MSE 3.6612932613167324 Test RE 0.0264771004339899\n",
      "86 Train Loss 0.018189289 Test MSE 3.334512895844182 Test RE 0.025267912560434803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 Train Loss 0.018145025 Test MSE 3.3035870826450306 Test RE 0.02515046646611621\n",
      "88 Train Loss 0.017888006 Test MSE 3.0697074737702454 Test RE 0.024243854028168324\n",
      "89 Train Loss 0.016707474 Test MSE 3.146253357495991 Test RE 0.0245442638352489\n",
      "90 Train Loss 0.013519505 Test MSE 3.164009778714701 Test RE 0.02461342627433642\n",
      "91 Train Loss 0.011791379 Test MSE 2.8790017105135077 Test RE 0.023478704297244257\n",
      "92 Train Loss 0.01065338 Test MSE 1.4519858856872145 Test RE 0.016673791329393436\n",
      "93 Train Loss 0.010015438 Test MSE 0.9880736323009996 Test RE 0.013754595392050152\n",
      "94 Train Loss 0.009930821 Test MSE 1.1446182285856543 Test RE 0.014804150573360635\n",
      "95 Train Loss 0.009758596 Test MSE 1.0638305984430576 Test RE 0.014272150065053772\n",
      "96 Train Loss 0.009665883 Test MSE 1.0744304878586868 Test RE 0.014343076881864189\n",
      "97 Train Loss 0.009043047 Test MSE 0.32643198049804867 Test RE 0.007905867194674107\n",
      "98 Train Loss 0.007765838 Test MSE 0.11784796844006395 Test RE 0.004750225369669736\n",
      "99 Train Loss 0.0074663814 Test MSE 0.06629248662710595 Test RE 0.0035627497601861124\n",
      "Training time: 88.98\n",
      "Training time: 88.98\n",
      "1D_FODE_rowdy_tune74\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 47.736443 Test MSE 5214.599031258692 Test RE 0.9992257666426883\n",
      "1 Train Loss 47.736443 Test MSE 5214.599031258692 Test RE 0.9992257666426883\n",
      "2 Train Loss 47.692944 Test MSE 5210.085241710776 Test RE 0.9987932049776446\n",
      "3 Train Loss 47.544594 Test MSE 5200.930465519141 Test RE 0.9979153164180216\n",
      "4 Train Loss 47.003872 Test MSE 5159.369460707856 Test RE 0.9939201128284332\n",
      "5 Train Loss 46.400845 Test MSE 5110.0302631071745 Test RE 0.9891562530147329\n",
      "6 Train Loss 45.727554 Test MSE 5065.125989005981 Test RE 0.9848005690533642\n",
      "7 Train Loss 44.435955 Test MSE 4934.801707671684 Test RE 0.9720486867209339\n",
      "8 Train Loss 42.73212 Test MSE 4835.5381307364205 Test RE 0.9622226392852659\n",
      "9 Train Loss 41.149933 Test MSE 4692.461582670337 Test RE 0.9478803659482883\n",
      "10 Train Loss 39.575092 Test MSE 4501.651518062688 Test RE 0.928408484801683\n",
      "11 Train Loss 36.815628 Test MSE 4173.520443894249 Test RE 0.8939319039489942\n",
      "12 Train Loss 34.161903 Test MSE 4069.9991214440142 Test RE 0.8827756034600053\n",
      "13 Train Loss 28.742207 Test MSE 3478.9689302847223 Test RE 0.8161658889109247\n",
      "14 Train Loss 24.263767 Test MSE 3115.6645843925467 Test RE 0.7723755429930682\n",
      "15 Train Loss 20.776585 Test MSE 2684.3734216213925 Test RE 0.7169264935553966\n",
      "16 Train Loss 18.552883 Test MSE 2412.5479618429104 Test RE 0.6796591102989191\n",
      "17 Train Loss 17.129663 Test MSE 2197.820187888225 Test RE 0.6487079851040813\n",
      "18 Train Loss 15.381607 Test MSE 2005.5328337717117 Test RE 0.6196808179820341\n",
      "19 Train Loss 12.432215 Test MSE 1598.4661568554475 Test RE 0.5532289362940023\n",
      "20 Train Loss 11.4050045 Test MSE 1499.5756663635566 Test RE 0.5358427473908117\n",
      "21 Train Loss 8.737717 Test MSE 1134.5781579466086 Test RE 0.4660906318737007\n",
      "22 Train Loss 7.86084 Test MSE 1068.6094760521203 Test RE 0.4523375851697383\n",
      "23 Train Loss 4.8337693 Test MSE 640.4080224480315 Test RE 0.35017210473802257\n",
      "24 Train Loss 4.157763 Test MSE 546.2324224358382 Test RE 0.3234014047887819\n",
      "25 Train Loss 2.7118018 Test MSE 344.88890554612766 Test RE 0.25697612543378645\n",
      "26 Train Loss 1.8294363 Test MSE 236.82964002133227 Test RE 0.21294683623105426\n",
      "27 Train Loss 1.4958063 Test MSE 220.93307749162264 Test RE 0.20567596126298957\n",
      "28 Train Loss 1.2385446 Test MSE 205.13407959431694 Test RE 0.198185589695111\n",
      "29 Train Loss 0.98993576 Test MSE 187.3317945068225 Test RE 0.18939081431495405\n",
      "30 Train Loss 0.61050636 Test MSE 122.04765756839686 Test RE 0.1528684594482402\n",
      "31 Train Loss 0.2196567 Test MSE 41.61994733175349 Test RE 0.08926966968969585\n",
      "32 Train Loss 0.19552644 Test MSE 54.926154484131686 Test RE 0.10255167582951116\n",
      "33 Train Loss 0.18317226 Test MSE 56.45398599328719 Test RE 0.10396818687105461\n",
      "34 Train Loss 0.13961 Test MSE 56.876310332147256 Test RE 0.1043563479795333\n",
      "35 Train Loss 0.08847044 Test MSE 18.530033766819606 Test RE 0.05956501699620614\n",
      "36 Train Loss 0.07978749 Test MSE 9.226213448615933 Test RE 0.04203053456181813\n",
      "37 Train Loss 0.057754897 Test MSE 4.559882064887052 Test RE 0.029548127876657716\n",
      "38 Train Loss 0.04010882 Test MSE 2.6135156852098413 Test RE 0.022369986403134378\n",
      "39 Train Loss 0.03816289 Test MSE 2.938217750024431 Test RE 0.023718933292948705\n",
      "40 Train Loss 0.03488041 Test MSE 2.1381819973248204 Test RE 0.02023370896689684\n",
      "41 Train Loss 0.027126767 Test MSE 0.8860169899949931 Test RE 0.013024893760932786\n",
      "42 Train Loss 0.017032929 Test MSE 0.6294736992410556 Test RE 0.01097847353298505\n",
      "43 Train Loss 0.013310179 Test MSE 0.9819886867555684 Test RE 0.0137121768831007\n",
      "44 Train Loss 0.011223948 Test MSE 0.8260069825014454 Test RE 0.012576072256414916\n",
      "45 Train Loss 0.009611004 Test MSE 0.8123444431474358 Test RE 0.012471631541401402\n",
      "46 Train Loss 0.008792966 Test MSE 0.8476681424567785 Test RE 0.012739902240566902\n",
      "47 Train Loss 0.0069489405 Test MSE 0.8597589862253979 Test RE 0.012830439305209258\n",
      "48 Train Loss 0.0064001996 Test MSE 0.5330518753132643 Test RE 0.010102710367328955\n",
      "49 Train Loss 0.006318329 Test MSE 0.8793729247499187 Test RE 0.0129759663197259\n",
      "50 Train Loss 0.0060964026 Test MSE 0.5589429914988008 Test RE 0.010345153097697943\n",
      "51 Train Loss 0.0051283045 Test MSE 0.2091538975379316 Test RE 0.006328286688751237\n",
      "52 Train Loss 0.0041424474 Test MSE 0.184623739811165 Test RE 0.005945617094784666\n",
      "53 Train Loss 0.0030317686 Test MSE 0.20570789416371796 Test RE 0.006275937989902339\n",
      "54 Train Loss 0.0023364045 Test MSE 0.0497048363895705 Test RE 0.0030849809670567816\n",
      "55 Train Loss 0.0021898653 Test MSE 0.08891577590954564 Test RE 0.00412612685561878\n",
      "56 Train Loss 0.0020546094 Test MSE 0.034498147605351054 Test RE 0.002570106101822169\n",
      "57 Train Loss 0.001899284 Test MSE 0.022747813027593922 Test RE 0.002087002597852015\n",
      "58 Train Loss 0.001794656 Test MSE 0.02690126632301492 Test RE 0.0022695488066897117\n",
      "59 Train Loss 0.0016594813 Test MSE 0.01206207269595599 Test RE 0.0015197219473447295\n",
      "60 Train Loss 0.0016168864 Test MSE 0.01748029674290989 Test RE 0.0018294795745625247\n",
      "61 Train Loss 0.0016011978 Test MSE 0.01680064755488794 Test RE 0.0017935610968367356\n",
      "62 Train Loss 0.0015570668 Test MSE 0.028618633559914338 Test RE 0.0023408717019173667\n",
      "63 Train Loss 0.0014819455 Test MSE 0.007351805148693899 Test RE 0.0011864519615860246\n",
      "64 Train Loss 0.0014322327 Test MSE 0.007195104189047976 Test RE 0.0011737394677156063\n",
      "65 Train Loss 0.0014171863 Test MSE 0.00995446899696939 Test RE 0.0013805820225019313\n",
      "66 Train Loss 0.0013272692 Test MSE 0.03186276082290036 Test RE 0.002469988131557894\n",
      "67 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "68 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "69 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "70 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "71 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "72 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "73 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "74 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "75 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "76 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "77 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "78 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "80 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "81 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "82 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "83 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "84 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "85 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "86 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "87 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "88 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "89 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "90 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "91 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "92 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "93 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "94 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "95 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "96 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "97 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "98 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "99 Train Loss 0.0013016311 Test MSE 0.020695100075768334 Test RE 0.0019906134315188667\n",
      "Training time: 69.41\n",
      "Training time: 69.41\n",
      "1D_FODE_rowdy_tune74\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 47.76903 Test MSE 5223.447700690546 Test RE 1.0000732019579988\n",
      "1 Train Loss 47.740826 Test MSE 5220.518720758177 Test RE 0.9997927736614673\n",
      "2 Train Loss nan Test MSE nan Test RE nan\n",
      "3 Train Loss nan Test MSE nan Test RE nan\n",
      "4 Train Loss nan Test MSE nan Test RE nan\n",
      "5 Train Loss nan Test MSE nan Test RE nan\n",
      "6 Train Loss nan Test MSE nan Test RE nan\n",
      "7 Train Loss nan Test MSE nan Test RE nan\n",
      "8 Train Loss nan Test MSE nan Test RE nan\n",
      "9 Train Loss nan Test MSE nan Test RE nan\n",
      "10 Train Loss nan Test MSE nan Test RE nan\n",
      "11 Train Loss nan Test MSE nan Test RE nan\n",
      "12 Train Loss nan Test MSE nan Test RE nan\n",
      "13 Train Loss nan Test MSE nan Test RE nan\n",
      "14 Train Loss nan Test MSE nan Test RE nan\n",
      "15 Train Loss nan Test MSE nan Test RE nan\n",
      "16 Train Loss nan Test MSE nan Test RE nan\n",
      "17 Train Loss nan Test MSE nan Test RE nan\n",
      "18 Train Loss nan Test MSE nan Test RE nan\n",
      "19 Train Loss nan Test MSE nan Test RE nan\n",
      "20 Train Loss nan Test MSE nan Test RE nan\n",
      "21 Train Loss nan Test MSE nan Test RE nan\n",
      "22 Train Loss nan Test MSE nan Test RE nan\n",
      "23 Train Loss nan Test MSE nan Test RE nan\n",
      "24 Train Loss nan Test MSE nan Test RE nan\n",
      "25 Train Loss nan Test MSE nan Test RE nan\n",
      "26 Train Loss nan Test MSE nan Test RE nan\n",
      "27 Train Loss nan Test MSE nan Test RE nan\n",
      "28 Train Loss nan Test MSE nan Test RE nan\n",
      "29 Train Loss nan Test MSE nan Test RE nan\n",
      "30 Train Loss nan Test MSE nan Test RE nan\n",
      "31 Train Loss nan Test MSE nan Test RE nan\n",
      "32 Train Loss nan Test MSE nan Test RE nan\n",
      "33 Train Loss nan Test MSE nan Test RE nan\n",
      "34 Train Loss nan Test MSE nan Test RE nan\n",
      "35 Train Loss nan Test MSE nan Test RE nan\n",
      "36 Train Loss nan Test MSE nan Test RE nan\n",
      "37 Train Loss nan Test MSE nan Test RE nan\n",
      "38 Train Loss nan Test MSE nan Test RE nan\n",
      "39 Train Loss nan Test MSE nan Test RE nan\n",
      "40 Train Loss nan Test MSE nan Test RE nan\n",
      "41 Train Loss nan Test MSE nan Test RE nan\n",
      "42 Train Loss nan Test MSE nan Test RE nan\n",
      "43 Train Loss nan Test MSE nan Test RE nan\n",
      "44 Train Loss nan Test MSE nan Test RE nan\n",
      "45 Train Loss nan Test MSE nan Test RE nan\n",
      "46 Train Loss nan Test MSE nan Test RE nan\n",
      "47 Train Loss nan Test MSE nan Test RE nan\n",
      "48 Train Loss nan Test MSE nan Test RE nan\n",
      "49 Train Loss nan Test MSE nan Test RE nan\n",
      "50 Train Loss nan Test MSE nan Test RE nan\n",
      "51 Train Loss nan Test MSE nan Test RE nan\n",
      "52 Train Loss nan Test MSE nan Test RE nan\n",
      "53 Train Loss nan Test MSE nan Test RE nan\n",
      "54 Train Loss nan Test MSE nan Test RE nan\n",
      "55 Train Loss nan Test MSE nan Test RE nan\n",
      "56 Train Loss nan Test MSE nan Test RE nan\n",
      "57 Train Loss nan Test MSE nan Test RE nan\n",
      "58 Train Loss nan Test MSE nan Test RE nan\n",
      "59 Train Loss nan Test MSE nan Test RE nan\n",
      "60 Train Loss nan Test MSE nan Test RE nan\n",
      "61 Train Loss nan Test MSE nan Test RE nan\n",
      "62 Train Loss nan Test MSE nan Test RE nan\n",
      "63 Train Loss nan Test MSE nan Test RE nan\n",
      "64 Train Loss nan Test MSE nan Test RE nan\n",
      "65 Train Loss nan Test MSE nan Test RE nan\n",
      "66 Train Loss nan Test MSE nan Test RE nan\n",
      "67 Train Loss nan Test MSE nan Test RE nan\n",
      "68 Train Loss nan Test MSE nan Test RE nan\n",
      "69 Train Loss nan Test MSE nan Test RE nan\n",
      "70 Train Loss nan Test MSE nan Test RE nan\n",
      "71 Train Loss nan Test MSE nan Test RE nan\n",
      "72 Train Loss nan Test MSE nan Test RE nan\n",
      "73 Train Loss nan Test MSE nan Test RE nan\n",
      "74 Train Loss nan Test MSE nan Test RE nan\n",
      "75 Train Loss nan Test MSE nan Test RE nan\n",
      "76 Train Loss nan Test MSE nan Test RE nan\n",
      "77 Train Loss nan Test MSE nan Test RE nan\n",
      "78 Train Loss nan Test MSE nan Test RE nan\n",
      "79 Train Loss nan Test MSE nan Test RE nan\n",
      "80 Train Loss nan Test MSE nan Test RE nan\n",
      "81 Train Loss nan Test MSE nan Test RE nan\n",
      "82 Train Loss nan Test MSE nan Test RE nan\n",
      "83 Train Loss nan Test MSE nan Test RE nan\n",
      "84 Train Loss nan Test MSE nan Test RE nan\n",
      "85 Train Loss nan Test MSE nan Test RE nan\n",
      "86 Train Loss nan Test MSE nan Test RE nan\n",
      "87 Train Loss nan Test MSE nan Test RE nan\n",
      "88 Train Loss nan Test MSE nan Test RE nan\n",
      "89 Train Loss nan Test MSE nan Test RE nan\n",
      "90 Train Loss nan Test MSE nan Test RE nan\n",
      "91 Train Loss nan Test MSE nan Test RE nan\n",
      "92 Train Loss nan Test MSE nan Test RE nan\n",
      "93 Train Loss nan Test MSE nan Test RE nan\n",
      "94 Train Loss nan Test MSE nan Test RE nan\n",
      "95 Train Loss nan Test MSE nan Test RE nan\n",
      "96 Train Loss nan Test MSE nan Test RE nan\n",
      "97 Train Loss nan Test MSE nan Test RE nan\n",
      "98 Train Loss nan Test MSE nan Test RE nan\n",
      "99 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 196.28\n",
      "Training time: 196.28\n",
      "1D_FODE_rowdy_tune74\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 47.573215 Test MSE 5208.5582835197465 Test RE 0.9986468323987593\n",
      "1 Train Loss nan Test MSE nan Test RE nan\n",
      "2 Train Loss nan Test MSE nan Test RE nan\n",
      "3 Train Loss nan Test MSE nan Test RE nan\n",
      "4 Train Loss nan Test MSE nan Test RE nan\n",
      "5 Train Loss nan Test MSE nan Test RE nan\n",
      "6 Train Loss nan Test MSE nan Test RE nan\n",
      "7 Train Loss nan Test MSE nan Test RE nan\n",
      "8 Train Loss nan Test MSE nan Test RE nan\n",
      "9 Train Loss nan Test MSE nan Test RE nan\n",
      "10 Train Loss nan Test MSE nan Test RE nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Train Loss nan Test MSE nan Test RE nan\n",
      "12 Train Loss nan Test MSE nan Test RE nan\n",
      "13 Train Loss nan Test MSE nan Test RE nan\n",
      "14 Train Loss nan Test MSE nan Test RE nan\n",
      "15 Train Loss nan Test MSE nan Test RE nan\n",
      "16 Train Loss nan Test MSE nan Test RE nan\n",
      "17 Train Loss nan Test MSE nan Test RE nan\n",
      "18 Train Loss nan Test MSE nan Test RE nan\n",
      "19 Train Loss nan Test MSE nan Test RE nan\n",
      "20 Train Loss nan Test MSE nan Test RE nan\n",
      "21 Train Loss nan Test MSE nan Test RE nan\n",
      "22 Train Loss nan Test MSE nan Test RE nan\n",
      "23 Train Loss nan Test MSE nan Test RE nan\n",
      "24 Train Loss nan Test MSE nan Test RE nan\n",
      "25 Train Loss nan Test MSE nan Test RE nan\n",
      "26 Train Loss nan Test MSE nan Test RE nan\n",
      "27 Train Loss nan Test MSE nan Test RE nan\n",
      "28 Train Loss nan Test MSE nan Test RE nan\n",
      "29 Train Loss nan Test MSE nan Test RE nan\n",
      "30 Train Loss nan Test MSE nan Test RE nan\n",
      "31 Train Loss nan Test MSE nan Test RE nan\n",
      "32 Train Loss nan Test MSE nan Test RE nan\n",
      "33 Train Loss nan Test MSE nan Test RE nan\n",
      "34 Train Loss nan Test MSE nan Test RE nan\n",
      "35 Train Loss nan Test MSE nan Test RE nan\n",
      "36 Train Loss nan Test MSE nan Test RE nan\n",
      "37 Train Loss nan Test MSE nan Test RE nan\n",
      "38 Train Loss nan Test MSE nan Test RE nan\n",
      "39 Train Loss nan Test MSE nan Test RE nan\n",
      "40 Train Loss nan Test MSE nan Test RE nan\n",
      "41 Train Loss nan Test MSE nan Test RE nan\n",
      "42 Train Loss nan Test MSE nan Test RE nan\n",
      "43 Train Loss nan Test MSE nan Test RE nan\n",
      "44 Train Loss nan Test MSE nan Test RE nan\n",
      "45 Train Loss nan Test MSE nan Test RE nan\n",
      "46 Train Loss nan Test MSE nan Test RE nan\n",
      "47 Train Loss nan Test MSE nan Test RE nan\n",
      "48 Train Loss nan Test MSE nan Test RE nan\n",
      "49 Train Loss nan Test MSE nan Test RE nan\n",
      "50 Train Loss nan Test MSE nan Test RE nan\n",
      "51 Train Loss nan Test MSE nan Test RE nan\n",
      "52 Train Loss nan Test MSE nan Test RE nan\n",
      "53 Train Loss nan Test MSE nan Test RE nan\n",
      "54 Train Loss nan Test MSE nan Test RE nan\n",
      "55 Train Loss nan Test MSE nan Test RE nan\n",
      "56 Train Loss nan Test MSE nan Test RE nan\n",
      "57 Train Loss nan Test MSE nan Test RE nan\n",
      "58 Train Loss nan Test MSE nan Test RE nan\n",
      "59 Train Loss nan Test MSE nan Test RE nan\n",
      "60 Train Loss nan Test MSE nan Test RE nan\n",
      "61 Train Loss nan Test MSE nan Test RE nan\n",
      "62 Train Loss nan Test MSE nan Test RE nan\n",
      "63 Train Loss nan Test MSE nan Test RE nan\n",
      "64 Train Loss nan Test MSE nan Test RE nan\n",
      "65 Train Loss nan Test MSE nan Test RE nan\n",
      "66 Train Loss nan Test MSE nan Test RE nan\n",
      "67 Train Loss nan Test MSE nan Test RE nan\n",
      "68 Train Loss nan Test MSE nan Test RE nan\n",
      "69 Train Loss nan Test MSE nan Test RE nan\n",
      "70 Train Loss nan Test MSE nan Test RE nan\n",
      "71 Train Loss nan Test MSE nan Test RE nan\n",
      "72 Train Loss nan Test MSE nan Test RE nan\n",
      "73 Train Loss nan Test MSE nan Test RE nan\n",
      "74 Train Loss nan Test MSE nan Test RE nan\n",
      "75 Train Loss nan Test MSE nan Test RE nan\n",
      "76 Train Loss nan Test MSE nan Test RE nan\n",
      "77 Train Loss nan Test MSE nan Test RE nan\n",
      "78 Train Loss nan Test MSE nan Test RE nan\n",
      "79 Train Loss nan Test MSE nan Test RE nan\n",
      "80 Train Loss nan Test MSE nan Test RE nan\n",
      "81 Train Loss nan Test MSE nan Test RE nan\n",
      "82 Train Loss nan Test MSE nan Test RE nan\n",
      "83 Train Loss nan Test MSE nan Test RE nan\n",
      "84 Train Loss nan Test MSE nan Test RE nan\n",
      "85 Train Loss nan Test MSE nan Test RE nan\n",
      "86 Train Loss nan Test MSE nan Test RE nan\n",
      "87 Train Loss nan Test MSE nan Test RE nan\n",
      "88 Train Loss nan Test MSE nan Test RE nan\n",
      "89 Train Loss nan Test MSE nan Test RE nan\n",
      "90 Train Loss nan Test MSE nan Test RE nan\n",
      "91 Train Loss nan Test MSE nan Test RE nan\n",
      "92 Train Loss nan Test MSE nan Test RE nan\n",
      "93 Train Loss nan Test MSE nan Test RE nan\n",
      "94 Train Loss nan Test MSE nan Test RE nan\n",
      "95 Train Loss nan Test MSE nan Test RE nan\n",
      "96 Train Loss nan Test MSE nan Test RE nan\n",
      "97 Train Loss nan Test MSE nan Test RE nan\n",
      "98 Train Loss nan Test MSE nan Test RE nan\n",
      "99 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 198.19\n",
      "Training time: 198.19\n",
      "1D_FODE_rowdy_tune74\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 47.769363 Test MSE 5222.299021667661 Test RE 0.9999632337591365\n",
      "1 Train Loss 47.768593 Test MSE 5219.33684408603 Test RE 0.9996795953831631\n",
      "2 Train Loss 47.73469 Test MSE 5189.543977917002 Test RE 0.99682234118621\n",
      "3 Train Loss 47.666634 Test MSE 5170.192474999222 Test RE 0.9949620595352332\n",
      "4 Train Loss 47.381695 Test MSE 5120.395624280167 Test RE 0.9901589640744406\n",
      "5 Train Loss 46.820667 Test MSE 5090.866969557957 Test RE 0.9872997770771539\n",
      "6 Train Loss 45.930458 Test MSE 5018.518872901221 Test RE 0.9802592419257571\n",
      "7 Train Loss 45.54915 Test MSE 4986.028358205065 Test RE 0.9770809293139592\n",
      "8 Train Loss 45.24569 Test MSE 4954.911633942258 Test RE 0.974027282224914\n",
      "9 Train Loss 44.884315 Test MSE 4929.808054261938 Test RE 0.9715567416399548\n",
      "10 Train Loss 44.595955 Test MSE 4897.01887427648 Test RE 0.9683203380570586\n",
      "11 Train Loss 44.354786 Test MSE 4883.017736659296 Test RE 0.9669350778395653\n",
      "12 Train Loss 44.15761 Test MSE 4866.947213050599 Test RE 0.9653426241395145\n",
      "13 Train Loss 43.846584 Test MSE 4845.180893240175 Test RE 0.9631815670283631\n",
      "14 Train Loss 43.582462 Test MSE 4828.1638198828205 Test RE 0.9614886531172102\n",
      "15 Train Loss 43.32675 Test MSE 4793.163665654063 Test RE 0.9579973195435777\n",
      "16 Train Loss 43.098434 Test MSE 4778.648721540036 Test RE 0.9565456874178291\n",
      "17 Train Loss 42.87949 Test MSE 4757.519806622796 Test RE 0.9544286493429823\n",
      "18 Train Loss 42.56059 Test MSE 4723.812836257151 Test RE 0.9510415816191852\n",
      "19 Train Loss 42.361374 Test MSE 4706.489864750965 Test RE 0.9492961697520584\n",
      "20 Train Loss 42.12539 Test MSE 4687.341393060847 Test RE 0.9473630839452652\n",
      "21 Train Loss 41.94829 Test MSE 4668.149519016614 Test RE 0.9454216505786668\n",
      "22 Train Loss 41.662685 Test MSE 4636.866789125601 Test RE 0.9422485425647557\n",
      "23 Train Loss 41.35366 Test MSE 4608.785133799545 Test RE 0.9393910006950702\n",
      "24 Train Loss 41.067043 Test MSE 4578.447799373201 Test RE 0.9362941247117932\n",
      "25 Train Loss 40.72229 Test MSE 4548.913934107965 Test RE 0.933269396248442\n",
      "26 Train Loss 40.363167 Test MSE 4529.599665460448 Test RE 0.9312860008279373\n",
      "27 Train Loss 40.019337 Test MSE 4499.412185112495 Test RE 0.9281775390790676\n",
      "28 Train Loss 39.637867 Test MSE 4474.727627202255 Test RE 0.9256279657021237\n",
      "29 Train Loss 39.210583 Test MSE 4436.4157198744715 Test RE 0.9216569077149218\n",
      "30 Train Loss 38.926136 Test MSE 4400.851008980409 Test RE 0.9179552236692082\n",
      "31 Train Loss 38.520397 Test MSE 4367.958058795185 Test RE 0.91451828762168\n",
      "32 Train Loss 38.140873 Test MSE 4328.159779293786 Test RE 0.9103424760487931\n",
      "33 Train Loss 37.60874 Test MSE 4288.202580121665 Test RE 0.9061306309035398\n",
      "34 Train Loss 37.313004 Test MSE 4261.097595536047 Test RE 0.9032623439719535\n",
      "35 Train Loss 36.753094 Test MSE 4205.0768950345755 Test RE 0.8973050988364015\n",
      "36 Train Loss 36.35287 Test MSE 4150.341776494735 Test RE 0.8914461129566954\n",
      "37 Train Loss 35.976414 Test MSE 4102.2010236669175 Test RE 0.8862609907212435\n",
      "38 Train Loss 35.52429 Test MSE 4065.9173804745196 Test RE 0.882332831224996\n",
      "39 Train Loss 34.873608 Test MSE 4030.5947138365805 Test RE 0.8784918366102974\n",
      "40 Train Loss 34.379356 Test MSE 3983.378615904932 Test RE 0.873331165315981\n",
      "41 Train Loss 34.133347 Test MSE 3962.5173368781543 Test RE 0.8710413100335569\n",
      "42 Train Loss 33.812492 Test MSE 3931.8248735961906 Test RE 0.8676613407144418\n",
      "43 Train Loss 33.45777 Test MSE 3893.0667675270893 Test RE 0.8633742479632414\n",
      "44 Train Loss 32.896946 Test MSE 3850.55215487689 Test RE 0.8586470254681674\n",
      "45 Train Loss 32.505997 Test MSE 3797.8603278492706 Test RE 0.8527518280933543\n",
      "46 Train Loss 32.00512 Test MSE 3736.156047103159 Test RE 0.8457960803614354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 Train Loss 31.493307 Test MSE 3695.264354088223 Test RE 0.8411547873475831\n",
      "48 Train Loss 31.138603 Test MSE 3633.5430102299256 Test RE 0.8341003766932428\n",
      "49 Train Loss 30.747017 Test MSE 3567.802567177318 Test RE 0.8265203891558484\n",
      "50 Train Loss 30.096975 Test MSE 3493.9278300040082 Test RE 0.8179186852194235\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1927a7d344bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c811153ade4d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-6196c2358d2e>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for tune_reps in range(5):\n",
    "for tune_reps in range(74,75):\n",
    "  label = \"1D_FODE_rowdy_tune\"+str(tune_reps)  \n",
    "  max_reps = 10\n",
    "  max_iter = 100\n",
    "\n",
    "  train_loss_full = []\n",
    "  test_mse_full = []\n",
    "  test_re_full = []\n",
    "  alpha_full = []\n",
    "  omega_full = []\n",
    "  elapsed_time= np.zeros((max_reps,1))\n",
    "  time_threshold = np.empty((max_reps,1))\n",
    "  time_threshold[:] = np.nan\n",
    "  epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "  \n",
    "  n_val = lrnr_tune[tune_reps,1]\n",
    "  rowdy_terms = int(lrnr_tune[tune_reps,2])\n",
    "\n",
    "  for reps in range(max_reps):  \n",
    "      print(label) \n",
    "      train_loss = []\n",
    "      test_mse_loss = []\n",
    "      test_re_loss = []   \n",
    "      alpha_val = []\n",
    "      omega_val = []\n",
    "      \n",
    "      torch.manual_seed(reps*36)\n",
    "      N_f = 10000 #Total number of collocation points\n",
    "    \n",
    "      layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "      PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "      \n",
    "    \n",
    "      PINN.to(device)\n",
    "\n",
    "      'Neural Network Summary'\n",
    "      print(PINN)\n",
    "\n",
    "      params = list(PINN.parameters())\n",
    "      \n",
    "      optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lrnr_tune[tune_reps,0], \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "    \n",
    "\n",
    "      \n",
    "      train_model(max_iter,reps)\n",
    "\n",
    "      \n",
    "      torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "      train_loss_full.append(train_loss)\n",
    "      test_mse_full.append(test_mse_loss)\n",
    "      test_re_full.append(test_re_loss)\n",
    "      alpha_full.append(alpha_val)\n",
    "      omega_full.append(omega_val)\n",
    "      \n",
    "      \n",
    "      print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "  mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"alpha\": alpha_full,\"omega\": omega_full, \"label\": label}\n",
    "  savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "77a1e198-62ae-4129-82a3-1a1f3e433466"
   },
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2yA4xTDHldi"
   },
   "outputs": [],
   "source": [
    "#3,4,8,9,13,14,18,19,23,24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ky6HsA0AWWTD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF7H51LTWXDq",
    "outputId": "1986cfc6-aa7b-43ff-e3e8-c586ef7bafd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   0.0013891956150622548\n",
      "1   0.0013014935764086072\n",
      "2   0.0005858716456776863\n",
      "3   0.0006306781566012866\n",
      "4   0.06291599062512308\n",
      "5   nan\n",
      "6   0.09717181563862645\n",
      "7   nan\n",
      "8   0.11368680706548429\n",
      "9   nan\n",
      "10   0.39919870230752547\n",
      "11   nan\n",
      "12   nan\n",
      "13   0.20249851877485195\n",
      "14   nan\n",
      "15   0.13537710462142372\n",
      "16   nan\n",
      "17   nan\n",
      "18   nan\n",
      "19   nan\n",
      "20   nan\n",
      "21   nan\n",
      "22   nan\n",
      "23   nan\n",
      "24   nan\n",
      "25   nan\n",
      "26   nan\n",
      "27   0.10758706242782048\n",
      "28   nan\n",
      "29   nan\n",
      "30   0.032931817914684566\n",
      "31   nan\n",
      "32   0.11350014754172119\n",
      "33   nan\n",
      "34   nan\n",
      "35   nan\n",
      "36   nan\n",
      "37   nan\n",
      "38   nan\n",
      "39   nan\n",
      "40   nan\n",
      "41   nan\n",
      "42   nan\n",
      "43   nan\n",
      "44   nan\n",
      "45   nan\n",
      "46   nan\n",
      "47   nan\n",
      "48   0.9004347758176603\n",
      "49   nan\n",
      "50   0.053437242536784155\n",
      "51   0.06575718970605161\n",
      "52   nan\n",
      "53   nan\n",
      "54   nan\n",
      "55   0.018006802049445766\n",
      "56   0.1449684520195847\n",
      "57   nan\n",
      "58   nan\n",
      "59   0.7680885852944002\n",
      "60   0.010590970869461562\n",
      "61   nan\n",
      "62   nan\n",
      "63   nan\n",
      "64   0.9257600402297508\n",
      "65   0.046682751023253324\n",
      "66   0.12031330631615815\n",
      "67   nan\n",
      "68   0.8023275786768794\n",
      "69   nan\n"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(70):\n",
    "#  if tune_reps not in s:\n",
    "    label = \"1D_FODE_rowdy_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2eNXAFRRtWs",
    "outputId": "737b4c47-e8bf-4e68-c774-00d25a78ecb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05, 5.  , 2.  ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrnr_tune[2]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "atanh_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
