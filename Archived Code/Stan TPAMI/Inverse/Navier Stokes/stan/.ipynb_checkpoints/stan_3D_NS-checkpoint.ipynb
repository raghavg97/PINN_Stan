{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLsZ-c_nCQr2",
    "outputId": "0238c820-5951-4e75-a35b-19e4de8c9b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SV23gJi7JexL",
    "outputId": "6f051579-557f-463f-d7b4-955ed617736e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOyXTKXGJf97",
    "outputId": "11b7b7db-47b0-4cf8-c699-473f1c6b8c5f"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/MURI Aug17 Thin Plate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "APjvgycyCTj0",
    "outputId": "19bce659-211e-4bec-d94d-7c94148b0d09"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lxFUD2gACQr7"
   },
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "data = scipy.io.loadmat('../cylinder_nektar_wake.mat')\n",
    "           \n",
    "U_star = data['U_star'] # N x 2 x T\n",
    "P_star = data['p_star'] # N x T\n",
    "t_star = data['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2\n",
    "\n",
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]\n",
    "\n",
    "# # Rearrange Data \n",
    "XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
    "YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
    "TT = np.tile(t_star, (1,N)).T # N x T\n",
    "\n",
    "UU = U_star[:,0,:] # N x T\n",
    "VV = U_star[:,1,:] # N x T\n",
    "PP = P_star # N x T\n",
    "\n",
    "x = XX.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "y = YY.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "t = TT.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "\n",
    "u_true = UU.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "v_true = VV.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "p_true = PP.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "\n",
    "label = \"3D_NS_stan\"\n",
    "\n",
    "loss_thresh = 10000\n",
    "\n",
    "N_train = x.shape[0]\n",
    "xyt = np.hstack((x,y,t))\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]\n",
    "\n",
    "# xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "np.random.seed(1234)\n",
    "idx = np.random.choice(N_train, 5000, replace=False)\n",
    "u_true_test = u_true[idx,:]\n",
    "v_true_test = v_true[idx,:]\n",
    "p_true_test = p_true[idx,:]\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "v_true_norm = np.linalg.norm(v_true,2)\n",
    "p_true_norm = np.linalg.norm(p_true,2)\n",
    "\n",
    "# u_true_test = torch.from_numpy(u_true_test).float().to(device)\n",
    "# v_true_test = torch.from_numpy(v_true_test).float().to(device)\n",
    "# p_true_test = torch.from_numpy(p_true_test).float().to(device)\n",
    "\n",
    "x_tensor = torch.from_numpy(x[idx,:]).float().to(device)\n",
    "y_tensor = torch.from_numpy(y[idx,:]).float().to(device)\n",
    "t_tensor = torch.from_numpy(t[idx,:]).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ],\n",
       "       [ 0.1],\n",
       "       [ 0.2],\n",
       "       ...,\n",
       "       [19.7],\n",
       "       [19.8],\n",
       "       [19.9]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CUcT7YuXCQr7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# x = np.linspace(1,8,100).reshape(-1,1)\n",
    "# y = np.linspace(-2,2,100).reshape(-1,1)\n",
    "# t = np.linspace(0,20,100).reshape(-1,1)\n",
    "\n",
    "# X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "# X = X.flatten('F').reshape(-1,1)\n",
    "# Y = Y.flatten('F').reshape(-1,1)\n",
    "# T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "# xyt = np.hstack((X,Y,T))\n",
    "\n",
    "# initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "# DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "# NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "# NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "# NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "# NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "# xyt_initial = xyt[initial_pts,:]\n",
    "# xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "# xyt_NBC_x0 = xyt[NBC_pts_x0,:]\n",
    "# xyt_NBC_x1 = xyt[NBC_pts_x1,:]\n",
    "\n",
    "# #xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "# xyt_NBC_y1 = xyt[NBC_pts_y1,:]\n",
    "\n",
    "# u_initial = 300*np.ones((np.shape(xyt_initial)[0],1))\n",
    "# u_DBC = 1000*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "# xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "# #xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "# xyt_NBC_x = np.vstack((xyt_NBC_x0,xyt_NBC_x1))\n",
    "# #xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "# xyt_NBC_y = np.vstack((xyt_NBC_y1))\n",
    "\n",
    "# u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "\n",
    "# lb_xyt = xyt[0]\n",
    "# ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fea_data = scipy.io.loadmat('./../3D_HTTP_FEA.mat')\n",
    "# xy = fea_data['xy']\n",
    "# t = fea_data['t']/3000\n",
    "# xyt = np.zeros((497*101,3))\n",
    "# u_true = np.ones((497*101,1))\n",
    "\n",
    "\n",
    "# for i in range(101):\n",
    "#     t_temp = t[0,i]*np.ones((497,1))\n",
    "#     xyt[497*i:497*(i+1)] = np.hstack((xy,t_temp))\n",
    "#     u_true[497*i:497*(i+1)] = fea_data['u'][:,i].reshape(-1,1)\n",
    "#     #print(i)\n",
    "# #print(xyt)\n",
    "\n",
    "# xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "# u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gp2G6x6BCQr8"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_T,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    idx = np.random.choice(N_train, N_T, replace=False)\n",
    "    x_train = x[idx,:]\n",
    "    y_train = y[idx,:]\n",
    "    t_train = t[idx,:]\n",
    "    u_train = u_true[idx,:]\n",
    "    v_train = v_true[idx,:]\n",
    "    \n",
    "    return x_train,y_train,t_train,u_train,v_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VRolFlBzCQr9"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requires_grad = True\n",
    "        \n",
    "        self.lambda2 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda2.requires_grad = True\n",
    "    \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = 2.0*(xyt- lbxyt)/(ubxyt - lbxyt)-1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_uv(self,x_train,y_train,t_train,u_train,v_train):\n",
    "        x1 = x_train.clone()\n",
    "        x1.requires_grad = True\n",
    "        \n",
    "        y1 = y_train.clone()\n",
    "        y1.requires_grad = True\n",
    "        \n",
    "        t1 = t_train.clone()\n",
    "        t1.requires_grad = True\n",
    "        \n",
    "#         print(torch.cat((x1,y1,t1),dim=1).shape)\n",
    "        psi_p = self.forward(torch.cat((x1,y1,t1),dim =1))\n",
    "        \n",
    "        psi = psi_p[:,0:1]\n",
    "        \n",
    "#         print(psi.shape)\n",
    "        psi_x = autograd.grad(psi,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        psi_y = autograd.grad(psi,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "                \n",
    "        loss_u = self.loss_function(psi_y,u_train)\n",
    "        loss_v = self.loss_function(-1*psi_x,v_train)\n",
    "                \n",
    "        return loss_u + loss_v\n",
    "    \n",
    "    def loss_PDE(self, x_train,y_train,t_train,fg_hat):\n",
    "        \n",
    "        x1 = x_train.clone()\n",
    "        x1.requires_grad = True\n",
    "        \n",
    "        y1 = y_train.clone()\n",
    "        y1.requires_grad = True\n",
    "        \n",
    "        t1 = t_train.clone()\n",
    "        t1.requires_grad = True\n",
    "        \n",
    "        psi_p = self.forward(torch.cat((x1,y1,t1),dim =1))\n",
    "        psi = psi_p[:,0:1]\n",
    "        p = psi_p[:,1:2]\n",
    "        \n",
    "        \n",
    "        u = autograd.grad(psi,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v = -1*autograd.grad(psi,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        u_t = autograd.grad(u,t1,torch.ones([t1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_x = autograd.grad(u,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        u_xx = autograd.grad(u_x,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "                \n",
    "        u_y = autograd.grad(u,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        u_yy = autograd.grad(u_y,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        #---------------------------------------------------------------------------------\n",
    "        \n",
    "        v_t = autograd.grad(v,t1,torch.ones([t1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        v_x = autograd.grad(v,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v_xx = autograd.grad(v_x,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "                \n",
    "        v_y = autograd.grad(v,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v_yy = autograd.grad(v_y,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        #------------------------------------------------------------------------------------\n",
    "        p_x = autograd.grad(p,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        p_y = autograd.grad(p,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "            \n",
    "            \n",
    "\n",
    "        f = u_t + self.lambda1*(u*u_x + v*u_y) + p_x - self.lambda2*(u_xx + u_yy)\n",
    "        g = v_t + self.lambda1*(u*v_x + v*v_y) + p_y - self.lambda2*(v_xx + v_yy)\n",
    "        \n",
    "        loss_f = self.loss_function(f,fg_hat)\n",
    "        loss_g = self.loss_function(g,fg_hat)\n",
    "                \n",
    "        return loss_f + loss_g\n",
    "    \n",
    "    def loss(self,x_train,y_train,t_train,u_train,v_train,fg_hat):\n",
    "\n",
    "        loss_uv = self.loss_uv(x_train,y_train,t_train,u_train,v_train)\n",
    "        loss_fg = self.loss_PDE(x_train,y_train,t_train,fg_hat)\n",
    "        loss_val = loss_uv + loss_fg\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "       \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        x1 = x_tensor.clone()\n",
    "        x1.requires_grad = True\n",
    "        \n",
    "        y1 = y_tensor.clone()\n",
    "        y1.requires_grad = True\n",
    "        \n",
    "        t1 = t_tensor.clone()\n",
    "        t1.requires_grad = True\n",
    "        \n",
    "        psi_p_pred = self.forward(torch.cat((x1,y1,t1),dim =1))\n",
    "        psi = psi_p_pred[:,0:1]\n",
    "        p_pred = psi_p_pred[:,1:2]\n",
    "        \n",
    "        u_pred = autograd.grad(psi,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v_pred = -1*autograd.grad(psi,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "   \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        v_pred = v_pred.cpu().detach().numpy()\n",
    "        p_pred = p_pred.cpu().detach().numpy()\n",
    "    \n",
    "        return u_pred,v_pred,p_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred,v_pred,p_pred = self.test()\n",
    "        \n",
    "        test_mse_u = np.mean(np.square(u_pred.reshape(-1,1) - u_true_test.reshape(-1,1)))\n",
    "        test_re_u = np.linalg.norm(u_pred.reshape(-1,1) - u_true_test.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        test_mse_v = np.mean(np.square(v_pred.reshape(-1,1) - v_true_test.reshape(-1,1)))\n",
    "        test_re_v = np.linalg.norm(v_pred.reshape(-1,1) - v_true_test.reshape(-1,1),2)/v_true_norm\n",
    "        \n",
    "        test_mse_p = np.mean(np.square(p_pred.reshape(-1,1) - p_true_test.reshape(-1,1)))\n",
    "        test_re_p = np.linalg.norm(p_pred.reshape(-1,1) - p_true_test.reshape(-1,1),2)/p_true_norm\n",
    "        \n",
    "        return test_mse_u, test_re_u, test_mse_v, test_re_v, test_mse_p, test_re_p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_train,y_train,t_train,u_train,v_train,fg_hat,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_train,y_train,t_train,u_train,v_train,fg_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np,x_train,y_train,t_train):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    lambda2_val.append(PINN.lambda2.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse_u, test_re_u, test_mse_v, test_re_v, test_mse_p, test_re_p = PINN.test_loss()\n",
    "    \n",
    "    test_mse_u_loss.append(test_mse_u)\n",
    "    test_re_u_loss.append(test_re_u)\n",
    "    \n",
    "    test_mse_v_loss.append(test_mse_v)\n",
    "    test_re_v_loss.append(test_re_v)\n",
    "    \n",
    "    test_mse_p_loss.append(test_mse_p)\n",
    "    test_re_p_loss.append(test_re_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    x_train,y_train,t_train,u_train,v_train = trainingdata(N_T,(reps)*22)\n",
    "\n",
    "    x_train = torch.from_numpy(x_train).float().to(device)\n",
    "    y_train = torch.from_numpy(y_train).float().to(device)\n",
    "    t_train = torch.from_numpy(t_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    v_train = torch.from_numpy(v_train).float().to(device)\n",
    "        \n",
    "    fg_hat = torch.zeros(x_train.shape[0],1).to(device)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(x_train,y_train,t_train,u_train,v_train,fg_hat,i)\n",
    "\n",
    "        loss_np = PINN.loss(x_train,y_train,t_train,u_train,v_train,fg_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np,x_train,y_train,t_train)\n",
    "        print(i,\"Loss\",train_loss[-1], \"L1\",lambda1_val[-1],\"L2\",lambda2_val[-1])\n",
    "        # print(i,\"Loss\",train_loss[-1],\"RE\",test_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVnXJfj0CQr-",
    "outputId": "1f2921b0-e258-465d-aa27-cdeb80b78a0b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D_NS_stan\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Loss 0.10068623 L1 0.008027752 L2 -0.006728693\n",
      "1 Loss 0.09513332 L1 -0.0050416896 L2 -0.001609435\n",
      "2 Loss 0.093713276 L1 -0.026541134 L2 -0.00042546913\n",
      "3 Loss 0.09259156 L1 0.0014318295 L2 6.8328314e-05\n",
      "4 Loss 0.091504276 L1 0.073196284 L2 0.0027645521\n",
      "5 Loss 0.09003718 L1 0.10398707 L2 0.004214231\n",
      "6 Loss 0.082979634 L1 0.35676813 L2 0.014351057\n",
      "7 Loss 0.061600704 L1 0.8774551 L2 0.018129487\n",
      "8 Loss 0.029808648 L1 0.8159813 L2 0.015028691\n",
      "9 Loss 0.020303521 L1 0.8613947 L2 0.01601677\n",
      "10 Loss 0.015799273 L1 0.881113 L2 0.017717158\n",
      "11 Loss 0.012311762 L1 0.9159664 L2 0.016329879\n",
      "12 Loss 0.010136499 L1 0.9322494 L2 0.01695079\n",
      "13 Loss 0.008867378 L1 0.9345453 L2 0.017123893\n",
      "14 Loss 0.0075701526 L1 0.9441068 L2 0.015243669\n",
      "15 Loss 0.006797065 L1 0.954977 L2 0.015364078\n",
      "16 Loss 0.0060537863 L1 0.95657504 L2 0.014906781\n",
      "17 Loss 0.0053052194 L1 0.9594421 L2 0.014461713\n",
      "18 Loss 0.0046386877 L1 0.963813 L2 0.0148510365\n",
      "19 Loss 0.004150344 L1 0.9656332 L2 0.014279593\n",
      "20 Loss 0.0036241596 L1 0.9716038 L2 0.013709635\n",
      "21 Loss 0.0032878325 L1 0.9734237 L2 0.013024597\n",
      "22 Loss 0.0029875361 L1 0.9759389 L2 0.013339705\n",
      "23 Loss 0.0027160435 L1 0.98097664 L2 0.012626389\n",
      "24 Loss 0.0024405709 L1 0.98069197 L2 0.012435038\n",
      "25 Loss 0.0022009057 L1 0.98196876 L2 0.012452781\n",
      "26 Loss 0.001989191 L1 0.98559064 L2 0.012289677\n",
      "27 Loss 0.0018407266 L1 0.985699 L2 0.012207381\n",
      "28 Loss 0.001708048 L1 0.9843246 L2 0.011838846\n",
      "29 Loss 0.0015874945 L1 0.9866503 L2 0.01204022\n",
      "30 Loss 0.0014828395 L1 0.98764414 L2 0.012006868\n",
      "31 Loss 0.0014013164 L1 0.98477376 L2 0.012023707\n",
      "32 Loss 0.0013293776 L1 0.98928845 L2 0.011967592\n",
      "33 Loss 0.0012558333 L1 0.9875413 L2 0.012010086\n",
      "34 Loss 0.0011926175 L1 0.9883555 L2 0.011958633\n",
      "35 Loss 0.0011361325 L1 0.9884347 L2 0.011963352\n",
      "36 Loss 0.0010837289 L1 0.98900753 L2 0.01181605\n",
      "37 Loss 0.0010349054 L1 0.98937 L2 0.011842509\n",
      "38 Loss 0.0009908583 L1 0.9914319 L2 0.011784651\n",
      "39 Loss 0.0009504527 L1 0.99169344 L2 0.011715831\n",
      "40 Loss 0.00091238355 L1 0.990614 L2 0.011533895\n",
      "41 Loss 0.00087865465 L1 0.9917551 L2 0.01145881\n",
      "42 Loss 0.00084925356 L1 0.9922985 L2 0.011531944\n",
      "43 Loss 0.000815695 L1 0.99192923 L2 0.011374255\n",
      "44 Loss 0.0007854196 L1 0.99292725 L2 0.011417795\n",
      "45 Loss 0.00076011097 L1 0.9927861 L2 0.0114778625\n",
      "46 Loss 0.00073286326 L1 0.99323845 L2 0.011464475\n",
      "47 Loss 0.0007057703 L1 0.99316746 L2 0.011408111\n",
      "48 Loss 0.0006829089 L1 0.99274945 L2 0.011468246\n",
      "49 Loss 0.000661863 L1 0.99334145 L2 0.011591541\n",
      "50 Loss 0.0006400994 L1 0.9931397 L2 0.01157711\n",
      "51 Loss 0.00061922707 L1 0.9941468 L2 0.01157732\n",
      "52 Loss 0.00060012465 L1 0.993771 L2 0.011472859\n",
      "53 Loss 0.00058381865 L1 0.99470824 L2 0.011591316\n",
      "54 Loss 0.00056723796 L1 0.99446976 L2 0.011529652\n",
      "55 Loss 0.0005476834 L1 0.99444395 L2 0.011483616\n",
      "56 Loss 0.0005298283 L1 0.99477005 L2 0.011497796\n",
      "57 Loss 0.00051382475 L1 0.99488133 L2 0.011466402\n",
      "58 Loss 0.000497562 L1 0.99460965 L2 0.011563289\n",
      "59 Loss 0.00048131068 L1 0.9951754 L2 0.011547979\n",
      "60 Loss 0.00046661653 L1 0.9946494 L2 0.011464997\n",
      "61 Loss 0.00045384478 L1 0.9955572 L2 0.01149868\n",
      "62 Loss 0.0004412409 L1 0.99573493 L2 0.011493507\n",
      "63 Loss 0.0004296455 L1 0.9961101 L2 0.011427171\n",
      "64 Loss 0.00041768 L1 0.9964544 L2 0.011453824\n",
      "65 Loss 0.0004072993 L1 0.9956802 L2 0.011421856\n",
      "66 Loss 0.00039838138 L1 0.9962715 L2 0.011402\n",
      "67 Loss 0.0003892351 L1 0.99533284 L2 0.011406364\n",
      "68 Loss 0.00037863542 L1 0.9965225 L2 0.01137533\n",
      "69 Loss 0.00036887752 L1 0.99543786 L2 0.011292513\n",
      "70 Loss 0.00035806355 L1 0.9968343 L2 0.011342587\n",
      "71 Loss 0.00034839517 L1 0.99618936 L2 0.011287985\n",
      "72 Loss 0.0003395789 L1 0.996358 L2 0.0113073345\n",
      "73 Loss 0.00033126175 L1 0.99648935 L2 0.011239118\n",
      "74 Loss 0.00032294705 L1 0.99696 L2 0.0111736795\n",
      "75 Loss 0.00031473296 L1 0.9968854 L2 0.011206068\n",
      "76 Loss 0.00030717245 L1 0.99735314 L2 0.011193697\n",
      "77 Loss 0.00030037592 L1 0.99721295 L2 0.011129841\n",
      "78 Loss 0.00029395963 L1 0.99724823 L2 0.011155121\n",
      "79 Loss 0.00028781372 L1 0.99736 L2 0.011207712\n",
      "80 Loss 0.00028210366 L1 0.99704385 L2 0.011229233\n",
      "81 Loss 0.00027681454 L1 0.99729383 L2 0.011234991\n",
      "82 Loss 0.00027158618 L1 0.99697834 L2 0.011262691\n",
      "83 Loss 0.0002669457 L1 0.9972065 L2 0.011255798\n",
      "84 Loss 0.0002622467 L1 0.99751717 L2 0.011255867\n",
      "85 Loss 0.00025748086 L1 0.9976456 L2 0.011241697\n",
      "86 Loss 0.00025283755 L1 0.9975951 L2 0.011172679\n",
      "87 Loss 0.0002485746 L1 0.9977659 L2 0.011219736\n",
      "88 Loss 0.00024399131 L1 0.99766463 L2 0.011206355\n",
      "89 Loss 0.00023961348 L1 0.9977293 L2 0.01117031\n",
      "90 Loss 0.00023521908 L1 0.99761975 L2 0.011133832\n",
      "91 Loss 0.00023062715 L1 0.997855 L2 0.011126012\n",
      "92 Loss 0.00022697615 L1 0.99825376 L2 0.011103236\n",
      "93 Loss 0.00022296325 L1 0.9979011 L2 0.011110463\n",
      "94 Loss 0.00021928851 L1 0.9983028 L2 0.011149188\n",
      "95 Loss 0.00021522855 L1 0.99778116 L2 0.011163112\n",
      "96 Loss 0.00021145178 L1 0.99835455 L2 0.011148591\n",
      "97 Loss 0.00020800413 L1 0.99805826 L2 0.0111567825\n",
      "98 Loss 0.00020422465 L1 0.9984433 L2 0.011136988\n",
      "99 Loss 0.00020083114 L1 0.9983524 L2 0.0111684\n",
      "100 Loss 0.00019760785 L1 0.9978151 L2 0.011165808\n",
      "101 Loss 0.00019478201 L1 0.9984098 L2 0.011168332\n",
      "102 Loss 0.0001919241 L1 0.9980945 L2 0.011129443\n",
      "103 Loss 0.00018925699 L1 0.9982254 L2 0.011166711\n",
      "104 Loss 0.00018675686 L1 0.99786216 L2 0.011135402\n",
      "105 Loss 0.00018428372 L1 0.99813426 L2 0.0111303395\n",
      "106 Loss 0.00018167547 L1 0.99804664 L2 0.011105576\n",
      "107 Loss 0.00017912095 L1 0.9982709 L2 0.0111251045\n",
      "108 Loss 0.0001772086 L1 0.9981728 L2 0.011100415\n",
      "109 Loss 0.00017485028 L1 0.99816346 L2 0.011129897\n",
      "110 Loss 0.00017258528 L1 0.9984069 L2 0.011106632\n",
      "111 Loss 0.00017028382 L1 0.9983252 L2 0.0110987695\n",
      "112 Loss 0.00016798824 L1 0.9985997 L2 0.011087735\n",
      "113 Loss 0.00016581491 L1 0.9983949 L2 0.01108879\n",
      "114 Loss 0.00016396437 L1 0.99865043 L2 0.011096343\n",
      "115 Loss 0.0001620084 L1 0.998393 L2 0.011079854\n",
      "116 Loss 0.00015996123 L1 0.9987383 L2 0.011074697\n",
      "117 Loss 0.00015796906 L1 0.9986263 L2 0.011084424\n",
      "118 Loss 0.00015616236 L1 0.99870586 L2 0.011093089\n",
      "119 Loss 0.00015440516 L1 0.9986968 L2 0.011062246\n",
      "120 Loss 0.00015237414 L1 0.9985176 L2 0.0110666435\n",
      "121 Loss 0.00015047082 L1 0.99882674 L2 0.011059555\n",
      "122 Loss 0.00014862177 L1 0.99856365 L2 0.011042028\n",
      "123 Loss 0.00014686736 L1 0.99863666 L2 0.011021288\n",
      "124 Loss 0.00014530327 L1 0.99850935 L2 0.011002562\n",
      "125 Loss 0.00014348167 L1 0.9987393 L2 0.010988373\n",
      "126 Loss 0.00014177593 L1 0.99844766 L2 0.010961266\n",
      "127 Loss 0.00014003752 L1 0.99837774 L2 0.010982053\n",
      "128 Loss 0.0001386094 L1 0.99854136 L2 0.010965824\n",
      "129 Loss 0.0001371048 L1 0.9985711 L2 0.010972741\n",
      "130 Loss 0.00013545649 L1 0.99848944 L2 0.01098264\n",
      "131 Loss 0.00013396311 L1 0.9986116 L2 0.010953394\n",
      "132 Loss 0.00013243266 L1 0.9985006 L2 0.01092346\n",
      "133 Loss 0.00013096446 L1 0.9986398 L2 0.010916404\n",
      "134 Loss 0.00012964247 L1 0.9987322 L2 0.010921076\n",
      "135 Loss 0.00012848008 L1 0.9986625 L2 0.010917463\n",
      "136 Loss 0.0001269101 L1 0.9988135 L2 0.010923866\n",
      "137 Loss 0.00012552782 L1 0.998674 L2 0.010895707\n",
      "138 Loss 0.00012408497 L1 0.9986911 L2 0.010896778\n",
      "139 Loss 0.00012272314 L1 0.9987402 L2 0.010907448\n",
      "140 Loss 0.00012134584 L1 0.99872774 L2 0.010872443\n",
      "141 Loss 0.00011983916 L1 0.99876434 L2 0.0108608985\n",
      "142 Loss 0.000118412376 L1 0.99856544 L2 0.010894217\n",
      "143 Loss 0.00011720031 L1 0.9988708 L2 0.010864682\n",
      "144 Loss 0.00011592793 L1 0.99864274 L2 0.010856951\n",
      "145 Loss 0.00011474453 L1 0.9987328 L2 0.010865322\n",
      "146 Loss 0.00011339415 L1 0.99881774 L2 0.010856888\n",
      "147 Loss 0.00011238704 L1 0.99862415 L2 0.010858303\n",
      "148 Loss 0.00011116032 L1 0.9989037 L2 0.010876983\n",
      "149 Loss 0.00010994742 L1 0.9986061 L2 0.010864541\n",
      "150 Loss 0.00010904574 L1 0.9988959 L2 0.010851028\n",
      "151 Loss 0.00010803796 L1 0.99871755 L2 0.010848259\n",
      "152 Loss 0.00010697769 L1 0.9989502 L2 0.010865992\n",
      "153 Loss 0.00010583553 L1 0.99868786 L2 0.010850562\n",
      "154 Loss 0.00010481226 L1 0.99894893 L2 0.010836003\n",
      "155 Loss 0.00010378679 L1 0.99877787 L2 0.010839518\n",
      "156 Loss 0.00010289643 L1 0.9988514 L2 0.010847369\n",
      "157 Loss 0.00010196229 L1 0.9989419 L2 0.010825785\n",
      "158 Loss 0.00010107916 L1 0.9986038 L2 0.010830148\n",
      "159 Loss 0.00010011295 L1 0.9989741 L2 0.010839121\n",
      "160 Loss 9.9236335e-05 L1 0.99878687 L2 0.010828828\n",
      "161 Loss 9.8352015e-05 L1 0.99876714 L2 0.0108238915\n",
      "162 Loss 9.744895e-05 L1 0.9988275 L2 0.010839424\n",
      "163 Loss 9.6479365e-05 L1 0.99882936 L2 0.010829638\n",
      "164 Loss 9.5575895e-05 L1 0.9987594 L2 0.010820537\n",
      "165 Loss 9.469349e-05 L1 0.9988201 L2 0.010832158\n",
      "166 Loss 9.389165e-05 L1 0.9987661 L2 0.0108326655\n",
      "167 Loss 9.302647e-05 L1 0.99888045 L2 0.010796338\n",
      "168 Loss 9.2300856e-05 L1 0.9988593 L2 0.010810276\n",
      "169 Loss 9.1545895e-05 L1 0.9988603 L2 0.01080764\n",
      "170 Loss 9.084682e-05 L1 0.99893457 L2 0.010795486\n",
      "171 Loss 9.017789e-05 L1 0.9987747 L2 0.010797524\n",
      "172 Loss 8.954888e-05 L1 0.9988503 L2 0.0108102225\n",
      "173 Loss 8.883924e-05 L1 0.9988802 L2 0.01080465\n",
      "174 Loss 8.814737e-05 L1 0.99881774 L2 0.010817546\n",
      "175 Loss 8.745563e-05 L1 0.9989397 L2 0.010815592\n",
      "176 Loss 8.685145e-05 L1 0.99886054 L2 0.010816055\n",
      "177 Loss 8.623616e-05 L1 0.99885446 L2 0.010816771\n",
      "178 Loss 8.592556e-05 L1 0.99882704 L2 0.010803269\n",
      "179 Loss 8.524103e-05 L1 0.9988779 L2 0.010796043\n",
      "180 Loss 8.447365e-05 L1 0.9987046 L2 0.010795116\n",
      "181 Loss 8.384947e-05 L1 0.9988363 L2 0.010785783\n",
      "182 Loss 8.328699e-05 L1 0.9987652 L2 0.010765676\n",
      "183 Loss 8.261902e-05 L1 0.9987832 L2 0.010775622\n",
      "184 Loss 8.1940685e-05 L1 0.99866843 L2 0.010764614\n",
      "185 Loss 8.124299e-05 L1 0.99883217 L2 0.010754837\n",
      "186 Loss 8.05476e-05 L1 0.99877375 L2 0.010733939\n",
      "187 Loss 7.989982e-05 L1 0.9986826 L2 0.010739195\n",
      "188 Loss 7.9279285e-05 L1 0.99871874 L2 0.010743106\n",
      "189 Loss 7.878596e-05 L1 0.9987213 L2 0.010758919\n",
      "190 Loss 7.8250654e-05 L1 0.9988228 L2 0.0107641695\n",
      "191 Loss 7.7717516e-05 L1 0.9988481 L2 0.010747714\n",
      "192 Loss 7.719737e-05 L1 0.99886906 L2 0.010734299\n",
      "193 Loss 7.6651355e-05 L1 0.998963 L2 0.010748471\n",
      "194 Loss 7.609791e-05 L1 0.998799 L2 0.01072369\n",
      "195 Loss 7.552872e-05 L1 0.9988901 L2 0.010719286\n",
      "196 Loss 7.5180506e-05 L1 0.99875426 L2 0.0107167885\n",
      "197 Loss 7.465169e-05 L1 0.9989504 L2 0.010721743\n",
      "198 Loss 7.415113e-05 L1 0.9988265 L2 0.010703677\n",
      "199 Loss 7.362883e-05 L1 0.9989241 L2 0.010706887\n",
      "200 Loss 7.3079966e-05 L1 0.998832 L2 0.010700828\n",
      "201 Loss 7.259658e-05 L1 0.998874 L2 0.010690496\n",
      "202 Loss 7.209039e-05 L1 0.9988835 L2 0.010685743\n",
      "203 Loss 7.198317e-05 L1 0.9989785 L2 0.010683772\n",
      "204 Loss 7.193368e-05 L1 0.9989669 L2 0.01068392\n",
      "205 Loss 7.1460934e-05 L1 0.9988034 L2 0.010683649\n",
      "206 Loss 7.1007074e-05 L1 0.99887675 L2 0.01070275\n",
      "207 Loss 7.089393e-05 L1 0.99888927 L2 0.01069078\n",
      "208 Loss 7.035255e-05 L1 0.9988924 L2 0.010699225\n",
      "209 Loss 7.019671e-05 L1 0.99890655 L2 0.010697675\n",
      "210 Loss 6.971264e-05 L1 0.9988873 L2 0.010678683\n",
      "211 Loss 6.934039e-05 L1 0.9989723 L2 0.010684768\n",
      "212 Loss 6.8972986e-05 L1 0.9988202 L2 0.010675843\n",
      "213 Loss 6.8892274e-05 L1 0.9988429 L2 0.010677087\n",
      "214 Loss 6.876743e-05 L1 0.99890685 L2 0.010681605\n",
      "215 Loss 6.834864e-05 L1 0.9989144 L2 0.010667013\n",
      "216 Loss 6.794712e-05 L1 0.9988515 L2 0.010668736\n",
      "217 Loss 6.7564266e-05 L1 0.99884886 L2 0.010687172\n",
      "218 Loss 6.746169e-05 L1 0.9988314 L2 0.010693153\n",
      "219 Loss 6.732742e-05 L1 0.9987763 L2 0.010684584\n",
      "220 Loss 6.7156085e-05 L1 0.99875456 L2 0.010696949\n",
      "221 Loss 6.678073e-05 L1 0.99884427 L2 0.010692261\n",
      "222 Loss 6.659188e-05 L1 0.99884003 L2 0.01067689\n",
      "223 Loss 6.6197106e-05 L1 0.99893767 L2 0.010680492\n",
      "224 Loss 6.582521e-05 L1 0.9990043 L2 0.010670348\n",
      "225 Loss 6.547567e-05 L1 0.999012 L2 0.010674602\n",
      "226 Loss 6.5059045e-05 L1 0.9989115 L2 0.010669308\n",
      "227 Loss 6.5049055e-05 L1 0.9989156 L2 0.0106701115\n",
      "228 Loss 6.502568e-05 L1 0.9989438 L2 0.010673297\n",
      "229 Loss 6.46781e-05 L1 0.9990703 L2 0.010670373\n",
      "230 Loss 6.463524e-05 L1 0.9990162 L2 0.010693439\n",
      "231 Loss 6.4218155e-05 L1 0.9990118 L2 0.010673699\n",
      "232 Loss 6.380906e-05 L1 0.9989887 L2 0.010658055\n",
      "233 Loss 6.355583e-05 L1 0.9991339 L2 0.0106562525\n",
      "234 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "235 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "236 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "237 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "238 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "239 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "240 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "241 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "242 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "243 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "244 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "245 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "246 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "247 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "248 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "249 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "250 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "251 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "252 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "253 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "254 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "255 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "256 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "257 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "258 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "259 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "260 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "261 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "262 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "263 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "264 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "265 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "266 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "267 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "268 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "269 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "270 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "271 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "272 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "273 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "274 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "275 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "276 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "277 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "278 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "279 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "280 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "281 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "282 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "283 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "284 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "285 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "286 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "287 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "288 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "289 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "290 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "291 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "292 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "293 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "294 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "295 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "296 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "297 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "298 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "299 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "300 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "301 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "302 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "303 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "304 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "305 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "306 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "307 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "308 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "309 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "310 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "311 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "312 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "313 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "314 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "315 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "316 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "317 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "318 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "319 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "320 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "321 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "322 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "323 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "324 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "325 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "326 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "327 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "328 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "329 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "330 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "331 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "332 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "333 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "334 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "335 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "336 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "337 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "338 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "339 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "340 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "341 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "342 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "343 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "344 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "345 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "346 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "347 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "348 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "349 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "350 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "351 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "352 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "353 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "354 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "355 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "356 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "357 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "358 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "359 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "360 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "361 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "362 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "363 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "364 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "365 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "366 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "367 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "368 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "369 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "370 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "371 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "372 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "373 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "374 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "375 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "376 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "377 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "378 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "379 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "380 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "381 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "382 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "383 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "384 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "385 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "386 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "387 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "388 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "389 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "390 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "391 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "392 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "393 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "394 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "395 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "396 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "397 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "398 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "399 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "400 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "401 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "402 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "403 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "404 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "405 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "406 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "407 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "408 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "409 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "410 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "411 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "412 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "413 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "414 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "415 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "416 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "417 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "418 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "419 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "420 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "421 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "422 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "423 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "424 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "425 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "426 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "427 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "428 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "429 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "430 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "431 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "432 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "433 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "434 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "435 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "436 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "437 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "438 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "439 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "440 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "441 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "442 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "443 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "444 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "445 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "446 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "447 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "448 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "449 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "450 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "451 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "452 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "453 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "454 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "455 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "456 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "457 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "458 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "459 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "460 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "461 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "462 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "463 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "464 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "465 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "466 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "467 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "468 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "469 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "470 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "471 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "472 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "473 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "474 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "475 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "476 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "477 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "478 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "479 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "480 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "481 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "482 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "483 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "484 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "485 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "486 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "487 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "488 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "489 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "490 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "491 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "492 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "493 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "494 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "495 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "496 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "497 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "498 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "499 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "500 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "501 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "502 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "503 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "504 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "505 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "506 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "507 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "508 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "509 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "510 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "511 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "512 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "513 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "514 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "515 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "516 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "517 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "518 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "519 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "520 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "521 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "522 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "523 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "524 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "525 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "526 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "527 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "528 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "529 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "530 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "531 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "532 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "533 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "534 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "535 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "536 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "537 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "538 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "539 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "540 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "541 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "542 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "543 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "544 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "545 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "546 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "547 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "548 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "549 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "550 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "551 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "552 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "553 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "554 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "555 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "556 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "557 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "558 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "559 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "560 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "561 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "562 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "563 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "564 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "565 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "566 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "567 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "568 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "569 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "570 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "571 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "572 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "573 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "574 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "575 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "576 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "577 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "578 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "579 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "580 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "581 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "582 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "583 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "584 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "585 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "586 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "587 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "588 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "589 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "590 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "591 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "592 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "593 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "594 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "595 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "596 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "597 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "598 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "599 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "600 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "601 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "602 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "603 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "604 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "605 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "606 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "607 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "608 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "609 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "610 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "611 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "612 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "613 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "614 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "615 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "616 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "617 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "618 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "619 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "620 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "621 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "622 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "623 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "624 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "625 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "626 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "627 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "628 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "629 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "630 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "631 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "632 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "633 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "634 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "635 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "636 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "637 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "638 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "639 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "640 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "641 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "642 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "643 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "644 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "645 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "646 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "647 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "648 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "649 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "650 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "651 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "652 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "653 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "654 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "655 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "656 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "657 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "658 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "659 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "660 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "661 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "662 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "663 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "664 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "665 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "666 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "667 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "668 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "669 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "670 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "671 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "672 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "673 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "674 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "675 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "676 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "677 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "678 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "679 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "680 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "681 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "682 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "683 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "684 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "685 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "686 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "687 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "688 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "689 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "690 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "691 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "692 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "693 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "694 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "695 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "696 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "697 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "698 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "699 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "700 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "701 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "702 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "703 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "704 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "705 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "706 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "707 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "708 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "709 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "710 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "711 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "712 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "713 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "714 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "715 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "716 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "717 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "718 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "719 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "720 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "721 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "722 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "723 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "724 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "725 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "726 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "727 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "728 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "729 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "730 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "731 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "732 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "733 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "734 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "735 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "736 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "737 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "738 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "739 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "740 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "741 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "742 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "743 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "744 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "745 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "746 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "747 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "748 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "749 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "750 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "751 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "752 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "753 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "754 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "755 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "756 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "757 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "758 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "759 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "760 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "761 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "762 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "763 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "764 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "765 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "766 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "767 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "768 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "769 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "770 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "771 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "772 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "773 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "774 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "775 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "776 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "777 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "778 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "779 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "780 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "781 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "782 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "783 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "784 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "785 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "786 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "787 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "788 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "789 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "790 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "791 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "792 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "793 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "794 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "795 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "796 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "797 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "798 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "799 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "800 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "801 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "802 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "803 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "804 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "805 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "806 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "807 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "808 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "809 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "810 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "811 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "812 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "813 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "814 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "815 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "816 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "817 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "818 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "819 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "820 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "821 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "822 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "823 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "824 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "825 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "826 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "827 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "828 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "829 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "830 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "831 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "832 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "833 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "834 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "835 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "836 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "837 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "838 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "839 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "840 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "841 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "842 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "843 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "844 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "845 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "846 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "847 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "848 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "849 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "850 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "851 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "852 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "853 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "854 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "855 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "856 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "857 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "858 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "859 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "860 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "861 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "862 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "863 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "864 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "865 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "866 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "867 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "868 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "869 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "870 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "871 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "872 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "873 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "874 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "875 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "876 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "877 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "878 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "879 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "880 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "881 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "882 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "883 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "884 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "885 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "886 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "887 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "888 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "889 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "890 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "891 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "892 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "893 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "894 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "895 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "896 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "897 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "898 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "899 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "900 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "901 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "902 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "903 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "904 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "905 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "906 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "907 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "908 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "909 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "910 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "911 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "912 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "913 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "914 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "915 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "916 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "917 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "918 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "919 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "920 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "921 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "922 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "923 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "924 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "925 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "926 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "927 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "928 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "929 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "930 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "931 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "932 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "933 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "934 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "935 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "936 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "937 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "938 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "939 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "940 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "941 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "942 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "943 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "944 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "945 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "946 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "947 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "948 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "949 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "950 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "951 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "952 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "953 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "954 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "955 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "956 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "957 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "958 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "959 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "960 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "961 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "962 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "963 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "964 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "965 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "966 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "967 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "968 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "969 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "970 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "971 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "972 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "973 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "974 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "975 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "976 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "977 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "978 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "979 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "980 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "981 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "982 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "983 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "984 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "985 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "986 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "987 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "988 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "989 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "990 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "991 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "992 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "993 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "994 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "995 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "996 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "997 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "998 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "999 Loss 6.352659e-05 L1 0.9991319 L2 0.010656148\n",
      "Training time: 1016.74\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "\n",
    "test_mse_u_full = []\n",
    "test_re_u_full = []\n",
    "test_mse_v_full = []\n",
    "test_re_v_full = []\n",
    "test_mse_p_full = []\n",
    "test_re_p_full = []\n",
    "\n",
    "\n",
    "beta_full = []\n",
    "lambda1_full = []\n",
    "lambda2_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 1.0\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    train_loss = []\n",
    "    \n",
    "    test_mse_u_loss = []\n",
    "    test_re_u_loss = []\n",
    "    \n",
    "    test_mse_v_loss = []\n",
    "    test_re_v_loss = []\n",
    "    \n",
    "    test_mse_p_loss = []\n",
    "    test_re_p_loss = []\n",
    "    \n",
    "    lambda1_val = []\n",
    "    lambda2_val = []\n",
    "    beta_val = []\n",
    "\n",
    "    print(reps)\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_T = 5000\n",
    "\n",
    "    layers = np.array([3,50,50,50,50,2]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    \n",
    "    test_mse_u_full.append(test_mse_u_loss)\n",
    "    test_re_u_full.append(test_re_u_loss)\n",
    "    test_mse_v_full.append(test_mse_v_loss)\n",
    "    test_re_v_full.append(test_re_v_loss)\n",
    "    test_mse_p_full.append(test_mse_p_loss)\n",
    "    test_re_p_full.append(test_re_p_loss)\n",
    "\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "    lambda1_full.append(lambda1_val)\n",
    "    lambda2_full.append(lambda2_val)\n",
    "\n",
    "    #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_u\": test_mse_u_full,\"test_re_u_loss\": test_re_u_full,\"test_mse_v\": test_mse_v_full,\"test_re_v_loss\": test_re_v_full,\"test_mse_p\": test_mse_p_full,\"test_re_p_loss\": test_re_p_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold, \"lambda1\":lambda1_full,\"lambda2\":lambda2_full}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('../cylinder_nektar_wake.mat')\n",
    "           \n",
    "U_star = data['U_star'] # N x 2 x T\n",
    "P_star = data['p_star'] # N x T\n",
    "t_star = data['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2\n",
    "\n",
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]\n",
    "\n",
    "# # Rearrange Data \n",
    "XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
    "YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
    "TT = np.tile(t_star, (1,N)).T # N x T\n",
    "\n",
    "UU = U_star[:,0,:] # N x T\n",
    "VV = U_star[:,1,:] # N x T\n",
    "PP = P_star # N x T\n",
    "\n",
    "x = XX.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "y = YY.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "t = TT.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "\n",
    "u_true = UU.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "v_true = VV.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "p_true = PP.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "\n",
    "label = \"3D_NS_stan\"\n",
    "\n",
    "loss_thresh = 10000\n",
    "\n",
    "N_train = x.shape[0]\n",
    "xyt = np.hstack((x,y,t))\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]\n",
    "\n",
    "# xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "np.random.seed(1234)\n",
    "idx = np.where(t==5)\n",
    "idx = idx[0]\n",
    "u_true_test = u_true[idx,:]\n",
    "v_true_test = v_true[idx,:]\n",
    "p_true_test = p_true[idx,:]\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "v_true_norm = np.linalg.norm(v_true,2)\n",
    "p_true_norm = np.linalg.norm(p_true,2)\n",
    "\n",
    "# u_true_test = torch.from_numpy(u_true_test).float().to(device)\n",
    "# v_true_test = torch.from_numpy(v_true_test).float().to(device)\n",
    "# p_true_test = torch.from_numpy(p_true_test).float().to(device)\n",
    "\n",
    "x_tensor = torch.from_numpy(x[idx,:]).float().to(device)\n",
    "y_tensor = torch.from_numpy(y[idx,:]).float().to(device)\n",
    "t_tensor = torch.from_numpy(t[idx,:]).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u,v,p = PINN.test()\n",
    "fig,ax = plt.subplots(1,1)\n",
    "img = ax.imshow(p.reshape(50,100),cmap = 'jet')\n",
    "cbar = fig.colorbar(img, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HT_stan_v3_15Aug2022_MP4Video.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
