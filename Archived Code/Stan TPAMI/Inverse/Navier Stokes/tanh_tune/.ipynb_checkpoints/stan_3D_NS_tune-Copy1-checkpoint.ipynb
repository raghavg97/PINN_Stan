{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLsZ-c_nCQr2",
    "outputId": "0238c820-5951-4e75-a35b-19e4de8c9b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SV23gJi7JexL",
    "outputId": "6f051579-557f-463f-d7b4-955ed617736e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOyXTKXGJf97",
    "outputId": "11b7b7db-47b0-4cf8-c699-473f1c6b8c5f"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/MURI Aug17 Thin Plate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "APjvgycyCTj0",
    "outputId": "19bce659-211e-4bec-d94d-7c94148b0d09"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1]).reshape(-1,1)\n",
    "b_value = np.array([0.0,0.25,0.5,1.0]).reshape(-1,1)\n",
    "\n",
    "\n",
    "LR_tune, B_value = np.meshgrid(lr_tune,b_value)\n",
    "\n",
    "LR_tune = LR_tune.flatten('F').reshape(-1,1)\n",
    "B_value = B_value.flatten('F').reshape(-1,1)\n",
    "\n",
    "\n",
    "lrb_tune = np.hstack((LR_tune,B_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lxFUD2gACQr7"
   },
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "loss_thresh = 1\n",
    "\n",
    "data = scipy.io.loadmat('../cylinder_nektar_wake.mat')\n",
    "           \n",
    "U_star = data['U_star'] # N x 2 x T\n",
    "P_star = data['p_star'] # N x T\n",
    "t_star = data['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2\n",
    "\n",
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]\n",
    "\n",
    "# # Rearrange Data \n",
    "XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
    "YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
    "TT = np.tile(t_star, (1,N)).T # N x T\n",
    "\n",
    "UU = U_star[:,0,:] # N x T\n",
    "VV = U_star[:,1,:] # N x T\n",
    "PP = P_star # N x T\n",
    "\n",
    "x = XX.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "y = YY.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "t = TT.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "\n",
    "u_true = UU.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "v_true = VV.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "p_true = PP.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "\n",
    "label = \"3D_NS_stan\"\n",
    "\n",
    "loss_thresh = 10000\n",
    "\n",
    "N_train = x.shape[0]\n",
    "xyt = np.hstack((x,y,t))\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]\n",
    "\n",
    "# xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "np.random.seed(1234)\n",
    "idx = np.random.choice(N_train, 5000, replace=False)\n",
    "u_true_test = u_true[idx,:]\n",
    "v_true_test = v_true[idx,:]\n",
    "p_true_test = p_true[idx,:]\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "v_true_norm = np.linalg.norm(v_true,2)\n",
    "p_true_norm = np.linalg.norm(p_true,2)\n",
    "\n",
    "# u_true_test = torch.from_numpy(u_true_test).float().to(device)\n",
    "# v_true_test = torch.from_numpy(v_true_test).float().to(device)\n",
    "# p_true_test = torch.from_numpy(p_true_test).float().to(device)\n",
    "\n",
    "x_tensor = torch.from_numpy(x[idx,:]).float().to(device)\n",
    "y_tensor = torch.from_numpy(y[idx,:]).float().to(device)\n",
    "t_tensor = torch.from_numpy(t[idx,:]).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gp2G6x6BCQr8"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_T,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    idx = np.random.choice(N_train, N_T, replace=False)\n",
    "    x_train = x[idx,:]\n",
    "    y_train = y[idx,:]\n",
    "    t_train = t[idx,:]\n",
    "    u_train = u_true[idx,:]\n",
    "    v_train = v_true[idx,:]\n",
    "    \n",
    "    return x_train,y_train,t_train,u_train,v_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VRolFlBzCQr9"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requires_grad = True\n",
    "        \n",
    "        self.lambda2 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda2.requires_grad = True\n",
    "    \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_uv(self,x_train,y_train,t_train,u_train,v_train):\n",
    "        x1 = x_train.clone()\n",
    "        x1.requires_grad = True\n",
    "        \n",
    "        y1 = y_train.clone()\n",
    "        y1.requires_grad = True\n",
    "        \n",
    "        t1 = t_train.clone()\n",
    "        t1.requires_grad = True\n",
    "        \n",
    "#         print(torch.cat((x1,y1,t1),dim=1).shape)\n",
    "        psi_p = self.forward(torch.cat((x1,y1,t1),dim =1))\n",
    "        \n",
    "        psi = psi_p[:,0:1]\n",
    "        \n",
    "#         print(psi.shape)\n",
    "        psi_x = autograd.grad(psi,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        psi_y = autograd.grad(psi,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "                \n",
    "        loss_u = self.loss_function(psi_y,u_train)\n",
    "        loss_v = self.loss_function(-1*psi_x,v_train)\n",
    "                \n",
    "        return loss_u + loss_v\n",
    "    \n",
    "    def loss_PDE(self, x_train,y_train,t_train,fg_hat):\n",
    "        \n",
    "        x1 = x_train.clone()\n",
    "        x1.requires_grad = True\n",
    "        \n",
    "        y1 = y_train.clone()\n",
    "        y1.requires_grad = True\n",
    "        \n",
    "        t1 = t_train.clone()\n",
    "        t1.requires_grad = True\n",
    "        \n",
    "        psi_p = self.forward(torch.cat((x1,y1,t1),dim =1))\n",
    "        psi = psi_p[:,0:1]\n",
    "        p = psi_p[:,1:2]\n",
    "        \n",
    "        \n",
    "        u = autograd.grad(psi,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v = -1*autograd.grad(psi,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        u_t = autograd.grad(u,t1,torch.ones([t1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_x = autograd.grad(u,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        u_xx = autograd.grad(u_x,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "                \n",
    "        u_y = autograd.grad(u,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        u_yy = autograd.grad(u_y,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        #---------------------------------------------------------------------------------\n",
    "        \n",
    "        v_t = autograd.grad(v,t1,torch.ones([t1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        v_x = autograd.grad(v,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v_xx = autograd.grad(v_x,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "                \n",
    "        v_y = autograd.grad(v,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v_yy = autograd.grad(v_y,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        #------------------------------------------------------------------------------------\n",
    "        p_x = autograd.grad(p,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        p_y = autograd.grad(p,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "            \n",
    "            \n",
    "\n",
    "        f = u_t + self.lambda1*(u*u_x + v*u_y) + p_x - self.lambda2*(u_xx + u_yy)\n",
    "        g = v_t + self.lambda1*(u*v_x + v*v_y) + p_y - self.lambda2*(v_xx + v_yy)\n",
    "        \n",
    "        loss_f = self.loss_function(f,fg_hat)\n",
    "        loss_g = self.loss_function(g,fg_hat)\n",
    "                \n",
    "        return loss_f + loss_g\n",
    "    \n",
    "    def loss(self,x_train,y_train,t_train,u_train,v_train,fg_hat):\n",
    "\n",
    "        loss_uv = self.loss_uv(x_train,y_train,t_train,u_train,v_train)\n",
    "        loss_fg = self.loss_PDE(x_train,y_train,t_train,fg_hat)\n",
    "        loss_val = loss_uv + loss_fg\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "       \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        x1 = x_tensor.clone()\n",
    "        x1.requires_grad = True\n",
    "        \n",
    "        y1 = y_tensor.clone()\n",
    "        y1.requires_grad = True\n",
    "        \n",
    "        t1 = t_tensor.clone()\n",
    "        t1.requires_grad = True\n",
    "        \n",
    "        psi_p_pred = self.forward(torch.cat((x1,y1,t1),dim =1))\n",
    "        psi = psi_p_pred[:,0:1]\n",
    "        p_pred = psi_p_pred[:,1:2]\n",
    "        \n",
    "        u_pred = autograd.grad(psi,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v_pred = -1*autograd.grad(psi,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "   \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        v_pred = v_pred.cpu().detach().numpy()\n",
    "        p_pred = p_pred.cpu().detach().numpy()\n",
    "    \n",
    "        return u_pred,v_pred,p_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred,v_pred,p_pred = self.test()\n",
    "        \n",
    "        test_mse_u = np.mean(np.square(u_pred.reshape(-1,1) - u_true_test.reshape(-1,1)))\n",
    "        test_re_u = np.linalg.norm(u_pred.reshape(-1,1) - u_true_test.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        test_mse_v = np.mean(np.square(v_pred.reshape(-1,1) - v_true_test.reshape(-1,1)))\n",
    "        test_re_v = np.linalg.norm(v_pred.reshape(-1,1) - v_true_test.reshape(-1,1),2)/v_true_norm\n",
    "        \n",
    "        test_mse_p = np.mean(np.square(p_pred.reshape(-1,1) - p_true_test.reshape(-1,1)))\n",
    "        test_re_p = np.linalg.norm(p_pred.reshape(-1,1) - p_true_test.reshape(-1,1),2)/p_true_norm\n",
    "        \n",
    "        return test_mse_u, test_re_u, test_mse_v, test_re_v, test_mse_p, test_re_p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_train,y_train,t_train,u_train,v_train,fg_hat,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_train,y_train,t_train,u_train,v_train,fg_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np,x_train,y_train,t_train):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    lambda2_val.append(PINN.lambda2.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse_u, test_re_u, test_mse_v, test_re_v, test_mse_p, test_re_p = PINN.test_loss()\n",
    "    \n",
    "    test_mse_u_loss.append(test_mse_u)\n",
    "    test_re_u_loss.append(test_re_u)\n",
    "    \n",
    "    test_mse_v_loss.append(test_mse_v)\n",
    "    test_re_v_loss.append(test_re_v)\n",
    "    \n",
    "    test_mse_p_loss.append(test_mse_p)\n",
    "    test_re_p_loss.append(test_re_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    x_train,y_train,t_train,u_train,v_train = trainingdata(N_T,(reps)*22)\n",
    "\n",
    "    x_train = torch.from_numpy(x_train).float().to(device)\n",
    "    y_train = torch.from_numpy(y_train).float().to(device)\n",
    "    t_train = torch.from_numpy(t_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    v_train = torch.from_numpy(v_train).float().to(device)\n",
    "        \n",
    "    fg_hat = torch.zeros(x_train.shape[0],1).to(device)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(x_train,y_train,t_train,u_train,v_train,fg_hat,i)\n",
    "\n",
    "        loss_np = PINN.loss(x_train,y_train,t_train,u_train,v_train,fg_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np,x_train,y_train,t_train)\n",
    "        print(i,\"Loss %.4f\"%train_loss[-1],\"TestP: %.4f\"%test_re_p_loss[-1], \"L1 %.4f\"%lambda1_val[-1],\"L2 %.4f\"%lambda2_val[-1])\n",
    "\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVnXJfj0CQr-",
    "outputId": "1f2921b0-e258-465d-aa27-cdeb80b78a0b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inv_NS_stan0\n",
      "0\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Loss 0.1580 TestP: 0.1052 L1 0.0015 L2 0.0143\n",
      "1 Loss 0.1491 TestP: 0.1137 L1 -0.0042 L2 0.0291\n",
      "2 Loss 0.1300 TestP: 0.2773 L1 -0.0228 L2 0.0395\n",
      "3 Loss 0.1246 TestP: 0.3243 L1 -0.0246 L2 0.0223\n",
      "4 Loss 0.1205 TestP: 0.3511 L1 -0.0359 L2 0.0096\n",
      "5 Loss 0.1164 TestP: 0.4051 L1 -0.0275 L2 -0.0111\n",
      "6 Loss 0.1141 TestP: 0.4020 L1 -0.0285 L2 -0.0135\n",
      "7 Loss 0.1115 TestP: 0.4193 L1 -0.0380 L2 0.0193\n",
      "8 Loss 0.1085 TestP: 0.3321 L1 -0.0305 L2 0.0065\n",
      "9 Loss 0.1039 TestP: 0.2361 L1 -0.0317 L2 0.0019\n",
      "10 Loss 0.1011 TestP: 0.1605 L1 -0.0362 L2 0.0041\n",
      "11 Loss 0.0989 TestP: 0.1986 L1 -0.0312 L2 -0.0055\n",
      "12 Loss 0.0977 TestP: 0.1658 L1 -0.0296 L2 -0.0028\n",
      "13 Loss 0.0972 TestP: 0.2246 L1 -0.0299 L2 0.0068\n",
      "14 Loss 0.0969 TestP: 0.2004 L1 -0.0301 L2 -0.0002\n",
      "15 Loss 0.0967 TestP: 0.3069 L1 -0.0289 L2 -0.0033\n",
      "16 Loss 0.0964 TestP: 0.3134 L1 -0.0283 L2 0.0034\n",
      "17 Loss 0.0961 TestP: 0.3443 L1 -0.0258 L2 -0.0011\n",
      "18 Loss 0.0959 TestP: 0.4572 L1 -0.0213 L2 0.0003\n",
      "19 Loss 0.0957 TestP: 0.4432 L1 -0.0206 L2 0.0005\n",
      "20 Loss 0.0956 TestP: 0.4784 L1 -0.0189 L2 0.0001\n",
      "21 Loss 0.0955 TestP: 0.5502 L1 -0.0167 L2 -0.0005\n",
      "22 Loss 0.0955 TestP: 0.6057 L1 -0.0145 L2 -0.0005\n",
      "23 Loss 0.0954 TestP: 0.6167 L1 -0.0121 L2 -0.0008\n",
      "24 Loss 0.0953 TestP: 0.6086 L1 -0.0067 L2 -0.0001\n",
      "25 Loss 0.0953 TestP: 0.6418 L1 -0.0045 L2 0.0004\n",
      "26 Loss 0.0953 TestP: 0.6666 L1 0.0036 L2 0.0003\n",
      "27 Loss 0.0952 TestP: 0.6566 L1 0.0047 L2 0.0004\n",
      "28 Loss 0.0952 TestP: 0.6737 L1 0.0112 L2 0.0009\n",
      "29 Loss 0.0952 TestP: 0.6979 L1 0.0174 L2 0.0011\n",
      "30 Loss 0.0952 TestP: 0.7319 L1 0.0255 L2 0.0014\n",
      "31 Loss 0.0951 TestP: 0.7090 L1 0.0289 L2 0.0017\n",
      "32 Loss 0.0951 TestP: 0.6988 L1 0.0190 L2 0.0018\n",
      "33 Loss 0.0951 TestP: 0.6629 L1 0.0113 L2 0.0013\n",
      "34 Loss 0.0951 TestP: 0.6617 L1 0.0032 L2 0.0010\n",
      "35 Loss 0.0950 TestP: 0.6365 L1 -0.0127 L2 0.0006\n",
      "36 Loss 0.0950 TestP: 0.7160 L1 -0.0123 L2 0.0008\n",
      "37 Loss 0.0950 TestP: 0.7312 L1 -0.0171 L2 0.0006\n",
      "38 Loss 0.0949 TestP: 0.6827 L1 -0.0270 L2 0.0005\n",
      "39 Loss 0.0948 TestP: 0.6330 L1 -0.0421 L2 0.0005\n",
      "40 Loss 0.0948 TestP: 0.6995 L1 -0.0523 L2 0.0002\n",
      "41 Loss 0.0948 TestP: 0.7162 L1 -0.0510 L2 -0.0002\n",
      "42 Loss 0.0947 TestP: 0.6712 L1 -0.0247 L2 0.0011\n",
      "43 Loss 0.0946 TestP: 0.6863 L1 -0.0059 L2 0.0012\n",
      "44 Loss 0.0946 TestP: 0.6772 L1 0.0098 L2 0.0019\n",
      "45 Loss 0.0945 TestP: 0.7091 L1 0.0169 L2 0.0008\n",
      "46 Loss 0.0945 TestP: 0.6663 L1 0.0239 L2 0.0017\n",
      "47 Loss 0.0944 TestP: 0.6462 L1 0.0090 L2 0.0015\n",
      "48 Loss 0.0944 TestP: 0.6033 L1 0.0174 L2 0.0017\n",
      "49 Loss 0.0943 TestP: 0.6172 L1 0.0158 L2 0.0018\n",
      "50 Loss 0.0943 TestP: 0.6302 L1 0.0108 L2 0.0017\n",
      "51 Loss 0.0943 TestP: 0.5843 L1 0.0014 L2 0.0015\n",
      "52 Loss 0.0943 TestP: 0.5690 L1 0.0068 L2 0.0013\n",
      "53 Loss 0.0943 TestP: 0.5546 L1 0.0081 L2 0.0017\n",
      "54 Loss 0.0942 TestP: 0.5470 L1 0.0044 L2 0.0013\n",
      "55 Loss 0.0942 TestP: 0.5509 L1 -0.0025 L2 0.0014\n",
      "56 Loss 0.0942 TestP: 0.5350 L1 -0.0022 L2 0.0009\n",
      "57 Loss 0.0942 TestP: 0.5358 L1 -0.0013 L2 0.0010\n",
      "58 Loss 0.0942 TestP: 0.5296 L1 -0.0044 L2 0.0012\n",
      "59 Loss 0.0942 TestP: 0.5412 L1 -0.0086 L2 0.0011\n",
      "60 Loss 0.0942 TestP: 0.5704 L1 -0.0050 L2 0.0008\n",
      "61 Loss 0.0942 TestP: 0.5736 L1 0.0008 L2 0.0013\n",
      "62 Loss 0.0941 TestP: 0.5501 L1 -0.0006 L2 0.0006\n",
      "63 Loss 0.0941 TestP: 0.5537 L1 -0.0020 L2 0.0009\n",
      "64 Loss 0.0941 TestP: 0.5425 L1 -0.0073 L2 0.0014\n",
      "65 Loss 0.0941 TestP: 0.5392 L1 -0.0188 L2 0.0003\n",
      "66 Loss 0.0940 TestP: 0.5448 L1 -0.0187 L2 0.0008\n",
      "67 Loss 0.0940 TestP: 0.5172 L1 -0.0172 L2 0.0000\n",
      "68 Loss 0.0940 TestP: 0.5077 L1 -0.0177 L2 0.0009\n",
      "69 Loss 0.0940 TestP: 0.5110 L1 -0.0103 L2 0.0002\n",
      "70 Loss 0.0939 TestP: 0.5295 L1 0.0035 L2 0.0011\n",
      "71 Loss 0.0939 TestP: 0.4910 L1 0.0102 L2 0.0013\n",
      "72 Loss 0.0939 TestP: 0.4933 L1 0.0074 L2 0.0012\n",
      "73 Loss 0.0939 TestP: 0.4815 L1 -0.0023 L2 0.0009\n",
      "74 Loss 0.0939 TestP: 0.4720 L1 -0.0024 L2 0.0011\n",
      "75 Loss 0.0938 TestP: 0.4799 L1 -0.0088 L2 0.0008\n",
      "76 Loss 0.0938 TestP: 0.5116 L1 -0.0033 L2 0.0008\n",
      "77 Loss 0.0938 TestP: 0.5188 L1 0.0010 L2 0.0013\n",
      "78 Loss 0.0938 TestP: 0.5328 L1 0.0137 L2 0.0014\n",
      "79 Loss 0.0938 TestP: 0.5614 L1 0.0115 L2 0.0013\n",
      "80 Loss 0.0938 TestP: 0.5695 L1 0.0079 L2 0.0013\n",
      "81 Loss 0.0938 TestP: 0.5954 L1 0.0114 L2 0.0011\n",
      "82 Loss 0.0937 TestP: 0.6072 L1 0.0113 L2 0.0012\n",
      "83 Loss 0.0937 TestP: 0.6269 L1 0.0075 L2 0.0011\n",
      "84 Loss 0.0937 TestP: 0.6687 L1 0.0034 L2 0.0010\n",
      "85 Loss 0.0937 TestP: 0.6909 L1 0.0060 L2 0.0009\n",
      "86 Loss 0.0937 TestP: 0.6894 L1 0.0089 L2 0.0013\n",
      "87 Loss 0.0937 TestP: 0.7213 L1 0.0103 L2 0.0009\n",
      "88 Loss 0.0936 TestP: 0.8198 L1 0.0069 L2 0.0009\n",
      "89 Loss 0.0936 TestP: 0.9036 L1 0.0160 L2 0.0008\n",
      "90 Loss 0.0935 TestP: 0.9296 L1 0.0240 L2 0.0013\n",
      "91 Loss 0.0934 TestP: 0.9024 L1 -0.0006 L2 0.0008\n",
      "92 Loss 0.0932 TestP: 0.9524 L1 -0.0042 L2 0.0001\n",
      "93 Loss 0.0931 TestP: 0.9458 L1 -0.0096 L2 0.0008\n",
      "94 Loss 0.0931 TestP: 0.9113 L1 0.0172 L2 0.0010\n",
      "95 Loss 0.0930 TestP: 0.8309 L1 0.0339 L2 0.0015\n",
      "96 Loss 0.0929 TestP: 0.8116 L1 0.0348 L2 0.0017\n",
      "97 Loss 0.0928 TestP: 0.8198 L1 0.0318 L2 0.0012\n",
      "98 Loss 0.0928 TestP: 0.8079 L1 0.0289 L2 0.0019\n",
      "99 Loss 0.0928 TestP: 0.7700 L1 0.0192 L2 0.0005\n",
      "Training time: 531.34\n",
      "Inv_NS_stan0\n",
      "1\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Loss 0.1709 TestP: 0.0790 L1 0.0015 L2 -0.0140\n",
      "1 Loss 0.1376 TestP: 0.2934 L1 0.0104 L2 -0.0109\n",
      "2 Loss 0.1316 TestP: 0.0994 L1 0.0069 L2 0.0654\n",
      "3 Loss 0.1168 TestP: 0.1649 L1 0.0130 L2 -0.0023\n",
      "4 Loss 0.1046 TestP: 0.2359 L1 0.0150 L2 0.0052\n",
      "5 Loss 0.1000 TestP: 0.2591 L1 0.0152 L2 0.0057\n",
      "6 Loss 0.0980 TestP: 0.2993 L1 0.0120 L2 0.0011\n",
      "7 Loss 0.0973 TestP: 0.2482 L1 0.0050 L2 -0.0003\n",
      "8 Loss 0.0969 TestP: 0.2066 L1 0.0020 L2 -0.0011\n",
      "9 Loss 0.0968 TestP: 0.2011 L1 0.0019 L2 -0.0004\n",
      "10 Loss 0.0966 TestP: 0.1670 L1 -0.0015 L2 -0.0004\n",
      "11 Loss 0.0965 TestP: 0.1614 L1 -0.0054 L2 -0.0004\n",
      "12 Loss 0.0963 TestP: 0.1483 L1 -0.0069 L2 -0.0005\n",
      "13 Loss 0.0962 TestP: 0.1315 L1 -0.0057 L2 -0.0001\n",
      "14 Loss 0.0961 TestP: 0.1267 L1 -0.0052 L2 0.0003\n",
      "15 Loss 0.0959 TestP: 0.1204 L1 -0.0049 L2 -0.0003\n",
      "16 Loss 0.0957 TestP: 0.1269 L1 -0.0048 L2 -0.0003\n",
      "17 Loss 0.0956 TestP: 0.1111 L1 -0.0046 L2 -0.0003\n",
      "18 Loss 0.0955 TestP: 0.1296 L1 -0.0059 L2 0.0001\n",
      "19 Loss 0.0954 TestP: 0.1401 L1 -0.0045 L2 -0.0003\n",
      "20 Loss 0.0953 TestP: 0.1362 L1 -0.0063 L2 0.0001\n",
      "21 Loss 0.0953 TestP: 0.1514 L1 -0.0051 L2 0.0000\n",
      "22 Loss 0.0952 TestP: 0.1542 L1 0.0026 L2 0.0002\n",
      "23 Loss 0.0951 TestP: 0.1545 L1 0.0031 L2 -0.0002\n",
      "24 Loss 0.0951 TestP: 0.1550 L1 -0.0028 L2 -0.0007\n",
      "25 Loss 0.0950 TestP: 0.1487 L1 -0.0129 L2 -0.0004\n",
      "26 Loss 0.0950 TestP: 0.1481 L1 -0.0214 L2 -0.0007\n",
      "27 Loss 0.0949 TestP: 0.1412 L1 -0.0225 L2 -0.0007\n",
      "28 Loss 0.0949 TestP: 0.1493 L1 -0.0243 L2 -0.0008\n",
      "29 Loss 0.0949 TestP: 0.1462 L1 -0.0157 L2 -0.0005\n",
      "30 Loss 0.0948 TestP: 0.1435 L1 -0.0123 L2 -0.0003\n",
      "31 Loss 0.0948 TestP: 0.1422 L1 -0.0048 L2 -0.0000\n",
      "32 Loss 0.0947 TestP: 0.1401 L1 0.0028 L2 0.0001\n",
      "33 Loss 0.0947 TestP: 0.1500 L1 0.0051 L2 -0.0001\n",
      "34 Loss 0.0947 TestP: 0.1381 L1 -0.0041 L2 -0.0000\n",
      "35 Loss 0.0946 TestP: 0.1502 L1 -0.0190 L2 -0.0007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 Loss 0.0944 TestP: 0.1283 L1 -0.0356 L2 -0.0012\n",
      "37 Loss 0.0939 TestP: 0.1333 L1 -0.0247 L2 -0.0008\n",
      "38 Loss 0.0935 TestP: 0.1424 L1 -0.0167 L2 -0.0009\n",
      "39 Loss 0.0929 TestP: 0.1829 L1 0.0385 L2 0.0003\n",
      "40 Loss 0.0926 TestP: 0.2067 L1 0.0707 L2 0.0009\n",
      "41 Loss 0.0921 TestP: 0.1985 L1 0.1102 L2 0.0013\n",
      "42 Loss 0.0898 TestP: 0.2012 L1 0.1231 L2 -0.0002\n",
      "43 Loss 0.0860 TestP: 0.2190 L1 0.2097 L2 0.0035\n",
      "44 Loss 0.0821 TestP: 0.2206 L1 0.5537 L2 0.0151\n",
      "45 Loss 0.0779 TestP: 0.1898 L1 0.7652 L2 0.0183\n",
      "46 Loss 0.0571 TestP: 0.1592 L1 0.8700 L2 0.0147\n",
      "47 Loss 0.0449 TestP: 0.2343 L1 0.7731 L2 0.0146\n",
      "48 Loss 0.0370 TestP: 0.2447 L1 0.8275 L2 0.0117\n",
      "49 Loss 0.0329 TestP: 0.1619 L1 0.8233 L2 0.0105\n",
      "50 Loss 0.0290 TestP: 0.1012 L1 0.8497 L2 0.0107\n",
      "51 Loss 0.0262 TestP: 0.0759 L1 0.8356 L2 0.0099\n",
      "52 Loss 0.0236 TestP: 0.0756 L1 0.8323 L2 0.0091\n",
      "53 Loss 0.0213 TestP: 0.0630 L1 0.8704 L2 0.0131\n",
      "54 Loss 0.0200 TestP: 0.0766 L1 0.8619 L2 0.0123\n",
      "55 Loss 0.0189 TestP: 0.0960 L1 0.8656 L2 0.0129\n",
      "56 Loss 0.0179 TestP: 0.1636 L1 0.8943 L2 0.0130\n",
      "57 Loss 0.0166 TestP: 0.1780 L1 0.8989 L2 0.0134\n",
      "58 Loss 0.0152 TestP: 0.2187 L1 0.9016 L2 0.0139\n",
      "59 Loss 0.0137 TestP: 0.3022 L1 0.9258 L2 0.0127\n",
      "60 Loss 0.0124 TestP: 0.3431 L1 0.9158 L2 0.0124\n",
      "61 Loss 0.0115 TestP: 0.3892 L1 0.9214 L2 0.0130\n",
      "62 Loss 0.0107 TestP: 0.4383 L1 0.9274 L2 0.0149\n",
      "63 Loss 0.0100 TestP: 0.5176 L1 0.9378 L2 0.0159\n",
      "64 Loss 0.0094 TestP: 0.5811 L1 0.9367 L2 0.0156\n",
      "65 Loss 0.0089 TestP: 0.6352 L1 0.9388 L2 0.0157\n",
      "66 Loss 0.0084 TestP: 0.7300 L1 0.9578 L2 0.0162\n",
      "67 Loss 0.0079 TestP: 0.8173 L1 0.9517 L2 0.0157\n",
      "68 Loss 0.0075 TestP: 0.8721 L1 0.9533 L2 0.0159\n",
      "69 Loss 0.0071 TestP: 0.9446 L1 0.9618 L2 0.0152\n",
      "70 Loss 0.0067 TestP: 0.9855 L1 0.9628 L2 0.0153\n",
      "71 Loss 0.0062 TestP: 1.0198 L1 0.9651 L2 0.0145\n",
      "72 Loss 0.0059 TestP: 1.0540 L1 0.9739 L2 0.0142\n",
      "73 Loss 0.0055 TestP: 1.0621 L1 0.9754 L2 0.0140\n",
      "74 Loss 0.0052 TestP: 1.0491 L1 0.9808 L2 0.0136\n",
      "75 Loss 0.0050 TestP: 1.0494 L1 0.9795 L2 0.0135\n",
      "76 Loss 0.0047 TestP: 1.0648 L1 0.9802 L2 0.0129\n",
      "77 Loss 0.0044 TestP: 1.0733 L1 0.9852 L2 0.0131\n",
      "78 Loss 0.0042 TestP: 1.0991 L1 0.9854 L2 0.0133\n",
      "79 Loss 0.0039 TestP: 1.1312 L1 0.9802 L2 0.0131\n",
      "80 Loss 0.0037 TestP: 1.1354 L1 0.9858 L2 0.0132\n",
      "81 Loss 0.0036 TestP: 1.1377 L1 0.9866 L2 0.0128\n",
      "82 Loss 0.0034 TestP: 1.1440 L1 0.9872 L2 0.0127\n",
      "83 Loss 0.0033 TestP: 1.1415 L1 0.9843 L2 0.0127\n",
      "84 Loss 0.0031 TestP: 1.1398 L1 0.9849 L2 0.0125\n",
      "85 Loss 0.0030 TestP: 1.1367 L1 0.9835 L2 0.0123\n",
      "86 Loss 0.0028 TestP: 1.1349 L1 0.9845 L2 0.0122\n",
      "87 Loss 0.0027 TestP: 1.1331 L1 0.9859 L2 0.0119\n",
      "88 Loss 0.0026 TestP: 1.1424 L1 0.9881 L2 0.0120\n",
      "89 Loss 0.0025 TestP: 1.1450 L1 0.9845 L2 0.0122\n",
      "90 Loss 0.0024 TestP: 1.1568 L1 0.9863 L2 0.0122\n",
      "91 Loss 0.0023 TestP: 1.1710 L1 0.9905 L2 0.0120\n",
      "92 Loss 0.0022 TestP: 1.1817 L1 0.9869 L2 0.0115\n",
      "93 Loss 0.0021 TestP: 1.1839 L1 0.9869 L2 0.0115\n",
      "94 Loss 0.0020 TestP: 1.1893 L1 0.9892 L2 0.0116\n",
      "95 Loss 0.0019 TestP: 1.1841 L1 0.9885 L2 0.0115\n",
      "96 Loss 0.0019 TestP: 1.1722 L1 0.9893 L2 0.0113\n",
      "97 Loss 0.0018 TestP: 1.1623 L1 0.9902 L2 0.0113\n",
      "98 Loss 0.0017 TestP: 1.1522 L1 0.9888 L2 0.0113\n",
      "99 Loss 0.0017 TestP: 1.1427 L1 0.9880 L2 0.0112\n",
      "Training time: 531.40\n",
      "Inv_NS_stan0\n",
      "2\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Loss 0.1673 TestP: 0.0838 L1 -0.0013 L2 0.0075\n",
      "1 Loss 0.1513 TestP: 0.0797 L1 -0.0041 L2 0.0048\n",
      "2 Loss 0.1353 TestP: 0.3018 L1 -0.0047 L2 0.0073\n",
      "3 Loss 0.1296 TestP: 0.3445 L1 -0.0021 L2 -0.0333\n",
      "4 Loss 0.1254 TestP: 0.4494 L1 0.0007 L2 -0.0224\n",
      "5 Loss 0.1212 TestP: 0.5918 L1 0.0025 L2 0.0336\n",
      "6 Loss 0.1186 TestP: 0.5204 L1 0.0014 L2 0.0248\n",
      "7 Loss 0.1163 TestP: 0.5243 L1 0.0021 L2 -0.0046\n",
      "8 Loss 0.1131 TestP: 0.3786 L1 -0.0008 L2 0.0100\n",
      "9 Loss 0.1112 TestP: 0.4729 L1 -0.0016 L2 -0.0110\n",
      "10 Loss 0.1087 TestP: 0.5637 L1 -0.0058 L2 0.0062\n",
      "11 Loss 0.1069 TestP: 0.5249 L1 -0.0073 L2 0.0061\n",
      "12 Loss 0.1054 TestP: 0.2817 L1 -0.0065 L2 -0.0006\n",
      "13 Loss 0.1029 TestP: 0.0594 L1 -0.0027 L2 0.0023\n",
      "14 Loss 0.1010 TestP: 0.1080 L1 0.0007 L2 0.0061\n",
      "15 Loss 0.0998 TestP: 0.1435 L1 -0.0002 L2 0.0013\n",
      "16 Loss 0.0982 TestP: 0.0882 L1 -0.0045 L2 0.0006\n",
      "17 Loss 0.0973 TestP: 0.2150 L1 -0.0033 L2 0.0009\n",
      "18 Loss 0.0966 TestP: 0.3707 L1 0.0038 L2 0.0015\n",
      "19 Loss 0.0962 TestP: 0.5209 L1 0.0096 L2 0.0012\n",
      "20 Loss 0.0960 TestP: 0.5729 L1 0.0124 L2 0.0002\n",
      "21 Loss 0.0957 TestP: 0.5688 L1 0.0118 L2 0.0011\n",
      "22 Loss 0.0955 TestP: 0.6067 L1 0.0052 L2 0.0005\n",
      "23 Loss 0.0953 TestP: 0.6291 L1 0.0040 L2 0.0004\n",
      "24 Loss 0.0952 TestP: 0.5321 L1 -0.0064 L2 0.0004\n",
      "25 Loss 0.0949 TestP: 0.3842 L1 -0.0221 L2 0.0006\n",
      "26 Loss 0.0946 TestP: 0.4225 L1 -0.0140 L2 0.0003\n",
      "27 Loss 0.0944 TestP: 0.4648 L1 -0.0133 L2 0.0005\n",
      "28 Loss 0.0941 TestP: 0.2692 L1 -0.0153 L2 0.0003\n",
      "29 Loss 0.0939 TestP: 0.1936 L1 -0.0124 L2 0.0004\n",
      "30 Loss 0.0938 TestP: 0.1619 L1 -0.0120 L2 0.0002\n",
      "31 Loss 0.0937 TestP: 0.1593 L1 -0.0109 L2 0.0003\n",
      "32 Loss 0.0937 TestP: 0.1289 L1 -0.0149 L2 0.0004\n",
      "33 Loss 0.0936 TestP: 0.0890 L1 -0.0122 L2 -0.0001\n",
      "34 Loss 0.0935 TestP: 0.1053 L1 -0.0067 L2 0.0004\n",
      "35 Loss 0.0934 TestP: 0.0575 L1 -0.0040 L2 0.0007\n",
      "36 Loss 0.0933 TestP: 0.0595 L1 -0.0116 L2 0.0003\n",
      "37 Loss 0.0932 TestP: 0.0589 L1 -0.0127 L2 0.0002\n",
      "38 Loss 0.0932 TestP: 0.0716 L1 -0.0083 L2 0.0003\n",
      "39 Loss 0.0931 TestP: 0.0593 L1 -0.0052 L2 0.0004\n",
      "40 Loss 0.0930 TestP: 0.0639 L1 -0.0090 L2 0.0002\n",
      "41 Loss 0.0930 TestP: 0.0571 L1 -0.0104 L2 0.0001\n",
      "42 Loss 0.0930 TestP: 0.0559 L1 -0.0133 L2 0.0003\n",
      "43 Loss 0.0929 TestP: 0.0558 L1 -0.0131 L2 0.0003\n",
      "44 Loss 0.0929 TestP: 0.0581 L1 -0.0111 L2 0.0002\n",
      "45 Loss 0.0929 TestP: 0.0574 L1 -0.0048 L2 0.0002\n",
      "46 Loss 0.0928 TestP: 0.0559 L1 -0.0032 L2 0.0003\n",
      "47 Loss 0.0928 TestP: 0.0640 L1 0.0014 L2 0.0004\n",
      "48 Loss 0.0927 TestP: 0.0572 L1 0.0029 L2 0.0005\n",
      "49 Loss 0.0927 TestP: 0.0695 L1 0.0079 L2 0.0006\n",
      "50 Loss 0.0926 TestP: 0.0857 L1 0.0046 L2 0.0005\n",
      "51 Loss 0.0925 TestP: 0.0712 L1 0.0026 L2 0.0005\n",
      "52 Loss 0.0925 TestP: 0.0623 L1 -0.0004 L2 0.0004\n",
      "53 Loss 0.0924 TestP: 0.0624 L1 -0.0032 L2 0.0003\n",
      "54 Loss 0.0922 TestP: 0.0925 L1 0.0050 L2 0.0004\n",
      "55 Loss 0.0920 TestP: 0.1124 L1 0.0091 L2 0.0004\n",
      "56 Loss 0.0919 TestP: 0.1308 L1 0.0128 L2 0.0005\n",
      "57 Loss 0.0916 TestP: 0.1835 L1 0.0190 L2 0.0008\n",
      "58 Loss 0.0915 TestP: 0.1681 L1 0.0247 L2 0.0005\n",
      "59 Loss 0.0914 TestP: 0.1492 L1 0.0269 L2 0.0006\n",
      "60 Loss 0.0912 TestP: 0.1150 L1 0.0291 L2 0.0009\n",
      "61 Loss 0.0911 TestP: 0.1194 L1 0.0322 L2 0.0007\n",
      "62 Loss 0.0909 TestP: 0.1153 L1 0.0434 L2 0.0010\n",
      "63 Loss 0.0909 TestP: 0.1179 L1 0.0456 L2 0.0014\n",
      "64 Loss 0.0908 TestP: 0.1101 L1 0.0488 L2 0.0013\n",
      "65 Loss 0.0907 TestP: 0.0804 L1 0.0453 L2 0.0009\n",
      "66 Loss 0.0906 TestP: 0.0912 L1 0.0440 L2 0.0012\n",
      "67 Loss 0.0905 TestP: 0.0989 L1 0.0478 L2 0.0010\n",
      "68 Loss 0.0904 TestP: 0.0902 L1 0.0509 L2 0.0009\n",
      "69 Loss 0.0904 TestP: 0.0916 L1 0.0522 L2 0.0014\n",
      "70 Loss 0.0903 TestP: 0.1013 L1 0.0523 L2 0.0010\n",
      "71 Loss 0.0902 TestP: 0.0734 L1 0.0487 L2 0.0010\n",
      "72 Loss 0.0902 TestP: 0.0577 L1 0.0474 L2 0.0012\n",
      "73 Loss 0.0901 TestP: 0.0553 L1 0.0494 L2 0.0011\n",
      "74 Loss 0.0900 TestP: 0.0598 L1 0.0466 L2 0.0013\n",
      "75 Loss 0.0899 TestP: 0.0665 L1 0.0429 L2 0.0014\n",
      "76 Loss 0.0898 TestP: 0.0595 L1 0.0408 L2 0.0008\n",
      "77 Loss 0.0897 TestP: 0.0590 L1 0.0375 L2 0.0009\n",
      "78 Loss 0.0896 TestP: 0.0582 L1 0.0321 L2 0.0007\n",
      "79 Loss 0.0895 TestP: 0.0628 L1 0.0319 L2 0.0008\n",
      "80 Loss 0.0895 TestP: 0.0572 L1 0.0344 L2 0.0007\n",
      "81 Loss 0.0895 TestP: 0.0588 L1 0.0328 L2 0.0006\n",
      "82 Loss 0.0894 TestP: 0.0604 L1 0.0301 L2 0.0008\n",
      "83 Loss 0.0894 TestP: 0.0580 L1 0.0293 L2 0.0007\n",
      "84 Loss 0.0893 TestP: 0.0579 L1 0.0303 L2 0.0006\n",
      "85 Loss 0.0892 TestP: 0.0554 L1 0.0326 L2 0.0006\n",
      "86 Loss 0.0892 TestP: 0.0611 L1 0.0339 L2 0.0002\n",
      "87 Loss 0.0891 TestP: 0.0584 L1 0.0327 L2 0.0003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 Loss 0.0890 TestP: 0.0552 L1 0.0350 L2 0.0001\n",
      "89 Loss 0.0890 TestP: 0.0554 L1 0.0367 L2 0.0000\n",
      "90 Loss 0.0889 TestP: 0.0576 L1 0.0339 L2 -0.0004\n",
      "91 Loss 0.0888 TestP: 0.0660 L1 0.0333 L2 -0.0005\n",
      "92 Loss 0.0886 TestP: 0.0563 L1 0.0296 L2 -0.0011\n",
      "93 Loss 0.0885 TestP: 0.0573 L1 0.0371 L2 -0.0011\n",
      "94 Loss 0.0882 TestP: 0.0673 L1 0.0493 L2 -0.0003\n",
      "95 Loss 0.0879 TestP: 0.0554 L1 0.0698 L2 -0.0000\n",
      "96 Loss 0.0872 TestP: 0.0558 L1 0.1131 L2 -0.0011\n",
      "97 Loss 0.0860 TestP: 0.0628 L1 0.1646 L2 -0.0006\n",
      "98 Loss 0.0835 TestP: 0.0573 L1 0.2288 L2 0.0042\n",
      "99 Loss 0.0811 TestP: 0.0583 L1 0.3374 L2 0.0067\n",
      "Training time: 521.08\n",
      "Inv_NS_stan0\n",
      "3\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Loss 0.1680 TestP: 0.1744 L1 -0.0029 L2 0.0271\n",
      "1 Loss 0.1448 TestP: 0.0573 L1 -0.0125 L2 0.0568\n",
      "2 Loss 0.1313 TestP: 0.0640 L1 -0.0160 L2 0.0030\n",
      "3 Loss 0.1159 TestP: 0.0632 L1 -0.0156 L2 -0.0238\n",
      "4 Loss 0.1073 TestP: 0.0638 L1 -0.0174 L2 -0.0065\n",
      "5 Loss 0.1015 TestP: 0.0919 L1 -0.0231 L2 0.0002\n",
      "6 Loss 0.0990 TestP: 0.0767 L1 -0.0270 L2 0.0002\n",
      "7 Loss 0.0976 TestP: 0.0958 L1 -0.0237 L2 -0.0020\n",
      "8 Loss 0.0971 TestP: 0.0778 L1 -0.0195 L2 0.0005\n",
      "9 Loss 0.0969 TestP: 0.0813 L1 -0.0114 L2 -0.0014\n",
      "10 Loss 0.0968 TestP: 0.0657 L1 -0.0011 L2 0.0003\n",
      "11 Loss 0.0966 TestP: 0.0609 L1 0.0051 L2 0.0006\n",
      "12 Loss 0.0964 TestP: 0.0555 L1 0.0068 L2 0.0005\n",
      "13 Loss 0.0962 TestP: 0.0602 L1 0.0102 L2 0.0005\n",
      "14 Loss 0.0961 TestP: 0.0623 L1 0.0007 L2 0.0005\n",
      "15 Loss 0.0960 TestP: 0.0800 L1 -0.0045 L2 0.0003\n",
      "16 Loss 0.0960 TestP: 0.0836 L1 -0.0124 L2 -0.0006\n",
      "17 Loss 0.0959 TestP: 0.0776 L1 -0.0194 L2 -0.0003\n",
      "18 Loss 0.0958 TestP: 0.0600 L1 -0.0201 L2 -0.0009\n",
      "19 Loss 0.0958 TestP: 0.0585 L1 -0.0160 L2 -0.0007\n",
      "20 Loss 0.0957 TestP: 0.0577 L1 -0.0103 L2 -0.0002\n",
      "21 Loss 0.0957 TestP: 0.0555 L1 0.0028 L2 0.0001\n",
      "22 Loss 0.0956 TestP: 0.0624 L1 0.0074 L2 0.0006\n",
      "23 Loss 0.0955 TestP: 0.0691 L1 0.0070 L2 0.0000\n",
      "24 Loss 0.0954 TestP: 0.0926 L1 -0.0016 L2 0.0006\n",
      "25 Loss 0.0953 TestP: 0.1078 L1 -0.0170 L2 0.0002\n",
      "26 Loss 0.0952 TestP: 0.1175 L1 -0.0238 L2 0.0001\n",
      "27 Loss 0.0950 TestP: 0.0921 L1 -0.0210 L2 -0.0001\n",
      "28 Loss 0.0948 TestP: 0.0904 L1 -0.0061 L2 0.0003\n",
      "29 Loss 0.0946 TestP: 0.1239 L1 0.0133 L2 0.0013\n",
      "30 Loss 0.0943 TestP: 0.1594 L1 0.0291 L2 0.0008\n",
      "31 Loss 0.0940 TestP: 0.1840 L1 0.0453 L2 0.0013\n",
      "32 Loss 0.0937 TestP: 0.1706 L1 0.0405 L2 0.0021\n",
      "33 Loss 0.0934 TestP: 0.1486 L1 0.0349 L2 0.0027\n",
      "34 Loss 0.0931 TestP: 0.1467 L1 0.0313 L2 0.0019\n",
      "35 Loss 0.0928 TestP: 0.1312 L1 0.0246 L2 0.0020\n",
      "36 Loss 0.0925 TestP: 0.1228 L1 0.0172 L2 0.0016\n",
      "37 Loss 0.0922 TestP: 0.1189 L1 0.0126 L2 0.0017\n",
      "38 Loss 0.0920 TestP: 0.1157 L1 0.0145 L2 0.0014\n",
      "39 Loss 0.0916 TestP: 0.0959 L1 0.0432 L2 0.0031\n",
      "40 Loss 0.0894 TestP: 0.0793 L1 0.1231 L2 0.0060\n",
      "41 Loss 0.0850 TestP: 0.0604 L1 0.2831 L2 0.0099\n",
      "42 Loss 0.0746 TestP: 0.1107 L1 0.5688 L2 0.0170\n",
      "43 Loss 0.0641 TestP: 0.1275 L1 0.8757 L2 0.0171\n",
      "44 Loss 0.0529 TestP: 0.0701 L1 0.8465 L2 0.0189\n",
      "45 Loss 0.0435 TestP: 0.0903 L1 0.8567 L2 0.0127\n",
      "46 Loss 0.0337 TestP: 0.0875 L1 0.8682 L2 0.0098\n",
      "47 Loss 0.0263 TestP: 0.0675 L1 0.8270 L2 0.0077\n",
      "48 Loss 0.0214 TestP: 0.0927 L1 0.8433 L2 0.0112\n",
      "49 Loss 0.0191 TestP: 0.1134 L1 0.8453 L2 0.0137\n",
      "50 Loss 0.0170 TestP: 0.1544 L1 0.8929 L2 0.0179\n",
      "51 Loss 0.0148 TestP: 0.2321 L1 0.8952 L2 0.0173\n",
      "52 Loss 0.0132 TestP: 0.2791 L1 0.8995 L2 0.0162\n",
      "53 Loss 0.0119 TestP: 0.2616 L1 0.9026 L2 0.0155\n",
      "54 Loss 0.0110 TestP: 0.2714 L1 0.9204 L2 0.0148\n",
      "55 Loss 0.0099 TestP: 0.2700 L1 0.9201 L2 0.0159\n",
      "56 Loss 0.0092 TestP: 0.3099 L1 0.9305 L2 0.0166\n",
      "57 Loss 0.0085 TestP: 0.3458 L1 0.9342 L2 0.0167\n",
      "58 Loss 0.0079 TestP: 0.3971 L1 0.9394 L2 0.0163\n",
      "59 Loss 0.0074 TestP: 0.4379 L1 0.9446 L2 0.0158\n",
      "60 Loss 0.0069 TestP: 0.5056 L1 0.9470 L2 0.0153\n",
      "61 Loss 0.0064 TestP: 0.5581 L1 0.9540 L2 0.0160\n",
      "62 Loss 0.0060 TestP: 0.6476 L1 0.9596 L2 0.0161\n",
      "63 Loss 0.0056 TestP: 0.7256 L1 0.9632 L2 0.0164\n",
      "64 Loss 0.0051 TestP: 0.7711 L1 0.9681 L2 0.0162\n",
      "65 Loss 0.0048 TestP: 0.8164 L1 0.9729 L2 0.0158\n",
      "66 Loss 0.0045 TestP: 0.8424 L1 0.9711 L2 0.0153\n",
      "67 Loss 0.0043 TestP: 0.8701 L1 0.9770 L2 0.0150\n",
      "68 Loss 0.0040 TestP: 0.8659 L1 0.9809 L2 0.0150\n",
      "69 Loss 0.0038 TestP: 0.8604 L1 0.9784 L2 0.0149\n",
      "70 Loss 0.0036 TestP: 0.8619 L1 0.9815 L2 0.0147\n",
      "71 Loss 0.0034 TestP: 0.8477 L1 0.9800 L2 0.0147\n",
      "72 Loss 0.0032 TestP: 0.8283 L1 0.9859 L2 0.0144\n",
      "73 Loss 0.0030 TestP: 0.8249 L1 0.9844 L2 0.0140\n",
      "74 Loss 0.0028 TestP: 0.8071 L1 0.9807 L2 0.0136\n",
      "75 Loss 0.0027 TestP: 0.7982 L1 0.9885 L2 0.0136\n",
      "76 Loss 0.0025 TestP: 0.7880 L1 0.9833 L2 0.0130\n",
      "77 Loss 0.0024 TestP: 0.7913 L1 0.9853 L2 0.0128\n",
      "78 Loss 0.0023 TestP: 0.7922 L1 0.9870 L2 0.0128\n",
      "79 Loss 0.0022 TestP: 0.7897 L1 0.9841 L2 0.0128\n",
      "80 Loss 0.0021 TestP: 0.7754 L1 0.9843 L2 0.0127\n",
      "81 Loss 0.0020 TestP: 0.7650 L1 0.9850 L2 0.0126\n",
      "82 Loss 0.0019 TestP: 0.7483 L1 0.9866 L2 0.0125\n",
      "83 Loss 0.0018 TestP: 0.7419 L1 0.9861 L2 0.0124\n",
      "84 Loss 0.0017 TestP: 0.7252 L1 0.9871 L2 0.0121\n",
      "85 Loss 0.0017 TestP: 0.7144 L1 0.9868 L2 0.0121\n",
      "86 Loss 0.0016 TestP: 0.6986 L1 0.9872 L2 0.0120\n",
      "87 Loss 0.0015 TestP: 0.6774 L1 0.9872 L2 0.0121\n",
      "88 Loss 0.0014 TestP: 0.6540 L1 0.9883 L2 0.0119\n",
      "89 Loss 0.0014 TestP: 0.6416 L1 0.9892 L2 0.0120\n",
      "90 Loss 0.0013 TestP: 0.6233 L1 0.9882 L2 0.0119\n",
      "91 Loss 0.0013 TestP: 0.6140 L1 0.9880 L2 0.0119\n",
      "92 Loss 0.0012 TestP: 0.6027 L1 0.9897 L2 0.0120\n",
      "93 Loss 0.0012 TestP: 0.5847 L1 0.9892 L2 0.0119\n",
      "94 Loss 0.0011 TestP: 0.5761 L1 0.9896 L2 0.0119\n",
      "95 Loss 0.0011 TestP: 0.5650 L1 0.9900 L2 0.0117\n",
      "96 Loss 0.0011 TestP: 0.5544 L1 0.9913 L2 0.0117\n",
      "97 Loss 0.0010 TestP: 0.5487 L1 0.9905 L2 0.0119\n",
      "98 Loss 0.0010 TestP: 0.5291 L1 0.9924 L2 0.0119\n",
      "99 Loss 0.0009 TestP: 0.5193 L1 0.9920 L2 0.0118\n",
      "Training time: 529.98\n",
      "Inv_NS_stan0\n",
      "4\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Loss 0.1718 TestP: 0.1915 L1 -0.0011 L2 0.0093\n",
      "1 Loss 0.1532 TestP: 0.2785 L1 -0.0029 L2 0.0374\n",
      "2 Loss 0.1463 TestP: 0.4446 L1 -0.0083 L2 0.0685\n",
      "3 Loss 0.1283 TestP: 0.5498 L1 -0.0087 L2 0.0410\n",
      "4 Loss 0.1251 TestP: 0.5263 L1 -0.0058 L2 -0.0325\n",
      "5 Loss 0.1180 TestP: 0.3058 L1 -0.0100 L2 0.0375\n",
      "6 Loss 0.1108 TestP: 0.2726 L1 -0.0151 L2 -0.0080\n",
      "7 Loss 0.1009 TestP: 0.1231 L1 -0.0194 L2 -0.0030\n",
      "8 Loss 0.0972 TestP: 0.2642 L1 -0.0185 L2 0.0028\n",
      "9 Loss 0.0966 TestP: 0.3061 L1 -0.0153 L2 0.0016\n",
      "10 Loss 0.0963 TestP: 0.2372 L1 -0.0087 L2 0.0014\n",
      "11 Loss 0.0962 TestP: 0.2427 L1 -0.0004 L2 -0.0003\n",
      "12 Loss 0.0960 TestP: 0.2603 L1 0.0048 L2 0.0002\n",
      "13 Loss 0.0959 TestP: 0.3185 L1 0.0110 L2 -0.0001\n",
      "14 Loss 0.0958 TestP: 0.3253 L1 0.0054 L2 -0.0003\n",
      "15 Loss 0.0957 TestP: 0.3362 L1 0.0006 L2 0.0002\n",
      "16 Loss 0.0956 TestP: 0.3687 L1 -0.0017 L2 0.0006\n",
      "17 Loss 0.0956 TestP: 0.3812 L1 -0.0002 L2 0.0005\n",
      "18 Loss 0.0956 TestP: 0.4082 L1 0.0005 L2 0.0006\n",
      "19 Loss 0.0955 TestP: 0.4211 L1 0.0028 L2 0.0009\n",
      "20 Loss 0.0955 TestP: 0.4456 L1 0.0065 L2 0.0014\n",
      "21 Loss 0.0955 TestP: 0.4198 L1 0.0138 L2 0.0014\n",
      "22 Loss 0.0954 TestP: 0.4367 L1 0.0157 L2 0.0014\n",
      "23 Loss 0.0954 TestP: 0.4281 L1 0.0063 L2 0.0011\n",
      "24 Loss 0.0954 TestP: 0.4191 L1 0.0005 L2 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 Loss 0.0953 TestP: 0.3943 L1 -0.0087 L2 0.0005\n",
      "26 Loss 0.0953 TestP: 0.3857 L1 0.0067 L2 0.0008\n",
      "27 Loss 0.0952 TestP: 0.3701 L1 0.0048 L2 0.0008\n",
      "28 Loss 0.0952 TestP: 0.3913 L1 0.0046 L2 0.0008\n",
      "29 Loss 0.0951 TestP: 0.3956 L1 -0.0041 L2 0.0008\n",
      "30 Loss 0.0951 TestP: 0.4195 L1 -0.0231 L2 0.0006\n",
      "31 Loss 0.0951 TestP: 0.4459 L1 -0.0248 L2 0.0009\n",
      "32 Loss 0.0950 TestP: 0.4265 L1 -0.0212 L2 0.0008\n",
      "33 Loss 0.0950 TestP: 0.4277 L1 -0.0178 L2 0.0007\n",
      "34 Loss 0.0950 TestP: 0.4381 L1 -0.0122 L2 0.0008\n",
      "35 Loss 0.0949 TestP: 0.4492 L1 -0.0047 L2 0.0010\n",
      "36 Loss 0.0949 TestP: 0.4717 L1 -0.0015 L2 0.0014\n",
      "37 Loss 0.0948 TestP: 0.4942 L1 -0.0131 L2 0.0010\n",
      "38 Loss 0.0948 TestP: 0.5213 L1 0.0006 L2 0.0011\n",
      "39 Loss 0.0947 TestP: 0.5256 L1 -0.0008 L2 0.0010\n",
      "40 Loss 0.0947 TestP: 0.5425 L1 0.0077 L2 0.0011\n",
      "41 Loss 0.0946 TestP: 0.5734 L1 0.0062 L2 0.0012\n",
      "42 Loss 0.0946 TestP: 0.5845 L1 0.0095 L2 0.0012\n",
      "43 Loss 0.0945 TestP: 0.6721 L1 0.0001 L2 0.0015\n",
      "44 Loss 0.0945 TestP: 0.6563 L1 0.0131 L2 0.0014\n",
      "45 Loss 0.0944 TestP: 0.6723 L1 0.0101 L2 0.0012\n",
      "46 Loss 0.0943 TestP: 0.6894 L1 0.0045 L2 0.0014\n",
      "47 Loss 0.0942 TestP: 0.6950 L1 -0.0084 L2 0.0014\n",
      "48 Loss 0.0941 TestP: 0.6929 L1 0.0102 L2 0.0013\n",
      "49 Loss 0.0939 TestP: 0.6847 L1 0.0134 L2 0.0014\n",
      "50 Loss 0.0938 TestP: 0.6652 L1 -0.0005 L2 0.0013\n",
      "51 Loss 0.0936 TestP: 0.6840 L1 0.0074 L2 0.0018\n",
      "52 Loss 0.0933 TestP: 0.6678 L1 0.0175 L2 0.0020\n",
      "53 Loss 0.0931 TestP: 0.6896 L1 0.0254 L2 0.0029\n",
      "54 Loss 0.0929 TestP: 0.7012 L1 0.0462 L2 0.0031\n",
      "55 Loss 0.0926 TestP: 0.7281 L1 0.0450 L2 0.0032\n",
      "56 Loss 0.0924 TestP: 0.6957 L1 0.0225 L2 0.0028\n",
      "57 Loss 0.0922 TestP: 0.6981 L1 0.0181 L2 0.0023\n",
      "58 Loss 0.0921 TestP: 0.6995 L1 0.0272 L2 0.0025\n",
      "59 Loss 0.0919 TestP: 0.6984 L1 0.0453 L2 0.0030\n",
      "60 Loss 0.0917 TestP: 0.7150 L1 0.0827 L2 0.0040\n",
      "61 Loss 0.0916 TestP: 0.7299 L1 0.0807 L2 0.0042\n",
      "62 Loss 0.0914 TestP: 0.8078 L1 0.0568 L2 0.0033\n",
      "63 Loss 0.0911 TestP: 0.8363 L1 0.0740 L2 0.0039\n",
      "64 Loss 0.0909 TestP: 0.8398 L1 0.0724 L2 0.0038\n",
      "65 Loss 0.0904 TestP: 0.8580 L1 0.0835 L2 0.0031\n",
      "66 Loss 0.0898 TestP: 0.8706 L1 0.1064 L2 0.0040\n",
      "67 Loss 0.0889 TestP: 0.8392 L1 0.1608 L2 0.0032\n",
      "68 Loss 0.0871 TestP: 0.7759 L1 0.3123 L2 0.0039\n",
      "69 Loss 0.0831 TestP: 0.8160 L1 0.4233 L2 0.0094\n",
      "70 Loss 0.0770 TestP: 0.7854 L1 0.6188 L2 0.0160\n",
      "71 Loss 0.0599 TestP: 0.7513 L1 0.8191 L2 0.0182\n",
      "72 Loss 0.0442 TestP: 0.7568 L1 0.7588 L2 0.0156\n",
      "73 Loss 0.0376 TestP: 0.6752 L1 0.8130 L2 0.0148\n",
      "74 Loss 0.0321 TestP: 0.7052 L1 0.8370 L2 0.0142\n",
      "75 Loss 0.0282 TestP: 0.7493 L1 0.8396 L2 0.0134\n",
      "76 Loss 0.0253 TestP: 0.7756 L1 0.8275 L2 0.0156\n",
      "77 Loss 0.0232 TestP: 0.8075 L1 0.8442 L2 0.0156\n",
      "78 Loss 0.0211 TestP: 0.7901 L1 0.8622 L2 0.0170\n",
      "79 Loss 0.0192 TestP: 0.8059 L1 0.8656 L2 0.0169\n",
      "80 Loss 0.0178 TestP: 0.7863 L1 0.8805 L2 0.0160\n",
      "81 Loss 0.0165 TestP: 0.7960 L1 0.8905 L2 0.0156\n",
      "82 Loss 0.0153 TestP: 0.7731 L1 0.8966 L2 0.0143\n",
      "83 Loss 0.0140 TestP: 0.7262 L1 0.9032 L2 0.0146\n",
      "84 Loss 0.0129 TestP: 0.7111 L1 0.9172 L2 0.0146\n",
      "85 Loss 0.0120 TestP: 0.7034 L1 0.9116 L2 0.0151\n",
      "86 Loss 0.0112 TestP: 0.7010 L1 0.9222 L2 0.0150\n",
      "87 Loss 0.0105 TestP: 0.6983 L1 0.9218 L2 0.0150\n",
      "88 Loss 0.0098 TestP: 0.6975 L1 0.9276 L2 0.0156\n",
      "89 Loss 0.0092 TestP: 0.6910 L1 0.9345 L2 0.0166\n",
      "90 Loss 0.0086 TestP: 0.7018 L1 0.9371 L2 0.0172\n",
      "91 Loss 0.0082 TestP: 0.7069 L1 0.9369 L2 0.0164\n",
      "92 Loss 0.0077 TestP: 0.7415 L1 0.9451 L2 0.0158\n",
      "93 Loss 0.0072 TestP: 0.7474 L1 0.9524 L2 0.0158\n",
      "94 Loss 0.0067 TestP: 0.7099 L1 0.9490 L2 0.0154\n",
      "95 Loss 0.0063 TestP: 0.7049 L1 0.9541 L2 0.0155\n",
      "96 Loss 0.0060 TestP: 0.6981 L1 0.9548 L2 0.0148\n",
      "97 Loss 0.0056 TestP: 0.7065 L1 0.9550 L2 0.0144\n",
      "98 Loss 0.0053 TestP: 0.6950 L1 0.9625 L2 0.0143\n",
      "99 Loss 0.0051 TestP: 0.6866 L1 0.9634 L2 0.0142\n",
      "Training time: 530.70\n",
      "Inv_NS_stan0\n",
      "5\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Loss 0.1627 TestP: 0.0537 L1 -0.0021 L2 0.0201\n",
      "1 Loss 0.1477 TestP: 0.2494 L1 -0.0114 L2 0.0307\n",
      "2 Loss 0.1320 TestP: 0.2746 L1 -0.0161 L2 0.0088\n",
      "3 Loss 0.1280 TestP: 0.0985 L1 -0.0077 L2 -0.0842\n",
      "4 Loss 0.1144 TestP: 0.1423 L1 -0.0102 L2 -0.0307\n",
      "5 Loss 0.1015 TestP: 0.2373 L1 -0.0141 L2 0.0007\n",
      "6 Loss 0.0983 TestP: 0.1830 L1 -0.0144 L2 0.0013\n",
      "7 Loss 0.0969 TestP: 0.1265 L1 -0.0173 L2 0.0010\n",
      "8 Loss 0.0964 TestP: 0.1393 L1 -0.0188 L2 -0.0006\n",
      "9 Loss 0.0959 TestP: 0.1231 L1 -0.0171 L2 -0.0006\n",
      "10 Loss 0.0958 TestP: 0.0851 L1 -0.0114 L2 -0.0002\n",
      "11 Loss 0.0956 TestP: 0.0737 L1 -0.0100 L2 -0.0008\n",
      "12 Loss 0.0955 TestP: 0.0575 L1 -0.0080 L2 0.0001\n",
      "13 Loss 0.0954 TestP: 0.0663 L1 0.0055 L2 0.0007\n",
      "14 Loss 0.0954 TestP: 0.0651 L1 0.0091 L2 0.0006\n",
      "15 Loss 0.0953 TestP: 0.0747 L1 0.0105 L2 0.0006\n",
      "16 Loss 0.0953 TestP: 0.0735 L1 0.0069 L2 0.0004\n",
      "17 Loss 0.0952 TestP: 0.0887 L1 0.0045 L2 0.0013\n",
      "18 Loss 0.0951 TestP: 0.1027 L1 0.0086 L2 0.0013\n",
      "19 Loss 0.0951 TestP: 0.1048 L1 0.0169 L2 0.0013\n",
      "20 Loss 0.0950 TestP: 0.1129 L1 0.0143 L2 0.0013\n",
      "21 Loss 0.0950 TestP: 0.1299 L1 0.0117 L2 0.0009\n",
      "22 Loss 0.0949 TestP: 0.1935 L1 -0.0003 L2 0.0008\n",
      "23 Loss 0.0948 TestP: 0.2226 L1 -0.0074 L2 0.0008\n",
      "24 Loss 0.0947 TestP: 0.2612 L1 -0.0062 L2 0.0006\n",
      "25 Loss 0.0947 TestP: 0.2469 L1 -0.0081 L2 0.0004\n",
      "26 Loss 0.0946 TestP: 0.2161 L1 -0.0148 L2 -0.0001\n",
      "27 Loss 0.0945 TestP: 0.2321 L1 -0.0097 L2 -0.0002\n",
      "28 Loss 0.0945 TestP: 0.2574 L1 0.0024 L2 0.0002\n",
      "29 Loss 0.0944 TestP: 0.2774 L1 0.0072 L2 0.0004\n",
      "30 Loss 0.0944 TestP: 0.2792 L1 0.0117 L2 0.0002\n",
      "31 Loss 0.0944 TestP: 0.2952 L1 0.0061 L2 -0.0001\n",
      "32 Loss 0.0943 TestP: 0.2806 L1 -0.0063 L2 -0.0003\n",
      "33 Loss 0.0943 TestP: 0.2792 L1 -0.0263 L2 -0.0010\n",
      "34 Loss 0.0942 TestP: 0.3068 L1 -0.0365 L2 -0.0010\n",
      "35 Loss 0.0941 TestP: 0.3138 L1 -0.0261 L2 -0.0005\n",
      "36 Loss 0.0941 TestP: 0.3182 L1 -0.0094 L2 -0.0003\n",
      "37 Loss 0.0939 TestP: 0.2904 L1 -0.0150 L2 -0.0008\n",
      "38 Loss 0.0938 TestP: 0.3095 L1 -0.0261 L2 -0.0003\n",
      "39 Loss 0.0936 TestP: 0.3284 L1 -0.0164 L2 -0.0005\n",
      "40 Loss 0.0934 TestP: 0.3251 L1 0.0233 L2 0.0008\n",
      "41 Loss 0.0929 TestP: 0.1942 L1 0.0371 L2 0.0022\n",
      "42 Loss 0.0922 TestP: 0.2164 L1 0.0934 L2 0.0044\n",
      "43 Loss 0.0906 TestP: 0.2312 L1 0.1387 L2 0.0050\n",
      "44 Loss 0.0867 TestP: 0.3665 L1 0.2420 L2 0.0073\n",
      "45 Loss 0.0762 TestP: 0.3431 L1 0.6204 L2 0.0195\n",
      "46 Loss 0.0696 TestP: 0.4392 L1 0.7346 L2 0.0159\n",
      "47 Loss 0.0655 TestP: 0.3209 L1 0.8698 L2 0.0159\n",
      "48 Loss 0.0628 TestP: 0.3325 L1 0.8694 L2 0.0193\n",
      "49 Loss 0.0602 TestP: 0.4063 L1 0.8346 L2 0.0153\n",
      "50 Loss 0.0573 TestP: 0.4709 L1 0.8561 L2 0.0168\n",
      "51 Loss 0.0544 TestP: 0.5979 L1 0.8556 L2 0.0159\n",
      "52 Loss 0.0498 TestP: 0.6993 L1 0.8383 L2 0.0146\n",
      "53 Loss 0.0394 TestP: 0.7172 L1 0.7982 L2 0.0143\n",
      "54 Loss 0.0331 TestP: 0.6338 L1 0.8335 L2 0.0147\n",
      "55 Loss 0.0300 TestP: 0.5807 L1 0.8360 L2 0.0144\n",
      "56 Loss 0.0275 TestP: 0.5756 L1 0.7836 L2 0.0133\n",
      "57 Loss 0.0260 TestP: 0.6332 L1 0.8170 L2 0.0106\n",
      "58 Loss 0.0245 TestP: 0.6705 L1 0.8392 L2 0.0090\n",
      "59 Loss 0.0234 TestP: 0.6871 L1 0.8369 L2 0.0109\n",
      "60 Loss 0.0222 TestP: 0.7001 L1 0.8395 L2 0.0118\n",
      "61 Loss 0.0210 TestP: 0.7469 L1 0.8527 L2 0.0127\n",
      "62 Loss 0.0197 TestP: 0.7452 L1 0.8481 L2 0.0134\n",
      "63 Loss 0.0190 TestP: 0.7588 L1 0.8638 L2 0.0119\n",
      "64 Loss 0.0181 TestP: 0.8225 L1 0.8592 L2 0.0132\n",
      "65 Loss 0.0173 TestP: 0.8756 L1 0.8703 L2 0.0144\n",
      "66 Loss 0.0166 TestP: 0.9028 L1 0.8897 L2 0.0140\n",
      "67 Loss 0.0158 TestP: 0.9214 L1 0.8866 L2 0.0146\n",
      "68 Loss 0.0151 TestP: 0.9779 L1 0.8932 L2 0.0129\n",
      "69 Loss 0.0145 TestP: 1.0187 L1 0.8964 L2 0.0128\n",
      "70 Loss 0.0139 TestP: 1.0734 L1 0.9005 L2 0.0143\n",
      "71 Loss 0.0134 TestP: 1.0954 L1 0.9013 L2 0.0160\n",
      "72 Loss 0.0128 TestP: 1.1571 L1 0.9109 L2 0.0150\n",
      "73 Loss 0.0123 TestP: 1.1913 L1 0.9065 L2 0.0139\n",
      "74 Loss 0.0118 TestP: 1.2681 L1 0.9149 L2 0.0140\n",
      "75 Loss 0.0112 TestP: 1.3128 L1 0.9143 L2 0.0144\n",
      "76 Loss 0.0108 TestP: 1.3511 L1 0.9161 L2 0.0141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 Loss 0.0103 TestP: 1.3909 L1 0.9162 L2 0.0159\n",
      "78 Loss 0.0100 TestP: 1.4103 L1 0.9266 L2 0.0161\n",
      "79 Loss 0.0096 TestP: 1.4306 L1 0.9367 L2 0.0174\n",
      "80 Loss 0.0093 TestP: 1.4301 L1 0.9383 L2 0.0170\n",
      "81 Loss 0.0089 TestP: 1.4495 L1 0.9341 L2 0.0165\n",
      "82 Loss 0.0086 TestP: 1.4806 L1 0.9399 L2 0.0159\n",
      "83 Loss 0.0082 TestP: 1.5218 L1 0.9457 L2 0.0158\n",
      "84 Loss 0.0079 TestP: 1.5602 L1 0.9434 L2 0.0162\n",
      "85 Loss 0.0076 TestP: 1.5912 L1 0.9483 L2 0.0156\n",
      "86 Loss 0.0072 TestP: 1.6224 L1 0.9547 L2 0.0151\n",
      "87 Loss 0.0069 TestP: 1.6544 L1 0.9564 L2 0.0147\n",
      "88 Loss 0.0065 TestP: 1.6880 L1 0.9578 L2 0.0152\n",
      "89 Loss 0.0061 TestP: 1.7316 L1 0.9561 L2 0.0149\n",
      "90 Loss 0.0059 TestP: 1.7779 L1 0.9642 L2 0.0144\n",
      "91 Loss 0.0056 TestP: 1.8384 L1 0.9681 L2 0.0147\n",
      "92 Loss 0.0053 TestP: 1.9129 L1 0.9664 L2 0.0143\n",
      "93 Loss 0.0050 TestP: 1.9493 L1 0.9700 L2 0.0148\n",
      "94 Loss 0.0049 TestP: 1.9615 L1 0.9689 L2 0.0149\n",
      "95 Loss 0.0047 TestP: 1.9540 L1 0.9684 L2 0.0147\n",
      "96 Loss 0.0045 TestP: 1.9595 L1 0.9741 L2 0.0151\n",
      "97 Loss 0.0043 TestP: 1.9711 L1 0.9743 L2 0.0150\n",
      "98 Loss 0.0042 TestP: 1.9572 L1 0.9729 L2 0.0145\n",
      "99 Loss 0.0040 TestP: 1.9527 L1 0.9737 L2 0.0143\n",
      "Training time: 520.80\n",
      "Inv_NS_stan0\n",
      "6\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Loss 0.1612 TestP: 0.0579 L1 0.0007 L2 0.0012\n",
      "1 Loss 0.1509 TestP: 0.1306 L1 0.0057 L2 0.0043\n",
      "2 Loss 0.1282 TestP: 0.1510 L1 0.0112 L2 0.0080\n",
      "3 Loss 0.1239 TestP: 0.2084 L1 0.0145 L2 0.0053\n",
      "4 Loss 0.1198 TestP: 0.1686 L1 0.0200 L2 -0.0010\n",
      "5 Loss 0.1161 TestP: 0.2217 L1 0.0206 L2 -0.0023\n",
      "6 Loss 0.1139 TestP: 0.1914 L1 0.0153 L2 0.0004\n",
      "7 Loss 0.1098 TestP: 0.2072 L1 0.0048 L2 0.0111\n",
      "8 Loss 0.1023 TestP: 0.0627 L1 0.0054 L2 -0.0203\n",
      "9 Loss 0.0998 TestP: 0.1218 L1 0.0032 L2 0.0097\n",
      "10 Loss 0.0978 TestP: 0.1661 L1 0.0058 L2 -0.0009\n",
      "11 Loss 0.0972 TestP: 0.1507 L1 0.0052 L2 -0.0044\n",
      "12 Loss 0.0965 TestP: 0.1211 L1 0.0081 L2 0.0027\n",
      "13 Loss 0.0961 TestP: 0.1096 L1 0.0065 L2 0.0004\n",
      "14 Loss 0.0958 TestP: 0.1662 L1 -0.0020 L2 0.0003\n",
      "15 Loss 0.0957 TestP: 0.1475 L1 -0.0076 L2 -0.0004\n"
     ]
    }
   ],
   "source": [
    "nan_tune = []\n",
    "for tune_reps in range(20):\n",
    "\n",
    "    label = \"Inv_NS_stan\" + str(tune_reps)\n",
    "\n",
    "    max_reps = 10\n",
    "    max_iter = 100 #1000\n",
    "\n",
    "    train_loss_full = []\n",
    "\n",
    "    test_mse_u_full = []\n",
    "    test_re_u_full = []\n",
    "    test_mse_v_full = []\n",
    "    test_re_v_full = []\n",
    "    test_mse_p_full = []\n",
    "    test_re_p_full = []\n",
    "\n",
    "\n",
    "    beta_full = []\n",
    "    lambda1_full = []\n",
    "    lambda2_full = []\n",
    "\n",
    "    elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "    time_threshold = np.empty((max_reps,1))\n",
    "    time_threshold[:] = np.nan\n",
    "    epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "    beta_init = lrb_tune[tune_reps,1]\n",
    "\n",
    "    for reps in range(max_reps):\n",
    "        print(label)\n",
    "        print(reps)\n",
    "        train_loss = []\n",
    "\n",
    "        test_mse_u_loss = []\n",
    "        test_re_u_loss = []\n",
    "\n",
    "        test_mse_v_loss = []\n",
    "        test_re_v_loss = []\n",
    "\n",
    "        test_mse_p_loss = []\n",
    "        test_re_p_loss = []\n",
    "\n",
    "        lambda1_val = []\n",
    "        lambda2_val = []\n",
    "        beta_val = []\n",
    "\n",
    "        print(reps)\n",
    "\n",
    "        torch.manual_seed(reps*36)\n",
    "        N_T = 5000\n",
    "\n",
    "        layers = np.array([3,50,50,50,50,50,50,50,50,50,2]) #9 hidden layers\n",
    "\n",
    "        PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "        PINN.to(device)\n",
    "\n",
    "        'Neural Network Summary'\n",
    "        print(PINN)\n",
    "\n",
    "        params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "        optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lrb_tune[tune_reps,0], \n",
    "                                  max_iter = 20, \n",
    "                                  max_eval = 30, \n",
    "                                  tolerance_grad = 1e-8, \n",
    "                                  tolerance_change = 1e-8, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "        nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "        torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "        train_loss_full.append(train_loss)\n",
    "\n",
    "        test_mse_u_full.append(test_mse_u_loss)\n",
    "        test_re_u_full.append(test_re_u_loss)\n",
    "        test_mse_v_full.append(test_mse_v_loss)\n",
    "        test_re_v_full.append(test_re_v_loss)\n",
    "        test_mse_p_full.append(test_mse_p_loss)\n",
    "        test_re_p_full.append(test_re_p_loss)\n",
    "\n",
    "        #elapsed_time[reps] = time.time() - start_time\n",
    "        beta_full.append(beta_val)\n",
    "        lambda1_full.append(lambda1_val)\n",
    "        lambda2_full.append(lambda2_val)\n",
    "\n",
    "        #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "    mdic = {\"train_loss\": train_loss_full,\"test_mse_u\": test_mse_u_full,\"test_re_u_loss\": test_re_u_full,\"test_mse_v\": test_mse_v_full,\"test_re_v_loss\": test_re_v_full,\"test_mse_p\": test_mse_p_full,\"test_re_p_loss\": test_re_p_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold, \"lambda1\":lambda1_full,\"lambda2\":lambda2_full}\n",
    "    savemat(label+'.mat', mdic)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HT_stan_v3_15Aug2022_MP4Video.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
