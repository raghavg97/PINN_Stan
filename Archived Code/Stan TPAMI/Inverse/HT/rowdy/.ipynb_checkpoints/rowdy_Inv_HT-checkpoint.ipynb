{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_2D_4(xt): #True function for 2D_4 Heat Transfer in a rod x \\in [0,1] t \\in [0,0.1]\n",
    "    term1 = 4*u0/np.pi\n",
    "    \n",
    "    resol_n = 10000\n",
    "    \n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "\n",
    "    u = np.zeros((np.shape(xt)[0],1))\n",
    "    \n",
    "    for i in range(resol_n):\n",
    "        j = 2*i-1\n",
    "        term2 = np.sin(j*np.pi*x)/j\n",
    "        term3 = np.exp(-1*np.square(j*np.pi)*t)\n",
    "        \n",
    "        u = u + term2*term3\n",
    "        \n",
    "    u = term1*u\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = 50.0\n",
    "label = \"inv_HT_rowdy\"\n",
    "loss_thresh = 0.1\n",
    "\n",
    "x_ll = np.array(0.0)\n",
    "x_ul = np.array(1.0)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,100).reshape(-1,1)\n",
    "t = np.linspace(0,0.1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = true_2D_4(xt)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_f,N_train,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #X_Train\n",
    "    np.random.seed(seed)\n",
    "    x_train = np.random.uniform(x_ll,x_ul,(N_train,1))\n",
    "    t_train = np.random.uniform(0,0.1,(N_train,1))\n",
    "    \n",
    "    xt_train = np.hstack((x_train,t_train))\n",
    "    u_train = true_2D_4(xt_train)\n",
    "    \n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        self.omega1.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        self.omega.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "        \n",
    "    \n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requiresGrad = True\n",
    "        \n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    \n",
    "    def loss_PDE(self, xt_coll,f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_tt[:,[0]]\n",
    "                \n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        f = du_dt - self.lambda1*d2u_dx2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_train = self.loss_function(self.forward(xt_train),u_train)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "        \n",
    "        #print(self.iter,\"train_loss\",loss_train.cpu().detach().numpy(),\"F Loss\",(loss_f).cpu().detach().numpy())\n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                    \n",
    "    \n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_coll,f_hat, xt_train, u_train,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xt_coll, xt_train, u_train = trainingdata(N_f,N_train,123)\n",
    "    \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_train = torch.from_numpy(xt_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_coll,f_hat, xt_train, u_train,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_coll,f_hat, xt_train, u_train).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1],\"Lambda1\",lambda1_val[-1])\n",
    "\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_HT_rowdy\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 854.7841 Test MSE 857.1283222105515 Test RE 0.49284697300910685 Lambda1 0.0024360672\n",
      "1 Train Loss 854.4778 Test MSE 857.703886388977 Test RE 0.49301241933217305 Lambda1 0.005332648\n",
      "2 Train Loss 853.6976 Test MSE 855.9900124962384 Test RE 0.49251960144202356 Lambda1 0.033224843\n",
      "3 Train Loss 851.3819 Test MSE 853.941685711463 Test RE 0.4919299652766502 Lambda1 0.066705234\n",
      "4 Train Loss 848.34656 Test MSE 850.8087470597271 Test RE 0.4910267405003057 Lambda1 0.1016006\n",
      "5 Train Loss 817.06 Test MSE 803.0299731033404 Test RE 0.4770402769221276 Lambda1 0.15638186\n",
      "6 Train Loss 692.17914 Test MSE 640.3212158363001 Test RE 0.42597892813838467 Lambda1 0.15176353\n",
      "7 Train Loss 417.96747 Test MSE 354.9113855922405 Test RE 0.3171386232536177 Lambda1 0.12603427\n",
      "8 Train Loss 296.06763 Test MSE 279.963498386109 Test RE 0.28166953480310586 Lambda1 0.10876424\n",
      "9 Train Loss 269.6412 Test MSE 266.35752511645796 Test RE 0.2747398495637363 Lambda1 0.08890877\n",
      "10 Train Loss 252.92241 Test MSE 251.92083290191832 Test RE 0.2671906228679225 Lambda1 0.09946613\n",
      "11 Train Loss 244.05865 Test MSE 240.61947878573764 Test RE 0.2611286728397067 Lambda1 0.1076081\n",
      "12 Train Loss 235.6447 Test MSE 234.05056136675995 Test RE 0.2575395984334433 Lambda1 0.12272495\n",
      "13 Train Loss 224.89037 Test MSE 222.50549088694444 Test RE 0.2511074148531848 Lambda1 0.14778772\n",
      "14 Train Loss 215.0501 Test MSE 213.19305022141114 Test RE 0.24579649872827883 Lambda1 0.16790394\n",
      "15 Train Loss 202.91908 Test MSE 200.01939313366515 Test RE 0.23808126601778953 Lambda1 0.20930812\n",
      "16 Train Loss 193.91934 Test MSE 188.51185723937726 Test RE 0.23113116413944623 Lambda1 0.23837939\n",
      "17 Train Loss 184.95004 Test MSE 175.83474291831476 Test RE 0.22322432378734405 Lambda1 0.2574554\n",
      "18 Train Loss 169.448 Test MSE 159.2684851013715 Test RE 0.2124487084736237 Lambda1 0.28623638\n",
      "19 Train Loss 152.18599 Test MSE 134.86881946199728 Test RE 0.19549918383421497 Lambda1 0.33557144\n",
      "20 Train Loss 127.48723 Test MSE 116.0577932583246 Test RE 0.181353660069767 Lambda1 0.374683\n",
      "21 Train Loss 107.80628 Test MSE 92.08225129338872 Test RE 0.16153890511646468 Lambda1 0.423414\n",
      "22 Train Loss 90.80138 Test MSE 79.93821400787466 Test RE 0.15051035881799962 Lambda1 0.47931772\n",
      "23 Train Loss 78.760796 Test MSE 68.0806424867123 Test RE 0.1388995994759783 Lambda1 0.48534203\n",
      "24 Train Loss 72.63677 Test MSE 62.917532801781825 Test RE 0.13352882049040274 Lambda1 0.48837543\n",
      "25 Train Loss 62.60807 Test MSE 58.36093542304139 Test RE 0.12860276103248777 Lambda1 0.50196093\n",
      "26 Train Loss 59.28067 Test MSE 55.82196356748557 Test RE 0.12577424676620486 Lambda1 0.5084298\n",
      "27 Train Loss 53.69597 Test MSE 53.88733484227187 Test RE 0.12357554174142626 Lambda1 0.52330196\n",
      "28 Train Loss 52.44145 Test MSE 53.2273947524688 Test RE 0.12281651662496385 Lambda1 0.53131914\n",
      "29 Train Loss 51.35761 Test MSE 52.18412012524137 Test RE 0.12160693798609694 Lambda1 0.54202694\n",
      "30 Train Loss 49.97064 Test MSE 51.57624692295782 Test RE 0.1208965865144605 Lambda1 0.55965924\n",
      "31 Train Loss 49.37807 Test MSE 50.93054998711746 Test RE 0.1201374345490052 Lambda1 0.57632476\n",
      "32 Train Loss 48.56382 Test MSE 50.715583338375644 Test RE 0.1198836296092877 Lambda1 0.5699152\n",
      "33 Train Loss 47.26778 Test MSE 49.699422299319146 Test RE 0.11867653040587127 Lambda1 0.5788037\n",
      "34 Train Loss 46.49811 Test MSE 48.49519091517152 Test RE 0.11722993045022802 Lambda1 0.5924634\n",
      "35 Train Loss 45.844585 Test MSE 47.268816810959905 Test RE 0.11573815006905877 Lambda1 0.5990006\n",
      "36 Train Loss 45.296192 Test MSE 46.72934654713726 Test RE 0.1150758057924993 Lambda1 0.60015774\n",
      "37 Train Loss 44.80237 Test MSE 46.2083189108555 Test RE 0.11443246548958802 Lambda1 0.6100175\n",
      "38 Train Loss 44.13505 Test MSE 45.69150231072544 Test RE 0.11379073154616871 Lambda1 0.6254999\n",
      "39 Train Loss 42.907784 Test MSE 44.795083192940545 Test RE 0.11266897526334801 Lambda1 0.6489022\n",
      "40 Train Loss 42.123466 Test MSE 44.34585885267663 Test RE 0.11210260528922861 Lambda1 0.6580971\n",
      "41 Train Loss 41.123764 Test MSE 43.063657297915455 Test RE 0.11047006917980964 Lambda1 0.6710774\n",
      "42 Train Loss 39.972122 Test MSE 42.055576777159956 Test RE 0.10916941105899046 Lambda1 0.6776754\n",
      "43 Train Loss 39.476967 Test MSE 41.339089980152316 Test RE 0.10823547480767814 Lambda1 0.68972677\n",
      "44 Train Loss 38.784927 Test MSE 40.542977702512 Test RE 0.10718820345009422 Lambda1 0.70630234\n",
      "45 Train Loss 37.121834 Test MSE 39.83018975918651 Test RE 0.10624178489636082 Lambda1 0.7082114\n",
      "46 Train Loss 36.648525 Test MSE 39.53283716001656 Test RE 0.1058444675171073 Lambda1 0.7035317\n",
      "47 Train Loss 36.466198 Test MSE 39.26295819757031 Test RE 0.10548256440339898 Lambda1 0.70634645\n",
      "48 Train Loss 36.25461 Test MSE 39.15976282981258 Test RE 0.10534385256857917 Lambda1 0.701121\n",
      "49 Train Loss 35.97548 Test MSE 39.04443300487583 Test RE 0.10518861354717489 Lambda1 0.70484036\n",
      "50 Train Loss 35.292667 Test MSE 38.23306477426278 Test RE 0.10408993250550157 Lambda1 0.725188\n",
      "51 Train Loss 34.896328 Test MSE 37.767506246320366 Test RE 0.10345424735196726 Lambda1 0.73914415\n",
      "52 Train Loss 34.80226 Test MSE 37.802380210149096 Test RE 0.10350200040163407 Lambda1 0.73822874\n",
      "53 Train Loss 34.69114 Test MSE 37.62399429744508 Test RE 0.10325750346919947 Lambda1 0.74394786\n",
      "54 Train Loss 34.55529 Test MSE 37.45173542018457 Test RE 0.10302085358853455 Lambda1 0.74833536\n",
      "55 Train Loss 34.359158 Test MSE 37.22131103599343 Test RE 0.10270344316076269 Lambda1 0.7529512\n",
      "56 Train Loss 34.265415 Test MSE 37.00461228487722 Test RE 0.10240404216557726 Lambda1 0.75783813\n",
      "57 Train Loss 34.13153 Test MSE 36.744939195478636 Test RE 0.10204410880689624 Lambda1 0.7659793\n",
      "58 Train Loss 33.948345 Test MSE 36.46837017425801 Test RE 0.10165935452555729 Lambda1 0.7711245\n",
      "59 Train Loss 33.795353 Test MSE 36.42141984997218 Test RE 0.10159389400689343 Lambda1 0.77608293\n",
      "60 Train Loss 33.676304 Test MSE 36.239083522138046 Test RE 0.10133927048692602 Lambda1 0.78436506\n",
      "61 Train Loss 33.61948 Test MSE 36.17764556332933 Test RE 0.10125333124114097 Lambda1 0.7882038\n",
      "62 Train Loss 33.512577 Test MSE 36.291062518603 Test RE 0.10141192169080418 Lambda1 0.78604364\n",
      "63 Train Loss 33.29418 Test MSE 36.14173272861279 Test RE 0.10120306267114051 Lambda1 0.7984613\n",
      "64 Train Loss 32.97991 Test MSE 35.80210326624049 Test RE 0.1007264298496453 Lambda1 0.8140143\n",
      "65 Train Loss 32.87937 Test MSE 35.587722238466704 Test RE 0.10042440487899576 Lambda1 0.8209321\n",
      "66 Train Loss 32.812515 Test MSE 35.61033351477018 Test RE 0.10045630300638499 Lambda1 0.81732416\n",
      "67 Train Loss 32.73581 Test MSE 35.63032951568554 Test RE 0.10048450328172416 Lambda1 0.8193352\n",
      "68 Train Loss 32.688812 Test MSE 35.644778474779656 Test RE 0.1005048756629554 Lambda1 0.8209401\n",
      "69 Train Loss 32.66054 Test MSE 35.520838221833436 Test RE 0.10032999104724874 Lambda1 0.8233732\n",
      "70 Train Loss 32.59813 Test MSE 35.55494292372067 Test RE 0.10037814452100297 Lambda1 0.8231853\n",
      "71 Train Loss 32.512676 Test MSE 35.53403232613292 Test RE 0.10034862294796247 Lambda1 0.82523686\n",
      "72 Train Loss 32.3995 Test MSE 35.70733394989892 Test RE 0.10059302846550079 Lambda1 0.82044786\n",
      "73 Train Loss 32.36431 Test MSE 35.666027444728165 Test RE 0.10053482825335679 Lambda1 0.8238302\n",
      "74 Train Loss 32.31303 Test MSE 35.67107121978139 Test RE 0.10054193665831647 Lambda1 0.82228\n",
      "75 Train Loss 32.249466 Test MSE 35.60232685599419 Test RE 0.10044500902965509 Lambda1 0.8264281\n",
      "76 Train Loss 32.2068 Test MSE 35.59446917025126 Test RE 0.1004339239554616 Lambda1 0.82730526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 Train Loss 32.17279 Test MSE 35.592806839684926 Test RE 0.10043157869881218 Lambda1 0.8278278\n",
      "78 Train Loss 32.11348 Test MSE 35.46175456121355 Test RE 0.10024651426965797 Lambda1 0.8332376\n",
      "79 Train Loss 32.03644 Test MSE 35.26618317146788 Test RE 0.09996970258899124 Lambda1 0.8402441\n",
      "80 Train Loss 31.858912 Test MSE 35.19350723594352 Test RE 0.09986664155479374 Lambda1 0.8439208\n",
      "81 Train Loss 31.787418 Test MSE 35.17080896239277 Test RE 0.09983443155156647 Lambda1 0.84793264\n",
      "82 Train Loss 31.736036 Test MSE 35.10541574972308 Test RE 0.09974157710999175 Lambda1 0.8552657\n",
      "83 Train Loss 31.689947 Test MSE 35.07698645099761 Test RE 0.09970118224006003 Lambda1 0.86178863\n",
      "84 Train Loss 31.635178 Test MSE 35.096392648611015 Test RE 0.09972875805980208 Lambda1 0.8637746\n",
      "85 Train Loss 31.579285 Test MSE 35.17899535805717 Test RE 0.0998460496608632 Lambda1 0.87132263\n",
      "86 Train Loss 31.456526 Test MSE 35.223950558500356 Test RE 0.09990982587237512 Lambda1 0.87108326\n",
      "87 Train Loss 31.346876 Test MSE 35.04775038512467 Test RE 0.0996596239670525 Lambda1 0.8783207\n",
      "88 Train Loss 31.251148 Test MSE 34.858740498362074 Test RE 0.09939053224346675 Lambda1 0.89048904\n",
      "89 Train Loss 31.148726 Test MSE 34.74830884038272 Test RE 0.09923297394217231 Lambda1 0.89781845\n",
      "90 Train Loss 31.090704 Test MSE 34.675195368328374 Test RE 0.09912852155357361 Lambda1 0.90310663\n",
      "91 Train Loss 31.049824 Test MSE 34.67361788314534 Test RE 0.09912626669169279 Lambda1 0.9082392\n",
      "92 Train Loss 31.028816 Test MSE 34.58817377865903 Test RE 0.09900405591702427 Lambda1 0.9101263\n",
      "93 Train Loss 31.013908 Test MSE 34.58599414779013 Test RE 0.09900093641586845 Lambda1 0.91068345\n",
      "94 Train Loss 30.950554 Test MSE 34.57501135535284 Test RE 0.09898521627980045 Lambda1 0.90422887\n",
      "95 Train Loss 30.927319 Test MSE 34.589313919352186 Test RE 0.09900568765426863 Lambda1 0.90414083\n",
      "96 Train Loss 30.908035 Test MSE 34.544636775439876 Test RE 0.0989417268509341 Lambda1 0.90677506\n",
      "97 Train Loss 30.891771 Test MSE 34.533010092096 Test RE 0.09892507504842947 Lambda1 0.90935224\n",
      "98 Train Loss 30.8827 Test MSE 34.55910160944217 Test RE 0.09896243955775522 Lambda1 0.9089909\n",
      "99 Train Loss 30.87062 Test MSE 34.57755220237413 Test RE 0.09898885332374602 Lambda1 0.90744793\n",
      "100 Train Loss 30.83431 Test MSE 34.59876613024337 Test RE 0.09901921435672162 Lambda1 0.90291375\n",
      "101 Train Loss 30.803839 Test MSE 34.686321159788996 Test RE 0.09914442333572694 Lambda1 0.8954016\n",
      "102 Train Loss 30.768518 Test MSE 34.712678249844075 Test RE 0.0991820846122818 Lambda1 0.8931207\n",
      "103 Train Loss 30.744743 Test MSE 34.651674055184984 Test RE 0.09909489479946412 Lambda1 0.8991069\n",
      "104 Train Loss 30.736353 Test MSE 34.626795824737215 Test RE 0.09905931573513611 Lambda1 0.9020941\n",
      "105 Train Loss 30.722881 Test MSE 34.58891713521652 Test RE 0.09900511979104139 Lambda1 0.90661263\n",
      "106 Train Loss 30.634031 Test MSE 34.54096393235351 Test RE 0.09893646688685417 Lambda1 0.9052876\n",
      "107 Train Loss 30.54104 Test MSE 34.49681809618744 Test RE 0.0988732227125399 Lambda1 0.9038425\n",
      "108 Train Loss 30.500908 Test MSE 34.37861057248833 Test RE 0.09870367681146495 Lambda1 0.90884423\n",
      "109 Train Loss 30.474823 Test MSE 34.32660392333976 Test RE 0.09862899097892157 Lambda1 0.9156293\n",
      "110 Train Loss 30.466036 Test MSE 34.319732145177554 Test RE 0.09861911830886393 Lambda1 0.9174657\n",
      "111 Train Loss 30.455109 Test MSE 34.307435317753 Test RE 0.09860144901410745 Lambda1 0.9146492\n",
      "112 Train Loss 30.446213 Test MSE 34.29334128290384 Test RE 0.09858119341933767 Lambda1 0.9145562\n",
      "113 Train Loss 30.42612 Test MSE 34.29114827477451 Test RE 0.09857804130865323 Lambda1 0.9163238\n",
      "114 Train Loss 30.39082 Test MSE 34.23701377726855 Test RE 0.0985001993465155 Lambda1 0.91418654\n",
      "115 Train Loss 30.357452 Test MSE 34.225035687485885 Test RE 0.09848296730268757 Lambda1 0.915602\n",
      "116 Train Loss 30.332851 Test MSE 34.26412946330892 Test RE 0.09853919766379501 Lambda1 0.91181904\n",
      "117 Train Loss 30.318466 Test MSE 34.26625408196906 Test RE 0.0985422526804563 Lambda1 0.9098091\n",
      "118 Train Loss 30.30778 Test MSE 34.25128930101428 Test RE 0.0985207326120274 Lambda1 0.9107157\n",
      "119 Train Loss 30.295916 Test MSE 34.204658600165814 Test RE 0.09845364527603108 Lambda1 0.91445965\n",
      "120 Train Loss 30.27647 Test MSE 34.16298021940683 Test RE 0.0983936441030929 Lambda1 0.92051864\n",
      "121 Train Loss 30.252308 Test MSE 34.133128227754604 Test RE 0.09835064597831542 Lambda1 0.9214421\n",
      "122 Train Loss 30.218494 Test MSE 34.10416065786166 Test RE 0.09830890377546495 Lambda1 0.92180705\n",
      "123 Train Loss 30.11019 Test MSE 33.84014818326648 Test RE 0.09792764232332957 Lambda1 0.92600596\n",
      "124 Train Loss 30.081026 Test MSE 33.85052574049859 Test RE 0.09794265661652121 Lambda1 0.9250278\n",
      "125 Train Loss 30.064297 Test MSE 33.79056620054848 Test RE 0.09785587510653698 Lambda1 0.92466104\n",
      "126 Train Loss 30.048748 Test MSE 33.80140422521515 Test RE 0.0978715670507599 Lambda1 0.92367905\n",
      "127 Train Loss 29.991756 Test MSE 33.67936429665667 Test RE 0.09769472469765877 Lambda1 0.928992\n",
      "128 Train Loss 29.94277 Test MSE 33.60280101996603 Test RE 0.09758361683969542 Lambda1 0.92912436\n",
      "129 Train Loss 29.918205 Test MSE 33.569853096819486 Test RE 0.09753576419160656 Lambda1 0.929428\n",
      "130 Train Loss 29.898186 Test MSE 33.5358672391218 Test RE 0.09748637946543082 Lambda1 0.930249\n",
      "131 Train Loss 29.847067 Test MSE 33.382532391212365 Test RE 0.09726325738459458 Lambda1 0.9300157\n",
      "132 Train Loss 29.827333 Test MSE 33.40232419333921 Test RE 0.09729208578324787 Lambda1 0.9303288\n",
      "133 Train Loss 29.815748 Test MSE 33.38779543566557 Test RE 0.09727092427856286 Lambda1 0.9323174\n",
      "134 Train Loss 29.793615 Test MSE 33.3359830440134 Test RE 0.09719542070114476 Lambda1 0.93648976\n",
      "135 Train Loss 29.758 Test MSE 33.27713159677845 Test RE 0.09710958825452298 Lambda1 0.944619\n",
      "136 Train Loss 29.707388 Test MSE 33.14626104804366 Test RE 0.09691844640676633 Lambda1 0.9499283\n",
      "137 Train Loss 29.634401 Test MSE 33.10371755696614 Test RE 0.0968562286356651 Lambda1 0.9473181\n",
      "138 Train Loss 29.60915 Test MSE 32.99351160432673 Test RE 0.096694871658318 Lambda1 0.94820195\n",
      "139 Train Loss 29.592243 Test MSE 32.989707302258815 Test RE 0.0966892968181932 Lambda1 0.94666207\n",
      "140 Train Loss 29.579582 Test MSE 32.9748619191674 Test RE 0.09666753925670907 Lambda1 0.9467847\n",
      "141 Train Loss 29.573685 Test MSE 32.99307612545341 Test RE 0.0966942335220502 Lambda1 0.9468649\n",
      "142 Train Loss 29.566614 Test MSE 32.97309810130113 Test RE 0.09666495385978818 Lambda1 0.94836706\n",
      "143 Train Loss 29.543348 Test MSE 32.95956931423435 Test RE 0.0966451211200227 Lambda1 0.9487644\n",
      "144 Train Loss 29.52017 Test MSE 32.89824737733742 Test RE 0.09655517417462114 Lambda1 0.95023054\n",
      "145 Train Loss 29.457588 Test MSE 32.83394499958934 Test RE 0.09646076543530542 Lambda1 0.9467463\n",
      "146 Train Loss 29.431835 Test MSE 32.777764251671556 Test RE 0.09637820517505603 Lambda1 0.94742423\n",
      "147 Train Loss 29.408094 Test MSE 32.70226246148372 Test RE 0.09626714021362569 Lambda1 0.9502705\n",
      "148 Train Loss 29.383484 Test MSE 32.68025201841399 Test RE 0.09623473818848842 Lambda1 0.95415145\n",
      "149 Train Loss 29.366188 Test MSE 32.662696423378506 Test RE 0.09620888641349049 Lambda1 0.9571206\n",
      "150 Train Loss 29.362959 Test MSE 32.64968982393818 Test RE 0.09618972885479246 Lambda1 0.9579548\n",
      "151 Train Loss 29.359098 Test MSE 32.6235820957427 Test RE 0.0961512629836256 Lambda1 0.95739657\n",
      "152 Train Loss 29.326683 Test MSE 32.43873964632409 Test RE 0.09587848359770332 Lambda1 0.96288687\n",
      "153 Train Loss 29.261477 Test MSE 32.316398468912844 Test RE 0.09569751221467272 Lambda1 0.965333\n",
      "154 Train Loss 29.19891 Test MSE 32.25139888647328 Test RE 0.09560122318529733 Lambda1 0.9623453\n",
      "155 Train Loss 29.163694 Test MSE 32.260624369999874 Test RE 0.09561489552940615 Lambda1 0.96056396\n",
      "156 Train Loss 29.132015 Test MSE 32.21339276685546 Test RE 0.09554487675196793 Lambda1 0.9586138\n",
      "157 Train Loss 29.100958 Test MSE 32.214050154009634 Test RE 0.09554585165167251 Lambda1 0.95864683\n",
      "158 Train Loss 29.05141 Test MSE 32.19748243909602 Test RE 0.09552127883317124 Lambda1 0.9609273\n",
      "159 Train Loss 29.009823 Test MSE 32.129100745239704 Test RE 0.09541978986277554 Lambda1 0.9617866\n",
      "160 Train Loss 28.983957 Test MSE 32.07283661026794 Test RE 0.09533620419932537 Lambda1 0.9616984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 Train Loss 28.96605 Test MSE 32.03054991365991 Test RE 0.09527333506657377 Lambda1 0.962617\n",
      "162 Train Loss 28.949707 Test MSE 31.99679028582795 Test RE 0.09522311363274295 Lambda1 0.9634791\n",
      "163 Train Loss 28.91404 Test MSE 31.918563630806887 Test RE 0.09510664032338677 Lambda1 0.9644097\n",
      "164 Train Loss 28.902227 Test MSE 31.908975576438912 Test RE 0.09509235465345876 Lambda1 0.965084\n",
      "165 Train Loss 28.896921 Test MSE 31.900559536596575 Test RE 0.0950798134513657 Lambda1 0.9655087\n",
      "166 Train Loss 28.888067 Test MSE 31.880033938938954 Test RE 0.0950492201945768 Lambda1 0.9662536\n",
      "167 Train Loss 28.877932 Test MSE 31.841345714359882 Test RE 0.09499152888361427 Lambda1 0.9670428\n",
      "168 Train Loss 28.849169 Test MSE 31.84540866062934 Test RE 0.09499758913575507 Lambda1 0.9640977\n",
      "169 Train Loss 28.780136 Test MSE 31.87950538931667 Test RE 0.09504843226508844 Lambda1 0.96136045\n",
      "170 Train Loss 28.742327 Test MSE 31.846603984571928 Test RE 0.09499937199601684 Lambda1 0.9584896\n",
      "171 Train Loss 28.727238 Test MSE 31.847936835049303 Test RE 0.09500135994163224 Lambda1 0.9572121\n",
      "172 Train Loss 28.71853 Test MSE 31.826863520739828 Test RE 0.0949699242349753 Lambda1 0.95675063\n",
      "173 Train Loss 28.709702 Test MSE 31.831325408792065 Test RE 0.09497658103795487 Lambda1 0.9560562\n",
      "174 Train Loss 28.688286 Test MSE 31.83092779135957 Test RE 0.09497598784144566 Lambda1 0.95689714\n",
      "175 Train Loss 28.657942 Test MSE 31.814656537573597 Test RE 0.09495170994491327 Lambda1 0.956147\n",
      "176 Train Loss 28.608171 Test MSE 31.822258454009088 Test RE 0.09496305333078944 Lambda1 0.9568393\n",
      "177 Train Loss 28.529207 Test MSE 31.732466306241864 Test RE 0.09482898113574466 Lambda1 0.95948714\n",
      "178 Train Loss 28.46236 Test MSE 31.648050731963664 Test RE 0.09470276381426332 Lambda1 0.96147656\n",
      "179 Train Loss 28.383957 Test MSE 31.643081703750255 Test RE 0.09469532893016783 Lambda1 0.966104\n",
      "180 Train Loss 28.354746 Test MSE 31.589710494392587 Test RE 0.09461543568253418 Lambda1 0.96718425\n",
      "181 Train Loss 28.340883 Test MSE 31.498241760968266 Test RE 0.09447835579583508 Lambda1 0.9681858\n",
      "182 Train Loss 28.322145 Test MSE 31.44151714826667 Test RE 0.09439324528077564 Lambda1 0.96944064\n",
      "183 Train Loss 28.296738 Test MSE 31.468926949199993 Test RE 0.09443438096155088 Lambda1 0.967262\n",
      "184 Train Loss 28.2831 Test MSE 31.449945648029946 Test RE 0.09440589638998247 Lambda1 0.9674648\n",
      "185 Train Loss 28.259525 Test MSE 31.42999012994971 Test RE 0.09437594057491187 Lambda1 0.9676393\n",
      "186 Train Loss 28.235283 Test MSE 31.35260570652451 Test RE 0.09425968650331205 Lambda1 0.9694426\n",
      "187 Train Loss 28.210966 Test MSE 31.32020516489742 Test RE 0.09421096879274893 Lambda1 0.97248507\n",
      "188 Train Loss 28.161655 Test MSE 31.217939033318014 Test RE 0.09405703511293033 Lambda1 0.9737633\n",
      "189 Train Loss 28.130264 Test MSE 31.162442537491444 Test RE 0.09397339477786266 Lambda1 0.97541887\n",
      "190 Train Loss 28.10484 Test MSE 31.074917746112483 Test RE 0.09384133219640847 Lambda1 0.97849053\n",
      "191 Train Loss 28.06584 Test MSE 31.06364895615552 Test RE 0.09382431567271317 Lambda1 0.9794556\n",
      "192 Train Loss 28.049887 Test MSE 31.080511345107055 Test RE 0.0938497777080969 Lambda1 0.9794691\n",
      "193 Train Loss 28.029737 Test MSE 30.99888962730268 Test RE 0.09372646545344397 Lambda1 0.9811557\n",
      "194 Train Loss 28.008347 Test MSE 30.95261435946751 Test RE 0.09365648170229285 Lambda1 0.98196113\n",
      "195 Train Loss 27.990574 Test MSE 30.93805016662604 Test RE 0.09363444492799458 Lambda1 0.9810937\n",
      "196 Train Loss 27.973152 Test MSE 30.941807806984993 Test RE 0.09364013103110247 Lambda1 0.980723\n",
      "197 Train Loss 27.956396 Test MSE 30.96973949615145 Test RE 0.09368238675588131 Lambda1 0.9797844\n",
      "198 Train Loss 27.93007 Test MSE 31.038072536768514 Test RE 0.09378568234598211 Lambda1 0.97778606\n",
      "199 Train Loss 27.913622 Test MSE 31.03264128334829 Test RE 0.09377747635811381 Lambda1 0.9771092\n",
      "Training time: 591.44\n",
      "Training time: 591.44\n",
      "inv_HT_rowdy\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 854.7689 Test MSE 858.1530574954384 Test RE 0.49314149531783485 Lambda1 -0.043821998\n",
      "1 Train Loss 854.0756 Test MSE 857.1231779415164 Test RE 0.49284549403497846 Lambda1 -0.042613655\n",
      "2 Train Loss 845.3194 Test MSE 829.4918158107323 Test RE 0.48483640612235995 Lambda1 0.012784877\n",
      "3 Train Loss 580.645 Test MSE 521.6604052844522 Test RE 0.38448828151616715 Lambda1 -0.01509947\n",
      "4 Train Loss 325.04962 Test MSE 301.8009207031639 Test RE 0.29244853544815247 Lambda1 -0.0036216276\n",
      "5 Train Loss 287.18988 Test MSE 286.23394548911944 Test RE 0.284806396618343 Lambda1 -0.00021981905\n",
      "6 Train Loss 274.7867 Test MSE 275.321970307322 Test RE 0.27932486984015853 Lambda1 0.010209344\n",
      "7 Train Loss 258.65674 Test MSE 262.5712359110719 Test RE 0.27278013803950857 Lambda1 0.033963073\n",
      "8 Train Loss 245.71373 Test MSE 249.5274341178504 Test RE 0.26591835837089406 Lambda1 0.062083352\n",
      "9 Train Loss 232.71422 Test MSE 232.9234524973105 Test RE 0.2569187379773758 Lambda1 0.1030491\n",
      "10 Train Loss 216.64795 Test MSE 213.52806703761956 Test RE 0.24598954825692992 Lambda1 0.16463785\n",
      "11 Train Loss 205.16293 Test MSE 201.65015538145568 Test RE 0.23904983657362644 Lambda1 0.19539644\n",
      "12 Train Loss 195.49178 Test MSE 194.87482047705615 Test RE 0.23499955218959184 Lambda1 0.21738859\n",
      "13 Train Loss 188.64902 Test MSE 184.42038693318318 Test RE 0.22860916378288434 Lambda1 0.24441876\n",
      "14 Train Loss 182.0783 Test MSE 174.64955803149226 Test RE 0.22247074855939453 Lambda1 0.26727512\n",
      "15 Train Loss 155.68011 Test MSE 149.28185726059837 Test RE 0.2056802944865052 Lambda1 0.34360868\n",
      "16 Train Loss 143.15799 Test MSE 130.24220125064446 Test RE 0.19211666299952948 Lambda1 0.39125082\n",
      "17 Train Loss 137.04463 Test MSE 123.04394979990272 Test RE 0.18673223811350084 Lambda1 0.3948665\n",
      "18 Train Loss 124.51859 Test MSE 114.08319490723225 Test RE 0.1798042729099724 Lambda1 0.4298765\n",
      "19 Train Loss 101.91105 Test MSE 95.3879676119523 Test RE 0.164412929793319 Lambda1 0.45958942\n",
      "20 Train Loss 90.78 Test MSE 82.81399184070196 Test RE 0.1531937439806287 Lambda1 0.48462418\n",
      "21 Train Loss 88.1888 Test MSE 81.18395570265098 Test RE 0.15167858723916627 Lambda1 0.48839477\n",
      "22 Train Loss 83.84716 Test MSE 75.45864746080098 Test RE 0.1462324239418127 Lambda1 0.4991126\n",
      "23 Train Loss 70.691185 Test MSE 62.81308544737097 Test RE 0.13341794102362176 Lambda1 0.54027474\n",
      "24 Train Loss 61.900917 Test MSE 55.695178160469 Test RE 0.12563133344982994 Lambda1 0.55473727\n",
      "25 Train Loss 59.49574 Test MSE 52.31736530759527 Test RE 0.12176209254633243 Lambda1 0.5644697\n",
      "26 Train Loss 58.222107 Test MSE 50.978269486172685 Test RE 0.1201937028983703 Lambda1 0.574683\n",
      "27 Train Loss 57.522358 Test MSE 49.510198725089 Test RE 0.11845039283823107 Lambda1 0.5876976\n",
      "28 Train Loss 56.41086 Test MSE 49.48462811558452 Test RE 0.11841980075823036 Lambda1 0.59143096\n",
      "29 Train Loss 54.72913 Test MSE 50.13633512776695 Test RE 0.11919703785643941 Lambda1 0.5876551\n",
      "30 Train Loss 51.700214 Test MSE 50.16791226685563 Test RE 0.1192345686110319 Lambda1 0.57753766\n",
      "31 Train Loss 50.641827 Test MSE 50.38248294993443 Test RE 0.11948928266780254 Lambda1 0.57585806\n",
      "32 Train Loss 50.163475 Test MSE 50.36607481425993 Test RE 0.1194698239601286 Lambda1 0.57784957\n",
      "33 Train Loss 49.79716 Test MSE 49.94268243059761 Test RE 0.1189666145471746 Lambda1 0.58241504\n",
      "34 Train Loss 48.532986 Test MSE 49.21138755393861 Test RE 0.11809240733728349 Lambda1 0.59460413\n",
      "35 Train Loss 47.784798 Test MSE 48.730212584139124 Test RE 0.11751365212771452 Lambda1 0.6044833\n",
      "36 Train Loss 47.383198 Test MSE 48.40691078340853 Test RE 0.11712317978444957 Lambda1 0.60952264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 Train Loss 46.864864 Test MSE 48.2262026238756 Test RE 0.116904358713472 Lambda1 0.61409926\n",
      "38 Train Loss 46.68573 Test MSE 48.155306619554175 Test RE 0.11681839818040797 Lambda1 0.61378485\n",
      "39 Train Loss 45.956253 Test MSE 48.05422918348602 Test RE 0.11669573353996485 Lambda1 0.6125826\n",
      "40 Train Loss 45.126442 Test MSE 47.572556847761334 Test RE 0.11610940980637706 Lambda1 0.61157775\n",
      "41 Train Loss 44.87105 Test MSE 47.388838870565095 Test RE 0.11588499449403508 Lambda1 0.61478657\n",
      "42 Train Loss 44.64882 Test MSE 47.214212270618845 Test RE 0.11567128088214963 Lambda1 0.61884546\n",
      "43 Train Loss 44.481453 Test MSE 47.02461410368072 Test RE 0.11543879660216402 Lambda1 0.61983263\n",
      "44 Train Loss 43.789295 Test MSE 46.86476558642455 Test RE 0.11524242679781761 Lambda1 0.6263951\n",
      "45 Train Loss 42.928047 Test MSE 46.76708337470814 Test RE 0.11512226181839054 Lambda1 0.6339362\n",
      "46 Train Loss 42.069736 Test MSE 46.48861452863785 Test RE 0.11477900941739459 Lambda1 0.63562447\n",
      "47 Train Loss 41.641983 Test MSE 45.972818694443475 Test RE 0.11414049106853819 Lambda1 0.635698\n",
      "48 Train Loss 40.739014 Test MSE 45.70467200485889 Test RE 0.11380712935631511 Lambda1 0.64149684\n",
      "49 Train Loss 40.336803 Test MSE 45.23161882624132 Test RE 0.11321663331708394 Lambda1 0.6477409\n",
      "50 Train Loss 39.981895 Test MSE 44.70275669499082 Test RE 0.11255280517584491 Lambda1 0.65880924\n",
      "51 Train Loss 39.909344 Test MSE 44.51329993095901 Test RE 0.11231404437820651 Lambda1 0.66269827\n",
      "52 Train Loss 39.792587 Test MSE 44.23461959009635 Test RE 0.11196191525063195 Lambda1 0.66916966\n",
      "53 Train Loss 39.747707 Test MSE 44.217973490303145 Test RE 0.11194084685893618 Lambda1 0.671265\n",
      "54 Train Loss 39.61557 Test MSE 44.298860029843276 Test RE 0.11204318500362136 Lambda1 0.66836727\n",
      "55 Train Loss 38.99748 Test MSE 43.30226472745961 Test RE 0.11077569310029212 Lambda1 0.6890766\n",
      "56 Train Loss 38.60516 Test MSE 42.39339537360106 Test RE 0.10960699503561025 Lambda1 0.70971\n",
      "57 Train Loss 38.108147 Test MSE 41.701720306143514 Test RE 0.10870916406906664 Lambda1 0.73173827\n",
      "58 Train Loss 37.853607 Test MSE 41.2824022522004 Test RE 0.10816123843695705 Lambda1 0.7476927\n",
      "59 Train Loss 37.55472 Test MSE 40.734608259716836 Test RE 0.10744122286023224 Lambda1 0.7666203\n",
      "60 Train Loss 37.376896 Test MSE 40.57341307479508 Test RE 0.10722842867578271 Lambda1 0.769682\n",
      "61 Train Loss 37.301582 Test MSE 40.53379688072237 Test RE 0.10717606655776328 Lambda1 0.7697783\n",
      "62 Train Loss 37.165325 Test MSE 40.33473068640988 Test RE 0.1069125655698878 Lambda1 0.77653223\n",
      "63 Train Loss 36.93819 Test MSE 40.24531981703421 Test RE 0.10679400213035596 Lambda1 0.7810464\n",
      "64 Train Loss 36.554256 Test MSE 40.20984948004979 Test RE 0.10674693014441658 Lambda1 0.7822742\n",
      "65 Train Loss 36.47473 Test MSE 40.171280004148244 Test RE 0.10669572178403028 Lambda1 0.78210187\n",
      "66 Train Loss 36.36439 Test MSE 40.09249419812581 Test RE 0.10659104209620761 Lambda1 0.78954774\n",
      "67 Train Loss 36.09615 Test MSE 39.8291139450321 Test RE 0.10624035009039491 Lambda1 0.8024273\n",
      "68 Train Loss 35.85515 Test MSE 39.75917147578421 Test RE 0.10614702668005908 Lambda1 0.81549895\n",
      "69 Train Loss 35.558388 Test MSE 39.53995961254048 Test RE 0.10585400184725642 Lambda1 0.8268847\n",
      "70 Train Loss 35.41393 Test MSE 39.49678285336677 Test RE 0.1057961909494377 Lambda1 0.8385025\n",
      "71 Train Loss 35.347176 Test MSE 39.39016608040903 Test RE 0.105653302466419 Lambda1 0.84340686\n",
      "72 Train Loss 35.288456 Test MSE 39.290517705885854 Test RE 0.10551957814094598 Lambda1 0.8502275\n",
      "73 Train Loss 35.160854 Test MSE 39.27807349242612 Test RE 0.10550286657514558 Lambda1 0.861627\n",
      "74 Train Loss 35.074646 Test MSE 39.246712591044876 Test RE 0.10546073969110713 Lambda1 0.8727346\n",
      "75 Train Loss 34.982838 Test MSE 39.25591975460999 Test RE 0.10547310935565368 Lambda1 0.87201995\n",
      "76 Train Loss 34.91561 Test MSE 39.26998207694444 Test RE 0.10549199904243349 Lambda1 0.8759578\n",
      "77 Train Loss 34.88115 Test MSE 39.23577266857695 Test RE 0.1054460402086453 Lambda1 0.8746697\n",
      "78 Train Loss 34.857414 Test MSE 39.200056754701635 Test RE 0.10539803606745675 Lambda1 0.8712719\n",
      "79 Train Loss 34.843033 Test MSE 39.193593442964634 Test RE 0.10538934668659591 Lambda1 0.8723771\n",
      "80 Train Loss 34.81627 Test MSE 39.16487346091052 Test RE 0.10535072641006527 Lambda1 0.87275714\n",
      "81 Train Loss 34.8097 Test MSE 39.200624021033754 Test RE 0.10539879867529438 Lambda1 0.87265736\n",
      "82 Train Loss 34.79091 Test MSE 39.21603122216992 Test RE 0.10541950932723339 Lambda1 0.8706218\n",
      "83 Train Loss 34.776947 Test MSE 39.21863410526125 Test RE 0.10542300777030411 Lambda1 0.867855\n",
      "84 Train Loss 34.7657 Test MSE 39.24724616752559 Test RE 0.1054614565814575 Lambda1 0.86753684\n",
      "85 Train Loss 34.75644 Test MSE 39.23386302564948 Test RE 0.1054434740971426 Lambda1 0.8677561\n",
      "86 Train Loss 34.742508 Test MSE 39.2446781413986 Test RE 0.10545800624757823 Lambda1 0.8699432\n",
      "87 Train Loss 34.73978 Test MSE 39.24737662286276 Test RE 0.1054616318548816 Lambda1 0.87108\n",
      "88 Train Loss 34.71619 Test MSE 39.1837743377792 Test RE 0.10537614435158081 Lambda1 0.8696068\n",
      "89 Train Loss 34.696312 Test MSE 39.16717086046457 Test RE 0.10535381628552253 Lambda1 0.8667299\n",
      "90 Train Loss 34.676407 Test MSE 39.09008471625603 Test RE 0.1052500901394929 Lambda1 0.8650897\n",
      "91 Train Loss 34.661095 Test MSE 39.04578673374988 Test RE 0.10519043705462736 Lambda1 0.86407954\n",
      "92 Train Loss 34.585846 Test MSE 38.95259992516901 Test RE 0.10506483814217674 Lambda1 0.8672097\n",
      "93 Train Loss 34.52989 Test MSE 38.90879858722913 Test RE 0.10500574998215494 Lambda1 0.87146777\n",
      "94 Train Loss 34.46545 Test MSE 38.80833573672081 Test RE 0.10487009949090541 Lambda1 0.8702569\n",
      "95 Train Loss 34.350403 Test MSE 38.5788669012025 Test RE 0.10455959792999983 Lambda1 0.8661196\n",
      "96 Train Loss 34.25858 Test MSE 38.49220763053192 Test RE 0.1044420963885117 Lambda1 0.86190784\n",
      "97 Train Loss 34.238865 Test MSE 38.4163024191575 Test RE 0.10433906759344072 Lambda1 0.86026585\n",
      "98 Train Loss 34.23128 Test MSE 38.39003569458754 Test RE 0.10430339114868231 Lambda1 0.861396\n",
      "99 Train Loss 34.1937 Test MSE 38.36057671212354 Test RE 0.10426336433699929 Lambda1 0.8609478\n",
      "100 Train Loss 34.115902 Test MSE 38.278317843215554 Test RE 0.10415151527329972 Lambda1 0.8636863\n",
      "101 Train Loss 34.057465 Test MSE 38.11807064775943 Test RE 0.10393327824720798 Lambda1 0.8680128\n",
      "102 Train Loss 34.005997 Test MSE 38.03965952714719 Test RE 0.10382632477334117 Lambda1 0.86926645\n",
      "103 Train Loss 33.991257 Test MSE 38.055214035049055 Test RE 0.10384755002025343 Lambda1 0.87112224\n",
      "104 Train Loss 33.969162 Test MSE 38.04423670966849 Test RE 0.10383257111926024 Lambda1 0.87498003\n",
      "105 Train Loss 33.9445 Test MSE 38.03565851784539 Test RE 0.1038208644061858 Lambda1 0.8784381\n",
      "106 Train Loss 33.915848 Test MSE 37.988538735496554 Test RE 0.10375653618419148 Lambda1 0.88147235\n",
      "107 Train Loss 33.89726 Test MSE 38.00022373441715 Test RE 0.1037724923361919 Lambda1 0.8848927\n",
      "108 Train Loss 33.8479 Test MSE 37.9552042729152 Test RE 0.10371100366891618 Lambda1 0.8943443\n",
      "109 Train Loss 33.818535 Test MSE 37.94610685665806 Test RE 0.10369857376987775 Lambda1 0.9012806\n",
      "110 Train Loss 33.812256 Test MSE 37.922265071477014 Test RE 0.10366599140773171 Lambda1 0.9047354\n",
      "111 Train Loss 33.799313 Test MSE 37.899112211664935 Test RE 0.10363434073207205 Lambda1 0.9037123\n",
      "112 Train Loss 33.79684 Test MSE 37.884496964514625 Test RE 0.10361435625924736 Lambda1 0.9056687\n",
      "113 Train Loss 33.77407 Test MSE 37.8303428382878 Test RE 0.10354027382408706 Lambda1 0.90560734\n",
      "114 Train Loss 33.750793 Test MSE 37.73587716484029 Test RE 0.10341091846274145 Lambda1 0.9023384\n",
      "115 Train Loss 33.70111 Test MSE 37.68473063268589 Test RE 0.10334081404659207 Lambda1 0.9003962\n",
      "116 Train Loss 33.687637 Test MSE 37.64366373372862 Test RE 0.10328449092154068 Lambda1 0.89965504\n",
      "117 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "118 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "119 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "120 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "121 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "123 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "124 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "125 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "126 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "127 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "128 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "129 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "130 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "131 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "132 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "133 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "134 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "135 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "136 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "137 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "138 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "139 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "140 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "141 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "142 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "143 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "144 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "145 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "146 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "147 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "148 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "149 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "150 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "151 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "152 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "153 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "154 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "155 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "156 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "157 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "158 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "159 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "160 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "161 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "162 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "163 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "164 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "165 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "166 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "167 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "168 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "169 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "170 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "171 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "172 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "173 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "174 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "175 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "176 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "177 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "178 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "179 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "180 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "181 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "182 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "183 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "184 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "185 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "186 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "187 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "188 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "189 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "190 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "191 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "192 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "193 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "194 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "195 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "196 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "197 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "198 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "199 Train Loss 33.68508 Test MSE 37.62874959211265 Test RE 0.10326402861825379 Lambda1 0.8994143\n",
      "Training time: 414.09\n",
      "Training time: 414.09\n",
      "inv_HT_rowdy\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 1909.5084 Test MSE 1860.7207819308105 Test RE 0.7261557401212027 Lambda1 0.0002740399\n",
      "1 Train Loss 856.5955 Test MSE 858.2587504578389 Test RE 0.49317186284976855 Lambda1 0.0001602922\n",
      "2 Train Loss 854.73004 Test MSE 858.0610427929364 Test RE 0.49311505627877905 Lambda1 -0.00036317157\n",
      "3 Train Loss 854.68414 Test MSE 858.0036049727948 Test RE 0.4930985516675093 Lambda1 -0.0012307863\n",
      "4 Train Loss 854.4585 Test MSE 857.6335007224324 Test RE 0.4929921899032024 Lambda1 0.00063171046\n",
      "5 Train Loss 853.72705 Test MSE 856.2157389329603 Test RE 0.4925845364089858 Lambda1 0.0010973121\n",
      "6 Train Loss 847.7902 Test MSE 846.794658980369 Test RE 0.4898670469143873 Lambda1 0.0038172875\n",
      "7 Train Loss 756.1244 Test MSE 703.0606024839876 Test RE 0.4463602905717115 Lambda1 0.002354738\n",
      "8 Train Loss 519.6793 Test MSE 501.5640439075054 Test RE 0.37700956461681084 Lambda1 0.0035970379\n",
      "9 Train Loss 349.55774 Test MSE 322.9859263916686 Test RE 0.3025387235014539 Lambda1 0.00093276135\n",
      "10 Train Loss 282.4797 Test MSE 285.16657364635006 Test RE 0.2842748764003785 Lambda1 0.0010035229\n",
      "11 Train Loss 273.1002 Test MSE 279.2182776020965 Test RE 0.28129440400294 Lambda1 0.00457719\n",
      "12 Train Loss 268.1786 Test MSE 273.9527970110267 Test RE 0.27862946436453356 Lambda1 0.010551524\n",
      "13 Train Loss 258.42667 Test MSE 263.08669342889425 Test RE 0.27304775614571947 Lambda1 0.031109324\n",
      "14 Train Loss 247.3363 Test MSE 251.71154616502184 Test RE 0.2670796136438383 Lambda1 0.057815325\n",
      "15 Train Loss 229.62538 Test MSE 228.9647608243094 Test RE 0.25472612793815114 Lambda1 0.114825495\n",
      "16 Train Loss 209.92938 Test MSE 207.95523787552654 Test RE 0.24275830834022433 Lambda1 0.16610026\n",
      "17 Train Loss 164.83925 Test MSE 156.63976682535863 Test RE 0.21068818633315173 Lambda1 0.2686601\n",
      "18 Train Loss 138.62817 Test MSE 131.1705291498798 Test RE 0.19280012267085064 Lambda1 0.30885708\n",
      "19 Train Loss 103.71066 Test MSE 95.2718987292637 Test RE 0.1643128698264783 Lambda1 0.3621084\n",
      "20 Train Loss 81.093735 Test MSE 78.43476265072145 Test RE 0.14908826611838716 Lambda1 0.39411193\n",
      "21 Train Loss 65.97155 Test MSE 65.19690312518634 Test RE 0.13592603674717826 Lambda1 0.44519657\n",
      "22 Train Loss 56.096596 Test MSE 58.740223654752235 Test RE 0.12901997949022725 Lambda1 0.47650528\n",
      "23 Train Loss 51.75396 Test MSE 55.0297943388798 Test RE 0.12487862710002358 Lambda1 0.51038927\n",
      "24 Train Loss 48.574104 Test MSE 53.040613395674576 Test RE 0.12260083825323694 Lambda1 0.5411609\n",
      "25 Train Loss 45.96723 Test MSE 49.609725403475856 Test RE 0.11856938908323414 Lambda1 0.5801826\n",
      "26 Train Loss 42.817986 Test MSE 46.54354321075631 Test RE 0.11484679804226298 Lambda1 0.635405\n",
      "27 Train Loss 41.684856 Test MSE 45.14427096735554 Test RE 0.11310726281791741 Lambda1 0.6632373\n",
      "28 Train Loss 39.304394 Test MSE 42.7676141771457 Test RE 0.11008969851176169 Lambda1 0.6858474\n",
      "29 Train Loss 38.344807 Test MSE 41.800987435238376 Test RE 0.10883847327121113 Lambda1 0.70602506\n",
      "30 Train Loss 37.764465 Test MSE 41.375832121184395 Test RE 0.10828356391449942 Lambda1 0.7182564\n",
      "31 Train Loss 36.557667 Test MSE 40.833342673420276 Test RE 0.10757135453757388 Lambda1 0.73305947\n",
      "32 Train Loss 36.19946 Test MSE 40.28275625151278 Test RE 0.10684366079099145 Lambda1 0.74191624\n",
      "33 Train Loss 35.396587 Test MSE 39.678596890161785 Test RE 0.1060394151536117 Lambda1 0.7566056\n",
      "34 Train Loss 35.30189 Test MSE 39.40366842085366 Test RE 0.1056714090748777 Lambda1 0.7622594\n",
      "35 Train Loss 35.118458 Test MSE 39.03166973708251 Test RE 0.10517141954440706 Lambda1 0.76879996\n",
      "36 Train Loss 34.78019 Test MSE 38.372787599224274 Test RE 0.1042799575034633 Lambda1 0.77769196\n",
      "37 Train Loss 33.837357 Test MSE 37.49795858665831 Test RE 0.10308440847606039 Lambda1 0.7896494\n",
      "38 Train Loss 33.343266 Test MSE 36.93356492226865 Test RE 0.10230568911973029 Lambda1 0.8003754\n",
      "39 Train Loss 33.05843 Test MSE 36.74866401110246 Test RE 0.10204928075529365 Lambda1 0.8039494\n",
      "40 Train Loss 32.717773 Test MSE 36.333524555936556 Test RE 0.10147123240875103 Lambda1 0.8094626\n",
      "41 Train Loss 32.576252 Test MSE 36.09044264712283 Test RE 0.10113122665456013 Lambda1 0.8141916\n",
      "42 Train Loss 32.4127 Test MSE 36.116173180281415 Test RE 0.10116727077809179 Lambda1 0.81592226\n",
      "43 Train Loss 31.38125 Test MSE 35.1225938846029 Test RE 0.09976597740091585 Lambda1 0.85162175\n",
      "44 Train Loss 31.141916 Test MSE 34.93279257469088 Test RE 0.09949604624773034 Lambda1 0.8564828\n",
      "45 Train Loss 31.05317 Test MSE 34.72613680137671 Test RE 0.09920130983351057 Lambda1 0.8628783\n",
      "46 Train Loss 30.898243 Test MSE 34.52045626439449 Test RE 0.09890709223675126 Lambda1 0.8694585\n",
      "47 Train Loss 30.773478 Test MSE 34.371409969793326 Test RE 0.09869333952370492 Lambda1 0.87859505\n",
      "48 Train Loss 30.679987 Test MSE 34.39890562496147 Test RE 0.09873280685355472 Lambda1 0.88428426\n",
      "49 Train Loss 30.451788 Test MSE 34.18958330569395 Test RE 0.09843194675119608 Lambda1 0.8921171\n",
      "50 Train Loss 30.30061 Test MSE 33.915372899690006 Test RE 0.09803642567966538 Lambda1 0.9031811\n",
      "51 Train Loss 30.079119 Test MSE 33.38048585101787 Test RE 0.09726027594187149 Lambda1 0.92966276\n",
      "52 Train Loss 29.982052 Test MSE 33.154572502811966 Test RE 0.09693059683893175 Lambda1 0.94579744\n",
      "53 Train Loss 29.89795 Test MSE 32.819785270530105 Test RE 0.09643996370720474 Lambda1 0.96637106\n",
      "54 Train Loss 29.834032 Test MSE 32.555922721852504 Test RE 0.09605150522620931 Lambda1 0.9826008\n",
      "55 Train Loss 29.772373 Test MSE 32.393282391486025 Test RE 0.09581128153227009 Lambda1 0.99556094\n",
      "56 Train Loss 29.71995 Test MSE 32.572167720003854 Test RE 0.09607546648722343 Lambda1 0.98512155\n",
      "57 Train Loss 29.634317 Test MSE 32.57252857773602 Test RE 0.09607599868205963 Lambda1 0.9857591\n",
      "58 Train Loss 29.54236 Test MSE 32.51240141304987 Test RE 0.09598728211602575 Lambda1 0.99445736\n",
      "59 Train Loss 29.47183 Test MSE 32.490661404520154 Test RE 0.09595518492855042 Lambda1 1.0045463\n",
      "60 Train Loss 29.412508 Test MSE 32.474260200035644 Test RE 0.09593096290287297 Lambda1 1.0114592\n",
      "61 Train Loss 29.380262 Test MSE 32.48444150173283 Test RE 0.09594599982023003 Lambda1 1.0066407\n",
      "62 Train Loss 29.319254 Test MSE 32.50560205363255 Test RE 0.09597724462066573 Lambda1 1.0137717\n",
      "63 Train Loss 29.238876 Test MSE 32.534772250563094 Test RE 0.09602029946554266 Lambda1 1.0239931\n",
      "64 Train Loss 29.208065 Test MSE 32.56630143613143 Test RE 0.09606681444810773 Lambda1 1.0319396\n",
      "65 Train Loss 29.145563 Test MSE 32.44976542564513 Test RE 0.09589477654249305 Lambda1 1.0494767\n",
      "66 Train Loss 29.07394 Test MSE 32.43056540516924 Test RE 0.09586640262238574 Lambda1 1.0727298\n",
      "67 Train Loss 29.019543 Test MSE 32.35268893588776 Test RE 0.09575123003476108 Lambda1 1.076686\n",
      "68 Train Loss 29.002722 Test MSE 32.35418599668327 Test RE 0.09575344536460072 Lambda1 1.0801669\n",
      "69 Train Loss 28.955227 Test MSE 32.29881513775666 Test RE 0.09567147419564016 Lambda1 1.0829958\n",
      "70 Train Loss 28.926088 Test MSE 32.30004582594957 Test RE 0.09567329687290595 Lambda1 1.0805582\n",
      "71 Train Loss 28.776445 Test MSE 31.960880586271347 Test RE 0.09516966462568409 Lambda1 1.0718006\n",
      "72 Train Loss 28.68094 Test MSE 31.855615437739225 Test RE 0.09501281176247438 Lambda1 1.0753243\n",
      "73 Train Loss 28.64854 Test MSE 31.78131889919173 Test RE 0.09490194836017934 Lambda1 1.0718784\n",
      "74 Train Loss 28.621454 Test MSE 31.759134100766772 Test RE 0.09486881965110652 Lambda1 1.073986\n",
      "75 Train Loss 28.497782 Test MSE 31.480514960974116 Test RE 0.09445176646055149 Lambda1 1.0671325\n",
      "76 Train Loss 28.30606 Test MSE 31.317014845010252 Test RE 0.09420617044005485 Lambda1 1.0529493\n",
      "77 Train Loss 28.245266 Test MSE 31.235805397401315 Test RE 0.0940839461922451 Lambda1 1.0440238\n",
      "78 Train Loss 28.227161 Test MSE 31.163375026832476 Test RE 0.09397480077377154 Lambda1 1.0385473\n",
      "79 Train Loss 28.180044 Test MSE 31.091343405616254 Test RE 0.0938661303655685 Lambda1 1.0324407\n",
      "80 Train Loss 28.150843 Test MSE 30.9705247299033 Test RE 0.09368357440077185 Lambda1 1.0317236\n",
      "81 Train Loss 28.017914 Test MSE 30.82889543282196 Test RE 0.09346911980507061 Lambda1 1.028829\n",
      "82 Train Loss 27.927052 Test MSE 30.75415862539508 Test RE 0.09335575501627677 Lambda1 1.0312766\n",
      "83 Train Loss 27.843557 Test MSE 30.64122454231727 Test RE 0.09318418889634587 Lambda1 1.0348898\n",
      "84 Train Loss 27.701479 Test MSE 30.499171909891924 Test RE 0.09296793714826392 Lambda1 1.0311995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 Train Loss 27.559303 Test MSE 30.283460313988908 Test RE 0.0926385864438354 Lambda1 1.0293365\n",
      "86 Train Loss 27.520254 Test MSE 30.187186221815438 Test RE 0.0924912156453931 Lambda1 1.03059\n",
      "87 Train Loss 27.49524 Test MSE 30.18387861956219 Test RE 0.09248614838719464 Lambda1 1.0300671\n",
      "88 Train Loss 27.2458 Test MSE 30.102114066607157 Test RE 0.09236079642660767 Lambda1 1.012962\n",
      "89 Train Loss 27.165537 Test MSE 29.9924082474831 Test RE 0.0921923403915516 Lambda1 1.0045074\n",
      "90 Train Loss 27.1321 Test MSE 29.913293390485812 Test RE 0.09207066626555484 Lambda1 0.99887943\n",
      "91 Train Loss 27.110874 Test MSE 29.882775622941924 Test RE 0.09202368868668462 Lambda1 1.0001676\n",
      "92 Train Loss 27.074009 Test MSE 29.86709187280811 Test RE 0.09199953654665437 Lambda1 0.99735296\n",
      "93 Train Loss 26.997908 Test MSE 29.871102508979348 Test RE 0.09200571331613379 Lambda1 0.9965519\n",
      "94 Train Loss 26.983456 Test MSE 29.879182845239562 Test RE 0.09201815656009897 Lambda1 0.99825716\n",
      "95 Train Loss 26.95755 Test MSE 29.796413889764622 Test RE 0.09189061745465729 Lambda1 0.9997583\n",
      "96 Train Loss 26.934414 Test MSE 29.811518695563404 Test RE 0.0919139057281451 Lambda1 0.99724036\n",
      "97 Train Loss 26.859137 Test MSE 29.78839632013212 Test RE 0.09187825373542641 Lambda1 0.994572\n",
      "98 Train Loss 26.733099 Test MSE 29.529863957936083 Test RE 0.09147868093658552 Lambda1 0.98715025\n",
      "99 Train Loss 26.581247 Test MSE 29.415493600206187 Test RE 0.09130135876002404 Lambda1 0.97607374\n",
      "100 Train Loss 26.569708 Test MSE 29.39175947729661 Test RE 0.09126451771660611 Lambda1 0.976547\n",
      "101 Train Loss 26.504665 Test MSE 29.277732111792297 Test RE 0.0910873121722086 Lambda1 0.9801841\n",
      "102 Train Loss 26.462051 Test MSE 29.24454853892201 Test RE 0.0910356780600696 Lambda1 0.98051655\n",
      "103 Train Loss 26.402714 Test MSE 29.279012975584767 Test RE 0.09108930462783411 Lambda1 0.9847925\n",
      "104 Train Loss 26.344284 Test MSE 29.273364158538858 Test RE 0.09108051724764714 Lambda1 0.9905835\n",
      "105 Train Loss 26.2937 Test MSE 29.31174237252223 Test RE 0.09114020226769849 Lambda1 0.99204516\n",
      "106 Train Loss 26.227379 Test MSE 29.307360997539817 Test RE 0.09113339041870941 Lambda1 0.98727435\n",
      "107 Train Loss 26.164047 Test MSE 29.270209415095678 Test RE 0.09107560931487804 Lambda1 0.98398954\n",
      "108 Train Loss 26.122345 Test MSE 29.256095757388348 Test RE 0.09105364901821829 Lambda1 0.98361474\n",
      "109 Train Loss 26.09457 Test MSE 29.19162458680999 Test RE 0.09095326698083456 Lambda1 0.9796648\n",
      "110 Train Loss 26.081694 Test MSE 29.205459350399867 Test RE 0.09097481713282585 Lambda1 0.9777902\n",
      "111 Train Loss 26.07247 Test MSE 29.18005727393667 Test RE 0.09093524487423871 Lambda1 0.97496367\n",
      "112 Train Loss 26.06599 Test MSE 29.166577426598217 Test RE 0.09091423849298247 Lambda1 0.97446615\n",
      "113 Train Loss 26.042612 Test MSE 29.177434196524857 Test RE 0.09093115756993175 Lambda1 0.9726461\n",
      "114 Train Loss 25.98755 Test MSE 29.126977063772095 Test RE 0.09085249899495722 Lambda1 0.96932924\n",
      "115 Train Loss 25.944187 Test MSE 29.087708383162028 Test RE 0.09079123515287245 Lambda1 0.9657595\n",
      "116 Train Loss 25.930876 Test MSE 29.10610425188177 Test RE 0.09081994005524396 Lambda1 0.9647076\n",
      "117 Train Loss 25.92399 Test MSE 29.079058314935654 Test RE 0.09077773445416826 Lambda1 0.9641903\n",
      "118 Train Loss 25.91621 Test MSE 29.093172823506418 Test RE 0.09079976280969934 Lambda1 0.9636986\n",
      "119 Train Loss 25.90416 Test MSE 29.108455624375107 Test RE 0.09082360848153373 Lambda1 0.96423805\n",
      "120 Train Loss 25.898314 Test MSE 29.113291090042505 Test RE 0.09083115192885684 Lambda1 0.96445256\n",
      "121 Train Loss 25.887747 Test MSE 29.130135246319615 Test RE 0.09085742434300713 Lambda1 0.96624786\n",
      "122 Train Loss 25.883394 Test MSE 29.138017944654415 Test RE 0.09086971665158229 Lambda1 0.96606547\n",
      "123 Train Loss 25.878561 Test MSE 29.128587585448887 Test RE 0.09085501071967439 Lambda1 0.9648422\n",
      "124 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "125 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "126 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "127 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "128 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "129 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "130 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "131 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "132 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "133 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "134 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "135 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "136 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "137 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "138 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "139 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "140 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "141 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "142 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "143 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "144 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "145 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "146 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "147 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "148 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "149 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "150 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "151 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "152 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "153 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "154 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "155 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "156 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "157 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "158 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "159 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "160 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "161 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "162 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "163 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "164 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "165 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "166 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "167 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "168 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "169 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "171 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "172 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "173 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "174 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "175 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "176 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "177 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "178 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "179 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "180 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "181 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "182 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "183 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "184 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "185 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "186 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "187 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "188 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "189 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "190 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "191 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "192 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "193 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "194 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "195 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "196 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "197 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "198 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "199 Train Loss 25.87831 Test MSE 29.12717106632252 Test RE 0.09085280155960594 Lambda1 0.9647828\n",
      "Training time: 426.42\n",
      "Training time: 426.42\n",
      "inv_HT_rowdy\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 1118.1648 Test MSE 1074.804394841741 Test RE 0.5518917314419024 Lambda1 -0.009919163\n",
      "1 Train Loss 736.2415 Test MSE 714.8409555433973 Test RE 0.4500843207085879 Lambda1 -0.014960248\n",
      "2 Train Loss 617.17474 Test MSE 605.4729133054586 Test RE 0.41422521324976247 Lambda1 -0.01686439\n",
      "3 Train Loss 501.97028 Test MSE 481.9260689113493 Test RE 0.3695552533381684 Lambda1 -0.014008542\n",
      "4 Train Loss 353.70447 Test MSE 350.8993152712601 Test RE 0.31534099310092567 Lambda1 -0.0029781093\n",
      "5 Train Loss 301.8293 Test MSE 303.6459433235981 Test RE 0.293341097369594 Lambda1 0.012188473\n",
      "6 Train Loss 274.0709 Test MSE 272.1301792467281 Test RE 0.2777010516970898 Lambda1 0.030327642\n",
      "7 Train Loss 260.51672 Test MSE 263.26175938294676 Test RE 0.27313858821295484 Lambda1 0.03923112\n",
      "8 Train Loss 246.31625 Test MSE 247.7597097672796 Test RE 0.26497476303117823 Lambda1 0.06628968\n",
      "9 Train Loss 238.24792 Test MSE 241.3468425388695 Test RE 0.2615230553123068 Lambda1 0.0846094\n",
      "10 Train Loss 228.17618 Test MSE 230.8630478057891 Test RE 0.2557798822172617 Lambda1 0.11525645\n",
      "11 Train Loss 219.29231 Test MSE 221.53790764584053 Test RE 0.2505608394809469 Lambda1 0.14279409\n",
      "12 Train Loss 212.16913 Test MSE 215.70011390488403 Test RE 0.2472375081868424 Lambda1 0.15153909\n",
      "13 Train Loss 202.09901 Test MSE 201.46177669996501 Test RE 0.23893815202051646 Lambda1 0.17824854\n",
      "14 Train Loss 195.6198 Test MSE 195.77860783640384 Test RE 0.2355438604262305 Lambda1 0.19833332\n",
      "15 Train Loss 185.67883 Test MSE 177.2825534903779 Test RE 0.22414144625646878 Lambda1 0.21039306\n",
      "16 Train Loss 175.5525 Test MSE 169.50585995129993 Test RE 0.21917021263834302 Lambda1 0.2261695\n",
      "17 Train Loss 163.42569 Test MSE 159.0457996642326 Test RE 0.21230013613767132 Lambda1 0.24996407\n",
      "18 Train Loss 150.51016 Test MSE 140.38297803644414 Test RE 0.19945567437915926 Lambda1 0.26804888\n",
      "19 Train Loss 139.21059 Test MSE 127.37313121311385 Test RE 0.18998883655061125 Lambda1 0.28914502\n",
      "20 Train Loss 127.68261 Test MSE 117.04284947797791 Test RE 0.18212166573607702 Lambda1 0.3160631\n",
      "21 Train Loss 113.81075 Test MSE 106.25545214057229 Test RE 0.17352609054529455 Lambda1 0.3328706\n",
      "22 Train Loss 99.408035 Test MSE 93.50262417005865 Test RE 0.16278000985378244 Lambda1 0.36582133\n",
      "23 Train Loss 94.606476 Test MSE 86.05693751546167 Test RE 0.15616442774261455 Lambda1 0.38070527\n",
      "24 Train Loss 89.8707 Test MSE 81.62867450720576 Test RE 0.15209346105449098 Lambda1 0.39224514\n",
      "25 Train Loss 84.45618 Test MSE 77.65998767803094 Test RE 0.14835009517694253 Lambda1 0.39755738\n",
      "26 Train Loss 80.89513 Test MSE 71.31004249652506 Test RE 0.14215577856913714 Lambda1 0.4168949\n",
      "27 Train Loss 77.19342 Test MSE 68.37732262652388 Test RE 0.13920191709536897 Lambda1 0.43395072\n",
      "28 Train Loss 71.94639 Test MSE 68.22549081554703 Test RE 0.13904728231211547 Lambda1 0.44625744\n",
      "29 Train Loss 68.04533 Test MSE 64.90297400163455 Test RE 0.13561929094623915 Lambda1 0.45237198\n",
      "30 Train Loss 65.72365 Test MSE 63.31682130953882 Test RE 0.13395185202756235 Lambda1 0.45827276\n",
      "31 Train Loss 62.95602 Test MSE 59.68705373590477 Test RE 0.13005565526420274 Lambda1 0.4731481\n",
      "32 Train Loss 61.23655 Test MSE 58.82478962588324 Test RE 0.12911281856268014 Lambda1 0.4781391\n",
      "33 Train Loss 57.237827 Test MSE 56.361507635488486 Test RE 0.12638061705534664 Lambda1 0.48598477\n",
      "34 Train Loss 55.898766 Test MSE 56.114118452692196 Test RE 0.12610294899165075 Lambda1 0.48960564\n",
      "35 Train Loss 53.554363 Test MSE 54.61389912080793 Test RE 0.12440583843880683 Lambda1 0.50125664\n",
      "36 Train Loss 51.74904 Test MSE 53.28348678401045 Test RE 0.1228812127646605 Lambda1 0.5051698\n",
      "37 Train Loss 49.683407 Test MSE 52.58422918869742 Test RE 0.12207224361164501 Lambda1 0.511335\n",
      "38 Train Loss 48.715805 Test MSE 51.88218454260862 Test RE 0.12125462077449857 Lambda1 0.51161456\n",
      "39 Train Loss 48.166454 Test MSE 51.59952072171743 Test RE 0.1209238607505805 Lambda1 0.51524585\n",
      "40 Train Loss 47.285564 Test MSE 50.788933336843186 Test RE 0.1199702921917499 Lambda1 0.5264988\n",
      "41 Train Loss 46.557926 Test MSE 50.01726719879961 Test RE 0.11905541421333488 Lambda1 0.53373414\n",
      "42 Train Loss 45.261078 Test MSE 48.58414399622498 Test RE 0.11733739663166139 Lambda1 0.5476438\n",
      "43 Train Loss 44.470207 Test MSE 47.741446769416775 Test RE 0.11631533037146605 Lambda1 0.55722785\n",
      "44 Train Loss 44.174095 Test MSE 47.01529607904314 Test RE 0.11542735881726354 Lambda1 0.56636584\n",
      "45 Train Loss 43.19701 Test MSE 46.29441315084001 Test RE 0.11453901980335877 Lambda1 0.5742765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 Train Loss 43.0236 Test MSE 46.24306763964868 Test RE 0.11447548410097198 Lambda1 0.5752104\n",
      "47 Train Loss 42.870182 Test MSE 46.22667400815118 Test RE 0.11445519094838041 Lambda1 0.5767898\n",
      "48 Train Loss 42.33011 Test MSE 45.40124696790728 Test RE 0.11342872781921245 Lambda1 0.59170353\n",
      "49 Train Loss 42.12046 Test MSE 45.01563619721903 Test RE 0.11294600308723077 Lambda1 0.59856254\n",
      "50 Train Loss 41.450275 Test MSE 44.254800083535436 Test RE 0.11198745168654485 Lambda1 0.60945433\n",
      "51 Train Loss 40.882698 Test MSE 44.18631269664648 Test RE 0.11190076394223454 Lambda1 0.61102426\n",
      "52 Train Loss 40.728287 Test MSE 44.09162411541156 Test RE 0.11178080137330175 Lambda1 0.6121759\n",
      "53 Train Loss 40.36981 Test MSE 43.596131963197095 Test RE 0.11115094256346443 Lambda1 0.6198739\n",
      "54 Train Loss 39.935722 Test MSE 43.319093741802895 Test RE 0.11079721697028642 Lambda1 0.6262457\n",
      "55 Train Loss 39.459755 Test MSE 42.55965385435628 Test RE 0.10982171315887003 Lambda1 0.6358451\n",
      "56 Train Loss 39.149414 Test MSE 42.15163394762061 Test RE 0.10929401431527139 Lambda1 0.6398718\n",
      "57 Train Loss 38.97636 Test MSE 41.8509425785072 Test RE 0.10890348869807455 Lambda1 0.6442635\n",
      "58 Train Loss 38.614536 Test MSE 41.55055587923609 Test RE 0.1085119554483955 Lambda1 0.6475108\n",
      "59 Train Loss 38.433643 Test MSE 41.40547503310816 Test RE 0.10832234579854029 Lambda1 0.64840704\n",
      "60 Train Loss 38.14242 Test MSE 41.341774272950275 Test RE 0.10823898880604414 Lambda1 0.6464455\n",
      "61 Train Loss 37.888203 Test MSE 41.07686478762474 Test RE 0.10789164502858263 Lambda1 0.6473869\n",
      "62 Train Loss 37.83294 Test MSE 41.02791886235408 Test RE 0.107827345690177 Lambda1 0.64801496\n",
      "63 Train Loss 37.677868 Test MSE 40.861200676385934 Test RE 0.1076080428414605 Lambda1 0.65190816\n",
      "64 Train Loss 36.858517 Test MSE 39.70151824236043 Test RE 0.10607003891621665 Lambda1 0.66678345\n",
      "65 Train Loss 36.345776 Test MSE 38.79476954096389 Test RE 0.10485176821457728 Lambda1 0.67299134\n",
      "66 Train Loss 36.059837 Test MSE 38.11127883318585 Test RE 0.10392401850519442 Lambda1 0.6808148\n",
      "67 Train Loss 35.825073 Test MSE 37.62342040729205 Test RE 0.1032567159572467 Lambda1 0.68723655\n",
      "68 Train Loss 35.479107 Test MSE 37.56073000285728 Test RE 0.10317065381864156 Lambda1 0.6902082\n",
      "69 Train Loss 34.845203 Test MSE 37.617588828049975 Test RE 0.10324871332152447 Lambda1 0.69022274\n",
      "70 Train Loss 34.270023 Test MSE 37.47324886917725 Test RE 0.10305043854184168 Lambda1 0.6982973\n",
      "71 Train Loss 33.8972 Test MSE 36.9884246675218 Test RE 0.10238164146097979 Lambda1 0.7073966\n",
      "72 Train Loss 33.675716 Test MSE 36.71955451448524 Test RE 0.10200885490883806 Lambda1 0.71068865\n",
      "73 Train Loss 33.447914 Test MSE 36.69358279065949 Test RE 0.10197277311974065 Lambda1 0.71041965\n",
      "74 Train Loss 33.210365 Test MSE 36.23646733759875 Test RE 0.10133561246094928 Lambda1 0.7163967\n",
      "75 Train Loss 33.089176 Test MSE 36.08819859194662 Test RE 0.10112808250386703 Lambda1 0.71875787\n",
      "76 Train Loss 32.997017 Test MSE 36.021442792107244 Test RE 0.10103450605604829 Lambda1 0.7199402\n",
      "77 Train Loss 32.93167 Test MSE 35.94504957879548 Test RE 0.1009273136932216 Lambda1 0.7200606\n",
      "78 Train Loss 32.866074 Test MSE 36.0484173527373 Test RE 0.10107232868569867 Lambda1 0.7186282\n",
      "79 Train Loss 32.723858 Test MSE 35.941341182079725 Test RE 0.10092210729933772 Lambda1 0.72036064\n",
      "80 Train Loss 32.59931 Test MSE 35.68570537329267 Test RE 0.10056255834432261 Lambda1 0.7250379\n",
      "81 Train Loss 32.313927 Test MSE 35.26424901511009 Test RE 0.09996696115697798 Lambda1 0.7339252\n",
      "82 Train Loss 32.09283 Test MSE 35.321554810607246 Test RE 0.10004815331257866 Lambda1 0.73514646\n",
      "83 Train Loss 31.9779 Test MSE 35.213632991886854 Test RE 0.09989519233802992 Lambda1 0.7354454\n",
      "84 Train Loss 31.914534 Test MSE 35.25599147866893 Test RE 0.09995525625502516 Lambda1 0.7357483\n",
      "85 Train Loss 31.84656 Test MSE 35.22821466887742 Test RE 0.09991587308777702 Lambda1 0.73687696\n",
      "86 Train Loss 31.449831 Test MSE 34.965839352076806 Test RE 0.09954309725897907 Lambda1 0.74399465\n",
      "87 Train Loss 31.304905 Test MSE 34.90660247576509 Test RE 0.09945874175963236 Lambda1 0.7462338\n",
      "88 Train Loss 31.194294 Test MSE 34.83333300236373 Test RE 0.09935430424398356 Lambda1 0.74999285\n",
      "89 Train Loss 31.075008 Test MSE 34.74878277320942 Test RE 0.099233650660048 Lambda1 0.7528512\n",
      "90 Train Loss 31.026428 Test MSE 34.703460545747916 Test RE 0.09916891519042154 Lambda1 0.7540879\n",
      "91 Train Loss 30.969162 Test MSE 34.624587918518756 Test RE 0.09905615752831154 Lambda1 0.7565068\n",
      "92 Train Loss 30.940025 Test MSE 34.628638876217245 Test RE 0.09906195197370223 Lambda1 0.75715977\n",
      "93 Train Loss 30.92785 Test MSE 34.5905076179216 Test RE 0.09900739601321976 Lambda1 0.75709647\n",
      "94 Train Loss 30.88984 Test MSE 34.55296196882762 Test RE 0.09895364851902685 Lambda1 0.7577037\n",
      "95 Train Loss 30.822535 Test MSE 34.536402936180124 Test RE 0.09892993458878124 Lambda1 0.75833935\n",
      "96 Train Loss 30.750835 Test MSE 34.50670701558046 Test RE 0.09888739328424315 Lambda1 0.75732905\n",
      "97 Train Loss 30.708336 Test MSE 34.46237054301171 Test RE 0.0988238443715943 Lambda1 0.75776523\n",
      "98 Train Loss 30.68419 Test MSE 34.39174601932591 Test RE 0.09872253145767618 Lambda1 0.75809574\n",
      "99 Train Loss 30.624914 Test MSE 34.378411822087685 Test RE 0.0987033914970887 Lambda1 0.7593778\n",
      "100 Train Loss 30.596872 Test MSE 34.31345014119366 Test RE 0.0986100921013732 Lambda1 0.7602616\n",
      "101 Train Loss 30.52287 Test MSE 34.18364184124116 Test RE 0.09842339362965684 Lambda1 0.7636159\n",
      "102 Train Loss 30.426264 Test MSE 34.092451978716966 Test RE 0.09829202656455324 Lambda1 0.7677715\n",
      "103 Train Loss 30.300173 Test MSE 33.92117014152979 Test RE 0.09804480413072264 Lambda1 0.77020633\n",
      "104 Train Loss 30.227234 Test MSE 33.806243900266466 Test RE 0.09787857341097626 Lambda1 0.7709671\n",
      "105 Train Loss 30.207314 Test MSE 33.77708627209566 Test RE 0.09783635454318924 Lambda1 0.7716285\n",
      "106 Train Loss 30.178894 Test MSE 33.70761399335735 Test RE 0.09773568847345662 Lambda1 0.77237886\n",
      "107 Train Loss 30.099556 Test MSE 33.57668263105929 Test RE 0.09754568514602965 Lambda1 0.7744648\n",
      "108 Train Loss 30.058697 Test MSE 33.52966251553045 Test RE 0.0974773607032955 Lambda1 0.77561253\n",
      "109 Train Loss 30.04163 Test MSE 33.48856280776448 Test RE 0.09741759989464882 Lambda1 0.77661246\n",
      "110 Train Loss 30.035774 Test MSE 33.47666261372905 Test RE 0.09740028963629682 Lambda1 0.77709496\n",
      "111 Train Loss 30.028557 Test MSE 33.4635772246274 Test RE 0.0973812518177123 Lambda1 0.77761936\n",
      "112 Train Loss 30.002327 Test MSE 33.41093002373148 Test RE 0.097304618223526 Lambda1 0.7793305\n",
      "113 Train Loss 29.981598 Test MSE 33.40890518161112 Test RE 0.0973016696454021 Lambda1 0.78008974\n",
      "114 Train Loss 29.951656 Test MSE 33.36568882565977 Test RE 0.09723871660476988 Lambda1 0.78020954\n",
      "115 Train Loss 29.92833 Test MSE 33.32761948075279 Test RE 0.09718322740476733 Lambda1 0.780004\n",
      "116 Train Loss 29.877579 Test MSE 33.2969538391283 Test RE 0.09713850666078822 Lambda1 0.7808165\n",
      "117 Train Loss 29.861122 Test MSE 33.286034931000884 Test RE 0.09712257827583139 Lambda1 0.78200585\n",
      "118 Train Loss 29.845612 Test MSE 33.274049223531684 Test RE 0.09710509064739324 Lambda1 0.783165\n",
      "119 Train Loss 29.825891 Test MSE 33.240154954246655 Test RE 0.09705562049333709 Lambda1 0.78499\n",
      "120 Train Loss 29.809107 Test MSE 33.204635768744986 Test RE 0.09700375163157782 Lambda1 0.78590304\n",
      "121 Train Loss 29.794426 Test MSE 33.18965739282948 Test RE 0.09698187031116402 Lambda1 0.7862452\n",
      "122 Train Loss 29.786314 Test MSE 33.20176188091884 Test RE 0.09699955366453461 Lambda1 0.7867487\n",
      "123 Train Loss 29.780643 Test MSE 33.21069947617443 Test RE 0.09701260845991597 Lambda1 0.7867649\n",
      "124 Train Loss 29.776373 Test MSE 33.193180311001264 Test RE 0.09698701724780352 Lambda1 0.7869476\n",
      "125 Train Loss 29.76614 Test MSE 33.193686084532914 Test RE 0.0969877561537914 Lambda1 0.78737724\n",
      "126 Train Loss 29.72425 Test MSE 33.13470261723273 Test RE 0.0969015467203897 Lambda1 0.78770185\n",
      "127 Train Loss 29.644426 Test MSE 33.161362821841074 Test RE 0.09694052240770512 Lambda1 0.7900459\n",
      "128 Train Loss 29.607155 Test MSE 33.09002847403011 Test RE 0.09683620052187679 Lambda1 0.7924609\n",
      "129 Train Loss 29.587503 Test MSE 33.079837939736755 Test RE 0.09682128834659111 Lambda1 0.7933131\n",
      "130 Train Loss 29.57212 Test MSE 33.098711493327734 Test RE 0.09684890488455215 Lambda1 0.7937512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 Train Loss 29.560549 Test MSE 33.08027459908342 Test RE 0.0968219273730137 Lambda1 0.79343486\n",
      "132 Train Loss 29.545198 Test MSE 33.03479778115355 Test RE 0.09675535193559211 Lambda1 0.79523206\n",
      "133 Train Loss 29.517504 Test MSE 32.96685360796942 Test RE 0.09665580015152242 Lambda1 0.7984744\n",
      "134 Train Loss 29.466412 Test MSE 32.93477652909864 Test RE 0.09660876517636897 Lambda1 0.8015237\n",
      "135 Train Loss 29.427204 Test MSE 32.85849156229618 Test RE 0.09649681559401252 Lambda1 0.8040778\n",
      "136 Train Loss 29.332653 Test MSE 32.69884145029405 Test RE 0.09626210478866687 Lambda1 0.8093556\n",
      "137 Train Loss 29.262651 Test MSE 32.642241144731464 Test RE 0.09617875589752341 Lambda1 0.81243616\n",
      "138 Train Loss 29.23073 Test MSE 32.64695901954784 Test RE 0.09618570613929321 Lambda1 0.81381315\n",
      "139 Train Loss 29.212385 Test MSE 32.6288165654598 Test RE 0.09615897643283808 Lambda1 0.8150015\n",
      "140 Train Loss 29.183859 Test MSE 32.54417748980331 Test RE 0.09603417736552713 Lambda1 0.8163598\n",
      "141 Train Loss 29.139389 Test MSE 32.47808295493291 Test RE 0.09593660906331437 Lambda1 0.81860805\n",
      "142 Train Loss 29.098984 Test MSE 32.40129507162068 Test RE 0.09582313055816748 Lambda1 0.82047224\n",
      "143 Train Loss 29.067652 Test MSE 32.415408817252015 Test RE 0.09584399818156301 Lambda1 0.8218755\n",
      "144 Train Loss 29.051592 Test MSE 32.43642589965915 Test RE 0.09587506419115467 Lambda1 0.82354075\n",
      "145 Train Loss 29.032574 Test MSE 32.41122659457777 Test RE 0.09583781510556721 Lambda1 0.82400763\n",
      "146 Train Loss 29.014164 Test MSE 32.40118689320025 Test RE 0.09582297059537072 Lambda1 0.8252529\n",
      "147 Train Loss 29.003681 Test MSE 32.38269345533286 Test RE 0.09579562053110809 Lambda1 0.82621974\n",
      "148 Train Loss 28.973743 Test MSE 32.34276147011299 Test RE 0.09573653821126493 Lambda1 0.82641715\n",
      "149 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "150 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "151 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "152 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "153 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "154 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "155 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "156 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "157 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "158 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "159 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "160 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "161 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "162 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "163 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "164 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "165 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "166 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "167 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "168 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "169 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "170 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "171 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "172 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "173 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "174 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "175 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "176 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "177 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "178 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "179 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "180 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "181 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "182 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "183 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "184 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "185 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "186 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "187 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "188 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "189 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "190 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "191 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "192 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "193 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "194 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "195 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "196 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "197 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "198 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "199 Train Loss 28.966467 Test MSE 32.35353975461067 Test RE 0.09575248907081547 Lambda1 0.8264479\n",
      "Training time: 473.24\n",
      "Training time: 473.24\n",
      "inv_HT_rowdy\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 1106.0182 Test MSE 1095.9128138548988 Test RE 0.5572847682980174 Lambda1 -0.06230012\n",
      "1 Train Loss 844.1855 Test MSE 844.0024957124773 Test RE 0.4890587528750701 Lambda1 -0.07200373\n",
      "2 Train Loss 695.83496 Test MSE 632.9344295145112 Test RE 0.4235147407468869 Lambda1 -0.033886373\n",
      "3 Train Loss 396.6644 Test MSE 371.0716140432338 Test RE 0.3242784117586772 Lambda1 0.0036577662\n",
      "4 Train Loss 296.89743 Test MSE 296.6955524301817 Test RE 0.28996440498555376 Lambda1 0.033373628\n",
      "5 Train Loss 263.644 Test MSE 264.53773288384093 Test RE 0.27379971029191474 Lambda1 0.047972936\n",
      "6 Train Loss 249.18279 Test MSE 251.39539014539224 Test RE 0.2669118315936169 Lambda1 0.070705414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 234.85439 Test MSE 238.67411863811364 Test RE 0.2600709441894378 Lambda1 0.096555494\n",
      "8 Train Loss 223.51558 Test MSE 227.11187213718011 Test RE 0.25369335348253547 Lambda1 0.12782845\n",
      "9 Train Loss 216.49779 Test MSE 219.77038044133317 Test RE 0.24955929530791385 Lambda1 0.15745477\n",
      "10 Train Loss 212.63678 Test MSE 215.75506467217193 Test RE 0.24726899872565522 Lambda1 0.18278396\n",
      "11 Train Loss 202.19498 Test MSE 201.19573362427548 Test RE 0.23878033339577234 Lambda1 0.22293031\n",
      "12 Train Loss 182.70506 Test MSE 179.1141643719681 Test RE 0.2252963398539411 Lambda1 0.296995\n",
      "13 Train Loss 166.41876 Test MSE 163.30658038624696 Test RE 0.21512506425566402 Lambda1 0.32175925\n",
      "14 Train Loss 141.71855 Test MSE 129.81292834972447 Test RE 0.19179979740801248 Lambda1 0.3573107\n",
      "15 Train Loss 121.18919 Test MSE 109.88647087566227 Test RE 0.17646609832187654 Lambda1 0.37262082\n",
      "16 Train Loss 100.015854 Test MSE 92.69976761582161 Test RE 0.16207965118010575 Lambda1 0.39273575\n",
      "17 Train Loss 88.94818 Test MSE 80.77777850792086 Test RE 0.15129867451366308 Lambda1 0.40397346\n",
      "18 Train Loss 79.83098 Test MSE 73.43046132124807 Test RE 0.14425381215276686 Lambda1 0.42294398\n",
      "19 Train Loss 73.92652 Test MSE 65.56151555027647 Test RE 0.1363055886991347 Lambda1 0.44807193\n",
      "20 Train Loss 67.54221 Test MSE 61.6299635833113 Test RE 0.13215546471086395 Lambda1 0.46272662\n",
      "21 Train Loss 57.978992 Test MSE 58.1615524431522 Test RE 0.1283828953035937 Lambda1 0.46983126\n",
      "22 Train Loss 54.799698 Test MSE 56.05961211115173 Test RE 0.12604168918283418 Lambda1 0.4841817\n",
      "23 Train Loss 51.857613 Test MSE 55.10261227934898 Test RE 0.12496122234074504 Lambda1 0.49139902\n",
      "24 Train Loss 50.413258 Test MSE 54.48659664599843 Test RE 0.12426076171777616 Lambda1 0.4939805\n",
      "25 Train Loss 47.88912 Test MSE 53.21357132014576 Test RE 0.12280056754407902 Lambda1 0.50381845\n",
      "26 Train Loss 46.797432 Test MSE 52.12721132543717 Test RE 0.12154061136536395 Lambda1 0.5140781\n",
      "27 Train Loss 46.056725 Test MSE 51.531641298712216 Test RE 0.12084429660877098 Lambda1 0.51441836\n",
      "28 Train Loss 45.487392 Test MSE 50.90951214068716 Test RE 0.12011261944339213 Lambda1 0.5175125\n",
      "29 Train Loss 44.805088 Test MSE 50.18361479849515 Test RE 0.11925322733150115 Lambda1 0.5276062\n",
      "30 Train Loss 44.16549 Test MSE 48.74809507096084 Test RE 0.11753521209502109 Lambda1 0.5386139\n",
      "31 Train Loss 43.741013 Test MSE 48.258921052805555 Test RE 0.11694400809722297 Lambda1 0.5406073\n",
      "32 Train Loss 43.05161 Test MSE 47.882169243462 Test RE 0.11648662950686758 Lambda1 0.5431257\n",
      "33 Train Loss 42.50574 Test MSE 47.6299637148575 Test RE 0.11617944459393229 Lambda1 0.5484977\n",
      "34 Train Loss 41.51901 Test MSE 46.116354491672745 Test RE 0.11431853625780111 Lambda1 0.5657744\n",
      "35 Train Loss 41.014194 Test MSE 45.348907376878095 Test RE 0.11336332735521705 Lambda1 0.5780687\n",
      "36 Train Loss 40.420467 Test MSE 44.52546781871201 Test RE 0.11232939407489138 Lambda1 0.59218746\n",
      "37 Train Loss 39.970165 Test MSE 44.23971315641008 Test RE 0.11196836120890222 Lambda1 0.59562963\n",
      "38 Train Loss 38.94029 Test MSE 42.60685168645664 Test RE 0.10988259136214276 Lambda1 0.618874\n",
      "39 Train Loss 38.07837 Test MSE 42.143112904089186 Test RE 0.10928296674679931 Lambda1 0.62689024\n",
      "40 Train Loss 37.437157 Test MSE 40.97545549969665 Test RE 0.1077583829583919 Lambda1 0.6504806\n",
      "41 Train Loss 36.97274 Test MSE 40.444085169015494 Test RE 0.10705739676725724 Lambda1 0.6665024\n",
      "42 Train Loss 36.429203 Test MSE 39.834659135443836 Test RE 0.10624774546539383 Lambda1 0.6810249\n",
      "43 Train Loss 36.05999 Test MSE 39.331759401792645 Test RE 0.10557494347114918 Lambda1 0.69273484\n",
      "44 Train Loss 35.46296 Test MSE 39.10620679749504 Test RE 0.10527179226155663 Lambda1 0.693397\n",
      "45 Train Loss 35.267754 Test MSE 38.6534391651631 Test RE 0.10466060506441476 Lambda1 0.69999415\n",
      "46 Train Loss 35.112972 Test MSE 38.480047278980216 Test RE 0.10442559755652997 Lambda1 0.70259947\n",
      "47 Train Loss 34.94492 Test MSE 38.292949868916374 Test RE 0.10417141951842222 Lambda1 0.7045501\n",
      "48 Train Loss 34.85015 Test MSE 38.3074265115967 Test RE 0.10419110865075902 Lambda1 0.70421827\n",
      "49 Train Loss 34.619995 Test MSE 38.090193171007556 Test RE 0.10389526573185433 Lambda1 0.70875776\n",
      "50 Train Loss 34.50104 Test MSE 37.8743600917051 Test RE 0.10360049312387637 Lambda1 0.7119849\n",
      "51 Train Loss 34.289597 Test MSE 37.65510777111193 Test RE 0.10330018946994743 Lambda1 0.7180098\n",
      "52 Train Loss 33.90561 Test MSE 37.50857820755591 Test RE 0.10309900446852585 Lambda1 0.71930003\n",
      "53 Train Loss 33.710438 Test MSE 37.39378019483995 Test RE 0.10294111217831368 Lambda1 0.7238931\n",
      "54 Train Loss 33.48328 Test MSE 37.36420294999257 Test RE 0.10290039261520374 Lambda1 0.7296393\n",
      "55 Train Loss 33.355904 Test MSE 37.28290344734165 Test RE 0.10278838291437652 Lambda1 0.7345679\n",
      "56 Train Loss 33.224884 Test MSE 37.06852460553503 Test RE 0.10249243731451532 Lambda1 0.7360109\n",
      "57 Train Loss 32.93321 Test MSE 36.67252005501601 Test RE 0.10194350187242429 Lambda1 0.7458851\n",
      "58 Train Loss 32.333263 Test MSE 35.94473021627061 Test RE 0.10092686533560764 Lambda1 0.76782507\n",
      "59 Train Loss 32.037228 Test MSE 35.60192147678221 Test RE 0.10044443717890664 Lambda1 0.7776597\n",
      "60 Train Loss 31.80152 Test MSE 35.27002051620094 Test RE 0.09997514133713897 Lambda1 0.78460485\n",
      "61 Train Loss 31.701721 Test MSE 35.22864807310571 Test RE 0.09991648770633045 Lambda1 0.7847943\n",
      "62 Train Loss 31.548323 Test MSE 35.091969156270125 Test RE 0.09972247303639276 Lambda1 0.78359175\n",
      "63 Train Loss 31.424564 Test MSE 34.927757945588965 Test RE 0.09948887614045997 Lambda1 0.7838966\n",
      "64 Train Loss 31.34335 Test MSE 34.85771084113329 Test RE 0.09938906433420985 Lambda1 0.7816385\n",
      "65 Train Loss 31.252768 Test MSE 34.70064399928831 Test RE 0.09916489081475231 Lambda1 0.7809565\n",
      "66 Train Loss 31.17725 Test MSE 34.45245653354227 Test RE 0.09880962870700634 Lambda1 0.7832667\n",
      "67 Train Loss 31.10918 Test MSE 34.13336243094682 Test RE 0.0983509833923898 Lambda1 0.79053766\n",
      "68 Train Loss 30.713005 Test MSE 33.89936773007008 Test RE 0.09801329052515216 Lambda1 0.79989594\n",
      "69 Train Loss 30.600155 Test MSE 33.70038105940985 Test RE 0.09772520191535279 Lambda1 0.80625856\n",
      "70 Train Loss 30.519314 Test MSE 33.63385638516691 Test RE 0.09762869931648141 Lambda1 0.8091951\n",
      "71 Train Loss 30.401285 Test MSE 33.46284924542875 Test RE 0.09738019257782979 Lambda1 0.8105573\n",
      "72 Train Loss 30.32636 Test MSE 33.3586321825616 Test RE 0.09722843335831105 Lambda1 0.81080407\n",
      "73 Train Loss 30.247997 Test MSE 33.22477780706047 Test RE 0.0970331685642335 Lambda1 0.8136076\n",
      "74 Train Loss 30.117704 Test MSE 33.17669647223902 Test RE 0.09696293222686228 Lambda1 0.816116\n",
      "75 Train Loss 29.932714 Test MSE 33.20199288444785 Test RE 0.09699989110446196 Lambda1 0.81528145\n",
      "76 Train Loss 29.791376 Test MSE 33.160750358578014 Test RE 0.09693962719746994 Lambda1 0.81938463\n",
      "77 Train Loss 29.653952 Test MSE 32.96543969302116 Test RE 0.09665372739463055 Lambda1 0.82294655\n",
      "78 Train Loss 29.546713 Test MSE 32.64339223781416 Test RE 0.09618045170211172 Lambda1 0.8328974\n",
      "79 Train Loss 29.469036 Test MSE 32.69594640487525 Test RE 0.09625784333288125 Lambda1 0.83622944\n",
      "80 Train Loss 29.401884 Test MSE 32.51046480078124 Test RE 0.09598442331587051 Lambda1 0.84384984\n",
      "81 Train Loss 29.319347 Test MSE 32.47805150482678 Test RE 0.09593656261326262 Lambda1 0.8482783\n",
      "82 Train Loss 29.255703 Test MSE 32.43198808982881 Test RE 0.09586850536347152 Lambda1 0.85208744\n",
      "83 Train Loss 29.10862 Test MSE 32.188798489696296 Test RE 0.0955083964922596 Lambda1 0.86391526\n",
      "84 Train Loss 29.00749 Test MSE 32.110038434646825 Test RE 0.09539147921133924 Lambda1 0.86615413\n",
      "85 Train Loss 28.816225 Test MSE 31.867110877107066 Test RE 0.0950299534105837 Lambda1 0.8730531\n",
      "86 Train Loss 28.654875 Test MSE 31.65578591185374 Test RE 0.09471433637785023 Lambda1 0.88438207\n",
      "87 Train Loss 28.483704 Test MSE 31.488469551785673 Test RE 0.09446369888444571 Lambda1 0.8907278\n",
      "88 Train Loss 28.298014 Test MSE 31.59841622059258 Test RE 0.09462847219540829 Lambda1 0.8870384\n",
      "89 Train Loss 28.05312 Test MSE 31.343268639005277 Test RE 0.09424564979770017 Lambda1 0.88571054\n",
      "90 Train Loss 27.988638 Test MSE 31.253536216270835 Test RE 0.094110645499667 Lambda1 0.8849429\n",
      "91 Train Loss 27.925087 Test MSE 31.209140188834535 Test RE 0.09404377908813934 Lambda1 0.8878849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Train Loss 27.886538 Test MSE 31.238432735375618 Test RE 0.09408790295154257 Lambda1 0.886245\n",
      "93 Train Loss 27.813847 Test MSE 31.146108345656014 Test RE 0.09394876287275156 Lambda1 0.8831276\n",
      "94 Train Loss 27.775084 Test MSE 31.192277193035807 Test RE 0.09401836867761804 Lambda1 0.88372374\n",
      "95 Train Loss 27.655273 Test MSE 31.020685471294204 Test RE 0.09375940999564358 Lambda1 0.88994867\n",
      "96 Train Loss 27.518562 Test MSE 30.856997538627976 Test RE 0.09351171102902245 Lambda1 0.8950456\n",
      "97 Train Loss 27.464216 Test MSE 30.78074913936704 Test RE 0.09339610470027956 Lambda1 0.89819807\n",
      "98 Train Loss 27.347658 Test MSE 30.569286512843114 Test RE 0.09307473788409013 Lambda1 0.90065706\n",
      "99 Train Loss 27.204378 Test MSE 30.348501428602052 Test RE 0.09273801506124421 Lambda1 0.905311\n",
      "100 Train Loss 27.138485 Test MSE 30.315152625361144 Test RE 0.09268704793351863 Lambda1 0.90588427\n",
      "101 Train Loss 27.07871 Test MSE 30.242431452772482 Test RE 0.09257581052728327 Lambda1 0.9084819\n",
      "102 Train Loss 26.946568 Test MSE 30.185259366772044 Test RE 0.09248826373049733 Lambda1 0.9095234\n",
      "103 Train Loss 26.885761 Test MSE 30.216555403452517 Test RE 0.09253619716366972 Lambda1 0.9105202\n",
      "104 Train Loss 26.86382 Test MSE 30.182838986094144 Test RE 0.09248455560774495 Lambda1 0.9114152\n",
      "105 Train Loss 26.842932 Test MSE 30.1669516362071 Test RE 0.09246021184241564 Lambda1 0.9114373\n",
      "106 Train Loss 26.818579 Test MSE 30.088569671322198 Test RE 0.09234001529670725 Lambda1 0.9110087\n",
      "107 Train Loss 26.805471 Test MSE 30.06825826938111 Test RE 0.09230884279800677 Lambda1 0.90990233\n",
      "108 Train Loss 26.793314 Test MSE 30.076172589598897 Test RE 0.09232099038679462 Lambda1 0.90940523\n",
      "109 Train Loss 26.777575 Test MSE 30.0152886668384 Test RE 0.09222749924315414 Lambda1 0.9092057\n",
      "110 Train Loss 26.7714 Test MSE 30.008839620020787 Test RE 0.09221759076919415 Lambda1 0.9090648\n",
      "111 Train Loss 26.752527 Test MSE 29.98961321682945 Test RE 0.0921880445307629 Lambda1 0.9101551\n",
      "112 Train Loss 26.723843 Test MSE 29.877240006775985 Test RE 0.09201516485645893 Lambda1 0.91071856\n",
      "113 Train Loss 26.712782 Test MSE 29.88358279556917 Test RE 0.09202493151803863 Lambda1 0.91176844\n",
      "114 Train Loss 26.697071 Test MSE 29.930015645528446 Test RE 0.09209639753544163 Lambda1 0.9125797\n",
      "115 Train Loss 26.607538 Test MSE 29.887902512964203 Test RE 0.09203158244968264 Lambda1 0.91247815\n",
      "116 Train Loss 26.563738 Test MSE 29.875770250354822 Test RE 0.09201290156937784 Lambda1 0.91131604\n",
      "117 Train Loss 26.5517 Test MSE 29.846718200360677 Test RE 0.09196815270474068 Lambda1 0.9113062\n",
      "118 Train Loss 26.534485 Test MSE 29.799161318929773 Test RE 0.09189485382256224 Lambda1 0.91155374\n",
      "119 Train Loss 26.525625 Test MSE 29.766903741226603 Test RE 0.09184510228752328 Lambda1 0.91110504\n",
      "120 Train Loss 26.518366 Test MSE 29.738326577836567 Test RE 0.09180100460840188 Lambda1 0.91090995\n",
      "121 Train Loss 26.504225 Test MSE 29.711311032206506 Test RE 0.09175929718877379 Lambda1 0.91037446\n",
      "122 Train Loss 26.46946 Test MSE 29.666431417393742 Test RE 0.09168996873985699 Lambda1 0.91020596\n",
      "123 Train Loss 26.452055 Test MSE 29.681525808501238 Test RE 0.09171329187278564 Lambda1 0.9097919\n",
      "124 Train Loss 26.43425 Test MSE 29.682255263045036 Test RE 0.09171441884090178 Lambda1 0.91057837\n",
      "125 Train Loss 26.430136 Test MSE 29.698378273224865 Test RE 0.09173932449152934 Lambda1 0.9107382\n",
      "126 Train Loss 26.42677 Test MSE 29.688720585173495 Test RE 0.0917244068113308 Lambda1 0.9110528\n",
      "127 Train Loss 26.417759 Test MSE 29.657229132149904 Test RE 0.09167574689638218 Lambda1 0.91186035\n",
      "128 Train Loss 26.392403 Test MSE 29.606962167507387 Test RE 0.09159802190333788 Lambda1 0.91301996\n",
      "129 Train Loss 26.355217 Test MSE 29.59557419136432 Test RE 0.09158040414831088 Lambda1 0.91529703\n",
      "130 Train Loss 26.34471 Test MSE 29.596812711131758 Test RE 0.09158232036310522 Lambda1 0.91582674\n",
      "131 Train Loss 26.333961 Test MSE 29.585864066137855 Test RE 0.09156537943315012 Lambda1 0.9167969\n",
      "132 Train Loss 26.321901 Test MSE 29.589429071374713 Test RE 0.09157089593967456 Lambda1 0.91677725\n",
      "133 Train Loss 26.31529 Test MSE 29.575493352953043 Test RE 0.09154932985089459 Lambda1 0.9183834\n",
      "134 Train Loss 26.308323 Test MSE 29.55453085128756 Test RE 0.09151687995847323 Lambda1 0.9198084\n",
      "135 Train Loss 26.297247 Test MSE 29.49282720385244 Test RE 0.09142129603933116 Lambda1 0.92089623\n",
      "136 Train Loss 26.28835 Test MSE 29.43285998258238 Test RE 0.09132830612957363 Lambda1 0.921692\n",
      "137 Train Loss 26.275702 Test MSE 29.361405421923223 Test RE 0.09121737926895943 Lambda1 0.9226408\n",
      "138 Train Loss 26.257523 Test MSE 29.331023952415073 Test RE 0.09117017384303144 Lambda1 0.9227394\n",
      "139 Train Loss 26.238108 Test MSE 29.342746861298604 Test RE 0.09118839129211385 Lambda1 0.9218585\n",
      "140 Train Loss 26.227043 Test MSE 29.310730570084118 Test RE 0.09113862923472252 Lambda1 0.9212479\n",
      "141 Train Loss 26.217318 Test MSE 29.321073851960758 Test RE 0.0911547084890587 Lambda1 0.92009217\n",
      "142 Train Loss 26.199518 Test MSE 29.287716033579432 Test RE 0.0911028415697985 Lambda1 0.92015475\n",
      "143 Train Loss 26.176811 Test MSE 29.281892200617726 Test RE 0.09109378326512618 Lambda1 0.91889054\n",
      "144 Train Loss 26.148745 Test MSE 29.264599805797985 Test RE 0.0910668816174055 Lambda1 0.91738\n",
      "145 Train Loss 26.13626 Test MSE 29.236653387229953 Test RE 0.09102338877813958 Lambda1 0.9175261\n",
      "146 Train Loss 26.128756 Test MSE 29.20819146948422 Test RE 0.09097907229995787 Lambda1 0.9175334\n",
      "147 Train Loss 26.118355 Test MSE 29.18326419901698 Test RE 0.09094024168597821 Lambda1 0.91688997\n",
      "148 Train Loss 26.096067 Test MSE 29.165581903772942 Test RE 0.09091268692300294 Lambda1 0.91728413\n",
      "149 Train Loss 26.083261 Test MSE 29.189004503109587 Test RE 0.09094918515046067 Lambda1 0.91685045\n",
      "150 Train Loss 26.07884 Test MSE 29.208383486190613 Test RE 0.09097937135087858 Lambda1 0.916766\n",
      "151 Train Loss 26.078562 Test MSE 29.207214476874285 Test RE 0.09097755069557005 Lambda1 0.916772\n",
      "152 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "153 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "154 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "155 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "156 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "157 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "158 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "159 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "160 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "161 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "162 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "163 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "164 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "165 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "166 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "167 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "168 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "169 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "170 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "171 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "172 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "173 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "174 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "175 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "177 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "178 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "179 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "180 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "181 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "182 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "183 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "184 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "185 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "186 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "187 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "188 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "189 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "190 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "191 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "192 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "193 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "194 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "195 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "196 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "197 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "198 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "199 Train Loss 26.078527 Test MSE 29.20721856510373 Test RE 0.09097755706278267 Lambda1 0.916772\n",
      "Training time: 478.45\n",
      "Training time: 478.45\n",
      "inv_HT_rowdy\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 872.6822 Test MSE 870.1172372824781 Test RE 0.49656723116594 Lambda1 0.007871804\n",
      "1 Train Loss 854.5719 Test MSE 857.8264349305688 Test RE 0.49304763882689767 Lambda1 0.0006221568\n",
      "2 Train Loss 852.6468 Test MSE 854.2281571150938 Test RE 0.4920124721111714 Lambda1 0.0042350274\n",
      "3 Train Loss 835.8023 Test MSE 831.176513670283 Test RE 0.4853285077495529 Lambda1 -0.0011918236\n",
      "4 Train Loss 779.09875 Test MSE 752.098496679857 Test RE 0.4616645536391742 Lambda1 -0.003724418\n",
      "5 Train Loss 620.12476 Test MSE 588.5723769977641 Test RE 0.40840317391042796 Lambda1 -0.00068574597\n",
      "6 Train Loss 447.9131 Test MSE 406.92927706453077 Test RE 0.33958510988840895 Lambda1 -0.00094142964\n",
      "7 Train Loss 348.69406 Test MSE 332.5449044327275 Test RE 0.3069829953885551 Lambda1 1.3002293e-05\n",
      "8 Train Loss 314.19702 Test MSE 313.0605452238716 Test RE 0.2978539331174046 Lambda1 0.0007139649\n",
      "9 Train Loss 295.25366 Test MSE 300.82532675567563 Test RE 0.291975472002595 Lambda1 0.0018580719\n",
      "10 Train Loss 282.34677 Test MSE 286.85345645750965 Test RE 0.2851144406546919 Lambda1 0.009490675\n",
      "11 Train Loss 274.9556 Test MSE 282.977959022438 Test RE 0.28318188974564423 Lambda1 0.010834469\n",
      "12 Train Loss 270.31622 Test MSE 276.7056134548977 Test RE 0.28002587016770425 Lambda1 0.017790861\n",
      "13 Train Loss 265.7451 Test MSE 270.96662393119965 Test RE 0.277106728292397 Lambda1 0.027647026\n",
      "14 Train Loss 261.7797 Test MSE 267.45161834104687 Test RE 0.275303533586507 Lambda1 0.032277185\n",
      "15 Train Loss 256.87613 Test MSE 263.04374445458103 Test RE 0.2730254676757762 Lambda1 0.041806575\n",
      "16 Train Loss 252.04594 Test MSE 255.3482174319191 Test RE 0.2690020476304019 Lambda1 0.05800715\n",
      "17 Train Loss 246.6915 Test MSE 246.8101662228047 Test RE 0.26446651532369436 Lambda1 0.07823556\n",
      "18 Train Loss 238.87726 Test MSE 235.2820732282291 Test RE 0.25821626196180486 Lambda1 0.11101883\n",
      "19 Train Loss 232.17819 Test MSE 232.45556605327957 Test RE 0.25666056469839066 Lambda1 0.12631817\n",
      "20 Train Loss 224.7865 Test MSE 225.25872736198855 Test RE 0.2526562138090074 Lambda1 0.14787196\n",
      "21 Train Loss 214.49127 Test MSE 211.64234670942025 Test RE 0.2449009415501149 Lambda1 0.16644955\n",
      "22 Train Loss 201.62506 Test MSE 199.00637877054245 Test RE 0.23747760983578212 Lambda1 0.2016937\n",
      "23 Train Loss 188.4182 Test MSE 181.08261397496133 Test RE 0.22653095096845097 Lambda1 0.23571008\n",
      "24 Train Loss 178.6722 Test MSE 169.49787022113173 Test RE 0.2191650472372893 Lambda1 0.24283737\n",
      "25 Train Loss 170.74138 Test MSE 160.25893688544045 Test RE 0.21310826795114096 Lambda1 0.25881207\n",
      "26 Train Loss 160.77066 Test MSE 146.09512270400398 Test RE 0.2034731129284197 Lambda1 0.27520728\n",
      "27 Train Loss 150.9893 Test MSE 139.26855719514154 Test RE 0.1986624141950477 Lambda1 0.293833\n",
      "28 Train Loss 136.64969 Test MSE 126.40279150648301 Test RE 0.18926377719297538 Lambda1 0.32845202\n",
      "29 Train Loss 130.18274 Test MSE 124.44191363648464 Test RE 0.18779002126083247 Lambda1 0.34705272\n",
      "30 Train Loss 117.725586 Test MSE 110.9340623870403 Test RE 0.17730526385551307 Lambda1 0.35125756\n",
      "31 Train Loss 106.89297 Test MSE 98.76744867402405 Test RE 0.16730005676454815 Lambda1 0.35554686\n",
      "32 Train Loss 97.520546 Test MSE 85.87560366093865 Test RE 0.1559998109690709 Lambda1 0.37496844\n",
      "33 Train Loss 90.71321 Test MSE 83.33917436686444 Test RE 0.15367873162236245 Lambda1 0.39009094\n",
      "34 Train Loss 84.8873 Test MSE 79.41249080521052 Test RE 0.1500146177315191 Lambda1 0.39854294\n",
      "35 Train Loss 78.49219 Test MSE 74.21178597289175 Test RE 0.14501923583749637 Lambda1 0.42588952\n",
      "36 Train Loss 73.26527 Test MSE 70.91919054199883 Test RE 0.14176566376009697 Lambda1 0.43710205\n",
      "37 Train Loss 69.09247 Test MSE 66.71831198270503 Test RE 0.13750284864956433 Lambda1 0.4516173\n",
      "38 Train Loss 65.20282 Test MSE 64.62245387263617 Test RE 0.13532589038546877 Lambda1 0.46022174\n",
      "39 Train Loss 60.9576 Test MSE 61.57173818427799 Test RE 0.13209302249517255 Lambda1 0.4809095\n",
      "40 Train Loss 57.88917 Test MSE 58.79546611756478 Test RE 0.12908063389380373 Lambda1 0.49657816\n",
      "41 Train Loss 56.77 Test MSE 58.08287438346895 Test RE 0.12829603091473396 Lambda1 0.49940467\n",
      "42 Train Loss 54.58302 Test MSE 56.335824759242826 Test RE 0.12635181914226176 Lambda1 0.50506365\n",
      "43 Train Loss 53.786346 Test MSE 54.598277773687116 Test RE 0.12438804511325058 Lambda1 0.51750493\n",
      "44 Train Loss 52.78634 Test MSE 51.85243212692066 Test RE 0.12121984838327526 Lambda1 0.5416856\n",
      "45 Train Loss 52.154476 Test MSE 50.87797947629407 Test RE 0.12007541561430612 Lambda1 0.5491709\n",
      "46 Train Loss 51.284622 Test MSE 50.7666167139939 Test RE 0.1199439318632579 Lambda1 0.54870266\n",
      "47 Train Loss 49.677776 Test MSE 49.62515300258987 Test RE 0.11858782396465271 Lambda1 0.5578995\n",
      "48 Train Loss 48.829506 Test MSE 48.89461131698316 Test RE 0.11771171025155205 Lambda1 0.5704305\n",
      "49 Train Loss 48.477833 Test MSE 48.55390815790058 Test RE 0.11730087909304471 Lambda1 0.57715917\n",
      "50 Train Loss 47.658646 Test MSE 48.03689756430864 Test RE 0.11667468743803307 Lambda1 0.5864333\n",
      "51 Train Loss 46.182243 Test MSE 47.20939381618089 Test RE 0.11566537830545805 Lambda1 0.5905982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 Train Loss 44.978992 Test MSE 44.867241385536744 Test RE 0.11275968519870057 Lambda1 0.6158731\n",
      "53 Train Loss 43.61103 Test MSE 43.137411702199266 Test RE 0.11056462882767877 Lambda1 0.64060086\n",
      "54 Train Loss 42.01363 Test MSE 42.448459785914906 Test RE 0.10967815572159757 Lambda1 0.64802307\n",
      "55 Train Loss 41.49226 Test MSE 42.14617583397672 Test RE 0.10928693797673812 Lambda1 0.65514654\n",
      "56 Train Loss 40.93361 Test MSE 41.55702690194266 Test RE 0.10852040486592626 Lambda1 0.663956\n",
      "57 Train Loss 39.558422 Test MSE 40.37235931091126 Test RE 0.10696242378000356 Lambda1 0.6849572\n",
      "58 Train Loss 38.78819 Test MSE 39.89628415094691 Test RE 0.10632989740009885 Lambda1 0.69733083\n",
      "59 Train Loss 38.4325 Test MSE 40.14382996507311 Test RE 0.10665926162849525 Lambda1 0.6920243\n",
      "60 Train Loss 37.868427 Test MSE 39.85789256434904 Test RE 0.1062787252661896 Lambda1 0.6937775\n",
      "61 Train Loss 37.576176 Test MSE 39.31310260233268 Test RE 0.10554990105998287 Lambda1 0.70730877\n",
      "62 Train Loss 37.33056 Test MSE 38.79239968714614 Test RE 0.10484856562875156 Lambda1 0.71950793\n",
      "63 Train Loss 36.870384 Test MSE 38.522153284257314 Test RE 0.10448271472391289 Lambda1 0.72796476\n",
      "64 Train Loss 36.42377 Test MSE 38.54448241589433 Test RE 0.10451299172184035 Lambda1 0.72701603\n",
      "65 Train Loss 35.84442 Test MSE 38.27550194020294 Test RE 0.10414768430610907 Lambda1 0.7308241\n",
      "66 Train Loss 35.678127 Test MSE 38.33705474583486 Test RE 0.10423139329250665 Lambda1 0.73032033\n",
      "67 Train Loss 35.291367 Test MSE 38.098491318334915 Test RE 0.10390658217842068 Lambda1 0.73506016\n",
      "68 Train Loss 35.003693 Test MSE 38.069623070767484 Test RE 0.10386720831781265 Lambda1 0.7369272\n",
      "69 Train Loss 34.81802 Test MSE 37.898780872947455 Test RE 0.10363388771163709 Lambda1 0.7417657\n",
      "70 Train Loss 34.62814 Test MSE 37.70375605142122 Test RE 0.10336689694726477 Lambda1 0.7435606\n",
      "71 Train Loss 34.363293 Test MSE 37.43736420647979 Test RE 0.10300108578947773 Lambda1 0.74885845\n",
      "72 Train Loss 34.122307 Test MSE 37.36540230102951 Test RE 0.10290204409834124 Lambda1 0.7505923\n",
      "73 Train Loss 33.82811 Test MSE 36.85522954227567 Test RE 0.10219713733182219 Lambda1 0.76136637\n",
      "74 Train Loss 33.60057 Test MSE 36.730386092640245 Test RE 0.10202389914655616 Lambda1 0.7651568\n",
      "75 Train Loss 33.23028 Test MSE 36.24235394461806 Test RE 0.10134384310189135 Lambda1 0.7781146\n",
      "76 Train Loss 32.460705 Test MSE 35.488696411261124 Test RE 0.1002845878759888 Lambda1 0.7997672\n",
      "77 Train Loss 32.27617 Test MSE 35.16773036699777 Test RE 0.09983006206794655 Lambda1 0.8061553\n",
      "78 Train Loss 32.077385 Test MSE 34.84129508236597 Test RE 0.09936565862283744 Lambda1 0.817483\n",
      "79 Train Loss 31.990005 Test MSE 34.84041828781642 Test RE 0.09936440832752241 Lambda1 0.8196919\n",
      "80 Train Loss 31.930256 Test MSE 34.7881450227435 Test RE 0.09928983901933273 Lambda1 0.8229816\n",
      "81 Train Loss 31.854397 Test MSE 34.96974490487728 Test RE 0.09954865639861388 Lambda1 0.8162154\n",
      "82 Train Loss 31.78018 Test MSE 35.006919842718894 Test RE 0.09960155544214064 Lambda1 0.81540334\n",
      "83 Train Loss 31.417902 Test MSE 34.71423634519259 Test RE 0.0991843105052181 Lambda1 0.82013535\n",
      "84 Train Loss 31.238894 Test MSE 34.51462563384421 Test RE 0.09889873900030265 Lambda1 0.8199148\n",
      "85 Train Loss 31.09102 Test MSE 34.28160333424277 Test RE 0.09856432075575756 Lambda1 0.8222436\n",
      "86 Train Loss 30.82111 Test MSE 33.7772492394658 Test RE 0.09783659056286906 Lambda1 0.8296162\n",
      "87 Train Loss 30.685019 Test MSE 33.37676689211862 Test RE 0.09725485785051292 Lambda1 0.8363692\n",
      "88 Train Loss 30.323702 Test MSE 32.92237618204023 Test RE 0.09659057627199012 Lambda1 0.8441061\n",
      "89 Train Loss 30.082617 Test MSE 32.80848702713261 Test RE 0.09642336250210044 Lambda1 0.84718025\n",
      "90 Train Loss 29.97525 Test MSE 32.843447511345914 Test RE 0.09647472283894988 Lambda1 0.8454017\n",
      "91 Train Loss 29.917744 Test MSE 32.852297611112455 Test RE 0.09648772015617545 Lambda1 0.8446962\n",
      "92 Train Loss 29.868893 Test MSE 32.77367762063284 Test RE 0.09637219691837866 Lambda1 0.8471329\n",
      "93 Train Loss 29.732754 Test MSE 32.61714937515816 Test RE 0.09614178295933694 Lambda1 0.85733134\n",
      "94 Train Loss 29.600441 Test MSE 32.44049878682307 Test RE 0.09588108328928716 Lambda1 0.8647878\n",
      "95 Train Loss 29.335867 Test MSE 32.13077889785639 Test RE 0.09542228179182796 Lambda1 0.8792327\n",
      "96 Train Loss 29.200129 Test MSE 31.958350334078784 Test RE 0.09516589739502146 Lambda1 0.8842982\n",
      "97 Train Loss 29.016224 Test MSE 31.88483161511554 Test RE 0.0950563719784312 Lambda1 0.8885437\n",
      "98 Train Loss 28.79641 Test MSE 31.641751399883603 Test RE 0.09469333837047067 Lambda1 0.8969599\n",
      "99 Train Loss 28.73549 Test MSE 31.61158784393963 Test RE 0.09464819281561604 Lambda1 0.8973841\n",
      "100 Train Loss 28.677856 Test MSE 31.51046758450194 Test RE 0.09449668957542463 Lambda1 0.8985035\n",
      "101 Train Loss 28.536627 Test MSE 31.470591507007565 Test RE 0.09443687849519367 Lambda1 0.89867085\n",
      "102 Train Loss 28.487978 Test MSE 31.461537677146225 Test RE 0.09442329316213101 Lambda1 0.89703965\n",
      "103 Train Loss 28.448177 Test MSE 31.433550981991996 Test RE 0.09438128657154549 Lambda1 0.8971078\n",
      "104 Train Loss 28.401016 Test MSE 31.308238621236587 Test RE 0.09419296943146102 Lambda1 0.90016615\n",
      "105 Train Loss 28.365393 Test MSE 31.224183471183306 Test RE 0.09406644162665312 Lambda1 0.9037655\n",
      "106 Train Loss 28.31644 Test MSE 31.149668188143703 Test RE 0.09395413165331147 Lambda1 0.9069298\n",
      "107 Train Loss 28.281694 Test MSE 31.044559426156415 Test RE 0.09379548233537842 Lambda1 0.91242504\n",
      "108 Train Loss 28.223385 Test MSE 31.008309015343897 Test RE 0.09374070433268739 Lambda1 0.916136\n",
      "109 Train Loss 28.197412 Test MSE 30.970671291663802 Test RE 0.09368379606981798 Lambda1 0.91640747\n",
      "110 Train Loss 28.175375 Test MSE 30.909228629510313 Test RE 0.09359082037141886 Lambda1 0.9190265\n",
      "111 Train Loss 28.157293 Test MSE 30.80657241094947 Test RE 0.0934352734570048 Lambda1 0.9220459\n",
      "112 Train Loss 28.133024 Test MSE 30.782647968643623 Test RE 0.09339898540539462 Lambda1 0.9242344\n",
      "113 Train Loss 28.104355 Test MSE 30.693162869622146 Test RE 0.0932631312652542 Lambda1 0.92764884\n",
      "114 Train Loss 28.085527 Test MSE 30.667698195392674 Test RE 0.09322443521983999 Lambda1 0.92666787\n",
      "115 Train Loss 28.018518 Test MSE 30.741591312768858 Test RE 0.09333667872070693 Lambda1 0.9181133\n",
      "116 Train Loss 27.982992 Test MSE 30.79816164554966 Test RE 0.09342251780537487 Lambda1 0.9137367\n",
      "117 Train Loss 27.928524 Test MSE 30.763768750211668 Test RE 0.09337033987930747 Lambda1 0.9112324\n",
      "118 Train Loss 27.86506 Test MSE 30.726445994183678 Test RE 0.09331368401179493 Lambda1 0.9108694\n",
      "119 Train Loss 27.792358 Test MSE 30.760339033350228 Test RE 0.09336513501059476 Lambda1 0.90994745\n",
      "120 Train Loss 27.634214 Test MSE 30.616146082829164 Test RE 0.0931460475638451 Lambda1 0.9174306\n",
      "121 Train Loss 27.500494 Test MSE 30.507851054385636 Test RE 0.09298116414286495 Lambda1 0.92083067\n",
      "122 Train Loss 27.402418 Test MSE 30.472100397344004 Test RE 0.09292666813584931 Lambda1 0.92705977\n",
      "123 Train Loss 27.359468 Test MSE 30.417271293275267 Test RE 0.09284302802111877 Lambda1 0.929321\n",
      "124 Train Loss 27.342447 Test MSE 30.373981578559395 Test RE 0.09277693762161787 Lambda1 0.93243116\n",
      "125 Train Loss 27.317396 Test MSE 30.29083789429132 Test RE 0.09264986994617574 Lambda1 0.9366392\n",
      "126 Train Loss 27.305817 Test MSE 30.297988054576003 Test RE 0.09266080431408341 Lambda1 0.9376921\n",
      "127 Train Loss 27.29584 Test MSE 30.284394011704915 Test RE 0.09264001454632065 Lambda1 0.9396453\n",
      "128 Train Loss 27.283873 Test MSE 30.269714714856814 Test RE 0.09261755982751846 Lambda1 0.9402712\n",
      "129 Train Loss 27.258041 Test MSE 30.209065537762502 Test RE 0.09252472784439332 Lambda1 0.9434851\n",
      "130 Train Loss 27.25064 Test MSE 30.202220558126456 Test RE 0.0925142448031806 Lambda1 0.94340444\n",
      "131 Train Loss 27.239363 Test MSE 30.202196336688964 Test RE 0.09251420770609953 Lambda1 0.9429826\n",
      "132 Train Loss 27.22349 Test MSE 30.26679656509069 Test RE 0.09261309532517188 Lambda1 0.9403617\n",
      "133 Train Loss 27.2011 Test MSE 30.266320433757233 Test RE 0.0926123668673402 Lambda1 0.941182\n",
      "134 Train Loss 27.184702 Test MSE 30.307126988458254 Test RE 0.09267477813165992 Lambda1 0.9414695\n",
      "135 Train Loss 27.17581 Test MSE 30.32002865303245 Test RE 0.09269450173796313 Lambda1 0.941894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136 Train Loss 27.141432 Test MSE 30.301535209776954 Test RE 0.09266622831507236 Lambda1 0.9470441\n",
      "137 Train Loss 27.125088 Test MSE 30.299469757054442 Test RE 0.09266307004315776 Lambda1 0.94888526\n",
      "138 Train Loss 27.04809 Test MSE 30.24617896917992 Test RE 0.09258154615443645 Lambda1 0.96203524\n",
      "139 Train Loss 26.988073 Test MSE 30.262192872795126 Test RE 0.09260605165924322 Lambda1 0.9661244\n",
      "140 Train Loss 26.950977 Test MSE 30.24135257110953 Test RE 0.09257415921767613 Lambda1 0.96865946\n",
      "141 Train Loss 26.933704 Test MSE 30.213931167152204 Test RE 0.09253217880159756 Lambda1 0.9705746\n",
      "142 Train Loss 26.907955 Test MSE 30.20547541231797 Test RE 0.092519229739048 Lambda1 0.9738838\n",
      "143 Train Loss 26.879631 Test MSE 30.16600819613788 Test RE 0.09245876603258146 Lambda1 0.9773\n",
      "144 Train Loss 26.869534 Test MSE 30.16312941963942 Test RE 0.09245435420468093 Lambda1 0.97944033\n",
      "145 Train Loss 26.853321 Test MSE 30.161201926683354 Test RE 0.09245140013514414 Lambda1 0.9828474\n",
      "146 Train Loss 26.842218 Test MSE 30.171277280595387 Test RE 0.09246684054764778 Lambda1 0.98516935\n",
      "147 Train Loss 26.833954 Test MSE 30.19483674829399 Test RE 0.09250293521510607 Lambda1 0.9858989\n",
      "148 Train Loss 26.823915 Test MSE 30.192261572015507 Test RE 0.09249899055983864 Lambda1 0.9844793\n",
      "149 Train Loss 26.818787 Test MSE 30.202618245276867 Test RE 0.092514853890933 Lambda1 0.9844085\n",
      "150 Train Loss 26.809254 Test MSE 30.163050120005586 Test RE 0.09245423267217759 Lambda1 0.9832264\n",
      "151 Train Loss 26.802177 Test MSE 30.150996780356056 Test RE 0.09243575818742313 Lambda1 0.9832368\n",
      "152 Train Loss 26.794693 Test MSE 30.12771434619308 Test RE 0.0924000621025456 Lambda1 0.9839526\n",
      "153 Train Loss 26.783592 Test MSE 30.119506227610348 Test RE 0.09238747431837775 Lambda1 0.9857403\n",
      "154 Train Loss 26.775846 Test MSE 30.114674651861534 Test RE 0.09238006392161191 Lambda1 0.9875164\n",
      "155 Train Loss 26.769249 Test MSE 30.09917306793965 Test RE 0.09235628445755412 Lambda1 0.9874143\n",
      "156 Train Loss 26.756493 Test MSE 30.106866145539573 Test RE 0.09236808642081333 Lambda1 0.99002993\n",
      "157 Train Loss 26.733894 Test MSE 30.089481060456443 Test RE 0.09234141378540645 Lambda1 0.99201113\n",
      "158 Train Loss 26.728167 Test MSE 30.11899774164163 Test RE 0.09238669445943358 Lambda1 0.9928489\n",
      "159 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "160 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "161 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "162 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "163 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "164 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "165 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "166 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "167 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "168 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "169 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "170 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "171 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "172 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "173 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "174 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "175 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "176 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "177 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "178 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "179 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "180 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "181 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "182 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "183 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "184 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "185 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "186 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "187 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "188 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "189 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "190 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "191 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "192 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "193 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "194 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "195 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "196 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "197 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "198 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "199 Train Loss 26.72766 Test MSE 30.121469255021843 Test RE 0.09239048492864951 Lambda1 0.99287504\n",
      "Training time: 499.82\n",
      "Training time: 499.82\n",
      "inv_HT_rowdy\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 1300.1349 Test MSE 1285.3535569510127 Test RE 0.6035322371096487 Lambda1 -0.0044940286\n",
      "1 Train Loss 858.5078 Test MSE 858.9632185857216 Test RE 0.4933742217512554 Lambda1 0.007656871\n",
      "2 Train Loss 853.00366 Test MSE 855.9046441608854 Test RE 0.49249504120986587 Lambda1 0.0102173425\n",
      "3 Train Loss 847.4623 Test MSE 848.6886941900023 Test RE 0.49041458650090897 Lambda1 0.015523823\n",
      "4 Train Loss 836.698 Test MSE 835.1629908245443 Test RE 0.4864909784563528 Lambda1 0.016184712\n",
      "5 Train Loss 827.58545 Test MSE 828.764559890852 Test RE 0.48462381965728735 Lambda1 0.01637129\n",
      "6 Train Loss 813.63824 Test MSE 811.564782445767 Test RE 0.47956863013368983 Lambda1 0.018091561\n",
      "7 Train Loss 805.0005 Test MSE 803.2425169216763 Test RE 0.47710340361552794 Lambda1 0.019254765\n",
      "8 Train Loss 785.61285 Test MSE 785.2564286001341 Test RE 0.4717315472831744 Lambda1 0.021675063\n",
      "9 Train Loss 773.88696 Test MSE 768.3948161960654 Test RE 0.46663937720531373 Lambda1 0.023571275\n",
      "10 Train Loss 742.4436 Test MSE 732.9774695454333 Test RE 0.4557581917135974 Lambda1 0.025349421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Train Loss 702.945 Test MSE 685.0419548424828 Test RE 0.44060331036606226 Lambda1 0.028482227\n",
      "12 Train Loss 645.885 Test MSE 621.9410653763584 Test RE 0.4198206408673402 Lambda1 0.030476581\n",
      "13 Train Loss 583.90063 Test MSE 556.5811965659839 Test RE 0.397148965558408 Lambda1 0.03162663\n",
      "14 Train Loss 510.7876 Test MSE 497.68174386871567 Test RE 0.3755476300819883 Lambda1 0.03025778\n",
      "15 Train Loss 457.54333 Test MSE 423.91777945908177 Test RE 0.3466011397041965 Lambda1 0.03211805\n",
      "16 Train Loss 422.8244 Test MSE 391.6198569489633 Test RE 0.33313596627747605 Lambda1 0.032390006\n",
      "17 Train Loss 385.66812 Test MSE 365.8870829292979 Test RE 0.322005069785118 Lambda1 0.03415999\n",
      "18 Train Loss 351.3985 Test MSE 342.58654141714874 Test RE 0.31158340531738093 Lambda1 0.03542293\n",
      "19 Train Loss 319.4826 Test MSE 313.7402377888007 Test RE 0.29817709638497913 Lambda1 0.037045896\n",
      "20 Train Loss 286.06833 Test MSE 283.7599639631246 Test RE 0.28357290404156815 Lambda1 0.04179825\n",
      "21 Train Loss 272.30795 Test MSE 274.8672795122381 Test RE 0.27909412376539494 Lambda1 0.044435337\n",
      "22 Train Loss 266.56467 Test MSE 268.1327391321161 Test RE 0.2756538693353191 Lambda1 0.047696847\n",
      "23 Train Loss 261.48526 Test MSE 263.87413520114416 Test RE 0.273456078895019 Lambda1 0.05089918\n",
      "24 Train Loss 258.59012 Test MSE 261.6124376485019 Test RE 0.2722816441401099 Lambda1 0.05341659\n",
      "25 Train Loss 255.1691 Test MSE 258.5887064646679 Test RE 0.2707035477212148 Lambda1 0.056864806\n",
      "26 Train Loss 253.14981 Test MSE 258.6512767430442 Test RE 0.27073629658345083 Lambda1 0.05841193\n",
      "27 Train Loss 250.40047 Test MSE 256.0937211413316 Test RE 0.2693944448559883 Lambda1 0.06603583\n",
      "28 Train Loss 248.00938 Test MSE 253.8185416013868 Test RE 0.2681951024240638 Lambda1 0.06842739\n",
      "29 Train Loss 246.58015 Test MSE 251.0691115914031 Test RE 0.26673856691709263 Lambda1 0.07431904\n",
      "30 Train Loss 243.70325 Test MSE 247.38637808798504 Test RE 0.2647750518540571 Lambda1 0.07864755\n",
      "31 Train Loss 240.52876 Test MSE 242.07937997374592 Test RE 0.2619196428096746 Lambda1 0.0909989\n",
      "32 Train Loss 238.48195 Test MSE 235.2115468196597 Test RE 0.258177558565816 Lambda1 0.10948516\n",
      "33 Train Loss 232.75598 Test MSE 226.73726786389318 Test RE 0.25348404290373905 Lambda1 0.13392146\n",
      "34 Train Loss 228.56174 Test MSE 225.74770591064822 Test RE 0.25293029086167 Lambda1 0.13919035\n",
      "35 Train Loss 222.94176 Test MSE 221.4898097977241 Test RE 0.25053363851223026 Lambda1 0.14547148\n",
      "36 Train Loss 220.64954 Test MSE 221.2632993932841 Test RE 0.25040549945254975 Lambda1 0.14632191\n",
      "37 Train Loss 217.0138 Test MSE 218.02292894950892 Test RE 0.248565159752989 Lambda1 0.14899993\n",
      "38 Train Loss 213.56274 Test MSE 215.8578670791258 Test RE 0.24732790075189087 Lambda1 0.15095982\n",
      "39 Train Loss 211.0948 Test MSE 214.06924500093027 Test RE 0.24630107610876348 Lambda1 0.15665579\n",
      "40 Train Loss 208.1545 Test MSE 210.79730412670955 Test RE 0.2444115340206122 Lambda1 0.1658271\n",
      "41 Train Loss 203.93326 Test MSE 204.16436461405385 Test RE 0.24053547759444435 Lambda1 0.18405303\n",
      "42 Train Loss 200.78842 Test MSE 200.31971952525294 Test RE 0.2382599368623675 Lambda1 0.18904811\n",
      "43 Train Loss 193.47647 Test MSE 192.5250780255287 Test RE 0.2335784782283316 Lambda1 0.20809789\n",
      "44 Train Loss 187.20258 Test MSE 186.2886858151289 Test RE 0.22976422578727226 Lambda1 0.2195524\n",
      "45 Train Loss 181.28291 Test MSE 178.0964116411706 Test RE 0.22465534471576115 Lambda1 0.23677118\n",
      "46 Train Loss 175.6051 Test MSE 171.263629974031 Test RE 0.22030367556251446 Lambda1 0.23960835\n",
      "47 Train Loss 171.1628 Test MSE 162.3626922072761 Test RE 0.2145024675994589 Lambda1 0.2433339\n",
      "48 Train Loss 165.40005 Test MSE 156.48800769817956 Test RE 0.21058609985770751 Lambda1 0.24523912\n",
      "49 Train Loss 162.77347 Test MSE 153.86791700941492 Test RE 0.2088157286189942 Lambda1 0.2461182\n",
      "50 Train Loss 158.85776 Test MSE 147.87930283324533 Test RE 0.2047117956952701 Lambda1 0.25174585\n",
      "51 Train Loss 145.36652 Test MSE 136.09292682147435 Test RE 0.1963843825973746 Lambda1 0.26723796\n",
      "52 Train Loss 140.62796 Test MSE 130.7911899254284 Test RE 0.1925211362131141 Lambda1 0.27327102\n",
      "53 Train Loss 136.38322 Test MSE 129.78604215004322 Test RE 0.19177993407561525 Lambda1 0.27776468\n",
      "54 Train Loss 133.01602 Test MSE 121.60222217115069 Test RE 0.18563502733058695 Lambda1 0.28831908\n",
      "55 Train Loss 131.73265 Test MSE 117.40581072097955 Test RE 0.18240383563256626 Lambda1 0.2940828\n",
      "56 Train Loss 128.60846 Test MSE 115.52195428336216 Test RE 0.1809345206819095 Lambda1 0.29995522\n",
      "57 Train Loss 126.87193 Test MSE 116.81176972718234 Test RE 0.1819417939115416 Lambda1 0.30008137\n",
      "58 Train Loss 122.943344 Test MSE 115.24739037122961 Test RE 0.18071937697582496 Lambda1 0.3078814\n",
      "59 Train Loss 119.239586 Test MSE 110.1495883744221 Test RE 0.17667724149952335 Lambda1 0.314757\n",
      "60 Train Loss 117.55534 Test MSE 106.28510668965635 Test RE 0.1735503033215867 Lambda1 0.322509\n",
      "61 Train Loss 111.50489 Test MSE 102.83251110238254 Test RE 0.17070820320757504 Lambda1 0.32613635\n",
      "62 Train Loss 108.864006 Test MSE 99.59832538938423 Test RE 0.1680022850892847 Lambda1 0.3325199\n",
      "63 Train Loss 106.852936 Test MSE 95.81844655700719 Test RE 0.16478350395451283 Lambda1 0.33632383\n",
      "64 Train Loss 103.604965 Test MSE 96.14028923471804 Test RE 0.1650600159763454 Lambda1 0.33850276\n",
      "65 Train Loss 101.52714 Test MSE 94.82206074161795 Test RE 0.16392449913693777 Lambda1 0.3435033\n",
      "66 Train Loss 100.40116 Test MSE 94.65565737065603 Test RE 0.163780600307337 Lambda1 0.34613428\n",
      "67 Train Loss 97.81613 Test MSE 91.58766944511683 Test RE 0.1611045012117248 Lambda1 0.35168406\n",
      "68 Train Loss 94.18722 Test MSE 88.14858645737633 Test RE 0.15805085400311478 Lambda1 0.3587417\n",
      "69 Train Loss 92.44833 Test MSE 86.79806380303124 Test RE 0.15683543375835557 Lambda1 0.36190584\n",
      "70 Train Loss 90.72545 Test MSE 86.11008439166872 Test RE 0.1562126421616006 Lambda1 0.36751273\n",
      "71 Train Loss 87.47695 Test MSE 80.71234569016632 Test RE 0.15123738349769905 Lambda1 0.3783398\n",
      "72 Train Loss 85.16577 Test MSE 81.00726231218113 Test RE 0.1515134363661255 Lambda1 0.37606472\n",
      "73 Train Loss 83.42245 Test MSE 81.75043685224146 Test RE 0.15220685475851237 Lambda1 0.3727472\n",
      "74 Train Loss 82.64679 Test MSE 80.33077019662822 Test RE 0.15087946522587134 Lambda1 0.37602296\n",
      "75 Train Loss 82.0558 Test MSE 80.09452207186709 Test RE 0.15065743799578943 Lambda1 0.37796736\n",
      "76 Train Loss 81.066284 Test MSE 78.57475921802147 Test RE 0.14922125906468117 Lambda1 0.3837548\n",
      "77 Train Loss 79.38745 Test MSE 77.23278872062231 Test RE 0.14794150380868817 Lambda1 0.38688323\n",
      "78 Train Loss 78.357704 Test MSE 76.44529073929317 Test RE 0.14718533443208687 Lambda1 0.39083305\n",
      "79 Train Loss 77.35613 Test MSE 75.54279428813732 Test RE 0.14631393591168687 Lambda1 0.3954305\n",
      "80 Train Loss 75.97477 Test MSE 73.9439014754623 Test RE 0.14475725907679698 Lambda1 0.40177315\n",
      "81 Train Loss 74.79783 Test MSE 71.77873047796898 Test RE 0.1426221756167092 Lambda1 0.4104259\n",
      "82 Train Loss 73.983864 Test MSE 70.60396049669669 Test RE 0.14145024443241105 Lambda1 0.41692856\n",
      "83 Train Loss 72.65895 Test MSE 69.97593227521216 Test RE 0.14081973322254215 Lambda1 0.41954425\n",
      "84 Train Loss 71.85883 Test MSE 69.60825211845773 Test RE 0.1404492857509073 Lambda1 0.42136133\n",
      "85 Train Loss 71.06363 Test MSE 68.804644440587 Test RE 0.1396362085159493 Lambda1 0.4253221\n",
      "86 Train Loss 70.475784 Test MSE 67.91905078246772 Test RE 0.13873466010031746 Lambda1 0.42602894\n",
      "87 Train Loss 69.02991 Test MSE 66.86746800606143 Test RE 0.13765646410801918 Lambda1 0.4285641\n",
      "88 Train Loss 66.63856 Test MSE 65.75116298112174 Test RE 0.1365025894673029 Lambda1 0.43141207\n",
      "89 Train Loss 65.731964 Test MSE 65.60335916560398 Test RE 0.13634907921958322 Lambda1 0.43185368\n",
      "90 Train Loss 64.934265 Test MSE 65.12057957624626 Test RE 0.1358464517131048 Lambda1 0.43300945\n",
      "91 Train Loss 64.18686 Test MSE 64.4988354439581 Test RE 0.13519639374116596 Lambda1 0.4346643\n",
      "92 Train Loss 63.631016 Test MSE 63.75099039131284 Test RE 0.1344103273073992 Lambda1 0.43789443\n",
      "93 Train Loss 63.283173 Test MSE 63.320685398808926 Test RE 0.1339559393617544 Lambda1 0.4392164\n",
      "94 Train Loss 62.38452 Test MSE 62.03601614421297 Test RE 0.13259010691127882 Lambda1 0.4447648\n",
      "95 Train Loss 61.110085 Test MSE 60.64620941564446 Test RE 0.13109647106777308 Lambda1 0.45004416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 60.79376 Test MSE 60.043222148918474 Test RE 0.1304431163246734 Lambda1 0.45243514\n",
      "97 Train Loss 60.483963 Test MSE 60.092776396456806 Test RE 0.13049693320101954 Lambda1 0.45200577\n",
      "98 Train Loss 60.227673 Test MSE 59.95014598325096 Test RE 0.13034197373539957 Lambda1 0.45247087\n",
      "99 Train Loss 59.605225 Test MSE 59.48910757084541 Test RE 0.12983981785537155 Lambda1 0.45392966\n",
      "100 Train Loss 59.434288 Test MSE 59.374442582437446 Test RE 0.12971462466524478 Lambda1 0.4549001\n",
      "101 Train Loss 59.339867 Test MSE 59.50397340867049 Test RE 0.12985603979234017 Lambda1 0.45466232\n",
      "102 Train Loss 59.20995 Test MSE 59.571926141963466 Test RE 0.12993016555678596 Lambda1 0.4549212\n",
      "103 Train Loss 59.093178 Test MSE 59.29065843841546 Test RE 0.1296230712540637 Lambda1 0.45659977\n",
      "104 Train Loss 58.916687 Test MSE 58.724848733119785 Test RE 0.12900309325988127 Lambda1 0.4598168\n",
      "105 Train Loss 58.05327 Test MSE 57.351707538802465 Test RE 0.12748595657892403 Lambda1 0.46828657\n",
      "106 Train Loss 57.78871 Test MSE 56.97608306280698 Test RE 0.12706778670977806 Lambda1 0.4709432\n",
      "107 Train Loss 56.885086 Test MSE 56.4867708592565 Test RE 0.12652097931624426 Lambda1 0.47273776\n",
      "108 Train Loss 56.22195 Test MSE 55.783823244503836 Test RE 0.12573127183513172 Lambda1 0.4760708\n",
      "109 Train Loss 56.05404 Test MSE 55.239594533995444 Test RE 0.125116449508637 Lambda1 0.47933045\n",
      "110 Train Loss 55.753517 Test MSE 55.19183760246227 Test RE 0.125062353621716 Lambda1 0.479433\n",
      "111 Train Loss 55.200615 Test MSE 55.43263032867529 Test RE 0.12533486974131322 Lambda1 0.4780205\n",
      "112 Train Loss 54.43719 Test MSE 54.83480320455638 Test RE 0.12465718493189858 Lambda1 0.48243767\n",
      "113 Train Loss 53.55055 Test MSE 53.79853012308674 Test RE 0.1234736753540124 Lambda1 0.49062213\n",
      "114 Train Loss 53.16624 Test MSE 53.757203937744265 Test RE 0.12342624212222635 Lambda1 0.49132013\n",
      "115 Train Loss 52.81032 Test MSE 53.728399136825814 Test RE 0.1233931698597594 Lambda1 0.49269652\n",
      "116 Train Loss 52.584938 Test MSE 53.395940345519854 Test RE 0.12301081339391853 Lambda1 0.4965127\n",
      "117 Train Loss 52.22987 Test MSE 52.93512745132734 Test RE 0.12247886472370201 Lambda1 0.50122267\n",
      "118 Train Loss 51.897816 Test MSE 52.764169500614656 Test RE 0.12228092745703945 Lambda1 0.5020169\n",
      "119 Train Loss 51.68116 Test MSE 52.8802588538608 Test RE 0.12241537204710903 Lambda1 0.5009587\n",
      "120 Train Loss 51.457478 Test MSE 52.59383398695865 Test RE 0.12208339168547072 Lambda1 0.50418663\n",
      "121 Train Loss 51.167698 Test MSE 52.025900404880524 Test RE 0.12142244486845466 Lambda1 0.511704\n",
      "122 Train Loss 51.008095 Test MSE 51.90830299617331 Test RE 0.1212851378463114 Lambda1 0.51292634\n",
      "123 Train Loss 50.711983 Test MSE 51.73760310234037 Test RE 0.121085551193543 Lambda1 0.5144931\n",
      "124 Train Loss 50.440365 Test MSE 51.71099966025166 Test RE 0.12105441613452501 Lambda1 0.51309395\n",
      "125 Train Loss 49.841976 Test MSE 51.260134890838046 Test RE 0.12052552807106393 Lambda1 0.5167689\n",
      "126 Train Loss 49.477505 Test MSE 50.74887441925662 Test RE 0.11992297058303403 Lambda1 0.5215208\n",
      "127 Train Loss 49.101055 Test MSE 50.23233659961237 Test RE 0.11931110301947341 Lambda1 0.52715254\n",
      "128 Train Loss 49.02206 Test MSE 50.10642694553287 Test RE 0.11916147982713571 Lambda1 0.5288999\n",
      "129 Train Loss 48.929016 Test MSE 50.2553423937872 Test RE 0.11933842140300921 Lambda1 0.52662694\n",
      "130 Train Loss 48.738 Test MSE 50.25663625145933 Test RE 0.11933995761718008 Lambda1 0.5258597\n",
      "131 Train Loss 48.178833 Test MSE 49.55687408496753 Test RE 0.11850621378453431 Lambda1 0.5328348\n",
      "132 Train Loss 48.018177 Test MSE 49.16236358458153 Test RE 0.11803357135040525 Lambda1 0.5373033\n",
      "133 Train Loss 47.785843 Test MSE 48.71988707976156 Test RE 0.1175012014123525 Lambda1 0.5437746\n",
      "134 Train Loss 47.601418 Test MSE 48.834311756353834 Test RE 0.11763910353713428 Lambda1 0.54398614\n",
      "135 Train Loss 47.50237 Test MSE 48.826350710854314 Test RE 0.11762951429143932 Lambda1 0.5433056\n",
      "136 Train Loss 47.250145 Test MSE 48.69393646342154 Test RE 0.11746990377466572 Lambda1 0.5438455\n",
      "137 Train Loss 46.862843 Test MSE 48.417922774431084 Test RE 0.11713650108548483 Lambda1 0.54492337\n",
      "138 Train Loss 46.46437 Test MSE 47.95644634227354 Test RE 0.1165769442959245 Lambda1 0.5470077\n",
      "139 Train Loss 46.155502 Test MSE 47.67660380390527 Test RE 0.11623631314228171 Lambda1 0.5488886\n",
      "140 Train Loss 46.0156 Test MSE 47.73636609458084 Test RE 0.1163091410314327 Lambda1 0.54725426\n",
      "141 Train Loss 45.804413 Test MSE 48.1392425237922 Test RE 0.11679891187043608 Lambda1 0.5436844\n",
      "142 Train Loss 45.427944 Test MSE 48.21660121478064 Test RE 0.11689272082385213 Lambda1 0.54163027\n",
      "143 Train Loss 45.20595 Test MSE 47.98029938598577 Test RE 0.1166059327789273 Lambda1 0.54379123\n",
      "144 Train Loss 45.018894 Test MSE 47.721886150448036 Test RE 0.11629149957969087 Lambda1 0.54628867\n",
      "145 Train Loss 44.7847 Test MSE 47.27137459447198 Test RE 0.11574128140539583 Lambda1 0.5502991\n",
      "146 Train Loss 44.57943 Test MSE 47.168073410911056 Test RE 0.11561474869376569 Lambda1 0.5505255\n",
      "147 Train Loss 43.82576 Test MSE 46.56600907294279 Test RE 0.11487451210190311 Lambda1 0.5547165\n",
      "148 Train Loss 43.1914 Test MSE 46.20100672857872 Test RE 0.11442341101443007 Lambda1 0.55691576\n",
      "149 Train Loss 43.01455 Test MSE 46.21285952600568 Test RE 0.11443808764848716 Lambda1 0.5565832\n",
      "150 Train Loss 42.8158 Test MSE 46.04034532050345 Test RE 0.11422428725809836 Lambda1 0.55833894\n",
      "151 Train Loss 42.64728 Test MSE 45.87433395348174 Test RE 0.11401816744612311 Lambda1 0.56014144\n",
      "152 Train Loss 42.47559 Test MSE 45.50431763532528 Test RE 0.11355740872125095 Lambda1 0.56411844\n",
      "153 Train Loss 42.413036 Test MSE 45.33753541208102 Test RE 0.11334911262908266 Lambda1 0.56563354\n",
      "154 Train Loss 42.36465 Test MSE 45.27312507311617 Test RE 0.1132685673414306 Lambda1 0.5664477\n",
      "155 Train Loss 42.3046 Test MSE 45.33346355954859 Test RE 0.11334402246228656 Lambda1 0.56649333\n",
      "156 Train Loss 42.243996 Test MSE 45.42136157856215 Test RE 0.11345385182128 Lambda1 0.5663545\n",
      "157 Train Loss 42.187412 Test MSE 45.49307105834753 Test RE 0.11354337476659783 Lambda1 0.5664559\n",
      "158 Train Loss 42.16167 Test MSE 45.52554009436189 Test RE 0.11358388628045199 Lambda1 0.5663365\n",
      "159 Train Loss 42.13399 Test MSE 45.52832278129031 Test RE 0.1135873575579515 Lambda1 0.5659776\n",
      "160 Train Loss 42.115112 Test MSE 45.601581380548545 Test RE 0.11367870625592787 Lambda1 0.5654988\n",
      "161 Train Loss 42.04081 Test MSE 45.62241122413946 Test RE 0.11370466631321266 Lambda1 0.5656993\n",
      "162 Train Loss 41.98295 Test MSE 45.616595664110896 Test RE 0.11369741902652927 Lambda1 0.56621146\n",
      "163 Train Loss 41.89593 Test MSE 45.42911930211564 Test RE 0.11346354006058176 Lambda1 0.5675449\n",
      "164 Train Loss 41.83527 Test MSE 45.20859843950289 Test RE 0.11318781915510376 Lambda1 0.5689943\n",
      "165 Train Loss 41.791515 Test MSE 45.046051816501766 Test RE 0.11298415363703117 Lambda1 0.570927\n",
      "166 Train Loss 41.744793 Test MSE 44.930635529872085 Test RE 0.11283931769127344 Lambda1 0.5723567\n",
      "167 Train Loss 41.670086 Test MSE 44.94174786238526 Test RE 0.11285327064880231 Lambda1 0.5719703\n",
      "168 Train Loss 41.569496 Test MSE 44.92543890463678 Test RE 0.11283279207022415 Lambda1 0.5714856\n",
      "169 Train Loss 41.535595 Test MSE 45.01584725138399 Test RE 0.11294626785852402 Lambda1 0.57073545\n",
      "170 Train Loss 41.403866 Test MSE 44.90378018261152 Test RE 0.11280559023589186 Lambda1 0.5723323\n",
      "171 Train Loss 41.06457 Test MSE 44.614035041006424 Test RE 0.11244105780899182 Lambda1 0.57637453\n",
      "172 Train Loss 40.716454 Test MSE 44.4005684240914 Test RE 0.11217173457451672 Lambda1 0.57904536\n",
      "173 Train Loss 40.494843 Test MSE 44.396860380980655 Test RE 0.11216705055439233 Lambda1 0.57832134\n",
      "174 Train Loss 40.185642 Test MSE 44.06634536176897 Test RE 0.11174875351251584 Lambda1 0.5795657\n",
      "175 Train Loss 40.00387 Test MSE 43.80842793045706 Test RE 0.11142124452190771 Lambda1 0.5820134\n",
      "176 Train Loss 39.8548 Test MSE 43.45730252552568 Test RE 0.11097382447335863 Lambda1 0.5834549\n",
      "177 Train Loss 39.681797 Test MSE 43.30867929344225 Test RE 0.11078389765591733 Lambda1 0.584367\n",
      "178 Train Loss 39.57992 Test MSE 43.20517876205128 Test RE 0.11065144093783262 Lambda1 0.586282\n",
      "179 Train Loss 39.553776 Test MSE 43.09365312954553 Test RE 0.11050853624203603 Lambda1 0.5877865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 Train Loss 39.524822 Test MSE 42.93981507900631 Test RE 0.11031111021822446 Lambda1 0.5890622\n",
      "181 Train Loss 39.502644 Test MSE 42.890026070617175 Test RE 0.11024713842666098 Lambda1 0.5893335\n",
      "182 Train Loss 39.467785 Test MSE 42.92505512973639 Test RE 0.11029214965297453 Lambda1 0.58893406\n",
      "183 Train Loss 39.427315 Test MSE 42.95248362212223 Test RE 0.11032738157376953 Lambda1 0.588364\n",
      "184 Train Loss 39.34095 Test MSE 42.8593726933331 Test RE 0.11020773472987548 Lambda1 0.5898625\n",
      "185 Train Loss 39.28148 Test MSE 42.66792385346338 Test RE 0.1099613153776823 Lambda1 0.5924377\n",
      "186 Train Loss 39.217834 Test MSE 42.75330404188528 Test RE 0.11007127884641706 Lambda1 0.59242237\n",
      "187 Train Loss 39.15738 Test MSE 42.840818173346776 Test RE 0.11018387678646095 Lambda1 0.5931202\n",
      "188 Train Loss 39.12554 Test MSE 42.68209082078843 Test RE 0.10997956900866233 Lambda1 0.5949627\n",
      "189 Train Loss 39.087635 Test MSE 42.672736453437196 Test RE 0.10996751658053192 Lambda1 0.5951165\n",
      "190 Train Loss 38.933907 Test MSE 42.44101183925435 Test RE 0.10966853331252868 Lambda1 0.5984207\n",
      "191 Train Loss 38.77033 Test MSE 42.06216633612383 Test RE 0.10917796343360121 Lambda1 0.60307866\n",
      "192 Train Loss 38.706627 Test MSE 42.054870138377204 Test RE 0.1091684938956829 Lambda1 0.6039409\n",
      "193 Train Loss 38.6525 Test MSE 42.01955770683868 Test RE 0.10912265123189493 Lambda1 0.6040261\n",
      "194 Train Loss 38.612614 Test MSE 41.936296920923624 Test RE 0.10901448561320634 Lambda1 0.6055026\n",
      "195 Train Loss 38.574883 Test MSE 41.83386623201872 Test RE 0.1088812686077903 Lambda1 0.6067922\n",
      "196 Train Loss 38.528854 Test MSE 41.62570842446116 Test RE 0.10861004397369638 Lambda1 0.6089022\n",
      "197 Train Loss 38.50391 Test MSE 41.603413053032185 Test RE 0.10858095347051698 Lambda1 0.6089541\n",
      "198 Train Loss 38.48609 Test MSE 41.50438761989883 Test RE 0.10845165299983933 Lambda1 0.60983825\n",
      "199 Train Loss 38.457508 Test MSE 41.31648810963615 Test RE 0.10820588225185127 Lambda1 0.6117812\n",
      "Training time: 584.03\n",
      "Training time: 584.03\n",
      "inv_HT_rowdy\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 924.6576 Test MSE 912.2497757997505 Test RE 0.5084474262457742 Lambda1 0.0023801394\n",
      "1 Train Loss 854.97253 Test MSE 857.779497675591 Test RE 0.49303414972329085 Lambda1 0.0021610735\n",
      "2 Train Loss 854.4948 Test MSE 857.7406124696411 Test RE 0.4930229743852323 Lambda1 0.0014854912\n",
      "3 Train Loss 854.25977 Test MSE 857.379779504776 Test RE 0.4929192614139067 Lambda1 -0.0006255284\n",
      "4 Train Loss 853.4838 Test MSE 855.9913835571401 Test RE 0.49251999588243095 Lambda1 -0.0034021146\n",
      "5 Train Loss 850.01776 Test MSE 850.204422141853 Test RE 0.49085232268459195 Lambda1 0.0023557064\n",
      "6 Train Loss 801.8973 Test MSE 788.4401488488033 Test RE 0.4726868671169999 Lambda1 -0.0071611074\n",
      "7 Train Loss 487.31644 Test MSE 450.2234676153711 Test RE 0.35719324312117157 Lambda1 0.035263337\n",
      "8 Train Loss 300.60114 Test MSE 282.9767720120472 Test RE 0.2831812958121109 Lambda1 0.017307464\n",
      "9 Train Loss 272.98297 Test MSE 277.5029824513012 Test RE 0.2804290483306378 Lambda1 0.011322968\n",
      "10 Train Loss 266.41415 Test MSE 271.35414171305393 Test RE 0.2773048069659536 Lambda1 0.019527476\n",
      "11 Train Loss 252.06454 Test MSE 250.78739961907655 Test RE 0.2665888779772498 Lambda1 0.06633859\n",
      "12 Train Loss 239.08496 Test MSE 237.89598009467738 Test RE 0.25964664925174125 Lambda1 0.105527565\n",
      "13 Train Loss 229.88602 Test MSE 234.22756074474077 Test RE 0.2576369614423043 Lambda1 0.117722146\n",
      "14 Train Loss 223.54013 Test MSE 222.72318668484925 Test RE 0.2512302245521921 Lambda1 0.16610023\n",
      "15 Train Loss 209.44846 Test MSE 209.00114199752144 Test RE 0.24336801519439977 Lambda1 0.2200851\n",
      "16 Train Loss 197.00916 Test MSE 192.1036949349849 Test RE 0.23332271950388647 Lambda1 0.29940298\n",
      "17 Train Loss 189.27122 Test MSE 179.72666398566392 Test RE 0.22568122329330448 Lambda1 0.35236794\n",
      "18 Train Loss 172.15677 Test MSE 160.02755256597945 Test RE 0.21295436788175043 Lambda1 0.38438827\n",
      "19 Train Loss 130.18576 Test MSE 110.84931990954014 Test RE 0.17723752921405786 Lambda1 0.5396483\n",
      "20 Train Loss 118.498024 Test MSE 98.76274297450041 Test RE 0.1672960712754758 Lambda1 0.58776045\n",
      "21 Train Loss 102.22464 Test MSE 84.57442981087213 Test RE 0.1548134574644131 Lambda1 0.5813861\n",
      "22 Train Loss 90.630516 Test MSE 77.55036056217368 Test RE 0.14824535052649632 Lambda1 0.59977984\n",
      "23 Train Loss 82.20072 Test MSE 66.63195514135342 Test RE 0.13741383143524555 Lambda1 0.5886279\n",
      "24 Train Loss 73.48946 Test MSE 62.21845792786123 Test RE 0.13278493096772617 Lambda1 0.5774244\n",
      "25 Train Loss 69.98067 Test MSE 61.686360296589065 Test RE 0.13221591768806318 Lambda1 0.5839025\n",
      "26 Train Loss 64.91778 Test MSE 59.79092804316124 Test RE 0.13016877500865753 Lambda1 0.6051183\n",
      "27 Train Loss 58.119415 Test MSE 54.04622349594863 Test RE 0.12375759100565659 Lambda1 0.6470849\n",
      "28 Train Loss 55.619205 Test MSE 52.14097162443623 Test RE 0.12155665216972066 Lambda1 0.6949855\n",
      "29 Train Loss 54.35038 Test MSE 51.1482876096201 Test RE 0.12039396565781886 Lambda1 0.69885033\n",
      "30 Train Loss 51.83381 Test MSE 50.918299769294585 Test RE 0.12012298547816523 Lambda1 0.70001775\n",
      "31 Train Loss 49.088818 Test MSE 48.64191801192908 Test RE 0.11740714200434588 Lambda1 0.709263\n",
      "32 Train Loss 48.3409 Test MSE 47.83665003289 Test RE 0.11643124730219753 Lambda1 0.7332295\n",
      "33 Train Loss 46.929073 Test MSE 47.2856028370689 Test RE 0.11575869861766862 Lambda1 0.7436068\n",
      "34 Train Loss 46.50024 Test MSE 47.39617962567607 Test RE 0.11589396971314495 Lambda1 0.7463168\n",
      "35 Train Loss 46.210346 Test MSE 47.438043148784026 Test RE 0.11594514112660123 Lambda1 0.7475081\n",
      "36 Train Loss 45.849354 Test MSE 47.41761484981761 Test RE 0.1159201736437703 Lambda1 0.7495203\n",
      "37 Train Loss 45.436108 Test MSE 47.31881835745686 Test RE 0.11579934852163537 Lambda1 0.75471294\n",
      "38 Train Loss 44.82513 Test MSE 46.917385494675486 Test RE 0.11530710592958401 Lambda1 0.76629305\n",
      "39 Train Loss 44.42256 Test MSE 46.51528244805984 Test RE 0.11481192584824625 Lambda1 0.7800691\n",
      "40 Train Loss 44.044567 Test MSE 46.41741959522762 Test RE 0.11469108665348644 Lambda1 0.7846651\n",
      "41 Train Loss 43.491333 Test MSE 45.81360424565379 Test RE 0.11394267225568917 Lambda1 0.79920864\n",
      "42 Train Loss 42.822166 Test MSE 45.36653075365086 Test RE 0.1133853526992014 Lambda1 0.81255114\n",
      "43 Train Loss 42.076458 Test MSE 44.76918604987927 Test RE 0.11263640219730364 Lambda1 0.82252926\n",
      "44 Train Loss 41.563404 Test MSE 44.049506672058726 Test RE 0.11172740068231776 Lambda1 0.837014\n",
      "45 Train Loss 41.379555 Test MSE 43.70819936770025 Test RE 0.11129371214309211 Lambda1 0.84405965\n",
      "46 Train Loss 41.08365 Test MSE 43.66481021481994 Test RE 0.11123845776315106 Lambda1 0.84672594\n",
      "47 Train Loss 40.745266 Test MSE 43.56049232596843 Test RE 0.11110550058242637 Lambda1 0.8540411\n",
      "48 Train Loss 40.628803 Test MSE 43.33485647423739 Test RE 0.11081737330193796 Lambda1 0.8607643\n",
      "49 Train Loss 40.498817 Test MSE 43.186184472430995 Test RE 0.11062711542148652 Lambda1 0.86293733\n",
      "50 Train Loss 40.395123 Test MSE 42.87989659251352 Test RE 0.11023411894379777 Lambda1 0.86562526\n",
      "51 Train Loss 40.245853 Test MSE 42.57929995472154 Test RE 0.1098470578136139 Lambda1 0.8747965\n",
      "52 Train Loss 39.97805 Test MSE 42.60715229543103 Test RE 0.10988297899504125 Lambda1 0.8842993\n",
      "53 Train Loss 39.762367 Test MSE 42.48177728776263 Test RE 0.10972119009099653 Lambda1 0.89062804\n",
      "54 Train Loss 39.671993 Test MSE 42.30725780802636 Test RE 0.10949558522325704 Lambda1 0.8914077\n",
      "55 Train Loss 39.493156 Test MSE 42.16101812086619 Test RE 0.10930617964305817 Lambda1 0.9025629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 Train Loss 39.11989 Test MSE 41.73021850477521 Test RE 0.10874630265983574 Lambda1 0.92010504\n",
      "57 Train Loss 38.905785 Test MSE 41.62285463546292 Test RE 0.10860632084833802 Lambda1 0.9205377\n",
      "58 Train Loss 38.742355 Test MSE 41.67160704666581 Test RE 0.10866990695415907 Lambda1 0.92446846\n",
      "59 Train Loss 38.58437 Test MSE 41.62001630125217 Test RE 0.10860261775908504 Lambda1 0.9323183\n",
      "60 Train Loss 38.294033 Test MSE 41.445103547261226 Test RE 0.10837417018884045 Lambda1 0.9353705\n",
      "61 Train Loss 38.051037 Test MSE 41.20186722195713 Test RE 0.10805568473133889 Lambda1 0.923762\n",
      "62 Train Loss 38.018654 Test MSE 41.09990802229277 Test RE 0.10792190322785675 Lambda1 0.9240449\n",
      "63 Train Loss 37.827133 Test MSE 40.9287409685331 Test RE 0.10769693985996287 Lambda1 0.9262445\n",
      "64 Train Loss 37.419685 Test MSE 40.525625349355316 Test RE 0.1071652627742415 Lambda1 0.9245482\n",
      "65 Train Loss 37.271656 Test MSE 40.26174998854379 Test RE 0.10681579925772598 Lambda1 0.9269895\n",
      "66 Train Loss 36.893116 Test MSE 40.00126347394305 Test RE 0.10646969873219006 Lambda1 0.9497131\n",
      "67 Train Loss 36.57407 Test MSE 39.81355482368371 Test RE 0.10621959682974676 Lambda1 0.94878405\n",
      "68 Train Loss 36.397133 Test MSE 39.68430346257469 Test RE 0.10604704016927571 Lambda1 0.94172186\n",
      "69 Train Loss 36.24728 Test MSE 39.375579045979265 Test RE 0.10563373779809535 Lambda1 0.9441838\n",
      "70 Train Loss 36.141872 Test MSE 39.2724790360634 Test RE 0.10549535281302375 Lambda1 0.9592021\n",
      "71 Train Loss 35.845554 Test MSE 39.162381336363254 Test RE 0.10534737453777686 Lambda1 0.94870424\n",
      "72 Train Loss 35.713924 Test MSE 39.135562024940704 Test RE 0.10531129619128829 Lambda1 0.9467669\n",
      "73 Train Loss 35.67227 Test MSE 39.05708484476256 Test RE 0.10520565466676696 Lambda1 0.94401515\n",
      "74 Train Loss 35.61552 Test MSE 38.922049009753216 Test RE 0.10502362835629282 Lambda1 0.94929904\n",
      "75 Train Loss 35.585354 Test MSE 38.889972220173114 Test RE 0.10498034292701372 Lambda1 0.95236385\n",
      "76 Train Loss 35.158653 Test MSE 38.767268702505 Test RE 0.1048145979623618 Lambda1 0.9325123\n",
      "77 Train Loss 35.011436 Test MSE 38.88542002396557 Test RE 0.10497419860397687 Lambda1 0.91919184\n",
      "78 Train Loss 34.942944 Test MSE 38.999994586648626 Test RE 0.10512873629905564 Lambda1 0.9158297\n",
      "79 Train Loss 34.90463 Test MSE 38.93273149283503 Test RE 0.10503803967574532 Lambda1 0.9217393\n",
      "80 Train Loss 34.85879 Test MSE 38.92347523710295 Test RE 0.10502555253812285 Lambda1 0.9189228\n",
      "81 Train Loss 34.848885 Test MSE 38.897545992620195 Test RE 0.1049905648232518 Lambda1 0.92053944\n",
      "82 Train Loss 34.82268 Test MSE 38.792096276538864 Test RE 0.10484815559702587 Lambda1 0.9204252\n",
      "83 Train Loss 34.756496 Test MSE 38.66827898422232 Test RE 0.10468069377363734 Lambda1 0.9106669\n",
      "84 Train Loss 34.68232 Test MSE 38.525890442799664 Test RE 0.10448778270385271 Lambda1 0.9039694\n",
      "85 Train Loss 34.57718 Test MSE 38.513491758087525 Test RE 0.10447096783846452 Lambda1 0.9024589\n",
      "86 Train Loss 34.532322 Test MSE 38.45479792631108 Test RE 0.10439133159800744 Lambda1 0.9008753\n",
      "87 Train Loss 34.404667 Test MSE 38.37471164845329 Test RE 0.10428257182039706 Lambda1 0.886878\n",
      "88 Train Loss 34.33645 Test MSE 38.36615212169978 Test RE 0.10427094099367498 Lambda1 0.8814663\n",
      "89 Train Loss 34.26582 Test MSE 38.22778250958843 Test RE 0.10408274174576153 Lambda1 0.8697844\n",
      "90 Train Loss 34.203796 Test MSE 38.074267018040516 Test RE 0.10387354327848147 Lambda1 0.8686369\n",
      "91 Train Loss 34.17877 Test MSE 37.938656847473794 Test RE 0.10368839363133513 Lambda1 0.86918163\n",
      "92 Train Loss 34.13595 Test MSE 37.75532530237723 Test RE 0.10343756274477105 Lambda1 0.86548066\n",
      "93 Train Loss 34.098965 Test MSE 37.64587728522384 Test RE 0.1032875275837008 Lambda1 0.863911\n",
      "94 Train Loss 34.011032 Test MSE 37.54398927789666 Test RE 0.1031476598029615 Lambda1 0.86633325\n",
      "95 Train Loss 33.90306 Test MSE 37.48853455439316 Test RE 0.10307145401302285 Lambda1 0.87014234\n",
      "96 Train Loss 33.82248 Test MSE 37.510964786077615 Test RE 0.10310228438431586 Lambda1 0.8712175\n",
      "97 Train Loss 33.64797 Test MSE 37.40040728797409 Test RE 0.10295023361657445 Lambda1 0.8775102\n",
      "98 Train Loss 33.622643 Test MSE 37.3714180834714 Test RE 0.10291032731433777 Lambda1 0.8805487\n",
      "99 Train Loss 33.604774 Test MSE 37.35186304686123 Test RE 0.10288339926818206 Lambda1 0.88104486\n",
      "100 Train Loss 33.5309 Test MSE 37.222042424924695 Test RE 0.1027044522035723 Lambda1 0.8894348\n",
      "101 Train Loss 33.478012 Test MSE 37.096308971185714 Test RE 0.10253084124305864 Lambda1 0.89540523\n",
      "102 Train Loss 33.43689 Test MSE 37.01096455372001 Test RE 0.10241283120635195 Lambda1 0.8997328\n",
      "103 Train Loss 33.403168 Test MSE 36.920454480447425 Test RE 0.10228752959683751 Lambda1 0.900623\n",
      "104 Train Loss 33.389145 Test MSE 36.901337750828034 Test RE 0.10226104487098964 Lambda1 0.9018019\n",
      "105 Train Loss 33.370197 Test MSE 36.89333870519821 Test RE 0.10224996078640916 Lambda1 0.90167403\n",
      "106 Train Loss 33.34438 Test MSE 36.841189182397535 Test RE 0.10217766897836408 Lambda1 0.9009143\n",
      "107 Train Loss 33.335297 Test MSE 36.808274832908864 Test RE 0.10213201541671375 Lambda1 0.90202034\n",
      "108 Train Loss 33.291786 Test MSE 36.773814591701466 Test RE 0.10208419576157371 Lambda1 0.90034443\n",
      "109 Train Loss 33.25057 Test MSE 36.67981511600582 Test RE 0.10195364089589476 Lambda1 0.9009673\n",
      "110 Train Loss 33.20687 Test MSE 36.61765596147268 Test RE 0.1018672168059324 Lambda1 0.89951545\n",
      "111 Train Loss 33.13938 Test MSE 36.57791705761967 Test RE 0.10181192666909762 Lambda1 0.8919053\n",
      "112 Train Loss 33.112656 Test MSE 36.57555373825814 Test RE 0.10180863755364815 Lambda1 0.8870696\n",
      "113 Train Loss 33.061012 Test MSE 36.599909103188864 Test RE 0.10184252868612001 Lambda1 0.8825394\n",
      "114 Train Loss 33.036537 Test MSE 36.551989622377434 Test RE 0.10177583672343118 Lambda1 0.8810335\n",
      "115 Train Loss 33.028484 Test MSE 36.536032168764805 Test RE 0.10175361822719002 Lambda1 0.8829521\n",
      "116 Train Loss 33.004646 Test MSE 36.51731089952084 Test RE 0.10172754532243689 Lambda1 0.88823086\n",
      "117 Train Loss 32.980366 Test MSE 36.50111606040688 Test RE 0.10170498555714322 Lambda1 0.8883439\n",
      "118 Train Loss 32.915615 Test MSE 36.489750344964015 Test RE 0.1016891498780099 Lambda1 0.8871095\n",
      "119 Train Loss 32.877804 Test MSE 36.38101900681045 Test RE 0.10153753132442891 Lambda1 0.8872273\n",
      "120 Train Loss 32.82757 Test MSE 36.305837029223035 Test RE 0.10143256257241662 Lambda1 0.89459294\n",
      "121 Train Loss 32.572266 Test MSE 35.92425467333361 Test RE 0.10089811526945365 Lambda1 0.9188212\n",
      "122 Train Loss 32.478683 Test MSE 35.92049255497088 Test RE 0.10089283192266998 Lambda1 0.9329721\n",
      "123 Train Loss 32.462227 Test MSE 35.85253877527539 Test RE 0.1007973530692963 Lambda1 0.9371521\n",
      "124 Train Loss 32.43457 Test MSE 35.81209084427142 Test RE 0.10074047850649773 Lambda1 0.93963504\n",
      "125 Train Loss 32.417263 Test MSE 35.78496291381596 Test RE 0.10070231543962403 Lambda1 0.9405094\n",
      "126 Train Loss 32.39754 Test MSE 35.725270624618396 Test RE 0.1006182904733477 Lambda1 0.95024496\n",
      "127 Train Loss 32.38625 Test MSE 35.70765365334805 Test RE 0.1005934787912829 Lambda1 0.95833063\n",
      "128 Train Loss 32.346115 Test MSE 35.71353556573282 Test RE 0.10060176353704639 Lambda1 0.9655596\n",
      "129 Train Loss 32.29539 Test MSE 35.767034923920626 Test RE 0.10067708673844766 Lambda1 0.9648972\n",
      "130 Train Loss 32.2745 Test MSE 35.76130324020229 Test RE 0.10066901964062915 Lambda1 0.96262985\n",
      "131 Train Loss 32.26226 Test MSE 35.753629835670054 Test RE 0.10065821864228722 Lambda1 0.9674886\n",
      "132 Train Loss 32.242184 Test MSE 35.729277048231836 Test RE 0.1006239322527728 Lambda1 0.97381115\n",
      "133 Train Loss 32.22955 Test MSE 35.698888885717544 Test RE 0.10058113224247367 Lambda1 0.97792137\n",
      "134 Train Loss 32.21855 Test MSE 35.70679129815157 Test RE 0.10059226409605589 Lambda1 0.9837501\n",
      "135 Train Loss 32.210102 Test MSE 35.688326140309556 Test RE 0.10056625094608918 Lambda1 0.9875063\n",
      "136 Train Loss 32.20538 Test MSE 35.67672914630486 Test RE 0.10054991001481024 Lambda1 0.99211216\n",
      "137 Train Loss 32.197113 Test MSE 35.70673557128119 Test RE 0.10059218559986446 Lambda1 0.99165004\n",
      "138 Train Loss 32.17932 Test MSE 35.651665448669284 Test RE 0.10051458453271607 Lambda1 0.9870605\n",
      "139 Train Loss 32.172054 Test MSE 35.63096869108327 Test RE 0.10048540457754687 Lambda1 0.9878049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "141 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "142 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "143 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "144 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "145 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "146 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "147 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "148 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "149 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "150 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "151 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "152 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "153 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "154 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "155 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "156 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "157 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "158 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "159 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "160 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "161 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "162 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "163 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "164 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "165 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "166 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "167 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "168 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "169 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "170 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "171 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "172 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "173 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "174 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "175 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "176 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "177 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "178 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "179 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "180 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "181 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "182 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "183 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "184 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "185 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "186 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "187 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "188 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "189 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "190 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "191 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "192 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "193 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "194 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "195 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "196 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "197 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "198 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "199 Train Loss 32.166378 Test MSE 35.63506108395613 Test RE 0.1004911750345605 Lambda1 0.9854414\n",
      "Training time: 463.62\n",
      "Training time: 463.62\n",
      "inv_HT_rowdy\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 867.9472 Test MSE 866.6530131468821 Test RE 0.49557774623134143 Lambda1 -0.040644992\n",
      "1 Train Loss 840.5603 Test MSE 838.0285027550977 Test RE 0.48732485878523724 Lambda1 -0.040904198\n",
      "2 Train Loss 637.42145 Test MSE 641.5102790369193 Test RE 0.4263742617036986 Lambda1 0.0043006437\n",
      "3 Train Loss 530.70703 Test MSE 534.5705883577659 Test RE 0.3892169104024505 Lambda1 0.0003383256\n",
      "4 Train Loss 340.3199 Test MSE 344.0002762697084 Test RE 0.31222564130458963 Lambda1 0.00018096011\n",
      "5 Train Loss 282.246 Test MSE 288.4813052844435 Test RE 0.2859222862676043 Lambda1 0.0006898919\n",
      "6 Train Loss 274.72174 Test MSE 282.10313036682976 Test RE 0.28274382135602216 Lambda1 0.0015167169\n",
      "7 Train Loss 270.87265 Test MSE 278.2539662066656 Test RE 0.2808082431227188 Lambda1 0.0038976874\n",
      "8 Train Loss 267.55844 Test MSE 273.8805954708403 Test RE 0.2785927448961135 Lambda1 0.009693617\n",
      "9 Train Loss 256.71634 Test MSE 259.63209025693857 Test RE 0.2712491310110117 Lambda1 0.040968433\n",
      "10 Train Loss 236.02306 Test MSE 238.42046679555858 Test RE 0.25993271175208393 Lambda1 0.0916321\n",
      "11 Train Loss 216.66312 Test MSE 210.72587072158478 Test RE 0.24437011834037828 Lambda1 0.15567167\n",
      "12 Train Loss 196.03139 Test MSE 190.57197032185252 Test RE 0.2323906671434013 Lambda1 0.21609971\n",
      "13 Train Loss 175.47345 Test MSE 166.53525851166896 Test RE 0.21724123767164347 Lambda1 0.25883245\n",
      "14 Train Loss 148.61588 Test MSE 132.60401303988934 Test RE 0.193850758370623 Lambda1 0.33357608\n",
      "15 Train Loss 126.00757 Test MSE 112.71031108696509 Test RE 0.1787191107495326 Lambda1 0.35622415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 Train Loss 109.338554 Test MSE 101.60007365794621 Test RE 0.16968215924152413 Lambda1 0.36407083\n",
      "17 Train Loss 92.87723 Test MSE 81.41744107340473 Test RE 0.15189654474194586 Lambda1 0.3878462\n",
      "18 Train Loss 81.31505 Test MSE 74.31904401581913 Test RE 0.14512399590402472 Lambda1 0.40541708\n",
      "19 Train Loss 71.21938 Test MSE 66.82445869579225 Test RE 0.1376121865087649 Lambda1 0.43240044\n",
      "20 Train Loss 66.690056 Test MSE 64.53274928766636 Test RE 0.13523193257520588 Lambda1 0.44968173\n",
      "21 Train Loss 63.02802 Test MSE 62.212839339919505 Test RE 0.13277893531401855 Lambda1 0.4623935\n",
      "22 Train Loss 58.751278 Test MSE 60.01117889210638 Test RE 0.13040830490103328 Lambda1 0.49007836\n",
      "23 Train Loss 55.28611 Test MSE 57.79299601907413 Test RE 0.12797548236964112 Lambda1 0.5054127\n",
      "24 Train Loss 53.266624 Test MSE 57.23127508728493 Test RE 0.1273520327990866 Lambda1 0.52084994\n",
      "25 Train Loss 52.544376 Test MSE 56.04134522941509 Test RE 0.12602115232812805 Lambda1 0.5323052\n",
      "26 Train Loss 51.480404 Test MSE 55.01296941460522 Test RE 0.12485953531446527 Lambda1 0.5425025\n",
      "27 Train Loss 50.82052 Test MSE 54.029393941516766 Test RE 0.12373832095175374 Lambda1 0.5411805\n",
      "28 Train Loss 50.062305 Test MSE 53.481895392665265 Test RE 0.12310978298260676 Lambda1 0.5470095\n",
      "29 Train Loss 47.294983 Test MSE 49.9339492402423 Test RE 0.11895621258773496 Lambda1 0.58027166\n",
      "30 Train Loss 45.893867 Test MSE 49.33258340559522 Test RE 0.1182377345621334 Lambda1 0.59217405\n",
      "31 Train Loss 45.548607 Test MSE 48.922718985998436 Test RE 0.11774553940307693 Lambda1 0.596683\n",
      "32 Train Loss 45.079754 Test MSE 48.430016828060495 Test RE 0.11715112962168889 Lambda1 0.6091071\n",
      "33 Train Loss 43.90189 Test MSE 46.948416630153204 Test RE 0.1153452316578549 Lambda1 0.63428366\n",
      "34 Train Loss 41.169758 Test MSE 44.673400248008114 Test RE 0.11251584220821129 Lambda1 0.6649211\n",
      "35 Train Loss 39.86549 Test MSE 43.32366507312333 Test RE 0.11080306286158983 Lambda1 0.6791326\n",
      "36 Train Loss 39.18678 Test MSE 43.545861189163425 Test RE 0.11108683990830509 Lambda1 0.68499273\n",
      "37 Train Loss 38.76596 Test MSE 42.808553125883705 Test RE 0.11014237713979304 Lambda1 0.6952812\n",
      "38 Train Loss 38.364594 Test MSE 42.19435588022133 Test RE 0.10934938665281768 Lambda1 0.7081886\n",
      "39 Train Loss 37.56918 Test MSE 41.58727466504919 Test RE 0.10855989159848678 Lambda1 0.7284868\n",
      "40 Train Loss 37.093307 Test MSE 41.037233042321716 Test RE 0.107839584506293 Lambda1 0.74818957\n",
      "41 Train Loss 36.717506 Test MSE 41.091666749529494 Test RE 0.10791108254145157 Lambda1 0.75155437\n",
      "42 Train Loss 36.428116 Test MSE 40.96494274156354 Test RE 0.1077445587010012 Lambda1 0.7647602\n",
      "43 Train Loss 36.304237 Test MSE 40.96603580851495 Test RE 0.10774599616463246 Lambda1 0.77102405\n",
      "44 Train Loss 36.222816 Test MSE 40.817345762249545 Test RE 0.10755028134272986 Lambda1 0.77080756\n",
      "45 Train Loss 36.00257 Test MSE 40.48463835175434 Test RE 0.10711105641111422 Lambda1 0.7807782\n",
      "46 Train Loss 35.91817 Test MSE 40.5120058345836 Test RE 0.10714725365637087 Lambda1 0.78188586\n",
      "47 Train Loss 35.874214 Test MSE 40.545379684005944 Test RE 0.10719137860251732 Lambda1 0.7827526\n",
      "48 Train Loss 35.76033 Test MSE 40.496925397751454 Test RE 0.10712730922562033 Lambda1 0.7775824\n",
      "49 Train Loss 35.44752 Test MSE 40.298458258195744 Test RE 0.10686448231089192 Lambda1 0.79610765\n",
      "50 Train Loss 35.2557 Test MSE 40.23449767367011 Test RE 0.10677964247669841 Lambda1 0.8103733\n",
      "51 Train Loss 35.055782 Test MSE 40.04536168851201 Test RE 0.106528369758112 Lambda1 0.8078123\n",
      "52 Train Loss 34.957825 Test MSE 39.945554080486936 Test RE 0.10639553321339966 Lambda1 0.80739087\n",
      "53 Train Loss 34.78256 Test MSE 39.72012349514984 Test RE 0.10609488971316565 Lambda1 0.8178535\n",
      "54 Train Loss 34.71023 Test MSE 39.738462398095955 Test RE 0.10611937905492022 Lambda1 0.8239986\n",
      "55 Train Loss 34.63483 Test MSE 39.48853133839037 Test RE 0.10578513910684956 Lambda1 0.83210486\n",
      "56 Train Loss 34.58217 Test MSE 39.39452542827766 Test RE 0.105659148681379 Lambda1 0.8398148\n",
      "57 Train Loss 34.560196 Test MSE 39.281182943094024 Test RE 0.10550704256237438 Lambda1 0.84541196\n",
      "58 Train Loss 34.528168 Test MSE 39.253771737957265 Test RE 0.10547022366228528 Lambda1 0.8453541\n",
      "59 Train Loss 34.4526 Test MSE 39.04397740638957 Test RE 0.1051879998372396 Lambda1 0.84947574\n",
      "60 Train Loss 34.35938 Test MSE 39.01660757612661 Test RE 0.10515112497701373 Lambda1 0.8496627\n",
      "61 Train Loss 34.308605 Test MSE 38.982298135716064 Test RE 0.10510488223659602 Lambda1 0.85068965\n",
      "62 Train Loss 34.209347 Test MSE 38.911128736841775 Test RE 0.10500889419957277 Lambda1 0.8526909\n",
      "63 Train Loss 34.079582 Test MSE 38.77289292853428 Test RE 0.1048222007627483 Lambda1 0.85083777\n",
      "64 Train Loss 33.98354 Test MSE 38.785573687777905 Test RE 0.10483934052621637 Lambda1 0.8498245\n",
      "65 Train Loss 33.917355 Test MSE 38.76291371285926 Test RE 0.10480871053045931 Lambda1 0.8474884\n",
      "66 Train Loss 33.756477 Test MSE 38.45253865311026 Test RE 0.10438826498454427 Lambda1 0.8500157\n",
      "67 Train Loss 33.61307 Test MSE 38.19438623730954 Test RE 0.10403726781488673 Lambda1 0.845199\n",
      "68 Train Loss 33.497982 Test MSE 38.080132208712996 Test RE 0.10388154362522471 Lambda1 0.8420983\n",
      "69 Train Loss 33.440926 Test MSE 38.06122103621957 Test RE 0.10385574584512461 Lambda1 0.8396801\n",
      "70 Train Loss 33.35091 Test MSE 37.707565478052075 Test RE 0.10337211869047482 Lambda1 0.8440217\n",
      "71 Train Loss 33.075623 Test MSE 37.28371093920533 Test RE 0.10278949602958207 Lambda1 0.8643252\n",
      "72 Train Loss 32.759506 Test MSE 36.99707216015048 Test RE 0.1023936086204069 Lambda1 0.8878448\n",
      "73 Train Loss 32.547077 Test MSE 36.62138504972787 Test RE 0.10187240367768735 Lambda1 0.89695966\n",
      "74 Train Loss 32.482502 Test MSE 36.524947560589396 Test RE 0.10173818162279749 Lambda1 0.9025454\n",
      "75 Train Loss 32.431156 Test MSE 36.44205732527165 Test RE 0.10162267301339314 Lambda1 0.90825814\n",
      "76 Train Loss 32.285587 Test MSE 36.00968405057238 Test RE 0.1010180139957424 Lambda1 0.9188653\n",
      "77 Train Loss 32.1041 Test MSE 35.64896195031369 Test RE 0.10051077340408623 Lambda1 0.92432916\n",
      "78 Train Loss 31.95991 Test MSE 35.47886275438561 Test RE 0.10027069285234765 Lambda1 0.9228793\n",
      "79 Train Loss 31.86356 Test MSE 35.27888300179217 Test RE 0.09998770119086672 Lambda1 0.92408234\n",
      "80 Train Loss 31.685495 Test MSE 35.09443063329886 Test RE 0.0997259704216669 Lambda1 0.92957586\n",
      "81 Train Loss 31.62232 Test MSE 35.16229305263859 Test RE 0.09982234436187194 Lambda1 0.9299584\n",
      "82 Train Loss 31.59275 Test MSE 35.012129631418446 Test RE 0.09960896660193076 Lambda1 0.93451244\n",
      "83 Train Loss 31.56326 Test MSE 34.90141064088765 Test RE 0.09945134498466944 Lambda1 0.94049495\n",
      "84 Train Loss 31.537216 Test MSE 34.84878048457615 Test RE 0.09937633204892014 Lambda1 0.93997693\n",
      "85 Train Loss 31.50626 Test MSE 34.913891340115356 Test RE 0.09946912523132302 Lambda1 0.94052947\n",
      "86 Train Loss 31.361357 Test MSE 34.83492146945898 Test RE 0.09935656959200322 Lambda1 0.948144\n",
      "87 Train Loss 31.315022 Test MSE 34.7972153125166 Test RE 0.09930278206237357 Lambda1 0.9472349\n",
      "88 Train Loss 31.303389 Test MSE 34.749633235669535 Test RE 0.09923486500443009 Lambda1 0.94845766\n",
      "89 Train Loss 31.265347 Test MSE 34.777973704384316 Test RE 0.09927532283297658 Lambda1 0.9514216\n",
      "90 Train Loss 31.253607 Test MSE 34.79899000923742 Test RE 0.09930531430630686 Lambda1 0.9535294\n",
      "91 Train Loss 31.22616 Test MSE 34.72024669381095 Test RE 0.09919289641326814 Lambda1 0.9546721\n",
      "92 Train Loss 31.136396 Test MSE 34.79832243313131 Test RE 0.09930436177617642 Lambda1 0.9583352\n",
      "93 Train Loss 31.112965 Test MSE 34.77672160919164 Test RE 0.09927353573528157 Lambda1 0.9585595\n",
      "94 Train Loss 31.10144 Test MSE 34.710515281050476 Test RE 0.09917899451513776 Lambda1 0.9602459\n",
      "95 Train Loss 31.09233 Test MSE 34.61799539906324 Test RE 0.09904672693617868 Lambda1 0.9631091\n",
      "96 Train Loss 31.050035 Test MSE 34.509110496056046 Test RE 0.09889083710431597 Lambda1 0.9690779\n",
      "97 Train Loss 31.006172 Test MSE 34.40064438017259 Test RE 0.09873530213661663 Lambda1 0.96854806\n",
      "98 Train Loss 30.986109 Test MSE 34.33099255268154 Test RE 0.09863529559692066 Lambda1 0.9691178\n",
      "99 Train Loss 30.971775 Test MSE 34.258313376558455 Test RE 0.09853083415283771 Lambda1 0.9716291\n",
      "100 Train Loss 30.899473 Test MSE 34.30855441197569 Test RE 0.09860305717005219 Lambda1 0.9719978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 Train Loss 30.875366 Test MSE 34.266230619887224 Test RE 0.09854221894454246 Lambda1 0.97011554\n",
      "102 Train Loss 30.855785 Test MSE 34.19996927087662 Test RE 0.09844689623355996 Lambda1 0.9701682\n",
      "103 Train Loss 30.832636 Test MSE 34.17353397829445 Test RE 0.09840884100048403 Lambda1 0.97104764\n",
      "104 Train Loss 30.794695 Test MSE 34.0824513519502 Test RE 0.09827760909218126 Lambda1 0.96979415\n",
      "105 Train Loss 30.726233 Test MSE 33.93771170675202 Test RE 0.09806870685644395 Lambda1 0.96906275\n",
      "106 Train Loss 30.621975 Test MSE 33.799131813349625 Test RE 0.09786827712459477 Lambda1 0.964734\n",
      "107 Train Loss 30.485712 Test MSE 33.79104044273922 Test RE 0.09785656179555965 Lambda1 0.9570517\n",
      "108 Train Loss 30.361736 Test MSE 33.73418123187225 Test RE 0.0977741969231614 Lambda1 0.94924664\n",
      "109 Train Loss 30.312336 Test MSE 33.76084385369818 Test RE 0.09781282838590037 Lambda1 0.9455984\n",
      "110 Train Loss 30.292862 Test MSE 33.76683424022941 Test RE 0.09782150575580018 Lambda1 0.9425438\n",
      "111 Train Loss 30.22025 Test MSE 33.7478733496508 Test RE 0.09779403733363314 Lambda1 0.9378387\n",
      "112 Train Loss 30.149712 Test MSE 33.73129534145012 Test RE 0.09777001464211797 Lambda1 0.9333337\n",
      "113 Train Loss 30.133453 Test MSE 33.67497627329662 Test RE 0.0976883602560015 Lambda1 0.9319844\n",
      "114 Train Loss 30.105453 Test MSE 33.589259746175 Test RE 0.09756395271073706 Lambda1 0.93114054\n",
      "115 Train Loss 30.078773 Test MSE 33.58566364207386 Test RE 0.09755872991743611 Lambda1 0.9315082\n",
      "116 Train Loss 30.061989 Test MSE 33.56554943505382 Test RE 0.09752951193914443 Lambda1 0.93351716\n",
      "117 Train Loss 30.036068 Test MSE 33.49077812326168 Test RE 0.09742082199693879 Lambda1 0.93449116\n",
      "118 Train Loss 30.004963 Test MSE 33.457485353886405 Test RE 0.09737238753826095 Lambda1 0.9356182\n",
      "119 Train Loss 29.981684 Test MSE 33.47853220935049 Test RE 0.09740300939087253 Lambda1 0.9337107\n",
      "120 Train Loss 29.958729 Test MSE 33.49509055621293 Test RE 0.09742709398079731 Lambda1 0.93458277\n",
      "121 Train Loss 29.87601 Test MSE 33.32955294107953 Test RE 0.0971860463458202 Lambda1 0.93768555\n",
      "122 Train Loss 29.819138 Test MSE 33.25295730923988 Test RE 0.09707430904776945 Lambda1 0.94481826\n",
      "123 Train Loss 29.796783 Test MSE 33.21229257162709 Test RE 0.09701493524758457 Lambda1 0.9483254\n",
      "124 Train Loss 29.782799 Test MSE 33.20383814046931 Test RE 0.09700258653221715 Lambda1 0.9476181\n",
      "125 Train Loss 29.727825 Test MSE 33.16663464828119 Test RE 0.09694822765948145 Lambda1 0.95465165\n",
      "126 Train Loss 29.674908 Test MSE 33.11215736732021 Test RE 0.09686857462224979 Lambda1 0.9640233\n",
      "127 Train Loss 29.66791 Test MSE 33.100716853683444 Test RE 0.09685183874517554 Lambda1 0.9662958\n",
      "128 Train Loss 29.662382 Test MSE 33.088619763639606 Test RE 0.096834139242089 Lambda1 0.9670379\n",
      "129 Train Loss 29.653954 Test MSE 33.08122823690481 Test RE 0.09682332295372707 Lambda1 0.9662075\n",
      "130 Train Loss 29.628687 Test MSE 33.009017008434455 Test RE 0.09671759001837311 Lambda1 0.96491903\n",
      "131 Train Loss 29.607368 Test MSE 32.99113688741082 Test RE 0.09669139177593832 Lambda1 0.96663725\n",
      "132 Train Loss 29.58719 Test MSE 32.964109161219234 Test RE 0.09665177683433276 Lambda1 0.9709948\n",
      "133 Train Loss 29.56664 Test MSE 32.94733296773303 Test RE 0.09662717954805847 Lambda1 0.97301877\n",
      "134 Train Loss 29.54996 Test MSE 32.93138874401973 Test RE 0.09660379629264448 Lambda1 0.9730942\n",
      "135 Train Loss 29.533192 Test MSE 32.895315055117614 Test RE 0.0965508709485437 Lambda1 0.9741044\n",
      "136 Train Loss 29.521015 Test MSE 32.90284159123851 Test RE 0.09656191586563713 Lambda1 0.97290146\n",
      "137 Train Loss 29.513113 Test MSE 32.88774115214348 Test RE 0.09653975524674159 Lambda1 0.97440124\n",
      "138 Train Loss 29.493032 Test MSE 32.92959951213904 Test RE 0.09660117191288772 Lambda1 0.9771691\n",
      "139 Train Loss 29.4534 Test MSE 32.924802398644715 Test RE 0.09659413533074773 Lambda1 0.98151493\n",
      "140 Train Loss 29.432413 Test MSE 32.94667526455561 Test RE 0.09662621509487741 Lambda1 0.98463446\n",
      "141 Train Loss 29.418695 Test MSE 32.92684579447922 Test RE 0.09659713272142305 Lambda1 0.9841284\n",
      "142 Train Loss 29.395407 Test MSE 32.95882850349036 Test RE 0.09664403499924083 Lambda1 0.9802135\n",
      "143 Train Loss 29.388142 Test MSE 32.95231387732372 Test RE 0.09663448322051389 Lambda1 0.9776742\n",
      "144 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "145 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "146 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "147 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "148 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "149 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "150 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "151 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "152 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "153 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "154 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "155 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "156 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "157 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "158 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "159 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "160 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "161 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "162 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "163 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "164 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "165 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "166 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "167 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "168 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "169 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "170 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "171 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "172 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "173 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "174 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "175 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "176 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "177 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "178 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "179 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "180 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "181 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "182 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "183 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "184 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "186 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "187 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "188 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "189 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "190 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "191 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "192 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "193 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "194 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "195 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "196 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "197 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "198 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "199 Train Loss 29.386246 Test MSE 32.9514495657071 Test RE 0.09663321589139082 Lambda1 0.9768534\n",
      "Training time: 467.22\n",
      "Training time: 467.22\n",
      "inv_HT_rowdy\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 1102.6426 Test MSE 1021.1512775164007 Test RE 0.5379404638757707 Lambda1 -0.00028708347\n",
      "1 Train Loss 854.9277 Test MSE 858.0976741251085 Test RE 0.4931255819102569 Lambda1 -0.000416287\n",
      "2 Train Loss 854.7235 Test MSE 858.078517366748 Test RE 0.4931200774434443 Lambda1 -0.00040124095\n",
      "3 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "4 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "5 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "6 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "7 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "8 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "9 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "10 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "11 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "12 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "13 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "14 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "15 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "16 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "17 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "18 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "19 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "20 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "21 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "22 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "23 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "24 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "25 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "26 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "27 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "28 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "29 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "30 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "31 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "32 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "33 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "34 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "35 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "36 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "37 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "38 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "39 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "40 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "41 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "42 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "43 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "44 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "45 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "46 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "47 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "48 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "49 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "50 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "51 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "52 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "53 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "54 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "55 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "56 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "57 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "58 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "60 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "61 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "62 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "63 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "64 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "65 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "66 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "67 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "68 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "69 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "70 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "71 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "72 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "73 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "74 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "75 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "76 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "77 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "78 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "79 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "80 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "81 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "82 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "83 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "84 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "85 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "86 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "87 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "88 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "89 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "90 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "91 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "92 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "93 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "94 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "95 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "96 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "97 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "98 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "99 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "100 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "101 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "102 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "103 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "104 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "105 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "106 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "107 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "108 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "109 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "110 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "111 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "112 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "113 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "114 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "115 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "116 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "117 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "118 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "119 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "120 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "121 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "122 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "123 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "124 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "125 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "126 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "127 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "128 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "129 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "130 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "131 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "132 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "133 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "134 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "135 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "136 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "137 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "138 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "139 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "141 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "142 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "143 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "144 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "145 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "146 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "147 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "148 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "149 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "150 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "151 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "152 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "153 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "154 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "155 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "156 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "157 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "158 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "159 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "160 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "161 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "162 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "163 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "164 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "165 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "166 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "167 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "168 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "169 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "170 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "171 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "172 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "173 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "174 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "175 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "176 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "177 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "178 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "179 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "180 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "181 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "182 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "183 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "184 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "185 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "186 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "187 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "188 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "189 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "190 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "191 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "192 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "193 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "194 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "195 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "196 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "197 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "198 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "199 Train Loss 854.72156 Test MSE 858.0782124105556 Test RE 0.4931199898174104 Lambda1 -0.00014952116\n",
      "Training time: 302.62\n",
      "Training time: 302.62\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10#10\n",
    "max_iter = 100 #75\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "lambda1_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 2.0\n",
    "rowdy_terms = 3\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    'Generate Training data'\n",
    "    print(reps)\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    alpha_val = []\n",
    "    omega_val = []\n",
    "    lambda1_val = []\n",
    "\n",
    "    N_f = 50000 #Total number of collocation points \n",
    "    N_train = 5000\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)\n",
    "    lambda1_full.append(lambda1_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"alpha\": alpha_full,\"omega\": omega_full,\"lambda1\": lambda1_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'inv_HT_rowdy_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'inv_HT_rowdy_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d13b2c5c9bbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"inv_HT_rowdy_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'inv_HT_rowdy_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(25):\n",
    "    label = \"inv_HT_rowdy_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_tune[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
