{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_2D_4(xt): #True function for 2D_4 Heat Transfer in a rod x \\in [0,1] t \\in [0,0.1]\n",
    "    term1 = 4*u0/np.pi\n",
    "    \n",
    "    resol_n = 10000\n",
    "    \n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "\n",
    "    u = np.zeros((np.shape(xt)[0],1))\n",
    "    \n",
    "    for i in range(resol_n):\n",
    "        j = 2*i-1\n",
    "        term2 = np.sin(j*np.pi*x)/j\n",
    "        term3 = np.exp(-1*np.square(j*np.pi)*t)\n",
    "        \n",
    "        u = u + term2*term3\n",
    "        \n",
    "    u = term1*u\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = 50.0\n",
    "label = \"inv_HT_rowdy\"\n",
    "loss_thresh = 0.1\n",
    "\n",
    "x_ll = np.array(0.0)\n",
    "x_ul = np.array(1.0)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,100).reshape(-1,1)\n",
    "t = np.linspace(0,0.1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = true_2D_4(xt)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_f,N_train,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #X_Train\n",
    "    np.random.seed(seed)\n",
    "    x_train = np.random.uniform(x_ll,x_ul,(N_train,1))\n",
    "    t_train = np.random.uniform(0,0.1,(N_train,1))\n",
    "    \n",
    "    xt_train = np.hstack((x_train,t_train))\n",
    "    u_train = true_2D_4(xt_train)\n",
    "    \n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        self.omega1.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        self.omega.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "        \n",
    "    \n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requiresGrad = True\n",
    "        \n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    \n",
    "    def loss_PDE(self, xt_coll,f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_tt[:,[0]]\n",
    "                \n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        f = du_dt - self.lambda1*d2u_dx2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_train = self.loss_function(self.forward(xt_train),u_train)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "        \n",
    "        #print(self.iter,\"train_loss\",loss_train.cpu().detach().numpy(),\"F Loss\",(loss_f).cpu().detach().numpy())\n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                    \n",
    "    \n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_coll,f_hat, xt_train, u_train,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xt_coll, xt_train, u_train = trainingdata(N_f,N_train,123)\n",
    "    \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_train = torch.from_numpy(xt_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_coll,f_hat, xt_train, u_train,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_coll,f_hat, xt_train, u_train).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1],\"Lambda1\",lambda1_val[-1])\n",
    "\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_HT_rowdy\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 856.28436 Test MSE 856.281854632382 Test RE 0.49260355436263215 Lambda1 -0.060201727\n",
      "1 Train Loss 832.81964 Test MSE 824.5255512458574 Test RE 0.48338284104154816 Lambda1 -0.0669174\n",
      "2 Train Loss 757.71063 Test MSE 736.2318941722035 Test RE 0.45676885575238674 Lambda1 -0.076703794\n",
      "3 Train Loss 683.01636 Test MSE 680.5863457677648 Test RE 0.4391680999542071 Lambda1 -0.071755916\n",
      "4 Train Loss 580.9093 Test MSE 575.0925168092925 Test RE 0.40369933024434634 Lambda1 -0.06252667\n",
      "5 Train Loss 451.3916 Test MSE 396.8077362233783 Test RE 0.33533527135500635 Lambda1 -0.04479498\n",
      "6 Train Loss 345.55505 Test MSE 327.38692829416914 Test RE 0.3045929437785874 Lambda1 -0.025185062\n",
      "7 Train Loss 302.12817 Test MSE 298.6140334167632 Test RE 0.29090037254876683 Lambda1 -0.0019825534\n",
      "8 Train Loss 286.49106 Test MSE 291.16698311689134 Test RE 0.2872501300178238 Lambda1 0.008173314\n",
      "9 Train Loss 275.49124 Test MSE 279.57894164934845 Test RE 0.28147601825611746 Lambda1 0.013953182\n",
      "10 Train Loss 270.4651 Test MSE 272.9444545435685 Test RE 0.2781162136705122 Lambda1 0.021304931\n",
      "11 Train Loss 267.78973 Test MSE 270.80765121445006 Test RE 0.2770254288468566 Lambda1 0.02320968\n",
      "12 Train Loss 263.65427 Test MSE 265.5164559970772 Test RE 0.2743057377311499 Lambda1 0.029956967\n",
      "13 Train Loss 258.11752 Test MSE 263.08207875582343 Test RE 0.2730453614376552 Lambda1 0.03279335\n",
      "14 Train Loss 253.70366 Test MSE 256.60100671644045 Test RE 0.26966112907264445 Lambda1 0.046238177\n",
      "15 Train Loss 250.58134 Test MSE 252.28883660062718 Test RE 0.26738570648545523 Lambda1 0.055838402\n",
      "16 Train Loss 246.54305 Test MSE 245.6870091706628 Test RE 0.2638640763341174 Lambda1 0.073389746\n",
      "17 Train Loss 242.83423 Test MSE 243.32298769404014 Test RE 0.26259154644898763 Lambda1 0.07915877\n",
      "18 Train Loss 239.38522 Test MSE 237.2533922576241 Test RE 0.25929574252538834 Lambda1 0.09790136\n",
      "19 Train Loss 236.22678 Test MSE 233.75953569116714 Test RE 0.25737943229526594 Lambda1 0.10827925\n",
      "20 Train Loss 231.31752 Test MSE 230.74395726042016 Test RE 0.25571390176848957 Lambda1 0.12151762\n",
      "21 Train Loss 228.24962 Test MSE 229.84404348399374 Test RE 0.25521476568083884 Lambda1 0.12685879\n",
      "22 Train Loss 225.13629 Test MSE 225.45866976354327 Test RE 0.25276831931078386 Lambda1 0.14515337\n",
      "23 Train Loss 221.6926 Test MSE 221.3438845543884 Test RE 0.25045109474690114 Lambda1 0.15454909\n",
      "24 Train Loss 219.23386 Test MSE 219.66480557123222 Test RE 0.24949934556604028 Lambda1 0.15579304\n",
      "25 Train Loss 215.92372 Test MSE 215.29350803679648 Test RE 0.24700437052235916 Lambda1 0.17144182\n",
      "26 Train Loss 212.54375 Test MSE 210.49925650827865 Test RE 0.24423868540757135 Lambda1 0.1831251\n",
      "27 Train Loss 207.74425 Test MSE 205.0921906833567 Test RE 0.2410814154385335 Lambda1 0.19310598\n",
      "28 Train Loss 202.47128 Test MSE 194.06099009717948 Test RE 0.23450833976594654 Lambda1 0.217907\n",
      "29 Train Loss 195.97537 Test MSE 182.92587183994007 Test RE 0.2276809723274368 Lambda1 0.2358015\n",
      "30 Train Loss 191.80136 Test MSE 179.72907325130015 Test RE 0.22568273593528476 Lambda1 0.24436927\n",
      "31 Train Loss 187.41226 Test MSE 174.23582970406585 Test RE 0.22220708621071242 Lambda1 0.24675235\n",
      "32 Train Loss 182.06808 Test MSE 167.89151085226055 Test RE 0.21812404321103607 Lambda1 0.2527659\n",
      "33 Train Loss 173.38947 Test MSE 158.90194123748378 Test RE 0.21220410067818235 Lambda1 0.2637315\n",
      "34 Train Loss 165.86728 Test MSE 149.67755586534102 Test RE 0.20595271052349032 Lambda1 0.26586533\n",
      "35 Train Loss 156.96164 Test MSE 145.65173057900464 Test RE 0.20316411242757945 Lambda1 0.26590407\n",
      "36 Train Loss 148.77232 Test MSE 136.8535652848603 Test RE 0.1969324249456288 Lambda1 0.27243012\n",
      "37 Train Loss 145.14604 Test MSE 130.9413245201786 Test RE 0.19263160157712175 Lambda1 0.2800206\n",
      "38 Train Loss 139.44292 Test MSE 123.06407353157485 Test RE 0.18674750743730284 Lambda1 0.29261303\n",
      "39 Train Loss 134.35413 Test MSE 121.55327587778791 Test RE 0.18559766345331746 Lambda1 0.29854488\n",
      "40 Train Loss 129.39728 Test MSE 117.99150374371781 Test RE 0.18285824137112325 Lambda1 0.3011905\n",
      "41 Train Loss 124.85623 Test MSE 114.24518556578379 Test RE 0.17993188276726452 Lambda1 0.3081044\n",
      "42 Train Loss 120.17143 Test MSE 110.47857381954398 Test RE 0.1769408870852588 Lambda1 0.3196655\n",
      "43 Train Loss 114.77878 Test MSE 106.31198937831684 Test RE 0.17357224997028767 Lambda1 0.3292564\n",
      "44 Train Loss 109.04362 Test MSE 102.20516814003919 Test RE 0.17018669292886127 Lambda1 0.33616313\n",
      "45 Train Loss 105.370155 Test MSE 98.43791715919683 Test RE 0.16702073040880863 Lambda1 0.3442374\n",
      "46 Train Loss 99.9878 Test MSE 91.60979774394318 Test RE 0.16112396209145677 Lambda1 0.3537653\n",
      "47 Train Loss 96.75397 Test MSE 88.25179469405393 Test RE 0.1581433533763335 Lambda1 0.36141887\n",
      "48 Train Loss 93.24621 Test MSE 87.90090500063921 Test RE 0.157828650763672 Lambda1 0.36157417\n",
      "49 Train Loss 89.83884 Test MSE 85.70023186874327 Test RE 0.15584044121278026 Lambda1 0.36865088\n",
      "50 Train Loss 87.86719 Test MSE 83.77911624832932 Test RE 0.15408382753797206 Lambda1 0.37240934\n",
      "51 Train Loss 85.60298 Test MSE 82.53453186648728 Test RE 0.15293504555742174 Lambda1 0.37669224\n",
      "52 Train Loss 83.69666 Test MSE 80.87415454138176 Test RE 0.15138890489421972 Lambda1 0.380011\n",
      "53 Train Loss 81.33854 Test MSE 78.67420452693871 Test RE 0.1493156574573575 Lambda1 0.38548493\n",
      "54 Train Loss 78.778404 Test MSE 75.79209203030645 Test RE 0.14655516136872418 Lambda1 0.3902481\n",
      "55 Train Loss 77.51626 Test MSE 74.77399775426838 Test RE 0.14556751592460215 Lambda1 0.3954313\n",
      "56 Train Loss 76.33186 Test MSE 72.88194689650926 Test RE 0.14371402516699597 Lambda1 0.40228313\n",
      "57 Train Loss 74.922424 Test MSE 70.0669646354101 Test RE 0.14091130032162216 Lambda1 0.41064668\n",
      "58 Train Loss 73.81079 Test MSE 69.73977467560188 Test RE 0.14058191034068226 Lambda1 0.41264445\n",
      "59 Train Loss 72.76055 Test MSE 68.24306473873725 Test RE 0.13906518946742208 Lambda1 0.4190991\n",
      "60 Train Loss 71.067604 Test MSE 67.2937950875051 Test RE 0.1380945951988722 Lambda1 0.425977\n",
      "61 Train Loss 70.225235 Test MSE 65.89513667426866 Test RE 0.1366519559157367 Lambda1 0.4333277\n",
      "62 Train Loss 69.258865 Test MSE 65.33513447945528 Test RE 0.1360700565585772 Lambda1 0.4355781\n",
      "63 Train Loss 68.22726 Test MSE 65.2247900735302 Test RE 0.1359551037498426 Lambda1 0.4364328\n",
      "64 Train Loss 66.00601 Test MSE 63.6592147353902 Test RE 0.13431354419358355 Lambda1 0.44096154\n",
      "65 Train Loss 64.6909 Test MSE 63.305879256783335 Test RE 0.13394027712905537 Lambda1 0.44313887\n",
      "66 Train Loss 64.15173 Test MSE 62.83480601552787 Test RE 0.13344100678278384 Lambda1 0.44609186\n",
      "67 Train Loss 63.71707 Test MSE 62.22523518891947 Test RE 0.1327921626933809 Lambda1 0.45020044\n",
      "68 Train Loss 63.199173 Test MSE 62.30680450224747 Test RE 0.13287917094697602 Lambda1 0.4490747\n",
      "69 Train Loss 62.690823 Test MSE 61.50764317819343 Test RE 0.13202425143360305 Lambda1 0.45296353\n",
      "70 Train Loss 62.09994 Test MSE 61.210502112858954 Test RE 0.13170496330184076 Lambda1 0.45569822\n",
      "71 Train Loss 60.78072 Test MSE 60.206954089375955 Test RE 0.13062084799463897 Lambda1 0.45924783\n",
      "72 Train Loss 60.20795 Test MSE 59.45283272061086 Test RE 0.12980022541270428 Lambda1 0.46233782\n",
      "73 Train Loss 59.5238 Test MSE 59.30227834752512 Test RE 0.12963577253412964 Lambda1 0.4625739\n",
      "74 Train Loss 58.97964 Test MSE 59.09769546735177 Test RE 0.12941196854658524 Lambda1 0.4647016\n",
      "75 Train Loss 58.41348 Test MSE 58.63167790948618 Test RE 0.12890071670330486 Lambda1 0.46723017\n",
      "76 Train Loss 57.812466 Test MSE 57.744277254101284 Test RE 0.12792153014015403 Lambda1 0.47266713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 Train Loss 57.55304 Test MSE 57.29496307762919 Test RE 0.12742287291070326 Lambda1 0.47501564\n",
      "78 Train Loss 56.931114 Test MSE 56.14829050440956 Test RE 0.126141339870504 Lambda1 0.48195142\n",
      "79 Train Loss 56.253033 Test MSE 55.604251842785125 Test RE 0.12552874054128146 Lambda1 0.48496822\n",
      "80 Train Loss 55.68016 Test MSE 54.749396267721046 Test RE 0.12456006835147833 Lambda1 0.48980248\n",
      "81 Train Loss 55.19895 Test MSE 54.206765805330235 Test RE 0.12394126338383632 Lambda1 0.49413314\n",
      "82 Train Loss 54.740166 Test MSE 53.71027769821896 Test RE 0.12337235916779964 Lambda1 0.4962427\n",
      "83 Train Loss 54.085953 Test MSE 52.917307103002464 Test RE 0.12245824703692625 Lambda1 0.50102186\n",
      "84 Train Loss 53.573174 Test MSE 52.69928065792586 Test RE 0.12220571440113899 Lambda1 0.50312334\n",
      "85 Train Loss 53.164204 Test MSE 52.33578420230864 Test RE 0.12178352449144766 Lambda1 0.505277\n",
      "86 Train Loss 52.77444 Test MSE 51.85211477926768 Test RE 0.12121947743738462 Lambda1 0.5091128\n",
      "87 Train Loss 52.6487 Test MSE 51.61790815331924 Test RE 0.12094540437287761 Lambda1 0.5113542\n",
      "88 Train Loss 52.315517 Test MSE 51.1315611731895 Test RE 0.12037427852098002 Lambda1 0.5161007\n",
      "89 Train Loss 51.64586 Test MSE 51.350623077388974 Test RE 0.12063186145913465 Lambda1 0.5148247\n",
      "90 Train Loss 51.293907 Test MSE 50.90122651396037 Test RE 0.12010284475901217 Lambda1 0.51763517\n",
      "91 Train Loss 50.887447 Test MSE 50.95995639318301 Test RE 0.12017211216770779 Lambda1 0.5160417\n",
      "92 Train Loss 50.40426 Test MSE 50.26966211316823 Test RE 0.11935542229196115 Lambda1 0.52088743\n",
      "93 Train Loss 49.62198 Test MSE 49.37028979871968 Test RE 0.11828291227848393 Lambda1 0.52590585\n",
      "94 Train Loss 49.297882 Test MSE 49.08088889013439 Test RE 0.11793572478525735 Lambda1 0.52775663\n",
      "95 Train Loss 48.637722 Test MSE 48.51954394016735 Test RE 0.1172593616685409 Lambda1 0.5306521\n",
      "96 Train Loss 47.877975 Test MSE 47.845263428743905 Test RE 0.11644172904874964 Lambda1 0.5351446\n",
      "97 Train Loss 46.893013 Test MSE 47.11169944347519 Test RE 0.11554563827433176 Lambda1 0.54003805\n",
      "98 Train Loss 46.61338 Test MSE 46.71582016845512 Test RE 0.1150591495400705 Lambda1 0.5424295\n",
      "99 Train Loss 46.217976 Test MSE 46.85075976587275 Test RE 0.11522520506054039 Lambda1 0.5424712\n",
      "100 Train Loss 45.868004 Test MSE 46.48870255355176 Test RE 0.11477911808279301 Lambda1 0.54499835\n",
      "101 Train Loss 45.680325 Test MSE 46.457759731237125 Test RE 0.1147409133057036 Lambda1 0.54512906\n",
      "102 Train Loss 45.365353 Test MSE 46.051845007062305 Test RE 0.1142385515026396 Lambda1 0.5479157\n",
      "103 Train Loss 45.013412 Test MSE 46.11105389479202 Test RE 0.11431196620367956 Lambda1 0.5481097\n",
      "104 Train Loss 44.73002 Test MSE 45.80754818214807 Test RE 0.11393514101168259 Lambda1 0.5504316\n",
      "105 Train Loss 44.63668 Test MSE 45.92997540560083 Test RE 0.1140872933963741 Lambda1 0.5494276\n",
      "106 Train Loss 44.357204 Test MSE 45.695199151998935 Test RE 0.11379533478466053 Lambda1 0.55263114\n",
      "107 Train Loss 44.189762 Test MSE 45.46831227142272 Test RE 0.11351247359036327 Lambda1 0.5554383\n",
      "108 Train Loss 44.03266 Test MSE 45.30633886084815 Test RE 0.11331010841468982 Lambda1 0.55694073\n",
      "109 Train Loss 43.947083 Test MSE 45.18611767724555 Test RE 0.11315967334933211 Lambda1 0.55801094\n",
      "110 Train Loss 43.82445 Test MSE 44.972837804542245 Test RE 0.11289229889353583 Lambda1 0.55967945\n",
      "111 Train Loss 43.687153 Test MSE 44.71742941654213 Test RE 0.11257127518307415 Lambda1 0.56173396\n",
      "112 Train Loss 43.460514 Test MSE 44.41371069092825 Test RE 0.11218833438222382 Lambda1 0.56517595\n",
      "113 Train Loss 43.19974 Test MSE 44.21102853125342 Test RE 0.1119320556925348 Lambda1 0.56723416\n",
      "114 Train Loss 42.9612 Test MSE 44.013183284405976 Test RE 0.11168132576611339 Lambda1 0.5702764\n",
      "115 Train Loss 42.824066 Test MSE 43.78904136922801 Test RE 0.11139658814973487 Lambda1 0.57292867\n",
      "116 Train Loss 42.704166 Test MSE 43.888685483521016 Test RE 0.11152326033222675 Lambda1 0.57200867\n",
      "117 Train Loss 42.583862 Test MSE 43.70507938918439 Test RE 0.11128973988854211 Lambda1 0.57415855\n",
      "118 Train Loss 42.486977 Test MSE 43.74750117855688 Test RE 0.11134373778655422 Lambda1 0.5729503\n",
      "119 Train Loss 42.414925 Test MSE 43.77665560240351 Test RE 0.11138083272667557 Lambda1 0.57256305\n",
      "120 Train Loss 42.35271 Test MSE 43.58882464239079 Test RE 0.11114162694764967 Lambda1 0.57436574\n",
      "121 Train Loss 42.27885 Test MSE 43.38045515993006 Test RE 0.1108756612300454 Lambda1 0.57610613\n",
      "122 Train Loss 42.160732 Test MSE 43.16647456648571 Test RE 0.1106018677767902 Lambda1 0.5785211\n",
      "123 Train Loss 42.01207 Test MSE 42.87972934177087 Test RE 0.11023390396244634 Lambda1 0.5825488\n",
      "124 Train Loss 41.78694 Test MSE 42.44943839322344 Test RE 0.10967941997507953 Lambda1 0.5872631\n",
      "125 Train Loss 41.658237 Test MSE 42.29305531150396 Test RE 0.10947720491118954 Lambda1 0.58904094\n",
      "126 Train Loss 41.492577 Test MSE 42.08394055841718 Test RE 0.10920621872658388 Lambda1 0.5915861\n",
      "127 Train Loss 41.256813 Test MSE 41.546966980003646 Test RE 0.10850726902633559 Lambda1 0.59806544\n",
      "128 Train Loss 41.061195 Test MSE 41.6126251259665 Test RE 0.108592974122487 Lambda1 0.5972603\n",
      "129 Train Loss 40.864788 Test MSE 41.55870488092469 Test RE 0.10852259574835879 Lambda1 0.59779036\n",
      "130 Train Loss 40.674614 Test MSE 41.34400061258439 Test RE 0.1082419032129702 Lambda1 0.60078377\n",
      "131 Train Loss 40.57885 Test MSE 41.22890331510572 Test RE 0.10809113123735471 Lambda1 0.6023926\n",
      "132 Train Loss 40.379757 Test MSE 40.97306547774724 Test RE 0.10775524023974484 Lambda1 0.60669637\n",
      "133 Train Loss 40.219204 Test MSE 40.77505192820379 Test RE 0.10749454654922184 Lambda1 0.6088402\n",
      "134 Train Loss 40.017475 Test MSE 40.689652972389 Test RE 0.1073819196704522 Lambda1 0.61002016\n",
      "135 Train Loss 39.818077 Test MSE 40.445805777782105 Test RE 0.10705967400922647 Lambda1 0.61375517\n",
      "136 Train Loss 39.535652 Test MSE 40.13445510392989 Test RE 0.10664680673625994 Lambda1 0.61794156\n",
      "137 Train Loss 39.29925 Test MSE 39.826799756821245 Test RE 0.10623726360776016 Lambda1 0.62412596\n",
      "138 Train Loss 39.136242 Test MSE 39.66801671482388 Test RE 0.10602527666974643 Lambda1 0.62676907\n",
      "139 Train Loss 39.029892 Test MSE 39.480990900789344 Test RE 0.10577503865114428 Lambda1 0.6296843\n",
      "140 Train Loss 38.896797 Test MSE 39.32432266049733 Test RE 0.10556496208880019 Lambda1 0.6321542\n",
      "141 Train Loss 38.710697 Test MSE 39.35799481418821 Test RE 0.10561014836017928 Lambda1 0.6301881\n",
      "142 Train Loss 38.56748 Test MSE 39.23127152587691 Test RE 0.10543999163033273 Lambda1 0.6323541\n",
      "143 Train Loss 38.409386 Test MSE 39.176146111507514 Test RE 0.10536588663378388 Lambda1 0.6334896\n",
      "144 Train Loss 38.103127 Test MSE 39.01135529420428 Test RE 0.10514404719687069 Lambda1 0.6343742\n",
      "145 Train Loss 37.883106 Test MSE 38.798064533783695 Test RE 0.10485622085718792 Lambda1 0.63897634\n",
      "146 Train Loss 37.82129 Test MSE 38.85024652095749 Test RE 0.10492671106715021 Lambda1 0.63843733\n",
      "147 Train Loss 37.677 Test MSE 38.789778142179074 Test RE 0.1048450227972218 Lambda1 0.6405815\n",
      "148 Train Loss 37.481667 Test MSE 38.5523623319811 Test RE 0.10452367433406184 Lambda1 0.6423366\n",
      "149 Train Loss 37.052197 Test MSE 38.13724080311879 Test RE 0.10395940977243917 Lambda1 0.64661187\n",
      "150 Train Loss 36.95096 Test MSE 37.96693228309303 Test RE 0.10372702557927832 Lambda1 0.64978886\n",
      "151 Train Loss 36.89309 Test MSE 37.85616740899176 Test RE 0.10357560825078185 Lambda1 0.6516741\n",
      "152 Train Loss 36.794098 Test MSE 37.652696335607345 Test RE 0.10329688174162698 Lambda1 0.65538806\n",
      "153 Train Loss 36.722527 Test MSE 37.66007217946304 Test RE 0.10330699873769612 Lambda1 0.6550838\n",
      "154 Train Loss 36.569424 Test MSE 37.56914696612995 Test RE 0.10318221289857017 Lambda1 0.6568674\n",
      "155 Train Loss 36.430084 Test MSE 37.403227724197315 Test RE 0.10295411538037204 Lambda1 0.6608526\n",
      "156 Train Loss 36.36046 Test MSE 37.20741946308651 Test RE 0.10268427611006767 Lambda1 0.6632208\n",
      "157 Train Loss 36.224342 Test MSE 37.04614255401104 Test RE 0.10246149006990668 Lambda1 0.665349\n",
      "158 Train Loss 36.07016 Test MSE 36.908826200694186 Test RE 0.10227142034662695 Lambda1 0.66784304\n",
      "159 Train Loss 35.849155 Test MSE 36.72057779317384 Test RE 0.10201027625996067 Lambda1 0.6708012\n",
      "160 Train Loss 35.651558 Test MSE 36.48332399929256 Test RE 0.10168019505649663 Lambda1 0.6745418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 Train Loss 35.49566 Test MSE 36.470583369032035 Test RE 0.10166243923345093 Lambda1 0.6742605\n",
      "162 Train Loss 35.221207 Test MSE 36.279029431277266 Test RE 0.10139510763803541 Lambda1 0.6764082\n",
      "163 Train Loss 34.924862 Test MSE 36.08261330559487 Test RE 0.10112025652228515 Lambda1 0.67911094\n",
      "164 Train Loss 34.679047 Test MSE 36.003711869479375 Test RE 0.10100963676465367 Lambda1 0.6795372\n",
      "165 Train Loss 34.4626 Test MSE 35.763241143080705 Test RE 0.10067174722774691 Lambda1 0.68287283\n",
      "166 Train Loss 34.292892 Test MSE 35.59848030412347 Test RE 0.10043958273581352 Lambda1 0.68521297\n",
      "167 Train Loss 34.243416 Test MSE 35.55073421573947 Test RE 0.1003722033667512 Lambda1 0.68642133\n",
      "168 Train Loss 34.21713 Test MSE 35.52000516394549 Test RE 0.10032881453812516 Lambda1 0.6872072\n",
      "169 Train Loss 34.159313 Test MSE 35.61470534343577 Test RE 0.10046246925397807 Lambda1 0.6859534\n",
      "170 Train Loss 34.11338 Test MSE 35.459789086764864 Test RE 0.10024373614016306 Lambda1 0.6891276\n",
      "171 Train Loss 34.092342 Test MSE 35.45011480983511 Test RE 0.1002300607652798 Lambda1 0.6892306\n",
      "172 Train Loss 34.034706 Test MSE 35.39992802717467 Test RE 0.10015908771077678 Lambda1 0.6909988\n",
      "173 Train Loss 33.94658 Test MSE 35.48347555928242 Test RE 0.10027721101660192 Lambda1 0.6903495\n",
      "174 Train Loss 33.856003 Test MSE 35.38503878353765 Test RE 0.10013802199136274 Lambda1 0.6924687\n",
      "175 Train Loss 33.778664 Test MSE 35.43129569832119 Test RE 0.10020345307866924 Lambda1 0.69173384\n",
      "176 Train Loss 33.723904 Test MSE 35.347405533955644 Test RE 0.10008475764821628 Lambda1 0.69294965\n",
      "177 Train Loss 33.679554 Test MSE 35.34705242785814 Test RE 0.10008425774410849 Lambda1 0.6930678\n",
      "178 Train Loss 33.630634 Test MSE 35.3462697105817 Test RE 0.10008314961618929 Lambda1 0.6930743\n",
      "179 Train Loss 33.558235 Test MSE 35.20140047230163 Test RE 0.09987784002397723 Lambda1 0.6948593\n",
      "180 Train Loss 33.46066 Test MSE 35.075949675587516 Test RE 0.09969970878825606 Lambda1 0.6953356\n",
      "181 Train Loss 33.39368 Test MSE 34.98877580247171 Test RE 0.09957574041951793 Lambda1 0.69672203\n",
      "182 Train Loss 33.341736 Test MSE 34.93600844030353 Test RE 0.09950062587802291 Lambda1 0.69781756\n",
      "183 Train Loss 33.27814 Test MSE 34.795789778571745 Test RE 0.09930074797810533 Lambda1 0.70041764\n",
      "184 Train Loss 33.20837 Test MSE 34.72948311619001 Test RE 0.09920608938641247 Lambda1 0.7022254\n",
      "185 Train Loss 33.123276 Test MSE 34.65649811508439 Test RE 0.09910179234604397 Lambda1 0.7033437\n",
      "186 Train Loss 33.00278 Test MSE 34.62421429001433 Test RE 0.09905562307713019 Lambda1 0.7043449\n",
      "187 Train Loss 32.91361 Test MSE 34.58467157755041 Test RE 0.09899904349725787 Lambda1 0.7045768\n",
      "188 Train Loss 32.748516 Test MSE 34.51441664296379 Test RE 0.09889843957700825 Lambda1 0.70605135\n",
      "189 Train Loss 32.685455 Test MSE 34.59300813303369 Test RE 0.09901097452410337 Lambda1 0.7049607\n",
      "190 Train Loss 32.270554 Test MSE 34.0877272946746 Test RE 0.09828521545486216 Lambda1 0.7147669\n",
      "191 Train Loss 32.074875 Test MSE 33.89310906957198 Test RE 0.09800424227046395 Lambda1 0.71866155\n",
      "192 Train Loss 31.993519 Test MSE 33.79740588468847 Test RE 0.09786577830566268 Lambda1 0.72038496\n",
      "193 Train Loss 31.930996 Test MSE 33.790221605501124 Test RE 0.09785537613981819 Lambda1 0.7208028\n",
      "194 Train Loss 31.860294 Test MSE 33.74279503619589 Test RE 0.09778667912983181 Lambda1 0.7214708\n",
      "195 Train Loss 31.821997 Test MSE 33.73165145437609 Test RE 0.0977705307366489 Lambda1 0.7212512\n",
      "196 Train Loss 31.801334 Test MSE 33.77730383251437 Test RE 0.09783666962785662 Lambda1 0.72052217\n",
      "197 Train Loss 31.770073 Test MSE 33.752390165432644 Test RE 0.09780058149224925 Lambda1 0.7211957\n",
      "198 Train Loss 31.713478 Test MSE 33.68317579288104 Test RE 0.0977002526009849 Lambda1 0.72291\n",
      "199 Train Loss 31.628962 Test MSE 33.64850936641082 Test RE 0.09764996354066668 Lambda1 0.72352475\n",
      "Training time: 616.69\n",
      "Training time: 616.69\n",
      "inv_HT_rowdy\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 848.88184 Test MSE 852.05607624675 Test RE 0.491386543847842 Lambda1 -0.06582655\n",
      "1 Train Loss 762.83655 Test MSE 742.7704714685358 Test RE 0.45879268590301664 Lambda1 -0.046761297\n",
      "2 Train Loss 472.2571 Test MSE 444.7496948144775 Test RE 0.35501524267378776 Lambda1 -0.00045154133\n",
      "3 Train Loss 344.54166 Test MSE 351.8418564891396 Test RE 0.3157642235668138 Lambda1 0.000553455\n",
      "4 Train Loss 308.94348 Test MSE 309.30039787300836 Test RE 0.2960597785562752 Lambda1 0.0013416677\n",
      "5 Train Loss 285.66098 Test MSE 291.1374737012409 Test RE 0.2872355734252882 Lambda1 0.0042823427\n",
      "6 Train Loss 274.3219 Test MSE 280.6755757998359 Test RE 0.2820275156486944 Lambda1 0.00580447\n",
      "7 Train Loss 269.48865 Test MSE 276.3713698507931 Test RE 0.27985669192655077 Lambda1 0.00921554\n",
      "8 Train Loss 264.28986 Test MSE 270.19221359718716 Test RE 0.276710465727666 Lambda1 0.019286875\n",
      "9 Train Loss 259.01965 Test MSE 265.06927124068284 Test RE 0.2740746465242966 Lambda1 0.034121577\n",
      "10 Train Loss 249.48364 Test MSE 254.58509845252433 Test RE 0.26859978484560715 Lambda1 0.053203773\n",
      "11 Train Loss 237.95781 Test MSE 239.9559650351224 Test RE 0.26076839013366465 Lambda1 0.08250043\n",
      "12 Train Loss 224.80804 Test MSE 225.7371783335066 Test RE 0.25292439318447707 Lambda1 0.11554321\n",
      "13 Train Loss 208.09483 Test MSE 207.44400074155195 Test RE 0.24245972621944487 Lambda1 0.16474746\n",
      "14 Train Loss 181.50157 Test MSE 176.1771127606485 Test RE 0.22344153942207526 Lambda1 0.22003092\n",
      "15 Train Loss 148.30998 Test MSE 138.10920010470127 Test RE 0.19783379206578808 Lambda1 0.28055182\n",
      "16 Train Loss 124.18745 Test MSE 112.42401722949951 Test RE 0.1784919854932864 Lambda1 0.32245702\n",
      "17 Train Loss 104.568184 Test MSE 92.70444797524706 Test RE 0.16208374278393406 Lambda1 0.35436732\n",
      "18 Train Loss 88.89417 Test MSE 82.60710486944642 Test RE 0.1530022690364551 Lambda1 0.3940136\n",
      "19 Train Loss 79.24576 Test MSE 77.2571594460995 Test RE 0.14796484336082172 Lambda1 0.41954795\n",
      "20 Train Loss 70.72624 Test MSE 71.521999887612 Test RE 0.14236688916642587 Lambda1 0.44494715\n",
      "21 Train Loss 64.51608 Test MSE 67.81294098985649 Test RE 0.138626245303396 Lambda1 0.47249985\n",
      "22 Train Loss 59.264984 Test MSE 63.84558283330929 Test RE 0.134510008020107 Lambda1 0.49926814\n",
      "23 Train Loss 54.83781 Test MSE 57.04137386743427 Test RE 0.12714057148351762 Lambda1 0.5217088\n",
      "24 Train Loss 52.371513 Test MSE 50.71469617137131 Test RE 0.11988258104335711 Lambda1 0.55798423\n",
      "25 Train Loss 46.284435 Test MSE 47.17381617120205 Test RE 0.11562178658541566 Lambda1 0.5941536\n",
      "26 Train Loss 44.053844 Test MSE 45.99177304451222 Test RE 0.11416401840421679 Lambda1 0.6077106\n",
      "27 Train Loss 41.203 Test MSE 44.75166322444865 Test RE 0.11261435688719233 Lambda1 0.6264047\n",
      "28 Train Loss 39.473335 Test MSE 43.367272634612725 Test RE 0.11085881340926176 Lambda1 0.64676934\n",
      "29 Train Loss 38.461937 Test MSE 42.737130870334525 Test RE 0.11005045741394867 Lambda1 0.66454065\n",
      "30 Train Loss 37.73933 Test MSE 42.010184668888044 Test RE 0.10911047990202566 Lambda1 0.68672425\n",
      "31 Train Loss 36.824448 Test MSE 41.378167380961216 Test RE 0.10828661964377939 Lambda1 0.7008\n",
      "32 Train Loss 36.05373 Test MSE 40.79593499456587 Test RE 0.10752206985445742 Lambda1 0.71427786\n",
      "33 Train Loss 35.773567 Test MSE 40.21191089107367 Test RE 0.10674966637050552 Lambda1 0.73382306\n",
      "34 Train Loss 35.365345 Test MSE 39.819851978551306 Test RE 0.10622799666753667 Lambda1 0.7486361\n",
      "35 Train Loss 34.915054 Test MSE 39.441378840950655 Test RE 0.10572196223941198 Lambda1 0.7608632\n",
      "36 Train Loss 34.475193 Test MSE 38.648240396526894 Test RE 0.10465356656306544 Lambda1 0.7815695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 Train Loss 34.063187 Test MSE 37.82641172880244 Test RE 0.10353489403240985 Lambda1 0.807009\n",
      "38 Train Loss 33.839424 Test MSE 37.34423580533554 Test RE 0.10287289434908282 Lambda1 0.82499653\n",
      "39 Train Loss 33.480133 Test MSE 36.81867891149096 Test RE 0.10214644850876659 Lambda1 0.84021056\n",
      "40 Train Loss 33.321663 Test MSE 36.798871031134375 Test RE 0.10211896820677557 Lambda1 0.84094095\n",
      "41 Train Loss 33.024643 Test MSE 36.75842280082404 Test RE 0.10206282970180172 Lambda1 0.84388375\n",
      "42 Train Loss 32.74638 Test MSE 36.61766262040882 Test RE 0.10186722606823014 Lambda1 0.8537569\n",
      "43 Train Loss 32.68953 Test MSE 36.50788691596815 Test RE 0.10171441811578287 Lambda1 0.857277\n",
      "44 Train Loss 32.633312 Test MSE 36.46627892621426 Test RE 0.10165643969863854 Lambda1 0.85774934\n",
      "45 Train Loss 32.59558 Test MSE 36.416466505790034 Test RE 0.10158698534384614 Lambda1 0.86039925\n",
      "46 Train Loss 32.50282 Test MSE 36.17969986569927 Test RE 0.10125620597224018 Lambda1 0.86663634\n",
      "47 Train Loss 32.435375 Test MSE 36.14276167470872 Test RE 0.10120450327383473 Lambda1 0.8655213\n",
      "48 Train Loss 32.392036 Test MSE 36.17633288807174 Test RE 0.10125149427875332 Lambda1 0.8635722\n",
      "49 Train Loss 32.362564 Test MSE 36.241163405422824 Test RE 0.10134217854654184 Lambda1 0.8615889\n",
      "50 Train Loss 32.291515 Test MSE 36.13245943580876 Test RE 0.10119007843146596 Lambda1 0.8668622\n",
      "51 Train Loss 32.031445 Test MSE 35.88656692306709 Test RE 0.10084517581599177 Lambda1 0.8828912\n",
      "52 Train Loss 31.955317 Test MSE 35.712081773805835 Test RE 0.10059971591675657 Lambda1 0.8898113\n",
      "53 Train Loss 31.740618 Test MSE 35.508427101937485 Test RE 0.10031246167013533 Lambda1 0.8944967\n",
      "54 Train Loss 31.610872 Test MSE 35.26793350902782 Test RE 0.0999721834153587 Lambda1 0.90425915\n",
      "55 Train Loss 31.533428 Test MSE 35.12731256927293 Test RE 0.09977267890449909 Lambda1 0.9068422\n",
      "56 Train Loss 31.345535 Test MSE 34.85476697885102 Test RE 0.09938486735901422 Lambda1 0.9153477\n",
      "57 Train Loss 31.25314 Test MSE 34.77020840082896 Test RE 0.09926423900645828 Lambda1 0.91844696\n",
      "58 Train Loss 31.135603 Test MSE 34.73746167720905 Test RE 0.09921748426350051 Lambda1 0.91377443\n",
      "59 Train Loss 31.055689 Test MSE 34.72565190607671 Test RE 0.09920061723679174 Lambda1 0.90748304\n",
      "60 Train Loss 30.961954 Test MSE 34.57572180261654 Test RE 0.0989862332485887 Lambda1 0.9142482\n",
      "61 Train Loss 30.730213 Test MSE 34.37062131566857 Test RE 0.09869220725482428 Lambda1 0.9269097\n",
      "62 Train Loss 30.621246 Test MSE 34.34208679779995 Test RE 0.09865123157304091 Lambda1 0.9308813\n",
      "63 Train Loss 30.502306 Test MSE 34.17111056306021 Test RE 0.09840535160847652 Lambda1 0.9407588\n",
      "64 Train Loss 30.428679 Test MSE 34.084462221700534 Test RE 0.09828050824627016 Lambda1 0.94727755\n",
      "65 Train Loss 30.389925 Test MSE 33.99965682307613 Test RE 0.09815816645196884 Lambda1 0.9482546\n",
      "66 Train Loss 30.334555 Test MSE 33.96032765727503 Test RE 0.09810137770446208 Lambda1 0.9472552\n",
      "67 Train Loss 30.174736 Test MSE 33.78740626199166 Test RE 0.09785129948355928 Lambda1 0.9408527\n",
      "68 Train Loss 30.096348 Test MSE 33.73953922055957 Test RE 0.097781961336316 Lambda1 0.9376923\n",
      "69 Train Loss 30.055 Test MSE 33.73659912069948 Test RE 0.09777770083067813 Lambda1 0.9364081\n",
      "70 Train Loss 29.994139 Test MSE 33.668836624447835 Test RE 0.09767945453972203 Lambda1 0.9319777\n",
      "71 Train Loss 29.92382 Test MSE 33.57769512272006 Test RE 0.0975471558608305 Lambda1 0.9288716\n",
      "72 Train Loss 29.869776 Test MSE 33.54372320675102 Test RE 0.09749779716621654 Lambda1 0.9296155\n",
      "73 Train Loss 29.794832 Test MSE 33.41108103302583 Test RE 0.09730483811990603 Lambda1 0.9318382\n",
      "74 Train Loss 29.607882 Test MSE 33.20982479738223 Test RE 0.09701133092840881 Lambda1 0.9427278\n",
      "75 Train Loss 29.441132 Test MSE 33.03520211482481 Test RE 0.09675594405858944 Lambda1 0.9532927\n",
      "76 Train Loss 29.402082 Test MSE 33.02253223619705 Test RE 0.0967373880402851 Lambda1 0.95715696\n",
      "77 Train Loss 29.356573 Test MSE 32.9886009280584 Test RE 0.09668767547248998 Lambda1 0.9594337\n",
      "78 Train Loss 29.30468 Test MSE 32.90661860858312 Test RE 0.09656745802472279 Lambda1 0.9628181\n",
      "79 Train Loss 29.267097 Test MSE 32.86163462976905 Test RE 0.09650143066815833 Lambda1 0.9634943\n",
      "80 Train Loss 29.238943 Test MSE 32.87447140412996 Test RE 0.09652027705473269 Lambda1 0.95864433\n",
      "81 Train Loss 29.199362 Test MSE 32.83351908496806 Test RE 0.09646013979949451 Lambda1 0.95437807\n",
      "82 Train Loss 29.17178 Test MSE 32.76153267580501 Test RE 0.09635433893673544 Lambda1 0.95769465\n",
      "83 Train Loss 29.160992 Test MSE 32.736485037782124 Test RE 0.09631749832092949 Lambda1 0.9569399\n",
      "84 Train Loss 29.141975 Test MSE 32.74718173619241 Test RE 0.096333232983377 Lambda1 0.9516469\n",
      "85 Train Loss 29.127932 Test MSE 32.72595621926514 Test RE 0.0963020080963055 Lambda1 0.94924355\n",
      "86 Train Loss 29.097439 Test MSE 32.724769645478496 Test RE 0.09630026222716338 Lambda1 0.9495748\n",
      "87 Train Loss 29.072828 Test MSE 32.6766735492863 Test RE 0.09622946921952355 Lambda1 0.9553666\n",
      "88 Train Loss 29.061592 Test MSE 32.671268808738375 Test RE 0.09622151068572105 Lambda1 0.95636874\n",
      "89 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "90 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "91 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "92 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "93 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "94 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "95 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "96 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "97 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "98 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "99 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "100 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "101 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "102 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "103 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "104 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "105 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "106 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "107 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "108 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "109 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "110 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "111 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "112 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "113 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "114 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "115 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "116 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "117 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "118 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "119 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "120 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "121 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "123 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "124 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "125 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "126 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "127 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "128 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "129 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "130 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "131 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "132 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "133 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "134 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "135 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "136 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "137 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "138 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "139 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "140 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "141 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "142 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "143 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "144 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "145 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "146 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "147 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "148 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "149 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "150 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "151 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "152 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "153 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "154 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "155 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "156 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "157 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "158 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "159 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "160 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "161 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "162 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "163 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "164 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "165 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "166 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "167 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "168 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "169 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "170 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "171 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "172 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "173 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "174 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "175 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "176 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "177 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "178 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "179 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "180 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "181 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "182 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "183 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "184 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "185 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "186 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "187 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "188 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "189 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "190 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "191 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "192 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "193 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "194 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "195 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "196 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "197 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "198 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "199 Train Loss 29.057074 Test MSE 32.66841149379512 Test RE 0.09621730299526626 Lambda1 0.957203\n",
      "Training time: 378.99\n",
      "Training time: 378.99\n",
      "inv_HT_rowdy\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 854.82446 Test MSE 858.2180950977385 Test RE 0.49316018203840983 Lambda1 -0.09072175\n",
      "1 Train Loss 854.4488 Test MSE 857.7111084749391 Test RE 0.4930144949730989 Lambda1 -0.09425303\n",
      "2 Train Loss 853.2413 Test MSE 855.7520113749302 Test RE 0.49245112612989084 Lambda1 -0.087481916\n",
      "3 Train Loss 803.2538 Test MSE 766.2684305492634 Test RE 0.4659932622956125 Lambda1 -0.034067307\n",
      "4 Train Loss 643.87463 Test MSE 563.709659927569 Test RE 0.39968413444887974 Lambda1 -0.000118473086\n",
      "5 Train Loss 412.43616 Test MSE 359.24863567287906 Test RE 0.31907055985988836 Lambda1 -0.0069470135\n",
      "6 Train Loss 309.69113 Test MSE 305.7791447197967 Test RE 0.29436969740003766 Lambda1 -0.008074892\n",
      "7 Train Loss 288.76694 Test MSE 293.401432874451 Test RE 0.2883502191805724 Lambda1 -0.0029036207\n",
      "8 Train Loss 276.41116 Test MSE 282.6542888016081 Test RE 0.2830198916706297 Lambda1 0.0009738741\n",
      "9 Train Loss 272.03104 Test MSE 278.8056020438598 Test RE 0.28108645513624836 Lambda1 0.0042996285\n",
      "10 Train Loss 269.45013 Test MSE 274.2951040547204 Test RE 0.2788034853168147 Lambda1 0.013613413\n",
      "11 Train Loss 264.61566 Test MSE 268.71068912568745 Test RE 0.2759507901738815 Lambda1 0.02196721\n",
      "12 Train Loss 262.0461 Test MSE 264.1138327465437 Test RE 0.27358025149018583 Lambda1 0.033558354\n",
      "13 Train Loss 258.87567 Test MSE 259.4020958299898 Test RE 0.27112896171571904 Lambda1 0.046191394\n",
      "14 Train Loss 255.93016 Test MSE 257.20639022539405 Test RE 0.2699790394161859 Lambda1 0.04558114\n",
      "15 Train Loss 249.1884 Test MSE 249.29585837254092 Test RE 0.2657949359977586 Lambda1 0.07001274\n",
      "16 Train Loss 247.75278 Test MSE 250.0519221836875 Test RE 0.2661976819519956 Lambda1 0.06571031\n",
      "17 Train Loss 243.4942 Test MSE 243.27594680700736 Test RE 0.26256616221286044 Lambda1 0.085922524\n",
      "18 Train Loss 234.52895 Test MSE 232.79385535810664 Test RE 0.2568472540503744 Lambda1 0.122331835\n",
      "19 Train Loss 224.85454 Test MSE 223.88384933641032 Test RE 0.2518839836847937 Lambda1 0.14332661\n",
      "20 Train Loss 217.87422 Test MSE 217.45606285348507 Test RE 0.24824181103513168 Lambda1 0.19018485\n",
      "21 Train Loss 215.00479 Test MSE 214.63910197110152 Test RE 0.24662868762803244 Lambda1 0.21874705\n",
      "22 Train Loss 210.42117 Test MSE 208.15659501485666 Test RE 0.24287580788529795 Lambda1 0.2417074\n",
      "23 Train Loss 198.96718 Test MSE 194.7210295090837 Test RE 0.23490680562101068 Lambda1 0.24929461\n",
      "24 Train Loss 178.94855 Test MSE 176.03501949902278 Test RE 0.22335141437406486 Lambda1 0.2807236\n",
      "25 Train Loss 142.67189 Test MSE 135.80052682899992 Test RE 0.19617330010326864 Lambda1 0.40032524\n",
      "26 Train Loss 120.78755 Test MSE 102.4971383158643 Test RE 0.17042960629186593 Lambda1 0.42499873\n",
      "27 Train Loss 92.655655 Test MSE 71.933795759776 Test RE 0.14277614752662185 Lambda1 0.4515435\n",
      "28 Train Loss 77.057976 Test MSE 60.96040798075127 Test RE 0.13143562755494592 Lambda1 0.50920856\n",
      "29 Train Loss 67.37586 Test MSE 59.962732329364364 Test RE 0.1303556554626971 Lambda1 0.5076417\n",
      "30 Train Loss 55.710052 Test MSE 52.174994037140685 Test RE 0.1215963040599883 Lambda1 0.5333481\n",
      "31 Train Loss 52.00397 Test MSE 50.64209128294955 Test RE 0.11979673631226889 Lambda1 0.5429569\n",
      "32 Train Loss 48.278744 Test MSE 47.56907055227175 Test RE 0.11610515526179338 Lambda1 0.5764212\n",
      "33 Train Loss 46.256805 Test MSE 44.85441754497461 Test RE 0.11274356970420148 Lambda1 0.61245126\n",
      "34 Train Loss 44.16844 Test MSE 44.07842180319918 Test RE 0.11176406491193347 Lambda1 0.6217091\n",
      "35 Train Loss 42.854797 Test MSE 42.961242650237146 Test RE 0.11033863018263484 Lambda1 0.64208794\n",
      "36 Train Loss 41.36336 Test MSE 41.775251608119774 Test RE 0.10880496354368416 Lambda1 0.67574036\n",
      "37 Train Loss 39.778286 Test MSE 39.93710085418681 Test RE 0.10638427497544281 Lambda1 0.7247921\n",
      "38 Train Loss 38.911842 Test MSE 39.6461360947899 Test RE 0.10599603120975759 Lambda1 0.7424317\n",
      "39 Train Loss 37.84826 Test MSE 39.31909242353871 Test RE 0.10555794164841127 Lambda1 0.7494546\n",
      "40 Train Loss 37.326332 Test MSE 39.25949559272898 Test RE 0.10547791304104578 Lambda1 0.74800336\n",
      "41 Train Loss 36.475494 Test MSE 38.931158967865265 Test RE 0.10503591836802104 Lambda1 0.75328183\n",
      "42 Train Loss 35.51372 Test MSE 38.41982642257894 Test RE 0.10434385309824154 Lambda1 0.7846247\n",
      "43 Train Loss 35.23201 Test MSE 38.150794723418294 Test RE 0.10397788164340774 Lambda1 0.7988984\n",
      "44 Train Loss 34.75893 Test MSE 37.757462661078904 Test RE 0.10344049054428282 Lambda1 0.8182051\n",
      "45 Train Loss 34.55993 Test MSE 37.58914438031977 Test RE 0.10320967030861115 Lambda1 0.8336765\n",
      "46 Train Loss 34.395542 Test MSE 37.37910873179296 Test RE 0.10292091570598726 Lambda1 0.8474915\n",
      "47 Train Loss 34.18646 Test MSE 37.307952144547976 Test RE 0.10282290655025877 Lambda1 0.8492845\n",
      "48 Train Loss 33.916912 Test MSE 37.362016680304144 Test RE 0.10289738209601035 Lambda1 0.8448987\n",
      "49 Train Loss 33.740852 Test MSE 37.41392951392193 Test RE 0.10296884291274395 Lambda1 0.8408242\n",
      "50 Train Loss 33.531258 Test MSE 37.332364058503416 Test RE 0.10285654138311486 Lambda1 0.84739053\n",
      "51 Train Loss 33.46054 Test MSE 37.285160528334124 Test RE 0.10279149423582241 Lambda1 0.85263294\n",
      "52 Train Loss 33.2411 Test MSE 36.946677977187235 Test RE 0.10232384903832055 Lambda1 0.86371243\n",
      "53 Train Loss 33.12578 Test MSE 36.71102562298861 Test RE 0.10199700736612066 Lambda1 0.86828977\n",
      "54 Train Loss 32.984684 Test MSE 36.49301884654889 Test RE 0.10169370408435348 Lambda1 0.86979014\n",
      "55 Train Loss 32.70868 Test MSE 36.25930718205462 Test RE 0.10136754334723154 Lambda1 0.8749461\n",
      "56 Train Loss 32.53518 Test MSE 36.042661254372376 Test RE 0.10106425890712363 Lambda1 0.87661576\n",
      "57 Train Loss 32.130913 Test MSE 35.70955903736736 Test RE 0.10059616262276574 Lambda1 0.90012676\n",
      "58 Train Loss 31.93568 Test MSE 35.56724797651585 Test RE 0.10039551273224337 Lambda1 0.9046149\n",
      "59 Train Loss 31.77291 Test MSE 35.29505277919872 Test RE 0.1000106128240778 Lambda1 0.9056039\n",
      "60 Train Loss 31.58662 Test MSE 35.032334943839885 Test RE 0.09963770435689699 Lambda1 0.905713\n",
      "61 Train Loss 31.461903 Test MSE 34.85111030678333 Test RE 0.09937965390570326 Lambda1 0.9077764\n",
      "62 Train Loss 31.41832 Test MSE 34.71159275673519 Test RE 0.09918053384874992 Lambda1 0.90980965\n",
      "63 Train Loss 31.3408 Test MSE 34.676795412429094 Test RE 0.09913080860884131 Lambda1 0.91588426\n",
      "64 Train Loss 31.266417 Test MSE 34.677225592663234 Test RE 0.09913142348662483 Lambda1 0.9166866\n",
      "65 Train Loss 31.156738 Test MSE 34.75175593527584 Test RE 0.09923789586508257 Lambda1 0.90853494\n",
      "66 Train Loss 30.988295 Test MSE 34.61527783472213 Test RE 0.09904283920188311 Lambda1 0.92300594\n",
      "67 Train Loss 30.851807 Test MSE 34.444909311604675 Test RE 0.09879880539979333 Lambda1 0.9274601\n",
      "68 Train Loss 30.839062 Test MSE 34.40016739947583 Test RE 0.0987346176291415 Lambda1 0.9294453\n",
      "69 Train Loss 30.823416 Test MSE 34.390065729926015 Test RE 0.09872011976783006 Lambda1 0.9286979\n",
      "70 Train Loss 30.74752 Test MSE 34.260965374827514 Test RE 0.09853464780446074 Lambda1 0.93140805\n",
      "71 Train Loss 30.636995 Test MSE 34.217269590558296 Test RE 0.09847179314834896 Lambda1 0.93629414\n",
      "72 Train Loss 30.582497 Test MSE 34.172979720636995 Test RE 0.09840804295491337 Lambda1 0.9357449\n",
      "73 Train Loss 30.560299 Test MSE 34.19473497295416 Test RE 0.09843936230716216 Lambda1 0.9344834\n",
      "74 Train Loss 30.534939 Test MSE 34.24112722792967 Test RE 0.09850611638623455 Lambda1 0.93221635\n",
      "75 Train Loss 30.483486 Test MSE 34.151450822885536 Test RE 0.09837703965203974 Lambda1 0.93117374\n",
      "76 Train Loss 30.444405 Test MSE 34.12307496256077 Test RE 0.09833616125347334 Lambda1 0.9295597\n",
      "77 Train Loss 30.408566 Test MSE 34.06627800211983 Test RE 0.09825428819388636 Lambda1 0.93294024\n",
      "78 Train Loss 30.381685 Test MSE 34.02966298672716 Test RE 0.0982014713073467 Lambda1 0.93666416\n",
      "79 Train Loss 30.360174 Test MSE 34.02234977443547 Test RE 0.09819091864926875 Lambda1 0.93973553\n",
      "80 Train Loss 30.35396 Test MSE 34.04402857382102 Test RE 0.09822219694511002 Lambda1 0.93807447\n",
      "81 Train Loss 30.334225 Test MSE 34.069956833425714 Test RE 0.09825959331106515 Lambda1 0.93430895\n",
      "82 Train Loss 30.314487 Test MSE 34.073774337455184 Test RE 0.09826509810064045 Lambda1 0.9333903\n",
      "83 Train Loss 30.257387 Test MSE 33.88441317213514 Test RE 0.09799166907293676 Lambda1 0.9424142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 Train Loss 30.210346 Test MSE 33.86494010415101 Test RE 0.0979635075601643 Lambda1 0.9492271\n",
      "85 Train Loss 30.202803 Test MSE 33.83987995488536 Test RE 0.09792725421886747 Lambda1 0.950543\n",
      "86 Train Loss 30.198196 Test MSE 33.82374612406533 Test RE 0.09790390705988884 Lambda1 0.95253414\n",
      "87 Train Loss 30.179459 Test MSE 33.78570201415564 Test RE 0.09784883162664465 Lambda1 0.95501584\n",
      "88 Train Loss 30.125238 Test MSE 33.68861578134184 Test RE 0.09770814180348045 Lambda1 0.9680521\n",
      "89 Train Loss 30.078436 Test MSE 33.679006295221555 Test RE 0.09769420546362158 Lambda1 0.97219825\n",
      "90 Train Loss 30.071358 Test MSE 33.68565503483673 Test RE 0.09770384813648011 Lambda1 0.9714576\n",
      "91 Train Loss 30.05516 Test MSE 33.69143511465712 Test RE 0.0977122302180649 Lambda1 0.9667503\n",
      "92 Train Loss 30.023855 Test MSE 33.66094736282264 Test RE 0.09766800977361845 Lambda1 0.965123\n",
      "93 Train Loss 29.99273 Test MSE 33.60624678892279 Test RE 0.09758862002347331 Lambda1 0.96735996\n",
      "94 Train Loss 29.972584 Test MSE 33.601168546231804 Test RE 0.0975812464327445 Lambda1 0.9718777\n",
      "95 Train Loss 29.952095 Test MSE 33.59213174324054 Test RE 0.09756812364809433 Lambda1 0.9762366\n",
      "96 Train Loss 29.923735 Test MSE 33.51895846704704 Test RE 0.09746180006825987 Lambda1 0.982453\n",
      "97 Train Loss 29.907633 Test MSE 33.495639052314026 Test RE 0.09742789168221466 Lambda1 0.9889287\n",
      "98 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "99 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "100 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "101 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "102 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "103 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "104 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "105 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "106 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "107 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "108 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "109 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "110 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "111 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "112 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "113 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "114 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "115 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "116 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "117 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "118 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "119 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "120 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "121 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "122 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "123 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "124 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "125 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "126 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "127 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "128 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "129 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "130 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "131 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "132 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "133 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "134 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "135 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "136 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "137 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "138 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "139 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "140 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "141 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "142 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "143 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "144 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "145 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "146 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "147 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "148 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "149 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "150 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "151 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "152 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "153 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "154 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "155 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "156 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "157 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "158 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "159 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "160 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "161 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "162 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "163 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "164 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "165 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "166 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "167 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "169 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "170 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "171 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "172 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "173 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "174 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "175 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "176 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "177 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "178 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "179 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "180 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "181 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "182 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "183 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "184 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "185 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "186 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "187 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "188 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "189 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "190 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "191 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "192 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "193 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "194 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "195 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "196 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "197 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "198 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "199 Train Loss 29.905571 Test MSE 33.49511689497033 Test RE 0.09742713228653177 Lambda1 0.9893822\n",
      "Training time: 396.82\n",
      "Training time: 396.82\n",
      "inv_HT_rowdy\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 857.7575 Test MSE 856.7027545714201 Test RE 0.4927246075721363 Lambda1 -0.04257418\n",
      "1 Train Loss 651.37976 Test MSE 623.0904497183924 Test RE 0.42020838865018445 Lambda1 -0.025587678\n",
      "2 Train Loss 438.7334 Test MSE 411.99767219987615 Test RE 0.3416933697288954 Lambda1 -0.0041435966\n",
      "3 Train Loss 342.44998 Test MSE 338.49467744253485 Test RE 0.309717034788407 Lambda1 -0.0015888778\n",
      "4 Train Loss 305.8744 Test MSE 310.14540981412495 Test RE 0.29646392192848153 Lambda1 -0.0005065402\n",
      "5 Train Loss 284.62354 Test MSE 289.6511639283515 Test RE 0.2865014403245786 Lambda1 0.0037242374\n",
      "6 Train Loss 276.84665 Test MSE 283.14996845381995 Test RE 0.28326794336640726 Lambda1 0.008457874\n",
      "7 Train Loss 271.3236 Test MSE 277.8261110683886 Test RE 0.28059226871634574 Lambda1 0.012449522\n",
      "8 Train Loss 266.62805 Test MSE 270.8711835271281 Test RE 0.27705792245246547 Lambda1 0.022150759\n",
      "9 Train Loss 257.74146 Test MSE 262.2625021487995 Test RE 0.2726197221276415 Lambda1 0.038386304\n",
      "10 Train Loss 250.76549 Test MSE 255.0535475606842 Test RE 0.2688467896829263 Lambda1 0.058452155\n",
      "11 Train Loss 243.28098 Test MSE 247.61471515422784 Test RE 0.2648972170573651 Lambda1 0.078616746\n",
      "12 Train Loss 235.09152 Test MSE 236.2783852956301 Test RE 0.2587623983514868 Lambda1 0.1104596\n",
      "13 Train Loss 228.66612 Test MSE 228.5148111379101 Test RE 0.25447571755903686 Lambda1 0.13412015\n",
      "14 Train Loss 213.9179 Test MSE 209.5987798560563 Test RE 0.24371572169718012 Lambda1 0.18516012\n",
      "15 Train Loss 197.03687 Test MSE 190.23141499371243 Test RE 0.23218293127236037 Lambda1 0.22128427\n",
      "16 Train Loss 178.31393 Test MSE 168.1207953044469 Test RE 0.21827293515924096 Lambda1 0.26974812\n",
      "17 Train Loss 149.65591 Test MSE 139.04054882593152 Test RE 0.19849972403384644 Lambda1 0.30973518\n",
      "18 Train Loss 134.15858 Test MSE 118.9836826924481 Test RE 0.18362545038363545 Lambda1 0.33706892\n",
      "19 Train Loss 119.71015 Test MSE 113.09249884031682 Test RE 0.17902186233342704 Lambda1 0.35557014\n",
      "20 Train Loss 104.827644 Test MSE 92.64863824619898 Test RE 0.1620349467886172 Lambda1 0.39234746\n",
      "21 Train Loss 95.05399 Test MSE 87.35505928042586 Test RE 0.15733784663706654 Lambda1 0.4015317\n",
      "22 Train Loss 86.25237 Test MSE 78.39744372458198 Test RE 0.14905279411738453 Lambda1 0.41550678\n",
      "23 Train Loss 78.57149 Test MSE 73.98833887266856 Test RE 0.14480074927369832 Lambda1 0.42622653\n",
      "24 Train Loss 73.48039 Test MSE 69.80549436839877 Test RE 0.14064813384385272 Lambda1 0.43975028\n",
      "25 Train Loss 69.822296 Test MSE 66.80279013303812 Test RE 0.13758987357034722 Lambda1 0.44906172\n",
      "26 Train Loss 66.0579 Test MSE 64.46518242470536 Test RE 0.13516111899210653 Lambda1 0.45601255\n",
      "27 Train Loss 62.557533 Test MSE 60.741103714358424 Test RE 0.13119899557187334 Lambda1 0.47170705\n",
      "28 Train Loss 58.51602 Test MSE 57.26161338497379 Test RE 0.12738578298185582 Lambda1 0.48848334\n",
      "29 Train Loss 55.52419 Test MSE 54.13823434189168 Test RE 0.12386289159435653 Lambda1 0.5048\n",
      "30 Train Loss 54.04295 Test MSE 52.55863855680316 Test RE 0.1220425361683549 Lambda1 0.5146827\n",
      "31 Train Loss 51.038807 Test MSE 48.69689919281361 Test RE 0.11747347738431957 Lambda1 0.5404246\n",
      "32 Train Loss 48.83971 Test MSE 47.68462589240154 Test RE 0.11624609172019472 Lambda1 0.54959196\n",
      "33 Train Loss 47.35758 Test MSE 46.09094685201595 Test RE 0.114287040225836 Lambda1 0.56089526\n",
      "34 Train Loss 45.291557 Test MSE 44.50245506094765 Test RE 0.11230036188993286 Lambda1 0.5772375\n",
      "35 Train Loss 43.508442 Test MSE 44.51841014372825 Test RE 0.11232049112775078 Lambda1 0.5739063\n",
      "36 Train Loss 42.38584 Test MSE 43.85141408049375 Test RE 0.11147589606065132 Lambda1 0.5802292\n",
      "37 Train Loss 41.49437 Test MSE 42.478457403709946 Test RE 0.10971690273804927 Lambda1 0.59366065\n",
      "38 Train Loss 40.84168 Test MSE 41.55349712678748 Test RE 0.10851579600923261 Lambda1 0.60429806\n",
      "39 Train Loss 39.54738 Test MSE 40.80650707904074 Test RE 0.10753600088423189 Lambda1 0.61310595\n",
      "40 Train Loss 38.950943 Test MSE 40.835222630233275 Test RE 0.1075738307881085 Lambda1 0.6143513\n",
      "41 Train Loss 37.943264 Test MSE 40.25421112847475 Test RE 0.1068057983627893 Lambda1 0.6217174\n",
      "42 Train Loss 37.428032 Test MSE 39.87504119151688 Test RE 0.10630158571021564 Lambda1 0.62701714\n",
      "43 Train Loss 36.379734 Test MSE 38.07704615021489 Test RE 0.10387733419897928 Lambda1 0.6532176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 Train Loss 35.896687 Test MSE 37.47647604408358 Test RE 0.10305487576892615 Lambda1 0.66126996\n",
      "45 Train Loss 35.396877 Test MSE 37.04348335242715 Test RE 0.10245781262027866 Lambda1 0.6691574\n",
      "46 Train Loss 34.815857 Test MSE 36.31965748229988 Test RE 0.10145186677667774 Lambda1 0.6801373\n",
      "47 Train Loss 33.9653 Test MSE 35.815606476335624 Test RE 0.10074542317401713 Lambda1 0.68910456\n",
      "48 Train Loss 33.596237 Test MSE 35.72775846635607 Test RE 0.10062179384813526 Lambda1 0.69021714\n",
      "49 Train Loss 33.037617 Test MSE 35.58771422634759 Test RE 0.10042439357436195 Lambda1 0.6936596\n",
      "50 Train Loss 32.66017 Test MSE 35.279723588153 Test RE 0.09998889238216975 Lambda1 0.69957983\n",
      "51 Train Loss 32.269745 Test MSE 34.48493111346953 Test RE 0.09885618627817953 Lambda1 0.7156455\n",
      "52 Train Loss 31.878675 Test MSE 34.08189353201244 Test RE 0.0982768048449525 Lambda1 0.7270139\n",
      "53 Train Loss 31.570288 Test MSE 33.81456095346912 Test RE 0.09789061277331912 Lambda1 0.733173\n",
      "54 Train Loss 31.29751 Test MSE 33.51098012838903 Test RE 0.09745020022209468 Lambda1 0.7444265\n",
      "55 Train Loss 31.113863 Test MSE 33.24706555076156 Test RE 0.09706570885449504 Lambda1 0.7540878\n",
      "56 Train Loss 30.830332 Test MSE 33.05387030698892 Test RE 0.0967832785886361 Lambda1 0.75841755\n",
      "57 Train Loss 30.500853 Test MSE 32.880318650335795 Test RE 0.09652886050423068 Lambda1 0.7676616\n",
      "58 Train Loss 30.357508 Test MSE 32.80295101496411 Test RE 0.09641522705489719 Lambda1 0.77210295\n",
      "59 Train Loss 30.226372 Test MSE 32.62517459156105 Test RE 0.09615360973106754 Lambda1 0.78132164\n",
      "60 Train Loss 30.081503 Test MSE 32.578228954127056 Test RE 0.09608440523536291 Lambda1 0.78711486\n",
      "61 Train Loss 29.915625 Test MSE 32.544271733929826 Test RE 0.09603431641729165 Lambda1 0.7872065\n",
      "62 Train Loss 29.759226 Test MSE 32.47251731654598 Test RE 0.09592838857579412 Lambda1 0.7853216\n",
      "63 Train Loss 29.659761 Test MSE 32.37782965811635 Test RE 0.09578842613312144 Lambda1 0.7885938\n",
      "64 Train Loss 29.549383 Test MSE 32.21113262598242 Test RE 0.09554152490588007 Lambda1 0.79550576\n",
      "65 Train Loss 29.458609 Test MSE 32.243442158401095 Test RE 0.09558942959052892 Lambda1 0.79522395\n",
      "66 Train Loss 29.352709 Test MSE 32.160311085119204 Test RE 0.09546612420024665 Lambda1 0.8000461\n",
      "67 Train Loss 29.192669 Test MSE 32.02955709223966 Test RE 0.0952718585052741 Lambda1 0.80400354\n",
      "68 Train Loss 29.130701 Test MSE 32.012121662099766 Test RE 0.09524592414802407 Lambda1 0.8065736\n",
      "69 Train Loss 29.061533 Test MSE 31.971676580217142 Test RE 0.09518573684199713 Lambda1 0.8081575\n",
      "70 Train Loss 28.931585 Test MSE 31.842238636878452 Test RE 0.09499286079152632 Lambda1 0.81479293\n",
      "71 Train Loss 28.78901 Test MSE 31.706670328290144 Test RE 0.09479042907949978 Lambda1 0.82092077\n",
      "72 Train Loss 28.695076 Test MSE 31.631265040457603 Test RE 0.09467764596056272 Lambda1 0.8241405\n",
      "73 Train Loss 28.609642 Test MSE 31.544208318237786 Test RE 0.09454726855486525 Lambda1 0.8301815\n",
      "74 Train Loss 28.519781 Test MSE 31.46279981114937 Test RE 0.09442518712000124 Lambda1 0.8363234\n",
      "75 Train Loss 28.469044 Test MSE 31.44997958159548 Test RE 0.09440594732056026 Lambda1 0.8391455\n",
      "76 Train Loss 28.438889 Test MSE 31.42275744013503 Test RE 0.09436508102364465 Lambda1 0.84271306\n",
      "77 Train Loss 28.357153 Test MSE 31.285479501292727 Test RE 0.09415872702566092 Lambda1 0.84974825\n",
      "78 Train Loss 28.29449 Test MSE 31.183677803414117 Test RE 0.09400540783621562 Lambda1 0.85508776\n",
      "79 Train Loss 28.187532 Test MSE 31.22112619796889 Test RE 0.09406183632043852 Lambda1 0.858613\n",
      "80 Train Loss 28.067259 Test MSE 31.13905708980162 Test RE 0.0939381276072638 Lambda1 0.8630367\n",
      "81 Train Loss 28.02404 Test MSE 31.15147815172128 Test RE 0.09395686123433049 Lambda1 0.8635274\n",
      "82 Train Loss 27.979393 Test MSE 31.180431198188696 Test RE 0.09400051414801168 Lambda1 0.8651496\n",
      "83 Train Loss 27.906414 Test MSE 31.06923915600641 Test RE 0.09383275758308408 Lambda1 0.86982274\n",
      "84 Train Loss 27.879202 Test MSE 31.02810786578835 Test RE 0.09377062634522387 Lambda1 0.87144053\n",
      "85 Train Loss 27.847725 Test MSE 30.95805430285089 Test RE 0.09366471143643373 Lambda1 0.87448305\n",
      "86 Train Loss 27.799028 Test MSE 30.888399702384476 Test RE 0.09355928084445198 Lambda1 0.8757759\n",
      "87 Train Loss 27.756704 Test MSE 30.850919340731355 Test RE 0.0935025006274389 Lambda1 0.8778244\n",
      "88 Train Loss 27.70517 Test MSE 30.862369730735594 Test RE 0.09351985085228191 Lambda1 0.87927043\n",
      "89 Train Loss 27.668638 Test MSE 30.850281459990377 Test RE 0.09350153398278194 Lambda1 0.87963754\n",
      "90 Train Loss 27.599571 Test MSE 30.841790698423107 Test RE 0.09348866612831917 Lambda1 0.878666\n",
      "91 Train Loss 27.56289 Test MSE 30.86756684874884 Test RE 0.09352772473309238 Lambda1 0.88140565\n",
      "92 Train Loss 27.482805 Test MSE 30.74576911680621 Test RE 0.09334302076562201 Lambda1 0.88614875\n",
      "93 Train Loss 27.44556 Test MSE 30.652616899317525 Test RE 0.0932015101507467 Lambda1 0.8905664\n",
      "94 Train Loss 27.428213 Test MSE 30.64892963667668 Test RE 0.09319590428709346 Lambda1 0.890081\n",
      "95 Train Loss 27.401155 Test MSE 30.634218360439345 Test RE 0.09317353490471629 Lambda1 0.8930217\n",
      "96 Train Loss 27.360798 Test MSE 30.56703593137726 Test RE 0.09307131163245791 Lambda1 0.89647454\n",
      "97 Train Loss 27.328764 Test MSE 30.51822813053621 Test RE 0.09299697631174068 Lambda1 0.8988444\n",
      "98 Train Loss 27.261522 Test MSE 30.454112786667224 Test RE 0.09289923688802196 Lambda1 0.9023624\n",
      "99 Train Loss 27.233564 Test MSE 30.40185873462992 Test RE 0.09281950306529285 Lambda1 0.90231913\n",
      "100 Train Loss 27.199791 Test MSE 30.360884742358927 Test RE 0.09275693340580304 Lambda1 0.9040445\n",
      "101 Train Loss 27.136232 Test MSE 30.35853994981062 Test RE 0.09275335149493982 Lambda1 0.9044502\n",
      "102 Train Loss 27.07323 Test MSE 30.231676215356735 Test RE 0.09255934750971087 Lambda1 0.90654176\n",
      "103 Train Loss 27.032303 Test MSE 30.16276051772309 Test RE 0.09245378883408811 Lambda1 0.9092145\n",
      "104 Train Loss 27.008621 Test MSE 30.109853806322057 Test RE 0.09237266938972834 Lambda1 0.9118565\n",
      "105 Train Loss 26.969265 Test MSE 30.089320663004862 Test RE 0.09234116766372766 Lambda1 0.911879\n",
      "106 Train Loss 26.93549 Test MSE 30.10111920972158 Test RE 0.09235927017942643 Lambda1 0.9100234\n",
      "107 Train Loss 26.917343 Test MSE 30.071897766781664 Test RE 0.09231442921450014 Lambda1 0.91163546\n",
      "108 Train Loss 26.87874 Test MSE 30.050145515432558 Test RE 0.09228103574628178 Lambda1 0.9133448\n",
      "109 Train Loss 26.823536 Test MSE 29.972622147219884 Test RE 0.09216192556423118 Lambda1 0.9165819\n",
      "110 Train Loss 26.790642 Test MSE 29.95759485696043 Test RE 0.09213881918344086 Lambda1 0.91720855\n",
      "111 Train Loss 26.75276 Test MSE 29.98035873938132 Test RE 0.0921738193054904 Lambda1 0.91937697\n",
      "112 Train Loss 26.717728 Test MSE 29.913416475523327 Test RE 0.09207085568819055 Lambda1 0.9205296\n",
      "113 Train Loss 26.698437 Test MSE 29.881626378519897 Test RE 0.09202191912670313 Lambda1 0.9224507\n",
      "114 Train Loss 26.678885 Test MSE 29.841696118628548 Test RE 0.0919604149861902 Lambda1 0.9229878\n",
      "115 Train Loss 26.667143 Test MSE 29.814335772703622 Test RE 0.09191824838590575 Lambda1 0.9243384\n",
      "116 Train Loss 26.651651 Test MSE 29.82807050856326 Test RE 0.09193941819340654 Lambda1 0.9248423\n",
      "117 Train Loss 26.624674 Test MSE 29.762390441004488 Test RE 0.0918381391811477 Lambda1 0.9273484\n",
      "118 Train Loss 26.609362 Test MSE 29.776099041156158 Test RE 0.0918592871362063 Lambda1 0.9279729\n",
      "119 Train Loss 26.590061 Test MSE 29.77056622676388 Test RE 0.09185075237151194 Lambda1 0.92796195\n",
      "120 Train Loss 26.57484 Test MSE 29.77559211495139 Test RE 0.09185850519902358 Lambda1 0.92920476\n",
      "121 Train Loss 26.569838 Test MSE 29.75936553335443 Test RE 0.09183347206697046 Lambda1 0.92977685\n",
      "122 Train Loss 26.547594 Test MSE 29.73298415594469 Test RE 0.0917927583184837 Lambda1 0.93101996\n",
      "123 Train Loss 26.52977 Test MSE 29.724753912894837 Test RE 0.09178005308522472 Lambda1 0.93329793\n",
      "124 Train Loss 26.511377 Test MSE 29.69474017757256 Test RE 0.09173370521736308 Lambda1 0.93364245\n",
      "125 Train Loss 26.496847 Test MSE 29.668683467042598 Test RE 0.09169344887604787 Lambda1 0.93497396\n",
      "126 Train Loss 26.482613 Test MSE 29.643587014579808 Test RE 0.0916546593705093 Lambda1 0.93747675\n",
      "127 Train Loss 26.463354 Test MSE 29.596109687593277 Test RE 0.09158123266305633 Lambda1 0.94043124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 Train Loss 26.434896 Test MSE 29.592903444520122 Test RE 0.09157627188191132 Lambda1 0.940611\n",
      "129 Train Loss 26.400917 Test MSE 29.547232380538723 Test RE 0.09150557924596546 Lambda1 0.9428831\n",
      "130 Train Loss 26.358366 Test MSE 29.526012012343468 Test RE 0.0914727143937981 Lambda1 0.94776285\n",
      "131 Train Loss 26.32972 Test MSE 29.498301297593635 Test RE 0.09142977989069917 Lambda1 0.94879496\n",
      "132 Train Loss 26.308662 Test MSE 29.499596601393353 Test RE 0.09143178726122608 Lambda1 0.95023096\n",
      "133 Train Loss 26.301783 Test MSE 29.481110298395926 Test RE 0.09140313431673398 Lambda1 0.95139164\n",
      "134 Train Loss 26.28957 Test MSE 29.441690318623586 Test RE 0.09134200508976645 Lambda1 0.95406884\n",
      "135 Train Loss 26.277607 Test MSE 29.430960231877958 Test RE 0.09132535867876934 Lambda1 0.95712864\n",
      "136 Train Loss 26.264936 Test MSE 29.39729575938809 Test RE 0.09127311268221977 Lambda1 0.9571357\n",
      "137 Train Loss 26.257788 Test MSE 29.38318000231291 Test RE 0.09125119665640764 Lambda1 0.9573521\n",
      "138 Train Loss 26.24839 Test MSE 29.372992899718906 Test RE 0.09123537696147495 Lambda1 0.9582612\n",
      "139 Train Loss 26.239977 Test MSE 29.37526724782798 Test RE 0.09123890906648109 Lambda1 0.95965904\n",
      "140 Train Loss 26.23375 Test MSE 29.34426779955955 Test RE 0.0911907545697707 Lambda1 0.960882\n",
      "141 Train Loss 26.215858 Test MSE 29.30912248232268 Test RE 0.09113612911067834 Lambda1 0.9623413\n",
      "142 Train Loss 26.196535 Test MSE 29.272975066283742 Test RE 0.09107991193898804 Lambda1 0.9650067\n",
      "143 Train Loss 26.190731 Test MSE 29.26327039390059 Test RE 0.09106481313241627 Lambda1 0.9653076\n",
      "144 Train Loss 26.176027 Test MSE 29.2652094168597 Test RE 0.09106783011985646 Lambda1 0.9655831\n",
      "145 Train Loss 26.160442 Test MSE 29.216771361176612 Test RE 0.09099243384737374 Lambda1 0.96903133\n",
      "146 Train Loss 26.150892 Test MSE 29.21131870750662 Test RE 0.0909839426049088 Lambda1 0.9709777\n",
      "147 Train Loss 26.135689 Test MSE 29.15398796262137 Test RE 0.09089461526036817 Lambda1 0.9738698\n",
      "148 Train Loss 26.126104 Test MSE 29.129638944163265 Test RE 0.09085665035196128 Lambda1 0.975282\n",
      "149 Train Loss 26.108404 Test MSE 29.131027265073993 Test RE 0.09085881544375836 Lambda1 0.9776916\n",
      "150 Train Loss 26.099861 Test MSE 29.110079539762413 Test RE 0.09082614190000582 Lambda1 0.97967136\n",
      "151 Train Loss 26.089178 Test MSE 29.10072584282331 Test RE 0.09081154852761746 Lambda1 0.98145944\n",
      "152 Train Loss 26.079529 Test MSE 29.082605233841807 Test RE 0.0907832705936348 Lambda1 0.9826882\n",
      "153 Train Loss 26.052433 Test MSE 29.052366453427005 Test RE 0.09073606214529735 Lambda1 0.98587257\n",
      "154 Train Loss 26.024078 Test MSE 29.04005671490814 Test RE 0.0907168372821509 Lambda1 0.98572975\n",
      "155 Train Loss 26.009193 Test MSE 29.0302601340383 Test RE 0.09070153445741092 Lambda1 0.98545027\n",
      "156 Train Loss 25.993757 Test MSE 29.030581444463017 Test RE 0.09070203640379004 Lambda1 0.98457766\n",
      "157 Train Loss 25.988432 Test MSE 29.039374777888575 Test RE 0.09071577214075963 Lambda1 0.9847246\n",
      "158 Train Loss 25.981247 Test MSE 29.016953663595842 Test RE 0.09068074484657392 Lambda1 0.98505425\n",
      "159 Train Loss 25.975935 Test MSE 29.011970877895628 Test RE 0.09067295867227047 Lambda1 0.9851346\n",
      "160 Train Loss 25.970695 Test MSE 29.02856979601573 Test RE 0.09069889379068018 Lambda1 0.9844504\n",
      "161 Train Loss 25.965504 Test MSE 29.03529672485996 Test RE 0.09070940222541136 Lambda1 0.9842485\n",
      "162 Train Loss 25.957792 Test MSE 29.03063826453546 Test RE 0.0907021251669753 Lambda1 0.98463815\n",
      "163 Train Loss 25.953566 Test MSE 29.03554288774199 Test RE 0.09070978674430853 Lambda1 0.98526317\n",
      "164 Train Loss 25.944464 Test MSE 29.028617493948065 Test RE 0.0906989683060275 Lambda1 0.9861393\n",
      "165 Train Loss 25.93954 Test MSE 29.03569376590653 Test RE 0.09071002242284229 Lambda1 0.9866882\n",
      "166 Train Loss 25.933851 Test MSE 29.033328959325143 Test RE 0.09070632841732541 Lambda1 0.98608327\n",
      "167 Train Loss 25.922907 Test MSE 29.013095863306983 Test RE 0.0906747166494741 Lambda1 0.98642397\n",
      "168 Train Loss 25.914349 Test MSE 29.008805020277354 Test RE 0.0906680113091677 Lambda1 0.9866732\n",
      "169 Train Loss 25.906796 Test MSE 28.994605625765708 Test RE 0.09064581824667207 Lambda1 0.98762816\n",
      "170 Train Loss 25.903555 Test MSE 28.998629785429692 Test RE 0.09065210839240417 Lambda1 0.9875047\n",
      "171 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "172 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "173 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "174 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "175 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "176 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "177 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "178 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "179 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "180 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "181 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "182 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "183 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "184 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "185 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "186 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "187 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "188 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "189 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "190 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "191 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "192 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "193 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "194 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "195 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "196 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "197 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "198 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "199 Train Loss 25.90184 Test MSE 29.000509357637835 Test RE 0.09065504619367173 Lambda1 0.9874352\n",
      "Training time: 557.56\n",
      "Training time: 557.56\n",
      "inv_HT_rowdy\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 830.0523 Test MSE 813.9824584010343 Test RE 0.48028242362991774 Lambda1 -0.026484717\n",
      "1 Train Loss 572.5544 Test MSE 541.2832852482654 Test RE 0.39165301919636836 Lambda1 0.0054018535\n",
      "2 Train Loss 349.01797 Test MSE 342.0768749837308 Test RE 0.311351547537302 Lambda1 0.0033313502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Train Loss 284.86694 Test MSE 287.6323533858554 Test RE 0.2855012657229527 Lambda1 0.0058571454\n",
      "4 Train Loss 271.17224 Test MSE 278.61704540313383 Test RE 0.2809913894677452 Lambda1 0.010671814\n",
      "5 Train Loss 260.74567 Test MSE 263.46528777179753 Test RE 0.273244149910553 Lambda1 0.038656354\n",
      "6 Train Loss 250.57874 Test MSE 253.8481064905997 Test RE 0.26821072170773813 Lambda1 0.058567423\n",
      "7 Train Loss 241.23225 Test MSE 243.92097426647004 Test RE 0.2629140187776435 Lambda1 0.08857865\n",
      "8 Train Loss 234.61917 Test MSE 237.76996959686485 Test RE 0.25957787436687935 Lambda1 0.11653897\n",
      "9 Train Loss 226.99823 Test MSE 230.28450077515924 Test RE 0.25545918663672185 Lambda1 0.14311022\n",
      "10 Train Loss 218.65552 Test MSE 220.9675872568544 Test RE 0.25023811357628356 Lambda1 0.18853013\n",
      "11 Train Loss 208.1512 Test MSE 206.84500620434736 Test RE 0.2421094219397401 Lambda1 0.22512043\n",
      "12 Train Loss 197.09102 Test MSE 195.97108291241318 Test RE 0.2356596166510361 Lambda1 0.24299502\n",
      "13 Train Loss 181.29413 Test MSE 171.80283885462643 Test RE 0.22065020665440108 Lambda1 0.30767375\n",
      "14 Train Loss 161.43849 Test MSE 144.92475578607468 Test RE 0.20265646331536522 Lambda1 0.3449927\n",
      "15 Train Loss 138.60225 Test MSE 131.29804311807527 Test RE 0.1928938126823497 Lambda1 0.37061036\n",
      "16 Train Loss 125.81931 Test MSE 119.16794477698079 Test RE 0.18376757961533793 Lambda1 0.37074453\n",
      "17 Train Loss 111.19934 Test MSE 101.95206086807079 Test RE 0.1699758318142062 Lambda1 0.39598268\n",
      "18 Train Loss 103.125244 Test MSE 97.32324229717376 Test RE 0.1660723973818159 Lambda1 0.39893013\n",
      "19 Train Loss 96.04692 Test MSE 87.93823846353068 Test RE 0.157862163868928 Lambda1 0.40505636\n",
      "20 Train Loss 89.2941 Test MSE 79.62854112961966 Test RE 0.1502185446718574 Lambda1 0.42182052\n",
      "21 Train Loss 83.04103 Test MSE 75.21915668898353 Test RE 0.14600018317740285 Lambda1 0.43335125\n",
      "22 Train Loss 78.76779 Test MSE 73.44622712866298 Test RE 0.1442692972506361 Lambda1 0.43880424\n",
      "23 Train Loss 76.06891 Test MSE 70.64001594033994 Test RE 0.14148635714006821 Lambda1 0.4492532\n",
      "24 Train Loss 72.48581 Test MSE 66.54966769281134 Test RE 0.1373289552916695 Lambda1 0.46134838\n",
      "25 Train Loss 70.27968 Test MSE 64.77004041766348 Test RE 0.1354803327589014 Lambda1 0.46708068\n",
      "26 Train Loss 68.09575 Test MSE 63.66187271257787 Test RE 0.13431634817578408 Lambda1 0.47165167\n",
      "27 Train Loss 64.70586 Test MSE 60.530327809159154 Test RE 0.13097116287664962 Lambda1 0.48492\n",
      "28 Train Loss 63.378204 Test MSE 59.254285996902176 Test RE 0.12958330587621975 Lambda1 0.48898038\n",
      "29 Train Loss 60.57955 Test MSE 57.2468710142517 Test RE 0.12736938378162604 Lambda1 0.49991846\n",
      "30 Train Loss 58.76992 Test MSE 55.775319030712595 Test RE 0.1257216876375246 Lambda1 0.5087311\n",
      "31 Train Loss 55.944077 Test MSE 54.504493851152176 Test RE 0.12428116800013908 Lambda1 0.51577276\n",
      "32 Train Loss 54.828175 Test MSE 52.62974704075011 Test RE 0.12212506614499845 Lambda1 0.52385294\n",
      "33 Train Loss 51.52982 Test MSE 49.19344578802407 Test RE 0.11807087797600109 Lambda1 0.5442069\n",
      "34 Train Loss 50.568726 Test MSE 48.21602305827618 Test RE 0.11689202000206299 Lambda1 0.5529768\n",
      "35 Train Loss 49.523212 Test MSE 47.324596805820065 Test RE 0.11580641886016013 Lambda1 0.5554775\n",
      "36 Train Loss 48.349392 Test MSE 46.171800687773256 Test RE 0.11438723882517945 Lambda1 0.5626853\n",
      "37 Train Loss 47.831696 Test MSE 45.47611294637546 Test RE 0.1135222104366412 Lambda1 0.56806844\n",
      "38 Train Loss 46.81698 Test MSE 44.91722024910473 Test RE 0.1128224707880194 Lambda1 0.57418376\n",
      "39 Train Loss 46.159637 Test MSE 44.76197670571485 Test RE 0.11262733270828248 Lambda1 0.5788482\n",
      "40 Train Loss 45.33186 Test MSE 44.08223119922948 Test RE 0.11176889430882933 Lambda1 0.5839402\n",
      "41 Train Loss 43.514347 Test MSE 43.11211744914575 Test RE 0.11053220847983769 Lambda1 0.59433264\n",
      "42 Train Loss 42.180473 Test MSE 42.03999593559424 Test RE 0.10914918652516979 Lambda1 0.6055293\n",
      "43 Train Loss 41.449383 Test MSE 41.33599400208346 Test RE 0.10823142173187661 Lambda1 0.61561626\n",
      "44 Train Loss 41.09736 Test MSE 40.82322359492438 Test RE 0.10755802486192895 Lambda1 0.62117314\n",
      "45 Train Loss 40.729713 Test MSE 40.62809459669275 Test RE 0.10730066119209673 Lambda1 0.6248402\n",
      "46 Train Loss 40.07446 Test MSE 40.13617824182234 Test RE 0.10664909610559223 Lambda1 0.63220775\n",
      "47 Train Loss 39.86445 Test MSE 40.03484412299532 Test RE 0.10651437946513154 Lambda1 0.6333598\n",
      "48 Train Loss 39.538696 Test MSE 39.79753327194554 Test RE 0.10619822252624968 Lambda1 0.63876355\n",
      "49 Train Loss 38.827805 Test MSE 39.11636604293066 Test RE 0.10528546544303496 Lambda1 0.6489341\n",
      "50 Train Loss 38.06272 Test MSE 38.73121605891876 Test RE 0.10476584907654937 Lambda1 0.65525156\n",
      "51 Train Loss 37.59723 Test MSE 38.789324969151444 Test RE 0.10484441035397697 Lambda1 0.6545761\n",
      "52 Train Loss 37.242405 Test MSE 38.829146083447796 Test RE 0.10489821317578417 Lambda1 0.65318614\n",
      "53 Train Loss 36.921303 Test MSE 38.54729751051168 Test RE 0.10451680820308773 Lambda1 0.6590196\n",
      "54 Train Loss 36.342354 Test MSE 38.23390110442275 Test RE 0.10409107095823038 Lambda1 0.66374177\n",
      "55 Train Loss 35.82098 Test MSE 37.40463529650523 Test RE 0.10295605256610789 Lambda1 0.67860895\n",
      "56 Train Loss 35.434223 Test MSE 36.86140763294967 Test RE 0.10220570269325421 Lambda1 0.690675\n",
      "57 Train Loss 35.15563 Test MSE 36.7549420034403 Test RE 0.1020579972251315 Lambda1 0.6932119\n",
      "58 Train Loss 34.81132 Test MSE 36.48582715359287 Test RE 0.10168368318235838 Lambda1 0.6990452\n",
      "59 Train Loss 34.500225 Test MSE 36.2829419767941 Test RE 0.10140057501525918 Lambda1 0.7047288\n",
      "60 Train Loss 34.177406 Test MSE 36.08957606473991 Test RE 0.10113001249564386 Lambda1 0.7105851\n",
      "61 Train Loss 33.9291 Test MSE 35.86146726434288 Test RE 0.10080990325502985 Lambda1 0.716751\n",
      "62 Train Loss 33.681683 Test MSE 35.699328823945024 Test RE 0.1005817520005403 Lambda1 0.72051775\n",
      "63 Train Loss 33.346367 Test MSE 35.67676998166877 Test RE 0.10054996755919691 Lambda1 0.72254646\n",
      "64 Train Loss 33.071045 Test MSE 35.578562046806674 Test RE 0.10041147955052257 Lambda1 0.7254587\n",
      "65 Train Loss 32.75396 Test MSE 35.43462033654065 Test RE 0.10020815418271735 Lambda1 0.7300268\n",
      "66 Train Loss 32.505108 Test MSE 35.258884450499586 Test RE 0.09995935714393496 Lambda1 0.7327073\n",
      "67 Train Loss 32.30675 Test MSE 35.167607886891076 Test RE 0.09982988822665553 Lambda1 0.7338845\n",
      "68 Train Loss 32.097237 Test MSE 35.173299983492434 Test RE 0.09983796694468815 Lambda1 0.7350472\n",
      "69 Train Loss 31.885181 Test MSE 34.880110676539914 Test RE 0.09942099329747593 Lambda1 0.74248487\n",
      "70 Train Loss 31.753382 Test MSE 34.85076801761619 Test RE 0.0993791658773135 Lambda1 0.7424706\n",
      "71 Train Loss 31.58003 Test MSE 34.78719888045323 Test RE 0.09928848880427557 Lambda1 0.74492115\n",
      "72 Train Loss 31.379057 Test MSE 34.7693872755104 Test RE 0.09926306689871359 Lambda1 0.74546427\n",
      "73 Train Loss 31.18625 Test MSE 34.491990626689244 Test RE 0.09886630433291978 Lambda1 0.75273323\n",
      "74 Train Loss 31.07324 Test MSE 34.34635391082474 Test RE 0.09865736024661526 Lambda1 0.7578848\n",
      "75 Train Loss 30.94044 Test MSE 34.26355129693226 Test RE 0.09853836629448696 Lambda1 0.76211447\n",
      "76 Train Loss 30.712263 Test MSE 33.916421421287325 Test RE 0.09803794110624617 Lambda1 0.7719258\n",
      "77 Train Loss 30.318592 Test MSE 33.4231978908974 Test RE 0.09732248079981987 Lambda1 0.78621405\n",
      "78 Train Loss 30.084196 Test MSE 33.06362329579234 Test RE 0.09679755613925885 Lambda1 0.7936429\n",
      "79 Train Loss 29.939499 Test MSE 32.8355273182123 Test RE 0.09646308970408718 Lambda1 0.8010284\n",
      "80 Train Loss 29.644156 Test MSE 32.392117557731254 Test RE 0.09580955887235586 Lambda1 0.8173916\n",
      "81 Train Loss 29.506805 Test MSE 32.27431457768559 Test RE 0.09563518107814008 Lambda1 0.82413816\n",
      "82 Train Loss 29.363558 Test MSE 32.136042997821086 Test RE 0.0954300981578673 Lambda1 0.8261008\n",
      "83 Train Loss 29.293491 Test MSE 32.14559836263026 Test RE 0.09544428474623724 Lambda1 0.82652044\n",
      "84 Train Loss 29.166725 Test MSE 32.02588204559074 Test RE 0.09526639263874223 Lambda1 0.82821035\n",
      "85 Train Loss 29.022768 Test MSE 31.96926775397282 Test RE 0.09518215100846532 Lambda1 0.82925975\n",
      "86 Train Loss 28.918386 Test MSE 31.801264850014768 Test RE 0.09493172391318151 Lambda1 0.83603203\n",
      "87 Train Loss 28.804691 Test MSE 31.651639170760102 Test RE 0.09470813263527082 Lambda1 0.8428714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 Train Loss 28.709114 Test MSE 31.671014123191707 Test RE 0.09473711509698221 Lambda1 0.84409904\n",
      "89 Train Loss 28.610928 Test MSE 31.653212708554197 Test RE 0.09471048677834004 Lambda1 0.84393513\n",
      "90 Train Loss 28.49987 Test MSE 31.714104146888175 Test RE 0.09480154051992408 Lambda1 0.8465398\n",
      "91 Train Loss 28.43319 Test MSE 31.652250559468936 Test RE 0.0947090473304466 Lambda1 0.84679663\n",
      "92 Train Loss 28.344526 Test MSE 31.638406992993655 Test RE 0.09468833388418263 Lambda1 0.8441906\n",
      "93 Train Loss 28.260384 Test MSE 31.65807211795568 Test RE 0.09471775648855137 Lambda1 0.8447275\n",
      "94 Train Loss 28.21863 Test MSE 31.638539339594544 Test RE 0.09468853192933126 Lambda1 0.8467016\n",
      "95 Train Loss 28.200003 Test MSE 31.588146104872827 Test RE 0.09461309287492017 Lambda1 0.8492372\n",
      "96 Train Loss 28.131779 Test MSE 31.3864624774514 Test RE 0.09431056692378623 Lambda1 0.8583256\n",
      "97 Train Loss 28.064972 Test MSE 31.27786579253133 Test RE 0.09414726898253514 Lambda1 0.86447203\n",
      "98 Train Loss 28.027483 Test MSE 31.261633418410923 Test RE 0.09412283585733044 Lambda1 0.8644885\n",
      "99 Train Loss 28.000164 Test MSE 31.22094760811748 Test RE 0.09406156729565561 Lambda1 0.86478645\n",
      "100 Train Loss 27.977253 Test MSE 31.180315329803435 Test RE 0.09400033949202578 Lambda1 0.86620337\n",
      "101 Train Loss 27.91437 Test MSE 31.2322663237251 Test RE 0.0940786160999816 Lambda1 0.8669629\n",
      "102 Train Loss 27.862682 Test MSE 31.153690491428844 Test RE 0.09396019752552436 Lambda1 0.86700714\n",
      "103 Train Loss 27.827345 Test MSE 31.154475075597887 Test RE 0.09396138067940112 Lambda1 0.8665347\n",
      "104 Train Loss 27.784277 Test MSE 31.168082257303976 Test RE 0.09398189795663396 Lambda1 0.86582726\n",
      "105 Train Loss 27.75643 Test MSE 31.10181763044924 Test RE 0.0938819401064087 Lambda1 0.86962795\n",
      "106 Train Loss 27.730816 Test MSE 31.023410651214178 Test RE 0.0937635283064756 Lambda1 0.87349534\n",
      "107 Train Loss 27.708336 Test MSE 30.962041987922508 Test RE 0.09367074368479328 Lambda1 0.87724006\n",
      "108 Train Loss 27.687511 Test MSE 30.887373046343086 Test RE 0.09355772598890764 Lambda1 0.8798401\n",
      "109 Train Loss 27.658197 Test MSE 30.882877708052575 Test RE 0.09355091756057303 Lambda1 0.87847507\n",
      "110 Train Loss 27.595037 Test MSE 30.85303069575961 Test RE 0.09350570010417274 Lambda1 0.8784823\n",
      "111 Train Loss 27.530027 Test MSE 30.90662808311618 Test RE 0.09358688315912352 Lambda1 0.8746157\n",
      "112 Train Loss 27.49978 Test MSE 30.92612152272305 Test RE 0.09361639208322992 Lambda1 0.8737351\n",
      "113 Train Loss 27.479319 Test MSE 30.88187130864458 Test RE 0.0935493932473924 Lambda1 0.87486815\n",
      "114 Train Loss 27.468254 Test MSE 30.862572154633387 Test RE 0.09352015754651594 Lambda1 0.8748504\n",
      "115 Train Loss 27.442358 Test MSE 30.846673709319187 Test RE 0.09349606660827561 Lambda1 0.8754624\n",
      "116 Train Loss 27.413908 Test MSE 30.81599566482883 Test RE 0.09344956256667146 Lambda1 0.87593234\n",
      "117 Train Loss 27.380701 Test MSE 30.760847756299423 Test RE 0.09336590705654664 Lambda1 0.8791942\n",
      "118 Train Loss 27.35208 Test MSE 30.701020529548416 Test RE 0.09327506850150265 Lambda1 0.88209736\n",
      "119 Train Loss 27.320461 Test MSE 30.66693225041622 Test RE 0.09322327104323248 Lambda1 0.88515025\n",
      "120 Train Loss 27.278603 Test MSE 30.65761253307151 Test RE 0.09320910463513775 Lambda1 0.8870004\n",
      "121 Train Loss 27.245068 Test MSE 30.60252258315283 Test RE 0.09312532130484431 Lambda1 0.88846284\n",
      "122 Train Loss 27.232956 Test MSE 30.591215852283746 Test RE 0.09310811618363163 Lambda1 0.88788545\n",
      "123 Train Loss 27.209534 Test MSE 30.516591670281244 Test RE 0.09299448291836422 Lambda1 0.88970715\n",
      "124 Train Loss 27.169455 Test MSE 30.494873435182033 Test RE 0.09296138558664775 Lambda1 0.88962215\n",
      "125 Train Loss 27.083038 Test MSE 30.433991312586503 Test RE 0.09286854188032817 Lambda1 0.89295477\n",
      "126 Train Loss 27.046831 Test MSE 30.359300012904413 Test RE 0.09275451258434274 Lambda1 0.89465123\n",
      "127 Train Loss 26.984007 Test MSE 30.32418616272884 Test RE 0.0927008566969943 Lambda1 0.8971964\n",
      "128 Train Loss 26.939932 Test MSE 30.341136534925475 Test RE 0.09272676167094362 Lambda1 0.8945695\n",
      "129 Train Loss 26.918627 Test MSE 30.328364437575615 Test RE 0.09270724295769094 Lambda1 0.8960864\n",
      "130 Train Loss 26.897541 Test MSE 30.332093124854968 Test RE 0.09271294167735293 Lambda1 0.89448303\n",
      "131 Train Loss 26.857677 Test MSE 30.29302382653942 Test RE 0.09265321291551716 Lambda1 0.895615\n",
      "132 Train Loss 26.825706 Test MSE 30.266507876108143 Test RE 0.09261265364572362 Lambda1 0.89745843\n",
      "133 Train Loss 26.759424 Test MSE 30.19349592366664 Test RE 0.09250088136080613 Lambda1 0.90024936\n",
      "134 Train Loss 26.729246 Test MSE 30.1101930412159 Test RE 0.09237318975001692 Lambda1 0.9031859\n",
      "135 Train Loss 26.71149 Test MSE 30.100469909478935 Test RE 0.09235827405002149 Lambda1 0.9039356\n",
      "136 Train Loss 26.685911 Test MSE 30.091389087364025 Test RE 0.09234434150471128 Lambda1 0.9039158\n",
      "137 Train Loss 26.653603 Test MSE 30.04796053218185 Test RE 0.09227768075116652 Lambda1 0.9039325\n",
      "138 Train Loss 26.630398 Test MSE 29.968473690350375 Test RE 0.09215554736012133 Lambda1 0.9047652\n",
      "139 Train Loss 26.596819 Test MSE 29.93222833323201 Test RE 0.09209980175685291 Lambda1 0.9067259\n",
      "140 Train Loss 26.551384 Test MSE 29.94008414364523 Test RE 0.09211188690980948 Lambda1 0.9074663\n",
      "141 Train Loss 26.530134 Test MSE 29.88252354994033 Test RE 0.09202330055780669 Lambda1 0.90959173\n",
      "142 Train Loss 26.517807 Test MSE 29.85618754618375 Test RE 0.09198274072699475 Lambda1 0.91038835\n",
      "143 Train Loss 26.496 Test MSE 29.82142559108145 Test RE 0.0919291767687114 Lambda1 0.910587\n",
      "144 Train Loss 26.47999 Test MSE 29.79139781783644 Test RE 0.09188288247438739 Lambda1 0.9111775\n",
      "145 Train Loss 26.45464 Test MSE 29.757549838302225 Test RE 0.09183067052666073 Lambda1 0.91188204\n",
      "146 Train Loss 26.422632 Test MSE 29.7437097428594 Test RE 0.09180931303816448 Lambda1 0.91369814\n",
      "147 Train Loss 26.373009 Test MSE 29.678864610878854 Test RE 0.09170918034788023 Lambda1 0.915742\n",
      "148 Train Loss 26.34458 Test MSE 29.64626675972148 Test RE 0.09165880201319203 Lambda1 0.91572666\n",
      "149 Train Loss 26.319387 Test MSE 29.631155570681287 Test RE 0.09163543903735229 Lambda1 0.9159192\n",
      "150 Train Loss 26.303656 Test MSE 29.592990186157277 Test RE 0.0915764060943238 Lambda1 0.91701937\n",
      "151 Train Loss 26.282303 Test MSE 29.570506178565616 Test RE 0.09154161076204287 Lambda1 0.91669095\n",
      "152 Train Loss 26.267492 Test MSE 29.589658584818057 Test RE 0.09157125107851201 Lambda1 0.9170617\n",
      "153 Train Loss 26.254007 Test MSE 29.5428292629914 Test RE 0.09149876092829777 Lambda1 0.9172226\n",
      "154 Train Loss 26.242863 Test MSE 29.52332414123961 Test RE 0.09146855073530055 Lambda1 0.91781366\n",
      "155 Train Loss 26.22192 Test MSE 29.484895756839204 Test RE 0.09140900233962757 Lambda1 0.91894126\n",
      "156 Train Loss 26.209993 Test MSE 29.485895628614323 Test RE 0.09141055222651452 Lambda1 0.9202956\n",
      "157 Train Loss 26.19904 Test MSE 29.511151884507825 Test RE 0.09144969287464265 Lambda1 0.92040277\n",
      "158 Train Loss 26.188726 Test MSE 29.492179921092536 Test RE 0.09142029281652908 Lambda1 0.91986835\n",
      "159 Train Loss 26.178305 Test MSE 29.468575691291235 Test RE 0.09138370112422758 Lambda1 0.9205511\n",
      "160 Train Loss 26.164568 Test MSE 29.49293432788699 Test RE 0.09142146206968686 Lambda1 0.92064774\n",
      "161 Train Loss 26.128489 Test MSE 29.48315879065916 Test RE 0.09140630983080272 Lambda1 0.92146635\n",
      "162 Train Loss 26.109152 Test MSE 29.486087061418594 Test RE 0.09141084896075945 Lambda1 0.92257696\n",
      "163 Train Loss 26.075241 Test MSE 29.41513513105858 Test RE 0.09130080244061621 Lambda1 0.9253355\n",
      "164 Train Loss 26.037605 Test MSE 29.346149811022794 Test RE 0.0911936788085851 Lambda1 0.9267641\n",
      "165 Train Loss 26.023409 Test MSE 29.328224039280915 Test RE 0.09116582222783014 Lambda1 0.92826456\n",
      "166 Train Loss 26.01008 Test MSE 29.288950524519315 Test RE 0.09110476156327449 Lambda1 0.93021786\n",
      "167 Train Loss 25.999111 Test MSE 29.27616738114118 Test RE 0.09108487808658496 Lambda1 0.9301187\n",
      "168 Train Loss 25.983183 Test MSE 29.219061930944083 Test RE 0.09099600064177665 Lambda1 0.9320349\n",
      "169 Train Loss 25.961226 Test MSE 29.143109858772664 Test RE 0.09087765611715205 Lambda1 0.9347522\n",
      "170 Train Loss 25.951052 Test MSE 29.128533474710498 Test RE 0.09085492633120235 Lambda1 0.9351767\n",
      "171 Train Loss 25.936497 Test MSE 29.08650279614072 Test RE 0.090789353638738 Lambda1 0.9361014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 Train Loss 25.92551 Test MSE 29.102222873318766 Test RE 0.09081388430971915 Lambda1 0.93701833\n",
      "173 Train Loss 25.914524 Test MSE 29.081051768880315 Test RE 0.09078084593958204 Lambda1 0.9373484\n",
      "174 Train Loss 25.904148 Test MSE 29.046034729206173 Test RE 0.09072617401743235 Lambda1 0.9385581\n",
      "175 Train Loss 25.901445 Test MSE 29.045108719919146 Test RE 0.09072472779680169 Lambda1 0.93954325\n",
      "176 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "177 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "178 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "179 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "180 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "181 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "182 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "183 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "184 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "185 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "186 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "187 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "188 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "189 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "190 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "191 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "192 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "193 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "194 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "195 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "196 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "197 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "198 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "199 Train Loss 25.901175 Test MSE 29.043451271890948 Test RE 0.0907221391739129 Lambda1 0.9395303\n",
      "Training time: 561.30\n",
      "Training time: 561.30\n",
      "inv_HT_rowdy\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 854.6869 Test MSE 858.042782555509 Test RE 0.49310980930597453 Lambda1 0.020762188\n",
      "1 Train Loss 854.3513 Test MSE 856.5716254690491 Test RE 0.49268689728830983 Lambda1 0.022211641\n",
      "2 Train Loss 853.7516 Test MSE 856.1208812194071 Test RE 0.4925572496312808 Lambda1 0.04474833\n",
      "3 Train Loss 842.0558 Test MSE 838.1647385982194 Test RE 0.4873644686641134 Lambda1 0.15217212\n",
      "4 Train Loss 772.6452 Test MSE 751.9327497927576 Test RE 0.4616136801968808 Lambda1 0.07846347\n",
      "5 Train Loss 683.4146 Test MSE 661.0609053384729 Test RE 0.43282257819084435 Lambda1 0.0290301\n",
      "6 Train Loss 648.9778 Test MSE 634.8010184921278 Test RE 0.4241387753473212 Lambda1 0.0029273727\n",
      "7 Train Loss 487.39096 Test MSE 451.1654463905207 Test RE 0.35756671614947333 Lambda1 -0.0007344485\n",
      "8 Train Loss 284.6018 Test MSE 291.024234762348 Test RE 0.2871797073518685 Lambda1 -0.0006300114\n",
      "9 Train Loss 274.5811 Test MSE 281.9240941140196 Test RE 0.2826540856830093 Lambda1 -0.00022028998\n",
      "10 Train Loss 271.56567 Test MSE 279.63150011961983 Test RE 0.2815024745650789 Lambda1 0.00034147914\n",
      "11 Train Loss 269.9603 Test MSE 277.8119417844306 Test RE 0.28058511344535664 Lambda1 0.002615079\n",
      "12 Train Loss 266.5344 Test MSE 272.5586372661238 Test RE 0.2779195802817801 Lambda1 0.013537842\n",
      "13 Train Loss 256.36926 Test MSE 259.4153351608562 Test RE 0.2711358805500106 Lambda1 0.040409703\n",
      "14 Train Loss 245.60219 Test MSE 246.46668753888383 Test RE 0.26428242599219914 Lambda1 0.06374407\n",
      "15 Train Loss 234.9077 Test MSE 235.10004960592644 Test RE 0.25811635942191935 Lambda1 0.08626698\n",
      "16 Train Loss 217.11696 Test MSE 209.25685761146286 Test RE 0.24351685164749123 Lambda1 0.1492678\n",
      "17 Train Loss 204.80257 Test MSE 198.65207209232747 Test RE 0.23726611564347302 Lambda1 0.17020671\n",
      "18 Train Loss 195.7598 Test MSE 190.67758000906133 Test RE 0.23245505044789883 Lambda1 0.18727452\n",
      "19 Train Loss 178.9425 Test MSE 169.07875943912268 Test RE 0.2188939190934774 Lambda1 0.23945038\n",
      "20 Train Loss 166.11284 Test MSE 154.05385644640086 Test RE 0.20894186068357806 Lambda1 0.27387553\n",
      "21 Train Loss 154.16489 Test MSE 141.93871365669912 Test RE 0.20055782130313904 Lambda1 0.31031662\n",
      "22 Train Loss 143.48651 Test MSE 130.20263084496966 Test RE 0.19208747617786257 Lambda1 0.37510797\n",
      "23 Train Loss 129.74551 Test MSE 114.52843047886651 Test RE 0.18015479474576077 Lambda1 0.40123022\n",
      "24 Train Loss 121.69545 Test MSE 105.36593738045264 Test RE 0.17279822944431739 Lambda1 0.45351973\n",
      "25 Train Loss 114.1474 Test MSE 97.6738553839178 Test RE 0.16637127156130518 Lambda1 0.45639774\n",
      "26 Train Loss 98.21062 Test MSE 85.83399487413054 Test RE 0.15596201356767134 Lambda1 0.46718836\n",
      "27 Train Loss 92.14293 Test MSE 84.09134572099464 Test RE 0.15437068167214918 Lambda1 0.47268376\n",
      "28 Train Loss 87.84498 Test MSE 78.5964577544731 Test RE 0.14924186147858431 Lambda1 0.48128584\n",
      "29 Train Loss 84.5301 Test MSE 76.82753269837941 Test RE 0.14755285384991568 Lambda1 0.4781072\n",
      "30 Train Loss 79.61975 Test MSE 70.9760960724079 Test RE 0.14182252871151385 Lambda1 0.4950565\n",
      "31 Train Loss 76.368515 Test MSE 69.10368623514871 Test RE 0.13993932605584675 Lambda1 0.50954103\n",
      "32 Train Loss 72.03818 Test MSE 64.72651162456795 Test RE 0.13543480023730994 Lambda1 0.52171\n",
      "33 Train Loss 67.11973 Test MSE 61.90079444980926 Test RE 0.1324455228388554 Lambda1 0.5385151\n",
      "34 Train Loss 64.28513 Test MSE 60.99090981186065 Test RE 0.1314685056645883 Lambda1 0.55188257\n",
      "35 Train Loss 61.959064 Test MSE 59.689787571321105 Test RE 0.13005863368802323 Lambda1 0.5457082\n",
      "36 Train Loss 59.542778 Test MSE 57.275917037547266 Test RE 0.12740169214073588 Lambda1 0.5590468\n",
      "37 Train Loss 56.752987 Test MSE 53.32081532940832 Test RE 0.1229242483664807 Lambda1 0.5934726\n",
      "38 Train Loss 53.94384 Test MSE 51.79328160342069 Test RE 0.12115068804483951 Lambda1 0.6042729\n",
      "39 Train Loss 52.799328 Test MSE 51.28962444563923 Test RE 0.12056019178329695 Lambda1 0.6097411\n",
      "40 Train Loss 51.595985 Test MSE 50.182626203710605 Test RE 0.11925205270807405 Lambda1 0.6172581\n",
      "41 Train Loss 49.998302 Test MSE 49.70397672367581 Test RE 0.11868196800326537 Lambda1 0.6245942\n",
      "42 Train Loss 49.13627 Test MSE 49.345642631279645 Test RE 0.11825338335832102 Lambda1 0.6285686\n",
      "43 Train Loss 47.990803 Test MSE 48.115129861141405 Test RE 0.11676965626345402 Lambda1 0.64376986\n",
      "44 Train Loss 46.02626 Test MSE 46.3985721204428 Test RE 0.114667799524897 Lambda1 0.659958\n",
      "45 Train Loss 45.2163 Test MSE 45.86154943473914 Test RE 0.11400227872529624 Lambda1 0.665966\n",
      "46 Train Loss 44.559387 Test MSE 45.207496519723314 Test RE 0.11318643971998463 Lambda1 0.67573756\n",
      "47 Train Loss 44.068256 Test MSE 44.971399003277206 Test RE 0.11289049301592671 Lambda1 0.6840185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 Train Loss 42.970936 Test MSE 44.25690960267487 Test RE 0.11199012074003832 Lambda1 0.6982015\n",
      "49 Train Loss 42.21843 Test MSE 43.42109188303878 Test RE 0.11092758056629233 Lambda1 0.7142209\n",
      "50 Train Loss 41.736706 Test MSE 43.058453180442804 Test RE 0.11046339398519 Lambda1 0.7238327\n",
      "51 Train Loss 41.274593 Test MSE 42.51974840921569 Test RE 0.1097702147122438 Lambda1 0.73893774\n",
      "52 Train Loss 40.962345 Test MSE 41.967022640561105 Test RE 0.10905441445121117 Lambda1 0.7520757\n",
      "53 Train Loss 40.72516 Test MSE 41.68795967190941 Test RE 0.10869122679384165 Lambda1 0.7552879\n",
      "54 Train Loss 39.773746 Test MSE 41.41175427091664 Test RE 0.1083305591565909 Lambda1 0.76027066\n",
      "55 Train Loss 39.3139 Test MSE 41.20117323195752 Test RE 0.10805477470121845 Lambda1 0.76544744\n",
      "56 Train Loss 39.080467 Test MSE 41.133934590638795 Test RE 0.1079665681973455 Lambda1 0.76687306\n",
      "57 Train Loss 38.935604 Test MSE 41.167879984201235 Test RE 0.10801110820886275 Lambda1 0.7658511\n",
      "58 Train Loss 38.716423 Test MSE 40.98279051426545 Test RE 0.1077680274386015 Lambda1 0.77027273\n",
      "59 Train Loss 38.56786 Test MSE 40.79437674905132 Test RE 0.10752001637311157 Lambda1 0.78085065\n",
      "60 Train Loss 38.385803 Test MSE 40.64724018084517 Test RE 0.10732594039722293 Lambda1 0.7911434\n",
      "61 Train Loss 38.198982 Test MSE 40.63435303734055 Test RE 0.10730892528824774 Lambda1 0.79654366\n",
      "62 Train Loss 38.00989 Test MSE 40.599903655409804 Test RE 0.1072634279477064 Lambda1 0.8063603\n",
      "63 Train Loss 37.871998 Test MSE 40.46775920252763 Test RE 0.1070887253237792 Lambda1 0.809176\n",
      "64 Train Loss 37.740017 Test MSE 40.404715446992356 Test RE 0.10700527732695633 Lambda1 0.81170285\n",
      "65 Train Loss 37.49648 Test MSE 40.3024522259131 Test RE 0.10686977783255856 Lambda1 0.82577163\n",
      "66 Train Loss 37.28554 Test MSE 40.03366344659296 Test RE 0.10651280883404365 Lambda1 0.8370455\n",
      "67 Train Loss 37.174366 Test MSE 39.9235420821459 Test RE 0.1063662145435223 Lambda1 0.8462675\n",
      "68 Train Loss 37.038334 Test MSE 39.77925587149116 Test RE 0.10617383344744084 Lambda1 0.8603726\n",
      "69 Train Loss 36.92508 Test MSE 39.7645240190799 Test RE 0.10615417141439737 Lambda1 0.8689323\n",
      "70 Train Loss 36.81449 Test MSE 39.71239764225599 Test RE 0.10608457109755508 Lambda1 0.87524235\n",
      "71 Train Loss 36.75061 Test MSE 39.63845757297325 Test RE 0.10598576624663536 Lambda1 0.8774269\n",
      "72 Train Loss 36.661743 Test MSE 39.575864810626115 Test RE 0.10590205255977975 Lambda1 0.87921554\n",
      "73 Train Loss 36.463985 Test MSE 39.456508444046484 Test RE 0.10574223762036553 Lambda1 0.8771289\n",
      "74 Train Loss 36.355404 Test MSE 39.30458158419649 Test RE 0.10553846159957106 Lambda1 0.875628\n",
      "75 Train Loss 36.286613 Test MSE 39.14753149984603 Test RE 0.10532739950608758 Lambda1 0.8806127\n",
      "76 Train Loss 36.21157 Test MSE 39.021000221245004 Test RE 0.10515704397670197 Lambda1 0.8846255\n",
      "77 Train Loss 36.1624 Test MSE 39.0090304624386 Test RE 0.10514091418776393 Lambda1 0.8826642\n",
      "78 Train Loss 36.057354 Test MSE 38.879343767814376 Test RE 0.10496599662185561 Lambda1 0.8778276\n",
      "79 Train Loss 35.839405 Test MSE 38.838329316843264 Test RE 0.10491061684573304 Lambda1 0.8795823\n",
      "80 Train Loss 35.72868 Test MSE 38.604412107488365 Test RE 0.10459420955425704 Lambda1 0.889046\n",
      "81 Train Loss 35.64202 Test MSE 38.44275631900834 Test RE 0.10437498594141319 Lambda1 0.8976265\n",
      "82 Train Loss 35.39384 Test MSE 38.015690505326425 Test RE 0.10379360881818403 Lambda1 0.91330844\n",
      "83 Train Loss 35.05932 Test MSE 37.55439002892347 Test RE 0.10316194622882628 Lambda1 0.92776215\n",
      "84 Train Loss 34.798668 Test MSE 37.41497283815375 Test RE 0.10297027859647717 Lambda1 0.93440837\n",
      "85 Train Loss 34.535362 Test MSE 37.18239061197611 Test RE 0.10264973324399314 Lambda1 0.94671226\n",
      "86 Train Loss 34.453613 Test MSE 37.121132757278346 Test RE 0.10256514085519758 Lambda1 0.9504236\n",
      "87 Train Loss 34.414852 Test MSE 37.071807335400436 Test RE 0.10249697549805356 Lambda1 0.95350945\n",
      "88 Train Loss 34.353992 Test MSE 37.12610308373198 Test RE 0.10257200709461158 Lambda1 0.9556211\n",
      "89 Train Loss 34.29674 Test MSE 37.14018318667008 Test RE 0.1025914555068292 Lambda1 0.9520505\n",
      "90 Train Loss 34.245842 Test MSE 37.02966452306184 Test RE 0.102438700228984 Lambda1 0.95206964\n",
      "91 Train Loss 34.171543 Test MSE 37.016005512437545 Test RE 0.10241980537310005 Lambda1 0.9484087\n",
      "92 Train Loss 34.127903 Test MSE 36.99507817339874 Test RE 0.1023908492905708 Lambda1 0.9475999\n",
      "93 Train Loss 34.057804 Test MSE 36.90533017751195 Test RE 0.10226657663090512 Lambda1 0.94159144\n",
      "94 Train Loss 34.013885 Test MSE 36.90030518749576 Test RE 0.10225961414042059 Lambda1 0.938425\n",
      "95 Train Loss 33.930717 Test MSE 36.695271610755675 Test RE 0.1019751197383307 Lambda1 0.9466375\n",
      "96 Train Loss 33.849598 Test MSE 36.52267058924993 Test RE 0.10173501038574419 Lambda1 0.950794\n",
      "97 Train Loss 33.760998 Test MSE 36.35187042858777 Test RE 0.10149684709103254 Lambda1 0.9534972\n",
      "98 Train Loss 33.676357 Test MSE 36.23376349570522 Test RE 0.10133183173112446 Lambda1 0.95706445\n",
      "99 Train Loss 33.597694 Test MSE 36.20209334002967 Test RE 0.10128753745787308 Lambda1 0.9622345\n",
      "100 Train Loss 33.52379 Test MSE 36.133483656610665 Test RE 0.10119151260243639 Lambda1 0.96529984\n",
      "101 Train Loss 33.43993 Test MSE 36.096246761489255 Test RE 0.10113935835875587 Lambda1 0.96468157\n",
      "102 Train Loss 33.40123 Test MSE 36.02759206024642 Test RE 0.10104312955510775 Lambda1 0.96362025\n",
      "103 Train Loss 33.330055 Test MSE 35.94766357520642 Test RE 0.10093098344528176 Lambda1 0.9668508\n",
      "104 Train Loss 33.266132 Test MSE 35.92484230080954 Test RE 0.10089894048159632 Lambda1 0.96688217\n",
      "105 Train Loss 33.209496 Test MSE 35.83536142495272 Test RE 0.10077320360957691 Lambda1 0.9695966\n",
      "106 Train Loss 33.059666 Test MSE 35.62655115683961 Test RE 0.10047917528472242 Lambda1 0.9728771\n",
      "107 Train Loss 32.970272 Test MSE 35.530987144149954 Test RE 0.10034432303678216 Lambda1 0.974587\n",
      "108 Train Loss 32.915054 Test MSE 35.385967151939205 Test RE 0.10013933560255059 Lambda1 0.97797525\n",
      "109 Train Loss 32.86625 Test MSE 35.29705410958621 Test RE 0.10001344822770389 Lambda1 0.9810285\n",
      "110 Train Loss 32.808796 Test MSE 35.2180377461736 Test RE 0.09990143991858601 Lambda1 0.9848322\n",
      "111 Train Loss 32.740257 Test MSE 35.12609870096689 Test RE 0.0997709550046307 Lambda1 0.9861699\n",
      "112 Train Loss 32.64134 Test MSE 35.047319303876364 Test RE 0.09965901106712688 Lambda1 0.98548317\n",
      "113 Train Loss 32.5139 Test MSE 34.95199906325093 Test RE 0.09952339457771772 Lambda1 0.9813187\n",
      "114 Train Loss 32.42616 Test MSE 34.85061080313277 Test RE 0.09937894172354679 Lambda1 0.97781533\n",
      "115 Train Loss 32.358562 Test MSE 34.745321435002566 Test RE 0.0992287081879235 Lambda1 0.9791565\n",
      "116 Train Loss 32.299126 Test MSE 34.645289707639265 Test RE 0.0990857655811933 Lambda1 0.97982043\n",
      "117 Train Loss 32.19979 Test MSE 34.43790533753448 Test RE 0.09878876009171524 Lambda1 0.9826891\n",
      "118 Train Loss 32.129562 Test MSE 34.3407304858497 Test RE 0.09864928347992999 Lambda1 0.983585\n",
      "119 Train Loss 32.093143 Test MSE 34.30915262400708 Test RE 0.09860391679936366 Lambda1 0.98765755\n",
      "120 Train Loss 32.054276 Test MSE 34.261902132917285 Test RE 0.09853599485490896 Lambda1 0.98947966\n",
      "121 Train Loss 32.024 Test MSE 34.17904513704767 Test RE 0.09841677586729564 Lambda1 0.98981017\n",
      "122 Train Loss 31.971712 Test MSE 34.11636614461625 Test RE 0.09832649401461221 Lambda1 0.9925356\n",
      "123 Train Loss 31.945114 Test MSE 34.10236826619911 Test RE 0.09830632036084104 Lambda1 0.994851\n",
      "124 Train Loss 31.90229 Test MSE 34.03179971463153 Test RE 0.09820455430188542 Lambda1 0.99716437\n",
      "125 Train Loss 31.883686 Test MSE 34.02501962645653 Test RE 0.09819477126499762 Lambda1 0.9962988\n",
      "126 Train Loss 31.870308 Test MSE 34.00937085212237 Test RE 0.09817218781677106 Lambda1 0.995143\n",
      "127 Train Loss 31.843296 Test MSE 34.026771959166744 Test RE 0.09819729981158111 Lambda1 0.99353135\n",
      "128 Train Loss 31.765509 Test MSE 33.92039317579206 Test RE 0.09804368126424325 Lambda1 0.99321026\n",
      "129 Train Loss 31.627258 Test MSE 33.803290658207096 Test RE 0.09787429808488958 Lambda1 0.9906635\n",
      "130 Train Loss 31.556326 Test MSE 33.82089701300129 Test RE 0.097899783551835 Lambda1 0.98570484\n",
      "131 Train Loss 31.468544 Test MSE 33.742871282390915 Test RE 0.0977867896105728 Lambda1 0.9842219\n",
      "132 Train Loss 31.431211 Test MSE 33.723964452675744 Test RE 0.09775938978999661 Lambda1 0.98650855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 Train Loss 31.401682 Test MSE 33.715160792567545 Test RE 0.09774662888496859 Lambda1 0.9843588\n",
      "134 Train Loss 31.356787 Test MSE 33.63274371459929 Test RE 0.0976270844335869 Lambda1 0.983859\n",
      "135 Train Loss 31.287497 Test MSE 33.49038135368554 Test RE 0.09742024491669608 Lambda1 0.9822369\n",
      "136 Train Loss 31.235178 Test MSE 33.36166653951506 Test RE 0.09723285528818093 Lambda1 0.98157406\n",
      "137 Train Loss 31.214787 Test MSE 33.32638115675437 Test RE 0.09718142191366841 Lambda1 0.98009896\n",
      "138 Train Loss 31.191042 Test MSE 33.32163319668714 Test RE 0.09717449902060442 Lambda1 0.9763334\n",
      "139 Train Loss 31.123545 Test MSE 33.303117765008196 Test RE 0.0971474973758288 Lambda1 0.97259235\n",
      "140 Train Loss 31.087196 Test MSE 33.289610863945974 Test RE 0.09712779509575063 Lambda1 0.97219586\n",
      "141 Train Loss 31.03865 Test MSE 33.31444444650943 Test RE 0.09716401632796837 Lambda1 0.96889144\n",
      "142 Train Loss 30.993608 Test MSE 33.4023158249449 Test RE 0.09729207359579375 Lambda1 0.96647435\n",
      "143 Train Loss 30.932272 Test MSE 33.34176200570484 Test RE 0.09720384499556697 Lambda1 0.9645424\n",
      "144 Train Loss 30.800182 Test MSE 33.1722582321137 Test RE 0.09695644636154663 Lambda1 0.96406543\n",
      "145 Train Loss 30.723965 Test MSE 33.091628098180834 Test RE 0.09683854110089955 Lambda1 0.96416366\n",
      "146 Train Loss 30.673159 Test MSE 33.01672224417512 Test RE 0.09672887766663468 Lambda1 0.96439093\n",
      "147 Train Loss 30.643196 Test MSE 33.004288790353876 Test RE 0.09671066284700516 Lambda1 0.9651716\n",
      "148 Train Loss 30.60875 Test MSE 32.98007650283625 Test RE 0.09667518236747807 Lambda1 0.9682356\n",
      "149 Train Loss 30.540112 Test MSE 32.94883545543971 Test RE 0.09662938275361715 Lambda1 0.97156054\n",
      "150 Train Loss 30.505186 Test MSE 33.010571982935225 Test RE 0.09671986805675109 Lambda1 0.9739869\n",
      "151 Train Loss 30.481998 Test MSE 32.98016978508588 Test RE 0.09667531908747809 Lambda1 0.97658217\n",
      "152 Train Loss 30.389843 Test MSE 33.039650963127244 Test RE 0.09676245889728637 Lambda1 0.9740879\n",
      "153 Train Loss 30.335056 Test MSE 32.95263432667848 Test RE 0.09663495308710095 Lambda1 0.9751469\n",
      "154 Train Loss 30.285006 Test MSE 33.00596138546822 Test RE 0.09671311337296358 Lambda1 0.97789997\n",
      "155 Train Loss 30.23086 Test MSE 32.9321033444369 Test RE 0.09660484442242691 Lambda1 0.9774658\n",
      "156 Train Loss 30.167967 Test MSE 32.85707497575507 Test RE 0.09649473549908025 Lambda1 0.97925794\n",
      "157 Train Loss 30.073183 Test MSE 32.71392860830591 Test RE 0.0962843097665102 Lambda1 0.97821265\n",
      "158 Train Loss 30.037113 Test MSE 32.64717994840863 Test RE 0.0961860315932278 Lambda1 0.97771645\n",
      "159 Train Loss 29.961334 Test MSE 32.54124744676899 Test RE 0.09602985415584897 Lambda1 0.98337424\n",
      "160 Train Loss 29.895893 Test MSE 32.42327576679031 Test RE 0.09585562774575689 Lambda1 0.98484975\n",
      "161 Train Loss 29.829472 Test MSE 32.34339920061951 Test RE 0.09573748206712741 Lambda1 0.990539\n",
      "162 Train Loss 29.813025 Test MSE 32.343515376100925 Test RE 0.0957376540085182 Lambda1 0.9925222\n",
      "163 Train Loss 29.799795 Test MSE 32.30632532494032 Test RE 0.0956825964134946 Lambda1 0.99466234\n",
      "164 Train Loss 29.792377 Test MSE 32.29039489887683 Test RE 0.09565900269826182 Lambda1 0.9955653\n",
      "165 Train Loss 29.775517 Test MSE 32.286931916351364 Test RE 0.09565387308715145 Lambda1 0.99509203\n",
      "166 Train Loss 29.745981 Test MSE 32.207794477049156 Test RE 0.09553657413146031 Lambda1 0.99567795\n",
      "167 Train Loss 29.696583 Test MSE 32.200582649960495 Test RE 0.0955258774687756 Lambda1 0.994894\n",
      "168 Train Loss 29.63393 Test MSE 32.24001149209658 Test RE 0.09558434415087569 Lambda1 0.9930909\n",
      "169 Train Loss 29.565939 Test MSE 32.265450885444345 Test RE 0.09562204773925218 Lambda1 0.99009675\n",
      "170 Train Loss 29.510353 Test MSE 32.24227270693207 Test RE 0.09558769608745038 Lambda1 0.99034524\n",
      "171 Train Loss 29.485586 Test MSE 32.21843064100952 Test RE 0.09555234762364397 Lambda1 0.98887396\n",
      "172 Train Loss 29.39277 Test MSE 32.20824852352239 Test RE 0.09553724753819888 Lambda1 0.9952628\n",
      "173 Train Loss 29.264915 Test MSE 32.17726129708095 Test RE 0.0954912787747232 Lambda1 1.0040387\n",
      "174 Train Loss 29.096302 Test MSE 31.876136529074596 Test RE 0.09504341002053879 Lambda1 1.0069132\n",
      "175 Train Loss 29.055874 Test MSE 31.90788531899822 Test RE 0.0950907300943662 Lambda1 1.0073385\n",
      "176 Train Loss 29.02335 Test MSE 31.873925818564846 Test RE 0.09504011418341068 Lambda1 1.008866\n",
      "177 Train Loss 28.98848 Test MSE 31.875187887531357 Test RE 0.095041995752566 Lambda1 1.0086083\n",
      "178 Train Loss 28.964878 Test MSE 31.843819800202535 Test RE 0.09499521925263693 Lambda1 1.0084958\n",
      "179 Train Loss 28.926918 Test MSE 31.801163816565722 Test RE 0.09493157311277854 Lambda1 1.0082067\n",
      "180 Train Loss 28.88964 Test MSE 31.765356281032005 Test RE 0.09487811244206001 Lambda1 1.0076021\n",
      "181 Train Loss 28.857037 Test MSE 31.678697331412426 Test RE 0.09474860574326925 Lambda1 1.0084245\n",
      "182 Train Loss 28.85347 Test MSE 31.68345830888273 Test RE 0.09475572533889351 Lambda1 1.008426\n",
      "183 Train Loss 28.827133 Test MSE 31.691143862456467 Test RE 0.09476721723536355 Lambda1 1.0080855\n",
      "184 Train Loss 28.796988 Test MSE 31.669509796460048 Test RE 0.09473486513323734 Lambda1 1.008411\n",
      "185 Train Loss 28.76306 Test MSE 31.67217856461855 Test RE 0.09473885667063502 Lambda1 1.0072823\n",
      "186 Train Loss 28.746754 Test MSE 31.66156526391694 Test RE 0.09472298191448648 Lambda1 1.007909\n",
      "187 Train Loss 28.738058 Test MSE 31.66224301223128 Test RE 0.09472399573040544 Lambda1 1.0077691\n",
      "188 Train Loss 28.712042 Test MSE 31.6729266970862 Test RE 0.09473997558318363 Lambda1 1.0089681\n",
      "189 Train Loss 28.672606 Test MSE 31.64636115063672 Test RE 0.09470023585198323 Lambda1 1.0113553\n",
      "190 Train Loss 28.642235 Test MSE 31.573004063459205 Test RE 0.094590413368921 Lambda1 1.0117024\n",
      "191 Train Loss 28.632544 Test MSE 31.558321460660746 Test RE 0.0945684168087045 Lambda1 1.0120623\n",
      "192 Train Loss 28.612371 Test MSE 31.502723257985462 Test RE 0.09448507663780656 Lambda1 1.0117693\n",
      "193 Train Loss 28.556532 Test MSE 31.468868132141022 Test RE 0.09443429271012818 Lambda1 1.0144141\n",
      "194 Train Loss 28.41814 Test MSE 31.393404049503513 Test RE 0.09432099542306294 Lambda1 1.0172666\n",
      "195 Train Loss 28.342209 Test MSE 31.283630934388142 Test RE 0.09415594520354147 Lambda1 1.0157956\n",
      "196 Train Loss 28.285349 Test MSE 31.235604697227608 Test RE 0.09408364393183535 Lambda1 1.0157573\n",
      "197 Train Loss 28.264448 Test MSE 31.235604392678397 Test RE 0.09408364347317448 Lambda1 1.0151453\n",
      "198 Train Loss 28.255455 Test MSE 31.215637037197016 Test RE 0.09405356718826367 Lambda1 1.0151591\n",
      "199 Train Loss 28.24217 Test MSE 31.2271601069442 Test RE 0.09407092524848118 Lambda1 1.0139686\n",
      "Training time: 617.94\n",
      "Training time: 617.94\n",
      "inv_HT_rowdy\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 840.31085 Test MSE 842.4662521421243 Test RE 0.4886134606141353 Lambda1 -0.013500532\n",
      "1 Train Loss 584.3628 Test MSE 565.243409838058 Test RE 0.400227498470976 Lambda1 -0.016837567\n",
      "2 Train Loss 348.4702 Test MSE 337.95990460673437 Test RE 0.3094722839297453 Lambda1 0.0088206865\n",
      "3 Train Loss 283.6941 Test MSE 287.0703147088687 Test RE 0.2852221920804816 Lambda1 0.010987555\n",
      "4 Train Loss 273.97723 Test MSE 279.0152580044206 Test RE 0.2811921208324904 Lambda1 0.011908729\n",
      "5 Train Loss 266.37064 Test MSE 273.25039528792655 Test RE 0.2782720388760596 Lambda1 0.02185569\n",
      "6 Train Loss 257.0294 Test MSE 258.95620277448444 Test RE 0.27089583615503876 Lambda1 0.055608667\n",
      "7 Train Loss 249.73915 Test MSE 251.0900694672413 Test RE 0.26674969962289385 Lambda1 0.08088126\n",
      "8 Train Loss 239.49887 Test MSE 243.11712348036147 Test RE 0.26248043972288215 Lambda1 0.10377232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 231.80211 Test MSE 235.47058952680752 Test RE 0.2583196872388495 Lambda1 0.124638304\n",
      "10 Train Loss 226.21886 Test MSE 229.414874871967 Test RE 0.25497638372840375 Lambda1 0.15131897\n",
      "11 Train Loss 223.31718 Test MSE 227.7773842703693 Test RE 0.25406478386607867 Lambda1 0.15819755\n",
      "12 Train Loss 218.9158 Test MSE 221.8287189839501 Test RE 0.2507252403205302 Lambda1 0.18127212\n",
      "13 Train Loss 214.11586 Test MSE 216.4725929818309 Test RE 0.24767982393602767 Lambda1 0.2087099\n",
      "14 Train Loss 209.01251 Test MSE 209.84499564309218 Test RE 0.24385882617076607 Lambda1 0.2350667\n",
      "15 Train Loss 201.86426 Test MSE 199.90032423951862 Test RE 0.2380103921573059 Lambda1 0.24963282\n",
      "16 Train Loss 194.35258 Test MSE 192.62150455332667 Test RE 0.23363696500410513 Lambda1 0.25609508\n",
      "17 Train Loss 186.93768 Test MSE 183.61030634425566 Test RE 0.22810651962212158 Lambda1 0.27732396\n",
      "18 Train Loss 174.29785 Test MSE 165.029816730409 Test RE 0.21625710224062109 Lambda1 0.30938035\n",
      "19 Train Loss 161.33401 Test MSE 146.87078577802367 Test RE 0.20401254793618684 Lambda1 0.32638395\n",
      "20 Train Loss 144.9572 Test MSE 133.04971652811994 Test RE 0.19417626692020415 Lambda1 0.33340612\n",
      "21 Train Loss 136.56654 Test MSE 121.37276296927693 Test RE 0.1854598011810599 Lambda1 0.35425162\n",
      "22 Train Loss 127.48883 Test MSE 110.62511712131389 Test RE 0.1770581990133788 Lambda1 0.37810016\n",
      "23 Train Loss 113.77258 Test MSE 98.62687831342909 Test RE 0.1671809598183698 Lambda1 0.40352517\n",
      "24 Train Loss 104.645485 Test MSE 93.42982298124402 Test RE 0.16271662721638183 Lambda1 0.4068331\n",
      "25 Train Loss 97.700714 Test MSE 86.8209408585153 Test RE 0.15685610067406344 Lambda1 0.41541415\n",
      "26 Train Loss 92.12557 Test MSE 84.31910032997577 Test RE 0.1545795905678611 Lambda1 0.42873383\n",
      "27 Train Loss 87.79748 Test MSE 82.61124336330859 Test RE 0.1530061015822266 Lambda1 0.43774587\n",
      "28 Train Loss 81.34483 Test MSE 72.2729581994048 Test RE 0.143112341112201 Lambda1 0.45744646\n",
      "29 Train Loss 76.62327 Test MSE 67.47174110299801 Test RE 0.13827705748565725 Lambda1 0.48239303\n",
      "30 Train Loss 71.91378 Test MSE 64.25565399434583 Test RE 0.13494128601082825 Lambda1 0.49708194\n",
      "31 Train Loss 68.86402 Test MSE 61.504945331927644 Test RE 0.1320213559801187 Lambda1 0.5065868\n",
      "32 Train Loss 64.92858 Test MSE 55.77447530359663 Test RE 0.1257207367223176 Lambda1 0.5168896\n",
      "33 Train Loss 60.210865 Test MSE 52.07145147116069 Test RE 0.12147558870332074 Lambda1 0.5242414\n",
      "34 Train Loss 56.95114 Test MSE 51.601905733541344 Test RE 0.1209266553647658 Lambda1 0.5319944\n",
      "35 Train Loss 54.964825 Test MSE 49.42609729357186 Test RE 0.11834974608337036 Lambda1 0.5450294\n",
      "36 Train Loss 53.04292 Test MSE 47.28284905492994 Test RE 0.11575532783609005 Lambda1 0.55560166\n",
      "37 Train Loss 50.667793 Test MSE 46.80224662420842 Test RE 0.11516553276766364 Lambda1 0.56654274\n",
      "38 Train Loss 49.214237 Test MSE 45.77571608953128 Test RE 0.1138955468356439 Lambda1 0.58179855\n",
      "39 Train Loss 47.936752 Test MSE 45.00001910732883 Test RE 0.11292640944107679 Lambda1 0.58831066\n",
      "40 Train Loss 46.202255 Test MSE 44.60874651766651 Test RE 0.11243439326070145 Lambda1 0.5900978\n",
      "41 Train Loss 44.210266 Test MSE 43.29388541145152 Test RE 0.1107649746141655 Lambda1 0.6105375\n",
      "42 Train Loss 42.93379 Test MSE 42.72658943320924 Test RE 0.11003688418735529 Lambda1 0.6233376\n",
      "43 Train Loss 41.545288 Test MSE 41.85847934437262 Test RE 0.10891329424958987 Lambda1 0.645304\n",
      "44 Train Loss 41.218796 Test MSE 41.5896511976443 Test RE 0.10856299341794053 Lambda1 0.64291924\n",
      "45 Train Loss 40.18407 Test MSE 40.98989342431921 Test RE 0.10777736591253445 Lambda1 0.6431023\n",
      "46 Train Loss 39.306522 Test MSE 40.671813000882196 Test RE 0.10735837682513186 Lambda1 0.64792264\n",
      "47 Train Loss 38.459858 Test MSE 40.182179581070194 Test RE 0.10671019554928123 Lambda1 0.65675783\n",
      "48 Train Loss 37.639675 Test MSE 39.98211409645848 Test RE 0.10644421113080367 Lambda1 0.6633897\n",
      "49 Train Loss 37.118214 Test MSE 39.447868262330324 Test RE 0.10573065927491142 Lambda1 0.66649956\n",
      "50 Train Loss 36.616993 Test MSE 38.79491459086915 Test RE 0.10485196422972329 Lambda1 0.6718702\n",
      "51 Train Loss 35.858753 Test MSE 38.71635155140169 Test RE 0.10474574330199377 Lambda1 0.6701377\n",
      "52 Train Loss 35.37807 Test MSE 38.533489753789574 Test RE 0.1044980874105562 Lambda1 0.6772325\n",
      "53 Train Loss 35.19375 Test MSE 38.33994918409586 Test RE 0.10423532794059784 Lambda1 0.6798977\n",
      "54 Train Loss 34.89775 Test MSE 38.14962714175942 Test RE 0.10397629054160641 Lambda1 0.68080354\n",
      "55 Train Loss 34.74571 Test MSE 37.98498597306246 Test RE 0.10375168431366305 Lambda1 0.6867981\n",
      "56 Train Loss 34.575264 Test MSE 37.69193341589981 Test RE 0.10335068947763054 Lambda1 0.6900163\n",
      "57 Train Loss 34.445503 Test MSE 37.74318303750042 Test RE 0.10342092843858779 Lambda1 0.68890303\n",
      "58 Train Loss 34.307064 Test MSE 37.698583316530616 Test RE 0.10335980603709166 Lambda1 0.688416\n",
      "59 Train Loss 34.149868 Test MSE 37.515973159950136 Test RE 0.10310916713919999 Lambda1 0.6927649\n",
      "60 Train Loss 33.761765 Test MSE 37.341111710784354 Test RE 0.10286859125700526 Lambda1 0.6984979\n",
      "61 Train Loss 33.0873 Test MSE 36.48349424397264 Test RE 0.10168043229491387 Lambda1 0.71655446\n",
      "62 Train Loss 32.801147 Test MSE 36.16889755611212 Test RE 0.1012410886198872 Lambda1 0.72349524\n",
      "63 Train Loss 32.289486 Test MSE 35.676404272685524 Test RE 0.10054945220816242 Lambda1 0.73339194\n",
      "64 Train Loss 31.935518 Test MSE 35.6480921949586 Test RE 0.10050954727698679 Lambda1 0.7356407\n",
      "65 Train Loss 31.840183 Test MSE 35.61919780293739 Test RE 0.10046880525112516 Lambda1 0.7382242\n",
      "66 Train Loss 31.795925 Test MSE 35.566160150438044 Test RE 0.10039397741958718 Lambda1 0.73986703\n",
      "67 Train Loss 31.632837 Test MSE 35.40384504351319 Test RE 0.10016462887925869 Lambda1 0.7455453\n",
      "68 Train Loss 31.551674 Test MSE 35.3671678259069 Test RE 0.1001127318063267 Lambda1 0.74783766\n",
      "69 Train Loss 31.43102 Test MSE 35.28609265524178 Test RE 0.09999791749857459 Lambda1 0.75223523\n",
      "70 Train Loss 31.34363 Test MSE 35.01252931556834 Test RE 0.0996095351479167 Lambda1 0.7576027\n",
      "71 Train Loss 31.213512 Test MSE 34.82082736580165 Test RE 0.09933646787504845 Lambda1 0.7671348\n",
      "72 Train Loss 31.157108 Test MSE 34.67523675835302 Test RE 0.09912858071590397 Lambda1 0.77257293\n",
      "73 Train Loss 31.097883 Test MSE 34.6060898974172 Test RE 0.09902969385324888 Lambda1 0.77470255\n",
      "74 Train Loss 31.050432 Test MSE 34.58173513591437 Test RE 0.09899484060807469 Lambda1 0.7741026\n",
      "75 Train Loss 30.98253 Test MSE 34.609745670458736 Test RE 0.09903492444308876 Lambda1 0.77432334\n",
      "76 Train Loss 30.88341 Test MSE 34.45646921733407 Test RE 0.0988153827270572 Lambda1 0.7752245\n",
      "77 Train Loss 30.833025 Test MSE 34.38389825555407 Test RE 0.09871126719520525 Lambda1 0.77642804\n",
      "78 Train Loss 30.805248 Test MSE 34.32073032863032 Test RE 0.09862055245840566 Lambda1 0.77898586\n",
      "79 Train Loss 30.767334 Test MSE 34.188376160338755 Test RE 0.09843020904771048 Lambda1 0.7815106\n",
      "80 Train Loss 30.64767 Test MSE 33.98281292517337 Test RE 0.09813384898652401 Lambda1 0.7842304\n",
      "81 Train Loss 30.574791 Test MSE 33.93074397915495 Test RE 0.09805863913123225 Lambda1 0.7850555\n",
      "82 Train Loss 30.45066 Test MSE 33.82515378424002 Test RE 0.09790594429722731 Lambda1 0.78608185\n",
      "83 Train Loss 30.261478 Test MSE 33.79364058583253 Test RE 0.0978603266421799 Lambda1 0.7882389\n",
      "84 Train Loss 30.152262 Test MSE 33.7177531448355 Test RE 0.09775038667254035 Lambda1 0.7910964\n",
      "85 Train Loss 30.135035 Test MSE 33.723706513090654 Test RE 0.0977590159302641 Lambda1 0.79063547\n",
      "86 Train Loss 30.122301 Test MSE 33.663123607315 Test RE 0.09767116693421078 Lambda1 0.7916234\n",
      "87 Train Loss 30.086649 Test MSE 33.55494884268422 Test RE 0.09751410995319441 Lambda1 0.7938245\n",
      "88 Train Loss 30.065483 Test MSE 33.58115267384201 Test RE 0.09755217802800101 Lambda1 0.79408455\n",
      "89 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "90 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "91 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "92 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "93 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "95 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "96 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "97 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "98 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "99 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "100 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "101 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "102 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "103 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "104 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "105 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "106 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "107 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "108 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "109 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "110 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "111 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "112 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "113 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "114 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "115 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "116 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "117 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "118 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "119 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "120 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "121 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "122 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "123 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "124 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "125 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "126 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "127 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "128 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "129 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "130 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "131 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "132 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "133 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "134 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "135 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "136 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "137 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "138 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "139 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "140 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "141 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "142 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "143 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "144 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "145 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "146 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "147 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "148 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "149 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "150 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "151 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "152 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "153 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "154 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "155 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "156 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "157 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "158 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "159 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "160 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "161 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "162 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "163 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "164 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "165 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "166 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "167 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "168 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "169 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "170 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "171 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "172 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "173 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "174 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "175 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "176 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "178 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "179 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "180 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "181 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "182 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "183 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "184 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "185 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "186 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "187 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "188 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "189 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "190 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "191 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "192 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "193 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "194 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "195 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "196 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "197 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "198 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "199 Train Loss 30.064766 Test MSE 33.57952529162778 Test RE 0.09754981425263524 Lambda1 0.79416233\n",
      "Training time: 365.18\n",
      "Training time: 365.18\n",
      "inv_HT_rowdy\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 854.4444 Test MSE 857.4453841571315 Test RE 0.4929381195552919 Lambda1 -0.1052174\n",
      "1 Train Loss 854.27386 Test MSE 856.9224713235609 Test RE 0.49278778754295693 Lambda1 -0.09185451\n",
      "2 Train Loss 852.90436 Test MSE 855.0296037778083 Test RE 0.4922432238749519 Lambda1 -0.024371462\n",
      "3 Train Loss 849.6901 Test MSE 849.6159715801135 Test RE 0.49068242688436825 Lambda1 0.06235647\n",
      "4 Train Loss 816.13306 Test MSE 802.1202084568927 Test RE 0.47676997731764736 Lambda1 0.1849561\n",
      "5 Train Loss 623.40216 Test MSE 589.2072657039304 Test RE 0.4086233852954587 Lambda1 0.15503801\n",
      "6 Train Loss 333.11783 Test MSE 292.40706230979197 Test RE 0.2878611787598857 Lambda1 0.0361604\n",
      "7 Train Loss 274.6482 Test MSE 281.4821030175389 Test RE 0.2824324309969108 Lambda1 0.012375779\n",
      "8 Train Loss 267.1667 Test MSE 272.60324179949765 Test RE 0.2779423202803828 Lambda1 0.018211467\n",
      "9 Train Loss 258.61154 Test MSE 262.44952217765103 Test RE 0.27271690769873064 Lambda1 0.04382292\n",
      "10 Train Loss 242.39023 Test MSE 244.52777300794426 Test RE 0.2632408393813565 Lambda1 0.07865234\n",
      "11 Train Loss 230.29442 Test MSE 229.42916125572282 Test RE 0.25498432269434906 Lambda1 0.11693122\n",
      "12 Train Loss 211.16362 Test MSE 208.06712673745633 Test RE 0.2428236067634085 Lambda1 0.19360663\n",
      "13 Train Loss 199.19493 Test MSE 196.59605765103555 Test RE 0.23603509059256264 Lambda1 0.21544279\n",
      "14 Train Loss 189.90982 Test MSE 185.81767713845633 Test RE 0.22947357626122505 Lambda1 0.21557753\n",
      "15 Train Loss 167.2674 Test MSE 159.80220644777236 Test RE 0.21280437700432864 Lambda1 0.2685111\n",
      "16 Train Loss 140.36713 Test MSE 120.75098363853118 Test RE 0.1849841461232415 Lambda1 0.34900764\n",
      "17 Train Loss 127.05267 Test MSE 102.0027877359692 Test RE 0.1700181128099755 Lambda1 0.38850123\n",
      "18 Train Loss 118.04959 Test MSE 101.74949271892261 Test RE 0.16980688569670474 Lambda1 0.3929477\n",
      "19 Train Loss 110.48394 Test MSE 93.86661968142909 Test RE 0.16309654450088346 Lambda1 0.41388717\n",
      "20 Train Loss 96.02999 Test MSE 78.6553207166558 Test RE 0.14929773660399906 Lambda1 0.46916482\n",
      "21 Train Loss 88.05545 Test MSE 75.10130675921562 Test RE 0.14588576514340232 Lambda1 0.4815526\n",
      "22 Train Loss 81.88144 Test MSE 67.217556886723 Test RE 0.13801634825938686 Lambda1 0.49322447\n",
      "23 Train Loss 71.25798 Test MSE 60.61453276431048 Test RE 0.1310622295245283 Lambda1 0.51569575\n",
      "24 Train Loss 63.95242 Test MSE 59.774741861607694 Test RE 0.13015115462621107 Lambda1 0.5299211\n",
      "25 Train Loss 58.83001 Test MSE 58.521963155101254 Test RE 0.12878005724805594 Lambda1 0.5128864\n",
      "26 Train Loss 56.900272 Test MSE 58.45872219600348 Test RE 0.12871045623583666 Lambda1 0.51973873\n",
      "27 Train Loss 55.270123 Test MSE 56.880816755576184 Test RE 0.126961511036547 Lambda1 0.53351766\n",
      "28 Train Loss 54.082043 Test MSE 56.14490636613413 Test RE 0.1261375384531777 Lambda1 0.5402679\n",
      "29 Train Loss 52.88749 Test MSE 54.46968178391431 Test RE 0.12424147241612295 Lambda1 0.549966\n",
      "30 Train Loss 50.913403 Test MSE 53.233533775903055 Test RE 0.122823598990524 Lambda1 0.57105774\n",
      "31 Train Loss 49.979286 Test MSE 52.80109916505957 Test RE 0.12232371220838363 Lambda1 0.57085013\n",
      "32 Train Loss 48.921432 Test MSE 51.9851492554007 Test RE 0.12137488131369503 Lambda1 0.5767202\n",
      "33 Train Loss 48.433285 Test MSE 51.522696031229664 Test RE 0.1208338076019668 Lambda1 0.58484876\n",
      "34 Train Loss 47.795757 Test MSE 50.93603773678692 Test RE 0.1201439067587933 Lambda1 0.5936709\n",
      "35 Train Loss 46.371925 Test MSE 50.09440601718319 Test RE 0.11914718507872121 Lambda1 0.6045812\n",
      "36 Train Loss 45.52945 Test MSE 49.128015433488585 Test RE 0.11799233102919947 Lambda1 0.62101805\n",
      "37 Train Loss 45.007946 Test MSE 48.46826426321602 Test RE 0.11719738033783499 Lambda1 0.6334479\n",
      "38 Train Loss 43.269547 Test MSE 47.2971443535166 Test RE 0.11577282500423153 Lambda1 0.65289193\n",
      "39 Train Loss 41.980198 Test MSE 46.24581134965122 Test RE 0.11447888010050289 Lambda1 0.67024815\n",
      "40 Train Loss 40.71113 Test MSE 45.43303420302494 Test RE 0.11346842887367763 Lambda1 0.69323754\n",
      "41 Train Loss 40.045403 Test MSE 44.14092168169664 Test RE 0.11184327335236283 Lambda1 0.7030951\n",
      "42 Train Loss 39.436684 Test MSE 43.36368955749397 Test RE 0.11085423364370814 Lambda1 0.71576655\n",
      "43 Train Loss 38.379135 Test MSE 42.88311359718064 Test RE 0.11023825394705407 Lambda1 0.7323287\n",
      "44 Train Loss 37.958103 Test MSE 42.23964310360274 Test RE 0.10940805329114371 Lambda1 0.7385188\n",
      "45 Train Loss 37.639137 Test MSE 42.05375983558192 Test RE 0.10916705279167324 Lambda1 0.74413574\n",
      "46 Train Loss 37.49595 Test MSE 41.6040218069588 Test RE 0.10858174786255281 Lambda1 0.75361973\n",
      "47 Train Loss 37.315857 Test MSE 41.500714387675075 Test RE 0.10844685378529995 Lambda1 0.7604126\n",
      "48 Train Loss 36.9267 Test MSE 41.351797688164936 Test RE 0.10825210941581022 Lambda1 0.76581615\n",
      "49 Train Loss 36.617558 Test MSE 41.14031956818986 Test RE 0.10797494737832665 Lambda1 0.7790506\n",
      "50 Train Loss 36.40037 Test MSE 40.687529545031055 Test RE 0.10737911772125594 Lambda1 0.78606564\n",
      "51 Train Loss 36.195152 Test MSE 40.223352517134046 Test RE 0.10676485220560354 Lambda1 0.7952101\n",
      "52 Train Loss 36.0148 Test MSE 39.83694606061469 Test RE 0.10625079528634644 Lambda1 0.8078032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 Train Loss 35.882675 Test MSE 39.640226882195506 Test RE 0.10598813161996994 Lambda1 0.8116282\n",
      "54 Train Loss 35.56951 Test MSE 39.25222060420115 Test RE 0.10546813978558828 Lambda1 0.8228824\n",
      "55 Train Loss 35.391785 Test MSE 39.055422166607805 Test RE 0.10520341531626057 Lambda1 0.8229684\n",
      "56 Train Loss 35.136673 Test MSE 39.172053083772546 Test RE 0.10536038230520516 Lambda1 0.81265104\n",
      "57 Train Loss 35.052593 Test MSE 39.24386070616392 Test RE 0.10545690793892766 Lambda1 0.8107341\n",
      "58 Train Loss 35.026802 Test MSE 39.28717359298669 Test RE 0.10551508752957595 Lambda1 0.8081372\n",
      "59 Train Loss 34.87071 Test MSE 39.05345151877022 Test RE 0.10520076112016341 Lambda1 0.8135976\n",
      "60 Train Loss 34.6975 Test MSE 38.855923992784504 Test RE 0.1049343776424842 Lambda1 0.8213214\n",
      "61 Train Loss 34.59317 Test MSE 38.723583921009194 Test RE 0.10475552630742321 Lambda1 0.82111204\n",
      "62 Train Loss 34.549084 Test MSE 38.63311721839596 Test RE 0.10463308892460678 Lambda1 0.82201296\n",
      "63 Train Loss 34.473473 Test MSE 38.517985569463995 Test RE 0.10447706257447076 Lambda1 0.82527995\n",
      "64 Train Loss 34.361786 Test MSE 38.589103986646634 Test RE 0.10457346970262349 Lambda1 0.8226236\n",
      "65 Train Loss 34.26182 Test MSE 38.60125944833307 Test RE 0.10458993858357085 Lambda1 0.81985784\n",
      "66 Train Loss 34.118343 Test MSE 38.45548354194052 Test RE 0.10439226219722215 Lambda1 0.81877464\n",
      "67 Train Loss 33.902866 Test MSE 38.147496631366664 Test RE 0.10397338716250316 Lambda1 0.8273689\n",
      "68 Train Loss 33.78008 Test MSE 37.98401185749025 Test RE 0.1037503539619898 Lambda1 0.83203876\n",
      "69 Train Loss 33.684864 Test MSE 37.69868062373787 Test RE 0.10335993943265578 Lambda1 0.8371007\n",
      "70 Train Loss 33.608368 Test MSE 37.61315366599363 Test RE 0.10324262656412944 Lambda1 0.84159845\n",
      "71 Train Loss 33.572376 Test MSE 37.52977909374582 Test RE 0.10312813755733441 Lambda1 0.8444093\n",
      "72 Train Loss 33.44038 Test MSE 37.33072519492421 Test RE 0.1028542836947277 Lambda1 0.8532745\n",
      "73 Train Loss 33.280785 Test MSE 37.19071994273924 Test RE 0.1026612300263314 Lambda1 0.86118716\n",
      "74 Train Loss 33.139072 Test MSE 36.93488393018319 Test RE 0.10230751592428006 Lambda1 0.86992484\n",
      "75 Train Loss 33.0046 Test MSE 36.67103127555596 Test RE 0.10194143257187137 Lambda1 0.8792116\n",
      "76 Train Loss 32.9293 Test MSE 36.57458512298077 Test RE 0.10180728946668136 Lambda1 0.8841873\n",
      "77 Train Loss 32.809803 Test MSE 36.51558035917851 Test RE 0.10172513488061523 Lambda1 0.8873261\n",
      "78 Train Loss 32.591835 Test MSE 36.41835856004585 Test RE 0.10158962433672314 Lambda1 0.8924352\n",
      "79 Train Loss 32.531563 Test MSE 36.330399470080174 Test RE 0.10146686848953232 Lambda1 0.89528364\n",
      "80 Train Loss 32.486572 Test MSE 36.283173082215406 Test RE 0.10140089795193434 Lambda1 0.8978738\n",
      "81 Train Loss 32.36812 Test MSE 36.19123050202339 Test RE 0.10127234009551944 Lambda1 0.89967763\n",
      "82 Train Loss 32.16975 Test MSE 35.95926353123291 Test RE 0.10094726684756003 Lambda1 0.90136933\n",
      "83 Train Loss 32.02232 Test MSE 35.72692368648504 Test RE 0.1006206183260433 Lambda1 0.91099215\n",
      "84 Train Loss 31.88966 Test MSE 35.352336450136896 Test RE 0.10009173825023993 Lambda1 0.9258647\n",
      "85 Train Loss 31.859594 Test MSE 35.228431570127064 Test RE 0.0999161806799241 Lambda1 0.93015003\n",
      "86 Train Loss 31.803541 Test MSE 34.94253889025375 Test RE 0.09950992507280469 Lambda1 0.94135165\n",
      "87 Train Loss 31.749567 Test MSE 34.966826789157636 Test RE 0.09954450280011948 Lambda1 0.9379845\n",
      "88 Train Loss 31.644936 Test MSE 34.96274148222032 Test RE 0.09953868754943629 Lambda1 0.93909687\n",
      "89 Train Loss 31.565401 Test MSE 34.8759668647563 Test RE 0.09941508743719735 Lambda1 0.9437088\n",
      "90 Train Loss 31.452738 Test MSE 34.910352383651876 Test RE 0.0994640838880222 Lambda1 0.9406379\n",
      "91 Train Loss 31.39534 Test MSE 35.03245184311535 Test RE 0.0996378705971112 Lambda1 0.9380185\n",
      "92 Train Loss 31.368858 Test MSE 35.05496637582553 Test RE 0.09966988291234251 Lambda1 0.9363026\n",
      "93 Train Loss 31.30886 Test MSE 35.039310097030146 Test RE 0.0996476231028479 Lambda1 0.9415762\n",
      "94 Train Loss 31.278059 Test MSE 34.93185426964363 Test RE 0.09949470999227517 Lambda1 0.9449037\n",
      "95 Train Loss 31.247326 Test MSE 34.97058417263789 Test RE 0.09954985096661656 Lambda1 0.94500256\n",
      "96 Train Loss 31.229948 Test MSE 34.94936959484881 Test RE 0.09951965089280165 Lambda1 0.9469447\n",
      "97 Train Loss 31.144583 Test MSE 34.85015795660349 Test RE 0.09937829605975497 Lambda1 0.94740295\n",
      "98 Train Loss 31.066126 Test MSE 34.84688906612843 Test RE 0.09937363518585102 Lambda1 0.9480371\n",
      "99 Train Loss 31.03721 Test MSE 34.73718792035961 Test RE 0.09921709330920932 Lambda1 0.95382947\n",
      "100 Train Loss 31.025024 Test MSE 34.66138738279925 Test RE 0.09910878263837081 Lambda1 0.9575633\n",
      "101 Train Loss 31.019203 Test MSE 34.658409778646906 Test RE 0.09910452555169458 Lambda1 0.9588906\n",
      "102 Train Loss 31.002165 Test MSE 34.62460552055936 Test RE 0.09905618270681028 Lambda1 0.961018\n",
      "103 Train Loss 30.974962 Test MSE 34.558168203252976 Test RE 0.0989611031113054 Lambda1 0.963316\n",
      "104 Train Loss 30.949858 Test MSE 34.56671300500394 Test RE 0.09897333684340015 Lambda1 0.96048856\n",
      "105 Train Loss 30.932207 Test MSE 34.54164834511586 Test RE 0.09893744707150257 Lambda1 0.9627511\n",
      "106 Train Loss 30.925074 Test MSE 34.5224653444196 Test RE 0.09890997037617487 Lambda1 0.96218896\n",
      "107 Train Loss 30.914984 Test MSE 34.46804825603868 Test RE 0.09883198470424466 Lambda1 0.9643468\n",
      "108 Train Loss 30.903795 Test MSE 34.406944328135296 Test RE 0.09874434264734949 Lambda1 0.96769553\n",
      "109 Train Loss 30.879684 Test MSE 34.30369496226143 Test RE 0.09859607387427813 Lambda1 0.9734209\n",
      "110 Train Loss 30.859758 Test MSE 34.263153727348914 Test RE 0.0985377946086956 Lambda1 0.9744214\n",
      "111 Train Loss 30.816126 Test MSE 34.25629289182688 Test RE 0.09852792853721057 Lambda1 0.9762913\n",
      "112 Train Loss 30.796795 Test MSE 34.23106736794822 Test RE 0.09849164503827368 Lambda1 0.97639376\n",
      "113 Train Loss 30.777912 Test MSE 34.26196856824283 Test RE 0.09853609038766277 Lambda1 0.9772133\n",
      "114 Train Loss 30.765554 Test MSE 34.27959156288046 Test RE 0.0985614286537594 Lambda1 0.976193\n",
      "115 Train Loss 30.745335 Test MSE 34.294618263187786 Test RE 0.09858302883495731 Lambda1 0.9733216\n",
      "116 Train Loss 30.726568 Test MSE 34.267915617675506 Test RE 0.09854464175815192 Lambda1 0.97393554\n",
      "117 Train Loss 30.703709 Test MSE 34.237600341033485 Test RE 0.09850104311753906 Lambda1 0.9756151\n",
      "118 Train Loss 30.685852 Test MSE 34.21756941667907 Test RE 0.09847222457305675 Lambda1 0.97462666\n",
      "119 Train Loss 30.6774 Test MSE 34.21042701111277 Test RE 0.09846194673504743 Lambda1 0.975551\n",
      "120 Train Loss 30.651417 Test MSE 34.13275188446241 Test RE 0.09835010378207022 Lambda1 0.9798121\n",
      "121 Train Loss 30.59456 Test MSE 34.057868918341505 Test RE 0.09824216066509088 Lambda1 0.98370326\n",
      "122 Train Loss 30.502089 Test MSE 33.94116368087871 Test RE 0.09807369425856552 Lambda1 0.98215985\n",
      "123 Train Loss 30.43537 Test MSE 33.89347791797424 Test RE 0.0980047755442004 Lambda1 0.9831947\n",
      "124 Train Loss 30.395859 Test MSE 33.885452254028024 Test RE 0.09799317154234476 Lambda1 0.98206186\n",
      "125 Train Loss 30.355936 Test MSE 33.865742524601394 Test RE 0.09796466816248596 Lambda1 0.9819696\n",
      "126 Train Loss 30.34299 Test MSE 33.825166251840876 Test RE 0.09790596234078343 Lambda1 0.98173565\n",
      "127 Train Loss 30.332335 Test MSE 33.804963491064996 Test RE 0.09787671982125071 Lambda1 0.9807995\n",
      "128 Train Loss 30.324781 Test MSE 33.79032099415457 Test RE 0.09785552005285651 Lambda1 0.98073995\n",
      "129 Train Loss 30.285524 Test MSE 33.727354145417856 Test RE 0.09776430270538176 Lambda1 0.9873635\n",
      "130 Train Loss 30.254955 Test MSE 33.616184194718834 Test RE 0.09760304749054277 Lambda1 0.9939661\n",
      "131 Train Loss 30.21386 Test MSE 33.520012476412035 Test RE 0.09746333240767546 Lambda1 0.9967023\n",
      "132 Train Loss 30.178707 Test MSE 33.458549496048036 Test RE 0.09737393602939774 Lambda1 0.9992524\n",
      "133 Train Loss 30.15984 Test MSE 33.435100207078676 Test RE 0.09733980798428277 Lambda1 0.9978906\n",
      "134 Train Loss 30.142094 Test MSE 33.439876336372144 Test RE 0.09734676012296173 Lambda1 0.99579126\n",
      "135 Train Loss 30.116894 Test MSE 33.35535511896218 Test RE 0.09722365750923527 Lambda1 0.99690616\n",
      "136 Train Loss 30.068058 Test MSE 33.223931338509054 Test RE 0.0970319324980473 Lambda1 1.0025021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 Train Loss 30.049355 Test MSE 33.19364437640278 Test RE 0.09698769522082944 Lambda1 1.0061638\n",
      "138 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "139 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "140 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "141 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "142 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "143 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "144 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "145 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "146 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "147 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "148 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "149 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "150 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "151 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "152 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "153 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "154 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "155 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "156 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "157 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "158 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "159 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "160 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "161 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "162 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "163 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "164 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "165 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "166 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "167 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "168 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "169 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "170 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "171 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "172 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "173 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "174 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "175 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "176 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "177 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "178 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "179 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "180 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "181 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "182 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "183 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "184 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "185 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "186 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "187 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "188 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "189 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "190 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "191 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "192 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "193 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "194 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "195 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "196 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "197 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "198 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "199 Train Loss 30.045923 Test MSE 33.17971498842005 Test RE 0.09696734311694462 Lambda1 1.0068104\n",
      "Training time: 470.41\n",
      "Training time: 470.41\n",
      "inv_HT_rowdy\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 843.5059 Test MSE 842.5022330859388 Test RE 0.488623894614143 Lambda1 -0.023940355\n",
      "1 Train Loss 770.1182 Test MSE 766.9605822386704 Test RE 0.46620367499733506 Lambda1 -0.02505302\n",
      "2 Train Loss 639.33734 Test MSE 626.3676104198087 Test RE 0.4213119880181315 Lambda1 -0.005003994\n",
      "3 Train Loss 490.99316 Test MSE 469.11672952087196 Test RE 0.3646108860614068 Lambda1 0.008138508\n",
      "4 Train Loss 382.7163 Test MSE 378.10177942660954 Test RE 0.32733581923021393 Lambda1 0.0023139878\n",
      "5 Train Loss 320.33838 Test MSE 317.99824340153236 Test RE 0.30019367056698265 Lambda1 0.0035151015\n",
      "6 Train Loss 283.46576 Test MSE 287.32048344809164 Test RE 0.28534644410852816 Lambda1 0.006523514\n",
      "7 Train Loss 272.1749 Test MSE 278.55346333917 Test RE 0.2809593256866663 Lambda1 0.011617589\n",
      "8 Train Loss 268.45065 Test MSE 274.02128245677113 Test RE 0.2786642894709767 Lambda1 0.016722945\n",
      "9 Train Loss 264.44846 Test MSE 271.34193105752627 Test RE 0.27729856768110994 Lambda1 0.02444693\n",
      "10 Train Loss 260.43323 Test MSE 265.59550430671385 Test RE 0.2743465671991009 Lambda1 0.033822123\n",
      "11 Train Loss 254.95593 Test MSE 260.7728557756369 Test RE 0.2718443819805001 Lambda1 0.045041062\n",
      "12 Train Loss 247.04828 Test MSE 249.57368685073507 Test RE 0.2659430027169849 Lambda1 0.07785806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Train Loss 238.95068 Test MSE 239.0889016553787 Test RE 0.26029682998109743 Lambda1 0.11098295\n",
      "14 Train Loss 229.45401 Test MSE 231.75484317286717 Test RE 0.2562734290556248 Lambda1 0.14089748\n",
      "15 Train Loss 222.28923 Test MSE 221.35633690109452 Test RE 0.25045813957673524 Lambda1 0.18779756\n",
      "16 Train Loss 217.49713 Test MSE 215.69208825298432 Test RE 0.24723290860504618 Lambda1 0.2115823\n",
      "17 Train Loss 210.02245 Test MSE 207.40498633742286 Test RE 0.2424369252068467 Lambda1 0.23143113\n",
      "18 Train Loss 198.72029 Test MSE 195.3855626259385 Test RE 0.2353073026645768 Lambda1 0.2741727\n",
      "19 Train Loss 182.80391 Test MSE 175.8223784709418 Test RE 0.22321647524194194 Lambda1 0.28241214\n",
      "20 Train Loss 161.63368 Test MSE 156.357220055084 Test RE 0.21049808091883737 Lambda1 0.2968876\n",
      "21 Train Loss 146.10225 Test MSE 135.4825392502006 Test RE 0.1959434879136258 Lambda1 0.30554304\n",
      "22 Train Loss 128.78685 Test MSE 112.88698993034454 Test RE 0.17885913130746164 Lambda1 0.32737055\n",
      "23 Train Loss 113.56748 Test MSE 104.5526099252679 Test RE 0.17213001629978208 Lambda1 0.3436035\n",
      "24 Train Loss 104.607635 Test MSE 96.13981361970691 Test RE 0.16505960769216482 Lambda1 0.36511797\n",
      "25 Train Loss 94.53862 Test MSE 86.2132389105693 Test RE 0.15630618066551966 Lambda1 0.3855531\n",
      "26 Train Loss 88.925224 Test MSE 83.43736326660236 Test RE 0.15376923590042832 Lambda1 0.39953774\n",
      "27 Train Loss 82.699814 Test MSE 78.79762910228862 Test RE 0.1494327352176006 Lambda1 0.41499555\n",
      "28 Train Loss 76.96344 Test MSE 73.67907066760745 Test RE 0.14449780178513724 Lambda1 0.42898342\n",
      "29 Train Loss 71.818596 Test MSE 67.88174282109283 Test RE 0.13869655136456505 Lambda1 0.44268814\n",
      "30 Train Loss 66.04787 Test MSE 63.15464076123901 Test RE 0.1337801890043333 Lambda1 0.4498044\n",
      "31 Train Loss 62.388668 Test MSE 60.40141417006049 Test RE 0.13083162151762295 Lambda1 0.46017516\n",
      "32 Train Loss 60.788036 Test MSE 58.80902337646496 Test RE 0.12909551496219912 Lambda1 0.46472156\n",
      "33 Train Loss 57.23885 Test MSE 54.695063847758526 Test RE 0.12449824730626081 Lambda1 0.48243\n",
      "34 Train Loss 54.416035 Test MSE 53.29010134469038 Test RE 0.12288883970561039 Lambda1 0.49058902\n",
      "35 Train Loss 51.21951 Test MSE 50.43638337748051 Test RE 0.11955318187848969 Lambda1 0.50954133\n",
      "36 Train Loss 49.49642 Test MSE 49.23926916787275 Test RE 0.11812585630950248 Lambda1 0.5209338\n",
      "37 Train Loss 48.355633 Test MSE 48.67318298486878 Test RE 0.11744486812216162 Lambda1 0.5273889\n",
      "38 Train Loss 47.10151 Test MSE 47.75040644653145 Test RE 0.11632624435693925 Lambda1 0.53898835\n",
      "39 Train Loss 43.87293 Test MSE 45.51629929240012 Test RE 0.11357235803207952 Lambda1 0.55659354\n",
      "40 Train Loss 42.339855 Test MSE 43.53690254952266 Test RE 0.11107541243701022 Lambda1 0.5767943\n",
      "41 Train Loss 41.27957 Test MSE 42.009018869714325 Test RE 0.1091089659621228 Lambda1 0.59774745\n",
      "42 Train Loss 40.23899 Test MSE 41.05984538200117 Test RE 0.10786929130315961 Lambda1 0.61024165\n",
      "43 Train Loss 39.132225 Test MSE 40.59522102784503 Test RE 0.10725724210583744 Lambda1 0.6173487\n",
      "44 Train Loss 38.425957 Test MSE 40.34276328542548 Test RE 0.10692321077567117 Lambda1 0.61925524\n",
      "45 Train Loss 37.659687 Test MSE 39.35500366989753 Test RE 0.10560613518323893 Lambda1 0.63491106\n",
      "46 Train Loss 37.041912 Test MSE 38.8967580677928 Test RE 0.10498950145164976 Lambda1 0.64355135\n",
      "47 Train Loss 36.121925 Test MSE 38.18801936772851 Test RE 0.10402859613053868 Lambda1 0.660526\n",
      "48 Train Loss 35.62644 Test MSE 37.70824074808088 Test RE 0.10337304428432009 Lambda1 0.66937447\n",
      "49 Train Loss 35.118477 Test MSE 37.54015253602707 Test RE 0.10314238917157668 Lambda1 0.673181\n",
      "50 Train Loss 34.738167 Test MSE 37.263403737884396 Test RE 0.10276149920141915 Lambda1 0.67948794\n",
      "51 Train Loss 34.261276 Test MSE 36.81570890675568 Test RE 0.10214232856797867 Lambda1 0.6900078\n",
      "52 Train Loss 33.710873 Test MSE 36.48195502850671 Test RE 0.10167828735583946 Lambda1 0.7017275\n",
      "53 Train Loss 33.1349 Test MSE 36.12751772571965 Test RE 0.10118315848831687 Lambda1 0.71042216\n",
      "54 Train Loss 32.78844 Test MSE 35.86909833762493 Test RE 0.1008206285110992 Lambda1 0.72413737\n",
      "55 Train Loss 32.48493 Test MSE 35.59486875901873 Test RE 0.10043448769691062 Lambda1 0.73440677\n",
      "56 Train Loss 32.342846 Test MSE 35.51865502886927 Test RE 0.1003269077430914 Lambda1 0.7360674\n",
      "57 Train Loss 32.03437 Test MSE 35.180080538306164 Test RE 0.0998475896440884 Lambda1 0.7510708\n",
      "58 Train Loss 31.785633 Test MSE 34.93921083240354 Test RE 0.09950518611158736 Lambda1 0.7583548\n",
      "59 Train Loss 31.609373 Test MSE 34.83141522671461 Test RE 0.09935156919293428 Lambda1 0.76233405\n",
      "60 Train Loss 31.468596 Test MSE 34.68507456415573 Test RE 0.09914264173837768 Lambda1 0.7682974\n",
      "61 Train Loss 31.232529 Test MSE 34.359805435059684 Test RE 0.09867667760944931 Lambda1 0.7802178\n",
      "62 Train Loss 30.965769 Test MSE 33.985996803968426 Test RE 0.09813844600095534 Lambda1 0.79346675\n",
      "63 Train Loss 30.510893 Test MSE 33.67668489065786 Test RE 0.097690838504387 Lambda1 0.80472845\n",
      "64 Train Loss 30.454464 Test MSE 33.64401702542488 Test RE 0.09764344480351554 Lambda1 0.80439097\n",
      "65 Train Loss 30.32161 Test MSE 33.46810513732041 Test RE 0.09738783985952075 Lambda1 0.80861765\n",
      "66 Train Loss 30.205446 Test MSE 33.42801568007431 Test RE 0.09732949482494038 Lambda1 0.8118893\n",
      "67 Train Loss 30.083296 Test MSE 33.37709147837568 Test RE 0.09725533074702872 Lambda1 0.8140396\n",
      "68 Train Loss 29.937422 Test MSE 33.21859786324854 Test RE 0.09702414386254136 Lambda1 0.8181482\n",
      "69 Train Loss 29.762867 Test MSE 33.09872829427612 Test RE 0.09684892946486318 Lambda1 0.82528484\n",
      "70 Train Loss 29.675488 Test MSE 32.95939353676703 Test RE 0.09664486340947574 Lambda1 0.8292722\n",
      "71 Train Loss 29.581696 Test MSE 32.8230266004567 Test RE 0.09644472586546135 Lambda1 0.83120894\n",
      "72 Train Loss 29.530823 Test MSE 32.81437431234454 Test RE 0.09643201340958724 Lambda1 0.83182216\n",
      "73 Train Loss 29.418484 Test MSE 32.72089826062961 Test RE 0.09629456583255215 Lambda1 0.833585\n",
      "74 Train Loss 29.282698 Test MSE 32.62084184771072 Test RE 0.09614722474157424 Lambda1 0.83584005\n",
      "75 Train Loss 29.227678 Test MSE 32.55189089535532 Test RE 0.09604555738381702 Lambda1 0.83724517\n",
      "76 Train Loss 29.126331 Test MSE 32.57115445720666 Test RE 0.09607397210628663 Lambda1 0.8389668\n",
      "77 Train Loss 29.026754 Test MSE 32.43952552979365 Test RE 0.09587964500008529 Lambda1 0.84196687\n",
      "78 Train Loss 28.97101 Test MSE 32.362996264216825 Test RE 0.09576648163830558 Lambda1 0.8448144\n",
      "79 Train Loss 28.931429 Test MSE 32.3159198326548 Test RE 0.09569680352696427 Lambda1 0.8476298\n",
      "80 Train Loss 28.892838 Test MSE 32.245887814454726 Test RE 0.0955930547375146 Lambda1 0.84986526\n",
      "81 Train Loss 28.83295 Test MSE 32.187812057178604 Test RE 0.09550693304357194 Lambda1 0.85382384\n",
      "82 Train Loss 28.789034 Test MSE 32.15163170186998 Test RE 0.09545324119378092 Lambda1 0.8563331\n",
      "83 Train Loss 28.736609 Test MSE 32.086813172583014 Test RE 0.0953569745365724 Lambda1 0.85984313\n",
      "84 Train Loss 28.685558 Test MSE 31.968892128973526 Test RE 0.09518159183237314 Lambda1 0.864882\n",
      "85 Train Loss 28.611856 Test MSE 31.766140127957787 Test RE 0.0948792830484483 Lambda1 0.8701979\n",
      "86 Train Loss 28.55435 Test MSE 31.728661022802104 Test RE 0.09482329513001797 Lambda1 0.8727975\n",
      "87 Train Loss 28.462692 Test MSE 31.677344322447794 Test RE 0.09474658234753955 Lambda1 0.87809753\n",
      "88 Train Loss 28.43863 Test MSE 31.65870907533291 Test RE 0.0947187093398505 Lambda1 0.8792907\n",
      "89 Train Loss 28.42333 Test MSE 31.683946709649877 Test RE 0.09475645566621686 Lambda1 0.8783866\n",
      "90 Train Loss 28.320889 Test MSE 31.583937038647402 Test RE 0.09460678914884484 Lambda1 0.8809989\n",
      "91 Train Loss 28.218637 Test MSE 31.40262142705219 Test RE 0.09433484114075398 Lambda1 0.8883339\n",
      "92 Train Loss 28.173588 Test MSE 31.437684046499957 Test RE 0.09438749126544388 Lambda1 0.8886829\n",
      "93 Train Loss 28.065895 Test MSE 31.22457233158369 Test RE 0.0940670273681554 Lambda1 0.89355457\n",
      "94 Train Loss 27.899382 Test MSE 30.964043511386645 Test RE 0.09367377128164246 Lambda1 0.90314585\n",
      "95 Train Loss 27.740831 Test MSE 30.740255453511804 Test RE 0.09333465075117897 Lambda1 0.9108024\n",
      "96 Train Loss 27.594294 Test MSE 30.55317340647251 Test RE 0.09305020474966669 Lambda1 0.9140893\n",
      "97 Train Loss 27.546656 Test MSE 30.50543358147213 Test RE 0.0929774801092289 Lambda1 0.91479576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 Train Loss 27.504045 Test MSE 30.471902142582515 Test RE 0.09292636583991054 Lambda1 0.91452116\n",
      "99 Train Loss 27.467125 Test MSE 30.45844491980251 Test RE 0.09290584416556587 Lambda1 0.9145075\n",
      "100 Train Loss 27.443773 Test MSE 30.460130906799076 Test RE 0.09290841547014196 Lambda1 0.9155562\n",
      "101 Train Loss 27.387518 Test MSE 30.337072839174827 Test RE 0.09272055185140092 Lambda1 0.9204262\n",
      "102 Train Loss 27.26428 Test MSE 30.118459791334555 Test RE 0.09238586940421431 Lambda1 0.92668045\n",
      "103 Train Loss 27.144928 Test MSE 29.998899387730223 Test RE 0.0922023162666231 Lambda1 0.93466353\n",
      "104 Train Loss 27.105528 Test MSE 29.92403927088804 Test RE 0.09208720225035014 Lambda1 0.93911535\n",
      "105 Train Loss 27.083447 Test MSE 29.86615087595542 Test RE 0.09199808725998458 Lambda1 0.9422255\n",
      "106 Train Loss 27.055025 Test MSE 29.84190401229373 Test RE 0.09196073530904234 Lambda1 0.9427863\n",
      "107 Train Loss 26.982363 Test MSE 29.69833485104918 Test RE 0.09173925742519855 Lambda1 0.94917995\n",
      "108 Train Loss 26.92695 Test MSE 29.680356792412763 Test RE 0.09171148577679675 Lambda1 0.9521921\n",
      "109 Train Loss 26.86792 Test MSE 29.615856431531462 Test RE 0.09161177940756002 Lambda1 0.95601255\n",
      "110 Train Loss 26.843834 Test MSE 29.562302195875127 Test RE 0.09152891131952684 Lambda1 0.9598812\n",
      "111 Train Loss 26.826912 Test MSE 29.528296015152456 Test RE 0.0914762522894057 Lambda1 0.96295214\n",
      "112 Train Loss 26.795094 Test MSE 29.469068687970413 Test RE 0.091384465526166 Lambda1 0.96764404\n",
      "113 Train Loss 26.742802 Test MSE 29.38008075509165 Test RE 0.09124638408207895 Lambda1 0.96864194\n",
      "114 Train Loss 26.710758 Test MSE 29.386777011050103 Test RE 0.09125678184676049 Lambda1 0.9680811\n",
      "115 Train Loss 26.676558 Test MSE 29.327148868512356 Test RE 0.09116415114598458 Lambda1 0.9721559\n",
      "116 Train Loss 26.63976 Test MSE 29.333100111145225 Test RE 0.09117340046756434 Lambda1 0.9716852\n",
      "117 Train Loss 26.611519 Test MSE 29.331544773580134 Test RE 0.0911709832785536 Lambda1 0.9706941\n",
      "118 Train Loss 26.567677 Test MSE 29.261322440798768 Test RE 0.09106178214963517 Lambda1 0.97377473\n",
      "119 Train Loss 26.522997 Test MSE 29.279446428804473 Test RE 0.09108997887881173 Lambda1 0.97349316\n",
      "120 Train Loss 26.471495 Test MSE 29.262291898136027 Test RE 0.09106329062181436 Lambda1 0.9748013\n",
      "121 Train Loss 26.422146 Test MSE 29.264242612058354 Test RE 0.09106632585003684 Lambda1 0.97732955\n",
      "122 Train Loss 26.380016 Test MSE 29.20308791909482 Test RE 0.09097112356125167 Lambda1 0.97850496\n",
      "123 Train Loss 26.333017 Test MSE 29.22802415198885 Test RE 0.09100995495262781 Lambda1 0.9788745\n",
      "124 Train Loss 26.251074 Test MSE 29.179336778258875 Test RE 0.09093412220928446 Lambda1 0.9763385\n",
      "125 Train Loss 26.22202 Test MSE 29.200240245079524 Test RE 0.0909666880299638 Lambda1 0.9758563\n",
      "126 Train Loss 26.195848 Test MSE 29.1815366953067 Test RE 0.0909375500417399 Lambda1 0.97594994\n",
      "127 Train Loss 26.1578 Test MSE 29.200174365618672 Test RE 0.0909665854136892 Lambda1 0.9745017\n",
      "128 Train Loss 26.133902 Test MSE 29.172505209152167 Test RE 0.09092347667765836 Lambda1 0.97582424\n",
      "129 Train Loss 26.125425 Test MSE 29.175951771890425 Test RE 0.09092884756043369 Lambda1 0.97595644\n",
      "130 Train Loss 26.121286 Test MSE 29.176582997322676 Test RE 0.09092983118371206 Lambda1 0.97638124\n",
      "131 Train Loss 26.098251 Test MSE 29.153742395371665 Test RE 0.09089423245188731 Lambda1 0.9774374\n",
      "132 Train Loss 26.083893 Test MSE 29.119982544056665 Test RE 0.09084158973120411 Lambda1 0.97860974\n",
      "133 Train Loss 26.067522 Test MSE 29.12309723529694 Test RE 0.09084644783745151 Lambda1 0.98026615\n",
      "134 Train Loss 26.051146 Test MSE 29.10358461750215 Test RE 0.09081600895553527 Lambda1 0.9822817\n",
      "135 Train Loss 26.043709 Test MSE 29.09388350081062 Test RE 0.09080087181449883 Lambda1 0.98301786\n",
      "136 Train Loss 26.030735 Test MSE 29.084655124175693 Test RE 0.09078646997120504 Lambda1 0.9832004\n",
      "137 Train Loss 26.020338 Test MSE 29.101850931809086 Test RE 0.0908133039835822 Lambda1 0.9826373\n",
      "138 Train Loss 26.000578 Test MSE 29.102470680992557 Test RE 0.0908142709525129 Lambda1 0.98293185\n",
      "139 Train Loss 25.983862 Test MSE 29.103237928892742 Test RE 0.09081546804330684 Lambda1 0.98277456\n",
      "140 Train Loss 25.969858 Test MSE 29.07640103226725 Test RE 0.09077358666490906 Lambda1 0.9832409\n",
      "141 Train Loss 25.95854 Test MSE 29.11323746521456 Test RE 0.09083106827621996 Lambda1 0.98199695\n",
      "142 Train Loss 25.945501 Test MSE 29.117043069173874 Test RE 0.09083700467848445 Lambda1 0.98231363\n",
      "143 Train Loss 25.908304 Test MSE 29.094418491159075 Test RE 0.09080170665263133 Lambda1 0.9836708\n",
      "144 Train Loss 25.892197 Test MSE 29.075344024824503 Test RE 0.0907719367146863 Lambda1 0.98332304\n",
      "145 Train Loss 25.887125 Test MSE 29.06471368256323 Test RE 0.09075534146874856 Lambda1 0.98345816\n",
      "146 Train Loss 25.882187 Test MSE 29.06281023330642 Test RE 0.09075236963462745 Lambda1 0.9834704\n",
      "147 Train Loss 25.874245 Test MSE 29.05136985861385 Test RE 0.09073450585444323 Lambda1 0.98347837\n",
      "148 Train Loss 25.859049 Test MSE 29.007729273475306 Test RE 0.09066633015188795 Lambda1 0.98425865\n",
      "149 Train Loss 25.83565 Test MSE 29.026026888123702 Test RE 0.09069492108403869 Lambda1 0.98326576\n",
      "150 Train Loss 25.823093 Test MSE 28.98348981154974 Test RE 0.09062844089932977 Lambda1 0.9831978\n",
      "151 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "152 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "153 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "154 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "155 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "156 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "157 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "158 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "159 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "160 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "161 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "162 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "163 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "164 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "165 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "166 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "167 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "168 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "169 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "170 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "171 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "172 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "173 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "174 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "175 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "176 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "177 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "178 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "179 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "180 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "182 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "183 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "184 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "185 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "186 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "187 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "188 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "189 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "190 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "191 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "192 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "193 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "194 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "195 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "196 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "197 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "198 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "199 Train Loss 25.820301 Test MSE 28.990962000572633 Test RE 0.09064012253609548 Lambda1 0.98328465\n",
      "Training time: 499.93\n",
      "Training time: 499.93\n",
      "inv_HT_rowdy\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 854.7177 Test MSE 858.076268367137 Test RE 0.4931194312161083 Lambda1 0.0012601403\n",
      "1 Train Loss 854.7175 Test MSE 858.0729359864196 Test RE 0.49311847368858996 Lambda1 0.0012600097\n",
      "2 Train Loss 854.5919 Test MSE 857.8721417861749 Test RE 0.4930607739764677 Lambda1 0.0012154557\n",
      "3 Train Loss 854.0837 Test MSE 856.7348626014741 Test RE 0.49273384080279586 Lambda1 0.006486348\n",
      "4 Train Loss 684.945 Test MSE 646.1568881708978 Test RE 0.4279156398965708 Lambda1 -0.016106475\n",
      "5 Train Loss 345.06253 Test MSE 333.46160658805246 Test RE 0.3074058229521113 Lambda1 0.0029817584\n",
      "6 Train Loss 291.87555 Test MSE 296.1278256956749 Test RE 0.2896868488018229 Lambda1 0.005679131\n",
      "7 Train Loss 274.25922 Test MSE 281.8519417557027 Test RE 0.2826179137765929 Lambda1 0.0060846354\n",
      "8 Train Loss 268.9849 Test MSE 275.3251515481689 Test RE 0.27932648358191275 Lambda1 0.009938512\n",
      "9 Train Loss 265.32898 Test MSE 271.497958720464 Test RE 0.27737828265421777 Lambda1 0.015715731\n",
      "10 Train Loss 260.14655 Test MSE 267.51532889647206 Test RE 0.2753363221273247 Lambda1 0.026016159\n",
      "11 Train Loss 251.09744 Test MSE 254.1125954121571 Test RE 0.26835041212544597 Lambda1 0.05332791\n",
      "12 Train Loss 240.18983 Test MSE 237.43892632109095 Test RE 0.25939710839147745 Lambda1 0.09911779\n",
      "13 Train Loss 224.53802 Test MSE 223.58791550859473 Test RE 0.25171745617423835 Lambda1 0.14969084\n",
      "14 Train Loss 204.87045 Test MSE 200.21051022676107 Test RE 0.2381949813303243 Lambda1 0.21686606\n",
      "15 Train Loss 178.76448 Test MSE 171.54005221956484 Test RE 0.22048139071546233 Lambda1 0.26060903\n",
      "16 Train Loss 152.53008 Test MSE 138.99293847821977 Test RE 0.19846573592705383 Lambda1 0.28157246\n",
      "17 Train Loss 110.04198 Test MSE 97.94519031859413 Test RE 0.16660219840652554 Lambda1 0.35749608\n",
      "18 Train Loss 86.145546 Test MSE 83.06366720960425 Test RE 0.15342450159854876 Lambda1 0.39003542\n",
      "19 Train Loss 74.03996 Test MSE 70.73474314075094 Test RE 0.14158119089800617 Lambda1 0.42016056\n",
      "20 Train Loss 63.64175 Test MSE 62.44015889966851 Test RE 0.13302129467299714 Lambda1 0.451474\n",
      "21 Train Loss 56.759182 Test MSE 56.329634880292566 Test RE 0.12634487752071397 Lambda1 0.48533005\n",
      "22 Train Loss 51.401962 Test MSE 51.83384132167227 Test RE 0.1211981157804591 Lambda1 0.5184998\n",
      "23 Train Loss 47.601604 Test MSE 48.84953246985073 Test RE 0.11765743502915546 Lambda1 0.5568409\n",
      "24 Train Loss 44.8734 Test MSE 46.863371483947994 Test RE 0.11524071270680052 Lambda1 0.5777768\n",
      "25 Train Loss 43.24942 Test MSE 45.40896278391254 Test RE 0.1134383658585938 Lambda1 0.5896105\n",
      "26 Train Loss 42.208324 Test MSE 44.558260480528354 Test RE 0.11237075132535952 Lambda1 0.59693533\n",
      "27 Train Loss 39.94694 Test MSE 42.145493957414686 Test RE 0.10928605390479704 Lambda1 0.63289666\n",
      "28 Train Loss 38.576485 Test MSE 40.0515044100875 Test RE 0.10653653985566351 Lambda1 0.66760355\n",
      "29 Train Loss 37.724457 Test MSE 39.15156422937084 Test RE 0.10533282444552962 Lambda1 0.68676424\n",
      "30 Train Loss 36.923683 Test MSE 38.72908104730442 Test RE 0.10476296149116111 Lambda1 0.69553304\n",
      "31 Train Loss 36.3781 Test MSE 38.913513019680686 Test RE 0.10501211136490884 Lambda1 0.6936754\n",
      "32 Train Loss 35.6799 Test MSE 38.79123885561328 Test RE 0.10484699686228337 Lambda1 0.70229185\n",
      "33 Train Loss 35.38872 Test MSE 38.82314791561821 Test RE 0.10489011073918766 Lambda1 0.70334905\n",
      "34 Train Loss 34.867306 Test MSE 38.06346648713915 Test RE 0.10385880932462246 Lambda1 0.71683776\n",
      "35 Train Loss 34.54647 Test MSE 37.801348136847814 Test RE 0.10350058749615348 Lambda1 0.7193248\n",
      "36 Train Loss 34.238995 Test MSE 37.257623149913 Test RE 0.10275352831221253 Lambda1 0.7362418\n",
      "37 Train Loss 33.931805 Test MSE 37.40878274519541 Test RE 0.1029617603227267 Lambda1 0.7369597\n",
      "38 Train Loss 33.643578 Test MSE 37.10472150924486 Test RE 0.10254246633100725 Lambda1 0.75060976\n",
      "39 Train Loss 33.3372 Test MSE 36.6878990971228 Test RE 0.10196487522053177 Lambda1 0.7629762\n",
      "40 Train Loss 33.13507 Test MSE 36.35121231305613 Test RE 0.10149592833564225 Lambda1 0.7775981\n",
      "41 Train Loss 32.936295 Test MSE 35.9560151210317 Test RE 0.10094270716661506 Lambda1 0.7873067\n",
      "42 Train Loss 32.830814 Test MSE 35.86420501794524 Test RE 0.10081375122096034 Lambda1 0.79235643\n",
      "43 Train Loss 32.516697 Test MSE 35.776690691481285 Test RE 0.10069067535364142 Lambda1 0.8001675\n",
      "44 Train Loss 32.158165 Test MSE 35.76676845895239 Test RE 0.1006767117148157 Lambda1 0.8082727\n",
      "45 Train Loss 31.833694 Test MSE 35.474432700832814 Test RE 0.10026443252758181 Lambda1 0.8225832\n",
      "46 Train Loss 31.457806 Test MSE 35.207951638363326 Test RE 0.09988713348768898 Lambda1 0.83697337\n",
      "47 Train Loss 31.36624 Test MSE 34.97561399191061 Test RE 0.09955700983695816 Lambda1 0.8479271\n",
      "48 Train Loss 31.33217 Test MSE 34.83208624293528 Test RE 0.09935252617663665 Lambda1 0.85539156\n",
      "49 Train Loss 31.233625 Test MSE 34.62727851337687 Test RE 0.09906000616343869 Lambda1 0.86899626\n",
      "50 Train Loss 31.102423 Test MSE 34.42580498858107 Test RE 0.09877140300392633 Lambda1 0.8821283\n",
      "51 Train Loss 30.905527 Test MSE 34.282989526643 Test RE 0.09856631348243806 Lambda1 0.8992789\n",
      "52 Train Loss 30.822678 Test MSE 34.22306971044188 Test RE 0.09848013870032708 Lambda1 0.90148497\n",
      "53 Train Loss 30.727306 Test MSE 34.30297862465974 Test RE 0.09859504441602451 Lambda1 0.8967502\n",
      "54 Train Loss 30.70288 Test MSE 34.31551357686472 Test RE 0.09861305701049525 Lambda1 0.89755285\n",
      "55 Train Loss 30.661415 Test MSE 34.223124692450384 Test RE 0.09848021780827527 Lambda1 0.90306604\n",
      "56 Train Loss 30.58894 Test MSE 34.1177060018896 Test RE 0.09832842479138575 Lambda1 0.9102627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 Train Loss 30.567392 Test MSE 34.03600777901071 Test RE 0.09821062565744641 Lambda1 0.91561806\n",
      "58 Train Loss 30.534641 Test MSE 34.00462839956647 Test RE 0.09816534274491079 Lambda1 0.9168202\n",
      "59 Train Loss 30.51318 Test MSE 34.007343114732954 Test RE 0.09816926111777334 Lambda1 0.91639054\n",
      "60 Train Loss 30.436317 Test MSE 33.89356197161853 Test RE 0.09800489706689358 Lambda1 0.92465854\n",
      "61 Train Loss 30.376461 Test MSE 33.87687706539667 Test RE 0.09798077148502755 Lambda1 0.91962487\n",
      "62 Train Loss 30.3529 Test MSE 33.89285033416371 Test RE 0.09800386819418817 Lambda1 0.9169118\n",
      "63 Train Loss 30.233076 Test MSE 33.75644269696972 Test RE 0.097806452603048 Lambda1 0.9314587\n",
      "64 Train Loss 30.158358 Test MSE 33.59927350897798 Test RE 0.09757849470248305 Lambda1 0.9423337\n",
      "65 Train Loss 30.102716 Test MSE 33.602156966198244 Test RE 0.09758268165876785 Lambda1 0.94585514\n",
      "66 Train Loss 30.065825 Test MSE 33.52952361045885 Test RE 0.09747715879082793 Lambda1 0.9527236\n",
      "67 Train Loss 30.027185 Test MSE 33.483653987615156 Test RE 0.09741045980020276 Lambda1 0.95967686\n",
      "68 Train Loss 30.017391 Test MSE 33.478907494376344 Test RE 0.09740355531965203 Lambda1 0.95952666\n",
      "69 Train Loss 30.011263 Test MSE 33.487926950883306 Test RE 0.09741667504239951 Lambda1 0.95979434\n",
      "70 Train Loss 29.98829 Test MSE 33.482924453820516 Test RE 0.09740939861659524 Lambda1 0.9544997\n",
      "71 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "72 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "73 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "74 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "75 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "76 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "77 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "78 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "79 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "80 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "81 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "82 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "83 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "84 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "85 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "86 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "87 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "88 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "89 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "90 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "91 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "92 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "93 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "94 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "95 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "96 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "97 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "98 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "99 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "100 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "101 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "102 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "103 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "104 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "105 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "106 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "107 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "108 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "109 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "110 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "111 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "112 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "113 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "114 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "115 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "116 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "117 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "118 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "119 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "120 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "121 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "122 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "123 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "124 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "125 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "126 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "127 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "128 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "129 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "130 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "131 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "132 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "133 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "134 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "135 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "136 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "137 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "138 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "139 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "140 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "142 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "143 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "144 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "145 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "146 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "147 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "148 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "149 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "150 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "151 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "152 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "153 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "154 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "155 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "156 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "157 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "158 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "159 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "160 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "161 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "162 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "163 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "164 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "165 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "166 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "167 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "168 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "169 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "170 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "171 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "172 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "173 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "174 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "175 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "176 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "177 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "178 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "179 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "180 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "181 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "182 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "183 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "184 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "185 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "186 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "187 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "188 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "189 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "190 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "191 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "192 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "193 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "194 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "195 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "196 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "197 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "198 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "199 Train Loss 29.987356 Test MSE 33.48518964420066 Test RE 0.09741269353638696 Lambda1 0.9541989\n",
      "Training time: 308.90\n",
      "Training time: 308.90\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10#10\n",
    "max_iter = 200 #75\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "lambda1_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val =1.0\n",
    "rowdy_terms = 2\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    'Generate Training data'\n",
    "    print(reps)\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    alpha_val = []\n",
    "    omega_val = []\n",
    "    lambda1_val = []\n",
    "\n",
    "    N_f = 50000 #Total number of collocation points \n",
    "    N_train = 5000\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)\n",
    "    lambda1_full.append(lambda1_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"alpha\": alpha_full,\"omega\": omega_full,\"lambda1\": lambda1_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'inv_HT_rowdy_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'inv_HT_rowdy_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23466/73527267.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"inv_HT_rowdy_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'inv_HT_rowdy_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(25):\n",
    "    label = \"inv_HT_rowdy_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_tune[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
