{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_2D_4(xt): #True function for 2D_4 Heat Transfer in a rod x \\in [0,1] t \\in [0,0.1]\n",
    "    term1 = 4*u0/np.pi\n",
    "    \n",
    "    resol_n = 10000\n",
    "    \n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "\n",
    "    u = np.zeros((np.shape(xt)[0],1))\n",
    "    \n",
    "    for i in range(resol_n):\n",
    "        j = 2*i-1\n",
    "        term2 = np.sin(j*np.pi*x)/j\n",
    "        term3 = np.exp(-1*np.square(j*np.pi)*t)\n",
    "        \n",
    "        u = u + term2*term3\n",
    "        \n",
    "    u = term1*u\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = 50.0\n",
    "loss_thresh = 0.1\n",
    "\n",
    "x_ll = np.array(0.0)\n",
    "x_ul = np.array(1.0)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,100).reshape(-1,1)\n",
    "t = np.linspace(0,0.1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = true_2D_4(xt)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_f,N_train,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #X_Train\n",
    "    np.random.seed(seed)\n",
    "    x_train = np.random.uniform(x_ll,x_ul,(N_train,1))\n",
    "    t_train = np.random.uniform(0,0.1,(N_train,1))\n",
    "    \n",
    "    xt_train = np.hstack((x_train,t_train))\n",
    "    u_train = true_2D_4(xt_train)\n",
    "    \n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "\n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requiresGrad = True\n",
    "        \n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) \n",
    "            \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    \n",
    "    def loss_PDE(self, xt_coll,f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_tt[:,[0]]\n",
    "                \n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        f = du_dt - self.lambda1*d2u_dx2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_train = self.loss_function(self.forward(xt_train),u_train)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "        \n",
    "        #print(self.iter,\"train_loss\",loss_train.cpu().detach().numpy(),\"F Loss\",(loss_f).cpu().detach().numpy())\n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                    \n",
    "    \n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_coll,f_hat, xt_train, u_train,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xt_coll, xt_train, u_train = trainingdata(N_f,N_train,123)\n",
    "    \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_train = torch.from_numpy(xt_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    nan_flag = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_coll,f_hat, xt_train, u_train,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_coll,f_hat, xt_train, u_train).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1],\"Lambda1\",lambda1_val[-1])\n",
    "\n",
    "        if(np.isnan(loss_np)):\n",
    "            nan_flag =1\n",
    "            print(\"NAN BREAK!\")\n",
    "            break\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_HT_tanh_tune0\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 883.27856 Test MSE 897.0191177867621 Test RE 0.5041851153101342 Lambda1 -0.085184895\n",
      "1 Train Loss 838.0775 Test MSE 858.1838132046178 Test RE 0.49315033219162496 Lambda1 -0.09501643\n",
      "2 Train Loss 838.06415 Test MSE 858.2621806971904 Test RE 0.4931728483892925 Lambda1 -0.09515731\n",
      "3 Train Loss 838.06396 Test MSE 858.266050688174 Test RE 0.4931739602711672 Lambda1 -0.09516343\n",
      "4 Train Loss 838.06384 Test MSE 858.2688378733404 Test RE 0.4931747610520145 Lambda1 -0.09516769\n",
      "5 Train Loss 838.0638 Test MSE 858.2712011661802 Test RE 0.4931754400441483 Lambda1 -0.09517127\n",
      "6 Train Loss 838.06366 Test MSE 858.2732802205471 Test RE 0.49317603737161897 Lambda1 -0.09517426\n",
      "7 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "8 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "9 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "10 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "11 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "12 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "13 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "14 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "15 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "16 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "17 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "18 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "19 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "20 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "21 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "22 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "23 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "24 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "25 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "26 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "27 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "28 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "29 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "30 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "31 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "32 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "33 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "34 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "35 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "36 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "37 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "38 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "39 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "40 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "41 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "42 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "43 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "44 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "45 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "46 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "47 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "48 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "49 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "50 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "51 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "52 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "53 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "54 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "55 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "56 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "57 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "58 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "59 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "60 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "61 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "62 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "63 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "64 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "65 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "66 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "67 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "68 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "69 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "70 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "71 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "72 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "73 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "74 Train Loss 838.06366 Test MSE 858.2738772876293 Test RE 0.49317620891320907 Lambda1 -0.09517513\n",
      "Training time: 37.13\n",
      "Training time: 37.13\n",
      "inv_HT_tanh_tune0\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 844.02405 Test MSE 861.8906852105796 Test RE 0.49421425075595643 Lambda1 -0.057177227\n",
      "1 Train Loss 838.0745 Test MSE 858.1867902808484 Test RE 0.49315118757064524 Lambda1 -0.059186283\n",
      "2 Train Loss 838.0629 Test MSE 858.2552610825053 Test RE 0.4931708603179086 Lambda1 -0.059258513\n",
      "3 Train Loss 838.0623 Test MSE 858.266005279913 Test RE 0.49317394722499575 Lambda1 -0.05926856\n",
      "4 Train Loss 838.0622 Test MSE 858.2680187675059 Test RE 0.4931745257164165 Lambda1 -0.0592704\n",
      "5 Train Loss 838.0622 Test MSE 858.2705088540131 Test RE 0.493175241137657 Lambda1 -0.059272625\n",
      "6 Train Loss 838.062 Test MSE 858.2728311808875 Test RE 0.49317590835931374 Lambda1 -0.05927466\n",
      "7 Train Loss 838.0619 Test MSE 858.2750785756826 Test RE 0.49317655405157135 Lambda1 -0.059276585\n",
      "8 Train Loss 838.06177 Test MSE 858.2773200512892 Test RE 0.49317719804236254 Lambda1 -0.05927847\n",
      "9 Train Loss 838.06177 Test MSE 858.2805279711423 Test RE 0.49317811969752545 Lambda1 -0.05928109\n",
      "10 Train Loss 838.06165 Test MSE 858.283192909478 Test RE 0.49317888534943904 Lambda1 -0.05928321\n",
      "11 Train Loss 838.0615 Test MSE 858.2861500728732 Test RE 0.4931797349578799 Lambda1 -0.059285525\n",
      "12 Train Loss 838.0614 Test MSE 858.2897646219214 Test RE 0.493180773434654 Lambda1 -0.059288327\n",
      "13 Train Loss 838.0612 Test MSE 858.2928390364839 Test RE 0.493181656726554 Lambda1 -0.05929073\n",
      "14 Train Loss 837.99335 Test MSE 857.984222613409 Test RE 0.4930929820712617 Lambda1 -0.060385436\n",
      "15 Train Loss 837.8584 Test MSE 857.8369270582315 Test RE 0.4930506540656112 Lambda1 -0.068874255\n",
      "16 Train Loss 837.8416 Test MSE 857.9245552799453 Test RE 0.49307583603631583 Lambda1 -0.1001408\n",
      "17 Train Loss 837.3462 Test MSE 856.5851290278705 Test RE 0.4926907807932496 Lambda1 -0.6969236\n",
      "18 Train Loss 834.277 Test MSE 853.0831588375117 Test RE 0.4916826174152835 Lambda1 -1.1221985\n",
      "19 Train Loss 827.2118 Test MSE 843.0912933641143 Test RE 0.48879468266819415 Lambda1 -0.7607504\n",
      "20 Train Loss 812.7517 Test MSE 822.5852029073042 Test RE 0.48281373584677534 Lambda1 -0.6212108\n",
      "21 Train Loss 797.14246 Test MSE 805.9663163105485 Test RE 0.4779116490172964 Lambda1 -0.60309136\n",
      "22 Train Loss 784.2755 Test MSE 795.1618600488788 Test RE 0.474697496281712 Lambda1 -0.49602672\n",
      "23 Train Loss 756.9052 Test MSE 759.7675195125219 Test RE 0.46401234201426333 Lambda1 -0.7155726\n",
      "24 Train Loss 728.8696 Test MSE 732.2323792357988 Test RE 0.45552648789705646 Lambda1 -1.0641254\n",
      "25 Train Loss 714.69196 Test MSE 717.3437683167358 Test RE 0.4508715535125146 Lambda1 -1.1174138\n",
      "26 Train Loss 696.1234 Test MSE 702.1237010935905 Test RE 0.44606278066249294 Lambda1 -1.3972691\n",
      "27 Train Loss 684.34454 Test MSE 688.8234619495651 Test RE 0.4418177261670581 Lambda1 -1.6313422\n",
      "28 Train Loss 676.08307 Test MSE 680.3140281383919 Test RE 0.4390802307352717 Lambda1 -1.8021826\n",
      "29 Train Loss 672.8998 Test MSE 676.5488787819547 Test RE 0.437863515869539 Lambda1 -1.8508366\n",
      "30 Train Loss 666.412 Test MSE 668.0111044516626 Test RE 0.43509191338304126 Lambda1 -2.0850472\n",
      "31 Train Loss 658.37695 Test MSE 661.8213927658612 Test RE 0.43307146714728373 Lambda1 -2.2589922\n",
      "32 Train Loss 653.1069 Test MSE 656.001424500573 Test RE 0.4311630762481989 Lambda1 -2.414809\n",
      "33 Train Loss 650.0391 Test MSE 652.8503180166455 Test RE 0.4301262825733586 Lambda1 -2.519973\n",
      "34 Train Loss 646.93945 Test MSE 653.7007327009834 Test RE 0.4304063365569998 Lambda1 -2.5366352\n",
      "35 Train Loss 644.13983 Test MSE 652.1389307917027 Test RE 0.4298918722230236 Lambda1 -2.5753932\n",
      "36 Train Loss 640.44025 Test MSE 647.860609757913 Test RE 0.42847941100858655 Lambda1 -2.5933874\n",
      "37 Train Loss 638.8095 Test MSE 646.5625065442538 Test RE 0.4280499286569662 Lambda1 -2.6015763\n",
      "38 Train Loss 636.5234 Test MSE 643.437230598927 Test RE 0.42701414739557636 Lambda1 -2.6390636\n",
      "39 Train Loss 634.6477 Test MSE 641.3029263285841 Test RE 0.42630534854200747 Lambda1 -2.6603932\n",
      "40 Train Loss 633.5968 Test MSE 640.6296990550716 Test RE 0.42608152627556567 Lambda1 -2.6450038\n",
      "41 Train Loss 633.24054 Test MSE 639.9724896400396 Test RE 0.4258629158631101 Lambda1 -2.64447\n",
      "42 Train Loss 632.40076 Test MSE 638.8622701216692 Test RE 0.42549336360882706 Lambda1 -2.6696038\n",
      "43 Train Loss 631.86017 Test MSE 638.4439752327262 Test RE 0.42535404497121915 Lambda1 -2.6781218\n",
      "44 Train Loss 631.2961 Test MSE 638.1130786932564 Test RE 0.42524380317345 Lambda1 -2.6942196\n",
      "45 Train Loss 630.898 Test MSE 637.5301725118355 Test RE 0.425049532061 Lambda1 -2.715295\n",
      "46 Train Loss 630.14343 Test MSE 635.7934635550124 Test RE 0.42447019419200144 Lambda1 -2.7510192\n",
      "47 Train Loss 628.8934 Test MSE 633.9996593088439 Test RE 0.42387097896141546 Lambda1 -2.7650921\n",
      "48 Train Loss 627.7167 Test MSE 634.4043558581992 Test RE 0.42400624067207393 Lambda1 -2.718302\n",
      "49 Train Loss 627.4961 Test MSE 634.5299340319721 Test RE 0.42404820388857667 Lambda1 -2.7135947\n",
      "50 Train Loss 627.12024 Test MSE 633.7474304154648 Test RE 0.4237866546704177 Lambda1 -2.7054675\n",
      "51 Train Loss 626.60034 Test MSE 632.826211013917 Test RE 0.4234785331341103 Lambda1 -2.680049\n",
      "52 Train Loss 626.0664 Test MSE 631.9120204805158 Test RE 0.42317254076403793 Lambda1 -2.6435418\n",
      "53 Train Loss 625.572 Test MSE 631.5868253919592 Test RE 0.42306364004948005 Lambda1 -2.6392703\n",
      "54 Train Loss 625.21094 Test MSE 631.154933317576 Test RE 0.4229189654988178 Lambda1 -2.6298916\n",
      "55 Train Loss 624.9353 Test MSE 630.6934854415648 Test RE 0.42276433568340016 Lambda1 -2.606915\n",
      "56 Train Loss 624.7017 Test MSE 630.3299159796352 Test RE 0.4226424647880622 Lambda1 -2.5807881\n",
      "57 Train Loss 624.3985 Test MSE 630.1790812680886 Test RE 0.4225918936769543 Lambda1 -2.5438292\n",
      "58 Train Loss 623.88684 Test MSE 630.7342333370524 Test RE 0.42277799245893855 Lambda1 -2.4901164\n",
      "59 Train Loss 623.7838 Test MSE 630.7664701013344 Test RE 0.42278879639158284 Lambda1 -2.489838\n",
      "60 Train Loss 623.509 Test MSE 630.5743849063444 Test RE 0.42272441626363544 Lambda1 -2.502648\n",
      "61 Train Loss 622.54736 Test MSE 629.468299679634 Test RE 0.42235350486694934 Lambda1 -2.4985802\n",
      "62 Train Loss 621.97174 Test MSE 628.8670726412108 Test RE 0.4221517544289796 Lambda1 -2.4835744\n",
      "63 Train Loss 621.4868 Test MSE 628.4411139589267 Test RE 0.4220087594519808 Lambda1 -2.468452\n",
      "64 Train Loss 621.35016 Test MSE 628.269986718016 Test RE 0.42195129813223714 Lambda1 -2.4717245\n",
      "65 Train Loss 621.3058 Test MSE 628.1278788471246 Test RE 0.4219035750251562 Lambda1 -2.474485\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-df62abe347cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mnan_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-bd85a7c3daae>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-77392118dd0b>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(xt_coll, f_hat, xt_train, u_train, seed)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mg_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mgtd_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgtd_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mls_func_evals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-77392118dd0b>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nan_tune = []\n",
    "for tune_reps in range(5):\n",
    "    label = \"inv_HT_tanh_tune\" + str(tune_reps)\n",
    "    max_reps = 10 #10\n",
    "    max_iter = 75 #75\n",
    "\n",
    "    train_loss_full = []\n",
    "    test_mse_full = []\n",
    "    test_re_full = []\n",
    "\n",
    "   \n",
    "    lambda1_full = []\n",
    "    elapsed_time= np.zeros((max_reps,1))\n",
    "    \n",
    "    time_threshold = np.empty((max_reps,1))\n",
    "    time_threshold[:] = np.nan\n",
    "    epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "    \n",
    "\n",
    "    for reps in range(max_reps):\n",
    "        print(label)\n",
    "        'Generate Training data'\n",
    "        print(reps)\n",
    "        torch.manual_seed(reps*36)\n",
    "        \n",
    "        train_loss = []\n",
    "        test_mse_loss = []\n",
    "        test_re_loss = []   \n",
    "        \n",
    "        lambda1_val = []\n",
    "\n",
    "        N_f = 50000 #Total number of collocation points \n",
    "        N_train = 500\n",
    "\n",
    "        layers = np.array([2,50,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "        \n",
    "        PINN = Sequentialmodel(layers)\n",
    "\n",
    "        PINN.to(device)\n",
    "\n",
    "        'Neural Network Summary'\n",
    "        print(PINN)\n",
    "\n",
    "        params = list(PINN.parameters())\n",
    "\n",
    "        optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lr_tune[tune_reps], \n",
    "                                  max_iter = 10, \n",
    "                                  max_eval = 15, \n",
    "                                  tolerance_grad = -1, \n",
    "                                  tolerance_change = -1, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "        \n",
    "        \n",
    "        nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "        torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "        train_loss_full.append(train_loss)\n",
    "        test_mse_full.append(test_mse_loss)\n",
    "        test_re_full.append(test_re_loss)\n",
    "        #elapsed_time[reps] = time.time() - start_time\n",
    "\n",
    "\n",
    "        lambda1_full.append(lambda1_val)\n",
    "        \n",
    "        if(nan_flag == 1):\n",
    "            nan_tune.append(tune_reps)\n",
    "            break\n",
    "\n",
    "        print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "    mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time,\"lambda1\": lambda1_full, \"label\": label}\n",
    "    savemat(label+'.mat', mdic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9040898\n",
      "-0.9851011\n",
      "-1.9162918\n",
      "-1.2635077\n",
      "-1.9005547\n"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"inv_HT_tanh_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"lambda1\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
