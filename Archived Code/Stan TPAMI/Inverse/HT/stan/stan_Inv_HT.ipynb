{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_2D_4(xt): #True function for 2D_4 Heat Transfer in a rod x \\in [0,1] t \\in [0,0.1]\n",
    "    term1 = 4*u0/np.pi\n",
    "    \n",
    "    resol_n = 10000\n",
    "    \n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "\n",
    "    u = np.zeros((np.shape(xt)[0],1))\n",
    "    \n",
    "    for i in range(resol_n):\n",
    "        j = 2*i-1\n",
    "        term2 = np.sin(j*np.pi*x)/j\n",
    "        term3 = np.exp(-1*np.square(j*np.pi)*t)\n",
    "        \n",
    "        u = u + term2*term3\n",
    "        \n",
    "    u = term1*u\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = 50.0\n",
    "loss_thresh = 0.1\n",
    "label = \"inv_HT_stan\"\n",
    "\n",
    "x_ll = np.array(0.0)\n",
    "x_ul = np.array(1.0)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,100).reshape(-1,1)\n",
    "t = np.linspace(0,0.1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = true_2D_4(xt)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_f,N_train,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #X_Train\n",
    "    np.random.seed(seed)\n",
    "    x_train = np.random.uniform(x_ll,x_ul,(N_train,1))\n",
    "    t_train = np.random.uniform(0,0.1,(N_train,1))\n",
    "    \n",
    "    xt_train = np.hstack((x_train,t_train))\n",
    "    u_train = true_2D_4(xt_train)\n",
    "    \n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requiresGrad = True\n",
    "        \n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    \n",
    "    def loss_PDE(self, xt_coll,f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_tt[:,[0]]\n",
    "                \n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        f = du_dt - self.lambda1*d2u_dx2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_train = self.loss_function(self.forward(xt_train),u_train)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "        \n",
    "        #print(self.iter,\"train_loss\",loss_train.cpu().detach().numpy(),\"F Loss\",(loss_f).cpu().detach().numpy())\n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                    \n",
    "    \n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_coll,f_hat, xt_train, u_train,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xt_coll, xt_train, u_train = trainingdata(N_f,N_train,123)\n",
    "    \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_train = torch.from_numpy(xt_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_coll,f_hat, xt_train, u_train,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_coll,f_hat, xt_train, u_train).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1],\"Lambda1\",lambda1_val[-1])\n",
    "\n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_HT_stan\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 853.5218 Test MSE 856.0882215965379 Test RE 0.49254785441163884 Lambda1 -0.020152098\n",
      "1 Train Loss 764.9929 Test MSE 768.1692004433863 Test RE 0.4665708649500331 Lambda1 0.008244848\n",
      "2 Train Loss 632.5328 Test MSE 652.8966713208255 Test RE 0.43014155209224497 Lambda1 -0.00020087621\n",
      "3 Train Loss 373.37878 Test MSE 361.2794454604883 Test RE 0.3199711317691607 Lambda1 -0.0034302128\n",
      "4 Train Loss 277.1521 Test MSE 283.26660319847446 Test RE 0.28332627902999813 Lambda1 0.0007200466\n",
      "5 Train Loss 272.43024 Test MSE 279.87984510050836 Test RE 0.2816274501339122 Lambda1 3.7489128e-05\n",
      "6 Train Loss 271.2937 Test MSE 278.6246828330235 Test RE 0.28099524069896986 Lambda1 0.00033425714\n",
      "7 Train Loss 270.82715 Test MSE 278.7760602278606 Test RE 0.28107156299589525 Lambda1 0.00051782967\n",
      "8 Train Loss 270.57843 Test MSE 278.736749876279 Test RE 0.2810517452764569 Lambda1 0.000447338\n",
      "9 Train Loss 270.44202 Test MSE 278.60063772992095 Test RE 0.28098311559712497 Lambda1 0.00073091866\n",
      "10 Train Loss 269.85434 Test MSE 277.2749598350419 Test RE 0.28031381117940274 Lambda1 0.0033351656\n",
      "11 Train Loss 266.7711 Test MSE 273.1189319303669 Test RE 0.2782050911638304 Lambda1 0.014593004\n",
      "12 Train Loss 258.44778 Test MSE 262.23714898156805 Test RE 0.27260654460436023 Lambda1 0.0403233\n",
      "13 Train Loss 245.49292 Test MSE 241.2585941509609 Test RE 0.26147523803431555 Lambda1 0.0926839\n",
      "14 Train Loss 216.57497 Test MSE 214.48902898020228 Test RE 0.24654245269611075 Lambda1 0.15985937\n",
      "15 Train Loss 200.5144 Test MSE 200.11958581144825 Test RE 0.23814088776931353 Lambda1 0.19895081\n",
      "16 Train Loss 189.21924 Test MSE 181.9380415290739 Test RE 0.22706538245990332 Lambda1 0.25403038\n",
      "17 Train Loss 166.865 Test MSE 155.17457709774405 Test RE 0.2097004951160566 Lambda1 0.36317644\n",
      "18 Train Loss 146.67194 Test MSE 135.79985325439114 Test RE 0.1961728135899951 Lambda1 0.43847537\n",
      "19 Train Loss 135.2959 Test MSE 122.80853285432262 Test RE 0.18655351750102098 Lambda1 0.51258117\n",
      "20 Train Loss 127.19053 Test MSE 117.77763778597699 Test RE 0.1826924460014822 Lambda1 0.5284536\n",
      "21 Train Loss 110.14038 Test MSE 102.566667816579 Test RE 0.170487402425005 Lambda1 0.5919469\n",
      "22 Train Loss 101.00218 Test MSE 94.75091415263624 Test RE 0.1638629899408275 Lambda1 0.6167545\n",
      "23 Train Loss 94.8694 Test MSE 88.17727486135462 Test RE 0.15807657113636234 Lambda1 0.6569701\n",
      "24 Train Loss 88.2242 Test MSE 82.25827183765985 Test RE 0.15267887851342837 Lambda1 0.6910878\n",
      "25 Train Loss 82.90756 Test MSE 77.28436042003607 Test RE 0.14799088906114005 Lambda1 0.7284222\n",
      "26 Train Loss 77.36122 Test MSE 73.1064167831139 Test RE 0.14393516820317087 Lambda1 0.759376\n",
      "27 Train Loss 74.13521 Test MSE 68.21592466909946 Test RE 0.13903753380551415 Lambda1 0.7812057\n",
      "28 Train Loss 69.16106 Test MSE 64.69485204624068 Test RE 0.13540167367904743 Lambda1 0.78801084\n",
      "29 Train Loss 65.7203 Test MSE 61.99871122039957 Test RE 0.1325502348501106 Lambda1 0.8154253\n",
      "30 Train Loss 63.990494 Test MSE 59.92856226599798 Test RE 0.13031850825820962 Lambda1 0.838163\n",
      "31 Train Loss 61.234505 Test MSE 56.48057675955199 Test RE 0.12651404224908935 Lambda1 0.8632696\n",
      "32 Train Loss 58.54629 Test MSE 53.75768724801186 Test RE 0.12342679695982636 Lambda1 0.89711237\n",
      "33 Train Loss 55.095646 Test MSE 51.947801125443114 Test RE 0.12133127328918006 Lambda1 0.9069464\n",
      "34 Train Loss 52.390858 Test MSE 49.505481781780816 Test RE 0.1184447501917605 Lambda1 0.9496296\n",
      "35 Train Loss 49.670563 Test MSE 46.24512726647968 Test RE 0.11447803339284768 Lambda1 1.0071169\n",
      "36 Train Loss 47.15071 Test MSE 45.17454258906577 Test RE 0.11314517866425235 Lambda1 1.05199\n",
      "37 Train Loss 45.069935 Test MSE 43.83153954216641 Test RE 0.11145063139010901 Lambda1 1.059953\n",
      "38 Train Loss 43.676476 Test MSE 42.62656329110971 Test RE 0.10990800642634699 Lambda1 1.0594076\n",
      "39 Train Loss 41.54179 Test MSE 41.52118254763841 Test RE 0.10847359349280829 Lambda1 1.0843341\n",
      "40 Train Loss 40.126507 Test MSE 40.52987465873803 Test RE 0.10717088102708958 Lambda1 1.0868256\n",
      "41 Train Loss 39.26714 Test MSE 40.01360881329896 Test RE 0.10648612700281662 Lambda1 1.1025039\n",
      "42 Train Loss 38.56789 Test MSE 39.578386426626224 Test RE 0.10590542633389492 Lambda1 1.1076722\n",
      "43 Train Loss 38.11541 Test MSE 39.592959437592015 Test RE 0.10592492206168952 Lambda1 1.1083148\n",
      "44 Train Loss 38.008583 Test MSE 39.4673726707494 Test RE 0.10575679451626957 Lambda1 1.1121942\n",
      "45 Train Loss 37.428814 Test MSE 39.235689832487715 Test RE 0.10544592889769598 Lambda1 1.1142834\n",
      "46 Train Loss 36.81098 Test MSE 38.65002003836567 Test RE 0.10465597603526584 Lambda1 1.1274085\n",
      "47 Train Loss 36.406517 Test MSE 38.29921686408029 Test RE 0.10417994347747819 Lambda1 1.1412301\n",
      "48 Train Loss 36.232063 Test MSE 38.140512943441884 Test RE 0.10396386948784157 Lambda1 1.1527117\n",
      "49 Train Loss 35.562275 Test MSE 37.7587451469362 Test RE 0.1034422472809696 Lambda1 1.1863933\n",
      "50 Train Loss 35.380432 Test MSE 37.630855235421706 Test RE 0.10326691782094471 Lambda1 1.1976073\n",
      "51 Train Loss 35.230186 Test MSE 37.6437832844219 Test RE 0.10328465492952327 Lambda1 1.1932207\n",
      "52 Train Loss 34.98468 Test MSE 37.488888608738065 Test RE 0.10307194073263463 Lambda1 1.1892184\n",
      "53 Train Loss 34.86796 Test MSE 37.539880106308956 Test RE 0.10314201491759677 Lambda1 1.1807837\n",
      "54 Train Loss 34.592705 Test MSE 37.28826303917741 Test RE 0.10279577080439603 Lambda1 1.1866689\n",
      "55 Train Loss 34.510822 Test MSE 37.15209986586219 Test RE 0.10260791276832001 Lambda1 1.1933724\n",
      "56 Train Loss 34.40389 Test MSE 37.0515860042515 Test RE 0.10246901748703438 Lambda1 1.2033123\n",
      "57 Train Loss 34.302116 Test MSE 36.84792118781111 Test RE 0.1021870040353422 Lambda1 1.2098701\n",
      "58 Train Loss 34.20672 Test MSE 36.608943264796714 Test RE 0.10185509709293086 Lambda1 1.2153366\n",
      "59 Train Loss 34.097664 Test MSE 36.459267089172684 Test RE 0.10164666583798711 Lambda1 1.2190163\n",
      "60 Train Loss 33.786728 Test MSE 36.144376554855754 Test RE 0.1012067641873624 Lambda1 1.2190514\n",
      "61 Train Loss 33.551758 Test MSE 35.94829709517068 Test RE 0.10093187281478384 Lambda1 1.2133774\n",
      "62 Train Loss 33.395214 Test MSE 35.95431653721373 Test RE 0.10094032284130303 Lambda1 1.2154813\n",
      "63 Train Loss 33.200798 Test MSE 35.92465447009594 Test RE 0.10089867670946051 Lambda1 1.2200882\n",
      "64 Train Loss 33.091034 Test MSE 35.92844231584813 Test RE 0.10090399587753303 Lambda1 1.2227843\n",
      "65 Train Loss 33.017746 Test MSE 35.90294382382827 Test RE 0.10086818363202571 Lambda1 1.2259214\n",
      "66 Train Loss 32.894073 Test MSE 35.69611972991491 Test RE 0.10057723113747905 Lambda1 1.2290107\n",
      "67 Train Loss 32.72128 Test MSE 35.3857033566554 Test RE 0.10013896234252954 Lambda1 1.2232397\n",
      "68 Train Loss 32.576263 Test MSE 35.33778978280908 Test RE 0.10007114341587649 Lambda1 1.2200296\n",
      "69 Train Loss 32.472836 Test MSE 35.18512886703476 Test RE 0.09985475343344924 Lambda1 1.2163411\n",
      "70 Train Loss 32.30685 Test MSE 34.9741470079104 Test RE 0.09955492195263914 Lambda1 1.2095944\n",
      "71 Train Loss 32.13623 Test MSE 34.80648082044737 Test RE 0.09931600193451198 Lambda1 1.2057847\n",
      "72 Train Loss 32.04764 Test MSE 34.88041817477602 Test RE 0.09942143153737971 Lambda1 1.2069103\n",
      "73 Train Loss 31.977179 Test MSE 34.853035343570625 Test RE 0.0993823985363086 Lambda1 1.2044195\n",
      "74 Train Loss 31.92371 Test MSE 34.86757623675328 Test RE 0.09940312783766708 Lambda1 1.2035882\n",
      "75 Train Loss 31.696125 Test MSE 34.75393175212858 Test RE 0.09924100247222786 Lambda1 1.201437\n",
      "76 Train Loss 31.538902 Test MSE 34.68604185048287 Test RE 0.09914402415796886 Lambda1 1.1981504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 Train Loss 31.488708 Test MSE 34.54180708493354 Test RE 0.09893767441002027 Lambda1 1.1907473\n",
      "78 Train Loss 31.437145 Test MSE 34.40666098426087 Test RE 0.09874393606282461 Lambda1 1.1846766\n",
      "79 Train Loss 31.394976 Test MSE 34.31038093816427 Test RE 0.098605681860487 Lambda1 1.1813765\n",
      "80 Train Loss 31.333452 Test MSE 34.1587296911521 Test RE 0.09838752288692172 Lambda1 1.1703073\n",
      "81 Train Loss 31.264444 Test MSE 34.01214962012362 Test RE 0.09817619836085362 Lambda1 1.1603189\n",
      "82 Train Loss 31.170023 Test MSE 33.9198489968767 Test RE 0.09804289481230213 Lambda1 1.1467794\n",
      "83 Train Loss 31.046858 Test MSE 33.85027939004651 Test RE 0.09794230022232783 Lambda1 1.1438252\n",
      "84 Train Loss 30.937456 Test MSE 33.85313978348691 Test RE 0.09794643826195494 Lambda1 1.1427876\n",
      "85 Train Loss 30.881643 Test MSE 33.86596372289691 Test RE 0.09796498809615074 Lambda1 1.1451486\n",
      "86 Train Loss 30.81854 Test MSE 33.77845385909801 Test RE 0.09783833515175094 Lambda1 1.1433234\n",
      "87 Train Loss 30.70519 Test MSE 33.63837812673786 Test RE 0.09763526170568454 Lambda1 1.1386541\n",
      "88 Train Loss 30.625319 Test MSE 33.49930726222245 Test RE 0.09743322634940817 Lambda1 1.1338171\n",
      "89 Train Loss 30.565409 Test MSE 33.389934612662756 Test RE 0.0972740403332193 Lambda1 1.1289235\n",
      "90 Train Loss 30.516531 Test MSE 33.289326440064606 Test RE 0.09712738016864932 Lambda1 1.1252465\n",
      "91 Train Loss 30.452795 Test MSE 33.27144743708035 Test RE 0.09710129412045906 Lambda1 1.1239309\n",
      "92 Train Loss 30.34713 Test MSE 33.24283323412905 Test RE 0.09705953047631237 Lambda1 1.1235063\n",
      "93 Train Loss 30.299154 Test MSE 33.228447532399876 Test RE 0.09703852714401881 Lambda1 1.125463\n",
      "94 Train Loss 30.24368 Test MSE 33.23101541400991 Test RE 0.097042276621557 Lambda1 1.1303705\n",
      "95 Train Loss 30.201372 Test MSE 33.2438663040345 Test RE 0.09706103859838297 Lambda1 1.1318314\n",
      "96 Train Loss 30.153872 Test MSE 33.19502026640704 Test RE 0.09698970528962568 Lambda1 1.1317549\n",
      "97 Train Loss 30.064484 Test MSE 33.0955005696716 Test RE 0.09684420708865546 Lambda1 1.1267024\n",
      "98 Train Loss 29.964865 Test MSE 33.0198485163207 Test RE 0.09673345706787911 Lambda1 1.126348\n",
      "99 Train Loss 29.88275 Test MSE 32.95078540456587 Test RE 0.0966322420290692 Lambda1 1.120924\n",
      "100 Train Loss 29.809828 Test MSE 32.81782341198058 Test RE 0.09643708123260444 Lambda1 1.1129038\n",
      "101 Train Loss 29.754051 Test MSE 32.796185271454796 Test RE 0.09640528352366162 Lambda1 1.1106834\n",
      "102 Train Loss 29.678146 Test MSE 32.692292473932646 Test RE 0.09625246454165423 Lambda1 1.1023651\n",
      "103 Train Loss 29.573027 Test MSE 32.58434939128516 Test RE 0.0960934304489053 Lambda1 1.0911324\n",
      "104 Train Loss 29.506807 Test MSE 32.48291097939743 Test RE 0.09594373951931875 Lambda1 1.0854845\n",
      "105 Train Loss 29.426586 Test MSE 32.330921826078814 Test RE 0.09571901357462005 Lambda1 1.0728351\n",
      "106 Train Loss 29.386547 Test MSE 32.23225542280296 Test RE 0.09557284596344712 Lambda1 1.0674859\n",
      "107 Train Loss 29.312117 Test MSE 32.17839096845826 Test RE 0.09549295500217826 Lambda1 1.0662097\n",
      "108 Train Loss 29.268456 Test MSE 32.09449676343326 Test RE 0.09536839106640614 Lambda1 1.0655301\n",
      "109 Train Loss 29.218718 Test MSE 32.02614029135569 Test RE 0.09526677673577992 Lambda1 1.0627271\n",
      "110 Train Loss 29.090023 Test MSE 32.00071077353033 Test RE 0.0952289471805429 Lambda1 1.0590647\n",
      "111 Train Loss 28.968292 Test MSE 31.91837473353881 Test RE 0.0951063588976415 Lambda1 1.0610243\n",
      "112 Train Loss 28.906364 Test MSE 31.892812073338487 Test RE 0.09506826706938525 Lambda1 1.0620424\n",
      "113 Train Loss 28.852064 Test MSE 31.903389986444765 Test RE 0.0950840314443756 Lambda1 1.0644794\n",
      "114 Train Loss 28.713545 Test MSE 31.79745334859749 Test RE 0.09492603478030677 Lambda1 1.0625553\n",
      "115 Train Loss 28.642303 Test MSE 31.742330218086963 Test RE 0.09484371859772436 Lambda1 1.0621439\n",
      "116 Train Loss 28.551363 Test MSE 31.651776939426323 Test RE 0.09470833875096117 Lambda1 1.0581716\n",
      "117 Train Loss 28.505686 Test MSE 31.618418515020156 Test RE 0.09465841811267717 Lambda1 1.052813\n",
      "118 Train Loss 28.462646 Test MSE 31.60863379681356 Test RE 0.09464377035895903 Lambda1 1.0511541\n",
      "119 Train Loss 28.366837 Test MSE 31.5593960089813 Test RE 0.09457002680352927 Lambda1 1.0529162\n",
      "120 Train Loss 28.32852 Test MSE 31.503498130139498 Test RE 0.0944862386548813 Lambda1 1.0540943\n",
      "121 Train Loss 28.2948 Test MSE 31.393352612210734 Test RE 0.09432091815175722 Lambda1 1.0512023\n",
      "122 Train Loss 28.259966 Test MSE 31.307184366271848 Test RE 0.09419138351915768 Lambda1 1.0544765\n",
      "123 Train Loss 28.21886 Test MSE 31.28016481958765 Test RE 0.0941507289874929 Lambda1 1.0570414\n",
      "124 Train Loss 28.194124 Test MSE 31.229818049810106 Test RE 0.09407492865166456 Lambda1 1.0521488\n",
      "125 Train Loss 28.162138 Test MSE 31.174962941556952 Test RE 0.09399227113382667 Lambda1 1.0502207\n",
      "126 Train Loss 28.11817 Test MSE 31.10577056283605 Test RE 0.09388790595019122 Lambda1 1.0441818\n",
      "127 Train Loss 28.081078 Test MSE 31.136671706442453 Test RE 0.09393452950958792 Lambda1 1.0443505\n",
      "128 Train Loss 28.035906 Test MSE 31.164057009349314 Test RE 0.0939758290452676 Lambda1 1.0414116\n",
      "129 Train Loss 28.013588 Test MSE 31.178481707747252 Test RE 0.09399757551051942 Lambda1 1.0384539\n",
      "130 Train Loss 27.988247 Test MSE 31.1964907960215 Test RE 0.09402471869013485 Lambda1 1.0362022\n",
      "131 Train Loss 27.96881 Test MSE 31.19472327656414 Test RE 0.09402205504320577 Lambda1 1.0355506\n",
      "132 Train Loss 27.949106 Test MSE 31.196384050820132 Test RE 0.09402455782755287 Lambda1 1.0335648\n",
      "133 Train Loss 27.875233 Test MSE 31.16817444503812 Test RE 0.0939820369445214 Lambda1 1.0319724\n",
      "134 Train Loss 27.829748 Test MSE 31.119721124981652 Test RE 0.09390895738566059 Lambda1 1.0315436\n",
      "135 Train Loss 27.736551 Test MSE 31.040484769250032 Test RE 0.09378932671651348 Lambda1 1.0200844\n",
      "136 Train Loss 27.683702 Test MSE 30.950181793219265 Test RE 0.09365280139810188 Lambda1 1.0167055\n",
      "137 Train Loss 27.65453 Test MSE 30.896242600823374 Test RE 0.09357115794673392 Lambda1 1.0138022\n",
      "138 Train Loss 27.636707 Test MSE 30.857108839341365 Test RE 0.09351187967651883 Lambda1 1.0129247\n",
      "139 Train Loss 27.613464 Test MSE 30.80710709598665 Test RE 0.0934360842941409 Lambda1 1.0097431\n",
      "140 Train Loss 27.579428 Test MSE 30.7660588061441 Test RE 0.09337381506018312 Lambda1 1.009543\n",
      "141 Train Loss 27.550224 Test MSE 30.69381082755552 Test RE 0.09326411569079421 Lambda1 1.007487\n",
      "142 Train Loss 27.52612 Test MSE 30.654423297101236 Test RE 0.09320425635218194 Lambda1 1.0074219\n",
      "143 Train Loss 27.503117 Test MSE 30.63510123036214 Test RE 0.09317487751313429 Lambda1 1.0084122\n",
      "144 Train Loss 27.47062 Test MSE 30.604304919973995 Test RE 0.09312803314456311 Lambda1 1.0099468\n",
      "145 Train Loss 27.441198 Test MSE 30.613579146545927 Test RE 0.09314214268012161 Lambda1 1.0096265\n",
      "146 Train Loss 27.424274 Test MSE 30.595848583014916 Test RE 0.09311516605872942 Lambda1 1.0069547\n",
      "147 Train Loss 27.416477 Test MSE 30.603586823178333 Test RE 0.09312694056404786 Lambda1 1.0049833\n",
      "148 Train Loss 27.404219 Test MSE 30.60626554696196 Test RE 0.09313101616298733 Lambda1 1.0053765\n",
      "149 Train Loss 27.398071 Test MSE 30.606870709946286 Test RE 0.09313193687585575 Lambda1 1.0049094\n",
      "150 Train Loss 27.38752 Test MSE 30.595017295031035 Test RE 0.09311390108246581 Lambda1 1.0058887\n",
      "151 Train Loss 27.367882 Test MSE 30.571193293389083 Test RE 0.0930776406396391 Lambda1 1.002813\n",
      "152 Train Loss 27.355478 Test MSE 30.54372252677926 Test RE 0.09303581222986085 Lambda1 1.0017548\n",
      "153 Train Loss 27.340872 Test MSE 30.493696278936625 Test RE 0.09295959133208351 Lambda1 1.0008905\n",
      "154 Train Loss 27.321903 Test MSE 30.486776256504392 Test RE 0.09294904293953951 Lambda1 1.0003304\n",
      "155 Train Loss 27.294931 Test MSE 30.444805802176962 Test RE 0.0928850404832401 Lambda1 0.99805385\n",
      "156 Train Loss 27.217367 Test MSE 30.416915113988768 Test RE 0.09284248443422097 Lambda1 0.99769175\n",
      "157 Train Loss 27.186798 Test MSE 30.395216730631514 Test RE 0.09280936320522366 Lambda1 0.9949978\n",
      "158 Train Loss 27.173157 Test MSE 30.405108967642214 Test RE 0.09282446455387976 Lambda1 0.99337846\n",
      "159 Train Loss 27.151688 Test MSE 30.358468792929052 Test RE 0.09275324279334997 Lambda1 0.9898207\n",
      "160 Train Loss 27.12907 Test MSE 30.324966572940173 Test RE 0.09270204954400638 Lambda1 0.98921734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 Train Loss 27.115973 Test MSE 30.30602014799491 Test RE 0.09267308583778189 Lambda1 0.9910456\n",
      "162 Train Loss 27.093533 Test MSE 30.2659705450524 Test RE 0.09261183155095092 Lambda1 0.9915842\n",
      "163 Train Loss 27.035936 Test MSE 30.233407891820946 Test RE 0.09256199838072594 Lambda1 0.9879446\n",
      "164 Train Loss 26.985628 Test MSE 30.159520691529945 Test RE 0.09244882340245382 Lambda1 0.98937106\n",
      "165 Train Loss 26.94237 Test MSE 30.175982265951916 Test RE 0.09247405002318836 Lambda1 0.9884902\n",
      "166 Train Loss 26.933655 Test MSE 30.16347004302062 Test RE 0.09245487623317127 Lambda1 0.98699623\n",
      "167 Train Loss 26.926556 Test MSE 30.17225560619496 Test RE 0.09246833968775812 Lambda1 0.986599\n",
      "168 Train Loss 26.912912 Test MSE 30.14583165620356 Test RE 0.09242784032951812 Lambda1 0.98657227\n",
      "169 Train Loss 26.893147 Test MSE 30.110537811730815 Test RE 0.09237371859851992 Lambda1 0.98121697\n",
      "170 Train Loss 26.863592 Test MSE 30.06036574375271 Test RE 0.09229672706925435 Lambda1 0.9756916\n",
      "171 Train Loss 26.791954 Test MSE 29.96160538508587 Test RE 0.09214498645022179 Lambda1 0.9699802\n",
      "172 Train Loss 26.714664 Test MSE 29.817233963732747 Test RE 0.09192271587046186 Lambda1 0.96191394\n",
      "173 Train Loss 26.664925 Test MSE 29.743213731660774 Test RE 0.09180854752105762 Lambda1 0.9553871\n",
      "174 Train Loss 26.621035 Test MSE 29.710150806550065 Test RE 0.09175750557261035 Lambda1 0.9497758\n",
      "175 Train Loss 26.601995 Test MSE 29.705998616943294 Test RE 0.09175109349015378 Lambda1 0.9469543\n",
      "176 Train Loss 26.589138 Test MSE 29.688904376629164 Test RE 0.0917246907261615 Lambda1 0.94247097\n",
      "177 Train Loss 26.568165 Test MSE 29.63918673680219 Test RE 0.09164785653439529 Lambda1 0.9378566\n",
      "178 Train Loss 26.54064 Test MSE 29.619240833299987 Test RE 0.09161701380305294 Lambda1 0.9431765\n",
      "179 Train Loss 26.517756 Test MSE 29.611085176538367 Test RE 0.09160439956414401 Lambda1 0.94187856\n",
      "180 Train Loss 26.49805 Test MSE 29.60481988210894 Test RE 0.09159470794202147 Lambda1 0.94508296\n",
      "181 Train Loss 26.48342 Test MSE 29.59097127971455 Test RE 0.09157328225741979 Lambda1 0.94443387\n",
      "182 Train Loss 26.47072 Test MSE 29.54973730743267 Test RE 0.09150945794982536 Lambda1 0.94278675\n",
      "183 Train Loss 26.46094 Test MSE 29.531515457962826 Test RE 0.09148123893908747 Lambda1 0.9408398\n",
      "184 Train Loss 26.454521 Test MSE 29.525224199292104 Test RE 0.09147149404817678 Lambda1 0.9395222\n",
      "185 Train Loss 26.439537 Test MSE 29.48674751084677 Test RE 0.09141187269620354 Lambda1 0.9375746\n",
      "186 Train Loss 26.424303 Test MSE 29.491146590422666 Test RE 0.09141869123570054 Lambda1 0.93665564\n",
      "187 Train Loss 26.411495 Test MSE 29.46472171183715 Test RE 0.09137772522593285 Lambda1 0.9377764\n",
      "188 Train Loss 26.398926 Test MSE 29.455533543201827 Test RE 0.09136347667097845 Lambda1 0.9355201\n",
      "189 Train Loss 26.387993 Test MSE 29.451682009351696 Test RE 0.09135750324282184 Lambda1 0.93228734\n",
      "190 Train Loss 26.382116 Test MSE 29.44521595050124 Test RE 0.09134747401225381 Lambda1 0.92971236\n",
      "191 Train Loss 26.369286 Test MSE 29.42043818386693 Test RE 0.09130903206721053 Lambda1 0.93061715\n",
      "192 Train Loss 26.334858 Test MSE 29.413308975318582 Test RE 0.09129796832016235 Lambda1 0.9329545\n",
      "193 Train Loss 26.301788 Test MSE 29.38129267173569 Test RE 0.09124826600119527 Lambda1 0.9315353\n",
      "194 Train Loss 26.28504 Test MSE 29.35598602259046 Test RE 0.09120896062873633 Lambda1 0.93400514\n",
      "195 Train Loss 26.275457 Test MSE 29.366891117369423 Test RE 0.09122590010412755 Lambda1 0.9343029\n",
      "196 Train Loss 26.268728 Test MSE 29.369966039290173 Test RE 0.09123067597870552 Lambda1 0.9334094\n",
      "197 Train Loss 26.264524 Test MSE 29.367008316128306 Test RE 0.0912260821382346 Lambda1 0.9347996\n",
      "198 Train Loss 26.255163 Test MSE 29.378545133519044 Test RE 0.09124399944352124 Lambda1 0.93287855\n",
      "199 Train Loss 26.23956 Test MSE 29.37119151978022 Test RE 0.09123257928780593 Lambda1 0.93265265\n",
      "Training time: 981.63\n",
      "Training time: 981.63\n",
      "inv_HT_stan\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 827.4731 Test MSE 831.406754199711 Test RE 0.4853957224467294 Lambda1 0.0090445895\n",
      "1 Train Loss 635.1853 Test MSE 644.7896909507917 Test RE 0.42746268885272265 Lambda1 -0.00027805878\n",
      "2 Train Loss 576.85016 Test MSE 571.1565803819989 Test RE 0.40231549819454965 Lambda1 0.000493032\n",
      "3 Train Loss 290.51968 Test MSE 294.10866001268283 Test RE 0.28869753575021573 Lambda1 0.002603796\n",
      "4 Train Loss 273.77002 Test MSE 281.28785758830213 Test RE 0.28233496356268206 Lambda1 0.0008141206\n",
      "5 Train Loss 271.32922 Test MSE 279.21137786628736 Test RE 0.2812909284620852 Lambda1 0.00045710747\n",
      "6 Train Loss 270.845 Test MSE 278.8328083759167 Test RE 0.28110016925251474 Lambda1 0.0003964477\n",
      "7 Train Loss 270.5995 Test MSE 278.91308660954456 Test RE 0.28114063185298016 Lambda1 0.00042194672\n",
      "8 Train Loss 270.54752 Test MSE 278.9215258668528 Test RE 0.281144885149614 Lambda1 0.000427953\n",
      "9 Train Loss 270.4681 Test MSE 278.84752867475703 Test RE 0.2811075891539042 Lambda1 0.00040055395\n",
      "10 Train Loss 270.2721 Test MSE 278.76572068835867 Test RE 0.28106635060880364 Lambda1 0.0010823358\n",
      "11 Train Loss 270.14517 Test MSE 278.5184332518193 Test RE 0.2809416588103536 Lambda1 0.0013500935\n",
      "12 Train Loss 269.81357 Test MSE 278.12408275984944 Test RE 0.2807426976066134 Lambda1 0.0028616597\n",
      "13 Train Loss 268.5408 Test MSE 276.4260317411591 Test RE 0.27988436617423884 Lambda1 0.008477482\n",
      "14 Train Loss 266.18124 Test MSE 273.7170515329584 Test RE 0.2785095536093806 Lambda1 0.016419766\n",
      "15 Train Loss 261.95087 Test MSE 268.03118935926153 Test RE 0.2756016652687537 Lambda1 0.035420608\n",
      "16 Train Loss 253.8391 Test MSE 258.52844671776097 Test RE 0.2706720044300435 Lambda1 0.068906404\n",
      "17 Train Loss 245.86469 Test MSE 250.2283122959112 Test RE 0.26629155517818065 Lambda1 0.102875486\n",
      "18 Train Loss 239.40955 Test MSE 242.9393689010105 Test RE 0.2623844661630762 Lambda1 0.1313614\n",
      "19 Train Loss 234.37828 Test MSE 237.72517902983375 Test RE 0.2595534238700337 Lambda1 0.16609366\n",
      "20 Train Loss 225.61569 Test MSE 226.69356283247933 Test RE 0.2534596114062667 Lambda1 0.21246822\n",
      "21 Train Loss 207.71518 Test MSE 201.32139087818885 Test RE 0.2388548871575826 Lambda1 0.23419257\n",
      "22 Train Loss 190.97498 Test MSE 186.81482843005466 Test RE 0.23008846317354412 Lambda1 0.28944865\n",
      "23 Train Loss 178.43272 Test MSE 173.8239725572297 Test RE 0.22194430523501707 Lambda1 0.33764565\n",
      "24 Train Loss 162.63014 Test MSE 159.0923133962153 Test RE 0.21233117798149817 Lambda1 0.41826612\n",
      "25 Train Loss 154.25821 Test MSE 148.38815514572002 Test RE 0.20506369961949245 Lambda1 0.4692136\n",
      "26 Train Loss 140.22195 Test MSE 137.5217733269058 Test RE 0.1974126155477598 Lambda1 0.51905626\n",
      "27 Train Loss 133.90727 Test MSE 131.54864446552355 Test RE 0.19307780782995274 Lambda1 0.54110193\n",
      "28 Train Loss 130.35287 Test MSE 127.62099643163961 Test RE 0.19017360368515898 Lambda1 0.55825263\n",
      "29 Train Loss 125.96036 Test MSE 123.18844241036686 Test RE 0.186841847371194 Lambda1 0.56398535\n",
      "30 Train Loss 122.00826 Test MSE 119.36458111169321 Test RE 0.18391913248932162 Lambda1 0.5731084\n",
      "31 Train Loss 117.49518 Test MSE 115.26935297921128 Test RE 0.18073659593297806 Lambda1 0.5828674\n",
      "32 Train Loss 113.45519 Test MSE 109.66569267049131 Test RE 0.17628873591833824 Lambda1 0.60935146\n",
      "33 Train Loss 109.382515 Test MSE 105.16395869096824 Test RE 0.17263252930073886 Lambda1 0.64520794\n",
      "34 Train Loss 106.91731 Test MSE 101.58563373367022 Test RE 0.16967010076308478 Lambda1 0.66976595\n",
      "35 Train Loss 101.79837 Test MSE 94.86547760111908 Test RE 0.16396202348723488 Lambda1 0.7199377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 Train Loss 96.79539 Test MSE 89.74241868152322 Test RE 0.1594733273556439 Lambda1 0.7806508\n",
      "37 Train Loss 92.92994 Test MSE 87.37560332452281 Test RE 0.15735634679986849 Lambda1 0.80123496\n",
      "38 Train Loss 89.673195 Test MSE 83.32124318353003 Test RE 0.15366219804272507 Lambda1 0.8177233\n",
      "39 Train Loss 85.298546 Test MSE 79.88547146435836 Test RE 0.15046069790747266 Lambda1 0.83541375\n",
      "40 Train Loss 81.097084 Test MSE 75.10829916772686 Test RE 0.14589255643084256 Lambda1 0.87394035\n",
      "41 Train Loss 78.12879 Test MSE 73.58518420164117 Test RE 0.14440570838353325 Lambda1 0.8897321\n",
      "42 Train Loss 73.73964 Test MSE 68.50383947725669 Test RE 0.1393306384940783 Lambda1 0.9335494\n",
      "43 Train Loss 69.342026 Test MSE 63.59012644512902 Test RE 0.1342406402765859 Lambda1 0.9864053\n",
      "44 Train Loss 67.194725 Test MSE 63.18355322677295 Test RE 0.13381080807263426 Lambda1 0.9899833\n",
      "45 Train Loss 63.922356 Test MSE 60.337821718586575 Test RE 0.130762731619327 Lambda1 1.0407912\n",
      "46 Train Loss 62.802055 Test MSE 58.82863739649465 Test RE 0.12911704117349013 Lambda1 1.0625011\n",
      "47 Train Loss 61.752556 Test MSE 57.05114428095298 Test RE 0.12715145974483613 Lambda1 1.0777034\n",
      "48 Train Loss 60.678505 Test MSE 56.68949589773323 Test RE 0.1267478111775238 Lambda1 1.0945327\n",
      "49 Train Loss 59.18101 Test MSE 56.46508200113197 Test RE 0.12649668726531682 Lambda1 1.1036236\n",
      "50 Train Loss 58.20486 Test MSE 55.35172004859627 Test RE 0.1252433660322033 Lambda1 1.1316738\n",
      "51 Train Loss 57.60667 Test MSE 54.51730529485531 Test RE 0.12429577347137283 Lambda1 1.1524762\n",
      "52 Train Loss 56.706345 Test MSE 53.62486391758353 Test RE 0.12327422252754643 Lambda1 1.1621569\n",
      "53 Train Loss 55.778694 Test MSE 52.59834401278965 Test RE 0.1220886260200406 Lambda1 1.1870883\n",
      "54 Train Loss 54.313656 Test MSE 51.15132698662432 Test RE 0.12039754268094019 Lambda1 1.210583\n",
      "55 Train Loss 52.817844 Test MSE 49.76916996353516 Test RE 0.11875977592802947 Lambda1 1.2109842\n",
      "56 Train Loss 51.155308 Test MSE 48.38605258311165 Test RE 0.11709794328488628 Lambda1 1.2349037\n",
      "57 Train Loss 49.924774 Test MSE 46.63405004255973 Test RE 0.11495840720299798 Lambda1 1.2539649\n",
      "58 Train Loss 48.848995 Test MSE 46.09480539028437 Test RE 0.11429182393846873 Lambda1 1.2649801\n",
      "59 Train Loss 47.751335 Test MSE 45.363442665026774 Test RE 0.1133814935769504 Lambda1 1.2675799\n",
      "60 Train Loss 46.74846 Test MSE 44.8924420194403 Test RE 0.11279134768291739 Lambda1 1.2767167\n",
      "61 Train Loss 45.987236 Test MSE 43.733527155609146 Test RE 0.11132595340830889 Lambda1 1.2897184\n",
      "62 Train Loss 45.232197 Test MSE 43.5969271693177 Test RE 0.1111519562715941 Lambda1 1.2957795\n",
      "63 Train Loss 44.383667 Test MSE 43.4825804137926 Test RE 0.111006094954579 Lambda1 1.3077053\n",
      "64 Train Loss 43.848816 Test MSE 43.385741578390615 Test RE 0.11088241677565243 Lambda1 1.3144886\n",
      "65 Train Loss 42.94927 Test MSE 42.43756830318203 Test RE 0.10966408413477939 Lambda1 1.3237458\n",
      "66 Train Loss 41.973587 Test MSE 41.26802537452265 Test RE 0.10814240285302937 Lambda1 1.3347616\n",
      "67 Train Loss 41.36355 Test MSE 40.72965281793978 Test RE 0.10743468744795623 Lambda1 1.3401991\n",
      "68 Train Loss 40.705128 Test MSE 39.89581801686572 Test RE 0.10632927623781703 Lambda1 1.3483248\n",
      "69 Train Loss 40.147736 Test MSE 39.65584731274948 Test RE 0.10600901214102665 Lambda1 1.36188\n",
      "70 Train Loss 39.88809 Test MSE 39.6803424175758 Test RE 0.1060417475530035 Lambda1 1.364211\n",
      "71 Train Loss 39.728664 Test MSE 39.491844324913366 Test RE 0.10578957656473735 Lambda1 1.3655834\n",
      "72 Train Loss 39.35502 Test MSE 39.32577213809615 Test RE 0.10556690761039389 Lambda1 1.3681914\n",
      "73 Train Loss 39.00477 Test MSE 39.15008341420025 Test RE 0.10533083244441323 Lambda1 1.3709635\n",
      "74 Train Loss 38.803787 Test MSE 39.02992588211055 Test RE 0.1051690700964756 Lambda1 1.3706542\n",
      "75 Train Loss 38.314526 Test MSE 38.80261568428853 Test RE 0.10486237068004955 Lambda1 1.374076\n",
      "76 Train Loss 38.080704 Test MSE 38.54320903967974 Test RE 0.1045112653339294 Lambda1 1.3719003\n",
      "77 Train Loss 37.82923 Test MSE 38.3702256442196 Test RE 0.10427647632532758 Lambda1 1.3738787\n",
      "78 Train Loss 37.64435 Test MSE 38.34747957488501 Test RE 0.10424556392462418 Lambda1 1.3803898\n",
      "79 Train Loss 37.399155 Test MSE 38.17997098504687 Test RE 0.10401763318869651 Lambda1 1.3787059\n",
      "80 Train Loss 37.02073 Test MSE 38.15109795660939 Test RE 0.10397829486523487 Lambda1 1.3774536\n",
      "81 Train Loss 36.825195 Test MSE 38.2227866516151 Test RE 0.104075940415016 Lambda1 1.3775462\n",
      "82 Train Loss 36.67395 Test MSE 38.1947273621395 Test RE 0.10403773240693215 Lambda1 1.3808978\n",
      "83 Train Loss 36.461308 Test MSE 38.05882568041885 Test RE 0.1038524777500575 Lambda1 1.3841307\n",
      "84 Train Loss 36.322514 Test MSE 37.792092230492685 Test RE 0.10348791532382902 Lambda1 1.3886392\n",
      "85 Train Loss 36.19015 Test MSE 37.72613342902741 Test RE 0.10339756679587782 Lambda1 1.3913846\n",
      "86 Train Loss 36.068233 Test MSE 37.60235196617254 Test RE 0.10322780095355397 Lambda1 1.3952729\n",
      "87 Train Loss 35.870346 Test MSE 37.50925982501747 Test RE 0.10309994123773729 Lambda1 1.3972771\n",
      "88 Train Loss 35.610317 Test MSE 37.31493059746952 Test RE 0.1028325226135954 Lambda1 1.3975588\n",
      "89 Train Loss 35.413666 Test MSE 37.15443679451258 Test RE 0.10261113982145206 Lambda1 1.4037734\n",
      "90 Train Loss 35.16935 Test MSE 36.966819219864156 Test RE 0.10235173583070455 Lambda1 1.4117709\n",
      "91 Train Loss 35.005676 Test MSE 36.66460460696566 Test RE 0.10193249946464358 Lambda1 1.4110591\n",
      "92 Train Loss 34.86485 Test MSE 36.574350934185375 Test RE 0.101806963527719 Lambda1 1.4164729\n",
      "93 Train Loss 34.750237 Test MSE 36.43918658449807 Test RE 0.10161867024683825 Lambda1 1.4172292\n",
      "94 Train Loss 34.64488 Test MSE 36.36094958455458 Test RE 0.10150952110426674 Lambda1 1.4175205\n",
      "95 Train Loss 34.49631 Test MSE 36.17994485235496 Test RE 0.10125654879400715 Lambda1 1.4157915\n",
      "96 Train Loss 34.326717 Test MSE 36.04193247926238 Test RE 0.101063237152817 Lambda1 1.4118712\n",
      "97 Train Loss 34.194004 Test MSE 36.007623733211226 Test RE 0.10101512404890366 Lambda1 1.408695\n",
      "98 Train Loss 34.08381 Test MSE 35.939446920151646 Test RE 0.10091944775161017 Lambda1 1.4072973\n",
      "99 Train Loss 33.83018 Test MSE 35.88922840900275 Test RE 0.10084891527986296 Lambda1 1.4030248\n",
      "100 Train Loss 33.61423 Test MSE 35.66366361616929 Test RE 0.10053149663706866 Lambda1 1.3955442\n",
      "101 Train Loss 33.396797 Test MSE 35.29926038076688 Test RE 0.10001657388997363 Lambda1 1.3884594\n",
      "102 Train Loss 33.249603 Test MSE 35.16724820216322 Test RE 0.09982937770887869 Lambda1 1.3855137\n",
      "103 Train Loss 33.061367 Test MSE 34.888084652595545 Test RE 0.09943235701307483 Lambda1 1.3751339\n",
      "104 Train Loss 32.894844 Test MSE 34.66649791042045 Test RE 0.09911608874382533 Lambda1 1.3673142\n",
      "105 Train Loss 32.649998 Test MSE 34.37759404265441 Test RE 0.09870221753239246 Lambda1 1.3547653\n",
      "106 Train Loss 32.443035 Test MSE 34.125765550585946 Test RE 0.09834003805652909 Lambda1 1.3485206\n",
      "107 Train Loss 32.275314 Test MSE 34.1256193959148 Test RE 0.0983398274697323 Lambda1 1.3503472\n",
      "108 Train Loss 32.145195 Test MSE 34.10402330867627 Test RE 0.09830870581338079 Lambda1 1.3502122\n",
      "109 Train Loss 32.05137 Test MSE 34.00414163469511 Test RE 0.09816464014038777 Lambda1 1.3493222\n",
      "110 Train Loss 31.95634 Test MSE 33.879536254348 Test RE 0.09798461694385883 Lambda1 1.3444028\n",
      "111 Train Loss 31.790089 Test MSE 33.78314518998711 Test RE 0.09784512906940691 Lambda1 1.3393718\n",
      "112 Train Loss 31.719275 Test MSE 33.83183708180451 Test RE 0.097915616126733 Lambda1 1.3419282\n",
      "113 Train Loss 31.632423 Test MSE 33.79375025662797 Test RE 0.09786048543553527 Lambda1 1.3389709\n",
      "114 Train Loss 31.553696 Test MSE 33.678369808347746 Test RE 0.09769328231623614 Lambda1 1.3305442\n",
      "115 Train Loss 31.46854 Test MSE 33.60443522659488 Test RE 0.09758598970519794 Lambda1 1.3253177\n",
      "116 Train Loss 31.340118 Test MSE 33.49029069637468 Test RE 0.09742011305998012 Lambda1 1.3184556\n",
      "117 Train Loss 31.192705 Test MSE 33.415324681799426 Test RE 0.09731101742379568 Lambda1 1.3116031\n",
      "118 Train Loss 31.009346 Test MSE 33.27709005049089 Test RE 0.09710952763415184 Lambda1 1.3077357\n",
      "119 Train Loss 30.954472 Test MSE 33.2883057119127 Test RE 0.09712589108157782 Lambda1 1.3073655\n",
      "120 Train Loss 30.85258 Test MSE 33.22314047395077 Test RE 0.0970307776140522 Lambda1 1.3037893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 Train Loss 30.741447 Test MSE 33.16229072308844 Test RE 0.09694187866382332 Lambda1 1.3031678\n",
      "122 Train Loss 30.618544 Test MSE 33.10371784260395 Test RE 0.09685622905353057 Lambda1 1.2999494\n",
      "123 Train Loss 30.511875 Test MSE 33.00567681557088 Test RE 0.09671269645282385 Lambda1 1.2954569\n",
      "124 Train Loss 30.465982 Test MSE 32.98790483354472 Test RE 0.09668665536079091 Lambda1 1.2924812\n",
      "125 Train Loss 30.402527 Test MSE 32.892882028395 Test RE 0.09654730030037485 Lambda1 1.2896543\n",
      "126 Train Loss 30.313667 Test MSE 32.83337649865577 Test RE 0.09645993035026497 Lambda1 1.2868564\n",
      "127 Train Loss 30.246109 Test MSE 32.7666650174071 Test RE 0.09636188595874097 Lambda1 1.2827632\n",
      "128 Train Loss 30.090582 Test MSE 32.634455542664654 Test RE 0.09616728526698486 Lambda1 1.2747065\n",
      "129 Train Loss 29.901697 Test MSE 32.480680473599264 Test RE 0.09594044537580282 Lambda1 1.2623174\n",
      "130 Train Loss 29.691942 Test MSE 32.4112528106858 Test RE 0.09583785386519271 Lambda1 1.2578385\n",
      "131 Train Loss 29.621506 Test MSE 32.39020021537947 Test RE 0.09580672326824546 Lambda1 1.2549546\n",
      "132 Train Loss 29.488127 Test MSE 32.21670622929744 Test RE 0.09554979048804044 Lambda1 1.2512718\n",
      "133 Train Loss 29.409983 Test MSE 32.05307065510823 Test RE 0.09530682261417744 Lambda1 1.2448745\n",
      "134 Train Loss 29.321 Test MSE 31.991595775821573 Test RE 0.09521538383407789 Lambda1 1.2404691\n",
      "135 Train Loss 29.217987 Test MSE 31.836391794244193 Test RE 0.09498413914018465 Lambda1 1.234383\n",
      "136 Train Loss 29.135185 Test MSE 31.734863595279517 Test RE 0.09483256308524289 Lambda1 1.2266674\n",
      "137 Train Loss 29.02598 Test MSE 31.58193545256344 Test RE 0.09460379131750048 Lambda1 1.2113296\n",
      "138 Train Loss 28.877968 Test MSE 31.443551726407204 Test RE 0.09439629932116697 Lambda1 1.1956279\n",
      "139 Train Loss 28.74165 Test MSE 31.22637129149964 Test RE 0.09406973709901316 Lambda1 1.1747357\n",
      "140 Train Loss 28.642313 Test MSE 31.06692853798113 Test RE 0.09382926834906773 Lambda1 1.1584218\n",
      "141 Train Loss 28.44268 Test MSE 31.005139246209776 Test RE 0.09373591297200719 Lambda1 1.1523554\n",
      "142 Train Loss 28.32455 Test MSE 30.982327740125434 Test RE 0.09370142432202075 Lambda1 1.1464043\n",
      "143 Train Loss 28.1945 Test MSE 30.832585321915488 Test RE 0.09347471326473879 Lambda1 1.1304235\n",
      "144 Train Loss 28.092058 Test MSE 30.783323674493545 Test RE 0.09340001049418091 Lambda1 1.1218569\n",
      "145 Train Loss 27.953201 Test MSE 30.7457969614334 Test RE 0.0933430630332429 Lambda1 1.1045467\n",
      "146 Train Loss 27.773754 Test MSE 30.60137647330108 Test RE 0.09312357744782664 Lambda1 1.0835667\n",
      "147 Train Loss 27.717379 Test MSE 30.513558833042747 Test RE 0.09298986175782303 Lambda1 1.0736645\n",
      "148 Train Loss 27.569843 Test MSE 30.466764324285933 Test RE 0.09291853142729073 Lambda1 1.0737231\n",
      "149 Train Loss 27.398663 Test MSE 30.417280658469593 Test RE 0.09284304231386918 Lambda1 1.0653257\n",
      "150 Train Loss 27.352793 Test MSE 30.36731176424938 Test RE 0.09276675063116487 Lambda1 1.05757\n",
      "151 Train Loss 27.247091 Test MSE 30.238208095339807 Test RE 0.09256934619274125 Lambda1 1.0329078\n",
      "152 Train Loss 27.206104 Test MSE 30.217172772091498 Test RE 0.09253714248409976 Lambda1 1.0283827\n",
      "153 Train Loss 27.152618 Test MSE 30.18923752592566 Test RE 0.09249435811098092 Lambda1 1.0182208\n",
      "154 Train Loss 27.102457 Test MSE 30.136749802047817 Test RE 0.09241391668981944 Lambda1 1.0115671\n",
      "155 Train Loss 27.05651 Test MSE 30.119179807237945 Test RE 0.0923869736920519 Lambda1 1.001971\n",
      "156 Train Loss 26.983753 Test MSE 30.08246723913326 Test RE 0.09233065082271745 Lambda1 0.98317486\n",
      "157 Train Loss 26.894468 Test MSE 30.02832427297 Test RE 0.09224752421881405 Lambda1 0.966255\n",
      "158 Train Loss 26.82868 Test MSE 29.996772280759643 Test RE 0.09219904735225681 Lambda1 0.9606336\n",
      "159 Train Loss 26.792433 Test MSE 29.983476598759257 Test RE 0.0921786120689465 Lambda1 0.95979923\n",
      "160 Train Loss 26.765385 Test MSE 30.00547166811237 Test RE 0.09221241574194189 Lambda1 0.9572029\n",
      "161 Train Loss 26.72374 Test MSE 29.954810084554694 Test RE 0.09213453660325495 Lambda1 0.9518298\n",
      "162 Train Loss 26.677925 Test MSE 29.941579829282908 Test RE 0.0921141876499323 Lambda1 0.9400747\n",
      "163 Train Loss 26.658758 Test MSE 29.916008448937674 Test RE 0.09207484453448292 Lambda1 0.9378573\n",
      "164 Train Loss 26.622887 Test MSE 29.864124459834827 Test RE 0.09199496617534558 Lambda1 0.94409597\n",
      "165 Train Loss 26.604895 Test MSE 29.83361089224908 Test RE 0.09194795639236822 Lambda1 0.945767\n",
      "166 Train Loss 26.524382 Test MSE 29.7713888538672 Test RE 0.09185202138325708 Lambda1 0.93715554\n",
      "167 Train Loss 26.484589 Test MSE 29.716722707053297 Test RE 0.09176765341496755 Lambda1 0.92995214\n",
      "168 Train Loss 26.420397 Test MSE 29.716951164848137 Test RE 0.09176800616240288 Lambda1 0.9141124\n",
      "169 Train Loss 26.388899 Test MSE 29.694407516053737 Test RE 0.09173319138294346 Lambda1 0.9090753\n",
      "170 Train Loss 26.359846 Test MSE 29.61955219351481 Test RE 0.09161749534507313 Lambda1 0.91357815\n",
      "171 Train Loss 26.289547 Test MSE 29.525600593327887 Test RE 0.09147207709565583 Lambda1 0.9130536\n",
      "172 Train Loss 26.222414 Test MSE 29.462196703614325 Test RE 0.0913738097901586 Lambda1 0.9071315\n",
      "173 Train Loss 26.199629 Test MSE 29.429301918985047 Test RE 0.09132278573938929 Lambda1 0.9099719\n",
      "174 Train Loss 26.094393 Test MSE 29.375668569924624 Test RE 0.09123953231298489 Lambda1 0.9025385\n",
      "175 Train Loss 26.032429 Test MSE 29.335626448660367 Test RE 0.09117732657530413 Lambda1 0.90499336\n",
      "176 Train Loss 25.999033 Test MSE 29.274901050671318 Test RE 0.0910829081428714 Lambda1 0.90564865\n",
      "177 Train Loss 25.960379 Test MSE 29.22951748011327 Test RE 0.09101227987867666 Lambda1 0.90579885\n",
      "178 Train Loss 25.941353 Test MSE 29.191870157409884 Test RE 0.0909536495460682 Lambda1 0.908238\n",
      "179 Train Loss 25.918968 Test MSE 29.150200671387662 Test RE 0.09088871116998049 Lambda1 0.9058479\n",
      "180 Train Loss 25.903948 Test MSE 29.110743332122045 Test RE 0.09082717744089593 Lambda1 0.9042455\n",
      "181 Train Loss 25.875631 Test MSE 29.1018320165724 Test RE 0.09081327447076146 Lambda1 0.9020768\n",
      "182 Train Loss 25.816584 Test MSE 29.09410305338485 Test RE 0.09080121442134843 Lambda1 0.90191\n",
      "183 Train Loss 25.80131 Test MSE 29.08596628395382 Test RE 0.09078851631189212 Lambda1 0.8991527\n",
      "184 Train Loss 25.779898 Test MSE 29.0821159707557 Test RE 0.09078250695692495 Lambda1 0.8950648\n",
      "185 Train Loss 25.75905 Test MSE 29.052413597798697 Test RE 0.09073613576568401 Lambda1 0.89838886\n",
      "186 Train Loss 25.743797 Test MSE 29.021086913859577 Test RE 0.09068720301704133 Lambda1 0.9044614\n",
      "187 Train Loss 25.72979 Test MSE 29.017492828286468 Test RE 0.09068158731317455 Lambda1 0.9040475\n",
      "188 Train Loss 25.714075 Test MSE 28.992566870029183 Test RE 0.09064263131022814 Lambda1 0.90718794\n",
      "189 Train Loss 25.695189 Test MSE 28.98634260240675 Test RE 0.09063290098380501 Lambda1 0.9074022\n",
      "190 Train Loss 25.686152 Test MSE 28.979041863695375 Test RE 0.09062148649099454 Lambda1 0.9089178\n",
      "191 Train Loss 25.67004 Test MSE 28.949015712617005 Test RE 0.09057452635242348 Lambda1 0.9124704\n",
      "192 Train Loss 25.659632 Test MSE 28.908395641329985 Test RE 0.09051095881586366 Lambda1 0.91862357\n",
      "193 Train Loss 25.642393 Test MSE 28.908669887953124 Test RE 0.09051138814210939 Lambda1 0.92076737\n",
      "194 Train Loss 25.619268 Test MSE 28.91484565527474 Test RE 0.09052105561254058 Lambda1 0.92025423\n",
      "195 Train Loss 25.607065 Test MSE 28.899650175443163 Test RE 0.09049726693915114 Lambda1 0.92244285\n",
      "196 Train Loss 25.597044 Test MSE 28.884646886331996 Test RE 0.09047377300553194 Lambda1 0.92893183\n",
      "197 Train Loss 25.590477 Test MSE 28.883086602322106 Test RE 0.09047132937711763 Lambda1 0.930006\n",
      "198 Train Loss 25.57887 Test MSE 28.884903147389483 Test RE 0.09047417434077094 Lambda1 0.9305749\n",
      "199 Train Loss 25.56563 Test MSE 28.864798172897835 Test RE 0.09044268215282911 Lambda1 0.9344028\n",
      "Training time: 994.57\n",
      "Training time: 994.57\n",
      "inv_HT_stan\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 834.21124 Test MSE 837.2075746626348 Test RE 0.48708610993167895 Lambda1 -0.18881664\n",
      "1 Train Loss 627.2664 Test MSE 630.1755583985422 Test RE 0.42259071247446955 Lambda1 0.0008117685\n",
      "2 Train Loss 487.41022 Test MSE 488.7310201785967 Test RE 0.37215522697934017 Lambda1 0.00030354602\n",
      "3 Train Loss 320.43982 Test MSE 321.6927082259444 Test RE 0.30193244167676475 Lambda1 0.0015299714\n",
      "4 Train Loss 276.68643 Test MSE 283.70548610173546 Test RE 0.2835456817606556 Lambda1 0.0009562677\n",
      "5 Train Loss 272.20422 Test MSE 278.98511699274223 Test RE 0.2811769323371318 Lambda1 0.0008680413\n",
      "6 Train Loss 270.79596 Test MSE 278.7129279847749 Test RE 0.28103973514998265 Lambda1 0.00021577333\n",
      "7 Train Loss 270.57153 Test MSE 278.68033831071176 Test RE 0.28102330379678425 Lambda1 0.0002848722\n",
      "8 Train Loss 270.33795 Test MSE 278.53104947062883 Test RE 0.28094802173086353 Lambda1 0.0005034763\n",
      "9 Train Loss 269.98495 Test MSE 277.96459314855645 Test RE 0.28066219043833907 Lambda1 0.0019891362\n",
      "10 Train Loss 268.61203 Test MSE 275.0869376846735 Test RE 0.2792056195066172 Lambda1 0.009102101\n",
      "11 Train Loss 265.41516 Test MSE 270.7528756147056 Test RE 0.2769974108028078 Lambda1 0.019486798\n",
      "12 Train Loss 258.7167 Test MSE 258.38712512043315 Test RE 0.27059801445058596 Lambda1 0.0511429\n",
      "13 Train Loss 247.64162 Test MSE 250.30176521327877 Test RE 0.26633063640006394 Lambda1 0.06834678\n",
      "14 Train Loss 237.90639 Test MSE 236.9493982136761 Test RE 0.25912957076635285 Lambda1 0.10268903\n",
      "15 Train Loss 223.90715 Test MSE 219.29265734646 Test RE 0.249287909560766 Lambda1 0.1497401\n",
      "16 Train Loss 198.94334 Test MSE 188.22650018718522 Test RE 0.23095616218701817 Lambda1 0.25263634\n",
      "17 Train Loss 181.77097 Test MSE 169.3838240653657 Test RE 0.21909130254380124 Lambda1 0.33945945\n",
      "18 Train Loss 159.49048 Test MSE 146.0796137000735 Test RE 0.20346231260530456 Lambda1 0.4003754\n",
      "19 Train Loss 143.32724 Test MSE 136.40421566584033 Test RE 0.19660885183140636 Lambda1 0.45700732\n",
      "20 Train Loss 134.89961 Test MSE 128.74401176005247 Test RE 0.19100849804101594 Lambda1 0.4981818\n",
      "21 Train Loss 131.02235 Test MSE 125.41756926037841 Test RE 0.18852474425921636 Lambda1 0.5142312\n",
      "22 Train Loss 123.67196 Test MSE 116.43467321688479 Test RE 0.18164788056116554 Lambda1 0.54232466\n",
      "23 Train Loss 114.73724 Test MSE 106.668788663613 Test RE 0.17386327349498296 Lambda1 0.59198654\n",
      "24 Train Loss 109.227005 Test MSE 100.93033552260334 Test RE 0.1691219701146328 Lambda1 0.6198843\n",
      "25 Train Loss 101.98424 Test MSE 95.9334439915881 Test RE 0.16488235756071062 Lambda1 0.6538907\n",
      "26 Train Loss 98.1384 Test MSE 92.77083466148238 Test RE 0.1621417673832131 Lambda1 0.6971245\n",
      "27 Train Loss 93.09395 Test MSE 88.26091430930823 Test RE 0.15815152414094324 Lambda1 0.735698\n",
      "28 Train Loss 89.28267 Test MSE 84.11775540557606 Test RE 0.15439492055744172 Lambda1 0.76764697\n",
      "29 Train Loss 86.20178 Test MSE 82.0194063198938 Test RE 0.15245703922180698 Lambda1 0.7840934\n",
      "30 Train Loss 83.7205 Test MSE 79.62880871515483 Test RE 0.15021879707052832 Lambda1 0.79986995\n",
      "31 Train Loss 80.3193 Test MSE 75.29526891637133 Test RE 0.14607403131640811 Lambda1 0.82280326\n",
      "32 Train Loss 77.12832 Test MSE 72.25931181385126 Test RE 0.14309882943072216 Lambda1 0.8473812\n",
      "33 Train Loss 74.478004 Test MSE 70.9150045679581 Test RE 0.1417614798703713 Lambda1 0.8654064\n",
      "34 Train Loss 71.57444 Test MSE 67.4880742594574 Test RE 0.13829379311536819 Lambda1 0.8980611\n",
      "35 Train Loss 69.21734 Test MSE 64.18349941834464 Test RE 0.13486549994691488 Lambda1 0.9358251\n",
      "36 Train Loss 66.11342 Test MSE 60.77809999666191 Test RE 0.13123894492992716 Lambda1 0.9751347\n",
      "37 Train Loss 63.577377 Test MSE 58.83982722342412 Test RE 0.12912932030072244 Lambda1 1.0096927\n",
      "38 Train Loss 62.01762 Test MSE 56.70923399004252 Test RE 0.1267698747206038 Lambda1 1.0596651\n",
      "39 Train Loss 59.615547 Test MSE 54.49573065942301 Test RE 0.12427117668193548 Lambda1 1.1047581\n",
      "40 Train Loss 58.122448 Test MSE 52.89435255760313 Test RE 0.12243168409893687 Lambda1 1.1309228\n",
      "41 Train Loss 56.30764 Test MSE 50.61074387051657 Test RE 0.11975965353270038 Lambda1 1.1563278\n",
      "42 Train Loss 54.958073 Test MSE 49.56850979102731 Test RE 0.11852012530099798 Lambda1 1.1656272\n",
      "43 Train Loss 53.98619 Test MSE 49.471017658734645 Test RE 0.11840351430231015 Lambda1 1.1629732\n",
      "44 Train Loss 53.174294 Test MSE 48.53719159129783 Test RE 0.11728068466547342 Lambda1 1.190405\n",
      "45 Train Loss 52.118107 Test MSE 47.75376848479916 Test RE 0.11633033946764434 Lambda1 1.2178742\n",
      "46 Train Loss 50.736195 Test MSE 45.88129826933949 Test RE 0.1140268218322296 Lambda1 1.253649\n",
      "47 Train Loss 49.262184 Test MSE 45.553991155329115 Test RE 0.11361937270557057 Lambda1 1.2665893\n",
      "48 Train Loss 48.48021 Test MSE 44.48349440606031 Test RE 0.11227643607222965 Lambda1 1.2794677\n",
      "49 Train Loss 46.86239 Test MSE 43.61698030071134 Test RE 0.11117751642774913 Lambda1 1.3077587\n",
      "50 Train Loss 46.502293 Test MSE 43.48608269982057 Test RE 0.11101056533402251 Lambda1 1.3137286\n",
      "51 Train Loss 45.713913 Test MSE 42.9309993800204 Test RE 0.11029978600247393 Lambda1 1.3134586\n",
      "52 Train Loss 44.8699 Test MSE 42.84921437752938 Test RE 0.1101946735107422 Lambda1 1.3091252\n",
      "53 Train Loss 44.213238 Test MSE 42.64692435211942 Test RE 0.10993425269490006 Lambda1 1.3176386\n",
      "54 Train Loss 43.71718 Test MSE 42.17051933132502 Test RE 0.10931849531473696 Lambda1 1.3375582\n",
      "55 Train Loss 43.233166 Test MSE 41.87198321034419 Test RE 0.10893086096428409 Lambda1 1.3584697\n",
      "56 Train Loss 42.30635 Test MSE 41.541414547394645 Test RE 0.10850001820307245 Lambda1 1.3818346\n",
      "57 Train Loss 41.93645 Test MSE 41.167276028406846 Test RE 0.10801031591431247 Lambda1 1.3954835\n",
      "58 Train Loss 41.33013 Test MSE 40.718711429149444 Test RE 0.10742025614842214 Lambda1 1.4135394\n",
      "59 Train Loss 40.98437 Test MSE 40.43150102570771 Test RE 0.1070407400616996 Lambda1 1.4255339\n",
      "60 Train Loss 40.528976 Test MSE 40.40689561324503 Test RE 0.10700816419480107 Lambda1 1.433077\n",
      "61 Train Loss 40.21473 Test MSE 39.99504050137829 Test RE 0.10646141669648561 Lambda1 1.4454105\n",
      "62 Train Loss 39.986435 Test MSE 39.63321516811272 Test RE 0.10597875741351827 Lambda1 1.4370081\n",
      "63 Train Loss 39.775288 Test MSE 39.54572937405933 Test RE 0.10586172479479469 Lambda1 1.431914\n",
      "64 Train Loss 39.548847 Test MSE 39.171391887170984 Test RE 0.1053594930970171 Lambda1 1.4438827\n",
      "65 Train Loss 39.31368 Test MSE 38.94515951126556 Test RE 0.10505480334068343 Lambda1 1.4399137\n",
      "66 Train Loss 39.133446 Test MSE 38.68264143213755 Test RE 0.10470013259364673 Lambda1 1.436389\n",
      "67 Train Loss 38.851112 Test MSE 38.78706336656685 Test RE 0.1048413538449248 Lambda1 1.435453\n",
      "68 Train Loss 38.492756 Test MSE 38.551905688559785 Test RE 0.1045230553033889 Lambda1 1.4448526\n",
      "69 Train Loss 38.049503 Test MSE 38.31586574118614 Test RE 0.10420258481014699 Lambda1 1.4515278\n",
      "70 Train Loss 37.72159 Test MSE 37.83838401916048 Test RE 0.10355127744932868 Lambda1 1.4548378\n",
      "71 Train Loss 37.54435 Test MSE 37.69979236459688 Test RE 0.10336146347300403 Lambda1 1.4552283\n",
      "72 Train Loss 37.382977 Test MSE 37.545244170655494 Test RE 0.10314938362310251 Lambda1 1.4518319\n",
      "73 Train Loss 37.08771 Test MSE 37.36878737202355 Test RE 0.10290670513253959 Lambda1 1.4472243\n",
      "74 Train Loss 36.73204 Test MSE 37.30055519942089 Test RE 0.10281271283743251 Lambda1 1.4445583\n",
      "75 Train Loss 36.404564 Test MSE 36.76863140064708 Test RE 0.1020770012327751 Lambda1 1.4452468\n",
      "76 Train Loss 36.22012 Test MSE 36.58276978098352 Test RE 0.10181868004339317 Lambda1 1.4470417\n",
      "77 Train Loss 36.00949 Test MSE 36.30666018632714 Test RE 0.10143371244896597 Lambda1 1.4503113\n",
      "78 Train Loss 35.803707 Test MSE 36.2109939452698 Test RE 0.10129998791168118 Lambda1 1.4527153\n",
      "79 Train Loss 35.59675 Test MSE 36.08779554179392 Test RE 0.10112751777904712 Lambda1 1.4563333\n",
      "80 Train Loss 35.321636 Test MSE 35.99636766169396 Test RE 0.10099933402700718 Lambda1 1.4620075\n",
      "81 Train Loss 35.21691 Test MSE 36.03003346615431 Test RE 0.1010465530855229 Lambda1 1.4601884\n",
      "82 Train Loss 35.143497 Test MSE 35.95839292645427 Test RE 0.10094604482997363 Lambda1 1.4590262\n",
      "83 Train Loss 35.04147 Test MSE 35.878038578847 Test RE 0.10083319231424186 Lambda1 1.4609233\n",
      "84 Train Loss 34.898994 Test MSE 35.95891521904246 Test RE 0.10094677794365421 Lambda1 1.4634994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 Train Loss 34.823425 Test MSE 35.87755140565859 Test RE 0.10083250772561185 Lambda1 1.4683388\n",
      "86 Train Loss 34.781055 Test MSE 35.86549407997326 Test RE 0.1008155629718526 Lambda1 1.4684489\n",
      "87 Train Loss 34.657528 Test MSE 35.90404457891941 Test RE 0.10086972988845798 Lambda1 1.4696169\n",
      "88 Train Loss 34.377705 Test MSE 35.68825808331098 Test RE 0.10056615505702407 Lambda1 1.4635357\n",
      "89 Train Loss 34.202415 Test MSE 35.69951551046909 Test RE 0.10058201499194289 Lambda1 1.4665365\n",
      "90 Train Loss 34.134163 Test MSE 35.70352321484114 Test RE 0.10058766061013684 Lambda1 1.46663\n",
      "91 Train Loss 34.1002 Test MSE 35.64700666153574 Test RE 0.10050801693837974 Lambda1 1.4643637\n",
      "92 Train Loss 34.022114 Test MSE 35.624372457024634 Test RE 0.10047610289488319 Lambda1 1.4637463\n",
      "93 Train Loss 33.864304 Test MSE 35.605314123934484 Test RE 0.10044922293767503 Lambda1 1.4577485\n",
      "94 Train Loss 33.73312 Test MSE 35.541970930300955 Test RE 0.10035983168865208 Lambda1 1.4588165\n",
      "95 Train Loss 33.508877 Test MSE 35.34435891341071 Test RE 0.1000804443634802 Lambda1 1.448109\n",
      "96 Train Loss 33.453716 Test MSE 35.32783258430597 Test RE 0.10005704380117468 Lambda1 1.4506252\n",
      "97 Train Loss 33.40732 Test MSE 35.324100102619774 Test RE 0.10005175801202403 Lambda1 1.4483731\n",
      "98 Train Loss 33.39053 Test MSE 35.32584891489765 Test RE 0.10005423464379627 Lambda1 1.4497615\n",
      "99 Train Loss 33.330338 Test MSE 35.27426356890168 Test RE 0.09998115475916144 Lambda1 1.4512708\n",
      "100 Train Loss 33.14425 Test MSE 35.166810382455026 Test RE 0.09982875628685553 Lambda1 1.4424533\n",
      "101 Train Loss 32.978565 Test MSE 34.96335756605371 Test RE 0.09953956453881767 Lambda1 1.427553\n",
      "102 Train Loss 32.67124 Test MSE 34.834316597437095 Test RE 0.09935570697673826 Lambda1 1.4119221\n",
      "103 Train Loss 32.507008 Test MSE 34.67583309001612 Test RE 0.09912943310023893 Lambda1 1.4000381\n",
      "104 Train Loss 32.403996 Test MSE 34.58261371386858 Test RE 0.09899609812352016 Lambda1 1.3919309\n",
      "105 Train Loss 32.269768 Test MSE 34.42870612239256 Test RE 0.09877556475149028 Lambda1 1.3700458\n",
      "106 Train Loss 32.177128 Test MSE 34.32027716887235 Test RE 0.09861990137941089 Lambda1 1.3511032\n",
      "107 Train Loss 32.104965 Test MSE 34.23479562377171 Test RE 0.0984970084709616 Lambda1 1.3313774\n",
      "108 Train Loss 32.06605 Test MSE 34.18393651182798 Test RE 0.09842381784450197 Lambda1 1.3244582\n",
      "109 Train Loss 31.952158 Test MSE 34.08087675800445 Test RE 0.09827533887525884 Lambda1 1.3092203\n",
      "110 Train Loss 31.817577 Test MSE 33.97522397458965 Test RE 0.09812289088080788 Lambda1 1.2894348\n",
      "111 Train Loss 31.67421 Test MSE 33.89529600789086 Test RE 0.0980074040603005 Lambda1 1.2807362\n",
      "112 Train Loss 31.554209 Test MSE 33.82355513199194 Test RE 0.09790363064318844 Lambda1 1.2731599\n",
      "113 Train Loss 31.403849 Test MSE 33.77420320103621 Test RE 0.09783217900271499 Lambda1 1.2800318\n",
      "114 Train Loss 31.32534 Test MSE 33.756327464831784 Test RE 0.09780628566519328 Lambda1 1.2801902\n",
      "115 Train Loss 31.268764 Test MSE 33.723514633999336 Test RE 0.09775873781822116 Lambda1 1.2751088\n",
      "116 Train Loss 31.20429 Test MSE 33.75985357152402 Test RE 0.09781139383925394 Lambda1 1.2727116\n",
      "117 Train Loss 31.140736 Test MSE 33.781568678927485 Test RE 0.0978428460414817 Lambda1 1.2771333\n",
      "118 Train Loss 31.043869 Test MSE 33.769916426247384 Test RE 0.09782597015413938 Lambda1 1.273212\n",
      "119 Train Loss 30.968178 Test MSE 33.75666488958671 Test RE 0.09780677449472508 Lambda1 1.2685336\n",
      "120 Train Loss 30.895636 Test MSE 33.70219750792324 Test RE 0.0977278355711193 Lambda1 1.2694027\n",
      "121 Train Loss 30.8012 Test MSE 33.66876354013373 Test RE 0.09767934852418175 Lambda1 1.2666163\n",
      "122 Train Loss 30.746607 Test MSE 33.64449020548177 Test RE 0.09764413144518919 Lambda1 1.2642273\n",
      "123 Train Loss 30.658932 Test MSE 33.609859213307836 Test RE 0.09759386491201685 Lambda1 1.2589519\n",
      "124 Train Loss 30.615139 Test MSE 33.559268765313135 Test RE 0.09752038681946006 Lambda1 1.2513218\n",
      "125 Train Loss 30.582268 Test MSE 33.52476570389943 Test RE 0.09747024244341952 Lambda1 1.2445865\n",
      "126 Train Loss 30.545965 Test MSE 33.461751141604026 Test RE 0.09737859476948982 Lambda1 1.236954\n",
      "127 Train Loss 30.528357 Test MSE 33.42129585088054 Test RE 0.09731971155722273 Lambda1 1.2289623\n",
      "128 Train Loss 30.492455 Test MSE 33.38432475422283 Test RE 0.09726586846159146 Lambda1 1.2174555\n",
      "129 Train Loss 30.468676 Test MSE 33.33652525285821 Test RE 0.09719621113834924 Lambda1 1.212143\n",
      "130 Train Loss 30.398912 Test MSE 33.249360901729695 Test RE 0.09706905946634231 Lambda1 1.1977588\n",
      "131 Train Loss 30.27099 Test MSE 33.145096426033845 Test RE 0.0969167437358803 Lambda1 1.179966\n",
      "132 Train Loss 30.211794 Test MSE 33.08194870730414 Test RE 0.0968243772972636 Lambda1 1.1749198\n",
      "133 Train Loss 30.1655 Test MSE 33.062492546773 Test RE 0.0967959009262696 Lambda1 1.1661031\n",
      "134 Train Loss 30.123062 Test MSE 33.01275491387069 Test RE 0.09672306596107431 Lambda1 1.1566552\n",
      "135 Train Loss 30.088787 Test MSE 32.91618344252294 Test RE 0.09658149144155925 Lambda1 1.1412692\n",
      "136 Train Loss 30.023497 Test MSE 32.81954283090299 Test RE 0.09643960750572711 Lambda1 1.1292278\n",
      "137 Train Loss 29.90614 Test MSE 32.785209134247665 Test RE 0.09638914984590571 Lambda1 1.1272438\n",
      "138 Train Loss 29.86392 Test MSE 32.79824935534703 Test RE 0.0964083171914553 Lambda1 1.1284724\n",
      "139 Train Loss 29.816853 Test MSE 32.74883711203317 Test RE 0.09633566778360807 Lambda1 1.1191646\n",
      "140 Train Loss 29.783302 Test MSE 32.71549841890607 Test RE 0.09628661988827375 Lambda1 1.1147332\n",
      "141 Train Loss 29.750744 Test MSE 32.715157432638215 Test RE 0.09628611810009534 Lambda1 1.1059365\n",
      "142 Train Loss 29.722925 Test MSE 32.703227736998095 Test RE 0.09626856096590075 Lambda1 1.1010288\n",
      "143 Train Loss 29.626776 Test MSE 32.6964676247126 Test RE 0.09625861057321242 Lambda1 1.1027347\n",
      "144 Train Loss 29.54799 Test MSE 32.624892010950056 Test RE 0.09615319331636689 Lambda1 1.084718\n",
      "145 Train Loss 29.475475 Test MSE 32.55601239788643 Test RE 0.0960516375141525 Lambda1 1.0676067\n",
      "146 Train Loss 29.437641 Test MSE 32.54970801126916 Test RE 0.09604233698973352 Lambda1 1.0708443\n",
      "147 Train Loss 29.405428 Test MSE 32.52218168426692 Test RE 0.0960017183160912 Lambda1 1.0604596\n",
      "148 Train Loss 29.336784 Test MSE 32.44999186821428 Test RE 0.09589511113076205 Lambda1 1.0427593\n",
      "149 Train Loss 29.281595 Test MSE 32.422774505265295 Test RE 0.09585488678231753 Lambda1 1.0246985\n",
      "150 Train Loss 29.257477 Test MSE 32.40497264187029 Test RE 0.09582856840092417 Lambda1 1.0214628\n",
      "151 Train Loss 29.21767 Test MSE 32.36663362199412 Test RE 0.09577186320382694 Lambda1 1.0126405\n",
      "152 Train Loss 29.198067 Test MSE 32.37125881316753 Test RE 0.09577870585786688 Lambda1 1.0122284\n",
      "153 Train Loss 29.178621 Test MSE 32.36211785768158 Test RE 0.09576518196763444 Lambda1 1.0031748\n",
      "154 Train Loss 29.155317 Test MSE 32.35336427855469 Test RE 0.09575222940383558 Lambda1 1.0036861\n",
      "155 Train Loss 29.098457 Test MSE 32.32668387144916 Test RE 0.09571273991912276 Lambda1 1.0064926\n",
      "156 Train Loss 29.07537 Test MSE 32.31944504441855 Test RE 0.0957020229714319 Lambda1 1.0065719\n",
      "157 Train Loss 29.065918 Test MSE 32.316269365655415 Test RE 0.09569732105981825 Lambda1 1.0074017\n",
      "158 Train Loss 29.051735 Test MSE 32.3021888093355 Test RE 0.09567647059727494 Lambda1 1.0104088\n",
      "159 Train Loss 29.03802 Test MSE 32.293514544621715 Test RE 0.09566362349949703 Lambda1 1.0077565\n",
      "160 Train Loss 29.018686 Test MSE 32.30035094934752 Test RE 0.09567374876224152 Lambda1 1.0029844\n",
      "161 Train Loss 29.00936 Test MSE 32.3054514571215 Test RE 0.09568130232469894 Lambda1 1.0005662\n",
      "162 Train Loss 28.993906 Test MSE 32.29527874480612 Test RE 0.09566623652380982 Lambda1 0.9988295\n",
      "163 Train Loss 28.945524 Test MSE 32.29186385023238 Test RE 0.09566117852831976 Lambda1 0.9838566\n",
      "164 Train Loss 28.914139 Test MSE 32.28920217596547 Test RE 0.09565723598499981 Lambda1 0.9777785\n",
      "165 Train Loss 28.889126 Test MSE 32.27677439325667 Test RE 0.09563882546886468 Lambda1 0.9713818\n",
      "166 Train Loss 28.877714 Test MSE 32.27575594925616 Test RE 0.09563731658890054 Lambda1 0.9659455\n",
      "167 Train Loss 28.871698 Test MSE 32.276337196583654 Test RE 0.09563817774122327 Lambda1 0.96341443\n",
      "168 Train Loss 28.86515 Test MSE 32.26426203485374 Test RE 0.09562028608131785 Lambda1 0.9631632\n",
      "169 Train Loss 28.86055 Test MSE 32.274206493456845 Test RE 0.09563502094052635 Lambda1 0.96304125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 Train Loss 28.843233 Test MSE 32.29177413248366 Test RE 0.09566104563862997 Lambda1 0.9556026\n",
      "171 Train Loss 28.815449 Test MSE 32.31533835258396 Test RE 0.09569594255759217 Lambda1 0.9481754\n",
      "172 Train Loss 28.805632 Test MSE 32.32440122424033 Test RE 0.09570936063227074 Lambda1 0.94543946\n",
      "173 Train Loss 28.795906 Test MSE 32.30171165970282 Test RE 0.09567576395535571 Lambda1 0.9439663\n",
      "174 Train Loss 28.770992 Test MSE 32.317434877141835 Test RE 0.09569904674355993 Lambda1 0.93831855\n",
      "175 Train Loss 28.74667 Test MSE 32.31686979528471 Test RE 0.09569821007394709 Lambda1 0.93270546\n",
      "176 Train Loss 28.71908 Test MSE 32.313895314992784 Test RE 0.09569380588827142 Lambda1 0.93124026\n",
      "177 Train Loss 28.699205 Test MSE 32.30941302768337 Test RE 0.09568716877437997 Lambda1 0.93344635\n",
      "178 Train Loss 28.684208 Test MSE 32.28184811738182 Test RE 0.09564634211136176 Lambda1 0.93552905\n",
      "179 Train Loss 28.67781 Test MSE 32.26764962417209 Test RE 0.09562530578016112 Lambda1 0.93854845\n",
      "180 Train Loss 28.675425 Test MSE 32.25121888634945 Test RE 0.09560095640220238 Lambda1 0.93977535\n",
      "181 Train Loss 28.672457 Test MSE 32.241571032871555 Test RE 0.09558665596580056 Lambda1 0.9409351\n",
      "182 Train Loss 28.667828 Test MSE 32.238984400300055 Test RE 0.09558282159128095 Lambda1 0.94073397\n",
      "183 Train Loss 28.659163 Test MSE 32.242756463932885 Test RE 0.09558841317467538 Lambda1 0.94052035\n",
      "184 Train Loss 28.62874 Test MSE 32.19054847646385 Test RE 0.09551099267736561 Lambda1 0.9462102\n",
      "185 Train Loss 28.61196 Test MSE 32.20130315811633 Test RE 0.09552694618839397 Lambda1 0.94883114\n",
      "186 Train Loss 28.606113 Test MSE 32.2100816707866 Test RE 0.09553996627268618 Lambda1 0.9480101\n",
      "187 Train Loss 28.600111 Test MSE 32.21470879291305 Test RE 0.09554682839773511 Lambda1 0.9461604\n",
      "188 Train Loss 28.588585 Test MSE 32.18596193284294 Test RE 0.09550418818101439 Lambda1 0.9501833\n",
      "189 Train Loss 28.538412 Test MSE 32.11937192846787 Test RE 0.09540534202707614 Lambda1 0.959838\n",
      "190 Train Loss 28.505173 Test MSE 32.093916675168416 Test RE 0.09536752920004192 Lambda1 0.96623164\n",
      "191 Train Loss 28.489502 Test MSE 32.04589211377592 Test RE 0.09529614965427548 Lambda1 0.9703138\n",
      "192 Train Loss 28.479687 Test MSE 32.02002350592739 Test RE 0.09525767863258454 Lambda1 0.9731231\n",
      "193 Train Loss 28.473532 Test MSE 31.993155814289615 Test RE 0.09521770534769929 Lambda1 0.97858787\n",
      "194 Train Loss 28.467485 Test MSE 31.993747584492684 Test RE 0.09521858595386079 Lambda1 0.98188454\n",
      "195 Train Loss 28.461626 Test MSE 31.97150348889756 Test RE 0.09518547917882682 Lambda1 0.98597324\n",
      "196 Train Loss 28.454073 Test MSE 31.956823749780206 Test RE 0.09516362442883011 Lambda1 0.9900959\n",
      "197 Train Loss 28.446907 Test MSE 31.936331377556815 Test RE 0.09513310761088326 Lambda1 0.9937127\n",
      "198 Train Loss 28.437706 Test MSE 31.899094611045097 Test RE 0.09507763031650444 Lambda1 0.9971304\n",
      "199 Train Loss 28.429083 Test MSE 31.85335083214793 Test RE 0.0950094344871342 Lambda1 0.9988092\n",
      "Training time: 998.67\n",
      "Training time: 998.67\n",
      "inv_HT_stan\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 853.22314 Test MSE 856.6101567708528 Test RE 0.4926979784714158 Lambda1 -0.035095625\n",
      "1 Train Loss 786.81445 Test MSE 791.1935746722506 Test RE 0.47351151684580833 Lambda1 0.16357261\n",
      "2 Train Loss 586.87836 Test MSE 569.7267763560269 Test RE 0.4018116146783073 Lambda1 0.048023604\n",
      "3 Train Loss 381.2681 Test MSE 380.30631262657465 Test RE 0.328288702706515 Lambda1 0.006584961\n",
      "4 Train Loss 278.53873 Test MSE 283.8741247299596 Test RE 0.2836299410603911 Lambda1 0.00010108782\n",
      "5 Train Loss 272.54883 Test MSE 280.880890128744 Test RE 0.2821306484285253 Lambda1 0.0007898623\n",
      "6 Train Loss 271.108 Test MSE 279.18541884200977 Test RE 0.281277851975326 Lambda1 0.00084436627\n",
      "7 Train Loss 270.6536 Test MSE 278.4825484659912 Test RE 0.28092355972820265 Lambda1 0.0013381571\n",
      "8 Train Loss 270.16257 Test MSE 278.0439236911564 Test RE 0.2807022377969371 Lambda1 0.0026365307\n",
      "9 Train Loss 269.13345 Test MSE 276.3194931159365 Test RE 0.2798304252174479 Lambda1 0.01003303\n",
      "10 Train Loss 266.6167 Test MSE 273.000940198395 Test RE 0.27814499015506494 Lambda1 0.019701988\n",
      "11 Train Loss 263.2981 Test MSE 269.26327539453297 Test RE 0.2762343819874289 Lambda1 0.029771598\n",
      "12 Train Loss 259.10022 Test MSE 262.7744230297161 Test RE 0.27288566119721347 Lambda1 0.04931934\n",
      "13 Train Loss 253.20541 Test MSE 258.645412718215 Test RE 0.27073322756058776 Lambda1 0.064720064\n",
      "14 Train Loss 247.13414 Test MSE 248.80216027868286 Test RE 0.2655316193791327 Lambda1 0.098316394\n",
      "15 Train Loss 241.49927 Test MSE 244.57393762547034 Test RE 0.2632656869457164 Lambda1 0.122575365\n",
      "16 Train Loss 236.16966 Test MSE 238.85457821149026 Test RE 0.26016924438213235 Lambda1 0.1638762\n",
      "17 Train Loss 231.50067 Test MSE 232.87179346673946 Test RE 0.2568902459797421 Lambda1 0.1985642\n",
      "18 Train Loss 214.71184 Test MSE 210.2192523494516 Test RE 0.24407618933477307 Lambda1 0.23731814\n",
      "19 Train Loss 199.05702 Test MSE 194.12966472549763 Test RE 0.23454983019768885 Lambda1 0.26281255\n",
      "20 Train Loss 183.57718 Test MSE 175.14772265960937 Test RE 0.22278780673469864 Lambda1 0.3262666\n",
      "21 Train Loss 171.36209 Test MSE 163.49902338416416 Test RE 0.21525178028565567 Lambda1 0.38262355\n",
      "22 Train Loss 160.40648 Test MSE 155.09868594972963 Test RE 0.2096492097898763 Lambda1 0.41648033\n",
      "23 Train Loss 148.52718 Test MSE 141.65291877982398 Test RE 0.20035580707379325 Lambda1 0.48646423\n",
      "24 Train Loss 139.88264 Test MSE 130.03301351621516 Test RE 0.19196231748334974 Lambda1 0.53056663\n",
      "25 Train Loss 131.35725 Test MSE 120.25131717325442 Test RE 0.1846010180153085 Lambda1 0.5775348\n",
      "26 Train Loss 125.8232 Test MSE 118.09424743530504 Test RE 0.18293783796081686 Lambda1 0.5816246\n",
      "27 Train Loss 118.07137 Test MSE 109.75340489280676 Test RE 0.1763592209799709 Lambda1 0.6237544\n",
      "28 Train Loss 111.41201 Test MSE 104.35849454697326 Test RE 0.17197015129693727 Lambda1 0.6581903\n",
      "29 Train Loss 106.96126 Test MSE 98.0318779760175 Test RE 0.16667590881573363 Lambda1 0.6806447\n",
      "30 Train Loss 101.21994 Test MSE 94.52808796094416 Test RE 0.1636701978261332 Lambda1 0.68643403\n",
      "31 Train Loss 98.045525 Test MSE 91.10664526917326 Test RE 0.1606808787915351 Lambda1 0.7098274\n",
      "32 Train Loss 94.476364 Test MSE 87.8721494042189 Test RE 0.15780283289399605 Lambda1 0.7244793\n",
      "33 Train Loss 92.0187 Test MSE 83.78883781018499 Test RE 0.15409276706894115 Lambda1 0.742217\n",
      "34 Train Loss 89.22137 Test MSE 80.39768083800615 Test RE 0.15094228885134675 Lambda1 0.7603935\n",
      "35 Train Loss 86.16717 Test MSE 78.20167781604397 Test RE 0.14886657825992336 Lambda1 0.77089596\n",
      "36 Train Loss 83.21895 Test MSE 75.37616931234018 Test RE 0.14615248428269828 Lambda1 0.7940128\n",
      "37 Train Loss 78.03993 Test MSE 72.7392361824751 Test RE 0.14357325243909558 Lambda1 0.820261\n",
      "38 Train Loss 75.51964 Test MSE 71.3635994691018 Test RE 0.1422091511650499 Lambda1 0.8415611\n",
      "39 Train Loss 70.13082 Test MSE 65.26621059889416 Test RE 0.13599826554512495 Lambda1 0.8852966\n",
      "40 Train Loss 68.68063 Test MSE 63.61474205079901 Test RE 0.13426661989896999 Lambda1 0.90202147\n",
      "41 Train Loss 65.67703 Test MSE 59.28770879131289 Test RE 0.12961984690909925 Lambda1 0.9310216\n",
      "42 Train Loss 62.317635 Test MSE 56.1694343370441 Test RE 0.12616508823677167 Lambda1 0.948131\n",
      "43 Train Loss 60.59723 Test MSE 55.39619446604042 Test RE 0.12529367167135735 Lambda1 0.95679075\n",
      "44 Train Loss 58.025368 Test MSE 52.89429226284914 Test RE 0.12243161431842137 Lambda1 0.97161096\n",
      "45 Train Loss 56.91923 Test MSE 50.86495217783533 Test RE 0.1200600419847152 Lambda1 0.9815411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 Train Loss 56.02681 Test MSE 49.690917389958436 Test RE 0.11866637559649564 Lambda1 0.9909264\n",
      "47 Train Loss 53.67382 Test MSE 48.427252270425775 Test RE 0.11714778587236684 Lambda1 1.010618\n",
      "48 Train Loss 51.95685 Test MSE 46.16778752338426 Test RE 0.1143822675574138 Lambda1 1.0298637\n",
      "49 Train Loss 50.694706 Test MSE 45.50230860548277 Test RE 0.113554901896042 Lambda1 1.0381596\n",
      "50 Train Loss 49.99433 Test MSE 44.65143229673929 Test RE 0.11248817421711607 Lambda1 1.040215\n",
      "51 Train Loss 48.19606 Test MSE 43.621942027613834 Test RE 0.11118383984563956 Lambda1 1.0568402\n",
      "52 Train Loss 46.247044 Test MSE 40.98787398944118 Test RE 0.10777471096476092 Lambda1 1.0822045\n",
      "53 Train Loss 45.442917 Test MSE 39.98624176943501 Test RE 0.10644970553204368 Lambda1 1.0890626\n",
      "54 Train Loss 44.701855 Test MSE 39.17324002590202 Test RE 0.1053619785417996 Lambda1 1.0949408\n",
      "55 Train Loss 43.47062 Test MSE 38.65318837805088 Test RE 0.1046602655399889 Lambda1 1.10282\n",
      "56 Train Loss 42.256275 Test MSE 38.785269438758306 Test RE 0.10483892932527114 Lambda1 1.1042669\n",
      "57 Train Loss 39.755936 Test MSE 37.284032930336636 Test RE 0.10278993988607896 Lambda1 1.1107411\n",
      "58 Train Loss 39.27899 Test MSE 37.26287444326814 Test RE 0.10276076937944942 Lambda1 1.1146204\n",
      "59 Train Loss 38.32084 Test MSE 36.61752351208543 Test RE 0.10186703257427844 Lambda1 1.1208584\n",
      "60 Train Loss 37.20352 Test MSE 34.74856307801494 Test RE 0.0992333369628635 Lambda1 1.1428176\n",
      "61 Train Loss 36.367844 Test MSE 34.44864007431921 Test RE 0.09880415575383131 Lambda1 1.1380237\n",
      "62 Train Loss 35.563187 Test MSE 33.98261675316267 Test RE 0.09813356573830823 Lambda1 1.1485041\n",
      "63 Train Loss 34.328182 Test MSE 33.398354882927585 Test RE 0.09728630483877895 Lambda1 1.163283\n",
      "64 Train Loss 33.723564 Test MSE 33.02198528905385 Test RE 0.0967365869136977 Lambda1 1.1754109\n",
      "65 Train Loss 33.361008 Test MSE 32.79470303235472 Test RE 0.09640310495786039 Lambda1 1.1815032\n",
      "66 Train Loss 32.931267 Test MSE 32.70902942302021 Test RE 0.09627709980494568 Lambda1 1.182631\n",
      "67 Train Loss 32.345253 Test MSE 32.131222554723465 Test RE 0.09542294057766984 Lambda1 1.1904187\n",
      "68 Train Loss 32.077908 Test MSE 31.986919837725488 Test RE 0.09520842517041109 Lambda1 1.2033765\n",
      "69 Train Loss 31.768261 Test MSE 32.009100156227255 Test RE 0.09524142908653971 Lambda1 1.2057316\n",
      "70 Train Loss 31.436317 Test MSE 31.857721551781015 Test RE 0.09501595256680137 Lambda1 1.2061933\n",
      "71 Train Loss 31.286705 Test MSE 31.74782999062158 Test RE 0.09485193469721485 Lambda1 1.2058041\n",
      "72 Train Loss 30.951687 Test MSE 31.41519648809624 Test RE 0.0943537272654917 Lambda1 1.2165476\n",
      "73 Train Loss 30.745228 Test MSE 31.458523583795234 Test RE 0.09441877006094462 Lambda1 1.2174413\n",
      "74 Train Loss 30.56129 Test MSE 31.370190499597502 Test RE 0.09428611659889158 Lambda1 1.221856\n",
      "75 Train Loss 30.376919 Test MSE 31.26603914881573 Test RE 0.09412946803206206 Lambda1 1.2250897\n",
      "76 Train Loss 30.134659 Test MSE 31.161621554508166 Test RE 0.09397215689244907 Lambda1 1.2149204\n",
      "77 Train Loss 29.978975 Test MSE 31.085641230148454 Test RE 0.09385752241186897 Lambda1 1.2135167\n",
      "78 Train Loss 29.880962 Test MSE 30.991211249883648 Test RE 0.09371485678366273 Lambda1 1.2104218\n",
      "79 Train Loss 29.682632 Test MSE 30.88337696389321 Test RE 0.09355167373449033 Lambda1 1.2166287\n",
      "80 Train Loss 29.292791 Test MSE 30.869398967331346 Test RE 0.09353050032179909 Lambda1 1.2233666\n",
      "81 Train Loss 29.220036 Test MSE 30.85059454534383 Test RE 0.0935020084336647 Lambda1 1.2197535\n",
      "82 Train Loss 29.13631 Test MSE 30.82098373778347 Test RE 0.09345712543076704 Lambda1 1.2183312\n",
      "83 Train Loss 29.049788 Test MSE 30.729303170534415 Test RE 0.09331802241548738 Lambda1 1.2236738\n",
      "84 Train Loss 29.012648 Test MSE 30.68699417000687 Test RE 0.09325375880087493 Lambda1 1.226987\n",
      "85 Train Loss 28.884129 Test MSE 30.610322988953506 Test RE 0.09313718910121074 Lambda1 1.2267992\n",
      "86 Train Loss 28.837906 Test MSE 30.57730817621001 Test RE 0.09308694891887764 Lambda1 1.2265363\n",
      "87 Train Loss 28.791786 Test MSE 30.549426894969955 Test RE 0.09304449954278997 Lambda1 1.2274588\n",
      "88 Train Loss 28.756243 Test MSE 30.559304336394796 Test RE 0.09305954020659134 Lambda1 1.2248948\n",
      "89 Train Loss 28.666616 Test MSE 30.488105993994694 Test RE 0.09295106999019151 Lambda1 1.222589\n",
      "90 Train Loss 28.570997 Test MSE 30.40593271123596 Test RE 0.09282572195835437 Lambda1 1.2202536\n",
      "91 Train Loss 28.518682 Test MSE 30.418738792868698 Test RE 0.0928452676280393 Lambda1 1.2233255\n",
      "92 Train Loss 28.43521 Test MSE 30.38404698292442 Test RE 0.09279230867264357 Lambda1 1.2244635\n",
      "93 Train Loss 28.388948 Test MSE 30.380949953569328 Test RE 0.092787579417394 Lambda1 1.2246811\n",
      "94 Train Loss 28.274733 Test MSE 30.304973812863025 Test RE 0.09267148602457653 Lambda1 1.2223421\n",
      "95 Train Loss 28.215597 Test MSE 30.290837523469445 Test RE 0.09264986937906369 Lambda1 1.2188525\n",
      "96 Train Loss 28.153414 Test MSE 30.295698447270826 Test RE 0.09265730307726229 Lambda1 1.2173455\n",
      "97 Train Loss 28.079004 Test MSE 30.30679704096961 Test RE 0.0926742736646159 Lambda1 1.2170264\n",
      "98 Train Loss 28.0123 Test MSE 30.300302957061323 Test RE 0.09266434409746384 Lambda1 1.2176045\n",
      "99 Train Loss 27.999361 Test MSE 30.30749427262483 Test RE 0.09267533968068992 Lambda1 1.2165877\n",
      "100 Train Loss 27.9782 Test MSE 30.29998786782094 Test RE 0.09266386229347429 Lambda1 1.2155613\n",
      "101 Train Loss 27.938957 Test MSE 30.241064076706483 Test RE 0.09257371765028209 Lambda1 1.2101941\n",
      "102 Train Loss 27.89366 Test MSE 30.2020347046761 Test RE 0.09251396015328173 Lambda1 1.206481\n",
      "103 Train Loss 27.767155 Test MSE 30.11715139270805 Test RE 0.0923838626804594 Lambda1 1.2027649\n",
      "104 Train Loss 27.689594 Test MSE 30.0830074881397 Test RE 0.09233147989897011 Lambda1 1.1986021\n",
      "105 Train Loss 27.64857 Test MSE 30.035568575269707 Test RE 0.09225865085788035 Lambda1 1.1946139\n",
      "106 Train Loss 27.621063 Test MSE 30.008696661694465 Test RE 0.09221737111244799 Lambda1 1.191758\n",
      "107 Train Loss 27.595278 Test MSE 29.99117989431388 Test RE 0.09219045248191653 Lambda1 1.189987\n",
      "108 Train Loss 27.568407 Test MSE 29.96231649969116 Test RE 0.09214607993730353 Lambda1 1.185722\n",
      "109 Train Loss 27.555738 Test MSE 29.946450814164745 Test RE 0.09212168004964974 Lambda1 1.1834104\n",
      "110 Train Loss 27.531652 Test MSE 29.92026286422851 Test RE 0.09208139137546877 Lambda1 1.178779\n",
      "111 Train Loss 27.494415 Test MSE 29.9109878184448 Test RE 0.09206711801625946 Lambda1 1.1772623\n",
      "112 Train Loss 27.43137 Test MSE 29.890866971422383 Test RE 0.0920361464541015 Lambda1 1.1781218\n",
      "113 Train Loss 27.414957 Test MSE 29.903363291554793 Test RE 0.09205538298161005 Lambda1 1.1803243\n",
      "114 Train Loss 27.393593 Test MSE 29.873914421767434 Test RE 0.09201004368783003 Lambda1 1.1796714\n",
      "115 Train Loss 27.352919 Test MSE 29.857898610235203 Test RE 0.0919853764638222 Lambda1 1.1793355\n",
      "116 Train Loss 27.310238 Test MSE 29.882063365739757 Test RE 0.0920225919859224 Lambda1 1.1815675\n",
      "117 Train Loss 27.27086 Test MSE 29.84018055985116 Test RE 0.09195807977737455 Lambda1 1.1789923\n",
      "118 Train Loss 27.246077 Test MSE 29.806668159006804 Test RE 0.09190642791561256 Lambda1 1.1742883\n",
      "119 Train Loss 27.213295 Test MSE 29.76275480017119 Test RE 0.09183870133299361 Lambda1 1.1699506\n",
      "120 Train Loss 27.151669 Test MSE 29.704304209749342 Test RE 0.09174847674712128 Lambda1 1.1685046\n",
      "121 Train Loss 27.10253 Test MSE 29.701144334370685 Test RE 0.09174359662164933 Lambda1 1.1642065\n",
      "122 Train Loss 27.07515 Test MSE 29.675234399043774 Test RE 0.09170357140802723 Lambda1 1.1609765\n",
      "123 Train Loss 27.058043 Test MSE 29.654139902936585 Test RE 0.09167097209477104 Lambda1 1.1574745\n",
      "124 Train Loss 26.997423 Test MSE 29.594469064424203 Test RE 0.09157869428257262 Lambda1 1.1519815\n",
      "125 Train Loss 26.910454 Test MSE 29.523559625679795 Test RE 0.09146891552107825 Lambda1 1.1428529\n",
      "126 Train Loss 26.844118 Test MSE 29.457631382243125 Test RE 0.09136673009118068 Lambda1 1.1335064\n",
      "127 Train Loss 26.789927 Test MSE 29.390730502301384 Test RE 0.0912629201646004 Lambda1 1.1240541\n",
      "128 Train Loss 26.708725 Test MSE 29.33624919798132 Test RE 0.09117829434595957 Lambda1 1.1157848\n",
      "129 Train Loss 26.662344 Test MSE 29.311346815392508 Test RE 0.09113958730462365 Lambda1 1.1136436\n",
      "130 Train Loss 26.5861 Test MSE 29.236574466814357 Test RE 0.09102326592535571 Lambda1 1.1037197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 Train Loss 26.49245 Test MSE 29.190076448713558 Test RE 0.09095085515746838 Lambda1 1.0976654\n",
      "132 Train Loss 26.43582 Test MSE 29.12849904572837 Test RE 0.09085487263739792 Lambda1 1.0965645\n",
      "133 Train Loss 26.3939 Test MSE 29.122575064752517 Test RE 0.09084563340567285 Lambda1 1.0943569\n",
      "134 Train Loss 26.371252 Test MSE 29.107201209510794 Test RE 0.09082165146063487 Lambda1 1.0932822\n",
      "135 Train Loss 26.339481 Test MSE 29.098731768569454 Test RE 0.09080843712630594 Lambda1 1.0915766\n",
      "136 Train Loss 26.312733 Test MSE 29.101568957191873 Test RE 0.0908128640268812 Lambda1 1.0886236\n",
      "137 Train Loss 26.275106 Test MSE 29.054764155647717 Test RE 0.0907398063079192 Lambda1 1.0787038\n",
      "138 Train Loss 26.242573 Test MSE 29.023000097731494 Test RE 0.09069019219582951 Lambda1 1.0756621\n",
      "139 Train Loss 26.224735 Test MSE 28.987633508840577 Test RE 0.09063491912893629 Lambda1 1.0713931\n",
      "140 Train Loss 26.210915 Test MSE 28.976224051585607 Test RE 0.09061708053908342 Lambda1 1.0701755\n",
      "141 Train Loss 26.185902 Test MSE 28.965371521376014 Test RE 0.0906001094406303 Lambda1 1.069171\n",
      "142 Train Loss 26.118973 Test MSE 28.942260162236284 Test RE 0.09056395748743352 Lambda1 1.0668045\n",
      "143 Train Loss 26.07984 Test MSE 28.92151761461631 Test RE 0.09053149865577917 Lambda1 1.061937\n",
      "144 Train Loss 26.002611 Test MSE 28.875813144475874 Test RE 0.09045993722844699 Lambda1 1.0495937\n",
      "145 Train Loss 25.916924 Test MSE 28.792113253383018 Test RE 0.09032873777908748 Lambda1 1.0364326\n",
      "146 Train Loss 25.886396 Test MSE 28.7572958265494 Test RE 0.09027410536015494 Lambda1 1.0344263\n",
      "147 Train Loss 25.87343 Test MSE 28.743077720474968 Test RE 0.09025178605895502 Lambda1 1.0327519\n",
      "148 Train Loss 25.862648 Test MSE 28.726409019731125 Test RE 0.09022561283219545 Lambda1 1.0292528\n",
      "149 Train Loss 25.847061 Test MSE 28.739428740033052 Test RE 0.09024605707164556 Lambda1 1.0305575\n",
      "150 Train Loss 25.828762 Test MSE 28.74794045607925 Test RE 0.09025942010596442 Lambda1 1.0329726\n",
      "151 Train Loss 25.817368 Test MSE 28.738566744607954 Test RE 0.09024470366493118 Lambda1 1.0304052\n",
      "152 Train Loss 25.787384 Test MSE 28.705947881658386 Test RE 0.09019347432900951 Lambda1 1.0248356\n",
      "153 Train Loss 25.775965 Test MSE 28.711918902931114 Test RE 0.09020285425183416 Lambda1 1.022946\n",
      "154 Train Loss 25.757448 Test MSE 28.692226420151897 Test RE 0.09017191548311525 Lambda1 1.0210598\n",
      "155 Train Loss 25.714855 Test MSE 28.631292063040124 Test RE 0.09007611448188707 Lambda1 1.0150932\n",
      "156 Train Loss 25.696728 Test MSE 28.65410347365942 Test RE 0.09011199051081326 Lambda1 1.0152601\n",
      "157 Train Loss 25.668007 Test MSE 28.649865735135286 Test RE 0.0901053268025232 Lambda1 1.0193955\n",
      "158 Train Loss 25.645027 Test MSE 28.652827744553733 Test RE 0.09010998451944212 Lambda1 1.0187682\n",
      "159 Train Loss 25.630089 Test MSE 28.648342368785908 Test RE 0.09010293123724018 Lambda1 1.015563\n",
      "160 Train Loss 25.626354 Test MSE 28.633440633580246 Test RE 0.09007949419759827 Lambda1 1.0136256\n",
      "161 Train Loss 25.615786 Test MSE 28.63754302481172 Test RE 0.09008594693404216 Lambda1 1.0127299\n",
      "162 Train Loss 25.597748 Test MSE 28.601307471152705 Test RE 0.09002893527957508 Lambda1 1.0058886\n",
      "163 Train Loss 25.582783 Test MSE 28.576288116237627 Test RE 0.08998954969037651 Lambda1 1.0012603\n",
      "164 Train Loss 25.553638 Test MSE 28.56228412505918 Test RE 0.08996749701358102 Lambda1 0.99955475\n",
      "165 Train Loss 25.532871 Test MSE 28.560080328321305 Test RE 0.08996402610942957 Lambda1 0.9966671\n",
      "166 Train Loss 25.496374 Test MSE 28.56050853744948 Test RE 0.0899647005345773 Lambda1 0.9950513\n",
      "167 Train Loss 25.461264 Test MSE 28.541585393289342 Test RE 0.08993489194279086 Lambda1 0.9867606\n",
      "168 Train Loss 25.450365 Test MSE 28.5414196477443 Test RE 0.0899346308092708 Lambda1 0.98256063\n",
      "169 Train Loss 25.447504 Test MSE 28.54113983205714 Test RE 0.08993418995554822 Lambda1 0.9820357\n",
      "170 Train Loss 25.44131 Test MSE 28.53567637554202 Test RE 0.08992558176653942 Lambda1 0.9793799\n",
      "171 Train Loss 25.427383 Test MSE 28.528334372913715 Test RE 0.08991401245374851 Lambda1 0.97712284\n",
      "172 Train Loss 25.415422 Test MSE 28.537445312488607 Test RE 0.08992836898308844 Lambda1 0.97559434\n",
      "173 Train Loss 25.385582 Test MSE 28.527523453202512 Test RE 0.08991273453891818 Lambda1 0.97056454\n",
      "174 Train Loss 25.36177 Test MSE 28.52275585389334 Test RE 0.08990522099146084 Lambda1 0.96983004\n",
      "175 Train Loss 25.339808 Test MSE 28.52637138500258 Test RE 0.0899109189826496 Lambda1 0.9722423\n",
      "176 Train Loss 25.331514 Test MSE 28.512445532910007 Test RE 0.08988897018676427 Lambda1 0.9723564\n",
      "177 Train Loss 25.32697 Test MSE 28.51254185823062 Test RE 0.08988912202531053 Lambda1 0.96927214\n",
      "178 Train Loss 25.322062 Test MSE 28.507063924899935 Test RE 0.08988048669786293 Lambda1 0.96599597\n",
      "179 Train Loss 25.311277 Test MSE 28.51369301279687 Test RE 0.08989093658167983 Lambda1 0.9693915\n",
      "180 Train Loss 25.308203 Test MSE 28.506831706101483 Test RE 0.08988012061349196 Lambda1 0.9701107\n",
      "181 Train Loss 25.304094 Test MSE 28.51131874731913 Test RE 0.08988719400455349 Lambda1 0.9677144\n",
      "182 Train Loss 25.299616 Test MSE 28.507663841751004 Test RE 0.08988143243775064 Lambda1 0.965217\n",
      "183 Train Loss 25.2939 Test MSE 28.506952688695694 Test RE 0.08988031133827566 Lambda1 0.96624714\n",
      "184 Train Loss 25.286907 Test MSE 28.50726672001362 Test RE 0.08988080639566047 Lambda1 0.9665348\n",
      "185 Train Loss 25.274935 Test MSE 28.51382722968206 Test RE 0.08989114814437207 Lambda1 0.96603394\n",
      "186 Train Loss 25.258554 Test MSE 28.504951172241295 Test RE 0.08987715596628879 Lambda1 0.96465296\n",
      "187 Train Loss 25.230366 Test MSE 28.51268496415458 Test RE 0.08988934760410644 Lambda1 0.9642068\n",
      "188 Train Loss 25.211678 Test MSE 28.498504340718476 Test RE 0.08986699184350516 Lambda1 0.96449804\n",
      "189 Train Loss 25.19145 Test MSE 28.47737230757285 Test RE 0.08983366685905578 Lambda1 0.9674329\n",
      "190 Train Loss 25.184574 Test MSE 28.47646377329941 Test RE 0.089832233833295 Lambda1 0.9688215\n",
      "191 Train Loss 25.180115 Test MSE 28.469228931934182 Test RE 0.08982082154467456 Lambda1 0.971574\n",
      "192 Train Loss 25.177244 Test MSE 28.463045931689646 Test RE 0.08981106728824871 Lambda1 0.9709774\n",
      "193 Train Loss 25.174082 Test MSE 28.46757795723467 Test RE 0.0898182170826044 Lambda1 0.9712331\n",
      "194 Train Loss 25.172693 Test MSE 28.461927021025062 Test RE 0.089809301989678 Lambda1 0.9711875\n",
      "195 Train Loss 25.165968 Test MSE 28.44840495890483 Test RE 0.08978796557010046 Lambda1 0.9709108\n",
      "196 Train Loss 25.161203 Test MSE 28.443678078476864 Test RE 0.08978050584416711 Lambda1 0.9698116\n",
      "197 Train Loss 25.153421 Test MSE 28.449870105475373 Test RE 0.08979027766521203 Lambda1 0.9685349\n",
      "198 Train Loss 25.146633 Test MSE 28.445032849432494 Test RE 0.08978264393919595 Lambda1 0.97074956\n",
      "199 Train Loss 25.1367 Test MSE 28.42695560585237 Test RE 0.08975411029892699 Lambda1 0.96888787\n",
      "Training time: 812.99\n",
      "Training time: 812.99\n",
      "inv_HT_stan\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 852.1879 Test MSE 854.0709391703405 Test RE 0.4919671933712394 Lambda1 0.017298052\n",
      "1 Train Loss 720.9474 Test MSE 713.9464658067131 Test RE 0.4498026343945992 Lambda1 0.6087032\n",
      "2 Train Loss 596.49475 Test MSE 590.1261717032178 Test RE 0.40894189816636467 Lambda1 0.5729487\n",
      "3 Train Loss 477.1429 Test MSE 475.5639907441411 Test RE 0.3671078337403555 Lambda1 0.5522187\n",
      "4 Train Loss 366.41998 Test MSE 343.32841703146266 Test RE 0.3119205918304416 Lambda1 0.58462787\n",
      "5 Train Loss 305.65582 Test MSE 297.0258221640008 Test RE 0.29012574854131545 Lambda1 0.55364835\n",
      "6 Train Loss 264.82852 Test MSE 254.893350075886 Test RE 0.2687623459556895 Lambda1 0.4662819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 243.92575 Test MSE 240.05176752475919 Test RE 0.2608204408671057 Lambda1 0.39480406\n",
      "8 Train Loss 200.56308 Test MSE 190.91886150089562 Test RE 0.23260207709245084 Lambda1 0.36293483\n",
      "9 Train Loss 184.10538 Test MSE 171.58128775154148 Test RE 0.22050788925330134 Lambda1 0.35285404\n",
      "10 Train Loss 159.39017 Test MSE 151.64709231033638 Test RE 0.20730329944994363 Lambda1 0.3671469\n",
      "11 Train Loss 143.29337 Test MSE 138.17693776280765 Test RE 0.19788230134325713 Lambda1 0.36886767\n",
      "12 Train Loss 132.53726 Test MSE 129.8276392415165 Test RE 0.1918106648392027 Lambda1 0.36822337\n",
      "13 Train Loss 120.70205 Test MSE 115.65782742050821 Test RE 0.18104089404002258 Lambda1 0.38757232\n",
      "14 Train Loss 111.13056 Test MSE 101.36984044704107 Test RE 0.1694897940988138 Lambda1 0.40900958\n",
      "15 Train Loss 95.32895 Test MSE 83.83424146016398 Test RE 0.15413451145099616 Lambda1 0.4418447\n",
      "16 Train Loss 83.34101 Test MSE 76.38757853959503 Test RE 0.14712976532415561 Lambda1 0.46192905\n",
      "17 Train Loss 76.57713 Test MSE 68.04831183439671 Test RE 0.13886661471139486 Lambda1 0.47419882\n",
      "18 Train Loss 69.35448 Test MSE 59.959100709404375 Test RE 0.13035170793270673 Lambda1 0.478798\n",
      "19 Train Loss 62.84232 Test MSE 56.44370166414443 Test RE 0.12647273619882704 Lambda1 0.4919662\n",
      "20 Train Loss 58.049515 Test MSE 53.66721316238706 Test RE 0.12332288968995075 Lambda1 0.5028126\n",
      "21 Train Loss 53.273968 Test MSE 50.27674566816749 Test RE 0.11936383124958211 Lambda1 0.5166656\n",
      "22 Train Loss 50.355568 Test MSE 48.767249820882824 Test RE 0.11755830157653344 Lambda1 0.5240284\n",
      "23 Train Loss 48.257732 Test MSE 47.77827590796367 Test RE 0.11636018623429645 Lambda1 0.5381266\n",
      "24 Train Loss 46.53311 Test MSE 47.57963308647431 Test RE 0.11611804490404873 Lambda1 0.54215145\n",
      "25 Train Loss 43.727993 Test MSE 45.97168729629416 Test RE 0.11413908655236361 Lambda1 0.56031364\n",
      "26 Train Loss 42.966084 Test MSE 45.40649006957936 Test RE 0.11343527721182983 Lambda1 0.5628697\n",
      "27 Train Loss 41.583836 Test MSE 44.47612314828582 Test RE 0.112267133151541 Lambda1 0.57646793\n",
      "28 Train Loss 40.70067 Test MSE 43.66918634791713 Test RE 0.11124403184097435 Lambda1 0.5833635\n",
      "29 Train Loss 40.107826 Test MSE 42.87800897441906 Test RE 0.11023169260623515 Lambda1 0.59246016\n",
      "30 Train Loss 39.106075 Test MSE 40.88753728336044 Test RE 0.10764271600538508 Lambda1 0.6196251\n",
      "31 Train Loss 38.599026 Test MSE 40.141200723762346 Test RE 0.1066557687190118 Lambda1 0.635565\n",
      "32 Train Loss 38.01081 Test MSE 39.16571328160291 Test RE 0.10535185593300106 Lambda1 0.64958715\n",
      "33 Train Loss 37.17578 Test MSE 39.15484969361593 Test RE 0.10533724393649342 Lambda1 0.6487067\n",
      "34 Train Loss 36.637505 Test MSE 38.65689220840554 Test RE 0.1046652798039983 Lambda1 0.6532065\n",
      "35 Train Loss 36.157234 Test MSE 38.65658199741712 Test RE 0.10466485984803205 Lambda1 0.65660983\n",
      "36 Train Loss 35.393623 Test MSE 37.54257339183907 Test RE 0.10314571479507655 Lambda1 0.68485093\n",
      "37 Train Loss 34.886673 Test MSE 36.594718502220616 Test RE 0.10183530677376207 Lambda1 0.7101593\n",
      "38 Train Loss 34.171852 Test MSE 35.87145330708646 Test RE 0.10082393812320666 Lambda1 0.7294564\n",
      "39 Train Loss 33.627865 Test MSE 35.61656783461837 Test RE 0.10046509609049356 Lambda1 0.74380076\n",
      "40 Train Loss 32.94255 Test MSE 35.36128140766943 Test RE 0.10010440020997358 Lambda1 0.75147283\n",
      "41 Train Loss 32.330452 Test MSE 34.649699551930674 Test RE 0.0990920714729586 Lambda1 0.76516265\n",
      "42 Train Loss 31.80583 Test MSE 34.25529485348337 Test RE 0.09852649324812161 Lambda1 0.7768609\n",
      "43 Train Loss 31.530487 Test MSE 34.22364091615062 Test RE 0.09848096054628042 Lambda1 0.77894723\n",
      "44 Train Loss 31.382866 Test MSE 34.00106876911061 Test RE 0.09816020459893798 Lambda1 0.78607684\n",
      "45 Train Loss 31.096668 Test MSE 33.725754039557124 Test RE 0.09776198359240765 Lambda1 0.79767877\n",
      "46 Train Loss 30.923754 Test MSE 33.73744160267436 Test RE 0.09777892169227015 Lambda1 0.7994363\n",
      "47 Train Loss 30.71446 Test MSE 33.77072960917279 Test RE 0.09782714797656418 Lambda1 0.80609405\n",
      "48 Train Loss 30.536398 Test MSE 33.72836339805999 Test RE 0.09776576543711961 Lambda1 0.8109069\n",
      "49 Train Loss 30.34237 Test MSE 33.531541601578496 Test RE 0.09748009210242875 Lambda1 0.82417464\n",
      "50 Train Loss 30.240448 Test MSE 33.43322426686109 Test RE 0.09733707722796914 Lambda1 0.82857615\n",
      "51 Train Loss 29.994036 Test MSE 33.08561606944422 Test RE 0.096829743973486 Lambda1 0.8434429\n",
      "52 Train Loss 29.70698 Test MSE 32.95659850380611 Test RE 0.0966407654683912 Lambda1 0.8571191\n",
      "53 Train Loss 29.53179 Test MSE 32.91689567833567 Test RE 0.09658253634436612 Lambda1 0.8579983\n",
      "54 Train Loss 29.45596 Test MSE 32.838745132967645 Test RE 0.09646781618138892 Lambda1 0.8636426\n",
      "55 Train Loss 29.37057 Test MSE 32.78814724865652 Test RE 0.09639346880577879 Lambda1 0.8658979\n",
      "56 Train Loss 29.298662 Test MSE 32.937227012739264 Test RE 0.09661235915544134 Lambda1 0.86575246\n",
      "57 Train Loss 29.16065 Test MSE 32.749841014251125 Test RE 0.0963371443375552 Lambda1 0.8685485\n",
      "58 Train Loss 29.074879 Test MSE 32.50199436961959 Test RE 0.09597191838215435 Lambda1 0.8747201\n",
      "59 Train Loss 29.041094 Test MSE 32.40751555827661 Test RE 0.09583232830624804 Lambda1 0.8763485\n",
      "60 Train Loss 29.002577 Test MSE 32.40201811735858 Test RE 0.09582419971480871 Lambda1 0.8783655\n",
      "61 Train Loss 28.754938 Test MSE 32.15742307902237 Test RE 0.095461837659953 Lambda1 0.8809475\n",
      "62 Train Loss 28.622845 Test MSE 31.981340762172476 Test RE 0.0952001218051048 Lambda1 0.881558\n",
      "63 Train Loss 28.572687 Test MSE 32.01026545593624 Test RE 0.09524316271539784 Lambda1 0.8802503\n",
      "64 Train Loss 28.530642 Test MSE 32.03744421743535 Test RE 0.09528358790291651 Lambda1 0.87974644\n",
      "65 Train Loss 28.391594 Test MSE 32.00408021843412 Test RE 0.09523396051051541 Lambda1 0.884756\n",
      "66 Train Loss 28.246609 Test MSE 31.70755347010581 Test RE 0.09479174919302574 Lambda1 0.8909694\n",
      "67 Train Loss 28.12847 Test MSE 31.663568236407645 Test RE 0.09472597804757232 Lambda1 0.8989152\n",
      "68 Train Loss 28.105452 Test MSE 31.63727834611034 Test RE 0.09468664494570253 Lambda1 0.9018265\n",
      "69 Train Loss 28.048601 Test MSE 31.45871715398591 Test RE 0.09441906054869048 Lambda1 0.907721\n",
      "70 Train Loss 27.966679 Test MSE 31.31252607944843 Test RE 0.09419941876577234 Lambda1 0.9176184\n",
      "71 Train Loss 27.871138 Test MSE 31.266233201364777 Test RE 0.0941297601386954 Lambda1 0.92014736\n",
      "72 Train Loss 27.791723 Test MSE 31.30338966383006 Test RE 0.09418567493961309 Lambda1 0.91976273\n",
      "73 Train Loss 27.641197 Test MSE 31.0179893484356 Test RE 0.09375533541802615 Lambda1 0.9251704\n",
      "74 Train Loss 27.596409 Test MSE 31.015901798353962 Test RE 0.09375218043836175 Lambda1 0.92354065\n",
      "75 Train Loss 27.550562 Test MSE 31.006377060323185 Test RE 0.09373778405662114 Lambda1 0.9197013\n",
      "76 Train Loss 27.514914 Test MSE 30.963592043272346 Test RE 0.09367308837866473 Lambda1 0.9172366\n",
      "77 Train Loss 27.493612 Test MSE 30.88076622633047 Test RE 0.09354771943844806 Lambda1 0.91849035\n",
      "78 Train Loss 27.437012 Test MSE 30.699813043016405 Test RE 0.09327323420585873 Lambda1 0.92522055\n",
      "79 Train Loss 27.332342 Test MSE 30.61433119345032 Test RE 0.09314328672847923 Lambda1 0.9223512\n",
      "80 Train Loss 27.267103 Test MSE 30.50788503681011 Test RE 0.09298121592829725 Lambda1 0.92359656\n",
      "81 Train Loss 27.216871 Test MSE 30.398748537954344 Test RE 0.09281475509418798 Lambda1 0.92316467\n",
      "82 Train Loss 27.16524 Test MSE 30.333666299151467 Test RE 0.09271534592497731 Lambda1 0.9242556\n",
      "83 Train Loss 27.121923 Test MSE 30.338830898241408 Test RE 0.09272323842973623 Lambda1 0.92691064\n",
      "84 Train Loss 27.071949 Test MSE 30.184574570887104 Test RE 0.09248721461011618 Lambda1 0.9300192\n",
      "85 Train Loss 26.992441 Test MSE 30.06201803479526 Test RE 0.09229926361457065 Lambda1 0.9362714\n",
      "86 Train Loss 26.92953 Test MSE 29.97964468105458 Test RE 0.09217272162224281 Lambda1 0.93749815\n",
      "87 Train Loss 26.900108 Test MSE 29.959171982301875 Test RE 0.09214124448752287 Lambda1 0.9396146\n",
      "88 Train Loss 26.879728 Test MSE 29.949338446847705 Test RE 0.09212612143008893 Lambda1 0.94145685\n",
      "89 Train Loss 26.804264 Test MSE 29.867028787265184 Test RE 0.09199943938547439 Lambda1 0.94195825\n",
      "90 Train Loss 26.742117 Test MSE 29.774443418909552 Test RE 0.09185673330279026 Lambda1 0.94322836\n",
      "91 Train Loss 26.70727 Test MSE 29.71099808795507 Test RE 0.09175881394486451 Lambda1 0.94620234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Train Loss 26.673971 Test MSE 29.67315465521575 Test RE 0.09170035789881317 Lambda1 0.9457593\n",
      "93 Train Loss 26.656607 Test MSE 29.667412344004067 Test RE 0.0916914846010926 Lambda1 0.94556123\n",
      "94 Train Loss 26.636015 Test MSE 29.658731818323734 Test RE 0.09167806940149129 Lambda1 0.94613636\n",
      "95 Train Loss 26.615244 Test MSE 29.624673013106577 Test RE 0.09162541471512455 Lambda1 0.9475967\n",
      "96 Train Loss 26.592428 Test MSE 29.639496288970317 Test RE 0.09164833511902845 Lambda1 0.94645375\n",
      "97 Train Loss 26.562365 Test MSE 29.656342670756498 Test RE 0.09167437678157385 Lambda1 0.9470338\n",
      "98 Train Loss 26.522501 Test MSE 29.6616953109084 Test RE 0.09168264951122289 Lambda1 0.9467019\n",
      "99 Train Loss 26.508085 Test MSE 29.700918374082924 Test RE 0.09174324763763898 Lambda1 0.9468807\n",
      "100 Train Loss 26.500467 Test MSE 29.68984466071392 Test RE 0.09172614323149642 Lambda1 0.9470578\n",
      "101 Train Loss 26.492666 Test MSE 29.723519979707827 Test RE 0.09177814807990362 Lambda1 0.9451347\n",
      "102 Train Loss 26.47903 Test MSE 29.7196155081919 Test RE 0.09177211990871119 Lambda1 0.94443333\n",
      "103 Train Loss 26.471273 Test MSE 29.715331307945004 Test RE 0.09176550501308874 Lambda1 0.94359875\n",
      "104 Train Loss 26.435242 Test MSE 29.65562726018752 Test RE 0.09167327102800338 Lambda1 0.93990505\n",
      "105 Train Loss 26.39543 Test MSE 29.588563614598407 Test RE 0.09156955675817635 Lambda1 0.93552583\n",
      "106 Train Loss 26.366564 Test MSE 29.503967854348605 Test RE 0.09143856119564507 Lambda1 0.936639\n",
      "107 Train Loss 26.354546 Test MSE 29.461012655574805 Test RE 0.09137197367338036 Lambda1 0.93811244\n",
      "108 Train Loss 26.346298 Test MSE 29.478179238614395 Test RE 0.09139859047989261 Lambda1 0.9381617\n",
      "109 Train Loss 26.332006 Test MSE 29.48178820611355 Test RE 0.09140418520208633 Lambda1 0.9391116\n",
      "110 Train Loss 26.27225 Test MSE 29.445451281883283 Test RE 0.09134783904412161 Lambda1 0.9367978\n",
      "111 Train Loss 26.211618 Test MSE 29.345927858872972 Test RE 0.09119333394782037 Lambda1 0.93369025\n",
      "112 Train Loss 26.195072 Test MSE 29.340589552366932 Test RE 0.09118503909833574 Lambda1 0.93191314\n",
      "113 Train Loss 26.175768 Test MSE 29.363719479303654 Test RE 0.09122097375103297 Lambda1 0.9287086\n",
      "114 Train Loss 26.165592 Test MSE 29.35337772573617 Test RE 0.09120490855347077 Lambda1 0.9280602\n",
      "115 Train Loss 26.154413 Test MSE 29.316538211571473 Test RE 0.09114765791223395 Lambda1 0.9260115\n",
      "116 Train Loss 26.14469 Test MSE 29.329110267049632 Test RE 0.09116719962239668 Lambda1 0.9226761\n",
      "117 Train Loss 26.137667 Test MSE 29.338196946724807 Test RE 0.09118132113816096 Lambda1 0.9205399\n",
      "118 Train Loss 26.131063 Test MSE 29.335959821992834 Test RE 0.091177844648489 Lambda1 0.9189123\n",
      "119 Train Loss 26.115929 Test MSE 29.31649904705248 Test RE 0.09114759702927179 Lambda1 0.9192775\n",
      "120 Train Loss 26.095163 Test MSE 29.32369710752442 Test RE 0.09115878604688088 Lambda1 0.91742676\n",
      "121 Train Loss 26.08421 Test MSE 29.30964181535673 Test RE 0.09113693653493354 Lambda1 0.917122\n",
      "122 Train Loss 26.06724 Test MSE 29.30111242622369 Test RE 0.09112367470521268 Lambda1 0.91951865\n",
      "123 Train Loss 26.051527 Test MSE 29.26494741432271 Test RE 0.09106742246762353 Lambda1 0.91913164\n",
      "124 Train Loss 26.03939 Test MSE 29.223845661267877 Test RE 0.0910034492469597 Lambda1 0.9192577\n",
      "125 Train Loss 26.022339 Test MSE 29.191300545163273 Test RE 0.09095276216603554 Lambda1 0.9211002\n",
      "126 Train Loss 26.014725 Test MSE 29.196547566648974 Test RE 0.09096093599942393 Lambda1 0.92135036\n",
      "127 Train Loss 26.00673 Test MSE 29.204231226272437 Test RE 0.09097290431335651 Lambda1 0.9186726\n",
      "128 Train Loss 25.999685 Test MSE 29.198491522713947 Test RE 0.09096396411635446 Lambda1 0.91707397\n",
      "129 Train Loss 25.988409 Test MSE 29.21016527661687 Test RE 0.09098214630247399 Lambda1 0.91544354\n",
      "130 Train Loss 25.963652 Test MSE 29.15378716585065 Test RE 0.09089430224355773 Lambda1 0.9172499\n",
      "131 Train Loss 25.951406 Test MSE 29.12705285077514 Test RE 0.09085261719181172 Lambda1 0.920195\n",
      "132 Train Loss 25.93311 Test MSE 29.07927942610021 Test RE 0.09077807958110135 Lambda1 0.9245288\n",
      "133 Train Loss 25.911179 Test MSE 29.065629456883254 Test RE 0.09075677122232138 Lambda1 0.92790914\n",
      "134 Train Loss 25.903242 Test MSE 29.083314830468524 Test RE 0.09078437811321761 Lambda1 0.9302104\n",
      "135 Train Loss 25.892548 Test MSE 29.03701703441537 Test RE 0.09071208940201471 Lambda1 0.93299884\n",
      "136 Train Loss 25.880617 Test MSE 29.075280764579155 Test RE 0.09077183796679 Lambda1 0.931575\n",
      "137 Train Loss 25.867273 Test MSE 29.042048733929185 Test RE 0.0907199486150299 Lambda1 0.930503\n",
      "138 Train Loss 25.83443 Test MSE 29.013863654085412 Test RE 0.09067591643096282 Lambda1 0.92770916\n",
      "139 Train Loss 25.817251 Test MSE 28.98698112277161 Test RE 0.09063389922347777 Lambda1 0.9256895\n",
      "140 Train Loss 25.788168 Test MSE 28.945770815860055 Test RE 0.09056944995835606 Lambda1 0.9216311\n",
      "141 Train Loss 25.76427 Test MSE 28.96364923667409 Test RE 0.09059741585353155 Lambda1 0.9222778\n",
      "142 Train Loss 25.757761 Test MSE 28.949526246918243 Test RE 0.09057532501858724 Lambda1 0.92167556\n",
      "143 Train Loss 25.747719 Test MSE 28.928435256822254 Test RE 0.0905423249734081 Lambda1 0.91740024\n",
      "144 Train Loss 25.733185 Test MSE 28.964932460960163 Test RE 0.09059942277461527 Lambda1 0.910703\n",
      "145 Train Loss 25.729982 Test MSE 28.950976840847982 Test RE 0.09057759425038212 Lambda1 0.90991056\n",
      "146 Train Loss 25.723425 Test MSE 28.944191360126098 Test RE 0.09056697891708644 Lambda1 0.9104093\n",
      "147 Train Loss 25.714666 Test MSE 28.921726952507598 Test RE 0.09053182629485715 Lambda1 0.9101401\n",
      "148 Train Loss 25.699432 Test MSE 28.915003357117413 Test RE 0.09052130246355228 Lambda1 0.90982395\n",
      "149 Train Loss 25.693008 Test MSE 28.916808355749502 Test RE 0.0905241277835997 Lambda1 0.9121271\n",
      "150 Train Loss 25.684038 Test MSE 28.90666576924953 Test RE 0.09050825069734751 Lambda1 0.9129822\n",
      "151 Train Loss 25.678068 Test MSE 28.908887960798527 Test RE 0.09051172952825422 Lambda1 0.91317177\n",
      "152 Train Loss 25.675173 Test MSE 28.921249079762404 Test RE 0.09053107836457039 Lambda1 0.9125282\n",
      "153 Train Loss 25.6712 Test MSE 28.916930521122513 Test RE 0.09052431900287293 Lambda1 0.9137201\n",
      "154 Train Loss 25.664871 Test MSE 28.896798810871474 Test RE 0.09049280240299955 Lambda1 0.9154\n",
      "155 Train Loss 25.653837 Test MSE 28.858566683303938 Test RE 0.09043291899684845 Lambda1 0.915137\n",
      "156 Train Loss 25.646423 Test MSE 28.830417787562986 Test RE 0.09038880371063014 Lambda1 0.91450167\n",
      "157 Train Loss 25.639557 Test MSE 28.819324335149616 Test RE 0.0903714120034344 Lambda1 0.91373986\n",
      "158 Train Loss 25.63147 Test MSE 28.79745054302695 Test RE 0.09033710966016328 Lambda1 0.9132366\n",
      "159 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "160 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "161 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "162 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "163 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "164 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "165 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "166 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "167 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "168 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "169 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "170 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "171 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "172 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "173 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "174 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "176 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "177 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "178 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "179 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "180 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "181 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "182 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "183 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "184 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "185 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "186 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "187 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "188 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "189 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "190 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "191 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "192 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "193 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "194 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "195 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "196 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "197 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "198 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "199 Train Loss 25.631067 Test MSE 28.797619247266393 Test RE 0.09033737427093212 Lambda1 0.91319907\n",
      "Training time: 560.09\n",
      "Training time: 560.09\n",
      "inv_HT_stan\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 852.4694 Test MSE 855.6680622926071 Test RE 0.4924269708643997 Lambda1 0.049456112\n",
      "1 Train Loss 725.9983 Test MSE 717.9320913097039 Test RE 0.451056404729291 Lambda1 0.45406482\n",
      "2 Train Loss 591.10284 Test MSE 571.0137451941097 Test RE 0.402265189395783 Lambda1 0.48795256\n",
      "3 Train Loss 414.24228 Test MSE 389.09751892255105 Test RE 0.33206140520364413 Lambda1 0.46441257\n",
      "4 Train Loss 324.31393 Test MSE 305.2298432311502 Test RE 0.2941051757681544 Lambda1 0.47075275\n",
      "5 Train Loss 275.11313 Test MSE 265.3987322304063 Test RE 0.27424492062520045 Lambda1 0.40914574\n",
      "6 Train Loss 240.90593 Test MSE 232.10690507683307 Test RE 0.2564680093952854 Lambda1 0.35681453\n",
      "7 Train Loss 220.12782 Test MSE 215.0577793332878 Test RE 0.24686910874074797 Lambda1 0.37397513\n",
      "8 Train Loss 204.67876 Test MSE 197.94233569088956 Test RE 0.2368418888114974 Lambda1 0.42102638\n",
      "9 Train Loss 186.48924 Test MSE 175.99620423741314 Test RE 0.22332678881502954 Lambda1 0.44681656\n",
      "10 Train Loss 159.83464 Test MSE 154.63122673487348 Test RE 0.20933303557652314 Lambda1 0.52155215\n",
      "11 Train Loss 151.08853 Test MSE 146.93087321550752 Test RE 0.20405427624045736 Lambda1 0.56231886\n",
      "12 Train Loss 144.8574 Test MSE 141.06228476676202 Test RE 0.19993767038004723 Lambda1 0.5903464\n",
      "13 Train Loss 139.29506 Test MSE 133.39720151778553 Test RE 0.19442966595926492 Lambda1 0.60389674\n",
      "14 Train Loss 131.0091 Test MSE 126.85013510701198 Test RE 0.18959838674514956 Lambda1 0.6256217\n",
      "15 Train Loss 124.46793 Test MSE 119.46635219495083 Test RE 0.18399752115860232 Lambda1 0.6404203\n",
      "16 Train Loss 116.51743 Test MSE 110.65555501701671 Test RE 0.17708255563544828 Lambda1 0.667538\n",
      "17 Train Loss 109.045685 Test MSE 100.46608524884283 Test RE 0.16873256580308937 Lambda1 0.72321385\n",
      "18 Train Loss 102.994385 Test MSE 95.05077367426425 Test RE 0.16412207484972405 Lambda1 0.76990277\n",
      "19 Train Loss 96.55852 Test MSE 84.69169531603998 Test RE 0.15492074751534807 Lambda1 0.8642513\n",
      "20 Train Loss 92.49205 Test MSE 82.8755864527848 Test RE 0.15325070389246573 Lambda1 0.8729097\n",
      "21 Train Loss 86.6076 Test MSE 78.88014935698018 Test RE 0.14951096092535204 Lambda1 0.9004599\n",
      "22 Train Loss 79.43312 Test MSE 70.31594070769253 Test RE 0.14116143553903782 Lambda1 0.95987487\n",
      "23 Train Loss 71.68625 Test MSE 60.460198471524926 Test RE 0.13089527048396726 Lambda1 1.0299342\n",
      "24 Train Loss 67.65504 Test MSE 58.15637211159079 Test RE 0.12837717777417584 Lambda1 1.0250572\n",
      "25 Train Loss 63.69729 Test MSE 58.38927679732887 Test RE 0.12863398342953547 Lambda1 1.0058979\n",
      "26 Train Loss 61.322975 Test MSE 57.18339065769047 Test RE 0.12729874501602131 Lambda1 1.0077184\n",
      "27 Train Loss 59.571163 Test MSE 55.77559686443851 Test RE 0.1257220007660016 Lambda1 1.0219946\n",
      "28 Train Loss 56.72675 Test MSE 53.06279578402846 Test RE 0.12262647233766458 Lambda1 1.0368382\n",
      "29 Train Loss 54.450317 Test MSE 50.80409554475179 Test RE 0.11998819844272379 Lambda1 1.0447809\n",
      "30 Train Loss 52.23561 Test MSE 48.285412731198015 Test RE 0.1169761018317568 Lambda1 1.066716\n",
      "31 Train Loss 50.528374 Test MSE 45.63651218753483 Test RE 0.11372223686048642 Lambda1 1.0951934\n",
      "32 Train Loss 47.681366 Test MSE 44.38216165147861 Test RE 0.11214848111495597 Lambda1 1.0994004\n",
      "33 Train Loss 46.036293 Test MSE 42.4482438868306 Test RE 0.10967787680166015 Lambda1 1.0942745\n",
      "34 Train Loss 43.907738 Test MSE 39.913559387428485 Test RE 0.10635291552523854 Lambda1 1.1187329\n",
      "35 Train Loss 41.868645 Test MSE 38.143154155240516 Test RE 0.1039674691489877 Lambda1 1.13218\n",
      "36 Train Loss 40.29059 Test MSE 37.13725078284331 Test RE 0.10258740537178221 Lambda1 1.1492536\n",
      "37 Train Loss 38.41243 Test MSE 35.973325705601155 Test RE 0.10096700305981185 Lambda1 1.1584179\n",
      "38 Train Loss 36.9037 Test MSE 34.63567733338452 Test RE 0.09907201889955447 Lambda1 1.1777898\n",
      "39 Train Loss 36.140987 Test MSE 34.29073908705237 Test RE 0.09857745315330504 Lambda1 1.1765362\n",
      "40 Train Loss 35.416416 Test MSE 34.01455113456265 Test RE 0.09817966428992454 Lambda1 1.1759896\n",
      "41 Train Loss 34.389515 Test MSE 33.34945164426315 Test RE 0.09721505345153225 Lambda1 1.1878417\n",
      "42 Train Loss 33.301746 Test MSE 32.75626066432576 Test RE 0.09634658591700031 Lambda1 1.1981497\n",
      "43 Train Loss 32.42695 Test MSE 32.74042344834721 Test RE 0.09632329195521916 Lambda1 1.1988963\n",
      "44 Train Loss 31.82313 Test MSE 32.51102829150584 Test RE 0.0959852551418412 Lambda1 1.2007194\n",
      "45 Train Loss 31.366882 Test MSE 32.23247442603731 Test RE 0.09557317064948838 Lambda1 1.2021523\n",
      "46 Train Loss 31.024529 Test MSE 32.088154692090015 Test RE 0.09535896790848693 Lambda1 1.2003227\n",
      "47 Train Loss 30.575422 Test MSE 32.002825743910016 Test RE 0.09523209403369104 Lambda1 1.2009492\n",
      "48 Train Loss 30.168198 Test MSE 31.76665677390915 Test RE 0.09488005460580382 Lambda1 1.2111578\n",
      "49 Train Loss 29.939857 Test MSE 31.704397885556688 Test RE 0.09478703216523462 Lambda1 1.2138152\n",
      "50 Train Loss 29.804106 Test MSE 31.619497192507243 Test RE 0.0946600327575352 Lambda1 1.2149867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 Train Loss 29.633112 Test MSE 31.56384014244083 Test RE 0.09457668515379503 Lambda1 1.2112073\n",
      "52 Train Loss 29.528616 Test MSE 31.56141523645156 Test RE 0.09457305213616811 Lambda1 1.2097399\n",
      "53 Train Loss 29.390894 Test MSE 31.435319832548704 Test RE 0.09438394207875858 Lambda1 1.2119449\n",
      "54 Train Loss 29.333618 Test MSE 31.3712178322647 Test RE 0.09428766045968871 Lambda1 1.216018\n",
      "55 Train Loss 29.243486 Test MSE 31.336715903129807 Test RE 0.09423579761638005 Lambda1 1.2171322\n",
      "56 Train Loss 29.132849 Test MSE 31.274291820808994 Test RE 0.0941418899504132 Lambda1 1.2164518\n",
      "57 Train Loss 28.993422 Test MSE 31.11768448908945 Test RE 0.09390588439085806 Lambda1 1.2176067\n",
      "58 Train Loss 28.811779 Test MSE 31.09040849004357 Test RE 0.09386471907938501 Lambda1 1.2123178\n",
      "59 Train Loss 28.763622 Test MSE 31.068945915626013 Test RE 0.09383231477181479 Lambda1 1.2079904\n",
      "60 Train Loss 28.691534 Test MSE 31.032186785988415 Test RE 0.09377678963327 Lambda1 1.2019972\n",
      "61 Train Loss 28.6322 Test MSE 31.011123653541638 Test RE 0.09374495867915758 Lambda1 1.2025974\n",
      "62 Train Loss 28.45378 Test MSE 30.857868064543442 Test RE 0.09351303007818819 Lambda1 1.1933817\n",
      "63 Train Loss 28.254004 Test MSE 30.651711851720396 Test RE 0.09320013420908177 Lambda1 1.1885753\n",
      "64 Train Loss 28.037395 Test MSE 30.641096061467064 Test RE 0.09318399353215037 Lambda1 1.1782331\n",
      "65 Train Loss 27.915167 Test MSE 30.609219412021524 Test RE 0.09313551017446667 Lambda1 1.1718036\n",
      "66 Train Loss 27.827576 Test MSE 30.55181071062453 Test RE 0.09304812966993854 Lambda1 1.1669376\n",
      "67 Train Loss 27.793524 Test MSE 30.53929686721475 Test RE 0.09302907173272613 Lambda1 1.1668113\n",
      "68 Train Loss 27.752966 Test MSE 30.547938837560388 Test RE 0.0930422334242877 Lambda1 1.1695727\n",
      "69 Train Loss 27.625605 Test MSE 30.53965607226136 Test RE 0.09302961883788596 Lambda1 1.1627095\n",
      "70 Train Loss 27.520956 Test MSE 30.433497173765097 Test RE 0.0928677879513145 Lambda1 1.1550286\n",
      "71 Train Loss 27.45644 Test MSE 30.394204226430272 Test RE 0.0928078173920523 Lambda1 1.1446292\n",
      "72 Train Loss 27.425154 Test MSE 30.350290722095675 Test RE 0.09274074885489807 Lambda1 1.1384536\n",
      "73 Train Loss 27.393839 Test MSE 30.34585252608196 Test RE 0.09273396775559785 Lambda1 1.1296881\n",
      "74 Train Loss 27.316843 Test MSE 30.317747010641256 Test RE 0.09269101394964285 Lambda1 1.129378\n",
      "75 Train Loss 27.270725 Test MSE 30.299109652785816 Test RE 0.09266251939876566 Lambda1 1.1293834\n",
      "76 Train Loss 27.178936 Test MSE 30.20956379830439 Test RE 0.09252549088075539 Lambda1 1.1164227\n",
      "77 Train Loss 27.091366 Test MSE 30.14597000805024 Test RE 0.09242805242430902 Lambda1 1.1099472\n",
      "78 Train Loss 26.956161 Test MSE 30.040875354589442 Test RE 0.09226680077312613 Lambda1 1.0991641\n",
      "79 Train Loss 26.902388 Test MSE 29.993808702053794 Test RE 0.09219449276418346 Lambda1 1.0928526\n",
      "80 Train Loss 26.862371 Test MSE 29.96994275816919 Test RE 0.09215780608526129 Lambda1 1.0858645\n",
      "81 Train Loss 26.78437 Test MSE 29.949606357355563 Test RE 0.09212653348427881 Lambda1 1.0833929\n",
      "82 Train Loss 26.749432 Test MSE 29.914897051890822 Test RE 0.09207313420168704 Lambda1 1.0800744\n",
      "83 Train Loss 26.69364 Test MSE 29.852885864740184 Test RE 0.09197765457685629 Lambda1 1.0707885\n",
      "84 Train Loss 26.63112 Test MSE 29.791523140281438 Test RE 0.09188307573445939 Lambda1 1.0646123\n",
      "85 Train Loss 26.593853 Test MSE 29.721716033914905 Test RE 0.09177536299058148 Lambda1 1.0587096\n",
      "86 Train Loss 26.521872 Test MSE 29.69863067952356 Test RE 0.09173971433662334 Lambda1 1.0601894\n",
      "87 Train Loss 26.475557 Test MSE 29.632331980369244 Test RE 0.09163725806442394 Lambda1 1.0628422\n",
      "88 Train Loss 26.416397 Test MSE 29.463319010174892 Test RE 0.09137555012966272 Lambda1 1.0524344\n",
      "89 Train Loss 26.298626 Test MSE 29.37647827215906 Test RE 0.09124078975405275 Lambda1 1.0387235\n",
      "90 Train Loss 26.252739 Test MSE 29.31544920301633 Test RE 0.0911459649855257 Lambda1 1.0369656\n",
      "91 Train Loss 26.200201 Test MSE 29.311872275819077 Test RE 0.09114040422429809 Lambda1 1.0370207\n",
      "92 Train Loss 26.147541 Test MSE 29.25357934551452 Test RE 0.09104973302387856 Lambda1 1.0324932\n",
      "93 Train Loss 26.113104 Test MSE 29.259300279536447 Test RE 0.09105863559345696 Lambda1 1.034017\n",
      "94 Train Loss 26.085331 Test MSE 29.26447862533182 Test RE 0.09106669306983162 Lambda1 1.026433\n",
      "95 Train Loss 26.051403 Test MSE 29.18727992638885 Test RE 0.09094649833110868 Lambda1 1.0262872\n",
      "96 Train Loss 26.003223 Test MSE 29.15587162657908 Test RE 0.09089755160213793 Lambda1 1.029382\n",
      "97 Train Loss 25.963043 Test MSE 29.13087385848402 Test RE 0.09085857620812703 Lambda1 1.0242863\n",
      "98 Train Loss 25.920465 Test MSE 29.100197138541994 Test RE 0.0908107235881841 Lambda1 1.0216676\n",
      "99 Train Loss 25.88845 Test MSE 29.066647364821886 Test RE 0.09075836040567682 Lambda1 1.0230315\n",
      "100 Train Loss 25.84801 Test MSE 29.089824729107686 Test RE 0.09079453795976618 Lambda1 1.015863\n",
      "101 Train Loss 25.832504 Test MSE 29.084128372600144 Test RE 0.09078564785156124 Lambda1 1.0115517\n",
      "102 Train Loss 25.812386 Test MSE 29.088526335497036 Test RE 0.09079251167799002 Lambda1 1.0096722\n",
      "103 Train Loss 25.799644 Test MSE 29.047005579827626 Test RE 0.09072769024550341 Lambda1 1.0110633\n",
      "104 Train Loss 25.786652 Test MSE 29.02318840685244 Test RE 0.09069048640667284 Lambda1 1.0079737\n",
      "105 Train Loss 25.753082 Test MSE 28.965915754990693 Test RE 0.09060096058444186 Lambda1 1.0059642\n",
      "106 Train Loss 25.704376 Test MSE 28.91776175514686 Test RE 0.09052562008068243 Lambda1 1.000751\n",
      "107 Train Loss 25.68604 Test MSE 28.894475502247865 Test RE 0.09048916450982611 Lambda1 1.0031159\n",
      "108 Train Loss 25.66507 Test MSE 28.861308983522246 Test RE 0.09043721561221857 Lambda1 1.005054\n",
      "109 Train Loss 25.654402 Test MSE 28.840184608127583 Test RE 0.09040411282753276 Lambda1 1.006913\n",
      "110 Train Loss 25.642256 Test MSE 28.827018192151513 Test RE 0.09038347436674617 Lambda1 1.0055537\n",
      "111 Train Loss 25.624662 Test MSE 28.847401086203753 Test RE 0.09041542271512285 Lambda1 1.0059537\n",
      "112 Train Loss 25.598331 Test MSE 28.830011432670837 Test RE 0.0903881667087268 Lambda1 1.0056467\n",
      "113 Train Loss 25.584497 Test MSE 28.824058842294832 Test RE 0.09037883491383451 Lambda1 1.0009431\n",
      "114 Train Loss 25.547165 Test MSE 28.799433483930876 Test RE 0.0903402198324688 Lambda1 0.9907003\n",
      "115 Train Loss 25.52822 Test MSE 28.7869106144853 Test RE 0.09032057635946478 Lambda1 0.9859086\n",
      "116 Train Loss 25.498884 Test MSE 28.801236003563226 Test RE 0.09034304692752297 Lambda1 0.9804298\n",
      "117 Train Loss 25.474117 Test MSE 28.793923634577414 Test RE 0.09033157756504277 Lambda1 0.9721212\n",
      "118 Train Loss 25.463507 Test MSE 28.770855531221628 Test RE 0.09029538597626685 Lambda1 0.97302246\n",
      "119 Train Loss 25.445625 Test MSE 28.75719707557868 Test RE 0.09027395036186789 Lambda1 0.97452605\n",
      "120 Train Loss 25.39445 Test MSE 28.730620528284735 Test RE 0.0902322264679923 Lambda1 0.96959317\n",
      "121 Train Loss 25.374727 Test MSE 28.712483532719215 Test RE 0.09020374118259386 Lambda1 0.9660129\n",
      "122 Train Loss 25.36229 Test MSE 28.698470510017255 Test RE 0.09018172669305752 Lambda1 0.9594658\n",
      "123 Train Loss 25.357275 Test MSE 28.704131020356606 Test RE 0.09019062001416776 Lambda1 0.95539254\n",
      "124 Train Loss 25.352373 Test MSE 28.710018318631942 Test RE 0.09019986871529984 Lambda1 0.94941777\n",
      "125 Train Loss 25.34533 Test MSE 28.714162685034754 Test RE 0.09020637877411876 Lambda1 0.9435304\n",
      "126 Train Loss 25.339153 Test MSE 28.706860287443547 Test RE 0.09019490769735183 Lambda1 0.943748\n",
      "127 Train Loss 25.330769 Test MSE 28.701933919317646 Test RE 0.09018716821664001 Lambda1 0.94570595\n",
      "128 Train Loss 25.32376 Test MSE 28.71011362923862 Test RE 0.09020001843650478 Lambda1 0.94541097\n",
      "129 Train Loss 25.318035 Test MSE 28.68911158059464 Test RE 0.09016702080069382 Lambda1 0.94563\n",
      "130 Train Loss 25.312124 Test MSE 28.68454182135264 Test RE 0.09015983936559012 Lambda1 0.94346577\n",
      "131 Train Loss 25.307163 Test MSE 28.67948308878818 Test RE 0.09015188883549391 Lambda1 0.9439948\n",
      "132 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "133 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "134 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "136 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "137 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "138 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "139 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "140 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "141 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "142 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "143 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "144 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "145 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "146 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "147 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "148 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "149 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "150 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "151 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "152 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "153 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "154 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "155 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "156 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "157 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "158 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "159 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "160 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "161 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "162 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "163 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "164 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "165 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "166 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "167 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "168 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "169 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "170 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "171 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "172 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "173 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "174 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "175 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "176 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "177 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "178 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "179 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "180 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "181 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "182 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "183 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "184 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "185 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "186 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "187 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "188 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "189 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "190 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "191 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "192 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "193 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "194 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "195 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "196 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "197 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "198 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "199 Train Loss 25.30205 Test MSE 28.675082617641095 Test RE 0.09014497228816519 Lambda1 0.94415754\n",
      "Training time: 490.77\n",
      "Training time: 490.77\n",
      "inv_HT_stan\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 847.12726 Test MSE 850.867653977291 Test RE 0.49104373866304096 Lambda1 0.0023201485\n",
      "1 Train Loss 740.145 Test MSE 745.5123054370821 Test RE 0.4596386906145114 Lambda1 0.20108867\n",
      "2 Train Loss 655.66034 Test MSE 662.2743619813294 Test RE 0.43321964495674015 Lambda1 0.17023773\n",
      "3 Train Loss 583.3464 Test MSE 589.3942670804545 Test RE 0.408688224171334 Lambda1 0.12907705\n",
      "4 Train Loss 445.0255 Test MSE 423.41888204907457 Test RE 0.3463971268920444 Lambda1 0.09767881\n",
      "5 Train Loss 293.23447 Test MSE 292.4071087385622 Test RE 0.28786120161336937 Lambda1 0.05004996\n",
      "6 Train Loss 273.6472 Test MSE 277.62129770400196 Test RE 0.28048882336017594 Lambda1 0.043015238\n",
      "7 Train Loss 260.85437 Test MSE 264.6163417100348 Test RE 0.2738403878068322 Lambda1 0.054337684\n",
      "8 Train Loss 246.24127 Test MSE 247.48831719619744 Test RE 0.26482959841588466 Lambda1 0.1022694\n",
      "9 Train Loss 226.2334 Test MSE 226.99633064497354 Test RE 0.2536288129603321 Lambda1 0.14132978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Train Loss 207.52885 Test MSE 202.4526334010606 Test RE 0.23952502035446593 Lambda1 0.2172895\n",
      "11 Train Loss 185.57205 Test MSE 173.8780042722442 Test RE 0.2219787973120089 Lambda1 0.2980865\n",
      "12 Train Loss 153.81061 Test MSE 143.88471611940238 Test RE 0.20192798091493633 Lambda1 0.41995907\n",
      "13 Train Loss 129.29813 Test MSE 114.93932952505111 Test RE 0.1804776803197772 Lambda1 0.5217536\n",
      "14 Train Loss 115.099335 Test MSE 107.27490829485161 Test RE 0.17435654180004317 Lambda1 0.5313796\n",
      "15 Train Loss 103.634766 Test MSE 95.57240900448912 Test RE 0.16457180677650002 Lambda1 0.5488548\n",
      "16 Train Loss 91.49078 Test MSE 86.16323679268605 Test RE 0.15626084671373053 Lambda1 0.59544253\n",
      "17 Train Loss 84.85209 Test MSE 79.41262492374102 Test RE 0.15001474441015236 Lambda1 0.61604595\n",
      "18 Train Loss 79.62089 Test MSE 75.29829987286854 Test RE 0.1460769713389698 Lambda1 0.64396673\n",
      "19 Train Loss 74.799965 Test MSE 67.99940299731034 Test RE 0.13881670145659428 Lambda1 0.6780329\n",
      "20 Train Loss 68.6359 Test MSE 60.480390767643996 Test RE 0.13091712664293145 Lambda1 0.72492605\n",
      "21 Train Loss 62.937313 Test MSE 58.99588007790437 Test RE 0.12930044296313992 Lambda1 0.7317814\n",
      "22 Train Loss 57.566692 Test MSE 54.576408232687264 Test RE 0.12436313057358477 Lambda1 0.7702739\n",
      "23 Train Loss 54.52141 Test MSE 49.018332604131864 Test RE 0.1178605430455676 Lambda1 0.81535155\n",
      "24 Train Loss 48.824192 Test MSE 43.3164874179884 Test RE 0.11079388382381547 Lambda1 0.8601902\n",
      "25 Train Loss 45.83077 Test MSE 42.56401649635031 Test RE 0.10982734173543116 Lambda1 0.8679458\n",
      "26 Train Loss 42.50395 Test MSE 39.1733454054017 Test RE 0.1053621202582508 Lambda1 0.9102037\n",
      "27 Train Loss 39.56275 Test MSE 36.91380544085697 Test RE 0.10227831865310982 Lambda1 0.9455631\n",
      "28 Train Loss 37.444027 Test MSE 36.22346140059647 Test RE 0.1013174252179552 Lambda1 0.95322806\n",
      "29 Train Loss 36.670982 Test MSE 35.93491634346624 Test RE 0.10091308652812557 Lambda1 0.9518563\n",
      "30 Train Loss 35.136806 Test MSE 33.843402282761716 Test RE 0.09793235061581823 Lambda1 0.98880726\n",
      "31 Train Loss 33.611748 Test MSE 34.01646310246973 Test RE 0.09818242360502889 Lambda1 0.99640983\n",
      "32 Train Loss 33.063454 Test MSE 33.83148618407031 Test RE 0.09791510834382942 Lambda1 1.0050353\n",
      "33 Train Loss 32.80225 Test MSE 33.837374616000986 Test RE 0.09792362912475824 Lambda1 1.0068396\n",
      "34 Train Loss 31.859426 Test MSE 33.29722505981605 Test RE 0.0971389022813465 Lambda1 1.0304067\n",
      "35 Train Loss 31.366117 Test MSE 32.96191537198189 Test RE 0.09664856065223268 Lambda1 1.0518956\n",
      "36 Train Loss 30.701347 Test MSE 32.83724323699116 Test RE 0.09646561015492496 Lambda1 1.0578041\n",
      "37 Train Loss 30.336777 Test MSE 32.928414047497256 Test RE 0.09659943307752338 Lambda1 1.0750252\n",
      "38 Train Loss 30.118746 Test MSE 32.99253382823415 Test RE 0.09669343885184074 Lambda1 1.0807102\n",
      "39 Train Loss 29.988194 Test MSE 32.94953404031989 Test RE 0.09663040712165682 Lambda1 1.0872834\n",
      "40 Train Loss 29.83848 Test MSE 32.82635916982386 Test RE 0.09644962182745012 Lambda1 1.0926874\n",
      "41 Train Loss 29.710617 Test MSE 32.66507095324732 Test RE 0.09621238347202497 Lambda1 1.1019726\n",
      "42 Train Loss 29.489517 Test MSE 32.464950092001914 Test RE 0.09591721060112382 Lambda1 1.0880508\n",
      "43 Train Loss 29.4358 Test MSE 32.457958779696106 Test RE 0.09590688218109757 Lambda1 1.0874648\n",
      "44 Train Loss 29.31554 Test MSE 32.48964945075868 Test RE 0.09595369060737899 Lambda1 1.0912228\n",
      "45 Train Loss 29.233107 Test MSE 32.40954940867316 Test RE 0.09583533541058199 Lambda1 1.091738\n",
      "46 Train Loss 29.115263 Test MSE 32.23346638942121 Test RE 0.09557464128365553 Lambda1 1.078551\n",
      "47 Train Loss 29.000689 Test MSE 32.10042936657679 Test RE 0.09537720498670635 Lambda1 1.0670927\n",
      "48 Train Loss 28.942884 Test MSE 32.15482580206064 Test RE 0.09545798247171751 Lambda1 1.0675714\n",
      "49 Train Loss 28.902668 Test MSE 32.173881456781594 Test RE 0.0954862635289438 Lambda1 1.0744507\n",
      "50 Train Loss 28.859024 Test MSE 32.147281433942254 Test RE 0.09544678333769463 Lambda1 1.0728799\n",
      "51 Train Loss 28.785547 Test MSE 32.17256549716703 Test RE 0.09548431074386213 Lambda1 1.0727186\n",
      "52 Train Loss 28.71967 Test MSE 32.09725183046124 Test RE 0.09537248430197277 Lambda1 1.0692172\n",
      "53 Train Loss 28.643574 Test MSE 31.947789922982956 Test RE 0.0951501726466372 Lambda1 1.0683663\n",
      "54 Train Loss 28.586445 Test MSE 31.866381260533405 Test RE 0.09502886552053083 Lambda1 1.0655082\n",
      "55 Train Loss 28.57018 Test MSE 31.829052269187684 Test RE 0.0949731897421231 Lambda1 1.0675493\n",
      "56 Train Loss 28.554583 Test MSE 31.847247247148747 Test RE 0.09500033142692313 Lambda1 1.0718023\n",
      "57 Train Loss 28.535358 Test MSE 31.80066916312045 Test RE 0.09493083479998313 Lambda1 1.0715995\n",
      "58 Train Loss 28.50467 Test MSE 31.74840784712378 Test RE 0.09485279791460358 Lambda1 1.0680288\n",
      "59 Train Loss 28.472008 Test MSE 31.720586112063355 Test RE 0.09481122814795119 Lambda1 1.0652835\n",
      "60 Train Loss 28.426323 Test MSE 31.72872177202033 Test RE 0.09482338590659196 Lambda1 1.0684161\n",
      "61 Train Loss 28.403112 Test MSE 31.690549563360726 Test RE 0.09476632865374911 Lambda1 1.0658699\n",
      "62 Train Loss 28.275406 Test MSE 31.56382900407362 Test RE 0.09457666846650449 Lambda1 1.0479087\n",
      "63 Train Loss 28.168224 Test MSE 31.504509257427284 Test RE 0.09448775494439389 Lambda1 1.0378201\n",
      "64 Train Loss 28.124945 Test MSE 31.430681690968164 Test RE 0.09437697885653872 Lambda1 1.0370183\n",
      "65 Train Loss 28.092228 Test MSE 31.374248720942973 Test RE 0.09429221508842513 Lambda1 1.0301206\n",
      "66 Train Loss 28.067629 Test MSE 31.35751417538994 Test RE 0.09426706471987296 Lambda1 1.0326903\n",
      "67 Train Loss 28.02723 Test MSE 31.225522441345475 Test RE 0.09406845850578882 Lambda1 1.0286742\n",
      "68 Train Loss 27.982721 Test MSE 31.116358565271053 Test RE 0.0939038837057615 Lambda1 1.0255866\n",
      "69 Train Loss 27.894117 Test MSE 31.1026152159908 Test RE 0.09388314386882035 Lambda1 1.020798\n",
      "70 Train Loss 27.829824 Test MSE 30.97359490037416 Test RE 0.09368821780637109 Lambda1 1.0142016\n",
      "71 Train Loss 27.788439 Test MSE 30.945580712270107 Test RE 0.09364583988590364 Lambda1 1.0122164\n",
      "72 Train Loss 27.758633 Test MSE 30.987321430645963 Test RE 0.09370897535345592 Lambda1 1.0172372\n",
      "73 Train Loss 27.741602 Test MSE 30.99075799656756 Test RE 0.09371417148090756 Lambda1 1.0228517\n",
      "74 Train Loss 27.68781 Test MSE 30.954541407951748 Test RE 0.09365939709047308 Lambda1 1.0261177\n",
      "75 Train Loss 27.594744 Test MSE 30.771224602910923 Test RE 0.09338165372923964 Lambda1 1.0292304\n",
      "76 Train Loss 27.487932 Test MSE 30.613339849059248 Test RE 0.09314177864682618 Lambda1 1.0336777\n",
      "77 Train Loss 27.40032 Test MSE 30.522152874071804 Test RE 0.0930029559765184 Lambda1 1.0274976\n",
      "78 Train Loss 27.32013 Test MSE 30.54663719021787 Test RE 0.0930402511388927 Lambda1 1.024473\n",
      "79 Train Loss 27.287144 Test MSE 30.48374625191536 Test RE 0.09294442383809029 Lambda1 1.0209733\n",
      "80 Train Loss 27.24681 Test MSE 30.42855997228095 Test RE 0.09286025471293581 Lambda1 1.0182875\n",
      "81 Train Loss 27.217644 Test MSE 30.418647340789867 Test RE 0.0928451280611234 Lambda1 1.0175583\n",
      "82 Train Loss 27.180908 Test MSE 30.338640172186942 Test RE 0.09272294697543998 Lambda1 1.0158085\n",
      "83 Train Loss 27.089302 Test MSE 30.22574810896497 Test RE 0.09255027211877088 Lambda1 1.0109352\n",
      "84 Train Loss 27.044321 Test MSE 30.18625511106338 Test RE 0.09248978920854643 Lambda1 1.0132968\n",
      "85 Train Loss 27.01147 Test MSE 30.170311933916597 Test RE 0.09246536127200089 Lambda1 1.0129361\n",
      "86 Train Loss 26.981197 Test MSE 30.113723708679387 Test RE 0.09237860534889807 Lambda1 1.0125399\n",
      "87 Train Loss 26.961592 Test MSE 30.070969250414638 Test RE 0.09231300402808919 Lambda1 1.0132775\n",
      "88 Train Loss 26.946617 Test MSE 30.045866793295136 Test RE 0.09227446574537423 Lambda1 1.0134453\n",
      "89 Train Loss 26.922878 Test MSE 30.0493086121446 Test RE 0.09227975071357003 Lambda1 1.011888\n",
      "90 Train Loss 26.885864 Test MSE 30.047791703754566 Test RE 0.09227742151364551 Lambda1 1.0152572\n",
      "91 Train Loss 26.87246 Test MSE 30.019371951032767 Test RE 0.09223377235095465 Lambda1 1.0191739\n",
      "92 Train Loss 26.854372 Test MSE 29.98921978305266 Test RE 0.09218743982123849 Lambda1 1.0219215\n",
      "93 Train Loss 26.820057 Test MSE 29.95158827887399 Test RE 0.09212958168024434 Lambda1 1.0326307\n",
      "94 Train Loss 26.784275 Test MSE 29.93848447734946 Test RE 0.0921094261577214 Lambda1 1.0391318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 Train Loss 26.748966 Test MSE 29.901212627169315 Test RE 0.09205207258820086 Lambda1 1.0378906\n",
      "96 Train Loss 26.70886 Test MSE 29.872083532121835 Test RE 0.09200722412399424 Lambda1 1.036228\n",
      "97 Train Loss 26.684345 Test MSE 29.861791285251396 Test RE 0.09199137249039427 Lambda1 1.0291097\n",
      "98 Train Loss 26.673304 Test MSE 29.867523359316703 Test RE 0.09200020109771966 Lambda1 1.0278687\n",
      "99 Train Loss 26.662766 Test MSE 29.883097103070615 Test RE 0.09202418368266642 Lambda1 1.0244478\n",
      "100 Train Loss 26.630917 Test MSE 29.829776718294934 Test RE 0.0919420476911274 Lambda1 1.0206659\n",
      "101 Train Loss 26.573505 Test MSE 29.7847550757414 Test RE 0.0918726381024681 Lambda1 1.0170518\n",
      "102 Train Loss 26.507856 Test MSE 29.77296308125858 Test RE 0.09185444978955881 Lambda1 1.0228224\n",
      "103 Train Loss 26.437252 Test MSE 29.661508727464465 Test RE 0.09168236115124856 Lambda1 1.0316238\n",
      "104 Train Loss 26.421246 Test MSE 29.634388634658627 Test RE 0.09164043808565793 Lambda1 1.0324675\n",
      "105 Train Loss 26.39649 Test MSE 29.613426108401462 Test RE 0.09160802042806113 Lambda1 1.0336705\n",
      "106 Train Loss 26.37488 Test MSE 29.587606204178698 Test RE 0.09156807526758907 Lambda1 1.0337125\n",
      "107 Train Loss 26.359709 Test MSE 29.591938936318318 Test RE 0.09157477951761542 Lambda1 1.0343359\n",
      "108 Train Loss 26.345482 Test MSE 29.53954115098804 Test RE 0.0914936688881405 Lambda1 1.0337929\n",
      "109 Train Loss 26.322216 Test MSE 29.500050250683486 Test RE 0.09143249028449933 Lambda1 1.0307856\n",
      "110 Train Loss 26.303827 Test MSE 29.451478251821676 Test RE 0.09135718721993048 Lambda1 1.0304908\n",
      "111 Train Loss 26.286518 Test MSE 29.459794520834166 Test RE 0.09137008465946048 Lambda1 1.0328025\n",
      "112 Train Loss 26.267847 Test MSE 29.472357956163002 Test RE 0.09138956544350088 Lambda1 1.0310783\n",
      "113 Train Loss 26.207764 Test MSE 29.400523409176515 Test RE 0.09127812316956903 Lambda1 1.024321\n",
      "114 Train Loss 26.186588 Test MSE 29.401305708196478 Test RE 0.0912793375409989 Lambda1 1.0217582\n",
      "115 Train Loss 26.14276 Test MSE 29.314255184740738 Test RE 0.09114410877900182 Lambda1 1.0142184\n",
      "116 Train Loss 26.116543 Test MSE 29.278222501016526 Test RE 0.09108807500536013 Lambda1 1.0086284\n",
      "117 Train Loss 26.094296 Test MSE 29.206486333423218 Test RE 0.09097641664150502 Lambda1 1.001561\n",
      "118 Train Loss 26.080374 Test MSE 29.199263821702637 Test RE 0.09096516710509289 Lambda1 0.9966119\n",
      "119 Train Loss 26.069582 Test MSE 29.201560392326968 Test RE 0.09096874431553745 Lambda1 0.99007237\n",
      "120 Train Loss 26.038263 Test MSE 29.20838487945602 Test RE 0.09097937352077633 Lambda1 0.98134816\n",
      "121 Train Loss 26.009502 Test MSE 29.19815886332811 Test RE 0.09096344593714299 Lambda1 0.9799779\n",
      "122 Train Loss 25.993467 Test MSE 29.192351362582656 Test RE 0.09095439919290556 Lambda1 0.9755007\n",
      "123 Train Loss 25.973139 Test MSE 29.20882432187327 Test RE 0.09098005791406973 Lambda1 0.96502113\n",
      "124 Train Loss 25.955845 Test MSE 29.232847963781992 Test RE 0.09101746481335907 Lambda1 0.9570511\n",
      "125 Train Loss 25.939007 Test MSE 29.2630956179525 Test RE 0.09106454118805386 Lambda1 0.95250005\n",
      "126 Train Loss 25.903633 Test MSE 29.278168213446904 Test RE 0.0910879905577386 Lambda1 0.94745845\n",
      "127 Train Loss 25.879812 Test MSE 29.27178282722372 Test RE 0.09107805715425853 Lambda1 0.9423456\n",
      "128 Train Loss 25.860416 Test MSE 29.254596714171324 Test RE 0.09105131625473958 Lambda1 0.939195\n",
      "129 Train Loss 25.841381 Test MSE 29.259337555539386 Test RE 0.09105869359725156 Lambda1 0.9343717\n",
      "130 Train Loss 25.81217 Test MSE 29.17553555437859 Test RE 0.09092819897296071 Lambda1 0.9336848\n",
      "131 Train Loss 25.783302 Test MSE 29.112929971859 Test RE 0.09083058859710907 Lambda1 0.9398205\n",
      "132 Train Loss 25.768705 Test MSE 29.115368087966157 Test RE 0.09083439190520859 Lambda1 0.94547975\n",
      "133 Train Loss 25.753819 Test MSE 29.071924238158086 Test RE 0.09076659834642993 Lambda1 0.9449779\n",
      "134 Train Loss 25.746424 Test MSE 29.05570330107006 Test RE 0.0907412728003788 Lambda1 0.9434323\n",
      "135 Train Loss 25.73198 Test MSE 29.059645542694557 Test RE 0.09074742842301031 Lambda1 0.9434316\n",
      "136 Train Loss 25.721388 Test MSE 29.03349646766344 Test RE 0.09070659008288313 Lambda1 0.944551\n",
      "137 Train Loss 25.687723 Test MSE 29.02112254388756 Test RE 0.09068725868667561 Lambda1 0.94560766\n",
      "138 Train Loss 25.664394 Test MSE 28.98211983627428 Test RE 0.09062629898696353 Lambda1 0.9421565\n",
      "139 Train Loss 25.642221 Test MSE 28.997180891043712 Test RE 0.09064984368244057 Lambda1 0.9351955\n",
      "140 Train Loss 25.629425 Test MSE 29.01269387672012 Test RE 0.0906740884823476 Lambda1 0.92973685\n",
      "141 Train Loss 25.609152 Test MSE 28.972582005279435 Test RE 0.0906113854910674 Lambda1 0.9328046\n",
      "142 Train Loss 25.58848 Test MSE 28.928604693025044 Test RE 0.09054259012990784 Lambda1 0.935602\n",
      "143 Train Loss 25.568506 Test MSE 28.927561458154493 Test RE 0.09054095752370543 Lambda1 0.9376866\n",
      "144 Train Loss 25.557207 Test MSE 28.877685540816298 Test RE 0.09046287003087465 Lambda1 0.94191813\n",
      "145 Train Loss 25.542992 Test MSE 28.875569631543428 Test RE 0.09045955579830484 Lambda1 0.94444245\n",
      "146 Train Loss 25.531448 Test MSE 28.881886044934195 Test RE 0.09046944908722544 Lambda1 0.94390386\n",
      "147 Train Loss 25.513908 Test MSE 28.907576368848222 Test RE 0.09050967625310641 Lambda1 0.9431425\n",
      "148 Train Loss 25.490847 Test MSE 28.860345749095845 Test RE 0.0904357064470175 Lambda1 0.95118934\n",
      "149 Train Loss 25.476372 Test MSE 28.84524126616752 Test RE 0.09041203792672689 Lambda1 0.9564019\n",
      "150 Train Loss 25.46562 Test MSE 28.798330642377557 Test RE 0.09033849007791903 Lambda1 0.9608125\n",
      "151 Train Loss 25.450123 Test MSE 28.75631190733318 Test RE 0.09027256100098961 Lambda1 0.962916\n",
      "152 Train Loss 25.434546 Test MSE 28.734181535493978 Test RE 0.09023781819638636 Lambda1 0.9663589\n",
      "153 Train Loss 25.415583 Test MSE 28.70378190017574 Test RE 0.09019007153107154 Lambda1 0.97002417\n",
      "154 Train Loss 25.40254 Test MSE 28.713909909275767 Test RE 0.09020598172203871 Lambda1 0.9678166\n",
      "155 Train Loss 25.392405 Test MSE 28.705764299274573 Test RE 0.0901931859225865 Lambda1 0.9734586\n",
      "156 Train Loss 25.38186 Test MSE 28.694278147110982 Test RE 0.09017513943722677 Lambda1 0.97961795\n",
      "157 Train Loss 25.367239 Test MSE 28.6500947029292 Test RE 0.09010568685964995 Lambda1 0.98768085\n",
      "158 Train Loss 25.347927 Test MSE 28.645292978711932 Test RE 0.09009813573646593 Lambda1 0.99187094\n",
      "159 Train Loss 25.334017 Test MSE 28.61561424186981 Test RE 0.09005144932556296 Lambda1 0.99072325\n",
      "160 Train Loss 25.310667 Test MSE 28.62807419373301 Test RE 0.09007105251493046 Lambda1 0.9853603\n",
      "161 Train Loss 25.300375 Test MSE 28.62530745473638 Test RE 0.09006669998473443 Lambda1 0.98100764\n",
      "162 Train Loss 25.294188 Test MSE 28.590062128541398 Test RE 0.09001123493919463 Lambda1 0.983665\n",
      "163 Train Loss 25.289267 Test MSE 28.57620245640434 Test RE 0.08998941481464372 Lambda1 0.9865406\n",
      "164 Train Loss 25.282509 Test MSE 28.565815734144465 Test RE 0.08997305889734959 Lambda1 0.98940206\n",
      "165 Train Loss 25.277203 Test MSE 28.530207743456966 Test RE 0.08991696459765866 Lambda1 0.9930507\n",
      "166 Train Loss 25.272808 Test MSE 28.50508845027176 Test RE 0.08987737238735936 Lambda1 0.99610806\n",
      "167 Train Loss 25.267078 Test MSE 28.51243391207662 Test RE 0.08988895186871346 Lambda1 0.9944045\n",
      "168 Train Loss 25.260624 Test MSE 28.52401940544704 Test RE 0.08990721235975929 Lambda1 0.99013424\n",
      "169 Train Loss 25.246502 Test MSE 28.49025669543777 Test RE 0.0898539868679387 Lambda1 0.9879792\n",
      "170 Train Loss 25.235477 Test MSE 28.48993759952207 Test RE 0.08985348367625447 Lambda1 0.9928363\n",
      "171 Train Loss 25.227934 Test MSE 28.470687167131466 Test RE 0.0898231218915954 Lambda1 0.995421\n",
      "172 Train Loss 25.222406 Test MSE 28.46143834825696 Test RE 0.08980853100272845 Lambda1 0.99586654\n",
      "173 Train Loss 25.218138 Test MSE 28.4438461828703 Test RE 0.08978077114873032 Lambda1 0.9972967\n",
      "174 Train Loss 25.21316 Test MSE 28.450871381981568 Test RE 0.08979185770941175 Lambda1 0.99892396\n",
      "175 Train Loss 25.194607 Test MSE 28.441465166684154 Test RE 0.08977701332459943 Lambda1 0.9921145\n",
      "176 Train Loss 25.180311 Test MSE 28.470887185272243 Test RE 0.08982343741299781 Lambda1 0.9838806\n",
      "177 Train Loss 25.17104 Test MSE 28.458717169571795 Test RE 0.08980423763394343 Lambda1 0.9824641\n",
      "178 Train Loss 25.165102 Test MSE 28.438850514871646 Test RE 0.08977288658527091 Lambda1 0.9825855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 Train Loss 25.161768 Test MSE 28.424462178605648 Test RE 0.08975017388985616 Lambda1 0.98322904\n",
      "180 Train Loss 25.157473 Test MSE 28.402175879290933 Test RE 0.08971498252113526 Lambda1 0.9862229\n",
      "181 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "182 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "183 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "184 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "185 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "186 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "187 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "188 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "189 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "190 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "191 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "192 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "193 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "194 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "195 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "196 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "197 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "198 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "199 Train Loss 25.15572 Test MSE 28.40264206140571 Test RE 0.08971571879128651 Lambda1 0.9880209\n",
      "Training time: 534.80\n",
      "Training time: 534.80\n",
      "inv_HT_stan\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 818.89215 Test MSE 807.4880013892638 Test RE 0.4783625909961944 Lambda1 0.043007154\n",
      "1 Train Loss 652.64325 Test MSE 666.0075185284104 Test RE 0.43443893121602145 Lambda1 0.001054767\n",
      "2 Train Loss 606.0327 Test MSE 621.4269197359332 Test RE 0.41964707652806393 Lambda1 0.0001843024\n",
      "3 Train Loss 363.9663 Test MSE 344.5846406779009 Test RE 0.3124907226802287 Lambda1 0.0014248318\n",
      "4 Train Loss 281.09988 Test MSE 285.62632785410045 Test RE 0.28450394240395943 Lambda1 2.6445741e-05\n",
      "5 Train Loss 273.70953 Test MSE 280.8856030436014 Test RE 0.2821330153609047 Lambda1 0.00011190859\n",
      "6 Train Loss 271.27524 Test MSE 278.71659402451047 Test RE 0.2810415834665567 Lambda1 0.00088535756\n",
      "7 Train Loss 270.8778 Test MSE 278.46771766450155 Test RE 0.2809160792288991 Lambda1 0.0012373191\n",
      "8 Train Loss 270.104 Test MSE 277.87414277049567 Test RE 0.2806165226314285 Lambda1 0.0027248678\n",
      "9 Train Loss 269.35574 Test MSE 275.98116056093113 Test RE 0.2796590570300975 Lambda1 0.008937622\n",
      "10 Train Loss 266.86206 Test MSE 273.7286976963084 Test RE 0.27851547858416115 Lambda1 0.017873947\n",
      "11 Train Loss 261.49588 Test MSE 266.0731939151001 Test RE 0.2745931708286573 Lambda1 0.047646735\n",
      "12 Train Loss 254.2274 Test MSE 255.07623776945826 Test RE 0.2688587480622127 Lambda1 0.07805686\n",
      "13 Train Loss 246.19414 Test MSE 248.79140310794125 Test RE 0.26552587907559433 Lambda1 0.103523135\n",
      "14 Train Loss 237.21939 Test MSE 241.31223457553713 Test RE 0.2615043040730191 Lambda1 0.1393798\n",
      "15 Train Loss 234.30605 Test MSE 238.06536374021405 Test RE 0.25973906794303125 Lambda1 0.1636382\n",
      "16 Train Loss 230.38466 Test MSE 235.16761707806322 Test RE 0.25815344792329226 Lambda1 0.20131335\n",
      "17 Train Loss 224.68369 Test MSE 225.2224930203131 Test RE 0.25263589228835415 Lambda1 0.24654049\n",
      "18 Train Loss 204.67859 Test MSE 198.57637564742845 Test RE 0.2372209061661154 Lambda1 0.26529056\n",
      "19 Train Loss 186.06305 Test MSE 186.0718799755963 Test RE 0.22963048516920284 Lambda1 0.29454744\n",
      "20 Train Loss 174.03409 Test MSE 168.4184679527125 Test RE 0.21846608541015564 Lambda1 0.32710412\n",
      "21 Train Loss 158.0616 Test MSE 150.04569826229053 Test RE 0.20620583251034721 Lambda1 0.40537974\n",
      "22 Train Loss 144.87192 Test MSE 138.62135140472387 Test RE 0.198200266838263 Lambda1 0.45449165\n",
      "23 Train Loss 133.51071 Test MSE 124.5818182131053 Test RE 0.1878955536433418 Lambda1 0.5151761\n",
      "24 Train Loss 128.3795 Test MSE 122.62684874875913 Test RE 0.18641547190611987 Lambda1 0.52189153\n",
      "25 Train Loss 123.0852 Test MSE 117.58384725457842 Test RE 0.18254208366506797 Lambda1 0.5395139\n",
      "26 Train Loss 119.6441 Test MSE 114.21576357261013 Test RE 0.17990871200743253 Lambda1 0.57087064\n",
      "27 Train Loss 116.042725 Test MSE 111.788440055305 Test RE 0.17798672768762233 Lambda1 0.5935251\n",
      "28 Train Loss 110.98291 Test MSE 106.21707146352888 Test RE 0.17349474791658415 Lambda1 0.6226381\n",
      "29 Train Loss 106.20468 Test MSE 102.3109961485155 Test RE 0.17027477976090918 Lambda1 0.6384752\n",
      "30 Train Loss 102.842514 Test MSE 97.20213890212216 Test RE 0.16596903979169608 Lambda1 0.6705704\n",
      "31 Train Loss 98.54845 Test MSE 90.37848729267235 Test RE 0.16003748016200742 Lambda1 0.7197446\n",
      "32 Train Loss 95.07212 Test MSE 86.66182430370613 Test RE 0.15671229983588608 Lambda1 0.7510779\n",
      "33 Train Loss 91.03346 Test MSE 84.80784735302973 Test RE 0.15502694561941777 Lambda1 0.80531317\n",
      "34 Train Loss 87.7833 Test MSE 80.0444044711174 Test RE 0.1506102951284581 Lambda1 0.85798347\n",
      "35 Train Loss 85.8094 Test MSE 79.67423079348234 Test RE 0.15026163506672702 Lambda1 0.8470158\n",
      "36 Train Loss 82.384285 Test MSE 76.80954650846233 Test RE 0.14753558094718877 Lambda1 0.89372283\n",
      "37 Train Loss 78.98238 Test MSE 73.01526890658153 Test RE 0.14384541221917524 Lambda1 0.9589491\n",
      "38 Train Loss 77.26174 Test MSE 72.8452755515297 Test RE 0.14367786497970322 Lambda1 0.9538593\n",
      "39 Train Loss 75.51861 Test MSE 70.06060267509282 Test RE 0.14090490292410376 Lambda1 0.9766778\n",
      "40 Train Loss 73.045944 Test MSE 68.82635080191962 Test RE 0.1396582328647881 Lambda1 0.9793456\n",
      "41 Train Loss 70.32451 Test MSE 65.67034611078095 Test RE 0.13641867381078213 Lambda1 1.0273807\n",
      "42 Train Loss 67.59762 Test MSE 62.39670164575755 Test RE 0.13297499637352425 Lambda1 1.0535583\n",
      "43 Train Loss 66.16027 Test MSE 60.995010704682755 Test RE 0.13147292541492953 Lambda1 1.0653942\n",
      "44 Train Loss 64.82999 Test MSE 58.4339150546947 Test RE 0.1286831439960113 Lambda1 1.0841322\n",
      "45 Train Loss 62.366074 Test MSE 56.314418425423256 Test RE 0.12632781145076555 Lambda1 1.1034645\n",
      "46 Train Loss 61.18776 Test MSE 55.175794041599616 Test RE 0.12504417528772382 Lambda1 1.1075406\n",
      "47 Train Loss 59.110977 Test MSE 53.23753801468597 Test RE 0.12282821831349008 Lambda1 1.1110646\n",
      "48 Train Loss 53.593075 Test MSE 49.402651666661306 Test RE 0.11832167272454405 Lambda1 1.1467382\n",
      "49 Train Loss 52.37939 Test MSE 48.42851028462941 Test RE 0.1171493074600531 Lambda1 1.1625808\n",
      "50 Train Loss 51.01461 Test MSE 46.98832538324551 Test RE 0.11539424615988587 Lambda1 1.1879401\n",
      "51 Train Loss 50.528934 Test MSE 46.42288155336651 Test RE 0.11469783433024709 Lambda1 1.1988797\n",
      "52 Train Loss 49.801384 Test MSE 46.53397551850775 Test RE 0.11483499323305758 Lambda1 1.2002827\n",
      "53 Train Loss 47.994545 Test MSE 44.776306037515994 Test RE 0.11264535855745365 Lambda1 1.2391218\n",
      "54 Train Loss 47.188927 Test MSE 43.71123330354475 Test RE 0.11129757471496797 Lambda1 1.2444754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 Train Loss 46.451317 Test MSE 42.131665456251454 Test RE 0.10926812332470026 Lambda1 1.2715474\n",
      "56 Train Loss 45.88726 Test MSE 42.01938075812078 Test RE 0.10912242146824902 Lambda1 1.277944\n",
      "57 Train Loss 45.264294 Test MSE 42.00231848539414 Test RE 0.10910026424546794 Lambda1 1.2740788\n",
      "58 Train Loss 44.345627 Test MSE 42.06119330278651 Test RE 0.10917670060738752 Lambda1 1.2740512\n",
      "59 Train Loss 42.811115 Test MSE 40.21158900990686 Test RE 0.10674923912425972 Lambda1 1.3092668\n",
      "60 Train Loss 41.71053 Test MSE 39.295541282313366 Test RE 0.10552632364537813 Lambda1 1.3205192\n",
      "61 Train Loss 40.89683 Test MSE 38.850951039556065 Test RE 0.10492766244443762 Lambda1 1.3244792\n",
      "62 Train Loss 40.153263 Test MSE 38.880891938596356 Test RE 0.10496808646771745 Lambda1 1.3170894\n",
      "63 Train Loss 39.78109 Test MSE 38.19132024031708 Test RE 0.10403309201337146 Lambda1 1.3285902\n",
      "64 Train Loss 39.44091 Test MSE 37.77307499864947 Test RE 0.10346187414418996 Lambda1 1.3352462\n",
      "65 Train Loss 38.81558 Test MSE 37.61860171628463 Test RE 0.10325010334552999 Lambda1 1.347131\n",
      "66 Train Loss 38.417526 Test MSE 37.555030016735465 Test RE 0.10316282524865485 Lambda1 1.3557999\n",
      "67 Train Loss 38.234844 Test MSE 37.31542166789732 Test RE 0.10283319925755431 Lambda1 1.3606157\n",
      "68 Train Loss 38.12179 Test MSE 37.25767453149432 Test RE 0.10275359916532265 Lambda1 1.3657942\n",
      "69 Train Loss 37.797527 Test MSE 36.8630911051799 Test RE 0.10220803655010931 Lambda1 1.3764488\n",
      "70 Train Loss 37.476826 Test MSE 36.57379498697716 Test RE 0.10180618976809247 Lambda1 1.3789322\n",
      "71 Train Loss 37.156925 Test MSE 36.53531493083807 Test RE 0.10175261946093077 Lambda1 1.375537\n",
      "72 Train Loss 36.84096 Test MSE 36.500258477668694 Test RE 0.10170379078610983 Lambda1 1.3654991\n",
      "73 Train Loss 36.594288 Test MSE 36.504477190273384 Test RE 0.10170966809609401 Lambda1 1.3660495\n",
      "74 Train Loss 36.25121 Test MSE 36.34951883920998 Test RE 0.10149356414073563 Lambda1 1.3712325\n",
      "75 Train Loss 35.91691 Test MSE 36.05837852473969 Test RE 0.10108629225723224 Lambda1 1.379788\n",
      "76 Train Loss 35.684853 Test MSE 35.94114385755151 Test RE 0.10092183025855564 Lambda1 1.384446\n",
      "77 Train Loss 35.347515 Test MSE 35.76171581777102 Test RE 0.10066960034736816 Lambda1 1.3902566\n",
      "78 Train Loss 35.194996 Test MSE 35.56702517785957 Test RE 0.10039519828536268 Lambda1 1.3938315\n",
      "79 Train Loss 34.846817 Test MSE 35.47072294339375 Test RE 0.10025918978765584 Lambda1 1.3977175\n",
      "80 Train Loss 34.552715 Test MSE 35.28892922053379 Test RE 0.10000193671550163 Lambda1 1.403173\n",
      "81 Train Loss 34.414097 Test MSE 35.15825690350306 Test RE 0.0998166150792175 Lambda1 1.4014114\n",
      "82 Train Loss 34.10689 Test MSE 35.06008207947793 Test RE 0.0996771552482749 Lambda1 1.4041492\n",
      "83 Train Loss 33.934723 Test MSE 35.037510098730316 Test RE 0.09964506357962571 Lambda1 1.4055307\n",
      "84 Train Loss 33.72523 Test MSE 34.99351560599566 Test RE 0.0995824847746947 Lambda1 1.4031218\n",
      "85 Train Loss 33.60918 Test MSE 34.9812044253274 Test RE 0.09956496601746205 Lambda1 1.3997828\n",
      "86 Train Loss 33.43759 Test MSE 34.908707062978465 Test RE 0.09946173999530047 Lambda1 1.3978032\n",
      "87 Train Loss 33.38305 Test MSE 34.853806890985425 Test RE 0.09938349855252751 Lambda1 1.3968719\n",
      "88 Train Loss 33.332237 Test MSE 34.773946510763174 Test RE 0.09926957476201537 Lambda1 1.3962023\n",
      "89 Train Loss 33.224712 Test MSE 34.65681738323762 Test RE 0.09910224882570975 Lambda1 1.3966917\n",
      "90 Train Loss 33.141514 Test MSE 34.56348441317567 Test RE 0.0989687145935444 Lambda1 1.3958122\n",
      "91 Train Loss 33.04403 Test MSE 34.562201448200526 Test RE 0.09896687776234928 Lambda1 1.3970605\n",
      "92 Train Loss 32.96498 Test MSE 34.57171249996766 Test RE 0.09898049400075815 Lambda1 1.3986938\n",
      "93 Train Loss 32.78589 Test MSE 34.435537919370795 Test RE 0.09878536443913054 Lambda1 1.3969158\n",
      "94 Train Loss 32.71369 Test MSE 34.38039992942177 Test RE 0.09870624546981714 Lambda1 1.3965317\n",
      "95 Train Loss 32.659798 Test MSE 34.37372163356293 Test RE 0.09869665829912902 Lambda1 1.3974516\n",
      "96 Train Loss 32.58873 Test MSE 34.31620594375776 Test RE 0.09861405183855387 Lambda1 1.3931963\n",
      "97 Train Loss 32.46402 Test MSE 34.23117062486149 Test RE 0.09849179358660073 Lambda1 1.3898313\n",
      "98 Train Loss 32.267212 Test MSE 34.03915609196623 Test RE 0.09821516776822078 Lambda1 1.3886011\n",
      "99 Train Loss 32.184017 Test MSE 34.04205613654277 Test RE 0.09821935151322109 Lambda1 1.3891884\n",
      "100 Train Loss 32.074028 Test MSE 33.97908867731619 Test RE 0.09812847149185568 Lambda1 1.3845072\n",
      "101 Train Loss 31.94153 Test MSE 33.90483082851688 Test RE 0.09802118793917164 Lambda1 1.3783826\n",
      "102 Train Loss 31.765957 Test MSE 33.81167265903008 Test RE 0.09788643198636912 Lambda1 1.3703612\n",
      "103 Train Loss 31.642454 Test MSE 33.757026264388266 Test RE 0.09780729801814252 Lambda1 1.361454\n",
      "104 Train Loss 31.584385 Test MSE 33.74293583286098 Test RE 0.09778688314411003 Lambda1 1.3564616\n",
      "105 Train Loss 31.55188 Test MSE 33.71094192521405 Test RE 0.09774051304637872 Lambda1 1.3528744\n",
      "106 Train Loss 31.46882 Test MSE 33.59828409806656 Test RE 0.09757705797591043 Lambda1 1.3450035\n",
      "107 Train Loss 31.358643 Test MSE 33.55978136392062 Test RE 0.09752113160064033 Lambda1 1.3425713\n",
      "108 Train Loss 31.225657 Test MSE 33.516292927063915 Test RE 0.09745792474662389 Lambda1 1.3366443\n",
      "109 Train Loss 31.067259 Test MSE 33.36621932303367 Test RE 0.09723948962460809 Lambda1 1.3293642\n",
      "110 Train Loss 30.941126 Test MSE 33.319067096258294 Test RE 0.09717075724235606 Lambda1 1.3275548\n",
      "111 Train Loss 30.88708 Test MSE 33.32190142530036 Test RE 0.09717489013181596 Lambda1 1.3283558\n",
      "112 Train Loss 30.827133 Test MSE 33.31757287635849 Test RE 0.09716857836822058 Lambda1 1.3353413\n",
      "113 Train Loss 30.736065 Test MSE 33.24378253001033 Test RE 0.09706091630215481 Lambda1 1.3413252\n",
      "114 Train Loss 30.677244 Test MSE 33.245351785168396 Test RE 0.0970632071297102 Lambda1 1.3408449\n",
      "115 Train Loss 30.561817 Test MSE 33.174389201666074 Test RE 0.0969595605287464 Lambda1 1.3403671\n",
      "116 Train Loss 30.462826 Test MSE 33.10768621483738 Test RE 0.09686203429349648 Lambda1 1.3352233\n",
      "117 Train Loss 30.403513 Test MSE 33.0672615933977 Test RE 0.09680288175989223 Lambda1 1.3314296\n",
      "118 Train Loss 30.276688 Test MSE 32.93403292251582 Test RE 0.09660767454623854 Lambda1 1.3257371\n",
      "119 Train Loss 30.174183 Test MSE 32.86561120950721 Test RE 0.0965072693008021 Lambda1 1.3225518\n",
      "120 Train Loss 30.113758 Test MSE 32.79132977406968 Test RE 0.09639814682582719 Lambda1 1.3167218\n",
      "121 Train Loss 30.078966 Test MSE 32.73217592396386 Test RE 0.09631115896101597 Lambda1 1.3095531\n",
      "122 Train Loss 30.058798 Test MSE 32.71031985144431 Test RE 0.09627899893621057 Lambda1 1.3061951\n",
      "123 Train Loss 30.02848 Test MSE 32.6614173368963 Test RE 0.09620700260253263 Lambda1 1.3026314\n",
      "124 Train Loss 29.98574 Test MSE 32.564094282459756 Test RE 0.09606355896919831 Lambda1 1.2928244\n",
      "125 Train Loss 29.926813 Test MSE 32.48538199159109 Test RE 0.09594738872492532 Lambda1 1.2827107\n",
      "126 Train Loss 29.868837 Test MSE 32.45506609069014 Test RE 0.09590260842243285 Lambda1 1.280034\n",
      "127 Train Loss 29.806398 Test MSE 32.41554365356634 Test RE 0.09584419751944735 Lambda1 1.27593\n",
      "128 Train Loss 29.68121 Test MSE 32.30573361172047 Test RE 0.09568172016223597 Lambda1 1.2640339\n",
      "129 Train Loss 29.621525 Test MSE 32.27194280435735 Test RE 0.09563166699670925 Lambda1 1.2616832\n",
      "130 Train Loss 29.560844 Test MSE 32.26543291663209 Test RE 0.09562202111301864 Lambda1 1.2596424\n",
      "131 Train Loss 29.543287 Test MSE 32.235047207340735 Test RE 0.09557698487666835 Lambda1 1.257158\n",
      "132 Train Loss 29.51805 Test MSE 32.19313440167606 Test RE 0.09551482888606232 Lambda1 1.2531888\n",
      "133 Train Loss 29.463272 Test MSE 32.09791851321187 Test RE 0.09537347477399731 Lambda1 1.2463603\n",
      "134 Train Loss 29.43785 Test MSE 32.04632263598981 Test RE 0.0952967897826792 Lambda1 1.2435641\n",
      "135 Train Loss 29.385172 Test MSE 32.0013489367319 Test RE 0.09522989671112163 Lambda1 1.2399501\n",
      "136 Train Loss 29.31725 Test MSE 31.994247477697527 Test RE 0.09521932983261458 Lambda1 1.2394465\n",
      "137 Train Loss 29.266136 Test MSE 31.93662938748281 Test RE 0.09513355147126105 Lambda1 1.2320749\n",
      "138 Train Loss 29.23772 Test MSE 31.906724658810248 Test RE 0.0950890005998032 Lambda1 1.2281935\n",
      "139 Train Loss 29.188633 Test MSE 31.928011232994066 Test RE 0.09512071462876946 Lambda1 1.2287873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 Train Loss 29.166319 Test MSE 31.90364330697736 Test RE 0.0950844089385805 Lambda1 1.2259386\n",
      "141 Train Loss 29.132519 Test MSE 31.851433445812575 Test RE 0.09500657493659824 Lambda1 1.2153877\n",
      "142 Train Loss 29.112415 Test MSE 31.81729841615399 Test RE 0.09495565224240363 Lambda1 1.2110981\n",
      "143 Train Loss 29.07971 Test MSE 31.801870644496674 Test RE 0.09493262810427107 Lambda1 1.2115037\n",
      "144 Train Loss 29.039087 Test MSE 31.75331193072172 Test RE 0.0948601234487042 Lambda1 1.2038358\n",
      "145 Train Loss 28.968697 Test MSE 31.643549037311505 Test RE 0.09469602820058536 Lambda1 1.1908774\n",
      "146 Train Loss 28.901608 Test MSE 31.53272357825688 Test RE 0.09453005541644204 Lambda1 1.1807193\n",
      "147 Train Loss 28.837843 Test MSE 31.423821185163753 Test RE 0.09436667826628788 Lambda1 1.1693431\n",
      "148 Train Loss 28.73088 Test MSE 31.350377205695068 Test RE 0.0942563365182373 Lambda1 1.1557764\n",
      "149 Train Loss 28.677101 Test MSE 31.320000146063713 Test RE 0.09421066044460033 Lambda1 1.1517606\n",
      "150 Train Loss 28.583023 Test MSE 31.186864092897427 Test RE 0.09401021036132715 Lambda1 1.1386375\n",
      "151 Train Loss 28.471792 Test MSE 31.08881455004973 Test RE 0.09386231292496834 Lambda1 1.1308128\n",
      "152 Train Loss 28.39349 Test MSE 31.076914267307142 Test RE 0.09384434673696085 Lambda1 1.1266786\n",
      "153 Train Loss 28.344707 Test MSE 31.08533996883647 Test RE 0.09385706760849782 Lambda1 1.1246697\n",
      "154 Train Loss 28.282965 Test MSE 31.025314124955823 Test RE 0.09376640474192773 Lambda1 1.1158453\n",
      "155 Train Loss 28.237595 Test MSE 30.979558270341197 Test RE 0.09369723630449991 Lambda1 1.1080881\n",
      "156 Train Loss 28.209208 Test MSE 30.93761265746085 Test RE 0.09363378286182619 Lambda1 1.1039227\n",
      "157 Train Loss 28.169785 Test MSE 30.908229265130522 Test RE 0.09358930735908672 Lambda1 1.1018952\n",
      "158 Train Loss 28.12171 Test MSE 30.933702322194016 Test RE 0.0936278652905909 Lambda1 1.1041484\n",
      "159 Train Loss 28.09752 Test MSE 30.93301544289133 Test RE 0.09362682578682954 Lambda1 1.1054432\n",
      "160 Train Loss 28.067478 Test MSE 30.91351229467853 Test RE 0.0935973054547675 Lambda1 1.1035212\n",
      "161 Train Loss 28.038036 Test MSE 30.875316618146897 Test RE 0.09353946477101181 Lambda1 1.0985863\n",
      "162 Train Loss 27.991098 Test MSE 30.842979639275494 Test RE 0.09349046808956521 Lambda1 1.0941862\n",
      "163 Train Loss 27.924656 Test MSE 30.794522755078784 Test RE 0.09341699857413059 Lambda1 1.0830818\n",
      "164 Train Loss 27.846436 Test MSE 30.76039770808755 Test RE 0.09336522405662505 Lambda1 1.0702842\n",
      "165 Train Loss 27.837296 Test MSE 30.76436298041086 Test RE 0.0933712416415101 Lambda1 1.0682794\n",
      "166 Train Loss 27.804045 Test MSE 30.752644711996037 Test RE 0.09335345720885697 Lambda1 1.0672231\n",
      "167 Train Loss 27.763369 Test MSE 30.740091301646387 Test RE 0.09333440154899675 Lambda1 1.0664577\n",
      "168 Train Loss 27.73408 Test MSE 30.779459419791877 Test RE 0.09339414802200435 Lambda1 1.0635657\n",
      "169 Train Loss 27.688341 Test MSE 30.714605297182953 Test RE 0.09329570266887156 Lambda1 1.0526341\n",
      "170 Train Loss 27.628021 Test MSE 30.669721511782566 Test RE 0.09322751043297636 Lambda1 1.0421445\n",
      "171 Train Loss 27.61071 Test MSE 30.668248400445126 Test RE 0.09322527147962198 Lambda1 1.0392077\n",
      "172 Train Loss 27.584232 Test MSE 30.646009445838317 Test RE 0.0931914643880182 Lambda1 1.0332481\n",
      "173 Train Loss 27.511969 Test MSE 30.60670350345543 Test RE 0.0931316824838637 Lambda1 1.0337034\n",
      "174 Train Loss 27.472813 Test MSE 30.56130398510861 Test RE 0.09306258483325874 Lambda1 1.0366813\n",
      "175 Train Loss 27.424618 Test MSE 30.519981337609316 Test RE 0.09299964751228353 Lambda1 1.0285728\n",
      "176 Train Loss 27.377197 Test MSE 30.486669534076913 Test RE 0.09294888025004926 Lambda1 1.0213109\n",
      "177 Train Loss 27.347399 Test MSE 30.441846295775758 Test RE 0.0928805257467276 Lambda1 1.0161716\n",
      "178 Train Loss 27.326385 Test MSE 30.422168626106817 Test RE 0.09285050181626676 Lambda1 1.0144224\n",
      "179 Train Loss 27.294043 Test MSE 30.379048943881493 Test RE 0.09278467640009157 Lambda1 1.0098783\n",
      "180 Train Loss 27.22281 Test MSE 30.312925921916392 Test RE 0.09268364385435279 Lambda1 0.99607474\n",
      "181 Train Loss 27.162718 Test MSE 30.260467511555646 Test RE 0.0926034117122604 Lambda1 0.984362\n",
      "182 Train Loss 27.123913 Test MSE 30.196904404482503 Test RE 0.09250610232930263 Lambda1 0.9754781\n",
      "183 Train Loss 27.083637 Test MSE 30.19079674317966 Test RE 0.09249674665982772 Lambda1 0.9816135\n",
      "184 Train Loss 27.048246 Test MSE 30.172925560378154 Test RE 0.09246936628000088 Lambda1 0.98250926\n",
      "185 Train Loss 27.000391 Test MSE 30.141469479795322 Test RE 0.09242115281902372 Lambda1 0.98341167\n",
      "186 Train Loss 26.938772 Test MSE 30.098863124095708 Test RE 0.09235580894057818 Lambda1 0.97745144\n",
      "187 Train Loss 26.85512 Test MSE 30.03705767641374 Test RE 0.09226093782573734 Lambda1 0.9661101\n",
      "188 Train Loss 26.830526 Test MSE 29.996274345173717 Test RE 0.0921982821136379 Lambda1 0.95925856\n",
      "189 Train Loss 26.815887 Test MSE 29.97360460735033 Test RE 0.09216343602059018 Lambda1 0.9550907\n",
      "190 Train Loss 26.803198 Test MSE 29.993559682582017 Test RE 0.09219411004734065 Lambda1 0.9516335\n",
      "191 Train Loss 26.778164 Test MSE 29.98665193776095 Test RE 0.09218349293377992 Lambda1 0.9447325\n",
      "192 Train Loss 26.755875 Test MSE 29.991018669720624 Test RE 0.0921902046859273 Lambda1 0.93934995\n",
      "193 Train Loss 26.736008 Test MSE 29.983901698698325 Test RE 0.09217926551190796 Lambda1 0.9362513\n",
      "194 Train Loss 26.723621 Test MSE 29.95797222146772 Test RE 0.09213939950056535 Lambda1 0.9362286\n",
      "195 Train Loss 26.688164 Test MSE 29.92337575298259 Test RE 0.09208618130117757 Lambda1 0.94117475\n",
      "196 Train Loss 26.665855 Test MSE 29.91419269227949 Test RE 0.09207205024377373 Lambda1 0.94074994\n",
      "197 Train Loss 26.645418 Test MSE 29.888026917471592 Test RE 0.09203177398423008 Lambda1 0.93803626\n",
      "198 Train Loss 26.638004 Test MSE 29.868735072434195 Test RE 0.09200206728391985 Lambda1 0.93424094\n",
      "199 Train Loss 26.627249 Test MSE 29.84499409846859 Test RE 0.09196549638663327 Lambda1 0.92927015\n",
      "Training time: 351.28\n",
      "Training time: 351.28\n",
      "inv_HT_stan\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 854.0174 Test MSE 856.9361868246999 Test RE 0.4927917311928455 Lambda1 -0.03974425\n",
      "1 Train Loss 676.58606 Test MSE 655.3243828477512 Test RE 0.4309405228288848 Lambda1 0.4233351\n",
      "2 Train Loss 479.28625 Test MSE 463.3822301969588 Test RE 0.3623755256578881 Lambda1 0.21071236\n",
      "3 Train Loss 293.54855 Test MSE 289.47747926924376 Test RE 0.28641552945486126 Lambda1 0.060962778\n",
      "4 Train Loss 263.43512 Test MSE 264.41956678846105 Test RE 0.2737385518020424 Lambda1 0.081012316\n",
      "5 Train Loss 247.64323 Test MSE 245.6239116361734 Test RE 0.2638301912666612 Lambda1 0.1475441\n",
      "6 Train Loss 218.88445 Test MSE 209.74861049366044 Test RE 0.24380281561656741 Lambda1 0.25292456\n",
      "7 Train Loss 195.28886 Test MSE 188.00302716122684 Test RE 0.23081901943405736 Lambda1 0.2941479\n",
      "8 Train Loss 176.18672 Test MSE 163.11892475483282 Test RE 0.21500142859607 Lambda1 0.37967265\n",
      "9 Train Loss 157.37085 Test MSE 145.93364023600014 Test RE 0.20336062996401894 Lambda1 0.44386092\n",
      "10 Train Loss 141.60818 Test MSE 133.35696882851292 Test RE 0.19440034368657733 Lambda1 0.5201881\n",
      "11 Train Loss 125.313446 Test MSE 117.00605382200582 Test RE 0.18209303599511542 Lambda1 0.5855731\n",
      "12 Train Loss 111.4595 Test MSE 102.41368777095697 Test RE 0.1703602124540792 Lambda1 0.65061396\n",
      "13 Train Loss 103.59033 Test MSE 90.77704810967468 Test RE 0.16038996726513846 Lambda1 0.7261628\n",
      "14 Train Loss 95.301476 Test MSE 82.49451119752867 Test RE 0.15289796226130842 Lambda1 0.77882725\n",
      "15 Train Loss 88.16513 Test MSE 80.5797541038852 Test RE 0.15111310853886334 Lambda1 0.7971955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 Train Loss 78.85812 Test MSE 70.152165659709 Test RE 0.14099694795642795 Lambda1 0.8480421\n",
      "17 Train Loss 75.175896 Test MSE 66.81889705549777 Test RE 0.13760645982259603 Lambda1 0.8779568\n",
      "18 Train Loss 71.087814 Test MSE 64.4444901724405 Test RE 0.13513942501458237 Lambda1 0.8786881\n",
      "19 Train Loss 67.98253 Test MSE 60.07933412746819 Test RE 0.13048233682943267 Lambda1 0.9049527\n",
      "20 Train Loss 62.180206 Test MSE 57.08807672081674 Test RE 0.1271926092602992 Lambda1 0.9604816\n",
      "21 Train Loss 60.08065 Test MSE 55.543888793439095 Test RE 0.12546058608481073 Lambda1 0.9716917\n",
      "22 Train Loss 57.16109 Test MSE 53.51225056973577 Test RE 0.12314471526316212 Lambda1 0.9965424\n",
      "23 Train Loss 55.463993 Test MSE 52.201697245113785 Test RE 0.12162741662747821 Lambda1 0.9968163\n",
      "24 Train Loss 53.286446 Test MSE 51.29800196625437 Test RE 0.12057003738329668 Lambda1 1.0119094\n",
      "25 Train Loss 49.661278 Test MSE 47.364741799170126 Test RE 0.1158555271749819 Lambda1 1.0547546\n",
      "26 Train Loss 46.368183 Test MSE 44.14731581876261 Test RE 0.11185137371927846 Lambda1 1.0866592\n",
      "27 Train Loss 42.76059 Test MSE 41.58081497958949 Test RE 0.10855146005310977 Lambda1 1.1042633\n",
      "28 Train Loss 40.684082 Test MSE 39.53397565235918 Test RE 0.1058459915950241 Lambda1 1.1380664\n",
      "29 Train Loss 40.018124 Test MSE 39.267129796261855 Test RE 0.10548816789407313 Lambda1 1.1478355\n",
      "30 Train Loss 39.520084 Test MSE 38.75734420594537 Test RE 0.10480118073262676 Lambda1 1.155517\n",
      "31 Train Loss 38.51584 Test MSE 36.94227686663121 Test RE 0.10231775441752183 Lambda1 1.1795298\n",
      "32 Train Loss 37.91753 Test MSE 36.63165383520585 Test RE 0.1018866853949854 Lambda1 1.1854222\n",
      "33 Train Loss 36.875877 Test MSE 36.18969230087068 Test RE 0.10127018793208983 Lambda1 1.1948857\n",
      "34 Train Loss 36.22859 Test MSE 35.4991121323609 Test RE 0.10029930326107603 Lambda1 1.2049861\n",
      "35 Train Loss 35.72776 Test MSE 35.39151551029661 Test RE 0.10014718599052615 Lambda1 1.2129203\n",
      "36 Train Loss 35.02231 Test MSE 35.707643751152204 Test RE 0.10059346484334325 Lambda1 1.2077268\n",
      "37 Train Loss 34.783092 Test MSE 35.43044585164687 Test RE 0.10020225134330614 Lambda1 1.2066556\n",
      "38 Train Loss 34.09964 Test MSE 35.03864906235912 Test RE 0.09964668314649248 Lambda1 1.2050507\n",
      "39 Train Loss 33.884956 Test MSE 34.98464233455195 Test RE 0.09956985845771762 Lambda1 1.205763\n",
      "40 Train Loss 33.52863 Test MSE 34.848412567051284 Test RE 0.09937580746254851 Lambda1 1.2239224\n",
      "41 Train Loss 33.44871 Test MSE 34.72684086192964 Test RE 0.0992023154647364 Lambda1 1.2260077\n",
      "42 Train Loss 33.384563 Test MSE 34.73147782802098 Test RE 0.09920893833061029 Lambda1 1.2344916\n",
      "43 Train Loss 32.981304 Test MSE 34.54844519252999 Test RE 0.09894718067841209 Lambda1 1.2439888\n",
      "44 Train Loss 32.763878 Test MSE 34.27695226928545 Test RE 0.09855763430349443 Lambda1 1.2464343\n",
      "45 Train Loss 32.67666 Test MSE 34.290180680804745 Test RE 0.09857665050962801 Lambda1 1.2391856\n",
      "46 Train Loss 32.544468 Test MSE 34.27626038471771 Test RE 0.0985566395993885 Lambda1 1.239901\n",
      "47 Train Loss 32.331432 Test MSE 34.16222802919485 Test RE 0.09839256089631573 Lambda1 1.237663\n",
      "48 Train Loss 32.238403 Test MSE 34.11966586636259 Test RE 0.09833124894988834 Lambda1 1.2357867\n",
      "49 Train Loss 32.126137 Test MSE 34.060081778314384 Test RE 0.09824535218324447 Lambda1 1.2293885\n",
      "50 Train Loss 32.02566 Test MSE 34.02240833811038 Test RE 0.09819100315869564 Lambda1 1.2326518\n",
      "51 Train Loss 31.96384 Test MSE 33.918999394969816 Test RE 0.09804166694787388 Lambda1 1.2292889\n",
      "52 Train Loss 31.93624 Test MSE 33.96292695343148 Test RE 0.09810513193288854 Lambda1 1.2302771\n",
      "53 Train Loss 31.714964 Test MSE 33.870415796254456 Test RE 0.09797142720096748 Lambda1 1.2281041\n",
      "54 Train Loss 31.618092 Test MSE 33.833958597749515 Test RE 0.09791868610906825 Lambda1 1.2301598\n",
      "55 Train Loss 31.583265 Test MSE 33.787988816893005 Test RE 0.09785214304503256 Lambda1 1.2317355\n",
      "56 Train Loss 31.528769 Test MSE 33.68066445496358 Test RE 0.0976966103832214 Lambda1 1.2309933\n",
      "57 Train Loss 31.46685 Test MSE 33.60405965298589 Test RE 0.0975854443780038 Lambda1 1.2263832\n",
      "58 Train Loss 31.379091 Test MSE 33.519002547855514 Test RE 0.09746186415428351 Lambda1 1.2189904\n",
      "59 Train Loss 31.32029 Test MSE 33.48542939313936 Test RE 0.09741304226590493 Lambda1 1.2180179\n",
      "60 Train Loss 31.27554 Test MSE 33.361867644730395 Test RE 0.09723314834915271 Lambda1 1.2146398\n",
      "61 Train Loss 31.253233 Test MSE 33.340235420241626 Test RE 0.09720161968313 Lambda1 1.2179329\n",
      "62 Train Loss 31.174688 Test MSE 33.303878445849094 Test RE 0.0971486068487998 Lambda1 1.2156\n",
      "63 Train Loss 31.106041 Test MSE 33.336022626905006 Test RE 0.09719547840567769 Lambda1 1.2187309\n",
      "64 Train Loss 31.033487 Test MSE 33.25084668759293 Test RE 0.09707122826329415 Lambda1 1.2193408\n",
      "65 Train Loss 30.908842 Test MSE 33.153758199159235 Test RE 0.09692940648391032 Lambda1 1.2090623\n",
      "66 Train Loss 30.844942 Test MSE 33.07852523794738 Test RE 0.09681936725587093 Lambda1 1.2010132\n",
      "67 Train Loss 30.788717 Test MSE 33.04998277626213 Test RE 0.09677758698536354 Lambda1 1.1966379\n",
      "68 Train Loss 30.688734 Test MSE 32.979494024615384 Test RE 0.09667432864846293 Lambda1 1.1913419\n",
      "69 Train Loss 30.636765 Test MSE 32.87703945078753 Test RE 0.09652404690598522 Lambda1 1.1793429\n",
      "70 Train Loss 30.574934 Test MSE 32.821896365653444 Test RE 0.09644306535247023 Lambda1 1.1703221\n",
      "71 Train Loss 30.50838 Test MSE 32.788800729334554 Test RE 0.09639442938087651 Lambda1 1.1736035\n",
      "72 Train Loss 30.447447 Test MSE 32.7848346084205 Test RE 0.09638859928774045 Lambda1 1.1721752\n",
      "73 Train Loss 30.384237 Test MSE 32.77576264575315 Test RE 0.0963752624159104 Lambda1 1.1743509\n",
      "74 Train Loss 30.319637 Test MSE 32.79584373686457 Test RE 0.0964047815472206 Lambda1 1.1766385\n",
      "75 Train Loss 30.285528 Test MSE 32.79471944961569 Test RE 0.09640312908789646 Lambda1 1.1764303\n",
      "76 Train Loss 30.262049 Test MSE 32.775780503862514 Test RE 0.09637528867128337 Lambda1 1.174198\n",
      "77 Train Loss 30.215366 Test MSE 32.78746294133429 Test RE 0.09639246290721136 Lambda1 1.1759994\n",
      "78 Train Loss 30.173231 Test MSE 32.849357821821506 Test RE 0.09648340295565948 Lambda1 1.1774799\n",
      "79 Train Loss 30.125505 Test MSE 32.856160133557744 Test RE 0.09649339213436495 Lambda1 1.1811116\n",
      "80 Train Loss 30.057686 Test MSE 32.82337016186234 Test RE 0.09644523061170676 Lambda1 1.1738634\n",
      "81 Train Loss 30.035624 Test MSE 32.81150263512015 Test RE 0.0964277938003272 Lambda1 1.1720427\n",
      "82 Train Loss 30.02056 Test MSE 32.78688559112463 Test RE 0.09639161422224653 Lambda1 1.1698314\n",
      "83 Train Loss 29.99551 Test MSE 32.74982864268705 Test RE 0.09633712614141698 Lambda1 1.1692685\n",
      "84 Train Loss 29.964735 Test MSE 32.716931605836464 Test RE 0.09628872890724899 Lambda1 1.1677693\n",
      "85 Train Loss 29.921005 Test MSE 32.72918017003345 Test RE 0.0963067515067782 Lambda1 1.171678\n",
      "86 Train Loss 29.87544 Test MSE 32.747532487584316 Test RE 0.0963337488892243 Lambda1 1.1755164\n",
      "87 Train Loss 29.839298 Test MSE 32.75769908903372 Test RE 0.09634870132616927 Lambda1 1.176485\n",
      "88 Train Loss 29.80752 Test MSE 32.73508825796811 Test RE 0.09631544349170265 Lambda1 1.1728224\n",
      "89 Train Loss 29.770292 Test MSE 32.654292918501184 Test RE 0.09619650923793659 Lambda1 1.167537\n",
      "90 Train Loss 29.730473 Test MSE 32.63449109127532 Test RE 0.09616733764433258 Lambda1 1.1634842\n",
      "91 Train Loss 29.677385 Test MSE 32.60001487558061 Test RE 0.09611652695484257 Lambda1 1.1575887\n",
      "92 Train Loss 29.6364 Test MSE 32.56780815454648 Test RE 0.09606903674474344 Lambda1 1.1532761\n",
      "93 Train Loss 29.592722 Test MSE 32.51070451046587 Test RE 0.09598477717659769 Lambda1 1.1463901\n",
      "94 Train Loss 29.555529 Test MSE 32.469542304824074 Test RE 0.09592399417337462 Lambda1 1.1399604\n",
      "95 Train Loss 29.520452 Test MSE 32.477651541406544 Test RE 0.09593597188764849 Lambda1 1.1387899\n",
      "96 Train Loss 29.475302 Test MSE 32.46141356803566 Test RE 0.0959119861551976 Lambda1 1.1382159\n",
      "97 Train Loss 29.401985 Test MSE 32.39335820416622 Test RE 0.09581139364974105 Lambda1 1.1317466\n",
      "98 Train Loss 29.262705 Test MSE 32.31941755412006 Test RE 0.09570198227027302 Lambda1 1.118707\n",
      "99 Train Loss 29.187965 Test MSE 32.24952019699269 Test RE 0.09559843869169135 Lambda1 1.108662\n",
      "100 Train Loss 29.147837 Test MSE 32.208715793395086 Test RE 0.09553794055190856 Lambda1 1.1017029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 Train Loss 29.092958 Test MSE 32.142554603970616 Test RE 0.09543976599001287 Lambda1 1.0892309\n",
      "102 Train Loss 29.055206 Test MSE 32.092624179142284 Test RE 0.09536560884555857 Lambda1 1.0731002\n",
      "103 Train Loss 29.038334 Test MSE 32.07984165163713 Test RE 0.09534661484059509 Lambda1 1.0705818\n",
      "104 Train Loss 28.996239 Test MSE 32.03341618101224 Test RE 0.09527759775862425 Lambda1 1.0602585\n",
      "105 Train Loss 28.970634 Test MSE 32.01380022233794 Test RE 0.0952484212322081 Lambda1 1.0524716\n",
      "106 Train Loss 28.94973 Test MSE 31.985756965271406 Test RE 0.09520669452141674 Lambda1 1.0403985\n",
      "107 Train Loss 28.932192 Test MSE 31.976392675824055 Test RE 0.09519275693806749 Lambda1 1.0381137\n",
      "108 Train Loss 28.90613 Test MSE 31.968256463164384 Test RE 0.09518064553770475 Lambda1 1.0348632\n",
      "109 Train Loss 28.876152 Test MSE 31.952630871063203 Test RE 0.09515738127670327 Lambda1 1.0340947\n",
      "110 Train Loss 28.839748 Test MSE 31.932743434876215 Test RE 0.09512776351358347 Lambda1 1.0365891\n",
      "111 Train Loss 28.789762 Test MSE 31.91120735472868 Test RE 0.0950956800711206 Lambda1 1.0291992\n",
      "112 Train Loss 28.764032 Test MSE 31.880978360743857 Test RE 0.09505062806464774 Lambda1 1.0257485\n",
      "113 Train Loss 28.752073 Test MSE 31.868236917390185 Test RE 0.0950316323611918 Lambda1 1.0280964\n",
      "114 Train Loss 28.743715 Test MSE 31.854658810781412 Test RE 0.09501138513018047 Lambda1 1.0276538\n",
      "115 Train Loss 28.73359 Test MSE 31.845346553259134 Test RE 0.09499749649990014 Lambda1 1.0253463\n",
      "116 Train Loss 28.711039 Test MSE 31.82692220400899 Test RE 0.0949700117890482 Lambda1 1.023304\n",
      "117 Train Loss 28.678728 Test MSE 31.803266145658178 Test RE 0.09493471095563806 Lambda1 1.0157471\n",
      "118 Train Loss 28.655863 Test MSE 31.769010602951912 Test RE 0.09488356972690672 Lambda1 1.0060878\n",
      "119 Train Loss 28.615753 Test MSE 31.718057256600808 Test RE 0.09480744876206243 Lambda1 1.0006951\n",
      "120 Train Loss 28.579994 Test MSE 31.72196498387264 Test RE 0.0948132888146197 Lambda1 1.0007147\n",
      "121 Train Loss 28.547882 Test MSE 31.707920959451574 Test RE 0.09479229850782851 Lambda1 0.99393034\n",
      "122 Train Loss 28.524727 Test MSE 31.72284072743634 Test RE 0.09481459755376019 Lambda1 0.988687\n",
      "123 Train Loss 28.502823 Test MSE 31.740011258967936 Test RE 0.09484025409587925 Lambda1 0.9842778\n",
      "124 Train Loss 28.474756 Test MSE 31.732125078093443 Test RE 0.09482847127300147 Lambda1 0.97848654\n",
      "125 Train Loss 28.46308 Test MSE 31.72619777058427 Test RE 0.09481961425887421 Lambda1 0.974864\n",
      "126 Train Loss 28.441538 Test MSE 31.734779796893452 Test RE 0.09483243787876526 Lambda1 0.9685311\n",
      "127 Train Loss 28.413277 Test MSE 31.74512328472382 Test RE 0.09484789124348161 Lambda1 0.9699682\n",
      "128 Train Loss 28.381735 Test MSE 31.75702644533857 Test RE 0.09486567167327518 Lambda1 0.9701412\n",
      "129 Train Loss 28.3692 Test MSE 31.76118280245514 Test RE 0.09487187947597603 Lambda1 0.97073865\n",
      "130 Train Loss 28.338654 Test MSE 31.718745807461673 Test RE 0.09480847781935395 Lambda1 0.9729881\n",
      "131 Train Loss 28.317564 Test MSE 31.684165159647918 Test RE 0.09475678232236336 Lambda1 0.974016\n",
      "132 Train Loss 28.297085 Test MSE 31.653283708652122 Test RE 0.09471059299899644 Lambda1 0.97373044\n",
      "133 Train Loss 28.261919 Test MSE 31.643080442701198 Test RE 0.09469532704325531 Lambda1 0.97700423\n",
      "134 Train Loss 28.239054 Test MSE 31.640222500292435 Test RE 0.0946910505964249 Lambda1 0.97019833\n",
      "135 Train Loss 28.194572 Test MSE 31.634770034825454 Test RE 0.094682891331293 Lambda1 0.96420336\n",
      "136 Train Loss 28.17114 Test MSE 31.629439092979954 Test RE 0.09467491323862529 Lambda1 0.96053845\n",
      "137 Train Loss 28.156908 Test MSE 31.619506086849317 Test RE 0.09466004607113486 Lambda1 0.95973694\n",
      "138 Train Loss 28.149967 Test MSE 31.610942823481754 Test RE 0.09464722718344985 Lambda1 0.95804447\n",
      "139 Train Loss 28.13771 Test MSE 31.59294611248871 Test RE 0.09462028111435826 Lambda1 0.9597232\n",
      "140 Train Loss 28.112825 Test MSE 31.57806168068064 Test RE 0.09459798919138056 Lambda1 0.9564882\n",
      "141 Train Loss 28.102999 Test MSE 31.56682544522637 Test RE 0.09458115757167511 Lambda1 0.9568055\n",
      "142 Train Loss 28.092426 Test MSE 31.56471974937232 Test RE 0.09457800295492244 Lambda1 0.9572401\n",
      "143 Train Loss 28.084677 Test MSE 31.566050727979526 Test RE 0.09457999695285234 Lambda1 0.95785445\n",
      "144 Train Loss 28.077847 Test MSE 31.554622487144666 Test RE 0.09456287443027789 Lambda1 0.95888144\n",
      "145 Train Loss 28.057817 Test MSE 31.534095459930203 Test RE 0.09453211173485337 Lambda1 0.9615622\n",
      "146 Train Loss 28.045753 Test MSE 31.54507657707679 Test RE 0.09454856975944634 Lambda1 0.962114\n",
      "147 Train Loss 28.035799 Test MSE 31.528834296028155 Test RE 0.09452422551403736 Lambda1 0.9623846\n",
      "148 Train Loss 28.024565 Test MSE 31.52161951033713 Test RE 0.094513409842232 Lambda1 0.9624142\n",
      "149 Train Loss 28.019703 Test MSE 31.52251585485396 Test RE 0.0945147536179605 Lambda1 0.96292526\n",
      "150 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "151 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "152 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "153 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "154 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "155 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "156 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "157 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "158 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "159 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "160 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "161 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "162 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "163 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "164 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "165 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "166 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "167 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "168 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "169 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "170 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "171 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "172 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "173 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "174 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "175 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "176 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "177 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "178 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "179 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "180 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "181 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "182 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "183 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "184 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "186 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "187 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "188 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "189 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "190 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "191 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "192 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "193 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "194 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "195 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "196 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "197 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "198 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "199 Train Loss 28.018822 Test MSE 31.524791513235197 Test RE 0.09451816513832507 Lambda1 0.9629169\n",
      "Training time: 290.74\n",
      "Training time: 290.74\n",
      "inv_HT_stan\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 803.6847 Test MSE 808.0799317676331 Test RE 0.4785378911094916 Lambda1 0.042439166\n",
      "1 Train Loss 642.35504 Test MSE 657.3765690443623 Test RE 0.43161475297620777 Lambda1 0.0015822174\n",
      "2 Train Loss 592.1068 Test MSE 577.5211721272118 Test RE 0.40455085721892226 Lambda1 0.011132826\n",
      "3 Train Loss 339.4969 Test MSE 325.9062444862483 Test RE 0.3039033668558725 Lambda1 0.010837827\n",
      "4 Train Loss 277.1862 Test MSE 284.2648266894088 Test RE 0.28382505689011595 Lambda1 -0.00061213586\n",
      "5 Train Loss 272.44223 Test MSE 279.95353755983115 Test RE 0.2816645239958467 Lambda1 0.00017109726\n",
      "6 Train Loss 271.2148 Test MSE 278.73961303407276 Test RE 0.281053188741355 Lambda1 0.00010110331\n",
      "7 Train Loss 270.7633 Test MSE 278.9466160628958 Test RE 0.28115752996366355 Lambda1 0.00025715734\n",
      "8 Train Loss 270.54938 Test MSE 278.87627570839066 Test RE 0.2811220787947068 Lambda1 0.0003237528\n",
      "9 Train Loss 270.44394 Test MSE 278.7162229854773 Test RE 0.2810413963994169 Lambda1 0.0005435508\n",
      "10 Train Loss 270.23132 Test MSE 278.27922913445326 Test RE 0.28082099024983737 Lambda1 0.0016680344\n",
      "11 Train Loss 269.62234 Test MSE 277.51572733654467 Test RE 0.2804354878920672 Lambda1 0.0034489548\n",
      "12 Train Loss 263.5564 Test MSE 268.96081495874284 Test RE 0.27607919289357913 Lambda1 0.027233139\n",
      "13 Train Loss 256.59946 Test MSE 261.1706718691067 Test RE 0.27205165596420944 Lambda1 0.043838967\n",
      "14 Train Loss 244.27509 Test MSE 248.42935736216157 Test RE 0.2653326097089165 Lambda1 0.08317916\n",
      "15 Train Loss 229.33452 Test MSE 228.30224520566404 Test RE 0.2543573325556663 Lambda1 0.13684472\n",
      "16 Train Loss 204.57045 Test MSE 203.13727111474316 Test RE 0.23992968156658045 Lambda1 0.1782021\n",
      "17 Train Loss 185.9231 Test MSE 181.68828488335936 Test RE 0.22690947618597473 Lambda1 0.22964013\n",
      "18 Train Loss 159.5077 Test MSE 146.68665847148245 Test RE 0.2038846257580984 Lambda1 0.34401634\n",
      "19 Train Loss 142.60127 Test MSE 138.95522640540543 Test RE 0.19843880987556 Lambda1 0.40954274\n",
      "20 Train Loss 134.09648 Test MSE 128.6288369944985 Test RE 0.1909230405465102 Lambda1 0.44846463\n",
      "21 Train Loss 124.8539 Test MSE 119.83839337859192 Test RE 0.18428380027354968 Lambda1 0.49691078\n",
      "22 Train Loss 120.008156 Test MSE 115.79002947588647 Test RE 0.18114433339790334 Lambda1 0.51709056\n",
      "23 Train Loss 115.18466 Test MSE 109.18978151641798 Test RE 0.17590580396586175 Lambda1 0.54853636\n",
      "24 Train Loss 107.90122 Test MSE 100.35685079242911 Test RE 0.1686408113428835 Lambda1 0.60312796\n",
      "25 Train Loss 99.16311 Test MSE 88.26286251531357 Test RE 0.158153269591074 Lambda1 0.66472965\n",
      "26 Train Loss 90.70549 Test MSE 82.96974332889371 Test RE 0.15333773501967096 Lambda1 0.6725653\n",
      "27 Train Loss 84.66377 Test MSE 80.14270532491126 Test RE 0.15070274742430265 Lambda1 0.678755\n",
      "28 Train Loss 78.347595 Test MSE 75.0010668194192 Test RE 0.1457883736076415 Lambda1 0.727465\n",
      "29 Train Loss 74.71074 Test MSE 69.55512218280018 Test RE 0.1403956751104678 Lambda1 0.77192545\n",
      "30 Train Loss 71.19499 Test MSE 66.9071398981741 Test RE 0.13769729324953578 Lambda1 0.8115438\n",
      "31 Train Loss 65.95537 Test MSE 62.468309025707796 Test RE 0.13305127653345292 Lambda1 0.84499615\n",
      "32 Train Loss 62.74106 Test MSE 57.91425897065219 Test RE 0.12810967296261866 Lambda1 0.86883026\n",
      "33 Train Loss 59.12071 Test MSE 54.80991669928505 Test RE 0.12462889419702483 Lambda1 0.88219863\n",
      "34 Train Loss 56.840973 Test MSE 53.60712320000777 Test RE 0.12325382943080454 Lambda1 0.90181565\n",
      "35 Train Loss 54.51025 Test MSE 49.95868108059004 Test RE 0.11898566791728216 Lambda1 0.93201566\n",
      "36 Train Loss 52.422367 Test MSE 48.545149639551205 Test RE 0.11729029880943002 Lambda1 0.95244783\n",
      "37 Train Loss 50.76429 Test MSE 45.98239826916596 Test RE 0.11415238244739959 Lambda1 0.97852504\n",
      "38 Train Loss 49.634026 Test MSE 45.40848556944473 Test RE 0.1134377697805477 Lambda1 0.9888947\n",
      "39 Train Loss 47.837635 Test MSE 43.26169208282988 Test RE 0.11072378454379339 Lambda1 1.0336341\n",
      "40 Train Loss 45.89914 Test MSE 41.28637313404345 Test RE 0.10816644023175087 Lambda1 1.0504954\n",
      "41 Train Loss 44.193916 Test MSE 41.01707555641446 Test RE 0.10781309585502616 Lambda1 1.0637928\n",
      "42 Train Loss 43.27345 Test MSE 40.32430852921296 Test RE 0.10689875202094537 Lambda1 1.0714233\n",
      "43 Train Loss 42.036606 Test MSE 39.71367506509524 Test RE 0.1060862772872125 Lambda1 1.0798897\n",
      "44 Train Loss 40.716557 Test MSE 37.978447465863134 Test RE 0.10374275433365815 Lambda1 1.1071718\n",
      "45 Train Loss 39.394882 Test MSE 37.00922225639731 Test RE 0.10241042062743609 Lambda1 1.1192205\n",
      "46 Train Loss 38.537327 Test MSE 36.9259077471825 Test RE 0.10229508341212352 Lambda1 1.1289635\n",
      "47 Train Loss 37.995613 Test MSE 36.63184211305107 Test RE 0.10188694723111515 Lambda1 1.128294\n",
      "48 Train Loss 37.40271 Test MSE 36.45532926780055 Test RE 0.10164117646239781 Lambda1 1.1380463\n",
      "49 Train Loss 37.06007 Test MSE 35.80631514637972 Test RE 0.10073235457380855 Lambda1 1.1546364\n",
      "50 Train Loss 36.625504 Test MSE 35.4096324993298 Test RE 0.10017281548546586 Lambda1 1.1575093\n",
      "51 Train Loss 36.147785 Test MSE 34.87698521078116 Test RE 0.09941653884097469 Lambda1 1.1659855\n",
      "52 Train Loss 35.52312 Test MSE 34.657135362591525 Test RE 0.0991027034606005 Lambda1 1.1658105\n",
      "53 Train Loss 35.2427 Test MSE 34.441739569456715 Test RE 0.09879425938778587 Lambda1 1.1677191\n",
      "54 Train Loss 34.29397 Test MSE 33.53834579958165 Test RE 0.09748998189830446 Lambda1 1.1833113\n",
      "55 Train Loss 33.880917 Test MSE 33.395151536673566 Test RE 0.09728163920188951 Lambda1 1.1814295\n",
      "56 Train Loss 33.04498 Test MSE 32.81844746040328 Test RE 0.09643799812952329 Lambda1 1.1783943\n",
      "57 Train Loss 32.323322 Test MSE 32.10981788993416 Test RE 0.09539115161717363 Lambda1 1.1799005\n",
      "58 Train Loss 32.022568 Test MSE 31.505685959491558 Test RE 0.09448951949970269 Lambda1 1.1895562\n",
      "59 Train Loss 31.420599 Test MSE 31.115665770929454 Test RE 0.09390283833213162 Lambda1 1.2012753\n",
      "60 Train Loss 30.63657 Test MSE 30.970234344217083 Test RE 0.09368313520200068 Lambda1 1.1977916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 Train Loss 30.22435 Test MSE 30.741099012012665 Test RE 0.09333593136347014 Lambda1 1.2108686\n",
      "62 Train Loss 29.99869 Test MSE 30.664352009731413 Test RE 0.0932193491717107 Lambda1 1.219795\n",
      "63 Train Loss 29.535173 Test MSE 30.6514101610117 Test RE 0.09319967554491543 Lambda1 1.2253789\n",
      "64 Train Loss 29.192097 Test MSE 30.374512523359257 Test RE 0.09277774850011901 Lambda1 1.2408155\n",
      "65 Train Loss 28.985783 Test MSE 30.3576580110467 Test RE 0.09275200420731629 Lambda1 1.2499037\n",
      "66 Train Loss 28.876522 Test MSE 30.341625608768496 Test RE 0.09272750900700465 Lambda1 1.2508084\n",
      "67 Train Loss 28.734058 Test MSE 30.29689563935026 Test RE 0.09265913382385613 Lambda1 1.2574664\n",
      "68 Train Loss 28.567966 Test MSE 30.295464073816465 Test RE 0.09265694466905075 Lambda1 1.2522889\n",
      "69 Train Loss 28.471191 Test MSE 30.289935823856112 Test RE 0.09264849036514468 Lambda1 1.2495896\n",
      "70 Train Loss 28.420841 Test MSE 30.265352132470625 Test RE 0.09261088539569172 Lambda1 1.2493453\n",
      "71 Train Loss 28.347445 Test MSE 30.228572669714794 Test RE 0.09255459637500019 Lambda1 1.2421207\n",
      "72 Train Loss 28.246767 Test MSE 30.19144213492276 Test RE 0.09249773531074067 Lambda1 1.2410203\n",
      "73 Train Loss 28.173508 Test MSE 30.16003062724449 Test RE 0.09244960495926358 Lambda1 1.2400224\n",
      "74 Train Loss 28.078716 Test MSE 30.055646048604288 Test RE 0.09228948115750141 Lambda1 1.2411876\n",
      "75 Train Loss 27.986109 Test MSE 29.934919733148707 Test RE 0.09210394230770337 Lambda1 1.2349031\n",
      "76 Train Loss 27.817461 Test MSE 29.854268191236258 Test RE 0.09197978404731603 Lambda1 1.2343019\n",
      "77 Train Loss 27.6742 Test MSE 29.844561704268482 Test RE 0.09196483018627909 Lambda1 1.2337475\n",
      "78 Train Loss 27.617914 Test MSE 29.77976372485572 Test RE 0.09186493973822403 Lambda1 1.2320151\n",
      "79 Train Loss 27.553356 Test MSE 29.76995689764038 Test RE 0.09184981238897366 Lambda1 1.2336637\n",
      "80 Train Loss 27.461475 Test MSE 29.690144745552644 Test RE 0.09172660678318659 Lambda1 1.2314023\n",
      "81 Train Loss 27.318937 Test MSE 29.678897193309542 Test RE 0.091709230688539 Lambda1 1.230757\n",
      "82 Train Loss 27.133389 Test MSE 29.523460106999792 Test RE 0.09146876135820764 Lambda1 1.2252661\n",
      "83 Train Loss 27.065361 Test MSE 29.447131673078474 Test RE 0.09135044552335397 Lambda1 1.220811\n",
      "84 Train Loss 26.959791 Test MSE 29.32854591754186 Test RE 0.09116632250046448 Lambda1 1.212105\n",
      "85 Train Loss 26.86617 Test MSE 29.259209377441216 Test RE 0.09105849414395005 Lambda1 1.2051247\n",
      "86 Train Loss 26.783085 Test MSE 29.225944953400706 Test RE 0.09100671780025886 Lambda1 1.201962\n",
      "87 Train Loss 26.747223 Test MSE 29.214087122747156 Test RE 0.09098825386809296 Lambda1 1.2009238\n",
      "88 Train Loss 26.661688 Test MSE 29.209296125239153 Test RE 0.09098079270099134 Lambda1 1.2033305\n",
      "89 Train Loss 26.607006 Test MSE 29.136776482810127 Test RE 0.09086778082176918 Lambda1 1.1990418\n",
      "90 Train Loss 26.545341 Test MSE 29.087643065941485 Test RE 0.09079113321575215 Lambda1 1.1922405\n",
      "91 Train Loss 26.525492 Test MSE 29.089594184919864 Test RE 0.09079417817426244 Lambda1 1.1917356\n",
      "92 Train Loss 26.483833 Test MSE 29.05236392544472 Test RE 0.09073605819761268 Lambda1 1.1895596\n",
      "93 Train Loss 26.416607 Test MSE 29.050229893873855 Test RE 0.09073272564317304 Lambda1 1.1911209\n",
      "94 Train Loss 26.388813 Test MSE 29.044617068510053 Test RE 0.09072395993723795 Lambda1 1.191868\n",
      "95 Train Loss 26.330656 Test MSE 29.02650399784725 Test RE 0.09069566647111969 Lambda1 1.1932\n",
      "96 Train Loss 26.296719 Test MSE 28.96442028544107 Test RE 0.09059862175407803 Lambda1 1.1875422\n",
      "97 Train Loss 26.269636 Test MSE 28.980577542093663 Test RE 0.0906238875989442 Lambda1 1.1878179\n",
      "98 Train Loss 26.24914 Test MSE 29.00316663679871 Test RE 0.09065919940084681 Lambda1 1.1874849\n",
      "99 Train Loss 26.217772 Test MSE 28.97209998709994 Test RE 0.09061063173503817 Lambda1 1.1838948\n",
      "100 Train Loss 26.145605 Test MSE 28.89323256482184 Test RE 0.09048721822810908 Lambda1 1.1762656\n",
      "101 Train Loss 26.062843 Test MSE 28.745242423595254 Test RE 0.09025518452321359 Lambda1 1.1642853\n",
      "102 Train Loss 26.017149 Test MSE 28.699523670725583 Test RE 0.09018338139761399 Lambda1 1.1592551\n",
      "103 Train Loss 25.986578 Test MSE 28.65408737461646 Test RE 0.09011196519651557 Lambda1 1.1533586\n",
      "104 Train Loss 25.948925 Test MSE 28.634788654217246 Test RE 0.09008161457826716 Lambda1 1.1520189\n",
      "105 Train Loss 25.906895 Test MSE 28.56865587113094 Test RE 0.08997753154156596 Lambda1 1.1464268\n",
      "106 Train Loss 25.86937 Test MSE 28.505729200224543 Test RE 0.08987838253329508 Lambda1 1.1401458\n",
      "107 Train Loss 25.85344 Test MSE 28.459456641423703 Test RE 0.08980540436387963 Lambda1 1.1354468\n",
      "108 Train Loss 25.835281 Test MSE 28.439013926093235 Test RE 0.08977314450490766 Lambda1 1.1351353\n",
      "109 Train Loss 25.817654 Test MSE 28.4358700005897 Test RE 0.08976818216901059 Lambda1 1.1354941\n",
      "110 Train Loss 25.782434 Test MSE 28.390088987591874 Test RE 0.08969589083826641 Lambda1 1.1319263\n",
      "111 Train Loss 25.746525 Test MSE 28.324216774871562 Test RE 0.08959177179039086 Lambda1 1.1220832\n",
      "112 Train Loss 25.730513 Test MSE 28.277877101809565 Test RE 0.08951845373698239 Lambda1 1.1166521\n",
      "113 Train Loss 25.711569 Test MSE 28.262753636323474 Test RE 0.08949451257793704 Lambda1 1.1160914\n",
      "114 Train Loss 25.667845 Test MSE 28.22234772027864 Test RE 0.08943051667050868 Lambda1 1.1109641\n",
      "115 Train Loss 25.624704 Test MSE 28.17418024125378 Test RE 0.08935416791796477 Lambda1 1.1023418\n",
      "116 Train Loss 25.591253 Test MSE 28.150242132667064 Test RE 0.08931620009971017 Lambda1 1.0955864\n",
      "117 Train Loss 25.566313 Test MSE 28.1141406400401 Test RE 0.08925890960479849 Lambda1 1.0914626\n",
      "118 Train Loss 25.551964 Test MSE 28.060145795240587 Test RE 0.08917315494310861 Lambda1 1.0850453\n",
      "119 Train Loss 25.534456 Test MSE 28.02399830252067 Test RE 0.08911569934816721 Lambda1 1.0821583\n",
      "120 Train Loss 25.486618 Test MSE 27.948236468006204 Test RE 0.0889951573376019 Lambda1 1.07494\n",
      "121 Train Loss 25.453773 Test MSE 27.891063602794258 Test RE 0.08890408338105449 Lambda1 1.0683081\n",
      "122 Train Loss 25.423265 Test MSE 27.86623140708474 Test RE 0.08886449766932152 Lambda1 1.0647976\n",
      "123 Train Loss 25.385475 Test MSE 27.807027813931235 Test RE 0.08877004831867281 Lambda1 1.0550442\n",
      "124 Train Loss 25.350557 Test MSE 27.77906883138341 Test RE 0.08872540952388856 Lambda1 1.0488871\n",
      "125 Train Loss 25.324144 Test MSE 27.770521286679966 Test RE 0.08871175818884873 Lambda1 1.04475\n",
      "126 Train Loss 25.268246 Test MSE 27.721588026902943 Test RE 0.08863356611236618 Lambda1 1.0372472\n",
      "127 Train Loss 25.18661 Test MSE 27.697687596374053 Test RE 0.08859534973782589 Lambda1 1.0368487\n",
      "128 Train Loss 25.13505 Test MSE 27.666838506949272 Test RE 0.088545998193688 Lambda1 1.0341066\n",
      "129 Train Loss 25.11453 Test MSE 27.645889997069176 Test RE 0.08851246964342337 Lambda1 1.0327871\n",
      "130 Train Loss 25.084208 Test MSE 27.62688171103427 Test RE 0.0884820354696403 Lambda1 1.0279622\n",
      "131 Train Loss 25.059855 Test MSE 27.61579239404451 Test RE 0.08846427552423279 Lambda1 1.0250384\n",
      "132 Train Loss 25.018948 Test MSE 27.591030396517425 Test RE 0.08842460540710695 Lambda1 1.0222504\n",
      "133 Train Loss 24.955591 Test MSE 27.543723726229747 Test RE 0.08834876793072464 Lambda1 1.0188981\n",
      "134 Train Loss 24.883293 Test MSE 27.488965398096074 Test RE 0.08826090330992449 Lambda1 1.0082486\n",
      "135 Train Loss 24.853369 Test MSE 27.455030893451262 Test RE 0.08820640844437379 Lambda1 1.0044613\n",
      "136 Train Loss 24.815289 Test MSE 27.435399967870694 Test RE 0.08817486808506396 Lambda1 0.99733514\n",
      "137 Train Loss 24.773266 Test MSE 27.409621358389767 Test RE 0.08813343330219009 Lambda1 0.9927471\n",
      "138 Train Loss 24.723173 Test MSE 27.3896705995008 Test RE 0.0881013524329951 Lambda1 0.99102306\n",
      "139 Train Loss 24.686216 Test MSE 27.343730202083954 Test RE 0.08802743570362911 Lambda1 0.9822545\n",
      "140 Train Loss 24.64595 Test MSE 27.315704945437016 Test RE 0.08798231340496283 Lambda1 0.98250186\n",
      "141 Train Loss 24.624353 Test MSE 27.342033876041754 Test RE 0.08802470517735356 Lambda1 0.98508126\n",
      "142 Train Loss 24.603441 Test MSE 27.323205743654842 Test RE 0.08799439239547359 Lambda1 0.9828336\n",
      "143 Train Loss 24.593384 Test MSE 27.31327357562806 Test RE 0.08797839766672043 Lambda1 0.98049784\n",
      "144 Train Loss 24.566307 Test MSE 27.2887473705139 Test RE 0.08793888828976548 Lambda1 0.97590435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 Train Loss 24.524166 Test MSE 27.27972906068657 Test RE 0.08792435618862854 Lambda1 0.9732682\n",
      "146 Train Loss 24.487959 Test MSE 27.236675425085128 Test RE 0.08785494644587963 Lambda1 0.9725433\n",
      "147 Train Loss 24.465658 Test MSE 27.20670080714276 Test RE 0.08780658989451949 Lambda1 0.9692893\n",
      "148 Train Loss 24.446379 Test MSE 27.184025913377084 Test RE 0.08776999192319719 Lambda1 0.9654478\n",
      "149 Train Loss 24.431944 Test MSE 27.178078224650967 Test RE 0.08776038964518361 Lambda1 0.96476597\n",
      "150 Train Loss 24.41667 Test MSE 27.14612282455665 Test RE 0.08770878108093855 Lambda1 0.96135914\n",
      "151 Train Loss 24.399351 Test MSE 27.130893276857304 Test RE 0.08768417439079673 Lambda1 0.95916057\n",
      "152 Train Loss 24.385336 Test MSE 27.134570546951913 Test RE 0.08769011646325328 Lambda1 0.9569889\n",
      "153 Train Loss 24.369576 Test MSE 27.095590977604175 Test RE 0.0876271091750103 Lambda1 0.95311147\n",
      "154 Train Loss 24.350689 Test MSE 27.096885976215905 Test RE 0.08762920316198013 Lambda1 0.95093703\n",
      "155 Train Loss 24.327925 Test MSE 27.093397956949893 Test RE 0.08762356299015386 Lambda1 0.94964135\n",
      "156 Train Loss 24.313631 Test MSE 27.113469588023197 Test RE 0.08765601411015181 Lambda1 0.9501512\n",
      "157 Train Loss 24.29818 Test MSE 27.130657160541812 Test RE 0.087683792838501 Lambda1 0.95375955\n",
      "158 Train Loss 24.277613 Test MSE 27.11653206191529 Test RE 0.08766096435589665 Lambda1 0.9535971\n",
      "159 Train Loss 24.248066 Test MSE 27.08590988845563 Test RE 0.08761145346103447 Lambda1 0.9490991\n",
      "160 Train Loss 24.22873 Test MSE 27.097734319303104 Test RE 0.08763057488873431 Lambda1 0.94541216\n",
      "161 Train Loss 24.21855 Test MSE 27.10271456004236 Test RE 0.08763862724660865 Lambda1 0.9452204\n",
      "162 Train Loss 24.213045 Test MSE 27.108630139554048 Test RE 0.08764819095619068 Lambda1 0.94502175\n",
      "163 Train Loss 24.202286 Test MSE 27.113376372519927 Test RE 0.08765586343031233 Lambda1 0.94467956\n",
      "164 Train Loss 24.182783 Test MSE 27.055340524942714 Test RE 0.08756200002799201 Lambda1 0.9426779\n",
      "165 Train Loss 24.154545 Test MSE 27.040866425482108 Test RE 0.0875385748811557 Lambda1 0.93994343\n",
      "166 Train Loss 24.132689 Test MSE 27.040873086154765 Test RE 0.08753858566235158 Lambda1 0.935932\n",
      "167 Train Loss 24.113474 Test MSE 26.98962074038916 Test RE 0.0874555874954645 Lambda1 0.9322017\n",
      "168 Train Loss 24.099524 Test MSE 26.9956138137486 Test RE 0.08746529675859287 Lambda1 0.93222755\n",
      "169 Train Loss 24.086075 Test MSE 26.96017883286323 Test RE 0.08740787356256602 Lambda1 0.93012863\n",
      "170 Train Loss 24.076471 Test MSE 26.950477686369616 Test RE 0.0873921460541233 Lambda1 0.9294994\n",
      "171 Train Loss 24.071745 Test MSE 26.95241636782114 Test RE 0.08739528927272858 Lambda1 0.93012565\n",
      "172 Train Loss 24.064888 Test MSE 26.945581188646663 Test RE 0.08738420677216772 Lambda1 0.93155557\n",
      "173 Train Loss 24.058119 Test MSE 26.942498438024042 Test RE 0.08737920796696098 Lambda1 0.9309396\n",
      "174 Train Loss 24.050886 Test MSE 26.92334704624361 Test RE 0.08734814679915096 Lambda1 0.9303877\n",
      "175 Train Loss 24.04526 Test MSE 26.915335319466873 Test RE 0.08733514950065971 Lambda1 0.93116\n",
      "176 Train Loss 24.040462 Test MSE 26.90056219809329 Test RE 0.087311178224842 Lambda1 0.9310856\n",
      "177 Train Loss 24.037088 Test MSE 26.90085417665035 Test RE 0.08731165206108507 Lambda1 0.9305744\n",
      "178 Train Loss 24.028261 Test MSE 26.908860238491588 Test RE 0.08732464366498266 Lambda1 0.92965543\n",
      "179 Train Loss 24.019417 Test MSE 26.870354512368333 Test RE 0.08726214189880413 Lambda1 0.9299469\n",
      "180 Train Loss 24.01334 Test MSE 26.862972834966897 Test RE 0.08725015498539439 Lambda1 0.9302807\n",
      "181 Train Loss 23.999996 Test MSE 26.881713144038052 Test RE 0.08728058366894362 Lambda1 0.9306821\n",
      "182 Train Loss 23.989046 Test MSE 26.90260493695758 Test RE 0.08731449322156883 Lambda1 0.9318805\n",
      "183 Train Loss 23.975512 Test MSE 26.896727528703476 Test RE 0.08730495490815686 Lambda1 0.9343525\n",
      "184 Train Loss 23.965174 Test MSE 26.89893424186016 Test RE 0.08730853625538607 Lambda1 0.9355115\n",
      "185 Train Loss 23.957462 Test MSE 26.88399129350644 Test RE 0.08728428198239664 Lambda1 0.9374487\n",
      "186 Train Loss 23.950739 Test MSE 26.8788085068392 Test RE 0.08727586809748059 Lambda1 0.9391005\n",
      "187 Train Loss 23.944511 Test MSE 26.887312252174766 Test RE 0.08728967289586757 Lambda1 0.93936235\n",
      "188 Train Loss 23.93518 Test MSE 26.87993005477123 Test RE 0.08727768891941727 Lambda1 0.9427985\n",
      "189 Train Loss 23.9299 Test MSE 26.87864255747359 Test RE 0.08727559867710626 Lambda1 0.94312465\n",
      "190 Train Loss 23.924763 Test MSE 26.859266725945076 Test RE 0.08724413611059931 Lambda1 0.94483525\n",
      "191 Train Loss 23.917343 Test MSE 26.866499432512185 Test RE 0.08725588194060308 Lambda1 0.9452816\n",
      "192 Train Loss 23.914171 Test MSE 26.861220089144545 Test RE 0.0872473085052187 Lambda1 0.94567794\n",
      "193 Train Loss 23.91038 Test MSE 26.85457100041326 Test RE 0.08723650946113182 Lambda1 0.94606763\n",
      "194 Train Loss 23.90588 Test MSE 26.858774002692197 Test RE 0.087243335876324 Lambda1 0.9470791\n",
      "195 Train Loss 23.899343 Test MSE 26.861593134736445 Test RE 0.08724791434350031 Lambda1 0.94737816\n",
      "196 Train Loss 23.894672 Test MSE 26.87435527766142 Test RE 0.0872686379492088 Lambda1 0.9474209\n",
      "197 Train Loss 23.892735 Test MSE 26.871696116690956 Test RE 0.08726432031824867 Lambda1 0.9480056\n",
      "198 Train Loss 23.890383 Test MSE 26.873044123786343 Test RE 0.08726650907937558 Lambda1 0.9473991\n",
      "199 Train Loss 23.88396 Test MSE 26.858953846007154 Test RE 0.08724362796159317 Lambda1 0.9492829\n",
      "Training time: 352.38\n",
      "Training time: 352.38\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10 #10\n",
    "max_iter = 200 #75\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "\n",
    "lambda1_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 0.5\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    'Generate Training data'\n",
    "    print(reps)\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "\n",
    "    lambda1_val = []\n",
    "\n",
    "    N_f = 50000 #Total number of collocation points \n",
    "    N_train = 5000\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr= 0.5, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "    lambda1_full.append(lambda1_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"lambda1\": lambda1_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
