{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_2D_4(xt): #True function for 2D_4 Heat Transfer in a rod x \\in [0,1] t \\in [0,0.1]\n",
    "    term1 = 4*u0/np.pi\n",
    "    \n",
    "    resol_n = 10000\n",
    "    \n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "\n",
    "    u = np.zeros((np.shape(xt)[0],1))\n",
    "    \n",
    "    for i in range(resol_n):\n",
    "        j = 2*i-1\n",
    "        term2 = np.sin(j*np.pi*x)/j\n",
    "        term3 = np.exp(-1*np.square(j*np.pi)*t)\n",
    "        \n",
    "        u = u + term2*term3\n",
    "        \n",
    "    u = term1*u\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = 50.0\n",
    "loss_thresh = 0.1\n",
    "label = \"inv_HT_stan\"\n",
    "\n",
    "x_ll = np.array(0.0)\n",
    "x_ul = np.array(1.0)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,100).reshape(-1,1)\n",
    "t = np.linspace(0,0.1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = true_2D_4(xt)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_f,N_train,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #X_Train\n",
    "    np.random.seed(seed)\n",
    "    x_train = np.random.uniform(x_ll,x_ul,(N_train,1))\n",
    "    t_train = np.random.uniform(0,0.1,(N_train,1))\n",
    "    \n",
    "    xt_train = np.hstack((x_train,t_train))\n",
    "    u_train = true_2D_4(xt_train)\n",
    "    \n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requiresGrad = True\n",
    "        \n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    \n",
    "    def loss_PDE(self, xt_coll,f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_tt[:,[0]]\n",
    "                \n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        f = du_dt - self.lambda1*d2u_dx2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_train = self.loss_function(self.forward(xt_train),u_train)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "        \n",
    "        #print(self.iter,\"train_loss\",loss_train.cpu().detach().numpy(),\"F Loss\",(loss_f).cpu().detach().numpy())\n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                    \n",
    "    \n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_coll,f_hat, xt_train, u_train,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xt_coll, xt_train, u_train = trainingdata(N_f,N_train,123)\n",
    "    \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_train = torch.from_numpy(xt_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_coll,f_hat, xt_train, u_train,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_coll,f_hat, xt_train, u_train).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1],\"Lambda1\",lambda1_val[-1])\n",
    "\n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_HT_stan\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 837.1386 Test MSE 856.5487781454734 Test RE 0.49268032652896426 Lambda1 -0.0042532547\n",
      "1 Train Loss 725.5127 Test MSE 726.2841342146576 Test RE 0.45367249421358996 Lambda1 0.21428204\n",
      "2 Train Loss 483.7136 Test MSE 486.7868054409421 Test RE 0.37141425631458413 Lambda1 0.05470079\n",
      "3 Train Loss 275.00858 Test MSE 293.9578125742363 Test RE 0.2886234902099412 Lambda1 0.00025507304\n",
      "4 Train Loss 262.11 Test MSE 285.9479737963908 Test RE 0.2846640883390851 Lambda1 0.00014479698\n",
      "5 Train Loss 256.8619 Test MSE 282.40705927067626 Test RE 0.2828960899366651 Lambda1 0.00012800789\n",
      "6 Train Loss 255.37175 Test MSE 282.52352437139757 Test RE 0.28295441731256615 Lambda1 0.00012495005\n",
      "7 Train Loss 254.726 Test MSE 282.04556355862843 Test RE 0.28271497110937016 Lambda1 6.549826e-05\n",
      "8 Train Loss 254.33267 Test MSE 282.15956892946616 Test RE 0.2827721033123307 Lambda1 8.184863e-05\n",
      "9 Train Loss 254.08357 Test MSE 283.1477976815754 Test RE 0.28326685752597575 Lambda1 0.00011199611\n",
      "10 Train Loss 253.95955 Test MSE 283.161033351054 Test RE 0.28327347806605774 Lambda1 0.00013577838\n",
      "11 Train Loss 253.91223 Test MSE 282.93702143310196 Test RE 0.2831614054590699 Lambda1 0.0002046027\n",
      "12 Train Loss 253.85124 Test MSE 282.82294905699587 Test RE 0.2831043182837301 Lambda1 0.0002561511\n",
      "13 Train Loss 253.79118 Test MSE 282.7769571347434 Test RE 0.28308129850895114 Lambda1 0.00030578187\n",
      "14 Train Loss 253.71815 Test MSE 282.71590590215345 Test RE 0.28305073838961 Lambda1 0.00035334445\n",
      "15 Train Loss 253.52634 Test MSE 282.5084543308311 Test RE 0.2829468706995514 Lambda1 0.00054272136\n",
      "16 Train Loss 253.39949 Test MSE 282.34947655431904 Test RE 0.2828672472515532 Lambda1 0.0007632571\n",
      "17 Train Loss 253.31274 Test MSE 282.3132051173484 Test RE 0.28284907769235385 Lambda1 0.0009054471\n",
      "18 Train Loss 252.90144 Test MSE 282.0745357127008 Test RE 0.2827294911948211 Lambda1 0.0012766868\n",
      "19 Train Loss 252.57445 Test MSE 281.7372308957264 Test RE 0.28256039662239324 Lambda1 0.0017311495\n",
      "20 Train Loss 252.32861 Test MSE 281.77577876140964 Test RE 0.28257972620769045 Lambda1 0.0021684964\n",
      "21 Train Loss 251.8887 Test MSE 280.7035542375498 Test RE 0.2820415718996287 Lambda1 0.0035978649\n",
      "22 Train Loss 251.22992 Test MSE 279.20904258149517 Test RE 0.2812897521206946 Lambda1 0.0058850376\n",
      "23 Train Loss 250.11772 Test MSE 277.4634582649246 Test RE 0.28040907714949925 Lambda1 0.009667006\n",
      "24 Train Loss 247.90576 Test MSE 272.7809002413484 Test RE 0.2780328745171323 Lambda1 0.016783524\n",
      "25 Train Loss 245.07817 Test MSE 265.913649072522 Test RE 0.2745108316554616 Lambda1 0.028323008\n",
      "26 Train Loss 239.23991 Test MSE 255.9216175389874 Test RE 0.269303908574341 Lambda1 0.045370635\n",
      "27 Train Loss 231.2738 Test MSE 246.379694078233 Test RE 0.2642357810030944 Lambda1 0.07019653\n",
      "28 Train Loss 214.50366 Test MSE 230.42475304154073 Test RE 0.25553696711295754 Lambda1 0.112180784\n",
      "29 Train Loss 199.44855 Test MSE 205.3301849169388 Test RE 0.24122125340851294 Lambda1 0.1755017\n",
      "30 Train Loss 178.44789 Test MSE 182.94855780918974 Test RE 0.22769509007730065 Lambda1 0.2403101\n",
      "31 Train Loss 165.70642 Test MSE 173.14033214381615 Test RE 0.22150742770011714 Lambda1 0.27589288\n",
      "32 Train Loss 155.0049 Test MSE 160.4679519877403 Test RE 0.2132471941576459 Lambda1 0.31345078\n",
      "33 Train Loss 146.18274 Test MSE 147.84666605649628 Test RE 0.20468920463040813 Lambda1 0.3472915\n",
      "34 Train Loss 136.25659 Test MSE 137.34170280797483 Test RE 0.19728332752463928 Lambda1 0.35928223\n",
      "35 Train Loss 127.94777 Test MSE 127.97898353320426 Test RE 0.19044014298384018 Lambda1 0.3854692\n",
      "36 Train Loss 120.31544 Test MSE 117.17409242654675 Test RE 0.1822237458193426 Lambda1 0.415144\n",
      "37 Train Loss 112.681984 Test MSE 108.64171880783357 Test RE 0.175463781413686 Lambda1 0.44581544\n",
      "38 Train Loss 102.89978 Test MSE 100.23277280187025 Test RE 0.1685365280554761 Lambda1 0.47926253\n",
      "39 Train Loss 94.98762 Test MSE 93.79217394622371 Test RE 0.16303185563391037 Lambda1 0.50631744\n",
      "40 Train Loss 88.80272 Test MSE 85.59009982906413 Test RE 0.15574027495411752 Lambda1 0.52954984\n",
      "41 Train Loss 80.74174 Test MSE 79.69123011136263 Test RE 0.15027766414563895 Lambda1 0.55069184\n",
      "42 Train Loss 76.51349 Test MSE 76.23403035537305 Test RE 0.14698181671080165 Lambda1 0.5650436\n",
      "43 Train Loss 70.707596 Test MSE 69.87773567384743 Test RE 0.14072089299742072 Lambda1 0.58640933\n",
      "44 Train Loss 66.44133 Test MSE 65.82449879887362 Test RE 0.136578692602251 Lambda1 0.5982827\n",
      "45 Train Loss 62.195667 Test MSE 58.79106783694422 Test RE 0.12907580577094027 Lambda1 0.62107104\n",
      "46 Train Loss 59.30709 Test MSE 56.63949860646457 Test RE 0.12669190624320872 Lambda1 0.63154346\n",
      "47 Train Loss 56.66748 Test MSE 54.48818473816981 Test RE 0.12426257258611019 Lambda1 0.6421316\n",
      "48 Train Loss 53.800816 Test MSE 51.284324578283005 Test RE 0.12055396275024444 Lambda1 0.6592488\n",
      "49 Train Loss 51.94883 Test MSE 50.04752765094707 Test RE 0.1190914230371588 Lambda1 0.6697844\n",
      "50 Train Loss 50.55257 Test MSE 49.45099834630427 Test RE 0.11837955485182787 Lambda1 0.6763575\n",
      "51 Train Loss 49.447487 Test MSE 47.91243966469707 Test RE 0.1165234442782934 Lambda1 0.6892128\n",
      "52 Train Loss 48.040966 Test MSE 47.08085722993762 Test RE 0.11550781044458243 Lambda1 0.7000397\n",
      "53 Train Loss 46.86144 Test MSE 46.08116465329164 Test RE 0.11427491161988591 Lambda1 0.7099545\n",
      "54 Train Loss 45.26793 Test MSE 45.01380433645506 Test RE 0.11294370495847006 Lambda1 0.7255953\n",
      "55 Train Loss 44.2352 Test MSE 44.54641942930735 Test RE 0.11235581945618169 Lambda1 0.73421687\n",
      "56 Train Loss 43.05342 Test MSE 44.16802858494792 Test RE 0.11187760951224039 Lambda1 0.7424614\n",
      "57 Train Loss 42.370678 Test MSE 43.70200679500176 Test RE 0.11128582782360198 Lambda1 0.75205725\n",
      "58 Train Loss 41.74039 Test MSE 42.47836102397435 Test RE 0.10971677826916874 Lambda1 0.7671032\n",
      "59 Train Loss 41.09957 Test MSE 41.86675911654818 Test RE 0.1089240654563804 Lambda1 0.7835329\n",
      "60 Train Loss 40.384056 Test MSE 41.065302324840616 Test RE 0.10787645909683767 Lambda1 0.7997631\n",
      "61 Train Loss 39.745426 Test MSE 40.12994444071041 Test RE 0.10664081361458641 Lambda1 0.81838816\n",
      "62 Train Loss 38.980717 Test MSE 39.095824970890256 Test RE 0.10525781767633326 Lambda1 0.8307739\n",
      "63 Train Loss 38.21471 Test MSE 38.321717991403155 Test RE 0.10421054230117642 Lambda1 0.85031587\n",
      "64 Train Loss 37.582893 Test MSE 37.93993541015379 Test RE 0.10369014080696724 Lambda1 0.8630941\n",
      "65 Train Loss 37.043053 Test MSE 37.59292085705674 Test RE 0.10321485477258496 Lambda1 0.87050027\n",
      "66 Train Loss 36.486313 Test MSE 36.93605044104745 Test RE 0.1023091314957106 Lambda1 0.8883017\n",
      "67 Train Loss 36.025417 Test MSE 36.546651994161294 Test RE 0.10176840537130712 Lambda1 0.8992782\n",
      "68 Train Loss 35.572346 Test MSE 36.496801176183574 Test RE 0.10169897398469037 Lambda1 0.900203\n",
      "69 Train Loss 35.09419 Test MSE 36.0415138365337 Test RE 0.10106265020437497 Lambda1 0.9162795\n",
      "70 Train Loss 34.69936 Test MSE 35.505493712999176 Test RE 0.10030831812497022 Lambda1 0.9364245\n",
      "71 Train Loss 34.322384 Test MSE 35.60608225331129 Test RE 0.10045030644985256 Lambda1 0.93834496\n",
      "72 Train Loss 34.06525 Test MSE 35.318435025231146 Test RE 0.1000437348256264 Lambda1 0.95424366\n",
      "73 Train Loss 33.60984 Test MSE 35.2886414681506 Test RE 0.10000152899762932 Lambda1 0.96288526\n",
      "74 Train Loss 33.22383 Test MSE 34.95010753431997 Test RE 0.09952070154252358 Lambda1 0.97924244\n",
      "75 Train Loss 32.79582 Test MSE 34.983545186877244 Test RE 0.09956829714841318 Lambda1 0.9752877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 Train Loss 32.316463 Test MSE 34.94566287168426 Test RE 0.09951437323570461 Lambda1 0.9753818\n",
      "77 Train Loss 31.976255 Test MSE 34.895655015815585 Test RE 0.09944314433826248 Lambda1 0.98997027\n",
      "78 Train Loss 31.685345 Test MSE 34.77232740160116 Test RE 0.09926726369057975 Lambda1 1.0050815\n",
      "79 Train Loss 31.349659 Test MSE 34.31969740917194 Test RE 0.09861906840120296 Lambda1 1.0177145\n",
      "80 Train Loss 31.082523 Test MSE 33.85674832329981 Test RE 0.09795165837161128 Lambda1 1.0322508\n",
      "81 Train Loss 30.745825 Test MSE 33.7264175560309 Test RE 0.09776294526649658 Lambda1 1.0420364\n",
      "82 Train Loss 30.432756 Test MSE 33.594984439372205 Test RE 0.09757226637863287 Lambda1 1.05421\n",
      "83 Train Loss 30.153343 Test MSE 33.39217590251884 Test RE 0.09727730502456848 Lambda1 1.0627618\n",
      "84 Train Loss 29.93246 Test MSE 33.278814406598315 Test RE 0.09711204361793543 Lambda1 1.077235\n",
      "85 Train Loss 29.789354 Test MSE 33.2337764010996 Test RE 0.09704630789933365 Lambda1 1.080278\n",
      "86 Train Loss 29.54783 Test MSE 32.9911059946967 Test RE 0.09669134650529136 Lambda1 1.0955338\n",
      "87 Train Loss 29.365831 Test MSE 32.91739150707147 Test RE 0.09658326375525308 Lambda1 1.1044434\n",
      "88 Train Loss 29.208397 Test MSE 32.72190762273287 Test RE 0.09629605105057966 Lambda1 1.1125724\n",
      "89 Train Loss 29.069738 Test MSE 32.771406479912685 Test RE 0.09636885767437835 Lambda1 1.113283\n",
      "90 Train Loss 28.921137 Test MSE 32.54199931554028 Test RE 0.09603096353912499 Lambda1 1.1213838\n",
      "91 Train Loss 28.739584 Test MSE 32.4885192047834 Test RE 0.0959520215802016 Lambda1 1.1236341\n",
      "92 Train Loss 28.597492 Test MSE 32.326671944595326 Test RE 0.0957127222626262 Lambda1 1.1180501\n",
      "93 Train Loss 28.485352 Test MSE 32.05209432317709 Test RE 0.09530537108705596 Lambda1 1.1188252\n",
      "94 Train Loss 28.3734 Test MSE 31.878670871306454 Test RE 0.09504718820332053 Lambda1 1.1195205\n",
      "95 Train Loss 28.307867 Test MSE 31.879177674979623 Test RE 0.095047943725294 Lambda1 1.1179097\n",
      "96 Train Loss 28.257748 Test MSE 31.80783184756653 Test RE 0.09494152516197986 Lambda1 1.113628\n",
      "97 Train Loss 28.210344 Test MSE 31.813095551427292 Test RE 0.09494938051348056 Lambda1 1.1119977\n",
      "98 Train Loss 28.169744 Test MSE 31.784075041349798 Test RE 0.09490606331826222 Lambda1 1.1121165\n",
      "99 Train Loss 28.12166 Test MSE 31.7113507276926 Test RE 0.09479742509471617 Lambda1 1.1079527\n",
      "100 Train Loss 28.058435 Test MSE 31.664990251650458 Test RE 0.09472810510209967 Lambda1 1.1060749\n",
      "101 Train Loss 28.003565 Test MSE 31.59414052067076 Test RE 0.0946220697125061 Lambda1 1.1037751\n",
      "102 Train Loss 27.958454 Test MSE 31.579096175834337 Test RE 0.09459953869053746 Lambda1 1.106043\n",
      "103 Train Loss 27.879133 Test MSE 31.442809420959406 Test RE 0.0943951850815702 Lambda1 1.1025046\n",
      "104 Train Loss 27.764168 Test MSE 31.35756052170058 Test RE 0.09426713438306479 Lambda1 1.103328\n",
      "105 Train Loss 27.674446 Test MSE 31.30878842936613 Test RE 0.09419379649544252 Lambda1 1.1052368\n",
      "106 Train Loss 27.631367 Test MSE 31.25997705661225 Test RE 0.09412034232903764 Lambda1 1.1054004\n",
      "107 Train Loss 27.54179 Test MSE 31.154252346720583 Test RE 0.09396104480549886 Lambda1 1.1102532\n",
      "108 Train Loss 27.441439 Test MSE 31.06018173573102 Test RE 0.09381907934870513 Lambda1 1.1105448\n",
      "109 Train Loss 27.372461 Test MSE 31.014834627639875 Test RE 0.09375056754890558 Lambda1 1.1094505\n",
      "110 Train Loss 27.288668 Test MSE 30.889917413867032 Test RE 0.093561579348939 Lambda1 1.1086342\n",
      "111 Train Loss 27.219309 Test MSE 30.747529533480794 Test RE 0.09334569300741064 Lambda1 1.1080455\n",
      "112 Train Loss 27.090796 Test MSE 30.47639231457822 Test RE 0.09293321214693824 Lambda1 1.10854\n",
      "113 Train Loss 26.96511 Test MSE 30.45528160437607 Test RE 0.09290101959028217 Lambda1 1.1060183\n",
      "114 Train Loss 26.888575 Test MSE 30.278464363174542 Test RE 0.09263094469969084 Lambda1 1.1064687\n",
      "115 Train Loss 26.79709 Test MSE 30.20154244388867 Test RE 0.09251320621102437 Lambda1 1.1053308\n",
      "116 Train Loss 26.761003 Test MSE 30.155408611745962 Test RE 0.09244252075095284 Lambda1 1.1052183\n",
      "117 Train Loss 26.676746 Test MSE 30.094096099193536 Test RE 0.09234849504515437 Lambda1 1.1020584\n",
      "118 Train Loss 26.530739 Test MSE 29.91645138292801 Test RE 0.09207552615829331 Lambda1 1.0968511\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e4b802f53c92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mnan_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d875a2162a6a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-77392118dd0b>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(xt_coll, f_hat, xt_train, u_train, seed)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                     \u001b[0mbe_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                     \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_stps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbe_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprev_flat_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 10 #10\n",
    "max_iter = 100 #75\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "\n",
    "lambda1_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 0.5\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    'Generate Training data'\n",
    "    print(reps)\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "\n",
    "    lambda1_val = []\n",
    "\n",
    "    N_f = 50000 #Total number of collocation points \n",
    "    N_train = 500\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr= 0.5, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "    lambda1_full.append(lambda1_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"lambda1\": lambda1_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
