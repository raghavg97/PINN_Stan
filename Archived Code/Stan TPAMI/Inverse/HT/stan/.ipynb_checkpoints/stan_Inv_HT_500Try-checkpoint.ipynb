{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_2D_4(xt): #True function for 2D_4 Heat Transfer in a rod x \\in [0,1] t \\in [0,0.1]\n",
    "    term1 = 4*u0/np.pi\n",
    "    \n",
    "    resol_n = 10000\n",
    "    \n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "\n",
    "    u = np.zeros((np.shape(xt)[0],1))\n",
    "    \n",
    "    for i in range(resol_n):\n",
    "        j = 2*i-1\n",
    "        term2 = np.sin(j*np.pi*x)/j\n",
    "        term3 = np.exp(-1*np.square(j*np.pi)*t)\n",
    "        \n",
    "        u = u + term2*term3\n",
    "        \n",
    "    u = term1*u\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = 50.0\n",
    "loss_thresh = 0.1\n",
    "label = \"inv_HT_stan_late_Try\"\n",
    "\n",
    "x_ll = np.array(0.0)\n",
    "x_ul = np.array(1.0)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,100).reshape(-1,1)\n",
    "t = np.linspace(0,0.1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = true_2D_4(xt)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_f,N_train,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #X_Train\n",
    "    np.random.seed(seed)\n",
    "    x_train = np.random.uniform(x_ll,x_ul,(N_train,1))\n",
    "    t_train = np.random.uniform(0,0.1,(N_train,1))\n",
    "    \n",
    "    xt_train = np.hstack((x_train,t_train))\n",
    "    u_train = true_2D_4(xt_train)\n",
    "    \n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requiresGrad = True\n",
    "        \n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    \n",
    "    def loss_PDE(self, xt_coll,f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_tt[:,[0]]\n",
    "                \n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        f = du_dt - self.lambda1*d2u_dx2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_train = self.loss_function(self.forward(xt_train),u_train)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "        \n",
    "        #print(self.iter,\"train_loss\",loss_train.cpu().detach().numpy(),\"F Loss\",(loss_f).cpu().detach().numpy())\n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                    \n",
    "    \n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_coll,f_hat, xt_train, u_train,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xt_coll, xt_train, u_train = trainingdata(N_f,N_train,123)\n",
    "    \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_train = torch.from_numpy(xt_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_coll,f_hat, xt_train, u_train,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_coll,f_hat, xt_train, u_train).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1],\"Lambda1\",lambda1_val[-1])\n",
    "\n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_HT_stan_late_Try\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 839.749 Test MSE 841.5359806202691 Test RE 0.4883436167271201 Lambda1 0.006605599\n",
      "1 Train Loss 765.5484 Test MSE 759.0615061228043 Test RE 0.46379670034843473 Lambda1 -0.022968855\n",
      "2 Train Loss 387.40503 Test MSE 375.3447787279512 Test RE 0.32614022021350164 Lambda1 0.0027274373\n",
      "3 Train Loss 279.45248 Test MSE 284.35544051793937 Test RE 0.28387029010824383 Lambda1 0.0013656046\n",
      "4 Train Loss 272.0106 Test MSE 279.12754061367076 Test RE 0.28124869445763157 Lambda1 0.0014618477\n",
      "5 Train Loss 269.18433 Test MSE 275.9649771316043 Test RE 0.2796508573600073 Lambda1 0.007891449\n",
      "6 Train Loss 259.06174 Test MSE 263.5600524463289 Test RE 0.27329328648781576 Lambda1 0.030248016\n",
      "7 Train Loss 229.4283 Test MSE 221.13702162881765 Test RE 0.25033403446122193 Lambda1 0.10518695\n",
      "8 Train Loss 189.5301 Test MSE 184.56744335987793 Test RE 0.22870029185214397 Lambda1 0.1787905\n",
      "9 Train Loss 143.68674 Test MSE 134.31309050198408 Test RE 0.19509598942797385 Lambda1 0.2820334\n",
      "10 Train Loss 112.94324 Test MSE 106.23736316875338 Test RE 0.17351131934013697 Lambda1 0.33701152\n",
      "11 Train Loss 83.06926 Test MSE 74.2347427404594 Test RE 0.14504166431986945 Lambda1 0.41631496\n",
      "12 Train Loss 66.51078 Test MSE 66.01090205225215 Test RE 0.13677193915139024 Lambda1 0.43687198\n",
      "13 Train Loss 59.464592 Test MSE 63.15077989016988 Test RE 0.13377609970899915 Lambda1 0.4532264\n",
      "14 Train Loss 54.119675 Test MSE 57.730816134473585 Test RE 0.1279066189881896 Lambda1 0.49529096\n",
      "15 Train Loss 48.468777 Test MSE 51.77651163979184 Test RE 0.12113107298035146 Lambda1 0.5338541\n",
      "16 Train Loss 43.89461 Test MSE 46.06721695231008 Test RE 0.11425761612687622 Lambda1 0.59644455\n",
      "17 Train Loss 41.30275 Test MSE 44.279723934691226 Test RE 0.1120189823419146 Lambda1 0.6219644\n",
      "18 Train Loss 38.267574 Test MSE 41.264443344688 Test RE 0.10813770941645112 Lambda1 0.6716769\n",
      "19 Train Loss 35.60085 Test MSE 38.544526829934796 Test RE 0.10451305193594387 Lambda1 0.7417768\n",
      "20 Train Loss 33.618664 Test MSE 37.183574150317654 Test RE 0.10265136693303631 Lambda1 0.78639615\n",
      "21 Train Loss 32.544598 Test MSE 36.17669851898643 Test RE 0.10125200594729422 Lambda1 0.84434587\n",
      "22 Train Loss 32.03857 Test MSE 35.90018393618518 Test RE 0.10086430664905359 Lambda1 0.86694115\n",
      "23 Train Loss 31.310823 Test MSE 35.15580586051396 Test RE 0.0998131356820533 Lambda1 0.90870243\n",
      "24 Train Loss 30.97721 Test MSE 34.8313073729131 Test RE 0.0993514153741381 Lambda1 0.9351988\n",
      "25 Train Loss 30.735714 Test MSE 34.44779582028104 Test RE 0.09880294501926543 Lambda1 0.9427861\n",
      "26 Train Loss 30.305607 Test MSE 33.91473590365932 Test RE 0.09803550501888667 Lambda1 0.9625891\n",
      "27 Train Loss 29.98619 Test MSE 33.50024447091681 Test RE 0.09743458928249016 Lambda1 0.9888685\n",
      "28 Train Loss 29.776073 Test MSE 33.22373140475233 Test RE 0.09703164054000393 Lambda1 0.9793948\n",
      "29 Train Loss 29.278423 Test MSE 32.60297859795727 Test RE 0.09612089591339595 Lambda1 0.9909659\n",
      "30 Train Loss 29.002054 Test MSE 32.317741700038724 Test RE 0.0956995010275891 Lambda1 1.0033909\n",
      "31 Train Loss 28.67197 Test MSE 32.037220194458534 Test RE 0.0952832547653826 Lambda1 1.0185814\n",
      "32 Train Loss 28.580196 Test MSE 31.96296824018339 Test RE 0.09517277277030824 Lambda1 1.0198536\n",
      "33 Train Loss 28.242556 Test MSE 31.690293386234018 Test RE 0.09476594562134388 Lambda1 1.0085008\n",
      "34 Train Loss 28.177055 Test MSE 31.56689648529993 Test RE 0.09458126399746415 Lambda1 1.0017457\n",
      "35 Train Loss 27.942904 Test MSE 31.29335914219754 Test RE 0.09417058380807547 Lambda1 0.99938244\n",
      "36 Train Loss 27.837048 Test MSE 31.154402291986138 Test RE 0.09396127092225341 Lambda1 0.9750051\n",
      "37 Train Loss 27.745125 Test MSE 31.15953441192284 Test RE 0.09396900980694833 Lambda1 0.98245215\n",
      "38 Train Loss 27.630724 Test MSE 30.89257120938323 Test RE 0.09356559826494742 Lambda1 0.9777225\n",
      "39 Train Loss 27.528759 Test MSE 30.853337868779665 Test RE 0.09350616557476275 Lambda1 0.9863159\n",
      "40 Train Loss 27.434898 Test MSE 30.729929893751514 Test RE 0.09331897401977048 Lambda1 0.984162\n",
      "41 Train Loss 27.25033 Test MSE 30.518963878333697 Test RE 0.09299809731240656 Lambda1 0.9731199\n",
      "42 Train Loss 27.154427 Test MSE 30.393230188245315 Test RE 0.09280633028152317 Lambda1 0.9756417\n",
      "43 Train Loss 27.030643 Test MSE 30.284248210941303 Test RE 0.09263979154365612 Lambda1 0.973191\n",
      "44 Train Loss 26.947876 Test MSE 30.129614381745725 Test RE 0.09240297570948422 Lambda1 0.9772119\n",
      "45 Train Loss 26.899277 Test MSE 30.01493343277988 Test RE 0.09222695348052357 Lambda1 0.97409165\n",
      "46 Train Loss 26.860233 Test MSE 29.957341397124853 Test RE 0.09213842940683382 Lambda1 0.977657\n",
      "47 Train Loss 26.808853 Test MSE 29.91319713952631 Test RE 0.09207051813915194 Lambda1 0.978821\n",
      "48 Train Loss 26.719719 Test MSE 29.92074013769926 Test RE 0.09208212579130921 Lambda1 0.9805243\n",
      "49 Train Loss 26.685896 Test MSE 29.941193758334663 Test RE 0.09211359378136198 Lambda1 0.97742915\n",
      "50 Train Loss 26.650095 Test MSE 29.894415769127693 Test RE 0.09204160979465627 Lambda1 0.978002\n",
      "51 Train Loss 26.57597 Test MSE 29.795311381034878 Test RE 0.09188891739864598 Lambda1 0.9842604\n",
      "52 Train Loss 26.516459 Test MSE 29.81851805830534 Test RE 0.09192469519875707 Lambda1 0.9863735\n",
      "53 Train Loss 26.46244 Test MSE 29.75829917441493 Test RE 0.0918318267307982 Lambda1 0.9835962\n",
      "54 Train Loss 26.379082 Test MSE 29.557973077006842 Test RE 0.09152220930353018 Lambda1 0.9830027\n",
      "55 Train Loss 26.297207 Test MSE 29.463938064435357 Test RE 0.09137651007118294 Lambda1 0.9805465\n",
      "56 Train Loss 26.211334 Test MSE 29.49078632438765 Test RE 0.09141813284523746 Lambda1 0.98459315\n",
      "57 Train Loss 26.127398 Test MSE 29.47970561148643 Test RE 0.0914009567475315 Lambda1 0.9824147\n",
      "58 Train Loss 26.034018 Test MSE 29.392209960618672 Test RE 0.09126521711307514 Lambda1 0.97329676\n",
      "59 Train Loss 26.013332 Test MSE 29.397791280044963 Test RE 0.09127388192852279 Lambda1 0.9716945\n",
      "60 Train Loss 25.98749 Test MSE 29.390900199154828 Test RE 0.09126318363215918 Lambda1 0.96956676\n",
      "61 Train Loss 25.962442 Test MSE 29.410652271510667 Test RE 0.09129384506531961 Lambda1 0.9665885\n",
      "62 Train Loss 25.898628 Test MSE 29.30981989773903 Test RE 0.09113721340385988 Lambda1 0.965468\n",
      "63 Train Loss 25.873144 Test MSE 29.318983614733252 Test RE 0.09115145931813924 Lambda1 0.96536314\n",
      "64 Train Loss 25.834873 Test MSE 29.3104578934462 Test RE 0.09113820530409511 Lambda1 0.9642533\n",
      "65 Train Loss 25.81049 Test MSE 29.27666303873239 Test RE 0.09108564913560228 Lambda1 0.96283007\n",
      "66 Train Loss 25.796547 Test MSE 29.21009458728205 Test RE 0.09098203621286591 Lambda1 0.96125317\n",
      "67 Train Loss 25.783293 Test MSE 29.169499541545076 Test RE 0.0909187925960858 Lambda1 0.9615768\n",
      "68 Train Loss 25.736334 Test MSE 29.070322091649192 Test RE 0.090764097249649 Lambda1 0.959587\n",
      "69 Train Loss 25.689152 Test MSE 29.026937792552623 Test RE 0.0906963441819616 Lambda1 0.96343553\n",
      "70 Train Loss 25.666578 Test MSE 28.992347157612933 Test RE 0.09064228785410434 Lambda1 0.9667125\n",
      "71 Train Loss 25.650177 Test MSE 28.98346347206702 Test RE 0.0906283997188713 Lambda1 0.96959734\n",
      "72 Train Loss 25.61012 Test MSE 28.96542682437806 Test RE 0.09060019593107359 Lambda1 0.9697529\n",
      "73 Train Loss 25.538431 Test MSE 28.910110107492574 Test RE 0.0905136427364355 Lambda1 0.96756876\n",
      "74 Train Loss 25.512554 Test MSE 28.857673603392847 Test RE 0.09043151968188946 Lambda1 0.9692647\n",
      "75 Train Loss 25.49711 Test MSE 28.83581966335563 Test RE 0.09039727126432676 Lambda1 0.9679134\n",
      "76 Train Loss 25.467659 Test MSE 28.78021288456735 Test RE 0.09031006849312817 Lambda1 0.96066433\n",
      "77 Train Loss 25.43632 Test MSE 28.724704941695002 Test RE 0.09022293665771683 Lambda1 0.9607018\n",
      "78 Train Loss 25.394253 Test MSE 28.68980897545976 Test RE 0.0901681167153406 Lambda1 0.961688\n",
      "79 Train Loss 25.371979 Test MSE 28.68069169373344 Test RE 0.09015378839676629 Lambda1 0.96253604\n",
      "80 Train Loss 25.354088 Test MSE 28.643205896258728 Test RE 0.0900948534229704 Lambda1 0.9603444\n",
      "81 Train Loss 25.32037 Test MSE 28.628817634809007 Test RE 0.09007222203274386 Lambda1 0.95502317\n",
      "82 Train Loss 25.3043 Test MSE 28.62075545104315 Test RE 0.09005953848521431 Lambda1 0.95541215\n",
      "83 Train Loss 25.285324 Test MSE 28.625495255391154 Test RE 0.09006699543234692 Lambda1 0.9528282\n",
      "84 Train Loss 25.261639 Test MSE 28.585083780833934 Test RE 0.0900033978325129 Lambda1 0.9511041\n",
      "85 Train Loss 25.251898 Test MSE 28.547198686177442 Test RE 0.08994373528599449 Lambda1 0.94966835\n",
      "86 Train Loss 25.239977 Test MSE 28.528841614034434 Test RE 0.08991481179732602 Lambda1 0.9498271\n",
      "87 Train Loss 25.21817 Test MSE 28.469735989500826 Test RE 0.08982162142802447 Lambda1 0.95013547\n",
      "88 Train Loss 25.205257 Test MSE 28.455641352537647 Test RE 0.08979938448356933 Lambda1 0.94763696\n",
      "89 Train Loss 25.180582 Test MSE 28.46946576745294 Test RE 0.08982119515360996 Lambda1 0.9442641\n",
      "90 Train Loss 25.151924 Test MSE 28.42481456456873 Test RE 0.0897507302170655 Lambda1 0.94471705\n",
      "91 Train Loss 25.145033 Test MSE 28.432669272576984 Test RE 0.08976312989487556 Lambda1 0.94122225\n",
      "92 Train Loss 25.125711 Test MSE 28.446341230650358 Test RE 0.0897847087737641 Lambda1 0.9402169\n",
      "93 Train Loss 25.11264 Test MSE 28.456428879704642 Test RE 0.08980062710124029 Lambda1 0.93799376\n",
      "94 Train Loss 25.08579 Test MSE 28.44713687277819 Test RE 0.08978596440092272 Lambda1 0.9381978\n",
      "95 Train Loss 25.054491 Test MSE 28.395072393071814 Test RE 0.08970376279985835 Lambda1 0.9357215\n",
      "96 Train Loss 25.025993 Test MSE 28.418361665535862 Test RE 0.0897405421950486 Lambda1 0.9363084\n",
      "97 Train Loss 25.011316 Test MSE 28.41076511843349 Test RE 0.08972854706620147 Lambda1 0.9319517\n",
      "98 Train Loss 24.994156 Test MSE 28.36374600753518 Test RE 0.08965426706176208 Lambda1 0.92836314\n",
      "99 Train Loss 24.972517 Test MSE 28.340191436380177 Test RE 0.08961703279760781 Lambda1 0.9276831\n",
      "100 Train Loss 24.939968 Test MSE 28.274441060752793 Test RE 0.08951301488410073 Lambda1 0.92739797\n",
      "101 Train Loss 24.916555 Test MSE 28.25451022340072 Test RE 0.08948146016922422 Lambda1 0.92433816\n",
      "102 Train Loss 24.888145 Test MSE 28.2533314925686 Test RE 0.08947959364145133 Lambda1 0.92492986\n",
      "103 Train Loss 24.860023 Test MSE 28.225119856568433 Test RE 0.08943490871286486 Lambda1 0.92815423\n",
      "104 Train Loss 24.82838 Test MSE 28.176388264307157 Test RE 0.08935766921233419 Lambda1 0.9328334\n",
      "105 Train Loss 24.79014 Test MSE 28.126681661696978 Test RE 0.08927881547933093 Lambda1 0.9370072\n",
      "106 Train Loss 24.732578 Test MSE 28.04440539822272 Test RE 0.08914814050147632 Lambda1 0.93772125\n",
      "107 Train Loss 24.691383 Test MSE 28.01916753147446 Test RE 0.08910801814419704 Lambda1 0.9352948\n",
      "108 Train Loss 24.659874 Test MSE 27.950368491907582 Test RE 0.08899855175900559 Lambda1 0.9340273\n",
      "109 Train Loss 24.636362 Test MSE 27.866053721734787 Test RE 0.08886421435249431 Lambda1 0.93940365\n",
      "110 Train Loss 24.606054 Test MSE 27.82751857345414 Test RE 0.08880274923790563 Lambda1 0.9457769\n",
      "111 Train Loss 24.578547 Test MSE 27.74671304223798 Test RE 0.08867372281895403 Lambda1 0.9573911\n",
      "112 Train Loss 24.536303 Test MSE 27.6649647633808 Test RE 0.08854299974296514 Lambda1 0.9641444\n",
      "113 Train Loss 24.499596 Test MSE 27.615898618714894 Test RE 0.0884644456638278 Lambda1 0.9616138\n",
      "114 Train Loss 24.482264 Test MSE 27.593916813241584 Test RE 0.0884292305258751 Lambda1 0.9646779\n",
      "115 Train Loss 24.471048 Test MSE 27.53789704563784 Test RE 0.08833942265693487 Lambda1 0.964191\n",
      "116 Train Loss 24.45796 Test MSE 27.538701186715752 Test RE 0.08834071245840104 Lambda1 0.96674913\n",
      "117 Train Loss 24.446898 Test MSE 27.510686364105407 Test RE 0.08829576699961322 Lambda1 0.9736382\n",
      "118 Train Loss 24.432787 Test MSE 27.508615464361863 Test RE 0.08829244365245932 Lambda1 0.97158736\n",
      "119 Train Loss 24.422474 Test MSE 27.48638837205212 Test RE 0.08825676608660253 Lambda1 0.9682371\n",
      "120 Train Loss 24.400394 Test MSE 27.49101965220089 Test RE 0.08826420112293934 Lambda1 0.9668217\n",
      "121 Train Loss 24.390331 Test MSE 27.50996680437466 Test RE 0.0882946122757191 Lambda1 0.9634563\n",
      "122 Train Loss 24.377289 Test MSE 27.480629294071033 Test RE 0.08824751961488819 Lambda1 0.96418166\n",
      "123 Train Loss 24.342947 Test MSE 27.46591500120872 Test RE 0.08822389072163045 Lambda1 0.96222967\n",
      "124 Train Loss 24.32107 Test MSE 27.42015640683598 Test RE 0.08815036897556063 Lambda1 0.95610934\n",
      "125 Train Loss 24.300365 Test MSE 27.360906647431246 Test RE 0.08805507935456293 Lambda1 0.95692986\n",
      "126 Train Loss 24.295086 Test MSE 27.335319403757858 Test RE 0.08801389625698906 Lambda1 0.95653504\n",
      "127 Train Loss 24.263727 Test MSE 27.30322433742582 Test RE 0.0879622114485174 Lambda1 0.95290726\n",
      "128 Train Loss 24.251781 Test MSE 27.30827127749617 Test RE 0.0879703408820729 Lambda1 0.9526275\n",
      "129 Train Loss 24.238916 Test MSE 27.240359432072413 Test RE 0.0878608878335093 Lambda1 0.9552558\n",
      "130 Train Loss 24.213104 Test MSE 27.18236686694855 Test RE 0.08776731357242813 Lambda1 0.95751864\n",
      "131 Train Loss 24.189741 Test MSE 27.090953299502825 Test RE 0.08761960973134829 Lambda1 0.9623167\n",
      "132 Train Loss 24.177702 Test MSE 27.10272360711115 Test RE 0.08763864187378963 Lambda1 0.9641731\n",
      "133 Train Loss 24.15777 Test MSE 27.059495073553 Test RE 0.08756872266785241 Lambda1 0.9664865\n",
      "134 Train Loss 24.125488 Test MSE 27.072878984094476 Test RE 0.0875903761957676 Lambda1 0.96630466\n",
      "135 Train Loss 24.095133 Test MSE 27.048867872852007 Test RE 0.08755152534437927 Lambda1 0.966909\n",
      "136 Train Loss 24.084002 Test MSE 27.062155559127568 Test RE 0.0875730274340087 Lambda1 0.9627511\n",
      "137 Train Loss 24.074474 Test MSE 27.03105327611875 Test RE 0.08752268953475892 Lambda1 0.96245563\n",
      "138 Train Loss 24.058043 Test MSE 27.020414586988824 Test RE 0.08750546456185745 Lambda1 0.9585071\n",
      "139 Train Loss 24.050034 Test MSE 26.996929635573796 Test RE 0.08746742835198491 Lambda1 0.95403844\n",
      "140 Train Loss 24.038837 Test MSE 27.0089127525936 Test RE 0.08748683826564507 Lambda1 0.9523419\n",
      "141 Train Loss 24.027246 Test MSE 27.014381999976642 Test RE 0.08749569576659139 Lambda1 0.95087343\n",
      "142 Train Loss 24.017738 Test MSE 27.014924020859652 Test RE 0.08749657352600854 Lambda1 0.9488258\n",
      "143 Train Loss 23.999773 Test MSE 27.00218901560922 Test RE 0.08747594787716824 Lambda1 0.95006204\n",
      "144 Train Loss 23.989714 Test MSE 26.983928373854194 Test RE 0.08744636440344075 Lambda1 0.9498649\n",
      "145 Train Loss 23.977709 Test MSE 26.955295978614345 Test RE 0.08739995782810406 Lambda1 0.9460008\n",
      "146 Train Loss 23.969856 Test MSE 26.972727962384653 Test RE 0.08742821402693411 Lambda1 0.9415182\n",
      "147 Train Loss 23.942112 Test MSE 26.950642904108587 Test RE 0.08739241392897906 Lambda1 0.93439907\n",
      "148 Train Loss 23.917963 Test MSE 26.915158916280216 Test RE 0.08733486330277775 Lambda1 0.9378711\n",
      "149 Train Loss 23.911354 Test MSE 26.88391358845805 Test RE 0.0872841558397755 Lambda1 0.94041586\n",
      "150 Train Loss 23.906404 Test MSE 26.88576511235937 Test RE 0.08728716146464045 Lambda1 0.94251615\n",
      "151 Train Loss 23.90179 Test MSE 26.871655407476066 Test RE 0.08726425421778188 Lambda1 0.94337857\n",
      "152 Train Loss 23.892225 Test MSE 26.883835660607513 Test RE 0.08728402933528505 Lambda1 0.9433158\n",
      "153 Train Loss 23.885195 Test MSE 26.854913919271265 Test RE 0.08723706644170509 Lambda1 0.9460449\n",
      "154 Train Loss 23.879873 Test MSE 26.850792916760966 Test RE 0.08723037273261702 Lambda1 0.94678915\n",
      "155 Train Loss 23.877924 Test MSE 26.83937285449257 Test RE 0.08721182053871603 Lambda1 0.9483565\n",
      "156 Train Loss 23.869564 Test MSE 26.86873045617642 Test RE 0.08725950477759954 Lambda1 0.94665974\n",
      "157 Train Loss 23.854805 Test MSE 26.881413515514403 Test RE 0.0872800972448612 Lambda1 0.9463472\n",
      "158 Train Loss 23.84879 Test MSE 26.8844991456136 Test RE 0.08728510640048065 Lambda1 0.95089376\n",
      "159 Train Loss 23.84364 Test MSE 26.875292994172696 Test RE 0.08727016045138687 Lambda1 0.95067966\n",
      "160 Train Loss 23.840223 Test MSE 26.875530238221447 Test RE 0.08727054564310391 Lambda1 0.95092535\n",
      "161 Train Loss 23.83669 Test MSE 26.887603267332203 Test RE 0.08729014528501763 Lambda1 0.94868875\n",
      "162 Train Loss 23.82646 Test MSE 26.858506041752676 Test RE 0.08724290067655364 Lambda1 0.9529677\n",
      "163 Train Loss 23.819136 Test MSE 26.843548525055418 Test RE 0.08721860448390212 Lambda1 0.95683014\n",
      "164 Train Loss 23.808973 Test MSE 26.855995843007932 Test RE 0.08723882371604559 Lambda1 0.95369726\n",
      "165 Train Loss 23.79429 Test MSE 26.806090204389655 Test RE 0.08715772946606082 Lambda1 0.9610594\n",
      "166 Train Loss 23.78008 Test MSE 26.81431810153609 Test RE 0.08717110459476715 Lambda1 0.9677042\n",
      "167 Train Loss 23.77464 Test MSE 26.796625489239 Test RE 0.08714234124821069 Lambda1 0.96969664\n",
      "168 Train Loss 23.768244 Test MSE 26.78907218149979 Test RE 0.08713005874437708 Lambda1 0.9708616\n",
      "169 Train Loss 23.763931 Test MSE 26.76147780468244 Test RE 0.08708517254750371 Lambda1 0.97260714\n",
      "170 Train Loss 23.76042 Test MSE 26.77748432548751 Test RE 0.08711121225773455 Lambda1 0.96978277\n",
      "171 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "172 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "173 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "174 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "175 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "176 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "177 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "178 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "179 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "180 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "181 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "182 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "183 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "184 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "185 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "186 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "187 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "188 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "189 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "190 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "191 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "192 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "193 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "194 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "195 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "196 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "197 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "198 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "199 Train Loss 23.760138 Test MSE 26.77739342617287 Test RE 0.08711106440298994 Lambda1 0.96995866\n",
      "Training time: 320.36\n",
      "Training time: 320.36\n",
      "inv_HT_stan_late_Try\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 819.4199 Test MSE 818.2707029116165 Test RE 0.4815458802305865 Lambda1 0.07781963\n",
      "1 Train Loss 543.4611 Test MSE 523.0694324033855 Test RE 0.3850071910191994 Lambda1 0.007937028\n",
      "2 Train Loss 300.30756 Test MSE 300.8463159343291 Test RE 0.29198565767831225 Lambda1 0.0027970525\n",
      "3 Train Loss 273.18695 Test MSE 279.38731303969513 Test RE 0.2813795372701854 Lambda1 0.0016595455\n",
      "4 Train Loss 270.01645 Test MSE 277.71107962903665 Test RE 0.2805341743311498 Lambda1 0.003061564\n",
      "5 Train Loss 265.93616 Test MSE 271.08555403729775 Test RE 0.2771675341523425 Lambda1 0.01673694\n",
      "6 Train Loss 256.36423 Test MSE 260.2159199543006 Test RE 0.2715539361118382 Lambda1 0.037662886\n",
      "7 Train Loss 238.81937 Test MSE 240.57364918686343 Test RE 0.2611038037137961 Lambda1 0.07415751\n",
      "8 Train Loss 222.90755 Test MSE 219.88750867131338 Test RE 0.2496257886744662 Lambda1 0.11884115\n",
      "9 Train Loss 198.81482 Test MSE 195.2577981496762 Test RE 0.23523035524262073 Lambda1 0.15591605\n",
      "10 Train Loss 181.61453 Test MSE 173.00974076018835 Test RE 0.22142387576938224 Lambda1 0.1870881\n",
      "11 Train Loss 165.93503 Test MSE 156.66185172622232 Test RE 0.21070303844830954 Lambda1 0.21225448\n",
      "12 Train Loss 154.9099 Test MSE 142.76058058809699 Test RE 0.2011376276118359 Lambda1 0.24147585\n",
      "13 Train Loss 141.46387 Test MSE 126.05328426106074 Test RE 0.18900193625591047 Lambda1 0.27381498\n",
      "14 Train Loss 130.67912 Test MSE 113.59982337139465 Test RE 0.17942295239564723 Lambda1 0.29737332\n",
      "15 Train Loss 111.23593 Test MSE 92.99573258870176 Test RE 0.1623381829602353 Lambda1 0.33806992\n",
      "16 Train Loss 97.7623 Test MSE 84.55717845538759 Test RE 0.1547976673597575 Lambda1 0.36238554\n",
      "17 Train Loss 86.167694 Test MSE 76.82965907113699 Test RE 0.14755489576240974 Lambda1 0.38658208\n",
      "18 Train Loss 80.17054 Test MSE 72.26819220409328 Test RE 0.14310762230720916 Lambda1 0.41049853\n",
      "19 Train Loss 73.52729 Test MSE 66.80153424924255 Test RE 0.13758858022837414 Lambda1 0.43660542\n",
      "20 Train Loss 63.91187 Test MSE 62.73258637386467 Test RE 0.1333324217103985 Lambda1 0.4644338\n",
      "21 Train Loss 60.544075 Test MSE 60.39320178764082 Test RE 0.1308227270583885 Lambda1 0.47437638\n",
      "22 Train Loss 56.758392 Test MSE 57.14369943671616 Test RE 0.1272545580621755 Lambda1 0.4892383\n",
      "23 Train Loss 53.915115 Test MSE 56.50615632144182 Test RE 0.12654268756074782 Lambda1 0.4933554\n",
      "24 Train Loss 51.113525 Test MSE 54.398961674964 Test RE 0.12416079244928473 Lambda1 0.5130447\n",
      "25 Train Loss 48.433407 Test MSE 52.66644961879047 Test RE 0.1221676421009488 Lambda1 0.5266687\n",
      "26 Train Loss 45.738792 Test MSE 48.70166999666499 Test RE 0.11747923164384885 Lambda1 0.56343096\n",
      "27 Train Loss 43.662464 Test MSE 47.177766406193115 Test RE 0.11562662744527287 Lambda1 0.58665615\n",
      "28 Train Loss 41.82571 Test MSE 44.99344079854343 Test RE 0.11291815508966878 Lambda1 0.61004055\n",
      "29 Train Loss 39.976463 Test MSE 43.084242555157395 Test RE 0.11049646943513766 Lambda1 0.63655096\n",
      "30 Train Loss 38.900864 Test MSE 41.906713000623434 Test RE 0.10897602674734125 Lambda1 0.6582439\n",
      "31 Train Loss 37.56764 Test MSE 40.349608680160344 Test RE 0.1069322818020265 Lambda1 0.6770714\n",
      "32 Train Loss 36.382523 Test MSE 38.79941673743962 Test RE 0.10485804808402915 Lambda1 0.7062642\n",
      "33 Train Loss 35.693447 Test MSE 38.11827113306361 Test RE 0.10393355156990182 Lambda1 0.72342986\n",
      "34 Train Loss 35.042686 Test MSE 37.564324022323824 Test RE 0.10317558967144062 Lambda1 0.7385621\n",
      "35 Train Loss 34.39055 Test MSE 37.252653184912866 Test RE 0.10274667470175851 Lambda1 0.7445349\n",
      "36 Train Loss 33.97187 Test MSE 36.86029285231385 Test RE 0.10220415720425911 Lambda1 0.75464314\n",
      "37 Train Loss 33.638638 Test MSE 36.73241493820045 Test RE 0.10202671681196311 Lambda1 0.7525666\n",
      "38 Train Loss 33.081116 Test MSE 36.22353135213318 Test RE 0.10131752304552182 Lambda1 0.76637167\n",
      "39 Train Loss 32.635494 Test MSE 35.59786995592328 Test RE 0.10043872169651079 Lambda1 0.7883314\n",
      "40 Train Loss 31.94302 Test MSE 35.05967639390277 Test RE 0.09967657855679793 Lambda1 0.81231576\n",
      "41 Train Loss 31.53998 Test MSE 34.46196961758828 Test RE 0.09882326952566438 Lambda1 0.82794\n",
      "42 Train Loss 31.148628 Test MSE 33.975691011274954 Test RE 0.09812356529626215 Lambda1 0.8417244\n",
      "43 Train Loss 30.74104 Test MSE 33.56058253783873 Test RE 0.09752229565650755 Lambda1 0.8557729\n",
      "44 Train Loss 29.918285 Test MSE 32.60694379480826 Test RE 0.09612674088067054 Lambda1 0.90061945\n",
      "45 Train Loss 29.634386 Test MSE 32.27568192782688 Test RE 0.09563720692121476 Lambda1 0.91878676\n",
      "46 Train Loss 29.346603 Test MSE 32.226430210588724 Test RE 0.09556420931579958 Lambda1 0.92201114\n",
      "47 Train Loss 29.088242 Test MSE 32.05546803581783 Test RE 0.09531038674175016 Lambda1 0.9259105\n",
      "48 Train Loss 28.868383 Test MSE 31.869453996849096 Test RE 0.0950334470198394 Lambda1 0.9315302\n",
      "49 Train Loss 28.56639 Test MSE 31.71752689608123 Test RE 0.09480665611676306 Lambda1 0.9410636\n",
      "50 Train Loss 28.451212 Test MSE 31.481064382621664 Test RE 0.09445259067862398 Lambda1 0.95049495\n",
      "51 Train Loss 28.295818 Test MSE 31.34819291579555 Test RE 0.09425305287752152 Lambda1 0.954631\n",
      "52 Train Loss 28.149992 Test MSE 31.184657890455867 Test RE 0.09400688509554884 Lambda1 0.9670061\n",
      "53 Train Loss 28.052103 Test MSE 30.930642017393385 Test RE 0.09362323382278814 Lambda1 0.97940844\n",
      "54 Train Loss 27.97864 Test MSE 30.844192773549153 Test RE 0.09349230668261306 Lambda1 0.9842305\n",
      "55 Train Loss 27.847353 Test MSE 30.73554672422252 Test RE 0.09332750207221627 Lambda1 0.98553884\n",
      "56 Train Loss 27.780115 Test MSE 30.70452711823308 Test RE 0.0932803951641866 Lambda1 0.983888\n",
      "57 Train Loss 27.650019 Test MSE 30.593535135781845 Test RE 0.0931116456285813 Lambda1 0.9854263\n",
      "58 Train Loss 27.506691 Test MSE 30.584390385384417 Test RE 0.09309772853149638 Lambda1 0.9812627\n",
      "59 Train Loss 27.435432 Test MSE 30.54888138574943 Test RE 0.09304366880938242 Lambda1 0.9844666\n",
      "60 Train Loss 27.353542 Test MSE 30.489802405575944 Test RE 0.09295365593433429 Lambda1 0.9886022\n",
      "61 Train Loss 27.269184 Test MSE 30.479699160330195 Test RE 0.09293825387659493 Lambda1 0.99198383\n",
      "62 Train Loss 27.194565 Test MSE 30.40868113010215 Test RE 0.09282991716267455 Lambda1 0.9953299\n",
      "63 Train Loss 27.067114 Test MSE 30.20338969417543 Test RE 0.09251603541145248 Lambda1 0.99930364\n",
      "64 Train Loss 26.987453 Test MSE 30.1570982502539 Test RE 0.09244511053935482 Lambda1 1.0022696\n",
      "65 Train Loss 26.876701 Test MSE 30.080532444682305 Test RE 0.09232768158980982 Lambda1 1.0076113\n",
      "66 Train Loss 26.820747 Test MSE 30.026770807168436 Test RE 0.09224513805123624 Lambda1 1.0044754\n",
      "67 Train Loss 26.761677 Test MSE 29.974720920773024 Test RE 0.09216515223597621 Lambda1 1.0027308\n",
      "68 Train Loss 26.692978 Test MSE 29.88879382438896 Test RE 0.09203295471374981 Lambda1 1.0086509\n",
      "69 Train Loss 26.666883 Test MSE 29.88815575113495 Test RE 0.09203197233753464 Lambda1 1.0070802\n",
      "70 Train Loss 26.632942 Test MSE 29.870231207215543 Test RE 0.09200437146199812 Lambda1 1.00473\n",
      "71 Train Loss 26.588541 Test MSE 29.837852172866434 Test RE 0.09195449202808831 Lambda1 1.0059831\n",
      "72 Train Loss 26.548882 Test MSE 29.744488921401167 Test RE 0.09181051556768723 Lambda1 1.0111703\n",
      "73 Train Loss 26.508305 Test MSE 29.737132496663918 Test RE 0.09179916154989391 Lambda1 1.0090562\n",
      "74 Train Loss 26.478695 Test MSE 29.732211405454848 Test RE 0.09179156547891759 Lambda1 1.011048\n",
      "75 Train Loss 26.42182 Test MSE 29.673516147510085 Test RE 0.09170091646550747 Lambda1 1.0088267\n",
      "76 Train Loss 26.375383 Test MSE 29.673955042568686 Test RE 0.09170159462798214 Lambda1 1.0039183\n",
      "77 Train Loss 26.327765 Test MSE 29.59459716510612 Test RE 0.09157889248312939 Lambda1 1.0030901\n",
      "78 Train Loss 26.237215 Test MSE 29.484339034944707 Test RE 0.09140813936162019 Lambda1 1.0067875\n",
      "79 Train Loss 26.155277 Test MSE 29.408016167335656 Test RE 0.09128975359742568 Lambda1 1.0070437\n",
      "80 Train Loss 26.11874 Test MSE 29.365728481332077 Test RE 0.09122409426829102 Lambda1 1.0127624\n",
      "81 Train Loss 26.082462 Test MSE 29.33653096470848 Test RE 0.09117873221631792 Lambda1 1.0113796\n",
      "82 Train Loss 26.052105 Test MSE 29.292921499234893 Test RE 0.0911109373127756 Lambda1 1.0123674\n",
      "83 Train Loss 25.99268 Test MSE 29.237007205788455 Test RE 0.09102393955367372 Lambda1 1.009436\n",
      "84 Train Loss 25.955318 Test MSE 29.23655764301623 Test RE 0.09102323973628779 Lambda1 1.0068575\n",
      "85 Train Loss 25.924175 Test MSE 29.17441912811829 Test RE 0.09092645923454012 Lambda1 1.0079798\n",
      "86 Train Loss 25.906214 Test MSE 29.136546435486537 Test RE 0.09086742210104637 Lambda1 1.0073329\n",
      "87 Train Loss 25.878187 Test MSE 29.02672282178395 Test RE 0.09069600833704637 Lambda1 1.0134798\n",
      "88 Train Loss 25.84662 Test MSE 28.88906644436476 Test RE 0.09048069430870469 Lambda1 1.0191226\n",
      "89 Train Loss 25.802301 Test MSE 28.85387651614821 Test RE 0.09042557000502516 Lambda1 1.0167501\n",
      "90 Train Loss 25.769863 Test MSE 28.873749740740234 Test RE 0.09045670513406255 Lambda1 1.0122395\n",
      "91 Train Loss 25.732164 Test MSE 28.900605788700574 Test RE 0.09049876314537905 Lambda1 1.0133535\n",
      "92 Train Loss 25.703644 Test MSE 28.84722619142828 Test RE 0.0904151486313505 Lambda1 1.0172007\n",
      "93 Train Loss 25.667559 Test MSE 28.85414538344416 Test RE 0.09042599130754651 Lambda1 1.0171099\n",
      "94 Train Loss 25.632505 Test MSE 28.866426191316826 Test RE 0.090445232669126 Lambda1 1.0124677\n",
      "95 Train Loss 25.566517 Test MSE 28.763680976418954 Test RE 0.09028412684658 Lambda1 1.0114537\n",
      "96 Train Loss 25.54589 Test MSE 28.767290215647673 Test RE 0.0902897910519228 Lambda1 1.010375\n",
      "97 Train Loss 25.527447 Test MSE 28.679268438963838 Test RE 0.0901515514667336 Lambda1 1.0128733\n",
      "98 Train Loss 25.512186 Test MSE 28.6573459085864 Test RE 0.09011708880328702 Lambda1 1.0116702\n",
      "99 Train Loss 25.486227 Test MSE 28.607772025497326 Test RE 0.09003910901136734 Lambda1 1.0083125\n",
      "100 Train Loss 25.45292 Test MSE 28.529502056399 Test RE 0.08991585255474686 Lambda1 1.0047374\n",
      "101 Train Loss 25.428146 Test MSE 28.497196502365398 Test RE 0.08986492975551526 Lambda1 1.0009542\n",
      "102 Train Loss 25.371183 Test MSE 28.434816763544106 Test RE 0.0897665196904145 Lambda1 1.0003413\n",
      "103 Train Loss 25.297316 Test MSE 28.47105979281736 Test RE 0.08982370969428335 Lambda1 0.99327046\n",
      "104 Train Loss 25.235851 Test MSE 28.35822363376154 Test RE 0.08964553886918962 Lambda1 0.98779774\n",
      "105 Train Loss 25.202862 Test MSE 28.301324735157202 Test RE 0.0895555597923462 Lambda1 0.9861989\n",
      "106 Train Loss 25.177475 Test MSE 28.258742871678695 Test RE 0.08948816227380871 Lambda1 0.98543966\n",
      "107 Train Loss 25.152395 Test MSE 28.22686200851818 Test RE 0.0894376687861114 Lambda1 0.98259294\n",
      "108 Train Loss 25.120161 Test MSE 28.137768736967327 Test RE 0.08929640986592864 Lambda1 0.9794069\n",
      "109 Train Loss 25.091072 Test MSE 28.12424875041825 Test RE 0.08927495416115329 Lambda1 0.9781084\n",
      "110 Train Loss 25.058956 Test MSE 28.09103841307232 Test RE 0.08922222871514955 Lambda1 0.97528476\n",
      "111 Train Loss 25.02573 Test MSE 27.998934620498122 Test RE 0.08907583945456549 Lambda1 0.98032546\n",
      "112 Train Loss 24.983799 Test MSE 27.981963912120293 Test RE 0.08904884004816593 Lambda1 0.9766312\n",
      "113 Train Loss 24.940521 Test MSE 27.9599030261594 Test RE 0.08901373022412543 Lambda1 0.97159344\n",
      "114 Train Loss 24.888742 Test MSE 27.930288514813558 Test RE 0.0889665770470325 Lambda1 0.9674572\n",
      "115 Train Loss 24.869003 Test MSE 27.898445381834765 Test RE 0.08891584748749944 Lambda1 0.96737355\n",
      "116 Train Loss 24.829372 Test MSE 27.82500710023871 Test RE 0.08879874186013849 Lambda1 0.96451944\n",
      "117 Train Loss 24.792328 Test MSE 27.773996413135748 Test RE 0.08871730858761102 Lambda1 0.9607374\n",
      "118 Train Loss 24.74738 Test MSE 27.72333516327556 Test RE 0.08863635910689462 Lambda1 0.9542194\n",
      "119 Train Loss 24.732382 Test MSE 27.720475505179934 Test RE 0.08863178757505921 Lambda1 0.9523443\n",
      "120 Train Loss 24.711678 Test MSE 27.70676451829988 Test RE 0.08860986551998007 Lambda1 0.9528563\n",
      "121 Train Loss 24.678038 Test MSE 27.684485213410046 Test RE 0.08857423228625243 Lambda1 0.9500825\n",
      "122 Train Loss 24.65671 Test MSE 27.710987090931134 Test RE 0.08861661743318301 Lambda1 0.94664085\n",
      "123 Train Loss 24.644606 Test MSE 27.709734102735542 Test RE 0.08861461395293173 Lambda1 0.9496263\n",
      "124 Train Loss 24.62037 Test MSE 27.647776748204215 Test RE 0.08851548995048067 Lambda1 0.94877857\n",
      "125 Train Loss 24.583023 Test MSE 27.5750989059966 Test RE 0.08839907284892003 Lambda1 0.9454638\n",
      "126 Train Loss 24.545162 Test MSE 27.593994822350528 Test RE 0.08842935552227647 Lambda1 0.9446409\n",
      "127 Train Loss 24.529573 Test MSE 27.587051970719894 Test RE 0.08841823008497215 Lambda1 0.94414383\n",
      "128 Train Loss 24.515171 Test MSE 27.55920375030178 Test RE 0.08837359117108792 Lambda1 0.94339466\n",
      "129 Train Loss 24.495903 Test MSE 27.534249543759238 Test RE 0.0883335720126385 Lambda1 0.9462614\n",
      "130 Train Loss 24.474293 Test MSE 27.477916693340994 Test RE 0.08824316407064027 Lambda1 0.9476361\n",
      "131 Train Loss 24.424929 Test MSE 27.437310954357237 Test RE 0.08817793889954943 Lambda1 0.9426582\n",
      "132 Train Loss 24.406803 Test MSE 27.4220960932635 Test RE 0.08815348677416147 Lambda1 0.94028205\n",
      "133 Train Loss 24.402094 Test MSE 27.415185359788943 Test RE 0.08814237814780639 Lambda1 0.93914086\n",
      "134 Train Loss 24.396206 Test MSE 27.41635490166056 Test RE 0.08814425822085999 Lambda1 0.9380312\n",
      "135 Train Loss 24.37467 Test MSE 27.361605052151297 Test RE 0.08805620317862693 Lambda1 0.9429226\n",
      "136 Train Loss 24.348589 Test MSE 27.33921739267297 Test RE 0.08802017137960927 Lambda1 0.948384\n",
      "137 Train Loss 24.33661 Test MSE 27.326810213153955 Test RE 0.08800019630162577 Lambda1 0.95334363\n",
      "138 Train Loss 24.311186 Test MSE 27.316452047840123 Test RE 0.08798351658365547 Lambda1 0.9586228\n",
      "139 Train Loss 24.300003 Test MSE 27.32214665337731 Test RE 0.08799268697866706 Lambda1 0.9573676\n",
      "140 Train Loss 24.283403 Test MSE 27.302326813382965 Test RE 0.08796076566966954 Lambda1 0.9643996\n",
      "141 Train Loss 24.265488 Test MSE 27.28242323813297 Test RE 0.0879286978367524 Lambda1 0.9671654\n",
      "142 Train Loss 24.252304 Test MSE 27.264444479582167 Test RE 0.08789972113369719 Lambda1 0.9662187\n",
      "143 Train Loss 24.228422 Test MSE 27.233240678191645 Test RE 0.08784940669086547 Lambda1 0.9661988\n",
      "144 Train Loss 24.206503 Test MSE 27.192814132980583 Test RE 0.0877841781889678 Lambda1 0.96772826\n",
      "145 Train Loss 24.173428 Test MSE 27.194161125303445 Test RE 0.08778635235066434 Lambda1 0.96940744\n",
      "146 Train Loss 24.147364 Test MSE 27.171914953856646 Test RE 0.08775043821097334 Lambda1 0.9710703\n",
      "147 Train Loss 24.11451 Test MSE 27.11520555950164 Test RE 0.08765882020444943 Lambda1 0.9718346\n",
      "148 Train Loss 24.082031 Test MSE 27.067614638685964 Test RE 0.08758185976844515 Lambda1 0.9737757\n",
      "149 Train Loss 24.070917 Test MSE 27.06582270150265 Test RE 0.08757896066206378 Lambda1 0.9754413\n",
      "150 Train Loss 24.054081 Test MSE 27.052201240165154 Test RE 0.0875569198844291 Lambda1 0.97475266\n",
      "151 Train Loss 24.040663 Test MSE 27.06056947966998 Test RE 0.08757046112300726 Lambda1 0.9748366\n",
      "152 Train Loss 24.02347 Test MSE 27.049303008204866 Test RE 0.08755222956258979 Lambda1 0.9738595\n",
      "153 Train Loss 24.010979 Test MSE 27.03836701398775 Test RE 0.08753452915323964 Lambda1 0.9738243\n",
      "154 Train Loss 24.00208 Test MSE 27.036505754457874 Test RE 0.08753151626269728 Lambda1 0.9747846\n",
      "155 Train Loss 23.99356 Test MSE 27.044214007997645 Test RE 0.08754399322702516 Lambda1 0.9735761\n",
      "156 Train Loss 23.970879 Test MSE 26.994885349864756 Test RE 0.08746411664577793 Lambda1 0.97358984\n",
      "157 Train Loss 23.957344 Test MSE 27.001832741897157 Test RE 0.087475370785373 Lambda1 0.9714494\n",
      "158 Train Loss 23.946926 Test MSE 26.97728361484335 Test RE 0.08743559696095578 Lambda1 0.97046816\n",
      "159 Train Loss 23.937881 Test MSE 26.961245342309045 Test RE 0.0874096024160996 Lambda1 0.9697027\n",
      "160 Train Loss 23.9301 Test MSE 26.96011067242208 Test RE 0.08740776307066193 Lambda1 0.9710647\n",
      "161 Train Loss 23.919796 Test MSE 26.95249562523957 Test RE 0.08739541777178082 Lambda1 0.97445655\n",
      "162 Train Loss 23.914694 Test MSE 26.934478923592263 Test RE 0.08736620265913943 Lambda1 0.9752897\n",
      "163 Train Loss 23.899479 Test MSE 26.900567464492074 Test RE 0.08731118677141915 Lambda1 0.9774955\n",
      "164 Train Loss 23.890127 Test MSE 26.91260749180533 Test RE 0.08733072374726851 Lambda1 0.97813326\n",
      "165 Train Loss 23.878458 Test MSE 26.880318105388987 Test RE 0.08727831890689863 Lambda1 0.97684246\n",
      "166 Train Loss 23.86289 Test MSE 26.8580278211148 Test RE 0.08724212398515348 Lambda1 0.9758237\n",
      "167 Train Loss 23.857315 Test MSE 26.85697687444428 Test RE 0.08724041708922503 Lambda1 0.97575533\n",
      "168 Train Loss 23.843117 Test MSE 26.84494896755719 Test RE 0.08722087957520577 Lambda1 0.9742084\n",
      "169 Train Loss 23.836973 Test MSE 26.847639779159067 Test RE 0.08722525077133071 Lambda1 0.97349054\n",
      "170 Train Loss 23.825394 Test MSE 26.832254273941974 Test RE 0.08720025421890197 Lambda1 0.97103435\n",
      "171 Train Loss 23.810001 Test MSE 26.795205445434952 Test RE 0.08714003223394166 Lambda1 0.970434\n",
      "172 Train Loss 23.802372 Test MSE 26.79181666053877 Test RE 0.08713452176747263 Lambda1 0.9678893\n",
      "173 Train Loss 23.794315 Test MSE 26.768580965488365 Test RE 0.08709672906444486 Lambda1 0.96366125\n",
      "174 Train Loss 23.786436 Test MSE 26.74778242710596 Test RE 0.08706288646607473 Lambda1 0.95960945\n",
      "175 Train Loss 23.77828 Test MSE 26.754484825000365 Test RE 0.0870737937919099 Lambda1 0.95658916\n",
      "176 Train Loss 23.768278 Test MSE 26.742577158636692 Test RE 0.08705441459089656 Lambda1 0.953588\n",
      "177 Train Loss 23.754063 Test MSE 26.714196852368193 Test RE 0.08700820949321819 Lambda1 0.9545115\n",
      "178 Train Loss 23.746328 Test MSE 26.723359043020114 Test RE 0.08702312885364126 Lambda1 0.95714015\n",
      "179 Train Loss 23.737125 Test MSE 26.71804360473657 Test RE 0.08701447370901658 Lambda1 0.9591787\n",
      "180 Train Loss 23.729551 Test MSE 26.707760818780272 Test RE 0.08699772777268956 Lambda1 0.9625941\n",
      "181 Train Loss 23.723051 Test MSE 26.69247949909235 Test RE 0.08697283556402871 Lambda1 0.96207744\n",
      "182 Train Loss 23.71951 Test MSE 26.688830697240856 Test RE 0.08696689086546049 Lambda1 0.9609571\n",
      "183 Train Loss 23.713976 Test MSE 26.68741286568706 Test RE 0.08696458079723257 Lambda1 0.9573865\n",
      "184 Train Loss 23.712152 Test MSE 26.68207091090286 Test RE 0.08695587661690199 Lambda1 0.9546548\n",
      "185 Train Loss 23.704493 Test MSE 26.672352145775385 Test RE 0.08694003862752751 Lambda1 0.9518035\n",
      "186 Train Loss 23.69447 Test MSE 26.670007801849568 Test RE 0.08693621778283524 Lambda1 0.94952506\n",
      "187 Train Loss 23.690615 Test MSE 26.66750848667059 Test RE 0.08693214417886312 Lambda1 0.9489627\n",
      "188 Train Loss 23.688818 Test MSE 26.65694256518277 Test RE 0.08691492079989487 Lambda1 0.9508609\n",
      "189 Train Loss 23.686049 Test MSE 26.654000499242485 Test RE 0.0869101243667805 Lambda1 0.94994754\n",
      "190 Train Loss 23.682995 Test MSE 26.641399870112515 Test RE 0.08688957863848222 Lambda1 0.94824636\n",
      "191 Train Loss 23.67803 Test MSE 26.640840112171734 Test RE 0.08688866582257668 Lambda1 0.94729316\n",
      "192 Train Loss 23.671486 Test MSE 26.638243457090685 Test RE 0.08688443124527648 Lambda1 0.9475934\n",
      "193 Train Loss 23.66825 Test MSE 26.645818888980006 Test RE 0.08689678454298441 Lambda1 0.9475431\n",
      "194 Train Loss 23.664328 Test MSE 26.642396314916194 Test RE 0.08689120355046114 Lambda1 0.9474173\n",
      "195 Train Loss 23.656507 Test MSE 26.629449492977933 Test RE 0.0868700886772168 Lambda1 0.9524884\n",
      "196 Train Loss 23.642786 Test MSE 26.637715034684092 Test RE 0.08688356947847084 Lambda1 0.9538869\n",
      "197 Train Loss 23.63834 Test MSE 26.639984506220465 Test RE 0.08688727053894126 Lambda1 0.952548\n",
      "198 Train Loss 23.634634 Test MSE 26.63825098139835 Test RE 0.08688444351607706 Lambda1 0.95054156\n",
      "199 Train Loss 23.632248 Test MSE 26.63800810203445 Test RE 0.08688404742238286 Lambda1 0.9501598\n",
      "Training time: 355.02\n",
      "Training time: 355.02\n",
      "inv_HT_stan_late_Try\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 838.09326 Test MSE 839.3527472954772 Test RE 0.487709739814826 Lambda1 0.09879435\n",
      "1 Train Loss 602.52057 Test MSE 584.5391033655246 Test RE 0.40700144878110117 Lambda1 0.00010659918\n",
      "2 Train Loss 299.81015 Test MSE 294.4428103118512 Test RE 0.28886149043104575 Lambda1 -0.0009887102\n",
      "3 Train Loss 272.3477 Test MSE 280.1405868055337 Test RE 0.2817586044990606 Lambda1 0.00046045607\n",
      "4 Train Loss 270.33078 Test MSE 278.1764876925124 Test RE 0.28076914553059307 Lambda1 0.0019449524\n",
      "5 Train Loss 267.65067 Test MSE 275.1866162749992 Test RE 0.27925620042861315 Lambda1 0.0072879046\n",
      "6 Train Loss 262.3073 Test MSE 266.3406390684635 Test RE 0.2747311406980604 Lambda1 0.023307864\n",
      "7 Train Loss 255.6972 Test MSE 255.63533059221268 Test RE 0.2691532378962618 Lambda1 0.042967953\n",
      "8 Train Loss 242.38597 Test MSE 240.01005610108422 Test RE 0.2607977797872524 Lambda1 0.07214847\n",
      "9 Train Loss 226.50557 Test MSE 223.49264692307256 Test RE 0.251663823309103 Lambda1 0.10502851\n",
      "10 Train Loss 211.13528 Test MSE 205.70125217191827 Test RE 0.2414391193427637 Lambda1 0.1405411\n",
      "11 Train Loss 198.73618 Test MSE 194.52308177253022 Test RE 0.23478737554698897 Lambda1 0.15889063\n",
      "12 Train Loss 180.63077 Test MSE 173.8845534569053 Test RE 0.22198297773279155 Lambda1 0.20064834\n",
      "13 Train Loss 169.45363 Test MSE 159.17805296260042 Test RE 0.2123883860595443 Lambda1 0.2366195\n",
      "14 Train Loss 158.45874 Test MSE 145.88441211530397 Test RE 0.20332632702322204 Lambda1 0.26049164\n",
      "15 Train Loss 146.8587 Test MSE 131.4771443178112 Test RE 0.19302532926507096 Lambda1 0.2878006\n",
      "16 Train Loss 134.12392 Test MSE 112.52589804678816 Test RE 0.178572843622164 Lambda1 0.3269381\n",
      "17 Train Loss 119.339455 Test MSE 103.09388216238617 Test RE 0.17092501143208086 Lambda1 0.34536862\n",
      "18 Train Loss 107.41927 Test MSE 93.68894128592291 Test RE 0.16294211016224783 Lambda1 0.36182648\n",
      "19 Train Loss 96.689835 Test MSE 84.0253710289333 Test RE 0.1543101132719048 Lambda1 0.39073613\n",
      "20 Train Loss 87.157524 Test MSE 77.15740728723756 Test RE 0.1478692885921544 Lambda1 0.40639314\n",
      "21 Train Loss 81.23527 Test MSE 72.38548404497516 Test RE 0.14322370761931558 Lambda1 0.42424226\n",
      "22 Train Loss 75.663086 Test MSE 67.4932763669874 Test RE 0.13829912298532962 Lambda1 0.44576013\n",
      "23 Train Loss 68.36055 Test MSE 63.85334522979322 Test RE 0.1345181846881917 Lambda1 0.47016394\n",
      "24 Train Loss 63.891647 Test MSE 60.50777992111312 Test RE 0.13094676685659093 Lambda1 0.4852002\n",
      "25 Train Loss 61.67933 Test MSE 58.31499211962776 Test RE 0.1285521312841378 Lambda1 0.500075\n",
      "26 Train Loss 59.48681 Test MSE 56.869206555121856 Test RE 0.1269485530315151 Lambda1 0.5125002\n",
      "27 Train Loss 56.672043 Test MSE 54.71915282879212 Test RE 0.12452566025332117 Lambda1 0.5285659\n",
      "28 Train Loss 54.267536 Test MSE 52.83149589193554 Test RE 0.1223589170199378 Lambda1 0.5515441\n",
      "29 Train Loss 53.279297 Test MSE 53.20074032728385 Test RE 0.12278576165754164 Lambda1 0.5530581\n",
      "30 Train Loss 51.3549 Test MSE 51.77275325720893 Test RE 0.12112667653529913 Lambda1 0.56289595\n",
      "31 Train Loss 49.53475 Test MSE 49.82647021535878 Test RE 0.11882812152757857 Lambda1 0.5904898\n",
      "32 Train Loss 47.733826 Test MSE 47.63057902170191 Test RE 0.11618019502255951 Lambda1 0.6212645\n",
      "33 Train Loss 46.29167 Test MSE 45.84563122641775 Test RE 0.11398249232972214 Lambda1 0.6454889\n",
      "34 Train Loss 44.5235 Test MSE 44.40782639277375 Test RE 0.11218090231377391 Lambda1 0.6731839\n",
      "35 Train Loss 42.649773 Test MSE 44.04905186013331 Test RE 0.1117268238871476 Lambda1 0.6771835\n",
      "36 Train Loss 41.25127 Test MSE 42.86170176100357 Test RE 0.11021072914823501 Lambda1 0.6863053\n",
      "37 Train Loss 40.122536 Test MSE 41.42344895205343 Test RE 0.10834585435404266 Lambda1 0.70295745\n",
      "38 Train Loss 38.874184 Test MSE 40.392005316091854 Test RE 0.10698844565221693 Lambda1 0.71931976\n",
      "39 Train Loss 38.02252 Test MSE 39.81030999276915 Test RE 0.10621526825803373 Lambda1 0.72348446\n",
      "40 Train Loss 36.675053 Test MSE 38.75819698591205 Test RE 0.10480233369934357 Lambda1 0.7494731\n",
      "41 Train Loss 36.16881 Test MSE 38.38357115685558 Test RE 0.1042946089011117 Lambda1 0.7578577\n",
      "42 Train Loss 35.417274 Test MSE 38.116374683115964 Test RE 0.10393096610072636 Lambda1 0.76024455\n",
      "43 Train Loss 35.048443 Test MSE 37.8383517280028 Test RE 0.10355123326415235 Lambda1 0.76892614\n",
      "44 Train Loss 34.72128 Test MSE 37.86152046267485 Test RE 0.10358293104955148 Lambda1 0.7673289\n",
      "45 Train Loss 34.393314 Test MSE 37.55935401201667 Test RE 0.10316876403674924 Lambda1 0.77825695\n",
      "46 Train Loss 33.968746 Test MSE 37.183047154484605 Test RE 0.10265063950115888 Lambda1 0.7939127\n",
      "47 Train Loss 33.48886 Test MSE 36.82435961235279 Test RE 0.10215432821894296 Lambda1 0.80388165\n",
      "48 Train Loss 32.90234 Test MSE 36.158277813791166 Test RE 0.10122622456156476 Lambda1 0.80373454\n",
      "49 Train Loss 32.468582 Test MSE 35.73257238939503 Test RE 0.10062857246057988 Lambda1 0.8100749\n",
      "50 Train Loss 32.072544 Test MSE 35.53975197101667 Test RE 0.10035669880307493 Lambda1 0.82005674\n",
      "51 Train Loss 31.768747 Test MSE 34.937177774352385 Test RE 0.09950229104386975 Lambda1 0.83571154\n",
      "52 Train Loss 31.504625 Test MSE 34.52269798985405 Test RE 0.09891030365066776 Lambda1 0.85242045\n",
      "53 Train Loss 31.27259 Test MSE 34.35083338619112 Test RE 0.09866379351763603 Lambda1 0.8611293\n",
      "54 Train Loss 31.05133 Test MSE 34.0938494603461 Test RE 0.09829404108515573 Lambda1 0.88205796\n",
      "55 Train Loss 30.780384 Test MSE 33.8625598906673 Test RE 0.0979600647937188 Lambda1 0.9015596\n",
      "56 Train Loss 30.59135 Test MSE 33.62917162880976 Test RE 0.09762189987915357 Lambda1 0.8952632\n",
      "57 Train Loss 30.44761 Test MSE 33.5564157184073 Test RE 0.09751624137526824 Lambda1 0.8989313\n",
      "58 Train Loss 30.295471 Test MSE 33.335254375450916 Test RE 0.09719435843106845 Lambda1 0.9074935\n",
      "59 Train Loss 30.239304 Test MSE 33.146973924176734 Test RE 0.09691948861294035 Lambda1 0.9129528\n",
      "60 Train Loss 30.020033 Test MSE 32.8990071582715 Test RE 0.09655628913305683 Lambda1 0.9277664\n",
      "61 Train Loss 29.891115 Test MSE 32.83974889415475 Test RE 0.09646929050553506 Lambda1 0.925783\n",
      "62 Train Loss 29.773344 Test MSE 32.728088250446234 Test RE 0.09630514498768868 Lambda1 0.933393\n",
      "63 Train Loss 29.672935 Test MSE 32.664505439388414 Test RE 0.09621155063024388 Lambda1 0.93662494\n",
      "64 Train Loss 29.505816 Test MSE 32.48509664053923 Test RE 0.09594696732387135 Lambda1 0.9365947\n",
      "65 Train Loss 29.316954 Test MSE 32.4716401117057 Test RE 0.09592709287373537 Lambda1 0.9404412\n",
      "66 Train Loss 29.214144 Test MSE 32.20951148392857 Test RE 0.0955391206387814 Lambda1 0.9527703\n",
      "67 Train Loss 29.158316 Test MSE 32.15060082012224 Test RE 0.09545171091696952 Lambda1 0.9534446\n",
      "68 Train Loss 28.936539 Test MSE 32.01598871872343 Test RE 0.09525167681670266 Lambda1 0.9552181\n",
      "69 Train Loss 28.723976 Test MSE 31.735669589977544 Test RE 0.09483376734530702 Lambda1 0.9604016\n",
      "70 Train Loss 28.665253 Test MSE 31.609178049009422 Test RE 0.0946445851657369 Lambda1 0.9650847\n",
      "71 Train Loss 28.578083 Test MSE 31.385547678879085 Test RE 0.09430919251275319 Lambda1 0.9716471\n",
      "72 Train Loss 28.3865 Test MSE 31.24080920224745 Test RE 0.09409148175675862 Lambda1 0.9824212\n",
      "73 Train Loss 28.277176 Test MSE 31.21474021580158 Test RE 0.09405221610487692 Lambda1 0.98306686\n",
      "74 Train Loss 28.235172 Test MSE 31.229521145746475 Test RE 0.09407448146213969 Lambda1 0.98193514\n",
      "75 Train Loss 28.1742 Test MSE 31.218835839482477 Test RE 0.09405838610415088 Lambda1 0.98466814\n",
      "76 Train Loss 28.130457 Test MSE 31.17056958402337 Test RE 0.09398564793105303 Lambda1 0.9855831\n",
      "77 Train Loss 28.07098 Test MSE 31.133562775819506 Test RE 0.09392983981076185 Lambda1 0.9836184\n",
      "78 Train Loss 28.028334 Test MSE 31.112239777669906 Test RE 0.0938976685994715 Lambda1 0.9828595\n",
      "79 Train Loss 28.0009 Test MSE 31.12761542849445 Test RE 0.0939208678202028 Lambda1 0.98156077\n",
      "80 Train Loss 27.966242 Test MSE 31.110877212570266 Test RE 0.09389561244588174 Lambda1 0.98225635\n",
      "81 Train Loss 27.915878 Test MSE 31.06210079849885 Test RE 0.09382197762410288 Lambda1 0.9854365\n",
      "82 Train Loss 27.838377 Test MSE 31.00380023315658 Test RE 0.09373388887262822 Lambda1 0.9873218\n",
      "83 Train Loss 27.751707 Test MSE 30.920521220961596 Test RE 0.09360791536909926 Lambda1 0.9900078\n",
      "84 Train Loss 27.643469 Test MSE 30.88313175767772 Test RE 0.09355130234541861 Lambda1 0.9919247\n",
      "85 Train Loss 27.588478 Test MSE 30.902741446804814 Test RE 0.09358099850529636 Lambda1 0.9932744\n",
      "86 Train Loss 27.552298 Test MSE 30.821210579044088 Test RE 0.09345746935056724 Lambda1 0.9920975\n",
      "87 Train Loss 27.525272 Test MSE 30.80222998994145 Test RE 0.09342868801822546 Lambda1 0.9912728\n",
      "88 Train Loss 27.512571 Test MSE 30.79854613263394 Test RE 0.09342310095120979 Lambda1 0.9916474\n",
      "89 Train Loss 27.496634 Test MSE 30.772210433477873 Test RE 0.09338314957076867 Lambda1 0.99019367\n",
      "90 Train Loss 27.490454 Test MSE 30.74569445069808 Test RE 0.09334290742377835 Lambda1 0.99065804\n",
      "91 Train Loss 27.459099 Test MSE 30.692208773026323 Test RE 0.09326168171233677 Lambda1 0.99199647\n",
      "92 Train Loss 27.399326 Test MSE 30.693360224509483 Test RE 0.09326343110247294 Lambda1 0.99268246\n",
      "93 Train Loss 27.343855 Test MSE 30.63791329274286 Test RE 0.09317915377704294 Lambda1 0.99573755\n",
      "94 Train Loss 27.328701 Test MSE 30.623125655293336 Test RE 0.09315666422570706 Lambda1 0.9965506\n",
      "95 Train Loss 27.312384 Test MSE 30.59498300569663 Test RE 0.09311384890379643 Lambda1 0.99407667\n",
      "96 Train Loss 27.298487 Test MSE 30.577826719996928 Test RE 0.0930877382207692 Lambda1 0.99127465\n",
      "97 Train Loss 27.290976 Test MSE 30.563270961526445 Test RE 0.09306557961654012 Lambda1 0.9906255\n",
      "98 Train Loss 27.274843 Test MSE 30.518924809103307 Test RE 0.09299803778605323 Lambda1 0.9919775\n",
      "99 Train Loss 27.236208 Test MSE 30.447212138884357 Test RE 0.09288871119586575 Lambda1 0.9922323\n",
      "100 Train Loss 27.185438 Test MSE 30.366923552633683 Test RE 0.09276615767045891 Lambda1 0.99286026\n",
      "101 Train Loss 27.168713 Test MSE 30.346637258130986 Test RE 0.09273516678017651 Lambda1 0.9902186\n",
      "102 Train Loss 27.155563 Test MSE 30.34952436728849 Test RE 0.09273957798032532 Lambda1 0.987851\n",
      "103 Train Loss 27.135458 Test MSE 30.33965194821523 Test RE 0.09272449309076249 Lambda1 0.9881561\n",
      "104 Train Loss 27.09647 Test MSE 30.292489989192248 Test RE 0.09265239652353886 Lambda1 0.98538035\n",
      "105 Train Loss 27.058685 Test MSE 30.166133863340132 Test RE 0.09245895861726613 Lambda1 0.9847185\n",
      "106 Train Loss 27.031103 Test MSE 30.112121942096646 Test RE 0.09237614848019378 Lambda1 0.98243374\n",
      "107 Train Loss 26.996984 Test MSE 30.039017748306062 Test RE 0.09226394802605196 Lambda1 0.9804799\n",
      "108 Train Loss 26.98611 Test MSE 30.005235549861148 Test RE 0.09221205292349627 Lambda1 0.9790722\n",
      "109 Train Loss 26.974556 Test MSE 29.995776628627954 Test RE 0.09219751720529752 Lambda1 0.9797133\n",
      "110 Train Loss 26.964336 Test MSE 30.003551849351528 Test RE 0.0922094657140355 Lambda1 0.9837706\n",
      "111 Train Loss 26.950819 Test MSE 30.0268029828215 Test RE 0.09224518747457888 Lambda1 0.98284096\n",
      "112 Train Loss 26.909454 Test MSE 29.983280586786847 Test RE 0.09217831076731069 Lambda1 0.98153085\n",
      "113 Train Loss 26.890326 Test MSE 30.01361546135589 Test RE 0.09222492859141515 Lambda1 0.9835193\n",
      "114 Train Loss 26.85471 Test MSE 30.001407556408537 Test RE 0.09220617064349752 Lambda1 0.98050445\n",
      "115 Train Loss 26.819334 Test MSE 29.961840387292956 Test RE 0.09214534781658464 Lambda1 0.977328\n",
      "116 Train Loss 26.807854 Test MSE 29.94765500146437 Test RE 0.09212353219972032 Lambda1 0.9780282\n",
      "117 Train Loss 26.797153 Test MSE 29.95728978086461 Test RE 0.0921383500299102 Lambda1 0.98153645\n",
      "118 Train Loss 26.781784 Test MSE 29.93496534271955 Test RE 0.09210401247357852 Lambda1 0.98564935\n",
      "119 Train Loss 26.761343 Test MSE 29.9423529317996 Test RE 0.09211537685322445 Lambda1 0.9898101\n",
      "120 Train Loss 26.742676 Test MSE 29.916752697692196 Test RE 0.09207598984372913 Lambda1 0.9848724\n",
      "121 Train Loss 26.731394 Test MSE 29.88487939806053 Test RE 0.09202692790619002 Lambda1 0.98253185\n",
      "122 Train Loss 26.719154 Test MSE 29.865682749328112 Test RE 0.09199736626109782 Lambda1 0.9807522\n",
      "123 Train Loss 26.698936 Test MSE 29.821149947372323 Test RE 0.09192875191045409 Lambda1 0.97139126\n",
      "124 Train Loss 26.682676 Test MSE 29.798184964295 Test RE 0.09189334836574056 Lambda1 0.9688692\n",
      "125 Train Loss 26.662546 Test MSE 29.79305188467194 Test RE 0.09188543318248252 Lambda1 0.96396613\n",
      "126 Train Loss 26.62697 Test MSE 29.793941561877862 Test RE 0.09188680510915682 Lambda1 0.96040785\n",
      "127 Train Loss 26.60636 Test MSE 29.77471957695054 Test RE 0.09185715928752393 Lambda1 0.95927626\n",
      "128 Train Loss 26.555332 Test MSE 29.740483809680295 Test RE 0.09180433419150122 Lambda1 0.95830625\n",
      "129 Train Loss 26.524168 Test MSE 29.68269247191034 Test RE 0.09171509429849391 Lambda1 0.95531756\n",
      "130 Train Loss 26.503222 Test MSE 29.611591995820344 Test RE 0.09160518350497358 Lambda1 0.9554768\n",
      "131 Train Loss 26.492048 Test MSE 29.61876839350972 Test RE 0.09161628313450494 Lambda1 0.9583772\n",
      "132 Train Loss 26.478529 Test MSE 29.598668004122423 Test RE 0.09158519076296924 Lambda1 0.957023\n",
      "133 Train Loss 26.449984 Test MSE 29.581900844356372 Test RE 0.09155924633426193 Lambda1 0.9528559\n",
      "134 Train Loss 26.432766 Test MSE 29.587499526113067 Test RE 0.09156791019316775 Lambda1 0.9505149\n",
      "135 Train Loss 26.418243 Test MSE 29.563054016297237 Test RE 0.09153007518130564 Lambda1 0.9466903\n",
      "136 Train Loss 26.396622 Test MSE 29.52399780482125 Test RE 0.09146959429461825 Lambda1 0.93898195\n",
      "137 Train Loss 26.373632 Test MSE 29.486199056561688 Test RE 0.09141102256062314 Lambda1 0.9348949\n",
      "138 Train Loss 26.350786 Test MSE 29.458023136946338 Test RE 0.09136733762860896 Lambda1 0.926651\n",
      "139 Train Loss 26.330624 Test MSE 29.444119572421705 Test RE 0.09134577335737347 Lambda1 0.92167014\n",
      "140 Train Loss 26.307878 Test MSE 29.406722456771764 Test RE 0.09128774557660496 Lambda1 0.92051095\n",
      "141 Train Loss 26.284582 Test MSE 29.413952186594404 Test RE 0.09129896656829713 Lambda1 0.9141814\n",
      "142 Train Loss 26.262917 Test MSE 29.41701507153607 Test RE 0.09130371994092774 Lambda1 0.91065073\n",
      "143 Train Loss 26.247395 Test MSE 29.405103546866517 Test RE 0.0912852327384297 Lambda1 0.91049707\n",
      "144 Train Loss 26.231398 Test MSE 29.36459081430934 Test RE 0.0912223271803575 Lambda1 0.91020983\n",
      "145 Train Loss 26.21235 Test MSE 29.33912408266143 Test RE 0.09118276186766212 Lambda1 0.9143246\n",
      "146 Train Loss 26.191742 Test MSE 29.32184325842721 Test RE 0.09115590446439067 Lambda1 0.91774327\n",
      "147 Train Loss 26.159811 Test MSE 29.290544133293746 Test RE 0.09110724002989744 Lambda1 0.91829365\n",
      "148 Train Loss 26.125675 Test MSE 29.279558511098276 Test RE 0.09109015322575557 Lambda1 0.91965556\n",
      "149 Train Loss 26.114176 Test MSE 29.26893653940418 Test RE 0.09107362898757819 Lambda1 0.9225742\n",
      "150 Train Loss 26.099129 Test MSE 29.24616729085407 Test RE 0.09103819754058698 Lambda1 0.9226224\n",
      "151 Train Loss 26.07044 Test MSE 29.239693671329803 Test RE 0.09102812136096879 Lambda1 0.92300934\n",
      "152 Train Loss 26.057076 Test MSE 29.244998579128634 Test RE 0.09103637852493901 Lambda1 0.9235262\n",
      "153 Train Loss 26.043304 Test MSE 29.253907824527822 Test RE 0.09105024420649584 Lambda1 0.92101175\n",
      "154 Train Loss 26.035048 Test MSE 29.248216316303633 Test RE 0.09104138661344224 Lambda1 0.9217182\n",
      "155 Train Loss 26.023495 Test MSE 29.25875989069445 Test RE 0.09105779471043217 Lambda1 0.92103183\n",
      "156 Train Loss 26.015368 Test MSE 29.282419077091593 Test RE 0.09109460279815182 Lambda1 0.9194382\n",
      "157 Train Loss 26.004662 Test MSE 29.254756454290323 Test RE 0.0910515648400881 Lambda1 0.92046756\n",
      "158 Train Loss 25.996796 Test MSE 29.25107676075095 Test RE 0.09104583837998995 Lambda1 0.9205904\n",
      "159 Train Loss 25.987593 Test MSE 29.25754249373584 Test RE 0.09105590032673527 Lambda1 0.92195827\n",
      "160 Train Loss 25.983967 Test MSE 29.264837576159234 Test RE 0.09106725156884007 Lambda1 0.9202043\n",
      "161 Train Loss 25.97637 Test MSE 29.267740799851612 Test RE 0.09107176862868725 Lambda1 0.920981\n",
      "162 Train Loss 25.967064 Test MSE 29.240523743471837 Test RE 0.09102941342941999 Lambda1 0.924329\n",
      "163 Train Loss 25.955847 Test MSE 29.211736802982898 Test RE 0.09098459371962024 Lambda1 0.9281982\n",
      "164 Train Loss 25.943935 Test MSE 29.18214839513594 Test RE 0.09093850314768677 Lambda1 0.93096495\n",
      "165 Train Loss 25.936356 Test MSE 29.185278443397017 Test RE 0.0909433800036947 Lambda1 0.93060094\n",
      "166 Train Loss 25.926218 Test MSE 29.166897332252866 Test RE 0.09091473707563767 Lambda1 0.9318929\n",
      "167 Train Loss 25.91942 Test MSE 29.16434323877921 Test RE 0.09091075636734292 Lambda1 0.9319011\n",
      "168 Train Loss 25.902853 Test MSE 29.168164573781322 Test RE 0.09091671208308648 Lambda1 0.92986906\n",
      "169 Train Loss 25.8796 Test MSE 29.113484692633136 Test RE 0.09083145394069457 Lambda1 0.9298842\n",
      "170 Train Loss 25.867561 Test MSE 29.098677346922113 Test RE 0.09080835220942553 Lambda1 0.93223625\n",
      "171 Train Loss 25.855143 Test MSE 29.07984947497035 Test RE 0.09077896935019318 Lambda1 0.9322734\n",
      "172 Train Loss 25.827656 Test MSE 29.062957102445154 Test RE 0.09075259894324247 Lambda1 0.93261915\n",
      "173 Train Loss 25.809704 Test MSE 29.048455050439284 Test RE 0.090729953912231 Lambda1 0.9289469\n",
      "174 Train Loss 25.79529 Test MSE 29.05111298230873 Test RE 0.09073410470991466 Lambda1 0.92544335\n",
      "175 Train Loss 25.78342 Test MSE 29.050609631640725 Test RE 0.09073331865965087 Lambda1 0.92221576\n",
      "176 Train Loss 25.772293 Test MSE 29.050737993035327 Test RE 0.09073351911400625 Lambda1 0.9192513\n",
      "177 Train Loss 25.764826 Test MSE 29.037845652378888 Test RE 0.0907133837004349 Lambda1 0.9177526\n",
      "178 Train Loss 25.759865 Test MSE 29.04538698512779 Test RE 0.09072516238761504 Lambda1 0.9143655\n",
      "179 Train Loss 25.749016 Test MSE 29.04470195201526 Test RE 0.09072409250848308 Lambda1 0.9121749\n",
      "180 Train Loss 25.738455 Test MSE 29.044146601236715 Test RE 0.0907232251569032 Lambda1 0.9115282\n",
      "181 Train Loss 25.730047 Test MSE 29.003427588013064 Test RE 0.09065960724553597 Lambda1 0.91384387\n",
      "182 Train Loss 25.72578 Test MSE 29.002489888720504 Test RE 0.09065814169225868 Lambda1 0.9149577\n",
      "183 Train Loss 25.718384 Test MSE 28.990920703440153 Test RE 0.09064005797841575 Lambda1 0.91373694\n",
      "184 Train Loss 25.715965 Test MSE 28.97285534107821 Test RE 0.09061181291719414 Lambda1 0.91303015\n",
      "185 Train Loss 25.70961 Test MSE 28.967595210628335 Test RE 0.09060358708673857 Lambda1 0.91217655\n",
      "186 Train Loss 25.703617 Test MSE 28.935090472214902 Test RE 0.09055273936354859 Lambda1 0.9128376\n",
      "187 Train Loss 25.699018 Test MSE 28.925677822878807 Test RE 0.09053800966171358 Lambda1 0.9159139\n",
      "188 Train Loss 25.68797 Test MSE 28.907749344553284 Test RE 0.09050994704633104 Lambda1 0.916638\n",
      "189 Train Loss 25.683538 Test MSE 28.89608629625794 Test RE 0.09049168674582907 Lambda1 0.91635686\n",
      "190 Train Loss 25.679358 Test MSE 28.87857981546306 Test RE 0.09046427073220036 Lambda1 0.9190124\n",
      "191 Train Loss 25.665888 Test MSE 28.836845730308937 Test RE 0.09039887955617865 Lambda1 0.9198161\n",
      "192 Train Loss 25.641773 Test MSE 28.781091338175 Test RE 0.09031144674214313 Lambda1 0.92644\n",
      "193 Train Loss 25.630058 Test MSE 28.731643633579537 Test RE 0.09023383305088399 Lambda1 0.9290538\n",
      "194 Train Loss 25.597937 Test MSE 28.712079633328567 Test RE 0.09020310673095917 Lambda1 0.93024445\n",
      "195 Train Loss 25.580624 Test MSE 28.72135769633166 Test RE 0.09021767973505446 Lambda1 0.93511873\n",
      "196 Train Loss 25.567055 Test MSE 28.691738886506773 Test RE 0.09017114938659927 Lambda1 0.9327687\n",
      "197 Train Loss 25.55686 Test MSE 28.700694507590157 Test RE 0.09018522095690117 Lambda1 0.930301\n",
      "198 Train Loss 25.537373 Test MSE 28.70390946537599 Test RE 0.09019027194197543 Lambda1 0.9307276\n",
      "199 Train Loss 25.518187 Test MSE 28.682432973194505 Test RE 0.09015652509083054 Lambda1 0.9291202\n",
      "Training time: 355.91\n",
      "Training time: 355.91\n",
      "inv_HT_stan_late_Try\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 832.49915 Test MSE 833.517667317015 Test RE 0.4860115333255965 Lambda1 0.111404404\n",
      "1 Train Loss 723.68994 Test MSE 703.175540535367 Test RE 0.4463967751117247 Lambda1 0.079168014\n",
      "2 Train Loss 498.23093 Test MSE 455.68664699170375 Test RE 0.3593538664105967 Lambda1 0.06080162\n",
      "3 Train Loss 305.86526 Test MSE 305.6917803666447 Test RE 0.29432764211954776 Lambda1 0.0009018411\n",
      "4 Train Loss 276.5144 Test MSE 282.4071200561102 Test RE 0.28289612038201006 Lambda1 6.2109666e-07\n",
      "5 Train Loss 271.6914 Test MSE 279.42644334929116 Test RE 0.2813992412481321 Lambda1 0.00086097594\n",
      "6 Train Loss 269.33603 Test MSE 276.40536670877754 Test RE 0.2798739041935885 Lambda1 0.005079192\n",
      "7 Train Loss 264.04678 Test MSE 269.45487508052594 Test RE 0.2763326445687368 Lambda1 0.02405962\n",
      "8 Train Loss 250.76541 Test MSE 251.83104757363392 Test RE 0.26714300486149956 Lambda1 0.06793114\n",
      "9 Train Loss 236.15103 Test MSE 234.90972580822847 Test RE 0.2580118600136226 Lambda1 0.09449539\n",
      "10 Train Loss 217.67758 Test MSE 217.09285191304133 Test RE 0.24803440862562193 Lambda1 0.13054816\n",
      "11 Train Loss 198.33064 Test MSE 193.75144940982676 Test RE 0.23432123662322024 Lambda1 0.18272741\n",
      "12 Train Loss 174.38287 Test MSE 164.98492594893312 Test RE 0.21622768752399177 Lambda1 0.24391349\n",
      "13 Train Loss 153.93303 Test MSE 146.45277118821582 Test RE 0.20372201708819032 Lambda1 0.27525923\n",
      "14 Train Loss 138.84148 Test MSE 130.8232114387907 Test RE 0.19254470217674471 Lambda1 0.31746343\n",
      "15 Train Loss 127.21693 Test MSE 118.91133354071589 Test RE 0.18356961429996077 Lambda1 0.35457006\n",
      "16 Train Loss 113.51939 Test MSE 106.07090582855967 Test RE 0.17337533349576517 Lambda1 0.3938633\n",
      "17 Train Loss 104.74968 Test MSE 99.5059957286931 Test RE 0.16792439627702566 Lambda1 0.40999863\n",
      "18 Train Loss 97.42974 Test MSE 93.8596990142435 Test RE 0.16309053193905215 Lambda1 0.4316666\n",
      "19 Train Loss 90.22821 Test MSE 85.27181361856192 Test RE 0.15545042745872478 Lambda1 0.44913667\n",
      "20 Train Loss 82.17333 Test MSE 79.89838861900138 Test RE 0.1504728618561126 Lambda1 0.46224272\n",
      "21 Train Loss 78.323 Test MSE 75.40695760029193 Test RE 0.1461823300907849 Lambda1 0.47000065\n",
      "22 Train Loss 71.09417 Test MSE 67.93788824632507 Test RE 0.13875389791306705 Lambda1 0.48630816\n",
      "23 Train Loss 66.01265 Test MSE 63.19378713500558 Test RE 0.13382164437428085 Lambda1 0.49540073\n",
      "24 Train Loss 60.960754 Test MSE 59.23365032695242 Test RE 0.12956073981927385 Lambda1 0.50849015\n",
      "25 Train Loss 56.706074 Test MSE 54.777378877171095 Test RE 0.12459189583024759 Lambda1 0.52138567\n",
      "26 Train Loss 53.859974 Test MSE 52.817370523258695 Test RE 0.12234255859276515 Lambda1 0.5238182\n",
      "27 Train Loss 50.570213 Test MSE 51.041360931185665 Test RE 0.12026805663435113 Lambda1 0.5324565\n",
      "28 Train Loss 47.836662 Test MSE 48.933677139640444 Test RE 0.11775872552084055 Lambda1 0.5419266\n",
      "29 Train Loss 44.98645 Test MSE 45.7157957398249 Test RE 0.11382097786752607 Lambda1 0.5746209\n",
      "30 Train Loss 43.13768 Test MSE 45.12516044163194 Test RE 0.1130833199342021 Lambda1 0.58164495\n",
      "31 Train Loss 42.076492 Test MSE 44.30282871177003 Test RE 0.11204820379873558 Lambda1 0.5921985\n",
      "32 Train Loss 40.7834 Test MSE 43.111258322229716 Test RE 0.11053110714598881 Lambda1 0.6063113\n",
      "33 Train Loss 39.64538 Test MSE 41.95468636640392 Test RE 0.10903838491242418 Lambda1 0.6242488\n",
      "34 Train Loss 38.252342 Test MSE 40.42944836936875 Test RE 0.10703802286549355 Lambda1 0.63736665\n",
      "35 Train Loss 36.760143 Test MSE 39.16681982363759 Test RE 0.10535334416625233 Lambda1 0.6550827\n",
      "36 Train Loss 35.85024 Test MSE 38.330204037933406 Test RE 0.10422207996970194 Lambda1 0.67008\n",
      "37 Train Loss 35.17489 Test MSE 37.742630273317886 Test RE 0.10342017111514983 Lambda1 0.68340963\n",
      "38 Train Loss 34.18531 Test MSE 36.81574524305121 Test RE 0.1021423789740829 Lambda1 0.7086366\n",
      "39 Train Loss 33.242027 Test MSE 35.99544575988438 Test RE 0.1009980406733853 Lambda1 0.73298675\n",
      "40 Train Loss 32.480892 Test MSE 35.52674792610295 Test RE 0.10033833679507888 Lambda1 0.753147\n",
      "41 Train Loss 31.6859 Test MSE 34.890550850106216 Test RE 0.09943587132892746 Lambda1 0.77163494\n",
      "42 Train Loss 31.290117 Test MSE 34.187105073706256 Test RE 0.09842837926665737 Lambda1 0.79036653\n",
      "43 Train Loss 30.644068 Test MSE 33.48613323829689 Test RE 0.09741406604459066 Lambda1 0.8093388\n",
      "44 Train Loss 30.214594 Test MSE 33.15313910525464 Test RE 0.09692850147814125 Lambda1 0.822899\n",
      "45 Train Loss 29.914509 Test MSE 33.04667724829949 Test RE 0.09677274720948059 Lambda1 0.8236401\n",
      "46 Train Loss 29.610746 Test MSE 32.67314263434401 Test RE 0.09622426998696529 Lambda1 0.8380285\n",
      "47 Train Loss 29.457666 Test MSE 32.41107107649753 Test RE 0.09583758517704453 Lambda1 0.84847975\n",
      "48 Train Loss 29.199198 Test MSE 32.07206674101766 Test RE 0.09533505997791776 Lambda1 0.85815835\n",
      "49 Train Loss 28.92148 Test MSE 31.98194368575187 Test RE 0.09520101917410667 Lambda1 0.8620073\n",
      "50 Train Loss 28.548063 Test MSE 31.672609539618005 Test RE 0.09473950124170108 Lambda1 0.87601656\n",
      "51 Train Loss 28.286669 Test MSE 31.493587721431364 Test RE 0.09447137568802398 Lambda1 0.884178\n",
      "52 Train Loss 28.11657 Test MSE 31.298737721790165 Test RE 0.09417867629530072 Lambda1 0.89256555\n",
      "53 Train Loss 27.998936 Test MSE 31.306445042606395 Test RE 0.0941902713410513 Lambda1 0.89278567\n",
      "54 Train Loss 27.866312 Test MSE 31.09223127628079 Test RE 0.09386747061625303 Lambda1 0.90689963\n",
      "55 Train Loss 27.804857 Test MSE 31.138452360260906 Test RE 0.09393721544996239 Lambda1 0.9098274\n",
      "56 Train Loss 27.723415 Test MSE 31.05713668908751 Test RE 0.09381448036603773 Lambda1 0.91262937\n",
      "57 Train Loss 27.629545 Test MSE 30.967196039146366 Test RE 0.09367853974232582 Lambda1 0.9140932\n",
      "58 Train Loss 27.57772 Test MSE 30.884102876515385 Test RE 0.09355277319232835 Lambda1 0.9182253\n",
      "59 Train Loss 27.525324 Test MSE 30.742065372514045 Test RE 0.09333739838077756 Lambda1 0.92812496\n",
      "60 Train Loss 27.42504 Test MSE 30.676913154698674 Test RE 0.09323844009964956 Lambda1 0.92822784\n",
      "61 Train Loss 27.359655 Test MSE 30.724937497800916 Test RE 0.09331139339408841 Lambda1 0.92464894\n",
      "62 Train Loss 27.301329 Test MSE 30.6461720747268 Test RE 0.09319171165682151 Lambda1 0.93051463\n",
      "63 Train Loss 27.239765 Test MSE 30.55267338108282 Test RE 0.09304944332867389 Lambda1 0.9357864\n",
      "64 Train Loss 27.213951 Test MSE 30.500770092987302 Test RE 0.09297037291667798 Lambda1 0.9402177\n",
      "65 Train Loss 27.181421 Test MSE 30.495040210274585 Test RE 0.09296163978712052 Lambda1 0.93747914\n",
      "66 Train Loss 27.09649 Test MSE 30.400264226974844 Test RE 0.09281706894851557 Lambda1 0.94602513\n",
      "67 Train Loss 27.05459 Test MSE 30.334663202936003 Test RE 0.09271686943880074 Lambda1 0.94955856\n",
      "68 Train Loss 26.984882 Test MSE 30.231903707250144 Test RE 0.09255969576135022 Lambda1 0.9542363\n",
      "69 Train Loss 26.922081 Test MSE 30.184855693753182 Test RE 0.09248764529715876 Lambda1 0.9541633\n",
      "70 Train Loss 26.86766 Test MSE 30.157829730528196 Test RE 0.09244623169106854 Lambda1 0.95745933\n",
      "71 Train Loss 26.79433 Test MSE 30.128680854869494 Test RE 0.09240154420544382 Lambda1 0.9582923\n",
      "72 Train Loss 26.7415 Test MSE 30.084947585155717 Test RE 0.09233445714685128 Lambda1 0.9569886\n",
      "73 Train Loss 26.70358 Test MSE 30.01018025637999 Test RE 0.09221965064348375 Lambda1 0.95833534\n",
      "74 Train Loss 26.606762 Test MSE 29.913116840480747 Test RE 0.0920703945619285 Lambda1 0.96042585\n",
      "75 Train Loss 26.564716 Test MSE 29.92717409182735 Test RE 0.09209202561873371 Lambda1 0.9564853\n",
      "76 Train Loss 26.528631 Test MSE 29.895838565332046 Test RE 0.09204380008490848 Lambda1 0.9575132\n",
      "77 Train Loss 26.502792 Test MSE 29.859040321446926 Test RE 0.09198713512294728 Lambda1 0.96107674\n",
      "78 Train Loss 26.472073 Test MSE 29.824483535476425 Test RE 0.09193388994219552 Lambda1 0.96351665\n",
      "79 Train Loss 26.455711 Test MSE 29.8249046138352 Test RE 0.09193453892635665 Lambda1 0.9627352\n",
      "80 Train Loss 26.41911 Test MSE 29.802443363115447 Test RE 0.09189991427817985 Lambda1 0.9657763\n",
      "81 Train Loss 26.409214 Test MSE 29.795751328315724 Test RE 0.09188959579615633 Lambda1 0.96570456\n",
      "82 Train Loss 26.38587 Test MSE 29.751908264983793 Test RE 0.09182196527336703 Lambda1 0.9661544\n",
      "83 Train Loss 26.350723 Test MSE 29.717158691209857 Test RE 0.09176832658974225 Lambda1 0.9638129\n",
      "84 Train Loss 26.327122 Test MSE 29.69706473427498 Test RE 0.09173729568532112 Lambda1 0.96613437\n",
      "85 Train Loss 26.310411 Test MSE 29.68408799918407 Test RE 0.09171725025876087 Lambda1 0.96658355\n",
      "86 Train Loss 26.293402 Test MSE 29.680384877611562 Test RE 0.09171152916803221 Lambda1 0.9679014\n",
      "87 Train Loss 26.277697 Test MSE 29.672714649976626 Test RE 0.09169967801175785 Lambda1 0.9653002\n",
      "88 Train Loss 26.264408 Test MSE 29.650300107209883 Test RE 0.09166503684924547 Lambda1 0.9643917\n",
      "89 Train Loss 26.252502 Test MSE 29.663813101938903 Test RE 0.091685922439913 Lambda1 0.9627945\n",
      "90 Train Loss 26.233809 Test MSE 29.66071404169579 Test RE 0.09168113297441632 Lambda1 0.96177214\n",
      "91 Train Loss 26.210281 Test MSE 29.638768411739672 Test RE 0.0916472097769297 Lambda1 0.95874584\n",
      "92 Train Loss 26.18528 Test MSE 29.619132332407794 Test RE 0.09161684599765754 Lambda1 0.95581025\n",
      "93 Train Loss 26.160238 Test MSE 29.606857866805395 Test RE 0.09159786056043577 Lambda1 0.9531668\n",
      "94 Train Loss 26.139364 Test MSE 29.601934456450458 Test RE 0.0915902442066977 Lambda1 0.95586944\n",
      "95 Train Loss 26.115759 Test MSE 29.56054686418636 Test RE 0.09152619390627484 Lambda1 0.9565844\n",
      "96 Train Loss 26.101982 Test MSE 29.54442801316398 Test RE 0.09150123668441935 Lambda1 0.95517474\n",
      "97 Train Loss 26.084877 Test MSE 29.533237387248384 Test RE 0.09148390595305196 Lambda1 0.95399183\n",
      "98 Train Loss 26.059423 Test MSE 29.513363549022532 Test RE 0.09145311958350356 Lambda1 0.95620203\n",
      "99 Train Loss 26.026007 Test MSE 29.489191395265433 Test RE 0.09141566076103808 Lambda1 0.9561697\n",
      "100 Train Loss 26.009813 Test MSE 29.465167039145918 Test RE 0.09137841576092412 Lambda1 0.9576009\n",
      "101 Train Loss 26.00103 Test MSE 29.438922487195995 Test RE 0.09133771142979287 Lambda1 0.95843977\n",
      "102 Train Loss 25.983646 Test MSE 29.403192316560855 Test RE 0.09128226607759098 Lambda1 0.9611822\n",
      "103 Train Loss 25.942234 Test MSE 29.377294653996778 Test RE 0.09124205755078153 Lambda1 0.9641597\n",
      "104 Train Loss 25.933132 Test MSE 29.391935742241888 Test RE 0.09126479137683248 Lambda1 0.96501803\n",
      "105 Train Loss 25.916082 Test MSE 29.36446749163143 Test RE 0.09122213562663287 Lambda1 0.9657596\n",
      "106 Train Loss 25.88423 Test MSE 29.320281908323953 Test RE 0.09115347746527064 Lambda1 0.9720884\n",
      "107 Train Loss 25.84598 Test MSE 29.26489372735047 Test RE 0.09106733893532878 Lambda1 0.97666484\n",
      "108 Train Loss 25.814182 Test MSE 29.207886321295618 Test RE 0.09097859705362757 Lambda1 0.9785094\n",
      "109 Train Loss 25.797344 Test MSE 29.181237708941616 Test RE 0.09093708417941292 Lambda1 0.98101115\n",
      "110 Train Loss 25.76174 Test MSE 29.127297217627028 Test RE 0.0908529983035156 Lambda1 0.9807113\n",
      "111 Train Loss 25.748367 Test MSE 29.108693234249216 Test RE 0.09082397917351438 Lambda1 0.9818255\n",
      "112 Train Loss 25.72563 Test MSE 29.064975493634545 Test RE 0.09075575022384466 Lambda1 0.9825937\n",
      "113 Train Loss 25.704693 Test MSE 29.006487698046147 Test RE 0.09066438980193388 Lambda1 0.9822487\n",
      "114 Train Loss 25.680056 Test MSE 29.004394842172484 Test RE 0.09066111896601847 Lambda1 0.9843729\n",
      "115 Train Loss 25.65257 Test MSE 28.97368809021163 Test RE 0.09061311510791026 Lambda1 0.98342997\n",
      "116 Train Loss 25.623117 Test MSE 28.94563932696348 Test RE 0.09056924424798413 Lambda1 0.9817529\n",
      "117 Train Loss 25.595293 Test MSE 28.887644393667706 Test RE 0.09047846734650594 Lambda1 0.9825044\n",
      "118 Train Loss 25.561398 Test MSE 28.82270011498065 Test RE 0.09037670472045184 Lambda1 0.98335445\n",
      "119 Train Loss 25.517843 Test MSE 28.77827308752904 Test RE 0.09030702497576353 Lambda1 0.9850751\n",
      "120 Train Loss 25.493694 Test MSE 28.76973217100834 Test RE 0.09029362316399864 Lambda1 0.9826951\n",
      "121 Train Loss 25.47915 Test MSE 28.78388489745618 Test RE 0.09031582955472663 Lambda1 0.9812135\n",
      "122 Train Loss 25.46335 Test MSE 28.773862893717503 Test RE 0.09030010505494071 Lambda1 0.981827\n",
      "123 Train Loss 25.446554 Test MSE 28.76524410298783 Test RE 0.0902865800027019 Lambda1 0.98368084\n",
      "124 Train Loss 25.433872 Test MSE 28.73692098714747 Test RE 0.09024211962825973 Lambda1 0.984655\n",
      "125 Train Loss 25.425436 Test MSE 28.7399963232902 Test RE 0.09024694821493484 Lambda1 0.98545957\n",
      "126 Train Loss 25.411652 Test MSE 28.731726832651184 Test RE 0.0902339636971812 Lambda1 0.9887302\n",
      "127 Train Loss 25.398645 Test MSE 28.701216970685973 Test RE 0.09018604181221691 Lambda1 0.99140936\n",
      "128 Train Loss 25.380836 Test MSE 28.68660566491902 Test RE 0.0901630827929852 Lambda1 0.9910054\n",
      "129 Train Loss 25.363989 Test MSE 28.666807315063686 Test RE 0.09013196394291745 Lambda1 0.9919453\n",
      "130 Train Loss 25.354773 Test MSE 28.647345382088712 Test RE 0.0901013633942524 Lambda1 0.99254876\n",
      "131 Train Loss 25.340626 Test MSE 28.607162027635635 Test RE 0.09003814906319835 Lambda1 0.9938658\n",
      "132 Train Loss 25.32992 Test MSE 28.57944271163121 Test RE 0.08999451661931528 Lambda1 0.9921661\n",
      "133 Train Loss 25.312897 Test MSE 28.546828053179237 Test RE 0.08994315140690975 Lambda1 0.9927533\n",
      "134 Train Loss 25.29148 Test MSE 28.519770492162195 Test RE 0.08990051586003882 Lambda1 0.9947246\n",
      "135 Train Loss 25.282024 Test MSE 28.527503265080515 Test RE 0.08991270272456216 Lambda1 0.9947589\n",
      "136 Train Loss 25.271473 Test MSE 28.521785170904174 Test RE 0.08990369115662299 Lambda1 0.9926411\n",
      "137 Train Loss 25.25448 Test MSE 28.49980772028937 Test RE 0.08986904685419365 Lambda1 0.9931819\n",
      "138 Train Loss 25.237917 Test MSE 28.451893165220042 Test RE 0.08979347008518378 Lambda1 0.9966324\n",
      "139 Train Loss 25.226536 Test MSE 28.433971856589174 Test RE 0.08976518602751712 Lambda1 0.99733835\n",
      "140 Train Loss 25.214571 Test MSE 28.421392391505528 Test RE 0.08974532733614733 Lambda1 0.9970543\n",
      "141 Train Loss 25.192797 Test MSE 28.398907386308064 Test RE 0.08970982021897943 Lambda1 0.9978062\n",
      "142 Train Loss 25.182858 Test MSE 28.40265561848203 Test RE 0.08971574020272247 Lambda1 0.99863964\n",
      "143 Train Loss 25.173882 Test MSE 28.375439921564638 Test RE 0.08967274665985246 Lambda1 0.9996112\n",
      "144 Train Loss 25.160755 Test MSE 28.361539018111618 Test RE 0.08965077898461812 Lambda1 0.99895173\n",
      "145 Train Loss 25.147234 Test MSE 28.353885235987434 Test RE 0.08963868137192257 Lambda1 0.9997477\n",
      "146 Train Loss 25.130955 Test MSE 28.340375322122895 Test RE 0.08961732353784403 Lambda1 1.0020604\n",
      "147 Train Loss 25.122545 Test MSE 28.32204503088165 Test RE 0.08958833702424596 Lambda1 1.0014745\n",
      "148 Train Loss 25.117678 Test MSE 28.30941696486154 Test RE 0.0895683622383705 Lambda1 1.0027386\n",
      "149 Train Loss 25.112642 Test MSE 28.307639842107903 Test RE 0.08956555086869708 Lambda1 1.0033618\n",
      "150 Train Loss 25.088444 Test MSE 28.315714912173863 Test RE 0.08957832474408026 Lambda1 1.0037596\n",
      "151 Train Loss 25.074665 Test MSE 28.3269419410599 Test RE 0.08959608164645466 Lambda1 1.0017246\n",
      "152 Train Loss 25.06928 Test MSE 28.31816530580254 Test RE 0.08958220063787758 Lambda1 1.0011097\n",
      "153 Train Loss 25.058752 Test MSE 28.307858131803386 Test RE 0.08956589620301748 Lambda1 1.0004373\n",
      "154 Train Loss 25.03154 Test MSE 28.304169774412806 Test RE 0.0895600610424219 Lambda1 0.99387276\n",
      "155 Train Loss 25.013992 Test MSE 28.283866933510087 Test RE 0.08952793415304225 Lambda1 0.9895742\n",
      "156 Train Loss 25.006735 Test MSE 28.267952204290907 Test RE 0.08950274287850205 Lambda1 0.9860325\n",
      "157 Train Loss 25.003479 Test MSE 28.25259372278496 Test RE 0.08947842535849285 Lambda1 0.9842941\n",
      "158 Train Loss 24.994333 Test MSE 28.208030187139247 Test RE 0.08940782920960649 Lambda1 0.9872657\n",
      "159 Train Loss 24.978092 Test MSE 28.17882806842596 Test RE 0.0893615378857789 Lambda1 0.98446137\n",
      "160 Train Loss 24.958988 Test MSE 28.149132701524366 Test RE 0.08931444005889413 Lambda1 0.9814973\n",
      "161 Train Loss 24.937954 Test MSE 28.14145174369139 Test RE 0.08930225376421067 Lambda1 0.98022634\n",
      "162 Train Loss 24.910295 Test MSE 28.119861198928625 Test RE 0.0892679901753841 Lambda1 0.97902805\n",
      "163 Train Loss 24.89871 Test MSE 28.11996381648498 Test RE 0.08926815305767201 Lambda1 0.97817373\n",
      "164 Train Loss 24.888903 Test MSE 28.106161525594583 Test RE 0.0892462423565899 Lambda1 0.97637194\n",
      "165 Train Loss 24.881659 Test MSE 28.11043031875604 Test RE 0.08925301950551036 Lambda1 0.97454786\n",
      "166 Train Loss 24.87029 Test MSE 28.099515998474114 Test RE 0.08923569087332174 Lambda1 0.9717832\n",
      "167 Train Loss 24.86343 Test MSE 28.09510443105646 Test RE 0.08922868568649037 Lambda1 0.96957654\n",
      "168 Train Loss 24.852104 Test MSE 28.10205629115206 Test RE 0.08923972438850955 Lambda1 0.968096\n",
      "169 Train Loss 24.844198 Test MSE 28.103400001103232 Test RE 0.08924185787780088 Lambda1 0.96693057\n",
      "170 Train Loss 24.835958 Test MSE 28.115594568333623 Test RE 0.08926121759596171 Lambda1 0.9671402\n",
      "171 Train Loss 24.83265 Test MSE 28.11790185633346 Test RE 0.08926488010273471 Lambda1 0.96457213\n",
      "172 Train Loss 24.822683 Test MSE 28.12473673691356 Test RE 0.08927572866687276 Lambda1 0.9637798\n",
      "173 Train Loss 24.81276 Test MSE 28.104709830137157 Test RE 0.08924393752322651 Lambda1 0.9639687\n",
      "174 Train Loss 24.799217 Test MSE 28.08165785864129 Test RE 0.08920733030085562 Lambda1 0.96505994\n",
      "175 Train Loss 24.79247 Test MSE 28.075310387482574 Test RE 0.08919724768820439 Lambda1 0.9648741\n",
      "176 Train Loss 24.784702 Test MSE 28.06875512969232 Test RE 0.08918683382136748 Lambda1 0.9632645\n",
      "177 Train Loss 24.77591 Test MSE 28.052472553235116 Test RE 0.08916096159978641 Lambda1 0.9635824\n",
      "178 Train Loss 24.77147 Test MSE 28.04299298610065 Test RE 0.08914589557066178 Lambda1 0.96437806\n",
      "179 Train Loss 24.756935 Test MSE 28.0711625731508 Test RE 0.08919065849501136 Lambda1 0.9609094\n",
      "180 Train Loss 24.74771 Test MSE 28.070206845305847 Test RE 0.0891891401624312 Lambda1 0.96154046\n",
      "181 Train Loss 24.739529 Test MSE 28.060465247601712 Test RE 0.08917366254014726 Lambda1 0.961341\n",
      "182 Train Loss 24.72989 Test MSE 28.05311970819948 Test RE 0.08916199004079173 Lambda1 0.9615093\n",
      "183 Train Loss 24.725758 Test MSE 28.055759118186742 Test RE 0.08916618439625491 Lambda1 0.9609101\n",
      "184 Train Loss 24.721584 Test MSE 28.059867780164666 Test RE 0.0891727131859091 Lambda1 0.9597724\n",
      "185 Train Loss 24.71244 Test MSE 28.06227051441361 Test RE 0.08917653098268641 Lambda1 0.9600286\n",
      "186 Train Loss 24.699505 Test MSE 28.043603805460677 Test RE 0.08914686643248096 Lambda1 0.9598794\n",
      "187 Train Loss 24.688864 Test MSE 27.999369532504 Test RE 0.08907653126663082 Lambda1 0.96141154\n",
      "188 Train Loss 24.684551 Test MSE 27.98295852964624 Test RE 0.08905042265241452 Lambda1 0.9613218\n",
      "189 Train Loss 24.677555 Test MSE 27.97819492152028 Test RE 0.0890428426931464 Lambda1 0.9629706\n",
      "190 Train Loss 24.67262 Test MSE 27.962400700295117 Test RE 0.08901770595906558 Lambda1 0.9638493\n",
      "191 Train Loss 24.666546 Test MSE 27.960781080250513 Test RE 0.08901512790880797 Lambda1 0.9625005\n",
      "192 Train Loss 24.661573 Test MSE 27.950516736305154 Test RE 0.08899878777591533 Lambda1 0.96228236\n",
      "193 Train Loss 24.656006 Test MSE 27.943036216088867 Test RE 0.08898687740967218 Lambda1 0.96335775\n",
      "194 Train Loss 24.649221 Test MSE 27.94435349928522 Test RE 0.0889889748828303 Lambda1 0.9615826\n",
      "195 Train Loss 24.644295 Test MSE 27.945940298612683 Test RE 0.08899150143333655 Lambda1 0.9621233\n",
      "196 Train Loss 24.63714 Test MSE 27.92817308511486 Test RE 0.0889632078354522 Lambda1 0.9641407\n",
      "197 Train Loss 24.63208 Test MSE 27.919601083323457 Test RE 0.08894955403684361 Lambda1 0.9652589\n",
      "198 Train Loss 24.622972 Test MSE 27.905443191705317 Test RE 0.08892699823754507 Lambda1 0.9658614\n",
      "199 Train Loss 24.61453 Test MSE 27.897755199177507 Test RE 0.08891474763131384 Lambda1 0.9682102\n",
      "Training time: 353.59\n",
      "Training time: 353.59\n",
      "inv_HT_stan_late_Try\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 845.7167 Test MSE 848.1355327638965 Test RE 0.4902547383421945 Lambda1 0.060626794\n",
      "1 Train Loss 832.2067 Test MSE 832.6469940703081 Test RE 0.48575762880798984 Lambda1 0.086064495\n",
      "2 Train Loss 812.3464 Test MSE 809.3325508331102 Test RE 0.47890864253343307 Lambda1 0.102726445\n",
      "3 Train Loss 747.32025 Test MSE 742.8578791233729 Test RE 0.4588196799843391 Lambda1 0.09799739\n",
      "4 Train Loss 479.84695 Test MSE 470.6913630237936 Test RE 0.36522229843351944 Lambda1 0.06013595\n",
      "5 Train Loss 340.67734 Test MSE 299.96741118730125 Test RE 0.2915588362883522 Lambda1 0.053131957\n",
      "6 Train Loss 269.72855 Test MSE 268.457749603282 Test RE 0.27582088225210677 Lambda1 0.029840788\n",
      "7 Train Loss 239.01083 Test MSE 225.0866107733596 Test RE 0.25255967007882096 Lambda1 0.11208816\n",
      "8 Train Loss 198.75911 Test MSE 186.97042438507202 Test RE 0.2301842622678724 Lambda1 0.2079035\n",
      "9 Train Loss 148.5794 Test MSE 129.51261476094 Test RE 0.19157781089902753 Lambda1 0.32243937\n",
      "10 Train Loss 111.85268 Test MSE 103.04129400426847 Test RE 0.17088141147296637 Lambda1 0.3340471\n",
      "11 Train Loss 93.05129 Test MSE 86.02521318182573 Test RE 0.15613564058599716 Lambda1 0.38327113\n",
      "12 Train Loss 71.848724 Test MSE 71.58948634073636 Test RE 0.14243404033001622 Lambda1 0.45047137\n",
      "13 Train Loss 62.61532 Test MSE 63.206998779420985 Test RE 0.13383563239164128 Lambda1 0.5057399\n",
      "14 Train Loss 59.453285 Test MSE 61.709055467767826 Test RE 0.13224023738309476 Lambda1 0.5208499\n",
      "15 Train Loss 55.57625 Test MSE 57.86528961900154 Test RE 0.12805549999705984 Lambda1 0.54608744\n",
      "16 Train Loss 52.268826 Test MSE 55.37337408812339 Test RE 0.12526786174441026 Lambda1 0.5613189\n",
      "17 Train Loss 50.174973 Test MSE 52.76263900553015 Test RE 0.12227915398351256 Lambda1 0.5970068\n",
      "18 Train Loss 47.335434 Test MSE 50.80955657348684 Test RE 0.11999464714911962 Lambda1 0.614365\n",
      "19 Train Loss 45.565186 Test MSE 48.89729691376109 Test RE 0.11771494293753193 Lambda1 0.6618238\n",
      "20 Train Loss 43.922363 Test MSE 46.789998483233745 Test RE 0.11515046238022855 Lambda1 0.6862842\n",
      "21 Train Loss 42.555534 Test MSE 46.2899199973864 Test RE 0.11453346131535505 Lambda1 0.7031612\n",
      "22 Train Loss 41.613983 Test MSE 45.7551517894857 Test RE 0.1138699607224282 Lambda1 0.70532966\n",
      "23 Train Loss 40.963367 Test MSE 45.28340628874562 Test RE 0.11328142786802144 Lambda1 0.7253952\n",
      "24 Train Loss 40.550335 Test MSE 44.81627870327897 Test RE 0.11269562767514732 Lambda1 0.7277935\n",
      "25 Train Loss 39.79077 Test MSE 43.82430522438386 Test RE 0.11144143364615527 Lambda1 0.7324595\n",
      "26 Train Loss 38.949013 Test MSE 42.842130814607934 Test RE 0.11018556478871318 Lambda1 0.7396564\n",
      "27 Train Loss 38.410976 Test MSE 43.10017640747917 Test RE 0.11051690000984221 Lambda1 0.7442627\n",
      "28 Train Loss 37.853363 Test MSE 42.533595051738594 Test RE 0.10978808670415548 Lambda1 0.75039715\n",
      "29 Train Loss 37.650715 Test MSE 42.291845699066855 Test RE 0.10947563933577129 Lambda1 0.74838763\n",
      "30 Train Loss 37.202976 Test MSE 42.046554622864406 Test RE 0.10915770041111829 Lambda1 0.7487262\n",
      "31 Train Loss 36.77564 Test MSE 41.411378709886726 Test RE 0.10833006793339446 Lambda1 0.7574339\n",
      "32 Train Loss 36.34692 Test MSE 40.97810268665228 Test RE 0.10776186372480685 Lambda1 0.7599711\n",
      "33 Train Loss 36.01454 Test MSE 40.29349044119958 Test RE 0.10685789521579322 Lambda1 0.7717999\n",
      "34 Train Loss 35.643894 Test MSE 39.620804133519464 Test RE 0.10596216263302807 Lambda1 0.78836185\n",
      "35 Train Loss 35.29556 Test MSE 39.0310390973845 Test RE 0.1051705699069125 Lambda1 0.7929292\n",
      "36 Train Loss 34.60115 Test MSE 38.34591190574531 Test RE 0.10424343309066111 Lambda1 0.7971917\n",
      "37 Train Loss 34.302624 Test MSE 38.14644310476119 Test RE 0.10397195142627394 Lambda1 0.79983425\n",
      "38 Train Loss 33.981216 Test MSE 37.714353205597725 Test RE 0.103381422263842 Lambda1 0.80484253\n",
      "39 Train Loss 33.85028 Test MSE 37.57730275391526 Test RE 0.1031934120690734 Lambda1 0.8056687\n",
      "40 Train Loss 33.63821 Test MSE 37.60915148580327 Test RE 0.10323713371725891 Lambda1 0.81079406\n",
      "41 Train Loss 33.46283 Test MSE 37.37093765428895 Test RE 0.10290966582909247 Lambda1 0.8217143\n",
      "42 Train Loss 33.28733 Test MSE 37.03333781341431 Test RE 0.10244378099008179 Lambda1 0.8389266\n",
      "43 Train Loss 33.016865 Test MSE 36.48124373390882 Test RE 0.10167729613197288 Lambda1 0.8630606\n",
      "44 Train Loss 32.616695 Test MSE 36.194772510172626 Test RE 0.10127729569764678 Lambda1 0.8598968\n",
      "45 Train Loss 32.296345 Test MSE 35.96576118763932 Test RE 0.10095638676497526 Lambda1 0.8699684\n",
      "46 Train Loss 32.180504 Test MSE 35.96287124102072 Test RE 0.10095233062352217 Lambda1 0.87420475\n",
      "47 Train Loss 32.08743 Test MSE 35.929960214324005 Test RE 0.1009061273421057 Lambda1 0.86972725\n",
      "48 Train Loss 31.851526 Test MSE 35.92917044153475 Test RE 0.100905018332367 Lambda1 0.8682761\n",
      "49 Train Loss 31.6235 Test MSE 35.779037595408965 Test RE 0.10069397788752031 Lambda1 0.87849\n",
      "50 Train Loss 31.487593 Test MSE 35.61321373409506 Test RE 0.10046036545548916 Lambda1 0.88503426\n",
      "51 Train Loss 31.418604 Test MSE 35.50742513268038 Test RE 0.10031104636227808 Lambda1 0.887429\n",
      "52 Train Loss 31.223623 Test MSE 35.38951721170065 Test RE 0.10014435866308297 Lambda1 0.8791098\n",
      "53 Train Loss 31.05014 Test MSE 35.100456727147204 Test RE 0.09973453206877869 Lambda1 0.8751149\n",
      "54 Train Loss 30.876465 Test MSE 34.861998088893 Test RE 0.09939517621637235 Lambda1 0.86356056\n",
      "55 Train Loss 30.776674 Test MSE 34.87581323378185 Test RE 0.09941486847189043 Lambda1 0.8637123\n",
      "56 Train Loss 30.585342 Test MSE 34.68889314498391 Test RE 0.09914809903919754 Lambda1 0.8658327\n",
      "57 Train Loss 30.394615 Test MSE 34.33004655762213 Test RE 0.09863393663322387 Lambda1 0.87788653\n",
      "58 Train Loss 30.23477 Test MSE 33.95146962899218 Test RE 0.09808858275354496 Lambda1 0.89068574\n",
      "59 Train Loss 30.144835 Test MSE 33.93175502271709 Test RE 0.09806010006025492 Lambda1 0.8943392\n",
      "60 Train Loss 30.07794 Test MSE 33.80240979815238 Test RE 0.09787302285221931 Lambda1 0.9015209\n",
      "61 Train Loss 29.946161 Test MSE 33.50782730340203 Test RE 0.09744561589403217 Lambda1 0.91201156\n",
      "62 Train Loss 29.64623 Test MSE 32.913797698571955 Test RE 0.09657799129573146 Lambda1 0.9229912\n",
      "63 Train Loss 29.398272 Test MSE 32.54657988904795 Test RE 0.09603772190398743 Lambda1 0.9331319\n",
      "64 Train Loss 29.315613 Test MSE 32.436738981429365 Test RE 0.09587552689106 Lambda1 0.9384178\n",
      "65 Train Loss 29.19487 Test MSE 32.383816043189945 Test RE 0.09579728095612094 Lambda1 0.9385464\n",
      "66 Train Loss 29.065859 Test MSE 32.379348037067274 Test RE 0.09579067213642672 Lambda1 0.93175256\n",
      "67 Train Loss 28.992306 Test MSE 32.27008052053483 Test RE 0.09562890769776054 Lambda1 0.9257208\n",
      "68 Train Loss 28.837986 Test MSE 32.12510774230376 Test RE 0.09541386029532754 Lambda1 0.9266649\n",
      "69 Train Loss 28.725832 Test MSE 32.10256254313364 Test RE 0.09538037399475045 Lambda1 0.91905814\n",
      "70 Train Loss 28.626324 Test MSE 31.906023077624187 Test RE 0.09508795516155674 Lambda1 0.91337514\n",
      "71 Train Loss 28.55188 Test MSE 31.85350136597942 Test RE 0.09500965898654995 Lambda1 0.9065786\n",
      "72 Train Loss 28.461775 Test MSE 31.70359067518974 Test RE 0.09478582549412368 Lambda1 0.91388756\n",
      "73 Train Loss 28.212437 Test MSE 31.484914953509392 Test RE 0.09445836693305612 Lambda1 0.91388786\n",
      "74 Train Loss 28.090813 Test MSE 31.25785518850871 Test RE 0.09411714791944104 Lambda1 0.92304415\n",
      "75 Train Loss 27.98036 Test MSE 31.059621182347566 Test RE 0.09381823275289172 Lambda1 0.91946346\n",
      "76 Train Loss 27.906496 Test MSE 30.87749131154883 Test RE 0.09354275892468072 Lambda1 0.9196197\n",
      "77 Train Loss 27.851421 Test MSE 30.978970502032595 Test RE 0.09369634745181125 Lambda1 0.9226577\n",
      "78 Train Loss 27.764906 Test MSE 30.707149293614812 Test RE 0.09328437816533991 Lambda1 0.9250371\n",
      "79 Train Loss 27.543844 Test MSE 30.419858927935874 Test RE 0.09284697707239536 Lambda1 0.9292772\n",
      "80 Train Loss 27.464758 Test MSE 30.38430579456977 Test RE 0.09279270387475332 Lambda1 0.9278193\n",
      "81 Train Loss 27.400248 Test MSE 30.326509178339677 Test RE 0.09270440735144646 Lambda1 0.92806536\n",
      "82 Train Loss 27.290344 Test MSE 30.25233412237877 Test RE 0.09259096593291383 Lambda1 0.9314961\n",
      "83 Train Loss 27.204018 Test MSE 30.10184701675349 Test RE 0.09236038673790906 Lambda1 0.938487\n",
      "84 Train Loss 27.054186 Test MSE 30.129553143918333 Test RE 0.0924028818058537 Lambda1 0.9328378\n",
      "85 Train Loss 26.998896 Test MSE 30.094086753470183 Test RE 0.09234848070573794 Lambda1 0.9335002\n",
      "86 Train Loss 26.914349 Test MSE 29.83881929860143 Test RE 0.09195598226328616 Lambda1 0.9374671\n",
      "87 Train Loss 26.777723 Test MSE 29.65721616480972 Test RE 0.09167572685420766 Lambda1 0.92533165\n",
      "88 Train Loss 26.688105 Test MSE 29.513494548014044 Test RE 0.09145332254669891 Lambda1 0.9245874\n",
      "89 Train Loss 26.649082 Test MSE 29.489736673284618 Test RE 0.09141650593036352 Lambda1 0.92566115\n",
      "90 Train Loss 26.599455 Test MSE 29.472203749849292 Test RE 0.09138932635732977 Lambda1 0.9227747\n",
      "91 Train Loss 26.547062 Test MSE 29.43299565476559 Test RE 0.09132851662044014 Lambda1 0.92185163\n",
      "92 Train Loss 26.507244 Test MSE 29.381216585540155 Test RE 0.09124814785224278 Lambda1 0.92136335\n",
      "93 Train Loss 26.394226 Test MSE 29.400324116147193 Test RE 0.09127781380221098 Lambda1 0.9180126\n",
      "94 Train Loss 26.333088 Test MSE 29.29695548537859 Test RE 0.09111721063047568 Lambda1 0.91741014\n",
      "95 Train Loss 26.273806 Test MSE 29.336607597948646 Test RE 0.09117885130533239 Lambda1 0.91653556\n",
      "96 Train Loss 26.169325 Test MSE 29.173299364180775 Test RE 0.09092471426154587 Lambda1 0.921754\n",
      "97 Train Loss 26.104006 Test MSE 29.120965801021736 Test RE 0.09084312338376907 Lambda1 0.9164195\n",
      "98 Train Loss 26.066053 Test MSE 29.059337740223146 Test RE 0.09074694781913185 Lambda1 0.9169701\n",
      "99 Train Loss 26.03331 Test MSE 29.037922383958193 Test RE 0.09071350355396467 Lambda1 0.9145283\n",
      "100 Train Loss 26.007723 Test MSE 29.03571192624931 Test RE 0.09071005079007982 Lambda1 0.91163987\n",
      "101 Train Loss 25.963934 Test MSE 28.99360196251882 Test RE 0.0906442493571904 Lambda1 0.90704757\n",
      "102 Train Loss 25.91329 Test MSE 28.91438563907083 Test RE 0.09052033554440507 Lambda1 0.903312\n",
      "103 Train Loss 25.884481 Test MSE 28.859468859122 Test RE 0.09043433254167112 Lambda1 0.903051\n",
      "104 Train Loss 25.846643 Test MSE 28.80468223515245 Test RE 0.09034845179537682 Lambda1 0.90473586\n",
      "105 Train Loss 25.807539 Test MSE 28.792537025088695 Test RE 0.09032940252054107 Lambda1 0.90508306\n",
      "106 Train Loss 25.784046 Test MSE 28.832596470627177 Test RE 0.09039221893750977 Lambda1 0.90251017\n",
      "107 Train Loss 25.77141 Test MSE 28.853155505565763 Test RE 0.09042444020535746 Lambda1 0.89945585\n",
      "108 Train Loss 25.751919 Test MSE 28.864243127919675 Test RE 0.09044181258156854 Lambda1 0.9026431\n",
      "109 Train Loss 25.731647 Test MSE 28.84538802721093 Test RE 0.09041226792910444 Lambda1 0.90277195\n",
      "110 Train Loss 25.719193 Test MSE 28.850690200559473 Test RE 0.09042057704752854 Lambda1 0.9081498\n",
      "111 Train Loss 25.705406 Test MSE 28.83314298390704 Test RE 0.09039307561224606 Lambda1 0.91413754\n",
      "112 Train Loss 25.689014 Test MSE 28.81729989836277 Test RE 0.09036823784116374 Lambda1 0.9148381\n",
      "113 Train Loss 25.674389 Test MSE 28.749987324430595 Test RE 0.0902626333076591 Lambda1 0.9157293\n",
      "114 Train Loss 25.663574 Test MSE 28.745204717379682 Test RE 0.09025512532763418 Lambda1 0.9185083\n",
      "115 Train Loss 25.635876 Test MSE 28.696091123080787 Test RE 0.09017798813719693 Lambda1 0.9222209\n",
      "116 Train Loss 25.612509 Test MSE 28.693351274826423 Test RE 0.09017368302295437 Lambda1 0.9221299\n",
      "117 Train Loss 25.58955 Test MSE 28.691781718898476 Test RE 0.09017121669246325 Lambda1 0.92224216\n",
      "118 Train Loss 25.5741 Test MSE 28.736390006419043 Test RE 0.09024128590902132 Lambda1 0.9188715\n",
      "119 Train Loss 25.566545 Test MSE 28.731874167333928 Test RE 0.0902341950542355 Lambda1 0.9202312\n",
      "120 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "121 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "122 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "123 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "124 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "125 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "126 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "127 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "128 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "129 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "130 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "131 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "132 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "133 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "134 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "135 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "136 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "137 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "138 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "139 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "140 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "141 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "142 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "143 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "144 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "145 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "146 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "147 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "148 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "149 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "150 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "151 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "152 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "153 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "154 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "155 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "156 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "157 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "158 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "159 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "160 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "161 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "162 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "163 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "164 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "165 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "166 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "167 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "168 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "169 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "170 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "171 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "172 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "173 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "174 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "175 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "176 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "177 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "178 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "179 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "180 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "181 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "182 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "183 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "184 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "185 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "186 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "187 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "188 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "189 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "190 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "191 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "192 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "193 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "194 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "195 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "196 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "197 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "198 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "199 Train Loss 25.566193 Test MSE 28.72918103983678 Test RE 0.09022996598993384 Lambda1 0.9204328\n",
      "Training time: 265.03\n",
      "Training time: 265.03\n",
      "inv_HT_stan_late_Try\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 842.9474 Test MSE 843.1619401423868 Test RE 0.48881516149799137 Lambda1 0.04523379\n",
      "1 Train Loss 761.47925 Test MSE 758.4401204418076 Test RE 0.4636068240133413 Lambda1 0.04273979\n",
      "2 Train Loss 459.9952 Test MSE 447.0511776383101 Test RE 0.3559326206844374 Lambda1 0.03711114\n",
      "3 Train Loss 292.88806 Test MSE 294.2590125748509 Test RE 0.2887713194805419 Lambda1 0.00046434812\n",
      "4 Train Loss 272.96893 Test MSE 279.6493817000541 Test RE 0.2815114750329611 Lambda1 0.0002676519\n",
      "5 Train Loss 269.79227 Test MSE 275.9501796519936 Test RE 0.27964335969965753 Lambda1 0.0053958865\n",
      "6 Train Loss 264.3035 Test MSE 267.00068806235623 Test RE 0.27507135127443794 Lambda1 0.023188442\n",
      "7 Train Loss 250.0368 Test MSE 250.95216485977554 Test RE 0.26667643693866894 Lambda1 0.055478416\n",
      "8 Train Loss 232.30821 Test MSE 232.683052170599 Test RE 0.25678612081795044 Lambda1 0.09150872\n",
      "9 Train Loss 215.13556 Test MSE 212.5912482905103 Test RE 0.24544933604450658 Lambda1 0.13038084\n",
      "10 Train Loss 199.08913 Test MSE 194.87869910391925 Test RE 0.23500189079608413 Lambda1 0.17677851\n",
      "11 Train Loss 179.20175 Test MSE 174.3527255529692 Test RE 0.22228161373691124 Lambda1 0.20708919\n",
      "12 Train Loss 162.30417 Test MSE 154.68880786111964 Test RE 0.20937200736213304 Lambda1 0.2363208\n",
      "13 Train Loss 139.73282 Test MSE 132.89850885944946 Test RE 0.19406589732252502 Lambda1 0.27241802\n",
      "14 Train Loss 121.39324 Test MSE 113.95388320612793 Test RE 0.1797023412066908 Lambda1 0.3105242\n",
      "15 Train Loss 113.37839 Test MSE 108.3313785192546 Test RE 0.17521299182731076 Lambda1 0.32688192\n",
      "16 Train Loss 102.94123 Test MSE 97.54605862852314 Test RE 0.1662623956093596 Lambda1 0.3439986\n",
      "17 Train Loss 95.08476 Test MSE 90.82519699027411 Test RE 0.1604324976942692 Lambda1 0.3632297\n",
      "18 Train Loss 87.23423 Test MSE 81.84162551154644 Test RE 0.1522917207924153 Lambda1 0.38838693\n",
      "19 Train Loss 79.68704 Test MSE 74.80215399295703 Test RE 0.14559492015657471 Lambda1 0.4137076\n",
      "20 Train Loss 71.06076 Test MSE 66.28043694560972 Test RE 0.1370508874314311 Lambda1 0.4465784\n",
      "21 Train Loss 65.05069 Test MSE 62.3304509280339 Test RE 0.1329043834371294 Lambda1 0.47351673\n",
      "22 Train Loss 61.389908 Test MSE 61.99727401783134 Test RE 0.13254869850654616 Lambda1 0.48161763\n",
      "23 Train Loss 58.556026 Test MSE 60.45014996117892 Test RE 0.13088439260754234 Lambda1 0.49388245\n",
      "24 Train Loss 56.54743 Test MSE 58.970654528574926 Test RE 0.12927279676623166 Lambda1 0.51080096\n",
      "25 Train Loss 54.282906 Test MSE 57.05830120225297 Test RE 0.12715943490873624 Lambda1 0.5283085\n",
      "26 Train Loss 52.01919 Test MSE 54.48862941927521 Test RE 0.12426307964195789 Lambda1 0.5400667\n",
      "27 Train Loss 50.12541 Test MSE 52.98597684590466 Test RE 0.12253767709926038 Lambda1 0.55479205\n",
      "28 Train Loss 48.365 Test MSE 52.018135188204724 Test RE 0.12141338297071258 Lambda1 0.56490135\n",
      "29 Train Loss 47.582798 Test MSE 51.451850601966385 Test RE 0.1207507037622519 Lambda1 0.5738436\n",
      "30 Train Loss 46.72537 Test MSE 50.46612396388746 Test RE 0.11958842486679976 Lambda1 0.58497906\n",
      "31 Train Loss 45.464523 Test MSE 48.89121845117566 Test RE 0.11770762609018212 Lambda1 0.60386854\n",
      "32 Train Loss 44.243202 Test MSE 48.00547626768421 Test RE 0.1166365222999245 Lambda1 0.62396246\n",
      "33 Train Loss 43.719334 Test MSE 47.76713590394357 Test RE 0.11634662014714353 Lambda1 0.6281555\n",
      "34 Train Loss 43.152508 Test MSE 47.35407298677152 Test RE 0.11584247832841815 Lambda1 0.6346132\n",
      "35 Train Loss 42.579605 Test MSE 46.703377875561735 Test RE 0.11504382609108421 Lambda1 0.64089936\n",
      "36 Train Loss 42.024323 Test MSE 46.1794895358074 Test RE 0.11439676270632511 Lambda1 0.65291876\n",
      "37 Train Loss 41.527363 Test MSE 45.879001051227505 Test RE 0.114023967207697 Lambda1 0.660872\n",
      "38 Train Loss 40.741695 Test MSE 44.92712117321425 Test RE 0.11283490460699984 Lambda1 0.67396116\n",
      "39 Train Loss 40.108677 Test MSE 44.49640369181347 Test RE 0.11229272642301234 Lambda1 0.68174887\n",
      "40 Train Loss 39.443584 Test MSE 43.79471225354586 Test RE 0.1114038011040928 Lambda1 0.69367373\n",
      "41 Train Loss 39.045017 Test MSE 43.340757836030654 Test RE 0.11082491862691367 Lambda1 0.7062518\n",
      "42 Train Loss 38.633953 Test MSE 43.011685479480995 Test RE 0.1104033880845403 Lambda1 0.71150464\n",
      "43 Train Loss 38.190304 Test MSE 42.15866702996259 Test RE 0.10930313189470035 Lambda1 0.7350065\n",
      "44 Train Loss 37.770588 Test MSE 41.554263396811756 Test RE 0.10851679655099264 Lambda1 0.75887984\n",
      "45 Train Loss 37.161957 Test MSE 40.78424189342537 Test RE 0.10750665953794594 Lambda1 0.78231937\n",
      "46 Train Loss 36.659073 Test MSE 40.77578235088353 Test RE 0.10749550934509106 Lambda1 0.7928348\n",
      "47 Train Loss 36.316853 Test MSE 40.49272883120833 Test RE 0.10712175845201553 Lambda1 0.79547226\n",
      "48 Train Loss 35.848446 Test MSE 40.0396902027742 Test RE 0.10652082586916248 Lambda1 0.8062427\n",
      "49 Train Loss 35.532566 Test MSE 39.5708812897887 Test RE 0.10589538458545124 Lambda1 0.8183506\n",
      "50 Train Loss 35.37771 Test MSE 39.52638474198986 Test RE 0.10583582937387127 Lambda1 0.8269676\n",
      "51 Train Loss 35.23187 Test MSE 39.24821253772055 Test RE 0.10546275494237407 Lambda1 0.84365994\n",
      "52 Train Loss 35.12823 Test MSE 39.01056913225623 Test RE 0.10514298775321901 Lambda1 0.8606975\n",
      "53 Train Loss 34.975925 Test MSE 38.8321162420474 Test RE 0.1049022250895587 Lambda1 0.8665222\n",
      "54 Train Loss 34.81498 Test MSE 38.45460448222226 Test RE 0.10439106903114315 Lambda1 0.87701595\n",
      "55 Train Loss 34.70284 Test MSE 38.31560077310558 Test RE 0.10420222451022829 Lambda1 0.8873951\n",
      "56 Train Loss 34.5665 Test MSE 38.15241539940579 Test RE 0.1039800901513512 Lambda1 0.8963865\n",
      "57 Train Loss 34.45437 Test MSE 38.04992381159078 Test RE 0.10384033161597847 Lambda1 0.90334594\n",
      "58 Train Loss 34.28667 Test MSE 38.060515059380585 Test RE 0.10385478265885312 Lambda1 0.9030629\n",
      "59 Train Loss 34.19302 Test MSE 38.05675133302179 Test RE 0.10384964753855584 Lambda1 0.89930826\n",
      "60 Train Loss 34.012115 Test MSE 37.98774422633819 Test RE 0.10375545117339599 Lambda1 0.895464\n",
      "61 Train Loss 33.865807 Test MSE 37.913654722167706 Test RE 0.10365422192259678 Lambda1 0.8953824\n",
      "62 Train Loss 33.795933 Test MSE 37.85567736413254 Test RE 0.10357493785990372 Lambda1 0.89486355\n",
      "63 Train Loss 33.68885 Test MSE 37.7506958216512 Test RE 0.10343122089922455 Lambda1 0.8989992\n",
      "64 Train Loss 33.587 Test MSE 37.66627542912818 Test RE 0.10331550659072401 Lambda1 0.89185584\n",
      "65 Train Loss 33.46271 Test MSE 37.684958699181706 Test RE 0.10334112675337383 Lambda1 0.8874666\n",
      "66 Train Loss 33.37139 Test MSE 37.50464883694426 Test RE 0.10309360403977023 Lambda1 0.8913891\n",
      "67 Train Loss 33.176193 Test MSE 37.196421498126504 Test RE 0.10266909900894536 Lambda1 0.8949421\n",
      "68 Train Loss 33.036373 Test MSE 36.88145477804259 Test RE 0.10223349129628856 Lambda1 0.90071344\n",
      "69 Train Loss 32.983326 Test MSE 36.61062892455663 Test RE 0.10185744202625649 Lambda1 0.90508384\n",
      "70 Train Loss 32.867928 Test MSE 36.443054646237506 Test RE 0.10162406357331798 Lambda1 0.9143372\n",
      "71 Train Loss 32.686512 Test MSE 35.95647835416437 Test RE 0.10094335740351838 Lambda1 0.91885805\n",
      "72 Train Loss 32.52361 Test MSE 35.83661280045199 Test RE 0.10077496310092961 Lambda1 0.92114955\n",
      "73 Train Loss 32.417316 Test MSE 35.60167412530172 Test RE 0.10044408824937748 Lambda1 0.9285915\n",
      "74 Train Loss 32.15386 Test MSE 35.44775056284985 Test RE 0.1002267184267374 Lambda1 0.9342891\n",
      "75 Train Loss 31.85227 Test MSE 35.16578879920312 Test RE 0.09982730628150918 Lambda1 0.9384637\n",
      "76 Train Loss 31.673567 Test MSE 35.173425502400505 Test RE 0.09983814508466109 Lambda1 0.9374137\n",
      "77 Train Loss 31.510708 Test MSE 35.091143687371456 Test RE 0.0997213001429065 Lambda1 0.9408606\n",
      "78 Train Loss 31.366894 Test MSE 35.07131740205129 Test RE 0.09969312519499818 Lambda1 0.9447135\n",
      "79 Train Loss 31.079748 Test MSE 34.869982968527296 Test RE 0.09940655842510451 Lambda1 0.95244604\n",
      "80 Train Loss 30.928078 Test MSE 34.56482951786775 Test RE 0.09897064035406111 Lambda1 0.95774865\n",
      "81 Train Loss 30.766785 Test MSE 34.56538379412206 Test RE 0.0989714338898122 Lambda1 0.95290035\n",
      "82 Train Loss 30.603317 Test MSE 34.37167428351722 Test RE 0.0986937189953933 Lambda1 0.95961416\n",
      "83 Train Loss 30.545137 Test MSE 34.267021331562546 Test RE 0.09854335589529724 Lambda1 0.95957744\n",
      "84 Train Loss 30.446543 Test MSE 34.10242426104279 Test RE 0.09830640106850244 Lambda1 0.95688283\n",
      "85 Train Loss 30.333471 Test MSE 34.09234924478935 Test RE 0.0982918784682304 Lambda1 0.9548249\n",
      "86 Train Loss 30.243277 Test MSE 33.99488599633291 Test RE 0.09815127944077112 Lambda1 0.9591302\n",
      "87 Train Loss 30.109676 Test MSE 33.924010131479996 Test RE 0.09804890835869265 Lambda1 0.9613805\n",
      "88 Train Loss 30.058542 Test MSE 33.90863668392508 Test RE 0.09802668927910999 Lambda1 0.9581814\n",
      "89 Train Loss 30.006546 Test MSE 33.88064812442464 Test RE 0.09798622477681275 Lambda1 0.9577843\n",
      "90 Train Loss 29.90322 Test MSE 33.77348319013186 Test RE 0.09783113618594326 Lambda1 0.9602301\n",
      "91 Train Loss 29.863518 Test MSE 33.76080459504759 Test RE 0.0978127715152726 Lambda1 0.9604343\n",
      "92 Train Loss 29.814394 Test MSE 33.67558144589664 Test RE 0.09768923803024084 Lambda1 0.96126384\n",
      "93 Train Loss 29.735838 Test MSE 33.55489492827868 Test RE 0.09751403161277723 Lambda1 0.9576726\n",
      "94 Train Loss 29.652962 Test MSE 33.48055475845062 Test RE 0.0974059515657633 Lambda1 0.95594203\n",
      "95 Train Loss 29.633068 Test MSE 33.47170346338763 Test RE 0.09739307504960178 Lambda1 0.9556486\n",
      "96 Train Loss 29.599268 Test MSE 33.38084004154308 Test RE 0.09726079194060488 Lambda1 0.9516016\n",
      "97 Train Loss 29.572699 Test MSE 33.303315675477954 Test RE 0.09714778603466268 Lambda1 0.951311\n",
      "98 Train Loss 29.51562 Test MSE 33.26757996192997 Test RE 0.09709565042616936 Lambda1 0.9520494\n",
      "99 Train Loss 29.484678 Test MSE 33.24664314358686 Test RE 0.09706509223793967 Lambda1 0.95204955\n",
      "100 Train Loss 29.449162 Test MSE 33.24809535942815 Test RE 0.09706721212000914 Lambda1 0.95225435\n",
      "101 Train Loss 29.3626 Test MSE 33.1753024395293 Test RE 0.09696089509035591 Lambda1 0.9498567\n",
      "102 Train Loss 29.313265 Test MSE 33.16115033749055 Test RE 0.09694021182974351 Lambda1 0.9513458\n",
      "103 Train Loss 29.28524 Test MSE 33.12385734656679 Test RE 0.09688568707133229 Lambda1 0.94878054\n",
      "104 Train Loss 29.238083 Test MSE 33.072405183517965 Test RE 0.09681041027845089 Lambda1 0.94673383\n",
      "105 Train Loss 29.141634 Test MSE 32.944167660113244 Test RE 0.09662253786591755 Lambda1 0.94700336\n",
      "106 Train Loss 29.107641 Test MSE 32.83252474913474 Test RE 0.09645867918085933 Lambda1 0.9465308\n",
      "107 Train Loss 29.058416 Test MSE 32.76353278793653 Test RE 0.09635728013827839 Lambda1 0.946422\n",
      "108 Train Loss 29.005064 Test MSE 32.6873933570368 Test RE 0.09624525229517199 Lambda1 0.94781154\n",
      "109 Train Loss 28.934366 Test MSE 32.628588979220964 Test RE 0.09615864107746451 Lambda1 0.9496789\n",
      "110 Train Loss 28.884712 Test MSE 32.654648379051 Test RE 0.0961970328133681 Lambda1 0.9507643\n",
      "111 Train Loss 28.814243 Test MSE 32.52536392893559 Test RE 0.09600641501028022 Lambda1 0.95385647\n",
      "112 Train Loss 28.698805 Test MSE 32.44131467764657 Test RE 0.09588228900447446 Lambda1 0.95990884\n",
      "113 Train Loss 28.639894 Test MSE 32.361285871718835 Test RE 0.09576395096353947 Lambda1 0.9631037\n",
      "114 Train Loss 28.568703 Test MSE 32.26439022023716 Test RE 0.09562047603004264 Lambda1 0.9661776\n",
      "115 Train Loss 28.519354 Test MSE 32.18543983306197 Test RE 0.095503413574401 Lambda1 0.9679447\n",
      "116 Train Loss 28.465416 Test MSE 32.19223421327234 Test RE 0.0955134934780008 Lambda1 0.967383\n",
      "117 Train Loss 28.399998 Test MSE 32.176254257156366 Test RE 0.09548978448529788 Lambda1 0.97138214\n",
      "118 Train Loss 28.35566 Test MSE 32.12190489204203 Test RE 0.09540910382996118 Lambda1 0.9751545\n",
      "119 Train Loss 28.303825 Test MSE 32.01616986980392 Test RE 0.09525194629018227 Lambda1 0.9794606\n",
      "120 Train Loss 28.274307 Test MSE 31.966283690139292 Test RE 0.09517770867583411 Lambda1 0.9827511\n",
      "121 Train Loss 28.256254 Test MSE 31.950821059069167 Test RE 0.0951546863591966 Lambda1 0.98464376\n",
      "122 Train Loss 28.238989 Test MSE 31.97290378600966 Test RE 0.09518756363651021 Lambda1 0.98543775\n",
      "123 Train Loss 28.204582 Test MSE 31.9442005277552 Test RE 0.09514482734466306 Lambda1 0.9867463\n",
      "124 Train Loss 28.186491 Test MSE 31.90468033370328 Test RE 0.09508595428359114 Lambda1 0.987712\n",
      "125 Train Loss 28.169582 Test MSE 31.919659254268698 Test RE 0.09510827260505116 Lambda1 0.98513955\n",
      "126 Train Loss 28.144281 Test MSE 31.866518408821193 Test RE 0.09502907001554499 Lambda1 0.9867881\n",
      "127 Train Loss 28.124165 Test MSE 31.87723362202252 Test RE 0.09504504557881312 Lambda1 0.984381\n",
      "128 Train Loss 28.069265 Test MSE 31.79019336365718 Test RE 0.09491519741988712 Lambda1 0.9882491\n",
      "129 Train Loss 28.03828 Test MSE 31.747392058397022 Test RE 0.09485128049719509 Lambda1 0.9908649\n",
      "130 Train Loss 28.01333 Test MSE 31.70857268475081 Test RE 0.09479327268430865 Lambda1 0.98831564\n",
      "131 Train Loss 28.001791 Test MSE 31.70398258860716 Test RE 0.09478641135397736 Lambda1 0.9877492\n",
      "132 Train Loss 27.990646 Test MSE 31.70888432376714 Test RE 0.09479373850801444 Lambda1 0.98576516\n",
      "133 Train Loss 27.916677 Test MSE 31.661914358325408 Test RE 0.09472350411185601 Lambda1 0.98721147\n",
      "134 Train Loss 27.865074 Test MSE 31.563602195397678 Test RE 0.09457632866540683 Lambda1 0.9922139\n",
      "135 Train Loss 27.819574 Test MSE 31.451172119206166 Test RE 0.0944077371718913 Lambda1 0.99455327\n",
      "136 Train Loss 27.805721 Test MSE 31.4163716543474 Test RE 0.09435549202095111 Lambda1 0.99474543\n",
      "137 Train Loss 27.795395 Test MSE 31.381097538070527 Test RE 0.09430250624956524 Lambda1 0.9952082\n",
      "138 Train Loss 27.764605 Test MSE 31.30403652240981 Test RE 0.09418664806900645 Lambda1 0.99384975\n",
      "139 Train Loss 27.741049 Test MSE 31.269461083583057 Test RE 0.09413461892573592 Lambda1 0.99255604\n",
      "140 Train Loss 27.720285 Test MSE 31.240265415607432 Test RE 0.094090662861303 Lambda1 0.9961125\n",
      "141 Train Loss 27.698639 Test MSE 31.16884926480086 Test RE 0.09398305433796397 Lambda1 1.002709\n",
      "142 Train Loss 27.670412 Test MSE 31.132395304513 Test RE 0.09392807866601838 Lambda1 1.0034947\n",
      "143 Train Loss 27.650793 Test MSE 31.095600040244115 Test RE 0.0938725556290438 Lambda1 1.0032626\n",
      "144 Train Loss 27.613968 Test MSE 31.066820052642328 Test RE 0.09382910452358943 Lambda1 1.0024681\n",
      "145 Train Loss 27.596968 Test MSE 31.031573941761682 Test RE 0.09377586364554844 Lambda1 1.0048563\n",
      "146 Train Loss 27.55222 Test MSE 30.98106206164761 Test RE 0.09369951037468384 Lambda1 1.0065136\n",
      "147 Train Loss 27.49667 Test MSE 30.91414925489201 Test RE 0.09359826971680278 Lambda1 1.008765\n",
      "148 Train Loss 27.459244 Test MSE 30.838732006697334 Test RE 0.09348403020911407 Lambda1 1.009716\n",
      "149 Train Loss 27.4394 Test MSE 30.79730005705536 Test RE 0.0934212110337187 Lambda1 1.0112317\n",
      "150 Train Loss 27.38676 Test MSE 30.7487811587335 Test RE 0.09334759287773231 Lambda1 1.0121173\n",
      "151 Train Loss 27.315826 Test MSE 30.655094043459528 Test RE 0.09320527604309156 Lambda1 1.0107574\n",
      "152 Train Loss 27.279505 Test MSE 30.63279156858087 Test RE 0.09317136509598456 Lambda1 1.0059537\n",
      "153 Train Loss 27.257475 Test MSE 30.641190348599256 Test RE 0.09318413690242572 Lambda1 1.002824\n",
      "154 Train Loss 27.225859 Test MSE 30.584553537343943 Test RE 0.09309797684536482 Lambda1 1.0055368\n",
      "155 Train Loss 27.187145 Test MSE 30.441156282270622 Test RE 0.0928794730973633 Lambda1 1.0038668\n",
      "156 Train Loss 27.134789 Test MSE 30.409991621343856 Test RE 0.09283191743822469 Lambda1 0.9979671\n",
      "157 Train Loss 27.077053 Test MSE 30.423563367620872 Test RE 0.09285263021425853 Lambda1 0.9901452\n",
      "158 Train Loss 26.997633 Test MSE 30.251844568066968 Test RE 0.09259021675948138 Lambda1 0.9901578\n",
      "159 Train Loss 26.959335 Test MSE 30.149190310676204 Test RE 0.09243298904355471 Lambda1 0.98866755\n",
      "160 Train Loss 26.932339 Test MSE 30.160057008591735 Test RE 0.09244964539265417 Lambda1 0.9905656\n",
      "161 Train Loss 26.87076 Test MSE 30.025409117160866 Test RE 0.09224304640595678 Lambda1 0.9905795\n",
      "162 Train Loss 26.822935 Test MSE 29.978109285807097 Test RE 0.09217036129789156 Lambda1 0.98553234\n",
      "163 Train Loss 26.78661 Test MSE 29.94640219289534 Test RE 0.09212160526491367 Lambda1 0.9838386\n",
      "164 Train Loss 26.752174 Test MSE 29.925937806549406 Test RE 0.09209012344796803 Lambda1 0.9792256\n",
      "165 Train Loss 26.650604 Test MSE 29.90077455340512 Test RE 0.0920513982719759 Lambda1 0.9756197\n",
      "166 Train Loss 26.59511 Test MSE 29.80650040514677 Test RE 0.09190616928758248 Lambda1 0.96956146\n",
      "167 Train Loss 26.560303 Test MSE 29.78906087907991 Test RE 0.09187927860051871 Lambda1 0.97158223\n",
      "168 Train Loss 26.536917 Test MSE 29.767126759646757 Test RE 0.09184544634600218 Lambda1 0.9774124\n",
      "169 Train Loss 26.469389 Test MSE 29.650732875792322 Test RE 0.0916657058071171 Lambda1 0.97371924\n",
      "170 Train Loss 26.43906 Test MSE 29.60251373986169 Test RE 0.0915911403719401 Lambda1 0.9757391\n",
      "171 Train Loss 26.408436 Test MSE 29.572170886711625 Test RE 0.09154418744976556 Lambda1 0.97440493\n",
      "172 Train Loss 26.372997 Test MSE 29.573583653711907 Test RE 0.09154637411815363 Lambda1 0.97279316\n",
      "173 Train Loss 26.344292 Test MSE 29.537783009569463 Test RE 0.09149094607667964 Lambda1 0.971252\n",
      "174 Train Loss 26.319855 Test MSE 29.535282372765185 Test RE 0.09148707323237487 Lambda1 0.96853006\n",
      "175 Train Loss 26.298115 Test MSE 29.523222676917708 Test RE 0.09146839355783629 Lambda1 0.96540505\n",
      "176 Train Loss 26.283564 Test MSE 29.508980846906756 Test RE 0.09144632898742057 Lambda1 0.9598731\n",
      "177 Train Loss 26.2404 Test MSE 29.45099561790433 Test RE 0.091356438662296 Lambda1 0.9597225\n",
      "178 Train Loss 26.215906 Test MSE 29.43864348527476 Test RE 0.09133727861066777 Lambda1 0.9598824\n",
      "179 Train Loss 26.18285 Test MSE 29.348509558427597 Test RE 0.09119734521324947 Lambda1 0.96041965\n",
      "180 Train Loss 26.162495 Test MSE 29.278930553747667 Test RE 0.09108917641734567 Lambda1 0.9627117\n",
      "181 Train Loss 26.149136 Test MSE 29.282980110045486 Test RE 0.09109547545196442 Lambda1 0.9599065\n",
      "182 Train Loss 26.117678 Test MSE 29.223202658662917 Test RE 0.09100244808206369 Lambda1 0.9573079\n",
      "183 Train Loss 26.046751 Test MSE 29.161459164774982 Test RE 0.09090626115499717 Lambda1 0.96022093\n",
      "184 Train Loss 26.00355 Test MSE 29.11195746078747 Test RE 0.09082907149644566 Lambda1 0.9672591\n",
      "185 Train Loss 25.982037 Test MSE 29.10172737855021 Test RE 0.09081311120732902 Lambda1 0.9697963\n",
      "186 Train Loss 25.94662 Test MSE 29.139624601525476 Test RE 0.09087222187418217 Lambda1 0.9690974\n",
      "187 Train Loss 25.92354 Test MSE 29.09781071649905 Test RE 0.09080699995094117 Lambda1 0.9707792\n",
      "188 Train Loss 25.908709 Test MSE 29.070432849311665 Test RE 0.09076427015467635 Lambda1 0.97097766\n",
      "189 Train Loss 25.880491 Test MSE 29.02641389223733 Test RE 0.09069552569986651 Lambda1 0.9734319\n",
      "190 Train Loss 25.867775 Test MSE 29.02928556276959 Test RE 0.09070001197964135 Lambda1 0.9724388\n",
      "191 Train Loss 25.844522 Test MSE 28.97522345968444 Test RE 0.09061551595462453 Lambda1 0.96630853\n",
      "192 Train Loss 25.80861 Test MSE 28.892774464574305 Test RE 0.09048650089090424 Lambda1 0.95854515\n",
      "193 Train Loss 25.762905 Test MSE 28.88307495077776 Test RE 0.09047131112888078 Lambda1 0.95980096\n",
      "194 Train Loss 25.6853 Test MSE 28.8153267181769 Test RE 0.09036514393803201 Lambda1 0.9666123\n",
      "195 Train Loss 25.63701 Test MSE 28.771613692182484 Test RE 0.0902965756869017 Lambda1 0.9663816\n",
      "196 Train Loss 25.605629 Test MSE 28.705454173634887 Test RE 0.09019269871564897 Lambda1 0.9662126\n",
      "197 Train Loss 25.582523 Test MSE 28.682150314397518 Test RE 0.09015608085379996 Lambda1 0.9679623\n",
      "198 Train Loss 25.566442 Test MSE 28.656106323072514 Test RE 0.09011513975597803 Lambda1 0.9685684\n",
      "199 Train Loss 25.550014 Test MSE 28.641462519432757 Test RE 0.09009211155688433 Lambda1 0.9715621\n",
      "Training time: 356.69\n",
      "Training time: 356.69\n",
      "inv_HT_stan_late_Try\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 818.4206 Test MSE 820.7689099509305 Test RE 0.48228040763204894 Lambda1 0.1694949\n",
      "1 Train Loss 692.52234 Test MSE 675.5019797279045 Test RE 0.4375246073404294 Lambda1 0.17181452\n",
      "2 Train Loss 531.79297 Test MSE 514.6780806274785 Test RE 0.38190646210218093 Lambda1 0.16916837\n",
      "3 Train Loss 385.4505 Test MSE 357.3268127238972 Test RE 0.3182159714936848 Lambda1 0.14847389\n",
      "4 Train Loss 278.48132 Test MSE 257.8933014707851 Test RE 0.27033931037106407 Lambda1 0.1367978\n",
      "5 Train Loss 244.30916 Test MSE 234.29034273839397 Test RE 0.2576714874355179 Lambda1 0.13017623\n",
      "6 Train Loss 222.38123 Test MSE 218.04682811328388 Test RE 0.2485787829455205 Lambda1 0.13680278\n",
      "7 Train Loss 201.74397 Test MSE 197.16199100566783 Test RE 0.23637457892557862 Lambda1 0.16677374\n",
      "8 Train Loss 189.03546 Test MSE 181.47092393483942 Test RE 0.22677370513424802 Lambda1 0.19552815\n",
      "9 Train Loss 171.44748 Test MSE 160.95798412038764 Test RE 0.21357254984218474 Lambda1 0.23767446\n",
      "10 Train Loss 161.14908 Test MSE 154.38176008445447 Test RE 0.20916410886098627 Lambda1 0.25633398\n",
      "11 Train Loss 149.91791 Test MSE 142.80245622184748 Test RE 0.20116712506843623 Lambda1 0.27401358\n",
      "12 Train Loss 139.22562 Test MSE 131.91744852655486 Test RE 0.19334827067250668 Lambda1 0.2929569\n",
      "13 Train Loss 129.31616 Test MSE 117.88122542638789 Test RE 0.18277276905955248 Lambda1 0.33032286\n",
      "14 Train Loss 118.62857 Test MSE 106.48105892041721 Test RE 0.17371021240992038 Lambda1 0.3581883\n",
      "15 Train Loss 110.48252 Test MSE 100.0828386687521 Test RE 0.16841042740765583 Lambda1 0.37837157\n",
      "16 Train Loss 104.65842 Test MSE 95.04344350870088 Test RE 0.164115746310093 Lambda1 0.39667988\n",
      "17 Train Loss 99.52527 Test MSE 89.1239191134078 Test RE 0.15892283674605362 Lambda1 0.41401878\n",
      "18 Train Loss 95.64403 Test MSE 85.90975753048824 Test RE 0.15603082948491223 Lambda1 0.42408592\n",
      "19 Train Loss 92.00655 Test MSE 84.28903893853054 Test RE 0.154552032803294 Lambda1 0.42876595\n",
      "20 Train Loss 88.60932 Test MSE 81.26117060303204 Test RE 0.15175070163302093 Lambda1 0.43561512\n",
      "21 Train Loss 84.18248 Test MSE 76.87770411606694 Test RE 0.1476010249117311 Lambda1 0.45277673\n",
      "22 Train Loss 80.666016 Test MSE 74.37233060501345 Test RE 0.1451760133726693 Lambda1 0.4648138\n",
      "23 Train Loss 76.82377 Test MSE 71.05569226324462 Test RE 0.14190202991385448 Lambda1 0.48441988\n",
      "24 Train Loss 74.16815 Test MSE 69.99860530838438 Test RE 0.14084254500792054 Lambda1 0.49416676\n",
      "25 Train Loss 71.45068 Test MSE 67.99873747366337 Test RE 0.1388160221416342 Lambda1 0.50526094\n",
      "26 Train Loss 68.203476 Test MSE 64.25430746473235 Test RE 0.13493987210109157 Lambda1 0.52268296\n",
      "27 Train Loss 65.58164 Test MSE 61.92055612130044 Test RE 0.13246666260026105 Lambda1 0.5354436\n",
      "28 Train Loss 63.854424 Test MSE 60.934692704901586 Test RE 0.13140790251137097 Lambda1 0.54304415\n",
      "29 Train Loss 61.88002 Test MSE 57.35666689833319 Test RE 0.12749146849002005 Lambda1 0.5648622\n",
      "30 Train Loss 59.17963 Test MSE 56.46738731542526 Test RE 0.1264992694952054 Lambda1 0.57267535\n",
      "31 Train Loss 57.61995 Test MSE 55.41001100378098 Test RE 0.12530929564004456 Lambda1 0.5838573\n",
      "32 Train Loss 55.903603 Test MSE 53.69249206692055 Test RE 0.12335193070356312 Lambda1 0.598843\n",
      "33 Train Loss 53.35853 Test MSE 52.75084273931272 Test RE 0.12226548410164435 Lambda1 0.6056215\n",
      "34 Train Loss 51.47832 Test MSE 52.23653504254816 Test RE 0.12166799504554741 Lambda1 0.61400557\n",
      "35 Train Loss 50.38815 Test MSE 51.58320065840239 Test RE 0.12090473614335656 Lambda1 0.62174785\n",
      "36 Train Loss 48.9645 Test MSE 50.680741418102116 Test RE 0.11984244213555119 Lambda1 0.63376164\n",
      "37 Train Loss 48.158367 Test MSE 49.71362306140654 Test RE 0.11869348409191682 Lambda1 0.6418986\n",
      "38 Train Loss 47.49567 Test MSE 48.630802455979804 Test RE 0.11739372641266824 Lambda1 0.6524207\n",
      "39 Train Loss 46.45039 Test MSE 47.93538706233046 Test RE 0.1165513450671204 Lambda1 0.6606717\n",
      "40 Train Loss 45.87875 Test MSE 47.51253458521923 Test RE 0.116036139106087 Lambda1 0.668463\n",
      "41 Train Loss 45.472687 Test MSE 47.319003779383735 Test RE 0.11579957540512535 Lambda1 0.669874\n",
      "42 Train Loss 44.49136 Test MSE 46.124314750354536 Test RE 0.11432840223555019 Lambda1 0.6906023\n",
      "43 Train Loss 43.800007 Test MSE 45.884438157280265 Test RE 0.11403072347986207 Lambda1 0.69708145\n",
      "44 Train Loss 43.36921 Test MSE 45.89910972368955 Test RE 0.11404895270620165 Lambda1 0.69600475\n",
      "45 Train Loss 43.048225 Test MSE 45.798037430546124 Test RE 0.11392331255633442 Lambda1 0.69583106\n",
      "46 Train Loss 42.37737 Test MSE 45.147204551912075 Test RE 0.1131109377507722 Lambda1 0.70482844\n",
      "47 Train Loss 42.053097 Test MSE 44.983483141542195 Test RE 0.11290565924076533 Lambda1 0.7074716\n",
      "48 Train Loss 41.56826 Test MSE 44.58017442502514 Test RE 0.11239838013862698 Lambda1 0.71440697\n",
      "49 Train Loss 41.20033 Test MSE 43.95461882348216 Test RE 0.11160699878798205 Lambda1 0.7222277\n",
      "50 Train Loss 40.685417 Test MSE 43.10698295513014 Test RE 0.11052562629488423 Lambda1 0.7344995\n",
      "51 Train Loss 40.225357 Test MSE 42.91614338638078 Test RE 0.11028070008873625 Lambda1 0.7344092\n",
      "52 Train Loss 39.807312 Test MSE 42.66957146358114 Test RE 0.10996343842012803 Lambda1 0.73921335\n",
      "53 Train Loss 39.329147 Test MSE 42.17419135471278 Test RE 0.10932325469806661 Lambda1 0.74428093\n",
      "54 Train Loss 38.890835 Test MSE 41.715133084411626 Test RE 0.10872664505879287 Lambda1 0.7482189\n",
      "55 Train Loss 38.74325 Test MSE 41.55628791011304 Test RE 0.10851943997436587 Lambda1 0.7506326\n",
      "56 Train Loss 38.193634 Test MSE 40.999940090011215 Test RE 0.1077905732757784 Lambda1 0.758781\n",
      "57 Train Loss 37.860985 Test MSE 40.86370394337556 Test RE 0.107611338969958 Lambda1 0.76174855\n",
      "58 Train Loss 37.526466 Test MSE 40.81707606643729 Test RE 0.10754992602922461 Lambda1 0.76227975\n",
      "59 Train Loss 37.14492 Test MSE 40.67588916707112 Test RE 0.10736375646756159 Lambda1 0.76725364\n",
      "60 Train Loss 36.95223 Test MSE 40.536429933424515 Test RE 0.1071795475498073 Lambda1 0.768487\n",
      "61 Train Loss 36.713398 Test MSE 40.04080034276058 Test RE 0.10652230255651848 Lambda1 0.78128207\n",
      "62 Train Loss 36.496323 Test MSE 39.549552524978935 Test RE 0.10586684185263508 Lambda1 0.79616493\n",
      "63 Train Loss 35.93389 Test MSE 39.1299026382092 Test RE 0.1053036813919878 Lambda1 0.8110976\n",
      "64 Train Loss 35.631027 Test MSE 39.10844242144163 Test RE 0.10527480130788783 Lambda1 0.81365484\n",
      "65 Train Loss 35.357445 Test MSE 38.62114084004903 Test RE 0.10461686938701843 Lambda1 0.81919724\n",
      "66 Train Loss 35.02943 Test MSE 38.46491941142462 Test RE 0.10440506884081785 Lambda1 0.823345\n",
      "67 Train Loss 34.846428 Test MSE 38.48267189128677 Test RE 0.10442915877922215 Lambda1 0.819484\n",
      "68 Train Loss 34.735535 Test MSE 38.43008685469052 Test RE 0.1043577852479998 Lambda1 0.8231938\n",
      "69 Train Loss 34.534065 Test MSE 38.29217644884731 Test RE 0.10417036751441658 Lambda1 0.8266684\n",
      "70 Train Loss 34.38642 Test MSE 38.18806392034335 Test RE 0.10402865681376734 Lambda1 0.82660127\n",
      "71 Train Loss 34.217205 Test MSE 38.05029230825179 Test RE 0.10384083443857313 Lambda1 0.8296213\n",
      "72 Train Loss 34.08593 Test MSE 37.802322167847294 Test RE 0.10350192094239614 Lambda1 0.8366603\n",
      "73 Train Loss 33.929478 Test MSE 37.539320308961486 Test RE 0.10314124588421875 Lambda1 0.84712917\n",
      "74 Train Loss 33.769337 Test MSE 37.386022165123244 Test RE 0.10293043310776226 Lambda1 0.85167783\n",
      "75 Train Loss 33.569992 Test MSE 37.26705125026597 Test RE 0.10276652846111059 Lambda1 0.8534862\n",
      "76 Train Loss 33.438538 Test MSE 37.20595392636736 Test RE 0.10268225381100497 Lambda1 0.85625374\n",
      "77 Train Loss 33.325733 Test MSE 37.05438642591359 Test RE 0.10247288980486538 Lambda1 0.8632056\n",
      "78 Train Loss 33.209602 Test MSE 37.0279789134385 Test RE 0.10243636867094529 Lambda1 0.86520135\n",
      "79 Train Loss 33.078106 Test MSE 36.9788956700647 Test RE 0.10236845277760905 Lambda1 0.8649038\n",
      "80 Train Loss 32.977295 Test MSE 36.771980477118284 Test RE 0.10208164997663277 Lambda1 0.8679169\n",
      "81 Train Loss 32.83501 Test MSE 36.409426784811245 Test RE 0.10157716590410845 Lambda1 0.876183\n",
      "82 Train Loss 32.66857 Test MSE 36.17366949976117 Test RE 0.10124776702133917 Lambda1 0.880808\n",
      "83 Train Loss 32.50985 Test MSE 36.13849568035676 Test RE 0.10119853042394998 Lambda1 0.88320136\n",
      "84 Train Loss 32.37223 Test MSE 36.00288393635127 Test RE 0.1010084753607181 Lambda1 0.886397\n",
      "85 Train Loss 32.264885 Test MSE 35.91561469085704 Test RE 0.10088598126169576 Lambda1 0.8894887\n",
      "86 Train Loss 32.112064 Test MSE 35.885325650819716 Test RE 0.100843431745561 Lambda1 0.890833\n",
      "87 Train Loss 32.04904 Test MSE 35.848493836957715 Test RE 0.1007916668530589 Lambda1 0.889992\n",
      "88 Train Loss 31.99285 Test MSE 35.743012323211715 Test RE 0.10064327163898619 Lambda1 0.89186794\n",
      "89 Train Loss 31.876963 Test MSE 35.514662199029004 Test RE 0.10032126845960278 Lambda1 0.8988049\n",
      "90 Train Loss 31.747698 Test MSE 35.21141559303812 Test RE 0.0998920470935202 Lambda1 0.908405\n",
      "91 Train Loss 31.67044 Test MSE 35.2019204444332 Test RE 0.09987857768641245 Lambda1 0.90960884\n",
      "92 Train Loss 31.539036 Test MSE 35.0215420168808 Test RE 0.0996223547476402 Lambda1 0.9146679\n",
      "93 Train Loss 31.413095 Test MSE 34.886431411984915 Test RE 0.09943000108611952 Lambda1 0.9190019\n",
      "94 Train Loss 31.264603 Test MSE 34.71898835078154 Test RE 0.09919109890558021 Lambda1 0.92173904\n",
      "95 Train Loss 31.17693 Test MSE 34.62405170081014 Test RE 0.09905539050298859 Lambda1 0.92484957\n",
      "96 Train Loss 31.133377 Test MSE 34.63984337056129 Test RE 0.09907797699443295 Lambda1 0.92500716\n",
      "97 Train Loss 31.053553 Test MSE 34.4406683076301 Test RE 0.09879272294765441 Lambda1 0.9343075\n",
      "98 Train Loss 30.96187 Test MSE 34.2338063580928 Test RE 0.09849558535141165 Lambda1 0.9476036\n",
      "99 Train Loss 30.889215 Test MSE 34.11189692225054 Test RE 0.09832005344944535 Lambda1 0.95303184\n",
      "100 Train Loss 30.799894 Test MSE 33.969758208354726 Test RE 0.0981149978003092 Lambda1 0.9625649\n",
      "101 Train Loss 30.727514 Test MSE 33.87032150858542 Test RE 0.0979712908355921 Lambda1 0.96524554\n",
      "102 Train Loss 30.649584 Test MSE 33.810878400659 Test RE 0.09788528227136695 Lambda1 0.9693002\n",
      "103 Train Loss 30.557499 Test MSE 33.798365874040066 Test RE 0.09786716819692007 Lambda1 0.96716833\n",
      "104 Train Loss 30.510231 Test MSE 33.85124745305877 Test RE 0.09794370070767117 Lambda1 0.9645276\n",
      "105 Train Loss 30.43639 Test MSE 33.87292850518434 Test RE 0.09797506118520899 Lambda1 0.9633084\n",
      "106 Train Loss 30.304705 Test MSE 33.76965165376039 Test RE 0.09782558665191428 Lambda1 0.9695729\n",
      "107 Train Loss 30.236214 Test MSE 33.6675703230315 Test RE 0.09767761763641498 Lambda1 0.97143817\n",
      "108 Train Loss 30.15559 Test MSE 33.60456873047825 Test RE 0.09758618355008122 Lambda1 0.9776594\n",
      "109 Train Loss 30.12762 Test MSE 33.50018697431219 Test RE 0.09743450566876512 Lambda1 0.98313206\n",
      "110 Train Loss 30.06521 Test MSE 33.42573443206887 Test RE 0.0973261737109043 Lambda1 0.98735505\n",
      "111 Train Loss 29.956772 Test MSE 33.42275805892483 Test RE 0.09732184044099375 Lambda1 0.9871115\n",
      "112 Train Loss 29.869715 Test MSE 33.38474418317388 Test RE 0.09726647946680639 Lambda1 0.9828095\n",
      "113 Train Loss 29.768898 Test MSE 33.286875242223 Test RE 0.0971238042055274 Lambda1 0.9836157\n",
      "114 Train Loss 29.72029 Test MSE 33.22132178887086 Test RE 0.09702812177176688 Lambda1 0.98735857\n",
      "115 Train Loss 29.671452 Test MSE 33.16802032981126 Test RE 0.09695025285689361 Lambda1 0.9872047\n",
      "116 Train Loss 29.605946 Test MSE 33.09124317677834 Test RE 0.09683797788691473 Lambda1 0.98636204\n",
      "117 Train Loss 29.535265 Test MSE 33.04033768713706 Test RE 0.09676346448839518 Lambda1 0.9881025\n",
      "118 Train Loss 29.478813 Test MSE 32.96906408297274 Test RE 0.09665904055249226 Lambda1 0.99161214\n",
      "119 Train Loss 29.42216 Test MSE 32.95943867354437 Test RE 0.09664492958539285 Lambda1 0.9896706\n",
      "120 Train Loss 29.39236 Test MSE 32.94522991597078 Test RE 0.09662409560795135 Lambda1 0.98935026\n",
      "121 Train Loss 29.349628 Test MSE 32.959399794987284 Test RE 0.09664487258477829 Lambda1 0.9905814\n",
      "122 Train Loss 29.297392 Test MSE 32.91417960048111 Test RE 0.09657855159589197 Lambda1 0.9921302\n",
      "123 Train Loss 29.230495 Test MSE 32.79479391041826 Test RE 0.09640323853006284 Lambda1 0.99059856\n",
      "124 Train Loss 29.194513 Test MSE 32.75567243318423 Test RE 0.09634572082563299 Lambda1 0.9869029\n",
      "125 Train Loss 29.152819 Test MSE 32.71145010402769 Test RE 0.09628066230495418 Lambda1 0.9846998\n",
      "126 Train Loss 29.120996 Test MSE 32.62788588315555 Test RE 0.0961576050362422 Lambda1 0.9848621\n",
      "127 Train Loss 29.098543 Test MSE 32.571889573417884 Test RE 0.09607505627319841 Lambda1 0.986074\n",
      "128 Train Loss 29.063793 Test MSE 32.524475226929056 Test RE 0.0960051033927274 Lambda1 0.98853797\n",
      "129 Train Loss 28.99063 Test MSE 32.475065745283736 Test RE 0.09593215271061019 Lambda1 0.9927178\n",
      "130 Train Loss 28.947348 Test MSE 32.43844516054624 Test RE 0.09587804839408245 Lambda1 0.99463516\n",
      "131 Train Loss 28.880001 Test MSE 32.41407007187205 Test RE 0.09584201899883373 Lambda1 0.9939365\n",
      "132 Train Loss 28.779911 Test MSE 32.32187615419953 Test RE 0.0957056223177181 Lambda1 0.9940559\n",
      "133 Train Loss 28.71261 Test MSE 32.27128509529658 Test RE 0.09563069249488372 Lambda1 0.99543864\n",
      "134 Train Loss 28.691017 Test MSE 32.26505894202239 Test RE 0.09562146695476917 Lambda1 0.9976311\n",
      "135 Train Loss 28.666576 Test MSE 32.21591031300028 Test RE 0.09554861019815282 Lambda1 0.99737984\n",
      "136 Train Loss 28.640028 Test MSE 32.14521009305395 Test RE 0.09544370833413438 Lambda1 0.9958981\n",
      "137 Train Loss 28.617641 Test MSE 32.10496250541901 Test RE 0.09538393920892825 Lambda1 0.997938\n",
      "138 Train Loss 28.570398 Test MSE 31.907852525960205 Test RE 0.09509068123003851 Lambda1 1.0022587\n",
      "139 Train Loss 28.49748 Test MSE 31.744663608052175 Test RE 0.0948472045313778 Lambda1 1.0055447\n",
      "140 Train Loss 28.403795 Test MSE 31.589175962933428 Test RE 0.09461463518230723 Lambda1 1.0024421\n",
      "141 Train Loss 28.3117 Test MSE 31.44066057646286 Test RE 0.0943919594785879 Lambda1 1.0065349\n",
      "142 Train Loss 28.235872 Test MSE 31.46681950865763 Test RE 0.09443121882362009 Lambda1 1.0101113\n",
      "143 Train Loss 28.208477 Test MSE 31.51734306731741 Test RE 0.0945069984501857 Lambda1 1.0102943\n",
      "144 Train Loss 28.193916 Test MSE 31.465666293326976 Test RE 0.0944294884211086 Lambda1 1.012407\n",
      "145 Train Loss 28.130117 Test MSE 31.333504536420328 Test RE 0.09423096888037173 Lambda1 1.0123779\n",
      "146 Train Loss 28.088102 Test MSE 31.267559527339518 Test RE 0.09413175662834417 Lambda1 1.0124094\n",
      "147 Train Loss 28.062508 Test MSE 31.219320651667065 Test RE 0.09405911644007439 Lambda1 1.0135034\n",
      "148 Train Loss 28.031837 Test MSE 31.166110884262668 Test RE 0.09397892574441717 Lambda1 1.0163571\n",
      "149 Train Loss 28.003391 Test MSE 31.107266822555708 Test RE 0.09389016403607 Lambda1 1.0156446\n",
      "150 Train Loss 27.989088 Test MSE 31.08268422839336 Test RE 0.0938530582372861 Lambda1 1.0152729\n",
      "151 Train Loss 27.959572 Test MSE 31.029447763567138 Test RE 0.09377265098846195 Lambda1 1.0146145\n",
      "152 Train Loss 27.935194 Test MSE 30.976636908096143 Test RE 0.09369281839092719 Lambda1 1.01456\n",
      "153 Train Loss 27.890614 Test MSE 30.91168170963374 Test RE 0.093594534168824 Lambda1 1.0141932\n",
      "154 Train Loss 27.834219 Test MSE 30.880623843878496 Test RE 0.09354750377720238 Lambda1 1.0172408\n",
      "155 Train Loss 27.80773 Test MSE 30.80234995734934 Test RE 0.09342886995938061 Lambda1 1.0188247\n",
      "156 Train Loss 27.784538 Test MSE 30.788272439696804 Test RE 0.09340751774310155 Lambda1 1.019806\n",
      "157 Train Loss 27.747509 Test MSE 30.775635059481697 Test RE 0.09338834571138815 Lambda1 1.0165974\n",
      "158 Train Loss 27.71202 Test MSE 30.716950193011684 Test RE 0.09329926391484254 Lambda1 1.015857\n",
      "159 Train Loss 27.679617 Test MSE 30.65764000831833 Test RE 0.09320914640196776 Lambda1 1.0174279\n",
      "160 Train Loss 27.636824 Test MSE 30.624439568240376 Test RE 0.09315866268980436 Lambda1 1.01379\n",
      "161 Train Loss 27.615458 Test MSE 30.60992689725646 Test RE 0.09313658651059707 Lambda1 1.0128664\n",
      "162 Train Loss 27.583796 Test MSE 30.622670145375455 Test RE 0.09315597138424557 Lambda1 1.0125868\n",
      "163 Train Loss 27.539213 Test MSE 30.668404697878625 Test RE 0.09322550903561012 Lambda1 1.00985\n",
      "164 Train Loss 27.490906 Test MSE 30.64524375851769 Test RE 0.09319030019128147 Lambda1 1.0087042\n",
      "165 Train Loss 27.453794 Test MSE 30.651821860182082 Test RE 0.09320030145576351 Lambda1 1.0082488\n",
      "166 Train Loss 27.423252 Test MSE 30.60042814815467 Test RE 0.09312213450432885 Lambda1 1.0075881\n",
      "167 Train Loss 27.399424 Test MSE 30.549071112127557 Test RE 0.09304395773667011 Lambda1 1.0062934\n",
      "168 Train Loss 27.357004 Test MSE 30.48509823452476 Test RE 0.09294648490122843 Lambda1 1.0050616\n",
      "169 Train Loss 27.34027 Test MSE 30.483998720016192 Test RE 0.09294480872276603 Lambda1 1.0027016\n",
      "170 Train Loss 27.31378 Test MSE 30.436448617370175 Test RE 0.09287229100583048 Lambda1 1.0011998\n",
      "171 Train Loss 27.28479 Test MSE 30.350924885797216 Test RE 0.09274171775021933 Lambda1 1.0033273\n",
      "172 Train Loss 27.24242 Test MSE 30.32309650241542 Test RE 0.09269919113948398 Lambda1 1.001819\n",
      "173 Train Loss 27.221931 Test MSE 30.316445183577873 Test RE 0.09268902387818688 Lambda1 1.0025828\n",
      "174 Train Loss 27.203812 Test MSE 30.297227375581762 Test RE 0.09265964110863031 Lambda1 1.0050766\n",
      "175 Train Loss 27.19248 Test MSE 30.321990125331386 Test RE 0.09269749999959159 Lambda1 1.0055735\n",
      "176 Train Loss 27.174688 Test MSE 30.31168264559864 Test RE 0.09268174313778532 Lambda1 1.0044609\n",
      "177 Train Loss 27.14968 Test MSE 30.265304124187725 Test RE 0.09261081194385594 Lambda1 1.0035573\n",
      "178 Train Loss 27.11789 Test MSE 30.232584317139903 Test RE 0.09256073765223445 Lambda1 1.0014247\n",
      "179 Train Loss 27.097172 Test MSE 30.226954676466416 Test RE 0.09255211933584889 Lambda1 0.99881023\n",
      "180 Train Loss 27.084995 Test MSE 30.199991724862823 Test RE 0.09251083110334793 Lambda1 0.9969016\n",
      "181 Train Loss 27.074718 Test MSE 30.19080294318701 Test RE 0.09249675615743197 Lambda1 0.99635285\n",
      "182 Train Loss 27.05307 Test MSE 30.203653019753308 Test RE 0.09251643870700363 Lambda1 0.99791616\n",
      "183 Train Loss 27.04039 Test MSE 30.178961647389787 Test RE 0.09247861505549415 Lambda1 0.9975908\n",
      "184 Train Loss 27.017462 Test MSE 30.181558932533754 Test RE 0.092482594452932 Lambda1 0.99875903\n",
      "185 Train Loss 26.999067 Test MSE 30.18337892034452 Test RE 0.09248538282210851 Lambda1 1.0002131\n",
      "186 Train Loss 26.937666 Test MSE 30.14155196402454 Test RE 0.09242127927739602 Lambda1 0.9972271\n",
      "187 Train Loss 26.872446 Test MSE 30.05657937345315 Test RE 0.09229091408955722 Lambda1 0.9962063\n",
      "188 Train Loss 26.818295 Test MSE 29.999535771407945 Test RE 0.09220329423146745 Lambda1 0.99398243\n",
      "189 Train Loss 26.790316 Test MSE 30.02672982911639 Test RE 0.09224507510694928 Lambda1 0.99247694\n",
      "190 Train Loss 26.758017 Test MSE 30.014924081275595 Test RE 0.09222693911332834 Lambda1 0.99185187\n",
      "191 Train Loss 26.736555 Test MSE 30.020154628857085 Test RE 0.09223497472217446 Lambda1 0.99078554\n",
      "192 Train Loss 26.69647 Test MSE 29.97137085587876 Test RE 0.09216000176486351 Lambda1 0.99189126\n",
      "193 Train Loss 26.666142 Test MSE 29.89928968645597 Test RE 0.09204911261586952 Lambda1 0.99219066\n",
      "194 Train Loss 26.651562 Test MSE 29.868768694931088 Test RE 0.09200211906613198 Lambda1 0.99068344\n",
      "195 Train Loss 26.62341 Test MSE 29.84497317307863 Test RE 0.09196546414648264 Lambda1 0.98899823\n",
      "196 Train Loss 26.577522 Test MSE 29.83021974426696 Test RE 0.09194273044118945 Lambda1 0.98721516\n",
      "197 Train Loss 26.56312 Test MSE 29.780357447155914 Test RE 0.09186585549417128 Lambda1 0.98614573\n",
      "198 Train Loss 26.550774 Test MSE 29.739981224033393 Test RE 0.0918035584856332 Lambda1 0.9852564\n",
      "199 Train Loss 26.538486 Test MSE 29.735474007021157 Test RE 0.0917966016177703 Lambda1 0.9853352\n",
      "Training time: 345.70\n",
      "Training time: 345.70\n",
      "inv_HT_stan_late_Try\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 816.8841 Test MSE 820.3290252169577 Test RE 0.482151153088779 Lambda1 0.14641552\n",
      "1 Train Loss 536.9461 Test MSE 519.2356638556362 Test RE 0.38359366643321224 Lambda1 0.0768834\n",
      "2 Train Loss 298.6255 Test MSE 299.2673387880224 Test RE 0.2912184134351314 Lambda1 0.017304864\n",
      "3 Train Loss 274.169 Test MSE 280.87643492394096 Test RE 0.2821284109057025 Lambda1 0.0074888743\n",
      "4 Train Loss 267.53278 Test MSE 273.5132997918213 Test RE 0.2784058746999029 Lambda1 0.01572893\n",
      "5 Train Loss 251.68944 Test MSE 253.4347912383664 Test RE 0.2679922825313755 Lambda1 0.057793044\n",
      "6 Train Loss 233.37265 Test MSE 232.14624898058594 Test RE 0.2564897451232852 Lambda1 0.10052402\n",
      "7 Train Loss 202.61075 Test MSE 195.00055127620848 Test RE 0.23507534935305888 Lambda1 0.17960067\n",
      "8 Train Loss 182.32649 Test MSE 171.36180743704094 Test RE 0.22036681143422862 Lambda1 0.22270529\n",
      "9 Train Loss 158.04675 Test MSE 143.49816355305896 Test RE 0.2016565543279111 Lambda1 0.28788474\n",
      "10 Train Loss 136.35141 Test MSE 127.37349267008949 Test RE 0.18998910612371867 Lambda1 0.33606008\n",
      "11 Train Loss 124.2629 Test MSE 116.25110119876645 Test RE 0.18150463018450874 Lambda1 0.36921555\n",
      "12 Train Loss 111.29205 Test MSE 100.64226074969827 Test RE 0.1688804441823916 Lambda1 0.4218246\n",
      "13 Train Loss 103.00228 Test MSE 91.14040176477782 Test RE 0.16071064347573272 Lambda1 0.45203814\n",
      "14 Train Loss 93.846405 Test MSE 84.10591996277702 Test RE 0.15438405842421404 Lambda1 0.45856836\n",
      "15 Train Loss 87.48743 Test MSE 82.09614732521803 Test RE 0.15252834534051438 Lambda1 0.46485\n",
      "16 Train Loss 81.940285 Test MSE 77.82210596437979 Test RE 0.14850485778729103 Lambda1 0.47155398\n",
      "17 Train Loss 78.22594 Test MSE 74.42075641837363 Test RE 0.14522326967380011 Lambda1 0.4765965\n",
      "18 Train Loss 73.58239 Test MSE 68.96810544286161 Test RE 0.13980197882353781 Lambda1 0.4878645\n",
      "19 Train Loss 70.62436 Test MSE 64.7447233998032 Test RE 0.13545385220385495 Lambda1 0.49660742\n",
      "20 Train Loss 65.82173 Test MSE 60.93326424165054 Test RE 0.1314063622356149 Lambda1 0.50409824\n",
      "21 Train Loss 61.709084 Test MSE 60.30400790925545 Test RE 0.13072608623177792 Lambda1 0.5038967\n",
      "22 Train Loss 58.682003 Test MSE 56.08792487595626 Test RE 0.1260735136827202 Lambda1 0.51622176\n",
      "23 Train Loss 56.293903 Test MSE 54.43602065648964 Test RE 0.1242030771638372 Lambda1 0.5266031\n",
      "24 Train Loss 54.11551 Test MSE 52.18440525215753 Test RE 0.12160727020750613 Lambda1 0.53946465\n",
      "25 Train Loss 51.67119 Test MSE 51.49326967223288 Test RE 0.1207992965309682 Lambda1 0.54753405\n",
      "26 Train Loss 50.162083 Test MSE 50.54309319705699 Test RE 0.11967958624062239 Lambda1 0.5597076\n",
      "27 Train Loss 47.343987 Test MSE 48.66626108903951 Test RE 0.11743651680838413 Lambda1 0.5726842\n",
      "28 Train Loss 46.40847 Test MSE 48.25668872917692 Test RE 0.11694130331346675 Lambda1 0.57932705\n",
      "29 Train Loss 44.767353 Test MSE 47.25960532121932 Test RE 0.11572687230922786 Lambda1 0.5880538\n",
      "30 Train Loss 43.464916 Test MSE 45.814191864993155 Test RE 0.11394340298517669 Lambda1 0.59864074\n",
      "31 Train Loss 42.593613 Test MSE 44.92683630546495 Test RE 0.11283454688236239 Lambda1 0.6085311\n",
      "32 Train Loss 41.292797 Test MSE 43.23104542993976 Test RE 0.11068455914138395 Lambda1 0.62684363\n",
      "33 Train Loss 40.35923 Test MSE 42.188396004058255 Test RE 0.1093416636789251 Lambda1 0.6411498\n",
      "34 Train Loss 39.47263 Test MSE 41.07080923748495 Test RE 0.10788369204397383 Lambda1 0.65583134\n",
      "35 Train Loss 38.506245 Test MSE 40.156027599211654 Test RE 0.10667546451497903 Lambda1 0.67097735\n",
      "36 Train Loss 37.72068 Test MSE 39.985623486038264 Test RE 0.1064488825447232 Lambda1 0.67448384\n",
      "37 Train Loss 37.04912 Test MSE 39.20602763774992 Test RE 0.10540606278267864 Lambda1 0.6910177\n",
      "38 Train Loss 36.354134 Test MSE 37.925243384597344 Test RE 0.10367006215244991 Lambda1 0.7212837\n",
      "39 Train Loss 35.414715 Test MSE 37.28580420202742 Test RE 0.10279238150429926 Lambda1 0.7372831\n",
      "40 Train Loss 34.736523 Test MSE 36.819279800727706 Test RE 0.10214728203203341 Lambda1 0.748086\n",
      "41 Train Loss 33.831707 Test MSE 35.774274891651444 Test RE 0.10068727575717054 Lambda1 0.77880204\n",
      "42 Train Loss 33.19805 Test MSE 35.14563004155817 Test RE 0.0997986892217767 Lambda1 0.8115729\n",
      "43 Train Loss 32.618633 Test MSE 34.76469048138472 Test RE 0.09925636223734652 Lambda1 0.8249496\n",
      "44 Train Loss 32.186424 Test MSE 34.4589719133729 Test RE 0.09881897131843131 Lambda1 0.8440083\n",
      "45 Train Loss 31.657206 Test MSE 34.179313515442736 Test RE 0.09841716225732496 Lambda1 0.8556232\n",
      "46 Train Loss 31.36416 Test MSE 33.97261663558042 Test RE 0.09811912571725949 Lambda1 0.8640199\n",
      "47 Train Loss 30.921417 Test MSE 33.63534854989232 Test RE 0.09763086493917907 Lambda1 0.8811662\n",
      "48 Train Loss 30.636028 Test MSE 33.291946819894 Test RE 0.09713120279960674 Lambda1 0.8938205\n",
      "49 Train Loss 30.207409 Test MSE 32.902977015252745 Test RE 0.09656211458386604 Lambda1 0.9040463\n",
      "50 Train Loss 29.836782 Test MSE 32.39403855119179 Test RE 0.09581239979201894 Lambda1 0.94088256\n",
      "51 Train Loss 29.480818 Test MSE 32.180344457827196 Test RE 0.09549585355670022 Lambda1 0.95033693\n",
      "52 Train Loss 29.261507 Test MSE 32.0839307511378 Test RE 0.09535269138822834 Lambda1 0.96553814\n",
      "53 Train Loss 29.05633 Test MSE 31.89549595154027 Test RE 0.09507226712873024 Lambda1 0.9761418\n",
      "54 Train Loss 28.920282 Test MSE 31.856724734040448 Test RE 0.09501446604605471 Lambda1 0.9675983\n",
      "55 Train Loss 28.832747 Test MSE 31.897237947235723 Test RE 0.09507486331377006 Lambda1 0.9581016\n",
      "56 Train Loss 28.72553 Test MSE 31.7050557103012 Test RE 0.09478801551353869 Lambda1 0.9701321\n",
      "57 Train Loss 28.65738 Test MSE 31.61336576344174 Test RE 0.09465085441083014 Lambda1 0.9771322\n",
      "58 Train Loss 28.572092 Test MSE 31.51896290849985 Test RE 0.09450942702357251 Lambda1 0.97813404\n",
      "59 Train Loss 28.504192 Test MSE 31.454628297988396 Test RE 0.09441292427673624 Lambda1 0.98653394\n",
      "60 Train Loss 28.444347 Test MSE 31.456798435540776 Test RE 0.09441618111856928 Lambda1 0.9854858\n",
      "61 Train Loss 28.399364 Test MSE 31.46822460925032 Test RE 0.0944333271377495 Lambda1 0.98959285\n",
      "62 Train Loss 28.327517 Test MSE 31.39764061762985 Test RE 0.09432735955996298 Lambda1 0.98912805\n",
      "63 Train Loss 28.219131 Test MSE 31.31939183331005 Test RE 0.09420974553680518 Lambda1 0.994312\n",
      "64 Train Loss 28.175295 Test MSE 31.293786702118897 Test RE 0.09417122713034864 Lambda1 0.9906293\n",
      "65 Train Loss 28.119074 Test MSE 31.18365637073916 Test RE 0.0940053755310509 Lambda1 0.9936586\n",
      "66 Train Loss 28.094358 Test MSE 31.121687961821184 Test RE 0.09391192496814434 Lambda1 0.99441975\n",
      "67 Train Loss 28.020277 Test MSE 31.045045914301625 Test RE 0.09379621725016402 Lambda1 0.99757224\n",
      "68 Train Loss 27.977537 Test MSE 31.043111572616343 Test RE 0.09379329509698513 Lambda1 0.9931226\n",
      "69 Train Loss 27.942745 Test MSE 31.066859542374864 Test RE 0.09382916415771128 Lambda1 0.9952006\n",
      "70 Train Loss 27.88575 Test MSE 31.050766267567607 Test RE 0.093804858287214 Lambda1 0.99533737\n",
      "71 Train Loss 27.844109 Test MSE 30.98780549763178 Test RE 0.09370970728577527 Lambda1 0.9903513\n",
      "72 Train Loss 27.796122 Test MSE 30.955993984514336 Test RE 0.09366159460080412 Lambda1 0.9871342\n",
      "73 Train Loss 27.74926 Test MSE 30.878814666841 Test RE 0.09354476344275325 Lambda1 0.984306\n",
      "74 Train Loss 27.6913 Test MSE 30.85810870143428 Test RE 0.09351339469574527 Lambda1 0.9775172\n",
      "75 Train Loss 27.604095 Test MSE 30.755093806637614 Test RE 0.09335717439972264 Lambda1 0.9631111\n",
      "76 Train Loss 27.56186 Test MSE 30.768955832222815 Test RE 0.09337821113852673 Lambda1 0.9642184\n",
      "77 Train Loss 27.504091 Test MSE 30.72796693682607 Test RE 0.09331599347194054 Lambda1 0.967124\n",
      "78 Train Loss 27.449432 Test MSE 30.62539267309769 Test RE 0.09316011233728094 Lambda1 0.96167964\n",
      "79 Train Loss 27.395283 Test MSE 30.518735246011808 Test RE 0.09299774896488686 Lambda1 0.9602336\n",
      "80 Train Loss 27.365002 Test MSE 30.499645414504503 Test RE 0.09296865881664784 Lambda1 0.9619117\n",
      "81 Train Loss 27.317768 Test MSE 30.414743115662482 Test RE 0.09283916954643445 Lambda1 0.9608308\n",
      "82 Train Loss 27.287212 Test MSE 30.389541857397 Test RE 0.09280069891502703 Lambda1 0.96323436\n",
      "83 Train Loss 27.232431 Test MSE 30.380880587396277 Test RE 0.09278747349044021 Lambda1 0.9589892\n",
      "84 Train Loss 27.174429 Test MSE 30.380370653944347 Test RE 0.09278669478300508 Lambda1 0.958044\n",
      "85 Train Loss 27.144527 Test MSE 30.340261998169474 Test RE 0.09272542530792362 Lambda1 0.96066296\n",
      "86 Train Loss 27.108248 Test MSE 30.329643716610608 Test RE 0.09270919817657408 Lambda1 0.9546431\n",
      "87 Train Loss 27.080595 Test MSE 30.33947193154538 Test RE 0.09272421800554975 Lambda1 0.95237327\n",
      "88 Train Loss 27.05169 Test MSE 30.295052422687778 Test RE 0.09265631516119226 Lambda1 0.94997287\n",
      "89 Train Loss 27.01524 Test MSE 30.242563889307 Test RE 0.09257601322933988 Lambda1 0.94359726\n",
      "90 Train Loss 26.972113 Test MSE 30.214425933002204 Test RE 0.09253293642519879 Lambda1 0.94492394\n",
      "91 Train Loss 26.933613 Test MSE 30.202917346473445 Test RE 0.09251531198424744 Lambda1 0.94487333\n",
      "92 Train Loss 26.86438 Test MSE 30.125938976748312 Test RE 0.09239733958164939 Lambda1 0.93630105\n",
      "93 Train Loss 26.807114 Test MSE 30.14702074194066 Test RE 0.09242966319418709 Lambda1 0.93034905\n",
      "94 Train Loss 26.759422 Test MSE 30.105286429078866 Test RE 0.09236566309817387 Lambda1 0.929821\n",
      "95 Train Loss 26.71145 Test MSE 30.089827824239357 Test RE 0.09234194587443857 Lambda1 0.9281937\n",
      "96 Train Loss 26.681332 Test MSE 30.07372328621858 Test RE 0.09231723115318534 Lambda1 0.92453855\n",
      "97 Train Loss 26.624489 Test MSE 30.002064647588202 Test RE 0.09220718038828332 Lambda1 0.9227369\n",
      "98 Train Loss 26.575584 Test MSE 29.895773361849407 Test RE 0.09204369971007571 Lambda1 0.92721075\n",
      "99 Train Loss 26.510178 Test MSE 29.829562012653657 Test RE 0.09194171680511343 Lambda1 0.9328886\n",
      "100 Train Loss 26.453703 Test MSE 29.745209042857685 Test RE 0.09181162693864084 Lambda1 0.93978995\n",
      "101 Train Loss 26.398722 Test MSE 29.643473928596208 Test RE 0.0916544845457287 Lambda1 0.9404627\n",
      "102 Train Loss 26.323128 Test MSE 29.488725500271663 Test RE 0.09141493862752231 Lambda1 0.93921024\n",
      "103 Train Loss 26.234802 Test MSE 29.397979955543192 Test RE 0.09127417482667545 Lambda1 0.93864053\n",
      "104 Train Loss 26.193785 Test MSE 29.348567196845643 Test RE 0.09119743476580607 Lambda1 0.93974257\n",
      "105 Train Loss 26.118994 Test MSE 29.352291256704152 Test RE 0.09120322063485062 Lambda1 0.9389658\n",
      "106 Train Loss 26.078083 Test MSE 29.32542325147861 Test RE 0.09116146904539983 Lambda1 0.9423634\n",
      "107 Train Loss 26.052021 Test MSE 29.30980391644509 Test RE 0.09113718855739544 Lambda1 0.9402157\n",
      "108 Train Loss 26.028013 Test MSE 29.30690059757361 Test RE 0.09113267459215212 Lambda1 0.9378507\n",
      "109 Train Loss 26.01355 Test MSE 29.30662001303574 Test RE 0.09113223833857244 Lambda1 0.9396154\n",
      "110 Train Loss 25.989761 Test MSE 29.27950781292735 Test RE 0.09109007436346404 Lambda1 0.945125\n",
      "111 Train Loss 25.971725 Test MSE 29.253855355348445 Test RE 0.09105016255358013 Lambda1 0.94863445\n",
      "112 Train Loss 25.949596 Test MSE 29.235617176348338 Test RE 0.09102177572994936 Lambda1 0.9489745\n",
      "113 Train Loss 25.924479 Test MSE 29.21366219030685 Test RE 0.09098759213252015 Lambda1 0.9497488\n",
      "114 Train Loss 25.902132 Test MSE 29.14787944390506 Test RE 0.09088509236857463 Lambda1 0.95253927\n",
      "115 Train Loss 25.881525 Test MSE 29.142723263049753 Test RE 0.09087705334981103 Lambda1 0.9544245\n",
      "116 Train Loss 25.86207 Test MSE 29.145638603751355 Test RE 0.09088159875458576 Lambda1 0.95235246\n",
      "117 Train Loss 25.83827 Test MSE 29.143542541983944 Test RE 0.09087833073784843 Lambda1 0.9538461\n",
      "118 Train Loss 25.821991 Test MSE 29.121038804670913 Test RE 0.09084323725147866 Lambda1 0.95506454\n",
      "119 Train Loss 25.807714 Test MSE 29.114892585177323 Test RE 0.09083365016320326 Lambda1 0.9549346\n",
      "120 Train Loss 25.792414 Test MSE 29.078183592207882 Test RE 0.09077636910829294 Lambda1 0.95426655\n",
      "121 Train Loss 25.763193 Test MSE 29.054604239879378 Test RE 0.09073955659421076 Lambda1 0.95341337\n",
      "122 Train Loss 25.730995 Test MSE 29.006581647584774 Test RE 0.09066453662892625 Lambda1 0.9573235\n",
      "123 Train Loss 25.715847 Test MSE 29.00758620848764 Test RE 0.09066610657021294 Lambda1 0.96063286\n",
      "124 Train Loss 25.692757 Test MSE 28.97227560043901 Test RE 0.09061090635116001 Lambda1 0.96144485\n",
      "125 Train Loss 25.678122 Test MSE 28.952001276197617 Test RE 0.09057919678816699 Lambda1 0.958793\n",
      "126 Train Loss 25.650925 Test MSE 28.920957828432975 Test RE 0.09053062251698164 Lambda1 0.9565766\n",
      "127 Train Loss 25.627535 Test MSE 28.91368138344817 Test RE 0.09051923315467675 Lambda1 0.9606565\n",
      "128 Train Loss 25.609838 Test MSE 28.874336519470297 Test RE 0.09045762426997973 Lambda1 0.96057075\n",
      "129 Train Loss 25.579245 Test MSE 28.851273981639444 Test RE 0.09042149185328764 Lambda1 0.9639239\n",
      "130 Train Loss 25.558842 Test MSE 28.817921631927288 Test RE 0.0903692126837719 Lambda1 0.9631913\n",
      "131 Train Loss 25.546972 Test MSE 28.808737121460226 Test RE 0.09035481082765151 Lambda1 0.963498\n",
      "132 Train Loss 25.534779 Test MSE 28.793060381480306 Test RE 0.09033022346687336 Lambda1 0.965642\n",
      "133 Train Loss 25.525822 Test MSE 28.794665627224603 Test RE 0.09033274143793875 Lambda1 0.9659857\n",
      "134 Train Loss 25.518063 Test MSE 28.794606440751686 Test RE 0.09033264859992542 Lambda1 0.9652504\n",
      "135 Train Loss 25.50876 Test MSE 28.812272131822574 Test RE 0.09036035420488406 Lambda1 0.9665191\n",
      "136 Train Loss 25.497597 Test MSE 28.79623416389505 Test RE 0.09033520175975313 Lambda1 0.9664943\n",
      "137 Train Loss 25.481144 Test MSE 28.74060268368083 Test RE 0.09024790023116908 Lambda1 0.9678076\n",
      "138 Train Loss 25.465412 Test MSE 28.72768380120303 Test RE 0.09022761476469336 Lambda1 0.9696054\n",
      "139 Train Loss 25.451181 Test MSE 28.703758164032728 Test RE 0.09019003424043007 Lambda1 0.9725342\n",
      "140 Train Loss 25.443829 Test MSE 28.684675476211275 Test RE 0.09016004941403058 Lambda1 0.97423744\n",
      "141 Train Loss 25.4304 Test MSE 28.648254675041215 Test RE 0.09010279333276051 Lambda1 0.97874284\n",
      "142 Train Loss 25.422203 Test MSE 28.628277994994654 Test RE 0.09007137311912732 Lambda1 0.9796478\n",
      "143 Train Loss 25.40348 Test MSE 28.631199243162946 Test RE 0.09007596847274715 Lambda1 0.9792861\n",
      "144 Train Loss 25.393154 Test MSE 28.628004894304077 Test RE 0.09007094349823166 Lambda1 0.9775999\n",
      "145 Train Loss 25.38555 Test MSE 28.603963918968592 Test RE 0.09003311606074822 Lambda1 0.97758293\n",
      "146 Train Loss 25.378452 Test MSE 28.582081718465545 Test RE 0.08999867154046791 Lambda1 0.97814405\n",
      "147 Train Loss 25.37026 Test MSE 28.58125886712251 Test RE 0.0899973760424443 Lambda1 0.9763144\n",
      "148 Train Loss 25.35864 Test MSE 28.570247039892106 Test RE 0.08998003721499691 Lambda1 0.97313\n",
      "149 Train Loss 25.35099 Test MSE 28.581997615627863 Test RE 0.08999853912972705 Lambda1 0.972938\n",
      "150 Train Loss 25.346369 Test MSE 28.579806431022945 Test RE 0.08999508928000413 Lambda1 0.96991175\n",
      "151 Train Loss 25.33777 Test MSE 28.57208771358356 Test RE 0.08998293570620491 Lambda1 0.9645221\n",
      "152 Train Loss 25.328012 Test MSE 28.556611612680236 Test RE 0.08995856273018386 Lambda1 0.9611749\n",
      "153 Train Loss 25.314642 Test MSE 28.57081301985681 Test RE 0.08998092846815971 Lambda1 0.95389974\n",
      "154 Train Loss 25.288586 Test MSE 28.56177111449022 Test RE 0.08996668905151686 Lambda1 0.9511162\n",
      "155 Train Loss 25.267282 Test MSE 28.55028242534257 Test RE 0.08994859312742848 Lambda1 0.9508296\n",
      "156 Train Loss 25.247122 Test MSE 28.52619692305925 Test RE 0.08991064404306086 Lambda1 0.94529766\n",
      "157 Train Loss 25.23378 Test MSE 28.513849252881204 Test RE 0.08989118285893806 Lambda1 0.9414604\n",
      "158 Train Loss 25.229425 Test MSE 28.50906097228907 Test RE 0.08988363490786903 Lambda1 0.9398084\n",
      "159 Train Loss 25.220346 Test MSE 28.497714817544242 Test RE 0.08986574699635053 Lambda1 0.9392556\n",
      "160 Train Loss 25.208225 Test MSE 28.476849593825108 Test RE 0.08983284238853138 Lambda1 0.9368099\n",
      "161 Train Loss 25.192373 Test MSE 28.44816987823781 Test RE 0.08978759459221133 Lambda1 0.9385511\n",
      "162 Train Loss 25.187286 Test MSE 28.44613369515207 Test RE 0.08978438125276397 Lambda1 0.93967515\n",
      "163 Train Loss 25.17911 Test MSE 28.423766878018064 Test RE 0.08974907617814094 Lambda1 0.94120497\n",
      "164 Train Loss 25.17419 Test MSE 28.418150749598798 Test RE 0.08974020917539931 Lambda1 0.9394513\n",
      "165 Train Loss 25.168474 Test MSE 28.39407328550758 Test RE 0.08970218462997205 Lambda1 0.936374\n",
      "166 Train Loss 25.16102 Test MSE 28.38839824040001 Test RE 0.08969321991790435 Lambda1 0.93712753\n",
      "167 Train Loss 25.146847 Test MSE 28.372717022828787 Test RE 0.08966844407164429 Lambda1 0.935699\n",
      "168 Train Loss 25.134352 Test MSE 28.38230453829056 Test RE 0.08968359286722412 Lambda1 0.9399387\n",
      "169 Train Loss 25.120369 Test MSE 28.365978963341075 Test RE 0.08965779603984837 Lambda1 0.9395655\n",
      "170 Train Loss 25.110424 Test MSE 28.3516602439607 Test RE 0.08963516423022291 Lambda1 0.9408504\n",
      "171 Train Loss 25.100594 Test MSE 28.357719375548193 Test RE 0.08964474183936143 Lambda1 0.941322\n",
      "172 Train Loss 25.096733 Test MSE 28.35674048625729 Test RE 0.08964319458808373 Lambda1 0.93990684\n",
      "173 Train Loss 25.089893 Test MSE 28.345832588463452 Test RE 0.08962595154746238 Lambda1 0.93917155\n",
      "174 Train Loss 25.073956 Test MSE 28.333457223362853 Test RE 0.08960638473890521 Lambda1 0.9378839\n",
      "175 Train Loss 25.065422 Test MSE 28.338777888032457 Test RE 0.08961479781644037 Lambda1 0.9338671\n",
      "176 Train Loss 25.060663 Test MSE 28.32885673790991 Test RE 0.08959910977769789 Lambda1 0.93287605\n",
      "177 Train Loss 25.05448 Test MSE 28.331184428565123 Test RE 0.08960279073680778 Lambda1 0.9324189\n",
      "178 Train Loss 25.045805 Test MSE 28.312765939559117 Test RE 0.0895736600039238 Lambda1 0.9323211\n",
      "179 Train Loss 25.036358 Test MSE 28.2821555427943 Test RE 0.08952522554899163 Lambda1 0.93445396\n",
      "180 Train Loss 25.026424 Test MSE 28.275004377491573 Test RE 0.08951390657155027 Lambda1 0.9358991\n",
      "181 Train Loss 25.015446 Test MSE 28.246751991062542 Test RE 0.08946917424364513 Lambda1 0.9394717\n",
      "182 Train Loss 25.00948 Test MSE 28.22640420889091 Test RE 0.08943694350716544 Lambda1 0.94202524\n",
      "183 Train Loss 25.003414 Test MSE 28.23217114532336 Test RE 0.08944607947117605 Lambda1 0.94061714\n",
      "184 Train Loss 24.998892 Test MSE 28.230440866790996 Test RE 0.08944333846692058 Lambda1 0.9403209\n",
      "185 Train Loss 24.994026 Test MSE 28.2278429873003 Test RE 0.08943922290312774 Lambda1 0.9410755\n",
      "186 Train Loss 24.989721 Test MSE 28.227137340076855 Test RE 0.0894381049832469 Lambda1 0.9401126\n",
      "187 Train Loss 24.981043 Test MSE 28.214214538077336 Test RE 0.08941762959421204 Lambda1 0.9381968\n",
      "188 Train Loss 24.972462 Test MSE 28.214069602050948 Test RE 0.08941739992535934 Lambda1 0.9376773\n",
      "189 Train Loss 24.965324 Test MSE 28.212506478480833 Test RE 0.08941492292747624 Lambda1 0.9377702\n",
      "190 Train Loss 24.957054 Test MSE 28.186294507832713 Test RE 0.08937337599904156 Lambda1 0.93957305\n",
      "191 Train Loss 24.951029 Test MSE 28.17372473228181 Test RE 0.08935344559367113 Lambda1 0.93948865\n",
      "192 Train Loss 24.941809 Test MSE 28.15941042661756 Test RE 0.08933074367821157 Lambda1 0.9387874\n",
      "193 Train Loss 24.931437 Test MSE 28.138161656399728 Test RE 0.08929703333704539 Lambda1 0.9400613\n",
      "194 Train Loss 24.919872 Test MSE 28.1228226231852 Test RE 0.08927269065078303 Lambda1 0.9420132\n",
      "195 Train Loss 24.914665 Test MSE 28.104916292692145 Test RE 0.08924426532411167 Lambda1 0.9423537\n",
      "196 Train Loss 24.89907 Test MSE 28.10175729801472 Test RE 0.0892392496521439 Lambda1 0.9382198\n",
      "197 Train Loss 24.888353 Test MSE 28.080221500525855 Test RE 0.08920504882388569 Lambda1 0.9388007\n",
      "198 Train Loss 24.870794 Test MSE 28.065805993758513 Test RE 0.08918214834450067 Lambda1 0.9432917\n",
      "199 Train Loss 24.856163 Test MSE 28.048585896156148 Test RE 0.089154784780864 Lambda1 0.9438944\n",
      "Training time: 349.76\n",
      "Training time: 349.76\n",
      "inv_HT_stan_late_Try\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 840.2322 Test MSE 838.5642520119164 Test RE 0.48748060660040293 Lambda1 -0.062273383\n",
      "1 Train Loss 741.6186 Test MSE 734.292379757659 Test RE 0.45616680766648054 Lambda1 0.057770718\n",
      "2 Train Loss 514.2783 Test MSE 484.2395596002053 Test RE 0.3704412180992385 Lambda1 0.1571995\n",
      "3 Train Loss 337.47598 Test MSE 305.9187613839626 Test RE 0.2944368933249267 Lambda1 0.14411828\n",
      "4 Train Loss 270.15756 Test MSE 263.50264652808795 Test RE 0.273263521911788 Lambda1 0.105421126\n",
      "5 Train Loss 244.58478 Test MSE 241.7038657965506 Test RE 0.2617164187458595 Lambda1 0.1145731\n",
      "6 Train Loss 218.04979 Test MSE 213.21605754896765 Test RE 0.24580976128066692 Lambda1 0.1733591\n",
      "7 Train Loss 190.87805 Test MSE 178.77637908205307 Test RE 0.2250838002620695 Lambda1 0.23749982\n",
      "8 Train Loss 171.48778 Test MSE 160.3830848818677 Test RE 0.2131907963989247 Lambda1 0.27378148\n",
      "9 Train Loss 153.80095 Test MSE 138.46472779415814 Test RE 0.19808826527928294 Lambda1 0.3042622\n",
      "10 Train Loss 134.99693 Test MSE 121.4313084222454 Test RE 0.1855045250497857 Lambda1 0.3389176\n",
      "11 Train Loss 119.84738 Test MSE 110.17880153870708 Test RE 0.1767006685462837 Lambda1 0.36624986\n",
      "12 Train Loss 106.98304 Test MSE 97.47292833559071 Test RE 0.16620006045411606 Lambda1 0.40956554\n",
      "13 Train Loss 96.7504 Test MSE 88.15042258487375 Test RE 0.15805250008741115 Lambda1 0.43778163\n",
      "14 Train Loss 88.40312 Test MSE 80.01676847787702 Test RE 0.15058429315835317 Lambda1 0.45759925\n",
      "15 Train Loss 81.78605 Test MSE 73.54718319644881 Test RE 0.14436841642869871 Lambda1 0.48115\n",
      "16 Train Loss 76.34938 Test MSE 68.79514241265555 Test RE 0.13962656619473152 Lambda1 0.49496454\n",
      "17 Train Loss 70.3181 Test MSE 61.77931759258164 Test RE 0.13231550054075883 Lambda1 0.51282376\n",
      "18 Train Loss 65.17096 Test MSE 59.06662080042858 Test RE 0.12937794046340997 Lambda1 0.5266327\n",
      "19 Train Loss 60.21416 Test MSE 54.975795546308255 Test RE 0.12481734257030801 Lambda1 0.54633874\n",
      "20 Train Loss 57.226147 Test MSE 53.433776085673344 Test RE 0.12305438768959201 Lambda1 0.55627465\n",
      "21 Train Loss 54.483532 Test MSE 51.63403237749009 Test RE 0.1209642931520895 Lambda1 0.56858236\n",
      "22 Train Loss 52.059425 Test MSE 48.98152943767252 Test RE 0.1178162896464848 Lambda1 0.58385456\n",
      "23 Train Loss 50.19172 Test MSE 47.03984367370596 Test RE 0.11545748831232644 Lambda1 0.60246885\n",
      "24 Train Loss 47.78065 Test MSE 45.39995356385499 Test RE 0.11342711211225624 Lambda1 0.6351586\n",
      "25 Train Loss 45.208084 Test MSE 44.31280478782808 Test RE 0.11206081855174839 Lambda1 0.6456868\n",
      "26 Train Loss 42.852623 Test MSE 43.62469837372188 Test RE 0.1111873524850252 Lambda1 0.6573157\n",
      "27 Train Loss 41.122936 Test MSE 42.747634460558906 Test RE 0.11006398024375 Lambda1 0.6741723\n",
      "28 Train Loss 40.251648 Test MSE 41.858913275999555 Test RE 0.10891385878037582 Lambda1 0.6834568\n",
      "29 Train Loss 39.510773 Test MSE 41.39775971461938 Test RE 0.10831225316788747 Lambda1 0.6946473\n",
      "30 Train Loss 38.374588 Test MSE 40.205808700632225 Test RE 0.10674156638850923 Lambda1 0.7185831\n",
      "31 Train Loss 37.727814 Test MSE 39.715480776186205 Test RE 0.10608868903819933 Lambda1 0.72885144\n",
      "32 Train Loss 37.112644 Test MSE 39.02497147834163 Test RE 0.10516239487788533 Lambda1 0.7521818\n",
      "33 Train Loss 36.507 Test MSE 38.46618948066238 Test RE 0.10440679249673157 Lambda1 0.76275086\n",
      "34 Train Loss 35.64146 Test MSE 37.72945665915167 Test RE 0.10340212075238972 Lambda1 0.77500993\n",
      "35 Train Loss 35.05302 Test MSE 37.36855003735987 Test RE 0.10290637834421236 Lambda1 0.7794817\n",
      "36 Train Loss 34.43783 Test MSE 36.57525072344142 Test RE 0.10180821582948552 Lambda1 0.7900545\n",
      "37 Train Loss 33.934322 Test MSE 36.128730710438276 Test RE 0.10118485709096903 Lambda1 0.79569674\n",
      "38 Train Loss 33.485474 Test MSE 35.760939629434674 Test RE 0.10066850785233791 Lambda1 0.7997885\n",
      "39 Train Loss 32.964367 Test MSE 35.20719826608698 Test RE 0.09988606480013129 Lambda1 0.8126276\n",
      "40 Train Loss 32.394176 Test MSE 34.92119058571944 Test RE 0.09947952240567742 Lambda1 0.81706405\n",
      "41 Train Loss 31.996065 Test MSE 34.803615202939106 Test RE 0.09931191350722286 Lambda1 0.8153984\n",
      "42 Train Loss 31.543774 Test MSE 34.39656510262299 Test RE 0.09872944787647783 Lambda1 0.8213929\n",
      "43 Train Loss 31.221357 Test MSE 34.0973860716152 Test RE 0.09829913905450142 Lambda1 0.8278743\n",
      "44 Train Loss 30.939283 Test MSE 33.97041729692602 Test RE 0.09811594961986085 Lambda1 0.8342906\n",
      "45 Train Loss 30.725397 Test MSE 33.9289776867312 Test RE 0.09805608683698301 Lambda1 0.8409635\n",
      "46 Train Loss 30.442608 Test MSE 33.59317188508528 Test RE 0.097569634178943 Lambda1 0.8491691\n",
      "47 Train Loss 30.206444 Test MSE 33.33478408931097 Test RE 0.09719367283076748 Lambda1 0.85870576\n",
      "48 Train Loss 29.941528 Test MSE 33.06954010945512 Test RE 0.09680621682772714 Lambda1 0.8694101\n",
      "49 Train Loss 29.74687 Test MSE 32.842497311731634 Test RE 0.09647332726518379 Lambda1 0.87728995\n",
      "50 Train Loss 29.598629 Test MSE 32.660591551094655 Test RE 0.09620578638343656 Lambda1 0.8840925\n",
      "51 Train Loss 29.368017 Test MSE 32.37447235727789 Test RE 0.09578345978880899 Lambda1 0.89071625\n",
      "52 Train Loss 29.182562 Test MSE 32.2404529585363 Test RE 0.09558499857272142 Lambda1 0.89806575\n",
      "53 Train Loss 29.029457 Test MSE 32.10219822984413 Test RE 0.09537983278505198 Lambda1 0.9005026\n",
      "54 Train Loss 28.912367 Test MSE 32.08330399840566 Test RE 0.0953517600364533 Lambda1 0.9007583\n",
      "55 Train Loss 28.747446 Test MSE 32.10038374526042 Test RE 0.09537713721136387 Lambda1 0.90766937\n",
      "56 Train Loss 28.661434 Test MSE 31.951570175230042 Test RE 0.09515580184689239 Lambda1 0.91197544\n",
      "57 Train Loss 28.562643 Test MSE 31.840578895091603 Test RE 0.09499038505989088 Lambda1 0.9159308\n",
      "58 Train Loss 28.435688 Test MSE 31.74428645108284 Test RE 0.09484664109194582 Lambda1 0.91960514\n",
      "59 Train Loss 28.344078 Test MSE 31.71404159466289 Test RE 0.0948014470276057 Lambda1 0.92142165\n",
      "60 Train Loss 28.24263 Test MSE 31.645280541084283 Test RE 0.09469861900174581 Lambda1 0.92154515\n",
      "61 Train Loss 28.15077 Test MSE 31.506486168675167 Test RE 0.09449071945613677 Lambda1 0.92733717\n",
      "62 Train Loss 28.094828 Test MSE 31.360816277190686 Test RE 0.09427202798398943 Lambda1 0.9338012\n",
      "63 Train Loss 27.97013 Test MSE 31.182028525557637 Test RE 0.09400292187082433 Lambda1 0.9408989\n",
      "64 Train Loss 27.89314 Test MSE 31.150914870587965 Test RE 0.09395601176634914 Lambda1 0.94463295\n",
      "65 Train Loss 27.826126 Test MSE 31.08287836970415 Test RE 0.09385335133821589 Lambda1 0.94976985\n",
      "66 Train Loss 27.70449 Test MSE 31.020703536201594 Test RE 0.09375943729605278 Lambda1 0.95046574\n",
      "67 Train Loss 27.624468 Test MSE 31.000400943804507 Test RE 0.09372875019056105 Lambda1 0.95482814\n",
      "68 Train Loss 27.583706 Test MSE 30.932604626493855 Test RE 0.09362620406336623 Lambda1 0.9575803\n",
      "69 Train Loss 27.538408 Test MSE 30.85778432307481 Test RE 0.0935129031912015 Lambda1 0.9593069\n",
      "70 Train Loss 27.47063 Test MSE 30.837927884163683 Test RE 0.09348281139918937 Lambda1 0.95880306\n",
      "71 Train Loss 27.406082 Test MSE 30.790946604037167 Test RE 0.09341157418423318 Lambda1 0.95885587\n",
      "72 Train Loss 27.345675 Test MSE 30.68319768138373 Test RE 0.09324799010643844 Lambda1 0.9589002\n",
      "73 Train Loss 27.306437 Test MSE 30.672945131398027 Test RE 0.09323240976234651 Lambda1 0.9576678\n",
      "74 Train Loss 27.201054 Test MSE 30.504693651428596 Test RE 0.09297635248637602 Lambda1 0.9657777\n",
      "75 Train Loss 27.142248 Test MSE 30.434057190997713 Test RE 0.09286864239341264 Lambda1 0.9650193\n",
      "76 Train Loss 27.095598 Test MSE 30.330965801551745 Test RE 0.0927112187756334 Lambda1 0.9673699\n",
      "77 Train Loss 27.053503 Test MSE 30.288194316441903 Test RE 0.09264582693341854 Lambda1 0.96833366\n",
      "78 Train Loss 27.019457 Test MSE 30.271503142226766 Test RE 0.09262029585152694 Lambda1 0.968894\n",
      "79 Train Loss 26.959377 Test MSE 30.20612890926873 Test RE 0.09252023056268477 Lambda1 0.9697274\n",
      "80 Train Loss 26.913857 Test MSE 30.172343286053337 Test RE 0.09246847404306027 Lambda1 0.9718833\n",
      "81 Train Loss 26.857838 Test MSE 30.0938019470154 Test RE 0.09234804371780342 Lambda1 0.97126126\n",
      "82 Train Loss 26.825657 Test MSE 30.064548537693813 Test RE 0.09230314822802095 Lambda1 0.97194284\n",
      "83 Train Loss 26.773867 Test MSE 30.016655586211755 Test RE 0.09222959927493461 Lambda1 0.96983063\n",
      "84 Train Loss 26.738607 Test MSE 30.03765292970122 Test RE 0.09226185200240301 Lambda1 0.9696266\n",
      "85 Train Loss 26.705263 Test MSE 29.965416637589172 Test RE 0.09215084689458551 Lambda1 0.97226644\n",
      "86 Train Loss 26.676651 Test MSE 29.863407717405142 Test RE 0.09199386222382526 Lambda1 0.9757956\n",
      "87 Train Loss 26.651152 Test MSE 29.84609298989229 Test RE 0.09196718945389745 Lambda1 0.97771573\n",
      "88 Train Loss 26.598543 Test MSE 29.816654394959333 Test RE 0.09192182249794718 Lambda1 0.97913903\n",
      "89 Train Loss 26.575724 Test MSE 29.787430629076123 Test RE 0.09187676445204877 Lambda1 0.98023885\n",
      "90 Train Loss 26.554703 Test MSE 29.74381710164399 Test RE 0.09180947872911985 Lambda1 0.98242974\n",
      "91 Train Loss 26.524721 Test MSE 29.756689883908003 Test RE 0.09182934362374674 Lambda1 0.9829982\n",
      "92 Train Loss 26.487797 Test MSE 29.71379772183965 Test RE 0.09176313700786992 Lambda1 0.9836664\n",
      "93 Train Loss 26.473104 Test MSE 29.67246769975073 Test RE 0.09169929642713401 Lambda1 0.9849108\n",
      "94 Train Loss 26.447512 Test MSE 29.675178750949424 Test RE 0.09170348542502674 Lambda1 0.9860604\n",
      "95 Train Loss 26.428976 Test MSE 29.636927352440956 Test RE 0.09164436332653575 Lambda1 0.98622394\n",
      "96 Train Loss 26.410517 Test MSE 29.629369621246262 Test RE 0.0916326774385638 Lambda1 0.9843378\n",
      "97 Train Loss 26.36852 Test MSE 29.528227345159777 Test RE 0.09147614592232606 Lambda1 0.9839345\n",
      "98 Train Loss 26.35909 Test MSE 29.520137460143136 Test RE 0.09146361414743477 Lambda1 0.9837271\n",
      "99 Train Loss 26.344334 Test MSE 29.498620594229504 Test RE 0.0914302747182122 Lambda1 0.98398596\n",
      "100 Train Loss 26.318314 Test MSE 29.530699566357903 Test RE 0.09147997521642989 Lambda1 0.9844831\n",
      "101 Train Loss 26.287796 Test MSE 29.478890751062426 Test RE 0.09139969351346067 Lambda1 0.9858366\n",
      "102 Train Loss 26.267645 Test MSE 29.422960134784937 Test RE 0.09131294553649881 Lambda1 0.98732334\n",
      "103 Train Loss 26.245104 Test MSE 29.350225587773043 Test RE 0.09120001136283262 Lambda1 0.98909265\n",
      "104 Train Loss 26.228699 Test MSE 29.300531859370036 Test RE 0.09112277194687363 Lambda1 0.990125\n",
      "105 Train Loss 26.216787 Test MSE 29.298677822679068 Test RE 0.09111988893375919 Lambda1 0.990447\n",
      "106 Train Loss 26.184292 Test MSE 29.25988364024471 Test RE 0.09105954333501054 Lambda1 0.99076766\n",
      "107 Train Loss 26.145975 Test MSE 29.25942461507117 Test RE 0.09105882906719033 Lambda1 0.9920289\n",
      "108 Train Loss 26.11889 Test MSE 29.237847469060856 Test RE 0.09102524754534423 Lambda1 0.99288905\n",
      "109 Train Loss 26.105587 Test MSE 29.22538370844584 Test RE 0.09100584396527212 Lambda1 0.991691\n",
      "110 Train Loss 26.093344 Test MSE 29.197746040917945 Test RE 0.09096280288492356 Lambda1 0.9928597\n",
      "111 Train Loss 26.084984 Test MSE 29.191724734890176 Test RE 0.09095342299796946 Lambda1 0.99303705\n",
      "112 Train Loss 26.065737 Test MSE 29.168644033918426 Test RE 0.0909174593148843 Lambda1 0.9946677\n",
      "113 Train Loss 26.059128 Test MSE 29.14148784791382 Test RE 0.09087512710434534 Lambda1 0.9966078\n",
      "114 Train Loss 26.045826 Test MSE 29.134107507693756 Test RE 0.09086361890975768 Lambda1 0.99761796\n",
      "115 Train Loss 26.02375 Test MSE 29.122975227290265 Test RE 0.09084625754166095 Lambda1 0.9981059\n",
      "116 Train Loss 26.009884 Test MSE 29.11821609311541 Test RE 0.09083883441284074 Lambda1 0.999423\n",
      "117 Train Loss 26.000376 Test MSE 29.105172503617474 Test RE 0.09081848637394715 Lambda1 0.99911153\n",
      "118 Train Loss 25.974148 Test MSE 29.109265431056475 Test RE 0.09082487184387741 Lambda1 0.99827546\n",
      "119 Train Loss 25.953201 Test MSE 29.09877915023001 Test RE 0.09080851105827016 Lambda1 0.9964708\n",
      "120 Train Loss 25.94447 Test MSE 29.07959256327566 Test RE 0.09077856834690756 Lambda1 0.99499804\n",
      "121 Train Loss 25.936546 Test MSE 29.045363531917946 Test RE 0.09072512575879163 Lambda1 0.9938628\n",
      "122 Train Loss 25.911806 Test MSE 28.999984505213767 Test RE 0.09065422585056257 Lambda1 0.99202436\n",
      "123 Train Loss 25.881483 Test MSE 28.953003663752327 Test RE 0.0905807648088512 Lambda1 0.99097043\n",
      "124 Train Loss 25.835617 Test MSE 28.903544689766342 Test RE 0.09050336443561165 Lambda1 0.99058175\n",
      "125 Train Loss 25.800772 Test MSE 28.829547198650868 Test RE 0.09038743897001245 Lambda1 0.99296284\n",
      "126 Train Loss 25.786411 Test MSE 28.79720968115948 Test RE 0.09033673186959461 Lambda1 0.99337417\n",
      "127 Train Loss 25.769651 Test MSE 28.76144875053263 Test RE 0.09028062349672282 Lambda1 0.9930861\n",
      "128 Train Loss 25.740917 Test MSE 28.783702178014977 Test RE 0.09031554289286434 Lambda1 0.9927952\n",
      "129 Train Loss 25.724041 Test MSE 28.76992790214835 Test RE 0.0902939303139511 Lambda1 0.99208194\n",
      "130 Train Loss 25.70774 Test MSE 28.764115961466228 Test RE 0.09028480951471178 Lambda1 0.9900329\n",
      "131 Train Loss 25.671785 Test MSE 28.766837025107503 Test RE 0.09028907985111057 Lambda1 0.9866804\n",
      "132 Train Loss 25.644712 Test MSE 28.769290774781293 Test RE 0.09029293050170299 Lambda1 0.9860513\n",
      "133 Train Loss 25.61678 Test MSE 28.771531730867252 Test RE 0.09029644707347993 Lambda1 0.9838745\n",
      "134 Train Loss 25.593126 Test MSE 28.73730218067094 Test RE 0.09024271815441161 Lambda1 0.980714\n",
      "135 Train Loss 25.559898 Test MSE 28.71022122537562 Test RE 0.09020018745646362 Lambda1 0.97583723\n",
      "136 Train Loss 25.537539 Test MSE 28.662915512521817 Test RE 0.09012584558260177 Lambda1 0.97275317\n",
      "137 Train Loss 25.51546 Test MSE 28.608619582937436 Test RE 0.09004044278792732 Lambda1 0.972129\n",
      "138 Train Loss 25.496002 Test MSE 28.58912142763377 Test RE 0.09000975410388652 Lambda1 0.97085226\n",
      "139 Train Loss 25.47051 Test MSE 28.595081317998677 Test RE 0.09001913564981044 Lambda1 0.9700681\n",
      "140 Train Loss 25.455933 Test MSE 28.589185681089468 Test RE 0.09000985525135466 Lambda1 0.9724745\n",
      "141 Train Loss 25.44179 Test MSE 28.54142361436555 Test RE 0.08993463705872529 Lambda1 0.9717603\n",
      "142 Train Loss 25.425646 Test MSE 28.515150759532798 Test RE 0.08989323436444273 Lambda1 0.97111994\n",
      "143 Train Loss 25.413588 Test MSE 28.460299903580243 Test RE 0.08980673483443186 Lambda1 0.9693235\n",
      "144 Train Loss 25.401033 Test MSE 28.449883293255606 Test RE 0.08979029847610336 Lambda1 0.96985424\n",
      "145 Train Loss 25.376741 Test MSE 28.437186870251143 Test RE 0.08977026073434155 Lambda1 0.9719692\n",
      "146 Train Loss 25.356783 Test MSE 28.437998011901776 Test RE 0.08977154102761664 Lambda1 0.97198045\n",
      "147 Train Loss 25.343657 Test MSE 28.430174211165472 Test RE 0.0897591913016845 Lambda1 0.9721979\n",
      "148 Train Loss 25.320726 Test MSE 28.427097458871174 Test RE 0.08975433423911584 Lambda1 0.9720589\n",
      "149 Train Loss 25.277962 Test MSE 28.37715680906183 Test RE 0.08967545949336667 Lambda1 0.97196144\n",
      "150 Train Loss 25.242752 Test MSE 28.338096676321282 Test RE 0.08961372072312189 Lambda1 0.97112674\n",
      "151 Train Loss 25.222944 Test MSE 28.303665607914155 Test RE 0.08955926339698961 Lambda1 0.9713047\n",
      "152 Train Loss 25.189339 Test MSE 28.283289976511952 Test RE 0.0895270210170895 Lambda1 0.97138906\n",
      "153 Train Loss 25.180693 Test MSE 28.28560493002016 Test RE 0.08953068478200014 Lambda1 0.97180057\n",
      "154 Train Loss 25.154053 Test MSE 28.25772723623337 Test RE 0.08948655413139055 Lambda1 0.97023016\n",
      "155 Train Loss 25.122862 Test MSE 28.23216070628497 Test RE 0.08944606293452463 Lambda1 0.9687993\n",
      "156 Train Loss 25.085793 Test MSE 28.203706986975995 Test RE 0.08940097756575983 Lambda1 0.96771264\n",
      "157 Train Loss 25.069965 Test MSE 28.190148590985466 Test RE 0.08937948607230223 Lambda1 0.96920294\n",
      "158 Train Loss 25.056477 Test MSE 28.183444071684445 Test RE 0.08936885678949114 Lambda1 0.9708934\n",
      "159 Train Loss 25.034893 Test MSE 28.129396382938243 Test RE 0.08928312386614669 Lambda1 0.9708241\n",
      "160 Train Loss 25.007826 Test MSE 28.12853352983885 Test RE 0.08928175450133902 Lambda1 0.97270447\n",
      "161 Train Loss 24.989914 Test MSE 28.123755057953858 Test RE 0.08927417059239132 Lambda1 0.97302085\n",
      "162 Train Loss 24.982397 Test MSE 28.115687186574704 Test RE 0.08926136461775315 Lambda1 0.9726692\n",
      "163 Train Loss 24.972616 Test MSE 28.094812233137734 Test RE 0.08922822168209232 Lambda1 0.9710328\n",
      "164 Train Loss 24.959448 Test MSE 28.063474500018962 Test RE 0.08917844398021069 Lambda1 0.9687423\n",
      "165 Train Loss 24.942116 Test MSE 28.016280459786252 Test RE 0.08910342721794226 Lambda1 0.96568084\n",
      "166 Train Loss 24.914814 Test MSE 27.97767024984691 Test RE 0.08904200778445347 Lambda1 0.9614681\n",
      "167 Train Loss 24.875032 Test MSE 27.935544825679134 Test RE 0.088974948138256 Lambda1 0.9572219\n",
      "168 Train Loss 24.845062 Test MSE 27.938856436868456 Test RE 0.08898022173688931 Lambda1 0.9531012\n",
      "169 Train Loss 24.827732 Test MSE 27.94006118434291 Test RE 0.08898214016798288 Lambda1 0.95216894\n",
      "170 Train Loss 24.810928 Test MSE 27.925853346057252 Test RE 0.08895951307688701 Lambda1 0.9495438\n",
      "171 Train Loss 24.791615 Test MSE 27.927012329594987 Test RE 0.08896135906419096 Lambda1 0.94968003\n",
      "172 Train Loss 24.770615 Test MSE 27.92407094014005 Test RE 0.08895667405007689 Lambda1 0.95116305\n",
      "173 Train Loss 24.75 Test MSE 27.921671612869524 Test RE 0.08895285224416292 Lambda1 0.95329726\n",
      "174 Train Loss 24.724588 Test MSE 27.920012006828813 Test RE 0.08895020861869 Lambda1 0.9552117\n",
      "175 Train Loss 24.715237 Test MSE 27.909233102812518 Test RE 0.08893303673644538 Lambda1 0.9567532\n",
      "176 Train Loss 24.703268 Test MSE 27.91880257489228 Test RE 0.08894828203663685 Lambda1 0.955827\n",
      "177 Train Loss 24.692316 Test MSE 27.900505175682152 Test RE 0.08891912983769248 Lambda1 0.9547546\n",
      "178 Train Loss 24.67879 Test MSE 27.897096275223745 Test RE 0.08891369757544744 Lambda1 0.95210147\n",
      "179 Train Loss 24.667896 Test MSE 27.877369214617392 Test RE 0.08888225494655037 Lambda1 0.95099163\n",
      "180 Train Loss 24.659061 Test MSE 27.87456063027131 Test RE 0.08887777748679104 Lambda1 0.9506565\n",
      "181 Train Loss 24.639477 Test MSE 27.851705439710592 Test RE 0.08884133323348777 Lambda1 0.9508171\n",
      "182 Train Loss 24.627048 Test MSE 27.834641319175304 Test RE 0.08881411350904385 Lambda1 0.9496874\n",
      "183 Train Loss 24.620737 Test MSE 27.809085294878017 Test RE 0.08877333236804894 Lambda1 0.9494861\n",
      "184 Train Loss 24.605064 Test MSE 27.768483346914245 Test RE 0.08870850307284354 Lambda1 0.9510495\n",
      "185 Train Loss 24.590342 Test MSE 27.75032292584317 Test RE 0.0886794909149261 Lambda1 0.95173323\n",
      "186 Train Loss 24.569798 Test MSE 27.72993591654693 Test RE 0.08864691036115119 Lambda1 0.95237905\n",
      "187 Train Loss 24.553022 Test MSE 27.66872081228106 Test RE 0.08854901024310806 Lambda1 0.95442146\n",
      "188 Train Loss 24.51838 Test MSE 27.62752518333581 Test RE 0.08848306590456376 Lambda1 0.9541559\n",
      "189 Train Loss 24.473976 Test MSE 27.60044534102208 Test RE 0.08843969077602744 Lambda1 0.9538212\n",
      "190 Train Loss 24.448137 Test MSE 27.539382215999602 Test RE 0.08834180478007196 Lambda1 0.95476353\n",
      "191 Train Loss 24.428267 Test MSE 27.513467287991674 Test RE 0.08830022958552178 Lambda1 0.9552616\n",
      "192 Train Loss 24.408546 Test MSE 27.466076373707118 Test RE 0.08822414989538731 Lambda1 0.95577556\n",
      "193 Train Loss 24.396328 Test MSE 27.443977405534593 Test RE 0.08818865055827149 Lambda1 0.956475\n",
      "194 Train Loss 24.390255 Test MSE 27.446589094668884 Test RE 0.08819284666763441 Lambda1 0.9573791\n",
      "195 Train Loss 24.378098 Test MSE 27.41343029762826 Test RE 0.08813955675837373 Lambda1 0.95784175\n",
      "196 Train Loss 24.368673 Test MSE 27.409631638171977 Test RE 0.0881334498290951 Lambda1 0.957488\n",
      "197 Train Loss 24.361198 Test MSE 27.381242212043773 Test RE 0.088087796054785 Lambda1 0.95699394\n",
      "198 Train Loss 24.356901 Test MSE 27.362811430174503 Test RE 0.08805814436404402 Lambda1 0.9573701\n",
      "199 Train Loss 24.34492 Test MSE 27.34425102708528 Test RE 0.08802827404336307 Lambda1 0.9565136\n",
      "Training time: 347.50\n",
      "Training time: 347.50\n",
      "inv_HT_stan_late_Try\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 784.6142 Test MSE 776.6523529929152 Test RE 0.4691400413654282 Lambda1 0.051969297\n",
      "1 Train Loss 480.63727 Test MSE 443.71271085562466 Test RE 0.35460112214759826 Lambda1 0.031582773\n",
      "2 Train Loss 291.86264 Test MSE 295.240727441245 Test RE 0.28925262171964855 Lambda1 0.00047389284\n",
      "3 Train Loss 275.471 Test MSE 281.9132239613616 Test RE 0.2826486364810041 Lambda1 0.0023547267\n",
      "4 Train Loss 270.67468 Test MSE 278.4230386356781 Test RE 0.2808935423955666 Lambda1 0.005944819\n",
      "5 Train Loss 266.85492 Test MSE 272.9852174217725 Test RE 0.2781369805223629 Lambda1 0.018107226\n",
      "6 Train Loss 258.2833 Test MSE 262.12172550855854 Test RE 0.2725465442242219 Lambda1 0.045572754\n",
      "7 Train Loss 245.09232 Test MSE 248.8586441351818 Test RE 0.2655617585843694 Lambda1 0.07009513\n",
      "8 Train Loss 225.99373 Test MSE 222.92905334862954 Test RE 0.25134630582569256 Lambda1 0.1226969\n",
      "9 Train Loss 204.83826 Test MSE 197.87950044350518 Test RE 0.23680429402447234 Lambda1 0.17862125\n",
      "10 Train Loss 184.17809 Test MSE 178.50610012121624 Test RE 0.22491359202062805 Lambda1 0.21418898\n",
      "11 Train Loss 158.22638 Test MSE 150.0045182634889 Test RE 0.2061775340028251 Lambda1 0.28261083\n",
      "12 Train Loss 140.74928 Test MSE 132.5097275431593 Test RE 0.19378182921564108 Lambda1 0.3309927\n",
      "13 Train Loss 128.38416 Test MSE 116.59676546789366 Test RE 0.1817742753574583 Lambda1 0.3848103\n",
      "14 Train Loss 106.87673 Test MSE 99.42511119856158 Test RE 0.16785613281671252 Lambda1 0.4340188\n",
      "15 Train Loss 94.85632 Test MSE 90.01322909885438 Test RE 0.15971376274443858 Lambda1 0.46010247\n",
      "16 Train Loss 84.69902 Test MSE 77.8064714406723 Test RE 0.14848993966527604 Lambda1 0.49303684\n",
      "17 Train Loss 76.99709 Test MSE 71.56403444928165 Test RE 0.14240871860897347 Lambda1 0.51325315\n",
      "18 Train Loss 71.14956 Test MSE 65.56784194272637 Test RE 0.1363121649797349 Lambda1 0.53401244\n",
      "19 Train Loss 66.08641 Test MSE 60.84143634972931 Test RE 0.13130730863134835 Lambda1 0.5535541\n",
      "20 Train Loss 61.156906 Test MSE 56.14236650954074 Test RE 0.1261346853458993 Lambda1 0.57809865\n",
      "21 Train Loss 57.06965 Test MSE 52.58874093251246 Test RE 0.1220774804182436 Lambda1 0.5966108\n",
      "22 Train Loss 53.155575 Test MSE 49.668429165021614 Test RE 0.11863952060687448 Lambda1 0.6242786\n",
      "23 Train Loss 49.69384 Test MSE 47.768201631027694 Test RE 0.1163479180379454 Lambda1 0.6376862\n",
      "24 Train Loss 45.729374 Test MSE 45.436918101244196 Test RE 0.11347327876339486 Lambda1 0.6526225\n",
      "25 Train Loss 43.642467 Test MSE 43.33219924848425 Test RE 0.11081397567568062 Lambda1 0.6704249\n",
      "26 Train Loss 41.503506 Test MSE 41.32217639609189 Test RE 0.10821333066892361 Lambda1 0.67907006\n",
      "27 Train Loss 39.711113 Test MSE 39.54074833758363 Test RE 0.1058550576056164 Lambda1 0.697207\n",
      "28 Train Loss 38.39794 Test MSE 38.25374225579778 Test RE 0.10411807603441756 Lambda1 0.70502603\n",
      "29 Train Loss 37.686203 Test MSE 37.63088859590935 Test RE 0.10326696359500366 Lambda1 0.7145531\n",
      "30 Train Loss 36.307705 Test MSE 36.6730700095149 Test RE 0.10194426626054401 Lambda1 0.7249229\n",
      "31 Train Loss 35.653458 Test MSE 36.37212970645421 Test RE 0.10152512577917906 Lambda1 0.72532415\n",
      "32 Train Loss 34.29491 Test MSE 35.93559290682208 Test RE 0.100914036492423 Lambda1 0.7283564\n",
      "33 Train Loss 33.803707 Test MSE 35.57593489140107 Test RE 0.10040777224173834 Lambda1 0.73379815\n",
      "34 Train Loss 32.603172 Test MSE 34.64874808917395 Test RE 0.09909071095520586 Lambda1 0.74746263\n",
      "35 Train Loss 31.780714 Test MSE 34.068070321666774 Test RE 0.09825687287256825 Lambda1 0.7615308\n",
      "36 Train Loss 31.186113 Test MSE 33.6798243803915 Test RE 0.0976953919846051 Lambda1 0.7678454\n",
      "37 Train Loss 30.66637 Test MSE 33.4180594292593 Test RE 0.09731499936345309 Lambda1 0.7752804\n",
      "38 Train Loss 29.996052 Test MSE 32.95803917017068 Test RE 0.0966428777248901 Lambda1 0.79344463\n",
      "39 Train Loss 29.698551 Test MSE 32.623800962379974 Test RE 0.09615158551516494 Lambda1 0.802971\n",
      "40 Train Loss 29.243666 Test MSE 32.356906132669664 Test RE 0.09575747045301289 Lambda1 0.8124317\n",
      "41 Train Loss 29.079039 Test MSE 32.37149349609506 Test RE 0.09577905304217743 Lambda1 0.81433135\n",
      "42 Train Loss 28.88626 Test MSE 32.014015084342844 Test RE 0.09524874086362124 Lambda1 0.8253109\n",
      "43 Train Loss 28.69463 Test MSE 31.807875195714978 Test RE 0.09494158985576258 Lambda1 0.83103925\n",
      "44 Train Loss 28.524458 Test MSE 31.628467087785236 Test RE 0.09467345849871951 Lambda1 0.8345818\n",
      "45 Train Loss 28.258337 Test MSE 31.468551892839436 Test RE 0.09443381821085993 Lambda1 0.84044373\n",
      "46 Train Loss 28.093157 Test MSE 31.16408729073914 Test RE 0.09397587470232088 Lambda1 0.85069454\n",
      "47 Train Loss 27.920746 Test MSE 30.999009112891123 Test RE 0.09372664608815738 Lambda1 0.8535473\n",
      "48 Train Loss 27.809797 Test MSE 30.95453421811557 Test RE 0.09365938621330076 Lambda1 0.85711545\n",
      "49 Train Loss 27.69368 Test MSE 30.6787915450705 Test RE 0.09324129461611241 Lambda1 0.8659196\n",
      "50 Train Loss 27.645863 Test MSE 30.561523653379624 Test RE 0.09306291928987065 Lambda1 0.87173164\n",
      "51 Train Loss 27.509947 Test MSE 30.392262063610048 Test RE 0.09280485217587925 Lambda1 0.878239\n",
      "52 Train Loss 27.448738 Test MSE 30.34656948317056 Test RE 0.09273506322462086 Lambda1 0.8808484\n",
      "53 Train Loss 27.301104 Test MSE 30.329502134338313 Test RE 0.09270898178771074 Lambda1 0.8812955\n",
      "54 Train Loss 27.204332 Test MSE 30.30681386376524 Test RE 0.09267429938558121 Lambda1 0.8830856\n",
      "55 Train Loss 27.150713 Test MSE 30.11638614129307 Test RE 0.09238268897498825 Lambda1 0.89191103\n",
      "56 Train Loss 27.05478 Test MSE 29.99210660954532 Test RE 0.09219187679461188 Lambda1 0.89834833\n",
      "57 Train Loss 26.995373 Test MSE 29.876927335622447 Test RE 0.09201468337686103 Lambda1 0.9054902\n",
      "58 Train Loss 26.93946 Test MSE 29.802823620282474 Test RE 0.09190050056382691 Lambda1 0.9087056\n",
      "59 Train Loss 26.890217 Test MSE 29.791507026001096 Test RE 0.09188305088460806 Lambda1 0.911219\n",
      "60 Train Loss 26.826366 Test MSE 29.822739031765366 Test RE 0.09193120118888838 Lambda1 0.91178143\n",
      "61 Train Loss 26.762869 Test MSE 29.757688931971696 Test RE 0.09183088514530499 Lambda1 0.91759\n",
      "62 Train Loss 26.67848 Test MSE 29.661315126798097 Test RE 0.09168206194537641 Lambda1 0.924256\n",
      "63 Train Loss 26.623676 Test MSE 29.57600423967853 Test RE 0.09155012055850531 Lambda1 0.9324542\n",
      "64 Train Loss 26.563385 Test MSE 29.526038349558704 Test RE 0.09147275519063901 Lambda1 0.9396463\n",
      "65 Train Loss 26.507454 Test MSE 29.50581189028525 Test RE 0.09144141866483488 Lambda1 0.94166374\n",
      "66 Train Loss 26.482203 Test MSE 29.53421957520215 Test RE 0.09148542718220391 Lambda1 0.9426182\n",
      "67 Train Loss 26.46215 Test MSE 29.495050272975817 Test RE 0.09142474148776814 Lambda1 0.94675153\n",
      "68 Train Loss 26.427937 Test MSE 29.418067219890364 Test RE 0.09130535274084772 Lambda1 0.94880074\n",
      "69 Train Loss 26.390425 Test MSE 29.3557020875977 Test RE 0.09120851953508381 Lambda1 0.9553683\n",
      "70 Train Loss 26.324327 Test MSE 29.282364845252665 Test RE 0.09109451844326807 Lambda1 0.96227145\n",
      "71 Train Loss 26.28746 Test MSE 29.250157511719262 Test RE 0.09104440775818742 Lambda1 0.9631864\n",
      "72 Train Loss 26.261335 Test MSE 29.281521142191107 Test RE 0.09109320609574889 Lambda1 0.9596766\n",
      "73 Train Loss 26.23346 Test MSE 29.27747763087013 Test RE 0.09108691630764025 Lambda1 0.95581007\n",
      "74 Train Loss 26.191927 Test MSE 29.219136177579674 Test RE 0.09099611625367793 Lambda1 0.95692515\n",
      "75 Train Loss 26.160532 Test MSE 29.172469795758445 Test RE 0.09092342149025102 Lambda1 0.9599138\n",
      "76 Train Loss 26.12905 Test MSE 29.126250314391648 Test RE 0.09085136555414257 Lambda1 0.9617636\n",
      "77 Train Loss 26.102596 Test MSE 29.124352198734332 Test RE 0.09084840517978615 Lambda1 0.9598513\n",
      "78 Train Loss 26.049963 Test MSE 29.11615271578427 Test RE 0.09083561584148774 Lambda1 0.9611877\n",
      "79 Train Loss 25.973225 Test MSE 29.056567879399168 Test RE 0.09074262283402977 Lambda1 0.95735425\n",
      "80 Train Loss 25.94109 Test MSE 29.017016559191607 Test RE 0.09068084312388529 Lambda1 0.9578368\n",
      "81 Train Loss 25.906378 Test MSE 28.945291410602582 Test RE 0.09056869994118545 Lambda1 0.96237725\n",
      "82 Train Loss 25.845871 Test MSE 28.88881705136074 Test RE 0.09048030375782096 Lambda1 0.9621686\n",
      "83 Train Loss 25.799644 Test MSE 28.880625137419365 Test RE 0.09046747423604873 Lambda1 0.96163523\n",
      "84 Train Loss 25.776537 Test MSE 28.836371365960083 Test RE 0.09039813602510051 Lambda1 0.9648523\n",
      "85 Train Loss 25.75196 Test MSE 28.819532191099885 Test RE 0.09037173789937891 Lambda1 0.9670924\n",
      "86 Train Loss 25.724339 Test MSE 28.82880012535227 Test RE 0.09038626783681393 Lambda1 0.968451\n",
      "87 Train Loss 25.70687 Test MSE 28.83946214617109 Test RE 0.09040298048489771 Lambda1 0.96795195\n",
      "88 Train Loss 25.686811 Test MSE 28.797382419594705 Test RE 0.09033700280907936 Lambda1 0.9697198\n",
      "89 Train Loss 25.64959 Test MSE 28.736807588029862 Test RE 0.09024194157531461 Lambda1 0.97312367\n",
      "90 Train Loss 25.619131 Test MSE 28.69730914101677 Test RE 0.09017990193864464 Lambda1 0.9759707\n",
      "91 Train Loss 25.601665 Test MSE 28.683327280660595 Test RE 0.0901579306031367 Lambda1 0.97813493\n",
      "92 Train Loss 25.574125 Test MSE 28.688317822436574 Test RE 0.0901657734406422 Lambda1 0.978335\n",
      "93 Train Loss 25.549406 Test MSE 28.659178370608117 Test RE 0.09011996997559604 Lambda1 0.9792724\n",
      "94 Train Loss 25.51039 Test MSE 28.618769901153325 Test RE 0.09005641451442188 Lambda1 0.976791\n",
      "95 Train Loss 25.476606 Test MSE 28.601704208657075 Test RE 0.09002955968689066 Lambda1 0.975507\n",
      "96 Train Loss 25.463049 Test MSE 28.60656030284717 Test RE 0.09003720212426315 Lambda1 0.97317487\n",
      "97 Train Loss 25.446115 Test MSE 28.594948888935946 Test RE 0.09001892720201074 Lambda1 0.97168773\n",
      "98 Train Loss 25.434763 Test MSE 28.574394479304868 Test RE 0.08998656801624666 Lambda1 0.9706002\n",
      "99 Train Loss 25.411276 Test MSE 28.574326990260097 Test RE 0.08998646174783372 Lambda1 0.96926206\n",
      "100 Train Loss 25.393143 Test MSE 28.570747772136883 Test RE 0.08998082572250515 Lambda1 0.9695784\n",
      "101 Train Loss 25.377504 Test MSE 28.535139617265283 Test RE 0.08992473600899273 Lambda1 0.97014004\n",
      "102 Train Loss 25.356483 Test MSE 28.510290340849 Test RE 0.08988557286934194 Lambda1 0.9699982\n",
      "103 Train Loss 25.33033 Test MSE 28.441885115868207 Test RE 0.0897776761182372 Lambda1 0.97324306\n",
      "104 Train Loss 25.312597 Test MSE 28.414502853353984 Test RE 0.0897344492390631 Lambda1 0.9748032\n",
      "105 Train Loss 25.283998 Test MSE 28.374982666063566 Test RE 0.08967202414203407 Lambda1 0.9788226\n",
      "106 Train Loss 25.252977 Test MSE 28.386129970067053 Test RE 0.08968963654324198 Lambda1 0.9803462\n",
      "107 Train Loss 25.22767 Test MSE 28.35144772817922 Test RE 0.08963482829006092 Lambda1 0.9802738\n",
      "108 Train Loss 25.203238 Test MSE 28.336394350258868 Test RE 0.08961102904511031 Lambda1 0.9809389\n",
      "109 Train Loss 25.186651 Test MSE 28.330111441759435 Test RE 0.08960109395772017 Lambda1 0.9808719\n",
      "110 Train Loss 25.16531 Test MSE 28.3158706087342 Test RE 0.08957857102106662 Lambda1 0.98007774\n",
      "111 Train Loss 25.140322 Test MSE 28.28871209788937 Test RE 0.08953560211059326 Lambda1 0.9800181\n",
      "112 Train Loss 25.11937 Test MSE 28.271150897537144 Test RE 0.08950780662911677 Lambda1 0.978763\n",
      "113 Train Loss 25.102896 Test MSE 28.26113395873614 Test RE 0.08949194817223788 Lambda1 0.9773947\n",
      "114 Train Loss 25.082392 Test MSE 28.29746215724794 Test RE 0.0895494482916723 Lambda1 0.974471\n",
      "115 Train Loss 25.053284 Test MSE 28.273517195339927 Test RE 0.08951155245635861 Lambda1 0.97398037\n",
      "116 Train Loss 25.032322 Test MSE 28.24022073531127 Test RE 0.0894588300482971 Lambda1 0.9753933\n",
      "117 Train Loss 25.017406 Test MSE 28.213941560491357 Test RE 0.0894171970273639 Lambda1 0.9765596\n",
      "118 Train Loss 25.001986 Test MSE 28.178772014620836 Test RE 0.0893614490059875 Lambda1 0.9771995\n",
      "119 Train Loss 24.980335 Test MSE 28.18292867103062 Test RE 0.0893680396257391 Lambda1 0.9750059\n",
      "120 Train Loss 24.967363 Test MSE 28.20249187244128 Test RE 0.08939905169131006 Lambda1 0.97568226\n",
      "121 Train Loss 24.961056 Test MSE 28.209999561343366 Test RE 0.08941095020738923 Lambda1 0.9741993\n",
      "122 Train Loss 24.953266 Test MSE 28.209655916465884 Test RE 0.08941040561843001 Lambda1 0.9738651\n",
      "123 Train Loss 24.94531 Test MSE 28.194576508801926 Test RE 0.08938650535915924 Lambda1 0.9729474\n",
      "124 Train Loss 24.935911 Test MSE 28.18552951761057 Test RE 0.08937216317160437 Lambda1 0.97237986\n",
      "125 Train Loss 24.926392 Test MSE 28.186341678888944 Test RE 0.08937345078423156 Lambda1 0.9738435\n",
      "126 Train Loss 24.917843 Test MSE 28.154920288964878 Test RE 0.0893236213109373 Lambda1 0.9735726\n",
      "127 Train Loss 24.911924 Test MSE 28.146987464825227 Test RE 0.0893110366811204 Lambda1 0.9736059\n",
      "128 Train Loss 24.900719 Test MSE 28.13533908755463 Test RE 0.08929255448465218 Lambda1 0.9733758\n",
      "129 Train Loss 24.88227 Test MSE 28.11517659079891 Test RE 0.08926055409723593 Lambda1 0.97392684\n",
      "130 Train Loss 24.871183 Test MSE 28.103732034741704 Test RE 0.08924238505976553 Lambda1 0.97327703\n",
      "131 Train Loss 24.86705 Test MSE 28.09489420452919 Test RE 0.08922835185125118 Lambda1 0.97371715\n",
      "132 Train Loss 24.854458 Test MSE 28.08761601474295 Test RE 0.08921679347094323 Lambda1 0.97377896\n",
      "133 Train Loss 24.823307 Test MSE 28.039863419586496 Test RE 0.08914092114097247 Lambda1 0.97133\n",
      "134 Train Loss 24.796999 Test MSE 27.99106266543296 Test RE 0.08906331665132278 Lambda1 0.96896785\n",
      "135 Train Loss 24.787844 Test MSE 27.964929177824338 Test RE 0.08902173054521807 Lambda1 0.9686587\n",
      "136 Train Loss 24.781136 Test MSE 27.95460184550113 Test RE 0.08900529134946976 Lambda1 0.96729666\n",
      "137 Train Loss 24.773706 Test MSE 27.920481284379267 Test RE 0.08895095614989816 Lambda1 0.9659596\n",
      "138 Train Loss 24.767002 Test MSE 27.896952184307157 Test RE 0.08891346795167947 Lambda1 0.9640518\n",
      "139 Train Loss 24.752916 Test MSE 27.837373258804977 Test RE 0.08881847190619893 Lambda1 0.96351534\n",
      "140 Train Loss 24.738129 Test MSE 27.797607095853238 Test RE 0.08875500988222049 Lambda1 0.9642845\n",
      "141 Train Loss 24.721071 Test MSE 27.766853854638384 Test RE 0.08870590026700408 Lambda1 0.96368414\n",
      "142 Train Loss 24.696127 Test MSE 27.7359217914492 Test RE 0.08865647765125846 Lambda1 0.9627538\n",
      "143 Train Loss 24.676756 Test MSE 27.708313979878977 Test RE 0.0886123431759794 Lambda1 0.9618995\n",
      "144 Train Loss 24.658634 Test MSE 27.68357304245668 Test RE 0.088572773066281 Lambda1 0.9618361\n",
      "145 Train Loss 24.63708 Test MSE 27.68218806145268 Test RE 0.08857055743569203 Lambda1 0.95943266\n",
      "146 Train Loss 24.607975 Test MSE 27.67425737750937 Test RE 0.08855786921514959 Lambda1 0.9561408\n",
      "147 Train Loss 24.582993 Test MSE 27.63869309897407 Test RE 0.08850094791836265 Lambda1 0.953358\n",
      "148 Train Loss 24.555412 Test MSE 27.653671105391282 Test RE 0.08852492496064317 Lambda1 0.9533862\n",
      "149 Train Loss 24.542124 Test MSE 27.640418061630584 Test RE 0.08850370959829842 Lambda1 0.95328087\n",
      "150 Train Loss 24.523123 Test MSE 27.646283685803926 Test RE 0.08851309986799075 Lambda1 0.95409334\n",
      "151 Train Loss 24.499498 Test MSE 27.589073536192814 Test RE 0.08842146964762769 Lambda1 0.95406383\n",
      "152 Train Loss 24.4799 Test MSE 27.560737501509966 Test RE 0.08837605026275229 Lambda1 0.95358086\n",
      "153 Train Loss 24.439276 Test MSE 27.53754977079673 Test RE 0.08833886563989692 Lambda1 0.9531947\n",
      "154 Train Loss 24.422558 Test MSE 27.53040051089255 Test RE 0.08832739768955827 Lambda1 0.9520041\n",
      "155 Train Loss 24.412685 Test MSE 27.51144384887632 Test RE 0.08829698256794853 Lambda1 0.95110387\n",
      "156 Train Loss 24.403769 Test MSE 27.495958006429507 Test RE 0.08827212844472085 Lambda1 0.94964904\n",
      "157 Train Loss 24.389696 Test MSE 27.484518333861175 Test RE 0.0882537637581729 Lambda1 0.95020086\n",
      "158 Train Loss 24.38306 Test MSE 27.473818175756488 Test RE 0.08823658279210049 Lambda1 0.95022297\n",
      "159 Train Loss 24.362066 Test MSE 27.449812135917465 Test RE 0.08819802473980458 Lambda1 0.9504316\n",
      "160 Train Loss 24.338625 Test MSE 27.409789665725466 Test RE 0.08813370389112639 Lambda1 0.9488155\n",
      "161 Train Loss 24.324755 Test MSE 27.386142802216416 Test RE 0.08809567851078999 Lambda1 0.94971496\n",
      "162 Train Loss 24.317062 Test MSE 27.382156699148403 Test RE 0.0880892670340845 Lambda1 0.9492602\n",
      "163 Train Loss 24.301592 Test MSE 27.388160902789036 Test RE 0.08809892436153585 Lambda1 0.9498909\n",
      "164 Train Loss 24.282087 Test MSE 27.35118637183382 Test RE 0.08803943667717598 Lambda1 0.9489832\n",
      "165 Train Loss 24.262278 Test MSE 27.364826759560582 Test RE 0.08806138713935065 Lambda1 0.94972265\n",
      "166 Train Loss 24.251167 Test MSE 27.353826965717225 Test RE 0.08804368641581703 Lambda1 0.950453\n",
      "167 Train Loss 24.246523 Test MSE 27.34233028230341 Test RE 0.08802518229992608 Lambda1 0.95016307\n",
      "168 Train Loss 24.238243 Test MSE 27.312144518371806 Test RE 0.08797657925278039 Lambda1 0.9513733\n",
      "169 Train Loss 24.224144 Test MSE 27.295103672400717 Test RE 0.08794912938966108 Lambda1 0.9510604\n",
      "170 Train Loss 24.216314 Test MSE 27.27922570298086 Test RE 0.08792354500743486 Lambda1 0.95165205\n",
      "171 Train Loss 24.208195 Test MSE 27.273356751027446 Test RE 0.08791408640284723 Lambda1 0.9513013\n",
      "172 Train Loss 24.201498 Test MSE 27.23774428026787 Test RE 0.08785667028500674 Lambda1 0.9512394\n",
      "173 Train Loss 24.19611 Test MSE 27.221625207339937 Test RE 0.08783067001049563 Lambda1 0.9511105\n",
      "174 Train Loss 24.186277 Test MSE 27.2217252615416 Test RE 0.08783083142296506 Lambda1 0.9507739\n",
      "175 Train Loss 24.18099 Test MSE 27.2048893563025 Test RE 0.08780366671817019 Lambda1 0.95014477\n",
      "176 Train Loss 24.177385 Test MSE 27.211451733396714 Test RE 0.08781425610193949 Lambda1 0.9501414\n",
      "177 Train Loss 24.172735 Test MSE 27.19687139451789 Test RE 0.0877907267955923 Lambda1 0.9500776\n",
      "178 Train Loss 24.16582 Test MSE 27.16997954612762 Test RE 0.0877473130006052 Lambda1 0.950233\n",
      "179 Train Loss 24.152502 Test MSE 27.136303667238376 Test RE 0.08769291685939053 Lambda1 0.94985586\n",
      "180 Train Loss 24.140835 Test MSE 27.13394010359142 Test RE 0.08768909776327555 Lambda1 0.95133877\n",
      "181 Train Loss 24.127226 Test MSE 27.111946879471024 Test RE 0.08765355266869083 Lambda1 0.95150423\n",
      "182 Train Loss 24.117624 Test MSE 27.112529113166907 Test RE 0.08765449385115484 Lambda1 0.9513168\n",
      "183 Train Loss 24.108902 Test MSE 27.094819094131736 Test RE 0.08762586103100267 Lambda1 0.95250404\n",
      "184 Train Loss 24.101929 Test MSE 27.089290658270695 Test RE 0.08761692097009413 Lambda1 0.9521625\n",
      "185 Train Loss 24.096363 Test MSE 27.108360606371075 Test RE 0.08764775522494031 Lambda1 0.95205003\n",
      "186 Train Loss 24.08925 Test MSE 27.08919213210073 Test RE 0.08761676163466511 Lambda1 0.95272326\n",
      "187 Train Loss 24.083393 Test MSE 27.040255629459786 Test RE 0.08753758621987479 Lambda1 0.9531086\n",
      "188 Train Loss 24.07487 Test MSE 27.02265148971593 Test RE 0.08750908658550759 Lambda1 0.9539693\n",
      "189 Train Loss 24.06698 Test MSE 27.013604608426487 Test RE 0.08749443682785438 Lambda1 0.95523435\n",
      "190 Train Loss 24.051722 Test MSE 26.99900391471501 Test RE 0.08747078851885799 Lambda1 0.9577746\n",
      "191 Train Loss 24.040071 Test MSE 26.939136932830028 Test RE 0.08737375682395707 Lambda1 0.9605615\n",
      "192 Train Loss 24.029037 Test MSE 26.93507599219327 Test RE 0.08736717099655306 Lambda1 0.9625876\n",
      "193 Train Loss 24.019331 Test MSE 26.90579429784491 Test RE 0.08731966872738321 Lambda1 0.96337664\n",
      "194 Train Loss 24.009153 Test MSE 26.902648181196493 Test RE 0.08731456339780823 Lambda1 0.9641658\n",
      "195 Train Loss 23.992092 Test MSE 26.875039733066654 Test RE 0.08726974925234549 Lambda1 0.9647287\n",
      "196 Train Loss 23.97918 Test MSE 26.850725130663502 Test RE 0.0872302626239454 Lambda1 0.9645758\n",
      "197 Train Loss 23.966887 Test MSE 26.852403246781837 Test RE 0.0872329884390197 Lambda1 0.96494526\n",
      "198 Train Loss 23.958124 Test MSE 26.826513196309534 Test RE 0.08719092495783119 Lambda1 0.96593535\n",
      "199 Train Loss 23.949217 Test MSE 26.81391566444703 Test RE 0.0871704504476604 Lambda1 0.96677345\n",
      "Training time: 344.75\n",
      "Training time: 344.75\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10 #10\n",
    "max_iter = 200 #75\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "\n",
    "lambda1_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 1.0\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    'Generate Training data'\n",
    "    print(reps)\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "\n",
    "    lambda1_val = []\n",
    "\n",
    "    N_f = 50000 #Total number of collocation points \n",
    "    N_train = 5000\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "    lambda1_full.append(lambda1_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"lambda1\": lambda1_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
