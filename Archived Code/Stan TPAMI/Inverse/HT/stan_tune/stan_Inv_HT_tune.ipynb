{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1]).reshape(-1,1)\n",
    "b_value = np.array([0.0,0.25,0.5,1.0]).reshape(-1,1)\n",
    "\n",
    "\n",
    "LR_tune, B_value = np.meshgrid(lr_tune,b_value)\n",
    "\n",
    "LR_tune = LR_tune.flatten('F').reshape(-1,1)\n",
    "B_value = B_value.flatten('F').reshape(-1,1)\n",
    "\n",
    "\n",
    "lrb_tune = np.hstack((LR_tune,B_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_2D_4(xt): #True function for 2D_4 Heat Transfer in a rod x \\in [0,1] t \\in [0,0.1]\n",
    "    term1 = 4*u0/np.pi\n",
    "    \n",
    "    resol_n = 10000\n",
    "    \n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "\n",
    "    u = np.zeros((np.shape(xt)[0],1))\n",
    "    \n",
    "    for i in range(resol_n):\n",
    "        j = 2*i-1\n",
    "        term2 = np.sin(j*np.pi*x)/j\n",
    "        term3 = np.exp(-1*np.square(j*np.pi)*t)\n",
    "        \n",
    "        u = u + term2*term3\n",
    "        \n",
    "    u = term1*u\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = 50.0\n",
    "loss_thresh = 0.1\n",
    "\n",
    "x_ll = np.array(0.0)\n",
    "x_ul = np.array(1.0)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,100).reshape(-1,1)\n",
    "t = np.linspace(0,0.1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = true_2D_4(xt)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_f,N_train,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #X_Train\n",
    "    np.random.seed(seed)\n",
    "    x_train = np.random.uniform(x_ll,x_ul,(N_train,1))\n",
    "    t_train = np.random.uniform(0,0.1,(N_train,1))\n",
    "    \n",
    "    xt_train = np.hstack((x_train,t_train))\n",
    "    u_train = true_2D_4(xt_train)\n",
    "    \n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requiresGrad = True\n",
    "        \n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    \n",
    "    def loss_PDE(self, xt_coll,f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_tt[:,[0]]\n",
    "                \n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        f = du_dt - self.lambda1*d2u_dx2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_train = self.loss_function(self.forward(xt_train),u_train)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "        \n",
    "        #print(self.iter,\"train_loss\",loss_train.cpu().detach().numpy(),\"F Loss\",(loss_f).cpu().detach().numpy())\n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                    \n",
    "    \n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_coll,f_hat, xt_train, u_train,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xt_coll, xt_train, u_train = trainingdata(N_f,N_train,123)\n",
    "    \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_train = torch.from_numpy(xt_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    nan_flag = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_coll,f_hat, xt_train, u_train,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_coll,f_hat, xt_train, u_train).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1],\"Lambda1\",lambda1_val[-1])\n",
    "\n",
    "        if(np.isnan(loss_np)):\n",
    "            nan_flag =1\n",
    "            print(\"NAN BREAK!\")\n",
    "            break\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_HT_stan_tune0\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.29047 Test MSE 858.052231756559 Test RE 0.4931125244857968 Lambda1 -0.04077347\n",
      "1 Train Loss 838.0643 Test MSE 858.2624949987687 Test RE 0.49317293869094725 Lambda1 -0.040838197\n",
      "2 Train Loss 838.0606 Test MSE 858.3053504694974 Test RE 0.4931852512962012 Lambda1 -0.04083841\n",
      "3 Train Loss 838.0304 Test MSE 858.386952692876 Test RE 0.49320869520023064 Lambda1 -0.040753536\n",
      "4 Train Loss 837.9046 Test MSE 857.95619500097 Test RE 0.49308492811671273 Lambda1 -0.044972103\n",
      "5 Train Loss 837.8989 Test MSE 857.9314376541628 Test RE 0.4930778137892514 Lambda1 -0.049783602\n",
      "6 Train Loss 837.8698 Test MSE 857.9189431704415 Test RE 0.4930742233070299 Lambda1 -0.0888808\n",
      "7 Train Loss 835.5527 Test MSE 851.3425291447785 Test RE 0.4911807470157248 Lambda1 -1.2372314\n",
      "8 Train Loss 831.4302 Test MSE 848.4846417454117 Test RE 0.49035562712529357 Lambda1 -1.1643729\n",
      "9 Train Loss 827.17194 Test MSE 842.6286977584567 Test RE 0.4886605659365822 Lambda1 -1.700886\n",
      "10 Train Loss 815.21674 Test MSE 827.749108998437 Test RE 0.4843268339104193 Lambda1 -1.7362456\n",
      "11 Train Loss 800.13354 Test MSE 808.4145463580436 Test RE 0.47863695877662904 Lambda1 -1.5189033\n",
      "12 Train Loss 784.08673 Test MSE 787.0503496138069 Test RE 0.4722700760625604 Lambda1 -1.550926\n",
      "13 Train Loss 772.27295 Test MSE 763.9410382999913 Test RE 0.4652850418837364 Lambda1 -1.5676645\n",
      "14 Train Loss 763.0195 Test MSE 761.1039506582337 Test RE 0.4644202615893255 Lambda1 -1.5327802\n",
      "15 Train Loss 744.5566 Test MSE 742.4641913775554 Test RE 0.45869808497334713 Lambda1 -1.5247469\n",
      "16 Train Loss 726.5127 Test MSE 710.5851863484866 Test RE 0.448742543645282 Lambda1 -1.4328597\n",
      "17 Train Loss 716.00006 Test MSE 701.3748803212399 Test RE 0.44582485237293407 Lambda1 -1.454234\n",
      "18 Train Loss 701.33527 Test MSE 688.834719797076 Test RE 0.4418213365959566 Lambda1 -1.4600571\n",
      "19 Train Loss 693.04407 Test MSE 684.6743756870538 Test RE 0.44048508525370406 Lambda1 -1.4170135\n",
      "20 Train Loss 679.6182 Test MSE 673.2334480447056 Test RE 0.43678932219400707 Lambda1 -1.4492985\n",
      "21 Train Loss 667.48834 Test MSE 657.6636177119808 Test RE 0.43170897668578345 Lambda1 -1.459753\n",
      "22 Train Loss 659.5412 Test MSE 648.0805553346642 Test RE 0.4285521381912772 Lambda1 -1.5652589\n",
      "23 Train Loss 648.2539 Test MSE 630.6212468360795 Test RE 0.4227401236205586 Lambda1 -1.5895152\n",
      "24 Train Loss 640.54626 Test MSE 625.0746028257375 Test RE 0.4208769072304223 Lambda1 -1.5550131\n",
      "25 Train Loss 636.1733 Test MSE 622.4373954367678 Test RE 0.41998812301373445 Lambda1 -1.544588\n",
      "26 Train Loss 630.8949 Test MSE 614.971927016385 Test RE 0.4174618718900515 Lambda1 -1.5366154\n",
      "27 Train Loss 614.0539 Test MSE 603.2393511479411 Test RE 0.41346047830352606 Lambda1 -1.5607084\n",
      "28 Train Loss 605.38153 Test MSE 592.942795966243 Test RE 0.4099166596609403 Lambda1 -1.5724063\n",
      "29 Train Loss 603.376 Test MSE 591.3498643043326 Test RE 0.40936567197008267 Lambda1 -1.5788924\n",
      "30 Train Loss 595.19965 Test MSE 583.3254014632935 Test RE 0.40657869253340834 Lambda1 -1.5919945\n",
      "31 Train Loss 593.623 Test MSE 581.1265407757712 Test RE 0.40581166439493943 Lambda1 -1.606276\n",
      "32 Train Loss 584.67145 Test MSE 573.3311750209957 Test RE 0.40308064911761315 Lambda1 -1.7255188\n",
      "33 Train Loss 572.3044 Test MSE 563.050526378309 Test RE 0.399450395082575 Lambda1 -1.8233086\n",
      "34 Train Loss 562.0538 Test MSE 555.5770791950123 Test RE 0.3967905594536292 Lambda1 -1.8836902\n",
      "35 Train Loss 556.25757 Test MSE 552.1040171885461 Test RE 0.3955483927850867 Lambda1 -1.9121749\n",
      "36 Train Loss 530.3576 Test MSE 516.738344007345 Test RE 0.3826700870547314 Lambda1 -2.0362575\n",
      "37 Train Loss 519.64246 Test MSE 497.05158846375923 Test RE 0.3753097990475748 Lambda1 -2.178868\n",
      "38 Train Loss 508.14883 Test MSE 496.1249974837187 Test RE 0.3749598143532678 Lambda1 -2.2378612\n",
      "39 Train Loss 481.19373 Test MSE 468.2349266550608 Test RE 0.3642680437209263 Lambda1 -2.364413\n",
      "40 Train Loss 458.83774 Test MSE 456.6654342748804 Test RE 0.35973959454777754 Lambda1 -2.4220753\n",
      "41 Train Loss 448.89948 Test MSE 448.65400528499606 Test RE 0.35657011839268965 Lambda1 -2.472944\n",
      "42 Train Loss 439.71838 Test MSE 440.35706304298833 Test RE 0.3532577137538344 Lambda1 -2.4957578\n",
      "43 Train Loss 435.91837 Test MSE 438.304238756505 Test RE 0.3524333564380507 Lambda1 -2.5229225\n",
      "44 Train Loss 424.38403 Test MSE 422.8938924074646 Test RE 0.346182314426348 Lambda1 -2.6139438\n",
      "45 Train Loss 421.66748 Test MSE 421.2623729501058 Test RE 0.3455138855411932 Lambda1 -2.6252763\n",
      "46 Train Loss 411.2598 Test MSE 416.3471342565053 Test RE 0.34349226400229227 Lambda1 -2.6483972\n",
      "47 Train Loss 404.4211 Test MSE 414.50751972189073 Test RE 0.3427325699212494 Lambda1 -2.6771812\n",
      "48 Train Loss 397.1654 Test MSE 407.9183940719204 Test RE 0.33999757166480876 Lambda1 -2.7406573\n",
      "49 Train Loss 387.62265 Test MSE 397.7809687261077 Test RE 0.33574625038719247 Lambda1 -2.8076875\n",
      "50 Train Loss 385.20813 Test MSE 392.9930474131204 Test RE 0.33371951536549865 Lambda1 -2.837455\n",
      "51 Train Loss 380.8446 Test MSE 388.7316470489861 Test RE 0.3319052483329721 Lambda1 -2.875912\n",
      "52 Train Loss 378.833 Test MSE 386.5169360421144 Test RE 0.33095842012208126 Lambda1 -2.8780904\n",
      "53 Train Loss 375.87244 Test MSE 383.28664478808196 Test RE 0.32957253623077915 Lambda1 -2.869574\n",
      "54 Train Loss 372.42075 Test MSE 380.57695584201383 Test RE 0.3284054944944786 Lambda1 -2.875781\n",
      "55 Train Loss 360.6211 Test MSE 368.1468006504389 Test RE 0.3229978904882364 Lambda1 -2.9245782\n",
      "56 Train Loss 350.83597 Test MSE 360.3866553280236 Test RE 0.3195755324962066 Lambda1 -2.9263945\n",
      "57 Train Loss 347.33353 Test MSE 355.7748749681096 Test RE 0.31752418353780437 Lambda1 -2.90105\n",
      "58 Train Loss 340.83063 Test MSE 346.2188896385404 Test RE 0.3132308653117529 Lambda1 -2.910898\n",
      "59 Train Loss 337.3259 Test MSE 340.8824383131483 Test RE 0.3108074959881788 Lambda1 -2.9525034\n",
      "60 Train Loss 322.6068 Test MSE 329.83034090149795 Test RE 0.3057274770858397 Lambda1 -3.0057135\n",
      "61 Train Loss 316.4822 Test MSE 322.8641289184224 Test RE 0.3024816746897278 Lambda1 -3.0102966\n",
      "62 Train Loss 314.51282 Test MSE 323.8528775097577 Test RE 0.3029444850120338 Lambda1 -3.0117874\n",
      "63 Train Loss 312.44785 Test MSE 325.12645861865354 Test RE 0.3035395789907071 Lambda1 -3.0343747\n",
      "64 Train Loss 305.9477 Test MSE 319.9403434296525 Test RE 0.301108956598437 Lambda1 -3.0779548\n",
      "65 Train Loss 300.02585 Test MSE 311.1776304038882 Test RE 0.29695685517537995 Lambda1 -3.112905\n",
      "66 Train Loss 290.80057 Test MSE 302.43215803063697 Test RE 0.29275421378093835 Lambda1 -3.1233413\n",
      "67 Train Loss 285.1102 Test MSE 297.78865082907595 Test RE 0.2904980635148827 Lambda1 -3.098459\n",
      "68 Train Loss 281.50574 Test MSE 293.261280088133 Test RE 0.2882813409996299 Lambda1 -3.1124403\n",
      "69 Train Loss 278.2965 Test MSE 286.2559555226653 Test RE 0.2848173465387503 Lambda1 -3.1409838\n",
      "70 Train Loss 273.708 Test MSE 273.9072575370039 Test RE 0.27860630496125627 Lambda1 -3.1656191\n",
      "71 Train Loss 268.84827 Test MSE 269.314146379162 Test RE 0.27626047476099463 Lambda1 -3.1659164\n",
      "72 Train Loss 266.37582 Test MSE 267.8083106959902 Test RE 0.2754870545294255 Lambda1 -3.1570098\n",
      "73 Train Loss 265.67618 Test MSE 267.27174856125504 Test RE 0.27521094279417496 Lambda1 -3.15686\n",
      "74 Train Loss 264.17563 Test MSE 266.2709705631254 Test RE 0.27469520671550235 Lambda1 -3.1752698\n",
      "Training time: 145.81\n",
      "Training time: 145.81\n",
      "inv_HT_stan_tune0\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 838.5584 Test MSE 858.0673331218202 Test RE 0.49311686375519515 Lambda1 -0.045517873\n",
      "1 Train Loss 838.03406 Test MSE 858.1834192835424 Test RE 0.4931502190093856 Lambda1 -0.04571812\n",
      "2 Train Loss 838.0248 Test MSE 858.2525266353397 Test RE 0.49317007468291385 Lambda1 -0.045733\n",
      "3 Train Loss 837.99133 Test MSE 858.3246163173344 Test RE 0.4931907863768592 Lambda1 -0.045677032\n",
      "4 Train Loss 837.86426 Test MSE 857.9083533103584 Test RE 0.49307118012725726 Lambda1 -0.051355284\n",
      "5 Train Loss 837.8577 Test MSE 857.852766159421 Test RE 0.49305520588828683 Lambda1 -0.054048974\n",
      "6 Train Loss 837.8537 Test MSE 857.8300561966475 Test RE 0.49304867951224657 Lambda1 -0.059168454\n",
      "7 Train Loss 837.8245 Test MSE 857.7935742123682 Test RE 0.493038195159727 Lambda1 -0.09518738\n",
      "8 Train Loss 837.2868 Test MSE 857.3657799538722 Test RE 0.492915237131654 Lambda1 -0.48834583\n",
      "9 Train Loss 836.5898 Test MSE 855.1002057334423 Test RE 0.4922635463390581 Lambda1 -0.55545044\n",
      "10 Train Loss 830.1159 Test MSE 846.5625588461727 Test RE 0.48979990785171856 Lambda1 -0.45223144\n",
      "11 Train Loss 812.44385 Test MSE 818.9450318711298 Test RE 0.4817442580162324 Lambda1 -0.5596913\n",
      "12 Train Loss 782.4513 Test MSE 789.7772123137752 Test RE 0.473087496525141 Lambda1 -0.50900644\n",
      "13 Train Loss 759.2054 Test MSE 762.6709878720008 Test RE 0.46489811328023795 Lambda1 -0.48799926\n",
      "14 Train Loss 744.73773 Test MSE 741.6259799028776 Test RE 0.45843908623562035 Lambda1 -0.4882845\n",
      "15 Train Loss 729.22076 Test MSE 730.6813600277586 Test RE 0.45504378261552897 Lambda1 -0.47157845\n",
      "16 Train Loss 726.8063 Test MSE 729.1136681990356 Test RE 0.45455536761931276 Lambda1 -0.45984533\n",
      "17 Train Loss 721.2751 Test MSE 722.397072712976 Test RE 0.4524568413913668 Lambda1 -0.40699986\n",
      "18 Train Loss 715.7586 Test MSE 717.3238963370364 Test RE 0.45086530840883776 Lambda1 -0.3984456\n",
      "19 Train Loss 711.4097 Test MSE 712.9307832255827 Test RE 0.44948256886943067 Lambda1 -0.45429215\n",
      "20 Train Loss 701.84607 Test MSE 704.0739102625648 Test RE 0.4466818400208392 Lambda1 -0.5332901\n",
      "21 Train Loss 689.8795 Test MSE 690.8546574393499 Test RE 0.4424686604205417 Lambda1 -0.5867755\n",
      "22 Train Loss 676.7129 Test MSE 674.0524765749959 Test RE 0.4370549315304793 Lambda1 -0.61441886\n",
      "23 Train Loss 665.7107 Test MSE 663.7447597003917 Test RE 0.43370030073378124 Lambda1 -0.6497467\n",
      "24 Train Loss 652.6366 Test MSE 648.7608780851208 Test RE 0.4287770155560121 Lambda1 -0.70083785\n",
      "25 Train Loss 645.4895 Test MSE 646.0132792201921 Test RE 0.4278680849358406 Lambda1 -0.7257384\n",
      "26 Train Loss 639.0753 Test MSE 639.9541581880981 Test RE 0.4258568165841513 Lambda1 -0.7647957\n",
      "27 Train Loss 626.1331 Test MSE 628.8107124342528 Test RE 0.42213283700239657 Lambda1 -0.80790395\n",
      "28 Train Loss 623.27783 Test MSE 626.9343763522635 Test RE 0.4215025560537766 Lambda1 -0.8066019\n",
      "29 Train Loss 620.4958 Test MSE 625.2528374947542 Test RE 0.4209369076761444 Lambda1 -0.8258251\n",
      "30 Train Loss 615.3127 Test MSE 623.5902280477163 Test RE 0.42037687859507944 Lambda1 -0.86132854\n",
      "31 Train Loss 612.9371 Test MSE 621.0697676621808 Test RE 0.419526467523337 Lambda1 -0.8555052\n",
      "32 Train Loss 612.0912 Test MSE 619.6493452437214 Test RE 0.4190464522835031 Lambda1 -0.8560818\n",
      "33 Train Loss 610.44147 Test MSE 617.7735444435738 Test RE 0.4184117034057819 Lambda1 -0.88782436\n",
      "34 Train Loss 605.99255 Test MSE 614.5037426568944 Test RE 0.41730293265719354 Lambda1 -0.9406708\n",
      "35 Train Loss 603.53314 Test MSE 611.9349174102028 Test RE 0.4164297882916077 Lambda1 -0.9764939\n",
      "36 Train Loss 601.7438 Test MSE 608.6903487597125 Test RE 0.4153243351293898 Lambda1 -0.99141717\n",
      "37 Train Loss 595.1231 Test MSE 601.5396785506716 Test RE 0.41287758931150903 Lambda1 -1.06447\n",
      "38 Train Loss 590.86523 Test MSE 596.0176107062745 Test RE 0.4109781347193127 Lambda1 -1.1011308\n",
      "39 Train Loss 581.72675 Test MSE 583.0878486162674 Test RE 0.4064958967551882 Lambda1 -1.1299113\n",
      "40 Train Loss 562.2823 Test MSE 564.449979947254 Test RE 0.3999465009662403 Lambda1 -1.2011018\n",
      "41 Train Loss 533.2091 Test MSE 525.0053101211071 Test RE 0.3857189880163583 Lambda1 -1.345767\n",
      "42 Train Loss 495.5859 Test MSE 492.48222630392837 Test RE 0.37358071704106127 Lambda1 -1.4657316\n",
      "43 Train Loss 470.8648 Test MSE 470.5145525767017 Test RE 0.3651536959602918 Lambda1 -1.5393966\n",
      "44 Train Loss 435.24863 Test MSE 436.4902066987102 Test RE 0.3517032833235092 Lambda1 -1.5816193\n",
      "45 Train Loss 383.10727 Test MSE 379.15893447613865 Test RE 0.327793107702943 Lambda1 -1.6599405\n",
      "46 Train Loss 362.42484 Test MSE 369.5134171867061 Test RE 0.3235968436327699 Lambda1 -1.7082734\n",
      "47 Train Loss 345.69614 Test MSE 362.4274445011227 Test RE 0.32047909757795495 Lambda1 -1.688913\n",
      "48 Train Loss 338.97946 Test MSE 355.13563149239224 Test RE 0.3172387972814132 Lambda1 -1.6989471\n",
      "49 Train Loss 334.191 Test MSE 349.3549859025492 Test RE 0.3146463104148689 Lambda1 -1.7039691\n",
      "50 Train Loss 327.37512 Test MSE 342.91482655905935 Test RE 0.31173265769198716 Lambda1 -1.7157758\n",
      "51 Train Loss 321.0489 Test MSE 339.6003749679955 Test RE 0.3102224698519833 Lambda1 -1.7155201\n",
      "52 Train Loss 310.4664 Test MSE 332.6143393986994 Test RE 0.30701504255123285 Lambda1 -1.7193835\n",
      "53 Train Loss 302.1681 Test MSE 320.57123587478634 Test RE 0.3014056897434256 Lambda1 -1.7738111\n",
      "54 Train Loss 298.34866 Test MSE 320.50370079761876 Test RE 0.30137393934534434 Lambda1 -1.7766033\n",
      "55 Train Loss 294.63232 Test MSE 319.87498494481764 Test RE 0.3010781992545201 Lambda1 -1.7893412\n",
      "56 Train Loss 291.4635 Test MSE 317.00627826569 Test RE 0.29972509212409487 Lambda1 -1.783923\n",
      "57 Train Loss 288.54202 Test MSE 314.95258656953513 Test RE 0.29875264591409445 Lambda1 -1.7847971\n",
      "58 Train Loss 286.94025 Test MSE 312.6136554091718 Test RE 0.29764126590752243 Lambda1 -1.8034886\n",
      "59 Train Loss 282.47327 Test MSE 309.6107159654345 Test RE 0.2962082582772212 Lambda1 -1.8293972\n",
      "60 Train Loss 281.61957 Test MSE 310.1266964902434 Test RE 0.29645497788336206 Lambda1 -1.829257\n",
      "61 Train Loss 280.10962 Test MSE 309.1004033075307 Test RE 0.29596404650796915 Lambda1 -1.8406777\n",
      "62 Train Loss 278.58057 Test MSE 307.98687902368727 Test RE 0.29543046503184767 Lambda1 -1.8567524\n",
      "63 Train Loss 277.7045 Test MSE 306.8908906297908 Test RE 0.2949043437076395 Lambda1 -1.861651\n",
      "64 Train Loss 276.73532 Test MSE 306.93821600387435 Test RE 0.29492708130070455 Lambda1 -1.8568631\n",
      "65 Train Loss 275.88214 Test MSE 307.34166736349323 Test RE 0.29512084937938615 Lambda1 -1.859257\n",
      "66 Train Loss 274.16214 Test MSE 305.9960426684049 Test RE 0.29447408134229813 Lambda1 -1.8487338\n",
      "67 Train Loss 273.23932 Test MSE 304.6706125230771 Test RE 0.29383562799359647 Lambda1 -1.8339065\n",
      "68 Train Loss 271.59006 Test MSE 303.01373218448344 Test RE 0.2930355603689037 Lambda1 -1.8074319\n",
      "69 Train Loss 270.55652 Test MSE 302.0464898628574 Test RE 0.29256749091391004 Lambda1 -1.778557\n",
      "70 Train Loss 269.63126 Test MSE 302.06200543268665 Test RE 0.2925750051430012 Lambda1 -1.7530513\n",
      "71 Train Loss 268.62958 Test MSE 300.6722895372074 Test RE 0.2919011950145617 Lambda1 -1.7259316\n",
      "72 Train Loss 267.7181 Test MSE 300.3853016082376 Test RE 0.2917618537421168 Lambda1 -1.7090904\n",
      "73 Train Loss 266.7605 Test MSE 300.4692807809938 Test RE 0.29180263504319515 Lambda1 -1.6851388\n",
      "74 Train Loss 266.3208 Test MSE 299.6571123335331 Test RE 0.29140799693376573 Lambda1 -1.6735281\n",
      "Training time: 141.33\n",
      "Training time: 141.33\n",
      "inv_HT_stan_tune0\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 841.20856 Test MSE 859.7117044771118 Test RE 0.4935891338827252 Lambda1 -0.13670315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 838.0571 Test MSE 858.2035491140375 Test RE 0.4931560027217493 Lambda1 -0.13790658\n",
      "2 Train Loss 838.0347 Test MSE 858.3112676588329 Test RE 0.49318695131209767 Lambda1 -0.13802315\n",
      "3 Train Loss 837.8281 Test MSE 857.7986319560845 Test RE 0.49303964868959277 Lambda1 -0.1390356\n",
      "4 Train Loss 837.8052 Test MSE 857.7224859838985 Test RE 0.49301776487258864 Lambda1 -0.13934816\n",
      "5 Train Loss 837.74603 Test MSE 857.6601007813348 Test RE 0.49299983508007483 Lambda1 -0.13998228\n",
      "6 Train Loss 836.5897 Test MSE 855.2095156933369 Test RE 0.49229500907793156 Lambda1 -0.13045202\n",
      "7 Train Loss 831.87164 Test MSE 846.6603369156414 Test RE 0.4898281930068563 Lambda1 0.044979576\n",
      "8 Train Loss 764.7346 Test MSE 765.2232890789355 Test RE 0.46567536125313075 Lambda1 0.012346218\n",
      "9 Train Loss 729.2855 Test MSE 718.809888280412 Test RE 0.4513320679843924 Lambda1 -0.00248334\n",
      "10 Train Loss 716.55865 Test MSE 710.4272953240401 Test RE 0.4486926858959548 Lambda1 -0.0018971446\n",
      "11 Train Loss 696.7299 Test MSE 696.9136388216585 Test RE 0.44440470942040583 Lambda1 -0.00034624067\n",
      "12 Train Loss 659.31146 Test MSE 657.4020498304598 Test RE 0.4316231178747636 Lambda1 0.00010410212\n",
      "13 Train Loss 642.5266 Test MSE 647.0049636318734 Test RE 0.4281963656467167 Lambda1 0.0005996355\n",
      "14 Train Loss 638.11945 Test MSE 643.4121962028445 Test RE 0.4270058403340849 Lambda1 0.00093940005\n",
      "15 Train Loss 620.6536 Test MSE 615.2294317875321 Test RE 0.41754926382480423 Lambda1 0.000836657\n",
      "16 Train Loss 570.15265 Test MSE 579.2763692738426 Test RE 0.40516514450036184 Lambda1 0.0021488187\n",
      "17 Train Loss 491.7602 Test MSE 505.96028855997724 Test RE 0.3786582177326568 Lambda1 0.001526202\n",
      "18 Train Loss 381.48853 Test MSE 398.3998279929715 Test RE 0.33600732236233294 Lambda1 -0.00045523886\n",
      "19 Train Loss 355.99768 Test MSE 374.3931719697091 Test RE 0.3257265283011358 Lambda1 5.44711e-05\n",
      "20 Train Loss 317.80194 Test MSE 330.5485014479134 Test RE 0.3060601360296553 Lambda1 -6.5635686e-05\n",
      "21 Train Loss 302.81396 Test MSE 321.99761629917697 Test RE 0.30207549719598936 Lambda1 -0.000733371\n",
      "22 Train Loss 282.31534 Test MSE 308.4600209694159 Test RE 0.2956573040818667 Lambda1 -0.0010333852\n",
      "23 Train Loss 276.779 Test MSE 303.7184525750135 Test RE 0.29337611952979037 Lambda1 5.6771518e-05\n",
      "24 Train Loss 267.3839 Test MSE 294.4685218511778 Test RE 0.28887410223744764 Lambda1 0.000482777\n",
      "25 Train Loss 265.3384 Test MSE 291.1200485308839 Test RE 0.2872269774798231 Lambda1 0.00029996067\n",
      "26 Train Loss 263.1701 Test MSE 287.79600110744985 Test RE 0.28558247179634527 Lambda1 0.00015951812\n",
      "27 Train Loss 260.48657 Test MSE 286.6925283692035 Test RE 0.2850344531944269 Lambda1 0.00013007721\n",
      "28 Train Loss 259.24734 Test MSE 285.6793268311346 Test RE 0.2845303365368538 Lambda1 0.0005758597\n",
      "29 Train Loss 258.8997 Test MSE 285.4994525146809 Test RE 0.28444074699766664 Lambda1 0.00076382956\n",
      "30 Train Loss 258.4123 Test MSE 285.44288535586367 Test RE 0.28441256690512856 Lambda1 0.00071629044\n",
      "31 Train Loss 257.20645 Test MSE 285.6519221723163 Test RE 0.28451668899033644 Lambda1 0.0009282105\n",
      "32 Train Loss 256.468 Test MSE 285.4033400554562 Test RE 0.2843928649427598 Lambda1 0.001144303\n",
      "33 Train Loss 256.1451 Test MSE 284.7653936628754 Test RE 0.28407484326268423 Lambda1 0.0012320377\n",
      "34 Train Loss 255.57658 Test MSE 283.4672312552127 Test RE 0.28342659643184054 Lambda1 0.0013678764\n",
      "35 Train Loss 255.53517 Test MSE 283.5722568459461 Test RE 0.28347909683709066 Lambda1 0.0013808277\n",
      "36 Train Loss 255.41623 Test MSE 283.5626076547105 Test RE 0.2834742737854343 Lambda1 0.0014067196\n",
      "37 Train Loss 255.04073 Test MSE 282.8047744252303 Test RE 0.2830952217810995 Lambda1 0.001303556\n",
      "38 Train Loss 254.93471 Test MSE 282.5100012473904 Test RE 0.2829476453570772 Lambda1 0.0011173078\n",
      "39 Train Loss 254.79562 Test MSE 282.3685199218193 Test RE 0.2828767862352682 Lambda1 0.00093053695\n",
      "40 Train Loss 254.70076 Test MSE 282.29418358843577 Test RE 0.28283954871453576 Lambda1 0.00087722734\n",
      "41 Train Loss 254.65575 Test MSE 282.21330338066394 Test RE 0.2827990275816319 Lambda1 0.00078676\n",
      "42 Train Loss 254.59935 Test MSE 281.98117462791356 Test RE 0.28268269839158766 Lambda1 0.0008012907\n",
      "43 Train Loss 254.39447 Test MSE 281.37595943876113 Test RE 0.28237917500776494 Lambda1 0.0012130905\n",
      "44 Train Loss 254.26881 Test MSE 281.29660628413063 Test RE 0.28233935416015266 Lambda1 0.0016501632\n",
      "45 Train Loss 254.1226 Test MSE 281.6784326071846 Test RE 0.28253091004263997 Lambda1 0.0018912292\n",
      "46 Train Loss 254.00934 Test MSE 282.0381069108348 Test RE 0.2827112339119604 Lambda1 0.0017901465\n",
      "47 Train Loss 253.8535 Test MSE 282.33164784324674 Test RE 0.2828583164079431 Lambda1 0.0018414834\n",
      "48 Train Loss 253.81036 Test MSE 282.31326789409326 Test RE 0.28284910914030437 Lambda1 0.0019301514\n",
      "49 Train Loss 253.76772 Test MSE 282.1406130106581 Test RE 0.2827626046178834 Lambda1 0.0020948055\n",
      "50 Train Loss 253.61353 Test MSE 281.9557049281708 Test RE 0.2826699315711657 Lambda1 0.0026193822\n",
      "51 Train Loss 253.43929 Test MSE 281.92122627544325 Test RE 0.28265264804718965 Lambda1 0.0032252856\n",
      "52 Train Loss 253.19637 Test MSE 281.54852846320506 Test RE 0.28246575388532863 Lambda1 0.0041215857\n",
      "53 Train Loss 253.04073 Test MSE 280.704525754333 Test RE 0.2820420599730418 Lambda1 0.0049083824\n",
      "54 Train Loss 252.81108 Test MSE 279.522913472076 Test RE 0.28144781266532926 Lambda1 0.0061170603\n",
      "55 Train Loss 252.2457 Test MSE 277.0141732929937 Test RE 0.2801819578263559 Lambda1 0.009649783\n",
      "56 Train Loss 251.65823 Test MSE 276.10535338505827 Test RE 0.2797219739001154 Lambda1 0.011829348\n",
      "57 Train Loss 250.50485 Test MSE 274.0201698756562 Test RE 0.2786637237540772 Lambda1 0.016617075\n",
      "58 Train Loss 249.51482 Test MSE 272.0685797073541 Test RE 0.27766961963565767 Lambda1 0.019504944\n",
      "59 Train Loss 248.55046 Test MSE 270.4451571662682 Test RE 0.27683995828140834 Lambda1 0.021037322\n",
      "60 Train Loss 247.12596 Test MSE 268.22343563118733 Test RE 0.2757004856627302 Lambda1 0.027008357\n",
      "61 Train Loss 245.10107 Test MSE 265.4419711434012 Test RE 0.2742672597848899 Lambda1 0.03503747\n",
      "62 Train Loss 243.1935 Test MSE 262.083716527223 Test RE 0.2725267831913385 Lambda1 0.043935116\n",
      "63 Train Loss 237.13977 Test MSE 254.4294969691935 Test RE 0.26851768869521175 Lambda1 0.06511851\n",
      "64 Train Loss 232.16689 Test MSE 250.1009005208538 Test RE 0.2662237511006051 Lambda1 0.07288337\n",
      "65 Train Loss 221.01996 Test MSE 237.48408349103272 Test RE 0.2594217738551457 Lambda1 0.10916552\n",
      "66 Train Loss 214.38675 Test MSE 231.54064642035382 Test RE 0.25615497279185556 Lambda1 0.12621555\n",
      "67 Train Loss 204.55058 Test MSE 218.42444954082217 Test RE 0.24879393872086733 Lambda1 0.15701756\n",
      "68 Train Loss 199.68752 Test MSE 207.61096680688536 Test RE 0.242557281231385 Lambda1 0.17503934\n",
      "69 Train Loss 194.23816 Test MSE 199.25391376604875 Test RE 0.2376252577431346 Lambda1 0.19205172\n",
      "70 Train Loss 185.29536 Test MSE 186.60900822882476 Test RE 0.22996168013110463 Lambda1 0.21697381\n",
      "71 Train Loss 181.02962 Test MSE 182.96994840561388 Test RE 0.22770840089942154 Lambda1 0.22923301\n",
      "72 Train Loss 173.36235 Test MSE 168.31894287584456 Test RE 0.2184015257827777 Lambda1 0.27533057\n",
      "73 Train Loss 163.46272 Test MSE 157.71847395472508 Test RE 0.21141239869928566 Lambda1 0.31603524\n",
      "74 Train Loss 155.4274 Test MSE 152.26299386849118 Test RE 0.20772384509664382 Lambda1 0.3451751\n",
      "Training time: 144.08\n",
      "Training time: 144.08\n",
      "inv_HT_stan_tune0\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 842.2528 Test MSE 864.4473423583081 Test RE 0.49494671067648893 Lambda1 -0.0437323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 838.0614 Test MSE 858.3573544349697 Test RE 0.49320019189996556 Lambda1 -0.043526206\n",
      "2 Train Loss 838.0543 Test MSE 858.2924653335411 Test RE 0.49318154936025194 Lambda1 -0.043521643\n",
      "3 Train Loss 838.04974 Test MSE 858.2376898932253 Test RE 0.49316581191140285 Lambda1 -0.043521218\n",
      "4 Train Loss 838.02734 Test MSE 858.0882708308769 Test RE 0.49312287999310733 Lambda1 -0.04356239\n",
      "5 Train Loss 837.9049 Test MSE 857.8576342344968 Test RE 0.49305660486165875 Lambda1 -0.04595989\n",
      "6 Train Loss 837.8946 Test MSE 857.9106680588648 Test RE 0.4930718453119494 Lambda1 -0.047422234\n",
      "7 Train Loss 837.8932 Test MSE 857.9298560066286 Test RE 0.49307735927993207 Lambda1 -0.048802134\n",
      "8 Train Loss 837.8726 Test MSE 858.0319055181053 Test RE 0.49310668382613415 Lambda1 -0.076861724\n",
      "9 Train Loss 837.7972 Test MSE 857.7433237857185 Test RE 0.4930237536068532 Lambda1 -0.14603963\n",
      "10 Train Loss 837.71173 Test MSE 857.2888705895488 Test RE 0.4928931283371084 Lambda1 -0.27991188\n",
      "11 Train Loss 836.1853 Test MSE 852.9684738773461 Test RE 0.49164956641896607 Lambda1 -0.68972105\n",
      "12 Train Loss 833.0644 Test MSE 849.4971385151795 Test RE 0.49064811059461333 Lambda1 -0.6242977\n",
      "13 Train Loss 825.10114 Test MSE 841.4891938167802 Test RE 0.4883300413394149 Lambda1 -0.66195226\n",
      "14 Train Loss 818.2285 Test MSE 833.8462716795724 Test RE 0.4861073259998078 Lambda1 -0.6769839\n",
      "15 Train Loss 804.0987 Test MSE 811.385086808893 Test RE 0.47951553445785744 Lambda1 -0.9416969\n",
      "16 Train Loss 784.0397 Test MSE 790.5064380109277 Test RE 0.47330585428250316 Lambda1 -1.0344748\n",
      "17 Train Loss 773.9795 Test MSE 776.8371241224319 Test RE 0.4691958439262303 Lambda1 -1.1142514\n",
      "18 Train Loss 762.8774 Test MSE 768.3109171423457 Test RE 0.46661390093315697 Lambda1 -1.0097195\n",
      "19 Train Loss 757.47845 Test MSE 761.7388779348665 Test RE 0.4646139352787817 Lambda1 -0.95155865\n",
      "20 Train Loss 743.2336 Test MSE 746.4299774513973 Test RE 0.4599214946938071 Lambda1 -0.9668009\n",
      "21 Train Loss 718.0484 Test MSE 706.0051759248156 Test RE 0.4472940417582941 Lambda1 -1.0496391\n",
      "22 Train Loss 682.166 Test MSE 679.4237607962411 Test RE 0.43879284377768685 Lambda1 -1.0934243\n",
      "23 Train Loss 668.75574 Test MSE 670.1547325306116 Test RE 0.4357894528419501 Lambda1 -1.107751\n",
      "24 Train Loss 642.8137 Test MSE 647.6484881933468 Test RE 0.4284092592177217 Lambda1 -1.1465211\n",
      "25 Train Loss 621.5155 Test MSE 627.0466194207282 Test RE 0.4215402861772665 Lambda1 -1.1246662\n",
      "26 Train Loss 606.91675 Test MSE 610.8336777732931 Test RE 0.416054915506008 Lambda1 -1.1009984\n",
      "27 Train Loss 589.69073 Test MSE 583.8291900773966 Test RE 0.40675422535723615 Lambda1 -1.0809627\n",
      "28 Train Loss 570.5007 Test MSE 574.5410501879626 Test RE 0.40350572652409883 Lambda1 -1.0465153\n",
      "29 Train Loss 563.4978 Test MSE 571.5913995819888 Test RE 0.40246860964201475 Lambda1 -1.0280311\n",
      "30 Train Loss 550.5394 Test MSE 560.1852156791863 Test RE 0.39843271613555653 Lambda1 -0.975238\n",
      "31 Train Loss 543.2163 Test MSE 555.5622235597274 Test RE 0.3967852545054402 Lambda1 -0.9584639\n",
      "32 Train Loss 533.20935 Test MSE 541.3134704688038 Test RE 0.3916639395113563 Lambda1 -0.9342623\n",
      "33 Train Loss 507.4548 Test MSE 507.97366075036257 Test RE 0.37941086869309376 Lambda1 -0.9080887\n",
      "34 Train Loss 488.56583 Test MSE 479.05411090743456 Test RE 0.36845245646144126 Lambda1 -0.853517\n",
      "35 Train Loss 462.8457 Test MSE 461.31476636741024 Test RE 0.3615662198800661 Lambda1 -0.81869876\n",
      "36 Train Loss 441.5552 Test MSE 436.76324115513955 Test RE 0.3518132652901458 Lambda1 -0.7861712\n",
      "37 Train Loss 417.15134 Test MSE 412.3246642744586 Test RE 0.34182893950349813 Lambda1 -0.7478437\n",
      "38 Train Loss 402.91208 Test MSE 401.6858248976287 Test RE 0.33739016893340906 Lambda1 -0.7514548\n",
      "39 Train Loss 389.8548 Test MSE 388.04781133830545 Test RE 0.33161318494073044 Lambda1 -0.7937508\n",
      "40 Train Loss 381.40247 Test MSE 377.41025620744836 Test RE 0.3270363444246688 Lambda1 -0.7996395\n",
      "41 Train Loss 369.20172 Test MSE 365.9370286728992 Test RE 0.32202704683021327 Lambda1 -0.7836089\n",
      "42 Train Loss 362.76178 Test MSE 355.12701852648314 Test RE 0.31723495032370086 Lambda1 -0.78393966\n",
      "43 Train Loss 356.78806 Test MSE 349.5782675120865 Test RE 0.31474684356259847 Lambda1 -0.7957111\n",
      "44 Train Loss 338.9065 Test MSE 341.4606188362755 Test RE 0.311070969087987 Lambda1 -0.7629461\n",
      "45 Train Loss 327.57645 Test MSE 328.638667219443 Test RE 0.3051746821838576 Lambda1 -0.7492717\n",
      "46 Train Loss 315.18753 Test MSE 315.6408868297349 Test RE 0.29907891613384197 Lambda1 -0.75487113\n",
      "47 Train Loss 299.44455 Test MSE 296.9391660895573 Test RE 0.29008342395105646 Lambda1 -0.7320405\n",
      "48 Train Loss 295.8557 Test MSE 292.41561123667555 Test RE 0.28786538673995915 Lambda1 -0.73323995\n",
      "49 Train Loss 289.00836 Test MSE 282.1540712462889 Test RE 0.2827693484899196 Lambda1 -0.72435206\n",
      "50 Train Loss 278.7542 Test MSE 278.36451161658846 Test RE 0.2808640176733293 Lambda1 -0.73172104\n",
      "51 Train Loss 263.47995 Test MSE 265.0746310713498 Test RE 0.274077417472175 Lambda1 -0.71741325\n",
      "52 Train Loss 260.34866 Test MSE 263.7578795471303 Test RE 0.2733958336618474 Lambda1 -0.716531\n",
      "53 Train Loss 253.8466 Test MSE 254.28826410664408 Test RE 0.2684431517674304 Lambda1 -0.7213769\n",
      "54 Train Loss 245.24382 Test MSE 249.32228595627234 Test RE 0.2658090239409401 Lambda1 -0.7286177\n",
      "55 Train Loss 240.67226 Test MSE 250.96612063250154 Test RE 0.26668385194544325 Lambda1 -0.74933314\n",
      "56 Train Loss 238.46074 Test MSE 249.87856880527292 Test RE 0.2661053925830251 Lambda1 -0.7657663\n",
      "57 Train Loss 235.78703 Test MSE 250.22486429589944 Test RE 0.26628972050080146 Lambda1 -0.79918975\n",
      "58 Train Loss 232.68356 Test MSE 245.31822748470688 Test RE 0.26366596903053413 Lambda1 -0.81585044\n",
      "59 Train Loss 228.90775 Test MSE 239.99169558868007 Test RE 0.26078780422096043 Lambda1 -0.8288192\n",
      "60 Train Loss 226.54428 Test MSE 236.11894344836068 Test RE 0.2586750765313033 Lambda1 -0.85439205\n",
      "61 Train Loss 223.98737 Test MSE 233.2183299981012 Test RE 0.2570813141201114 Lambda1 -0.87914896\n",
      "62 Train Loss 214.62096 Test MSE 222.7132097105365 Test RE 0.25122459750993176 Lambda1 -0.91202277\n",
      "63 Train Loss 210.01872 Test MSE 216.11809492178395 Test RE 0.2474769391464222 Lambda1 -0.914522\n",
      "64 Train Loss 202.2488 Test MSE 206.65051954244277 Test RE 0.24199557310239858 Lambda1 -0.9212412\n",
      "65 Train Loss 199.357 Test MSE 202.97225684979352 Test RE 0.2398322108673637 Lambda1 -0.93003106\n",
      "66 Train Loss 197.93568 Test MSE 199.15774236014582 Test RE 0.23756790500947284 Lambda1 -0.9456369\n",
      "67 Train Loss 193.72623 Test MSE 195.43143496378718 Test RE 0.23533492359638936 Lambda1 -0.9772169\n",
      "68 Train Loss 186.3328 Test MSE 190.6012363453884 Test RE 0.2324085105062803 Lambda1 -1.0125339\n",
      "69 Train Loss 183.16763 Test MSE 187.24907067770852 Test RE 0.2303557228397315 Lambda1 -1.0307838\n",
      "70 Train Loss 181.7495 Test MSE 185.7835018390017 Test RE 0.22945247307845248 Lambda1 -1.0392427\n",
      "71 Train Loss 175.37743 Test MSE 180.99786609460438 Test RE 0.22647793576109757 Lambda1 -1.0482628\n",
      "72 Train Loss 172.79642 Test MSE 181.5447744298828 Test RE 0.22681984378422043 Lambda1 -1.0463812\n",
      "73 Train Loss 169.0935 Test MSE 177.57516386000725 Test RE 0.2243263462070828 Lambda1 -1.0600405\n",
      "74 Train Loss 165.1539 Test MSE 169.78628197628782 Test RE 0.21935142984438766 Lambda1 -1.1013459\n",
      "Training time: 142.19\n",
      "Training time: 142.19\n",
      "inv_HT_stan_tune0\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 841.6439 Test MSE 860.0383273578666 Test RE 0.49368288751803774 Lambda1 -0.022054696\n",
      "1 Train Loss 838.06256 Test MSE 858.2167085082548 Test RE 0.4931597836484187 Lambda1 -0.022162681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Train Loss 838.0585 Test MSE 858.2532796284802 Test RE 0.49317029102576765 Lambda1 -0.022164248\n",
      "3 Train Loss 838.0578 Test MSE 858.2641169988377 Test RE 0.4931734047058129 Lambda1 -0.022164343\n",
      "4 Train Loss 838.0575 Test MSE 858.2693657200412 Test RE 0.49317491270649566 Lambda1 -0.02216409\n",
      "5 Train Loss 838.057 Test MSE 858.2791219916163 Test RE 0.49317771575114633 Lambda1 -0.022162851\n",
      "6 Train Loss 838.05597 Test MSE 858.2987984312683 Test RE 0.49318336888058667 Lambda1 -0.022157675\n",
      "7 Train Loss 838.0556 Test MSE 858.3025281022092 Test RE 0.4931844404244854 Lambda1 -0.022155944\n",
      "8 Train Loss 838.0519 Test MSE 858.3329933047831 Test RE 0.49319319306709525 Lambda1 -0.022131393\n",
      "9 Train Loss 837.8702 Test MSE 857.8979844886422 Test RE 0.49306820044849065 Lambda1 -0.020817071\n",
      "10 Train Loss 837.8638 Test MSE 857.8136799582966 Test RE 0.4930439732646795 Lambda1 -0.02141558\n",
      "11 Train Loss 837.7481 Test MSE 857.6300431070327 Test RE 0.4929911961342414 Lambda1 -0.07537261\n",
      "12 Train Loss 837.68066 Test MSE 857.7699644809958 Test RE 0.49303140997282857 Lambda1 -0.09752656\n",
      "13 Train Loss 837.05383 Test MSE 856.4378600149922 Test RE 0.4926484258647077 Lambda1 -0.1574449\n",
      "14 Train Loss 835.6305 Test MSE 853.0021718306534 Test RE 0.49165927804431386 Lambda1 -0.1680322\n",
      "15 Train Loss 827.0026 Test MSE 842.4756110193425 Test RE 0.4886161745867564 Lambda1 -0.24862437\n",
      "16 Train Loss 803.27045 Test MSE 808.6989390191555 Test RE 0.47872114137182276 Lambda1 -0.10666118\n",
      "17 Train Loss 774.7934 Test MSE 780.2651272485931 Test RE 0.47022993092732773 Lambda1 -0.061155926\n",
      "18 Train Loss 700.6881 Test MSE 694.3328556364946 Test RE 0.4435810951961002 Lambda1 -0.0058268956\n",
      "19 Train Loss 671.56256 Test MSE 670.3844515286575 Test RE 0.4358641374887194 Lambda1 0.0009922436\n",
      "20 Train Loss 652.69257 Test MSE 657.3605782473488 Test RE 0.43160950339309156 Lambda1 0.0007360209\n",
      "21 Train Loss 647.8795 Test MSE 653.5949457359901 Test RE 0.43037150928819734 Lambda1 0.00090025226\n",
      "22 Train Loss 641.7507 Test MSE 649.2889418461572 Test RE 0.42895148318767773 Lambda1 0.0011008658\n",
      "23 Train Loss 639.37286 Test MSE 646.697243997381 Test RE 0.42809452709271795 Lambda1 -0.00025540005\n",
      "24 Train Loss 633.78394 Test MSE 639.2399066512814 Test RE 0.42561910127299957 Lambda1 -0.0008631932\n",
      "25 Train Loss 591.0073 Test MSE 589.324292665815 Test RE 0.40866396318874304 Lambda1 -0.0008206713\n",
      "26 Train Loss 518.87573 Test MSE 543.3307085242072 Test RE 0.3923930408040252 Lambda1 -0.0005198077\n",
      "27 Train Loss 411.08514 Test MSE 420.1057936109137 Test RE 0.34503925392166934 Lambda1 -0.001101111\n",
      "28 Train Loss 310.71356 Test MSE 316.08665584873984 Test RE 0.2992900311746054 Lambda1 0.0029576938\n",
      "29 Train Loss 280.84067 Test MSE 291.48461530079516 Test RE 0.2874067669679686 Lambda1 3.908705e-05\n",
      "30 Train Loss 263.6129 Test MSE 286.6706193852938 Test RE 0.285023561849271 Lambda1 0.001113106\n",
      "31 Train Loss 261.29965 Test MSE 285.1152532789337 Test RE 0.2842492953034029 Lambda1 0.0015714341\n",
      "32 Train Loss 258.35782 Test MSE 282.1198471292499 Test RE 0.28275219859604445 Lambda1 0.0013277642\n",
      "33 Train Loss 256.831 Test MSE 281.1890120816883 Test RE 0.28228535247232145 Lambda1 0.00056117267\n",
      "34 Train Loss 256.15024 Test MSE 281.3971055779832 Test RE 0.28238978557516364 Lambda1 0.00035690932\n",
      "35 Train Loss 255.53285 Test MSE 281.2102781796242 Test RE 0.28229602677714016 Lambda1 0.00048304276\n",
      "36 Train Loss 255.25906 Test MSE 281.32773851926675 Test RE 0.2823549775480229 Lambda1 0.0005687633\n",
      "37 Train Loss 254.97789 Test MSE 282.12541970734765 Test RE 0.28275499111657126 Lambda1 0.0006470003\n",
      "38 Train Loss 254.69803 Test MSE 282.5035499442555 Test RE 0.2829444146901194 Lambda1 0.0007366023\n",
      "39 Train Loss 254.55519 Test MSE 282.5257589894387 Test RE 0.28295553632344655 Lambda1 0.0007151098\n",
      "40 Train Loss 254.41829 Test MSE 282.4442665247503 Test RE 0.2829147251649249 Lambda1 0.00073911244\n",
      "41 Train Loss 254.35107 Test MSE 282.22032374527834 Test RE 0.2828025450273293 Lambda1 0.00078721927\n",
      "42 Train Loss 254.28612 Test MSE 282.1901971502163 Test RE 0.28278745025132823 Lambda1 0.00091294944\n",
      "43 Train Loss 254.16893 Test MSE 282.14245669165575 Test RE 0.2827635284891115 Lambda1 0.0010494435\n",
      "44 Train Loss 254.01706 Test MSE 282.0935275652742 Test RE 0.2827390090097178 Lambda1 0.0010910288\n",
      "45 Train Loss 253.91937 Test MSE 281.862747929179 Test RE 0.2826233314950457 Lambda1 0.0011569521\n",
      "46 Train Loss 253.78322 Test MSE 282.073554553517 Test RE 0.28272899947571334 Lambda1 0.0011282633\n",
      "47 Train Loss 253.60359 Test MSE 282.7045254347654 Test RE 0.2830450413593971 Lambda1 0.0009837298\n",
      "48 Train Loss 253.53719 Test MSE 282.922995505549 Test RE 0.2831543868466584 Lambda1 0.0008751859\n",
      "49 Train Loss 253.49847 Test MSE 283.0790202434887 Test RE 0.2832324522575258 Lambda1 0.00081486756\n",
      "50 Train Loss 253.4561 Test MSE 283.0292015546595 Test RE 0.28320752831533547 Lambda1 0.00077977707\n",
      "51 Train Loss 253.41026 Test MSE 282.75096221096794 Test RE 0.28306828675938156 Lambda1 0.0007640307\n",
      "52 Train Loss 253.36882 Test MSE 282.7859949430699 Test RE 0.2830858222404824 Lambda1 0.0007349713\n",
      "53 Train Loss 253.34172 Test MSE 283.01838913022465 Test RE 0.28320211864518735 Lambda1 0.00071950676\n",
      "54 Train Loss 253.28607 Test MSE 283.2132018800434 Test RE 0.28329957148696594 Lambda1 0.00068620185\n",
      "55 Train Loss 253.10628 Test MSE 282.88053070647453 Test RE 0.28313313628457454 Lambda1 0.0005807578\n",
      "56 Train Loss 253.02911 Test MSE 282.60286415870394 Test RE 0.28299414492013014 Lambda1 0.0005772934\n",
      "57 Train Loss 252.96887 Test MSE 282.73046247441323 Test RE 0.2830580252019288 Lambda1 0.00057261204\n",
      "58 Train Loss 252.88225 Test MSE 282.95329457386157 Test RE 0.2831695483650973 Lambda1 0.0006339043\n",
      "59 Train Loss 252.76358 Test MSE 283.0207473554999 Test RE 0.28320329852082204 Lambda1 0.00062654226\n",
      "60 Train Loss 252.5283 Test MSE 282.7698115471019 Test RE 0.28307772184728214 Lambda1 0.0004237274\n",
      "61 Train Loss 252.40622 Test MSE 282.9019395973966 Test RE 0.2831438500861446 Lambda1 0.0003470733\n",
      "62 Train Loss 251.9419 Test MSE 283.6768795847734 Test RE 0.2835313861984785 Lambda1 0.00016481112\n",
      "63 Train Loss 251.80057 Test MSE 283.97825461797055 Test RE 0.28368195644942523 Lambda1 8.781545e-05\n",
      "64 Train Loss 251.66154 Test MSE 284.1389012825115 Test RE 0.2837621846289944 Lambda1 5.2386218e-05\n",
      "65 Train Loss 251.48009 Test MSE 283.6902431512926 Test RE 0.2835380644761072 Lambda1 4.2280077e-05\n",
      "66 Train Loss 251.36864 Test MSE 283.61852281955566 Test RE 0.28350222128100383 Lambda1 3.066639e-05\n",
      "67 Train Loss 251.31282 Test MSE 283.954115667307 Test RE 0.2836698993109544 Lambda1 3.344984e-05\n",
      "68 Train Loss 251.23805 Test MSE 284.3107403274263 Test RE 0.28384797726847644 Lambda1 3.998221e-05\n",
      "69 Train Loss 251.18236 Test MSE 284.52968334005516 Test RE 0.2839572495585421 Lambda1 4.281646e-05\n",
      "70 Train Loss 251.12747 Test MSE 284.9217007954972 Test RE 0.2841527966125151 Lambda1 3.9261213e-05\n",
      "71 Train Loss 251.0324 Test MSE 285.3838787545996 Test RE 0.28438316857855706 Lambda1 2.944311e-05\n",
      "72 Train Loss 250.8825 Test MSE 285.35376457734264 Test RE 0.2843681638923819 Lambda1 3.178493e-05\n",
      "73 Train Loss 250.8084 Test MSE 285.3736522590214 Test RE 0.28437807321493186 Lambda1 3.2724805e-05\n",
      "74 Train Loss 250.69795 Test MSE 285.9634794300702 Test RE 0.2846718062417199 Lambda1 3.780814e-05\n",
      "Training time: 142.69\n",
      "Training time: 142.69\n",
      "inv_HT_stan_tune0\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 841.1933 Test MSE 859.6305799696926 Test RE 0.4935658451919879 Lambda1 0.003085249\n",
      "1 Train Loss 837.9396 Test MSE 858.0711150224525 Test RE 0.4931179504513388 Lambda1 0.0028729476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Train Loss 837.9208 Test MSE 858.1110294102399 Test RE 0.49312941935634574 Lambda1 0.001944566\n",
      "3 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "4 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "5 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "6 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "7 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "8 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "9 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "10 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "11 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "12 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "13 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "14 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "15 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "16 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "17 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "18 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "19 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "20 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "21 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "22 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "23 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "24 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "25 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "26 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "27 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "28 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "29 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "30 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "31 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "32 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "33 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "34 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "35 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "36 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "37 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "38 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "39 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "40 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "41 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "42 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "43 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "44 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "45 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "46 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "47 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "48 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "49 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "50 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "51 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "52 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "53 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "54 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "55 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "56 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "57 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "58 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "59 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "60 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "61 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "62 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "63 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "64 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "65 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "66 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "67 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "68 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "69 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "70 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "71 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "72 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "73 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "74 Train Loss 837.90405 Test MSE 857.9531422004254 Test RE 0.4930840508625564 Lambda1 -0.0029952372\n",
      "Training time: 138.01\n",
      "Training time: 138.01\n",
      "inv_HT_stan_tune0\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 857.8131 Test MSE 873.7603710769098 Test RE 0.49760569536796145 Lambda1 -0.020418447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 838.1025 Test MSE 858.3545639051969 Test RE 0.4931993901995252 Lambda1 -0.021050537\n",
      "2 Train Loss 838.0969 Test MSE 858.2988563640753 Test RE 0.4931833855248466 Lambda1 -0.021038573\n",
      "3 Train Loss 838.06665 Test MSE 858.1473669740839 Test RE 0.49313986027334156 Lambda1 -0.020984609\n",
      "4 Train Loss 837.881 Test MSE 857.8762804131603 Test RE 0.49306196330997315 Lambda1 -0.0231689\n",
      "5 Train Loss 837.8706 Test MSE 857.8475323200036 Test RE 0.49305370179818564 Lambda1 -0.024077533\n",
      "6 Train Loss 837.6933 Test MSE 857.8121045566372 Test RE 0.49304352051913086 Lambda1 -0.05566827\n",
      "7 Train Loss 837.5577 Test MSE 857.3596182776962 Test RE 0.4929134658984176 Lambda1 -0.07240079\n",
      "8 Train Loss 833.74774 Test MSE 848.0737261011509 Test RE 0.4902368747106419 Lambda1 0.02956361\n",
      "9 Train Loss 799.0382 Test MSE 808.777484804382 Test RE 0.4787443889693903 Lambda1 0.092438646\n",
      "10 Train Loss 746.15845 Test MSE 743.6586874977222 Test RE 0.4590669195191753 Lambda1 0.025305571\n",
      "11 Train Loss 713.49146 Test MSE 701.1365187392709 Test RE 0.44574908936064384 Lambda1 0.0034956234\n",
      "12 Train Loss 676.04266 Test MSE 674.2922738161324 Test RE 0.4371326667595084 Lambda1 0.001093517\n",
      "13 Train Loss 663.2885 Test MSE 667.1594496284814 Test RE 0.4348144731553094 Lambda1 0.00043125966\n",
      "14 Train Loss 660.8334 Test MSE 667.7581877486074 Test RE 0.43500954017899474 Lambda1 0.00036323804\n",
      "15 Train Loss 659.20654 Test MSE 666.8893977330596 Test RE 0.4347264624424052 Lambda1 0.00022500669\n",
      "16 Train Loss 656.46735 Test MSE 664.357509959202 Test RE 0.43390044443565673 Lambda1 -5.815216e-05\n",
      "17 Train Loss 643.2141 Test MSE 644.5225479852835 Test RE 0.4273741285963341 Lambda1 4.734598e-05\n",
      "18 Train Loss 636.0389 Test MSE 633.2825790176919 Test RE 0.42363120318700925 Lambda1 -0.00039771968\n",
      "19 Train Loss 620.4516 Test MSE 611.8249513392227 Test RE 0.4163923699291133 Lambda1 -0.0017352698\n",
      "20 Train Loss 514.8342 Test MSE 470.8800853457963 Test RE 0.3652955085008022 Lambda1 0.0013921211\n",
      "21 Train Loss 369.0319 Test MSE 352.57222900824075 Test RE 0.31609179395812387 Lambda1 0.0023370767\n",
      "22 Train Loss 308.46417 Test MSE 311.12224661048094 Test RE 0.29693042761862026 Lambda1 0.002253527\n",
      "23 Train Loss 276.92737 Test MSE 290.85358039647946 Test RE 0.2870954950223643 Lambda1 0.0031105315\n",
      "24 Train Loss 268.52118 Test MSE 287.8582987645402 Test RE 0.28561337937697123 Lambda1 0.003180135\n",
      "25 Train Loss 263.2415 Test MSE 287.14174071524985 Test RE 0.28525767296097987 Lambda1 0.0027880054\n",
      "26 Train Loss 260.2861 Test MSE 286.0002405152745 Test RE 0.28469010317293764 Lambda1 0.0026676594\n",
      "27 Train Loss 258.60037 Test MSE 284.3956199121091 Test RE 0.28389034482107034 Lambda1 0.002624185\n",
      "28 Train Loss 257.50555 Test MSE 283.91201646141997 Test RE 0.2836488699975702 Lambda1 0.0023257395\n",
      "29 Train Loss 257.13046 Test MSE 283.79314743639793 Test RE 0.2835894843558992 Lambda1 0.0023409508\n",
      "30 Train Loss 256.87866 Test MSE 283.5137816162608 Test RE 0.28344986732055 Lambda1 0.0026293467\n",
      "31 Train Loss 256.2779 Test MSE 283.6981123005624 Test RE 0.28354199691286625 Lambda1 0.0026481014\n",
      "32 Train Loss 255.82532 Test MSE 283.82824092278156 Test RE 0.283607017964708 Lambda1 0.0025872826\n",
      "33 Train Loss 255.26807 Test MSE 283.18222034489696 Test RE 0.28328407557148083 Lambda1 0.0026268032\n",
      "34 Train Loss 254.76854 Test MSE 282.9389358240227 Test RE 0.28316236341201656 Lambda1 0.002733819\n",
      "35 Train Loss 254.41599 Test MSE 282.9122503272973 Test RE 0.28314900981245367 Lambda1 0.0030078986\n",
      "36 Train Loss 254.07243 Test MSE 282.12698379679927 Test RE 0.28275577490536724 Lambda1 0.0035072512\n",
      "37 Train Loss 253.80164 Test MSE 281.5083545385216 Test RE 0.28244560076498815 Lambda1 0.003931856\n",
      "38 Train Loss 253.63892 Test MSE 281.50303113683435 Test RE 0.282442930189725 Lambda1 0.004085739\n",
      "39 Train Loss 253.45337 Test MSE 281.1523551349243 Test RE 0.28226695193752466 Lambda1 0.0047454713\n",
      "40 Train Loss 252.91272 Test MSE 280.0253388952027 Test RE 0.28170064176123943 Lambda1 0.006943679\n",
      "41 Train Loss 252.27997 Test MSE 278.08613698540495 Test RE 0.28072354543115435 Lambda1 0.010477421\n",
      "42 Train Loss 251.84305 Test MSE 276.2996743461271 Test RE 0.27982038974335294 Lambda1 0.013408658\n",
      "43 Train Loss 250.40967 Test MSE 272.1085185165373 Test RE 0.2776899993998512 Lambda1 0.022992902\n",
      "44 Train Loss 249.04674 Test MSE 270.34764228824776 Test RE 0.27679004345027464 Lambda1 0.027695004\n",
      "45 Train Loss 247.40955 Test MSE 267.86709670275485 Test RE 0.27551728864573505 Lambda1 0.03655776\n",
      "46 Train Loss 245.73198 Test MSE 266.41990994189223 Test RE 0.27477202172421655 Lambda1 0.045862373\n",
      "47 Train Loss 244.45435 Test MSE 265.4355476748214 Test RE 0.27426394124830644 Lambda1 0.0565618\n",
      "48 Train Loss 242.33853 Test MSE 264.2371734001141 Test RE 0.2736441247603705 Lambda1 0.06126735\n",
      "49 Train Loss 238.15648 Test MSE 260.36350115483793 Test RE 0.2716309309690661 Lambda1 0.075321846\n",
      "50 Train Loss 235.49504 Test MSE 258.10754122871276 Test RE 0.27045157657696417 Lambda1 0.08794253\n",
      "51 Train Loss 233.14531 Test MSE 256.4402118148431 Test RE 0.26957662642370506 Lambda1 0.09507018\n",
      "52 Train Loss 231.67484 Test MSE 255.84667396523417 Test RE 0.2692644744786546 Lambda1 0.10190034\n",
      "53 Train Loss 227.84332 Test MSE 253.0006389034899 Test RE 0.26776263894902164 Lambda1 0.11687165\n",
      "54 Train Loss 225.0778 Test MSE 250.92583548323927 Test RE 0.2666624470045646 Lambda1 0.13575564\n",
      "55 Train Loss 222.50208 Test MSE 248.4581479119964 Test RE 0.26534798399980747 Lambda1 0.15107152\n",
      "56 Train Loss 220.72415 Test MSE 245.8053842333903 Test RE 0.2639276351792816 Lambda1 0.15472795\n",
      "57 Train Loss 218.78888 Test MSE 243.14970102117397 Test RE 0.26249802524026217 Lambda1 0.17347416\n",
      "58 Train Loss 217.12701 Test MSE 240.49606196273808 Test RE 0.26106169612419833 Lambda1 0.18764363\n",
      "59 Train Loss 215.88416 Test MSE 239.15030845925267 Test RE 0.2603302547241314 Lambda1 0.20649476\n",
      "60 Train Loss 214.60612 Test MSE 240.2415105325648 Test RE 0.2609235000530655 Lambda1 0.2032686\n",
      "61 Train Loss 214.17451 Test MSE 240.13826678318065 Test RE 0.26086742811271274 Lambda1 0.20344587\n",
      "62 Train Loss 213.16632 Test MSE 239.60552014660243 Test RE 0.2605779003084064 Lambda1 0.20518473\n",
      "63 Train Loss 211.98946 Test MSE 239.70917542880588 Test RE 0.2606342582654179 Lambda1 0.2089265\n",
      "64 Train Loss 211.44153 Test MSE 239.5861290145848 Test RE 0.26056735588793795 Lambda1 0.21425794\n",
      "65 Train Loss 210.30048 Test MSE 238.12180348055045 Test RE 0.2597698551532327 Lambda1 0.22729376\n",
      "66 Train Loss 209.11293 Test MSE 237.62935386289348 Test RE 0.25950110661617526 Lambda1 0.25264046\n",
      "67 Train Loss 208.38103 Test MSE 236.92209639014263 Test RE 0.25911464160073927 Lambda1 0.26349527\n",
      "68 Train Loss 208.05357 Test MSE 236.26152283595113 Test RE 0.25875316464956327 Lambda1 0.27365926\n",
      "69 Train Loss 207.77074 Test MSE 235.64361067779745 Test RE 0.2584145750174257 Lambda1 0.2785279\n",
      "70 Train Loss 207.38611 Test MSE 234.94023278513626 Test RE 0.25802861305842356 Lambda1 0.2817458\n",
      "71 Train Loss 206.36563 Test MSE 234.1432005391271 Test RE 0.2575905615560141 Lambda1 0.27154228\n",
      "72 Train Loss 205.13435 Test MSE 231.92681797790556 Test RE 0.25636849589297883 Lambda1 0.2726127\n",
      "73 Train Loss 202.38571 Test MSE 227.42848209615593 Test RE 0.2538701251048223 Lambda1 0.28498694\n",
      "74 Train Loss 199.68735 Test MSE 224.25292362198206 Test RE 0.25209151455452233 Lambda1 0.28861663\n",
      "Training time: 142.68\n",
      "Training time: 142.68\n",
      "inv_HT_stan_tune0\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 865.2279 Test MSE 890.4657037169486 Test RE 0.5023400092707402 Lambda1 -0.13223308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 838.0259 Test MSE 858.3994316675561 Test RE 0.4932122802479944 Lambda1 -0.1297274\n",
      "2 Train Loss 837.99646 Test MSE 858.2385020310816 Test RE 0.49316604524916874 Lambda1 -0.12967072\n",
      "3 Train Loss 837.99347 Test MSE 858.2033016169252 Test RE 0.4931559316111746 Lambda1 -0.1296687\n",
      "4 Train Loss 837.98724 Test MSE 858.1333819327016 Test RE 0.4931358419603356 Lambda1 -0.12971699\n",
      "5 Train Loss 837.94574 Test MSE 857.8844586615644 Test RE 0.4930643135169298 Lambda1 -0.13031954\n",
      "6 Train Loss 837.8783 Test MSE 857.8039664606144 Test RE 0.4930411817522794 Lambda1 -0.13237941\n",
      "7 Train Loss 837.8719 Test MSE 857.8468050075176 Test RE 0.4930534927842431 Lambda1 -0.13299488\n",
      "8 Train Loss 837.871 Test MSE 857.8628278206928 Test RE 0.4930580973751008 Lambda1 -0.13331634\n",
      "9 Train Loss 837.87036 Test MSE 857.8716432476675 Test RE 0.49306063070930334 Lambda1 -0.13361113\n",
      "10 Train Loss 837.86865 Test MSE 857.8926772294443 Test RE 0.49306667529932585 Lambda1 -0.13484617\n",
      "11 Train Loss 837.8635 Test MSE 857.9334204200405 Test RE 0.49307838356514294 Lambda1 -0.1391674\n",
      "12 Train Loss 837.83765 Test MSE 857.9364004866868 Test RE 0.493079239928314 Lambda1 -0.15964302\n",
      "13 Train Loss 837.8043 Test MSE 857.665160015883 Test RE 0.4930012891515172 Lambda1 -0.18019289\n",
      "14 Train Loss 837.7123 Test MSE 857.2001086601373 Test RE 0.4928676111053593 Lambda1 -0.29660425\n",
      "15 Train Loss 835.8958 Test MSE 854.2044265267583 Test RE 0.4920056379729942 Lambda1 -0.5746488\n",
      "16 Train Loss 830.125 Test MSE 845.6984565234527 Test RE 0.48954987004274286 Lambda1 -0.5279226\n",
      "17 Train Loss 812.8197 Test MSE 826.4549233193724 Test RE 0.48394806331360657 Lambda1 -0.77751344\n",
      "18 Train Loss 797.8236 Test MSE 808.9918668255505 Test RE 0.47880783496737406 Lambda1 -0.9557766\n",
      "19 Train Loss 779.18134 Test MSE 790.8030530940326 Test RE 0.47339464324215924 Lambda1 -0.92517\n",
      "20 Train Loss 762.4656 Test MSE 770.0241632139072 Test RE 0.4671338592304126 Lambda1 -1.0011677\n",
      "21 Train Loss 743.3342 Test MSE 747.1306967446704 Test RE 0.4601373222161067 Lambda1 -1.0470204\n",
      "22 Train Loss 723.7239 Test MSE 713.4742195162903 Test RE 0.4496538467937789 Lambda1 -1.0318323\n",
      "23 Train Loss 682.14197 Test MSE 676.7256862735251 Test RE 0.43792072717751346 Lambda1 -0.928346\n",
      "24 Train Loss 668.1001 Test MSE 664.3744020886486 Test RE 0.4339059606342204 Lambda1 -0.91298294\n",
      "25 Train Loss 652.61896 Test MSE 652.8279283849158 Test RE 0.43011890687684085 Lambda1 -0.92045987\n",
      "26 Train Loss 639.9407 Test MSE 643.2149144589417 Test RE 0.4269403714824415 Lambda1 -0.9350252\n",
      "27 Train Loss 636.7107 Test MSE 639.9489224229272 Test RE 0.42585507451339516 Lambda1 -0.9383574\n",
      "28 Train Loss 634.6574 Test MSE 637.891972948941 Test RE 0.4251701234472644 Lambda1 -0.9321763\n",
      "29 Train Loss 630.4367 Test MSE 634.8665852138637 Test RE 0.4241606788022622 Lambda1 -0.9505748\n",
      "30 Train Loss 629.57544 Test MSE 633.4208750275665 Test RE 0.423677456872824 Lambda1 -0.96208817\n",
      "31 Train Loss 626.3071 Test MSE 632.0247014821174 Test RE 0.42321026862700484 Lambda1 -0.97638285\n",
      "32 Train Loss 624.4709 Test MSE 630.1356972630603 Test RE 0.4225773469830561 Lambda1 -0.9849366\n",
      "33 Train Loss 621.51404 Test MSE 627.4641838560262 Test RE 0.4216806193932494 Lambda1 -1.028174\n",
      "34 Train Loss 620.239 Test MSE 626.6682947394702 Test RE 0.4214131001340721 Lambda1 -1.0563122\n",
      "35 Train Loss 619.9729 Test MSE 626.6510333224675 Test RE 0.42140729623637607 Lambda1 -1.0551411\n",
      "36 Train Loss 619.2237 Test MSE 626.4961311810243 Test RE 0.4213552090909077 Lambda1 -1.0620382\n",
      "37 Train Loss 618.7209 Test MSE 626.3705491292153 Test RE 0.4213129763451375 Lambda1 -1.0732908\n",
      "38 Train Loss 618.2478 Test MSE 626.2632907412485 Test RE 0.42127690242257415 Lambda1 -1.0758724\n",
      "39 Train Loss 618.0267 Test MSE 625.7641274279893 Test RE 0.4211089795246875 Lambda1 -1.08231\n",
      "40 Train Loss 617.6377 Test MSE 625.2482589759745 Test RE 0.42093536648276675 Lambda1 -1.1055887\n",
      "41 Train Loss 617.2878 Test MSE 625.238591165822 Test RE 0.42093211214428405 Lambda1 -1.1202886\n",
      "42 Train Loss 616.92615 Test MSE 624.9183916846001 Test RE 0.4208243136921754 Lambda1 -1.120148\n",
      "43 Train Loss 616.59753 Test MSE 624.6860166481215 Test RE 0.420746064948884 Lambda1 -1.1139959\n",
      "44 Train Loss 616.2621 Test MSE 624.8892382014365 Test RE 0.42081449750033567 Lambda1 -1.1110921\n",
      "45 Train Loss 616.19617 Test MSE 624.9744710931612 Test RE 0.42084319539697745 Lambda1 -1.1116493\n",
      "46 Train Loss 616.11304 Test MSE 624.8582619784206 Test RE 0.42080406732769726 Lambda1 -1.1118697\n",
      "47 Train Loss 615.94073 Test MSE 624.6271300954297 Test RE 0.4207262334906955 Lambda1 -1.1134291\n",
      "48 Train Loss 615.75494 Test MSE 624.4627370893236 Test RE 0.42067086525714187 Lambda1 -1.1183004\n",
      "49 Train Loss 615.4491 Test MSE 624.4298295699813 Test RE 0.4206597809952692 Lambda1 -1.1228765\n",
      "50 Train Loss 614.7916 Test MSE 623.7421838119101 Test RE 0.4204280939573203 Lambda1 -1.1205239\n",
      "51 Train Loss 614.5624 Test MSE 623.3033527146312 Test RE 0.42028017275805796 Lambda1 -1.1232995\n",
      "52 Train Loss 614.2788 Test MSE 622.9144969284011 Test RE 0.4201490537182848 Lambda1 -1.1297574\n",
      "53 Train Loss 613.78284 Test MSE 622.3096403636833 Test RE 0.41994501958901614 Lambda1 -1.13419\n",
      "54 Train Loss 613.39594 Test MSE 621.6219685138486 Test RE 0.41971292918514785 Lambda1 -1.1339805\n",
      "55 Train Loss 612.05493 Test MSE 618.9462142947923 Test RE 0.41880863377823957 Lambda1 -1.1367437\n",
      "56 Train Loss 609.8237 Test MSE 615.090269277466 Test RE 0.4175020371424436 Lambda1 -1.1650349\n",
      "57 Train Loss 607.243 Test MSE 612.7056136566775 Test RE 0.41669194026042505 Lambda1 -1.19037\n",
      "58 Train Loss 596.8901 Test MSE 599.8963600100082 Test RE 0.41231324298551936 Lambda1 -1.2879994\n",
      "59 Train Loss 587.88983 Test MSE 583.4407713145857 Test RE 0.40661889702656084 Lambda1 -1.3290114\n",
      "60 Train Loss 573.76605 Test MSE 562.2771265179373 Test RE 0.3991759605286632 Lambda1 -1.3666995\n",
      "61 Train Loss 565.3788 Test MSE 552.1180581776813 Test RE 0.3955534225034023 Lambda1 -1.3593259\n",
      "62 Train Loss 541.3486 Test MSE 537.7367461104093 Test RE 0.39036783681677584 Lambda1 -1.3320956\n",
      "63 Train Loss 490.45422 Test MSE 483.54095519194675 Test RE 0.37017390694207486 Lambda1 -1.3967713\n",
      "64 Train Loss 463.0304 Test MSE 460.7614149670058 Test RE 0.36134930377188507 Lambda1 -1.4141214\n",
      "65 Train Loss 421.5101 Test MSE 427.50530564802585 Test RE 0.34806465580980017 Lambda1 -1.411202\n",
      "66 Train Loss 406.28925 Test MSE 410.7246589118724 Test RE 0.34116506974558664 Lambda1 -1.4016521\n",
      "67 Train Loss 395.88168 Test MSE 395.9201587470632 Test RE 0.33496002330567454 Lambda1 -1.3979471\n",
      "68 Train Loss 387.55582 Test MSE 391.7567985974168 Test RE 0.3331942066860657 Lambda1 -1.3582456\n",
      "69 Train Loss 371.00812 Test MSE 385.52819994025356 Test RE 0.33053484220720486 Lambda1 -1.2423502\n",
      "70 Train Loss 359.96573 Test MSE 373.02940506636764 Test RE 0.3251327403320889 Lambda1 -1.2433172\n",
      "71 Train Loss 349.20947 Test MSE 368.14520236702964 Test RE 0.3229971893511778 Lambda1 -1.2597911\n",
      "72 Train Loss 339.7939 Test MSE 359.6779214960825 Test RE 0.31926114036571474 Lambda1 -1.2805297\n",
      "73 Train Loss 329.07227 Test MSE 352.5726583286741 Test RE 0.3160919864075193 Lambda1 -1.2893122\n",
      "74 Train Loss 321.4656 Test MSE 347.1121898883787 Test RE 0.31363469792014154 Lambda1 -1.2930204\n",
      "Training time: 145.21\n",
      "Training time: 145.21\n",
      "inv_HT_stan_tune0\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 840.9803 Test MSE 859.5519126682077 Test RE 0.4935432608502972 Lambda1 -0.0074713267\n",
      "1 Train Loss 838.0515 Test MSE 858.2426163472832 Test RE 0.49316722734401397 Lambda1 -0.0074651637\n",
      "2 Train Loss 838.0501 Test MSE 858.2621252618592 Test RE 0.4931728324622241 Lambda1 -0.0074657067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Train Loss 838.04803 Test MSE 858.287573170747 Test RE 0.49318014382078723 Lambda1 -0.007468077\n",
      "4 Train Loss 837.90533 Test MSE 858.0493520712705 Test RE 0.49311169702447155 Lambda1 -0.0077847196\n",
      "5 Train Loss 837.9 Test MSE 857.9738248585464 Test RE 0.4930899942100922 Lambda1 -0.008287707\n",
      "6 Train Loss 837.8987 Test MSE 857.9455569112591 Test RE 0.4930818711436464 Lambda1 -0.009401652\n",
      "7 Train Loss 837.89716 Test MSE 857.921902796735 Test RE 0.4930750738035652 Lambda1 -0.011070353\n",
      "8 Train Loss 837.80725 Test MSE 857.8298695285032 Test RE 0.49304862586730464 Lambda1 -0.11403294\n",
      "9 Train Loss 837.75226 Test MSE 857.7104828746043 Test RE 0.4930143151747457 Lambda1 -0.17813441\n",
      "10 Train Loss 837.6517 Test MSE 857.4234255834709 Test RE 0.4929318076154824 Lambda1 -0.42479032\n",
      "11 Train Loss 837.1037 Test MSE 857.3142742093864 Test RE 0.49290043111294324 Lambda1 -0.46338293\n",
      "12 Train Loss 835.79004 Test MSE 853.5003420614606 Test RE 0.49180282648624524 Lambda1 -0.59882104\n",
      "13 Train Loss 827.656 Test MSE 843.4411811642606 Test RE 0.48889609846821047 Lambda1 -0.7616301\n",
      "14 Train Loss 818.38007 Test MSE 830.4809802797913 Test RE 0.48512540237722157 Lambda1 -0.7920863\n",
      "15 Train Loss 787.3434 Test MSE 780.1329972151613 Test RE 0.4701901148924425 Lambda1 -0.8042461\n",
      "16 Train Loss 740.4512 Test MSE 737.8860500575937 Test RE 0.4572816989025601 Lambda1 -0.6098381\n",
      "17 Train Loss 704.9966 Test MSE 700.580267770652 Test RE 0.44557223538609914 Lambda1 -0.6841705\n",
      "18 Train Loss 678.172 Test MSE 673.7033456662458 Test RE 0.43694172880920296 Lambda1 -0.89120114\n",
      "19 Train Loss 657.75964 Test MSE 640.8208683590556 Test RE 0.42614509469366024 Lambda1 -1.2015359\n",
      "20 Train Loss 638.89465 Test MSE 627.5109697679268 Test RE 0.4216963400870118 Lambda1 -1.1169572\n",
      "21 Train Loss 625.02924 Test MSE 609.8777466635935 Test RE 0.4157292331005717 Lambda1 -1.021935\n",
      "22 Train Loss 587.18854 Test MSE 566.8670430355508 Test RE 0.4008019029355569 Lambda1 -1.0050038\n",
      "23 Train Loss 569.1731 Test MSE 561.0958946631283 Test RE 0.39875644563071677 Lambda1 -0.9982283\n",
      "24 Train Loss 552.33563 Test MSE 548.3988187379266 Test RE 0.3942188856300564 Lambda1 -0.974473\n",
      "25 Train Loss 537.38324 Test MSE 535.3820063842771 Test RE 0.3895121921399445 Lambda1 -1.0832908\n",
      "26 Train Loss 529.05133 Test MSE 530.2580363438105 Test RE 0.3876437625174013 Lambda1 -1.1430814\n",
      "27 Train Loss 502.45697 Test MSE 489.9287999695219 Test RE 0.37261098608718946 Lambda1 -1.3081759\n",
      "28 Train Loss 496.86322 Test MSE 488.6301735538111 Test RE 0.3721168290331908 Lambda1 -1.3241811\n",
      "29 Train Loss 483.8759 Test MSE 467.8913447954774 Test RE 0.36413437272331217 Lambda1 -1.4694053\n",
      "30 Train Loss 461.8101 Test MSE 445.16003292209615 Test RE 0.35517897825227585 Lambda1 -1.5517067\n",
      "31 Train Loss 450.7188 Test MSE 435.1875086005727 Test RE 0.3511780647189146 Lambda1 -1.5829505\n",
      "32 Train Loss 441.9268 Test MSE 434.6863607133908 Test RE 0.3509758038111544 Lambda1 -1.6113191\n",
      "33 Train Loss 437.53757 Test MSE 429.0585916847418 Test RE 0.34869640676412234 Lambda1 -1.6426508\n",
      "34 Train Loss 432.27185 Test MSE 418.8463905555343 Test RE 0.34452168232723024 Lambda1 -1.6921653\n",
      "35 Train Loss 421.99506 Test MSE 417.4073154034387 Test RE 0.34392931815912253 Lambda1 -1.7447811\n",
      "36 Train Loss 390.0917 Test MSE 390.3413078908319 Test RE 0.33259171541978916 Lambda1 -1.9421012\n",
      "37 Train Loss 380.8546 Test MSE 385.0276464725822 Test RE 0.3303201963008826 Lambda1 -2.0179021\n",
      "38 Train Loss 360.9322 Test MSE 355.84464226466406 Test RE 0.3175553151880449 Lambda1 -2.2129657\n",
      "39 Train Loss 351.73608 Test MSE 347.25755661094735 Test RE 0.31370036440649257 Lambda1 -2.250648\n",
      "40 Train Loss 337.18863 Test MSE 332.831550166424 Test RE 0.30711527288020685 Lambda1 -2.292302\n",
      "41 Train Loss 324.30908 Test MSE 316.8263685380523 Test RE 0.29964002896238656 Lambda1 -2.3285918\n",
      "42 Train Loss 296.8288 Test MSE 302.1686053646726 Test RE 0.29262662653846355 Lambda1 -2.343122\n",
      "43 Train Loss 289.20535 Test MSE 292.9166287674249 Test RE 0.2881118918416919 Lambda1 -2.371565\n",
      "44 Train Loss 286.48196 Test MSE 286.6345026207923 Test RE 0.2850056066553716 Lambda1 -2.3823805\n",
      "45 Train Loss 277.14023 Test MSE 278.8991385337539 Test RE 0.281133602028895 Lambda1 -2.3873584\n",
      "46 Train Loss 266.5257 Test MSE 273.15327737342426 Test RE 0.2782225831345883 Lambda1 -2.4155605\n",
      "47 Train Loss 246.88133 Test MSE 257.0971488938135 Test RE 0.26992170024592826 Lambda1 -2.4844263\n",
      "48 Train Loss 241.65147 Test MSE 250.79527417213907 Test RE 0.26659306329874133 Lambda1 -2.5014064\n",
      "49 Train Loss 232.69644 Test MSE 235.68401184248012 Test RE 0.2584367266945157 Lambda1 -2.5527976\n",
      "50 Train Loss 228.18396 Test MSE 224.0457386853136 Test RE 0.25197503528460413 Lambda1 -2.5902596\n",
      "51 Train Loss 221.33626 Test MSE 210.69148111448186 Test RE 0.24435017742087276 Lambda1 -2.6342669\n",
      "52 Train Loss 214.23662 Test MSE 208.1529830948013 Test RE 0.24287370069333764 Lambda1 -2.655239\n",
      "53 Train Loss 206.18855 Test MSE 200.22292722152608 Test RE 0.23820236760583977 Lambda1 -2.6865392\n",
      "54 Train Loss 202.8674 Test MSE 198.36254856358167 Test RE 0.23709315200413 Lambda1 -2.6806076\n",
      "55 Train Loss 197.76672 Test MSE 200.41258856601922 Test RE 0.2383151596030381 Lambda1 -2.669902\n",
      "56 Train Loss 194.3434 Test MSE 196.65938289074313 Test RE 0.2360731019734284 Lambda1 -2.683009\n",
      "57 Train Loss 186.80391 Test MSE 183.4647297113258 Test RE 0.22801607381752456 Lambda1 -2.7348442\n",
      "58 Train Loss 183.27867 Test MSE 180.1506550589358 Test RE 0.22594726744894403 Lambda1 -2.7440696\n",
      "59 Train Loss 180.50735 Test MSE 180.68256745225491 Test RE 0.22628058727123543 Lambda1 -2.7164001\n",
      "60 Train Loss 179.46823 Test MSE 179.96180562813262 Test RE 0.22582880770397126 Lambda1 -2.7095747\n",
      "61 Train Loss 178.12328 Test MSE 178.5547450565029 Test RE 0.22494423568446575 Lambda1 -2.6990645\n",
      "62 Train Loss 177.62976 Test MSE 178.04995607487425 Test RE 0.2246260426881975 Lambda1 -2.6912532\n",
      "63 Train Loss 176.51314 Test MSE 176.6303409690981 Test RE 0.2237287644752601 Lambda1 -2.674971\n",
      "64 Train Loss 171.48486 Test MSE 171.55617786293436 Test RE 0.22049175366183452 Lambda1 -2.6512682\n",
      "65 Train Loss 168.95566 Test MSE 169.8253848039551 Test RE 0.21937668738365343 Lambda1 -2.648325\n",
      "66 Train Loss 167.17606 Test MSE 167.79838733426664 Test RE 0.21806354195163483 Lambda1 -2.6280282\n",
      "67 Train Loss 158.90804 Test MSE 160.29630687024314 Test RE 0.21313311333162563 Lambda1 -2.6002383\n",
      "68 Train Loss 155.62538 Test MSE 156.6901604623658 Test RE 0.21072207456725703 Lambda1 -2.596263\n",
      "69 Train Loss 153.9086 Test MSE 154.43744663200525 Test RE 0.20920182891590894 Lambda1 -2.589475\n",
      "70 Train Loss 152.63805 Test MSE 153.70391693956896 Test RE 0.2087044158618332 Lambda1 -2.5821936\n",
      "71 Train Loss 151.90656 Test MSE 153.37022750161432 Test RE 0.20847774532556684 Lambda1 -2.5831\n",
      "72 Train Loss 151.36398 Test MSE 153.32938028211555 Test RE 0.20844998145170765 Lambda1 -2.5873067\n",
      "73 Train Loss 150.41948 Test MSE 153.7123343646396 Test RE 0.20871013051688683 Lambda1 -2.5998857\n",
      "74 Train Loss 148.89552 Test MSE 153.302330841368 Test RE 0.2084315938990116 Lambda1 -2.622223\n",
      "Training time: 145.40\n",
      "Training time: 145.40\n",
      "inv_HT_stan_tune0\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 843.1217 Test MSE 861.1629782327432 Test RE 0.4940055705195531 Lambda1 -0.011060776\n",
      "1 Train Loss 838.0393 Test MSE 858.1476007821447 Test RE 0.49313992745298396 Lambda1 -0.011133003\n",
      "2 Train Loss 838.00836 Test MSE 858.2846287466118 Test RE 0.4931792978731065 Lambda1 -0.011150217\n",
      "3 Train Loss 837.8971 Test MSE 858.1255290247724 Test RE 0.49313358557496206 Lambda1 -0.012118721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 837.84125 Test MSE 857.8026606188968 Test RE 0.4930408064719042 Lambda1 -0.017767275\n",
      "5 Train Loss 837.8335 Test MSE 857.7312907921553 Test RE 0.49302029536223857 Lambda1 -0.021669965\n",
      "6 Train Loss 837.7972 Test MSE 857.6542944928307 Test RE 0.49299816629303617 Lambda1 -0.03819424\n",
      "7 Train Loss 836.35876 Test MSE 854.6717153848172 Test RE 0.4921401943575884 Lambda1 -0.23855703\n",
      "8 Train Loss 818.3906 Test MSE 825.2354653858403 Test RE 0.48359089188977933 Lambda1 -0.034161195\n",
      "9 Train Loss 763.89856 Test MSE 747.0304265136473 Test RE 0.46010644433629266 Lambda1 0.20365801\n",
      "10 Train Loss 677.29315 Test MSE 679.3271272547267 Test RE 0.438761638205599 Lambda1 0.08156652\n",
      "11 Train Loss 642.7627 Test MSE 642.3006394834352 Test RE 0.42663683397373353 Lambda1 0.032616343\n",
      "12 Train Loss 614.91034 Test MSE 619.1602091021794 Test RE 0.4188810270861079 Lambda1 0.0121098645\n",
      "13 Train Loss 569.63544 Test MSE 561.3220516287598 Test RE 0.39883679950894696 Lambda1 0.0074834567\n",
      "14 Train Loss 394.90686 Test MSE 351.4475092416321 Test RE 0.3155872183858419 Lambda1 0.0005646553\n",
      "15 Train Loss 293.9951 Test MSE 305.11694470705044 Test RE 0.294050778873553 Lambda1 0.0005412792\n",
      "16 Train Loss 280.68982 Test MSE 293.71504614520074 Test RE 0.2885042850662975 Lambda1 0.0012764626\n",
      "17 Train Loss 264.03333 Test MSE 285.28466149715194 Test RE 0.2843337296071152 Lambda1 0.001512541\n",
      "18 Train Loss 261.56223 Test MSE 284.35398233315294 Test RE 0.2838695622588518 Lambda1 0.0013424237\n",
      "19 Train Loss 259.6232 Test MSE 284.46101120887283 Test RE 0.2839229805044342 Lambda1 0.0014816852\n",
      "20 Train Loss 257.77554 Test MSE 284.20812336036425 Test RE 0.283796747675021 Lambda1 0.0005731156\n",
      "21 Train Loss 256.45636 Test MSE 283.69194552328645 Test RE 0.2835389152042301 Lambda1 0.00029312575\n",
      "22 Train Loss 255.48204 Test MSE 283.1509268042984 Test RE 0.2832684227408899 Lambda1 0.0002384091\n",
      "23 Train Loss 255.38182 Test MSE 283.3015361242989 Test RE 0.28334374863399464 Lambda1 0.00013686753\n",
      "24 Train Loss 255.20358 Test MSE 283.78445856928414 Test RE 0.2835851430068787 Lambda1 0.00019119236\n",
      "25 Train Loss 255.03229 Test MSE 283.8265142129671 Test RE 0.28360615528140953 Lambda1 0.00040318957\n",
      "26 Train Loss 254.83975 Test MSE 283.74904417443946 Test RE 0.2835674476939969 Lambda1 0.0007755894\n",
      "27 Train Loss 254.61737 Test MSE 283.36806541252434 Test RE 0.28337701628161516 Lambda1 0.0010011148\n",
      "28 Train Loss 254.2916 Test MSE 282.1952041124079 Test RE 0.28278995901989173 Lambda1 0.0013239281\n",
      "29 Train Loss 254.22868 Test MSE 281.7850705552397 Test RE 0.28258438532161867 Lambda1 0.001510382\n",
      "30 Train Loss 254.1225 Test MSE 281.31876820870605 Test RE 0.2823504759797796 Lambda1 0.0019328805\n",
      "31 Train Loss 254.02542 Test MSE 281.2574655994706 Test RE 0.2823197105885165 Lambda1 0.002215309\n",
      "32 Train Loss 253.94618 Test MSE 281.38155151543555 Test RE 0.2823819810011331 Lambda1 0.0023575877\n",
      "33 Train Loss 253.48326 Test MSE 280.0134819420849 Test RE 0.28169467775335066 Lambda1 0.0044525056\n",
      "34 Train Loss 252.68689 Test MSE 277.24256491231563 Test RE 0.28029743572035243 Lambda1 0.00853757\n",
      "35 Train Loss 251.80502 Test MSE 275.83063802344617 Test RE 0.2795827823812789 Lambda1 0.01210545\n",
      "36 Train Loss 249.88211 Test MSE 270.05064268223003 Test RE 0.2766379631079694 Lambda1 0.024602575\n",
      "37 Train Loss 246.7544 Test MSE 266.4740036552059 Test RE 0.2747999150685691 Lambda1 0.03909459\n",
      "38 Train Loss 242.68571 Test MSE 261.03567216068353 Test RE 0.27198133481894277 Lambda1 0.061170254\n",
      "39 Train Loss 239.64589 Test MSE 257.77879612907736 Test RE 0.2702792880056966 Lambda1 0.081947595\n",
      "40 Train Loss 235.96477 Test MSE 257.45337120479735 Test RE 0.27010863123542034 Lambda1 0.09618364\n",
      "41 Train Loss 233.52167 Test MSE 256.8307275754062 Test RE 0.26978180850567185 Lambda1 0.11164853\n",
      "42 Train Loss 229.19685 Test MSE 252.9151588963662 Test RE 0.26771740134408367 Lambda1 0.1281482\n",
      "43 Train Loss 225.4932 Test MSE 248.26881047467967 Test RE 0.2652468605656186 Lambda1 0.12773114\n",
      "44 Train Loss 223.21025 Test MSE 245.3039911612328 Test RE 0.2636583183791291 Lambda1 0.13308878\n",
      "45 Train Loss 218.32953 Test MSE 240.52540398510183 Test RE 0.26107762121759365 Lambda1 0.16717109\n",
      "46 Train Loss 217.32709 Test MSE 239.00256006207061 Test RE 0.260249825556635 Lambda1 0.17760253\n",
      "47 Train Loss 214.62808 Test MSE 235.7984612724645 Test RE 0.2584994682141805 Lambda1 0.19770251\n",
      "48 Train Loss 210.94173 Test MSE 233.23223708480694 Test RE 0.2570889790382771 Lambda1 0.21550918\n",
      "49 Train Loss 208.83684 Test MSE 233.31438313247367 Test RE 0.2571342493331694 Lambda1 0.22469108\n",
      "50 Train Loss 206.13583 Test MSE 231.82476976169235 Test RE 0.2563120883882613 Lambda1 0.24785976\n",
      "51 Train Loss 204.51643 Test MSE 230.4711322658964 Test RE 0.25556268268793 Lambda1 0.27163354\n",
      "52 Train Loss 201.15463 Test MSE 225.61565547742651 Test RE 0.25285630463882136 Lambda1 0.30675635\n",
      "53 Train Loss 199.48552 Test MSE 223.65792989940704 Test RE 0.25175686453144464 Lambda1 0.32421824\n",
      "54 Train Loss 192.1198 Test MSE 213.01428741881227 Test RE 0.24569342668725425 Lambda1 0.38046005\n",
      "55 Train Loss 181.71262 Test MSE 191.02150218101502 Test RE 0.23266459377429155 Lambda1 0.40789625\n",
      "56 Train Loss 157.74069 Test MSE 159.86063514260454 Test RE 0.21284327742358064 Lambda1 0.41900855\n",
      "57 Train Loss 148.65431 Test MSE 145.62349356428726 Test RE 0.2031444180989303 Lambda1 0.42895815\n",
      "58 Train Loss 130.26419 Test MSE 117.08161947715308 Test RE 0.182151826793494 Lambda1 0.5034919\n",
      "59 Train Loss 119.9105 Test MSE 107.8902956160465 Test RE 0.17485592869512115 Lambda1 0.506487\n",
      "60 Train Loss 117.16478 Test MSE 110.11207671930723 Test RE 0.17664715504985115 Lambda1 0.49977863\n",
      "61 Train Loss 108.97155 Test MSE 98.80377561018788 Test RE 0.1673308206439259 Lambda1 0.53455746\n",
      "62 Train Loss 100.95833 Test MSE 91.99354957282253 Test RE 0.16146108213229135 Lambda1 0.58965147\n",
      "63 Train Loss 91.44279 Test MSE 83.64129021905183 Test RE 0.1539570327919683 Lambda1 0.6525743\n",
      "64 Train Loss 88.39948 Test MSE 82.99571714121528 Test RE 0.15336173445450957 Lambda1 0.6699979\n",
      "65 Train Loss 83.83246 Test MSE 80.53442223556436 Test RE 0.15107059659946612 Lambda1 0.70730925\n",
      "66 Train Loss 81.600136 Test MSE 76.824530737874 Test RE 0.1475499710802159 Lambda1 0.7287053\n",
      "67 Train Loss 75.09477 Test MSE 70.40485645991505 Test RE 0.14125065790963795 Lambda1 0.75570995\n",
      "68 Train Loss 70.32044 Test MSE 69.59512532675615 Test RE 0.14043604209523408 Lambda1 0.76737475\n",
      "69 Train Loss 64.58874 Test MSE 63.07838117503577 Test RE 0.13369939443925266 Lambda1 0.8280669\n",
      "70 Train Loss 60.63046 Test MSE 56.994668773165365 Test RE 0.1270885099009033 Lambda1 0.8833195\n",
      "71 Train Loss 55.032135 Test MSE 52.518517981090035 Test RE 0.12199594676152878 Lambda1 0.9510013\n",
      "72 Train Loss 53.247086 Test MSE 49.17035956022445 Test RE 0.11804316970085287 Lambda1 0.9741653\n",
      "73 Train Loss 51.69743 Test MSE 48.09248955956011 Test RE 0.11674218038103706 Lambda1 0.97788894\n",
      "74 Train Loss 50.513958 Test MSE 47.41880487694871 Test RE 0.11592162824335966 Lambda1 0.9905834\n",
      "Training time: 143.04\n",
      "Training time: 143.04\n",
      "inv_HT_stan_tune1\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.0496 Test MSE 858.3497587139314 Test RE 0.4931980096973304 Lambda1 -0.067959204\n",
      "1 Train Loss 837.99097 Test MSE 858.0665350011174 Test RE 0.49311663442187004 Lambda1 -0.06809015\n",
      "2 Train Loss 837.8554 Test MSE 857.8728857084155 Test RE 0.49306098776053764 Lambda1 -0.07057458\n",
      "3 Train Loss 837.8278 Test MSE 857.8939931958696 Test RE 0.49306705346955515 Lambda1 -0.07525864\n",
      "4 Train Loss 837.59827 Test MSE 857.5092848247286 Test RE 0.49295648719051643 Lambda1 -0.12466507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 836.18024 Test MSE 852.5746440333273 Test RE 0.4915360518910144 Lambda1 -0.17481315\n",
      "6 Train Loss 818.16077 Test MSE 826.0515146693828 Test RE 0.48382993668689245 Lambda1 -0.1547623\n",
      "7 Train Loss 752.81506 Test MSE 749.3505433110737 Test RE 0.4608203865531377 Lambda1 0.020590426\n",
      "8 Train Loss 672.5643 Test MSE 668.977456227262 Test RE 0.4354065038835836 Lambda1 0.004646206\n",
      "9 Train Loss 654.7213 Test MSE 658.9333205741327 Test RE 0.432125510153046 Lambda1 0.004232782\n",
      "10 Train Loss 648.435 Test MSE 652.6639422078318 Test RE 0.4300648819323543 Lambda1 0.0026129985\n",
      "11 Train Loss 642.1318 Test MSE 648.6836909030636 Test RE 0.4287515076415007 Lambda1 0.00029584678\n",
      "12 Train Loss 635.3422 Test MSE 645.7680815943104 Test RE 0.4277868774755028 Lambda1 -0.00041129233\n",
      "13 Train Loss 628.8577 Test MSE 636.7637814795745 Test RE 0.4247939738757665 Lambda1 0.0014606628\n",
      "14 Train Loss 610.53564 Test MSE 614.7033610650465 Test RE 0.4173707065188045 Lambda1 -5.6745805e-05\n",
      "15 Train Loss 409.40424 Test MSE 348.60360067406947 Test RE 0.314307761004723 Lambda1 0.00019498449\n",
      "16 Train Loss 315.5415 Test MSE 301.0991715299615 Test RE 0.2921083360971161 Lambda1 -0.0024665392\n",
      "17 Train Loss 273.28668 Test MSE 294.99840879373306 Test RE 0.2891338953968174 Lambda1 0.0005750756\n",
      "18 Train Loss 266.31094 Test MSE 291.86016633073575 Test RE 0.2875918559189751 Lambda1 0.0005432951\n",
      "19 Train Loss 260.67926 Test MSE 285.6385889351641 Test RE 0.2845100487886402 Lambda1 2.06926e-05\n",
      "20 Train Loss 258.19727 Test MSE 283.8531313166421 Test RE 0.2836194531889133 Lambda1 0.00019968957\n",
      "21 Train Loss 257.27216 Test MSE 283.4246898887384 Test RE 0.2834053279997451 Lambda1 0.00036937805\n",
      "22 Train Loss 256.36777 Test MSE 282.31788722326684 Test RE 0.282851423179256 Lambda1 0.00037963814\n",
      "23 Train Loss 255.61179 Test MSE 281.43216684891775 Test RE 0.2824073775052099 Lambda1 0.00013932941\n",
      "24 Train Loss 255.23701 Test MSE 281.62967190528036 Test RE 0.28250645484809805 Lambda1 0.00026591486\n",
      "25 Train Loss 254.99757 Test MSE 281.7033004128946 Test RE 0.2825433813004676 Lambda1 0.00044662133\n",
      "26 Train Loss 254.82448 Test MSE 281.5818407374588 Test RE 0.2824824637903632 Lambda1 0.0006084683\n",
      "27 Train Loss 254.7114 Test MSE 281.5146885786711 Test RE 0.28244877831137905 Lambda1 0.00069775153\n",
      "28 Train Loss 254.50832 Test MSE 281.77517851819135 Test RE 0.28257942522963025 Lambda1 0.0009152605\n",
      "29 Train Loss 254.35777 Test MSE 281.9515382525291 Test RE 0.2826678429482911 Lambda1 0.0009395425\n",
      "30 Train Loss 254.22061 Test MSE 282.48273581550575 Test RE 0.2829339911928615 Lambda1 0.00086649146\n",
      "31 Train Loss 254.0394 Test MSE 282.25389596796816 Test RE 0.28281936526825824 Lambda1 0.00079799135\n",
      "32 Train Loss 253.70956 Test MSE 281.7951471843164 Test RE 0.2825894378827284 Lambda1 0.00057863025\n",
      "33 Train Loss 253.53606 Test MSE 281.8029396108631 Test RE 0.28259334505117417 Lambda1 0.0005402986\n",
      "34 Train Loss 253.19861 Test MSE 281.7669202855463 Test RE 0.28257528429758466 Lambda1 0.000473495\n",
      "35 Train Loss 252.91557 Test MSE 281.9296923723872 Test RE 0.2826568920461012 Lambda1 0.0004111511\n",
      "36 Train Loss 252.81305 Test MSE 282.1141207318235 Test RE 0.28274932896511423 Lambda1 0.00043014644\n",
      "37 Train Loss 252.56633 Test MSE 282.8014384759359 Test RE 0.283093552088478 Lambda1 0.00036691275\n",
      "38 Train Loss 252.231 Test MSE 283.5288918371624 Test RE 0.2834574206271653 Lambda1 0.00023699139\n",
      "39 Train Loss 252.11783 Test MSE 283.6047937874426 Test RE 0.283495359496793 Lambda1 0.00019329468\n",
      "40 Train Loss 252.01305 Test MSE 283.7450058074462 Test RE 0.28356542979557725 Lambda1 0.00016902306\n",
      "41 Train Loss 251.78561 Test MSE 283.45809134100944 Test RE 0.28342202709300607 Lambda1 0.00013659526\n",
      "42 Train Loss 251.43314 Test MSE 283.60440607226633 Test RE 0.2834951657139423 Lambda1 8.886586e-05\n",
      "43 Train Loss 251.22913 Test MSE 284.24487007279566 Test RE 0.28381509384381215 Lambda1 6.573057e-05\n",
      "44 Train Loss 251.15277 Test MSE 283.93789161038467 Test RE 0.2836617952862534 Lambda1 5.5204593e-05\n",
      "45 Train Loss 251.08315 Test MSE 283.8892924626971 Test RE 0.2836375183020336 Lambda1 4.8743495e-05\n",
      "46 Train Loss 251.02171 Test MSE 284.3576274063849 Test RE 0.2838713816847981 Lambda1 4.4879864e-05\n",
      "47 Train Loss 250.92975 Test MSE 284.77008124380757 Test RE 0.28407718135988325 Lambda1 4.1037754e-05\n",
      "48 Train Loss 250.86484 Test MSE 284.6929765059766 Test RE 0.2840387201926034 Lambda1 3.369251e-05\n",
      "49 Train Loss 250.80223 Test MSE 284.97467446459694 Test RE 0.2841792107406516 Lambda1 2.7740783e-05\n",
      "50 Train Loss 250.74626 Test MSE 285.4446137375354 Test RE 0.28441342797532676 Lambda1 2.290177e-05\n",
      "51 Train Loss 250.67554 Test MSE 285.47096822626 Test RE 0.28442655731306066 Lambda1 2.0691192e-05\n",
      "52 Train Loss 250.50084 Test MSE 285.32435926990195 Test RE 0.28435351164356637 Lambda1 1.9751053e-05\n",
      "53 Train Loss 250.40733 Test MSE 285.76521149366505 Test RE 0.2845731029418524 Lambda1 1.5612064e-05\n",
      "54 Train Loss 250.35619 Test MSE 285.90176836736526 Test RE 0.28464108842490987 Lambda1 1.1294364e-05\n",
      "55 Train Loss 250.29689 Test MSE 285.9509520766178 Test RE 0.28466557078920846 Lambda1 1.261366e-05\n",
      "56 Train Loss 250.25168 Test MSE 286.141684280732 Test RE 0.28476049238426576 Lambda1 1.3191947e-05\n",
      "57 Train Loss 250.20686 Test MSE 286.37870362442357 Test RE 0.28487840560306066 Lambda1 1.3671956e-05\n",
      "58 Train Loss 250.16916 Test MSE 286.5380156958922 Test RE 0.28495763331491736 Lambda1 1.3028333e-05\n",
      "59 Train Loss 250.12163 Test MSE 286.8768874533852 Test RE 0.28512608489100383 Lambda1 1.3604581e-05\n",
      "60 Train Loss 250.06339 Test MSE 287.1303363701092 Test RE 0.28525200814671353 Lambda1 1.449461e-05\n",
      "61 Train Loss 250.02194 Test MSE 286.7789751116456 Test RE 0.28507742334598224 Lambda1 1.8518425e-05\n",
      "62 Train Loss 249.97636 Test MSE 286.7377495016529 Test RE 0.2850569321069276 Lambda1 1.7837912e-05\n",
      "63 Train Loss 249.95334 Test MSE 287.1494193038161 Test RE 0.28526148703840604 Lambda1 1.5125721e-05\n",
      "64 Train Loss 249.94235 Test MSE 287.3192744725765 Test RE 0.2853458437732806 Lambda1 1.3850002e-05\n",
      "65 Train Loss 249.92715 Test MSE 287.44219092532546 Test RE 0.28540687335199405 Lambda1 1.112714e-05\n",
      "66 Train Loss 249.91354 Test MSE 287.58069066153837 Test RE 0.2854756245956862 Lambda1 9.865309e-06\n",
      "67 Train Loss 249.90073 Test MSE 287.6709121195678 Test RE 0.2855204016063722 Lambda1 8.971759e-06\n",
      "68 Train Loss 249.88307 Test MSE 287.78322265120886 Test RE 0.28557613163940393 Lambda1 8.93182e-06\n",
      "69 Train Loss 249.86928 Test MSE 287.7819537964318 Test RE 0.2855755020769247 Lambda1 9.100517e-06\n",
      "70 Train Loss 249.85513 Test MSE 287.7081159770636 Test RE 0.285538863877779 Lambda1 9.550062e-06\n",
      "71 Train Loss 249.83806 Test MSE 287.52299454910656 Test RE 0.2854469862683061 Lambda1 1.0018511e-05\n",
      "72 Train Loss 249.78314 Test MSE 287.4071896670655 Test RE 0.28538949611211656 Lambda1 1.1039371e-05\n",
      "73 Train Loss 249.75558 Test MSE 287.91688832781597 Test RE 0.28564244421864915 Lambda1 1.1904693e-05\n",
      "74 Train Loss 249.74318 Test MSE 288.2554389891807 Test RE 0.2858103329935784 Lambda1 1.0271819e-05\n",
      "Training time: 144.03\n",
      "Training time: 144.03\n",
      "inv_HT_stan_tune1\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 853.31116 Test MSE 877.2725101628182 Test RE 0.4986047723350163 Lambda1 -0.046590418\n",
      "1 Train Loss 838.0199 Test MSE 858.508112615424 Test RE 0.49324350177471116 Lambda1 -0.045777734\n",
      "2 Train Loss 837.9106 Test MSE 858.0826702029334 Test RE 0.49312127071677675 Lambda1 -0.04564378\n",
      "3 Train Loss 837.89844 Test MSE 857.996525838518 Test RE 0.493096517458073 Lambda1 -0.04549682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 837.8223 Test MSE 857.6847961626602 Test RE 0.4930069327235765 Lambda1 -0.044346545\n",
      "5 Train Loss 834.2092 Test MSE 853.6615039707426 Test RE 0.49184925654455863 Lambda1 0.028900292\n",
      "6 Train Loss 786.48413 Test MSE 783.3883831351962 Test RE 0.4711701124125836 Lambda1 0.112049565\n",
      "7 Train Loss 709.2394 Test MSE 702.5368346426354 Test RE 0.44619399429265133 Lambda1 0.08100276\n",
      "8 Train Loss 676.7674 Test MSE 674.8697290359529 Test RE 0.43731980409581006 Lambda1 0.08240015\n",
      "9 Train Loss 656.4059 Test MSE 661.9752647919549 Test RE 0.43312180830578867 Lambda1 0.07102769\n",
      "10 Train Loss 641.23254 Test MSE 643.1934458692901 Test RE 0.426933246426789 Lambda1 0.042621717\n",
      "11 Train Loss 633.4581 Test MSE 635.7215901967028 Test RE 0.4244462013685811 Lambda1 0.024053834\n",
      "12 Train Loss 625.48126 Test MSE 632.4712722059795 Test RE 0.4233597564104916 Lambda1 0.019470913\n",
      "13 Train Loss 586.4143 Test MSE 583.4898346484173 Test RE 0.4066359935856887 Lambda1 0.013837115\n",
      "14 Train Loss 385.15054 Test MSE 367.3764795288 Test RE 0.3226597884204983 Lambda1 0.003218019\n",
      "15 Train Loss 301.73923 Test MSE 304.39039230595927 Test RE 0.29370046952817236 Lambda1 0.0046303445\n",
      "16 Train Loss 265.0103 Test MSE 287.0278988766991 Test RE 0.28520111991989305 Lambda1 0.0029951425\n",
      "17 Train Loss 261.6657 Test MSE 286.6334219681695 Test RE 0.28500506939913794 Lambda1 0.003175223\n",
      "18 Train Loss 257.61737 Test MSE 281.8716337834976 Test RE 0.28262778637597025 Lambda1 0.0038207686\n",
      "19 Train Loss 255.34789 Test MSE 281.0709998362766 Test RE 0.2822261100714953 Lambda1 0.0036604113\n",
      "20 Train Loss 254.8279 Test MSE 281.4527226748065 Test RE 0.2824176908445645 Lambda1 0.0032980086\n",
      "21 Train Loss 254.65866 Test MSE 281.25228933809194 Test RE 0.28231711267109766 Lambda1 0.0034541953\n",
      "22 Train Loss 254.11096 Test MSE 281.23958668733604 Test RE 0.282310737227566 Lambda1 0.0045683673\n",
      "23 Train Loss 253.86124 Test MSE 281.3430724482657 Test RE 0.28236267240279556 Lambda1 0.004934733\n",
      "24 Train Loss 253.43974 Test MSE 280.3886933837131 Test RE 0.2818833466736586 Lambda1 0.005962948\n",
      "25 Train Loss 252.63234 Test MSE 278.66270867876796 Test RE 0.28101441473090977 Lambda1 0.008638212\n",
      "26 Train Loss 252.1824 Test MSE 278.2461502467342 Test RE 0.28080429924112865 Lambda1 0.0092181135\n",
      "27 Train Loss 251.59059 Test MSE 277.53195386177987 Test RE 0.2804436863949938 Lambda1 0.010874481\n",
      "28 Train Loss 251.00241 Test MSE 276.8509756343208 Test RE 0.2800994137177441 Lambda1 0.012661734\n",
      "29 Train Loss 249.98431 Test MSE 276.15649313525984 Test RE 0.2797478775078767 Lambda1 0.0158087\n",
      "30 Train Loss 249.2724 Test MSE 274.8960549405533 Test RE 0.27910873234810313 Lambda1 0.019913815\n",
      "31 Train Loss 248.4267 Test MSE 272.0236509971606 Test RE 0.277646691863251 Lambda1 0.026142977\n",
      "32 Train Loss 247.17897 Test MSE 270.8292905637182 Test RE 0.2770364967214821 Lambda1 0.027695889\n",
      "33 Train Loss 245.51738 Test MSE 267.9392414867961 Test RE 0.27555438875495714 Lambda1 0.034496777\n",
      "34 Train Loss 243.57855 Test MSE 263.3371206058691 Test RE 0.2731776796958912 Lambda1 0.045471337\n",
      "35 Train Loss 242.11877 Test MSE 261.2698235457115 Test RE 0.27210329234808217 Lambda1 0.055346478\n",
      "36 Train Loss 235.60553 Test MSE 254.63176307443473 Test RE 0.2686244004518789 Lambda1 0.08769771\n",
      "37 Train Loss 231.89594 Test MSE 252.6443839153808 Test RE 0.26757405171743737 Lambda1 0.08572145\n",
      "38 Train Loss 229.50804 Test MSE 250.27718447765116 Test RE 0.2663175586582998 Lambda1 0.09197078\n",
      "39 Train Loss 225.36388 Test MSE 247.49082446480287 Test RE 0.2648309398878048 Lambda1 0.109124996\n",
      "40 Train Loss 222.80411 Test MSE 245.34612323778663 Test RE 0.26368095966486543 Lambda1 0.12202688\n",
      "41 Train Loss 221.41844 Test MSE 243.8112650742502 Test RE 0.2628548862475977 Lambda1 0.13188864\n",
      "42 Train Loss 220.57068 Test MSE 244.0779385388936 Test RE 0.2629985983650945 Lambda1 0.13346641\n",
      "43 Train Loss 218.8636 Test MSE 242.90440592823936 Test RE 0.2623655847625968 Lambda1 0.14938737\n",
      "44 Train Loss 217.99107 Test MSE 242.3464518963852 Test RE 0.26206408324056735 Lambda1 0.16193767\n",
      "45 Train Loss 215.52988 Test MSE 240.96838937636534 Test RE 0.26131792924386066 Lambda1 0.20338018\n",
      "46 Train Loss 214.03792 Test MSE 239.6473207633104 Test RE 0.26060062900411973 Lambda1 0.22067976\n",
      "47 Train Loss 212.63596 Test MSE 238.3460707718893 Test RE 0.2598921542672099 Lambda1 0.22729987\n",
      "48 Train Loss 209.31369 Test MSE 232.85916180232357 Test RE 0.25688327863660493 Lambda1 0.26346308\n",
      "49 Train Loss 205.70767 Test MSE 228.72318494308684 Test RE 0.2545917143969036 Lambda1 0.29191315\n",
      "50 Train Loss 196.5582 Test MSE 212.67148871803488 Test RE 0.2454956528655252 Lambda1 0.32130042\n",
      "51 Train Loss 178.00145 Test MSE 189.54910731297383 Test RE 0.23176616912748982 Lambda1 0.35624853\n",
      "52 Train Loss 170.17975 Test MSE 176.72384501628122 Test RE 0.22378797507573178 Lambda1 0.3905036\n",
      "53 Train Loss 161.94452 Test MSE 162.9152402661124 Test RE 0.21486715191593087 Lambda1 0.4275178\n",
      "54 Train Loss 157.68634 Test MSE 156.35263298714307 Test RE 0.21049499319420228 Lambda1 0.4445564\n",
      "55 Train Loss 153.09915 Test MSE 153.6954531330557 Test RE 0.2086986695602827 Lambda1 0.46490473\n",
      "56 Train Loss 143.96628 Test MSE 139.0799140153542 Test RE 0.19852782168703947 Lambda1 0.5121817\n",
      "57 Train Loss 137.0477 Test MSE 135.73882064530628 Test RE 0.19612872560347822 Lambda1 0.5423297\n",
      "58 Train Loss 132.6965 Test MSE 133.65563406875083 Test RE 0.19461791069555473 Lambda1 0.543329\n",
      "59 Train Loss 130.31322 Test MSE 133.22784314216366 Test RE 0.19430620478490326 Lambda1 0.55138505\n",
      "60 Train Loss 127.67286 Test MSE 133.4690198962077 Test RE 0.19448199743338712 Lambda1 0.5511675\n",
      "61 Train Loss 126.94308 Test MSE 133.00237887734988 Test RE 0.194141720938872 Lambda1 0.5548063\n",
      "62 Train Loss 125.576096 Test MSE 132.8152654679741 Test RE 0.19400510945480567 Lambda1 0.5612909\n",
      "63 Train Loss 124.56012 Test MSE 131.72788069823017 Test RE 0.19320929819660557 Lambda1 0.5597348\n",
      "64 Train Loss 123.41851 Test MSE 129.74796483882656 Test RE 0.19175179930934402 Lambda1 0.5584571\n",
      "65 Train Loss 122.30512 Test MSE 128.45415421614967 Test RE 0.19079335618251492 Lambda1 0.5544554\n",
      "66 Train Loss 120.61725 Test MSE 127.02252131593185 Test RE 0.1897271727743557 Lambda1 0.5503896\n",
      "67 Train Loss 119.34253 Test MSE 124.89407346076398 Test RE 0.18813087953438654 Lambda1 0.5502265\n",
      "68 Train Loss 118.100426 Test MSE 122.39976047383777 Test RE 0.1862427838481265 Lambda1 0.5406476\n",
      "69 Train Loss 116.778946 Test MSE 120.19930223938321 Test RE 0.1845610889378695 Lambda1 0.5340751\n",
      "70 Train Loss 115.821205 Test MSE 119.85225638414006 Test RE 0.18429445901735714 Lambda1 0.54088074\n",
      "71 Train Loss 113.60607 Test MSE 116.5326023292866 Test RE 0.18172425333075756 Lambda1 0.54713356\n",
      "72 Train Loss 112.08769 Test MSE 113.47395137521005 Test RE 0.17932352185075764 Lambda1 0.54693073\n",
      "73 Train Loss 108.125084 Test MSE 110.34557083939075 Test RE 0.1768343471849178 Lambda1 0.56648564\n",
      "74 Train Loss 105.01923 Test MSE 106.74226274464245 Test RE 0.17392314220346017 Lambda1 0.6055855\n",
      "Training time: 143.99\n",
      "Training time: 143.99\n",
      "inv_HT_stan_tune1\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 838.029 Test MSE 858.014370852519 Test RE 0.49310164525768546 Lambda1 -0.12956518\n",
      "1 Train Loss 837.9491 Test MSE 858.1612216924314 Test RE 0.49314384110842674 Lambda1 -0.12967296\n",
      "2 Train Loss 837.8585 Test MSE 857.9935129211761 Test RE 0.4930956516851309 Lambda1 -0.12979649\n",
      "3 Train Loss 837.802 Test MSE 857.6135210653428 Test RE 0.49298644743147024 Lambda1 -0.13237573\n",
      "4 Train Loss 837.6906 Test MSE 857.165565691203 Test RE 0.4928576803541102 Lambda1 -0.1452814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 836.34326 Test MSE 854.1218688043805 Test RE 0.49198186155303325 Lambda1 -0.1936246\n",
      "6 Train Loss 821.1414 Test MSE 831.0456097144058 Test RE 0.48529028847714506 Lambda1 -0.07807015\n",
      "7 Train Loss 777.5356 Test MSE 788.1524291850486 Test RE 0.47260061217652005 Lambda1 0.04867341\n",
      "8 Train Loss 736.1299 Test MSE 729.9218400871626 Test RE 0.4548072193841012 Lambda1 0.007717854\n",
      "9 Train Loss 678.8609 Test MSE 671.9069981808476 Test RE 0.4363588141710658 Lambda1 0.0029049993\n",
      "10 Train Loss 649.88086 Test MSE 649.5661724589081 Test RE 0.4290430493485531 Lambda1 0.00117594\n",
      "11 Train Loss 647.23346 Test MSE 650.1029137543835 Test RE 0.4292202734559317 Lambda1 0.0014947816\n",
      "12 Train Loss 641.9924 Test MSE 646.2883433879663 Test RE 0.42795916561274244 Lambda1 0.0025491363\n",
      "13 Train Loss 635.55145 Test MSE 640.3597086975018 Test RE 0.4259917317914022 Lambda1 0.0023469494\n",
      "14 Train Loss 621.4969 Test MSE 624.9025898184155 Test RE 0.4208189931162262 Lambda1 0.001963732\n",
      "15 Train Loss 583.77313 Test MSE 553.4872147549277 Test RE 0.39604357060223294 Lambda1 -0.00023733712\n",
      "16 Train Loss 496.40854 Test MSE 442.5474022259957 Test RE 0.35413517717810794 Lambda1 0.00020905597\n",
      "17 Train Loss 370.18094 Test MSE 343.1067886693125 Test RE 0.3118198987236611 Lambda1 0.0008693766\n",
      "18 Train Loss 321.13174 Test MSE 328.2825102266499 Test RE 0.3050092732230573 Lambda1 0.0015062231\n",
      "19 Train Loss 275.58823 Test MSE 293.4550751317432 Test RE 0.288376577350627 Lambda1 0.0011121941\n",
      "20 Train Loss 268.51828 Test MSE 291.01328815522953 Test RE 0.2871743063016893 Lambda1 0.0018433619\n",
      "21 Train Loss 265.10745 Test MSE 291.05286637002996 Test RE 0.28719383369171936 Lambda1 0.0014230934\n",
      "22 Train Loss 263.66467 Test MSE 290.8092039006308 Test RE 0.28707359263075566 Lambda1 0.00070788094\n",
      "23 Train Loss 261.92157 Test MSE 289.3151803465764 Test RE 0.28633522709526155 Lambda1 0.00082509895\n",
      "24 Train Loss 260.76996 Test MSE 287.2329648905636 Test RE 0.285302982168585 Lambda1 0.0012647334\n",
      "25 Train Loss 257.69183 Test MSE 284.22301576080696 Test RE 0.2838041829998748 Lambda1 0.0008739233\n",
      "26 Train Loss 255.90489 Test MSE 283.02972980773245 Test RE 0.2832077926081654 Lambda1 0.0010843341\n",
      "27 Train Loss 255.54404 Test MSE 283.1761286598318 Test RE 0.2832810286170307 Lambda1 0.0013026872\n",
      "28 Train Loss 254.92963 Test MSE 283.02392787177945 Test RE 0.2832048898002233 Lambda1 0.0010715857\n",
      "29 Train Loss 254.82101 Test MSE 282.97586736159434 Test RE 0.2831808431596009 Lambda1 0.0011547232\n",
      "30 Train Loss 254.75299 Test MSE 283.0626942703252 Test RE 0.28322428472875066 Lambda1 0.0012861749\n",
      "31 Train Loss 254.57991 Test MSE 282.54811608732825 Test RE 0.28296673165569075 Lambda1 0.0014701379\n",
      "32 Train Loss 254.49089 Test MSE 282.13170068374006 Test RE 0.2827581385953439 Lambda1 0.0015377213\n",
      "33 Train Loss 254.37898 Test MSE 281.8485538448451 Test RE 0.2826162152124993 Lambda1 0.0016017294\n",
      "34 Train Loss 254.27264 Test MSE 281.59435933576793 Test RE 0.28248874303973 Lambda1 0.0015436774\n",
      "35 Train Loss 254.13617 Test MSE 281.36660868729655 Test RE 0.2823744829235034 Lambda1 0.0015254003\n",
      "36 Train Loss 254.00398 Test MSE 281.6514835808992 Test RE 0.2825173944266743 Lambda1 0.001440884\n",
      "37 Train Loss 253.9393 Test MSE 281.8622709438109 Test RE 0.28262309235870753 Lambda1 0.0013564207\n",
      "38 Train Loss 253.89835 Test MSE 281.98322044618703 Test RE 0.2826837238436977 Lambda1 0.0013052619\n",
      "39 Train Loss 253.73775 Test MSE 282.5246096853054 Test RE 0.28295496079663557 Lambda1 0.0011157665\n",
      "40 Train Loss 253.6276 Test MSE 282.6599578065395 Test RE 0.28302272982518883 Lambda1 0.0009846034\n",
      "41 Train Loss 253.4842 Test MSE 282.46240343012505 Test RE 0.28292380857683375 Lambda1 0.0009760049\n",
      "42 Train Loss 253.40463 Test MSE 282.2590338651196 Test RE 0.2828219393517683 Lambda1 0.0010742694\n",
      "43 Train Loss 253.32541 Test MSE 282.24947259046087 Test RE 0.2828171491399102 Lambda1 0.0011602823\n",
      "44 Train Loss 253.19885 Test MSE 282.2008907354227 Test RE 0.2827928083096112 Lambda1 0.0011956407\n",
      "45 Train Loss 253.16562 Test MSE 282.24163453687737 Test RE 0.28281322220368105 Lambda1 0.0011685584\n",
      "46 Train Loss 252.94438 Test MSE 282.8321191382068 Test RE 0.2831089078486068 Lambda1 0.0008522934\n",
      "47 Train Loss 252.49081 Test MSE 283.53809620427205 Test RE 0.28346202161327305 Lambda1 0.0006193365\n",
      "48 Train Loss 252.30707 Test MSE 283.1552012096131 Test RE 0.28327056082232904 Lambda1 0.0005850404\n",
      "49 Train Loss 252.12692 Test MSE 282.75588234120494 Test RE 0.2830707495746576 Lambda1 0.00061032746\n",
      "50 Train Loss 251.96594 Test MSE 283.2042446526958 Test RE 0.2832950914720569 Lambda1 0.0005965901\n",
      "51 Train Loss 251.67462 Test MSE 284.37991948501866 Test RE 0.28388250844655094 Lambda1 0.00050484843\n",
      "52 Train Loss 251.54012 Test MSE 284.42731845589714 Test RE 0.283906165493326 Lambda1 0.00048326195\n",
      "53 Train Loss 251.3704 Test MSE 284.66657279931565 Test RE 0.2840255483731985 Lambda1 0.00046628527\n",
      "54 Train Loss 251.07733 Test MSE 285.44490857175083 Test RE 0.28441357485984337 Lambda1 0.0003675246\n",
      "55 Train Loss 250.91408 Test MSE 285.76451054136476 Test RE 0.28457275392754733 Lambda1 0.00035402676\n",
      "56 Train Loss 250.8031 Test MSE 285.91978180185083 Test RE 0.2846500552859174 Lambda1 0.00034817107\n",
      "57 Train Loss 250.64421 Test MSE 286.51323622069475 Test RE 0.2849453116457394 Lambda1 0.0003416616\n",
      "58 Train Loss 250.5186 Test MSE 286.9243093603719 Test RE 0.28514965016331495 Lambda1 0.00033014576\n",
      "59 Train Loss 250.44568 Test MSE 287.2726122484354 Test RE 0.2853226719683429 Lambda1 0.0003077866\n",
      "60 Train Loss 250.3561 Test MSE 287.6607714047418 Test RE 0.2855153691085344 Lambda1 0.00026102265\n",
      "61 Train Loss 250.22362 Test MSE 288.38370223838547 Test RE 0.285873913554319 Lambda1 0.00019969179\n",
      "62 Train Loss 250.10594 Test MSE 289.01628456117595 Test RE 0.2861872802980376 Lambda1 0.00019116119\n",
      "63 Train Loss 250.06155 Test MSE 289.0908320104172 Test RE 0.28622418679316824 Lambda1 0.00019519051\n",
      "64 Train Loss 250.03363 Test MSE 289.1860461254828 Test RE 0.28627131789531696 Lambda1 0.00018629576\n",
      "65 Train Loss 249.83798 Test MSE 289.576232667369 Test RE 0.28646437970570365 Lambda1 0.00011190816\n",
      "66 Train Loss 249.39304 Test MSE 288.43273492941046 Test RE 0.2858982155023781 Lambda1 7.498464e-05\n",
      "67 Train Loss 249.25058 Test MSE 287.93764923839893 Test RE 0.2856527424862826 Lambda1 8.080102e-05\n",
      "68 Train Loss 249.11926 Test MSE 287.8301904236239 Test RE 0.28559943447029124 Lambda1 7.357495e-05\n",
      "69 Train Loss 248.97382 Test MSE 288.2948974616236 Test RE 0.2858298942085779 Lambda1 6.778457e-05\n",
      "70 Train Loss 248.90225 Test MSE 288.4114919219327 Test RE 0.28588768713818186 Lambda1 7.0974434e-05\n",
      "71 Train Loss 248.76895 Test MSE 288.2958743572476 Test RE 0.2858303784796122 Lambda1 6.9614536e-05\n",
      "72 Train Loss 248.60606 Test MSE 288.3051356772088 Test RE 0.2858349695014857 Lambda1 5.5896577e-05\n",
      "73 Train Loss 248.54997 Test MSE 288.44955156974413 Test RE 0.2859065498153796 Lambda1 5.7441877e-05\n",
      "74 Train Loss 248.50723 Test MSE 288.50364520508975 Test RE 0.2859333569287056 Lambda1 5.6128487e-05\n",
      "Training time: 145.30\n",
      "Training time: 145.30\n",
      "inv_HT_stan_tune1\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 843.20917 Test MSE 865.603424274563 Test RE 0.495277562384716 Lambda1 -0.027209045\n",
      "1 Train Loss 838.103 Test MSE 858.4188560904013 Test RE 0.49321786058119 Lambda1 -0.026983954\n",
      "2 Train Loss 838.07043 Test MSE 858.2369120068035 Test RE 0.49316558841441577 Lambda1 -0.026989432\n",
      "3 Train Loss 837.8734 Test MSE 857.8376477819903 Test RE 0.4930508611872884 Lambda1 -0.027579885\n",
      "4 Train Loss 837.8311 Test MSE 857.8531518197769 Test RE 0.49305531671839203 Lambda1 -0.028471991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 837.7335 Test MSE 857.5775539444929 Test RE 0.49297610973937067 Lambda1 -0.033132896\n",
      "6 Train Loss 836.7652 Test MSE 856.5101948638574 Test RE 0.492669229999296 Lambda1 -0.026973862\n",
      "7 Train Loss 812.0629 Test MSE 811.5123926932976 Test RE 0.4795531508473451 Lambda1 0.1371085\n",
      "8 Train Loss 749.1887 Test MSE 750.6452663290497 Test RE 0.4612183159827792 Lambda1 0.09251079\n",
      "9 Train Loss 670.67725 Test MSE 679.1242427765227 Test RE 0.43869611412312276 Lambda1 0.015897233\n",
      "10 Train Loss 622.5622 Test MSE 628.1447264488846 Test RE 0.42190923312121437 Lambda1 0.012690027\n",
      "11 Train Loss 552.3364 Test MSE 550.0230939084443 Test RE 0.39480226263026763 Lambda1 0.004028604\n",
      "12 Train Loss 499.13004 Test MSE 499.27306014750235 Test RE 0.3761475497239807 Lambda1 0.0034208412\n",
      "13 Train Loss 394.43268 Test MSE 412.42484801907585 Test RE 0.34187046457609593 Lambda1 0.005183869\n",
      "14 Train Loss 338.4836 Test MSE 334.8739105823507 Test RE 0.3080561103412802 Lambda1 0.0065930956\n",
      "15 Train Loss 315.46548 Test MSE 320.9788461505963 Test RE 0.30159724939404925 Lambda1 0.005001819\n",
      "16 Train Loss 290.90002 Test MSE 308.6804252868399 Test RE 0.2957629134081505 Lambda1 0.0014840801\n",
      "17 Train Loss 280.01456 Test MSE 301.20796741308084 Test RE 0.29216110494728126 Lambda1 0.0015156984\n",
      "18 Train Loss 267.3778 Test MSE 290.67269392692106 Test RE 0.2870062065094848 Lambda1 0.0028426654\n",
      "19 Train Loss 264.65323 Test MSE 287.1226061181697 Test RE 0.2852481682794971 Lambda1 0.0032747588\n",
      "20 Train Loss 259.48837 Test MSE 281.9176199152406 Test RE 0.28265084018310216 Lambda1 0.0038264631\n",
      "21 Train Loss 257.27188 Test MSE 280.9578427557026 Test RE 0.2821692932927631 Lambda1 0.0028739048\n",
      "22 Train Loss 256.68964 Test MSE 280.90071055085554 Test RE 0.28214060255871565 Lambda1 0.0018939252\n",
      "23 Train Loss 255.90308 Test MSE 280.926575725528 Test RE 0.2821535919313141 Lambda1 0.0016853766\n",
      "24 Train Loss 255.67407 Test MSE 280.87812523296975 Test RE 0.2821292598261235 Lambda1 0.001754832\n",
      "25 Train Loss 255.38918 Test MSE 280.775276081163 Test RE 0.2820776014568542 Lambda1 0.0017815238\n",
      "26 Train Loss 255.30884 Test MSE 280.88409996066497 Test RE 0.28213226048083256 Lambda1 0.0016889425\n",
      "27 Train Loss 255.19748 Test MSE 281.0995404171095 Test RE 0.28224043864442644 Lambda1 0.0017400325\n",
      "28 Train Loss 255.07506 Test MSE 281.2414015687095 Test RE 0.28231164812293097 Lambda1 0.0021378647\n",
      "29 Train Loss 254.81876 Test MSE 281.0107569901363 Test RE 0.28219586323837803 Lambda1 0.0028837086\n",
      "30 Train Loss 254.70856 Test MSE 280.7473712928775 Test RE 0.2820635839987636 Lambda1 0.0028057992\n",
      "31 Train Loss 254.61537 Test MSE 280.8543764783664 Test RE 0.2821173323038443 Lambda1 0.00266043\n",
      "32 Train Loss 254.33926 Test MSE 280.9504191220577 Test RE 0.2821655654464858 Lambda1 0.003282596\n",
      "33 Train Loss 254.1503 Test MSE 280.53256512960286 Test RE 0.28195565673672396 Lambda1 0.0041950024\n",
      "34 Train Loss 253.56938 Test MSE 279.9319808576235 Test RE 0.2816536795621479 Lambda1 0.0057487767\n",
      "35 Train Loss 253.27867 Test MSE 279.66116076143754 Test RE 0.28151740371767253 Lambda1 0.0070321904\n",
      "36 Train Loss 252.93115 Test MSE 279.0849196125226 Test RE 0.28122722119396415 Lambda1 0.008340953\n",
      "37 Train Loss 252.6739 Test MSE 279.1339864208697 Test RE 0.28125194183406294 Lambda1 0.00851375\n",
      "38 Train Loss 251.95325 Test MSE 277.7152232180683 Test RE 0.2805362671788556 Lambda1 0.01111218\n",
      "39 Train Loss 251.48361 Test MSE 275.5998474070269 Test RE 0.27946579286551154 Lambda1 0.015432769\n",
      "40 Train Loss 249.19705 Test MSE 269.95228463656963 Test RE 0.2765875798776979 Lambda1 0.028112425\n",
      "41 Train Loss 247.97029 Test MSE 266.84176090996806 Test RE 0.2749894735569241 Lambda1 0.03370639\n",
      "42 Train Loss 246.98662 Test MSE 266.31559021451886 Test RE 0.27471822141159336 Lambda1 0.032958355\n",
      "43 Train Loss 243.92737 Test MSE 260.3022267282281 Test RE 0.2715989660261206 Lambda1 0.042851184\n",
      "44 Train Loss 238.74626 Test MSE 256.676886929014 Test RE 0.26970099725470537 Lambda1 0.057125304\n",
      "45 Train Loss 234.80098 Test MSE 255.2775321997157 Test RE 0.2689648126176318 Lambda1 0.0695094\n",
      "46 Train Loss 228.75989 Test MSE 246.76699578221147 Test RE 0.2644433849246107 Lambda1 0.09726143\n",
      "47 Train Loss 226.18263 Test MSE 242.15325776932394 Test RE 0.26195960608482993 Lambda1 0.11062528\n",
      "48 Train Loss 220.8027 Test MSE 236.10074135446007 Test RE 0.25866510588111086 Lambda1 0.14303926\n",
      "49 Train Loss 218.8334 Test MSE 234.6827340139806 Test RE 0.25788717226430014 Lambda1 0.1467243\n",
      "50 Train Loss 216.5495 Test MSE 235.89645656780326 Test RE 0.25855317742891426 Lambda1 0.1474937\n",
      "51 Train Loss 213.2782 Test MSE 232.24119820724084 Test RE 0.2565421926932497 Lambda1 0.16810144\n",
      "52 Train Loss 211.45717 Test MSE 229.02657291878984 Test RE 0.2547605089776607 Lambda1 0.17811616\n",
      "53 Train Loss 209.52837 Test MSE 227.70938741602944 Test RE 0.25402685890780863 Lambda1 0.19175151\n",
      "54 Train Loss 207.05266 Test MSE 224.82860838236132 Test RE 0.252414882075423 Lambda1 0.2157518\n",
      "55 Train Loss 206.14026 Test MSE 223.64793826716505 Test RE 0.25175124101051627 Lambda1 0.21968597\n",
      "56 Train Loss 201.59596 Test MSE 215.0738317298431 Test RE 0.24687832200170728 Lambda1 0.24629019\n",
      "57 Train Loss 188.8705 Test MSE 195.23507982673542 Test RE 0.23521667027157 Lambda1 0.28508842\n",
      "58 Train Loss 177.4727 Test MSE 175.51538451411827 Test RE 0.22302151698880043 Lambda1 0.32100624\n",
      "59 Train Loss 171.46063 Test MSE 161.4992137820182 Test RE 0.2139313229458401 Lambda1 0.33893648\n",
      "60 Train Loss 164.967 Test MSE 158.16703653554362 Test RE 0.21171282121001886 Lambda1 0.34937087\n",
      "61 Train Loss 154.64929 Test MSE 148.1445622236114 Test RE 0.20489531494336735 Lambda1 0.37161714\n",
      "62 Train Loss 150.57483 Test MSE 141.33733385604793 Test RE 0.20013249883329637 Lambda1 0.3787447\n",
      "63 Train Loss 146.48247 Test MSE 141.26087987266922 Test RE 0.20007836240637183 Lambda1 0.38346043\n",
      "64 Train Loss 140.92879 Test MSE 136.5200034983331 Test RE 0.19669228064515362 Lambda1 0.39535275\n",
      "65 Train Loss 133.78839 Test MSE 127.75604922664803 Test RE 0.19027420110348103 Lambda1 0.40604582\n",
      "66 Train Loss 129.4165 Test MSE 117.89364124621419 Test RE 0.1827823940613795 Lambda1 0.42325768\n",
      "67 Train Loss 125.85943 Test MSE 114.59108570782304 Test RE 0.18020406677586326 Lambda1 0.43575627\n",
      "68 Train Loss 120.64332 Test MSE 111.22761577732398 Test RE 0.17753970120052592 Lambda1 0.46678427\n",
      "69 Train Loss 115.4158 Test MSE 106.46837182701009 Test RE 0.17369986341750696 Lambda1 0.49809754\n",
      "70 Train Loss 113.00564 Test MSE 101.90418268821182 Test RE 0.16993591555822746 Lambda1 0.5216681\n",
      "71 Train Loss 108.17842 Test MSE 97.70608939952953 Test RE 0.16639872195610142 Lambda1 0.53561175\n",
      "72 Train Loss 102.7262 Test MSE 91.55896970914719 Test RE 0.16107925753564853 Lambda1 0.5349437\n",
      "73 Train Loss 100.23501 Test MSE 90.82840831523596 Test RE 0.1604353338914241 Lambda1 0.5413283\n",
      "74 Train Loss 92.335045 Test MSE 81.65766247038418 Test RE 0.15212046436138432 Lambda1 0.5770574\n",
      "Training time: 145.26\n",
      "Training time: 145.26\n",
      "inv_HT_stan_tune1\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 838.2012 Test MSE 858.0434926679753 Test RE 0.4931100133536966 Lambda1 -0.08263892\n",
      "1 Train Loss 838.04364 Test MSE 858.2392017702231 Test RE 0.493166246293234 Lambda1 -0.08275955\n",
      "2 Train Loss 838.04126 Test MSE 858.2678301308813 Test RE 0.49317447151960314 Lambda1 -0.082779616\n",
      "3 Train Loss 838.029 Test MSE 858.3525938304841 Test RE 0.49319882420952216 Lambda1 -0.08297401\n",
      "4 Train Loss 837.90936 Test MSE 858.0831418021344 Test RE 0.4931214062256114 Lambda1 -0.09099229\n",
      "5 Train Loss 837.8776 Test MSE 857.9010661551647 Test RE 0.4930690860260473 Lambda1 -0.10202447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "7 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "8 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "9 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "10 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "11 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "12 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "13 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "14 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "15 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "16 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "17 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "18 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "19 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "20 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "21 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "22 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "23 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "24 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "25 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "26 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "27 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "28 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "29 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "30 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "31 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "32 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "33 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "34 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "35 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "36 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "37 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "38 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "39 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "40 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "41 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "42 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "43 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "44 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "45 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "46 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "47 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "48 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "49 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "50 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "51 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "52 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "53 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "54 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "55 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "56 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "57 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "58 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "59 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "60 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "61 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "62 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "63 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "64 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "65 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "66 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "67 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "68 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "69 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "70 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "71 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "72 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "73 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "74 Train Loss 837.87683 Test MSE 857.8968970825989 Test RE 0.4930678879605722 Lambda1 -0.10299014\n",
      "Training time: 131.54\n",
      "Training time: 131.54\n",
      "inv_HT_stan_tune1\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 884.97235 Test MSE 911.7491303837005 Test RE 0.5083078883613587 Lambda1 0.03193815\n",
      "1 Train Loss 837.979 Test MSE 858.3371837350663 Test RE 0.4931943969641758 Lambda1 0.03102192\n",
      "2 Train Loss 837.92487 Test MSE 858.0845103851058 Test RE 0.49312179947263907 Lambda1 0.030851167\n",
      "3 Train Loss 837.9149 Test MSE 857.9589295698082 Test RE 0.4930857139223357 Lambda1 0.030139057\n",
      "4 Train Loss 837.90704 Test MSE 857.9327158100233 Test RE 0.49307818108553825 Lambda1 0.028262457\n",
      "5 Train Loss 837.9068 Test MSE 857.9352956413491 Test RE 0.4930789224359716 Lambda1 0.028053325\n",
      "6 Train Loss 837.90643 Test MSE 857.9378719263644 Test RE 0.49307966276621107 Lambda1 0.0277814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 837.9058 Test MSE 857.943902454639 Test RE 0.4930813957154839 Lambda1 0.02697734\n",
      "8 Train Loss 837.90546 Test MSE 857.9472331268477 Test RE 0.49308235282404456 Lambda1 0.02630247\n",
      "9 Train Loss 837.90405 Test MSE 857.9578686980356 Test RE 0.49308540907040216 Lambda1 0.022691939\n",
      "10 Train Loss 837.87823 Test MSE 857.9643043339481 Test RE 0.4930872584107554 Lambda1 -0.07526823\n",
      "11 Train Loss 837.87195 Test MSE 857.8820982382398 Test RE 0.4930636351963851 Lambda1 -0.087714255\n",
      "12 Train Loss 837.8714 Test MSE 857.8657382388983 Test RE 0.493058933758261 Lambda1 -0.090308905\n",
      "13 Train Loss 837.87054 Test MSE 857.8425570518613 Test RE 0.4930522720115165 Lambda1 -0.0956629\n",
      "14 Train Loss 837.86865 Test MSE 857.8021536454529 Test RE 0.4930406607748551 Lambda1 -0.11118586\n",
      "15 Train Loss 837.8359 Test MSE 857.4990012170275 Test RE 0.49295353131212755 Lambda1 -0.35204056\n",
      "16 Train Loss 837.5131 Test MSE 857.7902267692756 Test RE 0.4930372331456945 Lambda1 -0.86414\n",
      "17 Train Loss 831.9329 Test MSE 846.4591538988624 Test RE 0.4897699931810043 Lambda1 -1.5420095\n",
      "18 Train Loss 821.02435 Test MSE 834.3010062002066 Test RE 0.48623985621577626 Lambda1 -1.8944558\n",
      "19 Train Loss 800.3529 Test MSE 807.772207863148 Test RE 0.4784467667266074 Lambda1 -1.9066116\n",
      "20 Train Loss 782.74854 Test MSE 788.8706853978634 Test RE 0.4728159074749444 Lambda1 -2.0102124\n",
      "21 Train Loss 771.39246 Test MSE 773.5068761051865 Test RE 0.46818905839573727 Lambda1 -2.116967\n",
      "22 Train Loss 735.2581 Test MSE 725.1274559569465 Test RE 0.4533110914360128 Lambda1 -2.3890736\n",
      "23 Train Loss 718.1792 Test MSE 703.7317820247363 Test RE 0.4465732995407801 Lambda1 -2.5470345\n",
      "24 Train Loss 692.38446 Test MSE 687.9079090937319 Test RE 0.4415240065088678 Lambda1 -2.4673462\n",
      "25 Train Loss 672.0761 Test MSE 658.3141336901994 Test RE 0.43192243244696693 Lambda1 -2.4620106\n",
      "26 Train Loss 662.1027 Test MSE 645.8491260006964 Test RE 0.4278137204295224 Lambda1 -2.5187514\n",
      "27 Train Loss 651.1226 Test MSE 639.2770829306313 Test RE 0.42563147745946267 Lambda1 -2.4924076\n",
      "28 Train Loss 640.5801 Test MSE 632.713158403708 Test RE 0.4234407048243033 Lambda1 -2.5052958\n",
      "29 Train Loss 630.8439 Test MSE 618.3260953129123 Test RE 0.41859878011001384 Lambda1 -2.4979265\n",
      "30 Train Loss 621.4284 Test MSE 606.1038987885266 Test RE 0.4144409966730692 Lambda1 -2.5165374\n",
      "31 Train Loss 619.1382 Test MSE 606.7292540174267 Test RE 0.4146547438772387 Lambda1 -2.5289667\n",
      "32 Train Loss 615.8549 Test MSE 605.8261150706863 Test RE 0.4143460144803489 Lambda1 -2.5613108\n",
      "33 Train Loss 614.46454 Test MSE 604.9242489298106 Test RE 0.4140374904606128 Lambda1 -2.5822763\n",
      "34 Train Loss 613.74304 Test MSE 603.6444209122512 Test RE 0.41359927249212797 Lambda1 -2.6009352\n",
      "35 Train Loss 612.8408 Test MSE 601.3326969684962 Test RE 0.4128065504322545 Lambda1 -2.6228988\n",
      "36 Train Loss 611.4008 Test MSE 599.8443823792387 Test RE 0.4122953802919158 Lambda1 -2.645853\n",
      "37 Train Loss 606.61646 Test MSE 598.850896718401 Test RE 0.4119538089579069 Lambda1 -2.6793146\n",
      "38 Train Loss 603.73157 Test MSE 596.1385196173073 Test RE 0.4110198183860359 Lambda1 -2.6931705\n",
      "39 Train Loss 600.935 Test MSE 593.5998163087048 Test RE 0.41014370434392283 Lambda1 -2.7161555\n",
      "40 Train Loss 599.6766 Test MSE 589.6910635030329 Test RE 0.4087911111040229 Lambda1 -2.734757\n",
      "41 Train Loss 598.43427 Test MSE 590.0999455672667 Test RE 0.4089328110545799 Lambda1 -2.7507746\n",
      "42 Train Loss 598.04395 Test MSE 589.6524594636617 Test RE 0.40877773015982366 Lambda1 -2.7536876\n",
      "43 Train Loss 594.46704 Test MSE 585.8179676930836 Test RE 0.40744642778480306 Lambda1 -2.7877295\n",
      "44 Train Loss 591.3343 Test MSE 581.8702608414752 Test RE 0.40607125826547996 Lambda1 -2.8127034\n",
      "45 Train Loss 588.8102 Test MSE 580.979789022696 Test RE 0.4057604213960168 Lambda1 -2.836275\n",
      "46 Train Loss 587.1992 Test MSE 579.6028091632725 Test RE 0.40527928987535744 Lambda1 -2.8765755\n",
      "47 Train Loss 586.43994 Test MSE 579.1417305119857 Test RE 0.40511805635174564 Lambda1 -2.9146738\n",
      "48 Train Loss 584.0274 Test MSE 575.2106862957506 Test RE 0.4037408039992673 Lambda1 -2.9980755\n",
      "49 Train Loss 581.53174 Test MSE 571.0063577643223 Test RE 0.4022625872557476 Lambda1 -3.06281\n",
      "50 Train Loss 577.28705 Test MSE 564.3344867291953 Test RE 0.39990558195379994 Lambda1 -3.173201\n",
      "51 Train Loss 572.04767 Test MSE 555.885159662779 Test RE 0.3969005590232009 Lambda1 -3.2533696\n",
      "52 Train Loss 568.77234 Test MSE 554.2938187770127 Test RE 0.39633204523038534 Lambda1 -3.2953212\n",
      "53 Train Loss 566.0099 Test MSE 555.1262349186669 Test RE 0.3966295313388931 Lambda1 -3.3340597\n",
      "54 Train Loss 562.16315 Test MSE 551.4365665013312 Test RE 0.39530922690490633 Lambda1 -3.4028971\n",
      "55 Train Loss 557.7535 Test MSE 545.7995248458384 Test RE 0.3932835190713569 Lambda1 -3.454133\n",
      "56 Train Loss 554.8888 Test MSE 543.4296806927359 Test RE 0.39242877799055054 Lambda1 -3.4936275\n",
      "57 Train Loss 553.12134 Test MSE 542.2403991056196 Test RE 0.3919991327567462 Lambda1 -3.4970434\n",
      "58 Train Loss 548.7695 Test MSE 533.2012097262741 Test RE 0.3887180734863781 Lambda1 -3.5575235\n",
      "59 Train Loss 544.1931 Test MSE 529.0530481146805 Test RE 0.387203060245641 Lambda1 -3.5587156\n",
      "60 Train Loss 540.965 Test MSE 524.727387408897 Test RE 0.38561688023178164 Lambda1 -3.563347\n",
      "61 Train Loss 534.09576 Test MSE 512.964684967333 Test RE 0.3812702368513151 Lambda1 -3.5994828\n",
      "62 Train Loss 531.9502 Test MSE 508.4505046039937 Test RE 0.37958890676523205 Lambda1 -3.6166801\n",
      "63 Train Loss 525.5399 Test MSE 492.2259674052961 Test RE 0.373483509634644 Lambda1 -3.6626\n",
      "64 Train Loss 521.548 Test MSE 494.971103676999 Test RE 0.37452351737521694 Lambda1 -3.6479158\n",
      "65 Train Loss 519.5177 Test MSE 497.7025535421212 Test RE 0.37555548142668344 Lambda1 -3.6304014\n",
      "66 Train Loss 517.0241 Test MSE 497.0644630931275 Test RE 0.3753146596529768 Lambda1 -3.6140032\n",
      "67 Train Loss 515.2613 Test MSE 498.7544442529453 Test RE 0.3759521388373114 Lambda1 -3.5969229\n",
      "68 Train Loss 512.05914 Test MSE 497.29781139956486 Test RE 0.37540274557599945 Lambda1 -3.5902996\n",
      "69 Train Loss 504.1058 Test MSE 483.94369727763024 Test RE 0.37032803409455456 Lambda1 -3.601511\n",
      "70 Train Loss 501.0378 Test MSE 481.6411966586493 Test RE 0.36944601293140455 Lambda1 -3.606281\n",
      "71 Train Loss 499.24356 Test MSE 479.70783343909176 Test RE 0.36870376790119325 Lambda1 -3.594526\n",
      "72 Train Loss 482.10843 Test MSE 464.7672805539249 Test RE 0.36291669213634065 Lambda1 -3.6086237\n",
      "73 Train Loss 470.34683 Test MSE 455.8193875342626 Test RE 0.3594062021046876 Lambda1 -3.6315157\n",
      "74 Train Loss 467.66528 Test MSE 451.6473513601707 Test RE 0.35775762969355834 Lambda1 -3.6341403\n",
      "Training time: 148.52\n",
      "Training time: 148.52\n",
      "inv_HT_stan_tune1\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 843.5109 Test MSE 865.9923391010078 Test RE 0.49538881376761784 Lambda1 -0.010901719\n",
      "1 Train Loss 838.0246 Test MSE 858.3396845993584 Test RE 0.4931951154530059 Lambda1 -0.010920738\n",
      "2 Train Loss 837.98584 Test MSE 858.1154651699031 Test RE 0.49313069390048125 Lambda1 -0.01092356\n",
      "3 Train Loss 837.88354 Test MSE 857.8651213136246 Test RE 0.4930587564691126 Lambda1 -0.011570084\n",
      "4 Train Loss 837.8711 Test MSE 857.8910956903939 Test RE 0.49306622081091395 Lambda1 -0.0124417\n",
      "5 Train Loss 837.8389 Test MSE 857.9082908775268 Test RE 0.4930711621860457 Lambda1 -0.020549288\n",
      "6 Train Loss 837.7318 Test MSE 857.6768185276633 Test RE 0.4930046399007799 Lambda1 -0.050860714\n",
      "7 Train Loss 837.67694 Test MSE 857.3300818778623 Test RE 0.492904975287072 Lambda1 -0.04933835\n",
      "8 Train Loss 835.8981 Test MSE 852.6133303234477 Test RE 0.4915472036949841 Lambda1 -0.0037866305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 816.1239 Test MSE 818.9681087920916 Test RE 0.4817510454655831 Lambda1 0.35130018\n",
      "10 Train Loss 763.9254 Test MSE 767.109624643768 Test RE 0.4662489711593774 Lambda1 0.4775688\n",
      "11 Train Loss 737.7619 Test MSE 727.9105940358045 Test RE 0.4541801932007683 Lambda1 0.46854433\n",
      "12 Train Loss 700.25397 Test MSE 696.9741874148148 Test RE 0.4444240141764211 Lambda1 0.39781642\n",
      "13 Train Loss 649.125 Test MSE 647.312542433346 Test RE 0.4282981333945714 Lambda1 0.34140214\n",
      "14 Train Loss 637.4287 Test MSE 638.5241365851434 Test RE 0.4253807473026764 Lambda1 0.28070715\n",
      "15 Train Loss 624.0507 Test MSE 630.4255678318574 Test RE 0.42267453132973043 Lambda1 0.22070932\n",
      "16 Train Loss 620.1824 Test MSE 626.6425110306092 Test RE 0.4214044307115551 Lambda1 0.22705717\n",
      "17 Train Loss 617.5079 Test MSE 624.0327038655529 Test RE 0.42052599383991196 Lambda1 0.23489253\n",
      "18 Train Loss 611.32446 Test MSE 617.2800771462194 Test RE 0.4182445598444413 Lambda1 0.24052672\n",
      "19 Train Loss 604.2047 Test MSE 611.8447675943559 Test RE 0.41639911309201344 Lambda1 0.26005372\n",
      "20 Train Loss 592.4832 Test MSE 596.4447974731672 Test RE 0.4111253895742469 Lambda1 0.26234788\n",
      "21 Train Loss 557.0576 Test MSE 551.1681736645897 Test RE 0.39521301359199007 Lambda1 0.30708084\n",
      "22 Train Loss 506.76242 Test MSE 487.208132487938 Test RE 0.3715749560540032 Lambda1 0.35158706\n",
      "23 Train Loss 473.4045 Test MSE 453.40632258643114 Test RE 0.35845360838830265 Lambda1 0.39081594\n",
      "24 Train Loss 417.2223 Test MSE 398.70761657224057 Test RE 0.3361370905529555 Lambda1 0.41846356\n",
      "25 Train Loss 344.8236 Test MSE 338.1245475547339 Test RE 0.30954765710919663 Lambda1 0.42280984\n",
      "26 Train Loss 298.48257 Test MSE 314.795701171872 Test RE 0.2986782287353387 Lambda1 0.38689438\n",
      "27 Train Loss 274.8144 Test MSE 291.7688285990618 Test RE 0.28754685141636827 Lambda1 0.3658258\n",
      "28 Train Loss 260.8685 Test MSE 278.62089298541343 Test RE 0.2809933296465249 Lambda1 0.32666376\n",
      "29 Train Loss 248.29947 Test MSE 267.6706951371757 Test RE 0.2754162647612341 Lambda1 0.29231802\n",
      "30 Train Loss 234.30034 Test MSE 255.98319786812942 Test RE 0.269336306825808 Lambda1 0.22275327\n",
      "31 Train Loss 228.69458 Test MSE 253.40347589615615 Test RE 0.2679757249599838 Lambda1 0.19153526\n",
      "32 Train Loss 224.6417 Test MSE 250.69560795318563 Test RE 0.2665400858991535 Lambda1 0.19464074\n",
      "33 Train Loss 219.17935 Test MSE 245.00068855541716 Test RE 0.2634952696883495 Lambda1 0.22303885\n",
      "34 Train Loss 217.64682 Test MSE 243.37558721618828 Test RE 0.26261992733404926 Lambda1 0.22432645\n",
      "35 Train Loss 214.72966 Test MSE 239.87752736987355 Test RE 0.26072576619746146 Lambda1 0.22350575\n",
      "36 Train Loss 214.11163 Test MSE 239.10466037266693 Test RE 0.26030540812198383 Lambda1 0.21950726\n",
      "37 Train Loss 213.0724 Test MSE 237.99745136100356 Test RE 0.2597020177061227 Lambda1 0.21873672\n",
      "38 Train Loss 211.90576 Test MSE 236.88156864465788 Test RE 0.25909247865871643 Lambda1 0.23692639\n",
      "39 Train Loss 210.00592 Test MSE 235.88727732422237 Test RE 0.2585481469470728 Lambda1 0.25592574\n",
      "40 Train Loss 209.15756 Test MSE 235.3659393517332 Test RE 0.25826227836072285 Lambda1 0.2581994\n",
      "41 Train Loss 207.94212 Test MSE 235.70185544976567 Test RE 0.2584465096156048 Lambda1 0.2673146\n",
      "42 Train Loss 207.69545 Test MSE 235.6346336852303 Test RE 0.2584096527371094 Lambda1 0.26951155\n",
      "43 Train Loss 206.9809 Test MSE 234.7437643174423 Test RE 0.2579207024456665 Lambda1 0.2779155\n",
      "44 Train Loss 206.3023 Test MSE 233.82399879394043 Test RE 0.257414918191189 Lambda1 0.28350064\n",
      "45 Train Loss 205.50397 Test MSE 232.88317026428203 Test RE 0.2568965210046846 Lambda1 0.28115183\n",
      "46 Train Loss 204.69412 Test MSE 230.44808001014545 Test RE 0.25554990138298644 Lambda1 0.2817001\n",
      "47 Train Loss 201.46576 Test MSE 224.6516850259726 Test RE 0.2523155466804174 Lambda1 0.28444323\n",
      "48 Train Loss 198.60466 Test MSE 220.77803172111442 Test RE 0.2501307580193586 Lambda1 0.27741677\n",
      "49 Train Loss 194.31374 Test MSE 212.48182765714174 Test RE 0.24538616157527057 Lambda1 0.2828491\n",
      "50 Train Loss 185.56721 Test MSE 199.8770337054562 Test RE 0.23799652637033844 Lambda1 0.2812251\n",
      "51 Train Loss 178.93524 Test MSE 189.68531934275907 Test RE 0.23184942900483702 Lambda1 0.3077269\n",
      "52 Train Loss 167.39737 Test MSE 175.0836316354415 Test RE 0.22274704113134655 Lambda1 0.337605\n",
      "53 Train Loss 159.94644 Test MSE 165.00375558090707 Test RE 0.21624002614096768 Lambda1 0.34607568\n",
      "54 Train Loss 155.98085 Test MSE 157.59576616628652 Test RE 0.21133014138652217 Lambda1 0.3650706\n",
      "55 Train Loss 154.2554 Test MSE 155.06278772417264 Test RE 0.20962494630205858 Lambda1 0.3889119\n",
      "56 Train Loss 146.76305 Test MSE 145.519479755237 Test RE 0.2030718556345238 Lambda1 0.43345952\n",
      "57 Train Loss 138.31984 Test MSE 134.57924644349762 Test RE 0.1952891956813127 Lambda1 0.4923464\n",
      "58 Train Loss 131.00595 Test MSE 124.59708751936273 Test RE 0.18790706795132456 Lambda1 0.53646076\n",
      "59 Train Loss 126.793655 Test MSE 125.5558616819135 Test RE 0.1886286545826703 Lambda1 0.54235786\n",
      "60 Train Loss 124.142395 Test MSE 123.92152854622326 Test RE 0.18739696436416 Lambda1 0.55255455\n",
      "61 Train Loss 120.564156 Test MSE 120.11292718977751 Test RE 0.1844947643510177 Lambda1 0.5519747\n",
      "62 Train Loss 117.88305 Test MSE 118.99646451548838 Test RE 0.18363531310148604 Lambda1 0.5500044\n",
      "63 Train Loss 116.50997 Test MSE 117.23791907906069 Test RE 0.18227336919611453 Lambda1 0.5618959\n",
      "64 Train Loss 113.76966 Test MSE 111.83561711025955 Test RE 0.1780242807830066 Lambda1 0.5718044\n",
      "65 Train Loss 111.04832 Test MSE 110.37207268929924 Test RE 0.17685558118615066 Lambda1 0.56991863\n",
      "66 Train Loss 109.48747 Test MSE 108.37925579626449 Test RE 0.17525170542263147 Lambda1 0.58780664\n",
      "67 Train Loss 105.88219 Test MSE 102.34811870586327 Test RE 0.17030566823935636 Lambda1 0.6087061\n",
      "68 Train Loss 104.04899 Test MSE 100.81585008982174 Test RE 0.16902602524560792 Lambda1 0.6061654\n",
      "69 Train Loss 102.497894 Test MSE 99.32996301098173 Test RE 0.1677757958198827 Lambda1 0.6194365\n",
      "70 Train Loss 101.15365 Test MSE 99.27973120025321 Test RE 0.16773336779757247 Lambda1 0.62812275\n",
      "71 Train Loss 100.23771 Test MSE 98.33788091253608 Test RE 0.16693584251967442 Lambda1 0.63026714\n",
      "72 Train Loss 99.03285 Test MSE 95.83791537909664 Test RE 0.16480024383024908 Lambda1 0.64337516\n",
      "73 Train Loss 97.0725 Test MSE 91.23850929400518 Test RE 0.16079711820634926 Lambda1 0.66749984\n",
      "74 Train Loss 94.4888 Test MSE 86.79827509275466 Test RE 0.15683562464795497 Lambda1 0.69418865\n",
      "Training time: 144.30\n",
      "Training time: 144.30\n",
      "inv_HT_stan_tune1\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 938.254 Test MSE 968.033604764192 Test RE 0.5237624839506757 Lambda1 -0.13805167\n",
      "1 Train Loss 837.982 Test MSE 858.3723895726258 Test RE 0.4932045113714929 Lambda1 -0.12870061\n",
      "2 Train Loss 837.9069 Test MSE 858.0361382447228 Test RE 0.4931079000881394 Lambda1 -0.12829113\n",
      "3 Train Loss 837.87897 Test MSE 857.8535589249983 Test RE 0.49305543371124205 Lambda1 -0.12725502\n",
      "4 Train Loss 837.805 Test MSE 857.6569598078262 Test RE 0.49299893233268227 Lambda1 -0.1195273\n",
      "5 Train Loss 837.0936 Test MSE 856.4162163727958 Test RE 0.492642200793326 Lambda1 -0.014778024\n",
      "6 Train Loss 836.36835 Test MSE 855.9050221698047 Test RE 0.49249514996467825 Lambda1 -0.032109268\n",
      "7 Train Loss 824.8585 Test MSE 836.7547283303983 Test RE 0.4869543594551916 Lambda1 -0.027603857\n",
      "8 Train Loss 751.33826 Test MSE 742.6602095990951 Test RE 0.4587586314954321 Lambda1 -0.008995749\n",
      "9 Train Loss 676.65454 Test MSE 684.7236708045974 Test RE 0.4405009419689563 Lambda1 -0.0006984867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Train Loss 664.85297 Test MSE 677.0992406580937 Test RE 0.43804157720739495 Lambda1 -0.00022734063\n",
      "11 Train Loss 657.65027 Test MSE 668.016024018709 Test RE 0.4350935154970509 Lambda1 -0.00017544141\n",
      "12 Train Loss 654.43915 Test MSE 664.675993122954 Test RE 0.434004434701918 Lambda1 -8.025963e-05\n",
      "13 Train Loss 647.9034 Test MSE 655.1898888258276 Test RE 0.43089629900651943 Lambda1 0.00014397039\n",
      "14 Train Loss 641.2626 Test MSE 644.4720315471347 Test RE 0.42735737988688094 Lambda1 -0.00027673485\n",
      "15 Train Loss 628.6633 Test MSE 637.807938055785 Test RE 0.4251421169005335 Lambda1 -0.00068303227\n",
      "16 Train Loss 617.9729 Test MSE 624.9182539378513 Test RE 0.42082426731237205 Lambda1 -2.6721991e-05\n",
      "17 Train Loss 594.38855 Test MSE 583.6412759922162 Test RE 0.40668876014852207 Lambda1 -0.0006024621\n",
      "18 Train Loss 516.3266 Test MSE 508.2756366133874 Test RE 0.3795236264114961 Lambda1 0.00017176401\n",
      "19 Train Loss 400.7157 Test MSE 358.1238401228764 Test RE 0.3185706686030594 Lambda1 0.00013584722\n",
      "20 Train Loss 360.53693 Test MSE 340.9281952603661 Test RE 0.3108283552689425 Lambda1 -8.505328e-05\n",
      "21 Train Loss 315.47687 Test MSE 306.2159410567644 Test RE 0.294579871511807 Lambda1 -4.9146015e-06\n",
      "22 Train Loss 283.29657 Test MSE 297.44608709846284 Test RE 0.29033092696385177 Lambda1 8.610869e-05\n",
      "23 Train Loss 267.90598 Test MSE 291.138607455474 Test RE 0.28723613270442544 Lambda1 0.00022925797\n",
      "24 Train Loss 263.26624 Test MSE 287.9705184523005 Test RE 0.2856690462124217 Lambda1 0.00016241951\n",
      "25 Train Loss 259.53235 Test MSE 285.64142352144705 Test RE 0.2845114604786431 Lambda1 4.454222e-05\n",
      "26 Train Loss 258.27393 Test MSE 284.7170265576582 Test RE 0.2840507173302237 Lambda1 0.00018213125\n",
      "27 Train Loss 257.83472 Test MSE 284.22283613961196 Test RE 0.28380409332160855 Lambda1 0.00014942532\n",
      "28 Train Loss 257.7065 Test MSE 283.9092896098478 Test RE 0.28364750783221715 Lambda1 7.80919e-05\n",
      "29 Train Loss 257.36877 Test MSE 283.1339011643512 Test RE 0.2832599062604235 Lambda1 1.3115201e-05\n",
      "30 Train Loss 256.78723 Test MSE 282.5412947287215 Test RE 0.282963315902602 Lambda1 0.00020463766\n",
      "31 Train Loss 256.76385 Test MSE 282.46020001327355 Test RE 0.2829227050666043 Lambda1 0.0002296847\n",
      "32 Train Loss 256.43262 Test MSE 282.3060299869112 Test RE 0.2828454832945628 Lambda1 0.00022018031\n",
      "33 Train Loss 255.99681 Test MSE 282.7510094356519 Test RE 0.28306831039822206 Lambda1 0.0002208569\n",
      "34 Train Loss 255.45416 Test MSE 283.27632117691525 Test RE 0.2833311390011805 Lambda1 0.0002592006\n",
      "35 Train Loss 255.16861 Test MSE 283.70920905960253 Test RE 0.28354754218496026 Lambda1 0.00026298666\n",
      "36 Train Loss 255.01958 Test MSE 283.59321777479937 Test RE 0.28348957366484223 Lambda1 0.00018750103\n",
      "37 Train Loss 254.91463 Test MSE 283.5963304265398 Test RE 0.28349112941765703 Lambda1 0.00016196682\n",
      "38 Train Loss 254.88066 Test MSE 283.55132467561947 Test RE 0.2834686339971923 Lambda1 0.00018002753\n",
      "39 Train Loss 254.86691 Test MSE 283.54772955508486 Test RE 0.2834668369554424 Lambda1 0.00019187806\n",
      "40 Train Loss 254.796 Test MSE 283.60192876487076 Test RE 0.2834939275346523 Lambda1 0.00020129271\n",
      "41 Train Loss 254.63757 Test MSE 283.61727148443237 Test RE 0.2835015958693156 Lambda1 0.00015130505\n",
      "42 Train Loss 254.563 Test MSE 283.24029334420914 Test RE 0.2833131210259777 Lambda1 0.00017106702\n",
      "43 Train Loss 254.5169 Test MSE 282.9554380449846 Test RE 0.28317062091772693 Lambda1 0.00019627783\n",
      "44 Train Loss 254.47765 Test MSE 282.98794704046173 Test RE 0.2831868873095374 Lambda1 0.00021624795\n",
      "45 Train Loss 254.45813 Test MSE 283.17271024357404 Test RE 0.28327931877058143 Lambda1 0.00022613881\n",
      "46 Train Loss 254.39117 Test MSE 283.330636172071 Test RE 0.28335830045219185 Lambda1 0.00021400356\n",
      "47 Train Loss 254.3481 Test MSE 283.09643734346133 Test RE 0.2832411653936223 Lambda1 0.0001842883\n",
      "48 Train Loss 254.3441 Test MSE 283.02797780401 Test RE 0.2832069160538278 Lambda1 0.00018382675\n",
      "49 Train Loss 254.31787 Test MSE 282.8897087152041 Test RE 0.283137729349556 Lambda1 0.00018441353\n",
      "50 Train Loss 254.2518 Test MSE 282.8087908140935 Test RE 0.2830972320308581 Lambda1 0.00020745637\n",
      "51 Train Loss 254.21915 Test MSE 282.8684422748791 Test RE 0.28312708660680463 Lambda1 0.00021630994\n",
      "52 Train Loss 254.17963 Test MSE 282.9767327976676 Test RE 0.2831812761907491 Lambda1 0.00019697535\n",
      "53 Train Loss 254.13144 Test MSE 282.96276018868934 Test RE 0.2831742847500607 Lambda1 0.00021764557\n",
      "54 Train Loss 254.10732 Test MSE 282.94807838483763 Test RE 0.2831669382652577 Lambda1 0.00021814385\n",
      "55 Train Loss 254.0967 Test MSE 282.96218898659293 Test RE 0.28317399893537154 Lambda1 0.0002033002\n",
      "56 Train Loss 254.04668 Test MSE 283.33617079185234 Test RE 0.283361068018766 Lambda1 0.00017424974\n",
      "57 Train Loss 254.03238 Test MSE 283.41817379585785 Test RE 0.2834020701571502 Lambda1 0.00016869165\n",
      "58 Train Loss 254.02322 Test MSE 283.4061072787624 Test RE 0.2833960371771942 Lambda1 0.00016867422\n",
      "59 Train Loss 253.99062 Test MSE 283.17158890357564 Test RE 0.2832787578889466 Lambda1 0.00015315325\n",
      "60 Train Loss 253.97194 Test MSE 282.9488390008427 Test RE 0.2831673188671721 Lambda1 0.00014436309\n",
      "61 Train Loss 253.96419 Test MSE 282.8781298030913 Test RE 0.28313193475835874 Lambda1 0.00013732647\n",
      "62 Train Loss 253.85806 Test MSE 282.90886291715935 Test RE 0.28314731468490734 Lambda1 0.000103595135\n",
      "63 Train Loss 253.7143 Test MSE 283.1526273236834 Test RE 0.283269273352128 Lambda1 6.655503e-05\n",
      "64 Train Loss 253.69347 Test MSE 283.1189983269053 Test RE 0.28325245142744454 Lambda1 6.412114e-05\n",
      "65 Train Loss 253.67845 Test MSE 283.1738502162295 Test RE 0.28327988897092304 Lambda1 6.540912e-05\n",
      "66 Train Loss 253.62274 Test MSE 283.3245712996988 Test RE 0.2833552677036904 Lambda1 6.9110145e-05\n",
      "67 Train Loss 253.51251 Test MSE 283.35611582769144 Test RE 0.2833710412374781 Lambda1 5.89192e-05\n",
      "68 Train Loss 253.4367 Test MSE 283.4467587995649 Test RE 0.2834163614865732 Lambda1 4.400996e-05\n",
      "69 Train Loss 253.42168 Test MSE 283.45871582308644 Test RE 0.28342233929414623 Lambda1 4.1774936e-05\n",
      "70 Train Loss 253.39886 Test MSE 283.3500880878987 Test RE 0.2833680271928009 Lambda1 3.784844e-05\n",
      "71 Train Loss 253.36772 Test MSE 283.29957878122553 Test RE 0.2833427698149433 Lambda1 3.6928657e-05\n",
      "72 Train Loss 253.33025 Test MSE 283.53508358754243 Test RE 0.28346051570505765 Lambda1 3.41558e-05\n",
      "73 Train Loss 253.2777 Test MSE 283.6830875511502 Test RE 0.28353448857271 Lambda1 3.1787415e-05\n",
      "74 Train Loss 253.25798 Test MSE 283.5882732738352 Test RE 0.28348710230711527 Lambda1 3.365162e-05\n",
      "Training time: 145.48\n",
      "Training time: 145.48\n",
      "inv_HT_stan_tune1\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 838.26587 Test MSE 858.174134284997 Test RE 0.4931475512163774 Lambda1 0.0075774994\n",
      "1 Train Loss 838.1483 Test MSE 858.3586226856213 Test RE 0.4932005562594194 Lambda1 0.0076169604\n",
      "2 Train Loss 838.0986 Test MSE 858.4569890895808 Test RE 0.49322881541243074 Lambda1 0.0076622926\n",
      "3 Train Loss 837.9337 Test MSE 858.1257330112551 Test RE 0.4931336441867713 Lambda1 0.0063725067\n",
      "4 Train Loss 837.92334 Test MSE 858.0268090224879 Test RE 0.4931052193585726 Lambda1 0.005530909\n",
      "5 Train Loss 837.9213 Test MSE 857.9800071848322 Test RE 0.49309177074315924 Lambda1 0.004568344\n",
      "6 Train Loss 837.91925 Test MSE 857.9459450972222 Test RE 0.4930819826935213 Lambda1 0.0027242252\n",
      "7 Train Loss 837.9029 Test MSE 857.78771121666 Test RE 0.4930365102055325 Lambda1 -0.017268978\n",
      "8 Train Loss 837.8174 Test MSE 857.7723045052846 Test RE 0.4930320824752099 Lambda1 -0.102346174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 837.81165 Test MSE 857.8122412511256 Test RE 0.49304355980298376 Lambda1 -0.10396534\n",
      "10 Train Loss 837.7023 Test MSE 857.8536535209561 Test RE 0.49305546089598074 Lambda1 -0.06005315\n",
      "11 Train Loss 836.85815 Test MSE 857.2516262281006 Test RE 0.4928824215089648 Lambda1 -0.18818843\n",
      "12 Train Loss 832.6893 Test MSE 850.5439207962254 Test RE 0.49095031504162445 Lambda1 0.046549015\n",
      "13 Train Loss 813.92114 Test MSE 828.5351012062688 Test RE 0.48455672651187337 Lambda1 0.10298402\n",
      "14 Train Loss 748.48737 Test MSE 737.483485829072 Test RE 0.45715694361767917 Lambda1 0.030062396\n",
      "15 Train Loss 701.17377 Test MSE 696.9595325417496 Test RE 0.4444193418285285 Lambda1 0.009067308\n",
      "16 Train Loss 620.6912 Test MSE 578.1911430547962 Test RE 0.40478544494093466 Lambda1 0.004725619\n",
      "17 Train Loss 386.33765 Test MSE 358.95773053326485 Test RE 0.318941348424568 Lambda1 0.00870103\n",
      "18 Train Loss 320.5339 Test MSE 321.624304983194 Test RE 0.3019003392144012 Lambda1 0.0043227207\n",
      "19 Train Loss 281.46503 Test MSE 298.08461821830247 Test RE 0.29064238835865214 Lambda1 -0.00041156082\n",
      "20 Train Loss 268.47296 Test MSE 286.81636763606576 Test RE 0.2850960080714775 Lambda1 -0.0010891244\n",
      "21 Train Loss 263.7345 Test MSE 283.06381310497954 Test RE 0.2832248444648763 Lambda1 -0.00049578625\n",
      "22 Train Loss 260.4555 Test MSE 282.1742761628407 Test RE 0.2827794727968025 Lambda1 -0.0002630693\n",
      "23 Train Loss 259.61536 Test MSE 281.3958539009607 Test RE 0.28238915752819355 Lambda1 9.391319e-05\n",
      "24 Train Loss 259.2143 Test MSE 281.00037460117784 Test RE 0.2821906501029297 Lambda1 0.00019420279\n",
      "25 Train Loss 258.91086 Test MSE 280.9427862682894 Test RE 0.2821617324870359 Lambda1 0.00019355406\n",
      "26 Train Loss 258.52252 Test MSE 280.82862979308095 Test RE 0.28210440077552823 Lambda1 7.6053766e-05\n",
      "27 Train Loss 258.06534 Test MSE 280.571599180375 Test RE 0.2819752720861852 Lambda1 7.950315e-05\n",
      "28 Train Loss 257.77188 Test MSE 280.5901238682461 Test RE 0.2819845806150966 Lambda1 0.000106879575\n",
      "29 Train Loss 257.39902 Test MSE 280.6899558414833 Test RE 0.2820347402094085 Lambda1 5.5718425e-05\n",
      "30 Train Loss 257.3271 Test MSE 280.6452410556185 Test RE 0.2820122748075652 Lambda1 2.8243165e-05\n",
      "31 Train Loss 257.1764 Test MSE 280.6636429831598 Test RE 0.2820215204380272 Lambda1 4.3042386e-05\n",
      "32 Train Loss 257.08414 Test MSE 280.6720573885505 Test RE 0.28202574796388474 Lambda1 6.829751e-05\n",
      "33 Train Loss 256.69373 Test MSE 280.5727460551018 Test RE 0.28197584839181206 Lambda1 0.00015426613\n",
      "34 Train Loss 256.525 Test MSE 280.3882631910446 Test RE 0.28188313043063856 Lambda1 0.00017444859\n",
      "35 Train Loss 256.40094 Test MSE 280.27156120550194 Test RE 0.2818244622390009 Lambda1 0.00021328698\n",
      "36 Train Loss 256.00247 Test MSE 280.5709261378467 Test RE 0.28197493388113454 Lambda1 0.00026657106\n",
      "37 Train Loss 255.7554 Test MSE 280.93837046349813 Test RE 0.2821595149963454 Lambda1 0.00025098867\n",
      "38 Train Loss 255.60458 Test MSE 280.8363274304739 Test RE 0.2821082670524553 Lambda1 0.00027010244\n",
      "39 Train Loss 255.5322 Test MSE 280.9555923099761 Test RE 0.2821681632158349 Lambda1 0.00025600387\n",
      "40 Train Loss 255.40344 Test MSE 281.2678365585552 Test RE 0.2823249156044452 Lambda1 0.00024690205\n",
      "41 Train Loss 255.3185 Test MSE 281.38849214189423 Test RE 0.28238546363130645 Lambda1 0.00023738839\n",
      "42 Train Loss 255.18384 Test MSE 281.7531194915729 Test RE 0.2825683640192392 Lambda1 0.00023740386\n",
      "43 Train Loss 255.03467 Test MSE 282.05439659807155 Test RE 0.28271939807660873 Lambda1 0.00023003518\n",
      "44 Train Loss 254.91925 Test MSE 282.44785680042116 Test RE 0.28291652328715156 Lambda1 0.00020578544\n",
      "45 Train Loss 254.89632 Test MSE 282.62501429663774 Test RE 0.28300523510589476 Lambda1 0.00019885428\n",
      "46 Train Loss 254.82213 Test MSE 283.07074461017686 Test RE 0.2832283121678683 Lambda1 0.00017890222\n",
      "47 Train Loss 254.7651 Test MSE 283.2704149678011 Test RE 0.28332818530976156 Lambda1 0.00017643187\n",
      "48 Train Loss 254.71965 Test MSE 283.2110464010016 Test RE 0.2832984934167229 Lambda1 0.0002023587\n",
      "49 Train Loss 254.69914 Test MSE 283.1566651991112 Test RE 0.28327129311435195 Lambda1 0.00022021009\n",
      "50 Train Loss 254.65443 Test MSE 283.1902773474711 Test RE 0.2832881054930785 Lambda1 0.0002717138\n",
      "51 Train Loss 254.62971 Test MSE 283.2849878373268 Test RE 0.2833354731367949 Lambda1 0.00031203454\n",
      "52 Train Loss 254.60953 Test MSE 283.5354395279402 Test RE 0.2834606936283957 Lambda1 0.000314594\n",
      "53 Train Loss 254.51817 Test MSE 283.70650225601685 Test RE 0.2835461895511541 Lambda1 0.00032791522\n",
      "54 Train Loss 254.4519 Test MSE 283.35595040725843 Test RE 0.2833709585228928 Lambda1 0.0003460865\n",
      "55 Train Loss 254.41559 Test MSE 283.0991813977468 Test RE 0.2832425381188632 Lambda1 0.0003898364\n",
      "56 Train Loss 254.3893 Test MSE 282.8311422134186 Test RE 0.28310841890776534 Lambda1 0.00040348398\n",
      "57 Train Loss 254.36407 Test MSE 282.8666886196191 Test RE 0.283126208976076 Lambda1 0.00037447724\n",
      "58 Train Loss 254.3168 Test MSE 283.0056688908789 Test RE 0.2831957543251483 Lambda1 0.00038069603\n",
      "59 Train Loss 254.28278 Test MSE 283.10801016067273 Test RE 0.2832469547012618 Lambda1 0.00038338528\n",
      "60 Train Loss 254.26263 Test MSE 283.10484633213906 Test RE 0.28324537200619687 Lambda1 0.00037267338\n",
      "61 Train Loss 254.24568 Test MSE 282.991974182144 Test RE 0.28318890228874827 Lambda1 0.0003888904\n",
      "62 Train Loss 254.24286 Test MSE 282.9587270751768 Test RE 0.28317226667866674 Lambda1 0.00039358632\n",
      "63 Train Loss 254.23314 Test MSE 282.944360653638 Test RE 0.2831650779557176 Lambda1 0.0003973276\n",
      "64 Train Loss 254.19968 Test MSE 283.0882674374057 Test RE 0.28323707832274697 Lambda1 0.00039267205\n",
      "65 Train Loss 254.1778 Test MSE 283.2970331211453 Test RE 0.2833414967879914 Lambda1 0.00038109857\n",
      "66 Train Loss 254.1731 Test MSE 283.25724989882525 Test RE 0.28332160135632894 Lambda1 0.00037778955\n",
      "67 Train Loss 254.1585 Test MSE 283.0220263882494 Test RE 0.2832039384489681 Lambda1 0.00038398398\n",
      "68 Train Loss 254.13057 Test MSE 282.66466467909004 Test RE 0.2830250862721061 Lambda1 0.00043268414\n",
      "69 Train Loss 254.1069 Test MSE 282.4956001033132 Test RE 0.28294043353852394 Lambda1 0.00047943165\n",
      "70 Train Loss 254.09789 Test MSE 282.46278250052114 Test RE 0.2829239984215758 Lambda1 0.00047802075\n",
      "71 Train Loss 254.09464 Test MSE 282.4747135633371 Test RE 0.2829299736306325 Lambda1 0.00047949294\n",
      "72 Train Loss 254.09193 Test MSE 282.466250560407 Test RE 0.2829257352775772 Lambda1 0.00048219823\n",
      "73 Train Loss 254.0843 Test MSE 282.28992188336724 Test RE 0.2828374137373422 Lambda1 0.0005123435\n",
      "74 Train Loss 254.06004 Test MSE 282.1399464974538 Test RE 0.2827622706264048 Lambda1 0.0005865989\n",
      "Training time: 145.30\n",
      "Training time: 145.30\n",
      "inv_HT_stan_tune1\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 837.94214 Test MSE 857.7790991938974 Test RE 0.49303403520370487 Lambda1 -0.009997248\n",
      "1 Train Loss 837.8799 Test MSE 857.9309149248389 Test RE 0.4930776635754582 Lambda1 -0.010097075\n",
      "2 Train Loss 837.8757 Test MSE 857.9647207368478 Test RE 0.49308737806779607 Lambda1 -0.0102874525\n",
      "3 Train Loss 837.8665 Test MSE 857.9632631917631 Test RE 0.49308695922925166 Lambda1 -0.010826273\n",
      "4 Train Loss 837.5706 Test MSE 856.2695411921713 Test RE 0.492600012499525 Lambda1 -0.0277948\n",
      "5 Train Loss 832.34033 Test MSE 843.3074932538797 Test RE 0.48885735119727985 Lambda1 -0.081270196\n",
      "6 Train Loss 773.56323 Test MSE 770.3695551730251 Test RE 0.46723861320954335 Lambda1 -0.022274062\n",
      "7 Train Loss 730.4029 Test MSE 724.405852947032 Test RE 0.45308548139599625 Lambda1 -0.0067831716\n",
      "8 Train Loss 657.98755 Test MSE 660.2020703038927 Test RE 0.43254133027333846 Lambda1 0.0016885268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 656.499 Test MSE 658.9454570115438 Test RE 0.4321294896452766 Lambda1 0.0011667098\n",
      "10 Train Loss 648.0737 Test MSE 654.528908706882 Test RE 0.4306788919915324 Lambda1 0.000890087\n",
      "11 Train Loss 645.5633 Test MSE 652.4228466632201 Test RE 0.42998544112252934 Lambda1 0.0003354786\n",
      "12 Train Loss 639.0076 Test MSE 648.282456964319 Test RE 0.42861888810102794 Lambda1 -0.00024689315\n",
      "13 Train Loss 619.12225 Test MSE 622.8194309453474 Test RE 0.42011699200912184 Lambda1 0.0008304526\n",
      "14 Train Loss 580.67615 Test MSE 564.5224676704105 Test RE 0.3999721810810006 Lambda1 0.00021043408\n",
      "15 Train Loss 375.20993 Test MSE 349.60862811610286 Test RE 0.31476051102665326 Lambda1 0.0042726807\n",
      "16 Train Loss 322.2478 Test MSE 299.3864016779265 Test RE 0.29127633799523767 Lambda1 0.0034239895\n",
      "17 Train Loss 270.21063 Test MSE 283.9542599731257 Test RE 0.2836699713916351 Lambda1 -1.3007186e-05\n",
      "18 Train Loss 262.02344 Test MSE 283.28171949759536 Test RE 0.2833338386709831 Lambda1 0.0006214804\n",
      "19 Train Loss 261.09985 Test MSE 282.5224898412555 Test RE 0.2829538992581101 Lambda1 0.0009249127\n",
      "20 Train Loss 258.5046 Test MSE 281.84867694609363 Test RE 0.28261627693076014 Lambda1 0.0013289527\n",
      "21 Train Loss 257.00183 Test MSE 282.54685350316043 Test RE 0.2829660994275955 Lambda1 0.0010171563\n",
      "22 Train Loss 256.31705 Test MSE 283.158494140095 Test RE 0.2832722079535639 Lambda1 0.0010386639\n",
      "23 Train Loss 256.1913 Test MSE 283.02238230551234 Test RE 0.28320411652192634 Lambda1 0.0012488478\n",
      "24 Train Loss 255.71465 Test MSE 282.4100767069277 Test RE 0.2828976012633281 Lambda1 0.0015572074\n",
      "25 Train Loss 255.42395 Test MSE 282.31714102200584 Test RE 0.2828510493732776 Lambda1 0.00142465\n",
      "26 Train Loss 254.88919 Test MSE 281.86421212113265 Test RE 0.2826240655655703 Lambda1 0.0014691972\n",
      "27 Train Loss 254.63141 Test MSE 281.5892116940806 Test RE 0.2824861610322654 Lambda1 0.0014367695\n",
      "28 Train Loss 254.5883 Test MSE 281.5012861748081 Test RE 0.2824420547942638 Lambda1 0.0013547386\n",
      "29 Train Loss 254.53867 Test MSE 281.4974257809153 Test RE 0.2824401181400191 Lambda1 0.0013511684\n",
      "30 Train Loss 254.46498 Test MSE 281.5534022306978 Test RE 0.28246819869734546 Lambda1 0.0013798865\n",
      "31 Train Loss 254.41699 Test MSE 281.5011747385252 Test RE 0.2824419988899085 Lambda1 0.0013836334\n",
      "32 Train Loss 254.38556 Test MSE 281.5098385441197 Test RE 0.28244634523720985 Lambda1 0.0013533683\n",
      "33 Train Loss 254.23085 Test MSE 281.76404460066266 Test RE 0.2825738423264227 Lambda1 0.0012093855\n",
      "34 Train Loss 254.01062 Test MSE 281.94742521113704 Test RE 0.28266578119543545 Lambda1 0.0013660413\n",
      "35 Train Loss 253.83601 Test MSE 281.8749825986489 Test RE 0.2826294652703473 Lambda1 0.0016705369\n",
      "36 Train Loss 253.7166 Test MSE 281.5116404219021 Test RE 0.2824472491718332 Lambda1 0.001935411\n",
      "37 Train Loss 253.58145 Test MSE 281.1760623149309 Test RE 0.2822788522688906 Lambda1 0.0023354976\n",
      "38 Train Loss 253.53712 Test MSE 281.08137939113664 Test RE 0.28223132112911087 Lambda1 0.0025158848\n",
      "39 Train Loss 253.52274 Test MSE 281.01350596249097 Test RE 0.2821972435177983 Lambda1 0.0025829084\n",
      "40 Train Loss 253.46085 Test MSE 280.6567562095377 Test RE 0.28201806037055716 Lambda1 0.0029440226\n",
      "41 Train Loss 253.29689 Test MSE 280.08403481246717 Test RE 0.2817301637524683 Lambda1 0.0036056675\n",
      "42 Train Loss 253.14662 Test MSE 279.7652284616958 Test RE 0.2815697780690522 Lambda1 0.0041314876\n",
      "43 Train Loss 252.98247 Test MSE 279.13434539914994 Test RE 0.2812521226850357 Lambda1 0.00491773\n",
      "44 Train Loss 252.74034 Test MSE 278.56647508289007 Test RE 0.28096588767302044 Lambda1 0.006006433\n",
      "45 Train Loss 252.45885 Test MSE 277.67052675539753 Test RE 0.2805136910247546 Lambda1 0.0076417304\n",
      "46 Train Loss 252.18987 Test MSE 277.0223720476142 Test RE 0.28018610405125327 Lambda1 0.008276717\n",
      "47 Train Loss 251.66295 Test MSE 276.4061716442803 Test RE 0.279874311711504 Lambda1 0.009562575\n",
      "48 Train Loss 250.95955 Test MSE 274.4075180630404 Test RE 0.27886061030074527 Lambda1 0.01417496\n",
      "49 Train Loss 250.14221 Test MSE 273.2001370465842 Test RE 0.27824644676957677 Lambda1 0.016688028\n",
      "50 Train Loss 249.72409 Test MSE 272.930155734789 Test RE 0.2781089287035978 Lambda1 0.017519275\n",
      "51 Train Loss 249.28688 Test MSE 272.17685331117735 Test RE 0.2777248654488426 Lambda1 0.019398808\n",
      "52 Train Loss 247.7405 Test MSE 269.37385311957763 Test RE 0.27629109643194283 Lambda1 0.02823623\n",
      "53 Train Loss 246.97449 Test MSE 268.72370550005405 Test RE 0.27595747363576134 Lambda1 0.02885093\n",
      "54 Train Loss 246.40587 Test MSE 268.1883258265537 Test RE 0.2756824408059258 Lambda1 0.029456673\n",
      "55 Train Loss 244.01414 Test MSE 264.38604636591464 Test RE 0.2737212003577861 Lambda1 0.037762348\n",
      "56 Train Loss 242.52475 Test MSE 263.7196517766877 Test RE 0.27337602061985394 Lambda1 0.043804735\n",
      "57 Train Loss 241.01419 Test MSE 261.6475775403111 Test RE 0.2722999300181278 Lambda1 0.04976124\n",
      "58 Train Loss 237.89583 Test MSE 258.25752402848286 Test RE 0.2705301430409867 Lambda1 0.060097087\n",
      "59 Train Loss 235.40297 Test MSE 256.11786403364755 Test RE 0.26940714295682977 Lambda1 0.07012872\n",
      "60 Train Loss 229.60597 Test MSE 251.14512670158223 Test RE 0.26677894350289716 Lambda1 0.09310827\n",
      "61 Train Loss 225.2754 Test MSE 244.64536742392755 Test RE 0.26330412857670754 Lambda1 0.12197919\n",
      "62 Train Loss 222.46983 Test MSE 243.26199267172413 Test RE 0.2625586318007145 Lambda1 0.1317652\n",
      "63 Train Loss 216.98016 Test MSE 238.45057362231742 Test RE 0.25994912288949806 Lambda1 0.15540253\n",
      "64 Train Loss 215.77444 Test MSE 238.25853104348616 Test RE 0.25984442329792035 Lambda1 0.16215856\n",
      "65 Train Loss 211.93439 Test MSE 234.73792342675003 Test RE 0.2579174936362529 Lambda1 0.1821022\n",
      "66 Train Loss 210.03096 Test MSE 233.17171359589534 Test RE 0.25705561973138424 Lambda1 0.18481106\n",
      "67 Train Loss 208.3446 Test MSE 231.65273696798175 Test RE 0.25621696855258425 Lambda1 0.19754876\n",
      "68 Train Loss 206.88635 Test MSE 228.98410937604854 Test RE 0.2547368904623285 Lambda1 0.21653812\n",
      "69 Train Loss 205.74232 Test MSE 228.62351015964174 Test RE 0.2545362343648133 Lambda1 0.22799282\n",
      "70 Train Loss 205.02051 Test MSE 227.7244498152108 Test RE 0.25403526038550556 Lambda1 0.22966301\n",
      "71 Train Loss 204.12892 Test MSE 227.29283436777308 Test RE 0.25379440450490676 Lambda1 0.24051088\n",
      "72 Train Loss 203.1373 Test MSE 227.35665218449682 Test RE 0.25383003138267934 Lambda1 0.2582117\n",
      "73 Train Loss 201.79845 Test MSE 224.56923056902164 Test RE 0.25226923843502136 Lambda1 0.28533983\n",
      "74 Train Loss 198.2084 Test MSE 215.87462752682669 Test RE 0.24733750254666903 Lambda1 0.32886398\n",
      "Training time: 144.46\n",
      "Training time: 144.46\n",
      "inv_HT_stan_tune2\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 847.1368 Test MSE 860.0021119696213 Test RE 0.49367249315240724 Lambda1 -0.00037467064\n",
      "1 Train Loss 837.9126 Test MSE 857.8850251941908 Test RE 0.49306447632263833 Lambda1 -0.002038477\n",
      "2 Train Loss 837.4254 Test MSE 857.0176287994888 Test RE 0.49281514774290036 Lambda1 -0.0010836384\n",
      "3 Train Loss 834.91943 Test MSE 854.0328650273229 Test RE 0.4919562273980493 Lambda1 0.017563649\n",
      "4 Train Loss 816.56866 Test MSE 833.1144805891536 Test RE 0.48589397306878174 Lambda1 0.07315347\n",
      "5 Train Loss 756.21106 Test MSE 755.0065932632288 Test RE 0.4625562385813498 Lambda1 0.15258735\n",
      "6 Train Loss 691.5905 Test MSE 684.9735430784481 Test RE 0.44058130937614426 Lambda1 0.052860826\n",
      "7 Train Loss 592.1984 Test MSE 567.9606319231907 Test RE 0.4011883262778989 Lambda1 0.010111071\n",
      "8 Train Loss 442.15454 Test MSE 462.1871812384823 Test RE 0.36190794615565225 Lambda1 0.00069491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 351.23834 Test MSE 372.9123932413057 Test RE 0.3250817425304091 Lambda1 -0.00022233443\n",
      "10 Train Loss 319.60284 Test MSE 343.3324121076432 Test RE 0.31192240662764403 Lambda1 -0.00014791093\n",
      "11 Train Loss 292.5054 Test MSE 318.6339424957132 Test RE 0.30049357405987753 Lambda1 -3.930572e-05\n",
      "12 Train Loss 288.65714 Test MSE 313.33555844583816 Test RE 0.2979847317589801 Lambda1 -7.840801e-05\n",
      "13 Train Loss 281.6043 Test MSE 301.79849729195945 Test RE 0.2924473612892423 Lambda1 -1.3106506e-05\n",
      "14 Train Loss 270.31235 Test MSE 293.8270589075289 Test RE 0.2885592926014213 Lambda1 0.00041940634\n",
      "15 Train Loss 267.663 Test MSE 289.78963469346405 Test RE 0.28656991464525855 Lambda1 0.0008360204\n",
      "16 Train Loss 265.36548 Test MSE 287.62481590837314 Test RE 0.2854975248825455 Lambda1 0.00027396192\n",
      "17 Train Loss 261.19904 Test MSE 285.94955544342724 Test RE 0.28466487561077497 Lambda1 -0.00026818033\n",
      "18 Train Loss 259.4365 Test MSE 285.1108116940184 Test RE 0.28424708124735953 Lambda1 0.00012999683\n",
      "19 Train Loss 258.36856 Test MSE 283.87334157834226 Test RE 0.28362954982113936 Lambda1 0.00043216237\n",
      "20 Train Loss 257.26205 Test MSE 283.7794752282894 Test RE 0.2835826530754711 Lambda1 9.146906e-05\n",
      "21 Train Loss 256.6769 Test MSE 283.704287106638 Test RE 0.28354508260015604 Lambda1 6.0584607e-05\n",
      "22 Train Loss 255.61467 Test MSE 283.518469125045 Test RE 0.2834522105368894 Lambda1 1.15491475e-05\n",
      "23 Train Loss 255.48315 Test MSE 283.38785794470346 Test RE 0.28338691268749566 Lambda1 -5.806615e-06\n",
      "24 Train Loss 255.2881 Test MSE 283.33873210109635 Test RE 0.2833623487830268 Lambda1 -3.4517994e-05\n",
      "25 Train Loss 255.03459 Test MSE 283.72184141215047 Test RE 0.283553854691802 Lambda1 -1.671573e-05\n",
      "26 Train Loss 254.6794 Test MSE 283.978913914264 Test RE 0.2836822857534315 Lambda1 6.398945e-06\n",
      "27 Train Loss 254.46037 Test MSE 283.7381262276929 Test RE 0.28356199216183137 Lambda1 1.7618e-05\n",
      "28 Train Loss 254.39874 Test MSE 283.6151115077576 Test RE 0.2835005163194048 Lambda1 1.796123e-05\n",
      "29 Train Loss 254.29701 Test MSE 283.49209666379465 Test RE 0.2834390270813178 Lambda1 2.3797002e-05\n",
      "30 Train Loss 254.22292 Test MSE 283.76024361709466 Test RE 0.2835730437763421 Lambda1 1.9294272e-05\n",
      "31 Train Loss 254.2111 Test MSE 283.9043653808215 Test RE 0.28364504797725837 Lambda1 1.5902862e-05\n",
      "32 Train Loss 254.16972 Test MSE 283.94197713851975 Test RE 0.2836638360566313 Lambda1 1.1009491e-05\n",
      "33 Train Loss 254.01128 Test MSE 283.7162093218894 Test RE 0.28355104030003536 Lambda1 1.29957425e-05\n",
      "34 Train Loss 253.92274 Test MSE 283.46338642409273 Test RE 0.2834246742853395 Lambda1 1.5194919e-05\n",
      "35 Train Loss 253.91704 Test MSE 283.41289874960273 Test RE 0.28339943277161306 Lambda1 1.7557222e-05\n",
      "36 Train Loss 253.85194 Test MSE 283.22448419269074 Test RE 0.2833052143074373 Lambda1 1.6891225e-05\n",
      "37 Train Loss 253.79619 Test MSE 283.1764746054858 Test RE 0.28328120165388104 Lambda1 1.2442124e-05\n",
      "38 Train Loss 253.76024 Test MSE 283.248673281052 Test RE 0.2833173120407392 Lambda1 9.11787e-06\n",
      "39 Train Loss 253.73065 Test MSE 283.45322084190263 Test RE 0.2834195921428287 Lambda1 4.5483907e-06\n",
      "40 Train Loss 253.63335 Test MSE 283.6247374385375 Test RE 0.2835053272992866 Lambda1 7.822757e-06\n",
      "41 Train Loss 253.55632 Test MSE 283.59692406580825 Test RE 0.28349142612702977 Lambda1 1.7303499e-05\n",
      "42 Train Loss 253.55461 Test MSE 283.5919125884943 Test RE 0.28348892130944076 Lambda1 1.8768078e-05\n",
      "43 Train Loss 253.5537 Test MSE 283.58065319787096 Test RE 0.2834832936027902 Lambda1 1.9867868e-05\n",
      "44 Train Loss 253.55087 Test MSE 283.5494638222018 Test RE 0.2834677038402932 Lambda1 2.2524642e-05\n",
      "45 Train Loss 253.45763 Test MSE 283.52280249664847 Test RE 0.28345437670805707 Lambda1 2.5455169e-05\n",
      "46 Train Loss 253.41005 Test MSE 283.63974066661757 Test RE 0.28351282565587704 Lambda1 2.526461e-05\n",
      "47 Train Loss 253.39508 Test MSE 283.7257244493278 Test RE 0.28355579505421263 Lambda1 2.6437714e-05\n",
      "48 Train Loss 253.23796 Test MSE 284.0220707600807 Test RE 0.28370384081784067 Lambda1 4.84491e-05\n",
      "49 Train Loss 253.16505 Test MSE 283.92673376419276 Test RE 0.2836562217332501 Lambda1 4.8296624e-05\n",
      "50 Train Loss 253.14531 Test MSE 283.82214250972663 Test RE 0.28360397111844327 Lambda1 5.0826595e-05\n",
      "51 Train Loss 252.97917 Test MSE 283.8656368870394 Test RE 0.28362570075778304 Lambda1 2.932934e-05\n",
      "52 Train Loss 252.86044 Test MSE 284.37448763091163 Test RE 0.2838797972569992 Lambda1 1.8994842e-05\n",
      "53 Train Loss 252.82916 Test MSE 284.547281273272 Test RE 0.28396603068709053 Lambda1 2.3260705e-05\n",
      "54 Train Loss 252.69186 Test MSE 284.59532656731653 Test RE 0.2839900032506691 Lambda1 2.7626154e-05\n",
      "55 Train Loss 252.18814 Test MSE 283.5266451390689 Test RE 0.28345629755906454 Lambda1 5.2700274e-05\n",
      "56 Train Loss 251.97702 Test MSE 283.08132867883273 Test RE 0.2832336070984425 Lambda1 2.556218e-05\n",
      "57 Train Loss 251.78044 Test MSE 282.9134952487554 Test RE 0.2831496327934132 Lambda1 9.664171e-06\n",
      "58 Train Loss 251.64635 Test MSE 282.75878069879946 Test RE 0.28307220036349284 Lambda1 -1.8475548e-07\n",
      "59 Train Loss 251.50478 Test MSE 282.8305296109111 Test RE 0.2831081123060597 Lambda1 -3.271227e-06\n",
      "60 Train Loss 251.21712 Test MSE 283.6093700670443 Test RE 0.28349764674444505 Lambda1 -1.4159122e-06\n",
      "61 Train Loss 250.98532 Test MSE 284.25453707960696 Test RE 0.28381991999796163 Lambda1 -4.2195165e-06\n",
      "62 Train Loss 250.78093 Test MSE 284.47085235687075 Test RE 0.2839278917291631 Lambda1 -6.4663054e-06\n",
      "63 Train Loss 250.66144 Test MSE 284.700486484804 Test RE 0.2840424665279251 Lambda1 -1.9108395e-06\n",
      "64 Train Loss 250.55887 Test MSE 284.77833284340596 Test RE 0.2840812970910106 Lambda1 2.8597965e-06\n",
      "65 Train Loss 250.3584 Test MSE 284.95139052944506 Test RE 0.2841676010327198 Lambda1 1.2469959e-05\n",
      "66 Train Loss 250.15094 Test MSE 285.00523505058146 Test RE 0.284194447972518 Lambda1 2.0631081e-05\n",
      "67 Train Loss 249.97096 Test MSE 285.16694104566085 Test RE 0.2842750595255581 Lambda1 1.4697709e-05\n",
      "68 Train Loss 249.83313 Test MSE 285.3082771084675 Test RE 0.28434549781058843 Lambda1 1.7030532e-05\n",
      "69 Train Loss 249.56715 Test MSE 286.5748129258397 Test RE 0.28497592986609666 Lambda1 2.1836384e-05\n",
      "70 Train Loss 249.18883 Test MSE 288.0669658017822 Test RE 0.28571688047673 Lambda1 1.2970342e-05\n",
      "71 Train Loss 249.01643 Test MSE 287.81099567674005 Test RE 0.2855899113201215 Lambda1 1.2892117e-05\n",
      "72 Train Loss 248.77417 Test MSE 287.45775093330406 Test RE 0.2854145981628752 Lambda1 6.03172e-06\n",
      "73 Train Loss 248.61122 Test MSE 287.71596176607187 Test RE 0.2855427571680311 Lambda1 5.014742e-06\n",
      "74 Train Loss 248.51344 Test MSE 287.56796509054885 Test RE 0.2854693083153619 Lambda1 3.996025e-06\n",
      "Training time: 151.52\n",
      "Training time: 151.52\n",
      "inv_HT_stan_tune2\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 837.89075 Test MSE 858.080912037839 Test RE 0.49312076552707884 Lambda1 -0.06038239\n",
      "1 Train Loss 837.4488 Test MSE 857.3575440869989 Test RE 0.4929128696508675 Lambda1 -0.05953002\n",
      "2 Train Loss 835.6028 Test MSE 854.2827870548036 Test RE 0.49202820454854185 Lambda1 -0.044518862\n",
      "3 Train Loss 796.9681 Test MSE 803.555908990822 Test RE 0.477196467564694 Lambda1 0.050497808\n",
      "4 Train Loss 682.5564 Test MSE 674.4731850129444 Test RE 0.43719130371361165 Lambda1 0.0024143844\n",
      "5 Train Loss 556.6518 Test MSE 555.5376570191828 Test RE 0.396776481636813 Lambda1 0.011106497\n",
      "6 Train Loss 487.55795 Test MSE 501.0069053458715 Test RE 0.376800114864996 Lambda1 0.0016473078\n",
      "7 Train Loss 407.82275 Test MSE 415.8547902830508 Test RE 0.3432891085603984 Lambda1 0.0038853628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 316.35132 Test MSE 325.3244031353433 Test RE 0.30363196589327685 Lambda1 0.005625974\n",
      "9 Train Loss 286.35556 Test MSE 303.9989878686798 Test RE 0.29351157945803896 Lambda1 0.004496908\n",
      "10 Train Loss 262.42477 Test MSE 285.4350486540245 Test RE 0.28440866267086873 Lambda1 9.707646e-05\n",
      "11 Train Loss 258.85602 Test MSE 282.40661464085554 Test RE 0.2828958672366951 Lambda1 0.00041539728\n",
      "12 Train Loss 258.27374 Test MSE 281.7520270801946 Test RE 0.28256781623258864 Lambda1 0.00061101146\n",
      "13 Train Loss 257.6483 Test MSE 281.7479791545024 Test RE 0.2825657864022388 Lambda1 0.0009187691\n",
      "14 Train Loss 256.387 Test MSE 281.3370551565573 Test RE 0.28235965283732795 Lambda1 0.0010729068\n",
      "15 Train Loss 255.41672 Test MSE 281.3740444836358 Test RE 0.2823782141147847 Lambda1 0.0007089705\n",
      "16 Train Loss 255.20947 Test MSE 281.3528050193884 Test RE 0.2823675562817578 Lambda1 0.00067219936\n",
      "17 Train Loss 255.13202 Test MSE 281.36045166725495 Test RE 0.28237139336914946 Lambda1 0.0006065385\n",
      "18 Train Loss 254.93195 Test MSE 281.8095938083808 Test RE 0.28259668146213784 Lambda1 0.00047297345\n",
      "19 Train Loss 254.71599 Test MSE 282.59361887853527 Test RE 0.2829895158418609 Lambda1 0.00043112738\n",
      "20 Train Loss 254.61401 Test MSE 282.57798979729324 Test RE 0.2829816902453283 Lambda1 0.00045434193\n",
      "21 Train Loss 254.54362 Test MSE 282.424515918849 Test RE 0.28290483324005666 Lambda1 0.0004653511\n",
      "22 Train Loss 254.48653 Test MSE 282.4005285932312 Test RE 0.28289281892460244 Lambda1 0.0004723504\n",
      "23 Train Loss 254.45616 Test MSE 282.5014708963972 Test RE 0.28294337354203153 Lambda1 0.0004797601\n",
      "24 Train Loss 254.43114 Test MSE 282.55866783867276 Test RE 0.2829720152986273 Lambda1 0.0004847537\n",
      "25 Train Loss 254.37836 Test MSE 282.75717413770184 Test RE 0.2830713961914177 Lambda1 0.00047476307\n",
      "26 Train Loss 254.3042 Test MSE 283.1629170665997 Test RE 0.2832744202962801 Lambda1 0.00047339202\n",
      "27 Train Loss 254.28221 Test MSE 283.25494809290535 Test RE 0.28332045018956376 Lambda1 0.0004801302\n",
      "28 Train Loss 254.27007 Test MSE 283.19352780372844 Test RE 0.2832897312779774 Lambda1 0.0005069985\n",
      "29 Train Loss 254.24286 Test MSE 283.0447028096315 Test RE 0.28321528372003196 Lambda1 0.00055633957\n",
      "30 Train Loss 254.22685 Test MSE 283.0422386697096 Test RE 0.2832140509048634 Lambda1 0.00056635204\n",
      "31 Train Loss 254.20723 Test MSE 283.12655565393754 Test RE 0.28325623184687226 Lambda1 0.00057717663\n",
      "32 Train Loss 254.17593 Test MSE 283.0322526753289 Test RE 0.28320905483275804 Lambda1 0.0006582118\n",
      "33 Train Loss 254.15126 Test MSE 282.84260691062246 Test RE 0.2831141568179932 Lambda1 0.0007086506\n",
      "34 Train Loss 254.0992 Test MSE 282.61711616883474 Test RE 0.2830012807016152 Lambda1 0.00082062755\n",
      "35 Train Loss 254.08466 Test MSE 282.5584123438969 Test RE 0.28297188736433226 Lambda1 0.00087375013\n",
      "36 Train Loss 254.05315 Test MSE 282.4177178090592 Test RE 0.2829014283839245 Lambda1 0.00095459406\n",
      "37 Train Loss 253.91367 Test MSE 282.1304712974469 Test RE 0.2827575225366774 Lambda1 0.0013445579\n",
      "38 Train Loss 253.84659 Test MSE 282.20210147355414 Test RE 0.28279341494779053 Lambda1 0.0014138455\n",
      "39 Train Loss 253.83342 Test MSE 282.25664055298625 Test RE 0.28282074030668436 Lambda1 0.0014718018\n",
      "40 Train Loss 253.79071 Test MSE 282.0567794955311 Test RE 0.2827205923319025 Lambda1 0.0017697702\n",
      "41 Train Loss 253.60857 Test MSE 280.8718484964416 Test RE 0.2821261074608002 Lambda1 0.002692382\n",
      "42 Train Loss 253.35399 Test MSE 279.84967553269826 Test RE 0.2816122707496395 Lambda1 0.003415906\n",
      "43 Train Loss 253.20006 Test MSE 279.16751099908316 Test RE 0.2812688308015322 Lambda1 0.004337082\n",
      "44 Train Loss 252.89279 Test MSE 278.4085819352156 Test RE 0.28088624981206095 Lambda1 0.0060642813\n",
      "45 Train Loss 252.5935 Test MSE 278.52852149645264 Test RE 0.28094674677254794 Lambda1 0.0072102672\n",
      "46 Train Loss 252.28667 Test MSE 277.98012045532397 Test RE 0.2806700293271379 Lambda1 0.008771957\n",
      "47 Train Loss 251.93213 Test MSE 277.47927700212375 Test RE 0.2804170703719347 Lambda1 0.010814327\n",
      "48 Train Loss 251.06216 Test MSE 275.7882462720794 Test RE 0.2795612973510198 Lambda1 0.014378478\n",
      "49 Train Loss 250.62158 Test MSE 274.9677113761513 Test RE 0.2791451072494193 Lambda1 0.015493733\n",
      "50 Train Loss 249.50932 Test MSE 273.3170417044627 Test RE 0.2783059724107 Lambda1 0.020405283\n",
      "51 Train Loss 248.29445 Test MSE 271.30848956499 Test RE 0.27728147934595326 Lambda1 0.026615683\n",
      "52 Train Loss 247.42833 Test MSE 269.9246261582278 Test RE 0.2765734103596354 Lambda1 0.030386414\n",
      "53 Train Loss 246.86932 Test MSE 268.9361424423106 Test RE 0.27606652985074426 Lambda1 0.03399977\n",
      "54 Train Loss 244.9011 Test MSE 267.04622320748666 Test RE 0.2750948060452982 Lambda1 0.04081457\n",
      "55 Train Loss 243.58575 Test MSE 264.37529060603055 Test RE 0.2737156325347295 Lambda1 0.047689658\n",
      "56 Train Loss 240.62267 Test MSE 259.64518843527156 Test RE 0.27125597304790816 Lambda1 0.06561635\n",
      "57 Train Loss 235.75966 Test MSE 254.24303005786516 Test RE 0.26841927471058696 Lambda1 0.08383093\n",
      "58 Train Loss 232.428 Test MSE 251.7453613482477 Test RE 0.26709755291382653 Lambda1 0.09984791\n",
      "59 Train Loss 228.06955 Test MSE 248.01190971145402 Test RE 0.2651095904877747 Lambda1 0.12298106\n",
      "60 Train Loss 225.33386 Test MSE 245.3191515250732 Test RE 0.2636664656054762 Lambda1 0.1373038\n",
      "61 Train Loss 221.93645 Test MSE 239.88762633078872 Test RE 0.260731254480667 Lambda1 0.165924\n",
      "62 Train Loss 217.88974 Test MSE 237.71198032846792 Test RE 0.2595462184581042 Lambda1 0.19185463\n",
      "63 Train Loss 212.65327 Test MSE 232.2700354066688 Test RE 0.25655811951586804 Lambda1 0.22342807\n",
      "64 Train Loss 206.99452 Test MSE 225.4376503048451 Test RE 0.2527565362706973 Lambda1 0.25704157\n",
      "65 Train Loss 199.8859 Test MSE 214.75332345522796 Test RE 0.24669430136690723 Lambda1 0.26649433\n",
      "66 Train Loss 188.634 Test MSE 198.80388602087294 Test RE 0.23735676010923432 Lambda1 0.27672625\n",
      "67 Train Loss 182.6937 Test MSE 192.6214151255486 Test RE 0.23363691076915055 Lambda1 0.28944567\n",
      "68 Train Loss 174.57726 Test MSE 183.3291381740471 Test RE 0.22793179940541713 Lambda1 0.32055914\n",
      "69 Train Loss 167.7045 Test MSE 170.3830408792805 Test RE 0.21973657609471034 Lambda1 0.36897433\n",
      "70 Train Loss 160.95464 Test MSE 163.6659961448892 Test RE 0.21536166477225527 Lambda1 0.41142413\n",
      "71 Train Loss 156.53764 Test MSE 157.24549530159746 Test RE 0.21109516055701938 Lambda1 0.44261345\n",
      "72 Train Loss 150.11249 Test MSE 148.64513342755123 Test RE 0.20524118724232604 Lambda1 0.48596662\n",
      "73 Train Loss 146.8654 Test MSE 145.2306332801981 Test RE 0.20287021346930217 Lambda1 0.49918288\n",
      "74 Train Loss 142.2352 Test MSE 138.8344637818357 Test RE 0.1983525619494705 Lambda1 0.5193302\n",
      "Training time: 154.31\n",
      "Training time: 154.31\n",
      "inv_HT_stan_tune2\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 841.8296 Test MSE 857.1112994527165 Test RE 0.4928420789599258 Lambda1 -0.09522656\n",
      "1 Train Loss 835.0202 Test MSE 853.6450348582541 Test RE 0.49184451206459534 Lambda1 -0.09501724\n",
      "2 Train Loss 802.0994 Test MSE 808.5936874188902 Test RE 0.4786899877475772 Lambda1 -0.05400168\n",
      "3 Train Loss 733.3778 Test MSE 726.0036744066814 Test RE 0.4535848913180092 Lambda1 0.023091406\n",
      "4 Train Loss 626.40424 Test MSE 627.016812557016 Test RE 0.42153026703132773 Lambda1 0.001145985\n",
      "5 Train Loss 605.84753 Test MSE 606.150105574698 Test RE 0.4144567939823983 Lambda1 0.0012099397\n",
      "6 Train Loss 550.87555 Test MSE 552.9929745054268 Test RE 0.39586670618292286 Lambda1 0.00067131646\n",
      "7 Train Loss 486.072 Test MSE 471.3895049150584 Test RE 0.36549305176476965 Lambda1 -0.0014566447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 342.178 Test MSE 345.6551796818177 Test RE 0.31297576182911 Lambda1 0.00016335599\n",
      "9 Train Loss 288.29343 Test MSE 303.4722658979204 Test RE 0.29325719370665165 Lambda1 8.727256e-05\n",
      "10 Train Loss 271.87097 Test MSE 288.46658568532655 Test RE 0.2859149916612591 Lambda1 -0.0011626824\n",
      "11 Train Loss 264.55075 Test MSE 284.53521256099657 Test RE 0.2839600085936007 Lambda1 0.0004148285\n",
      "12 Train Loss 262.29205 Test MSE 282.9063324093798 Test RE 0.2831460483617575 Lambda1 0.0007770552\n",
      "13 Train Loss 259.77127 Test MSE 281.18185981440286 Test RE 0.28228176237270225 Lambda1 0.0007083414\n",
      "14 Train Loss 257.2847 Test MSE 281.3671103116149 Test RE 0.28237473463397655 Lambda1 0.0007046238\n",
      "15 Train Loss 255.7022 Test MSE 281.73727406752363 Test RE 0.2825604182713596 Lambda1 0.00075248966\n",
      "16 Train Loss 255.10109 Test MSE 281.832172015397 Test RE 0.28260800186911067 Lambda1 0.00091332645\n",
      "17 Train Loss 254.73296 Test MSE 281.84502108369657 Test RE 0.2826144440153438 Lambda1 0.0009499578\n",
      "18 Train Loss 254.67697 Test MSE 281.9001925059649 Test RE 0.2826421036805665 Lambda1 0.0009265193\n",
      "19 Train Loss 254.48077 Test MSE 282.1050251018961 Test RE 0.28274477087354943 Lambda1 0.0008429402\n",
      "20 Train Loss 254.38136 Test MSE 282.2908950929932 Test RE 0.2828379012854887 Lambda1 0.0008249862\n",
      "21 Train Loss 254.31082 Test MSE 282.47538558304484 Test RE 0.28293031018182807 Lambda1 0.00083688047\n",
      "22 Train Loss 254.23438 Test MSE 282.4639498160085 Test RE 0.28292458303173385 Lambda1 0.00092556496\n",
      "23 Train Loss 254.07257 Test MSE 282.1524599283313 Test RE 0.28276854107293364 Lambda1 0.0011788284\n",
      "24 Train Loss 254.01585 Test MSE 282.1586085470117 Test RE 0.28277162207825673 Lambda1 0.0012136669\n",
      "25 Train Loss 253.98935 Test MSE 282.16044862462434 Test RE 0.28277254411442226 Lambda1 0.0012658144\n",
      "26 Train Loss 253.94945 Test MSE 282.0757878819509 Test RE 0.2827301187324703 Lambda1 0.0013579298\n",
      "27 Train Loss 253.90138 Test MSE 281.9852570625754 Test RE 0.2826847446797323 Lambda1 0.0013491685\n",
      "28 Train Loss 253.86943 Test MSE 281.88117139888084 Test RE 0.282632567936081 Lambda1 0.001381976\n",
      "29 Train Loss 253.81282 Test MSE 281.5410646474275 Test RE 0.282462009794861 Lambda1 0.001591641\n",
      "30 Train Loss 253.69398 Test MSE 281.0347468463983 Test RE 0.2822079084956316 Lambda1 0.0020158554\n",
      "31 Train Loss 253.57713 Test MSE 280.72067531794073 Test RE 0.2820501731138336 Lambda1 0.0022456709\n",
      "32 Train Loss 253.41153 Test MSE 280.900310108324 Test RE 0.28214040145360425 Lambda1 0.0023668495\n",
      "33 Train Loss 253.33037 Test MSE 280.7105670940394 Test RE 0.2820450950198109 Lambda1 0.0026241285\n",
      "34 Train Loss 253.21735 Test MSE 280.47799432516666 Test RE 0.28192823158674357 Lambda1 0.0031186326\n",
      "35 Train Loss 253.09558 Test MSE 280.4490299604536 Test RE 0.2819136741470928 Lambda1 0.0034663163\n",
      "36 Train Loss 252.94661 Test MSE 280.37561209141006 Test RE 0.2818767710849324 Lambda1 0.0038332418\n",
      "37 Train Loss 252.7768 Test MSE 279.77735844760394 Test RE 0.2815758821164321 Lambda1 0.004538915\n",
      "38 Train Loss 252.57323 Test MSE 278.93541743100303 Test RE 0.28115188621123544 Lambda1 0.005636124\n",
      "39 Train Loss 252.39693 Test MSE 277.983247647803 Test RE 0.28067160804926894 Lambda1 0.006736394\n",
      "40 Train Loss 252.1064 Test MSE 276.8227179479035 Test RE 0.2800851187286129 Lambda1 0.008319933\n",
      "41 Train Loss 251.57523 Test MSE 276.08208174289837 Test RE 0.27971018541653 Lambda1 0.009991366\n",
      "42 Train Loss 251.07242 Test MSE 275.064890272954 Test RE 0.2791944305264243 Lambda1 0.011487556\n",
      "43 Train Loss 249.89389 Test MSE 272.37642025270986 Test RE 0.2778266642339638 Lambda1 0.01698233\n",
      "44 Train Loss 248.7676 Test MSE 270.87981552915113 Test RE 0.27706233699642496 Lambda1 0.020149536\n",
      "45 Train Loss 246.64003 Test MSE 267.7932976940716 Test RE 0.2754793326898152 Lambda1 0.027064336\n",
      "46 Train Loss 245.04413 Test MSE 266.37253183049955 Test RE 0.27474758894389073 Lambda1 0.03191965\n",
      "47 Train Loss 243.19957 Test MSE 263.5740195488395 Test RE 0.27330052784465775 Lambda1 0.036205377\n",
      "48 Train Loss 241.65254 Test MSE 260.8791239200891 Test RE 0.2718997663023692 Lambda1 0.04129917\n",
      "49 Train Loss 240.24475 Test MSE 260.2020337938523 Test RE 0.2715466904140423 Lambda1 0.043568935\n",
      "50 Train Loss 236.82947 Test MSE 253.33231994920698 Test RE 0.26793809839394617 Lambda1 0.056943733\n",
      "51 Train Loss 230.75247 Test MSE 247.29436756480484 Test RE 0.2647258083247517 Lambda1 0.076930106\n",
      "52 Train Loss 225.2351 Test MSE 241.6593454434573 Test RE 0.26169231436522217 Lambda1 0.08848212\n",
      "53 Train Loss 218.65144 Test MSE 230.4949436071218 Test RE 0.2555758841961045 Lambda1 0.11232583\n",
      "54 Train Loss 211.81766 Test MSE 220.6019079170142 Test RE 0.2500309682673847 Lambda1 0.13066664\n",
      "55 Train Loss 206.6589 Test MSE 214.473214595598 Test RE 0.2465333636786987 Lambda1 0.14418885\n",
      "56 Train Loss 202.68271 Test MSE 207.16621867919767 Test RE 0.24229733655146152 Lambda1 0.16667911\n",
      "57 Train Loss 197.86697 Test MSE 198.47964537867662 Test RE 0.23716312175690926 Lambda1 0.1869207\n",
      "58 Train Loss 191.92836 Test MSE 192.8163454155917 Test RE 0.2337550995862507 Lambda1 0.20654702\n",
      "59 Train Loss 183.93077 Test MSE 184.49444386334426 Test RE 0.22865505999279453 Lambda1 0.24023016\n",
      "60 Train Loss 176.03406 Test MSE 175.664148527479 Test RE 0.22311601169309053 Lambda1 0.27545214\n",
      "61 Train Loss 168.26036 Test MSE 165.18690053492406 Test RE 0.21636000003563743 Lambda1 0.30379996\n",
      "62 Train Loss 153.75792 Test MSE 148.82002477869352 Test RE 0.20536189202327368 Lambda1 0.33457908\n",
      "63 Train Loss 147.07542 Test MSE 141.36772071902456 Test RE 0.2001540114507358 Lambda1 0.35222152\n",
      "64 Train Loss 139.68611 Test MSE 134.37656048532796 Test RE 0.1951420805298359 Lambda1 0.37946686\n",
      "65 Train Loss 127.54623 Test MSE 124.33658824898133 Test RE 0.1877105333973322 Lambda1 0.43861353\n",
      "66 Train Loss 120.76275 Test MSE 118.04741995419714 Test RE 0.18290156452786793 Lambda1 0.4677159\n",
      "67 Train Loss 116.40065 Test MSE 113.15096110118674 Test RE 0.17906812831345353 Lambda1 0.47789338\n",
      "68 Train Loss 112.549385 Test MSE 105.86633552214248 Test RE 0.17320806537391842 Lambda1 0.49925265\n",
      "69 Train Loss 106.76645 Test MSE 99.91214777267335 Test RE 0.16826675445527542 Lambda1 0.518548\n",
      "70 Train Loss 102.55239 Test MSE 99.4239979265193 Test RE 0.16785519306386412 Lambda1 0.5202386\n",
      "71 Train Loss 99.118484 Test MSE 96.52946098491742 Test RE 0.16539375649352492 Lambda1 0.5378711\n",
      "72 Train Loss 97.55683 Test MSE 94.35023323159298 Test RE 0.16351615248013193 Lambda1 0.5503302\n",
      "73 Train Loss 95.41243 Test MSE 91.50421686863524 Test RE 0.16103108712778902 Lambda1 0.56550676\n",
      "74 Train Loss 91.302795 Test MSE 89.0557846892288 Test RE 0.1588620776009848 Lambda1 0.5883993\n",
      "Training time: 147.68\n",
      "Training time: 147.68\n",
      "inv_HT_stan_tune2\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 912.2902 Test MSE 940.6327828739874 Test RE 0.5162965535177964 Lambda1 -0.01542003\n",
      "1 Train Loss 837.9137 Test MSE 858.2950048547307 Test RE 0.49318227897406025 Lambda1 -0.014761022\n",
      "2 Train Loss 837.78357 Test MSE 857.7720530782491 Test RE 0.4930320102173371 Lambda1 -0.0141287325\n",
      "3 Train Loss 837.38727 Test MSE 856.6762597885754 Test RE 0.49271698839947387 Lambda1 -0.0027688483\n",
      "4 Train Loss 834.62 Test MSE 855.4191873177623 Test RE 0.4923553533365631 Lambda1 0.045951206\n",
      "5 Train Loss 798.5449 Test MSE 815.5805972188039 Test RE 0.48075367555827825 Lambda1 0.18471207\n",
      "6 Train Loss 732.9154 Test MSE 755.3515528934 Test RE 0.4626618966038246 Lambda1 0.12054385\n",
      "7 Train Loss 670.81305 Test MSE 674.9003252755035 Test RE 0.4373297172607214 Lambda1 0.07760174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 628.04803 Test MSE 647.7077982762685 Test RE 0.4284288751107988 Lambda1 0.062165964\n",
      "9 Train Loss 584.95074 Test MSE 602.6518734135353 Test RE 0.4132591005324267 Lambda1 0.031125437\n",
      "10 Train Loss 512.081 Test MSE 520.6939194875515 Test RE 0.3841319436218367 Lambda1 0.0011541166\n",
      "11 Train Loss 374.14966 Test MSE 355.3449326194353 Test RE 0.31733226672201104 Lambda1 -0.0015521946\n",
      "12 Train Loss 301.10248 Test MSE 306.1402759058957 Test RE 0.2945434743088578 Lambda1 -0.00453938\n",
      "13 Train Loss 279.87466 Test MSE 302.21761055776017 Test RE 0.29265035442201975 Lambda1 -0.0017598924\n",
      "14 Train Loss 273.72675 Test MSE 298.25299233081705 Test RE 0.2907244619422095 Lambda1 -0.00076219643\n",
      "15 Train Loss 270.57172 Test MSE 291.89903207888625 Test RE 0.28761100395944555 Lambda1 0.00026656874\n",
      "16 Train Loss 262.7961 Test MSE 282.2257848395274 Test RE 0.2828052811943434 Lambda1 -4.9342987e-05\n",
      "17 Train Loss 261.58984 Test MSE 282.38700592004784 Test RE 0.28288604572033166 Lambda1 1.7532539e-05\n",
      "18 Train Loss 260.48892 Test MSE 282.4982289711943 Test RE 0.2829417500391832 Lambda1 8.8490204e-05\n",
      "19 Train Loss 259.58948 Test MSE 282.407109793396 Test RE 0.28289611524176766 Lambda1 6.9915324e-05\n",
      "20 Train Loss 259.05957 Test MSE 282.16251112085837 Test RE 0.2827735775977549 Lambda1 0.000119191754\n",
      "21 Train Loss 258.40826 Test MSE 281.4258167885538 Test RE 0.2824041914550324 Lambda1 0.00019878833\n",
      "22 Train Loss 256.71912 Test MSE 280.5861532267407 Test RE 0.28198258542078025 Lambda1 0.00033521623\n",
      "23 Train Loss 256.3535 Test MSE 280.62676271883765 Test RE 0.2820029904820027 Lambda1 0.0003780383\n",
      "24 Train Loss 255.75757 Test MSE 280.8091229035788 Test RE 0.28209460285189203 Lambda1 0.00037246748\n",
      "25 Train Loss 255.43027 Test MSE 280.80943562724104 Test RE 0.28209475992946814 Lambda1 0.0004112252\n",
      "26 Train Loss 255.20294 Test MSE 280.90060303574836 Test RE 0.28214054856387183 Lambda1 0.00043087237\n",
      "27 Train Loss 254.95604 Test MSE 281.3377340260168 Test RE 0.28235999350562196 Lambda1 0.00042117017\n",
      "28 Train Loss 254.74843 Test MSE 281.6450084382216 Test RE 0.2825141468830633 Lambda1 0.00049386686\n",
      "29 Train Loss 254.58455 Test MSE 281.8923742729548 Test RE 0.2826381842487864 Lambda1 0.0005579698\n",
      "30 Train Loss 254.54913 Test MSE 282.02474411495103 Test RE 0.28270453648851135 Lambda1 0.0006154364\n",
      "31 Train Loss 254.50436 Test MSE 281.9989985228598 Test RE 0.2826916323693754 Lambda1 0.00074574107\n",
      "32 Train Loss 254.41272 Test MSE 281.9513352330419 Test RE 0.2826677411806413 Lambda1 0.0010198152\n",
      "33 Train Loss 254.2948 Test MSE 282.0578393192222 Test RE 0.282721123490382 Lambda1 0.0011547811\n",
      "34 Train Loss 254.26227 Test MSE 282.05983772359025 Test RE 0.2827221250405051 Lambda1 0.0011148162\n",
      "35 Train Loss 254.2277 Test MSE 281.92520163557333 Test RE 0.28265464087710435 Lambda1 0.001268989\n",
      "36 Train Loss 254.14229 Test MSE 281.6510936973071 Test RE 0.2825171988854804 Lambda1 0.0016879976\n",
      "37 Train Loss 254.11284 Test MSE 281.6249573855604 Test RE 0.28250409023960804 Lambda1 0.0018108812\n",
      "38 Train Loss 254.06003 Test MSE 281.6024893018816 Test RE 0.2824928209043177 Lambda1 0.0021115574\n",
      "39 Train Loss 253.82654 Test MSE 280.9373419344902 Test RE 0.2821589984960341 Lambda1 0.0031522545\n",
      "40 Train Loss 253.60843 Test MSE 280.0060914058659 Test RE 0.2816909602743897 Lambda1 0.003946685\n",
      "41 Train Loss 253.3165 Test MSE 278.9921844752665 Test RE 0.2811804938182811 Lambda1 0.00516857\n",
      "42 Train Loss 253.11069 Test MSE 278.4223427824945 Test RE 0.2808931913815326 Lambda1 0.0059428453\n",
      "43 Train Loss 252.84084 Test MSE 277.07166191549624 Test RE 0.2802110293338115 Lambda1 0.007327513\n",
      "44 Train Loss 252.15959 Test MSE 275.58750803030534 Test RE 0.2794595365625214 Lambda1 0.009583586\n",
      "45 Train Loss 251.43697 Test MSE 274.8384664953756 Test RE 0.2790794953337048 Lambda1 0.01215109\n",
      "46 Train Loss 250.88475 Test MSE 274.49180832281695 Test RE 0.2789034360744164 Lambda1 0.013922915\n",
      "47 Train Loss 249.60312 Test MSE 272.47873635831525 Test RE 0.27787884105836963 Lambda1 0.018062813\n",
      "48 Train Loss 248.13501 Test MSE 269.3234180410217 Test RE 0.2762652301214106 Lambda1 0.023025224\n",
      "49 Train Loss 246.36505 Test MSE 267.1264591615691 Test RE 0.27513613004578447 Lambda1 0.029806698\n",
      "50 Train Loss 245.77698 Test MSE 266.72088813011015 Test RE 0.274927184755269 Lambda1 0.031755485\n",
      "51 Train Loss 244.46745 Test MSE 264.3694943006584 Test RE 0.273712631974269 Lambda1 0.035570454\n",
      "52 Train Loss 242.51685 Test MSE 262.72676112924654 Test RE 0.27286091213888825 Lambda1 0.04119432\n",
      "53 Train Loss 241.74478 Test MSE 261.26107857014443 Test RE 0.2720987385184 Lambda1 0.045133177\n",
      "54 Train Loss 241.0777 Test MSE 260.603403739285 Test RE 0.27175604443159107 Lambda1 0.048000854\n",
      "55 Train Loss 240.10603 Test MSE 260.4498639371521 Test RE 0.27167597733281074 Lambda1 0.049552355\n",
      "56 Train Loss 238.27722 Test MSE 258.4619228143733 Test RE 0.2706371778626479 Lambda1 0.056783386\n",
      "57 Train Loss 236.59962 Test MSE 257.11071309665346 Test RE 0.26992882055904477 Lambda1 0.0618629\n",
      "58 Train Loss 235.1155 Test MSE 254.91767865591106 Test RE 0.2687751718105532 Lambda1 0.06572388\n",
      "59 Train Loss 232.7739 Test MSE 251.9856297856376 Test RE 0.2672249828817055 Lambda1 0.07818912\n",
      "60 Train Loss 229.03389 Test MSE 246.6174919882331 Test RE 0.26436326627028633 Lambda1 0.09334479\n",
      "61 Train Loss 227.94443 Test MSE 245.4435887159456 Test RE 0.26373332902499036 Lambda1 0.096245795\n",
      "62 Train Loss 226.76234 Test MSE 244.11840069865497 Test RE 0.26302039683319756 Lambda1 0.10218362\n",
      "63 Train Loss 221.1003 Test MSE 239.77743755488413 Test RE 0.2606713661110524 Lambda1 0.13262346\n",
      "64 Train Loss 218.19868 Test MSE 236.78914418636336 Test RE 0.25904192846523727 Lambda1 0.14320225\n",
      "65 Train Loss 217.4883 Test MSE 237.04147463343796 Test RE 0.259179913594488 Lambda1 0.14587773\n",
      "66 Train Loss 215.62877 Test MSE 236.16522773455358 Test RE 0.2587004281706843 Lambda1 0.16252308\n",
      "67 Train Loss 214.30997 Test MSE 234.72315711631387 Test RE 0.2579093812920309 Lambda1 0.17931512\n",
      "68 Train Loss 213.19301 Test MSE 235.09705779314632 Test RE 0.25811471706097333 Lambda1 0.1841124\n",
      "69 Train Loss 212.09311 Test MSE 234.62081279546888 Test RE 0.25785314815620186 Lambda1 0.18178846\n",
      "70 Train Loss 211.3668 Test MSE 234.67966600182623 Test RE 0.25788548657666704 Lambda1 0.18949394\n",
      "71 Train Loss 210.19833 Test MSE 234.0035094908913 Test RE 0.2575137101590838 Lambda1 0.2052249\n",
      "72 Train Loss 209.72975 Test MSE 233.67342489538072 Test RE 0.25733202205995076 Lambda1 0.20575325\n",
      "73 Train Loss 209.39632 Test MSE 234.185855040519 Test RE 0.257614023472879 Lambda1 0.20778239\n",
      "74 Train Loss 208.85234 Test MSE 234.02627486725208 Test RE 0.25752623615488307 Lambda1 0.21325403\n",
      "Training time: 140.77\n",
      "Training time: 140.77\n",
      "inv_HT_stan_tune2\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 838.72284 Test MSE 858.0872466466728 Test RE 0.4931225857058923 Lambda1 -0.118240826\n",
      "1 Train Loss 837.8048 Test MSE 857.4269632542909 Test RE 0.4929328245160131 Lambda1 -0.11744875\n",
      "2 Train Loss 837.6384 Test MSE 857.7958691230817 Test RE 0.49303885468779624 Lambda1 -0.116242185\n",
      "3 Train Loss 836.91064 Test MSE 856.8427236638065 Test RE 0.49276485688792004 Lambda1 -0.09343518\n",
      "4 Train Loss 830.52515 Test MSE 848.393165729912 Test RE 0.49032919353663895 Lambda1 -0.017620662\n",
      "5 Train Loss 793.4736 Test MSE 797.7222749071814 Test RE 0.47546114312084115 Lambda1 0.20826168\n",
      "6 Train Loss 700.4097 Test MSE 701.4584444820831 Test RE 0.44585141011766666 Lambda1 0.18568152\n",
      "7 Train Loss 530.73047 Test MSE 517.9997977396478 Test RE 0.38313688652070144 Lambda1 0.20433293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 397.24536 Test MSE 395.3560336348489 Test RE 0.3347213050767156 Lambda1 0.17054364\n",
      "9 Train Loss 333.19342 Test MSE 343.86503738805175 Test RE 0.31216426170647626 Lambda1 0.1419864\n",
      "10 Train Loss 284.7386 Test MSE 296.4544999942876 Test RE 0.2898465892172585 Lambda1 0.14199685\n",
      "11 Train Loss 271.05698 Test MSE 285.19998441188983 Test RE 0.2842915290587563 Lambda1 0.1316558\n",
      "12 Train Loss 248.52945 Test MSE 265.5052532992814 Test RE 0.2742999509020694 Lambda1 0.13395515\n",
      "13 Train Loss 237.98402 Test MSE 257.2310610173768 Test RE 0.2699919870669124 Lambda1 0.1316059\n",
      "14 Train Loss 225.96872 Test MSE 241.35877224517372 Test RE 0.26152951873783975 Lambda1 0.14210063\n",
      "15 Train Loss 209.29434 Test MSE 226.90775070438474 Test RE 0.2535793218137237 Lambda1 0.16269791\n",
      "16 Train Loss 202.48001 Test MSE 220.50945203212905 Test RE 0.2499785678660129 Lambda1 0.1808855\n",
      "17 Train Loss 197.00922 Test MSE 210.93717137711323 Test RE 0.24449260597894726 Lambda1 0.20784974\n",
      "18 Train Loss 190.50188 Test MSE 200.68392945397576 Test RE 0.23847643383919373 Lambda1 0.24134208\n",
      "19 Train Loss 184.30878 Test MSE 193.0198262718895 Test RE 0.23387840901180706 Lambda1 0.264492\n",
      "20 Train Loss 178.24287 Test MSE 185.91921716825277 Test RE 0.2295362655915037 Lambda1 0.2904789\n",
      "21 Train Loss 168.9607 Test MSE 173.84763722651738 Test RE 0.22195941264441682 Lambda1 0.34103757\n",
      "22 Train Loss 162.99551 Test MSE 162.50229294745964 Test RE 0.2145946632539316 Lambda1 0.38472328\n",
      "23 Train Loss 158.7872 Test MSE 159.89597013598555 Test RE 0.2128667991625158 Lambda1 0.4033445\n",
      "24 Train Loss 153.82689 Test MSE 155.67254431278917 Test RE 0.21003669815750048 Lambda1 0.42625916\n",
      "25 Train Loss 150.89827 Test MSE 152.86889373161125 Test RE 0.2081367323194359 Lambda1 0.43939373\n",
      "26 Train Loss 146.64941 Test MSE 149.78410927176176 Test RE 0.2060260049414501 Lambda1 0.45542666\n",
      "27 Train Loss 144.45592 Test MSE 146.56133191938554 Test RE 0.20379750939181943 Lambda1 0.47438848\n",
      "28 Train Loss 142.63863 Test MSE 144.17482425757686 Test RE 0.2021314474587756 Lambda1 0.48853952\n",
      "29 Train Loss 139.3698 Test MSE 143.84998013467026 Test RE 0.20190360518263364 Lambda1 0.4948825\n",
      "30 Train Loss 135.88295 Test MSE 142.807596190552 Test RE 0.20117074539630195 Lambda1 0.5101684\n",
      "31 Train Loss 131.71005 Test MSE 137.64226412971814 Test RE 0.1974990789348887 Lambda1 0.5348815\n",
      "32 Train Loss 129.15616 Test MSE 135.9810936023681 Test RE 0.19630367740611576 Lambda1 0.5370209\n",
      "33 Train Loss 127.4898 Test MSE 134.02758553019783 Test RE 0.194888524516564 Lambda1 0.5415916\n",
      "34 Train Loss 124.442604 Test MSE 130.79072718878336 Test RE 0.1925207956448254 Lambda1 0.55654836\n",
      "35 Train Loss 123.13897 Test MSE 128.44765474641045 Test RE 0.19078852928011197 Lambda1 0.56540066\n",
      "36 Train Loss 119.25656 Test MSE 120.90947205095526 Test RE 0.18510550426453165 Lambda1 0.6013231\n",
      "37 Train Loss 117.99689 Test MSE 118.67708113973339 Test RE 0.18338871135094528 Lambda1 0.6118971\n",
      "38 Train Loss 114.9291 Test MSE 115.31668933605373 Test RE 0.18077370264703646 Lambda1 0.63057107\n",
      "39 Train Loss 112.06167 Test MSE 113.76971996712648 Test RE 0.17955707214564343 Lambda1 0.62664896\n",
      "40 Train Loss 110.58494 Test MSE 110.93209698202857 Test RE 0.1773036932008819 Lambda1 0.6308688\n",
      "41 Train Loss 108.72833 Test MSE 109.386167352516 Test RE 0.1760639226503517 Lambda1 0.6362252\n",
      "42 Train Loss 107.123276 Test MSE 108.03472905429248 Test RE 0.1749729299333481 Lambda1 0.6445543\n",
      "43 Train Loss 105.07798 Test MSE 106.38658644583977 Test RE 0.17363313543761083 Lambda1 0.65258175\n",
      "44 Train Loss 102.89451 Test MSE 105.78670023146763 Test RE 0.17314290741086066 Lambda1 0.65965575\n",
      "45 Train Loss 100.613556 Test MSE 103.29395954437544 Test RE 0.17109079068001745 Lambda1 0.6767843\n",
      "46 Train Loss 97.37047 Test MSE 97.35666032125692 Test RE 0.16610090719568116 Lambda1 0.7042563\n",
      "47 Train Loss 95.96873 Test MSE 94.68061819245025 Test RE 0.16380219346414066 Lambda1 0.7132829\n",
      "48 Train Loss 94.37083 Test MSE 94.76710578536242 Test RE 0.16387699031240527 Lambda1 0.7138736\n",
      "49 Train Loss 92.5566 Test MSE 93.74970891297974 Test RE 0.16299494457469607 Lambda1 0.7160067\n",
      "50 Train Loss 90.22577 Test MSE 91.27109335741102 Test RE 0.16082582842876275 Lambda1 0.72766465\n",
      "51 Train Loss 89.05467 Test MSE 90.22082087327802 Test RE 0.15989782551933931 Lambda1 0.7377404\n",
      "52 Train Loss 87.53232 Test MSE 87.60113260223116 Test RE 0.1575592958563487 Lambda1 0.7524549\n",
      "53 Train Loss 86.343285 Test MSE 84.39127447022902 Test RE 0.15464573372287677 Lambda1 0.7712511\n",
      "54 Train Loss 84.29783 Test MSE 82.79286584824884 Test RE 0.1531742027398644 Lambda1 0.7953213\n",
      "55 Train Loss 83.2357 Test MSE 81.72603183279074 Test RE 0.15218413384920124 Lambda1 0.8067237\n",
      "56 Train Loss 82.11391 Test MSE 80.50155759210519 Test RE 0.15103976886283157 Lambda1 0.81404567\n",
      "57 Train Loss 79.86936 Test MSE 78.5811519808177 Test RE 0.1492273291864405 Lambda1 0.82588696\n",
      "58 Train Loss 78.80703 Test MSE 76.94248662459691 Test RE 0.14766320126844734 Lambda1 0.83449304\n",
      "59 Train Loss 77.62485 Test MSE 76.54409139657093 Test RE 0.14728041754330742 Lambda1 0.8406239\n",
      "60 Train Loss 76.69179 Test MSE 76.62364887283947 Test RE 0.14735693693922222 Lambda1 0.84246445\n",
      "61 Train Loss 76.25981 Test MSE 76.67328122361305 Test RE 0.14740465384732557 Lambda1 0.83963853\n",
      "62 Train Loss 74.742134 Test MSE 75.3503235860456 Test RE 0.14612742502984974 Lambda1 0.8474343\n",
      "63 Train Loss 73.45471 Test MSE 74.05774471256044 Test RE 0.14486864958060366 Lambda1 0.85932326\n",
      "64 Train Loss 72.830894 Test MSE 73.1105098787441 Test RE 0.1439391974812283 Lambda1 0.8680505\n",
      "65 Train Loss 71.883896 Test MSE 72.3009951439215 Test RE 0.1431400973007239 Lambda1 0.8791526\n",
      "66 Train Loss 70.27978 Test MSE 69.94147718370073 Test RE 0.14078506019967804 Lambda1 0.89225584\n",
      "67 Train Loss 68.979836 Test MSE 68.25640157018276 Test RE 0.13907877764954535 Lambda1 0.90976524\n",
      "68 Train Loss 67.97084 Test MSE 66.5813155016278 Test RE 0.1373616049208073 Lambda1 0.92986\n",
      "69 Train Loss 67.434105 Test MSE 65.54510763353264 Test RE 0.13628853120221696 Lambda1 0.9414728\n",
      "70 Train Loss 66.49504 Test MSE 65.16259069400932 Test RE 0.135890263831855 Lambda1 0.9481812\n",
      "71 Train Loss 65.27599 Test MSE 63.747981335824754 Test RE 0.1344071551770278 Lambda1 0.9556435\n",
      "72 Train Loss 64.16766 Test MSE 62.94876003090121 Test RE 0.13356195289113013 Lambda1 0.9664654\n",
      "73 Train Loss 62.646473 Test MSE 61.70965974241828 Test RE 0.13224088485069083 Lambda1 0.9841104\n",
      "74 Train Loss 61.793343 Test MSE 61.011203920735255 Test RE 0.1314903762537643 Lambda1 0.9892744\n",
      "Training time: 141.31\n",
      "Training time: 141.31\n",
      "inv_HT_stan_tune2\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 838.34973 Test MSE 859.0673793993093 Test RE 0.4934041349660379 Lambda1 0.028557653\n",
      "1 Train Loss 837.83295 Test MSE 857.9114841022748 Test RE 0.4930720798165135 Lambda1 0.028691603\n",
      "2 Train Loss 837.2957 Test MSE 856.6951067421213 Test RE 0.49272240827834396 Lambda1 0.04176902\n",
      "3 Train Loss 835.2609 Test MSE 853.5974603155069 Test RE 0.49183080636560195 Lambda1 0.13913657\n",
      "4 Train Loss 824.2126 Test MSE 837.5155468229245 Test RE 0.48717569056446336 Lambda1 0.3800482\n",
      "5 Train Loss 782.52124 Test MSE 761.9548218545802 Test RE 0.46467978687676453 Lambda1 0.48302516\n",
      "6 Train Loss 717.5218 Test MSE 718.7987027113436 Test RE 0.4513285563287088 Lambda1 0.5282278\n",
      "7 Train Loss 679.4463 Test MSE 675.6205283758965 Test RE 0.4375629978091109 Lambda1 0.59392995\n",
      "8 Train Loss 651.25476 Test MSE 640.3977773311507 Test RE 0.42600439395763684 Lambda1 0.62780017\n",
      "9 Train Loss 594.0746 Test MSE 570.5532923020405 Test RE 0.4021029677972404 Lambda1 0.6660746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Train Loss 534.17145 Test MSE 531.1665773764964 Test RE 0.38797571367051853 Lambda1 0.672599\n",
      "11 Train Loss 453.77945 Test MSE 462.68434568321743 Test RE 0.3621025419929377 Lambda1 0.6734029\n",
      "12 Train Loss 408.0175 Test MSE 418.7483683910948 Test RE 0.3444813659513173 Lambda1 0.6785892\n",
      "13 Train Loss 375.21713 Test MSE 383.18514793421804 Test RE 0.329528896841331 Lambda1 0.67187047\n",
      "14 Train Loss 347.00522 Test MSE 360.6630444461809 Test RE 0.31969805405706037 Lambda1 0.6714493\n",
      "15 Train Loss 325.6687 Test MSE 344.3204382020189 Test RE 0.3123709021131537 Lambda1 0.6515823\n",
      "16 Train Loss 315.7013 Test MSE 338.74879222479024 Test RE 0.30983326839048747 Lambda1 0.6660422\n",
      "17 Train Loss 299.43393 Test MSE 327.20794375207754 Test RE 0.30450967093360015 Lambda1 0.67292744\n",
      "18 Train Loss 285.85205 Test MSE 308.0875988637996 Test RE 0.2954787678637227 Lambda1 0.6797603\n",
      "19 Train Loss 274.72928 Test MSE 302.0527333433141 Test RE 0.2925705146702419 Lambda1 0.67076457\n",
      "20 Train Loss 266.05502 Test MSE 296.0781444581758 Test RE 0.28966254746373105 Lambda1 0.66101\n",
      "21 Train Loss 256.76257 Test MSE 284.33529426139 Test RE 0.2838602339880247 Lambda1 0.64838153\n",
      "22 Train Loss 250.1596 Test MSE 277.84567065720154 Test RE 0.28060214570880293 Lambda1 0.62612766\n",
      "23 Train Loss 246.62457 Test MSE 275.8365642067993 Test RE 0.27958578576397675 Lambda1 0.6065834\n",
      "24 Train Loss 243.65091 Test MSE 272.48748770098706 Test RE 0.2778833034128752 Lambda1 0.5821367\n",
      "25 Train Loss 240.3918 Test MSE 268.6219135807325 Test RE 0.2759052026582645 Lambda1 0.5652512\n",
      "26 Train Loss 238.28969 Test MSE 267.1498010467011 Test RE 0.2751481506739064 Lambda1 0.5509126\n",
      "27 Train Loss 235.76175 Test MSE 264.83960078333513 Test RE 0.2739558841906333 Lambda1 0.51501125\n",
      "28 Train Loss 233.72612 Test MSE 262.85873546247717 Test RE 0.2729294360221429 Lambda1 0.4829692\n",
      "29 Train Loss 231.31012 Test MSE 259.7703207616064 Test RE 0.27132132916437496 Lambda1 0.47246498\n",
      "30 Train Loss 229.46832 Test MSE 256.56927965779033 Test RE 0.2696444576280476 Lambda1 0.44943273\n",
      "31 Train Loss 226.21356 Test MSE 252.79897846522644 Test RE 0.26765590424751295 Lambda1 0.3998835\n",
      "32 Train Loss 225.09227 Test MSE 249.23541962076564 Test RE 0.2657627146683867 Lambda1 0.38309896\n",
      "33 Train Loss 223.34398 Test MSE 245.52124742370285 Test RE 0.2637750485292937 Lambda1 0.37553126\n",
      "34 Train Loss 219.08162 Test MSE 240.46199487131992 Test RE 0.26104320532736364 Lambda1 0.38485307\n",
      "35 Train Loss 216.12692 Test MSE 238.0412603639748 Test RE 0.25972591872456746 Lambda1 0.39005268\n",
      "36 Train Loss 213.14609 Test MSE 235.20275638995912 Test RE 0.2581727341579287 Lambda1 0.38751906\n",
      "37 Train Loss 211.13426 Test MSE 231.37523342328117 Test RE 0.2560634576892899 Lambda1 0.374573\n",
      "38 Train Loss 208.2045 Test MSE 223.5119860931274 Test RE 0.25167471150668025 Lambda1 0.3570374\n",
      "39 Train Loss 202.60138 Test MSE 211.96799961298998 Test RE 0.24508928298736032 Lambda1 0.34672177\n",
      "40 Train Loss 197.24016 Test MSE 204.36543288429127 Test RE 0.24065389235968906 Lambda1 0.33747494\n",
      "41 Train Loss 191.59412 Test MSE 201.54881624321592 Test RE 0.2389897618647053 Lambda1 0.34060374\n",
      "42 Train Loss 187.9446 Test MSE 199.62598909986454 Test RE 0.2378470181564668 Lambda1 0.33947337\n",
      "43 Train Loss 181.73506 Test MSE 190.74731762483555 Test RE 0.23249755512789344 Lambda1 0.35485893\n",
      "44 Train Loss 178.81418 Test MSE 186.45971861062526 Test RE 0.2298696755682211 Lambda1 0.3639513\n",
      "45 Train Loss 173.04874 Test MSE 178.87838907084168 Test RE 0.22514800763003862 Lambda1 0.3759365\n",
      "46 Train Loss 165.90643 Test MSE 173.61085542434543 Test RE 0.22180820593786738 Lambda1 0.39423996\n",
      "47 Train Loss 159.55643 Test MSE 162.83574413162347 Test RE 0.21481472222239417 Lambda1 0.41497898\n",
      "48 Train Loss 156.83707 Test MSE 160.39859871807005 Test RE 0.2132011071094345 Lambda1 0.42927536\n",
      "49 Train Loss 151.31857 Test MSE 156.15281255308162 Test RE 0.21036044269968582 Lambda1 0.45653573\n",
      "50 Train Loss 147.59607 Test MSE 154.6079313990235 Test RE 0.2093172668758966 Lambda1 0.47291946\n",
      "51 Train Loss 142.35054 Test MSE 145.9026834062462 Test RE 0.20333905942591096 Lambda1 0.49068674\n",
      "52 Train Loss 139.45274 Test MSE 141.10012027563056 Test RE 0.1999644820694026 Lambda1 0.502565\n",
      "53 Train Loss 134.85802 Test MSE 136.52026487245095 Test RE 0.19669246893348286 Lambda1 0.51956725\n",
      "54 Train Loss 132.02145 Test MSE 131.23712971231387 Test RE 0.1928490626544813 Lambda1 0.5346433\n",
      "55 Train Loss 128.89175 Test MSE 128.13134865875796 Test RE 0.19055347333022735 Lambda1 0.55096126\n",
      "56 Train Loss 126.28185 Test MSE 124.85733741954779 Test RE 0.18810320931818816 Lambda1 0.5618917\n",
      "57 Train Loss 122.693306 Test MSE 117.53792243690114 Test RE 0.18250643237951364 Lambda1 0.5757824\n",
      "58 Train Loss 118.74778 Test MSE 113.53594802514014 Test RE 0.17937250198839622 Lambda1 0.5878458\n",
      "59 Train Loss 116.84923 Test MSE 113.20140037785633 Test RE 0.17910803544480944 Lambda1 0.5968208\n",
      "60 Train Loss 113.36446 Test MSE 109.88039139630818 Test RE 0.1764612167525996 Lambda1 0.6177205\n",
      "61 Train Loss 108.31299 Test MSE 107.4492218330449 Test RE 0.17449814233718314 Lambda1 0.6388888\n",
      "62 Train Loss 106.45867 Test MSE 105.77478020131444 Test RE 0.17313315227711032 Lambda1 0.6483342\n",
      "63 Train Loss 103.44966 Test MSE 101.10743652958965 Test RE 0.16927028302585478 Lambda1 0.67860466\n",
      "64 Train Loss 100.73161 Test MSE 98.07112519289252 Test RE 0.16670926995843377 Lambda1 0.6898307\n",
      "65 Train Loss 97.86675 Test MSE 95.27058169622626 Test RE 0.1643117340969042 Lambda1 0.6881904\n",
      "66 Train Loss 94.341805 Test MSE 92.64245346451854 Test RE 0.16202953835782577 Lambda1 0.70439917\n",
      "67 Train Loss 91.719475 Test MSE 89.65837686234735 Test RE 0.1593986382176722 Lambda1 0.7207063\n",
      "68 Train Loss 89.97759 Test MSE 87.0362415940007 Test RE 0.15705046808553796 Lambda1 0.7284499\n",
      "69 Train Loss 88.86387 Test MSE 85.89972929762294 Test RE 0.15602172249170165 Lambda1 0.7303117\n",
      "70 Train Loss 87.52378 Test MSE 85.13140776204496 Test RE 0.15532239486827987 Lambda1 0.7318383\n",
      "71 Train Loss 86.275795 Test MSE 83.49383176610844 Test RE 0.1538212608509097 Lambda1 0.73833334\n",
      "72 Train Loss 83.9601 Test MSE 81.09584973133562 Test RE 0.15159625929076054 Lambda1 0.75405836\n",
      "73 Train Loss 82.48205 Test MSE 79.89582594993959 Test RE 0.1504704486958069 Lambda1 0.7660082\n",
      "74 Train Loss 81.1939 Test MSE 78.46863234049106 Test RE 0.14912045228267334 Lambda1 0.7762662\n",
      "Training time: 141.34\n",
      "Training time: 141.34\n",
      "inv_HT_stan_tune2\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 846.29205 Test MSE 867.3402945948117 Test RE 0.49577421118666143 Lambda1 -0.0070459885\n",
      "1 Train Loss 837.7573 Test MSE 857.6291732098186 Test RE 0.49299094611281313 Lambda1 -0.0073461332\n",
      "2 Train Loss 837.62964 Test MSE 857.4934303605705 Test RE 0.49295193004039467 Lambda1 -0.0073757526\n",
      "3 Train Loss 837.2313 Test MSE 856.6627724980588 Test RE 0.4927131097794903 Lambda1 -0.0067451736\n",
      "4 Train Loss 835.06415 Test MSE 852.9912786880715 Test RE 0.49165613870194586 Lambda1 0.005500449\n",
      "5 Train Loss 817.799 Test MSE 834.2167211961694 Test RE 0.48621529447902967 Lambda1 0.07069185\n",
      "6 Train Loss 747.1198 Test MSE 743.7517849720929 Test RE 0.4590956535567126 Lambda1 0.10555725\n",
      "7 Train Loss 703.0039 Test MSE 701.8107996753407 Test RE 0.44596337565010125 Lambda1 0.09790068\n",
      "8 Train Loss 651.2423 Test MSE 653.8408323152718 Test RE 0.4304524559264931 Lambda1 0.04710523\n",
      "9 Train Loss 543.6618 Test MSE 535.3141252116402 Test RE 0.3894874981997327 Lambda1 0.006454264\n",
      "10 Train Loss 463.17505 Test MSE 459.7688170519334 Test RE 0.36095987452345846 Lambda1 0.0024805064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Train Loss 356.47803 Test MSE 366.4682863506558 Test RE 0.3222607172169079 Lambda1 0.0005281943\n",
      "12 Train Loss 284.97165 Test MSE 299.7340069650989 Test RE 0.291445383453755 Lambda1 1.5312158e-05\n",
      "13 Train Loss 266.89926 Test MSE 289.78195822773773 Test RE 0.2865661190321082 Lambda1 -3.5988312e-05\n",
      "14 Train Loss 262.2485 Test MSE 286.10837176947615 Test RE 0.2847439160451011 Lambda1 0.0003999483\n",
      "15 Train Loss 258.22546 Test MSE 283.32169056069426 Test RE 0.2833538271744717 Lambda1 5.673973e-05\n",
      "16 Train Loss 256.26114 Test MSE 282.5694666173457 Test RE 0.28297742253492225 Lambda1 9.1172085e-05\n",
      "17 Train Loss 255.79683 Test MSE 282.2222821155693 Test RE 0.28280352623083416 Lambda1 0.000118426826\n",
      "18 Train Loss 255.09398 Test MSE 282.79385710200404 Test RE 0.28308975746057236 Lambda1 8.0332204e-05\n",
      "19 Train Loss 254.88586 Test MSE 283.1083948383479 Test RE 0.2832471471344306 Lambda1 4.5676035e-05\n",
      "20 Train Loss 254.78008 Test MSE 283.27127793073663 Test RE 0.2833286168788972 Lambda1 4.2353422e-05\n",
      "21 Train Loss 254.72546 Test MSE 283.287780816499 Test RE 0.28333686987181206 Lambda1 3.89516e-05\n",
      "22 Train Loss 254.61638 Test MSE 283.1681458567536 Test RE 0.28327703570877366 Lambda1 3.5645262e-05\n",
      "23 Train Loss 254.45277 Test MSE 283.3568765654781 Test RE 0.2833714216261515 Lambda1 2.628929e-05\n",
      "24 Train Loss 254.40327 Test MSE 283.6256085303025 Test RE 0.2835057626614637 Lambda1 2.1736825e-05\n",
      "25 Train Loss 254.3777 Test MSE 283.6062479507501 Test RE 0.28349608629695694 Lambda1 2.1331733e-05\n",
      "26 Train Loss 254.212 Test MSE 283.3858345131398 Test RE 0.28338590097328265 Lambda1 3.4105786e-05\n",
      "27 Train Loss 254.1648 Test MSE 283.25646693535725 Test RE 0.283321209785386 Lambda1 3.8323968e-05\n",
      "28 Train Loss 254.15747 Test MSE 283.19338549448645 Test RE 0.2832896600991756 Lambda1 4.0976855e-05\n",
      "29 Train Loss 254.15134 Test MSE 283.1974737738457 Test RE 0.28329170492640227 Lambda1 4.1178064e-05\n",
      "30 Train Loss 254.12407 Test MSE 283.2195543781034 Test RE 0.2833027486868189 Lambda1 3.5303987e-05\n",
      "31 Train Loss 254.09636 Test MSE 283.15392098055423 Test RE 0.2832699204462881 Lambda1 4.1181193e-05\n",
      "32 Train Loss 254.09009 Test MSE 283.1036762107454 Test RE 0.2832447866544867 Lambda1 4.402766e-05\n",
      "33 Train Loss 254.06973 Test MSE 283.11536640675143 Test RE 0.2832506346055563 Lambda1 3.756005e-05\n",
      "34 Train Loss 254.01617 Test MSE 283.1103509791913 Test RE 0.28324812568251734 Lambda1 2.6521137e-05\n",
      "35 Train Loss 253.98967 Test MSE 282.95093148799066 Test RE 0.28316836591667066 Lambda1 2.5158752e-05\n",
      "36 Train Loss 253.95778 Test MSE 282.88050858952545 Test RE 0.28313312521622486 Lambda1 2.370748e-05\n",
      "37 Train Loss 253.92595 Test MSE 282.9256660351267 Test RE 0.28315572320035315 Lambda1 2.580042e-05\n",
      "38 Train Loss 253.87337 Test MSE 282.9583532768964 Test RE 0.2831720796384182 Lambda1 1.9811278e-05\n",
      "39 Train Loss 253.76045 Test MSE 283.162833571123 Test RE 0.28327437853209936 Lambda1 1.3394494e-05\n",
      "40 Train Loss 253.64699 Test MSE 283.39184439084755 Test RE 0.2833889058969096 Lambda1 1.7419128e-05\n",
      "41 Train Loss 253.54504 Test MSE 283.243619546197 Test RE 0.28331478454970993 Lambda1 2.492589e-05\n",
      "42 Train Loss 253.35097 Test MSE 283.84356351220623 Test RE 0.28361467318469596 Lambda1 3.8872196e-05\n",
      "43 Train Loss 253.2213 Test MSE 284.3135546250651 Test RE 0.28384938212345034 Lambda1 3.194107e-05\n",
      "44 Train Loss 253.07512 Test MSE 284.1314002491738 Test RE 0.28375843906011033 Lambda1 3.287676e-05\n",
      "45 Train Loss 252.55853 Test MSE 283.3022849548982 Test RE 0.2833441231048355 Lambda1 3.171652e-05\n",
      "46 Train Loss 252.32768 Test MSE 283.97636284924215 Test RE 0.2836810115504074 Lambda1 1.7531163e-05\n",
      "47 Train Loss 251.9415 Test MSE 284.763977623681 Test RE 0.28407413695914235 Lambda1 2.8922088e-05\n",
      "48 Train Loss 251.47319 Test MSE 284.03362890125516 Test RE 0.2837096133545939 Lambda1 2.4959916e-05\n",
      "49 Train Loss 251.13892 Test MSE 283.8876117525409 Test RE 0.2836366786910023 Lambda1 2.8883494e-05\n",
      "50 Train Loss 250.97379 Test MSE 283.68802516482816 Test RE 0.2835369560756008 Lambda1 2.0146455e-05\n",
      "51 Train Loss 250.65479 Test MSE 284.6144540692991 Test RE 0.2839995464995833 Lambda1 -1.1391433e-06\n",
      "52 Train Loss 250.13206 Test MSE 285.4189297291853 Test RE 0.28440063207581395 Lambda1 2.0042226e-05\n",
      "53 Train Loss 249.74594 Test MSE 285.8667646165493 Test RE 0.2846236631904111 Lambda1 5.794474e-06\n",
      "54 Train Loss 249.65337 Test MSE 286.09379593817056 Test RE 0.28473666279326215 Lambda1 -8.268138e-06\n",
      "55 Train Loss 249.60635 Test MSE 286.1684160993014 Test RE 0.284773793466316 Lambda1 -5.4608186e-06\n",
      "56 Train Loss 249.47722 Test MSE 286.3128768805624 Test RE 0.2848456627780206 Lambda1 -1.2194358e-05\n",
      "57 Train Loss 249.21596 Test MSE 287.17569713238487 Test RE 0.28527453926916396 Lambda1 -1.2571432e-05\n",
      "58 Train Loss 248.97536 Test MSE 287.8766941100836 Test RE 0.2856225051740965 Lambda1 -9.250747e-06\n",
      "59 Train Loss 248.88701 Test MSE 288.52709315827883 Test RE 0.28594497621993187 Lambda1 -1.10959745e-05\n",
      "60 Train Loss 248.76996 Test MSE 288.8266049372689 Test RE 0.28609335340175973 Lambda1 -8.246225e-06\n",
      "61 Train Loss 248.71785 Test MSE 288.6483556137526 Test RE 0.28600505852812314 Lambda1 -8.913003e-06\n",
      "62 Train Loss 248.56163 Test MSE 289.19237392008364 Test RE 0.28627444988632206 Lambda1 -7.4511972e-06\n",
      "63 Train Loss 248.42476 Test MSE 289.99988658689347 Test RE 0.28667385373628856 Lambda1 -3.9506094e-06\n",
      "64 Train Loss 248.35043 Test MSE 290.0434933601389 Test RE 0.2866954062478814 Lambda1 -1.911683e-06\n",
      "65 Train Loss 248.2507 Test MSE 289.68442849994835 Test RE 0.28651789127532906 Lambda1 -5.3715246e-07\n",
      "66 Train Loss 248.15408 Test MSE 289.40246320628387 Test RE 0.2863784157602203 Lambda1 6.447008e-07\n",
      "67 Train Loss 247.89354 Test MSE 290.0012016833658 Test RE 0.2866745037423123 Lambda1 2.0667694e-06\n",
      "68 Train Loss 247.74292 Test MSE 290.59951718949566 Test RE 0.28697007738514496 Lambda1 -1.036361e-06\n",
      "69 Train Loss 247.68956 Test MSE 290.53991750448404 Test RE 0.2869406482184779 Lambda1 -2.7884416e-06\n",
      "70 Train Loss 247.65051 Test MSE 290.29996477669346 Test RE 0.2868221336719625 Lambda1 -1.2650045e-06\n",
      "71 Train Loss 247.60948 Test MSE 290.40793744110215 Test RE 0.2868754682821396 Lambda1 -5.545131e-07\n",
      "72 Train Loss 247.53207 Test MSE 290.62458449987867 Test RE 0.28698245423447044 Lambda1 -4.3986466e-07\n",
      "73 Train Loss 247.43608 Test MSE 290.63286848633237 Test RE 0.2869865442907652 Lambda1 9.1860744e-07\n",
      "74 Train Loss 247.38478 Test MSE 290.7548369059476 Test RE 0.2870467570667613 Lambda1 3.0770298e-06\n",
      "Training time: 143.28\n",
      "Training time: 143.28\n",
      "inv_HT_stan_tune2\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 837.8721 Test MSE 858.4725292941928 Test RE 0.4932332797257985 Lambda1 -0.09485989\n",
      "1 Train Loss 837.37067 Test MSE 857.3131592160731 Test RE 0.49290011058820676 Lambda1 -0.09413663\n",
      "2 Train Loss 837.2086 Test MSE 856.753400778985 Test RE 0.49273917170410075 Lambda1 -0.09222231\n",
      "3 Train Loss 835.47797 Test MSE 853.9811225515977 Test RE 0.4919413243306364 Lambda1 -0.07463166\n",
      "4 Train Loss 809.6247 Test MSE 816.7461648781001 Test RE 0.4810970817636842 Lambda1 0.082172625\n",
      "5 Train Loss 732.2895 Test MSE 734.8323752737194 Test RE 0.45633450841280493 Lambda1 0.10596444\n",
      "6 Train Loss 653.483 Test MSE 645.7001627057275 Test RE 0.42776438056515736 Lambda1 0.035148013\n",
      "7 Train Loss 572.24695 Test MSE 566.119470358454 Test RE 0.4005375310634919 Lambda1 0.013616128\n",
      "8 Train Loss 473.71378 Test MSE 476.0342055171726 Test RE 0.3672892781744295 Lambda1 0.0015299316\n",
      "9 Train Loss 364.9632 Test MSE 374.7649269947672 Test RE 0.325888203831521 Lambda1 -0.0040500504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Train Loss 276.31784 Test MSE 291.90427724832097 Test RE 0.28761358800650366 Lambda1 0.0007430538\n",
      "11 Train Loss 268.2927 Test MSE 288.59449228725316 Test RE 0.2859783722465494 Lambda1 -0.00025208807\n",
      "12 Train Loss 263.5718 Test MSE 285.77990559455463 Test RE 0.28458041924902405 Lambda1 -7.401063e-05\n",
      "13 Train Loss 261.48315 Test MSE 283.95797198973116 Test RE 0.28367182553568465 Lambda1 9.699157e-06\n",
      "14 Train Loss 258.72906 Test MSE 282.01701851815085 Test RE 0.282700664352705 Lambda1 0.000577341\n",
      "15 Train Loss 257.22055 Test MSE 280.0433224800234 Test RE 0.2817096872034693 Lambda1 0.0012823184\n",
      "16 Train Loss 256.58862 Test MSE 279.5008603116172 Test RE 0.2814367099329558 Lambda1 0.0014858934\n",
      "17 Train Loss 255.507 Test MSE 280.24514858051504 Test RE 0.28181118244139575 Lambda1 0.0017173667\n",
      "18 Train Loss 254.80486 Test MSE 280.8614150807902 Test RE 0.282120867408625 Lambda1 0.0017581569\n",
      "19 Train Loss 254.59926 Test MSE 280.8544600224052 Test RE 0.2821173742637025 Lambda1 0.0019425806\n",
      "20 Train Loss 254.42838 Test MSE 280.8771450264469 Test RE 0.2821287675393322 Lambda1 0.0020632686\n",
      "21 Train Loss 253.98546 Test MSE 280.9451478400075 Test RE 0.28216291839356145 Lambda1 0.0023457224\n",
      "22 Train Loss 253.62189 Test MSE 280.86837939346884 Test RE 0.2821243651581353 Lambda1 0.002809246\n",
      "23 Train Loss 253.53743 Test MSE 280.94815073596465 Test RE 0.2821644263456107 Lambda1 0.0029977227\n",
      "24 Train Loss 253.18335 Test MSE 281.08767862038843 Test RE 0.2822344836115157 Lambda1 0.003952233\n",
      "25 Train Loss 252.82927 Test MSE 280.02417792224526 Test RE 0.2817000578012888 Lambda1 0.004815412\n",
      "26 Train Loss 252.3052 Test MSE 277.6425983183506 Test RE 0.2804995834670348 Lambda1 0.007837559\n",
      "27 Train Loss 251.55359 Test MSE 275.8018741544988 Test RE 0.2795682044282375 Lambda1 0.011055907\n",
      "28 Train Loss 250.55112 Test MSE 274.5012237253634 Test RE 0.2789082193967677 Lambda1 0.014249353\n",
      "29 Train Loss 248.14223 Test MSE 270.0491222175343 Test RE 0.2766371843302478 Lambda1 0.024517227\n",
      "30 Train Loss 246.15086 Test MSE 266.32365492727905 Test RE 0.2747223809629949 Lambda1 0.03370492\n",
      "31 Train Loss 245.13748 Test MSE 264.9479474745114 Test RE 0.27401191655577484 Lambda1 0.03914922\n",
      "32 Train Loss 242.03601 Test MSE 260.19912421051373 Test RE 0.27154517219006213 Lambda1 0.049027637\n",
      "33 Train Loss 239.80075 Test MSE 259.68221504991646 Test RE 0.27127531354175854 Lambda1 0.057038072\n",
      "34 Train Loss 236.09026 Test MSE 255.46318216637735 Test RE 0.26906259684661527 Lambda1 0.07355225\n",
      "35 Train Loss 232.81676 Test MSE 252.74910882534525 Test RE 0.26762950271300534 Lambda1 0.08587452\n",
      "36 Train Loss 228.61203 Test MSE 247.0730866229467 Test RE 0.26460734245181133 Lambda1 0.108815834\n",
      "37 Train Loss 226.96141 Test MSE 246.84258226117421 Test RE 0.264483882264783 Lambda1 0.11523605\n",
      "38 Train Loss 224.2925 Test MSE 244.94857830819106 Test RE 0.26346724622865875 Lambda1 0.12862474\n",
      "39 Train Loss 218.17279 Test MSE 236.71440440146773 Test RE 0.25900104342789954 Lambda1 0.1585174\n",
      "40 Train Loss 214.18976 Test MSE 233.85112073653116 Test RE 0.2574298469201197 Lambda1 0.16878358\n",
      "41 Train Loss 210.54678 Test MSE 228.56871124038295 Test RE 0.25450572755808604 Lambda1 0.1825013\n",
      "42 Train Loss 202.16284 Test MSE 218.33846379567981 Test RE 0.24874496334734106 Lambda1 0.21201396\n",
      "43 Train Loss 193.6144 Test MSE 206.5766241853189 Test RE 0.2419523021049623 Lambda1 0.23268992\n",
      "44 Train Loss 177.58524 Test MSE 182.09213988956847 Test RE 0.22716152232416212 Lambda1 0.28155905\n",
      "45 Train Loss 171.14651 Test MSE 172.55659462401792 Test RE 0.22113370956176584 Lambda1 0.30236703\n",
      "46 Train Loss 167.00128 Test MSE 164.58465265441754 Test RE 0.2159652310377042 Lambda1 0.3128682\n",
      "47 Train Loss 160.53696 Test MSE 151.04421836040822 Test RE 0.2068908213171676 Lambda1 0.34241575\n",
      "48 Train Loss 155.04794 Test MSE 148.13308905416065 Test RE 0.20488738065219483 Lambda1 0.35595927\n",
      "49 Train Loss 151.81427 Test MSE 145.81078971326497 Test RE 0.20327501495542344 Lambda1 0.3771092\n",
      "50 Train Loss 147.64326 Test MSE 139.66573395297826 Test RE 0.19894549287070143 Lambda1 0.41056246\n",
      "51 Train Loss 142.01236 Test MSE 133.8882391213277 Test RE 0.19478718687674845 Lambda1 0.43491924\n",
      "52 Train Loss 136.37607 Test MSE 127.93365019108178 Test RE 0.19040641067645575 Lambda1 0.455339\n",
      "53 Train Loss 133.42963 Test MSE 126.922429311081 Test RE 0.18965240683767556 Lambda1 0.46158853\n",
      "54 Train Loss 128.00719 Test MSE 120.75267130527561 Test RE 0.18498543882700022 Lambda1 0.47721145\n",
      "55 Train Loss 126.651924 Test MSE 119.47445466202623 Test RE 0.18400376060825793 Lambda1 0.48047373\n",
      "56 Train Loss 119.884476 Test MSE 114.97943349826555 Test RE 0.18050916319360347 Lambda1 0.4988272\n",
      "57 Train Loss 116.77679 Test MSE 114.20165531835283 Test RE 0.17989760024772922 Lambda1 0.51124024\n",
      "58 Train Loss 115.831314 Test MSE 113.39106609258202 Test RE 0.17925801784845105 Lambda1 0.51608914\n",
      "59 Train Loss 112.19517 Test MSE 111.04454137076259 Test RE 0.17739353084190054 Lambda1 0.5335312\n",
      "60 Train Loss 110.62745 Test MSE 110.99243723595168 Test RE 0.17735190781016605 Lambda1 0.5418276\n",
      "61 Train Loss 109.46169 Test MSE 110.20097010005284 Test RE 0.17671844420972924 Lambda1 0.54165685\n",
      "62 Train Loss 108.32335 Test MSE 109.443394251215 Test RE 0.17610997177638762 Lambda1 0.539798\n",
      "63 Train Loss 106.11007 Test MSE 108.78604179545981 Test RE 0.17558028846502427 Lambda1 0.5450574\n",
      "64 Train Loss 104.405876 Test MSE 107.86969433129872 Test RE 0.1748392338258912 Lambda1 0.5524396\n",
      "65 Train Loss 103.67477 Test MSE 106.83735529543516 Test RE 0.17400059565538026 Lambda1 0.55834967\n",
      "66 Train Loss 102.816444 Test MSE 106.00713908549616 Test RE 0.17332321155746758 Lambda1 0.5641742\n",
      "67 Train Loss 100.51348 Test MSE 101.79268870441075 Test RE 0.16984292615870267 Lambda1 0.56802005\n",
      "68 Train Loss 97.62325 Test MSE 99.21385026337666 Test RE 0.16767770555349196 Lambda1 0.57273686\n",
      "69 Train Loss 95.66122 Test MSE 97.9866138570344 Test RE 0.1666374248582914 Lambda1 0.5777247\n",
      "70 Train Loss 94.43924 Test MSE 95.9357655476078 Test RE 0.1648843525965293 Lambda1 0.5838604\n",
      "71 Train Loss 93.41641 Test MSE 94.74146301967313 Test RE 0.16385481730449564 Lambda1 0.59255445\n",
      "72 Train Loss 90.77291 Test MSE 90.75290599540894 Test RE 0.16036863802784224 Lambda1 0.6079184\n",
      "73 Train Loss 89.30822 Test MSE 88.52881946104472 Test RE 0.1583913670270845 Lambda1 0.6185256\n",
      "74 Train Loss 87.68128 Test MSE 85.66149244663238 Test RE 0.15580521463924538 Lambda1 0.635085\n",
      "Training time: 141.42\n",
      "Training time: 141.42\n",
      "inv_HT_stan_tune2\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 839.6461 Test MSE 858.7558614534032 Test RE 0.49331466690856723 Lambda1 -0.011237264\n",
      "1 Train Loss 837.7423 Test MSE 857.8985587940283 Test RE 0.4930683654865728 Lambda1 -0.011199222\n",
      "2 Train Loss 836.1588 Test MSE 854.8934736113897 Test RE 0.49220403703332066 Lambda1 0.01848317\n",
      "3 Train Loss 821.77875 Test MSE 817.7649398140355 Test RE 0.48139703842291154 Lambda1 0.101999395\n",
      "4 Train Loss 754.934 Test MSE 772.1097502624491 Test RE 0.4677660403908311 Lambda1 0.12629001\n",
      "5 Train Loss 606.1463 Test MSE 600.4870190324892 Test RE 0.41251617522194817 Lambda1 0.051853344\n",
      "6 Train Loss 498.50616 Test MSE 510.7274256405283 Test RE 0.3804378866647378 Lambda1 0.03574838\n",
      "7 Train Loss 375.9336 Test MSE 363.5694224549485 Test RE 0.320983601624153 Lambda1 0.008828289\n",
      "8 Train Loss 270.3535 Test MSE 290.5745041630682 Test RE 0.2869577268061334 Lambda1 0.0046740025\n",
      "9 Train Loss 263.5885 Test MSE 283.1614827912082 Test RE 0.28327370287528314 Lambda1 0.0037154914\n",
      "10 Train Loss 260.9938 Test MSE 281.3633884535462 Test RE 0.2823728670344572 Lambda1 0.0017219057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Train Loss 259.5824 Test MSE 281.4633236314767 Test RE 0.28242300944565 Lambda1 0.00091408676\n",
      "12 Train Loss 256.79068 Test MSE 281.2640866245835 Test RE 0.2823230335845638 Lambda1 0.0002791276\n",
      "13 Train Loss 256.17538 Test MSE 281.1163264576986 Test RE 0.2822488656036547 Lambda1 0.00048296788\n",
      "14 Train Loss 255.69205 Test MSE 281.7486941774292 Test RE 0.28256614495119453 Lambda1 0.00049148354\n",
      "15 Train Loss 255.22 Test MSE 282.1572799463134 Test RE 0.282770956333875 Lambda1 0.0008155023\n",
      "16 Train Loss 254.94583 Test MSE 281.7148876488243 Test RE 0.2825491921361043 Lambda1 0.0011275021\n",
      "17 Train Loss 254.81445 Test MSE 281.762261042713 Test RE 0.28257294798303495 Lambda1 0.0012317016\n",
      "18 Train Loss 254.62236 Test MSE 281.66720393528016 Test RE 0.2825252786592231 Lambda1 0.0013678976\n",
      "19 Train Loss 254.50153 Test MSE 281.3528722747125 Test RE 0.2823675900307023 Lambda1 0.0014815581\n",
      "20 Train Loss 254.28424 Test MSE 281.2551569530748 Test RE 0.28231855190332666 Lambda1 0.0017731898\n",
      "21 Train Loss 254.00403 Test MSE 281.6877843929289 Test RE 0.2825356000478041 Lambda1 0.0024117965\n",
      "22 Train Loss 253.82976 Test MSE 281.4358826991956 Test RE 0.2824092418622236 Lambda1 0.0030472607\n",
      "23 Train Loss 253.71364 Test MSE 280.96462453917627 Test RE 0.2821726987849958 Lambda1 0.0034370362\n",
      "24 Train Loss 253.66046 Test MSE 280.7044051108859 Test RE 0.28204199936388363 Lambda1 0.0035850601\n",
      "25 Train Loss 253.34718 Test MSE 279.1297595380501 Test RE 0.2812498123486557 Lambda1 0.005208867\n",
      "26 Train Loss 252.96791 Test MSE 277.8489790928896 Test RE 0.28060381633297937 Lambda1 0.006662937\n",
      "27 Train Loss 252.56648 Test MSE 277.41222361890453 Test RE 0.2803831866715715 Lambda1 0.0074302955\n",
      "28 Train Loss 252.07469 Test MSE 276.7239347468152 Test RE 0.28003514058098955 Lambda1 0.00888079\n",
      "29 Train Loss 250.66652 Test MSE 273.00744195533986 Test RE 0.27814830226884896 Lambda1 0.01676051\n",
      "30 Train Loss 248.0846 Test MSE 266.26498466933367 Test RE 0.2746921190609032 Lambda1 0.035914786\n",
      "31 Train Loss 244.33833 Test MSE 261.4930598874366 Test RE 0.2722195139098811 Lambda1 0.045943663\n",
      "32 Train Loss 240.52777 Test MSE 257.8092871157733 Test RE 0.2702952723275588 Lambda1 0.05583777\n",
      "33 Train Loss 237.28294 Test MSE 254.06553366780295 Test RE 0.26832556167824156 Lambda1 0.06590201\n",
      "34 Train Loss 233.43614 Test MSE 246.8937513671312 Test RE 0.26451129386936567 Lambda1 0.086788476\n",
      "35 Train Loss 227.39383 Test MSE 238.90322360770992 Test RE 0.2601957362161005 Lambda1 0.10765535\n",
      "36 Train Loss 220.63522 Test MSE 233.26498469437416 Test RE 0.257107027045085 Lambda1 0.11648086\n",
      "37 Train Loss 214.43062 Test MSE 230.2324520769238 Test RE 0.2554303156761006 Lambda1 0.13215816\n",
      "38 Train Loss 208.60048 Test MSE 220.11529302795185 Test RE 0.24975505052275013 Lambda1 0.14595294\n",
      "39 Train Loss 199.63107 Test MSE 209.49117423374068 Test RE 0.2436531532301524 Lambda1 0.17838073\n",
      "40 Train Loss 194.61488 Test MSE 201.4657273382798 Test RE 0.23894049478152557 Lambda1 0.19914469\n",
      "41 Train Loss 184.12029 Test MSE 185.89207487669242 Test RE 0.22951951001416232 Lambda1 0.239832\n",
      "42 Train Loss 178.46277 Test MSE 179.43462356612397 Test RE 0.22549779242897555 Lambda1 0.25707966\n",
      "43 Train Loss 164.01097 Test MSE 167.21112178594927 Test RE 0.21768161500903602 Lambda1 0.29955566\n",
      "44 Train Loss 154.45404 Test MSE 154.90750079459548 Test RE 0.20951995603133597 Lambda1 0.34742662\n",
      "45 Train Loss 148.96332 Test MSE 150.06700009822106 Test RE 0.2062204694075461 Lambda1 0.38062128\n",
      "46 Train Loss 145.63786 Test MSE 150.02441319111463 Test RE 0.20619120609472727 Lambda1 0.39464256\n",
      "47 Train Loss 142.16275 Test MSE 147.49882036387535 Test RE 0.20444827195187576 Lambda1 0.40743017\n",
      "48 Train Loss 139.0835 Test MSE 143.82809247176766 Test RE 0.20188824415804474 Lambda1 0.42096934\n",
      "49 Train Loss 131.95447 Test MSE 131.66133027814715 Test RE 0.19316048626581606 Lambda1 0.48629528\n",
      "50 Train Loss 128.58197 Test MSE 127.42034833818875 Test RE 0.19002404764798494 Lambda1 0.49683413\n",
      "51 Train Loss 125.19038 Test MSE 126.05084085781725 Test RE 0.18900010445047705 Lambda1 0.51643604\n",
      "52 Train Loss 121.65643 Test MSE 118.63265769318505 Test RE 0.18335438492052347 Lambda1 0.5471867\n",
      "53 Train Loss 118.763504 Test MSE 114.71268819187272 Test RE 0.18029965646152663 Lambda1 0.55821747\n",
      "54 Train Loss 115.76155 Test MSE 113.95145547146234 Test RE 0.1797004269589503 Lambda1 0.57270396\n",
      "55 Train Loss 113.43942 Test MSE 110.27547098822309 Test RE 0.1767781689863767 Lambda1 0.5827027\n",
      "56 Train Loss 108.53325 Test MSE 105.54819729156378 Test RE 0.1729476163476256 Lambda1 0.59787256\n",
      "57 Train Loss 104.32985 Test MSE 103.83865719365657 Test RE 0.17154130209898083 Lambda1 0.6154131\n",
      "58 Train Loss 100.426384 Test MSE 98.96225720923375 Test RE 0.1674649664814576 Lambda1 0.64817554\n",
      "59 Train Loss 96.5266 Test MSE 93.02236238612203 Test RE 0.1623614244755672 Lambda1 0.68449837\n",
      "60 Train Loss 91.91779 Test MSE 88.77875457436475 Test RE 0.1586147951881929 Lambda1 0.69609964\n",
      "61 Train Loss 88.59341 Test MSE 88.25973943736636 Test RE 0.1581504715322768 Lambda1 0.70636886\n",
      "62 Train Loss 85.48219 Test MSE 87.01142702170748 Test RE 0.1570280784583153 Lambda1 0.7238146\n",
      "63 Train Loss 80.8003 Test MSE 82.43758976235284 Test RE 0.15284520315562894 Lambda1 0.7374312\n",
      "64 Train Loss 79.06497 Test MSE 80.25889554487033 Test RE 0.15081195164549313 Lambda1 0.75225735\n",
      "65 Train Loss 77.196014 Test MSE 76.87531644788413 Test RE 0.14759873279718752 Lambda1 0.7732115\n",
      "66 Train Loss 76.50462 Test MSE 75.83552771150607 Test RE 0.1465971499939568 Lambda1 0.7777296\n",
      "67 Train Loss 74.06804 Test MSE 72.92555676150627 Test RE 0.1437570153191898 Lambda1 0.8070736\n",
      "68 Train Loss 73.04694 Test MSE 71.87339498878148 Test RE 0.14271619239463215 Lambda1 0.81817615\n",
      "69 Train Loss 72.00443 Test MSE 69.93099578148289 Test RE 0.14077451080760117 Lambda1 0.8311628\n",
      "70 Train Loss 69.66529 Test MSE 66.63399248086422 Test RE 0.1374159322027193 Lambda1 0.8671192\n",
      "71 Train Loss 68.32575 Test MSE 66.5562759945266 Test RE 0.1373357734222079 Lambda1 0.87859225\n",
      "72 Train Loss 66.255516 Test MSE 62.404054702859526 Test RE 0.13298283127433863 Lambda1 0.897158\n",
      "73 Train Loss 63.489426 Test MSE 58.526033665047706 Test RE 0.128784535834904 Lambda1 0.9161593\n",
      "74 Train Loss 62.247707 Test MSE 57.71332559137754 Test RE 0.1278872417659936 Lambda1 0.91765064\n",
      "Training time: 143.48\n",
      "Training time: 143.48\n",
      "inv_HT_stan_tune2\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 839.76526 Test MSE 857.0613123258904 Test RE 0.49282770736155546 Lambda1 -0.02679054\n",
      "1 Train Loss 835.1545 Test MSE 853.778581381416 Test RE 0.49188298329859237 Lambda1 -0.0331447\n",
      "2 Train Loss 808.34796 Test MSE 811.1220471982418 Test RE 0.47943780206807524 Lambda1 -0.02045533\n",
      "3 Train Loss 656.273 Test MSE 664.5529222852703 Test RE 0.4339642528970924 Lambda1 0.002497123\n",
      "4 Train Loss 622.7932 Test MSE 625.8406578370597 Test RE 0.42113472936856705 Lambda1 0.0018726413\n",
      "5 Train Loss 544.49554 Test MSE 544.6350560390691 Test RE 0.39286375787771255 Lambda1 0.0033895236\n",
      "6 Train Loss 417.63745 Test MSE 406.10405655634787 Test RE 0.33924060922281324 Lambda1 -0.001990587\n",
      "7 Train Loss 297.21973 Test MSE 313.21818528297257 Test RE 0.2979289151025156 Lambda1 -0.0033675362\n",
      "8 Train Loss 269.04184 Test MSE 293.31110867706394 Test RE 0.28830583117832514 Lambda1 -0.0011265167\n",
      "9 Train Loss 264.04065 Test MSE 288.92108675228 Test RE 0.28614014342838373 Lambda1 -0.0002871109\n",
      "10 Train Loss 260.39066 Test MSE 283.88955333366295 Test RE 0.28363764862180674 Lambda1 -5.581311e-05\n",
      "11 Train Loss 258.5922 Test MSE 281.314724438002 Test RE 0.2823484466720776 Lambda1 0.00046561708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 Train Loss 256.33896 Test MSE 281.7968970415442 Test RE 0.28259031527612505 Lambda1 0.0009362082\n",
      "13 Train Loss 255.72685 Test MSE 281.1088889365933 Test RE 0.2822451318365583 Lambda1 0.0013005417\n",
      "14 Train Loss 255.51497 Test MSE 280.94393183439485 Test RE 0.2821623077546764 Lambda1 0.0014362143\n",
      "15 Train Loss 255.11107 Test MSE 281.0427271207301 Test RE 0.282211915260927 Lambda1 0.0015179492\n",
      "16 Train Loss 254.68642 Test MSE 281.11098433882137 Test RE 0.2822461837705229 Lambda1 0.0018644975\n",
      "17 Train Loss 254.56117 Test MSE 281.0036806542926 Test RE 0.28219231012657736 Lambda1 0.0020224205\n",
      "18 Train Loss 254.43027 Test MSE 280.685171490418 Test RE 0.2820323365627307 Lambda1 0.0023031656\n",
      "19 Train Loss 254.33595 Test MSE 280.65845908878737 Test RE 0.2820189159387271 Lambda1 0.0025920272\n",
      "20 Train Loss 254.16858 Test MSE 280.6894900430994 Test RE 0.2820345061943005 Lambda1 0.003252176\n",
      "21 Train Loss 253.96378 Test MSE 280.582828053554 Test RE 0.28198091455481034 Lambda1 0.0039024306\n",
      "22 Train Loss 253.80072 Test MSE 280.1899406583006 Test RE 0.28178342285977526 Lambda1 0.004370312\n",
      "23 Train Loss 253.06659 Test MSE 278.4706683707081 Test RE 0.280917567549753 Lambda1 0.0066809678\n",
      "24 Train Loss 252.38445 Test MSE 276.4044332164972 Test RE 0.279873431589976 Lambda1 0.009230804\n",
      "25 Train Loss 251.73329 Test MSE 274.1951523964415 Test RE 0.27875268344434884 Lambda1 0.013344677\n",
      "26 Train Loss 250.15886 Test MSE 272.14159701068553 Test RE 0.2777068773864895 Lambda1 0.018848324\n",
      "27 Train Loss 247.90723 Test MSE 268.09125216217467 Test RE 0.2756325431701062 Lambda1 0.02711039\n",
      "28 Train Loss 246.14113 Test MSE 267.17272135894274 Test RE 0.27515995368899365 Lambda1 0.031193102\n",
      "29 Train Loss 243.51659 Test MSE 263.87281428169814 Test RE 0.2734553944514602 Lambda1 0.041475825\n",
      "30 Train Loss 242.2067 Test MSE 261.5084941160423 Test RE 0.2722275474619293 Lambda1 0.05237847\n",
      "31 Train Loss 238.06041 Test MSE 252.94686038361988 Test RE 0.26773417925073306 Lambda1 0.08119595\n",
      "32 Train Loss 233.57133 Test MSE 248.69243516787793 Test RE 0.2654730614081727 Lambda1 0.10017343\n",
      "33 Train Loss 228.45973 Test MSE 243.8262549955303 Test RE 0.2628629665007629 Lambda1 0.1220112\n",
      "34 Train Loss 225.72417 Test MSE 244.4695546467752 Test RE 0.2632095006864282 Lambda1 0.12639093\n",
      "35 Train Loss 221.37851 Test MSE 241.8472063627666 Test RE 0.2617940116698387 Lambda1 0.13975625\n",
      "36 Train Loss 219.57841 Test MSE 241.10926425345525 Test RE 0.2613943038928557 Lambda1 0.15183276\n",
      "37 Train Loss 217.10297 Test MSE 238.1375590029477 Test RE 0.2597784489527345 Lambda1 0.16497938\n",
      "38 Train Loss 211.6668 Test MSE 230.27918945650597 Test RE 0.25545624064391265 Lambda1 0.19580026\n",
      "39 Train Loss 207.43349 Test MSE 225.72588304503364 Test RE 0.25291806527309313 Lambda1 0.22811492\n",
      "40 Train Loss 200.83044 Test MSE 214.09434782531503 Test RE 0.24631551693003126 Lambda1 0.2544889\n",
      "41 Train Loss 194.83899 Test MSE 204.9691846615813 Test RE 0.2410091091418866 Lambda1 0.26031289\n",
      "42 Train Loss 184.81133 Test MSE 187.82728139464584 Test RE 0.23071110907022846 Lambda1 0.2890238\n",
      "43 Train Loss 177.68097 Test MSE 184.26058276298437 Test RE 0.22851009497643193 Lambda1 0.30665916\n",
      "44 Train Loss 169.5276 Test MSE 170.45451308310794 Test RE 0.21978265876418154 Lambda1 0.32453948\n",
      "45 Train Loss 165.06367 Test MSE 167.28971960297156 Test RE 0.21773276976917638 Lambda1 0.33656493\n",
      "46 Train Loss 157.6178 Test MSE 157.79460649748384 Test RE 0.21146341815564362 Lambda1 0.36206403\n",
      "47 Train Loss 155.02753 Test MSE 153.61124966994254 Test RE 0.20864149298827525 Lambda1 0.3702057\n",
      "48 Train Loss 153.35732 Test MSE 149.8229254468489 Test RE 0.20605269877257995 Lambda1 0.3861326\n",
      "49 Train Loss 147.47714 Test MSE 145.4150858026474 Test RE 0.2029990018931129 Lambda1 0.41066036\n",
      "50 Train Loss 143.83339 Test MSE 140.55694618613444 Test RE 0.19957922280249465 Lambda1 0.42441693\n",
      "51 Train Loss 137.27159 Test MSE 132.28707346096917 Test RE 0.19361895643679986 Lambda1 0.45715797\n",
      "52 Train Loss 128.48413 Test MSE 125.85909212346755 Test RE 0.1888562961134826 Lambda1 0.48911858\n",
      "53 Train Loss 124.99125 Test MSE 124.43500872612735 Test RE 0.18778481123467433 Lambda1 0.50217235\n",
      "54 Train Loss 121.48161 Test MSE 122.22942369317676 Test RE 0.18611314698892936 Lambda1 0.5117339\n",
      "55 Train Loss 119.53247 Test MSE 119.26519676402924 Test RE 0.18384254993367044 Lambda1 0.52902\n",
      "56 Train Loss 117.1089 Test MSE 117.86122623305913 Test RE 0.18275726420295066 Lambda1 0.54455197\n",
      "57 Train Loss 114.37044 Test MSE 114.01021712899045 Test RE 0.17974675429168566 Lambda1 0.5456537\n",
      "58 Train Loss 110.15153 Test MSE 106.5058797183817 Test RE 0.1737304572069713 Lambda1 0.5612217\n",
      "59 Train Loss 108.20614 Test MSE 104.24706105972953 Test RE 0.17187831233318657 Lambda1 0.5683632\n",
      "60 Train Loss 104.22149 Test MSE 97.88589362497667 Test RE 0.166551759711511 Lambda1 0.5940658\n",
      "61 Train Loss 101.93795 Test MSE 96.36529926256877 Test RE 0.16525305915031074 Lambda1 0.6038684\n",
      "62 Train Loss 99.016136 Test MSE 96.38797566875883 Test RE 0.16527250144486283 Lambda1 0.609481\n",
      "63 Train Loss 97.45509 Test MSE 93.36634650133496 Test RE 0.16266134276159985 Lambda1 0.6181774\n",
      "64 Train Loss 95.65501 Test MSE 90.33904243959745 Test RE 0.16000255291324575 Lambda1 0.63661826\n",
      "65 Train Loss 93.80078 Test MSE 86.88287830699605 Test RE 0.15691204071845477 Lambda1 0.66199774\n",
      "66 Train Loss 92.02576 Test MSE 84.67571043588057 Test RE 0.15490612679938107 Lambda1 0.673116\n",
      "67 Train Loss 89.752884 Test MSE 83.48746053778741 Test RE 0.15381539186005447 Lambda1 0.6760398\n",
      "68 Train Loss 86.31703 Test MSE 80.93406388077588 Test RE 0.15144496687303144 Lambda1 0.68721104\n",
      "69 Train Loss 84.804665 Test MSE 79.17495195890099 Test RE 0.1497900876472451 Lambda1 0.6958876\n",
      "70 Train Loss 84.29835 Test MSE 79.30662900747396 Test RE 0.14991459497049647 Lambda1 0.69832426\n",
      "71 Train Loss 81.5776 Test MSE 79.08357833390485 Test RE 0.14970362839374315 Lambda1 0.70829433\n",
      "72 Train Loss 78.00775 Test MSE 75.34443024791354 Test RE 0.1461217104215742 Lambda1 0.72933286\n",
      "73 Train Loss 77.253006 Test MSE 74.27558963311931 Test RE 0.14508156266345762 Lambda1 0.7376909\n",
      "74 Train Loss 76.11344 Test MSE 72.23827863385898 Test RE 0.14307800137121918 Lambda1 0.7543899\n",
      "Training time: 142.06\n",
      "Training time: 142.06\n",
      "inv_HT_stan_tune3\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 3061.6855 Test MSE 3013.821192859852 Test RE 0.9241615845781496 Lambda1 -0.00258779\n",
      "1 Train Loss 865.7344 Test MSE 860.3681896948498 Test RE 0.4937775529381963 Lambda1 -0.02303295\n",
      "2 Train Loss 835.27466 Test MSE 852.6071853914272 Test RE 0.4915454323588541 Lambda1 -0.023196723\n",
      "3 Train Loss 826.34344 Test MSE 842.1845235310951 Test RE 0.4885317553223443 Lambda1 -0.022665413\n",
      "4 Train Loss 806.88184 Test MSE 824.7058238169413 Test RE 0.48343568106869905 Lambda1 -0.02073954\n",
      "5 Train Loss 777.9844 Test MSE 782.6193086267936 Test RE 0.4709387751155075 Lambda1 -0.0076532876\n",
      "6 Train Loss 745.4876 Test MSE 761.1130484774535 Test RE 0.4644230372935849 Lambda1 0.0043259156\n",
      "7 Train Loss 674.52405 Test MSE 695.0123844396032 Test RE 0.4437981038069446 Lambda1 0.0119373435\n",
      "8 Train Loss 502.37012 Test MSE 504.42556866390083 Test RE 0.37808349312550416 Lambda1 0.030902907\n",
      "9 Train Loss 343.3414 Test MSE 345.8381103741822 Test RE 0.3130585687754621 Lambda1 -0.004074299\n",
      "10 Train Loss 301.96848 Test MSE 314.3566527889027 Test RE 0.2984698714606241 Lambda1 0.00094015745\n",
      "11 Train Loss 275.9685 Test MSE 290.86602170558643 Test RE 0.2871016352349005 Lambda1 -0.00011222492\n",
      "12 Train Loss 265.43802 Test MSE 283.05645077583364 Test RE 0.283221161182186 Lambda1 -9.281223e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Train Loss 261.14398 Test MSE 281.0027578553863 Test RE 0.2821918467750505 Lambda1 0.0010659624\n",
      "14 Train Loss 259.44403 Test MSE 280.13302314760676 Test RE 0.28175480080158183 Lambda1 0.0013800602\n",
      "15 Train Loss 258.2943 Test MSE 279.4194746627773 Test RE 0.2813957322829453 Lambda1 0.0015553989\n",
      "16 Train Loss 256.86508 Test MSE 278.2782151655376 Test RE 0.28082047863419274 Lambda1 0.004208129\n",
      "17 Train Loss 255.58664 Test MSE 276.90607221099947 Test RE 0.2801272838580854 Lambda1 0.0072228196\n",
      "18 Train Loss 253.92223 Test MSE 275.32045914809225 Test RE 0.2793241032741379 Lambda1 0.010893025\n",
      "19 Train Loss 251.11116 Test MSE 270.8944884333385 Test RE 0.27706984079301866 Lambda1 0.019539176\n",
      "20 Train Loss 246.60097 Test MSE 264.47971706264264 Test RE 0.27376968511199284 Lambda1 0.033270296\n",
      "21 Train Loss 239.65512 Test MSE 254.8298709387222 Test RE 0.26872887732230916 Lambda1 0.053099398\n",
      "22 Train Loss 230.61133 Test MSE 243.90688406654186 Test RE 0.26290642499732314 Lambda1 0.085047215\n",
      "23 Train Loss 223.60712 Test MSE 235.2979462331254 Test RE 0.25822497193024535 Lambda1 0.106282406\n",
      "24 Train Loss 216.14632 Test MSE 229.09197471096766 Test RE 0.2547968816228035 Lambda1 0.120751634\n",
      "25 Train Loss 210.59007 Test MSE 222.87844253547863 Test RE 0.2513177730642526 Lambda1 0.13564704\n",
      "26 Train Loss 205.41069 Test MSE 218.23590064661485 Test RE 0.24868653327723483 Lambda1 0.14822485\n",
      "27 Train Loss 196.30492 Test MSE 208.806425189224 Test RE 0.24325462135089368 Lambda1 0.17062406\n",
      "28 Train Loss 186.93712 Test MSE 195.22995231346943 Test RE 0.23521358147083485 Lambda1 0.20101191\n",
      "29 Train Loss 172.9402 Test MSE 177.82347010924855 Test RE 0.22448313101273548 Lambda1 0.24502517\n",
      "30 Train Loss 160.91782 Test MSE 162.96527660934058 Test RE 0.2149001455792813 Lambda1 0.28802735\n",
      "31 Train Loss 146.9776 Test MSE 145.36705371048996 Test RE 0.20296547279943003 Lambda1 0.3346539\n",
      "32 Train Loss 140.20912 Test MSE 134.86917369693904 Test RE 0.19549944057479005 Lambda1 0.3671228\n",
      "33 Train Loss 136.0061 Test MSE 127.79609899022067 Test RE 0.19030402293846413 Lambda1 0.38584712\n",
      "34 Train Loss 132.01755 Test MSE 123.90817286571297 Test RE 0.18738686570957988 Lambda1 0.39440942\n",
      "35 Train Loss 116.57272 Test MSE 114.38544090992443 Test RE 0.18004229734304003 Lambda1 0.40883884\n",
      "36 Train Loss 113.25163 Test MSE 111.15092834659035 Test RE 0.17747848703610133 Lambda1 0.41356078\n",
      "37 Train Loss 110.70515 Test MSE 108.53067169236303 Test RE 0.17537408415652941 Lambda1 0.41169655\n",
      "38 Train Loss 107.06968 Test MSE 102.98687887204284 Test RE 0.17083628508634946 Lambda1 0.42085928\n",
      "39 Train Loss 101.958145 Test MSE 96.80498153970736 Test RE 0.16562962701765152 Lambda1 0.43546516\n",
      "40 Train Loss 97.8244 Test MSE 93.5777119248482 Test RE 0.1628453573917014 Lambda1 0.4402386\n",
      "41 Train Loss 94.395386 Test MSE 88.34039726868312 Test RE 0.15822271942878577 Lambda1 0.45209223\n",
      "42 Train Loss 92.02121 Test MSE 84.995659890549 Test RE 0.15519850935407514 Lambda1 0.45628098\n",
      "43 Train Loss 89.1783 Test MSE 82.04720310635935 Test RE 0.15248287125937673 Lambda1 0.4604284\n",
      "44 Train Loss 85.900345 Test MSE 79.51874058946365 Test RE 0.1501149400660507 Lambda1 0.46165285\n",
      "45 Train Loss 83.39752 Test MSE 78.1035919630889 Test RE 0.14877318968311015 Lambda1 0.46231008\n",
      "46 Train Loss 80.79765 Test MSE 77.44646481799343 Test RE 0.14814601363771832 Lambda1 0.46541035\n",
      "47 Train Loss 78.327385 Test MSE 78.15290268429064 Test RE 0.14882014626695966 Lambda1 0.46749032\n",
      "48 Train Loss 75.0315 Test MSE 75.24107737124092 Test RE 0.1460214556206844 Lambda1 0.47878218\n",
      "49 Train Loss 73.11408 Test MSE 72.58420370748554 Test RE 0.14342016869335966 Lambda1 0.48845264\n",
      "50 Train Loss 69.77034 Test MSE 68.37017318006446 Test RE 0.13919463951684513 Lambda1 0.5044279\n",
      "51 Train Loss 68.16389 Test MSE 65.89011403198737 Test RE 0.1366467478912487 Lambda1 0.5117628\n",
      "52 Train Loss 66.19335 Test MSE 62.80924067320492 Test RE 0.13341385772076303 Lambda1 0.5260027\n",
      "53 Train Loss 64.395424 Test MSE 60.892874302171705 Test RE 0.13136280331175887 Lambda1 0.5337667\n",
      "54 Train Loss 63.21234 Test MSE 57.574507756866026 Test RE 0.12773334560788593 Lambda1 0.5467355\n",
      "55 Train Loss 59.21341 Test MSE 51.396029614554585 Test RE 0.1206851837408318 Lambda1 0.57103395\n",
      "56 Train Loss 56.65193 Test MSE 51.833728883808355 Test RE 0.12119798432903381 Lambda1 0.57085496\n",
      "57 Train Loss 52.26053 Test MSE 47.42019437920075 Test RE 0.11592332664322408 Lambda1 0.5944223\n",
      "58 Train Loss 48.813164 Test MSE 45.70040695404513 Test RE 0.11380181912770125 Lambda1 0.6095337\n",
      "59 Train Loss 47.5221 Test MSE 45.30140696753066 Test RE 0.11330394097113171 Lambda1 0.6150538\n",
      "60 Train Loss 46.69549 Test MSE 44.61700355942693 Test RE 0.1124447985354124 Lambda1 0.6184153\n",
      "61 Train Loss 46.3654 Test MSE 44.456917311515014 Test RE 0.11224289073893355 Lambda1 0.61965644\n",
      "62 Train Loss 45.419277 Test MSE 43.76823658746854 Test RE 0.1113701219726166 Lambda1 0.63281494\n",
      "63 Train Loss 44.504444 Test MSE 43.10837793269906 Test RE 0.11052741463068937 Lambda1 0.64173037\n",
      "64 Train Loss 43.681934 Test MSE 42.5543140459502 Test RE 0.10981482347287211 Lambda1 0.6470752\n",
      "65 Train Loss 42.650978 Test MSE 42.087951041142674 Test RE 0.10921142212738334 Lambda1 0.6584806\n",
      "66 Train Loss 41.51653 Test MSE 41.409952588930985 Test RE 0.10832820258726317 Lambda1 0.67084575\n",
      "67 Train Loss 41.010094 Test MSE 41.13647742217009 Test RE 0.10796990530275358 Lambda1 0.67768216\n",
      "68 Train Loss 40.66958 Test MSE 40.864014358509 Test RE 0.10761174769605826 Lambda1 0.6820144\n",
      "69 Train Loss 40.368084 Test MSE 40.62112213915511 Test RE 0.10729145350696132 Lambda1 0.6856053\n",
      "70 Train Loss 40.24111 Test MSE 40.578950210626445 Test RE 0.10723574526654724 Lambda1 0.68647903\n",
      "71 Train Loss 40.095154 Test MSE 40.4870704362324 Test RE 0.10711427367124132 Lambda1 0.68901217\n",
      "72 Train Loss 39.93795 Test MSE 40.24626736206857 Test RE 0.1067952593141883 Lambda1 0.6945516\n",
      "73 Train Loss 39.525803 Test MSE 39.88429902776831 Test RE 0.1063139250775211 Lambda1 0.70727885\n",
      "74 Train Loss 39.056374 Test MSE 39.79216532692002 Test RE 0.10619106020489981 Lambda1 0.722368\n",
      "Training time: 143.87\n",
      "Training time: 143.87\n",
      "inv_HT_stan_tune3\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 925.8536 Test MSE 855.1334143169613 Test RE 0.4922731049944468 Lambda1 0.023954362\n",
      "1 Train Loss 835.8296 Test MSE 848.493078413689 Test RE 0.4903580649758314 Lambda1 0.020967182\n",
      "2 Train Loss 814.60315 Test MSE 826.9675861077737 Test RE 0.48409814028278275 Lambda1 0.017703118\n",
      "3 Train Loss 781.9685 Test MSE 796.3196387653895 Test RE 0.4750429572341623 Lambda1 0.021573605\n",
      "4 Train Loss 731.1944 Test MSE 749.621975000396 Test RE 0.46090383877209556 Lambda1 0.03012542\n",
      "5 Train Loss 609.17053 Test MSE 621.7387681130649 Test RE 0.41975235829307345 Lambda1 0.059790283\n",
      "6 Train Loss 494.694 Test MSE 481.0750569978402 Test RE 0.3692288185332107 Lambda1 0.06693115\n",
      "7 Train Loss 389.52844 Test MSE 387.76962388786404 Test RE 0.3314942986037073 Lambda1 0.0463077\n",
      "8 Train Loss 324.72778 Test MSE 329.77457404531316 Test RE 0.305701630189932 Lambda1 0.012616311\n",
      "9 Train Loss 288.11386 Test MSE 306.2513967098434 Test RE 0.29459692519519187 Lambda1 0.00054748775\n",
      "10 Train Loss 272.1263 Test MSE 295.02633575361216 Test RE 0.28914758096148846 Lambda1 -0.0012351695\n",
      "11 Train Loss 263.84167 Test MSE 287.0133674007594 Test RE 0.2851939003330784 Lambda1 -0.0006145258\n",
      "12 Train Loss 260.62192 Test MSE 282.2203142300177 Test RE 0.282802540259884 Lambda1 0.0014303795\n",
      "13 Train Loss 257.45624 Test MSE 281.0968787284797 Test RE 0.28223910239545275 Lambda1 0.0020310483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 Train Loss 256.01715 Test MSE 280.9528527368462 Test RE 0.2821667875140804 Lambda1 0.002495436\n",
      "15 Train Loss 254.94402 Test MSE 281.05387879557406 Test RE 0.2822175142392917 Lambda1 0.002173485\n",
      "16 Train Loss 254.39688 Test MSE 281.2811031210245 Test RE 0.28233157373679796 Lambda1 0.001679989\n",
      "17 Train Loss 253.97911 Test MSE 281.2451636866812 Test RE 0.28231353633385453 Lambda1 0.0015680598\n",
      "18 Train Loss 253.76277 Test MSE 281.22811225411186 Test RE 0.2823049781010102 Lambda1 0.0016816326\n",
      "19 Train Loss 253.51787 Test MSE 280.7841811330402 Test RE 0.2820820745995624 Lambda1 0.0015954929\n",
      "20 Train Loss 253.14886 Test MSE 280.3589978322623 Test RE 0.2818684193544314 Lambda1 0.0019269865\n",
      "21 Train Loss 253.00761 Test MSE 280.2330467480275 Test RE 0.2818050976464238 Lambda1 0.002433244\n",
      "22 Train Loss 252.85553 Test MSE 280.38399891881136 Test RE 0.28188098691906166 Lambda1 0.002631047\n",
      "23 Train Loss 252.69102 Test MSE 280.8273239545877 Test RE 0.2821037448893849 Lambda1 0.0025314696\n",
      "24 Train Loss 252.45844 Test MSE 280.65270169306984 Test RE 0.2820160232719047 Lambda1 0.00306687\n",
      "25 Train Loss 252.35295 Test MSE 280.25126802044343 Test RE 0.2818142592425406 Lambda1 0.0036290716\n",
      "26 Train Loss 252.03276 Test MSE 279.17492900706463 Test RE 0.2812725676987167 Lambda1 0.0049143587\n",
      "27 Train Loss 251.71059 Test MSE 278.2411166506941 Test RE 0.2808017592925975 Lambda1 0.0059410944\n",
      "28 Train Loss 251.38899 Test MSE 277.20687698844864 Test RE 0.28027939455938317 Lambda1 0.0076899524\n",
      "29 Train Loss 250.67749 Test MSE 276.19322265706063 Test RE 0.2797664804819028 Lambda1 0.00997855\n",
      "30 Train Loss 250.0631 Test MSE 275.0616075630954 Test RE 0.27919276452491615 Lambda1 0.011819737\n",
      "31 Train Loss 248.96715 Test MSE 272.1315723302877 Test RE 0.27770176249711126 Lambda1 0.017095076\n",
      "32 Train Loss 246.70068 Test MSE 267.7406095128448 Test RE 0.2754522311545446 Lambda1 0.025857737\n",
      "33 Train Loss 244.74992 Test MSE 265.1164744577676 Test RE 0.2740990488797027 Lambda1 0.030798825\n",
      "34 Train Loss 241.60962 Test MSE 260.02928676213185 Test RE 0.2714565360976125 Lambda1 0.03782446\n",
      "35 Train Loss 238.18782 Test MSE 256.20760020197065 Test RE 0.2694543349939572 Lambda1 0.045900382\n",
      "36 Train Loss 231.91199 Test MSE 245.38548461858025 Test RE 0.26370211025465856 Lambda1 0.06439871\n",
      "37 Train Loss 228.82619 Test MSE 242.22096531548277 Test RE 0.2619962262869258 Lambda1 0.07364187\n",
      "38 Train Loss 223.60062 Test MSE 236.46511162493957 Test RE 0.25886462566772245 Lambda1 0.08495693\n",
      "39 Train Loss 218.20639 Test MSE 228.86805761132422 Test RE 0.25467233049704796 Lambda1 0.098582536\n",
      "40 Train Loss 210.33592 Test MSE 223.37796039992296 Test RE 0.2515992436619018 Lambda1 0.11275603\n",
      "41 Train Loss 205.8279 Test MSE 217.88651987829832 Test RE 0.248487388465526 Lambda1 0.124466054\n",
      "42 Train Loss 201.32387 Test MSE 209.09770774736427 Test RE 0.2434242309181683 Lambda1 0.14122781\n",
      "43 Train Loss 197.69156 Test MSE 203.50571026606804 Test RE 0.24014716859038587 Lambda1 0.1527513\n",
      "44 Train Loss 193.87338 Test MSE 200.38161173809613 Test RE 0.23829674126658237 Lambda1 0.16019848\n",
      "45 Train Loss 184.98096 Test MSE 189.74993220236584 Test RE 0.23188891329168218 Lambda1 0.18619888\n",
      "46 Train Loss 171.97653 Test MSE 175.79351401079097 Test RE 0.22319815195577256 Lambda1 0.22284888\n",
      "47 Train Loss 168.91376 Test MSE 170.6162950482646 Test RE 0.21988693420065203 Lambda1 0.23242575\n",
      "48 Train Loss 163.54605 Test MSE 159.25431234051152 Test RE 0.21243925571928374 Lambda1 0.25583747\n",
      "49 Train Loss 154.88055 Test MSE 149.60546469224485 Test RE 0.2059031066913302 Lambda1 0.27863476\n",
      "50 Train Loss 148.24968 Test MSE 143.92683627816018 Test RE 0.2019575344925782 Lambda1 0.2932492\n",
      "51 Train Loss 145.10661 Test MSE 142.7009044365077 Test RE 0.20109558388330348 Lambda1 0.29659033\n",
      "52 Train Loss 139.47702 Test MSE 138.58182940279917 Test RE 0.19817201062210657 Lambda1 0.30499977\n",
      "53 Train Loss 134.99307 Test MSE 135.47592489509887 Test RE 0.19593870480417538 Lambda1 0.31037492\n",
      "54 Train Loss 132.00105 Test MSE 130.15900983132352 Test RE 0.19205529651959194 Lambda1 0.31861058\n",
      "55 Train Loss 126.57588 Test MSE 120.77269381480859 Test RE 0.18500077479905935 Lambda1 0.33804235\n",
      "56 Train Loss 120.42181 Test MSE 112.07663598182266 Test RE 0.17821600914169977 Lambda1 0.35663554\n",
      "57 Train Loss 116.65881 Test MSE 107.64290781478833 Test RE 0.17465534508981537 Lambda1 0.36745572\n",
      "58 Train Loss 112.24645 Test MSE 104.44741325422197 Test RE 0.1720433993240313 Lambda1 0.3770952\n",
      "59 Train Loss 108.49912 Test MSE 99.85039263978076 Test RE 0.16821474405305026 Lambda1 0.38550434\n",
      "60 Train Loss 104.80733 Test MSE 96.3893437401364 Test RE 0.16527367432859064 Lambda1 0.38904053\n",
      "61 Train Loss 101.55591 Test MSE 90.79125629883549 Test RE 0.16040251868578306 Lambda1 0.3988626\n",
      "62 Train Loss 97.655624 Test MSE 85.84596562029502 Test RE 0.1559728887261027 Lambda1 0.41110343\n",
      "63 Train Loss 93.722916 Test MSE 80.46439750111946 Test RE 0.1510049043236798 Lambda1 0.4261128\n",
      "64 Train Loss 91.00115 Test MSE 79.77921532351199 Test RE 0.15036060027658257 Lambda1 0.42905286\n",
      "65 Train Loss 87.16507 Test MSE 78.51234876842783 Test RE 0.14916198547623535 Lambda1 0.43521464\n",
      "66 Train Loss 85.56962 Test MSE 77.01120523442914 Test RE 0.1477291267779444 Lambda1 0.43875626\n",
      "67 Train Loss 82.85962 Test MSE 75.40783467685854 Test RE 0.146183180229336 Lambda1 0.44799843\n",
      "68 Train Loss 81.31661 Test MSE 73.94652247539673 Test RE 0.14475982457169873 Lambda1 0.4503439\n",
      "69 Train Loss 78.98308 Test MSE 72.69536447443208 Test RE 0.14352994875465372 Lambda1 0.45692936\n",
      "70 Train Loss 77.34709 Test MSE 71.94505885208257 Test RE 0.14278732473456204 Lambda1 0.46013042\n",
      "71 Train Loss 75.13514 Test MSE 70.57057938242825 Test RE 0.14141680208033228 Lambda1 0.46505606\n",
      "72 Train Loss 73.834526 Test MSE 70.33611775199626 Test RE 0.14118168710800236 Lambda1 0.4674477\n",
      "73 Train Loss 72.531525 Test MSE 69.59563291615194 Test RE 0.14043655422676227 Lambda1 0.47351062\n",
      "74 Train Loss 71.17784 Test MSE 68.13150703168843 Test RE 0.13895147726049306 Lambda1 0.48059127\n",
      "Training time: 142.72\n",
      "Training time: 142.72\n",
      "inv_HT_stan_tune3\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 3124.2747 Test MSE 3055.808985870197 Test RE 0.9305769101759552 Lambda1 0.025650576\n",
      "1 Train Loss 851.6822 Test MSE 851.1253819516446 Test RE 0.49111810165328235 Lambda1 0.088707216\n",
      "2 Train Loss 797.8572 Test MSE 815.9945985814663 Test RE 0.4808756790891707 Lambda1 0.0828538\n",
      "3 Train Loss 742.0081 Test MSE 758.3758244103738 Test RE 0.4635871726856418 Lambda1 0.07662171\n",
      "4 Train Loss 687.2515 Test MSE 703.3728965294479 Test RE 0.4464594144482331 Lambda1 0.071973085\n",
      "5 Train Loss 512.8336 Test MSE 517.0282077995215 Test RE 0.38277740118461223 Lambda1 0.0059655095\n",
      "6 Train Loss 364.2778 Test MSE 363.5214013669114 Test RE 0.3209624027968422 Lambda1 -0.010514175\n",
      "7 Train Loss 301.56073 Test MSE 314.30470988326886 Test RE 0.2984452115198987 Lambda1 0.003200775\n",
      "8 Train Loss 273.22824 Test MSE 293.24015368806545 Test RE 0.28827095698877353 Lambda1 0.002376831\n",
      "9 Train Loss 263.17957 Test MSE 284.3659765049272 Test RE 0.28387554906508583 Lambda1 0.0027437694\n",
      "10 Train Loss 258.77786 Test MSE 280.8471376830847 Test RE 0.2821136966069421 Lambda1 0.002685678\n",
      "11 Train Loss 255.7336 Test MSE 278.346867896282 Test RE 0.2808551164583905 Lambda1 0.004047686\n",
      "12 Train Loss 254.7986 Test MSE 276.9906473850152 Test RE 0.2801700601049613 Lambda1 0.006370288\n",
      "13 Train Loss 252.91707 Test MSE 273.3378935665091 Test RE 0.27831658844661794 Lambda1 0.015545005\n",
      "14 Train Loss 248.99368 Test MSE 268.84800399763543 Test RE 0.2760212885003972 Lambda1 0.024994597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Train Loss 245.61009 Test MSE 265.23391033454055 Test RE 0.27415974955545225 Lambda1 0.032066077\n",
      "16 Train Loss 240.68794 Test MSE 258.0365615983614 Test RE 0.27041438689860847 Lambda1 0.047881566\n",
      "17 Train Loss 236.02303 Test MSE 251.638679454776 Test RE 0.26704095307829706 Lambda1 0.065855175\n",
      "18 Train Loss 227.82645 Test MSE 240.09273809724547 Test RE 0.26084269753901806 Lambda1 0.094757035\n",
      "19 Train Loss 223.72266 Test MSE 236.9105031225679 Test RE 0.2591083019174263 Lambda1 0.10366732\n",
      "20 Train Loss 217.20697 Test MSE 229.77448141927704 Test RE 0.25517614251437976 Lambda1 0.12104954\n",
      "21 Train Loss 207.18332 Test MSE 218.86116411320586 Test RE 0.24904253196157067 Lambda1 0.14266282\n",
      "22 Train Loss 199.96274 Test MSE 214.19958818821598 Test RE 0.24637604900347404 Lambda1 0.15503477\n",
      "23 Train Loss 196.63004 Test MSE 210.2641395320884 Test RE 0.24410224619585308 Lambda1 0.1644988\n",
      "24 Train Loss 189.4663 Test MSE 202.61512458259486 Test RE 0.23962112406147712 Lambda1 0.18372867\n",
      "25 Train Loss 176.41472 Test MSE 183.5026978362504 Test RE 0.22803966662177924 Lambda1 0.22694625\n",
      "26 Train Loss 168.76593 Test MSE 173.10203515201266 Test RE 0.22148292867905334 Lambda1 0.24698988\n",
      "27 Train Loss 160.35245 Test MSE 164.12219133332886 Test RE 0.21566160058826347 Lambda1 0.26078364\n",
      "28 Train Loss 156.22334 Test MSE 160.7114068729754 Test RE 0.21340889745811323 Lambda1 0.26913407\n",
      "29 Train Loss 148.70389 Test MSE 148.0335808096685 Test RE 0.20481855265181512 Lambda1 0.28660837\n",
      "30 Train Loss 143.74927 Test MSE 145.18428737351255 Test RE 0.20283784098012275 Lambda1 0.29216242\n",
      "31 Train Loss 139.71631 Test MSE 139.50829508771946 Test RE 0.19883333012456272 Lambda1 0.30110133\n",
      "32 Train Loss 134.81395 Test MSE 129.26912715369124 Test RE 0.19139764015714844 Lambda1 0.32031965\n",
      "33 Train Loss 130.19629 Test MSE 123.33339787812324 Test RE 0.18695174317065202 Lambda1 0.3377248\n",
      "34 Train Loss 124.49669 Test MSE 118.14231056091076 Test RE 0.18297506106878533 Lambda1 0.36489502\n",
      "35 Train Loss 115.78296 Test MSE 106.5653019320443 Test RE 0.17377891466702486 Lambda1 0.39537734\n",
      "36 Train Loss 107.877975 Test MSE 94.92958376786036 Test RE 0.16401741350877652 Lambda1 0.4198884\n",
      "37 Train Loss 99.36421 Test MSE 88.48030029735438 Test RE 0.15834795703944884 Lambda1 0.43534327\n",
      "38 Train Loss 93.9195 Test MSE 86.63922320618109 Test RE 0.1566918634963713 Lambda1 0.44345888\n",
      "39 Train Loss 88.439 Test MSE 85.45628699774412 Test RE 0.1556184840053568 Lambda1 0.45940182\n",
      "40 Train Loss 84.59431 Test MSE 82.79424726027774 Test RE 0.15317548060240363 Lambda1 0.47079787\n",
      "41 Train Loss 80.99492 Test MSE 77.47751578310294 Test RE 0.14817570909070862 Lambda1 0.49727902\n",
      "42 Train Loss 76.29628 Test MSE 72.65640868657061 Test RE 0.1434914863881592 Lambda1 0.50938225\n",
      "43 Train Loss 69.68543 Test MSE 68.86742078067809 Test RE 0.13969989499845128 Lambda1 0.53200716\n",
      "44 Train Loss 65.98148 Test MSE 64.49012044226026 Test RE 0.13518725964908473 Lambda1 0.5550505\n",
      "45 Train Loss 63.83261 Test MSE 62.880537925355256 Test RE 0.13348955791638523 Lambda1 0.56365985\n",
      "46 Train Loss 61.70951 Test MSE 58.790079238727316 Test RE 0.1290747205325481 Lambda1 0.5786464\n",
      "47 Train Loss 58.23867 Test MSE 54.72775479861803 Test RE 0.1245354477209863 Lambda1 0.5976972\n",
      "48 Train Loss 56.91517 Test MSE 53.37567429928116 Test RE 0.1229874672424554 Lambda1 0.60590154\n",
      "49 Train Loss 55.11156 Test MSE 52.28037125247603 Test RE 0.12171903542651977 Lambda1 0.60507274\n",
      "50 Train Loss 53.202164 Test MSE 50.73087379123381 Test RE 0.11990170035521844 Lambda1 0.6050888\n",
      "51 Train Loss 51.81066 Test MSE 49.25506625907859 Test RE 0.11814480353713075 Lambda1 0.6139997\n",
      "52 Train Loss 51.01733 Test MSE 48.94341556418988 Test RE 0.1177704426804743 Lambda1 0.61801845\n",
      "53 Train Loss 50.125504 Test MSE 48.6676088550326 Test RE 0.1174381429436608 Lambda1 0.61933273\n",
      "54 Train Loss 49.374783 Test MSE 47.91550232480029 Test RE 0.1165271684259633 Lambda1 0.62206185\n",
      "55 Train Loss 48.896835 Test MSE 47.553323275604725 Test RE 0.11608593593167267 Lambda1 0.62314606\n",
      "56 Train Loss 48.41102 Test MSE 47.235653675628065 Test RE 0.1156975428184144 Lambda1 0.6260544\n",
      "57 Train Loss 47.93746 Test MSE 47.06836126742528 Test RE 0.11549248067851005 Lambda1 0.62644404\n",
      "58 Train Loss 47.188606 Test MSE 46.998168444583534 Test RE 0.11540633185518082 Lambda1 0.6247401\n",
      "59 Train Loss 46.590416 Test MSE 46.6556519896621 Test RE 0.1149850297879178 Lambda1 0.6303852\n",
      "60 Train Loss 45.967686 Test MSE 46.08975378196002 Test RE 0.11428556104893493 Lambda1 0.6421155\n",
      "61 Train Loss 45.49101 Test MSE 45.77491770178595 Test RE 0.11389455358842561 Lambda1 0.6479389\n",
      "62 Train Loss 45.292572 Test MSE 45.60203467339881 Test RE 0.11367927125406124 Lambda1 0.6523139\n",
      "63 Train Loss 44.964622 Test MSE 45.08690767653883 Test RE 0.11303537919827664 Lambda1 0.66219515\n",
      "64 Train Loss 44.649136 Test MSE 44.778560099897476 Test RE 0.1126481938345481 Lambda1 0.66532326\n",
      "65 Train Loss 44.127167 Test MSE 44.61975424671184 Test RE 0.11244826465467052 Lambda1 0.6677574\n",
      "66 Train Loss 43.75203 Test MSE 44.29043550996445 Test RE 0.11203253061071013 Lambda1 0.6765595\n",
      "67 Train Loss 43.109615 Test MSE 43.707868902494994 Test RE 0.11129329141231986 Lambda1 0.69516855\n",
      "68 Train Loss 42.808468 Test MSE 43.6642787719411 Test RE 0.1112377808214007 Lambda1 0.7010768\n",
      "69 Train Loss 42.53092 Test MSE 43.76123177414269 Test RE 0.11136120959469893 Lambda1 0.704703\n",
      "70 Train Loss 42.22532 Test MSE 43.361356134795585 Test RE 0.11085125104158731 Lambda1 0.70970154\n",
      "71 Train Loss 41.66678 Test MSE 42.548911930332714 Test RE 0.10980785295513983 Lambda1 0.72240746\n",
      "72 Train Loss 40.912815 Test MSE 42.21027548634737 Test RE 0.10937001304896421 Lambda1 0.7418424\n",
      "73 Train Loss 40.25705 Test MSE 41.260415678938784 Test RE 0.10813243183190685 Lambda1 0.75848836\n",
      "74 Train Loss 39.57589 Test MSE 40.816053082190486 Test RE 0.10754857827749856 Lambda1 0.7700996\n",
      "Training time: 141.57\n",
      "Training time: 141.57\n",
      "inv_HT_stan_tune3\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 2602.8735 Test MSE 2451.614847645658 Test RE 0.8335185932984654 Lambda1 0.07025549\n",
      "1 Train Loss 857.6286 Test MSE 854.1338733432449 Test RE 0.49198531890194414 Lambda1 0.1821364\n",
      "2 Train Loss 840.8147 Test MSE 852.789314582785 Test RE 0.4915979301522166 Lambda1 0.18073085\n",
      "3 Train Loss 834.80237 Test MSE 848.8607139550386 Test RE 0.4904642847755481 Lambda1 0.17626385\n",
      "4 Train Loss 827.86346 Test MSE 844.5824105812713 Test RE 0.48922674037693525 Lambda1 0.17280552\n",
      "5 Train Loss 819.34106 Test MSE 837.0281452077352 Test RE 0.48703391124100665 Lambda1 0.16847812\n",
      "6 Train Loss 796.25977 Test MSE 814.7914735008649 Test RE 0.48052104009560437 Lambda1 0.14930682\n",
      "7 Train Loss 770.3848 Test MSE 791.41352528533 Test RE 0.4735773300136876 Lambda1 0.14763935\n",
      "8 Train Loss 719.78394 Test MSE 730.6588912991701 Test RE 0.4550367861774042 Lambda1 0.14911187\n",
      "9 Train Loss 663.40576 Test MSE 681.2614225370294 Test RE 0.4393858524130451 Lambda1 0.14676392\n",
      "10 Train Loss 607.4083 Test MSE 594.6336309136474 Test RE 0.4105007025145312 Lambda1 0.14459649\n",
      "11 Train Loss 543.61536 Test MSE 534.7181664375453 Test RE 0.38927063195178174 Lambda1 0.13539207\n",
      "12 Train Loss 471.10605 Test MSE 461.0176586147079 Test RE 0.3614497685577192 Lambda1 0.13248728\n",
      "13 Train Loss 426.19934 Test MSE 402.4797029403767 Test RE 0.3377234075246823 Lambda1 0.13203694\n",
      "14 Train Loss 366.9444 Test MSE 330.23150337676407 Test RE 0.3059133440394909 Lambda1 0.13040894\n",
      "15 Train Loss 307.04544 Test MSE 280.55562850029446 Test RE 0.2819672466820446 Lambda1 0.12995055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 Train Loss 266.21844 Test MSE 267.4513358478248 Test RE 0.27530338819310224 Lambda1 0.13130459\n",
      "17 Train Loss 251.47835 Test MSE 265.27174063196065 Test RE 0.27417930055301587 Lambda1 0.131766\n",
      "18 Train Loss 242.6402 Test MSE 261.0728682276664 Test RE 0.27200071200884834 Lambda1 0.13638525\n",
      "19 Train Loss 237.22894 Test MSE 254.87398826366612 Test RE 0.26875213810818044 Lambda1 0.14143875\n",
      "20 Train Loss 233.60455 Test MSE 252.2904127151026 Test RE 0.26738654169843973 Lambda1 0.14500402\n",
      "21 Train Loss 229.62672 Test MSE 247.56032995545831 Test RE 0.26486812492742384 Lambda1 0.14658248\n",
      "22 Train Loss 226.10297 Test MSE 244.8187076401152 Test RE 0.2633973923758525 Lambda1 0.1485768\n",
      "23 Train Loss 222.70964 Test MSE 242.53323216395174 Test RE 0.2621650522696357 Lambda1 0.15319218\n",
      "24 Train Loss 216.69568 Test MSE 235.3175828019497 Test RE 0.25823574666654764 Lambda1 0.17024127\n",
      "25 Train Loss 213.32854 Test MSE 232.6883687440734 Test RE 0.25678905445335576 Lambda1 0.17378613\n",
      "26 Train Loss 209.08464 Test MSE 225.53687703857923 Test RE 0.25281215574314225 Lambda1 0.18722251\n",
      "27 Train Loss 202.9818 Test MSE 219.55647918804036 Test RE 0.249437818425721 Lambda1 0.20289007\n",
      "28 Train Loss 198.93861 Test MSE 212.56028338165385 Test RE 0.24543145997209964 Lambda1 0.2198242\n",
      "29 Train Loss 192.88905 Test MSE 202.8663119026925 Test RE 0.239769610372538 Lambda1 0.24234982\n",
      "30 Train Loss 185.7531 Test MSE 200.10081632229605 Test RE 0.23812971972795652 Lambda1 0.25801837\n",
      "31 Train Loss 176.16434 Test MSE 182.45310684230392 Test RE 0.22738656555246015 Lambda1 0.2882335\n",
      "32 Train Loss 167.02513 Test MSE 172.54119188082348 Test RE 0.22112383992520143 Lambda1 0.30956548\n",
      "33 Train Loss 162.42096 Test MSE 167.68921836089146 Test RE 0.21799259476725066 Lambda1 0.32552812\n",
      "34 Train Loss 155.98232 Test MSE 154.03010725088288 Test RE 0.20892575465262325 Lambda1 0.3582435\n",
      "35 Train Loss 151.17827 Test MSE 144.8536864594892 Test RE 0.20260676709730768 Lambda1 0.3889446\n",
      "36 Train Loss 146.23573 Test MSE 142.6031575055444 Test RE 0.2010266990943543 Lambda1 0.40705886\n",
      "37 Train Loss 136.5057 Test MSE 129.0013826368192 Test RE 0.19119932431571637 Lambda1 0.42263892\n",
      "38 Train Loss 133.79977 Test MSE 124.69749585815593 Test RE 0.18798276649813148 Lambda1 0.42929995\n",
      "39 Train Loss 125.99991 Test MSE 112.2273681903089 Test RE 0.17833581050119926 Lambda1 0.45614943\n",
      "40 Train Loss 121.10436 Test MSE 105.9586175421801 Test RE 0.17328354030345952 Lambda1 0.46527085\n",
      "41 Train Loss 111.38085 Test MSE 95.71475186201954 Test RE 0.16469431548802815 Lambda1 0.48396114\n",
      "42 Train Loss 103.858376 Test MSE 94.16494253532133 Test RE 0.16335551211601773 Lambda1 0.4916068\n",
      "43 Train Loss 96.94918 Test MSE 87.49602586333191 Test RE 0.15746474505698627 Lambda1 0.5024675\n",
      "44 Train Loss 89.43317 Test MSE 78.85980433957552 Test RE 0.14949167851215825 Lambda1 0.52799845\n",
      "45 Train Loss 87.08474 Test MSE 77.23170633516827 Test RE 0.14794046713569303 Lambda1 0.5311537\n",
      "46 Train Loss 81.7939 Test MSE 69.9449371560014 Test RE 0.1407885424422907 Lambda1 0.5552794\n",
      "47 Train Loss 75.75096 Test MSE 58.000877908061945 Test RE 0.12820544020424807 Lambda1 0.5882131\n",
      "48 Train Loss 70.961815 Test MSE 54.75142949003007 Test RE 0.12456238121678628 Lambda1 0.6007137\n",
      "49 Train Loss 64.81234 Test MSE 52.11170455618508 Test RE 0.12152253210931774 Lambda1 0.61315304\n",
      "50 Train Loss 62.59648 Test MSE 52.15840108870006 Test RE 0.12157696719487089 Lambda1 0.6136359\n",
      "51 Train Loss 60.94493 Test MSE 52.37989638322762 Test RE 0.12183483742601849 Lambda1 0.61501646\n",
      "52 Train Loss 59.513626 Test MSE 52.01652697659799 Test RE 0.1214115061260295 Lambda1 0.61649615\n",
      "53 Train Loss 57.912006 Test MSE 50.77187335205047 Test RE 0.11995014151004142 Lambda1 0.6232625\n",
      "54 Train Loss 56.1339 Test MSE 49.76683436011532 Test RE 0.1187569892732259 Lambda1 0.63442767\n",
      "55 Train Loss 55.22275 Test MSE 49.41811698460777 Test RE 0.11834019135704503 Lambda1 0.6420439\n",
      "56 Train Loss 54.73683 Test MSE 49.29686584761485 Test RE 0.11819492383204488 Lambda1 0.64509857\n",
      "57 Train Loss 53.409008 Test MSE 48.71893881164888 Test RE 0.11750005790411226 Lambda1 0.6528991\n",
      "58 Train Loss 53.01886 Test MSE 48.26554070367657 Test RE 0.11695202839630234 Lambda1 0.6562569\n",
      "59 Train Loss 52.43563 Test MSE 47.96589444205499 Test RE 0.11658842738570607 Lambda1 0.65733355\n",
      "60 Train Loss 51.99621 Test MSE 47.90931465553206 Test RE 0.11651964419243259 Lambda1 0.6545047\n",
      "61 Train Loss 51.66963 Test MSE 47.71177832475852 Test RE 0.1162791832553513 Lambda1 0.65600824\n",
      "62 Train Loss 51.188717 Test MSE 47.30133726013913 Test RE 0.11577795653905797 Lambda1 0.6637768\n",
      "63 Train Loss 50.43976 Test MSE 46.89650317534256 Test RE 0.1152814422254939 Lambda1 0.6685285\n",
      "64 Train Loss 49.510555 Test MSE 46.96126628650016 Test RE 0.11536101541898497 Lambda1 0.66630024\n",
      "65 Train Loss 48.94345 Test MSE 47.09016563811931 Test RE 0.11551922846850828 Lambda1 0.66737074\n",
      "66 Train Loss 48.032055 Test MSE 46.69965953905223 Test RE 0.11503924633483409 Lambda1 0.6738817\n",
      "67 Train Loss 47.460457 Test MSE 46.27859975842079 Test RE 0.11451945583177157 Lambda1 0.68444884\n",
      "68 Train Loss 46.966476 Test MSE 46.360846537459864 Test RE 0.11462117320570119 Lambda1 0.6918065\n",
      "69 Train Loss 46.504074 Test MSE 46.252678512559285 Test RE 0.11448737942091931 Lambda1 0.6951826\n",
      "70 Train Loss 46.280052 Test MSE 45.97111526289806 Test RE 0.11413837642427979 Lambda1 0.69813925\n",
      "71 Train Loss 46.084965 Test MSE 45.68967276164476 Test RE 0.11378845335655123 Lambda1 0.70217997\n",
      "72 Train Loss 45.640194 Test MSE 45.25335418662827 Test RE 0.11324383230792662 Lambda1 0.70717674\n",
      "73 Train Loss 45.254597 Test MSE 45.103249512361096 Test RE 0.11305586228697882 Lambda1 0.7099736\n",
      "74 Train Loss 44.84955 Test MSE 44.56761387728073 Test RE 0.11238254479594796 Lambda1 0.7220368\n",
      "Training time: 141.79\n",
      "Training time: 141.79\n",
      "inv_HT_stan_tune3\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 3127.0217 Test MSE 3083.962216636594 Test RE 0.9348537940714261 Lambda1 0.0074880645\n",
      "1 Train Loss 840.70825 Test MSE 858.1451489804584 Test RE 0.49313922298083485 Lambda1 0.033635195\n",
      "2 Train Loss 833.9733 Test MSE 852.4049926495964 Test RE 0.49148714478315814 Lambda1 0.0338203\n",
      "3 Train Loss 828.0852 Test MSE 847.5063349803669 Test RE 0.4900728544251163 Lambda1 0.03656377\n",
      "4 Train Loss 818.0358 Test MSE 836.5925095246511 Test RE 0.4869071550708753 Lambda1 0.039628007\n",
      "5 Train Loss 780.15894 Test MSE 791.5276994045449 Test RE 0.4736114893518978 Lambda1 0.059064522\n",
      "6 Train Loss 726.56525 Test MSE 736.3260757363862 Test RE 0.4567980706165744 Lambda1 0.072492585\n",
      "7 Train Loss 597.9512 Test MSE 611.3653457520647 Test RE 0.41623594265900316 Lambda1 0.021846179\n",
      "8 Train Loss 409.5501 Test MSE 409.6242156491169 Test RE 0.34070772611710676 Lambda1 0.0044897036\n",
      "9 Train Loss 318.48096 Test MSE 328.134553951589 Test RE 0.30494053194010085 Lambda1 -0.00036382326\n",
      "10 Train Loss 274.834 Test MSE 284.0870736743729 Test RE 0.283736304001704 Lambda1 -3.7295118e-05\n",
      "11 Train Loss 261.1157 Test MSE 281.89507703265366 Test RE 0.28263953920076884 Lambda1 0.00066166423\n",
      "12 Train Loss 257.84607 Test MSE 282.07961627024827 Test RE 0.28273203736046276 Lambda1 0.0007429835\n",
      "13 Train Loss 256.04285 Test MSE 281.30214000768336 Test RE 0.28234213126484436 Lambda1 0.0012790649\n",
      "14 Train Loss 255.08748 Test MSE 280.6927196353359 Test RE 0.2820361287235791 Lambda1 0.0020933065\n",
      "15 Train Loss 254.58786 Test MSE 280.589693389576 Test RE 0.2819843643059976 Lambda1 0.002454057\n",
      "16 Train Loss 254.19357 Test MSE 280.0514652064725 Test RE 0.28171378276308895 Lambda1 0.003130463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 Train Loss 253.72873 Test MSE 278.983851378491 Test RE 0.28117629455770443 Lambda1 0.004251247\n",
      "18 Train Loss 252.91837 Test MSE 277.7610603785586 Test RE 0.28055941761187814 Lambda1 0.0059579597\n",
      "19 Train Loss 251.5528 Test MSE 275.6010458054435 Test RE 0.27946640046925797 Lambda1 0.010309182\n",
      "20 Train Loss 249.41072 Test MSE 271.40285314071576 Test RE 0.27732969567191657 Lambda1 0.016158527\n",
      "21 Train Loss 247.40236 Test MSE 268.5940678325102 Test RE 0.2758909019152302 Lambda1 0.02170701\n",
      "22 Train Loss 242.73833 Test MSE 261.40746106616166 Test RE 0.2721749552191637 Lambda1 0.033626236\n",
      "23 Train Loss 240.88042 Test MSE 258.12746546452894 Test RE 0.27046201493335004 Lambda1 0.040099047\n",
      "24 Train Loss 237.32465 Test MSE 252.6326034261231 Test RE 0.2675678133242874 Lambda1 0.05190061\n",
      "25 Train Loss 233.1466 Test MSE 246.82241839927096 Test RE 0.26447307957964067 Lambda1 0.06425568\n",
      "26 Train Loss 226.27536 Test MSE 239.42655485705632 Test RE 0.26048056717809626 Lambda1 0.08110042\n",
      "27 Train Loss 220.4066 Test MSE 231.7442621731645 Test RE 0.2562675787786882 Lambda1 0.0951091\n",
      "28 Train Loss 217.06792 Test MSE 225.3552164643108 Test RE 0.25271032039454033 Lambda1 0.10891467\n",
      "29 Train Loss 202.61281 Test MSE 208.08046411724928 Test RE 0.24283138929690712 Lambda1 0.1508189\n",
      "30 Train Loss 197.16492 Test MSE 199.47352818170026 Test RE 0.23775617502208615 Lambda1 0.17311464\n",
      "31 Train Loss 191.14154 Test MSE 190.27682741889438 Test RE 0.2322106432062692 Lambda1 0.19812845\n",
      "32 Train Loss 184.81712 Test MSE 182.23642142912166 Test RE 0.22725150073937886 Lambda1 0.20505081\n",
      "33 Train Loss 175.78748 Test MSE 174.1345016200814 Test RE 0.22214246375951366 Lambda1 0.21831024\n",
      "34 Train Loss 164.8953 Test MSE 166.3191295951231 Test RE 0.21710022441951043 Lambda1 0.23894028\n",
      "35 Train Loss 162.02902 Test MSE 165.4921500045473 Test RE 0.21655981398144666 Lambda1 0.24052966\n",
      "36 Train Loss 152.14763 Test MSE 144.1465343978701 Test RE 0.20211161545670303 Lambda1 0.2762209\n",
      "37 Train Loss 142.25208 Test MSE 140.28633018883846 Test RE 0.1993870040856503 Lambda1 0.29175785\n",
      "38 Train Loss 135.32645 Test MSE 133.13281211522516 Test RE 0.19423689340186542 Lambda1 0.31554872\n",
      "39 Train Loss 130.42468 Test MSE 124.53081649180828 Test RE 0.18785708905124496 Lambda1 0.33716574\n",
      "40 Train Loss 123.018814 Test MSE 117.59836302954969 Test RE 0.18255335076551715 Lambda1 0.36447877\n",
      "41 Train Loss 115.87677 Test MSE 106.44293009352172 Test RE 0.1736791084759557 Lambda1 0.3898359\n",
      "42 Train Loss 112.37008 Test MSE 104.22507865026823 Test RE 0.17186018952664037 Lambda1 0.39873117\n",
      "43 Train Loss 107.08422 Test MSE 101.64450988395225 Test RE 0.16971926162861353 Lambda1 0.41203678\n",
      "44 Train Loss 102.60832 Test MSE 96.97491420393 Test RE 0.16577493742558205 Lambda1 0.42652324\n",
      "45 Train Loss 100.242134 Test MSE 94.08163310412743 Test RE 0.16328323433510006 Lambda1 0.43120623\n",
      "46 Train Loss 96.006035 Test MSE 91.36652269113812 Test RE 0.16090988292786002 Lambda1 0.4345808\n",
      "47 Train Loss 93.19326 Test MSE 89.65278691944648 Test RE 0.15939366911603323 Lambda1 0.44246963\n",
      "48 Train Loss 87.07045 Test MSE 81.21469877858853 Test RE 0.15170730365801707 Lambda1 0.46110836\n",
      "49 Train Loss 85.3257 Test MSE 78.60456343466608 Test RE 0.14924955696288053 Lambda1 0.46413085\n",
      "50 Train Loss 81.35574 Test MSE 73.95980787162674 Test RE 0.14477282792324103 Lambda1 0.47501454\n",
      "51 Train Loss 79.18976 Test MSE 72.2098363960215 Test RE 0.14304983168566485 Lambda1 0.4794578\n",
      "52 Train Loss 77.793144 Test MSE 70.91980244673742 Test RE 0.14176627534980665 Lambda1 0.48377186\n",
      "53 Train Loss 76.38513 Test MSE 71.15451080415205 Test RE 0.1420006685952142 Lambda1 0.48408905\n",
      "54 Train Loss 74.60277 Test MSE 71.32593244613234 Test RE 0.14217161590605726 Lambda1 0.48218358\n",
      "55 Train Loss 72.53963 Test MSE 67.69864510107647 Test RE 0.13850937165369615 Lambda1 0.49035743\n",
      "56 Train Loss 67.37927 Test MSE 62.27557248874866 Test RE 0.13284586315424132 Lambda1 0.51178783\n",
      "57 Train Loss 65.46625 Test MSE 62.62434729470846 Test RE 0.13321734587882597 Lambda1 0.51361024\n",
      "58 Train Loss 64.25593 Test MSE 61.455241418149846 Test RE 0.13196800007251025 Lambda1 0.5158672\n",
      "59 Train Loss 62.506706 Test MSE 59.75464690788043 Test RE 0.13012927580922506 Lambda1 0.5213263\n",
      "60 Train Loss 61.5446 Test MSE 58.71009963274932 Test RE 0.12898689228964458 Lambda1 0.5260396\n",
      "61 Train Loss 60.67293 Test MSE 57.46519466948316 Test RE 0.12761202837178987 Lambda1 0.5347072\n",
      "62 Train Loss 59.73904 Test MSE 57.00923000243245 Test RE 0.12710474340824132 Lambda1 0.53844535\n",
      "63 Train Loss 59.039856 Test MSE 56.67304516048422 Test RE 0.1267294193555818 Lambda1 0.53931946\n",
      "64 Train Loss 58.132656 Test MSE 55.39379471320821 Test RE 0.1252909577926992 Lambda1 0.54900694\n",
      "65 Train Loss 57.54879 Test MSE 54.885727600879946 Test RE 0.12471505528388975 Lambda1 0.55302036\n",
      "66 Train Loss 57.03369 Test MSE 54.686439949037236 Test RE 0.12448843195433339 Lambda1 0.5545191\n",
      "67 Train Loss 56.43031 Test MSE 54.13632710794427 Test RE 0.12386070979442189 Lambda1 0.5582965\n",
      "68 Train Loss 55.164536 Test MSE 53.38729943899904 Test RE 0.1230008597538226 Lambda1 0.5672764\n",
      "69 Train Loss 53.686188 Test MSE 52.68836469758708 Test RE 0.12219305709536722 Lambda1 0.57633483\n",
      "70 Train Loss 53.15928 Test MSE 51.85258142792143 Test RE 0.1212200228999846 Lambda1 0.5835774\n",
      "71 Train Loss 52.39273 Test MSE 51.20025019415318 Test RE 0.1204551054713096 Lambda1 0.5886687\n",
      "72 Train Loss 51.79225 Test MSE 50.56780200892096 Test RE 0.11970883632080284 Lambda1 0.5940126\n",
      "73 Train Loss 50.89601 Test MSE 49.60163265955257 Test RE 0.11855971768482523 Lambda1 0.6068561\n",
      "74 Train Loss 49.905144 Test MSE 48.45047199231701 Test RE 0.11717586730274111 Lambda1 0.62305355\n",
      "Training time: 143.21\n",
      "Training time: 143.21\n",
      "inv_HT_stan_tune3\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 2729.7153 Test MSE 2693.5350553993253 Test RE 0.8736761628259205 Lambda1 0.026196761\n",
      "1 Train Loss 846.8971 Test MSE 858.2778294626493 Test RE 0.4931773443995038 Lambda1 0.08398172\n",
      "2 Train Loss 835.67505 Test MSE 854.8957018235207 Test RE 0.4922046784783226 Lambda1 0.08508376\n",
      "3 Train Loss 815.95026 Test MSE 833.8428829828891 Test RE 0.48610633824462274 Lambda1 0.0861231\n",
      "4 Train Loss 797.1463 Test MSE 802.0993848865369 Test RE 0.47676378864573926 Lambda1 0.088510856\n",
      "5 Train Loss 757.94885 Test MSE 761.9158184427512 Test RE 0.46466789356726707 Lambda1 0.08483766\n",
      "6 Train Loss 637.0638 Test MSE 643.8258566768434 Test RE 0.4271430828809372 Lambda1 0.077206664\n",
      "7 Train Loss 479.46503 Test MSE 475.7460084712091 Test RE 0.36717808058653445 Lambda1 0.06059671\n",
      "8 Train Loss 348.83688 Test MSE 349.5583655631989 Test RE 0.3147378839599382 Lambda1 0.03161111\n",
      "9 Train Loss 295.34412 Test MSE 304.5806313618369 Test RE 0.29379223420635614 Lambda1 0.019566001\n",
      "10 Train Loss 278.85513 Test MSE 294.95714507363846 Test RE 0.2891136729871121 Lambda1 0.011700679\n",
      "11 Train Loss 268.44916 Test MSE 285.8353473294138 Test RE 0.2846080224266694 Lambda1 0.011122441\n",
      "12 Train Loss 262.60236 Test MSE 278.60658153967427 Test RE 0.28098611290052117 Lambda1 0.012389688\n",
      "13 Train Loss 259.30463 Test MSE 277.24492334944966 Test RE 0.2802986279299169 Lambda1 0.01176342\n",
      "14 Train Loss 256.24384 Test MSE 274.74033591164886 Test RE 0.27902966846790883 Lambda1 0.014141642\n",
      "15 Train Loss 251.9332 Test MSE 268.7941960732678 Test RE 0.27599366531628405 Lambda1 0.022566698\n",
      "16 Train Loss 246.38152 Test MSE 262.2152918820124 Test RE 0.2725951836805378 Lambda1 0.035709683\n",
      "17 Train Loss 241.0111 Test MSE 257.8666421379711 Test RE 0.27032533704978867 Lambda1 0.04278042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 Train Loss 229.67107 Test MSE 238.97686517436554 Test RE 0.26023583560234087 Lambda1 0.07914291\n",
      "19 Train Loss 214.14763 Test MSE 222.273544440107 Test RE 0.25097649974852154 Lambda1 0.10874452\n",
      "20 Train Loss 206.59222 Test MSE 214.72212183951075 Test RE 0.24667637954788238 Lambda1 0.13056163\n",
      "21 Train Loss 199.31529 Test MSE 200.84998581493372 Test RE 0.2385750773638309 Lambda1 0.1581023\n",
      "22 Train Loss 190.19241 Test MSE 192.44585234895902 Test RE 0.23353041353635912 Lambda1 0.16907677\n",
      "23 Train Loss 182.26753 Test MSE 185.03856095443902 Test RE 0.22899199027163689 Lambda1 0.18349421\n",
      "24 Train Loss 174.52557 Test MSE 179.55866631166884 Test RE 0.22557572201766454 Lambda1 0.19782056\n",
      "25 Train Loss 167.292 Test MSE 167.62757131955885 Test RE 0.217952521129973 Lambda1 0.2287408\n",
      "26 Train Loss 159.9784 Test MSE 158.68891612539733 Test RE 0.2120618117841114 Lambda1 0.25316462\n",
      "27 Train Loss 150.01558 Test MSE 145.77912206333238 Test RE 0.20325293980042483 Lambda1 0.28570536\n",
      "28 Train Loss 142.95735 Test MSE 136.59582741393803 Test RE 0.19674689501760928 Lambda1 0.30229372\n",
      "29 Train Loss 132.48827 Test MSE 122.60179759108827 Test RE 0.1863964297389557 Lambda1 0.32486653\n",
      "30 Train Loss 126.07322 Test MSE 118.61855936074078 Test RE 0.18334348965808567 Lambda1 0.34104544\n",
      "31 Train Loss 118.01553 Test MSE 108.93313526941142 Test RE 0.1756989525406456 Lambda1 0.3732121\n",
      "32 Train Loss 112.436386 Test MSE 102.46481262649361 Test RE 0.17040272900996964 Lambda1 0.38485143\n",
      "33 Train Loss 100.85768 Test MSE 95.37267903784566 Test RE 0.1643997533935466 Lambda1 0.4059155\n",
      "34 Train Loss 97.50892 Test MSE 90.361929625743 Test RE 0.16002281976688104 Lambda1 0.41675705\n",
      "35 Train Loss 94.63284 Test MSE 90.31483322781628 Test RE 0.15998111259725192 Lambda1 0.41038015\n",
      "36 Train Loss 90.275955 Test MSE 87.87944186937712 Test RE 0.15780938074672327 Lambda1 0.41917655\n",
      "37 Train Loss 85.5406 Test MSE 80.42216272778356 Test RE 0.15096526881273906 Lambda1 0.433584\n",
      "38 Train Loss 82.99658 Test MSE 78.13504428189206 Test RE 0.14880314215190057 Lambda1 0.44083846\n",
      "39 Train Loss 79.20535 Test MSE 75.65056524530902 Test RE 0.14641826600164523 Lambda1 0.4512582\n",
      "40 Train Loss 75.56902 Test MSE 71.74664972392115 Test RE 0.14259030030868727 Lambda1 0.46926132\n",
      "41 Train Loss 73.84715 Test MSE 69.07059253494269 Test RE 0.13990581362729623 Lambda1 0.47911245\n",
      "42 Train Loss 69.82372 Test MSE 64.38855332782403 Test RE 0.1350807627919132 Lambda1 0.49583066\n",
      "43 Train Loss 66.0751 Test MSE 60.96353578694329 Test RE 0.13143899941473278 Lambda1 0.5142429\n",
      "44 Train Loss 64.09036 Test MSE 58.77942482350846 Test RE 0.12906302401846612 Lambda1 0.5237715\n",
      "45 Train Loss 61.28816 Test MSE 56.745510072881345 Test RE 0.1268104146728791 Lambda1 0.53762305\n",
      "46 Train Loss 60.208424 Test MSE 56.38136974164282 Test RE 0.12640288370704994 Lambda1 0.5410213\n",
      "47 Train Loss 59.269943 Test MSE 55.578210527690736 Test RE 0.12549934246285555 Lambda1 0.54525995\n",
      "48 Train Loss 58.306847 Test MSE 54.665300626877624 Test RE 0.12446436880966216 Lambda1 0.55218256\n",
      "49 Train Loss 57.74056 Test MSE 54.30544263008826 Test RE 0.12405402208968795 Lambda1 0.55724764\n",
      "50 Train Loss 57.12474 Test MSE 53.60287236950363 Test RE 0.12324894256602385 Lambda1 0.564765\n",
      "51 Train Loss 56.13771 Test MSE 52.64730720087322 Test RE 0.122145438244271 Lambda1 0.57076895\n",
      "52 Train Loss 55.2151 Test MSE 51.96048912390871 Test RE 0.12134608967203542 Lambda1 0.57624555\n",
      "53 Train Loss 53.93946 Test MSE 50.25303238016928 Test RE 0.11933567864439498 Lambda1 0.5919477\n",
      "54 Train Loss 53.293564 Test MSE 49.79729103085272 Test RE 0.11879332259997953 Lambda1 0.5967453\n",
      "55 Train Loss 52.786137 Test MSE 49.79020648579392 Test RE 0.11878487207421447 Lambda1 0.5966033\n",
      "56 Train Loss 52.03669 Test MSE 49.40907295341513 Test RE 0.11832936211647982 Lambda1 0.59978396\n",
      "57 Train Loss 51.13614 Test MSE 48.73344867691773 Test RE 0.11751755400652397 Lambda1 0.60903686\n",
      "58 Train Loss 50.553482 Test MSE 48.177939327357706 Test RE 0.11684584693189948 Lambda1 0.6183105\n",
      "59 Train Loss 49.829994 Test MSE 47.586385002520466 Test RE 0.11612628363380725 Lambda1 0.6254512\n",
      "60 Train Loss 48.951763 Test MSE 47.333972405846495 Test RE 0.11581788964878705 Lambda1 0.6286979\n",
      "61 Train Loss 48.075638 Test MSE 46.84376073712854 Test RE 0.11521659800007901 Lambda1 0.6337496\n",
      "62 Train Loss 47.4742 Test MSE 46.33616722045056 Test RE 0.11459066094113027 Lambda1 0.64330715\n",
      "63 Train Loss 46.725647 Test MSE 45.24716247726676 Test RE 0.11323608485036175 Lambda1 0.657555\n",
      "64 Train Loss 46.247257 Test MSE 44.506791134605976 Test RE 0.11230583272047097 Lambda1 0.6698589\n",
      "65 Train Loss 45.83539 Test MSE 44.094543479478254 Test RE 0.11178450188853495 Lambda1 0.67957973\n",
      "66 Train Loss 45.555428 Test MSE 43.688402651464926 Test RE 0.11126850520703106 Lambda1 0.6864638\n",
      "67 Train Loss 45.006794 Test MSE 43.28278350749032 Test RE 0.11075077190467943 Lambda1 0.6958344\n",
      "68 Train Loss 44.438602 Test MSE 43.22043486318121 Test RE 0.11067097517685758 Lambda1 0.70121896\n",
      "69 Train Loss 43.57679 Test MSE 42.673637054349456 Test RE 0.109968676997363 Lambda1 0.7146947\n",
      "70 Train Loss 42.73594 Test MSE 42.2069173011398 Test RE 0.10936566230662499 Lambda1 0.7283968\n",
      "71 Train Loss 42.060738 Test MSE 41.86470297202809 Test RE 0.10892139070468128 Lambda1 0.7417888\n",
      "72 Train Loss 41.56504 Test MSE 41.37807715137567 Test RE 0.10828650157835071 Lambda1 0.7492102\n",
      "73 Train Loss 41.028107 Test MSE 40.828595444123735 Test RE 0.10756510130558881 Lambda1 0.7480094\n",
      "74 Train Loss 40.822403 Test MSE 40.720170895284056 Test RE 0.10742218124398292 Lambda1 0.7505807\n",
      "Training time: 142.28\n",
      "Training time: 142.28\n",
      "inv_HT_stan_tune3\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 2449.1846 Test MSE 2390.3195778223103 Test RE 0.8230328219677935 Lambda1 0.0076390407\n",
      "1 Train Loss 852.6713 Test MSE 858.621657916603 Test RE 0.4932761186070146 Lambda1 0.019295389\n",
      "2 Train Loss 836.29535 Test MSE 853.7275806051338 Test RE 0.49186829167409923 Lambda1 0.01610433\n",
      "3 Train Loss 828.24725 Test MSE 844.2509463082388 Test RE 0.4891307301490659 Lambda1 0.0077722236\n",
      "4 Train Loss 821.9283 Test MSE 837.9827816084409 Test RE 0.48731156487269406 Lambda1 0.0032607133\n",
      "5 Train Loss 811.83685 Test MSE 827.8644837732525 Test RE 0.48436058637947554 Lambda1 -0.0060236757\n",
      "6 Train Loss 790.3992 Test MSE 804.5576090338113 Test RE 0.4774938077001447 Lambda1 -0.021958703\n",
      "7 Train Loss 765.8869 Test MSE 782.8617199909468 Test RE 0.4710117046170319 Lambda1 -0.016359929\n",
      "8 Train Loss 706.1736 Test MSE 722.7690522209806 Test RE 0.4525733168168377 Lambda1 -0.0031986765\n",
      "9 Train Loss 622.2251 Test MSE 610.5298289337701 Test RE 0.41595142290821613 Lambda1 0.028559081\n",
      "10 Train Loss 462.7972 Test MSE 459.74810061244295 Test RE 0.36095174229769844 Lambda1 0.00998393\n",
      "11 Train Loss 342.79184 Test MSE 325.1282507666843 Test RE 0.30354041556843664 Lambda1 0.009302623\n",
      "12 Train Loss 285.85992 Test MSE 298.20919630525543 Test RE 0.29070311589749726 Lambda1 0.0017094292\n",
      "13 Train Loss 270.7855 Test MSE 287.3452599399439 Test RE 0.28535874697561764 Lambda1 0.0007458851\n",
      "14 Train Loss 264.63235 Test MSE 285.10257290424744 Test RE 0.28424297430232687 Lambda1 0.00034347124\n",
      "15 Train Loss 260.8937 Test MSE 282.7211277612358 Test RE 0.2830533523993158 Lambda1 0.0005568623\n",
      "16 Train Loss 258.8989 Test MSE 281.7100978625661 Test RE 0.28254679014023965 Lambda1 0.00059397484\n",
      "17 Train Loss 258.02203 Test MSE 281.1889550352985 Test RE 0.2822853238379147 Lambda1 0.0010798653\n",
      "18 Train Loss 256.87714 Test MSE 280.88375489394423 Test RE 0.28213208718073507 Lambda1 0.00052237243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 Train Loss 256.5344 Test MSE 281.32157295595437 Test RE 0.28235188349295715 Lambda1 0.00013099684\n",
      "20 Train Loss 256.00476 Test MSE 281.5529427037409 Test RE 0.28246796818724734 Lambda1 0.0003873704\n",
      "21 Train Loss 255.66104 Test MSE 281.2205397753625 Test RE 0.2823011773379551 Lambda1 0.0006730311\n",
      "22 Train Loss 255.30821 Test MSE 281.0020523225231 Test RE 0.2821914925156331 Lambda1 0.00068741513\n",
      "23 Train Loss 255.06631 Test MSE 280.96462712004455 Test RE 0.2821727000809785 Lambda1 0.0006938784\n",
      "24 Train Loss 254.76479 Test MSE 281.0191355233226 Test RE 0.2822000701410013 Lambda1 0.0006754262\n",
      "25 Train Loss 254.63773 Test MSE 280.9723757465449 Test RE 0.28217659102591575 Lambda1 0.00073060126\n",
      "26 Train Loss 254.51988 Test MSE 281.17175447243517 Test RE 0.28227668989154764 Lambda1 0.0007328006\n",
      "27 Train Loss 254.42065 Test MSE 281.34831016466575 Test RE 0.2823653007396228 Lambda1 0.0006734008\n",
      "28 Train Loss 254.27512 Test MSE 281.3824947724698 Test RE 0.2823824543060908 Lambda1 0.0007512521\n",
      "29 Train Loss 254.20563 Test MSE 281.1992850579414 Test RE 0.2822905089402346 Lambda1 0.0008637163\n",
      "30 Train Loss 254.06898 Test MSE 281.0638040338323 Test RE 0.28222249736094857 Lambda1 0.00094903586\n",
      "31 Train Loss 253.98598 Test MSE 281.07983007765125 Test RE 0.28223054330207137 Lambda1 0.0010338894\n",
      "32 Train Loss 253.87846 Test MSE 281.19058739191195 Test RE 0.2822861431973495 Lambda1 0.0010478514\n",
      "33 Train Loss 253.77292 Test MSE 281.0509495656698 Test RE 0.28221604355662444 Lambda1 0.0009629679\n",
      "34 Train Loss 253.62372 Test MSE 280.9229621014372 Test RE 0.2821517772217351 Lambda1 0.0011206229\n",
      "35 Train Loss 253.46617 Test MSE 280.9947366070843 Test RE 0.28218781915083346 Lambda1 0.0011459208\n",
      "36 Train Loss 253.35387 Test MSE 281.0091666835226 Test RE 0.28219506473054257 Lambda1 0.0010324697\n",
      "37 Train Loss 253.29665 Test MSE 281.03808988747153 Test RE 0.2822095869888326 Lambda1 0.0010660556\n",
      "38 Train Loss 253.23167 Test MSE 281.09377544631604 Test RE 0.2822375444450519 Lambda1 0.0011320802\n",
      "39 Train Loss 253.156 Test MSE 281.24890757425095 Test RE 0.2823154153823831 Lambda1 0.0011230314\n",
      "40 Train Loss 253.09897 Test MSE 281.2622457144147 Test RE 0.2823221096624945 Lambda1 0.0010724722\n",
      "41 Train Loss 253.04237 Test MSE 281.1948230769931 Test RE 0.2822882692832788 Lambda1 0.001022771\n",
      "42 Train Loss 253.00674 Test MSE 281.2096685466102 Test RE 0.28229572078357545 Lambda1 0.00097609835\n",
      "43 Train Loss 252.95287 Test MSE 281.3416954439106 Test RE 0.2823619814046096 Lambda1 0.00088299124\n",
      "44 Train Loss 252.88815 Test MSE 281.3976522181786 Test RE 0.28239005985931503 Lambda1 0.0008103456\n",
      "45 Train Loss 252.818 Test MSE 281.4325544558153 Test RE 0.28240757198015387 Lambda1 0.0007558467\n",
      "46 Train Loss 252.731 Test MSE 281.632910538254 Test RE 0.2825080792012484 Lambda1 0.00074628484\n",
      "47 Train Loss 252.6679 Test MSE 281.79609817690766 Test RE 0.28258991471889416 Lambda1 0.0007939491\n",
      "48 Train Loss 252.62967 Test MSE 281.69745152047847 Test RE 0.28254044811790335 Lambda1 0.00083381817\n",
      "49 Train Loss 252.60098 Test MSE 281.66607250818885 Test RE 0.2825247112217103 Lambda1 0.0008148701\n",
      "50 Train Loss 252.5645 Test MSE 281.87670097138107 Test RE 0.2826303267551477 Lambda1 0.0007744765\n",
      "51 Train Loss 252.527 Test MSE 282.0239093720246 Test RE 0.2827041181107065 Lambda1 0.00078652595\n",
      "52 Train Loss 252.49042 Test MSE 282.0181888200798 Test RE 0.2827012509215429 Lambda1 0.00084136234\n",
      "53 Train Loss 252.44043 Test MSE 282.0497936074316 Test RE 0.28271709114562626 Lambda1 0.0008039072\n",
      "54 Train Loss 252.40427 Test MSE 282.1260824119824 Test RE 0.28275532320811625 Lambda1 0.00072280463\n",
      "55 Train Loss 252.36087 Test MSE 282.1380915103845 Test RE 0.28276134108554335 Lambda1 0.0007148724\n",
      "56 Train Loss 252.3269 Test MSE 282.10316250895283 Test RE 0.28274383746358256 Lambda1 0.00070841855\n",
      "57 Train Loss 252.30283 Test MSE 282.05966913574537 Test RE 0.28272204054864464 Lambda1 0.00069448235\n",
      "58 Train Loss 252.28082 Test MSE 282.0812983254624 Test RE 0.2827328803320234 Lambda1 0.0006696745\n",
      "59 Train Loss 252.24818 Test MSE 282.21839695529565 Test RE 0.2828015796432038 Lambda1 0.00063530804\n",
      "60 Train Loss 252.20547 Test MSE 282.28889072271625 Test RE 0.2828368971565942 Lambda1 0.0006390129\n",
      "61 Train Loss 252.1683 Test MSE 282.36149244793944 Test RE 0.28287326615166297 Lambda1 0.00063682016\n",
      "62 Train Loss 252.12253 Test MSE 282.2998471802261 Test RE 0.2828423859669567 Lambda1 0.00060945185\n",
      "63 Train Loss 252.08232 Test MSE 282.38283214568634 Test RE 0.2828839551374384 Lambda1 0.000598531\n",
      "64 Train Loss 252.01852 Test MSE 282.3776321246183 Test RE 0.2828813505008756 Lambda1 0.0005583726\n",
      "65 Train Loss 251.98753 Test MSE 282.2460162121121 Test RE 0.2828154174695999 Lambda1 0.0005250116\n",
      "66 Train Loss 251.97008 Test MSE 282.1773629642203 Test RE 0.2827810195034893 Lambda1 0.00051876047\n",
      "67 Train Loss 251.94945 Test MSE 282.23287265860256 Test RE 0.28280883235832155 Lambda1 0.00050967664\n",
      "68 Train Loss 251.92917 Test MSE 282.36204569558487 Test RE 0.28287354327672426 Lambda1 0.00046243696\n",
      "69 Train Loss 251.90318 Test MSE 282.42776676304413 Test RE 0.28290646142181247 Lambda1 0.00043277015\n",
      "70 Train Loss 251.8854 Test MSE 282.476277348491 Test RE 0.28293075678228957 Lambda1 0.00044393633\n",
      "71 Train Loss 251.86324 Test MSE 282.5179597949478 Test RE 0.2829516307671918 Lambda1 0.0004377785\n",
      "72 Train Loss 251.81264 Test MSE 282.5346662531422 Test RE 0.2829599966949941 Lambda1 0.00036328906\n",
      "73 Train Loss 251.75786 Test MSE 282.6917943473325 Test RE 0.2830386680766678 Lambda1 0.00033755272\n",
      "74 Train Loss 251.72446 Test MSE 282.84374085286805 Test RE 0.28311472433288143 Lambda1 0.00038424152\n",
      "Training time: 143.23\n",
      "Training time: 143.23\n",
      "inv_HT_stan_tune3\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 1008.5342 Test MSE 998.9803865640479 Test RE 0.5320686264886191 Lambda1 0.13049622\n",
      "1 Train Loss 823.8146 Test MSE 839.4116052046181 Test RE 0.4877268393432622 Lambda1 0.14548609\n",
      "2 Train Loss 796.651 Test MSE 813.9464411246328 Test RE 0.4802717976907559 Lambda1 0.14178956\n",
      "3 Train Loss 702.55383 Test MSE 705.689562948021 Test RE 0.4471940512838996 Lambda1 0.11010187\n",
      "4 Train Loss 590.73474 Test MSE 579.5883739663124 Test RE 0.40527424303749976 Lambda1 0.06439084\n",
      "5 Train Loss 441.03403 Test MSE 446.92578037476903 Test RE 0.3558826978689038 Lambda1 0.030817164\n",
      "6 Train Loss 339.48334 Test MSE 340.69482667562977 Test RE 0.3107219545784331 Lambda1 0.015623517\n",
      "7 Train Loss 298.6352 Test MSE 312.09107706221 Test RE 0.2973923869244452 Lambda1 0.009210953\n",
      "8 Train Loss 272.43286 Test MSE 288.5387491933142 Test RE 0.2859507520224108 Lambda1 0.0017282894\n",
      "9 Train Loss 260.8902 Test MSE 280.86605709560604 Test RE 0.2821231988143732 Lambda1 0.002561724\n",
      "10 Train Loss 257.63864 Test MSE 279.22188283843684 Test RE 0.28129622001860843 Lambda1 0.003570421\n",
      "11 Train Loss 255.6795 Test MSE 279.0354463944218 Test RE 0.28120229359787463 Lambda1 0.0045844996\n",
      "12 Train Loss 254.67986 Test MSE 277.82507634614 Test RE 0.2805917462036624 Lambda1 0.006504298\n",
      "13 Train Loss 252.98509 Test MSE 275.99506359134915 Test RE 0.27966610109680534 Lambda1 0.010887001\n",
      "14 Train Loss 250.98492 Test MSE 272.9748936260643 Test RE 0.27813172115971035 Lambda1 0.016130764\n",
      "15 Train Loss 247.506 Test MSE 269.30132300886606 Test RE 0.2762538976231341 Lambda1 0.025249379\n",
      "16 Train Loss 243.89417 Test MSE 264.49973939076557 Test RE 0.27378004772653747 Lambda1 0.03226951\n",
      "17 Train Loss 239.59464 Test MSE 258.42378261438154 Test RE 0.27061720869890327 Lambda1 0.04752284\n",
      "18 Train Loss 235.15857 Test MSE 251.41782053680882 Test RE 0.26692373873979586 Lambda1 0.059235796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 Train Loss 230.03697 Test MSE 243.46317507181064 Test RE 0.2626671799105139 Lambda1 0.07440696\n",
      "20 Train Loss 225.76805 Test MSE 238.6178647270325 Test RE 0.2600402938826443 Lambda1 0.08579925\n",
      "21 Train Loss 218.34006 Test MSE 226.31972507561008 Test RE 0.25325053650451 Lambda1 0.107078195\n",
      "22 Train Loss 213.49028 Test MSE 223.5528214724012 Test RE 0.2516977007948855 Lambda1 0.11305515\n",
      "23 Train Loss 209.36655 Test MSE 217.08639033746496 Test RE 0.248030717336414 Lambda1 0.12325852\n",
      "24 Train Loss 206.18596 Test MSE 213.25507166651505 Test RE 0.24583224929675754 Lambda1 0.12801675\n",
      "25 Train Loss 202.75986 Test MSE 208.45721677705637 Test RE 0.2430511264011106 Lambda1 0.13667187\n",
      "26 Train Loss 198.64081 Test MSE 204.2149555802193 Test RE 0.2405652775266573 Lambda1 0.14622314\n",
      "27 Train Loss 192.72227 Test MSE 195.26843442489667 Test RE 0.235236762005246 Lambda1 0.16234508\n",
      "28 Train Loss 187.74937 Test MSE 187.80848233324124 Test RE 0.23069956319467075 Lambda1 0.17469692\n",
      "29 Train Loss 183.01161 Test MSE 182.8963241839353 Test RE 0.2276625831551701 Lambda1 0.18316403\n",
      "30 Train Loss 180.63026 Test MSE 177.6248470691597 Test RE 0.22435772580170002 Lambda1 0.19421633\n",
      "31 Train Loss 178.66876 Test MSE 176.10745726794602 Test RE 0.2233973637940256 Lambda1 0.1983486\n",
      "32 Train Loss 172.60329 Test MSE 166.70187724059952 Test RE 0.21734988556426255 Lambda1 0.2087186\n",
      "33 Train Loss 168.68616 Test MSE 162.45142972296304 Test RE 0.2145610765562746 Lambda1 0.21624602\n",
      "34 Train Loss 163.96487 Test MSE 156.0976744026649 Test RE 0.2103232998854608 Lambda1 0.22606382\n",
      "35 Train Loss 159.51334 Test MSE 149.3149827640984 Test RE 0.2057031133525003 Lambda1 0.23715195\n",
      "36 Train Loss 156.16245 Test MSE 148.06247984716046 Test RE 0.20483854396104734 Lambda1 0.23967521\n",
      "37 Train Loss 152.88516 Test MSE 143.55732321446146 Test RE 0.20169811828890552 Lambda1 0.25026155\n",
      "38 Train Loss 146.66016 Test MSE 139.36250767864354 Test RE 0.19872941166939215 Lambda1 0.25813243\n",
      "39 Train Loss 142.36707 Test MSE 136.16592162815664 Test RE 0.1964370419041716 Lambda1 0.26401505\n",
      "40 Train Loss 140.09358 Test MSE 135.1857141098156 Test RE 0.19572872649713596 Lambda1 0.26651454\n",
      "41 Train Loss 137.4149 Test MSE 132.52426966825362 Test RE 0.1937924621040464 Lambda1 0.26873565\n",
      "42 Train Loss 134.72311 Test MSE 128.62965924435144 Test RE 0.190923650775891 Lambda1 0.27665398\n",
      "43 Train Loss 132.53308 Test MSE 126.15660546738128 Test RE 0.18907937933178426 Lambda1 0.28123602\n",
      "44 Train Loss 130.21089 Test MSE 123.48027511905876 Test RE 0.18706303008227443 Lambda1 0.28637072\n",
      "45 Train Loss 127.18023 Test MSE 120.4868079120191 Test RE 0.18478168367864467 Lambda1 0.29202744\n",
      "46 Train Loss 124.46826 Test MSE 118.56272356712459 Test RE 0.18330033311375415 Lambda1 0.296825\n",
      "47 Train Loss 122.76071 Test MSE 117.01445526930969 Test RE 0.18209957333884785 Lambda1 0.30081433\n",
      "48 Train Loss 120.24474 Test MSE 114.3632745096355 Test RE 0.18002485157824527 Lambda1 0.30704156\n",
      "49 Train Loss 117.28745 Test MSE 110.44556838079006 Test RE 0.17691445459409597 Lambda1 0.313533\n",
      "50 Train Loss 113.4305 Test MSE 107.39194305222661 Test RE 0.17445162560553065 Lambda1 0.319554\n",
      "51 Train Loss 111.129845 Test MSE 106.22116971537633 Test RE 0.17349809492240442 Lambda1 0.3231053\n",
      "52 Train Loss 108.88273 Test MSE 105.02593855174564 Test RE 0.17251920821198885 Lambda1 0.32664686\n",
      "53 Train Loss 106.66629 Test MSE 102.95284529677565 Test RE 0.17080805503386584 Lambda1 0.3334349\n",
      "54 Train Loss 104.57482 Test MSE 101.26966392710457 Test RE 0.16940602611371877 Lambda1 0.33797044\n",
      "55 Train Loss 102.59133 Test MSE 99.95149700531363 Test RE 0.16829988614158803 Lambda1 0.34102964\n",
      "56 Train Loss 100.76406 Test MSE 97.1405740318494 Test RE 0.16591647160259715 Lambda1 0.34818286\n",
      "57 Train Loss 98.364914 Test MSE 93.63940225400646 Test RE 0.16289902577253418 Lambda1 0.35595804\n",
      "58 Train Loss 96.49518 Test MSE 93.20127355864346 Test RE 0.1625174854390904 Lambda1 0.3588592\n",
      "59 Train Loss 94.90383 Test MSE 91.4606015666722 Test RE 0.16099270498074092 Lambda1 0.36440265\n",
      "60 Train Loss 93.47203 Test MSE 90.04421303455491 Test RE 0.15974124834425263 Lambda1 0.36762646\n",
      "61 Train Loss 91.86207 Test MSE 88.60834398283204 Test RE 0.15846249172043828 Lambda1 0.37212685\n",
      "62 Train Loss 90.19217 Test MSE 85.71134685712454 Test RE 0.1558505468361579 Lambda1 0.38092548\n",
      "63 Train Loss 87.83324 Test MSE 82.7747579502819 Test RE 0.15315745120890967 Lambda1 0.3900191\n",
      "64 Train Loss 85.84732 Test MSE 81.50936604516848 Test RE 0.15198227051730914 Lambda1 0.3945974\n",
      "65 Train Loss 84.81403 Test MSE 80.58339060846316 Test RE 0.1511165183116862 Lambda1 0.39708722\n",
      "66 Train Loss 82.664345 Test MSE 78.87505871480865 Test RE 0.14950613639668947 Lambda1 0.40282002\n",
      "67 Train Loss 81.01563 Test MSE 77.70626402380316 Test RE 0.14839428832008275 Lambda1 0.40692613\n",
      "68 Train Loss 79.62729 Test MSE 76.43517794219156 Test RE 0.14717559868094696 Lambda1 0.41166848\n",
      "69 Train Loss 77.60963 Test MSE 74.04225482758537 Test RE 0.14485349845712614 Lambda1 0.4209966\n",
      "70 Train Loss 75.690796 Test MSE 71.83855499011845 Test RE 0.1426815979897028 Lambda1 0.43092465\n",
      "71 Train Loss 74.24484 Test MSE 70.14301399474554 Test RE 0.14098775081402767 Lambda1 0.43882757\n",
      "72 Train Loss 72.82334 Test MSE 68.6194167741404 Test RE 0.1394481258649906 Lambda1 0.44460276\n",
      "73 Train Loss 71.621475 Test MSE 68.15035068123294 Test RE 0.13897069136594759 Lambda1 0.4468708\n",
      "74 Train Loss 70.765465 Test MSE 66.58881039361043 Test RE 0.13736933592922135 Lambda1 0.45347917\n",
      "Training time: 150.36\n",
      "Training time: 150.36\n",
      "inv_HT_stan_tune3\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 2310.655 Test MSE 1679.9937792690353 Test RE 0.6899903350194972 Lambda1 -0.03262073\n",
      "1 Train Loss 848.7582 Test MSE 850.181931477467 Test RE 0.49084583032371115 Lambda1 -0.057598695\n",
      "2 Train Loss 828.3954 Test MSE 847.5070322319524 Test RE 0.4900730560188923 Lambda1 -0.05973623\n",
      "3 Train Loss 818.89874 Test MSE 836.9208112166662 Test RE 0.4870026835160306 Lambda1 -0.05520962\n",
      "4 Train Loss 796.7044 Test MSE 808.8373411934191 Test RE 0.47876210421292664 Lambda1 -0.038885336\n",
      "5 Train Loss 726.22766 Test MSE 754.6842224617167 Test RE 0.46245747749286936 Lambda1 0.0015662729\n",
      "6 Train Loss 615.71014 Test MSE 631.6200837648112 Test RE 0.42307477883712474 Lambda1 0.027026337\n",
      "7 Train Loss 487.97577 Test MSE 507.1615769308639 Test RE 0.3791074704138951 Lambda1 0.008605256\n",
      "8 Train Loss 401.2154 Test MSE 406.8606114639932 Test RE 0.3395564577360959 Lambda1 0.010175672\n",
      "9 Train Loss 342.2837 Test MSE 344.41230747554584 Test RE 0.31241257168997105 Lambda1 0.004066063\n",
      "10 Train Loss 298.1291 Test MSE 314.35627987967 Test RE 0.29846969442889476 Lambda1 -0.0002570919\n",
      "11 Train Loss 269.01666 Test MSE 286.35581334927923 Test RE 0.2848670201974517 Lambda1 0.0039978325\n",
      "12 Train Loss 261.22037 Test MSE 282.0030036679039 Test RE 0.28269363985340235 Lambda1 0.006062447\n",
      "13 Train Loss 257.30722 Test MSE 280.4982785777656 Test RE 0.28193842596952784 Lambda1 0.004872483\n",
      "14 Train Loss 255.71867 Test MSE 279.8013764877206 Test RE 0.28158796807601905 Lambda1 0.0049775923\n",
      "15 Train Loss 254.1648 Test MSE 278.3847240353071 Test RE 0.28087421444382715 Lambda1 0.007733334\n",
      "16 Train Loss 252.42148 Test MSE 275.83781885641326 Test RE 0.27958642161477537 Lambda1 0.011646168\n",
      "17 Train Loss 251.10065 Test MSE 273.71317153750823 Test RE 0.27850757963730927 Lambda1 0.015566344\n",
      "18 Train Loss 248.99712 Test MSE 271.17510681854327 Test RE 0.2772133113500136 Lambda1 0.022906808\n",
      "19 Train Loss 246.68361 Test MSE 265.8339307996041 Test RE 0.2744696807596063 Lambda1 0.03339033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Train Loss 243.2648 Test MSE 261.94609559631726 Test RE 0.27245522148781615 Lambda1 0.04333457\n",
      "21 Train Loss 237.02913 Test MSE 252.6229079275772 Test RE 0.26756267893505914 Lambda1 0.0631948\n",
      "22 Train Loss 229.91602 Test MSE 245.9037035388876 Test RE 0.2639804139000349 Lambda1 0.08346541\n",
      "23 Train Loss 223.1641 Test MSE 235.51081398872893 Test RE 0.2583417501358096 Lambda1 0.101372615\n",
      "24 Train Loss 218.68959 Test MSE 231.52977296065063 Test RE 0.2561489580304767 Lambda1 0.10970829\n",
      "25 Train Loss 211.72058 Test MSE 224.97070236315463 Test RE 0.25249463387043153 Lambda1 0.12208613\n",
      "26 Train Loss 206.78708 Test MSE 216.89051887022038 Test RE 0.24791879621597554 Lambda1 0.14079703\n",
      "27 Train Loss 198.33871 Test MSE 205.08225653146476 Test RE 0.24107557667794194 Lambda1 0.16309378\n",
      "28 Train Loss 194.71487 Test MSE 203.24419192524348 Test RE 0.23999281646324974 Lambda1 0.1693304\n",
      "29 Train Loss 188.25298 Test MSE 195.4532433932325 Test RE 0.23534805388401878 Lambda1 0.18456861\n",
      "30 Train Loss 184.01239 Test MSE 190.12696235331927 Test RE 0.2321191787889936 Lambda1 0.1936931\n",
      "31 Train Loss 178.88089 Test MSE 184.09428943210946 Test RE 0.22840695766480104 Lambda1 0.20744127\n",
      "32 Train Loss 174.19215 Test MSE 180.65720308417988 Test RE 0.2262647039859066 Lambda1 0.21756817\n",
      "33 Train Loss 167.23447 Test MSE 170.44550126526434 Test RE 0.2197768488051795 Lambda1 0.23406842\n",
      "34 Train Loss 161.86206 Test MSE 165.5894035656192 Test RE 0.21662343669642364 Lambda1 0.24364294\n",
      "35 Train Loss 152.11122 Test MSE 153.0172117343753 Test RE 0.20823767810200353 Lambda1 0.272587\n",
      "36 Train Loss 143.38603 Test MSE 144.12928509426868 Test RE 0.20209952224567368 Lambda1 0.2947593\n",
      "37 Train Loss 138.44508 Test MSE 139.35282019710147 Test RE 0.1987225044279936 Lambda1 0.302257\n",
      "38 Train Loss 134.38928 Test MSE 136.28436197518133 Test RE 0.19652245613259062 Lambda1 0.31028277\n",
      "39 Train Loss 131.56659 Test MSE 131.13191626662066 Test RE 0.19277174313840004 Lambda1 0.31851885\n",
      "40 Train Loss 127.08047 Test MSE 126.20180521289508 Test RE 0.18911324824586706 Lambda1 0.32213238\n",
      "41 Train Loss 122.24156 Test MSE 119.21793859563253 Test RE 0.1838061230327866 Lambda1 0.3386395\n",
      "42 Train Loss 118.93689 Test MSE 115.80992483813495 Test RE 0.18115989508897645 Lambda1 0.3481985\n",
      "43 Train Loss 114.67935 Test MSE 110.51032882867621 Test RE 0.17696631444007102 Lambda1 0.36398485\n",
      "44 Train Loss 111.02414 Test MSE 106.56890610169629 Test RE 0.1737818533505519 Lambda1 0.37133223\n",
      "45 Train Loss 107.61227 Test MSE 102.85936320040705 Test RE 0.17073048980816236 Lambda1 0.37545493\n",
      "46 Train Loss 105.51503 Test MSE 100.07603867290237 Test RE 0.16840470609881095 Lambda1 0.38280287\n",
      "47 Train Loss 102.13449 Test MSE 96.75766207830358 Test RE 0.16558914117374374 Lambda1 0.3923423\n",
      "48 Train Loss 99.85782 Test MSE 95.26595328610236 Test RE 0.16430774277389096 Lambda1 0.3985413\n",
      "49 Train Loss 98.0449 Test MSE 94.31467676394576 Test RE 0.1634853385411764 Lambda1 0.40014827\n",
      "50 Train Loss 94.84618 Test MSE 90.33745679698525 Test RE 0.16000114871428373 Lambda1 0.41065347\n",
      "51 Train Loss 92.752266 Test MSE 87.6174852297435 Test RE 0.1575740010785919 Lambda1 0.4205622\n",
      "52 Train Loss 88.87046 Test MSE 82.64834155807726 Test RE 0.1530404529175381 Lambda1 0.43421015\n",
      "53 Train Loss 85.73561 Test MSE 80.26508037034333 Test RE 0.1508177623884953 Lambda1 0.44505617\n",
      "54 Train Loss 82.89882 Test MSE 78.35026213002186 Test RE 0.14900793546781305 Lambda1 0.45271143\n",
      "55 Train Loss 79.87154 Test MSE 76.39318062505946 Test RE 0.14713516030082707 Lambda1 0.4625846\n",
      "56 Train Loss 77.1568 Test MSE 74.06253475158977 Test RE 0.14487333454142656 Lambda1 0.47167584\n",
      "57 Train Loss 74.88845 Test MSE 71.56684683719888 Test RE 0.14241151683456119 Lambda1 0.47884536\n",
      "58 Train Loss 72.47132 Test MSE 69.21626402057433 Test RE 0.1400532682289097 Lambda1 0.486178\n",
      "59 Train Loss 70.7104 Test MSE 68.42098356576132 Test RE 0.13924635227010737 Lambda1 0.490616\n",
      "60 Train Loss 69.453 Test MSE 66.97149840929002 Test RE 0.13776350339325655 Lambda1 0.49587288\n",
      "61 Train Loss 68.07266 Test MSE 65.2026354070381 Test RE 0.13593201210176145 Lambda1 0.50341713\n",
      "62 Train Loss 66.156876 Test MSE 63.63770766270177 Test RE 0.13429085356509293 Lambda1 0.5092265\n",
      "63 Train Loss 65.15111 Test MSE 62.65181657481204 Test RE 0.13324655962356252 Lambda1 0.51040596\n",
      "64 Train Loss 63.19571 Test MSE 59.773340111441854 Test RE 0.13014962855962767 Lambda1 0.51678467\n",
      "65 Train Loss 61.437317 Test MSE 58.099637998221304 Test RE 0.12831454368721676 Lambda1 0.5210098\n",
      "66 Train Loss 60.756836 Test MSE 57.72167317319785 Test RE 0.1278964901553736 Lambda1 0.521124\n",
      "67 Train Loss 59.762444 Test MSE 56.86569227643812 Test RE 0.12694463052658336 Lambda1 0.52402633\n",
      "68 Train Loss 58.554592 Test MSE 55.4020184082927 Test RE 0.1253002577188065 Lambda1 0.5304207\n",
      "69 Train Loss 57.22173 Test MSE 53.45093212062764 Test RE 0.12307414070073068 Lambda1 0.5388282\n",
      "70 Train Loss 55.88889 Test MSE 51.84951176449402 Test RE 0.12121643474518096 Lambda1 0.54725665\n",
      "71 Train Loss 55.003098 Test MSE 50.81607614998842 Test RE 0.12000234539764577 Lambda1 0.55294085\n",
      "72 Train Loss 53.870716 Test MSE 49.557899524118405 Test RE 0.11850743985341831 Lambda1 0.55937195\n",
      "73 Train Loss 52.839935 Test MSE 47.94527020999376 Test RE 0.11656335951883343 Lambda1 0.56915563\n",
      "74 Train Loss 51.65853 Test MSE 46.60341816248007 Test RE 0.11492064541099482 Lambda1 0.5798271\n",
      "Training time: 153.39\n",
      "Training time: 153.39\n",
      "inv_HT_stan_tune3\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 3015.6267 Test MSE 2960.9132562444506 Test RE 0.9160137922582461 Lambda1 0.024413563\n",
      "1 Train Loss 852.27625 Test MSE 853.1706225659277 Test RE 0.49170782204642266 Lambda1 0.13135637\n",
      "2 Train Loss 822.5045 Test MSE 841.5567554324288 Test RE 0.4883496445052116 Lambda1 0.12190667\n",
      "3 Train Loss 793.6225 Test MSE 809.3162491313268 Test RE 0.4789038193830624 Lambda1 0.08336669\n",
      "4 Train Loss 728.7558 Test MSE 735.3887349707859 Test RE 0.4565072267766967 Lambda1 0.05769075\n",
      "5 Train Loss 672.3028 Test MSE 689.3023777330149 Test RE 0.4419712899816475 Lambda1 0.046641357\n",
      "6 Train Loss 593.7302 Test MSE 578.2100119336627 Test RE 0.4047920498373485 Lambda1 -0.019650351\n",
      "7 Train Loss 476.25677 Test MSE 466.78927864700194 Test RE 0.36370528084829706 Lambda1 -0.02114971\n",
      "8 Train Loss 390.3972 Test MSE 391.17851518929615 Test RE 0.33294819712095697 Lambda1 0.009671578\n",
      "9 Train Loss 327.12943 Test MSE 334.8637537899556 Test RE 0.3080514386042697 Lambda1 0.0014380354\n",
      "10 Train Loss 287.73114 Test MSE 298.96022027310573 Test RE 0.2910689458637054 Lambda1 6.3527725e-05\n",
      "11 Train Loss 268.92328 Test MSE 289.2084333437464 Test RE 0.28628239846876263 Lambda1 0.0021818033\n",
      "12 Train Loss 263.59488 Test MSE 284.93336657195357 Test RE 0.2841586137002719 Lambda1 0.0028452154\n",
      "13 Train Loss 261.1169 Test MSE 282.7422472777362 Test RE 0.28306392436887196 Lambda1 0.0028065708\n",
      "14 Train Loss 260.0908 Test MSE 282.82971990176935 Test RE 0.28310770705385613 Lambda1 0.0026786786\n",
      "15 Train Loss 258.74985 Test MSE 281.88230914518687 Test RE 0.28263313832507514 Lambda1 0.0033040002\n",
      "16 Train Loss 256.83414 Test MSE 280.17882394002015 Test RE 0.28177783283396574 Lambda1 0.00481033\n",
      "17 Train Loss 255.7629 Test MSE 279.84000758670885 Test RE 0.2816074062885082 Lambda1 0.0065682516\n",
      "18 Train Loss 254.74695 Test MSE 278.4308961786003 Test RE 0.28089750599984437 Lambda1 0.008688126\n",
      "19 Train Loss 253.46346 Test MSE 277.0508175416903 Test RE 0.280200488859812 Lambda1 0.012125135\n",
      "20 Train Loss 252.36417 Test MSE 275.927278763991 Test RE 0.2796317557678497 Lambda1 0.016834555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Train Loss 250.83574 Test MSE 274.72046812483933 Test RE 0.2790195793010329 Lambda1 0.019985635\n",
      "22 Train Loss 249.38033 Test MSE 272.7551916680265 Test RE 0.2780197724323529 Lambda1 0.025102314\n",
      "23 Train Loss 246.73859 Test MSE 268.9146968102641 Test RE 0.27605552251792964 Lambda1 0.031941403\n",
      "24 Train Loss 243.91806 Test MSE 263.90813916035574 Test RE 0.2734736976923312 Lambda1 0.043501407\n",
      "25 Train Loss 241.15115 Test MSE 260.9305365882653 Test RE 0.2719265572613172 Lambda1 0.04850401\n",
      "26 Train Loss 237.37064 Test MSE 255.70040393222115 Test RE 0.2691874929142402 Lambda1 0.06032116\n",
      "27 Train Loss 234.12184 Test MSE 253.63982876796777 Test RE 0.2681006681361739 Lambda1 0.06644567\n",
      "28 Train Loss 230.21854 Test MSE 247.76478188795332 Test RE 0.26497747529035964 Lambda1 0.079147816\n",
      "29 Train Loss 221.47769 Test MSE 236.27903165553118 Test RE 0.25876275228467355 Lambda1 0.10430865\n",
      "30 Train Loss 216.0415 Test MSE 229.9151148059247 Test RE 0.2552542208012389 Lambda1 0.11285391\n",
      "31 Train Loss 209.26724 Test MSE 220.0692617200884 Test RE 0.24972893431661133 Lambda1 0.12878507\n",
      "32 Train Loss 203.03123 Test MSE 210.78245183639405 Test RE 0.24440292353335136 Lambda1 0.14690872\n",
      "33 Train Loss 196.04866 Test MSE 202.25986926487005 Test RE 0.2394109619975378 Lambda1 0.16831824\n",
      "34 Train Loss 189.98933 Test MSE 199.2111767347314 Test RE 0.23759977281665495 Lambda1 0.1744309\n",
      "35 Train Loss 182.33585 Test MSE 190.27889333646016 Test RE 0.23221190380839069 Lambda1 0.2019556\n",
      "36 Train Loss 172.95355 Test MSE 179.38149804907988 Test RE 0.22546440819849684 Lambda1 0.22383079\n",
      "37 Train Loss 168.2637 Test MSE 175.21801617402093 Test RE 0.22283250890577713 Lambda1 0.23701356\n",
      "38 Train Loss 161.75078 Test MSE 163.00518111174793 Test RE 0.21492645473746583 Lambda1 0.257612\n",
      "39 Train Loss 153.70135 Test MSE 154.97611843394125 Test RE 0.20956635524692874 Lambda1 0.28053144\n",
      "40 Train Loss 146.45734 Test MSE 148.2905720997598 Test RE 0.20499626151651854 Lambda1 0.3078284\n",
      "41 Train Loss 138.27599 Test MSE 137.5996186501079 Test RE 0.197468481154926 Lambda1 0.3384154\n",
      "42 Train Loss 134.27658 Test MSE 130.17146029653225 Test RE 0.19206448190218964 Lambda1 0.36164063\n",
      "43 Train Loss 128.66174 Test MSE 123.7467909312749 Test RE 0.1872647966512743 Lambda1 0.38869232\n",
      "44 Train Loss 122.43364 Test MSE 116.73414073734084 Test RE 0.18188132780844432 Lambda1 0.41745517\n",
      "45 Train Loss 112.713394 Test MSE 108.23462321922672 Test RE 0.17513472931082094 Lambda1 0.44617486\n",
      "46 Train Loss 109.31308 Test MSE 106.57820808638304 Test RE 0.17378943755557252 Lambda1 0.44287342\n",
      "47 Train Loss 104.64746 Test MSE 103.80049254304015 Test RE 0.17150977522980168 Lambda1 0.45882648\n",
      "48 Train Loss 101.66381 Test MSE 101.39752015624019 Test RE 0.16951293267720888 Lambda1 0.4692535\n",
      "49 Train Loss 98.13673 Test MSE 98.18049629467151 Test RE 0.16680220299972318 Lambda1 0.48476785\n",
      "50 Train Loss 94.90918 Test MSE 96.37489591798563 Test RE 0.16526128740880464 Lambda1 0.49068892\n",
      "51 Train Loss 92.608246 Test MSE 94.06533241346999 Test RE 0.1632690884029779 Lambda1 0.4974815\n",
      "52 Train Loss 89.60742 Test MSE 89.06785485318977 Test RE 0.15887284291153247 Lambda1 0.50601983\n",
      "53 Train Loss 84.96565 Test MSE 82.36591787594003 Test RE 0.1527787462994006 Lambda1 0.5185389\n",
      "54 Train Loss 81.8424 Test MSE 78.01582858585189 Test RE 0.1486895795236611 Lambda1 0.5278047\n",
      "55 Train Loss 78.84141 Test MSE 76.50435348281137 Test RE 0.14724218221894306 Lambda1 0.53519386\n",
      "56 Train Loss 75.4795 Test MSE 73.95824574529077 Test RE 0.14477129902093644 Lambda1 0.54671633\n",
      "57 Train Loss 72.14632 Test MSE 67.21324562526509 Test RE 0.1380119220787003 Lambda1 0.5577553\n",
      "58 Train Loss 68.4988 Test MSE 63.72229872611653 Test RE 0.13438007765879306 Lambda1 0.56770873\n",
      "59 Train Loss 66.932045 Test MSE 62.25774254407953 Test RE 0.1328268444294152 Lambda1 0.57222766\n",
      "60 Train Loss 63.916836 Test MSE 59.59133637649479 Test RE 0.12995133131178216 Lambda1 0.58187187\n",
      "61 Train Loss 61.13024 Test MSE 56.86064627285737 Test RE 0.12693899815629694 Lambda1 0.59278405\n",
      "62 Train Loss 59.036507 Test MSE 54.956727856113844 Test RE 0.12479569499979216 Lambda1 0.60325944\n",
      "63 Train Loss 56.54733 Test MSE 53.07540779387087 Test RE 0.12264104445327628 Lambda1 0.616221\n",
      "64 Train Loss 54.132614 Test MSE 49.932472056462764 Test RE 0.11895445304848391 Lambda1 0.6307888\n",
      "65 Train Loss 52.988388 Test MSE 49.54903405000666 Test RE 0.11849683940786623 Lambda1 0.6336615\n",
      "66 Train Loss 51.84221 Test MSE 48.306998075007414 Test RE 0.11700224520619829 Lambda1 0.63723\n",
      "67 Train Loss 50.538498 Test MSE 47.409788338268456 Test RE 0.11591060665036171 Lambda1 0.64207584\n",
      "68 Train Loss 49.7453 Test MSE 46.90473168226624 Test RE 0.11529155547994392 Lambda1 0.64864504\n",
      "69 Train Loss 48.78242 Test MSE 46.07583057064022 Test RE 0.11426829753505649 Lambda1 0.65642756\n",
      "70 Train Loss 46.80893 Test MSE 43.808394478674956 Test RE 0.1114212019816925 Lambda1 0.6751688\n",
      "71 Train Loss 45.988842 Test MSE 43.6243108032427 Test RE 0.11118685857867396 Lambda1 0.68126327\n",
      "72 Train Loss 45.29804 Test MSE 44.150219973873746 Test RE 0.11185505263373416 Lambda1 0.68034893\n",
      "73 Train Loss 44.543793 Test MSE 43.605726798081704 Test RE 0.11116317319289112 Lambda1 0.68570304\n",
      "74 Train Loss 43.764786 Test MSE 42.76826734458995 Test RE 0.11009053917979812 Lambda1 0.6927645\n",
      "Training time: 153.46\n",
      "Training time: 153.46\n",
      "inv_HT_stan_tune4\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.0807 Test MSE 858.4512269847583 Test RE 0.49322716009309564 Lambda1 -0.046823207\n",
      "1 Train Loss 838.0543 Test MSE 858.2699668293371 Test RE 0.4931750854098359 Lambda1 -0.04679323\n",
      "2 Train Loss 838.05084 Test MSE 858.2229246121643 Test RE 0.4931615696349172 Lambda1 -0.04678397\n",
      "3 Train Loss 837.966 Test MSE 857.9344078771354 Test RE 0.4930786673246961 Lambda1 -0.046963606\n",
      "4 Train Loss 837.87286 Test MSE 857.9187804718804 Test RE 0.4930741765529215 Lambda1 -0.07059016\n",
      "5 Train Loss 837.71387 Test MSE 857.6944872822985 Test RE 0.4930097179983608 Lambda1 -0.34235767\n",
      "6 Train Loss 835.49866 Test MSE 853.603520454401 Test RE 0.49183255224574285 Lambda1 -0.50891316\n",
      "7 Train Loss 828.05383 Test MSE 846.8830112499427 Test RE 0.4898926019506833 Lambda1 -0.7739374\n",
      "8 Train Loss 813.5756 Test MSE 819.8829723085493 Test RE 0.48202005071351894 Lambda1 -0.9403539\n",
      "9 Train Loss 791.839 Test MSE 786.7104215069148 Test RE 0.4721680780068759 Lambda1 -0.8892387\n",
      "10 Train Loss 767.17505 Test MSE 769.5393885594334 Test RE 0.46698679221692513 Lambda1 -0.8773775\n",
      "11 Train Loss 741.821 Test MSE 736.1112464118885 Test RE 0.45673142841616343 Lambda1 -0.95142144\n",
      "12 Train Loss 700.9614 Test MSE 691.9960360689906 Test RE 0.4428340164683798 Lambda1 -1.0085844\n",
      "13 Train Loss 668.8983 Test MSE 659.035975062772 Test RE 0.43215916901831974 Lambda1 -1.0377743\n",
      "14 Train Loss 646.19904 Test MSE 634.303276301521 Test RE 0.4239724608991813 Lambda1 -1.0461885\n",
      "15 Train Loss 628.03436 Test MSE 620.5715680074904 Test RE 0.4193581693033892 Lambda1 -1.0631466\n",
      "16 Train Loss 603.6285 Test MSE 598.3001482937655 Test RE 0.41176433349647934 Lambda1 -1.0946724\n",
      "17 Train Loss 582.26935 Test MSE 574.01392778977 Test RE 0.4033205824737603 Lambda1 -1.1397218\n",
      "18 Train Loss 562.45984 Test MSE 546.4422505984834 Test RE 0.39351501347770446 Lambda1 -1.2005358\n",
      "19 Train Loss 540.93353 Test MSE 530.8742211552817 Test RE 0.3878689272777132 Lambda1 -1.2413919\n",
      "20 Train Loss 530.4461 Test MSE 519.5119381197561 Test RE 0.38369570388172064 Lambda1 -1.2886695\n",
      "21 Train Loss 511.99792 Test MSE 505.0211785595215 Test RE 0.37830664184027557 Lambda1 -1.330854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 Train Loss 492.24554 Test MSE 486.00405960339987 Test RE 0.3711155219164097 Lambda1 -1.3899211\n",
      "23 Train Loss 468.2528 Test MSE 459.15538526107224 Test RE 0.360718994585097 Lambda1 -1.4359337\n",
      "24 Train Loss 446.70312 Test MSE 426.310763667279 Test RE 0.34757803179881896 Lambda1 -1.4815761\n",
      "25 Train Loss 428.58182 Test MSE 412.87100672202286 Test RE 0.3420553312917943 Lambda1 -1.5264834\n",
      "26 Train Loss 419.42883 Test MSE 409.30872224976355 Test RE 0.34057649395355344 Lambda1 -1.533194\n",
      "27 Train Loss 398.88745 Test MSE 391.1604557305383 Test RE 0.3329405114564826 Lambda1 -1.5455344\n",
      "28 Train Loss 382.48956 Test MSE 371.07241091536275 Test RE 0.3242787599506162 Lambda1 -1.5524135\n",
      "29 Train Loss 376.36423 Test MSE 366.12786228433055 Test RE 0.32211100331684206 Lambda1 -1.5524167\n",
      "30 Train Loss 357.40442 Test MSE 345.5873524777092 Test RE 0.3129450530279352 Lambda1 -1.5824915\n",
      "31 Train Loss 345.50052 Test MSE 338.1410688856877 Test RE 0.30955521952496357 Lambda1 -1.5999876\n",
      "32 Train Loss 338.73984 Test MSE 333.5990975930326 Test RE 0.3074691903367302 Lambda1 -1.6071413\n",
      "33 Train Loss 324.29465 Test MSE 322.2815736361403 Test RE 0.30220866223662307 Lambda1 -1.5958297\n",
      "34 Train Loss 312.94904 Test MSE 313.25519708232554 Test RE 0.2979465171448192 Lambda1 -1.6068306\n",
      "35 Train Loss 298.98193 Test MSE 305.9561953276015 Test RE 0.2944549072526508 Lambda1 -1.6139891\n",
      "36 Train Loss 285.03864 Test MSE 287.3965045619217 Test RE 0.28538419101672313 Lambda1 -1.6196251\n",
      "37 Train Loss 273.8482 Test MSE 271.1091704385676 Test RE 0.2771796070146776 Lambda1 -1.628705\n",
      "38 Train Loss 246.96484 Test MSE 240.48416819324586 Test RE 0.26105524062134683 Lambda1 -1.6485316\n",
      "39 Train Loss 231.90753 Test MSE 226.25112741538481 Test RE 0.25321215339220104 Lambda1 -1.6626145\n",
      "40 Train Loss 224.02534 Test MSE 219.70721548244384 Test RE 0.24952342938409577 Lambda1 -1.6607753\n",
      "41 Train Loss 218.66238 Test MSE 213.49992566008308 Test RE 0.24597333794570167 Lambda1 -1.6609787\n",
      "42 Train Loss 204.44702 Test MSE 190.46171049810337 Test RE 0.23232342991728788 Lambda1 -1.6792771\n",
      "43 Train Loss 197.47029 Test MSE 180.1394982969136 Test RE 0.22594027086344207 Lambda1 -1.6861681\n",
      "44 Train Loss 192.07054 Test MSE 181.31670397529444 Test RE 0.226677324776612 Lambda1 -1.6841539\n",
      "45 Train Loss 190.38112 Test MSE 179.20860837601245 Test RE 0.22535572958555314 Lambda1 -1.6900434\n",
      "46 Train Loss 188.92436 Test MSE 176.06249285569996 Test RE 0.22336884264967619 Lambda1 -1.6969676\n",
      "47 Train Loss 185.55956 Test MSE 175.2162576775561 Test RE 0.22283139072406588 Lambda1 -1.7018527\n",
      "48 Train Loss 182.36047 Test MSE 171.16223286161775 Test RE 0.2202384502024055 Lambda1 -1.7089262\n",
      "49 Train Loss 178.4737 Test MSE 170.493511007944 Test RE 0.21980779912939902 Lambda1 -1.7211552\n",
      "50 Train Loss 173.73264 Test MSE 166.93900730242603 Test RE 0.21750441854540856 Lambda1 -1.736443\n",
      "51 Train Loss 168.80527 Test MSE 164.0240928481227 Test RE 0.21559713874297545 Lambda1 -1.7468537\n",
      "52 Train Loss 163.81317 Test MSE 160.9106044637703 Test RE 0.21354111387801955 Lambda1 -1.7591194\n",
      "53 Train Loss 157.67606 Test MSE 156.47642580543695 Test RE 0.21057830683002657 Lambda1 -1.7767832\n",
      "54 Train Loss 156.66203 Test MSE 157.3257008350864 Test RE 0.21114898989409134 Lambda1 -1.7741139\n",
      "55 Train Loss 153.58786 Test MSE 152.01699371604533 Test RE 0.2075559751742763 Lambda1 -1.7689669\n",
      "56 Train Loss 149.98744 Test MSE 148.79185413160468 Test RE 0.20534245427894407 Lambda1 -1.7685658\n",
      "57 Train Loss 147.98569 Test MSE 145.15263213540342 Test RE 0.20281572691288433 Lambda1 -1.773363\n",
      "58 Train Loss 145.42671 Test MSE 138.87179802800318 Test RE 0.19837922984371903 Lambda1 -1.77078\n",
      "59 Train Loss 140.40637 Test MSE 135.51847212661824 Test RE 0.19596947039960974 Lambda1 -1.7763656\n",
      "60 Train Loss 134.49586 Test MSE 134.36048753498994 Test RE 0.19513040958531272 Lambda1 -1.7874016\n",
      "61 Train Loss 132.8835 Test MSE 133.86170014039374 Test RE 0.1947678808134715 Lambda1 -1.7886751\n",
      "62 Train Loss 131.59683 Test MSE 133.6927465701298 Test RE 0.19464492884637422 Lambda1 -1.7906489\n",
      "63 Train Loss 128.942 Test MSE 129.27114474963955 Test RE 0.19139913379141762 Lambda1 -1.7906971\n",
      "64 Train Loss 126.95767 Test MSE 125.4947591680774 Test RE 0.18858275036352687 Lambda1 -1.7882928\n",
      "65 Train Loss 125.96529 Test MSE 123.67813853604058 Test RE 0.18721284394881405 Lambda1 -1.788707\n",
      "66 Train Loss 122.15724 Test MSE 124.15478886902325 Test RE 0.18757325223800636 Lambda1 -1.7975391\n",
      "67 Train Loss 120.30821 Test MSE 122.28308689845299 Test RE 0.1861539977584839 Lambda1 -1.7978401\n",
      "68 Train Loss 116.38511 Test MSE 117.43346082861474 Test RE 0.18242531322701377 Lambda1 -1.7961905\n",
      "69 Train Loss 115.57454 Test MSE 117.50801940009312 Test RE 0.18248321500588194 Lambda1 -1.80115\n",
      "70 Train Loss 111.89368 Test MSE 120.20967728698722 Test RE 0.18456905399561055 Lambda1 -1.8073139\n",
      "71 Train Loss 110.10893 Test MSE 117.30222673860293 Test RE 0.18232335288277207 Lambda1 -1.8045433\n",
      "72 Train Loss 108.47942 Test MSE 113.67578312524438 Test RE 0.17948292892278475 Lambda1 -1.8011609\n",
      "73 Train Loss 107.89395 Test MSE 113.22332601117758 Test RE 0.17912538004903 Lambda1 -1.8035644\n",
      "74 Train Loss 107.133804 Test MSE 112.685202019744 Test RE 0.1786992025438219 Lambda1 -1.8060397\n",
      "Training time: 144.84\n",
      "Training time: 144.84\n",
      "inv_HT_stan_tune4\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 838.0353 Test MSE 858.2942701870155 Test RE 0.4931820679014258 Lambda1 -0.05776813\n",
      "1 Train Loss 838.00916 Test MSE 858.1161483526612 Test RE 0.4931308902017567 Lambda1 -0.05770013\n",
      "2 Train Loss 837.86365 Test MSE 857.8443540720295 Test RE 0.49305278843752354 Lambda1 -0.06140366\n",
      "3 Train Loss 837.86224 Test MSE 857.8622522838868 Test RE 0.49305793197970127 Lambda1 -0.0620202\n",
      "4 Train Loss 837.7637 Test MSE 857.7638025648572 Test RE 0.49302963908798536 Lambda1 -0.15815917\n",
      "5 Train Loss 835.42645 Test MSE 854.0707109941354 Test RE 0.49196712765350475 Lambda1 -0.6557495\n",
      "6 Train Loss 825.33136 Test MSE 836.777982995404 Test RE 0.48696112600304753 Lambda1 -0.7999729\n",
      "7 Train Loss 810.92255 Test MSE 822.0294813368439 Test RE 0.4826506187994077 Lambda1 -0.710107\n",
      "8 Train Loss 800.0563 Test MSE 809.5057525737083 Test RE 0.4789598843703259 Lambda1 -0.73555297\n",
      "9 Train Loss 786.041 Test MSE 793.4792648775086 Test RE 0.4741949906058564 Lambda1 -0.60602826\n",
      "10 Train Loss 780.79694 Test MSE 784.0728319599643 Test RE 0.4713758988570079 Lambda1 -0.5839751\n",
      "11 Train Loss 762.07495 Test MSE 759.4043759368079 Test RE 0.46390143753607915 Lambda1 -0.63510925\n",
      "12 Train Loss 742.30676 Test MSE 740.2808753314683 Test RE 0.45802315665071197 Lambda1 -0.6074825\n",
      "13 Train Loss 726.8412 Test MSE 721.6237712564002 Test RE 0.4522146067305468 Lambda1 -0.58670616\n",
      "14 Train Loss 720.7607 Test MSE 719.9954842231796 Test RE 0.4517041253345983 Lambda1 -0.571519\n",
      "15 Train Loss 714.63043 Test MSE 714.2109509227132 Test RE 0.4498859425218696 Lambda1 -0.55877286\n",
      "16 Train Loss 708.4994 Test MSE 706.181492783129 Test RE 0.4473498916015723 Lambda1 -0.52278274\n",
      "17 Train Loss 706.8349 Test MSE 704.8501565552201 Test RE 0.4469280070783363 Lambda1 -0.53904533\n",
      "18 Train Loss 705.02496 Test MSE 702.5769271546548 Test RE 0.44620672585488447 Lambda1 -0.5282334\n",
      "19 Train Loss 700.48505 Test MSE 697.6097124042632 Test RE 0.4446265885444174 Lambda1 -0.5078097\n",
      "20 Train Loss 698.2957 Test MSE 695.1435511860051 Test RE 0.4438399799003733 Lambda1 -0.50560147\n",
      "21 Train Loss 696.673 Test MSE 692.8090165258204 Test RE 0.44309406833626247 Lambda1 -0.5092526\n",
      "22 Train Loss 695.7033 Test MSE 691.6139904629475 Test RE 0.4427117569907592 Lambda1 -0.5086131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 Train Loss 694.37384 Test MSE 690.7680536043769 Test RE 0.4424409261616643 Lambda1 -0.51611555\n",
      "24 Train Loss 693.4774 Test MSE 690.6170124850993 Test RE 0.44239255216276263 Lambda1 -0.52159655\n",
      "25 Train Loss 692.6927 Test MSE 690.7617107159784 Test RE 0.4424388948286827 Lambda1 -0.5120526\n",
      "26 Train Loss 691.31445 Test MSE 690.5122228477794 Test RE 0.4423589880361128 Lambda1 -0.50324917\n",
      "27 Train Loss 690.08704 Test MSE 688.9013625080447 Test RE 0.44184270852935636 Lambda1 -0.5126035\n",
      "28 Train Loss 687.87524 Test MSE 686.6252856509602 Test RE 0.44111219756254755 Lambda1 -0.5284616\n",
      "29 Train Loss 686.28314 Test MSE 683.8954850558362 Test RE 0.44023446441176145 Lambda1 -0.52390695\n",
      "30 Train Loss 683.0447 Test MSE 678.3659551389394 Test RE 0.4384511289425066 Lambda1 -0.523162\n",
      "31 Train Loss 678.5613 Test MSE 670.9343787137763 Test RE 0.4360428740343754 Lambda1 -0.5234541\n",
      "32 Train Loss 671.1423 Test MSE 665.9650979054311 Test RE 0.43442509543860014 Lambda1 -0.489582\n",
      "33 Train Loss 665.40576 Test MSE 663.8241762365325 Test RE 0.43372624590690473 Lambda1 -0.47209537\n",
      "34 Train Loss 656.4771 Test MSE 654.6126825029459 Test RE 0.430706452616277 Lambda1 -0.49613553\n",
      "35 Train Loss 650.1783 Test MSE 646.9358897192581 Test RE 0.4281735080262507 Lambda1 -0.5339043\n",
      "36 Train Loss 639.80927 Test MSE 636.1372541396066 Test RE 0.42458493989401597 Lambda1 -0.5859451\n",
      "37 Train Loss 624.2346 Test MSE 613.8791931730863 Test RE 0.41709081630337447 Lambda1 -0.6338751\n",
      "38 Train Loss 613.2433 Test MSE 591.3784495378771 Test RE 0.4093755660045915 Lambda1 -0.6392458\n",
      "39 Train Loss 601.2417 Test MSE 581.424805505969 Test RE 0.40591579298927166 Lambda1 -0.6403059\n",
      "40 Train Loss 581.0814 Test MSE 553.2294713366285 Test RE 0.39595134669226917 Lambda1 -0.7156702\n",
      "41 Train Loss 538.76105 Test MSE 513.6193900913387 Test RE 0.38151346995041724 Lambda1 -0.8596427\n",
      "42 Train Loss 510.11917 Test MSE 492.4673861292893 Test RE 0.37357508836600256 Lambda1 -0.9750024\n",
      "43 Train Loss 476.9854 Test MSE 470.2390072098595 Test RE 0.3650467586256682 Lambda1 -1.063901\n",
      "44 Train Loss 443.0264 Test MSE 441.1137883779198 Test RE 0.3535611088167177 Lambda1 -1.1824728\n",
      "45 Train Loss 420.0184 Test MSE 415.0719129898284 Test RE 0.34296582281194077 Lambda1 -1.2647367\n",
      "46 Train Loss 400.2823 Test MSE 381.3394023111139 Test RE 0.32873429307637675 Lambda1 -1.3424193\n",
      "47 Train Loss 366.44354 Test MSE 348.8857401706405 Test RE 0.3144349264982964 Lambda1 -1.4228989\n",
      "48 Train Loss 347.1837 Test MSE 338.7011702815877 Test RE 0.3098114891528175 Lambda1 -1.4474226\n",
      "49 Train Loss 337.22662 Test MSE 337.3981182348743 Test RE 0.30921496106464885 Lambda1 -1.4554296\n",
      "50 Train Loss 328.5216 Test MSE 321.16972356600263 Test RE 0.301686911918768 Lambda1 -1.4613637\n",
      "51 Train Loss 319.36084 Test MSE 310.0280894357424 Test RE 0.2964078441207136 Lambda1 -1.4676484\n",
      "52 Train Loss 304.0022 Test MSE 297.0596288290076 Test RE 0.29014225873031174 Lambda1 -1.5000277\n",
      "53 Train Loss 288.28265 Test MSE 277.5355881351968 Test RE 0.2804455225902203 Lambda1 -1.5276148\n",
      "54 Train Loss 259.54138 Test MSE 251.59919281966208 Test RE 0.26702000049185975 Lambda1 -1.5326867\n",
      "55 Train Loss 253.4747 Test MSE 244.92173275945274 Test RE 0.26345280826651324 Lambda1 -1.5277946\n",
      "56 Train Loss 244.67046 Test MSE 231.05724616016042 Test RE 0.2558874386045626 Lambda1 -1.5102023\n",
      "57 Train Loss 233.79483 Test MSE 221.58871202311335 Test RE 0.2505895678774361 Lambda1 -1.5048902\n",
      "58 Train Loss 226.99077 Test MSE 217.7031922303424 Test RE 0.248382829003021 Lambda1 -1.4973261\n",
      "59 Train Loss 219.63351 Test MSE 207.84235277648315 Test RE 0.24269241070788805 Lambda1 -1.4976428\n",
      "60 Train Loss 213.69363 Test MSE 198.69036944105517 Test RE 0.23728898534015694 Lambda1 -1.4866986\n",
      "61 Train Loss 203.69106 Test MSE 197.96443596772065 Test RE 0.23685511014991417 Lambda1 -1.4737271\n",
      "62 Train Loss 197.94836 Test MSE 190.1153389842838 Test RE 0.23211208340368197 Lambda1 -1.4685992\n",
      "63 Train Loss 194.48242 Test MSE 185.34509284404524 Test RE 0.22918158399978542 Lambda1 -1.463771\n",
      "64 Train Loss 190.8886 Test MSE 177.4246503955639 Test RE 0.22423125607035008 Lambda1 -1.4513179\n",
      "65 Train Loss 183.24995 Test MSE 168.99543452208158 Test RE 0.2188399751048122 Lambda1 -1.421048\n",
      "66 Train Loss 180.05511 Test MSE 165.8692640375103 Test RE 0.21680641562634329 Lambda1 -1.4102802\n",
      "67 Train Loss 173.11588 Test MSE 158.87818996271508 Test RE 0.2121882408774294 Lambda1 -1.3855559\n",
      "68 Train Loss 155.03305 Test MSE 149.0331702476602 Test RE 0.2055089028005986 Lambda1 -1.3663597\n",
      "69 Train Loss 146.84605 Test MSE 143.88001793902254 Test RE 0.20192468417207127 Lambda1 -1.3674388\n",
      "70 Train Loss 139.91542 Test MSE 138.51909939659225 Test RE 0.19812715359211183 Lambda1 -1.3683289\n",
      "71 Train Loss 137.57416 Test MSE 134.8771367374945 Test RE 0.195505211897298 Lambda1 -1.3635552\n",
      "72 Train Loss 134.87933 Test MSE 136.88427737288154 Test RE 0.19695452106956446 Lambda1 -1.3675567\n",
      "73 Train Loss 132.47095 Test MSE 134.61172089415365 Test RE 0.19531275624871525 Lambda1 -1.36828\n",
      "74 Train Loss 128.69693 Test MSE 130.51803153642615 Test RE 0.19231999021021834 Lambda1 -1.3674992\n",
      "Training time: 141.40\n",
      "Training time: 141.40\n",
      "inv_HT_stan_tune4\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 838.0878 Test MSE 858.266594702742 Test RE 0.49317411657105603 Lambda1 -0.17170016\n",
      "1 Train Loss 837.7626 Test MSE 857.6745284770333 Test RE 0.49300398172376286 Lambda1 -0.17395057\n",
      "2 Train Loss 837.4844 Test MSE 856.7827868510462 Test RE 0.4927476219446456 Lambda1 -0.19738623\n",
      "3 Train Loss 833.3968 Test MSE 849.6071790557653 Test RE 0.4906798878852838 Lambda1 -0.47463277\n",
      "4 Train Loss 826.6021 Test MSE 841.7840615276788 Test RE 0.48841559214061997 Lambda1 -0.36552823\n",
      "5 Train Loss 802.2328 Test MSE 802.8088684708664 Test RE 0.4769745987531857 Lambda1 -0.41990876\n",
      "6 Train Loss 773.27094 Test MSE 778.6966196402541 Test RE 0.4697570594402 Lambda1 -0.4195912\n",
      "7 Train Loss 753.95447 Test MSE 757.9455172703921 Test RE 0.4634556329073042 Lambda1 -0.44660482\n",
      "8 Train Loss 730.4883 Test MSE 731.9197354368895 Test RE 0.4554292286518413 Lambda1 -0.49190167\n",
      "9 Train Loss 719.2227 Test MSE 724.4178885353772 Test RE 0.45308924525824756 Lambda1 -0.501326\n",
      "10 Train Loss 705.7782 Test MSE 705.6021963992945 Test RE 0.44716636842429774 Lambda1 -0.58288324\n",
      "11 Train Loss 688.1726 Test MSE 684.0002505384291 Test RE 0.44026818272905976 Lambda1 -0.74206656\n",
      "12 Train Loss 673.17523 Test MSE 665.7798476605707 Test RE 0.4343646696348355 Lambda1 -0.79806143\n",
      "13 Train Loss 651.02527 Test MSE 649.7159083955443 Test RE 0.4290924973214612 Lambda1 -0.8475943\n",
      "14 Train Loss 627.3219 Test MSE 625.0142992026197 Test RE 0.42085660484215814 Lambda1 -0.9573116\n",
      "15 Train Loss 601.71136 Test MSE 600.5834505335426 Test RE 0.4125492966349138 Lambda1 -0.9893567\n",
      "16 Train Loss 576.3564 Test MSE 573.664099766802 Test RE 0.40319766355256714 Lambda1 -1.0455052\n",
      "17 Train Loss 557.9649 Test MSE 550.1437105951015 Test RE 0.3948455491130878 Lambda1 -1.0280879\n",
      "18 Train Loss 515.9238 Test MSE 511.2944796256628 Test RE 0.38064902568126485 Lambda1 -1.0614711\n",
      "19 Train Loss 451.09958 Test MSE 424.3471240525007 Test RE 0.34677661437475316 Lambda1 -1.084048\n",
      "20 Train Loss 382.04602 Test MSE 366.6879858293918 Test RE 0.3223573011589501 Lambda1 -1.0948558\n",
      "21 Train Loss 334.68222 Test MSE 339.5472065191077 Test RE 0.31019818440593705 Lambda1 -1.0943688\n",
      "22 Train Loss 313.93475 Test MSE 327.7953114955116 Test RE 0.3047828595382321 Lambda1 -1.0637026\n",
      "23 Train Loss 295.32376 Test MSE 315.08162742743355 Test RE 0.2988138413946486 Lambda1 -1.049228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 Train Loss 285.3367 Test MSE 299.881587709248 Test RE 0.2915171244514832 Lambda1 -1.0487022\n",
      "25 Train Loss 258.0111 Test MSE 265.95412949723186 Test RE 0.2745317254535287 Lambda1 -0.9426837\n",
      "26 Train Loss 249.29015 Test MSE 259.2709314474491 Test RE 0.27106040606404447 Lambda1 -0.95307773\n",
      "27 Train Loss 238.08534 Test MSE 249.94767099075744 Test RE 0.2661421848400798 Lambda1 -0.9697983\n",
      "28 Train Loss 232.51118 Test MSE 244.09561582916905 Test RE 0.2630081220000831 Lambda1 -0.9702166\n",
      "29 Train Loss 225.57018 Test MSE 232.7446235786219 Test RE 0.2568200932905364 Lambda1 -0.9925127\n",
      "30 Train Loss 214.48363 Test MSE 223.46684108874203 Test RE 0.2516492935639363 Lambda1 -1.0239933\n",
      "31 Train Loss 203.36623 Test MSE 210.59712684025098 Test RE 0.24429545744513465 Lambda1 -1.070333\n",
      "32 Train Loss 196.81523 Test MSE 203.9256722067278 Test RE 0.24039482919318025 Lambda1 -1.1189502\n",
      "33 Train Loss 190.81274 Test MSE 203.6507463127197 Test RE 0.24023272833311524 Lambda1 -1.1294261\n",
      "34 Train Loss 184.09726 Test MSE 191.91486867295487 Test RE 0.23320802028177995 Lambda1 -1.2045946\n",
      "35 Train Loss 176.1557 Test MSE 189.05351323162614 Test RE 0.23146298352832834 Lambda1 -1.2353253\n",
      "36 Train Loss 173.52115 Test MSE 185.70581910439643 Test RE 0.22940449691520645 Lambda1 -1.2643007\n",
      "37 Train Loss 168.3881 Test MSE 175.40825311763604 Test RE 0.22295344246176413 Lambda1 -1.3162411\n",
      "38 Train Loss 164.68857 Test MSE 174.79050014694593 Test RE 0.22256049735367966 Lambda1 -1.3391525\n",
      "39 Train Loss 162.63213 Test MSE 171.83223651833035 Test RE 0.22066908388353562 Lambda1 -1.3414556\n",
      "40 Train Loss 161.26634 Test MSE 170.8617269848657 Test RE 0.22004503129803824 Lambda1 -1.3480518\n",
      "41 Train Loss 155.76082 Test MSE 163.86137064682305 Test RE 0.21549016925539685 Lambda1 -1.3841608\n",
      "42 Train Loss 152.38655 Test MSE 160.3651865680816 Test RE 0.21317890031201508 Lambda1 -1.3912803\n",
      "43 Train Loss 150.17848 Test MSE 159.30578413327976 Test RE 0.21247358366242233 Lambda1 -1.4012455\n",
      "44 Train Loss 144.11382 Test MSE 152.12984507263806 Test RE 0.20763300152290107 Lambda1 -1.4303149\n",
      "45 Train Loss 142.78316 Test MSE 152.38886671868437 Test RE 0.2078096879867733 Lambda1 -1.4433147\n",
      "46 Train Loss 141.94351 Test MSE 149.93244097863206 Test RE 0.20612799382031277 Lambda1 -1.4692923\n",
      "47 Train Loss 140.3466 Test MSE 149.93579606163902 Test RE 0.20613030010124828 Lambda1 -1.4875553\n",
      "48 Train Loss 137.91171 Test MSE 147.5329850503842 Test RE 0.20447194843510078 Lambda1 -1.5137054\n",
      "49 Train Loss 134.88232 Test MSE 144.32298109929434 Test RE 0.20223527788618317 Lambda1 -1.5490633\n",
      "50 Train Loss 133.4126 Test MSE 140.6853803475173 Test RE 0.19967038491855216 Lambda1 -1.5733695\n",
      "51 Train Loss 129.40598 Test MSE 137.33005553638586 Test RE 0.19727496203488898 Lambda1 -1.6132139\n",
      "52 Train Loss 128.05115 Test MSE 135.49332775351706 Test RE 0.19595128926905975 Lambda1 -1.6493601\n",
      "53 Train Loss 126.92374 Test MSE 134.12678414856995 Test RE 0.19496063316649034 Lambda1 -1.6850693\n",
      "54 Train Loss 126.12685 Test MSE 131.81356813617572 Test RE 0.19327212816938694 Lambda1 -1.714139\n",
      "55 Train Loss 124.55141 Test MSE 131.16631557949722 Test RE 0.1927970260018962 Lambda1 -1.7266943\n",
      "56 Train Loss 123.3964 Test MSE 130.89830685760091 Test RE 0.19259995670882343 Lambda1 -1.7392178\n",
      "57 Train Loss 122.9737 Test MSE 131.06199961519386 Test RE 0.19272034546127448 Lambda1 -1.7460506\n",
      "58 Train Loss 121.16766 Test MSE 132.61906008427218 Test RE 0.1938617565227735 Lambda1 -1.7643293\n",
      "59 Train Loss 117.171875 Test MSE 125.50948894123891 Test RE 0.1885938173580718 Lambda1 -1.861148\n",
      "60 Train Loss 114.52978 Test MSE 123.43990172952593 Test RE 0.18703244630681223 Lambda1 -1.9056087\n",
      "61 Train Loss 113.12321 Test MSE 122.09954136350832 Test RE 0.18601423776818063 Lambda1 -1.9479609\n",
      "62 Train Loss 111.21847 Test MSE 120.22239200024667 Test RE 0.18457881477610663 Lambda1 -2.0256066\n",
      "63 Train Loss 109.76136 Test MSE 118.33643482133131 Test RE 0.18312532611429613 Lambda1 -2.0786507\n",
      "64 Train Loss 108.38805 Test MSE 115.8661135182024 Test RE 0.1812038373548107 Lambda1 -2.1098003\n",
      "65 Train Loss 106.724106 Test MSE 112.76015939501659 Test RE 0.17875862736692116 Lambda1 -2.1346126\n",
      "66 Train Loss 106.1257 Test MSE 112.05198311174942 Test RE 0.17819640747489468 Lambda1 -2.1389313\n",
      "67 Train Loss 105.452126 Test MSE 111.16408419851624 Test RE 0.17748898992458376 Lambda1 -2.1292794\n",
      "68 Train Loss 104.224655 Test MSE 108.9368853003338 Test RE 0.17570197673908192 Lambda1 -2.1264052\n",
      "69 Train Loss 102.873604 Test MSE 108.85068697467129 Test RE 0.1756324492629761 Lambda1 -2.1471272\n",
      "70 Train Loss 102.13607 Test MSE 108.12342769513516 Test RE 0.17504474330754863 Lambda1 -2.1625729\n",
      "71 Train Loss 101.36877 Test MSE 108.67039027866119 Test RE 0.17548693307619231 Lambda1 -2.1708274\n",
      "72 Train Loss 99.63982 Test MSE 107.36127694966716 Test RE 0.17442671622567169 Lambda1 -2.1925611\n",
      "73 Train Loss 97.37219 Test MSE 106.73076646459417 Test RE 0.1739137760773472 Lambda1 -2.24573\n",
      "74 Train Loss 96.59004 Test MSE 105.27397016856014 Test RE 0.17272280070036097 Lambda1 -2.2688673\n",
      "Training time: 143.58\n",
      "Training time: 143.58\n",
      "inv_HT_stan_tune4\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 838.0614 Test MSE 858.3665776490575 Test RE 0.49320284165808487 Lambda1 -0.02548092\n",
      "1 Train Loss 838.0521 Test MSE 858.2714646870356 Test RE 0.49317551575565055 Lambda1 -0.02546857\n",
      "2 Train Loss 838.0173 Test MSE 858.0699703586673 Test RE 0.4931176215424418 Lambda1 -0.025495028\n",
      "3 Train Loss 837.8962 Test MSE 857.9054508310419 Test RE 0.4930703460462585 Lambda1 -0.028185414\n",
      "4 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "5 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "6 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "7 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "8 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "9 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "10 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "11 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "12 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "13 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "14 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "15 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "16 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "17 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "18 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "19 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "20 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "21 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "22 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "23 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "24 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "26 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "27 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "28 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "29 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "30 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "31 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "32 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "33 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "34 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "35 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "36 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "37 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "38 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "39 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "40 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "41 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "42 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "43 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "44 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "45 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "46 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "47 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "48 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "49 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "50 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "51 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "52 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "53 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "54 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "55 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "56 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "57 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "58 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "59 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "60 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "61 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "62 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "63 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "64 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "65 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "66 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "67 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "68 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "69 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "70 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "71 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "72 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "73 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "74 Train Loss 837.8949 Test MSE 857.932003487694 Test RE 0.4930779763896076 Lambda1 -0.028652955\n",
      "Training time: 120.27\n",
      "Training time: 120.27\n",
      "inv_HT_stan_tune4\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 838.17145 Test MSE 858.7027916131044 Test RE 0.49329942361518736 Lambda1 -0.054291327\n",
      "1 Train Loss 838.06525 Test MSE 858.2889847784925 Test RE 0.49318054938219025 Lambda1 -0.054195542\n",
      "2 Train Loss 838.0651 Test MSE 858.2840830721248 Test RE 0.4931791410979563 Lambda1 -0.05419418\n",
      "3 Train Loss 838.06323 Test MSE 858.2494492958516 Test RE 0.49316919052990615 Lambda1 -0.054186057\n",
      "4 Train Loss 838.05347 Test MSE 858.1822316282327 Test RE 0.49314987776958435 Lambda1 -0.054182027\n",
      "5 Train Loss 837.8572 Test MSE 857.8866755516887 Test RE 0.49306495058914274 Lambda1 -0.055968467\n",
      "6 Train Loss 837.85504 Test MSE 857.8307963778745 Test RE 0.4930488922264637 Lambda1 -0.056782525\n",
      "7 Train Loss 837.8418 Test MSE 857.7423682839183 Test RE 0.49302347899951876 Lambda1 -0.07003692\n",
      "8 Train Loss 835.6142 Test MSE 851.2585949273608 Test RE 0.49115653356086164 Lambda1 -0.42046842\n",
      "9 Train Loss 830.63824 Test MSE 846.1227836690514 Test RE 0.4896726698699144 Lambda1 -0.5138676\n",
      "10 Train Loss 813.12634 Test MSE 825.2159403863817 Test RE 0.48358517099668347 Lambda1 -0.63957095\n",
      "11 Train Loss 791.04486 Test MSE 800.1740986798413 Test RE 0.4761912547176486 Lambda1 -0.6419549\n",
      "12 Train Loss 766.68414 Test MSE 768.5854866568732 Test RE 0.4666972698565527 Lambda1 -0.5994163\n",
      "13 Train Loss 737.16254 Test MSE 735.8579923956618 Test RE 0.45665285400255007 Lambda1 -0.62559485\n",
      "14 Train Loss 711.58026 Test MSE 713.8078841767302 Test RE 0.4497589774746472 Lambda1 -0.627808\n",
      "15 Train Loss 692.74615 Test MSE 690.6605185991375 Test RE 0.44240648642543495 Lambda1 -0.70673203\n",
      "16 Train Loss 671.06635 Test MSE 666.867649536628 Test RE 0.4347193738654883 Lambda1 -0.7510965\n",
      "17 Train Loss 661.61615 Test MSE 655.2206924459218 Test RE 0.4309064281386351 Lambda1 -0.74188066\n",
      "18 Train Loss 645.9819 Test MSE 643.7432230632731 Test RE 0.4271156705700542 Lambda1 -0.7381064\n",
      "19 Train Loss 639.3948 Test MSE 639.9360617398276 Test RE 0.42585079541041987 Lambda1 -0.7635157\n",
      "20 Train Loss 633.9929 Test MSE 635.2890034574025 Test RE 0.42430176625204213 Lambda1 -0.80206543\n",
      "21 Train Loss 631.2969 Test MSE 632.1361540785073 Test RE 0.4232475818869996 Lambda1 -0.82446575\n",
      "22 Train Loss 628.0483 Test MSE 629.4071656049479 Test RE 0.42233299484470577 Lambda1 -0.8627256\n",
      "23 Train Loss 624.8714 Test MSE 627.4383349168759 Test RE 0.42167193355172006 Lambda1 -0.87268794\n",
      "24 Train Loss 621.80817 Test MSE 627.4117126533095 Test RE 0.42166298766826826 Lambda1 -0.85023123\n",
      "25 Train Loss 617.9551 Test MSE 623.5672930623173 Test RE 0.42036914801673636 Lambda1 -0.86447173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 Train Loss 616.85706 Test MSE 622.3477798406876 Test RE 0.4199578879723624 Lambda1 -0.8358475\n",
      "27 Train Loss 615.5883 Test MSE 620.9277290006727 Test RE 0.4194784919267078 Lambda1 -0.8175616\n",
      "28 Train Loss 614.3101 Test MSE 619.7321193914022 Test RE 0.41907443993079474 Lambda1 -0.81345844\n",
      "29 Train Loss 610.54193 Test MSE 615.9278067344369 Test RE 0.41778618620126234 Lambda1 -0.77354246\n",
      "30 Train Loss 606.9388 Test MSE 609.9688657412304 Test RE 0.4157602880535336 Lambda1 -0.710736\n",
      "31 Train Loss 601.17194 Test MSE 601.5454984178534 Test RE 0.41287958659199275 Lambda1 -0.6765718\n",
      "32 Train Loss 591.89087 Test MSE 598.3961600943452 Test RE 0.41179737096877184 Lambda1 -0.63540685\n",
      "33 Train Loss 579.7266 Test MSE 582.8174019881366 Test RE 0.4064016157687525 Lambda1 -0.67153704\n",
      "34 Train Loss 562.0597 Test MSE 567.8687779791708 Test RE 0.40115588369469685 Lambda1 -0.7154635\n",
      "35 Train Loss 549.7813 Test MSE 554.9880046367841 Test RE 0.3965801465125934 Lambda1 -0.8294989\n",
      "36 Train Loss 537.89685 Test MSE 541.5051272032937 Test RE 0.3917332693845059 Lambda1 -0.90165615\n",
      "37 Train Loss 519.89575 Test MSE 516.4119025194923 Test RE 0.382549194992788 Lambda1 -0.87172884\n",
      "38 Train Loss 502.65283 Test MSE 496.351790156152 Test RE 0.3750455068931254 Lambda1 -0.7964793\n",
      "39 Train Loss 489.2691 Test MSE 486.0844477157127 Test RE 0.37114621306200474 Lambda1 -0.7372048\n",
      "40 Train Loss 458.05423 Test MSE 457.6925009793608 Test RE 0.36014390481732056 Lambda1 -0.6327578\n",
      "41 Train Loss 422.71558 Test MSE 423.052538285977 Test RE 0.34624724235341475 Lambda1 -0.5702407\n",
      "42 Train Loss 388.1328 Test MSE 375.91110139439724 Test RE 0.32638616872904036 Lambda1 -0.5492928\n",
      "43 Train Loss 353.953 Test MSE 350.1904665411944 Test RE 0.31502232325145535 Lambda1 -0.54945475\n",
      "44 Train Loss 345.5106 Test MSE 344.2423844524898 Test RE 0.3123354945401872 Lambda1 -0.5583427\n",
      "45 Train Loss 331.15582 Test MSE 332.4267596457361 Test RE 0.30692845890039405 Lambda1 -0.5785461\n",
      "46 Train Loss 315.69208 Test MSE 326.7227359415505 Test RE 0.3042838125949143 Lambda1 -0.5972917\n",
      "47 Train Loss 301.33228 Test MSE 315.91946213764396 Test RE 0.29921086612776704 Lambda1 -0.668751\n",
      "48 Train Loss 291.77582 Test MSE 312.51102574440097 Test RE 0.2975924047488343 Lambda1 -0.6764042\n",
      "49 Train Loss 280.67126 Test MSE 302.77103879248835 Test RE 0.29291818608450587 Lambda1 -0.69246405\n",
      "50 Train Loss 271.99988 Test MSE 296.32825683414626 Test RE 0.28978486803283865 Lambda1 -0.713867\n",
      "51 Train Loss 267.89053 Test MSE 292.60618044129376 Test RE 0.28795917335858573 Lambda1 -0.7562419\n",
      "52 Train Loss 265.09814 Test MSE 290.6310261589694 Test RE 0.2869856346827019 Lambda1 -0.75704956\n",
      "53 Train Loss 262.69904 Test MSE 288.4171189311905 Test RE 0.28589047600905 Lambda1 -0.7854124\n",
      "54 Train Loss 257.773 Test MSE 282.7163232122638 Test RE 0.2830509472917071 Lambda1 -0.84070903\n",
      "55 Train Loss 251.48337 Test MSE 273.327789329058 Test RE 0.27831144425887133 Lambda1 -0.8488335\n",
      "56 Train Loss 247.80753 Test MSE 266.34560549316706 Test RE 0.27473370212685366 Lambda1 -0.8088216\n",
      "57 Train Loss 245.56358 Test MSE 261.04857998756904 Test RE 0.27198805927265124 Lambda1 -0.7911586\n",
      "58 Train Loss 236.83853 Test MSE 248.183912192803 Test RE 0.26520150463029735 Lambda1 -0.8195816\n",
      "59 Train Loss 227.13701 Test MSE 237.2626411781228 Test RE 0.2593007965781743 Lambda1 -0.8748945\n",
      "60 Train Loss 221.82559 Test MSE 233.31990984344395 Test RE 0.257137294791066 Lambda1 -0.8946923\n",
      "61 Train Loss 217.86877 Test MSE 231.54004030714586 Test RE 0.256154637518079 Lambda1 -0.9244814\n",
      "62 Train Loss 214.03548 Test MSE 224.77506366514658 Test RE 0.25238482298201065 Lambda1 -0.8956425\n",
      "63 Train Loss 210.46948 Test MSE 217.64532609162674 Test RE 0.24834981637167036 Lambda1 -0.88076615\n",
      "64 Train Loss 207.54446 Test MSE 217.06679530489632 Test RE 0.24801952299195068 Lambda1 -0.8854901\n",
      "65 Train Loss 204.93498 Test MSE 216.12842313851118 Test RE 0.24748285249885713 Lambda1 -0.8964305\n",
      "66 Train Loss 202.10222 Test MSE 214.89512937765573 Test RE 0.24677573652087725 Lambda1 -0.89944977\n",
      "67 Train Loss 195.76303 Test MSE 206.88053480858005 Test RE 0.2421302139343149 Lambda1 -0.8908866\n",
      "68 Train Loss 193.9442 Test MSE 203.42793935924118 Test RE 0.24010127737750572 Lambda1 -0.890137\n",
      "69 Train Loss 189.79979 Test MSE 200.17093051029383 Test RE 0.23817143572374855 Lambda1 -0.8886557\n",
      "70 Train Loss 187.114 Test MSE 198.3443719288787 Test RE 0.2370822889294844 Lambda1 -0.8951781\n",
      "71 Train Loss 184.21402 Test MSE 193.96533379764813 Test RE 0.23445053586374423 Lambda1 -0.90795475\n",
      "72 Train Loss 182.72357 Test MSE 190.95502134062033 Test RE 0.23262410334891154 Lambda1 -0.9236043\n",
      "73 Train Loss 178.92897 Test MSE 189.08692146769056 Test RE 0.23148343389783335 Lambda1 -0.96517247\n",
      "74 Train Loss 173.59097 Test MSE 187.2924873103287 Test RE 0.23038242708473441 Lambda1 -1.0014254\n",
      "Training time: 142.44\n",
      "Training time: 142.44\n",
      "inv_HT_stan_tune4\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 839.70624 Test MSE 858.5792100548495 Test RE 0.49326392535805114 Lambda1 0.0051894193\n",
      "1 Train Loss 837.9272 Test MSE 858.0276008719891 Test RE 0.4931054468951879 Lambda1 0.0046914206\n",
      "2 Train Loss 837.90656 Test MSE 857.9436347740952 Test RE 0.49308131879419215 Lambda1 -0.0011248055\n",
      "3 Train Loss 837.9038 Test MSE 857.9655289804907 Test RE 0.4930876103236425 Lambda1 -0.009402998\n",
      "4 Train Loss 837.88 Test MSE 857.9203436277852 Test RE 0.4930746257513055 Lambda1 -0.0992642\n",
      "5 Train Loss 837.8791 Test MSE 857.893838724966 Test RE 0.4930670090791544 Lambda1 -0.10780372\n",
      "6 Train Loss 837.8786 Test MSE 857.8781760900525 Test RE 0.49306250807713553 Lambda1 -0.118431814\n",
      "7 Train Loss 837.87286 Test MSE 857.8327506794043 Test RE 0.49304945385566834 Lambda1 -0.2571951\n",
      "8 Train Loss 837.71844 Test MSE 857.157650685961 Test RE 0.49285540484265405 Lambda1 -1.0397086\n",
      "9 Train Loss 833.0991 Test MSE 850.8441210704718 Test RE 0.49103694808498016 Lambda1 -1.5577755\n",
      "10 Train Loss 825.9734 Test MSE 838.4030380095481 Test RE 0.48743374526746885 Lambda1 -2.2674506\n",
      "11 Train Loss 807.18866 Test MSE 808.9331554630679 Test RE 0.4787904602745209 Lambda1 -2.7456994\n",
      "12 Train Loss 786.3034 Test MSE 794.6563962906982 Test RE 0.47454659585757364 Lambda1 -2.2988036\n",
      "13 Train Loss 766.7176 Test MSE 774.9348077501936 Test RE 0.4686210091237228 Lambda1 -2.1071348\n",
      "14 Train Loss 745.45105 Test MSE 745.1419786725653 Test RE 0.45952451566961405 Lambda1 -2.213286\n",
      "15 Train Loss 724.722 Test MSE 718.2916562274085 Test RE 0.4511693428186059 Lambda1 -2.1682253\n",
      "16 Train Loss 699.3446 Test MSE 696.0193765617954 Test RE 0.44411949335666967 Lambda1 -2.2708795\n",
      "17 Train Loss 685.5353 Test MSE 681.8318760877364 Test RE 0.439569773565109 Lambda1 -2.220223\n",
      "18 Train Loss 678.58075 Test MSE 674.4298026421104 Test RE 0.4371772433323572 Lambda1 -2.2818198\n",
      "19 Train Loss 669.56586 Test MSE 663.2095323116722 Test RE 0.4335254028603435 Lambda1 -2.4327672\n",
      "20 Train Loss 665.1097 Test MSE 657.7022824649126 Test RE 0.43172166681729274 Lambda1 -2.5142813\n",
      "21 Train Loss 659.00665 Test MSE 654.123902818818 Test RE 0.4305456247851659 Lambda1 -2.576973\n",
      "22 Train Loss 653.98694 Test MSE 651.1124717744177 Test RE 0.42955341661052165 Lambda1 -2.5587888\n",
      "23 Train Loss 651.3135 Test MSE 648.1495145718729 Test RE 0.4285749377045574 Lambda1 -2.6122744\n",
      "24 Train Loss 649.64886 Test MSE 646.3069104184175 Test RE 0.42796531292599116 Lambda1 -2.6639922\n",
      "25 Train Loss 647.7487 Test MSE 644.4864640291892 Test RE 0.427362165038822 Lambda1 -2.7387588\n",
      "26 Train Loss 645.78235 Test MSE 642.3376438017032 Test RE 0.42664912352852974 Lambda1 -2.8122172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 Train Loss 644.8644 Test MSE 640.4790231585175 Test RE 0.42603141621082563 Lambda1 -2.92128\n",
      "28 Train Loss 638.69006 Test MSE 630.5476149104003 Test RE 0.4227154431343059 Lambda1 -3.2547371\n",
      "29 Train Loss 635.9137 Test MSE 627.7091488861424 Test RE 0.4217629244302079 Lambda1 -3.3733025\n",
      "30 Train Loss 633.17615 Test MSE 622.86954836217 Test RE 0.4201338947852743 Lambda1 -3.5795748\n",
      "31 Train Loss 630.5347 Test MSE 620.5923299269002 Test RE 0.41936518429419933 Lambda1 -3.7453694\n",
      "32 Train Loss 624.5556 Test MSE 618.5887007300305 Test RE 0.4186876609117768 Lambda1 -3.9796152\n",
      "33 Train Loss 623.21747 Test MSE 617.2772460568219 Test RE 0.4182436007262545 Lambda1 -4.0366707\n",
      "34 Train Loss 621.74274 Test MSE 614.4919692838012 Test RE 0.4172989350520443 Lambda1 -4.1172023\n",
      "35 Train Loss 620.27496 Test MSE 612.8910741236191 Test RE 0.41675499993622467 Lambda1 -4.181085\n",
      "36 Train Loss 619.34424 Test MSE 612.0315207099299 Test RE 0.416462656904726 Lambda1 -4.243062\n",
      "37 Train Loss 618.16394 Test MSE 611.2266962193223 Test RE 0.416188741593593 Lambda1 -4.304151\n",
      "38 Train Loss 617.0273 Test MSE 609.6994631966204 Test RE 0.4156684643097476 Lambda1 -4.340552\n",
      "39 Train Loss 615.921 Test MSE 607.7360340489754 Test RE 0.414998631266576 Lambda1 -4.3839664\n",
      "40 Train Loss 614.99207 Test MSE 607.376431099846 Test RE 0.41487583386297705 Lambda1 -4.4144654\n",
      "41 Train Loss 614.16394 Test MSE 606.6933594702981 Test RE 0.4146424780566202 Lambda1 -4.4625797\n",
      "42 Train Loss 613.5583 Test MSE 605.5657391172062 Test RE 0.41425696472633533 Lambda1 -4.480094\n",
      "43 Train Loss 613.0278 Test MSE 605.2079589860035 Test RE 0.4141345710704033 Lambda1 -4.5161786\n",
      "44 Train Loss 612.61035 Test MSE 605.4634433585426 Test RE 0.4142219738760161 Lambda1 -4.559983\n",
      "45 Train Loss 612.0835 Test MSE 604.7458366833039 Test RE 0.4139764292563572 Lambda1 -4.6074543\n",
      "46 Train Loss 610.64594 Test MSE 602.4354269072849 Test RE 0.41318488146385385 Lambda1 -4.592123\n",
      "47 Train Loss 609.3141 Test MSE 601.2837887239534 Test RE 0.4127897626753627 Lambda1 -4.569935\n",
      "48 Train Loss 607.3133 Test MSE 600.4531936620515 Test RE 0.4125045565620091 Lambda1 -4.53461\n",
      "49 Train Loss 605.9085 Test MSE 599.2231852209862 Test RE 0.412081839022781 Lambda1 -4.500289\n",
      "50 Train Loss 604.9904 Test MSE 597.4586406471268 Test RE 0.4114746588567205 Lambda1 -4.4620075\n",
      "51 Train Loss 604.115 Test MSE 595.6731479472214 Test RE 0.4108593570860308 Lambda1 -4.42959\n",
      "52 Train Loss 602.4445 Test MSE 595.2769820245384 Test RE 0.41072270870002164 Lambda1 -4.4139614\n",
      "53 Train Loss 600.155 Test MSE 592.5929739730003 Test RE 0.40979572133393316 Lambda1 -4.2497573\n",
      "54 Train Loss 599.12555 Test MSE 591.6716488112689 Test RE 0.4094770354993312 Lambda1 -4.173391\n",
      "55 Train Loss 596.54767 Test MSE 586.3843860529912 Test RE 0.40764335702853205 Lambda1 -4.1940746\n",
      "56 Train Loss 591.88043 Test MSE 580.6154031135746 Test RE 0.4056331565812854 Lambda1 -4.1336956\n",
      "57 Train Loss 588.8081 Test MSE 576.4386172538809 Test RE 0.4041715170772008 Lambda1 -4.080654\n",
      "58 Train Loss 584.7853 Test MSE 570.3030683779936 Test RE 0.40201478425868553 Lambda1 -4.1174393\n",
      "59 Train Loss 577.1415 Test MSE 560.7880198084375 Test RE 0.39864703124254885 Lambda1 -4.0860586\n",
      "60 Train Loss 571.57556 Test MSE 554.6008211182767 Test RE 0.39644178668750796 Lambda1 -4.090838\n",
      "61 Train Loss 562.4341 Test MSE 547.7867593096113 Test RE 0.39399883341728775 Lambda1 -3.997344\n",
      "62 Train Loss 550.83276 Test MSE 535.1740468510009 Test RE 0.38943653528160466 Lambda1 -3.8657565\n",
      "63 Train Loss 534.6882 Test MSE 518.7360306062328 Test RE 0.38340906598035784 Lambda1 -3.796809\n",
      "64 Train Loss 526.37787 Test MSE 510.5467258497553 Test RE 0.38037057959956455 Lambda1 -3.8376002\n",
      "65 Train Loss 509.65753 Test MSE 491.8520933349102 Test RE 0.3733416415347699 Lambda1 -3.8188388\n",
      "66 Train Loss 497.3144 Test MSE 480.4142002788484 Test RE 0.36897512506236024 Lambda1 -3.7990894\n",
      "67 Train Loss 483.33728 Test MSE 463.1502054900193 Test RE 0.362284789978528 Lambda1 -3.8102794\n",
      "68 Train Loss 473.26544 Test MSE 450.037844632197 Test RE 0.3571196017901506 Lambda1 -3.8570757\n",
      "69 Train Loss 463.50967 Test MSE 427.8327640249984 Test RE 0.348197934705172 Lambda1 -3.9257696\n",
      "70 Train Loss 456.81293 Test MSE 416.77126773706505 Test RE 0.34366717752227927 Lambda1 -3.930191\n",
      "71 Train Loss 444.66113 Test MSE 407.24025707083314 Test RE 0.3397148425192241 Lambda1 -4.003534\n",
      "72 Train Loss 434.74124 Test MSE 386.5067041292186 Test RE 0.33095403951178676 Lambda1 -4.019378\n",
      "73 Train Loss 425.32928 Test MSE 379.5553048330369 Test RE 0.32796439937073163 Lambda1 -4.004383\n",
      "74 Train Loss 418.41376 Test MSE 382.5470645342628 Test RE 0.32925441526555876 Lambda1 -4.0313606\n",
      "Training time: 144.27\n",
      "Training time: 144.27\n",
      "inv_HT_stan_tune4\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 838.9489 Test MSE 860.0612821741419 Test RE 0.493689475785098 Lambda1 -0.028872328\n",
      "1 Train Loss 838.09644 Test MSE 858.3195078997718 Test RE 0.49318931873386285 Lambda1 -0.0287053\n",
      "2 Train Loss 838.0459 Test MSE 858.1037452557724 Test RE 0.49312732636466833 Lambda1 -0.028634138\n",
      "3 Train Loss 837.8568 Test MSE 857.8367560584023 Test RE 0.4930506049236478 Lambda1 -0.030535053\n",
      "4 Train Loss 837.5104 Test MSE 857.227660375632 Test RE 0.4928755317991081 Lambda1 -0.035088763\n",
      "5 Train Loss 831.93896 Test MSE 847.6302617415465 Test RE 0.4901086836095531 Lambda1 0.0330583\n",
      "6 Train Loss 759.2487 Test MSE 755.9426979845973 Test RE 0.46284290292993646 Lambda1 0.041852545\n",
      "7 Train Loss 728.8895 Test MSE 722.8975645317055 Test RE 0.45261355004031384 Lambda1 0.014256917\n",
      "8 Train Loss 673.04956 Test MSE 667.9598867047797 Test RE 0.43507523337988985 Lambda1 0.003971772\n",
      "9 Train Loss 654.8277 Test MSE 658.5714566460736 Test RE 0.43200683948949004 Lambda1 0.0008061474\n",
      "10 Train Loss 648.4229 Test MSE 652.14351179554 Test RE 0.4298933821258717 Lambda1 -0.0011032117\n",
      "11 Train Loss 643.9391 Test MSE 648.2936251571356 Test RE 0.4286225800702611 Lambda1 -0.0016432878\n",
      "12 Train Loss 639.4037 Test MSE 644.9391778304696 Test RE 0.42751223707072483 Lambda1 -0.0007017594\n",
      "13 Train Loss 633.7443 Test MSE 639.5980020824179 Test RE 0.42573829824038856 Lambda1 -0.00015033339\n",
      "14 Train Loss 629.4947 Test MSE 636.0210278922489 Test RE 0.4245461509627151 Lambda1 -0.00019271576\n",
      "15 Train Loss 613.6075 Test MSE 602.6211865631441 Test RE 0.41324857888453137 Lambda1 0.0004321976\n",
      "16 Train Loss 580.7848 Test MSE 563.1936227427767 Test RE 0.3995011509787001 Lambda1 0.00045400666\n",
      "17 Train Loss 551.3523 Test MSE 508.408153980585 Test RE 0.3795730977912395 Lambda1 0.00028953396\n",
      "18 Train Loss 387.31082 Test MSE 361.5352791529014 Test RE 0.32008440268789123 Lambda1 -0.00045256832\n",
      "19 Train Loss 305.35046 Test MSE 298.7497339634846 Test RE 0.2909664626374063 Lambda1 -0.0013979379\n",
      "20 Train Loss 282.53003 Test MSE 291.6487667084049 Test RE 0.2874876830482156 Lambda1 -0.00043069528\n",
      "21 Train Loss 276.97256 Test MSE 288.82847244249166 Test RE 0.28609427831649914 Lambda1 -0.00024576153\n",
      "22 Train Loss 272.8849 Test MSE 286.5737618723771 Test RE 0.2849754072709505 Lambda1 -0.0007062347\n",
      "23 Train Loss 265.31723 Test MSE 283.5766871767908 Test RE 0.283481311266131 Lambda1 0.00079950853\n",
      "24 Train Loss 261.97192 Test MSE 282.8486809139612 Test RE 0.28311719671908936 Lambda1 0.001418221\n",
      "25 Train Loss 260.39066 Test MSE 282.99861679915045 Test RE 0.28319222588897613 Lambda1 0.0013160374\n",
      "26 Train Loss 257.48193 Test MSE 282.4421332295856 Test RE 0.2829136567386434 Lambda1 0.0018316483\n",
      "27 Train Loss 256.41006 Test MSE 280.95276964086526 Test RE 0.2821667457865673 Lambda1 0.0018138328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 Train Loss 255.7719 Test MSE 280.4506910509847 Test RE 0.28191450902864207 Lambda1 0.00273224\n",
      "29 Train Loss 254.97151 Test MSE 278.19442949769746 Test RE 0.2807781998977114 Lambda1 0.00557261\n",
      "30 Train Loss 254.03519 Test MSE 276.63797899281417 Test RE 0.2799916450641171 Lambda1 0.00842204\n",
      "31 Train Loss 252.99854 Test MSE 275.57936371815134 Test RE 0.2794554071617875 Lambda1 0.011556528\n",
      "32 Train Loss 252.29892 Test MSE 274.48584465242817 Test RE 0.27890040629876023 Lambda1 0.0145392995\n",
      "33 Train Loss 251.63751 Test MSE 273.27740045795025 Test RE 0.2782857892643318 Lambda1 0.017834937\n",
      "34 Train Loss 250.48776 Test MSE 271.4910875114151 Test RE 0.27737477261636356 Lambda1 0.023486607\n",
      "35 Train Loss 248.36789 Test MSE 268.1265213641811 Test RE 0.27565067322854775 Lambda1 0.03129897\n",
      "36 Train Loss 246.1762 Test MSE 265.06646785289087 Test RE 0.27407319720588613 Lambda1 0.037692606\n",
      "37 Train Loss 243.61536 Test MSE 261.21616459620867 Test RE 0.27207534896302077 Lambda1 0.044862803\n",
      "38 Train Loss 240.88525 Test MSE 259.18376189513003 Test RE 0.2710148355859694 Lambda1 0.05002884\n",
      "39 Train Loss 238.1876 Test MSE 256.126007520855 Test RE 0.26941142593840056 Lambda1 0.05999226\n",
      "40 Train Loss 236.55252 Test MSE 253.78520679408396 Test RE 0.26817749038209254 Lambda1 0.06843218\n",
      "41 Train Loss 234.04796 Test MSE 249.2730684116735 Test RE 0.2657827865888692 Lambda1 0.084717534\n",
      "42 Train Loss 231.48175 Test MSE 244.83360732534234 Test RE 0.2634054074466038 Lambda1 0.098173074\n",
      "43 Train Loss 227.67233 Test MSE 241.34400711440955 Test RE 0.26152151907707366 Lambda1 0.11576383\n",
      "44 Train Loss 223.50276 Test MSE 240.23975740943047 Test RE 0.26092254802804343 Lambda1 0.13607109\n",
      "45 Train Loss 218.6365 Test MSE 236.58057746262514 Test RE 0.25892781971083784 Lambda1 0.14141548\n",
      "46 Train Loss 216.18716 Test MSE 234.87555077379383 Test RE 0.25799309134644255 Lambda1 0.1471537\n",
      "47 Train Loss 215.21707 Test MSE 233.77338981894655 Test RE 0.2573870591813759 Lambda1 0.14940695\n",
      "48 Train Loss 213.99013 Test MSE 233.11186463264409 Test RE 0.25702262795157776 Lambda1 0.14972194\n",
      "49 Train Loss 211.36133 Test MSE 230.3658586806588 Test RE 0.2555043086248328 Lambda1 0.16411003\n",
      "50 Train Loss 209.46492 Test MSE 228.13255987649168 Test RE 0.25426278961670146 Lambda1 0.17517139\n",
      "51 Train Loss 206.90015 Test MSE 225.32461415529318 Test RE 0.2526931613023284 Lambda1 0.18507516\n",
      "52 Train Loss 206.14145 Test MSE 225.22976580660833 Test RE 0.2526399712593259 Lambda1 0.18861903\n",
      "53 Train Loss 205.14133 Test MSE 224.13671323436628 Test RE 0.25202618776086066 Lambda1 0.18703753\n",
      "54 Train Loss 203.62218 Test MSE 223.88685263938285 Test RE 0.25188567313533283 Lambda1 0.18898164\n",
      "55 Train Loss 203.01282 Test MSE 223.76960265046205 Test RE 0.2518197079842995 Lambda1 0.1891522\n",
      "56 Train Loss 202.36407 Test MSE 222.92453643435053 Test RE 0.25134375946519893 Lambda1 0.19210385\n",
      "57 Train Loss 201.76569 Test MSE 221.62454236734226 Test RE 0.25060982691379624 Lambda1 0.19375236\n",
      "58 Train Loss 200.63942 Test MSE 220.121028011301 Test RE 0.24975830411705255 Lambda1 0.19876601\n",
      "59 Train Loss 198.8634 Test MSE 217.7347510807761 Test RE 0.24840083147788616 Lambda1 0.20397356\n",
      "60 Train Loss 197.70107 Test MSE 214.90532145242793 Test RE 0.24678158850771442 Lambda1 0.21501577\n",
      "61 Train Loss 196.32428 Test MSE 210.82783995201592 Test RE 0.2444292359491898 Lambda1 0.2315034\n",
      "62 Train Loss 194.10161 Test MSE 206.5734007658469 Test RE 0.24195041438700915 Lambda1 0.24710695\n",
      "63 Train Loss 190.9682 Test MSE 202.79413047146568 Test RE 0.23972695061982446 Lambda1 0.26208428\n",
      "64 Train Loss 185.81073 Test MSE 195.3875966226856 Test RE 0.23530852745579514 Lambda1 0.28380105\n",
      "65 Train Loss 178.17651 Test MSE 185.8270730714612 Test RE 0.2294793778919595 Lambda1 0.3009201\n",
      "66 Train Loss 171.86844 Test MSE 176.0534535953871 Test RE 0.22336310856344915 Lambda1 0.31236565\n",
      "67 Train Loss 167.22606 Test MSE 168.47152507265642 Test RE 0.21850049454795742 Lambda1 0.32054648\n",
      "68 Train Loss 159.87708 Test MSE 160.53158723221807 Test RE 0.21328947266901838 Lambda1 0.32911375\n",
      "69 Train Loss 157.06682 Test MSE 156.55181070614287 Test RE 0.2106290253749877 Lambda1 0.32623008\n",
      "70 Train Loss 151.13101 Test MSE 149.83409584855644 Test RE 0.20606038000197438 Lambda1 0.3374746\n",
      "71 Train Loss 147.81937 Test MSE 144.27113330995255 Test RE 0.2021989482805483 Lambda1 0.33974418\n",
      "72 Train Loss 143.84639 Test MSE 138.9337351564162 Test RE 0.19842346369933792 Lambda1 0.34873742\n",
      "73 Train Loss 137.56325 Test MSE 135.3372210570829 Test RE 0.1958383755013682 Lambda1 0.34538585\n",
      "74 Train Loss 134.95358 Test MSE 133.81560757321253 Test RE 0.194734345737542 Lambda1 0.34422475\n",
      "Training time: 141.14\n",
      "Training time: 141.14\n",
      "inv_HT_stan_tune4\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 838.0848 Test MSE 858.0936036531689 Test RE 0.4931244123136685 Lambda1 -0.14252976\n",
      "1 Train Loss 838.02716 Test MSE 858.2950559201786 Test RE 0.49318229364534083 Lambda1 -0.14284012\n",
      "2 Train Loss 837.8866 Test MSE 857.9349860279855 Test RE 0.4930788334643218 Lambda1 -0.1459732\n",
      "3 Train Loss 837.87695 Test MSE 857.8774699957874 Test RE 0.49306230516447813 Lambda1 -0.14807309\n",
      "4 Train Loss 837.87537 Test MSE 857.8703674669378 Test RE 0.4930602640824949 Lambda1 -0.15036125\n",
      "5 Train Loss 837.8639 Test MSE 857.8694328405497 Test RE 0.493059995494561 Lambda1 -0.16703483\n",
      "6 Train Loss 837.59924 Test MSE 857.7863246233746 Test RE 0.49303611171451794 Lambda1 -0.4238545\n",
      "7 Train Loss 835.1923 Test MSE 854.7120204079863 Test RE 0.49215179851373925 Lambda1 -0.7384645\n",
      "8 Train Loss 829.304 Test MSE 846.28347962766 Test RE 0.4897191670715198 Lambda1 -0.761017\n",
      "9 Train Loss 808.73517 Test MSE 817.2139739257045 Test RE 0.4812348414247066 Lambda1 -0.63575846\n",
      "10 Train Loss 787.0605 Test MSE 794.8550834569786 Test RE 0.474605917361809 Lambda1 -0.54832137\n",
      "11 Train Loss 765.17865 Test MSE 769.5646687622985 Test RE 0.46699446266502936 Lambda1 -0.5413141\n",
      "12 Train Loss 733.26965 Test MSE 737.3704977265135 Test RE 0.4571219223085958 Lambda1 -0.5088722\n",
      "13 Train Loss 679.9678 Test MSE 667.2012628531627 Test RE 0.4348280986165189 Lambda1 -0.6149746\n",
      "14 Train Loss 656.6259 Test MSE 631.4999037902627 Test RE 0.42303452716028794 Lambda1 -0.65231186\n",
      "15 Train Loss 621.8796 Test MSE 605.3625812339739 Test RE 0.41418747051425225 Lambda1 -0.70570195\n",
      "16 Train Loss 601.3796 Test MSE 594.5321884657893 Test RE 0.41046568601857397 Lambda1 -0.74815565\n",
      "17 Train Loss 570.95667 Test MSE 570.6786534466585 Test RE 0.4021471401124155 Lambda1 -0.81854457\n",
      "18 Train Loss 545.2204 Test MSE 533.2560486285822 Test RE 0.3887380624923996 Lambda1 -0.88982743\n",
      "19 Train Loss 526.7303 Test MSE 519.0522876328928 Test RE 0.38352592439023986 Lambda1 -0.941919\n",
      "20 Train Loss 505.05914 Test MSE 496.5982179724094 Test RE 0.3751385962892036 Lambda1 -1.0329238\n",
      "21 Train Loss 461.32483 Test MSE 451.19077518284826 Test RE 0.35757675305063613 Lambda1 -1.1658034\n",
      "22 Train Loss 439.92834 Test MSE 424.4580214509211 Test RE 0.34682192411056356 Lambda1 -1.2365636\n",
      "23 Train Loss 424.7859 Test MSE 407.2962153461926 Test RE 0.3397381815720157 Lambda1 -1.3043782\n",
      "24 Train Loss 412.4755 Test MSE 394.176860385514 Test RE 0.3342217690699175 Lambda1 -1.3378541\n",
      "25 Train Loss 387.72183 Test MSE 362.62544759971877 Test RE 0.3205666284628374 Lambda1 -1.3978633\n",
      "26 Train Loss 354.03677 Test MSE 348.9792492843982 Test RE 0.31447706144114096 Lambda1 -1.429865\n",
      "27 Train Loss 327.77692 Test MSE 308.77595519016336 Test RE 0.29580867597443666 Lambda1 -1.4768443\n",
      "28 Train Loss 302.4007 Test MSE 284.36390311827057 Test RE 0.28387451415778836 Lambda1 -1.4904954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 Train Loss 282.8698 Test MSE 272.51274717771497 Test RE 0.2778961829476998 Lambda1 -1.5045693\n",
      "30 Train Loss 268.79678 Test MSE 253.89689761138447 Test RE 0.2682364963198548 Lambda1 -1.5416718\n",
      "31 Train Loss 253.82784 Test MSE 246.4558753205802 Test RE 0.26427662904119636 Lambda1 -1.5615481\n",
      "32 Train Loss 242.95381 Test MSE 230.3330806523179 Test RE 0.2554861305288056 Lambda1 -1.5837382\n",
      "33 Train Loss 236.76894 Test MSE 226.77337526114945 Test RE 0.25350422548061713 Lambda1 -1.5906304\n",
      "34 Train Loss 228.8075 Test MSE 216.07877731366418 Test RE 0.24745442681606605 Lambda1 -1.5698737\n",
      "35 Train Loss 214.50761 Test MSE 206.38931344597893 Test RE 0.2418425836340913 Lambda1 -1.5655694\n",
      "36 Train Loss 208.85545 Test MSE 207.5238209796249 Test RE 0.24250636852213842 Lambda1 -1.532757\n",
      "37 Train Loss 202.09218 Test MSE 200.04728472240873 Test RE 0.23809786499146268 Lambda1 -1.5012887\n",
      "38 Train Loss 190.66158 Test MSE 190.46646921338024 Test RE 0.23232633221756305 Lambda1 -1.5044279\n",
      "39 Train Loss 184.52065 Test MSE 188.2705333135596 Test RE 0.2309831751930711 Lambda1 -1.4841603\n",
      "40 Train Loss 175.20712 Test MSE 180.07912917051746 Test RE 0.2259024086515207 Lambda1 -1.4701884\n",
      "41 Train Loss 169.41324 Test MSE 174.78807281373903 Test RE 0.22255895198833983 Lambda1 -1.4731495\n",
      "42 Train Loss 164.02376 Test MSE 168.90124617231925 Test RE 0.2187789821880897 Lambda1 -1.4835068\n",
      "43 Train Loss 160.97559 Test MSE 169.91568191826133 Test RE 0.2194350015437813 Lambda1 -1.4784393\n",
      "44 Train Loss 158.76875 Test MSE 166.43772274487574 Test RE 0.21717761182376838 Lambda1 -1.4892012\n",
      "45 Train Loss 155.59592 Test MSE 162.10073069264655 Test RE 0.21432935494001587 Lambda1 -1.4866059\n",
      "46 Train Loss 152.63948 Test MSE 159.65530473818 Test RE 0.21270654195184027 Lambda1 -1.489487\n",
      "47 Train Loss 151.44537 Test MSE 158.07091560494047 Test RE 0.21164848060356084 Lambda1 -1.4912302\n",
      "48 Train Loss 149.71431 Test MSE 158.54398929609164 Test RE 0.21196495414909997 Lambda1 -1.5055112\n",
      "49 Train Loss 146.80116 Test MSE 156.06623302326196 Test RE 0.21030211703401827 Lambda1 -1.5147021\n",
      "50 Train Loss 144.39761 Test MSE 155.29327758210852 Test RE 0.20978068477225378 Lambda1 -1.5161515\n",
      "51 Train Loss 143.71701 Test MSE 155.17069463983415 Test RE 0.20969787175321652 Lambda1 -1.5156\n",
      "52 Train Loss 143.15236 Test MSE 154.1291620988603 Test RE 0.20899292263397629 Lambda1 -1.5237716\n",
      "53 Train Loss 142.3085 Test MSE 153.4909259446635 Test RE 0.2085597625168458 Lambda1 -1.5277451\n",
      "54 Train Loss 141.18497 Test MSE 152.66414265280153 Test RE 0.20799729747778214 Lambda1 -1.5266763\n",
      "55 Train Loss 140.3583 Test MSE 151.78860235003575 Test RE 0.20739999981276458 Lambda1 -1.5203782\n",
      "56 Train Loss 139.51193 Test MSE 152.19618185060875 Test RE 0.20767826615563473 Lambda1 -1.5249003\n",
      "57 Train Loss 138.37291 Test MSE 151.71218593106286 Test RE 0.20734778653633207 Lambda1 -1.527685\n",
      "58 Train Loss 138.00209 Test MSE 150.70745087284712 Test RE 0.20666005120736217 Lambda1 -1.5283269\n",
      "59 Train Loss 137.36604 Test MSE 148.5742724858081 Test RE 0.20519226092645504 Lambda1 -1.5419589\n",
      "60 Train Loss 136.27873 Test MSE 147.213126997771 Test RE 0.20425017604194456 Lambda1 -1.5656469\n",
      "61 Train Loss 134.84448 Test MSE 145.1626795870713 Test RE 0.2028227462342742 Lambda1 -1.5687767\n",
      "62 Train Loss 134.2893 Test MSE 144.89976132258852 Test RE 0.20263898697752225 Lambda1 -1.5760005\n",
      "63 Train Loss 133.96394 Test MSE 144.28595528724156 Test RE 0.2022093346661651 Lambda1 -1.5776533\n",
      "64 Train Loss 132.32785 Test MSE 142.49693946206278 Test RE 0.20095181772196322 Lambda1 -1.5857846\n",
      "65 Train Loss 130.62674 Test MSE 140.9418979896401 Test RE 0.1998523357724385 Lambda1 -1.5893171\n",
      "66 Train Loss 128.56839 Test MSE 137.24456300417776 Test RE 0.1972135473589955 Lambda1 -1.6085781\n",
      "67 Train Loss 126.43455 Test MSE 135.88646277238078 Test RE 0.19623536050839405 Lambda1 -1.6238991\n",
      "68 Train Loss 125.31982 Test MSE 135.2648469046737 Test RE 0.19578600435122331 Lambda1 -1.6289189\n",
      "69 Train Loss 124.654175 Test MSE 133.83271219395206 Test RE 0.19474679103729636 Lambda1 -1.631709\n",
      "70 Train Loss 123.563835 Test MSE 133.34002553071622 Test RE 0.1943879937975124 Lambda1 -1.6375202\n",
      "71 Train Loss 121.61197 Test MSE 130.27002026726063 Test RE 0.19213717943441244 Lambda1 -1.6653819\n",
      "72 Train Loss 121.258606 Test MSE 129.88389531695395 Test RE 0.1918522174185609 Lambda1 -1.6708003\n",
      "73 Train Loss 120.95623 Test MSE 129.66360634690062 Test RE 0.19168945334974816 Lambda1 -1.6754704\n",
      "74 Train Loss 120.55516 Test MSE 129.40082092549832 Test RE 0.19149510914029577 Lambda1 -1.6789372\n",
      "Training time: 145.48\n",
      "Training time: 145.48\n",
      "inv_HT_stan_tune4\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 838.1331 Test MSE 858.6352415923011 Test RE 0.49328002048641756 Lambda1 -0.0067847176\n",
      "1 Train Loss 838.0448 Test MSE 858.2499103870387 Test RE 0.4931693230065055 Lambda1 -0.0067763194\n",
      "2 Train Loss 838.00385 Test MSE 858.0366529323954 Test RE 0.4931080479819798 Lambda1 -0.0068838815\n",
      "3 Train Loss 837.88776 Test MSE 857.9025590025218 Test RE 0.4930695150245596 Lambda1 -0.010786551\n",
      "4 Train Loss 837.88605 Test MSE 857.9214365554631 Test RE 0.49307493982168393 Lambda1 -0.01273205\n",
      "5 Train Loss 837.88226 Test MSE 857.9391563479277 Test RE 0.49308003186165184 Lambda1 -0.019206548\n",
      "6 Train Loss 837.7785 Test MSE 857.7568813090319 Test RE 0.49302764996747517 Lambda1 -0.18893483\n",
      "7 Train Loss 836.40717 Test MSE 856.0573004826834 Test RE 0.49253895914510676 Lambda1 -1.1013851\n",
      "8 Train Loss 828.36053 Test MSE 841.7518926102452 Test RE 0.4884062596100547 Lambda1 -1.1855372\n",
      "9 Train Loss 816.15796 Test MSE 830.611497899204 Test RE 0.48516352180837485 Lambda1 -1.1080594\n",
      "10 Train Loss 807.8702 Test MSE 816.649324180759 Test RE 0.48106855934142084 Lambda1 -1.303983\n",
      "11 Train Loss 795.7869 Test MSE 797.0886468555728 Test RE 0.4752722770364151 Lambda1 -1.2527751\n",
      "12 Train Loss 783.78107 Test MSE 779.3608467490876 Test RE 0.4699573677853513 Lambda1 -1.0214405\n",
      "13 Train Loss 765.71967 Test MSE 754.2210449154069 Test RE 0.4623155421051873 Lambda1 -0.9071887\n",
      "14 Train Loss 748.4573 Test MSE 746.8556235060634 Test RE 0.46005260938328735 Lambda1 -0.8703949\n",
      "15 Train Loss 735.07666 Test MSE 736.9207287165444 Test RE 0.45698248722362894 Lambda1 -0.8564246\n",
      "16 Train Loss 727.047 Test MSE 725.1316229445083 Test RE 0.45331239392356204 Lambda1 -0.88243085\n",
      "17 Train Loss 721.7437 Test MSE 715.4129907061832 Test RE 0.45026436954940485 Lambda1 -0.8841092\n",
      "18 Train Loss 709.33984 Test MSE 700.8991601060923 Test RE 0.4456736323364343 Lambda1 -0.9045618\n",
      "19 Train Loss 704.5774 Test MSE 693.352182271292 Test RE 0.44326772829187283 Lambda1 -0.9229069\n",
      "20 Train Loss 696.9602 Test MSE 684.4662306064075 Test RE 0.4404181251219317 Lambda1 -0.96761644\n",
      "21 Train Loss 692.9229 Test MSE 682.0254273031242 Test RE 0.4396321593451755 Lambda1 -0.99744\n",
      "22 Train Loss 686.87036 Test MSE 672.4876501557079 Test RE 0.4365473208329386 Lambda1 -1.0267669\n",
      "23 Train Loss 679.4357 Test MSE 661.0193776076347 Test RE 0.4328089830578734 Lambda1 -1.0638846\n",
      "24 Train Loss 663.15497 Test MSE 645.1839028407825 Test RE 0.427593340077951 Lambda1 -1.0516617\n",
      "25 Train Loss 656.8343 Test MSE 636.7267146093498 Test RE 0.4247816097835299 Lambda1 -1.0307117\n",
      "26 Train Loss 648.77814 Test MSE 627.720155861059 Test RE 0.42176662225234995 Lambda1 -1.0391169\n",
      "27 Train Loss 645.1899 Test MSE 618.3763908510484 Test RE 0.4186158044786698 Lambda1 -1.0406677\n",
      "28 Train Loss 634.76605 Test MSE 612.032344240013 Test RE 0.41646293709406224 Lambda1 -1.0527472\n",
      "29 Train Loss 613.9228 Test MSE 597.9889109043552 Test RE 0.4116572190926578 Lambda1 -1.0447915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Train Loss 602.18677 Test MSE 593.3033370346359 Test RE 0.41004126639528504 Lambda1 -1.0261564\n",
      "31 Train Loss 597.7559 Test MSE 586.8500916819064 Test RE 0.40780519978254387 Lambda1 -1.0215814\n",
      "32 Train Loss 594.0586 Test MSE 584.2913212891505 Test RE 0.4069151770903726 Lambda1 -1.0275886\n",
      "33 Train Loss 591.9866 Test MSE 585.4256642925553 Test RE 0.40730997807064656 Lambda1 -1.0181085\n",
      "34 Train Loss 585.94415 Test MSE 577.641992211461 Test RE 0.40459317195551514 Lambda1 -1.0088534\n",
      "35 Train Loss 577.94336 Test MSE 563.3212772124419 Test RE 0.3995464242346432 Lambda1 -1.0271583\n",
      "36 Train Loss 569.3799 Test MSE 560.1785972078059 Test RE 0.3984303624289217 Lambda1 -1.0313045\n",
      "37 Train Loss 562.7073 Test MSE 552.9166727406405 Test RE 0.39583939447039074 Lambda1 -1.0612593\n",
      "38 Train Loss 555.7267 Test MSE 544.9797254566012 Test RE 0.39298804909326446 Lambda1 -1.1006018\n",
      "39 Train Loss 548.75806 Test MSE 536.7181209773082 Test RE 0.3899979281383596 Lambda1 -1.1431535\n",
      "40 Train Loss 535.6101 Test MSE 525.4665165373398 Test RE 0.3858883739409807 Lambda1 -1.1903498\n",
      "41 Train Loss 525.5061 Test MSE 519.5794788716909 Test RE 0.38372064484285595 Lambda1 -1.1727164\n",
      "42 Train Loss 519.7387 Test MSE 509.3201324681519 Test RE 0.3799133828556392 Lambda1 -1.1751729\n",
      "43 Train Loss 514.6813 Test MSE 501.9464661050772 Test RE 0.37715326446550107 Lambda1 -1.1886092\n",
      "44 Train Loss 508.96008 Test MSE 495.55397602336353 Test RE 0.3747439698125495 Lambda1 -1.2208968\n",
      "45 Train Loss 492.19858 Test MSE 484.486919296913 Test RE 0.3705358205812268 Lambda1 -1.3103535\n",
      "46 Train Loss 485.62775 Test MSE 472.0800115048467 Test RE 0.3657606468256131 Lambda1 -1.3989655\n",
      "47 Train Loss 479.90933 Test MSE 467.8889552938774 Test RE 0.36413344291259137 Lambda1 -1.4403423\n",
      "48 Train Loss 478.12335 Test MSE 463.5278691206522 Test RE 0.3624324676724168 Lambda1 -1.4556204\n",
      "49 Train Loss 462.94662 Test MSE 452.2574516194856 Test RE 0.3579991835970727 Lambda1 -1.5074451\n",
      "50 Train Loss 452.437 Test MSE 442.63319958008157 Test RE 0.35416950388360774 Lambda1 -1.5311896\n",
      "51 Train Loss 442.05115 Test MSE 430.5533861122937 Test RE 0.3493032892132226 Lambda1 -1.5564872\n",
      "52 Train Loss 432.3037 Test MSE 416.2645090562536 Test RE 0.3434581788327551 Lambda1 -1.5952994\n",
      "53 Train Loss 427.17654 Test MSE 408.3098643320955 Test RE 0.34016067661675403 Lambda1 -1.6165721\n",
      "54 Train Loss 420.59933 Test MSE 401.663985473899 Test RE 0.3373809969555079 Lambda1 -1.6178042\n",
      "55 Train Loss 413.1573 Test MSE 400.28601941959397 Test RE 0.3368017827495911 Lambda1 -1.6134847\n",
      "56 Train Loss 394.87985 Test MSE 377.33960167111184 Test RE 0.327005730944455 Lambda1 -1.5723683\n",
      "57 Train Loss 386.61487 Test MSE 368.32823024145637 Test RE 0.3230774703689849 Lambda1 -1.5678902\n",
      "58 Train Loss 376.72406 Test MSE 353.7215365620047 Test RE 0.31660656948437904 Lambda1 -1.540122\n",
      "59 Train Loss 363.17896 Test MSE 350.7889677173953 Test RE 0.3152914064505599 Lambda1 -1.5041283\n",
      "60 Train Loss 355.8762 Test MSE 348.3753632901847 Test RE 0.3142048525340839 Lambda1 -1.4736243\n",
      "61 Train Loss 354.1161 Test MSE 345.9558889590966 Test RE 0.3131118718317299 Lambda1 -1.4570184\n",
      "62 Train Loss 347.50345 Test MSE 335.11530640031833 Test RE 0.30816712236383714 Lambda1 -1.459432\n",
      "63 Train Loss 343.8174 Test MSE 337.02152525515754 Test RE 0.3090423449631566 Lambda1 -1.4459957\n",
      "64 Train Loss 331.2346 Test MSE 315.6140055246551 Test RE 0.2990661804536711 Lambda1 -1.4823282\n",
      "65 Train Loss 319.6236 Test MSE 300.1355235386905 Test RE 0.2916405247889156 Lambda1 -1.4897825\n",
      "66 Train Loss 312.12125 Test MSE 294.8219507911753 Test RE 0.2890474074361705 Lambda1 -1.4914203\n",
      "67 Train Loss 302.3055 Test MSE 286.2611597545308 Test RE 0.2848199355657123 Lambda1 -1.4944082\n",
      "68 Train Loss 295.84167 Test MSE 284.89950236601675 Test RE 0.2841417271350682 Lambda1 -1.4878823\n",
      "69 Train Loss 290.95047 Test MSE 288.21718258443406 Test RE 0.2857913664154859 Lambda1 -1.4757088\n",
      "70 Train Loss 286.28192 Test MSE 284.59644956486255 Test RE 0.2839905635546031 Lambda1 -1.4672223\n",
      "71 Train Loss 275.42084 Test MSE 276.2877310765619 Test RE 0.2798143419503758 Lambda1 -1.4761447\n",
      "72 Train Loss 270.1835 Test MSE 266.11480779393764 Test RE 0.27461464319123485 Lambda1 -1.4951457\n",
      "73 Train Loss 266.76355 Test MSE 258.589128673623 Test RE 0.27070376871581453 Lambda1 -1.5246277\n",
      "74 Train Loss 258.96387 Test MSE 260.11485445911626 Test RE 0.27150119645170134 Lambda1 -1.5522832\n",
      "Training time: 144.58\n",
      "Training time: 144.58\n",
      "inv_HT_stan_tune4\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 839.90466 Test MSE 861.4386714734562 Test RE 0.49408463981395284 Lambda1 -0.0023895204\n",
      "1 Train Loss 838.0058 Test MSE 858.1685492389954 Test RE 0.4931459464976319 Lambda1 -0.002421339\n",
      "2 Train Loss 837.8792 Test MSE 857.7778624182451 Test RE 0.49303367976679185 Lambda1 -0.0033457514\n",
      "3 Train Loss 837.86566 Test MSE 857.8585388920067 Test RE 0.4930568648390868 Lambda1 -0.0042008897\n",
      "4 Train Loss 837.8644 Test MSE 857.875521488091 Test RE 0.4930617452149075 Lambda1 -0.005141952\n",
      "5 Train Loss 837.5146 Test MSE 858.021450526994 Test RE 0.49310367960119356 Lambda1 -0.20649964\n",
      "6 Train Loss 834.7403 Test MSE 849.3288585254517 Test RE 0.49059951104987376 Lambda1 -0.4445934\n",
      "7 Train Loss 813.6409 Test MSE 812.8237886197874 Test RE 0.47994047098669756 Lambda1 -0.794959\n",
      "8 Train Loss 787.2196 Test MSE 790.9069601109597 Test RE 0.4734257429004957 Lambda1 -1.1314684\n",
      "9 Train Loss 766.65936 Test MSE 769.8123380272368 Test RE 0.4670696031150471 Lambda1 -1.1998571\n",
      "10 Train Loss 744.0244 Test MSE 736.3824191869923 Test RE 0.4568155473079736 Lambda1 -1.1438025\n",
      "11 Train Loss 723.08167 Test MSE 716.0571295912933 Test RE 0.45046702699181146 Lambda1 -0.9647519\n",
      "12 Train Loss 693.7563 Test MSE 692.3134745328041 Test RE 0.44293557516605686 Lambda1 -0.91772664\n",
      "13 Train Loss 684.70306 Test MSE 682.8857235247921 Test RE 0.4399093445331191 Lambda1 -0.90992975\n",
      "14 Train Loss 663.1753 Test MSE 661.7250922416324 Test RE 0.4330399582485177 Lambda1 -0.96779996\n",
      "15 Train Loss 648.36597 Test MSE 646.4864015345577 Test RE 0.42802473565371735 Lambda1 -0.9799562\n",
      "16 Train Loss 639.4406 Test MSE 642.9081657577925 Test RE 0.4268385555735693 Lambda1 -0.9511911\n",
      "17 Train Loss 627.417 Test MSE 625.2944985038944 Test RE 0.42095093109450943 Lambda1 -0.96590847\n",
      "18 Train Loss 619.93207 Test MSE 615.2083762824701 Test RE 0.4175421186969815 Lambda1 -0.98034376\n",
      "19 Train Loss 612.80176 Test MSE 613.9450587068383 Test RE 0.417113191367305 Lambda1 -0.9580901\n",
      "20 Train Loss 596.5617 Test MSE 591.2824284323268 Test RE 0.4093423298508099 Lambda1 -0.91299057\n",
      "21 Train Loss 571.17505 Test MSE 561.8167808865487 Test RE 0.39901252107155244 Lambda1 -0.94238836\n",
      "22 Train Loss 559.1445 Test MSE 545.3154063717609 Test RE 0.39310906119170214 Lambda1 -0.9580705\n",
      "23 Train Loss 532.7555 Test MSE 518.2394544809725 Test RE 0.3832255069411079 Lambda1 -0.94526297\n",
      "24 Train Loss 511.81183 Test MSE 504.708290744165 Test RE 0.3781894330150085 Lambda1 -0.9007989\n",
      "25 Train Loss 486.47287 Test MSE 487.7520326391717 Test RE 0.3717823040935099 Lambda1 -0.8250271\n",
      "26 Train Loss 455.9469 Test MSE 462.59707680940784 Test RE 0.3620683915277828 Lambda1 -0.83470774\n",
      "27 Train Loss 426.28244 Test MSE 432.30197173569843 Test RE 0.3500118747752781 Lambda1 -0.921217\n",
      "28 Train Loss 413.19434 Test MSE 423.524939145102 Test RE 0.3464405065979206 Lambda1 -0.9640547\n",
      "29 Train Loss 402.7723 Test MSE 414.26955670291244 Test RE 0.34263417678987823 Lambda1 -1.0201833\n",
      "30 Train Loss 389.43237 Test MSE 406.1018415076168 Test RE 0.33923968404674876 Lambda1 -1.053402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 Train Loss 381.02078 Test MSE 399.24542202097285 Test RE 0.33636371707243606 Lambda1 -1.0614522\n",
      "32 Train Loss 366.8493 Test MSE 378.3046916814263 Test RE 0.3274236415360849 Lambda1 -1.0691285\n",
      "33 Train Loss 352.47537 Test MSE 368.8390281572902 Test RE 0.32330141481528324 Lambda1 -1.1136215\n",
      "34 Train Loss 339.37582 Test MSE 360.77790015236593 Test RE 0.31974895506206674 Lambda1 -1.1352139\n",
      "35 Train Loss 329.45734 Test MSE 348.1581270169435 Test RE 0.3141068729660518 Lambda1 -1.1981297\n",
      "36 Train Loss 322.44003 Test MSE 347.354289716144 Test RE 0.31374405402073385 Lambda1 -1.1860287\n",
      "37 Train Loss 317.9521 Test MSE 342.1093599102112 Test RE 0.31136633075236386 Lambda1 -1.2112933\n",
      "38 Train Loss 314.1936 Test MSE 341.66950345551993 Test RE 0.31116610159084795 Lambda1 -1.2229135\n",
      "39 Train Loss 312.76117 Test MSE 341.11704830094726 Test RE 0.3109144331497071 Lambda1 -1.2338231\n",
      "40 Train Loss 310.37222 Test MSE 336.29955581416925 Test RE 0.30871115135646593 Lambda1 -1.251389\n",
      "41 Train Loss 308.1301 Test MSE 332.2876552794289 Test RE 0.30686423489476694 Lambda1 -1.2851537\n",
      "42 Train Loss 305.24393 Test MSE 328.7119088092967 Test RE 0.3052086864504606 Lambda1 -1.2784667\n",
      "43 Train Loss 302.03693 Test MSE 327.0042226771242 Test RE 0.3044148616578652 Lambda1 -1.2868419\n",
      "44 Train Loss 297.71387 Test MSE 321.2531632452761 Test RE 0.30172609840269976 Lambda1 -1.3297842\n",
      "45 Train Loss 293.49258 Test MSE 310.60187967497455 Test RE 0.2966820084844005 Lambda1 -1.3755481\n",
      "46 Train Loss 285.2823 Test MSE 294.62986930563466 Test RE 0.28895323245806676 Lambda1 -1.4335334\n",
      "47 Train Loss 277.44528 Test MSE 289.0583891341298 Test RE 0.28620812575733656 Lambda1 -1.4401741\n",
      "48 Train Loss 267.24414 Test MSE 279.2988651739054 Test RE 0.28133499446455185 Lambda1 -1.4349996\n",
      "49 Train Loss 264.5153 Test MSE 278.303018591884 Test RE 0.28083299336671247 Lambda1 -1.4339608\n",
      "50 Train Loss 262.2761 Test MSE 271.7625962533975 Test RE 0.2775134343621812 Lambda1 -1.4231087\n",
      "51 Train Loss 255.23415 Test MSE 261.9893549279566 Test RE 0.2724777179945584 Lambda1 -1.3418738\n",
      "52 Train Loss 249.24593 Test MSE 260.49291598779394 Test RE 0.27169843026186047 Lambda1 -1.3090953\n",
      "53 Train Loss 246.0114 Test MSE 256.83150965475926 Test RE 0.2697822192638074 Lambda1 -1.264053\n",
      "54 Train Loss 241.74097 Test MSE 248.46239058610325 Test RE 0.26535024953270414 Lambda1 -1.2223505\n",
      "55 Train Loss 238.52675 Test MSE 246.5303072728062 Test RE 0.26431653302086056 Lambda1 -1.2100157\n",
      "56 Train Loss 236.37483 Test MSE 241.37863903533122 Test RE 0.2615402820614726 Lambda1 -1.180321\n",
      "57 Train Loss 231.29561 Test MSE 238.14860350316292 Test RE 0.25978447297070467 Lambda1 -1.1839108\n",
      "58 Train Loss 227.97261 Test MSE 232.20580116278808 Test RE 0.2565226415078675 Lambda1 -1.2113247\n",
      "59 Train Loss 223.03712 Test MSE 229.04266784844356 Test RE 0.2547694605163018 Lambda1 -1.1976366\n",
      "60 Train Loss 217.32439 Test MSE 223.37431983230286 Test RE 0.2515971933974304 Lambda1 -1.2116119\n",
      "61 Train Loss 212.80103 Test MSE 218.36076717990312 Test RE 0.24875766773465538 Lambda1 -1.2136016\n",
      "62 Train Loss 209.35492 Test MSE 214.054332600205 Test RE 0.24629249709893444 Lambda1 -1.1858928\n",
      "63 Train Loss 205.25542 Test MSE 213.1015750956166 Test RE 0.24574376089752797 Lambda1 -1.1198196\n",
      "64 Train Loss 203.11993 Test MSE 212.39154642168702 Test RE 0.24533402507070712 Lambda1 -1.1142306\n",
      "65 Train Loss 200.42128 Test MSE 212.32006575711063 Test RE 0.245292737846253 Lambda1 -1.124126\n",
      "66 Train Loss 199.16849 Test MSE 212.276592807259 Test RE 0.24526762447612138 Lambda1 -1.1160208\n",
      "67 Train Loss 197.02203 Test MSE 207.65208483161888 Test RE 0.24258129966998537 Lambda1 -1.1401184\n",
      "68 Train Loss 194.16158 Test MSE 202.30296858900286 Test RE 0.23943646854280906 Lambda1 -1.1476231\n",
      "69 Train Loss 192.16872 Test MSE 200.39052619658278 Test RE 0.23830204180979 Lambda1 -1.144106\n",
      "70 Train Loss 189.77907 Test MSE 199.28931440302134 Test RE 0.23764636576486373 Lambda1 -1.1521969\n",
      "71 Train Loss 188.14383 Test MSE 194.88745352868935 Test RE 0.23500716916483078 Lambda1 -1.1628046\n",
      "72 Train Loss 186.6301 Test MSE 194.04957670677624 Test RE 0.2345014435458756 Lambda1 -1.1584387\n",
      "73 Train Loss 185.18718 Test MSE 192.9400269151818 Test RE 0.2338300583415078 Lambda1 -1.1527092\n",
      "74 Train Loss 180.04906 Test MSE 188.49501722792849 Test RE 0.23112084028386454 Lambda1 -1.1701622\n",
      "Training time: 145.79\n",
      "Training time: 145.79\n",
      "inv_HT_stan_tune5\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 839.5238 Test MSE 858.5851617319122 Test RE 0.49326563500957094 Lambda1 -0.027318973\n",
      "1 Train Loss 838.00507 Test MSE 858.0741702720242 Test RE 0.49311882834889265 Lambda1 -0.027318869\n",
      "2 Train Loss 837.8809 Test MSE 857.9812908174487 Test RE 0.4930921396028283 Lambda1 -0.03395803\n",
      "3 Train Loss 837.71326 Test MSE 857.6199737280779 Test RE 0.492988302036847 Lambda1 -0.0950549\n",
      "4 Train Loss 832.10254 Test MSE 849.067689021539 Test RE 0.4905240753213289 Lambda1 -0.48531467\n",
      "5 Train Loss 820.1693 Test MSE 829.983546096227 Test RE 0.48498009254434066 Lambda1 -0.5116014\n",
      "6 Train Loss 798.41016 Test MSE 809.9342385789844 Test RE 0.47908662865620955 Lambda1 -0.5360303\n",
      "7 Train Loss 765.07385 Test MSE 759.0416488832453 Test RE 0.4637906337897011 Lambda1 -0.6826244\n",
      "8 Train Loss 725.88086 Test MSE 724.635144325499 Test RE 0.4531571817934178 Lambda1 -0.64725626\n",
      "9 Train Loss 718.28235 Test MSE 719.2220231926395 Test RE 0.45146143671623035 Lambda1 -0.6268161\n",
      "10 Train Loss 708.8944 Test MSE 711.6737529841342 Test RE 0.4490861331486384 Lambda1 -0.5804956\n",
      "11 Train Loss 700.48596 Test MSE 702.2091770370218 Test RE 0.44608993148834564 Lambda1 -0.59186333\n",
      "12 Train Loss 688.12787 Test MSE 686.1230404698952 Test RE 0.44095083807827745 Lambda1 -0.65538955\n",
      "13 Train Loss 670.22797 Test MSE 664.7546087965693 Test RE 0.43403010024611693 Lambda1 -0.7304052\n",
      "14 Train Loss 651.68646 Test MSE 634.5091424992296 Test RE 0.4240412564745407 Lambda1 -0.8031564\n",
      "15 Train Loss 638.95886 Test MSE 623.4808199083603 Test RE 0.420339999673985 Lambda1 -0.8468153\n",
      "16 Train Loss 622.6522 Test MSE 612.6259222518407 Test RE 0.4166648409119036 Lambda1 -0.86666673\n",
      "17 Train Loss 582.12 Test MSE 581.3186823058445 Test RE 0.4058787468843064 Lambda1 -0.86854863\n",
      "18 Train Loss 546.94104 Test MSE 533.1373873147331 Test RE 0.38869480865985695 Lambda1 -0.9341353\n",
      "19 Train Loss 508.35114 Test MSE 498.02484930209835 Test RE 0.3756770604194097 Lambda1 -0.9124017\n",
      "20 Train Loss 467.493 Test MSE 457.09753074075917 Test RE 0.3599097469649954 Lambda1 -0.85571617\n",
      "21 Train Loss 448.16663 Test MSE 433.42477309802695 Test RE 0.35046611624236085 Lambda1 -0.8508549\n",
      "22 Train Loss 398.56314 Test MSE 376.8425535942508 Test RE 0.3267902869125628 Lambda1 -0.8141946\n",
      "23 Train Loss 373.00287 Test MSE 369.32727283697074 Test RE 0.32351532654954596 Lambda1 -0.78208345\n",
      "24 Train Loss 351.50494 Test MSE 350.175552782351 Test RE 0.315015615163386 Lambda1 -0.78397036\n",
      "25 Train Loss 336.825 Test MSE 341.4434260809274 Test RE 0.31106313768065186 Lambda1 -0.76212543\n",
      "26 Train Loss 319.4301 Test MSE 318.54713437461277 Test RE 0.3004526382774828 Lambda1 -0.7706612\n",
      "27 Train Loss 306.8976 Test MSE 314.7822765737146 Test RE 0.2986718600366347 Lambda1 -0.7840774\n",
      "28 Train Loss 300.3566 Test MSE 315.6140489792416 Test RE 0.29906620104178905 Lambda1 -0.7937052\n",
      "29 Train Loss 289.16626 Test MSE 308.4713279581303 Test RE 0.29566272287674855 Lambda1 -0.8324197\n",
      "30 Train Loss 282.55423 Test MSE 309.0365700491021 Test RE 0.2959334847151926 Lambda1 -0.8486963\n",
      "31 Train Loss 279.31186 Test MSE 303.65416686268657 Test RE 0.29334506957093714 Lambda1 -0.8662814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 Train Loss 276.2459 Test MSE 304.0176584662574 Test RE 0.29352059256772733 Lambda1 -0.85872686\n",
      "33 Train Loss 274.52838 Test MSE 302.6627050524632 Test RE 0.29286577723816504 Lambda1 -0.862357\n",
      "34 Train Loss 271.8701 Test MSE 301.28088867328574 Test RE 0.29219646833232926 Lambda1 -0.87831265\n",
      "35 Train Loss 269.66754 Test MSE 300.2688389094625 Test RE 0.29170528861245226 Lambda1 -0.8804806\n",
      "36 Train Loss 266.64838 Test MSE 299.1312562798757 Test RE 0.2911521946516171 Lambda1 -0.88868636\n",
      "37 Train Loss 265.4698 Test MSE 299.1055545036073 Test RE 0.2911396862808651 Lambda1 -0.8983718\n",
      "38 Train Loss 264.80383 Test MSE 298.00074964023634 Test RE 0.29060149815974623 Lambda1 -0.9020023\n",
      "39 Train Loss 264.2856 Test MSE 297.08571080461417 Test RE 0.29015499576436216 Lambda1 -0.90256184\n",
      "40 Train Loss 263.57974 Test MSE 296.24103575714093 Test RE 0.2897422173441798 Lambda1 -0.9005824\n",
      "41 Train Loss 262.37564 Test MSE 295.7353481319492 Test RE 0.2894948146938696 Lambda1 -0.89335895\n",
      "42 Train Loss 261.1714 Test MSE 295.344896541839 Test RE 0.28930364538498055 Lambda1 -0.8782674\n",
      "43 Train Loss 259.61304 Test MSE 293.7184928145356 Test RE 0.28850597782253895 Lambda1 -0.8556056\n",
      "44 Train Loss 257.9648 Test MSE 292.118940363792 Test RE 0.2877193224665748 Lambda1 -0.85387385\n",
      "45 Train Loss 256.92993 Test MSE 290.2285828367579 Test RE 0.286786868116431 Lambda1 -0.82657194\n",
      "46 Train Loss 256.04865 Test MSE 289.7804200063418 Test RE 0.28656535845556425 Lambda1 -0.81168985\n",
      "47 Train Loss 255.31828 Test MSE 289.3970695201799 Test RE 0.2863757470848028 Lambda1 -0.79145217\n",
      "48 Train Loss 254.21248 Test MSE 288.467420601741 Test RE 0.28591540542659416 Lambda1 -0.7687394\n",
      "49 Train Loss 253.6334 Test MSE 287.4082328515361 Test RE 0.2853900140421318 Lambda1 -0.7586587\n",
      "50 Train Loss 253.07968 Test MSE 286.0625974419203 Test RE 0.2847211371194657 Lambda1 -0.72832406\n",
      "51 Train Loss 251.79219 Test MSE 285.20119891007096 Test RE 0.28429213437291523 Lambda1 -0.6876406\n",
      "52 Train Loss 251.1088 Test MSE 283.40402546727756 Test RE 0.28339499630652615 Lambda1 -0.6616379\n",
      "53 Train Loss 250.36328 Test MSE 282.08268488385363 Test RE 0.28273357521177667 Lambda1 -0.6222316\n",
      "54 Train Loss 249.03168 Test MSE 281.04140338973895 Test RE 0.2822112506411697 Lambda1 -0.6058361\n",
      "55 Train Loss 247.9434 Test MSE 280.3301764800076 Test RE 0.28185393072046055 Lambda1 -0.5885109\n",
      "56 Train Loss 246.76395 Test MSE 278.3727146845 Test RE 0.28086815600551845 Lambda1 -0.55113715\n",
      "57 Train Loss 245.70708 Test MSE 276.7476393763165 Test RE 0.28004713446057344 Lambda1 -0.5384407\n",
      "58 Train Loss 245.38382 Test MSE 276.48733983609424 Test RE 0.2799154020102326 Lambda1 -0.53344584\n",
      "59 Train Loss 244.28998 Test MSE 274.74048439590047 Test RE 0.279029743869116 Lambda1 -0.5202868\n",
      "60 Train Loss 243.36855 Test MSE 274.45406082313167 Test RE 0.27888425832663916 Lambda1 -0.5221695\n",
      "61 Train Loss 242.09265 Test MSE 272.95925670177525 Test RE 0.27812375488322744 Lambda1 -0.53083175\n",
      "62 Train Loss 240.81487 Test MSE 272.4277261627798 Test RE 0.2778528292717782 Lambda1 -0.549251\n",
      "63 Train Loss 239.63666 Test MSE 271.48558403740105 Test RE 0.27737196123017127 Lambda1 -0.5557513\n",
      "64 Train Loss 237.90288 Test MSE 269.8502313893748 Test RE 0.27653529410260796 Lambda1 -0.5375972\n",
      "65 Train Loss 236.02357 Test MSE 267.62704384053916 Test RE 0.27539380662989343 Lambda1 -0.52610034\n",
      "66 Train Loss 233.71416 Test MSE 265.1937960052841 Test RE 0.27413901662897977 Lambda1 -0.5328118\n",
      "67 Train Loss 231.26147 Test MSE 262.90571451510533 Test RE 0.2729538243946092 Lambda1 -0.5335772\n",
      "68 Train Loss 228.99033 Test MSE 259.68727373637387 Test RE 0.2712779557906155 Lambda1 -0.52757597\n",
      "69 Train Loss 227.82695 Test MSE 258.3619108918542 Test RE 0.27058481122568684 Lambda1 -0.52703625\n",
      "70 Train Loss 225.04286 Test MSE 254.14054913779114 Test RE 0.2683651716998294 Lambda1 -0.52923733\n",
      "71 Train Loss 221.6112 Test MSE 249.6142194504166 Test RE 0.2659645973886366 Lambda1 -0.5351773\n",
      "72 Train Loss 216.45114 Test MSE 242.06159727297256 Test RE 0.2619100225682196 Lambda1 -0.54744685\n",
      "73 Train Loss 213.24794 Test MSE 238.27208641560009 Test RE 0.25985181492779547 Lambda1 -0.5540247\n",
      "74 Train Loss 209.15858 Test MSE 231.78858699295097 Test RE 0.25629208525631003 Lambda1 -0.5572588\n",
      "Training time: 142.02\n",
      "Training time: 142.02\n",
      "inv_HT_stan_tune5\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 838.68774 Test MSE 857.9710841304774 Test RE 0.49308920664138794 Lambda1 -0.04791361\n",
      "1 Train Loss 837.86346 Test MSE 857.9597228914182 Test RE 0.4930859418909765 Lambda1 -0.048010472\n",
      "2 Train Loss 837.8254 Test MSE 858.0115199941054 Test RE 0.4931008260615335 Lambda1 -0.04737847\n",
      "3 Train Loss 837.10486 Test MSE 856.2934827380581 Test RE 0.49260689907133365 Lambda1 -0.031260226\n",
      "4 Train Loss 817.69836 Test MSE 830.2285789183325 Test RE 0.48505167665689236 Lambda1 0.12373882\n",
      "5 Train Loss 723.69293 Test MSE 714.722106797953 Test RE 0.45004690386781354 Lambda1 0.070931844\n",
      "6 Train Loss 664.2989 Test MSE 666.3018790582678 Test RE 0.434534926789144 Lambda1 0.078630686\n",
      "7 Train Loss 651.05597 Test MSE 655.905900414055 Test RE 0.43113168305535043 Lambda1 0.08688945\n",
      "8 Train Loss 639.6348 Test MSE 644.2166609322975 Test RE 0.42727270179013327 Lambda1 0.09762035\n",
      "9 Train Loss 627.5594 Test MSE 626.2490810911422 Test RE 0.42127212309778667 Lambda1 0.096855864\n",
      "10 Train Loss 610.6595 Test MSE 608.2683310822778 Test RE 0.41518033366583723 Lambda1 0.11927932\n",
      "11 Train Loss 558.4478 Test MSE 527.6150090521963 Test RE 0.38667646653586296 Lambda1 0.10996358\n",
      "12 Train Loss 442.29834 Test MSE 430.69995593793465 Test RE 0.34936273940850077 Lambda1 0.080802634\n",
      "13 Train Loss 337.4853 Test MSE 356.9421306360225 Test RE 0.3180446368169772 Lambda1 0.06300984\n",
      "14 Train Loss 287.91898 Test MSE 303.23065272871 Test RE 0.2931404303043155 Lambda1 0.0822842\n",
      "15 Train Loss 264.45657 Test MSE 281.57474836280596 Test RE 0.2824789062383623 Lambda1 0.096102\n",
      "16 Train Loss 257.95865 Test MSE 276.2848928723275 Test RE 0.27981290473076664 Lambda1 0.10090243\n",
      "17 Train Loss 247.85115 Test MSE 264.65456081810436 Test RE 0.27386016277127817 Lambda1 0.12278158\n",
      "18 Train Loss 237.96432 Test MSE 256.39542219773864 Test RE 0.2695530833899173 Lambda1 0.14860061\n",
      "19 Train Loss 231.74057 Test MSE 253.10418427875302 Test RE 0.26781742684763127 Lambda1 0.1538245\n",
      "20 Train Loss 222.73563 Test MSE 245.00510772596039 Test RE 0.2634976460598001 Lambda1 0.17829186\n",
      "21 Train Loss 220.57039 Test MSE 244.94495087782428 Test RE 0.2634652953852725 Lambda1 0.18051723\n",
      "22 Train Loss 217.38469 Test MSE 244.019897755143 Test RE 0.26296732648439525 Lambda1 0.2063541\n",
      "23 Train Loss 215.41072 Test MSE 243.58381685489178 Test RE 0.2627322507621659 Lambda1 0.21824132\n",
      "24 Train Loss 214.39622 Test MSE 243.20814394212098 Test RE 0.26252957006541094 Lambda1 0.22502774\n",
      "25 Train Loss 212.81276 Test MSE 242.72822221556626 Test RE 0.26227041784379246 Lambda1 0.24416219\n",
      "26 Train Loss 212.1986 Test MSE 242.88482635861658 Test RE 0.2623550104201173 Lambda1 0.2605589\n",
      "27 Train Loss 211.44464 Test MSE 241.80284823694336 Test RE 0.2617700022458412 Lambda1 0.26051325\n",
      "28 Train Loss 209.79689 Test MSE 239.83830408532145 Test RE 0.26070444919643126 Lambda1 0.27375886\n",
      "29 Train Loss 208.73381 Test MSE 237.14492108867782 Test RE 0.2592364613333587 Lambda1 0.2931822\n",
      "30 Train Loss 207.10687 Test MSE 234.82431204399072 Test RE 0.257964948869955 Lambda1 0.2897033\n",
      "31 Train Loss 204.6908 Test MSE 232.07939519329062 Test RE 0.2564528103340233 Lambda1 0.2836958\n",
      "32 Train Loss 202.04094 Test MSE 229.44828431381933 Test RE 0.25499494902098535 Lambda1 0.28115743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 Train Loss 200.2271 Test MSE 226.74214245627658 Test RE 0.2534867676978485 Lambda1 0.28373155\n",
      "34 Train Loss 195.59906 Test MSE 215.29689868646227 Test RE 0.24700631554614677 Lambda1 0.2995673\n",
      "35 Train Loss 188.63814 Test MSE 196.59001196260215 Test RE 0.23603146130923677 Lambda1 0.30913004\n",
      "36 Train Loss 182.22594 Test MSE 193.08662782280766 Test RE 0.23391887658764754 Lambda1 0.31394455\n",
      "37 Train Loss 167.80344 Test MSE 173.33911547198406 Test RE 0.22163454814645916 Lambda1 0.35518497\n",
      "38 Train Loss 158.00453 Test MSE 166.21146050494522 Test RE 0.21702994156041938 Lambda1 0.36537004\n",
      "39 Train Loss 151.07906 Test MSE 158.20242616610108 Test RE 0.2117365050939447 Lambda1 0.37919515\n",
      "40 Train Loss 142.08656 Test MSE 145.5934285850051 Test RE 0.2031234467300938 Lambda1 0.40259695\n",
      "41 Train Loss 135.21718 Test MSE 136.13692561834299 Test RE 0.19641612553395027 Lambda1 0.4264912\n",
      "42 Train Loss 130.4697 Test MSE 130.5610040349965 Test RE 0.19235164786800985 Lambda1 0.4494572\n",
      "43 Train Loss 123.859024 Test MSE 124.21545745227958 Test RE 0.18761907573672157 Lambda1 0.48596713\n",
      "44 Train Loss 120.2722 Test MSE 123.05049268262307 Test RE 0.18673720280658243 Lambda1 0.49531615\n",
      "45 Train Loss 116.38087 Test MSE 121.44736958251288 Test RE 0.18551679255902165 Lambda1 0.5027196\n",
      "46 Train Loss 114.54284 Test MSE 120.3444199782799 Test RE 0.18467246649341412 Lambda1 0.50476897\n",
      "47 Train Loss 113.014404 Test MSE 117.8932195739815 Test RE 0.18278206718060666 Lambda1 0.50597835\n",
      "48 Train Loss 111.03787 Test MSE 118.11489375873865 Test RE 0.1829538287003534 Lambda1 0.52160496\n",
      "49 Train Loss 107.05494 Test MSE 113.72103700030203 Test RE 0.1795186510863557 Lambda1 0.5500321\n",
      "50 Train Loss 104.16222 Test MSE 108.80856228919998 Test RE 0.17559846152334807 Lambda1 0.57536834\n",
      "51 Train Loss 100.9229 Test MSE 99.4066153348133 Test RE 0.16784051911245984 Lambda1 0.63752735\n",
      "52 Train Loss 96.93534 Test MSE 94.0864514836804 Test RE 0.1632874155471948 Lambda1 0.68577087\n",
      "53 Train Loss 94.42854 Test MSE 91.60005923262128 Test RE 0.16111539778244935 Lambda1 0.71172494\n",
      "54 Train Loss 92.10004 Test MSE 89.57260816778295 Test RE 0.15932237828115584 Lambda1 0.73029125\n",
      "55 Train Loss 88.38124 Test MSE 86.0853826436867 Test RE 0.1561902347855304 Lambda1 0.7568668\n",
      "56 Train Loss 86.483665 Test MSE 84.37800797917382 Test RE 0.15463357792305257 Lambda1 0.77235806\n",
      "57 Train Loss 83.4369 Test MSE 81.38734451396599 Test RE 0.15186846730618242 Lambda1 0.82264185\n",
      "58 Train Loss 80.80601 Test MSE 78.64224617984081 Test RE 0.14928532752689733 Lambda1 0.8658357\n",
      "59 Train Loss 79.38808 Test MSE 76.97958713012969 Test RE 0.14769879746124767 Lambda1 0.87491584\n",
      "60 Train Loss 76.49781 Test MSE 74.11915759116195 Test RE 0.14492870377990377 Lambda1 0.89275676\n",
      "61 Train Loss 74.60473 Test MSE 73.58074951135725 Test RE 0.1444013569352922 Lambda1 0.917159\n",
      "62 Train Loss 72.66614 Test MSE 72.57427530510529 Test RE 0.14341035952161246 Lambda1 0.94604236\n",
      "63 Train Loss 71.45535 Test MSE 72.39055861027089 Test RE 0.14322872786148322 Lambda1 0.9808964\n",
      "64 Train Loss 69.38773 Test MSE 70.21400784794947 Test RE 0.141059081740136 Lambda1 1.035475\n",
      "65 Train Loss 67.53869 Test MSE 67.0132478732026 Test RE 0.13780643700279205 Lambda1 1.0726205\n",
      "66 Train Loss 66.1914 Test MSE 64.33481212393106 Test RE 0.13502437918164584 Lambda1 1.09533\n",
      "67 Train Loss 63.69462 Test MSE 63.33065534266429 Test RE 0.13396648473582762 Lambda1 1.1135617\n",
      "68 Train Loss 61.53622 Test MSE 60.75617141029304 Test RE 0.13121526745260195 Lambda1 1.1392738\n",
      "69 Train Loss 60.546085 Test MSE 60.02007135606548 Test RE 0.13041796650255966 Lambda1 1.146446\n",
      "70 Train Loss 59.706646 Test MSE 60.05063307827003 Test RE 0.1304511661499281 Lambda1 1.1418349\n",
      "71 Train Loss 58.58547 Test MSE 58.45829527816019 Test RE 0.1287099862555784 Lambda1 1.1446316\n",
      "72 Train Loss 58.00066 Test MSE 58.20718450237077 Test RE 0.128433248389119 Lambda1 1.1472825\n",
      "73 Train Loss 57.40433 Test MSE 57.50685693230941 Test RE 0.12765827934693066 Lambda1 1.1574681\n",
      "74 Train Loss 56.463226 Test MSE 55.914866215446196 Test RE 0.12587886421626007 Lambda1 1.1764525\n",
      "Training time: 142.05\n",
      "Training time: 142.05\n",
      "inv_HT_stan_tune5\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 837.91077 Test MSE 858.0824108587057 Test RE 0.49312119619705425 Lambda1 -0.14289472\n",
      "1 Train Loss 836.97327 Test MSE 855.8661569444964 Test RE 0.492483968144403 Lambda1 -0.20283438\n",
      "2 Train Loss 832.71796 Test MSE 847.575628825843 Test RE 0.4900928886933767 Lambda1 -0.17348102\n",
      "3 Train Loss 772.12103 Test MSE 758.8012677457931 Test RE 0.4637171889739079 Lambda1 -0.02288652\n",
      "4 Train Loss 671.82153 Test MSE 673.4279480350853 Test RE 0.4368524127787778 Lambda1 0.0010716878\n",
      "5 Train Loss 644.24664 Test MSE 649.9243208969486 Test RE 0.4291613128369144 Lambda1 0.0003468413\n",
      "6 Train Loss 635.23724 Test MSE 641.7030446383784 Test RE 0.4264383168874854 Lambda1 0.0001690487\n",
      "7 Train Loss 625.2808 Test MSE 632.2911638046713 Test RE 0.42329947218772074 Lambda1 0.0006082579\n",
      "8 Train Loss 548.4657 Test MSE 540.649741658444 Test RE 0.3914237474687988 Lambda1 0.0012267091\n",
      "9 Train Loss 456.16034 Test MSE 439.13563811024324 Test RE 0.35276745547225963 Lambda1 0.00035640743\n",
      "10 Train Loss 338.38586 Test MSE 335.2809623975632 Test RE 0.3082432803664154 Lambda1 0.0014132919\n",
      "11 Train Loss 288.87186 Test MSE 295.4016973765514 Test RE 0.2893314635371248 Lambda1 0.0015190556\n",
      "12 Train Loss 266.76532 Test MSE 282.892083299826 Test RE 0.2831389176803195 Lambda1 0.00131951\n",
      "13 Train Loss 260.81607 Test MSE 280.6562397186599 Test RE 0.282017800872399 Lambda1 0.0013728216\n",
      "14 Train Loss 259.325 Test MSE 280.96782050381813 Test RE 0.28217430363365814 Lambda1 0.0021451886\n",
      "15 Train Loss 258.60135 Test MSE 280.6193509334398 Test RE 0.28199926639056555 Lambda1 0.0026181738\n",
      "16 Train Loss 257.6541 Test MSE 280.5080693568293 Test RE 0.28194334645026087 Lambda1 0.0022974717\n",
      "17 Train Loss 256.8709 Test MSE 281.3362026651363 Test RE 0.28235922504198485 Lambda1 0.002605638\n",
      "18 Train Loss 256.30243 Test MSE 281.4411080412406 Test RE 0.2824118635571221 Lambda1 0.0033471936\n",
      "19 Train Loss 255.4546 Test MSE 281.7734767868582 Test RE 0.2825785719341225 Lambda1 0.004703928\n",
      "20 Train Loss 254.9618 Test MSE 281.24938745994757 Test RE 0.28231565623500215 Lambda1 0.005597513\n",
      "21 Train Loss 254.62463 Test MSE 280.6444392804769 Test RE 0.282011871966968 Lambda1 0.006093274\n",
      "22 Train Loss 254.14886 Test MSE 279.708180838413 Test RE 0.2815410688086935 Lambda1 0.0076077194\n",
      "23 Train Loss 253.4694 Test MSE 277.0953198014894 Test RE 0.2802229920495885 Lambda1 0.010687903\n",
      "24 Train Loss 252.39737 Test MSE 274.2881197150268 Test RE 0.27879993572621625 Lambda1 0.016263764\n",
      "25 Train Loss 251.41223 Test MSE 272.52201408581175 Test RE 0.2779009078945543 Lambda1 0.019966612\n",
      "26 Train Loss 249.80893 Test MSE 269.5647954261911 Test RE 0.2763890018395247 Lambda1 0.027043909\n",
      "27 Train Loss 248.80772 Test MSE 268.8537567508475 Test RE 0.27602424160742456 Lambda1 0.027918093\n",
      "28 Train Loss 246.27061 Test MSE 266.3004915307906 Test RE 0.27471043376700116 Lambda1 0.03665001\n",
      "29 Train Loss 242.8008 Test MSE 262.782216117911 Test RE 0.27288970764555737 Lambda1 0.047910817\n",
      "30 Train Loss 238.60023 Test MSE 258.33872319959335 Test RE 0.27057266861129314 Lambda1 0.061177105\n",
      "31 Train Loss 234.85841 Test MSE 254.16623441896314 Test RE 0.26837873281937336 Lambda1 0.07587602\n",
      "32 Train Loss 229.32089 Test MSE 247.65962699408252 Test RE 0.2649212392201184 Lambda1 0.10355847\n",
      "33 Train Loss 226.79453 Test MSE 245.49255320741534 Test RE 0.2637596343043963 Lambda1 0.12189987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 Train Loss 224.21213 Test MSE 243.03097152898536 Test RE 0.26243392879547905 Lambda1 0.1387668\n",
      "35 Train Loss 220.81853 Test MSE 239.28482960873566 Test RE 0.26040346182571694 Lambda1 0.15555109\n",
      "36 Train Loss 216.60052 Test MSE 234.97023576759435 Test RE 0.25804508827153333 Lambda1 0.18263821\n",
      "37 Train Loss 212.61736 Test MSE 229.30094063630682 Test RE 0.25491306144569725 Lambda1 0.21079919\n",
      "38 Train Loss 207.27663 Test MSE 221.33789857507335 Test RE 0.25044770814958955 Lambda1 0.23331699\n",
      "39 Train Loss 196.058 Test MSE 206.28564469084864 Test RE 0.24178183758989674 Lambda1 0.2670121\n",
      "40 Train Loss 188.2802 Test MSE 196.71390958788217 Test RE 0.23610582706950806 Lambda1 0.2840537\n",
      "41 Train Loss 179.62279 Test MSE 187.886080573099 Test RE 0.23074721820746857 Lambda1 0.3007719\n",
      "42 Train Loss 174.90648 Test MSE 181.4048982402269 Test RE 0.22673244713752733 Lambda1 0.3178404\n",
      "43 Train Loss 169.90962 Test MSE 172.88040858795847 Test RE 0.22134109839337826 Lambda1 0.34482867\n",
      "44 Train Loss 167.21907 Test MSE 167.48759930620662 Test RE 0.21786150500373827 Lambda1 0.36423564\n",
      "45 Train Loss 163.10832 Test MSE 158.9766731040219 Test RE 0.21225399479617135 Lambda1 0.4069751\n",
      "46 Train Loss 159.02597 Test MSE 152.31873379537967 Test RE 0.20776186304237945 Lambda1 0.4573295\n",
      "47 Train Loss 156.22882 Test MSE 150.66947744527832 Test RE 0.20663401372601076 Lambda1 0.47412142\n",
      "48 Train Loss 153.18875 Test MSE 149.65445625919108 Test RE 0.20593681765953467 Lambda1 0.50569403\n",
      "49 Train Loss 146.12952 Test MSE 142.77480614644188 Test RE 0.20114764866866183 Lambda1 0.52836967\n",
      "50 Train Loss 142.39838 Test MSE 144.92668305575245 Test RE 0.20265781081585305 Lambda1 0.53162247\n",
      "51 Train Loss 139.18118 Test MSE 144.04584554386904 Test RE 0.20204101389128232 Lambda1 0.5390034\n",
      "52 Train Loss 136.74033 Test MSE 139.08856279388914 Test RE 0.19853399438452138 Lambda1 0.53699505\n",
      "53 Train Loss 133.82033 Test MSE 135.1168283669229 Test RE 0.19567885200833188 Lambda1 0.54710007\n",
      "54 Train Loss 131.54558 Test MSE 132.9870086997025 Test RE 0.19413050279758876 Lambda1 0.5526346\n",
      "55 Train Loss 130.10832 Test MSE 132.61530448959925 Test RE 0.19385901155031568 Lambda1 0.5450257\n",
      "56 Train Loss 128.78098 Test MSE 131.9695807953623 Test RE 0.19338647141276297 Lambda1 0.5468999\n",
      "57 Train Loss 126.69588 Test MSE 129.7504268986337 Test RE 0.1917536186140206 Lambda1 0.54655313\n",
      "58 Train Loss 125.52422 Test MSE 127.97420148478672 Test RE 0.190436584968037 Lambda1 0.55138594\n",
      "59 Train Loss 124.5916 Test MSE 126.8042079213129 Test RE 0.18956406077211058 Lambda1 0.5551751\n",
      "60 Train Loss 123.4983 Test MSE 124.87463848095669 Test RE 0.1881162412813607 Lambda1 0.55813247\n",
      "61 Train Loss 119.68867 Test MSE 123.21496805175023 Test RE 0.18686196221710225 Lambda1 0.5714385\n",
      "62 Train Loss 118.26888 Test MSE 121.9353753063717 Test RE 0.18588914517947477 Lambda1 0.58219576\n",
      "63 Train Loss 116.57113 Test MSE 120.60511216081467 Test RE 0.18487237864838305 Lambda1 0.59655255\n",
      "64 Train Loss 114.467514 Test MSE 119.58689522202779 Test RE 0.1840903256416694 Lambda1 0.60795575\n",
      "65 Train Loss 112.690475 Test MSE 117.66104927881165 Test RE 0.18260199965802623 Lambda1 0.6027408\n",
      "66 Train Loss 111.05061 Test MSE 113.41840869425633 Test RE 0.17927962927388932 Lambda1 0.58855057\n",
      "67 Train Loss 108.52973 Test MSE 110.67318810139957 Test RE 0.1770966642234074 Lambda1 0.59465593\n",
      "68 Train Loss 107.19484 Test MSE 109.34425441943665 Test RE 0.17603018866643402 Lambda1 0.5930312\n",
      "69 Train Loss 105.711334 Test MSE 106.05835352744488 Test RE 0.17336507467991602 Lambda1 0.59706855\n",
      "70 Train Loss 104.23943 Test MSE 103.59757862066508 Test RE 0.17134205566987673 Lambda1 0.60570735\n",
      "71 Train Loss 102.10065 Test MSE 102.73923678901382 Test RE 0.17063076513579856 Lambda1 0.6026665\n",
      "72 Train Loss 99.740425 Test MSE 100.23942177637144 Test RE 0.16854211792624937 Lambda1 0.6071854\n",
      "73 Train Loss 95.88598 Test MSE 94.32864697305706 Test RE 0.163497446093654 Lambda1 0.6198003\n",
      "74 Train Loss 93.56153 Test MSE 90.41502359975608 Test RE 0.1600698251805576 Lambda1 0.63526344\n",
      "Training time: 141.70\n",
      "Training time: 141.70\n",
      "inv_HT_stan_tune5\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 838.0982 Test MSE 858.4663634883166 Test RE 0.49323150844831937 Lambda1 -0.015104313\n",
      "1 Train Loss 838.05066 Test MSE 858.1812715706938 Test RE 0.4931496019235153 Lambda1 -0.015100296\n",
      "2 Train Loss 837.8332 Test MSE 857.842754403056 Test RE 0.49305232872614824 Lambda1 -0.018565694\n",
      "3 Train Loss 837.7862 Test MSE 857.7863237812463 Test RE 0.49303611147249976 Lambda1 -0.023964426\n",
      "4 Train Loss 837.13727 Test MSE 857.2718083426934 Test RE 0.49288822339411764 Lambda1 -0.070162766\n",
      "5 Train Loss 832.58453 Test MSE 850.356399023203 Test RE 0.49089619147066876 Lambda1 -0.21532428\n",
      "6 Train Loss 775.62317 Test MSE 778.1378358488017 Test RE 0.4695884830450772 Lambda1 0.010143516\n",
      "7 Train Loss 715.88776 Test MSE 701.5386957175294 Test RE 0.4458769134838744 Lambda1 -0.00091217423\n",
      "8 Train Loss 655.6005 Test MSE 655.9271997708686 Test RE 0.4311386831090148 Lambda1 0.0017235487\n",
      "9 Train Loss 649.245 Test MSE 653.1197353623513 Test RE 0.43021502537165396 Lambda1 0.001673863\n",
      "10 Train Loss 646.7831 Test MSE 653.0584668492725 Test RE 0.4301948458764713 Lambda1 0.00090161274\n",
      "11 Train Loss 642.78436 Test MSE 650.4393409951299 Test RE 0.42933131950123865 Lambda1 0.0008105306\n",
      "12 Train Loss 639.6413 Test MSE 648.4425195180117 Test RE 0.4286717983709561 Lambda1 0.0004823567\n",
      "13 Train Loss 629.52997 Test MSE 636.8683455322977 Test RE 0.4248288505108911 Lambda1 0.0006741461\n",
      "14 Train Loss 600.7462 Test MSE 607.2584555019713 Test RE 0.4148355395750988 Lambda1 0.0009570103\n",
      "15 Train Loss 546.77136 Test MSE 551.0386700059936 Test RE 0.39516658081483197 Lambda1 7.923036e-05\n",
      "16 Train Loss 491.98956 Test MSE 500.70366777869316 Test RE 0.3766860672907045 Lambda1 7.067043e-05\n",
      "17 Train Loss 439.86108 Test MSE 466.1421958272161 Test RE 0.3634531016871972 Lambda1 -0.00010229527\n",
      "18 Train Loss 381.52692 Test MSE 402.375365824748 Test RE 0.3376796297019276 Lambda1 -9.4774325e-05\n",
      "19 Train Loss 352.1028 Test MSE 376.55747419828356 Test RE 0.32666665596594585 Lambda1 -0.000121111625\n",
      "20 Train Loss 332.4766 Test MSE 363.774397914657 Test RE 0.32107407195312826 Lambda1 -6.3045713e-06\n",
      "21 Train Loss 309.3898 Test MSE 352.44886951536694 Test RE 0.31603649133990025 Lambda1 2.8353305e-05\n",
      "22 Train Loss 297.74643 Test MSE 338.06793661541786 Test RE 0.3095217428162154 Lambda1 -3.093261e-05\n",
      "23 Train Loss 292.89532 Test MSE 329.5780900981644 Test RE 0.30561054613238414 Lambda1 -7.694662e-05\n",
      "24 Train Loss 279.82886 Test MSE 308.6809542913087 Test RE 0.2957631668415123 Lambda1 -0.0005081244\n",
      "25 Train Loss 270.2403 Test MSE 296.03536852301096 Test RE 0.2896416221875038 Lambda1 -0.00102911\n",
      "26 Train Loss 261.3595 Test MSE 285.8187475883108 Test RE 0.2845997580742611 Lambda1 -0.00084828504\n",
      "27 Train Loss 259.4238 Test MSE 286.7683773366233 Test RE 0.28507215584980766 Lambda1 -0.00039696015\n",
      "28 Train Loss 257.99362 Test MSE 284.06889182036673 Test RE 0.28372722415422197 Lambda1 -0.00021480801\n",
      "29 Train Loss 256.662 Test MSE 284.1019191561105 Test RE 0.2837437174806724 Lambda1 -5.276245e-06\n",
      "30 Train Loss 256.13095 Test MSE 284.5135325972969 Test RE 0.2839491903193084 Lambda1 -4.7154886e-06\n",
      "31 Train Loss 255.80083 Test MSE 284.4836147618927 Test RE 0.2839342606806393 Lambda1 4.1647243e-05\n",
      "32 Train Loss 255.31473 Test MSE 283.88306603085704 Test RE 0.28363440783009003 Lambda1 0.00014520729\n",
      "33 Train Loss 255.07329 Test MSE 284.17330071061195 Test RE 0.2837793610194915 Lambda1 0.00011888895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 Train Loss 254.90952 Test MSE 283.3354429145839 Test RE 0.28336070404791813 Lambda1 0.00016122528\n",
      "35 Train Loss 254.86981 Test MSE 283.31617355457996 Test RE 0.28335106834509255 Lambda1 0.00014644086\n",
      "36 Train Loss 254.83533 Test MSE 283.3635537792348 Test RE 0.2833747603847355 Lambda1 0.00015431158\n",
      "37 Train Loss 254.70187 Test MSE 283.49702472322303 Test RE 0.2834414906390606 Lambda1 0.00018009265\n",
      "38 Train Loss 254.62254 Test MSE 283.81446334343144 Test RE 0.2836001344607953 Lambda1 0.00017153038\n",
      "39 Train Loss 254.5853 Test MSE 283.96724695687436 Test RE 0.2836764583082014 Lambda1 0.00018361011\n",
      "40 Train Loss 254.40431 Test MSE 283.27752465166253 Test RE 0.2833317408543593 Lambda1 0.0002596278\n",
      "41 Train Loss 254.32727 Test MSE 282.87892609281437 Test RE 0.28313233326016485 Lambda1 0.00032114997\n",
      "42 Train Loss 254.30908 Test MSE 282.9512211135496 Test RE 0.2831685108406949 Lambda1 0.00030583036\n",
      "43 Train Loss 254.26578 Test MSE 283.0384707681232 Test RE 0.2832121658042011 Lambda1 0.00026170234\n",
      "44 Train Loss 254.20772 Test MSE 283.1983270560003 Test RE 0.2832921317091187 Lambda1 0.00024782517\n",
      "45 Train Loss 254.16606 Test MSE 282.9202091288495 Test RE 0.2831529925154429 Lambda1 0.0002285273\n",
      "46 Train Loss 254.12743 Test MSE 282.84332644014773 Test RE 0.2831145169279276 Lambda1 0.00020734561\n",
      "47 Train Loss 254.0828 Test MSE 282.99308798980275 Test RE 0.28318945957956676 Lambda1 0.00015298495\n",
      "48 Train Loss 253.99147 Test MSE 282.92824107105946 Test RE 0.2831570117623154 Lambda1 0.00015831292\n",
      "49 Train Loss 253.96178 Test MSE 282.9255013948739 Test RE 0.28315564081327727 Lambda1 0.00016414032\n",
      "50 Train Loss 253.93512 Test MSE 282.94258515856814 Test RE 0.2831641895142809 Lambda1 0.00016676978\n",
      "51 Train Loss 253.89418 Test MSE 282.96376120311487 Test RE 0.2831747856309771 Lambda1 0.00013460078\n",
      "52 Train Loss 253.87013 Test MSE 283.0243482240125 Test RE 0.2832051001106457 Lambda1 9.212397e-05\n",
      "53 Train Loss 253.83386 Test MSE 282.9171951492352 Test RE 0.2831514842816856 Lambda1 7.089656e-05\n",
      "54 Train Loss 253.79866 Test MSE 282.939050419172 Test RE 0.28316242075483145 Lambda1 3.805382e-05\n",
      "55 Train Loss 253.66335 Test MSE 282.91663962462644 Test RE 0.2831512062892244 Lambda1 1.9778121e-05\n",
      "56 Train Loss 253.57663 Test MSE 282.9987893796063 Test RE 0.283192312238225 Lambda1 3.603742e-05\n",
      "57 Train Loss 253.40103 Test MSE 283.61791501926956 Test RE 0.2835019175053163 Lambda1 8.824273e-05\n",
      "58 Train Loss 253.32079 Test MSE 283.706718483794 Test RE 0.2835462976039307 Lambda1 0.000108571054\n",
      "59 Train Loss 253.26788 Test MSE 283.7757351071003 Test RE 0.283580784305745 Lambda1 0.00014655398\n",
      "60 Train Loss 253.16046 Test MSE 283.8973050114578 Test RE 0.2836415209951086 Lambda1 0.0001412697\n",
      "61 Train Loss 252.93314 Test MSE 283.87398287146453 Test RE 0.2836298701921467 Lambda1 0.00015346133\n",
      "62 Train Loss 252.85358 Test MSE 283.831900225018 Test RE 0.2836088461838569 Lambda1 0.00013576748\n",
      "63 Train Loss 252.68036 Test MSE 283.22401888192627 Test RE 0.2833049815856435 Lambda1 0.00016714603\n",
      "64 Train Loss 252.49684 Test MSE 283.3563401413685 Test RE 0.28337115340020214 Lambda1 0.00014064263\n",
      "65 Train Loss 251.97493 Test MSE 283.23058303145535 Test RE 0.2833082645796033 Lambda1 6.652961e-05\n",
      "66 Train Loss 251.52261 Test MSE 284.72326945491795 Test RE 0.2840538314568107 Lambda1 3.7192156e-05\n",
      "67 Train Loss 251.15334 Test MSE 284.99693749498846 Test RE 0.2841903109671145 Lambda1 4.2299376e-05\n",
      "68 Train Loss 250.87102 Test MSE 285.1100479234096 Test RE 0.28424670051870465 Lambda1 4.5645786e-05\n",
      "69 Train Loss 250.4674 Test MSE 285.92260273182194 Test RE 0.28465145948377774 Lambda1 4.450529e-05\n",
      "70 Train Loss 250.01637 Test MSE 287.4618661951424 Test RE 0.28541664116153564 Lambda1 3.2086107e-05\n",
      "71 Train Loss 249.74149 Test MSE 288.1331416796047 Test RE 0.2857496965956595 Lambda1 1.7298555e-05\n",
      "72 Train Loss 249.51044 Test MSE 288.0194019264146 Test RE 0.28569329158123064 Lambda1 2.5172865e-05\n",
      "73 Train Loss 249.25803 Test MSE 288.11318413293696 Test RE 0.28573980019563505 Lambda1 1.979094e-05\n",
      "74 Train Loss 248.95016 Test MSE 288.6805554806356 Test RE 0.28602101058157076 Lambda1 1.8616422e-05\n",
      "Training time: 147.81\n",
      "Training time: 147.81\n",
      "inv_HT_stan_tune5\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 838.1173 Test MSE 858.6109459766013 Test RE 0.49327304160527663 Lambda1 -0.04860927\n",
      "1 Train Loss 838.03467 Test MSE 858.2408977786027 Test RE 0.4931667335779394 Lambda1 -0.04856504\n",
      "2 Train Loss 837.9883 Test MSE 857.9828494648846 Test RE 0.4930925874893139 Lambda1 -0.048925012\n",
      "3 Train Loss 837.88916 Test MSE 857.9172881519493 Test RE 0.49307374771002993 Lambda1 -0.053884137\n",
      "4 Train Loss 837.8871 Test MSE 857.9453869599135 Test RE 0.49308182230608844 Lambda1 -0.055232003\n",
      "5 Train Loss 837.8524 Test MSE 857.9957548019685 Test RE 0.49309629589801335 Lambda1 -0.097730465\n",
      "6 Train Loss 837.82996 Test MSE 857.8100344140606 Test RE 0.49304292559222657 Lambda1 -0.12993462\n",
      "7 Train Loss 837.82446 Test MSE 857.7105684045928 Test RE 0.49301433975617986 Lambda1 -0.14686604\n",
      "8 Train Loss 836.8112 Test MSE 856.4342648276152 Test RE 0.492647391834486 Lambda1 -0.14224619\n",
      "9 Train Loss 827.9243 Test MSE 841.9375284309656 Test RE 0.4884601119978267 Lambda1 0.0009944618\n",
      "10 Train Loss 764.6461 Test MSE 751.8064435790885 Test RE 0.4615749086945201 Lambda1 -0.0027545516\n",
      "11 Train Loss 604.64233 Test MSE 594.1348574555304 Test RE 0.4103285042084756 Lambda1 -0.00028525005\n",
      "12 Train Loss 531.99744 Test MSE 495.8898666876893 Test RE 0.374870950610648 Lambda1 6.8110545e-05\n",
      "13 Train Loss 406.46384 Test MSE 438.0509232834666 Test RE 0.35233149827938626 Lambda1 -8.707419e-05\n",
      "14 Train Loss 364.2463 Test MSE 373.63185718955884 Test RE 0.32539518329811884 Lambda1 -0.00014470072\n",
      "15 Train Loss 328.549 Test MSE 337.3276893356871 Test RE 0.3091826864241931 Lambda1 0.00027318232\n",
      "16 Train Loss 315.34512 Test MSE 331.56450174825943 Test RE 0.3065301405971643 Lambda1 5.068253e-05\n",
      "17 Train Loss 296.08795 Test MSE 326.32171163574236 Test RE 0.30409701403734823 Lambda1 7.3645286e-05\n",
      "18 Train Loss 282.18777 Test MSE 302.2948070161523 Test RE 0.29268772836656076 Lambda1 -0.000661613\n",
      "19 Train Loss 262.3193 Test MSE 286.07398041990075 Test RE 0.2847268018624674 Lambda1 -0.0010554474\n",
      "20 Train Loss 259.23886 Test MSE 281.54312286242236 Test RE 0.28246304226677105 Lambda1 -0.0010022304\n",
      "21 Train Loss 257.657 Test MSE 281.35011297288486 Test RE 0.2823662054005913 Lambda1 -0.00031940523\n",
      "22 Train Loss 256.67273 Test MSE 280.9951387333212 Test RE 0.2821880210675786 Lambda1 0.00041324578\n",
      "23 Train Loss 256.12292 Test MSE 280.95883105073165 Test RE 0.28216978957053546 Lambda1 0.0005085565\n",
      "24 Train Loss 255.6422 Test MSE 280.96058579763246 Test RE 0.2821706707241706 Lambda1 0.0006161389\n",
      "25 Train Loss 255.2297 Test MSE 281.19661464520243 Test RE 0.28228916854927183 Lambda1 0.000867009\n",
      "26 Train Loss 254.91904 Test MSE 281.02692986721854 Test RE 0.2822039836630084 Lambda1 0.0011988663\n",
      "27 Train Loss 254.65924 Test MSE 280.95070212108243 Test RE 0.2821657075579657 Lambda1 0.0013786476\n",
      "28 Train Loss 254.43416 Test MSE 281.0912462025888 Test RE 0.2822362746745928 Lambda1 0.001618464\n",
      "29 Train Loss 254.24216 Test MSE 281.1600236518339 Test RE 0.28227080137021626 Lambda1 0.0016096763\n",
      "30 Train Loss 253.99808 Test MSE 281.5408729412991 Test RE 0.2824619136282368 Lambda1 0.001720621\n",
      "31 Train Loss 253.82695 Test MSE 281.63808010839534 Test RE 0.2825106720065891 Lambda1 0.0023350648\n",
      "32 Train Loss 253.60664 Test MSE 281.24590629518826 Test RE 0.28231390904834436 Lambda1 0.003077178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 Train Loss 253.37248 Test MSE 280.42474334662256 Test RE 0.2819014671575106 Lambda1 0.0035263347\n",
      "34 Train Loss 253.12978 Test MSE 279.451034806484 Test RE 0.28141162351422455 Lambda1 0.0051357285\n",
      "35 Train Loss 252.89108 Test MSE 278.28287029545874 Test RE 0.280822827453242 Lambda1 0.00651452\n",
      "36 Train Loss 252.18271 Test MSE 276.47761393314835 Test RE 0.27991047872156316 Lambda1 0.009906271\n",
      "37 Train Loss 250.90941 Test MSE 275.0485237752244 Test RE 0.27918612429915235 Lambda1 0.013863991\n",
      "38 Train Loss 250.41394 Test MSE 274.97569140027326 Test RE 0.2791491578496087 Lambda1 0.014206121\n",
      "39 Train Loss 249.36446 Test MSE 272.1781981810424 Test RE 0.27772555158974027 Lambda1 0.019473799\n",
      "40 Train Loss 248.73367 Test MSE 270.46673592375356 Test RE 0.2768510025597124 Lambda1 0.0224355\n",
      "41 Train Loss 247.28639 Test MSE 267.32498118854113 Test RE 0.2752383483696288 Lambda1 0.029919423\n",
      "42 Train Loss 246.51706 Test MSE 266.75420771294995 Test RE 0.2749443565881565 Lambda1 0.03322297\n",
      "43 Train Loss 244.89221 Test MSE 265.57514183839555 Test RE 0.2743360503035411 Lambda1 0.039687365\n",
      "44 Train Loss 242.56937 Test MSE 263.8658254748058 Test RE 0.2734517731242591 Lambda1 0.049692784\n",
      "45 Train Loss 240.75192 Test MSE 261.726604169722 Test RE 0.2723410489179072 Lambda1 0.056058053\n",
      "46 Train Loss 239.12427 Test MSE 259.552257466186 Test RE 0.27120742538062753 Lambda1 0.060297687\n",
      "47 Train Loss 235.06282 Test MSE 255.11256573797633 Test RE 0.26887789281769464 Lambda1 0.077396356\n",
      "48 Train Loss 230.34164 Test MSE 251.87102631452484 Test RE 0.2671642087941333 Lambda1 0.089518145\n",
      "49 Train Loss 224.9139 Test MSE 246.30496923884587 Test RE 0.26419570774600837 Lambda1 0.1182455\n",
      "50 Train Loss 219.92865 Test MSE 241.1846846144035 Test RE 0.26143518351511547 Lambda1 0.13101475\n",
      "51 Train Loss 216.06194 Test MSE 235.76484783848775 Test RE 0.2584810428087674 Lambda1 0.15016113\n",
      "52 Train Loss 213.9366 Test MSE 232.71867937471964 Test RE 0.25680577893504986 Lambda1 0.16292629\n",
      "53 Train Loss 211.0795 Test MSE 230.57074088102306 Test RE 0.255617903258778 Lambda1 0.17596091\n",
      "54 Train Loss 206.93294 Test MSE 228.3470363797282 Test RE 0.25438228282454817 Lambda1 0.1978077\n",
      "55 Train Loss 205.46185 Test MSE 225.92238250379322 Test RE 0.2530281267580846 Lambda1 0.21674003\n",
      "56 Train Loss 204.29936 Test MSE 224.26004604382524 Test RE 0.2520955178197292 Lambda1 0.2315117\n",
      "57 Train Loss 202.96904 Test MSE 222.93197132306494 Test RE 0.2513479507878091 Lambda1 0.23891552\n",
      "58 Train Loss 199.88269 Test MSE 216.7400718224931 Test RE 0.2478327963264965 Lambda1 0.25422856\n",
      "59 Train Loss 196.25621 Test MSE 213.07029025295796 Test RE 0.24572572176037769 Lambda1 0.26162943\n",
      "60 Train Loss 192.98164 Test MSE 206.49754759351435 Test RE 0.2419059885522082 Lambda1 0.27080026\n",
      "61 Train Loss 189.9527 Test MSE 201.60065844081848 Test RE 0.23902049624973595 Lambda1 0.27590296\n",
      "62 Train Loss 181.77887 Test MSE 188.75615344941 Test RE 0.23128087935295488 Lambda1 0.28870526\n",
      "63 Train Loss 175.41559 Test MSE 181.14960126836354 Test RE 0.22657284701750402 Lambda1 0.30428743\n",
      "64 Train Loss 168.85645 Test MSE 172.30209200461945 Test RE 0.22097057504006015 Lambda1 0.32076412\n",
      "65 Train Loss 160.39111 Test MSE 158.5994889124213 Test RE 0.2120020509332299 Lambda1 0.3528698\n",
      "66 Train Loss 147.88359 Test MSE 145.09785577171232 Test RE 0.20277745493615554 Lambda1 0.37928227\n",
      "67 Train Loss 139.79852 Test MSE 140.25798490767187 Test RE 0.19936685969168888 Lambda1 0.38477847\n",
      "68 Train Loss 130.44815 Test MSE 130.53787807560514 Test RE 0.19233461171917005 Lambda1 0.4015753\n",
      "69 Train Loss 120.83373 Test MSE 116.85787465057163 Test RE 0.18197769605110406 Lambda1 0.4387037\n",
      "70 Train Loss 113.80214 Test MSE 109.28661345725187 Test RE 0.17598378528179193 Lambda1 0.46841937\n",
      "71 Train Loss 110.43041 Test MSE 103.79777543247477 Test RE 0.17150753047132045 Lambda1 0.49316287\n",
      "72 Train Loss 104.54646 Test MSE 95.60299671118771 Test RE 0.16459814006402382 Lambda1 0.52409536\n",
      "73 Train Loss 100.388336 Test MSE 91.72659042989514 Test RE 0.16122663727916317 Lambda1 0.55195355\n",
      "74 Train Loss 96.20519 Test MSE 84.73583159841156 Test RE 0.15496111000439106 Lambda1 0.5746077\n",
      "Training time: 141.53\n",
      "Training time: 141.53\n",
      "inv_HT_stan_tune5\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 838.412 Test MSE 859.2213184578787 Test RE 0.49344834032614054 Lambda1 0.04047337\n",
      "1 Train Loss 837.9181 Test MSE 858.0206698875558 Test RE 0.4931034552849677 Lambda1 0.040197715\n",
      "2 Train Loss 837.9057 Test MSE 857.9372337349657 Test RE 0.49307947937341035 Lambda1 0.038094714\n",
      "3 Train Loss 837.9047 Test MSE 857.946592279538 Test RE 0.4930821686690371 Lambda1 0.0363046\n",
      "4 Train Loss 837.9041 Test MSE 857.9494574342068 Test RE 0.4930829920043769 Lambda1 0.034883153\n",
      "5 Train Loss 837.8744 Test MSE 857.8947128481775 Test RE 0.49306726027644016 Lambda1 -0.0736152\n",
      "6 Train Loss 837.86707 Test MSE 857.7611971835643 Test RE 0.4930288903205682 Lambda1 -0.1199576\n",
      "7 Train Loss 837.7634 Test MSE 857.7613896365527 Test RE 0.49302894563018024 Lambda1 -0.4092281\n",
      "8 Train Loss 836.47107 Test MSE 856.4061679330921 Test RE 0.4926393106682467 Lambda1 -1.1609058\n",
      "9 Train Loss 824.5189 Test MSE 840.6709569832374 Test RE 0.4880925654400928 Lambda1 -1.3478884\n",
      "10 Train Loss 798.9804 Test MSE 804.6945605710321 Test RE 0.47753444539200474 Lambda1 -1.5216924\n",
      "11 Train Loss 762.13837 Test MSE 769.3945207628539 Test RE 0.4669428344073501 Lambda1 -1.6368091\n",
      "12 Train Loss 727.753 Test MSE 721.8811627907211 Test RE 0.45229524836043306 Lambda1 -1.7256676\n",
      "13 Train Loss 698.0796 Test MSE 687.2245269316014 Test RE 0.441304642405335 Lambda1 -1.6626413\n",
      "14 Train Loss 669.8803 Test MSE 647.1803277448513 Test RE 0.4282543908494881 Lambda1 -1.5988048\n",
      "15 Train Loss 656.45544 Test MSE 643.751569552204 Test RE 0.4271184394571925 Lambda1 -1.5217935\n",
      "16 Train Loss 638.9409 Test MSE 631.2214062464617 Test RE 0.42294123572176145 Lambda1 -1.5258923\n",
      "17 Train Loss 630.20667 Test MSE 619.8623351290021 Test RE 0.41911846477587233 Lambda1 -1.5156851\n",
      "18 Train Loss 614.257 Test MSE 603.4144312666034 Test RE 0.4135204739402706 Lambda1 -1.4784294\n",
      "19 Train Loss 603.0907 Test MSE 590.4731035257024 Test RE 0.4090620878131553 Lambda1 -1.5056092\n",
      "20 Train Loss 597.3267 Test MSE 587.8621330238927 Test RE 0.40815668472347966 Lambda1 -1.537282\n",
      "21 Train Loss 592.68604 Test MSE 585.1281308221602 Test RE 0.40720646044182407 Lambda1 -1.5617884\n",
      "22 Train Loss 589.37 Test MSE 583.3695209727312 Test RE 0.40659406792529623 Lambda1 -1.5749531\n",
      "23 Train Loss 584.67975 Test MSE 578.010387320764 Test RE 0.4047221674154662 Lambda1 -1.5807103\n",
      "24 Train Loss 580.5955 Test MSE 571.2156961584395 Test RE 0.4023363178598544 Lambda1 -1.6150765\n",
      "25 Train Loss 575.1101 Test MSE 563.4995901957215 Test RE 0.3996096551836079 Lambda1 -1.6584716\n",
      "26 Train Loss 568.78925 Test MSE 558.4663992365597 Test RE 0.3978209909327595 Lambda1 -1.6604292\n",
      "27 Train Loss 563.9122 Test MSE 552.7338726867118 Test RE 0.3957739547324409 Lambda1 -1.649832\n",
      "28 Train Loss 561.857 Test MSE 550.9317316743274 Test RE 0.39512823459012664 Lambda1 -1.6304673\n",
      "29 Train Loss 554.00665 Test MSE 543.6698154254054 Test RE 0.39251547308187584 Lambda1 -1.5561055\n",
      "30 Train Loss 542.7823 Test MSE 527.6244650676931 Test RE 0.38667993156454344 Lambda1 -1.5361253\n",
      "31 Train Loss 529.9704 Test MSE 515.8165437323192 Test RE 0.38232861554395475 Lambda1 -1.4940281\n",
      "32 Train Loss 524.56165 Test MSE 505.77192342681195 Test RE 0.37858772539661045 Lambda1 -1.4400373\n",
      "33 Train Loss 508.99878 Test MSE 491.69174033982324 Test RE 0.37328077838971707 Lambda1 -1.3890887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 Train Loss 495.09335 Test MSE 481.99294806510517 Test RE 0.36958089491142004 Lambda1 -1.3852962\n",
      "35 Train Loss 472.07095 Test MSE 464.5194508740318 Test RE 0.36281991948842646 Lambda1 -1.3949615\n",
      "36 Train Loss 448.40085 Test MSE 445.96734732230306 Test RE 0.3555008975581584 Lambda1 -1.4068952\n",
      "37 Train Loss 434.78497 Test MSE 434.216835366313 Test RE 0.35078619979436154 Lambda1 -1.4209197\n",
      "38 Train Loss 427.3219 Test MSE 428.6883615818173 Test RE 0.34854593108271975 Lambda1 -1.4353075\n",
      "39 Train Loss 414.39438 Test MSE 410.28044884603116 Test RE 0.34098053011170437 Lambda1 -1.4761333\n",
      "40 Train Loss 397.83084 Test MSE 394.8099869658215 Test RE 0.3344900747536368 Lambda1 -1.5138395\n",
      "41 Train Loss 375.05927 Test MSE 374.2361143956898 Test RE 0.32565820015248265 Lambda1 -1.5450952\n",
      "42 Train Loss 368.67194 Test MSE 366.18462746575653 Test RE 0.32213597270972016 Lambda1 -1.5590957\n",
      "43 Train Loss 340.49817 Test MSE 339.38562776440506 Test RE 0.3101243693375637 Lambda1 -1.5958239\n",
      "44 Train Loss 315.99573 Test MSE 315.26622721573335 Test RE 0.2989013629922447 Lambda1 -1.6144928\n",
      "45 Train Loss 304.5548 Test MSE 298.45705699944426 Test RE 0.29082390176036965 Lambda1 -1.6104735\n",
      "46 Train Loss 284.86853 Test MSE 274.2793469062761 Test RE 0.278795477133413 Lambda1 -1.6113093\n",
      "47 Train Loss 273.9534 Test MSE 272.7312279858555 Test RE 0.2780075590556523 Lambda1 -1.6151599\n",
      "48 Train Loss 263.24664 Test MSE 261.1358854326121 Test RE 0.27203353750026665 Lambda1 -1.6142001\n",
      "49 Train Loss 258.12994 Test MSE 256.47008709011294 Test RE 0.2695923287999318 Lambda1 -1.6328803\n",
      "50 Train Loss 248.08418 Test MSE 241.3451511235083 Test RE 0.2615221389032015 Lambda1 -1.6477859\n",
      "51 Train Loss 238.92007 Test MSE 234.153882525426 Test RE 0.2575964373351544 Lambda1 -1.6580238\n",
      "52 Train Loss 236.73257 Test MSE 233.20208134389588 Test RE 0.25707235835291964 Lambda1 -1.664395\n",
      "53 Train Loss 228.33702 Test MSE 222.736423253894 Test RE 0.25123768982075345 Lambda1 -1.6793128\n",
      "54 Train Loss 211.59708 Test MSE 210.80863305553504 Test RE 0.24441810166549455 Lambda1 -1.6752802\n",
      "55 Train Loss 204.25108 Test MSE 206.69160950861414 Test RE 0.24201963085868392 Lambda1 -1.6781144\n",
      "56 Train Loss 201.56573 Test MSE 203.15769379713456 Test RE 0.23994174209221653 Lambda1 -1.6832472\n",
      "57 Train Loss 198.22029 Test MSE 200.77647956727438 Test RE 0.2385314170084363 Lambda1 -1.6852019\n",
      "58 Train Loss 195.73315 Test MSE 197.19464006827806 Test RE 0.236394149353232 Lambda1 -1.694969\n",
      "59 Train Loss 193.80191 Test MSE 192.61289732362985 Test RE 0.23363174494964264 Lambda1 -1.7098966\n",
      "60 Train Loss 186.34892 Test MSE 187.17888522289715 Test RE 0.2303125473611972 Lambda1 -1.7284343\n",
      "61 Train Loss 180.02327 Test MSE 179.7734177074465 Test RE 0.22571057550730936 Lambda1 -1.7432286\n",
      "62 Train Loss 177.57191 Test MSE 176.34976685521326 Test RE 0.22355099929753472 Lambda1 -1.7533207\n",
      "63 Train Loss 174.10275 Test MSE 172.84415002131263 Test RE 0.22131788601240637 Lambda1 -1.7569903\n",
      "64 Train Loss 173.01978 Test MSE 170.52173370532756 Test RE 0.21982599135341538 Lambda1 -1.7588621\n",
      "65 Train Loss 167.31541 Test MSE 164.81697197658144 Test RE 0.21611760005384534 Lambda1 -1.762399\n",
      "66 Train Loss 164.5657 Test MSE 158.35586770413906 Test RE 0.21183916249857915 Lambda1 -1.7764903\n",
      "67 Train Loss 160.61295 Test MSE 155.40949610074367 Test RE 0.20985916801715576 Lambda1 -1.7796663\n",
      "68 Train Loss 156.16718 Test MSE 151.12652509483604 Test RE 0.2069471829207982 Lambda1 -1.7885773\n",
      "69 Train Loss 152.75972 Test MSE 147.38089275806684 Test RE 0.20436652581929604 Lambda1 -1.7933717\n",
      "70 Train Loss 150.36101 Test MSE 143.748166408901 Test RE 0.20183214115836373 Lambda1 -1.7909173\n",
      "71 Train Loss 148.43988 Test MSE 142.35692569255335 Test RE 0.20085306845610973 Lambda1 -1.7862049\n",
      "72 Train Loss 145.30397 Test MSE 139.94541854386688 Test RE 0.19914459024018585 Lambda1 -1.793226\n",
      "73 Train Loss 142.43427 Test MSE 140.20360683166447 Test RE 0.1993282086400443 Lambda1 -1.7747983\n",
      "74 Train Loss 139.44006 Test MSE 141.79339021429897 Test RE 0.20045512480739053 Lambda1 -1.7705108\n",
      "Training time: 144.99\n",
      "Training time: 144.99\n",
      "inv_HT_stan_tune5\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 837.99976 Test MSE 858.2537209546123 Test RE 0.49317041782336596 Lambda1 -0.0013375261\n",
      "1 Train Loss 837.85175 Test MSE 857.8422136289213 Test RE 0.49305217331888557 Lambda1 -0.0015185402\n",
      "2 Train Loss 837.797 Test MSE 857.5785867083385 Test RE 0.49297640658001635 Lambda1 -0.0033954668\n",
      "3 Train Loss 837.4269 Test MSE 856.6548345264774 Test RE 0.4927108269957339 Lambda1 0.014476083\n",
      "4 Train Loss 824.9718 Test MSE 837.9945616564081 Test RE 0.48731499008235263 Lambda1 0.1875492\n",
      "5 Train Loss 762.1197 Test MSE 758.828224723581 Test RE 0.46372542584927623 Lambda1 0.26046583\n",
      "6 Train Loss 722.3334 Test MSE 720.1328722704477 Test RE 0.4517472199017758 Lambda1 0.24871169\n",
      "7 Train Loss 690.89624 Test MSE 690.0823670455467 Test RE 0.4422212785404623 Lambda1 0.22422574\n",
      "8 Train Loss 662.7257 Test MSE 658.5816479577344 Test RE 0.43201018210270503 Lambda1 0.2029764\n",
      "9 Train Loss 633.8362 Test MSE 634.2944150484647 Test RE 0.4239694994290718 Lambda1 0.22657509\n",
      "10 Train Loss 601.4097 Test MSE 600.2684975193474 Test RE 0.4124411096015264 Lambda1 0.22472578\n",
      "11 Train Loss 574.6678 Test MSE 570.5348775008497 Test RE 0.4020964787399885 Lambda1 0.22673571\n",
      "12 Train Loss 495.22836 Test MSE 496.4923387497886 Test RE 0.37509860268988005 Lambda1 0.2339635\n",
      "13 Train Loss 412.2414 Test MSE 415.7664851224385 Test RE 0.34325265856628645 Lambda1 0.22456191\n",
      "14 Train Loss 340.63632 Test MSE 357.2912948490005 Test RE 0.3182001559495702 Lambda1 0.2146345\n",
      "15 Train Loss 292.57507 Test MSE 306.06058740176877 Test RE 0.2945051368891508 Lambda1 0.20596045\n",
      "16 Train Loss 268.6099 Test MSE 280.78823150118524 Test RE 0.2820841091374846 Lambda1 0.1931531\n",
      "17 Train Loss 249.76534 Test MSE 267.8503411280406 Test RE 0.2755086714569016 Lambda1 0.1715087\n",
      "18 Train Loss 242.15607 Test MSE 263.5054614957783 Test RE 0.27326498152887513 Lambda1 0.15637372\n",
      "19 Train Loss 237.14984 Test MSE 258.2895630262615 Test RE 0.27054692327978497 Lambda1 0.14255215\n",
      "20 Train Loss 230.24805 Test MSE 253.88363614465985 Test RE 0.2682294910042036 Lambda1 0.1669683\n",
      "21 Train Loss 228.41957 Test MSE 252.53495325612784 Test RE 0.2675160967853086 Lambda1 0.1845044\n",
      "22 Train Loss 223.23657 Test MSE 246.47476254157937 Test RE 0.2642867553071343 Lambda1 0.20501295\n",
      "23 Train Loss 219.08038 Test MSE 243.1628691947129 Test RE 0.2625051331510205 Lambda1 0.23295657\n",
      "24 Train Loss 217.01074 Test MSE 241.6188960704911 Test RE 0.2616704121851306 Lambda1 0.2521999\n",
      "25 Train Loss 214.60838 Test MSE 239.69205056059045 Test RE 0.26062494821911847 Lambda1 0.27236152\n",
      "26 Train Loss 212.27815 Test MSE 238.92433413979808 Test RE 0.2602072319778937 Lambda1 0.27494273\n",
      "27 Train Loss 209.23198 Test MSE 235.64259489874226 Test RE 0.2584140180483762 Lambda1 0.27477714\n",
      "28 Train Loss 207.64809 Test MSE 233.59922229382292 Test RE 0.25729116114129236 Lambda1 0.28742215\n",
      "29 Train Loss 205.97195 Test MSE 230.01187042214983 Test RE 0.2553079247120155 Lambda1 0.29697102\n",
      "30 Train Loss 202.22316 Test MSE 224.02677050161938 Test RE 0.25196436869047745 Lambda1 0.30756152\n",
      "31 Train Loss 199.99825 Test MSE 219.15996143371297 Test RE 0.24921247499587243 Lambda1 0.3161888\n",
      "32 Train Loss 195.60744 Test MSE 211.45119591306798 Test RE 0.24479032194672815 Lambda1 0.31947485\n",
      "33 Train Loss 191.0104 Test MSE 201.6994333520756 Test RE 0.23907904352170806 Lambda1 0.30881068\n",
      "34 Train Loss 186.1478 Test MSE 192.52936742678023 Test RE 0.23358108024321345 Lambda1 0.30534106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 Train Loss 173.86044 Test MSE 172.73133902565698 Test RE 0.2212456499365795 Lambda1 0.33791736\n",
      "36 Train Loss 164.49892 Test MSE 161.8539141170507 Test RE 0.21416612251929457 Lambda1 0.3717921\n",
      "37 Train Loss 160.18309 Test MSE 156.78688813560896 Test RE 0.2107871058086529 Lambda1 0.38289383\n",
      "38 Train Loss 152.10579 Test MSE 154.75726186584706 Test RE 0.20941832864198734 Lambda1 0.4096847\n",
      "39 Train Loss 143.89581 Test MSE 143.49935220998117 Test RE 0.20165738953010776 Lambda1 0.45612022\n",
      "40 Train Loss 134.3762 Test MSE 131.95004643213494 Test RE 0.1933721581848277 Lambda1 0.52165806\n",
      "41 Train Loss 125.84307 Test MSE 124.03760665464142 Test RE 0.18748471180326723 Lambda1 0.55892336\n",
      "42 Train Loss 119.17557 Test MSE 121.02851444525402 Test RE 0.18519660540382618 Lambda1 0.5962912\n",
      "43 Train Loss 113.86912 Test MSE 114.22402328417138 Test RE 0.17991521709551084 Lambda1 0.6240816\n",
      "44 Train Loss 109.56692 Test MSE 109.98916435308675 Test RE 0.1765485365201174 Lambda1 0.6461851\n",
      "45 Train Loss 104.99527 Test MSE 103.96287245308775 Test RE 0.17164387314226426 Lambda1 0.6792235\n",
      "46 Train Loss 100.16496 Test MSE 95.18676401681061 Test RE 0.16423943864907276 Lambda1 0.72197086\n",
      "47 Train Loss 97.18484 Test MSE 92.97764391333978 Test RE 0.16232239392660452 Lambda1 0.7289666\n",
      "48 Train Loss 94.726 Test MSE 91.59093935793041 Test RE 0.16110737710634088 Lambda1 0.73863965\n",
      "49 Train Loss 90.74695 Test MSE 88.21467588413381 Test RE 0.1581100922371899 Lambda1 0.7757969\n",
      "50 Train Loss 87.39435 Test MSE 84.64732474941266 Test RE 0.1548801601715499 Lambda1 0.7918636\n",
      "51 Train Loss 84.67978 Test MSE 81.21253135360978 Test RE 0.151705279292981 Lambda1 0.8138324\n",
      "52 Train Loss 81.90704 Test MSE 80.94013147385787 Test RE 0.15145064364970232 Lambda1 0.83301854\n",
      "53 Train Loss 80.45331 Test MSE 78.77463691464844 Test RE 0.1494109323017419 Lambda1 0.86032504\n",
      "54 Train Loss 79.03616 Test MSE 74.7150659497462 Test RE 0.1455101413854793 Lambda1 0.9002678\n",
      "55 Train Loss 77.18338 Test MSE 72.11254233523461 Test RE 0.1429534279888224 Lambda1 0.92864096\n",
      "56 Train Loss 74.24116 Test MSE 66.67985867254662 Test RE 0.13746321783751736 Lambda1 0.9623276\n",
      "57 Train Loss 71.20833 Test MSE 65.34961008446487 Test RE 0.13608512951525686 Lambda1 0.9757329\n",
      "58 Train Loss 69.47007 Test MSE 64.08215551612675 Test RE 0.1347589835081795 Lambda1 0.99092895\n",
      "59 Train Loss 67.14995 Test MSE 61.3895771730504 Test RE 0.1318974780559309 Lambda1 1.0129707\n",
      "60 Train Loss 64.98983 Test MSE 61.82130830462206 Test RE 0.13236045958633696 Lambda1 1.0010582\n",
      "61 Train Loss 63.91622 Test MSE 60.62760406549809 Test RE 0.1310763603067799 Lambda1 1.0059676\n",
      "62 Train Loss 62.555595 Test MSE 58.597205743900595 Test RE 0.12886281790583212 Lambda1 1.0168284\n",
      "63 Train Loss 61.13436 Test MSE 56.852117658282296 Test RE 0.12692947791286366 Lambda1 1.0361756\n",
      "64 Train Loss 60.30455 Test MSE 56.18182595134113 Test RE 0.12617900419403383 Lambda1 1.0477059\n",
      "65 Train Loss 59.127472 Test MSE 57.045450708478796 Test RE 0.12714511487547972 Lambda1 1.0434679\n",
      "66 Train Loss 57.770103 Test MSE 56.33679598307381 Test RE 0.126352908283758 Lambda1 1.0661905\n",
      "67 Train Loss 56.686607 Test MSE 56.89527074569271 Test RE 0.1269776411131333 Lambda1 1.0900189\n",
      "68 Train Loss 54.809593 Test MSE 56.07704413395163 Test RE 0.12606128431325633 Lambda1 1.1129262\n",
      "69 Train Loss 52.380516 Test MSE 53.66335391739219 Test RE 0.12331845549472034 Lambda1 1.1256019\n",
      "70 Train Loss 50.752247 Test MSE 52.568377916931105 Test RE 0.12205384316930071 Lambda1 1.1315593\n",
      "71 Train Loss 49.959595 Test MSE 51.22337253722921 Test RE 0.12048230152875669 Lambda1 1.1341438\n",
      "72 Train Loss 48.5279 Test MSE 49.259140593426324 Test RE 0.11814968985150999 Lambda1 1.1516521\n",
      "73 Train Loss 47.74765 Test MSE 49.126776810730085 Test RE 0.11799084359981278 Lambda1 1.1548282\n",
      "74 Train Loss 47.357273 Test MSE 48.954896781049555 Test RE 0.11778425525044497 Lambda1 1.1612744\n",
      "Training time: 145.39\n",
      "Training time: 145.39\n",
      "inv_HT_stan_tune5\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 837.87286 Test MSE 857.8822458030706 Test RE 0.4930636776024738 Lambda1 -0.18637782\n",
      "1 Train Loss 837.87146 Test MSE 857.8535764056545 Test RE 0.49305543873478735 Lambda1 -0.18617015\n",
      "2 Train Loss 837.86237 Test MSE 857.7662291494794 Test RE 0.4930303364695318 Lambda1 -0.18379532\n",
      "3 Train Loss 837.48505 Test MSE 856.6301787380077 Test RE 0.49270373647289484 Lambda1 -0.11112164\n",
      "4 Train Loss 836.09015 Test MSE 851.5079652346382 Test RE 0.4912284687447367 Lambda1 -0.04078897\n",
      "5 Train Loss 824.68646 Test MSE 838.8033629717509 Test RE 0.4875501025558023 Lambda1 -0.15474522\n",
      "6 Train Loss 778.80756 Test MSE 783.2156210495049 Test RE 0.4711181555407276 Lambda1 -0.033671066\n",
      "7 Train Loss 707.7932 Test MSE 706.535558004229 Test RE 0.4474620236748081 Lambda1 -0.0037704152\n",
      "8 Train Loss 657.714 Test MSE 659.0688312069341 Test RE 0.4321699415007991 Lambda1 -0.0006316232\n",
      "9 Train Loss 642.47034 Test MSE 650.5015807757404 Test RE 0.4293518601160543 Lambda1 -0.0012825418\n",
      "10 Train Loss 628.7176 Test MSE 630.472649607924 Test RE 0.4226903142366364 Lambda1 0.0013525034\n",
      "11 Train Loss 599.8843 Test MSE 590.958122741084 Test RE 0.4092300567231496 Lambda1 0.001195339\n",
      "12 Train Loss 482.52286 Test MSE 442.3944376262729 Test RE 0.35407396924074275 Lambda1 -0.00029159177\n",
      "13 Train Loss 383.01047 Test MSE 356.4830499870928 Test RE 0.31784004466746474 Lambda1 4.712944e-05\n",
      "14 Train Loss 319.2564 Test MSE 322.3360088078383 Test RE 0.30223418353098874 Lambda1 4.2442858e-05\n",
      "15 Train Loss 289.85342 Test MSE 305.4018304979611 Test RE 0.29418802353530377 Lambda1 0.0005298254\n",
      "16 Train Loss 278.8462 Test MSE 300.04423174494156 Test RE 0.29159616747458084 Lambda1 0.0016736323\n",
      "17 Train Loss 273.38416 Test MSE 298.3387626545747 Test RE 0.29076626158783 Lambda1 0.0020612136\n",
      "18 Train Loss 267.54788 Test MSE 289.5498385578264 Test RE 0.2864513241720245 Lambda1 0.0020674434\n",
      "19 Train Loss 259.9625 Test MSE 284.6987199426662 Test RE 0.28404158529696794 Lambda1 0.0011188119\n",
      "20 Train Loss 257.46548 Test MSE 284.488942808276 Test RE 0.283936919547001 Lambda1 0.00090366515\n",
      "21 Train Loss 255.97026 Test MSE 282.8917465221609 Test RE 0.283138749144511 Lambda1 0.00042554893\n",
      "22 Train Loss 255.64258 Test MSE 283.1122139276993 Test RE 0.2832490576085371 Lambda1 0.0002745997\n",
      "23 Train Loss 255.2954 Test MSE 283.322630859985 Test RE 0.283354297377055 Lambda1 0.00033699052\n",
      "24 Train Loss 255.20901 Test MSE 283.1245841270454 Test RE 0.2832552456302149 Lambda1 0.00026498994\n",
      "25 Train Loss 255.01717 Test MSE 283.135809247572 Test RE 0.2832608607250098 Lambda1 0.0002806218\n",
      "26 Train Loss 254.76758 Test MSE 282.9250083184725 Test RE 0.2831553940744281 Lambda1 0.00022626137\n",
      "27 Train Loss 254.53615 Test MSE 282.56564986606867 Test RE 0.2829755113976381 Lambda1 0.00029398705\n",
      "28 Train Loss 254.15364 Test MSE 282.61628390123565 Test RE 0.28300086400185365 Lambda1 0.00038187276\n",
      "29 Train Loss 254.02007 Test MSE 282.60515017763794 Test RE 0.2829952895098482 Lambda1 0.00037871825\n",
      "30 Train Loss 253.8766 Test MSE 282.8806587512504 Test RE 0.2831332003641257 Lambda1 0.00030845107\n",
      "31 Train Loss 253.71855 Test MSE 283.39198141924544 Test RE 0.28338897441038985 Lambda1 0.00029485996\n",
      "32 Train Loss 253.65025 Test MSE 283.4479250029603 Test RE 0.28341694452513827 Lambda1 0.00033439996\n",
      "33 Train Loss 253.52687 Test MSE 283.26805507831403 Test RE 0.2833270051219214 Lambda1 0.0002835324\n",
      "34 Train Loss 253.3771 Test MSE 282.98358363127716 Test RE 0.2831847040627928 Lambda1 0.00027257236\n",
      "35 Train Loss 253.23828 Test MSE 282.53531752883515 Test RE 0.282960322822871 Lambda1 0.0002545858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 Train Loss 253.09691 Test MSE 283.1497105535225 Test RE 0.28326781436250176 Lambda1 0.0002424319\n",
      "37 Train Loss 252.72795 Test MSE 284.22149375023565 Test RE 0.28380342311499385 Lambda1 0.00025418648\n",
      "38 Train Loss 252.29391 Test MSE 285.2183678754827 Test RE 0.2843006913642327 Lambda1 0.00026369805\n",
      "39 Train Loss 251.87354 Test MSE 285.75181310747365 Test RE 0.2845664316171555 Lambda1 0.00018130172\n",
      "40 Train Loss 251.6407 Test MSE 286.6022934193587 Test RE 0.2849895931245348 Lambda1 0.0001240373\n",
      "41 Train Loss 251.44403 Test MSE 287.1495801102846 Test RE 0.2852615669129989 Lambda1 0.00010423866\n",
      "42 Train Loss 251.29187 Test MSE 287.5693543603358 Test RE 0.2854699978800115 Lambda1 8.768428e-05\n",
      "43 Train Loss 251.18405 Test MSE 287.4560964916089 Test RE 0.2854137768204526 Lambda1 7.342676e-05\n",
      "44 Train Loss 251.13626 Test MSE 287.3698010617144 Test RE 0.2853709324458772 Lambda1 6.497931e-05\n",
      "45 Train Loss 250.97552 Test MSE 287.2082969324214 Test RE 0.2852907308012393 Lambda1 7.348735e-05\n",
      "46 Train Loss 250.83818 Test MSE 286.7006691038344 Test RE 0.2850384999922027 Lambda1 6.40102e-05\n",
      "47 Train Loss 250.7785 Test MSE 287.07620306158674 Test RE 0.28522511728760264 Lambda1 7.2426716e-05\n",
      "48 Train Loss 250.54715 Test MSE 287.14668906397617 Test RE 0.2852601308905686 Lambda1 8.410524e-05\n",
      "49 Train Loss 250.25008 Test MSE 286.17574770500346 Test RE 0.28477744138075745 Lambda1 8.808224e-05\n",
      "50 Train Loss 250.17715 Test MSE 286.49556709382574 Test RE 0.2849365252933995 Lambda1 9.529254e-05\n",
      "51 Train Loss 250.06168 Test MSE 286.8015622202411 Test RE 0.28508864967094927 Lambda1 8.636569e-05\n",
      "52 Train Loss 249.841 Test MSE 286.93709057136556 Test RE 0.2851560011714698 Lambda1 5.2975764e-05\n",
      "53 Train Loss 249.67764 Test MSE 287.0590865266525 Test RE 0.2852166140776432 Lambda1 6.0400813e-05\n",
      "54 Train Loss 249.50693 Test MSE 287.66483823028983 Test RE 0.28551738734878296 Lambda1 5.6520526e-05\n",
      "55 Train Loss 249.43962 Test MSE 287.87912288944494 Test RE 0.285623710052082 Lambda1 4.7127614e-05\n",
      "56 Train Loss 249.35355 Test MSE 287.8615703482889 Test RE 0.2856150024071904 Lambda1 4.1005485e-05\n",
      "57 Train Loss 249.24081 Test MSE 287.56484517938907 Test RE 0.2854677597400604 Lambda1 3.6297843e-05\n",
      "58 Train Loss 249.0017 Test MSE 289.11649710094434 Test RE 0.28623689180802453 Lambda1 2.9732455e-05\n",
      "59 Train Loss 248.82602 Test MSE 289.849698650296 Test RE 0.2865996114133888 Lambda1 2.0979021e-05\n",
      "60 Train Loss 248.7565 Test MSE 290.00676132589643 Test RE 0.2866772516621753 Lambda1 2.3089444e-05\n",
      "61 Train Loss 248.72269 Test MSE 290.0748734209313 Test RE 0.286710914742574 Lambda1 2.1353828e-05\n",
      "62 Train Loss 248.68181 Test MSE 289.7069495442859 Test RE 0.2865290284924 Lambda1 1.7662278e-05\n",
      "63 Train Loss 248.5182 Test MSE 290.0035468698559 Test RE 0.28667566288201607 Lambda1 1.3244938e-05\n",
      "64 Train Loss 248.46344 Test MSE 290.302945362572 Test RE 0.2868236061072202 Lambda1 9.1862785e-06\n",
      "65 Train Loss 248.37462 Test MSE 289.6404166887786 Test RE 0.2864961250892837 Lambda1 6.6776015e-06\n",
      "66 Train Loss 248.32993 Test MSE 289.69178349183204 Test RE 0.2865215285495171 Lambda1 4.8274323e-06\n",
      "67 Train Loss 248.30873 Test MSE 289.7053691593842 Test RE 0.2865282469668466 Lambda1 1.2679297e-05\n",
      "68 Train Loss 248.26093 Test MSE 289.58468548327113 Test RE 0.28646856066518983 Lambda1 1.5567151e-05\n",
      "69 Train Loss 248.1443 Test MSE 289.89586763103773 Test RE 0.2866224361482861 Lambda1 1.1040693e-05\n",
      "70 Train Loss 248.09186 Test MSE 289.85157693476776 Test RE 0.2866005400235101 Lambda1 7.1647014e-06\n",
      "71 Train Loss 248.04944 Test MSE 290.2581593268366 Test RE 0.2868014806208041 Lambda1 3.5707285e-06\n",
      "72 Train Loss 248.01418 Test MSE 289.8289040588391 Test RE 0.2865893305182314 Lambda1 3.8724966e-06\n",
      "73 Train Loss 247.9474 Test MSE 289.5902441189623 Test RE 0.28647131006252347 Lambda1 4.7350404e-06\n",
      "74 Train Loss 247.78465 Test MSE 289.87389348314036 Test RE 0.28661157293176065 Lambda1 6.801456e-07\n",
      "Training time: 147.86\n",
      "Training time: 147.86\n",
      "inv_HT_stan_tune5\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 838.1316 Test MSE 858.2770381852761 Test RE 0.4931771170604358 Lambda1 -0.022307225\n",
      "1 Train Loss 838.07446 Test MSE 858.4305817508127 Test RE 0.4932212291496643 Lambda1 -0.022362296\n",
      "2 Train Loss 837.8963 Test MSE 857.9582165365077 Test RE 0.4930855090252276 Lambda1 -0.02359298\n",
      "3 Train Loss 837.8947 Test MSE 857.9176078666353 Test RE 0.4930738395853808 Lambda1 -0.024124585\n",
      "4 Train Loss 837.7923 Test MSE 857.5134570377926 Test RE 0.4929576864294229 Lambda1 -0.088934705\n",
      "5 Train Loss 837.7426 Test MSE 857.7373196158063 Test RE 0.49302202803028167 Lambda1 -0.10289164\n",
      "6 Train Loss 837.5093 Test MSE 857.0762431735525 Test RE 0.49283200011358463 Lambda1 -0.229563\n",
      "7 Train Loss 836.6834 Test MSE 854.8971449595798 Test RE 0.4922050939196558 Lambda1 -0.3483483\n",
      "8 Train Loss 830.93616 Test MSE 848.1289887735959 Test RE 0.4902528470003529 Lambda1 -0.32985345\n",
      "9 Train Loss 816.64185 Test MSE 827.3053710028161 Test RE 0.4841969980540099 Lambda1 -0.24910638\n",
      "10 Train Loss 781.1054 Test MSE 774.6971302736673 Test RE 0.4685491390777638 Lambda1 -0.23096779\n",
      "11 Train Loss 752.94403 Test MSE 737.1688815809081 Test RE 0.4570594235639753 Lambda1 -0.15985568\n",
      "12 Train Loss 695.4652 Test MSE 677.5719453982781 Test RE 0.43819445598096024 Lambda1 -0.06103045\n",
      "13 Train Loss 606.0723 Test MSE 562.4771694593009 Test RE 0.39924696220065026 Lambda1 -0.009175226\n",
      "14 Train Loss 445.29886 Test MSE 454.0206831551866 Test RE 0.35869637652299236 Lambda1 -0.0026240603\n",
      "15 Train Loss 313.41754 Test MSE 328.6493299688229 Test RE 0.305179632871473 Lambda1 0.00069684465\n",
      "16 Train Loss 270.0153 Test MSE 288.20986924255794 Test RE 0.28578774049837724 Lambda1 0.00076986314\n",
      "17 Train Loss 260.5971 Test MSE 280.55405562890155 Test RE 0.28196645628828204 Lambda1 5.448081e-05\n",
      "18 Train Loss 258.71347 Test MSE 280.83821897428425 Test RE 0.28210921710626913 Lambda1 3.8032107e-05\n",
      "19 Train Loss 257.8978 Test MSE 280.39899040603774 Test RE 0.2818885225821993 Lambda1 8.158982e-05\n",
      "20 Train Loss 257.15097 Test MSE 280.4732664913543 Test RE 0.28192585543697424 Lambda1 0.00010405791\n",
      "21 Train Loss 256.38348 Test MSE 281.4108913071945 Test RE 0.2823967026708991 Lambda1 1.5553758e-05\n",
      "22 Train Loss 256.30423 Test MSE 281.6678406624956 Test RE 0.2825255979925176 Lambda1 2.0700216e-05\n",
      "23 Train Loss 256.22913 Test MSE 281.66638890207895 Test RE 0.28252486990086284 Lambda1 2.5999447e-05\n",
      "24 Train Loss 256.03735 Test MSE 281.8480239671072 Test RE 0.2826159495519004 Lambda1 1.9853698e-05\n",
      "25 Train Loss 256.01343 Test MSE 281.78398264640794 Test RE 0.28258383982368884 Lambda1 1.4473355e-05\n",
      "26 Train Loss 255.78897 Test MSE 282.2642083627166 Test RE 0.2828245317484019 Lambda1 4.4451467e-06\n",
      "27 Train Loss 255.56705 Test MSE 282.40872998517335 Test RE 0.2828969267392238 Lambda1 8.229194e-06\n",
      "28 Train Loss 255.1759 Test MSE 283.6339914931852 Test RE 0.28350995234083876 Lambda1 1.3171383e-05\n",
      "29 Train Loss 255.09445 Test MSE 284.27855776089876 Test RE 0.28383191172180694 Lambda1 1.7752373e-05\n",
      "30 Train Loss 254.96877 Test MSE 283.7655933484116 Test RE 0.2835757168647627 Lambda1 6.311569e-06\n",
      "31 Train Loss 254.8913 Test MSE 283.71772769760287 Test RE 0.28355179904490607 Lambda1 -6.5123945e-06\n",
      "32 Train Loss 254.72537 Test MSE 283.231662047065 Test RE 0.2833088042348647 Lambda1 9.577584e-06\n",
      "33 Train Loss 254.60667 Test MSE 282.8815777760201 Test RE 0.2831336602863666 Lambda1 -4.7148555e-06\n",
      "34 Train Loss 254.59299 Test MSE 282.9036441760538 Test RE 0.283144703102984 Lambda1 3.1579855e-06\n",
      "35 Train Loss 254.55447 Test MSE 283.19686082047673 Test RE 0.28329139834757483 Lambda1 -4.2161428e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 Train Loss 254.4878 Test MSE 283.17427259203623 Test RE 0.2832801002378301 Lambda1 -8.584816e-06\n",
      "37 Train Loss 254.479 Test MSE 283.05363872344685 Test RE 0.2832197543343325 Lambda1 -1.9927934e-06\n",
      "38 Train Loss 254.44276 Test MSE 282.9975882036005 Test RE 0.2831917112388847 Lambda1 -8.441334e-06\n",
      "39 Train Loss 254.36264 Test MSE 283.0690692620765 Test RE 0.2832274740262979 Lambda1 1.8545292e-05\n",
      "40 Train Loss 254.35187 Test MSE 283.1817953337692 Test RE 0.283283862989403 Lambda1 2.2028566e-05\n",
      "41 Train Loss 254.34329 Test MSE 283.11228580470646 Test RE 0.28324909356440936 Lambda1 2.5775691e-05\n",
      "42 Train Loss 254.29448 Test MSE 282.8801891793849 Test RE 0.2831329653684904 Lambda1 2.574106e-05\n",
      "43 Train Loss 254.27426 Test MSE 283.0714642157982 Test RE 0.28322867217072967 Lambda1 1.7974773e-05\n",
      "44 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "45 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "46 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "47 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "48 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "49 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "50 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "51 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "52 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "53 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "54 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "55 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "56 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "57 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "58 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "59 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "60 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "61 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "62 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "63 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "64 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "65 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "66 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "67 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "68 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "69 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "70 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "71 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "72 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "73 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "74 Train Loss 254.2734 Test MSE 283.06791711853566 Test RE 0.28322689763157255 Lambda1 1.9002868e-05\n",
      "Training time: 144.96\n",
      "Training time: 144.96\n",
      "inv_HT_stan_tune5\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 840.1045 Test MSE 861.6653105709221 Test RE 0.49414963082044 Lambda1 -0.010980072\n",
      "1 Train Loss 837.90765 Test MSE 858.0168981945496 Test RE 0.4931023714900413 Lambda1 -0.011002885\n",
      "2 Train Loss 837.8612 Test MSE 857.7544604505113 Test RE 0.49302695422777576 Lambda1 -0.013784369\n",
      "3 Train Loss 837.6925 Test MSE 857.9683778401923 Test RE 0.49308842896740424 Lambda1 -0.041330885\n",
      "4 Train Loss 831.8662 Test MSE 841.2860080799958 Test RE 0.48827108176500406 Lambda1 -0.0778239\n",
      "5 Train Loss 788.93146 Test MSE 777.5724667364346 Test RE 0.46941785834766836 Lambda1 -0.0285531\n",
      "6 Train Loss 712.7444 Test MSE 703.2427314179852 Test RE 0.4464181019889552 Lambda1 -0.0064988513\n",
      "7 Train Loss 658.6492 Test MSE 660.5694324355931 Test RE 0.43266165495421416 Lambda1 0.00031946925\n",
      "8 Train Loss 652.94745 Test MSE 657.3983236024709 Test RE 0.4316218946290937 Lambda1 0.00036806567\n",
      "9 Train Loss 648.2631 Test MSE 654.3049745992014 Test RE 0.4306052115607842 Lambda1 0.0008135392\n",
      "10 Train Loss 644.7757 Test MSE 649.6858811721988 Test RE 0.42908258175301306 Lambda1 -0.00030332807\n",
      "11 Train Loss 638.89075 Test MSE 646.1599436236324 Test RE 0.42791665162800196 Lambda1 0.0003241146\n",
      "12 Train Loss 631.2999 Test MSE 641.8493140688712 Test RE 0.42648691517313575 Lambda1 0.00020274107\n",
      "13 Train Loss 595.3438 Test MSE 609.9410173687202 Test RE 0.41575079709409096 Lambda1 9.349557e-05\n",
      "14 Train Loss 563.678 Test MSE 583.0009081389654 Test RE 0.40646559063164917 Lambda1 -0.00014113735\n",
      "15 Train Loss 469.67822 Test MSE 451.2076153044474 Test RE 0.3575834260370392 Lambda1 0.0011437387\n",
      "16 Train Loss 383.49408 Test MSE 342.8371097718667 Test RE 0.3116973307950923 Lambda1 0.0020096803\n",
      "17 Train Loss 302.17014 Test MSE 305.4515153278735 Test RE 0.29421195280817525 Lambda1 -8.776075e-05\n",
      "18 Train Loss 295.87964 Test MSE 302.8255705000115 Test RE 0.292944563458224 Lambda1 -0.0013848129\n",
      "19 Train Loss 281.47977 Test MSE 292.7219119403858 Test RE 0.288016114487007 Lambda1 0.00035250012\n",
      "20 Train Loss 269.50424 Test MSE 288.6729718534502 Test RE 0.2860172536763761 Lambda1 -0.000375806\n",
      "21 Train Loss 262.65152 Test MSE 285.0776667637005 Test RE 0.28423055850752454 Lambda1 0.0005201167\n",
      "22 Train Loss 261.32855 Test MSE 284.92309402353766 Test RE 0.284153491345768 Lambda1 4.5592154e-05\n",
      "23 Train Loss 260.01257 Test MSE 284.53192093355597 Test RE 0.2839583661022353 Lambda1 -0.00028453022\n",
      "24 Train Loss 259.0054 Test MSE 282.89847939787865 Test RE 0.28314211850167614 Lambda1 -4.6838788e-05\n",
      "25 Train Loss 258.2196 Test MSE 282.2548541631407 Test RE 0.2828198453252643 Lambda1 0.000633358\n",
      "26 Train Loss 257.04474 Test MSE 283.0419294031693 Test RE 0.28321389617768555 Lambda1 0.0003961727\n",
      "27 Train Loss 256.4551 Test MSE 283.0713551733539 Test RE 0.283228617619223 Lambda1 0.00027531738\n",
      "28 Train Loss 256.2178 Test MSE 283.22480604104953 Test RE 0.28330537527744065 Lambda1 0.00011557596\n",
      "29 Train Loss 255.96051 Test MSE 283.1567426723207 Test RE 0.28327133186663545 Lambda1 0.00027515148\n",
      "30 Train Loss 255.52464 Test MSE 283.47530952597947 Test RE 0.28343063495944437 Lambda1 0.0004379408\n",
      "31 Train Loss 255.19916 Test MSE 283.0695291713755 Test RE 0.28322770410953596 Lambda1 0.00068757654\n",
      "32 Train Loss 255.10933 Test MSE 283.282780804284 Test RE 0.28333436942146184 Lambda1 0.00067915063\n",
      "33 Train Loss 254.72675 Test MSE 283.73002299178927 Test RE 0.2835579430300633 Lambda1 0.00079462625\n",
      "34 Train Loss 254.45358 Test MSE 283.4050524021355 Test RE 0.28339550975713107 Lambda1 0.00067444425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 Train Loss 254.32726 Test MSE 282.82311406829524 Test RE 0.28310440087144934 Lambda1 0.00071386754\n",
      "36 Train Loss 254.11092 Test MSE 282.4255254040901 Test RE 0.2829053388406375 Lambda1 0.0008345111\n",
      "37 Train Loss 254.00845 Test MSE 281.97132370578 Test RE 0.28267776063364164 Lambda1 0.00087596587\n",
      "38 Train Loss 253.81139 Test MSE 281.7440965792861 Test RE 0.2825638394731035 Lambda1 0.00073179195\n",
      "39 Train Loss 253.73964 Test MSE 281.76521091928436 Test RE 0.28257442716114445 Lambda1 0.000656694\n",
      "40 Train Loss 253.47606 Test MSE 282.4295168649627 Test RE 0.282907337954562 Lambda1 0.0005137292\n",
      "41 Train Loss 253.43196 Test MSE 282.71300137478954 Test RE 0.28304928440221583 Lambda1 0.00049581606\n",
      "42 Train Loss 253.40686 Test MSE 283.0726678424348 Test RE 0.28322927431763384 Lambda1 0.00045419508\n",
      "43 Train Loss 253.31573 Test MSE 283.38192725015705 Test RE 0.28338394733459893 Lambda1 0.00035165768\n",
      "44 Train Loss 253.17471 Test MSE 283.3661335282817 Test RE 0.2833760503073358 Lambda1 0.00027394795\n",
      "45 Train Loss 252.94281 Test MSE 282.91764333332895 Test RE 0.2831517085592185 Lambda1 0.0002623153\n",
      "46 Train Loss 252.77011 Test MSE 282.8063012721349 Test RE 0.28309598598757474 Lambda1 0.00014818495\n",
      "47 Train Loss 252.67738 Test MSE 283.2779175252991 Test RE 0.28333193732870526 Lambda1 0.00011746963\n",
      "48 Train Loss 252.57962 Test MSE 283.8657368332118 Test RE 0.2836257506886254 Lambda1 8.629555e-05\n",
      "49 Train Loss 252.46025 Test MSE 283.7094148157779 Test RE 0.283547645004406 Lambda1 8.643256e-05\n",
      "50 Train Loss 252.33357 Test MSE 283.83487523465266 Test RE 0.28361033251579904 Lambda1 9.051544e-05\n",
      "51 Train Loss 252.1894 Test MSE 283.97889405012063 Test RE 0.2836822758317341 Lambda1 8.91223e-05\n",
      "52 Train Loss 252.01997 Test MSE 283.1190359693782 Test RE 0.28325247025755207 Lambda1 7.161165e-05\n",
      "53 Train Loss 251.91464 Test MSE 283.5663702259742 Test RE 0.2834761544787797 Lambda1 6.427715e-05\n",
      "54 Train Loss 251.74565 Test MSE 283.4561672049034 Test RE 0.28342106514572407 Lambda1 4.9196766e-05\n",
      "55 Train Loss 251.62648 Test MSE 283.7399452567143 Test RE 0.2835629011102926 Lambda1 2.9547458e-05\n",
      "56 Train Loss 251.5697 Test MSE 283.76163181810307 Test RE 0.283573737418092 Lambda1 3.174916e-05\n",
      "57 Train Loss 251.53046 Test MSE 283.9164312514905 Test RE 0.2836510753380879 Lambda1 2.8753546e-05\n",
      "58 Train Loss 251.31845 Test MSE 283.7319999880481 Test RE 0.2835589309269558 Lambda1 2.7807522e-05\n",
      "59 Train Loss 251.06367 Test MSE 284.58810893984685 Test RE 0.2839864020899099 Lambda1 2.1265832e-05\n",
      "60 Train Loss 250.91092 Test MSE 284.6958146609359 Test RE 0.28404013600535566 Lambda1 1.5027017e-05\n",
      "61 Train Loss 250.86623 Test MSE 284.782130024864 Test RE 0.2840831910283536 Lambda1 1.9553065e-05\n",
      "62 Train Loss 250.81062 Test MSE 284.6073067302027 Test RE 0.283995980528536 Lambda1 1.3695093e-05\n",
      "63 Train Loss 250.76576 Test MSE 284.6919659587012 Test RE 0.2840382160796937 Lambda1 1.6599843e-05\n",
      "64 Train Loss 250.73856 Test MSE 284.7210749417777 Test RE 0.284052736777988 Lambda1 1.2104514e-05\n",
      "65 Train Loss 250.68544 Test MSE 285.10909596677556 Test RE 0.28424622598131083 Lambda1 1.35051705e-05\n",
      "66 Train Loss 250.50284 Test MSE 285.2387637129103 Test RE 0.2843108562899099 Lambda1 1.17286545e-05\n",
      "67 Train Loss 250.26416 Test MSE 284.8403804738512 Test RE 0.28411224328545176 Lambda1 9.0529e-06\n",
      "68 Train Loss 250.12566 Test MSE 284.65104278837305 Test RE 0.2840178007476308 Lambda1 7.663116e-06\n",
      "69 Train Loss 250.02197 Test MSE 284.6486979785255 Test RE 0.2840166309484602 Lambda1 4.2627235e-06\n",
      "70 Train Loss 249.97493 Test MSE 284.9372652194154 Test RE 0.2841605577170114 Lambda1 4.9797654e-06\n",
      "71 Train Loss 249.95338 Test MSE 284.7734885807785 Test RE 0.2840788808779348 Lambda1 6.7137694e-06\n",
      "72 Train Loss 249.94283 Test MSE 284.64957770402356 Test RE 0.28401706983414765 Lambda1 5.99219e-06\n",
      "73 Train Loss 249.93356 Test MSE 284.67775089806867 Test RE 0.28403112478217124 Lambda1 5.271018e-06\n",
      "74 Train Loss 249.92195 Test MSE 284.64916647514764 Test RE 0.2840168646765228 Lambda1 5.4085485e-06\n",
      "Training time: 160.06\n",
      "Training time: 160.06\n",
      "inv_HT_stan_tune6\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.25464 Test MSE 858.3865655239393 Test RE 0.4932085839711941 Lambda1 -0.0052111484\n",
      "1 Train Loss 837.2826 Test MSE 856.4698338473275 Test RE 0.4926576219266108 Lambda1 -0.0075303963\n",
      "2 Train Loss 833.92487 Test MSE 853.1590438013768 Test RE 0.49170448544043455 Lambda1 0.019719416\n",
      "3 Train Loss 785.73206 Test MSE 785.2582104633652 Test RE 0.47173208249727233 Lambda1 0.17679136\n",
      "4 Train Loss 724.6818 Test MSE 721.1325381238792 Test RE 0.45206066181870797 Lambda1 0.21361509\n",
      "5 Train Loss 682.9987 Test MSE 684.2642767894403 Test RE 0.4403531469833267 Lambda1 0.1991919\n",
      "6 Train Loss 644.15704 Test MSE 643.7176947384999 Test RE 0.4271072016221386 Lambda1 0.18966964\n",
      "7 Train Loss 601.5504 Test MSE 595.2582812951883 Test RE 0.410716257186886 Lambda1 0.17677376\n",
      "8 Train Loss 557.27374 Test MSE 541.7587595178031 Test RE 0.39182499943405036 Lambda1 0.17868505\n",
      "9 Train Loss 500.13824 Test MSE 509.6176821358161 Test RE 0.3800243411588869 Lambda1 0.16602272\n",
      "10 Train Loss 393.62836 Test MSE 404.85740881181874 Test RE 0.3387195129730989 Lambda1 0.1594772\n",
      "11 Train Loss 323.01346 Test MSE 340.5779507253678 Test RE 0.3106686531544615 Lambda1 0.14420867\n",
      "12 Train Loss 301.1738 Test MSE 321.63375934760757 Test RE 0.3019047764643288 Lambda1 0.13912903\n",
      "13 Train Loss 277.38837 Test MSE 291.97509314744576 Test RE 0.2876484733803709 Lambda1 0.14446265\n",
      "14 Train Loss 263.02936 Test MSE 286.73685344845273 Test RE 0.28505648670625017 Lambda1 0.14421652\n",
      "15 Train Loss 252.29697 Test MSE 274.2116886098794 Test RE 0.2787610888525659 Lambda1 0.15035081\n",
      "16 Train Loss 246.11858 Test MSE 267.5813930333035 Test RE 0.27537031781208826 Lambda1 0.1613894\n",
      "17 Train Loss 238.42162 Test MSE 257.0615829103244 Test RE 0.2699030295550866 Lambda1 0.18638642\n",
      "18 Train Loss 228.76688 Test MSE 245.72842085957444 Test RE 0.2638863131562193 Lambda1 0.22488648\n",
      "19 Train Loss 222.36237 Test MSE 238.76114421530022 Test RE 0.2601183535219462 Lambda1 0.23837434\n",
      "20 Train Loss 210.68767 Test MSE 224.14392656936255 Test RE 0.2520302431756732 Lambda1 0.271708\n",
      "21 Train Loss 199.56651 Test MSE 213.01175891497735 Test RE 0.24569196847845826 Lambda1 0.29211327\n",
      "22 Train Loss 191.2922 Test MSE 203.03305335575888 Test RE 0.23986812678258684 Lambda1 0.30396876\n",
      "23 Train Loss 183.83014 Test MSE 193.40091127752996 Test RE 0.2341091718512795 Lambda1 0.32251042\n",
      "24 Train Loss 177.08965 Test MSE 185.1958053619861 Test RE 0.2290892674638462 Lambda1 0.34461662\n",
      "25 Train Loss 170.13394 Test MSE 175.96866781045873 Test RE 0.2233093172383111 Lambda1 0.37016758\n",
      "26 Train Loss 163.32092 Test MSE 166.61386380156017 Test RE 0.21729250097789546 Lambda1 0.3955507\n",
      "27 Train Loss 157.7471 Test MSE 159.90872920982687 Test RE 0.2128752919626393 Lambda1 0.41406974\n",
      "28 Train Loss 151.2154 Test MSE 153.79313880182758 Test RE 0.20876498131626717 Lambda1 0.43132776\n",
      "29 Train Loss 145.46437 Test MSE 149.43695697194042 Test RE 0.20578711480994824 Lambda1 0.4450627\n",
      "30 Train Loss 141.69817 Test MSE 146.31821457517705 Test RE 0.20362840860381035 Lambda1 0.45566478\n",
      "31 Train Loss 137.56134 Test MSE 141.56374272685284 Test RE 0.20029273123642785 Lambda1 0.47210705\n",
      "32 Train Loss 133.68484 Test MSE 136.4223523291374 Test RE 0.19662192221199123 Lambda1 0.48999682\n",
      "33 Train Loss 130.35292 Test MSE 131.71346556049903 Test RE 0.1931987263464054 Lambda1 0.5054473\n",
      "34 Train Loss 125.51718 Test MSE 127.79244623217853 Test RE 0.19030130321727148 Lambda1 0.51658005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 Train Loss 122.1181 Test MSE 122.81642881308983 Test RE 0.18655951462178977 Lambda1 0.53359306\n",
      "36 Train Loss 117.319374 Test MSE 118.35889099234355 Test RE 0.18314270072327332 Lambda1 0.5511506\n",
      "37 Train Loss 112.63922 Test MSE 111.43472997081521 Test RE 0.17770492046779046 Lambda1 0.57898146\n",
      "38 Train Loss 109.936905 Test MSE 111.35008502736474 Test RE 0.17763741601722222 Lambda1 0.5796982\n",
      "39 Train Loss 105.95734 Test MSE 108.08245784875318 Test RE 0.17501157641725593 Lambda1 0.59393895\n",
      "40 Train Loss 102.01468 Test MSE 103.38308666766603 Test RE 0.17116458755119823 Lambda1 0.6113561\n",
      "41 Train Loss 98.440506 Test MSE 99.27536304721269 Test RE 0.16772967775393968 Lambda1 0.6293179\n",
      "42 Train Loss 95.98422 Test MSE 95.05716902418865 Test RE 0.16412759611169545 Lambda1 0.64796406\n",
      "43 Train Loss 93.68784 Test MSE 92.33762800670743 Test RE 0.1617627523356041 Lambda1 0.66163313\n",
      "44 Train Loss 90.79931 Test MSE 89.38483971225185 Test RE 0.15915529923863528 Lambda1 0.677713\n",
      "45 Train Loss 87.42562 Test MSE 86.30774391318583 Test RE 0.15639182685113642 Lambda1 0.6966857\n",
      "46 Train Loss 83.60634 Test MSE 82.94809756952003 Test RE 0.15331773177457944 Lambda1 0.7162403\n",
      "47 Train Loss 80.20065 Test MSE 78.74627499625632 Test RE 0.14938403302091519 Lambda1 0.73513603\n",
      "48 Train Loss 77.425804 Test MSE 74.41948715703677 Test RE 0.14522203126212427 Lambda1 0.7535177\n",
      "49 Train Loss 73.676056 Test MSE 69.883314771101 Test RE 0.14072651052258822 Lambda1 0.7760941\n",
      "50 Train Loss 69.37486 Test MSE 66.54029627585163 Test RE 0.13731928573015523 Lambda1 0.7939276\n",
      "51 Train Loss 66.777405 Test MSE 64.65067547992136 Test RE 0.1353554365960339 Lambda1 0.8053206\n",
      "52 Train Loss 64.447624 Test MSE 62.7398194573772 Test RE 0.13334010812028113 Lambda1 0.8282363\n",
      "53 Train Loss 62.57536 Test MSE 61.235212357340494 Test RE 0.13173154479803664 Lambda1 0.8387077\n",
      "54 Train Loss 60.40622 Test MSE 59.21692719050101 Test RE 0.12954244941452883 Lambda1 0.84849185\n",
      "55 Train Loss 58.18666 Test MSE 56.57472401321102 Test RE 0.12661944122749763 Lambda1 0.8636638\n",
      "56 Train Loss 55.293438 Test MSE 53.94824105886323 Test RE 0.12364535771228306 Lambda1 0.8821896\n",
      "57 Train Loss 53.759644 Test MSE 53.54333882964028 Test RE 0.12318048089634223 Lambda1 0.88959205\n",
      "58 Train Loss 52.441998 Test MSE 53.3542290282783 Test RE 0.12296275781651948 Lambda1 0.8956619\n",
      "59 Train Loss 50.38326 Test MSE 51.199196785695754 Test RE 0.12045386632619773 Lambda1 0.91874355\n",
      "60 Train Loss 49.192566 Test MSE 49.61993800357676 Test RE 0.1185815927331513 Lambda1 0.92303914\n",
      "61 Train Loss 47.79441 Test MSE 47.17823715540544 Test RE 0.11562720431666973 Lambda1 0.9373173\n",
      "62 Train Loss 46.60413 Test MSE 45.50642286768621 Test RE 0.11356003552656414 Lambda1 0.9470493\n",
      "63 Train Loss 44.927658 Test MSE 43.71081750199077 Test RE 0.11129704535659622 Lambda1 0.96435285\n",
      "64 Train Loss 43.403374 Test MSE 42.03386172836065 Test RE 0.10914122305918589 Lambda1 0.9807632\n",
      "65 Train Loss 42.740524 Test MSE 41.290319665608685 Test RE 0.10817160988020169 Lambda1 0.9880951\n",
      "66 Train Loss 42.27171 Test MSE 40.66271733325957 Test RE 0.10734637157332479 Lambda1 0.99331164\n",
      "67 Train Loss 41.702023 Test MSE 39.81596520745861 Test RE 0.10622281214321094 Lambda1 0.99837303\n",
      "68 Train Loss 40.683266 Test MSE 38.2285360031802 Test RE 0.1040837675087972 Lambda1 1.0079526\n",
      "69 Train Loss 40.199207 Test MSE 38.05749023431943 Test RE 0.10385065569432551 Lambda1 1.0098938\n",
      "70 Train Loss 39.587597 Test MSE 37.548004703641396 Test RE 0.10315317560859097 Lambda1 1.015491\n",
      "71 Train Loss 39.04326 Test MSE 37.518384631214246 Test RE 0.10311248093832578 Lambda1 1.0151947\n",
      "72 Train Loss 38.35806 Test MSE 37.52499784548099 Test RE 0.10312156814837242 Lambda1 1.0158855\n",
      "73 Train Loss 37.933167 Test MSE 37.39539475921282 Test RE 0.10294333451659196 Lambda1 1.017311\n",
      "74 Train Loss 37.41672 Test MSE 37.18769196957376 Test RE 0.10265705073405557 Lambda1 1.0196373\n",
      "Training time: 144.79\n",
      "Training time: 144.79\n",
      "inv_HT_stan_tune6\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 837.9207 Test MSE 857.4549119248608 Test RE 0.4929408582643384 Lambda1 -0.076346174\n",
      "1 Train Loss 835.63403 Test MSE 855.1303687975188 Test RE 0.4922722283893334 Lambda1 -0.09206225\n",
      "2 Train Loss 795.29175 Test MSE 790.2977761588808 Test RE 0.4732433833211843 Lambda1 -0.09530166\n",
      "3 Train Loss 440.04807 Test MSE 444.0094322468754 Test RE 0.35471966749371764 Lambda1 -0.0011119138\n",
      "4 Train Loss 362.7216 Test MSE 366.25125177892704 Test RE 0.3221652763863293 Lambda1 0.0004638565\n",
      "5 Train Loss 280.09177 Test MSE 294.2610769585198 Test RE 0.2887723324210551 Lambda1 -0.0008480157\n",
      "6 Train Loss 269.43216 Test MSE 289.4636475044207 Test RE 0.2864086866434969 Lambda1 -0.00015290114\n",
      "7 Train Loss 265.03976 Test MSE 286.44319508795957 Test RE 0.28491048059825264 Lambda1 0.00042154474\n",
      "8 Train Loss 260.42267 Test MSE 282.80238909258446 Test RE 0.2830940278873619 Lambda1 0.00079927046\n",
      "9 Train Loss 258.8642 Test MSE 280.6375743620367 Test RE 0.282008422762087 Lambda1 0.0012037769\n",
      "10 Train Loss 257.53497 Test MSE 280.40873971492016 Test RE 0.28189342308912524 Lambda1 0.00037093917\n",
      "11 Train Loss 255.71599 Test MSE 281.2636660594405 Test RE 0.2823228225102087 Lambda1 0.00055480935\n",
      "12 Train Loss 255.31746 Test MSE 281.52489168289077 Test RE 0.28245389674449645 Lambda1 0.0003950251\n",
      "13 Train Loss 255.07353 Test MSE 282.0792831555695 Test RE 0.2827318704178441 Lambda1 0.000238801\n",
      "14 Train Loss 254.97359 Test MSE 281.92271554807223 Test RE 0.2826533946144168 Lambda1 0.0003309842\n",
      "15 Train Loss 254.56209 Test MSE 282.51375624746254 Test RE 0.2829495257593525 Lambda1 0.00047663102\n",
      "16 Train Loss 254.36224 Test MSE 283.24679123939734 Test RE 0.28331637079031363 Lambda1 0.00044841834\n",
      "17 Train Loss 254.31783 Test MSE 283.3262533324463 Test RE 0.2833561088099345 Lambda1 0.00042013745\n",
      "18 Train Loss 254.19864 Test MSE 282.7533357523391 Test RE 0.2830694748594577 Lambda1 0.000480078\n",
      "19 Train Loss 254.16937 Test MSE 282.7604207551637 Test RE 0.28307302129943235 Lambda1 0.000503598\n",
      "20 Train Loss 254.0349 Test MSE 283.14282058140833 Test RE 0.2832643679185499 Lambda1 0.00050159334\n",
      "21 Train Loss 253.99002 Test MSE 283.0317297883542 Test RE 0.2832087932258337 Lambda1 0.00050415535\n",
      "22 Train Loss 253.90085 Test MSE 283.22972880316064 Test RE 0.28330783734798937 Lambda1 0.0004909182\n",
      "23 Train Loss 253.70322 Test MSE 283.3704003662764 Test RE 0.28337818379296376 Lambda1 0.0003834218\n",
      "24 Train Loss 253.65892 Test MSE 283.34546152667764 Test RE 0.2833657137562912 Lambda1 0.0004108568\n",
      "25 Train Loss 253.57632 Test MSE 283.7863167612094 Test RE 0.2835860714487724 Lambda1 0.00040913455\n",
      "26 Train Loss 253.32248 Test MSE 283.3265043283397 Test RE 0.2833562343210778 Lambda1 0.00029683704\n",
      "27 Train Loss 253.26521 Test MSE 283.2741141048883 Test RE 0.2833300352494679 Lambda1 0.0002985014\n",
      "28 Train Loss 253.18433 Test MSE 283.2830673819545 Test RE 0.2833345127363551 Lambda1 0.00028705216\n",
      "29 Train Loss 252.87277 Test MSE 283.8756287165644 Test RE 0.2836306924058465 Lambda1 0.00025893067\n",
      "30 Train Loss 252.48215 Test MSE 283.16460709980714 Test RE 0.28327526564437117 Lambda1 0.00019010819\n",
      "31 Train Loss 252.02954 Test MSE 283.59473819431366 Test RE 0.2834903335957488 Lambda1 0.00014070095\n",
      "32 Train Loss 251.79315 Test MSE 283.61197422799034 Test RE 0.2834989483089967 Lambda1 0.00014730864\n",
      "33 Train Loss 251.4577 Test MSE 283.73400726077864 Test RE 0.2835599339493553 Lambda1 0.00013651742\n",
      "34 Train Loss 251.03377 Test MSE 284.64563147600944 Test RE 0.284015101097691 Lambda1 7.797484e-05\n",
      "35 Train Loss 250.83473 Test MSE 284.7883090643324 Test RE 0.2840862729487816 Lambda1 7.477547e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 Train Loss 250.72723 Test MSE 285.02731867730546 Test RE 0.28420545816069026 Lambda1 7.965357e-05\n",
      "37 Train Loss 250.38553 Test MSE 285.6895863036637 Test RE 0.28453544559566435 Lambda1 7.1253206e-05\n",
      "38 Train Loss 250.26653 Test MSE 286.1995218769113 Test RE 0.28478927013860045 Lambda1 5.826006e-05\n",
      "39 Train Loss 250.14613 Test MSE 286.05022232164913 Test RE 0.28471497850863303 Lambda1 4.1556315e-05\n",
      "40 Train Loss 249.96634 Test MSE 286.4829715345442 Test RE 0.28493026171595703 Lambda1 3.8970542e-05\n",
      "41 Train Loss 249.85374 Test MSE 286.22316396294127 Test RE 0.28480103269036533 Lambda1 5.3711185e-05\n",
      "42 Train Loss 249.77823 Test MSE 286.71607165832694 Test RE 0.2850461565178712 Lambda1 5.3782045e-05\n",
      "43 Train Loss 249.72653 Test MSE 287.0914490371173 Test RE 0.28523269102129917 Lambda1 5.3600976e-05\n",
      "44 Train Loss 249.69154 Test MSE 287.1250793754319 Test RE 0.28524939683233386 Lambda1 5.6295914e-05\n",
      "45 Train Loss 249.63498 Test MSE 287.25374828924 Test RE 0.2853133038570554 Lambda1 6.5925255e-05\n",
      "46 Train Loss 249.5324 Test MSE 287.7500739672739 Test RE 0.2855596839371064 Lambda1 6.74535e-05\n",
      "47 Train Loss 249.45555 Test MSE 287.8405581914676 Test RE 0.28560457813149764 Lambda1 7.309662e-05\n",
      "48 Train Loss 249.41037 Test MSE 288.1422636427532 Test RE 0.28575421981287213 Lambda1 7.188178e-05\n",
      "49 Train Loss 249.40144 Test MSE 288.1899913530912 Test RE 0.28577788491104544 Lambda1 7.4530544e-05\n",
      "50 Train Loss 249.34242 Test MSE 288.0195929024916 Test RE 0.28569338629807023 Lambda1 7.4340904e-05\n",
      "51 Train Loss 249.30855 Test MSE 287.8795456420423 Test RE 0.28562391977225854 Lambda1 6.578523e-05\n",
      "52 Train Loss 249.27774 Test MSE 287.8709922076418 Test RE 0.28561967653295023 Lambda1 6.809056e-05\n",
      "53 Train Loss 249.24715 Test MSE 287.70993105334117 Test RE 0.28553976457187136 Lambda1 6.444044e-05\n",
      "54 Train Loss 249.06357 Test MSE 288.4229177477617 Test RE 0.2858933500029779 Lambda1 4.7181195e-05\n",
      "55 Train Loss 248.98134 Test MSE 288.8229521589414 Test RE 0.28609154429053657 Lambda1 2.979694e-05\n",
      "56 Train Loss 248.95097 Test MSE 288.5613604448796 Test RE 0.28596195602489166 Lambda1 2.4058163e-05\n",
      "57 Train Loss 248.92265 Test MSE 288.65739461019814 Test RE 0.2860095366039872 Lambda1 1.7580573e-05\n",
      "58 Train Loss 248.87956 Test MSE 288.80697465037434 Test RE 0.2860836309767156 Lambda1 1.0833089e-05\n",
      "59 Train Loss 248.8521 Test MSE 288.7660969482688 Test RE 0.2860633841400607 Lambda1 1.269276e-05\n",
      "60 Train Loss 248.8224 Test MSE 289.0138172117966 Test RE 0.28618605869643027 Lambda1 1.264231e-05\n",
      "61 Train Loss 248.80423 Test MSE 288.892200515597 Test RE 0.2861258389714009 Lambda1 1.164011e-05\n",
      "62 Train Loss 248.73508 Test MSE 288.89245326926846 Test RE 0.28612596413805536 Lambda1 7.5646126e-06\n",
      "63 Train Loss 248.58885 Test MSE 289.2195681787211 Test RE 0.2862879095054643 Lambda1 3.4379766e-06\n",
      "64 Train Loss 248.5438 Test MSE 288.951738663952 Test RE 0.2861553214652121 Lambda1 4.606158e-06\n",
      "65 Train Loss 248.49571 Test MSE 288.86573063552515 Test RE 0.2861127304656611 Lambda1 4.1314665e-06\n",
      "66 Train Loss 248.39885 Test MSE 288.7059867187255 Test RE 0.28603360877720674 Lambda1 -6.922676e-06\n",
      "67 Train Loss 248.34377 Test MSE 288.58987142002866 Test RE 0.2859760827479409 Lambda1 -1.0826996e-05\n",
      "68 Train Loss 248.27866 Test MSE 288.4864020383873 Test RE 0.28592481202763953 Lambda1 -1.1413976e-05\n",
      "69 Train Loss 248.06595 Test MSE 288.89697959796365 Test RE 0.28612820562113056 Lambda1 -6.941686e-06\n",
      "70 Train Loss 247.97726 Test MSE 289.0787284135418 Test RE 0.28621819494207645 Lambda1 -5.281771e-06\n",
      "71 Train Loss 247.91013 Test MSE 289.3396268522318 Test RE 0.28634732419275455 Lambda1 -1.6506528e-06\n",
      "72 Train Loss 247.84741 Test MSE 289.4846043933165 Test RE 0.28641905431218595 Lambda1 1.36636e-07\n",
      "73 Train Loss 247.75244 Test MSE 289.6499279540061 Test RE 0.2865008290568676 Lambda1 2.3074847e-06\n",
      "74 Train Loss 247.69441 Test MSE 289.9875153192866 Test RE 0.2866677389808205 Lambda1 3.0136125e-06\n",
      "Training time: 144.29\n",
      "Training time: 144.29\n",
      "inv_HT_stan_tune6\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 837.98615 Test MSE 856.8595251916202 Test RE 0.4927696880906801 Lambda1 -0.15239276\n",
      "1 Train Loss 831.4577 Test MSE 850.5260818324838 Test RE 0.49094516651808723 Lambda1 -0.16533315\n",
      "2 Train Loss 790.91656 Test MSE 792.9940894778787 Test RE 0.47404999442642043 Lambda1 -0.009159678\n",
      "3 Train Loss 692.14935 Test MSE 682.0564346570097 Test RE 0.4396421528692173 Lambda1 0.07065281\n",
      "4 Train Loss 524.4844 Test MSE 517.2318646680571 Test RE 0.38285278157061886 Lambda1 0.00448914\n",
      "5 Train Loss 443.2552 Test MSE 455.3407935165916 Test RE 0.35921747073689214 Lambda1 0.002328381\n",
      "6 Train Loss 368.87186 Test MSE 388.33455046850406 Test RE 0.3317356813373374 Lambda1 0.004577528\n",
      "7 Train Loss 316.93015 Test MSE 340.5695865441122 Test RE 0.31066483830844166 Lambda1 0.006882713\n",
      "8 Train Loss 289.19714 Test MSE 312.40953536962473 Test RE 0.2975440781063419 Lambda1 0.007239284\n",
      "9 Train Loss 277.0366 Test MSE 294.73691537367125 Test RE 0.2890057194961997 Lambda1 0.009560801\n",
      "10 Train Loss 266.0306 Test MSE 284.02562072273446 Test RE 0.2837056138048658 Lambda1 0.01158264\n",
      "11 Train Loss 260.7035 Test MSE 278.53286419741266 Test RE 0.2809489369664726 Lambda1 0.011589321\n",
      "12 Train Loss 256.11304 Test MSE 274.3923775843836 Test RE 0.2788529171052592 Lambda1 0.014075254\n",
      "13 Train Loss 253.2616 Test MSE 272.2368933071687 Test RE 0.2777554956808711 Lambda1 0.015394539\n",
      "14 Train Loss 250.365 Test MSE 270.61784843004483 Test RE 0.2769283314904861 Lambda1 0.01987749\n",
      "15 Train Loss 247.32092 Test MSE 268.3382517558761 Test RE 0.27575948773022696 Lambda1 0.024592815\n",
      "16 Train Loss 244.0296 Test MSE 263.7644894983468 Test RE 0.273399259382824 Lambda1 0.03277852\n",
      "17 Train Loss 241.82515 Test MSE 260.6596708738609 Test RE 0.27178538040422884 Lambda1 0.03700825\n",
      "18 Train Loss 237.39673 Test MSE 252.92665873409152 Test RE 0.2677234877164652 Lambda1 0.050688133\n",
      "19 Train Loss 228.83597 Test MSE 242.07032468532068 Test RE 0.2619147440444634 Lambda1 0.07918226\n",
      "20 Train Loss 222.81166 Test MSE 233.49691498095189 Test RE 0.257234813243855 Lambda1 0.10016884\n",
      "21 Train Loss 215.65614 Test MSE 224.82410800118112 Test RE 0.2524123557757724 Lambda1 0.12323358\n",
      "22 Train Loss 207.70192 Test MSE 217.54020473621176 Test RE 0.24828983341015026 Lambda1 0.14054899\n",
      "23 Train Loss 199.397 Test MSE 207.97744832154245 Test RE 0.24277127177023805 Lambda1 0.16768834\n",
      "24 Train Loss 191.19312 Test MSE 198.24699571715752 Test RE 0.23702408458230553 Lambda1 0.1995847\n",
      "25 Train Loss 183.59161 Test MSE 185.32377852773243 Test RE 0.22916840590819276 Lambda1 0.2358723\n",
      "26 Train Loss 171.65205 Test MSE 176.48802031760556 Test RE 0.22363861110755767 Lambda1 0.27571228\n",
      "27 Train Loss 163.55945 Test MSE 166.2531049040535 Test RE 0.21705712836036628 Lambda1 0.308059\n",
      "28 Train Loss 157.56956 Test MSE 161.06597291179492 Test RE 0.21364418212115532 Lambda1 0.32551882\n",
      "29 Train Loss 149.09471 Test MSE 148.0790528325774 Test RE 0.20485000767290085 Lambda1 0.3559477\n",
      "30 Train Loss 139.47488 Test MSE 135.37945726082816 Test RE 0.1958689318561623 Lambda1 0.39792866\n",
      "31 Train Loss 132.94044 Test MSE 126.91689554157625 Test RE 0.1896482724063116 Lambda1 0.42379615\n",
      "32 Train Loss 126.78763 Test MSE 121.9650168084462 Test RE 0.18591173787928889 Lambda1 0.43711418\n",
      "33 Train Loss 122.44508 Test MSE 118.76850592176201 Test RE 0.18345933596103223 Lambda1 0.44680634\n",
      "34 Train Loss 118.39595 Test MSE 112.54452564247492 Test RE 0.1785876235312248 Lambda1 0.47354084\n",
      "35 Train Loss 115.663864 Test MSE 107.72570474443692 Test RE 0.17472250300534123 Lambda1 0.48665923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 Train Loss 111.40829 Test MSE 100.60164741430981 Test RE 0.16884636560465882 Lambda1 0.51406\n",
      "37 Train Loss 107.19377 Test MSE 100.13364921157711 Test RE 0.1684531716960639 Lambda1 0.5194684\n",
      "38 Train Loss 102.39194 Test MSE 93.03248047697623 Test RE 0.16237025430386137 Lambda1 0.54010624\n",
      "39 Train Loss 100.69939 Test MSE 90.02293269303269 Test RE 0.15972237123337044 Lambda1 0.5498084\n",
      "40 Train Loss 98.92956 Test MSE 88.14366513349079 Test RE 0.15804644196242937 Lambda1 0.55967355\n",
      "41 Train Loss 97.06856 Test MSE 87.91739898638555 Test RE 0.15784345778646858 Lambda1 0.5655341\n",
      "42 Train Loss 95.41385 Test MSE 88.48945863265416 Test RE 0.15835615189273877 Lambda1 0.565047\n",
      "43 Train Loss 93.81601 Test MSE 86.88486615230356 Test RE 0.1569138357503747 Lambda1 0.56599194\n",
      "44 Train Loss 92.16277 Test MSE 86.59086610359212 Test RE 0.1566481291355438 Lambda1 0.56900555\n",
      "45 Train Loss 88.67171 Test MSE 83.49248810880819 Test RE 0.15382002313138454 Lambda1 0.5881616\n",
      "46 Train Loss 87.01935 Test MSE 81.03057932824565 Test RE 0.15153524050407244 Lambda1 0.6022372\n",
      "47 Train Loss 85.37216 Test MSE 78.3662986489129 Test RE 0.14902318395739686 Lambda1 0.6175345\n",
      "48 Train Loss 82.357315 Test MSE 71.94729186229424 Test RE 0.14278954061349305 Lambda1 0.63935745\n",
      "49 Train Loss 79.84273 Test MSE 71.78992165397935 Test RE 0.1426332934482124 Lambda1 0.64387435\n",
      "50 Train Loss 78.62777 Test MSE 69.65661352355806 Test RE 0.14049806693158026 Lambda1 0.6481183\n",
      "51 Train Loss 77.34202 Test MSE 68.33236291212762 Test RE 0.13915614528602846 Lambda1 0.65425426\n",
      "52 Train Loss 72.612816 Test MSE 64.09830741606963 Test RE 0.13477596543133719 Lambda1 0.6705607\n",
      "53 Train Loss 69.93766 Test MSE 62.18130025955802 Test RE 0.13274527460638336 Lambda1 0.67912894\n",
      "54 Train Loss 68.53786 Test MSE 61.079527610949334 Test RE 0.13156398071815265 Lambda1 0.6908925\n",
      "55 Train Loss 67.80344 Test MSE 60.02358438064962 Test RE 0.13042178318261552 Lambda1 0.6980977\n",
      "56 Train Loss 67.03779 Test MSE 58.32996972580055 Test RE 0.12856863887087439 Lambda1 0.70979846\n",
      "57 Train Loss 65.16742 Test MSE 56.9505755036266 Test RE 0.12703934010764714 Lambda1 0.71614385\n",
      "58 Train Loss 63.051582 Test MSE 56.998659104587716 Test RE 0.12709295870646217 Lambda1 0.7233127\n",
      "59 Train Loss 62.256626 Test MSE 55.695419231329545 Test RE 0.12563160534069998 Lambda1 0.7274477\n",
      "60 Train Loss 61.6922 Test MSE 54.67574293982134 Test RE 0.12447625600146961 Lambda1 0.7296007\n",
      "61 Train Loss 60.34948 Test MSE 52.65828106955082 Test RE 0.12215816765274726 Lambda1 0.7391606\n",
      "62 Train Loss 59.22462 Test MSE 53.12359472056589 Test RE 0.12269670445219626 Lambda1 0.74767834\n",
      "63 Train Loss 58.206604 Test MSE 51.19923811068323 Test RE 0.1204539149378345 Lambda1 0.7571298\n",
      "64 Train Loss 58.070168 Test MSE 51.2877728452837 Test RE 0.12055801559940875 Lambda1 0.7576733\n",
      "65 Train Loss 57.548584 Test MSE 50.23438075318579 Test RE 0.1193135306164451 Lambda1 0.7628283\n",
      "66 Train Loss 56.32244 Test MSE 48.94035951366065 Test RE 0.11776676580138869 Lambda1 0.7647484\n",
      "67 Train Loss 55.070374 Test MSE 48.487694764600725 Test RE 0.1172208696840446 Lambda1 0.773366\n",
      "68 Train Loss 54.07088 Test MSE 47.32456629479657 Test RE 0.11580638152890771 Lambda1 0.7816376\n",
      "69 Train Loss 53.493874 Test MSE 46.797021713471054 Test RE 0.11515910416148491 Lambda1 0.7878227\n",
      "70 Train Loss 52.769905 Test MSE 46.33577679676598 Test RE 0.11459017817566873 Lambda1 0.7924105\n",
      "71 Train Loss 51.81708 Test MSE 45.011764982510776 Test RE 0.11294114646777542 Lambda1 0.8021327\n",
      "72 Train Loss 50.962433 Test MSE 44.13225896377858 Test RE 0.11183229811406696 Lambda1 0.8106964\n",
      "73 Train Loss 49.733627 Test MSE 43.012518073441306 Test RE 0.1104044566400765 Lambda1 0.8201848\n",
      "74 Train Loss 49.050995 Test MSE 42.46297077927946 Test RE 0.10969690084220265 Lambda1 0.8260799\n",
      "Training time: 141.62\n",
      "Training time: 141.62\n",
      "inv_HT_stan_tune6\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 838.07526 Test MSE 858.7101633446081 Test RE 0.493301541031791 Lambda1 -0.02151191\n",
      "1 Train Loss 837.756 Test MSE 857.5952928514502 Test RE 0.4929812082940619 Lambda1 -0.019855805\n",
      "2 Train Loss 836.5717 Test MSE 856.1154555979346 Test RE 0.4925556888508565 Lambda1 0.020477254\n",
      "3 Train Loss 833.14026 Test MSE 853.3604656124247 Test RE 0.49176252512470753 Lambda1 0.06813767\n",
      "4 Train Loss 766.7234 Test MSE 767.3651653144107 Test RE 0.4663266234597234 Lambda1 0.13206379\n",
      "5 Train Loss 532.6863 Test MSE 472.28320608396234 Test RE 0.3658393544421798 Lambda1 0.028112236\n",
      "6 Train Loss 418.89658 Test MSE 398.5488704996992 Test RE 0.33607016713189886 Lambda1 0.02503482\n",
      "7 Train Loss 326.7134 Test MSE 335.4705958077693 Test RE 0.3083304385532793 Lambda1 0.006418465\n",
      "8 Train Loss 285.2682 Test MSE 302.20833807975 Test RE 0.29264586491727906 Lambda1 0.0038993056\n",
      "9 Train Loss 267.12756 Test MSE 287.99511847799823 Test RE 0.2856812476624883 Lambda1 0.00078216597\n",
      "10 Train Loss 263.01462 Test MSE 284.70763757649155 Test RE 0.2840460337873133 Lambda1 7.451435e-06\n",
      "11 Train Loss 261.83453 Test MSE 284.23889032936523 Test RE 0.2838121084770607 Lambda1 5.317814e-05\n",
      "12 Train Loss 258.826 Test MSE 282.2699004880837 Test RE 0.2828273834464349 Lambda1 0.0002615884\n",
      "13 Train Loss 257.7269 Test MSE 282.4109434958062 Test RE 0.2828980354056031 Lambda1 0.00013061208\n",
      "14 Train Loss 256.84045 Test MSE 282.7459161643589 Test RE 0.2830657608932797 Lambda1 0.00022621555\n",
      "15 Train Loss 255.52515 Test MSE 283.39873993940535 Test RE 0.28339235361446596 Lambda1 7.712113e-05\n",
      "16 Train Loss 255.23154 Test MSE 282.8430253315615 Test RE 0.28311436622924274 Lambda1 9.391222e-05\n",
      "17 Train Loss 254.83076 Test MSE 282.9385819771563 Test RE 0.2831621863488173 Lambda1 5.521269e-05\n",
      "18 Train Loss 254.75392 Test MSE 282.7812283595223 Test RE 0.28308343641174066 Lambda1 3.7858066e-05\n",
      "19 Train Loss 254.34087 Test MSE 283.43228391721783 Test RE 0.2834091247291634 Lambda1 4.4086355e-05\n",
      "20 Train Loss 254.2008 Test MSE 283.2052792721378 Test RE 0.28329560894724887 Lambda1 4.2080737e-05\n",
      "21 Train Loss 253.75534 Test MSE 283.28628359820175 Test RE 0.2833361211319526 Lambda1 4.8378406e-05\n",
      "22 Train Loss 253.62227 Test MSE 283.16230701317636 Test RE 0.28327411514929146 Lambda1 6.1594415e-05\n",
      "23 Train Loss 253.34285 Test MSE 282.7000203522122 Test RE 0.2830427860959631 Lambda1 6.577857e-05\n",
      "24 Train Loss 253.15921 Test MSE 282.8468168097444 Test RE 0.2831162637806219 Lambda1 5.8157362e-05\n",
      "25 Train Loss 252.98361 Test MSE 283.9389444739332 Test RE 0.28366232120564694 Lambda1 3.7169302e-05\n",
      "26 Train Loss 252.85474 Test MSE 284.00909052591135 Test RE 0.283697357898478 Lambda1 3.1585707e-05\n",
      "27 Train Loss 252.64288 Test MSE 284.0444004883269 Test RE 0.28371499295320346 Lambda1 1.8154109e-05\n",
      "28 Train Loss 252.55074 Test MSE 283.99206666257123 Test RE 0.2836888551822137 Lambda1 2.7063768e-05\n",
      "29 Train Loss 252.1507 Test MSE 283.76307584098873 Test RE 0.2835744589505329 Lambda1 3.610098e-05\n",
      "30 Train Loss 251.86519 Test MSE 284.4571992169711 Test RE 0.2839210781071183 Lambda1 5.0296494e-05\n",
      "31 Train Loss 251.69267 Test MSE 284.0300327691174 Test RE 0.28370781733401046 Lambda1 7.0781105e-05\n",
      "32 Train Loss 251.4974 Test MSE 284.59503663256436 Test RE 0.2839898585915962 Lambda1 0.00010510561\n",
      "33 Train Loss 251.17532 Test MSE 285.63837244150625 Test RE 0.2845099409694673 Lambda1 9.0904934e-05\n",
      "34 Train Loss 250.96515 Test MSE 286.16674913103066 Test RE 0.28477296404290187 Lambda1 9.168643e-05\n",
      "35 Train Loss 250.59215 Test MSE 285.94832799806625 Test RE 0.28466426464469846 Lambda1 8.582033e-05\n",
      "36 Train Loss 250.35727 Test MSE 286.43222335563246 Test RE 0.2849050240338828 Lambda1 6.656511e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 Train Loss 250.21129 Test MSE 286.58214692191086 Test RE 0.28497957638156846 Lambda1 6.361714e-05\n",
      "38 Train Loss 249.99886 Test MSE 286.38559962114164 Test RE 0.28488183551719 Lambda1 4.778381e-05\n",
      "39 Train Loss 249.90358 Test MSE 286.81568781343583 Test RE 0.28509567019879184 Lambda1 4.545071e-05\n",
      "40 Train Loss 249.74979 Test MSE 287.62596240471015 Test RE 0.28549809389036784 Lambda1 2.9345092e-05\n",
      "41 Train Loss 249.58707 Test MSE 286.70198837865274 Test RE 0.285039155804559 Lambda1 3.0410163e-05\n",
      "42 Train Loss 249.38158 Test MSE 287.126718238525 Test RE 0.2852502109096738 Lambda1 2.1498727e-05\n",
      "43 Train Loss 249.31146 Test MSE 286.79922679781544 Test RE 0.2850874889314889 Lambda1 1.613625e-05\n",
      "44 Train Loss 249.26108 Test MSE 286.8737731417209 Test RE 0.285124537234255 Lambda1 1.6003287e-05\n",
      "45 Train Loss 248.99551 Test MSE 287.9289831180936 Test RE 0.2856484437784456 Lambda1 1.0918471e-05\n",
      "46 Train Loss 248.86583 Test MSE 288.3421704911778 Test RE 0.28585332766385735 Lambda1 1.0542671e-05\n",
      "47 Train Loss 248.83545 Test MSE 288.51692420204824 Test RE 0.285939937199962 Lambda1 9.481569e-06\n",
      "48 Train Loss 248.68153 Test MSE 288.5224395393231 Test RE 0.28594267022448316 Lambda1 5.9829003e-06\n",
      "49 Train Loss 248.53244 Test MSE 289.4732128125475 Test RE 0.286413418782972 Lambda1 6.435181e-06\n",
      "50 Train Loss 248.48885 Test MSE 289.3749414790229 Test RE 0.2863647983639544 Lambda1 5.3413382e-06\n",
      "51 Train Loss 248.35742 Test MSE 289.11997783348954 Test RE 0.28623861483503316 Lambda1 3.5122275e-06\n",
      "52 Train Loss 248.20879 Test MSE 289.729676518827 Test RE 0.2865402671082472 Lambda1 2.7297833e-06\n",
      "53 Train Loss 248.11812 Test MSE 289.53772664809276 Test RE 0.286445332960146 Lambda1 1.273022e-06\n",
      "54 Train Loss 247.91829 Test MSE 289.68393332524926 Test RE 0.2865176463942494 Lambda1 -1.1986788e-06\n",
      "55 Train Loss 247.75383 Test MSE 289.91593658118813 Test RE 0.28663235714488355 Lambda1 -1.7289608e-06\n",
      "56 Train Loss 247.72092 Test MSE 290.3127396529013 Test RE 0.2868284445183045 Lambda1 -9.3797524e-07\n",
      "57 Train Loss 247.60307 Test MSE 289.807454810017 Test RE 0.2865787255725028 Lambda1 -1.295635e-06\n",
      "58 Train Loss 247.45851 Test MSE 289.3194808304572 Test RE 0.2863373551817506 Lambda1 -2.3405595e-07\n",
      "59 Train Loss 247.43568 Test MSE 289.4038278952903 Test RE 0.2863790909739185 Lambda1 -1.4109756e-06\n",
      "60 Train Loss 247.42847 Test MSE 289.4831029934348 Test RE 0.2864183115609561 Lambda1 -2.323354e-06\n",
      "61 Train Loss 247.40407 Test MSE 289.5960821898676 Test RE 0.2864741976449361 Lambda1 -1.5307307e-06\n",
      "62 Train Loss 247.3049 Test MSE 289.6856743551597 Test RE 0.2865185073930577 Lambda1 -1.5787078e-06\n",
      "63 Train Loss 247.27122 Test MSE 290.10956278765366 Test RE 0.2867280577695036 Lambda1 -5.746037e-07\n",
      "64 Train Loss 247.25262 Test MSE 290.0849061755297 Test RE 0.28671587290275957 Lambda1 -9.660823e-07\n",
      "65 Train Loss 247.10883 Test MSE 290.2962639040382 Test RE 0.2868203053982743 Lambda1 -2.5199122e-06\n",
      "66 Train Loss 246.9778 Test MSE 290.3880204555251 Test RE 0.28686563075396615 Lambda1 2.6312603e-07\n",
      "67 Train Loss 246.87871 Test MSE 290.2892405173792 Test RE 0.2868168357323822 Lambda1 2.6473768e-07\n",
      "68 Train Loss 246.7834 Test MSE 290.0164823783726 Test RE 0.28668205634542004 Lambda1 3.024235e-08\n",
      "69 Train Loss 246.70349 Test MSE 289.96654074887755 Test RE 0.2866573715665602 Lambda1 -4.3903557e-07\n",
      "70 Train Loss 246.52754 Test MSE 290.35578977564916 Test RE 0.2868497104503909 Lambda1 1.8228961e-07\n",
      "71 Train Loss 246.30751 Test MSE 290.55519030533856 Test RE 0.2869481899184693 Lambda1 7.1356266e-07\n",
      "72 Train Loss 246.20123 Test MSE 291.04497605347836 Test RE 0.28718994081540805 Lambda1 2.5223406e-07\n",
      "73 Train Loss 246.17531 Test MSE 290.7611346996615 Test RE 0.2870498657879997 Lambda1 4.2362558e-07\n",
      "74 Train Loss 246.1539 Test MSE 290.74316014350694 Test RE 0.2870409930862203 Lambda1 4.937233e-07\n",
      "Training time: 143.96\n",
      "Training time: 143.96\n",
      "inv_HT_stan_tune6\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 837.6551 Test MSE 857.3628978221942 Test RE 0.492914408635902 Lambda1 -0.045220606\n",
      "1 Train Loss 836.8916 Test MSE 856.5548551360607 Test RE 0.4926820742451906 Lambda1 -0.0018606434\n",
      "2 Train Loss 823.6212 Test MSE 836.3435901532092 Test RE 0.48683471261027034 Lambda1 0.33797967\n",
      "3 Train Loss 783.973 Test MSE 769.3524877380568 Test RE 0.4669300793855262 Lambda1 0.52363425\n",
      "4 Train Loss 663.5244 Test MSE 647.1810547774136 Test RE 0.42825463139666353 Lambda1 0.5333589\n",
      "5 Train Loss 534.57434 Test MSE 522.0538019904884 Test RE 0.38463323014188533 Lambda1 0.524806\n",
      "6 Train Loss 453.60617 Test MSE 434.0322019027442 Test RE 0.35071161291504976 Lambda1 0.43945193\n",
      "7 Train Loss 403.74554 Test MSE 389.7471792726453 Test RE 0.33233850431846346 Lambda1 0.40976924\n",
      "8 Train Loss 347.6007 Test MSE 332.1886045037123 Test RE 0.30681849529917393 Lambda1 0.42519766\n",
      "9 Train Loss 314.96472 Test MSE 307.16765465070637 Test RE 0.2950372908251625 Lambda1 0.42042464\n",
      "10 Train Loss 273.92145 Test MSE 277.80825329244675 Test RE 0.28058325078356766 Lambda1 0.41955218\n",
      "11 Train Loss 244.97357 Test MSE 249.14635426874133 Test RE 0.2657152246992296 Lambda1 0.42631915\n",
      "12 Train Loss 215.89836 Test MSE 212.0111854834403 Test RE 0.24511424867790818 Lambda1 0.40712255\n",
      "13 Train Loss 200.16014 Test MSE 205.65477981949988 Test RE 0.24141184464822818 Lambda1 0.4052803\n",
      "14 Train Loss 191.75876 Test MSE 201.0137931260597 Test RE 0.2386723449260387 Lambda1 0.4044155\n",
      "15 Train Loss 185.04669 Test MSE 194.00254632747004 Test RE 0.23447302462236685 Lambda1 0.40674704\n",
      "16 Train Loss 179.03174 Test MSE 184.32899684419917 Test RE 0.22855251277710575 Lambda1 0.4152007\n",
      "17 Train Loss 174.85904 Test MSE 180.4081181683099 Test RE 0.22610866657422277 Lambda1 0.42739725\n",
      "18 Train Loss 170.52794 Test MSE 173.18978227199662 Test RE 0.22153905750168917 Lambda1 0.44183794\n",
      "19 Train Loss 165.56407 Test MSE 167.16580409593567 Test RE 0.2176521148886815 Lambda1 0.4651691\n",
      "20 Train Loss 159.36725 Test MSE 159.14505723953638 Test RE 0.21236637212156817 Lambda1 0.48464835\n",
      "21 Train Loss 154.27267 Test MSE 154.5307445457389 Test RE 0.20926501031300995 Lambda1 0.4988918\n",
      "22 Train Loss 148.90283 Test MSE 149.7476981713693 Test RE 0.20600096193276474 Lambda1 0.5172393\n",
      "23 Train Loss 144.17453 Test MSE 145.03915883450262 Test RE 0.20273643565488672 Lambda1 0.5363611\n",
      "24 Train Loss 140.30695 Test MSE 141.3515909234015 Test RE 0.20014259252364078 Lambda1 0.5523087\n",
      "25 Train Loss 137.3132 Test MSE 137.88841572855247 Test RE 0.1976755981124133 Lambda1 0.57313883\n",
      "26 Train Loss 134.31438 Test MSE 135.8078739039484 Test RE 0.19617860671171353 Lambda1 0.57914275\n",
      "27 Train Loss 130.82607 Test MSE 134.1194637014313 Test RE 0.1949553127582944 Lambda1 0.5756128\n",
      "28 Train Loss 128.36919 Test MSE 132.62088933742928 Test RE 0.19386309351387826 Lambda1 0.5757139\n",
      "29 Train Loss 126.83566 Test MSE 131.31569014531033 Test RE 0.19290677512990242 Lambda1 0.5776244\n",
      "30 Train Loss 124.62794 Test MSE 127.97594470770431 Test RE 0.19043788199620715 Lambda1 0.5817196\n",
      "31 Train Loss 122.95389 Test MSE 127.16032043914385 Test RE 0.18983005670017886 Lambda1 0.5885347\n",
      "32 Train Loss 120.99457 Test MSE 125.00766975867394 Test RE 0.18821641647625192 Lambda1 0.5949561\n",
      "33 Train Loss 119.87166 Test MSE 123.40133722373439 Test RE 0.187003228131838 Lambda1 0.5980654\n",
      "34 Train Loss 118.10692 Test MSE 121.32357638396495 Test RE 0.1854222183721651 Lambda1 0.60668784\n",
      "35 Train Loss 116.30893 Test MSE 119.61730031988546 Test RE 0.18411372673683835 Lambda1 0.6265522\n",
      "36 Train Loss 114.493225 Test MSE 116.40450708289328 Test RE 0.1816243481840982 Lambda1 0.6314577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 Train Loss 111.09052 Test MSE 112.129461466358 Test RE 0.1782580037906702 Lambda1 0.62855756\n",
      "38 Train Loss 108.27001 Test MSE 107.32392948312683 Test RE 0.17439637492027837 Lambda1 0.6459419\n",
      "39 Train Loss 105.24937 Test MSE 102.12574641471822 Test RE 0.1701205556292278 Lambda1 0.6702688\n",
      "40 Train Loss 102.735146 Test MSE 97.93745072896667 Test RE 0.16659561585706023 Lambda1 0.6835461\n",
      "41 Train Loss 100.01524 Test MSE 93.6550896310075 Test RE 0.16291267041009636 Lambda1 0.698189\n",
      "42 Train Loss 97.66831 Test MSE 90.63969394242535 Test RE 0.1602685788161398 Lambda1 0.7136993\n",
      "43 Train Loss 95.48117 Test MSE 89.41791484145054 Test RE 0.15918474268354205 Lambda1 0.727069\n",
      "44 Train Loss 93.51013 Test MSE 89.3448044960504 Test RE 0.15911965264311284 Lambda1 0.7361116\n",
      "45 Train Loss 90.95273 Test MSE 88.658680350017 Test RE 0.15850749478293372 Lambda1 0.7482357\n",
      "46 Train Loss 87.30463 Test MSE 85.837583247594 Test RE 0.15596527360527387 Lambda1 0.76607144\n",
      "47 Train Loss 85.03103 Test MSE 82.56587976508601 Test RE 0.15296408635435335 Lambda1 0.7764682\n",
      "48 Train Loss 83.94373 Test MSE 80.47050550829637 Test RE 0.15101063556361388 Lambda1 0.7815421\n",
      "49 Train Loss 82.68613 Test MSE 78.31225179078649 Test RE 0.14897178671017963 Lambda1 0.7918426\n",
      "50 Train Loss 81.59494 Test MSE 77.34091669861007 Test RE 0.14804502862583663 Lambda1 0.801111\n",
      "51 Train Loss 80.43265 Test MSE 75.77167517702775 Test RE 0.1465354205442986 Lambda1 0.81311196\n",
      "52 Train Loss 79.45159 Test MSE 74.59904130914546 Test RE 0.1453971165154227 Lambda1 0.81758964\n",
      "53 Train Loss 78.2781 Test MSE 73.11914961265431 Test RE 0.14394770213828545 Lambda1 0.8282486\n",
      "54 Train Loss 76.5611 Test MSE 71.79383160039535 Test RE 0.1426371775654285 Lambda1 0.833667\n",
      "55 Train Loss 75.801445 Test MSE 70.00890232743961 Test RE 0.14085290382173438 Lambda1 0.84436077\n",
      "56 Train Loss 74.31113 Test MSE 68.82639758593929 Test RE 0.13965828033041955 Lambda1 0.85148245\n",
      "57 Train Loss 73.20169 Test MSE 68.01070576163367 Test RE 0.13882823793483678 Lambda1 0.8605642\n",
      "58 Train Loss 71.77531 Test MSE 66.46123561831502 Test RE 0.1372376826987328 Lambda1 0.8706221\n",
      "59 Train Loss 70.49364 Test MSE 65.31375919595312 Test RE 0.13604779614701584 Lambda1 0.87488186\n",
      "60 Train Loss 68.61196 Test MSE 64.37381084827818 Test RE 0.13506529778080176 Lambda1 0.8790003\n",
      "61 Train Loss 67.63396 Test MSE 62.8774580916018 Test RE 0.13348628877545518 Lambda1 0.89043444\n",
      "62 Train Loss 66.24212 Test MSE 61.48761025218693 Test RE 0.1320027496550218 Lambda1 0.9016187\n",
      "63 Train Loss 64.87964 Test MSE 60.62069480280919 Test RE 0.13106889121062043 Lambda1 0.9098536\n",
      "64 Train Loss 63.954037 Test MSE 60.018573422073224 Test RE 0.130416339057609 Lambda1 0.91659987\n",
      "65 Train Loss 62.91006 Test MSE 59.28253984012535 Test RE 0.1296141963849077 Lambda1 0.9248272\n",
      "66 Train Loss 62.153694 Test MSE 58.903020978682264 Test RE 0.12919864389689292 Lambda1 0.9235891\n",
      "67 Train Loss 61.644386 Test MSE 58.41309169209002 Test RE 0.12866021335261815 Lambda1 0.92740655\n",
      "68 Train Loss 60.408463 Test MSE 57.64456781671545 Test RE 0.12781103871740696 Lambda1 0.9358466\n",
      "69 Train Loss 59.790394 Test MSE 57.06337789556673 Test RE 0.12716509171134663 Lambda1 0.9459809\n",
      "70 Train Loss 59.343277 Test MSE 56.58351820941316 Test RE 0.12662928195548717 Lambda1 0.95375156\n",
      "71 Train Loss 58.85385 Test MSE 55.990743602356616 Test RE 0.12596424509785137 Lambda1 0.9599204\n",
      "72 Train Loss 58.22983 Test MSE 55.346842578863516 Test RE 0.12523784782803318 Lambda1 0.96298134\n",
      "73 Train Loss 57.75077 Test MSE 55.12240654337849 Test RE 0.12498366495488421 Lambda1 0.9682544\n",
      "74 Train Loss 57.424286 Test MSE 54.920254929103514 Test RE 0.12475427677949644 Lambda1 0.9756383\n",
      "Training time: 139.83\n",
      "Training time: 139.83\n",
      "inv_HT_stan_tune6\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 837.8859 Test MSE 858.0565976895198 Test RE 0.4931137790093595 Lambda1 0.0339127\n",
      "1 Train Loss 837.2954 Test MSE 856.7370361262442 Test RE 0.4927344658319474 Lambda1 0.046229716\n",
      "2 Train Loss 832.7629 Test MSE 850.5214591916839 Test RE 0.49094383236368 Lambda1 0.25562373\n",
      "3 Train Loss 782.72327 Test MSE 781.364539622859 Test RE 0.4705610969674492 Lambda1 0.4738347\n",
      "4 Train Loss 717.00446 Test MSE 710.8239590632907 Test RE 0.4488179311413697 Lambda1 0.56523013\n",
      "5 Train Loss 664.41583 Test MSE 664.4524643960101 Test RE 0.4339314513104546 Lambda1 0.6268074\n",
      "6 Train Loss 638.91724 Test MSE 634.7274500837443 Test RE 0.4241141974734435 Lambda1 0.68513525\n",
      "7 Train Loss 606.6421 Test MSE 600.0413960228046 Test RE 0.41236308214004186 Lambda1 0.7347605\n",
      "8 Train Loss 548.39465 Test MSE 536.9115554119559 Test RE 0.3900681998787016 Lambda1 0.8000559\n",
      "9 Train Loss 509.42688 Test MSE 502.93271027547416 Test RE 0.3775236054279755 Lambda1 0.79986215\n",
      "10 Train Loss 470.78726 Test MSE 469.9847558392391 Test RE 0.3649480575540474 Lambda1 0.7967648\n",
      "11 Train Loss 424.69186 Test MSE 423.8102769600098 Test RE 0.3465571891398978 Lambda1 0.7634523\n",
      "12 Train Loss 388.3145 Test MSE 387.13710075273855 Test RE 0.3312238243638164 Lambda1 0.74373025\n",
      "13 Train Loss 351.72363 Test MSE 353.84682704458476 Test RE 0.3166626365791883 Lambda1 0.71406484\n",
      "14 Train Loss 322.9819 Test MSE 335.5059388974534 Test RE 0.3083466800115088 Lambda1 0.71137923\n",
      "15 Train Loss 285.7401 Test MSE 301.03769047778377 Test RE 0.2920785119620673 Lambda1 0.6823713\n",
      "16 Train Loss 269.53458 Test MSE 295.81037594264274 Test RE 0.28953153466134857 Lambda1 0.67114955\n",
      "17 Train Loss 256.67923 Test MSE 274.6679588386693 Test RE 0.27899291252371416 Lambda1 0.65816987\n",
      "18 Train Loss 249.552 Test MSE 267.9436378548509 Test RE 0.2755566494046777 Lambda1 0.6442895\n",
      "19 Train Loss 237.3048 Test MSE 257.5319618863338 Test RE 0.2701498550141953 Lambda1 0.62680835\n",
      "20 Train Loss 224.29112 Test MSE 238.4960573569597 Test RE 0.2599739139666968 Lambda1 0.6071697\n",
      "21 Train Loss 213.5603 Test MSE 219.20827330963854 Test RE 0.2492399418256754 Lambda1 0.5857969\n",
      "22 Train Loss 204.0671 Test MSE 212.62825323713292 Test RE 0.24547069732920793 Lambda1 0.57242\n",
      "23 Train Loss 192.07329 Test MSE 198.6075818434194 Test RE 0.23723954501792843 Lambda1 0.55657125\n",
      "24 Train Loss 187.89726 Test MSE 193.36092245985287 Test RE 0.23408496764018819 Lambda1 0.55257887\n",
      "25 Train Loss 182.9704 Test MSE 185.11562897589704 Test RE 0.22903967255561755 Lambda1 0.555299\n",
      "26 Train Loss 176.41121 Test MSE 177.07364509464568 Test RE 0.22400934405089432 Lambda1 0.5613382\n",
      "27 Train Loss 170.13907 Test MSE 176.06206857826191 Test RE 0.2233685735110998 Lambda1 0.5667202\n",
      "28 Train Loss 163.32336 Test MSE 169.28076000401467 Test RE 0.2190246377501989 Lambda1 0.5675142\n",
      "29 Train Loss 159.91042 Test MSE 168.0812650692207 Test RE 0.2182472723977959 Lambda1 0.57526064\n",
      "30 Train Loss 155.25053 Test MSE 161.24385574741117 Test RE 0.21376212492787 Lambda1 0.5725279\n",
      "31 Train Loss 149.58145 Test MSE 155.29535032929368 Test RE 0.2097820847712741 Lambda1 0.5561937\n",
      "32 Train Loss 145.56865 Test MSE 150.90935055229787 Test RE 0.2067984339907225 Lambda1 0.5410613\n",
      "33 Train Loss 143.89333 Test MSE 149.13600446433634 Test RE 0.20557979206206997 Lambda1 0.5375392\n",
      "34 Train Loss 141.20117 Test MSE 144.4467792264088 Test RE 0.20232199651476487 Lambda1 0.5426123\n",
      "35 Train Loss 138.0245 Test MSE 140.30431917226886 Test RE 0.19939978742146058 Lambda1 0.5401914\n",
      "36 Train Loss 135.77519 Test MSE 136.04450618232522 Test RE 0.1963494435909686 Lambda1 0.54086554\n",
      "37 Train Loss 131.58731 Test MSE 133.6923837543412 Test RE 0.19464466473219047 Lambda1 0.5442498\n",
      "38 Train Loss 128.76628 Test MSE 131.3671224145988 Test RE 0.19294454922181226 Lambda1 0.5523163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Train Loss 126.76376 Test MSE 128.40772775117355 Test RE 0.19075887437992556 Lambda1 0.555354\n",
      "40 Train Loss 122.46737 Test MSE 124.17807829732669 Test RE 0.18759084426568773 Lambda1 0.5547237\n",
      "41 Train Loss 118.35001 Test MSE 120.27771710062945 Test RE 0.18462128052109159 Lambda1 0.5618826\n",
      "42 Train Loss 117.01963 Test MSE 118.32002759057241 Test RE 0.18311263060069283 Lambda1 0.5678789\n",
      "43 Train Loss 112.80141 Test MSE 111.03700816944786 Test RE 0.17738751359950305 Lambda1 0.58261794\n",
      "44 Train Loss 110.81662 Test MSE 108.99499434189588 Test RE 0.17574883190786955 Lambda1 0.5879422\n",
      "45 Train Loss 109.25741 Test MSE 103.85654346591457 Test RE 0.17155607550996674 Lambda1 0.61636275\n",
      "46 Train Loss 105.40993 Test MSE 101.17914069242272 Test RE 0.16933029459965646 Lambda1 0.6315478\n",
      "47 Train Loss 103.82561 Test MSE 101.63319031853555 Test RE 0.1697098110356771 Lambda1 0.62887716\n",
      "48 Train Loss 101.243866 Test MSE 97.46954493185743 Test RE 0.16619717592610186 Lambda1 0.64910287\n",
      "49 Train Loss 97.3975 Test MSE 91.73542268903195 Test RE 0.16123439926593502 Lambda1 0.6879448\n",
      "50 Train Loss 95.84799 Test MSE 90.54710735281152 Test RE 0.1601867023678187 Lambda1 0.7050449\n",
      "51 Train Loss 92.83357 Test MSE 88.14884979846592 Test RE 0.15805109008888024 Lambda1 0.72732943\n",
      "52 Train Loss 91.54306 Test MSE 87.43671401198593 Test RE 0.15741136486762236 Lambda1 0.7235812\n",
      "53 Train Loss 88.68202 Test MSE 85.84032837515596 Test RE 0.15596776750789831 Lambda1 0.7416256\n",
      "54 Train Loss 86.75569 Test MSE 82.49156106430995 Test RE 0.15289522830129215 Lambda1 0.76007676\n",
      "55 Train Loss 83.21817 Test MSE 78.20369962800876 Test RE 0.14886850263221396 Lambda1 0.7955671\n",
      "56 Train Loss 80.24982 Test MSE 76.82804841167564 Test RE 0.1475533490814964 Lambda1 0.81858677\n",
      "57 Train Loss 78.04519 Test MSE 74.36031192650383 Test RE 0.1451642825695933 Lambda1 0.82438374\n",
      "58 Train Loss 76.33293 Test MSE 72.29668210331607 Test RE 0.1431358278002966 Lambda1 0.8474012\n",
      "59 Train Loss 74.88881 Test MSE 70.42558172837433 Test RE 0.14127144654939378 Lambda1 0.86176085\n",
      "60 Train Loss 73.34273 Test MSE 68.985865350291 Test RE 0.13981997779832933 Lambda1 0.8690616\n",
      "61 Train Loss 72.06488 Test MSE 67.20827567815914 Test RE 0.13800681947856247 Lambda1 0.8792767\n",
      "62 Train Loss 71.30947 Test MSE 67.4082080650461 Test RE 0.13821193962703718 Lambda1 0.8753963\n",
      "63 Train Loss 70.65009 Test MSE 67.62748964454912 Test RE 0.1384365615652728 Lambda1 0.87311906\n",
      "64 Train Loss 69.88652 Test MSE 67.38770660374311 Test RE 0.13819092021193946 Lambda1 0.87719476\n",
      "65 Train Loss 69.18787 Test MSE 66.97175223446419 Test RE 0.13776376445813443 Lambda1 0.8853285\n",
      "66 Train Loss 68.079605 Test MSE 66.19243170271085 Test RE 0.1369598711113758 Lambda1 0.89113635\n",
      "67 Train Loss 66.818924 Test MSE 65.18967780914188 Test RE 0.135918504673271 Lambda1 0.90907145\n",
      "68 Train Loss 66.07342 Test MSE 64.20375255184213 Test RE 0.13488677670461635 Lambda1 0.92583644\n",
      "69 Train Loss 65.466156 Test MSE 63.13440926923793 Test RE 0.13375875914908072 Lambda1 0.9328388\n",
      "70 Train Loss 63.902058 Test MSE 62.55747829208156 Test RE 0.1331462034982523 Lambda1 0.9446647\n",
      "71 Train Loss 63.24564 Test MSE 61.64016716914603 Test RE 0.1321664042258375 Lambda1 0.9507493\n",
      "72 Train Loss 62.292065 Test MSE 59.06937029743544 Test RE 0.1293809516405831 Lambda1 0.97384423\n",
      "73 Train Loss 61.228874 Test MSE 58.6126755298561 Test RE 0.12887982681122226 Lambda1 0.9818181\n",
      "74 Train Loss 60.350807 Test MSE 56.857165284854105 Test RE 0.12693511251726777 Lambda1 0.9976942\n",
      "Training time: 145.07\n",
      "Training time: 145.07\n",
      "inv_HT_stan_tune6\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 837.9705 Test MSE 857.1809230528755 Test RE 0.4928620954636591 Lambda1 -0.014686363\n",
      "1 Train Loss 837.38367 Test MSE 857.2790944127031 Test RE 0.4928903179517813 Lambda1 -0.015528915\n",
      "2 Train Loss 835.81226 Test MSE 854.4743237577403 Test RE 0.4920833596913458 Lambda1 0.0028100454\n",
      "3 Train Loss 809.9209 Test MSE 824.8590182927823 Test RE 0.48348057964627544 Lambda1 0.15888876\n",
      "4 Train Loss 764.9273 Test MSE 765.387965240119 Test RE 0.4657254652598289 Lambda1 0.26835677\n",
      "5 Train Loss 712.1013 Test MSE 696.6849519831144 Test RE 0.44433178944799256 Lambda1 0.27076325\n",
      "6 Train Loss 669.28284 Test MSE 667.5473975292812 Test RE 0.4349408753572273 Lambda1 0.30243465\n",
      "7 Train Loss 643.6287 Test MSE 640.9703359406145 Test RE 0.4261947896756467 Lambda1 0.25449976\n",
      "8 Train Loss 618.51733 Test MSE 623.1638364615951 Test RE 0.42023313370699816 Lambda1 0.26050717\n",
      "9 Train Loss 591.18005 Test MSE 591.8380140704127 Test RE 0.40953459949219223 Lambda1 0.25822842\n",
      "10 Train Loss 549.7152 Test MSE 546.6089889207658 Test RE 0.39357504638013596 Lambda1 0.2153438\n",
      "11 Train Loss 477.37143 Test MSE 479.34921389950534 Test RE 0.3685659245228331 Lambda1 0.11562144\n",
      "12 Train Loss 393.46423 Test MSE 406.0975282239916 Test RE 0.3392378824778934 Lambda1 0.05163985\n",
      "13 Train Loss 305.7579 Test MSE 313.68481813251066 Test RE 0.29815075994480467 Lambda1 0.03885211\n",
      "14 Train Loss 281.51483 Test MSE 296.5542353890894 Test RE 0.28989534127374567 Lambda1 0.033389494\n",
      "15 Train Loss 267.0526 Test MSE 281.1936467212616 Test RE 0.28228767881762384 Lambda1 0.034468096\n",
      "16 Train Loss 258.93524 Test MSE 275.84549261125954 Test RE 0.2795903106078533 Lambda1 0.040618643\n",
      "17 Train Loss 255.90628 Test MSE 275.6468323703754 Test RE 0.27948961387007276 Lambda1 0.042462915\n",
      "18 Train Loss 251.78563 Test MSE 273.17655810213154 Test RE 0.27823443927362795 Lambda1 0.050587576\n",
      "19 Train Loss 249.24872 Test MSE 270.69093837335686 Test RE 0.276965726122204 Lambda1 0.057513922\n",
      "20 Train Loss 245.66742 Test MSE 267.36978286312467 Test RE 0.2752614113513473 Lambda1 0.07335722\n",
      "21 Train Loss 240.75839 Test MSE 262.2711142676195 Test RE 0.2726241982043705 Lambda1 0.09327668\n",
      "22 Train Loss 235.10332 Test MSE 256.05186854550715 Test RE 0.2693724308107806 Lambda1 0.11822656\n",
      "23 Train Loss 229.84657 Test MSE 254.44522688156306 Test RE 0.2685259890190391 Lambda1 0.14264163\n",
      "24 Train Loss 227.9217 Test MSE 253.48835978994336 Test RE 0.2680206038202833 Lambda1 0.15112579\n",
      "25 Train Loss 226.18356 Test MSE 252.70536818609548 Test RE 0.2676063437944634 Lambda1 0.14990704\n",
      "26 Train Loss 224.47296 Test MSE 250.59093308871175 Test RE 0.2664844348238622 Lambda1 0.15980443\n",
      "27 Train Loss 222.28165 Test MSE 248.62725894341563 Test RE 0.265438272119912 Lambda1 0.17804858\n",
      "28 Train Loss 220.00296 Test MSE 247.39989478053212 Test RE 0.2647822851425537 Lambda1 0.19380115\n",
      "29 Train Loss 218.69458 Test MSE 244.78238046300103 Test RE 0.2633778496726104 Lambda1 0.20602643\n",
      "30 Train Loss 217.43433 Test MSE 244.18102481039324 Test RE 0.26305413120645404 Lambda1 0.22077681\n",
      "31 Train Loss 216.24257 Test MSE 243.53776007477381 Test RE 0.26270741090681665 Lambda1 0.2320201\n",
      "32 Train Loss 214.46115 Test MSE 243.86457997195367 Test RE 0.26288362428668277 Lambda1 0.2493764\n",
      "33 Train Loss 213.9013 Test MSE 243.6937172202263 Test RE 0.2627915139647877 Lambda1 0.25802967\n",
      "34 Train Loss 212.98024 Test MSE 242.99842401146068 Test RE 0.2624163551939728 Lambda1 0.26931736\n",
      "35 Train Loss 212.54613 Test MSE 243.38117223169206 Test RE 0.2626229406353584 Lambda1 0.27349058\n",
      "36 Train Loss 212.07085 Test MSE 242.74034024391668 Test RE 0.2622769645911454 Lambda1 0.2936401\n",
      "37 Train Loss 211.4629 Test MSE 241.7893226682822 Test RE 0.2617626809142731 Lambda1 0.30550703\n",
      "38 Train Loss 211.20961 Test MSE 241.84023128374537 Test RE 0.26179023646169713 Lambda1 0.29983923\n",
      "39 Train Loss 210.76491 Test MSE 240.70466047009617 Test RE 0.26117488982108544 Lambda1 0.30948856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 Train Loss 210.44156 Test MSE 240.78885108562335 Test RE 0.2612205610438928 Lambda1 0.31694722\n",
      "41 Train Loss 209.86192 Test MSE 239.9155736846312 Test RE 0.26074644187595003 Lambda1 0.32632232\n",
      "42 Train Loss 209.52884 Test MSE 239.15755492661503 Test RE 0.2603341988135541 Lambda1 0.33432105\n",
      "43 Train Loss 208.65256 Test MSE 237.89379206837336 Test RE 0.25964545521095844 Lambda1 0.3408282\n",
      "44 Train Loss 207.54294 Test MSE 236.44696331441384 Test RE 0.2588546917588215 Lambda1 0.34703073\n",
      "45 Train Loss 206.67825 Test MSE 234.8252403798332 Test RE 0.25796545877855304 Lambda1 0.33790356\n",
      "46 Train Loss 206.39627 Test MSE 234.34567668343308 Test RE 0.2577019136537 Lambda1 0.33267722\n",
      "47 Train Loss 205.24156 Test MSE 232.61680072757187 Test RE 0.2567495610813286 Lambda1 0.32350042\n",
      "48 Train Loss 204.2961 Test MSE 231.816804118714 Test RE 0.2563076848292112 Lambda1 0.32925844\n",
      "49 Train Loss 203.0902 Test MSE 230.34891198782367 Test RE 0.25549491045989536 Lambda1 0.33987105\n",
      "50 Train Loss 201.41344 Test MSE 228.91312558250385 Test RE 0.25469740389609513 Lambda1 0.3384443\n",
      "51 Train Loss 199.73964 Test MSE 224.78581399878928 Test RE 0.2523908583236731 Lambda1 0.34489492\n",
      "52 Train Loss 195.72789 Test MSE 213.5069037523942 Test RE 0.24597735764413525 Lambda1 0.33604217\n",
      "53 Train Loss 192.02579 Test MSE 205.65492097078823 Test RE 0.2414119274948002 Lambda1 0.32234433\n",
      "54 Train Loss 187.96284 Test MSE 197.89640367582095 Test RE 0.23681440793851896 Lambda1 0.3247641\n",
      "55 Train Loss 181.36298 Test MSE 190.66291587706846 Test RE 0.2324461117538941 Lambda1 0.37334338\n",
      "56 Train Loss 178.18355 Test MSE 187.6530965780154 Test RE 0.23060410732604444 Lambda1 0.39395416\n",
      "57 Train Loss 174.62355 Test MSE 179.7175311188198 Test RE 0.2256754891893585 Lambda1 0.433292\n",
      "58 Train Loss 170.86644 Test MSE 174.83728873609448 Test RE 0.22259028328280187 Lambda1 0.4587781\n",
      "59 Train Loss 167.68205 Test MSE 170.4589132182566 Test RE 0.219785495495059 Lambda1 0.48340416\n",
      "60 Train Loss 164.01573 Test MSE 165.01650930823266 Test RE 0.2162483829599725 Lambda1 0.5196272\n",
      "61 Train Loss 157.33098 Test MSE 159.22825814304204 Test RE 0.21242187734931553 Lambda1 0.56776315\n",
      "62 Train Loss 153.70862 Test MSE 158.26267991808095 Test RE 0.21177682275734136 Lambda1 0.59263104\n",
      "63 Train Loss 151.57878 Test MSE 158.54461169571397 Test RE 0.21196537020643053 Lambda1 0.59782535\n",
      "64 Train Loss 150.1008 Test MSE 155.10039428017416 Test RE 0.20965036437458265 Lambda1 0.6066852\n",
      "65 Train Loss 146.88795 Test MSE 149.831408868553 Test RE 0.2060585323497436 Lambda1 0.62621087\n",
      "66 Train Loss 144.44861 Test MSE 147.67141436515328 Test RE 0.20456785334488287 Lambda1 0.64578354\n",
      "67 Train Loss 142.77458 Test MSE 143.32861279697315 Test RE 0.2015373851134407 Lambda1 0.6554991\n",
      "68 Train Loss 139.15302 Test MSE 141.9359951900053 Test RE 0.20055590071236462 Lambda1 0.6714111\n",
      "69 Train Loss 136.83418 Test MSE 140.37630992490966 Test RE 0.19945093730741667 Lambda1 0.67373675\n",
      "70 Train Loss 133.78215 Test MSE 136.85594044779273 Test RE 0.1969341338691978 Lambda1 0.6836268\n",
      "71 Train Loss 131.50775 Test MSE 134.3986459104445 Test RE 0.19515811613426634 Lambda1 0.68434465\n",
      "72 Train Loss 128.93109 Test MSE 129.98176665534834 Test RE 0.1919244869534205 Lambda1 0.7010592\n",
      "73 Train Loss 127.28327 Test MSE 127.96449585874416 Test RE 0.19042936342923267 Lambda1 0.71015096\n",
      "74 Train Loss 123.998116 Test MSE 127.5338385997558 Test RE 0.19010865375382605 Lambda1 0.7169934\n",
      "Training time: 141.83\n",
      "Training time: 141.83\n",
      "inv_HT_stan_tune6\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 837.4895 Test MSE 857.5989860056197 Test RE 0.49298226978172405 Lambda1 -0.17051487\n",
      "1 Train Loss 837.3167 Test MSE 856.9895723298032 Test RE 0.49280708095060083 Lambda1 -0.17050084\n",
      "2 Train Loss 832.5753 Test MSE 850.0406023918947 Test RE 0.4908050310118388 Lambda1 -0.17275877\n",
      "3 Train Loss 788.4368 Test MSE 785.1062963767237 Test RE 0.47168645023659905 Lambda1 -0.03990387\n",
      "4 Train Loss 682.7446 Test MSE 675.994229903522 Test RE 0.43768399424771454 Lambda1 0.017152643\n",
      "5 Train Loss 621.89545 Test MSE 622.770055898033 Test RE 0.42010033894207005 Lambda1 0.0017054186\n",
      "6 Train Loss 577.476 Test MSE 563.758977924305 Test RE 0.39970161790811787 Lambda1 -0.0044203764\n",
      "7 Train Loss 495.4343 Test MSE 516.6188142409604 Test RE 0.3826258256678341 Lambda1 -0.0009608065\n",
      "8 Train Loss 403.01035 Test MSE 410.02159336407635 Test RE 0.34087294686448755 Lambda1 -0.0012833853\n",
      "9 Train Loss 310.20004 Test MSE 327.4531886022765 Test RE 0.3046237657196515 Lambda1 -0.002092868\n",
      "10 Train Loss 279.5089 Test MSE 300.12774068940024 Test RE 0.291636743482159 Lambda1 0.00019312634\n",
      "11 Train Loss 271.19327 Test MSE 292.869672341534 Test RE 0.28808879781834706 Lambda1 0.0010838122\n",
      "12 Train Loss 266.85693 Test MSE 287.73275377484066 Test RE 0.2855510896333111 Lambda1 0.0012362204\n",
      "13 Train Loss 262.05887 Test MSE 282.4598388276339 Test RE 0.28292252417802094 Lambda1 0.0012468741\n",
      "14 Train Loss 259.81863 Test MSE 281.9208272796692 Test RE 0.2826524480316508 Lambda1 0.0010259072\n",
      "15 Train Loss 258.4168 Test MSE 282.06007290634 Test RE 0.2827222429079644 Lambda1 0.0010575777\n",
      "16 Train Loss 256.59775 Test MSE 281.5885805900382 Test RE 0.28248584447493547 Lambda1 0.0006657526\n",
      "17 Train Loss 255.74648 Test MSE 282.06581966038897 Test RE 0.28272512301591773 Lambda1 0.00058924436\n",
      "18 Train Loss 255.17943 Test MSE 281.3864818107616 Test RE 0.28238445490259373 Lambda1 0.00068436627\n",
      "19 Train Loss 254.8456 Test MSE 281.54637663749156 Test RE 0.2824646744653135 Lambda1 0.0005079564\n",
      "20 Train Loss 254.63277 Test MSE 282.29454954947516 Test RE 0.2828397320484967 Lambda1 0.00057302666\n",
      "21 Train Loss 254.32697 Test MSE 282.5300335764052 Test RE 0.2829576768600777 Lambda1 0.00091064273\n",
      "22 Train Loss 254.23827 Test MSE 282.4333504164927 Test RE 0.2829092579666594 Lambda1 0.0014001692\n",
      "23 Train Loss 254.0588 Test MSE 281.75550071136524 Test RE 0.2825695580716079 Lambda1 0.002039234\n",
      "24 Train Loss 253.78296 Test MSE 281.17978314939836 Test RE 0.2822807199766172 Lambda1 0.0027991417\n",
      "25 Train Loss 253.5844 Test MSE 280.94515483453347 Test RE 0.2821629219059835 Lambda1 0.0035789513\n",
      "26 Train Loss 253.36775 Test MSE 280.2053919748782 Test RE 0.28179119234792877 Lambda1 0.0043371473\n",
      "27 Train Loss 252.82246 Test MSE 278.4972789858748 Test RE 0.2809309894493539 Lambda1 0.0067890473\n",
      "28 Train Loss 252.4183 Test MSE 278.30681413441414 Test RE 0.28083490838355507 Lambda1 0.007216828\n",
      "29 Train Loss 251.99512 Test MSE 277.3616451135883 Test RE 0.2803576254169752 Lambda1 0.008698122\n",
      "30 Train Loss 251.22719 Test MSE 275.4128250144245 Test RE 0.27937095385936184 Lambda1 0.012675327\n",
      "31 Train Loss 250.25475 Test MSE 273.8913318764133 Test RE 0.2785982054059771 Lambda1 0.016337492\n",
      "32 Train Loss 249.27711 Test MSE 272.20688869758857 Test RE 0.2777401888377214 Lambda1 0.020227911\n",
      "33 Train Loss 247.63603 Test MSE 269.47236927492406 Test RE 0.2763416147878488 Lambda1 0.026541518\n",
      "34 Train Loss 246.07294 Test MSE 267.97912895768127 Test RE 0.27557489855145245 Lambda1 0.031086612\n",
      "35 Train Loss 244.06137 Test MSE 266.31396841949964 Test RE 0.2747173849278541 Lambda1 0.03749106\n",
      "36 Train Loss 242.75967 Test MSE 264.7169518972673 Test RE 0.2738924414974585 Lambda1 0.04418945\n",
      "37 Train Loss 239.74327 Test MSE 260.1394452060229 Test RE 0.2715140297430455 Lambda1 0.063036285\n",
      "38 Train Loss 237.62692 Test MSE 257.23489798349135 Test RE 0.2699940007161163 Lambda1 0.073636115\n",
      "39 Train Loss 232.84407 Test MSE 250.91895199063634 Test RE 0.26665878938683346 Lambda1 0.10251055\n",
      "40 Train Loss 227.62352 Test MSE 246.0846855958484 Test RE 0.2640775391776697 Lambda1 0.11383206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 Train Loss 223.85606 Test MSE 244.12974538022567 Test RE 0.26302650831019836 Lambda1 0.12813051\n",
      "42 Train Loss 221.41264 Test MSE 243.07697308179434 Test RE 0.26245876471853014 Lambda1 0.14329062\n",
      "43 Train Loss 219.27838 Test MSE 242.26185591618753 Test RE 0.2620183398371757 Lambda1 0.15811263\n",
      "44 Train Loss 217.2131 Test MSE 240.49375290538418 Test RE 0.26106044286484925 Lambda1 0.17409734\n",
      "45 Train Loss 215.40933 Test MSE 239.14659746599725 Test RE 0.2603282348907188 Lambda1 0.1846166\n",
      "46 Train Loss 213.75494 Test MSE 237.40301872425005 Test RE 0.25937749349700895 Lambda1 0.19711643\n",
      "47 Train Loss 211.90524 Test MSE 235.07222008618461 Test RE 0.25810108195404796 Lambda1 0.22393596\n",
      "48 Train Loss 209.92448 Test MSE 232.72120778433043 Test RE 0.2568071739851543 Lambda1 0.24490099\n",
      "49 Train Loss 206.32599 Test MSE 226.79980311437026 Test RE 0.2535189965649118 Lambda1 0.26651603\n",
      "50 Train Loss 199.63371 Test MSE 214.96079729351914 Test RE 0.24681343865617958 Lambda1 0.30118194\n",
      "51 Train Loss 192.80696 Test MSE 207.6269497878229 Test RE 0.24256661771743582 Lambda1 0.31484747\n",
      "52 Train Loss 183.81924 Test MSE 191.94702866669556 Test RE 0.23322755929496403 Lambda1 0.33330286\n",
      "53 Train Loss 177.8422 Test MSE 180.43129092077868 Test RE 0.22612318751700253 Lambda1 0.3476637\n",
      "54 Train Loss 169.6406 Test MSE 168.90486320732808 Test RE 0.21878132476109946 Lambda1 0.37012425\n",
      "55 Train Loss 161.73885 Test MSE 160.8001521065357 Test RE 0.21346781178471635 Lambda1 0.39239395\n",
      "56 Train Loss 154.34259 Test MSE 151.94711174533722 Test RE 0.20750826311419393 Lambda1 0.42687172\n",
      "57 Train Loss 146.64331 Test MSE 143.43519894013312 Test RE 0.20161230769885474 Lambda1 0.45785037\n",
      "58 Train Loss 139.92426 Test MSE 138.81528009488898 Test RE 0.19833885762561554 Lambda1 0.4863003\n",
      "59 Train Loss 137.56192 Test MSE 137.9396490981708 Test RE 0.19771231855176474 Lambda1 0.49345264\n",
      "60 Train Loss 135.82774 Test MSE 136.58365528434024 Test RE 0.19673812871009108 Lambda1 0.5037077\n",
      "61 Train Loss 133.4124 Test MSE 133.98324887422925 Test RE 0.19485628704791372 Lambda1 0.5067254\n",
      "62 Train Loss 129.92862 Test MSE 132.23035147158782 Test RE 0.1935774420634797 Lambda1 0.5173369\n",
      "63 Train Loss 128.67647 Test MSE 131.82203265309386 Test RE 0.19327833363426863 Lambda1 0.51829755\n",
      "64 Train Loss 125.3844 Test MSE 127.59197783763446 Test RE 0.1901519815207409 Lambda1 0.5297107\n",
      "65 Train Loss 124.1614 Test MSE 126.34615368050109 Test RE 0.18922137033708164 Lambda1 0.53496826\n",
      "66 Train Loss 121.81958 Test MSE 124.38574632020048 Test RE 0.1877476366186285 Lambda1 0.5337233\n",
      "67 Train Loss 120.67635 Test MSE 123.31860959280186 Test RE 0.1869405346146281 Lambda1 0.53843194\n",
      "68 Train Loss 120.190704 Test MSE 122.69618426402921 Test RE 0.18646816585694637 Lambda1 0.5422778\n",
      "69 Train Loss 117.56375 Test MSE 117.82026272789565 Test RE 0.18272550215119643 Lambda1 0.55346704\n",
      "70 Train Loss 116.19141 Test MSE 115.6849387480204 Test RE 0.18106211167603817 Lambda1 0.5641351\n",
      "71 Train Loss 114.46118 Test MSE 111.98192772317219 Test RE 0.17814069417988918 Lambda1 0.58124065\n",
      "72 Train Loss 113.24576 Test MSE 110.8535238257363 Test RE 0.17724089001345675 Lambda1 0.5846171\n",
      "73 Train Loss 111.66763 Test MSE 108.80716504699313 Test RE 0.17559733406442202 Lambda1 0.5951114\n",
      "74 Train Loss 108.36781 Test MSE 107.7923114106725 Test RE 0.17477651000921193 Lambda1 0.619232\n",
      "Training time: 140.41\n",
      "Training time: 140.41\n",
      "inv_HT_stan_tune6\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 839.1586 Test MSE 858.9377372676392 Test RE 0.49336690367380803 Lambda1 -0.014473605\n",
      "1 Train Loss 837.42975 Test MSE 857.1586908546132 Test RE 0.49285570388483513 Lambda1 -0.010893217\n",
      "2 Train Loss 830.0907 Test MSE 848.321370682776 Test RE 0.4903084461085708 Lambda1 0.044273075\n",
      "3 Train Loss 738.5056 Test MSE 744.6355560843484 Test RE 0.45936833526735527 Lambda1 0.15351222\n",
      "4 Train Loss 631.34814 Test MSE 633.2643537882814 Test RE 0.42362510730527403 Lambda1 0.10102071\n",
      "5 Train Loss 497.78268 Test MSE 490.6781992620605 Test RE 0.3728958516749385 Lambda1 0.057859275\n",
      "6 Train Loss 366.8948 Test MSE 367.3128493971521 Test RE 0.32263184464207145 Lambda1 0.031945396\n",
      "7 Train Loss 297.54285 Test MSE 306.6900723594884 Test RE 0.29480784056368925 Lambda1 0.02242496\n",
      "8 Train Loss 275.73007 Test MSE 293.3026895081727 Test RE 0.28830169339964634 Lambda1 0.01793534\n",
      "9 Train Loss 265.7709 Test MSE 283.495155146576 Test RE 0.28344055603231144 Lambda1 0.013444976\n",
      "10 Train Loss 261.35834 Test MSE 279.79205828043825 Test RE 0.28158327918396664 Lambda1 0.011524892\n",
      "11 Train Loss 256.29498 Test MSE 275.6281039481991 Test RE 0.27948011895160885 Lambda1 0.013696164\n",
      "12 Train Loss 254.87135 Test MSE 274.57581421783976 Test RE 0.27894611082778564 Lambda1 0.017025124\n",
      "13 Train Loss 250.02753 Test MSE 268.9999149130584 Test RE 0.27609925955559955 Lambda1 0.029888783\n",
      "14 Train Loss 245.85202 Test MSE 264.9748237607853 Test RE 0.27402581407041865 Lambda1 0.04641457\n",
      "15 Train Loss 239.95932 Test MSE 256.3412555529096 Test RE 0.26952460870570655 Lambda1 0.0709801\n",
      "16 Train Loss 232.37941 Test MSE 246.99997603688584 Test RE 0.2645681900101553 Lambda1 0.104612835\n",
      "17 Train Loss 224.85872 Test MSE 236.71975906723813 Test RE 0.2590039728148592 Lambda1 0.14974798\n",
      "18 Train Loss 216.24294 Test MSE 227.1147155849381 Test RE 0.25369494160193357 Lambda1 0.1779773\n",
      "19 Train Loss 206.282 Test MSE 218.2714047432239 Test RE 0.2487067614605897 Lambda1 0.19951592\n",
      "20 Train Loss 193.19553 Test MSE 199.62661421858303 Test RE 0.23784739055914708 Lambda1 0.24600802\n",
      "21 Train Loss 179.35002 Test MSE 178.87157822827697 Test RE 0.22514372130389662 Lambda1 0.2829373\n",
      "22 Train Loss 172.03369 Test MSE 173.93672204788524 Test RE 0.22201627474070781 Lambda1 0.29420146\n",
      "23 Train Loss 161.48584 Test MSE 162.21058726824606 Test RE 0.21440196874319628 Lambda1 0.3476569\n",
      "24 Train Loss 151.53877 Test MSE 149.55374141776178 Test RE 0.2058675100518853 Lambda1 0.3870456\n",
      "25 Train Loss 149.01784 Test MSE 146.09080234963534 Test RE 0.20347010433225574 Lambda1 0.39930668\n",
      "26 Train Loss 142.04523 Test MSE 137.49362929707038 Test RE 0.1973924141254805 Lambda1 0.41634506\n",
      "27 Train Loss 135.77013 Test MSE 132.55207429243166 Test RE 0.19381279062195578 Lambda1 0.42731473\n",
      "28 Train Loss 128.58047 Test MSE 125.9559777894494 Test RE 0.18892897242055845 Lambda1 0.4637477\n",
      "29 Train Loss 118.41631 Test MSE 118.72805435351704 Test RE 0.1834280909360984 Lambda1 0.49756074\n",
      "30 Train Loss 113.68335 Test MSE 116.12514496577519 Test RE 0.18140627483489863 Lambda1 0.512502\n",
      "31 Train Loss 109.87835 Test MSE 112.5940281681652 Test RE 0.17862689495051856 Lambda1 0.5317771\n",
      "32 Train Loss 107.29224 Test MSE 110.93043669928983 Test RE 0.17730236637406263 Lambda1 0.55087763\n",
      "33 Train Loss 101.784966 Test MSE 104.60523206094086 Test RE 0.1721733280336688 Lambda1 0.5885216\n",
      "34 Train Loss 98.2776 Test MSE 99.34435276881894 Test RE 0.1677879480726813 Lambda1 0.5921596\n",
      "35 Train Loss 95.01853 Test MSE 94.70341251561786 Test RE 0.16382190993571832 Lambda1 0.6177549\n",
      "36 Train Loss 90.94481 Test MSE 87.92525371696559 Test RE 0.15785050866694425 Lambda1 0.6639623\n",
      "37 Train Loss 85.9344 Test MSE 79.46499049406216 Test RE 0.15006419695666584 Lambda1 0.7154739\n",
      "38 Train Loss 82.47234 Test MSE 79.83730952970143 Test RE 0.15041533564885434 Lambda1 0.71323985\n",
      "39 Train Loss 79.19514 Test MSE 75.5935787650663 Test RE 0.14636310822777143 Lambda1 0.74804413\n",
      "40 Train Loss 77.32674 Test MSE 75.2131290356925 Test RE 0.1459943332306975 Lambda1 0.75488913\n",
      "41 Train Loss 74.30376 Test MSE 72.27873707590716 Test RE 0.14311806256068194 Lambda1 0.7886084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 Train Loss 71.60948 Test MSE 69.30658312725804 Test RE 0.14014461498403788 Lambda1 0.8130058\n",
      "43 Train Loss 69.07264 Test MSE 65.61307096298448 Test RE 0.13635917127617186 Lambda1 0.8602574\n",
      "44 Train Loss 66.24854 Test MSE 62.097790890794826 Test RE 0.13265610633162506 Lambda1 0.9036016\n",
      "45 Train Loss 63.421585 Test MSE 58.934970054116135 Test RE 0.12923367790342644 Lambda1 0.9362742\n",
      "46 Train Loss 62.100506 Test MSE 57.32848654999954 Test RE 0.1274601452316238 Lambda1 0.9504274\n",
      "47 Train Loss 60.205723 Test MSE 56.41053263950208 Test RE 0.12643557001683106 Lambda1 0.9621915\n",
      "48 Train Loss 58.99165 Test MSE 56.49024167833256 Test RE 0.12652486628549806 Lambda1 0.95675343\n",
      "49 Train Loss 57.02875 Test MSE 55.29233509998151 Test RE 0.12517616336172896 Lambda1 0.96392554\n",
      "50 Train Loss 54.87983 Test MSE 54.26806778900677 Test RE 0.12401132565669654 Lambda1 0.9778227\n",
      "51 Train Loss 53.200565 Test MSE 54.102204266592544 Test RE 0.12382166812141553 Lambda1 0.9869613\n",
      "52 Train Loss 52.11625 Test MSE 53.02141658210101 Test RE 0.12257864998771272 Lambda1 0.99599916\n",
      "53 Train Loss 51.592793 Test MSE 52.307308060434444 Test RE 0.12175038849443251 Lambda1 1.0078876\n",
      "54 Train Loss 50.557896 Test MSE 52.25792333548035 Test RE 0.12169290102749497 Lambda1 1.0197289\n",
      "55 Train Loss 49.782166 Test MSE 50.97885137157326 Test RE 0.12019438886478388 Lambda1 1.0225214\n",
      "56 Train Loss 48.15229 Test MSE 48.562036251758514 Test RE 0.1173106969706617 Lambda1 1.0482714\n",
      "57 Train Loss 47.099327 Test MSE 47.40812088383188 Test RE 0.1159085682806332 Lambda1 1.075799\n",
      "58 Train Loss 46.127743 Test MSE 45.94517882825414 Test RE 0.11410617402729463 Lambda1 1.0847092\n",
      "59 Train Loss 45.704006 Test MSE 45.992086039523045 Test RE 0.11416440687268643 Lambda1 1.0868338\n",
      "60 Train Loss 44.712994 Test MSE 45.36256540188424 Test RE 0.11338039725488575 Lambda1 1.1053137\n",
      "61 Train Loss 43.12119 Test MSE 43.216064592145955 Test RE 0.11066537974051031 Lambda1 1.1435549\n",
      "62 Train Loss 42.369347 Test MSE 42.07989781945156 Test RE 0.1092009732239856 Lambda1 1.1699858\n",
      "63 Train Loss 41.816395 Test MSE 41.343074551826206 Test RE 0.10824069095558356 Lambda1 1.1855224\n",
      "64 Train Loss 41.48032 Test MSE 40.98501861243235 Test RE 0.10777095689359785 Lambda1 1.1860815\n",
      "65 Train Loss 40.999084 Test MSE 40.908064440110095 Test RE 0.10766973305990009 Lambda1 1.1896211\n",
      "66 Train Loss 40.368633 Test MSE 40.217448083126605 Test RE 0.10675701584786366 Lambda1 1.2099535\n",
      "67 Train Loss 39.90454 Test MSE 40.12626926772605 Test RE 0.10663593032336381 Lambda1 1.2160993\n",
      "68 Train Loss 39.571747 Test MSE 39.84622568594948 Test RE 0.10626316960539064 Lambda1 1.2194366\n",
      "69 Train Loss 38.7463 Test MSE 38.80766702838612 Test RE 0.10486919597610182 Lambda1 1.2529684\n",
      "70 Train Loss 38.238716 Test MSE 38.36139856479133 Test RE 0.10426448122147447 Lambda1 1.266294\n",
      "71 Train Loss 37.91652 Test MSE 38.24468671755664 Test RE 0.10410575173660509 Lambda1 1.2736723\n",
      "72 Train Loss 37.644375 Test MSE 38.039142761099875 Test RE 0.10382561953435702 Lambda1 1.2874634\n",
      "73 Train Loss 37.516117 Test MSE 37.98657233655092 Test RE 0.10375385077708615 Lambda1 1.2927055\n",
      "74 Train Loss 37.169724 Test MSE 37.58223417685817 Test RE 0.10320018309343777 Lambda1 1.3083903\n",
      "Training time: 141.46\n",
      "Training time: 141.46\n",
      "inv_HT_stan_tune6\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 838.2576 Test MSE 857.7429956982529 Test RE 0.4930236593158661 Lambda1 -0.025342623\n",
      "1 Train Loss 831.3559 Test MSE 843.0840753860504 Test RE 0.48879259029879324 Lambda1 -0.03633642\n",
      "2 Train Loss 725.015 Test MSE 707.2013635812642 Test RE 0.4476728075154054 Lambda1 0.088675424\n",
      "3 Train Loss 634.2755 Test MSE 626.9322101364186 Test RE 0.42150182785451157 Lambda1 0.015723309\n",
      "4 Train Loss 608.85547 Test MSE 611.5615391156533 Test RE 0.4163027244774813 Lambda1 0.0039461884\n",
      "5 Train Loss 504.34183 Test MSE 484.6724444392321 Test RE 0.37060675864822323 Lambda1 0.00058609905\n",
      "6 Train Loss 363.73795 Test MSE 383.69018918077023 Test RE 0.32974598626171714 Lambda1 0.00053094496\n",
      "7 Train Loss 315.0288 Test MSE 340.883224639366 Test RE 0.3108078544635861 Lambda1 0.0005389161\n",
      "8 Train Loss 265.95206 Test MSE 290.20215420935915 Test RE 0.2867738102091749 Lambda1 0.001962383\n",
      "9 Train Loss 258.8576 Test MSE 284.8352771773656 Test RE 0.28410969814858494 Lambda1 0.0017771558\n",
      "10 Train Loss 256.9952 Test MSE 283.12379032631736 Test RE 0.2832548485464038 Lambda1 0.0010160198\n",
      "11 Train Loss 255.9693 Test MSE 282.0902850818961 Test RE 0.2827373840536626 Lambda1 0.0007509624\n",
      "12 Train Loss 255.81099 Test MSE 282.44773049399157 Test RE 0.2829164600291394 Lambda1 0.00062465866\n",
      "13 Train Loss 255.30649 Test MSE 282.8505131997286 Test RE 0.28311811373020684 Lambda1 0.0005227507\n",
      "14 Train Loss 255.15892 Test MSE 283.0381237439815 Test RE 0.2832119921855961 Lambda1 0.0004001129\n",
      "15 Train Loss 255.09315 Test MSE 283.09404810891755 Test RE 0.28323997016310165 Lambda1 0.000378416\n",
      "16 Train Loss 254.96051 Test MSE 282.9699299452359 Test RE 0.2831778722854307 Lambda1 0.0002968675\n",
      "17 Train Loss 254.70985 Test MSE 282.76589583433264 Test RE 0.28307576185218675 Lambda1 0.000369404\n",
      "18 Train Loss 254.64723 Test MSE 282.7454516489039 Test RE 0.2830655283727247 Lambda1 0.00043548946\n",
      "19 Train Loss 254.56769 Test MSE 282.6645520586965 Test RE 0.2830250298901034 Lambda1 0.0004461929\n",
      "20 Train Loss 254.51358 Test MSE 282.5608359282001 Test RE 0.28297310092712835 Lambda1 0.00051288295\n",
      "21 Train Loss 254.4591 Test MSE 282.67608196728105 Test RE 0.28303080213749277 Lambda1 0.000534595\n",
      "22 Train Loss 254.22964 Test MSE 282.3973060252918 Test RE 0.2828912048271823 Lambda1 0.00079514197\n",
      "23 Train Loss 254.17047 Test MSE 282.0834313060953 Test RE 0.2827339492838142 Lambda1 0.0010580721\n",
      "24 Train Loss 254.10825 Test MSE 281.7554773828717 Test RE 0.2825695463736601 Lambda1 0.0013440235\n",
      "25 Train Loss 253.98256 Test MSE 281.529213759899 Test RE 0.28245606490589825 Lambda1 0.0016430113\n",
      "26 Train Loss 253.81885 Test MSE 282.0216694362321 Test RE 0.28270299543934924 Lambda1 0.0017307363\n",
      "27 Train Loss 253.77835 Test MSE 282.12071147659196 Test RE 0.2827526317380701 Lambda1 0.0016254497\n",
      "28 Train Loss 253.60356 Test MSE 282.02912290850446 Test RE 0.2827067311540908 Lambda1 0.002186414\n",
      "29 Train Loss 253.33981 Test MSE 281.65663153964897 Test RE 0.28251997630770753 Lambda1 0.0026902624\n",
      "30 Train Loss 253.2694 Test MSE 281.3601313683966 Test RE 0.2823712326442499 Lambda1 0.0031808086\n",
      "31 Train Loss 252.99852 Test MSE 280.2212307474414 Test RE 0.2817991564408119 Lambda1 0.0047045974\n",
      "32 Train Loss 252.78278 Test MSE 279.7623005721998 Test RE 0.2815683046776713 Lambda1 0.0047984975\n",
      "33 Train Loss 252.55446 Test MSE 279.79106331560996 Test RE 0.281582778516239 Lambda1 0.00497778\n",
      "34 Train Loss 252.0293 Test MSE 279.42485779673643 Test RE 0.28139844287358773 Lambda1 0.005919196\n",
      "35 Train Loss 251.8456 Test MSE 279.00295231347604 Test RE 0.28118591991445907 Lambda1 0.006690713\n",
      "36 Train Loss 251.25174 Test MSE 277.47718479005607 Test RE 0.28041601318833004 Lambda1 0.009413058\n",
      "37 Train Loss 250.54222 Test MSE 276.44595635766876 Test RE 0.279894452938033 Lambda1 0.011323705\n",
      "38 Train Loss 249.78513 Test MSE 275.194311557679 Test RE 0.2792601049433173 Lambda1 0.014121443\n",
      "39 Train Loss 248.13432 Test MSE 272.4551835003049 Test RE 0.27786683097931925 Lambda1 0.020852204\n",
      "40 Train Loss 246.18663 Test MSE 269.57952833209293 Test RE 0.27639655467577895 Lambda1 0.029018726\n",
      "41 Train Loss 243.35014 Test MSE 265.2932677019924 Test RE 0.27419042529108845 Lambda1 0.039554294\n",
      "42 Train Loss 239.85489 Test MSE 261.37480917561595 Test RE 0.2721579562698157 Lambda1 0.049028836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 Train Loss 237.91985 Test MSE 258.7709312312641 Test RE 0.27079891190812455 Lambda1 0.056419745\n",
      "44 Train Loss 234.96924 Test MSE 254.69607591763724 Test RE 0.2686583218056362 Lambda1 0.06636555\n",
      "45 Train Loss 232.13159 Test MSE 251.42969815919855 Test RE 0.2669300437461893 Lambda1 0.07523245\n",
      "46 Train Loss 227.5518 Test MSE 247.5314934206661 Test RE 0.2648526981800637 Lambda1 0.08956622\n",
      "47 Train Loss 223.69849 Test MSE 240.77572571174684 Test RE 0.26121344139479535 Lambda1 0.10941716\n",
      "48 Train Loss 220.91333 Test MSE 239.65732836924806 Test RE 0.2606060702524659 Lambda1 0.12481107\n",
      "49 Train Loss 218.26765 Test MSE 238.27002891522505 Test RE 0.2598506930037613 Lambda1 0.14396662\n",
      "50 Train Loss 215.06711 Test MSE 234.70438602055566 Test RE 0.25789906842272026 Lambda1 0.15482189\n",
      "51 Train Loss 211.70091 Test MSE 232.39964063464151 Test RE 0.25662968844705375 Lambda1 0.16446397\n",
      "52 Train Loss 209.27728 Test MSE 229.53999154844368 Test RE 0.25504590286773793 Lambda1 0.1733394\n",
      "53 Train Loss 203.15274 Test MSE 221.35351100755696 Test RE 0.25045654086420926 Lambda1 0.2189618\n",
      "54 Train Loss 199.45485 Test MSE 219.05932257528266 Test RE 0.24915524889777194 Lambda1 0.2301869\n",
      "55 Train Loss 191.54619 Test MSE 207.5697373948845 Test RE 0.2425331953394311 Lambda1 0.26650435\n",
      "56 Train Loss 184.19719 Test MSE 201.56090932350824 Test RE 0.23899693153973017 Lambda1 0.27302152\n",
      "57 Train Loss 170.6727 Test MSE 181.76071196015388 Test RE 0.22695469856387465 Lambda1 0.31846392\n",
      "58 Train Loss 163.0376 Test MSE 168.77005092915326 Test RE 0.21869399661707495 Lambda1 0.33479065\n",
      "59 Train Loss 151.28868 Test MSE 154.06641955325054 Test RE 0.20895038012433553 Lambda1 0.34099987\n",
      "60 Train Loss 145.88295 Test MSE 147.80325041415546 Test RE 0.204659148606708 Lambda1 0.35096112\n",
      "61 Train Loss 139.55392 Test MSE 140.87072313858388 Test RE 0.1998018672577311 Lambda1 0.37280288\n",
      "62 Train Loss 133.11179 Test MSE 132.95198617172983 Test RE 0.19410493868903264 Lambda1 0.40398\n",
      "63 Train Loss 129.42126 Test MSE 124.00059967865168 Test RE 0.18745674141588387 Lambda1 0.43766126\n",
      "64 Train Loss 125.580154 Test MSE 121.75695595935574 Test RE 0.18575309622262798 Lambda1 0.44782206\n",
      "65 Train Loss 121.30549 Test MSE 116.11984545131402 Test RE 0.18140213543793324 Lambda1 0.46964827\n",
      "66 Train Loss 117.895676 Test MSE 115.17976369332007 Test RE 0.180666346514747 Lambda1 0.47505862\n",
      "67 Train Loss 113.75624 Test MSE 110.30336194683635 Test RE 0.1768005230095918 Lambda1 0.50319093\n",
      "68 Train Loss 109.06325 Test MSE 107.09060282502405 Test RE 0.1742066993103303 Lambda1 0.5230172\n",
      "69 Train Loss 105.988495 Test MSE 104.91783450096734 Test RE 0.1724303976422901 Lambda1 0.53932154\n",
      "70 Train Loss 103.68856 Test MSE 103.25004566407749 Test RE 0.17105441846985875 Lambda1 0.5427891\n",
      "71 Train Loss 101.12729 Test MSE 99.18740745363982 Test RE 0.16765535905083562 Lambda1 0.55203223\n",
      "72 Train Loss 99.47423 Test MSE 95.80495462317715 Test RE 0.16477190218846774 Lambda1 0.57066715\n",
      "73 Train Loss 97.884415 Test MSE 93.9524926105191 Test RE 0.16317113105943457 Lambda1 0.5769239\n",
      "74 Train Loss 96.1087 Test MSE 92.09575977039324 Test RE 0.16155075357004886 Lambda1 0.5773309\n",
      "Training time: 151.91\n",
      "Training time: 151.91\n",
      "inv_HT_stan_tune7\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 2306.2886 Test MSE 2161.444339394215 Test RE 0.7826384830640292 Lambda1 -0.0120763015\n",
      "1 Train Loss 837.42957 Test MSE 854.8741249346663 Test RE 0.4921984670114553 Lambda1 -0.022915078\n",
      "2 Train Loss 828.50714 Test MSE 844.3804025024194 Test RE 0.48916823000373444 Lambda1 -0.022241343\n",
      "3 Train Loss 813.2201 Test MSE 829.3054737085631 Test RE 0.48478194475944464 Lambda1 -0.018842332\n",
      "4 Train Loss 775.5934 Test MSE 791.5033160057267 Test RE 0.47360419437865975 Lambda1 0.0018506077\n",
      "5 Train Loss 730.2066 Test MSE 738.4679655117345 Test RE 0.45746197527788873 Lambda1 0.013296277\n",
      "6 Train Loss 555.28705 Test MSE 561.886787416413 Test RE 0.3990373802541165 Lambda1 0.007617627\n",
      "7 Train Loss 415.39203 Test MSE 402.2134063407804 Test RE 0.33761166341016974 Lambda1 -0.0011426175\n",
      "8 Train Loss 351.24164 Test MSE 360.30091294625225 Test RE 0.3195375138890983 Lambda1 -0.0021599494\n",
      "9 Train Loss 301.08643 Test MSE 310.8990104301612 Test RE 0.29682388186406206 Lambda1 -0.0064239507\n",
      "10 Train Loss 278.34164 Test MSE 292.7076569210682 Test RE 0.2880091014738122 Lambda1 -0.00540915\n",
      "11 Train Loss 265.98215 Test MSE 282.61274241611164 Test RE 0.2829990908441362 Lambda1 0.0015339189\n",
      "12 Train Loss 260.1529 Test MSE 279.1064545247474 Test RE 0.2812380710935367 Lambda1 0.0050933436\n",
      "13 Train Loss 254.12332 Test MSE 271.4289039075312 Test RE 0.277343005181732 Lambda1 0.017071204\n",
      "14 Train Loss 250.17058 Test MSE 265.4915166165912 Test RE 0.27429285495937394 Lambda1 0.029934391\n",
      "15 Train Loss 243.96616 Test MSE 259.132617799976 Test RE 0.2709880949208033 Lambda1 0.042523302\n",
      "16 Train Loss 236.31099 Test MSE 250.11560123862262 Test RE 0.2662315751882256 Lambda1 0.06192405\n",
      "17 Train Loss 225.91107 Test MSE 236.06946158880183 Test RE 0.2586479707127704 Lambda1 0.09294956\n",
      "18 Train Loss 216.33153 Test MSE 226.29266029840014 Test RE 0.25323539338281514 Lambda1 0.12058819\n",
      "19 Train Loss 207.51588 Test MSE 214.61776641995678 Test RE 0.2466164296338014 Lambda1 0.15296532\n",
      "20 Train Loss 197.5678 Test MSE 207.55481722823296 Test RE 0.24252447850824008 Lambda1 0.17926922\n",
      "21 Train Loss 191.04436 Test MSE 202.65868794418319 Test RE 0.23964688260408096 Lambda1 0.19527183\n",
      "22 Train Loss 177.93529 Test MSE 189.08119503287384 Test RE 0.23147992867171463 Lambda1 0.23132224\n",
      "23 Train Loss 163.03426 Test MSE 172.920986845039 Test RE 0.2213670733117884 Lambda1 0.2693939\n",
      "24 Train Loss 151.61374 Test MSE 160.4002754795559 Test RE 0.21320222147849302 Lambda1 0.2962958\n",
      "25 Train Loss 138.64085 Test MSE 141.05273177159594 Test RE 0.1999309001936003 Lambda1 0.33456627\n",
      "26 Train Loss 125.539665 Test MSE 121.42752993789273 Test RE 0.18550163892672986 Lambda1 0.36825085\n",
      "27 Train Loss 115.223015 Test MSE 114.5405691389664 Test RE 0.18016434163236333 Lambda1 0.3783485\n",
      "28 Train Loss 104.36547 Test MSE 100.60383439191342 Test RE 0.1688482008689049 Lambda1 0.40290567\n",
      "29 Train Loss 98.792305 Test MSE 93.95077026244444 Test RE 0.16316963541645785 Lambda1 0.4108406\n",
      "30 Train Loss 90.769714 Test MSE 82.94264209165507 Test RE 0.15331268985517577 Lambda1 0.43355858\n",
      "31 Train Loss 85.53467 Test MSE 76.29077896501592 Test RE 0.14703651316340388 Lambda1 0.44911477\n",
      "32 Train Loss 79.04136 Test MSE 72.21984188770374 Test RE 0.14305974193055523 Lambda1 0.46219084\n",
      "33 Train Loss 73.372536 Test MSE 69.01067668226528 Test RE 0.1398451192385353 Lambda1 0.47002712\n",
      "34 Train Loss 67.62379 Test MSE 64.21639329724097 Test RE 0.13490005463166044 Lambda1 0.4872544\n",
      "35 Train Loss 64.068375 Test MSE 60.07108674704422 Test RE 0.13047338055170213 Lambda1 0.49877357\n",
      "36 Train Loss 62.032642 Test MSE 58.118109744857996 Test RE 0.1283349397297925 Lambda1 0.5074153\n",
      "37 Train Loss 57.51814 Test MSE 53.50584066417105 Test RE 0.12313733966573932 Lambda1 0.52696353\n",
      "38 Train Loss 55.070305 Test MSE 52.08275056429128 Test RE 0.12148876760942255 Lambda1 0.5356574\n",
      "39 Train Loss 53.724274 Test MSE 51.34562141212854 Test RE 0.12062598640984143 Lambda1 0.5419447\n",
      "40 Train Loss 51.48223 Test MSE 49.79721795721468 Test RE 0.11879323543998287 Lambda1 0.55792177\n",
      "41 Train Loss 50.662975 Test MSE 49.676930029840136 Test RE 0.11864967288462958 Lambda1 0.5613803\n",
      "42 Train Loss 49.32667 Test MSE 48.899634057673396 Test RE 0.11771775611425413 Lambda1 0.5647377\n",
      "43 Train Loss 48.230274 Test MSE 48.01340248069319 Test RE 0.116646150865609 Lambda1 0.5737723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 Train Loss 47.472282 Test MSE 47.321368600741145 Test RE 0.11580246897711415 Lambda1 0.5815406\n",
      "45 Train Loss 46.311966 Test MSE 46.248451362258635 Test RE 0.11448214765452343 Lambda1 0.59152883\n",
      "46 Train Loss 45.24343 Test MSE 44.42028130049218 Test RE 0.11219663270350658 Lambda1 0.61378247\n",
      "47 Train Loss 44.603577 Test MSE 43.70834861295572 Test RE 0.1112939021537144 Lambda1 0.6259232\n",
      "48 Train Loss 43.332 Test MSE 43.25193571947623 Test RE 0.11071129864305979 Lambda1 0.63740015\n",
      "49 Train Loss 42.312256 Test MSE 42.997374922882116 Test RE 0.11038502022294205 Lambda1 0.64631623\n",
      "50 Train Loss 41.66889 Test MSE 42.400996496566776 Test RE 0.10961682084571471 Lambda1 0.6607732\n",
      "51 Train Loss 41.015175 Test MSE 41.64840320419475 Test RE 0.10863964761577033 Lambda1 0.68070984\n",
      "52 Train Loss 40.519043 Test MSE 41.16636183447462 Test RE 0.10800911662530006 Lambda1 0.69699275\n",
      "53 Train Loss 40.027664 Test MSE 40.96609046888582 Test RE 0.10774606804654671 Lambda1 0.7109375\n",
      "54 Train Loss 39.695835 Test MSE 40.98288690784501 Test RE 0.1077681541764308 Lambda1 0.721157\n",
      "55 Train Loss 39.389683 Test MSE 41.09712438795547 Test RE 0.10791824847268984 Lambda1 0.7256261\n",
      "56 Train Loss 38.855835 Test MSE 40.529081183599274 Test RE 0.10716983195098678 Lambda1 0.72954625\n",
      "57 Train Loss 38.420197 Test MSE 40.158665612277204 Test RE 0.10667896843038573 Lambda1 0.7420345\n",
      "58 Train Loss 37.711918 Test MSE 39.126755215490384 Test RE 0.10529944624390562 Lambda1 0.76722807\n",
      "59 Train Loss 37.15494 Test MSE 38.52397760309981 Test RE 0.10448518872279768 Lambda1 0.77877307\n",
      "60 Train Loss 36.851906 Test MSE 38.22671243118669 Test RE 0.10408128498491834 Lambda1 0.7847036\n",
      "61 Train Loss 36.31067 Test MSE 37.68965843142401 Test RE 0.1033475704447136 Lambda1 0.79916775\n",
      "62 Train Loss 35.915146 Test MSE 37.37479662802915 Test RE 0.10291497898781377 Lambda1 0.8108253\n",
      "63 Train Loss 35.6656 Test MSE 37.22639883499029 Test RE 0.10271046221255575 Lambda1 0.817021\n",
      "64 Train Loss 35.320267 Test MSE 37.35616250452391 Test RE 0.10288932039305801 Lambda1 0.81917715\n",
      "65 Train Loss 34.6693 Test MSE 36.92830336506676 Test RE 0.10229840162380369 Lambda1 0.8511664\n",
      "66 Train Loss 34.523087 Test MSE 36.67615201202112 Test RE 0.10194854986542254 Lambda1 0.8643627\n",
      "67 Train Loss 34.303238 Test MSE 36.32842149698713 Test RE 0.10146410631969077 Lambda1 0.8798107\n",
      "68 Train Loss 33.91736 Test MSE 36.21086781107971 Test RE 0.10129981148179532 Lambda1 0.886474\n",
      "69 Train Loss 33.449074 Test MSE 35.78425832672986 Test RE 0.1007013240469484 Lambda1 0.91523385\n",
      "70 Train Loss 33.24373 Test MSE 35.541284252680015 Test RE 0.10035886219847576 Lambda1 0.9220198\n",
      "71 Train Loss 33.08 Test MSE 35.31371533517663 Test RE 0.10003705005610757 Lambda1 0.93209785\n",
      "72 Train Loss 32.81821 Test MSE 35.12863409651131 Test RE 0.09977455566453412 Lambda1 0.9445594\n",
      "73 Train Loss 32.697178 Test MSE 35.0340549786149 Test RE 0.09964015035732907 Lambda1 0.94928175\n",
      "74 Train Loss 32.56793 Test MSE 34.98826296941929 Test RE 0.09957501067237969 Lambda1 0.95096207\n",
      "Training time: 152.78\n",
      "Training time: 152.78\n",
      "inv_HT_stan_tune7\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 1029.1453 Test MSE 846.4560687840487 Test RE 0.4897691006405723 Lambda1 0.060895998\n",
      "1 Train Loss 839.87384 Test MSE 852.9500982853532 Test RE 0.4916442705605786 Lambda1 0.05543554\n",
      "2 Train Loss 791.9087 Test MSE 811.306349774572 Test RE 0.4794922677330269 Lambda1 0.02473404\n",
      "3 Train Loss 713.98395 Test MSE 726.7034632499964 Test RE 0.4538034419992121 Lambda1 0.020030586\n",
      "4 Train Loss 595.582 Test MSE 608.82297028337 Test RE 0.41536957812224745 Lambda1 0.016015772\n",
      "5 Train Loss 421.3455 Test MSE 417.49985682366423 Test RE 0.34396744152506237 Lambda1 0.0023965253\n",
      "6 Train Loss 338.92587 Test MSE 353.1039089903546 Test RE 0.31633003786197944 Lambda1 0.0054102363\n",
      "7 Train Loss 296.12717 Test MSE 307.59780380953566 Test RE 0.29524379961815794 Lambda1 -0.00060689053\n",
      "8 Train Loss 269.6665 Test MSE 285.47071825083657 Test RE 0.28442643278259594 Lambda1 0.0010737436\n",
      "9 Train Loss 261.52512 Test MSE 281.13370207491516 Test RE 0.2822575882782332 Lambda1 0.0017352488\n",
      "10 Train Loss 259.62708 Test MSE 280.3910240078802 Test RE 0.2818845181951756 Lambda1 0.0016511735\n",
      "11 Train Loss 257.70874 Test MSE 279.36623302721637 Test RE 0.28136892190619384 Lambda1 0.0017742139\n",
      "12 Train Loss 256.43945 Test MSE 279.7570829960139 Test RE 0.2815656790363179 Lambda1 0.0014721352\n",
      "13 Train Loss 254.85373 Test MSE 279.23294274014745 Test RE 0.28130179099625957 Lambda1 0.002241175\n",
      "14 Train Loss 254.34166 Test MSE 279.191409797326 Test RE 0.2812808698872383 Lambda1 0.0026178523\n",
      "15 Train Loss 253.31242 Test MSE 278.1815507206211 Test RE 0.2807717006272143 Lambda1 0.0053280825\n",
      "16 Train Loss 252.4385 Test MSE 277.47973461776826 Test RE 0.2804173016021492 Lambda1 0.006930028\n",
      "17 Train Loss 251.83997 Test MSE 275.94481235235247 Test RE 0.27964064011997747 Lambda1 0.009576935\n",
      "18 Train Loss 250.30989 Test MSE 273.07723101447044 Test RE 0.2781838516073155 Lambda1 0.015167161\n",
      "19 Train Loss 247.04213 Test MSE 268.81142297280115 Test RE 0.276002509329729 Lambda1 0.02282665\n",
      "20 Train Loss 244.61476 Test MSE 265.9756513101348 Test RE 0.2745428331977765 Lambda1 0.028834779\n",
      "21 Train Loss 240.63794 Test MSE 260.42683126618107 Test RE 0.27166396434564566 Lambda1 0.039740704\n",
      "22 Train Loss 235.67441 Test MSE 253.29584872907978 Test RE 0.2679188107225112 Lambda1 0.052165188\n",
      "23 Train Loss 232.56534 Test MSE 250.3234994073024 Test RE 0.2663421991552649 Lambda1 0.058685444\n",
      "24 Train Loss 228.09424 Test MSE 244.86776917602126 Test RE 0.263423783400845 Lambda1 0.07072978\n",
      "25 Train Loss 222.58879 Test MSE 237.75040556292265 Test RE 0.2595671949383084 Lambda1 0.08796627\n",
      "26 Train Loss 217.4131 Test MSE 228.86643998115613 Test RE 0.2546714304885954 Lambda1 0.10646723\n",
      "27 Train Loss 211.04579 Test MSE 222.15516020016707 Test RE 0.25090965504271734 Lambda1 0.12254081\n",
      "28 Train Loss 206.98705 Test MSE 219.24688780408027 Test RE 0.2492618932109968 Lambda1 0.13051651\n",
      "29 Train Loss 200.49422 Test MSE 210.53892125464407 Test RE 0.24426169548691076 Lambda1 0.14701208\n",
      "30 Train Loss 194.39978 Test MSE 198.73361525113356 Test RE 0.23731480741725264 Lambda1 0.17250963\n",
      "31 Train Loss 183.25432 Test MSE 189.55976907890067 Test RE 0.23177268723230515 Lambda1 0.19156699\n",
      "32 Train Loss 177.12192 Test MSE 184.41903375392357 Test RE 0.22860832507477946 Lambda1 0.20402874\n",
      "33 Train Loss 173.29288 Test MSE 181.47724641646067 Test RE 0.22677765551942206 Lambda1 0.210361\n",
      "34 Train Loss 166.2929 Test MSE 169.85468392733148 Test RE 0.21939561054827003 Lambda1 0.23688985\n",
      "35 Train Loss 161.95428 Test MSE 164.3124748744022 Test RE 0.2157865835812445 Lambda1 0.24968742\n",
      "36 Train Loss 157.77489 Test MSE 157.99155035969827 Test RE 0.2115953410285593 Lambda1 0.264012\n",
      "37 Train Loss 152.71747 Test MSE 148.1588427602459 Test RE 0.2049051902452238 Lambda1 0.28872716\n",
      "38 Train Loss 146.32756 Test MSE 143.62776674041433 Test RE 0.2017475988160377 Lambda1 0.30112693\n",
      "39 Train Loss 139.6986 Test MSE 136.8528529226962 Test RE 0.19693191240000862 Lambda1 0.3189593\n",
      "40 Train Loss 135.5812 Test MSE 133.51229910854238 Test RE 0.19451352664310317 Lambda1 0.3294782\n",
      "41 Train Loss 131.88774 Test MSE 126.0706570371303 Test RE 0.18901496001490656 Lambda1 0.3524794\n",
      "42 Train Loss 128.53725 Test MSE 122.67003323883182 Test RE 0.1864482932195377 Lambda1 0.36548594\n",
      "43 Train Loss 124.37134 Test MSE 118.38727537241185 Test RE 0.18316465970055498 Lambda1 0.38340786\n",
      "44 Train Loss 119.79891 Test MSE 114.33725012079077 Test RE 0.18000436727801802 Lambda1 0.400503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 Train Loss 114.770775 Test MSE 110.08075394783418 Test RE 0.17662202850525063 Lambda1 0.41727245\n",
      "46 Train Loss 110.0768 Test MSE 104.90373510878857 Test RE 0.17241881121613578 Lambda1 0.43524808\n",
      "47 Train Loss 105.47702 Test MSE 98.73275339133508 Test RE 0.16727066938698737 Lambda1 0.4536074\n",
      "48 Train Loss 100.098404 Test MSE 91.11178550225384 Test RE 0.16068541153180135 Lambda1 0.47420865\n",
      "49 Train Loss 96.68495 Test MSE 85.79576514677962 Test RE 0.15592727762586134 Lambda1 0.4891282\n",
      "50 Train Loss 92.74321 Test MSE 83.53932289445221 Test RE 0.15386315945403872 Lambda1 0.49798155\n",
      "51 Train Loss 88.81386 Test MSE 82.04002176791207 Test RE 0.15247619793581005 Lambda1 0.5033696\n",
      "52 Train Loss 84.33652 Test MSE 77.64937798130005 Test RE 0.14833996123713813 Lambda1 0.51551527\n",
      "53 Train Loss 81.51242 Test MSE 75.50523166604985 Test RE 0.14627755508610885 Lambda1 0.52108395\n",
      "54 Train Loss 78.48384 Test MSE 72.91324476051808 Test RE 0.1437448795789634 Lambda1 0.5279344\n",
      "55 Train Loss 76.48776 Test MSE 72.1025955018502 Test RE 0.14294356851832787 Lambda1 0.5294634\n",
      "56 Train Loss 73.42586 Test MSE 68.86726381055354 Test RE 0.1396997357887541 Lambda1 0.5408285\n",
      "57 Train Loss 70.85122 Test MSE 66.96208566636268 Test RE 0.13775382182692117 Lambda1 0.5492792\n",
      "58 Train Loss 68.71335 Test MSE 65.64972814373829 Test RE 0.1363972570169784 Lambda1 0.55333036\n",
      "59 Train Loss 66.740524 Test MSE 64.2934007578335 Test RE 0.13498091558936978 Lambda1 0.55678993\n",
      "60 Train Loss 64.75914 Test MSE 62.48613729349384 Test RE 0.13307026139635997 Lambda1 0.5633999\n",
      "61 Train Loss 63.377655 Test MSE 60.47247061589069 Test RE 0.13090855429869552 Lambda1 0.5713493\n",
      "62 Train Loss 61.620083 Test MSE 57.24526996580651 Test RE 0.12736760267135666 Lambda1 0.5828572\n",
      "63 Train Loss 60.24561 Test MSE 54.99983932113847 Test RE 0.12484463414462244 Lambda1 0.5912186\n",
      "64 Train Loss 58.441044 Test MSE 52.0448355139514 Test RE 0.12144453903751877 Lambda1 0.6033905\n",
      "65 Train Loss 56.14015 Test MSE 50.190162651431315 Test RE 0.11926100703338714 Lambda1 0.6131102\n",
      "66 Train Loss 55.2804 Test MSE 49.80686164272593 Test RE 0.11880473757997798 Lambda1 0.6160081\n",
      "67 Train Loss 54.365967 Test MSE 48.910624553973356 Test RE 0.11773098426922943 Lambda1 0.62008035\n",
      "68 Train Loss 53.29397 Test MSE 47.58304585664033 Test RE 0.11612220926038998 Lambda1 0.6283025\n",
      "69 Train Loss 52.558556 Test MSE 47.382320211363435 Test RE 0.11587702383276119 Lambda1 0.630864\n",
      "70 Train Loss 51.796196 Test MSE 47.40645528742054 Test RE 0.1159065321464491 Lambda1 0.6338087\n",
      "71 Train Loss 51.22832 Test MSE 46.92858225658455 Test RE 0.11532086404056305 Lambda1 0.63976115\n",
      "72 Train Loss 50.46139 Test MSE 46.04160511837847 Test RE 0.11422585000191571 Lambda1 0.6486612\n",
      "73 Train Loss 49.815178 Test MSE 45.45807673773043 Test RE 0.11349969627265678 Lambda1 0.6539283\n",
      "74 Train Loss 49.06361 Test MSE 44.971807011158845 Test RE 0.1128910051203693 Lambda1 0.6601205\n",
      "Training time: 149.57\n",
      "Training time: 149.57\n",
      "inv_HT_stan_tune7\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 2192.6062 Test MSE 2041.403441237812 Test RE 0.7605952202828605 Lambda1 0.07319629\n",
      "1 Train Loss 790.61896 Test MSE 804.1909832095746 Test RE 0.4773850016266697 Lambda1 0.027152516\n",
      "2 Train Loss 663.7913 Test MSE 670.8923003163654 Test RE 0.4360292003625444 Lambda1 0.005988944\n",
      "3 Train Loss 434.89548 Test MSE 409.6543295635195 Test RE 0.3407202496134558 Lambda1 0.00898571\n",
      "4 Train Loss 322.07135 Test MSE 329.376423730576 Test RE 0.30551703142158254 Lambda1 0.0018241955\n",
      "5 Train Loss 280.9366 Test MSE 295.30502763228185 Test RE 0.28928411802851045 Lambda1 1.9314553e-05\n",
      "6 Train Loss 263.69476 Test MSE 283.7738418317127 Test RE 0.283579838316803 Lambda1 -0.00019395108\n",
      "7 Train Loss 259.01904 Test MSE 283.30326696331815 Test RE 0.2833446141811316 Lambda1 0.00012924137\n",
      "8 Train Loss 256.98227 Test MSE 281.23324365671954 Test RE 0.2823075536150318 Lambda1 0.00025615332\n",
      "9 Train Loss 255.53725 Test MSE 280.47195108678216 Test RE 0.28192519432763097 Lambda1 0.00071536505\n",
      "10 Train Loss 254.89452 Test MSE 280.37875870194574 Test RE 0.28187835280939577 Lambda1 0.0010436364\n",
      "11 Train Loss 254.3987 Test MSE 279.94109545398317 Test RE 0.28165826485229273 Lambda1 0.001645491\n",
      "12 Train Loss 254.11989 Test MSE 280.0390436522387 Test RE 0.28170753505102636 Lambda1 0.001580222\n",
      "13 Train Loss 253.59164 Test MSE 280.23364930656413 Test RE 0.2818054006156452 Lambda1 0.0021201696\n",
      "14 Train Loss 253.2959 Test MSE 280.6954679185138 Test RE 0.28203750943849726 Lambda1 0.0022387193\n",
      "15 Train Loss 252.91888 Test MSE 280.0343469594629 Test RE 0.28170517270312817 Lambda1 0.0036728496\n",
      "16 Train Loss 252.50531 Test MSE 279.2020899529231 Test RE 0.28128624987856715 Lambda1 0.0049954606\n",
      "17 Train Loss 251.978 Test MSE 278.44766378027606 Test RE 0.280905963945152 Lambda1 0.0061667087\n",
      "18 Train Loss 251.38838 Test MSE 277.1164499363561 Test RE 0.28023367616497297 Lambda1 0.0077193435\n",
      "19 Train Loss 250.12099 Test MSE 273.370723069845 Test RE 0.2783333016818636 Lambda1 0.014211875\n",
      "20 Train Loss 248.53 Test MSE 271.28357169124513 Test RE 0.27726874582708144 Lambda1 0.018389888\n",
      "21 Train Loss 246.72327 Test MSE 267.96702932024596 Test RE 0.27556867718262745 Lambda1 0.024498828\n",
      "22 Train Loss 244.12866 Test MSE 263.5457288957602 Test RE 0.2732858601283212 Lambda1 0.032833107\n",
      "23 Train Loss 240.12671 Test MSE 258.8174515384754 Test RE 0.27082325212745034 Lambda1 0.042875372\n",
      "24 Train Loss 237.99945 Test MSE 256.2255634487389 Test RE 0.26946378082981653 Lambda1 0.047726624\n",
      "25 Train Loss 235.94543 Test MSE 251.5068082454927 Test RE 0.2669709725246283 Lambda1 0.05610612\n",
      "26 Train Loss 232.83736 Test MSE 246.20850584020144 Test RE 0.26414396759696684 Lambda1 0.06589131\n",
      "27 Train Loss 227.94873 Test MSE 239.25377180434504 Test RE 0.2603865618361939 Lambda1 0.078111425\n",
      "28 Train Loss 222.73528 Test MSE 234.60113215932833 Test RE 0.2578423332153814 Lambda1 0.08570396\n",
      "29 Train Loss 216.208 Test MSE 226.9037158939303 Test RE 0.2535770672652479 Lambda1 0.09983554\n",
      "30 Train Loss 210.71878 Test MSE 219.37197057849076 Test RE 0.2493329864060629 Lambda1 0.11416355\n",
      "31 Train Loss 206.67274 Test MSE 213.08055690782675 Test RE 0.24573164175698023 Lambda1 0.12621851\n",
      "32 Train Loss 201.74628 Test MSE 204.70713733851727 Test RE 0.2408549981943617 Lambda1 0.14357424\n",
      "33 Train Loss 196.5608 Test MSE 200.18857499506976 Test RE 0.23818193255177816 Lambda1 0.1539971\n",
      "34 Train Loss 192.92876 Test MSE 197.88644100776074 Test RE 0.23680844690792768 Lambda1 0.16087754\n",
      "35 Train Loss 185.52174 Test MSE 185.31262065106336 Test RE 0.22916150697887244 Lambda1 0.18850292\n",
      "36 Train Loss 180.88892 Test MSE 175.85824338000415 Test RE 0.22323924034852832 Lambda1 0.2075817\n",
      "37 Train Loss 174.36287 Test MSE 170.45219353739705 Test RE 0.21978116335743458 Lambda1 0.22092102\n",
      "38 Train Loss 167.51524 Test MSE 164.7685255373571 Test RE 0.21608583482492685 Lambda1 0.23328961\n",
      "39 Train Loss 162.09668 Test MSE 160.700182862776 Test RE 0.2134014451387306 Lambda1 0.24033743\n",
      "40 Train Loss 157.54404 Test MSE 152.80868035041186 Test RE 0.20809573689369254 Lambda1 0.25347853\n",
      "41 Train Loss 153.33157 Test MSE 149.1647341244037 Test RE 0.2055995926231279 Lambda1 0.2591487\n",
      "42 Train Loss 148.27715 Test MSE 143.15553334406215 Test RE 0.20141566302092115 Lambda1 0.27009484\n",
      "43 Train Loss 142.73227 Test MSE 135.7231183147656 Test RE 0.19611738114382726 Lambda1 0.2858311\n",
      "44 Train Loss 137.84438 Test MSE 128.85835503659834 Test RE 0.19109330078244038 Lambda1 0.30116028\n",
      "45 Train Loss 134.19812 Test MSE 122.18195169496424 Test RE 0.18607700175997946 Lambda1 0.31449702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 Train Loss 131.51445 Test MSE 118.51810133787396 Test RE 0.1832658364398777 Lambda1 0.32093316\n",
      "47 Train Loss 128.2274 Test MSE 116.89401306754017 Test RE 0.18200583227007358 Lambda1 0.32613936\n",
      "48 Train Loss 123.81836 Test MSE 114.45403714763066 Test RE 0.18009627437496956 Lambda1 0.33687732\n",
      "49 Train Loss 116.73996 Test MSE 100.64101284554594 Test RE 0.16887939717063297 Lambda1 0.36891153\n",
      "50 Train Loss 113.91775 Test MSE 93.70750723664194 Test RE 0.1629582541451397 Lambda1 0.3866488\n",
      "51 Train Loss 107.20653 Test MSE 92.91767608145302 Test RE 0.162270038910483 Lambda1 0.40326872\n",
      "52 Train Loss 99.103966 Test MSE 88.46466060366522 Test RE 0.15833396170386405 Lambda1 0.42348477\n",
      "53 Train Loss 93.6642 Test MSE 85.23907052248316 Test RE 0.1554205792781033 Lambda1 0.43997228\n",
      "54 Train Loss 86.39573 Test MSE 78.95305598481981 Test RE 0.1495800392836049 Lambda1 0.47047815\n",
      "55 Train Loss 81.09205 Test MSE 73.65891796335556 Test RE 0.14447803890953803 Lambda1 0.48939073\n",
      "56 Train Loss 77.36749 Test MSE 70.32982482467678 Test RE 0.14117537124925672 Lambda1 0.5038394\n",
      "57 Train Loss 72.32765 Test MSE 63.09883459032105 Test RE 0.13372106896185357 Lambda1 0.5314434\n",
      "58 Train Loss 67.67719 Test MSE 60.9096253661322 Test RE 0.13138087041266636 Lambda1 0.5448372\n",
      "59 Train Loss 63.95352 Test MSE 59.61244926935551 Test RE 0.1299743498051567 Lambda1 0.55540466\n",
      "60 Train Loss 62.527855 Test MSE 58.43580700237798 Test RE 0.12868522720235887 Lambda1 0.56331277\n",
      "61 Train Loss 61.589672 Test MSE 56.72590667926826 Test RE 0.1267885087176418 Lambda1 0.57421714\n",
      "62 Train Loss 60.93976 Test MSE 55.90692753341917 Test RE 0.1258699278830701 Lambda1 0.577875\n",
      "63 Train Loss 60.01024 Test MSE 54.661939629788094 Test RE 0.12446054251760211 Lambda1 0.5853432\n",
      "64 Train Loss 58.38479 Test MSE 53.676245612335045 Test RE 0.12333326717089171 Lambda1 0.5908454\n",
      "65 Train Loss 56.88664 Test MSE 51.43861815827887 Test RE 0.12073517536419626 Lambda1 0.6088297\n",
      "66 Train Loss 56.489044 Test MSE 51.04897144229173 Test RE 0.12027702257146185 Lambda1 0.6108893\n",
      "67 Train Loss 55.6065 Test MSE 50.91082014806123 Test RE 0.12011416244759918 Lambda1 0.61017585\n",
      "68 Train Loss 54.16342 Test MSE 50.30062221133894 Test RE 0.11939217096553825 Lambda1 0.61169094\n",
      "69 Train Loss 53.23066 Test MSE 50.237922497722536 Test RE 0.11931773660635425 Lambda1 0.6121407\n",
      "70 Train Loss 52.28087 Test MSE 49.79029627159066 Test RE 0.11878497917549326 Lambda1 0.61836356\n",
      "71 Train Loss 51.185303 Test MSE 49.40679909698992 Test RE 0.11832663926559456 Lambda1 0.6243627\n",
      "72 Train Loss 50.72131 Test MSE 49.06664876700669 Test RE 0.11791861485598888 Lambda1 0.6258692\n",
      "73 Train Loss 49.786423 Test MSE 47.80734622881623 Test RE 0.11639558007707404 Lambda1 0.64045227\n",
      "74 Train Loss 48.90849 Test MSE 46.92563252727492 Test RE 0.11531723969627415 Lambda1 0.653682\n",
      "Training time: 148.41\n",
      "Training time: 148.41\n",
      "inv_HT_stan_tune7\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 2469.2046 Test MSE 2331.6056018115764 Test RE 0.812861801995028 Lambda1 0.079731375\n",
      "1 Train Loss 858.6306 Test MSE 864.1521986567043 Test RE 0.494862209939358 Lambda1 0.16870904\n",
      "2 Train Loss 843.03845 Test MSE 856.7103486384524 Test RE 0.4927267913957473 Lambda1 0.16765957\n",
      "3 Train Loss 825.9456 Test MSE 842.2411340519674 Test RE 0.48854817427307534 Lambda1 0.17711157\n",
      "4 Train Loss 811.6634 Test MSE 827.175606005983 Test RE 0.48415902278626216 Lambda1 0.18328513\n",
      "5 Train Loss 798.8572 Test MSE 817.8200060070823 Test RE 0.4814132461708336 Lambda1 0.18548577\n",
      "6 Train Loss 781.5412 Test MSE 804.0583928136183 Test RE 0.47734564575416927 Lambda1 0.18607993\n",
      "7 Train Loss 755.1753 Test MSE 776.0428857934257 Test RE 0.4689559296569447 Lambda1 0.18523823\n",
      "8 Train Loss 722.1641 Test MSE 735.0238944760054 Test RE 0.45639397172046237 Lambda1 0.18280618\n",
      "9 Train Loss 664.54144 Test MSE 664.8207505084397 Test RE 0.43405169225872214 Lambda1 0.17587082\n",
      "10 Train Loss 600.118 Test MSE 595.9090894660811 Test RE 0.41094071813429045 Lambda1 0.16560227\n",
      "11 Train Loss 480.9802 Test MSE 474.19014505540076 Test RE 0.36657718560206176 Lambda1 0.15222374\n",
      "12 Train Loss 364.1754 Test MSE 319.9410262565674 Test RE 0.301109277916449 Lambda1 0.12797505\n",
      "13 Train Loss 299.41913 Test MSE 268.82091145946663 Test RE 0.2760073804453068 Lambda1 0.116941206\n",
      "14 Train Loss 257.98267 Test MSE 245.44112034667288 Test RE 0.26373200286918064 Lambda1 0.10957385\n",
      "15 Train Loss 236.09834 Test MSE 235.10337929601656 Test RE 0.25811818724892843 Lambda1 0.107152276\n",
      "16 Train Loss 225.72684 Test MSE 231.3799093878731 Test RE 0.25606604512614384 Lambda1 0.10968286\n",
      "17 Train Loss 221.72328 Test MSE 228.46009041496973 Test RE 0.25444524704182936 Lambda1 0.11519632\n",
      "18 Train Loss 217.26434 Test MSE 226.3315932271566 Test RE 0.253257176615846 Lambda1 0.12372001\n",
      "19 Train Loss 205.15042 Test MSE 217.08490489828708 Test RE 0.24802986874527355 Lambda1 0.14602071\n",
      "20 Train Loss 201.1814 Test MSE 214.39573145013787 Test RE 0.24648882686766463 Lambda1 0.15575825\n",
      "21 Train Loss 198.05937 Test MSE 211.53502696323898 Test RE 0.2448388414080505 Lambda1 0.1684988\n",
      "22 Train Loss 191.9453 Test MSE 205.31657529857978 Test RE 0.24121325900768587 Lambda1 0.18102218\n",
      "23 Train Loss 187.67297 Test MSE 200.5812845929667 Test RE 0.2384154386431806 Lambda1 0.19968598\n",
      "24 Train Loss 183.56952 Test MSE 196.49474546620837 Test RE 0.23597426457037474 Lambda1 0.21708976\n",
      "25 Train Loss 181.12027 Test MSE 191.15224505626935 Test RE 0.23274420270405247 Lambda1 0.23656097\n",
      "26 Train Loss 172.8286 Test MSE 179.47464139623943 Test RE 0.22552293648727442 Lambda1 0.2702437\n",
      "27 Train Loss 164.20306 Test MSE 168.34411268929767 Test RE 0.21841785466435842 Lambda1 0.2938531\n",
      "28 Train Loss 153.87737 Test MSE 150.67129499708312 Test RE 0.20663526005306496 Lambda1 0.3274788\n",
      "29 Train Loss 138.30505 Test MSE 131.0672896217589 Test RE 0.19272423477170983 Lambda1 0.37388465\n",
      "30 Train Loss 123.94891 Test MSE 122.68106552880141 Test RE 0.18645667711467118 Lambda1 0.40695125\n",
      "31 Train Loss 113.933754 Test MSE 113.73585837619802 Test RE 0.1795303491274155 Lambda1 0.4292236\n",
      "32 Train Loss 102.494125 Test MSE 98.91628218189014 Test RE 0.16742606225147316 Lambda1 0.47171634\n",
      "33 Train Loss 96.56567 Test MSE 87.55623837112496 Test RE 0.15751891732625073 Lambda1 0.5002436\n",
      "34 Train Loss 81.71416 Test MSE 69.25681508768433 Test RE 0.14009428805266244 Lambda1 0.5363244\n",
      "35 Train Loss 75.1644 Test MSE 60.95494164027283 Test RE 0.13142973448416326 Lambda1 0.5622831\n",
      "36 Train Loss 68.912125 Test MSE 56.82375238860382 Test RE 0.1268978094524992 Lambda1 0.586555\n",
      "37 Train Loss 64.30171 Test MSE 54.24002796330678 Test RE 0.12397928374530393 Lambda1 0.6049679\n",
      "38 Train Loss 60.43505 Test MSE 52.30297507528105 Test RE 0.1217453456660897 Lambda1 0.6076762\n",
      "39 Train Loss 56.13638 Test MSE 47.628117612117585 Test RE 0.11617719305674262 Lambda1 0.6255632\n",
      "40 Train Loss 54.816116 Test MSE 47.15134148775835 Test RE 0.11559424087424486 Lambda1 0.6353821\n",
      "41 Train Loss 51.46 Test MSE 44.9619390387986 Test RE 0.11287861884428482 Lambda1 0.6580857\n",
      "42 Train Loss 47.21227 Test MSE 42.00157390359941 Test RE 0.10909929722229508 Lambda1 0.6779199\n",
      "43 Train Loss 45.72544 Test MSE 42.405197333984425 Test RE 0.10962225080109032 Lambda1 0.67317283\n",
      "44 Train Loss 45.162712 Test MSE 42.193145832310876 Test RE 0.10934781868308883 Lambda1 0.6779918\n",
      "45 Train Loss 44.421886 Test MSE 41.844501674128935 Test RE 0.10889510819450646 Lambda1 0.68789095\n",
      "46 Train Loss 43.762268 Test MSE 41.65931235147254 Test RE 0.10865387491173033 Lambda1 0.6965495\n",
      "47 Train Loss 43.254677 Test MSE 41.447584731081 Test RE 0.10837741414518481 Lambda1 0.6988174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 Train Loss 42.817413 Test MSE 41.00830258473882 Test RE 0.10780156539113653 Lambda1 0.7069717\n",
      "49 Train Loss 42.440712 Test MSE 41.028013201740514 Test RE 0.10782746965892882 Lambda1 0.7087382\n",
      "50 Train Loss 42.092216 Test MSE 40.88632856634095 Test RE 0.10764112492711167 Lambda1 0.71460825\n",
      "51 Train Loss 41.693516 Test MSE 40.69608379633519 Test RE 0.10739040495955321 Lambda1 0.71800506\n",
      "52 Train Loss 41.508602 Test MSE 40.57382493115945 Test RE 0.10722897290654092 Lambda1 0.7209801\n",
      "53 Train Loss 40.700912 Test MSE 39.86895039791772 Test RE 0.10629346677510233 Lambda1 0.74520206\n",
      "54 Train Loss 39.95324 Test MSE 39.52532817440671 Test RE 0.10583441483198407 Lambda1 0.75913805\n",
      "55 Train Loss 39.27685 Test MSE 39.15129451250889 Test RE 0.1053324616236557 Lambda1 0.7765359\n",
      "56 Train Loss 38.80256 Test MSE 39.02350139580086 Test RE 0.10516041410953675 Lambda1 0.78664595\n",
      "57 Train Loss 38.60561 Test MSE 39.02238601425714 Test RE 0.1051589112353843 Lambda1 0.7924447\n",
      "58 Train Loss 38.435665 Test MSE 38.91544174757181 Test RE 0.1050147137678114 Lambda1 0.80754834\n",
      "59 Train Loss 38.113556 Test MSE 38.6321542229562 Test RE 0.10463178483859643 Lambda1 0.82051116\n",
      "60 Train Loss 37.895683 Test MSE 38.53961521663836 Test RE 0.10450639283163315 Lambda1 0.83316773\n",
      "61 Train Loss 37.702347 Test MSE 38.35478927830874 Test RE 0.10425549896930757 Lambda1 0.85194653\n",
      "62 Train Loss 37.51104 Test MSE 38.51167214737109 Test RE 0.1044684998885418 Lambda1 0.85792166\n",
      "63 Train Loss 37.293854 Test MSE 38.60573119689519 Test RE 0.104595996499424 Lambda1 0.84581023\n",
      "64 Train Loss 37.102783 Test MSE 38.38477519903245 Test RE 0.1042962446806833 Lambda1 0.85006624\n",
      "65 Train Loss 36.99858 Test MSE 38.39838583909924 Test RE 0.10431473394921584 Lambda1 0.8552303\n",
      "66 Train Loss 36.848373 Test MSE 38.34059287522833 Test RE 0.10423620294273135 Lambda1 0.8654788\n",
      "67 Train Loss 36.744205 Test MSE 38.27290863720371 Test RE 0.10414415605614015 Lambda1 0.8688731\n",
      "68 Train Loss 36.674255 Test MSE 38.25891949607974 Test RE 0.1041251214374067 Lambda1 0.8720508\n",
      "69 Train Loss 36.55282 Test MSE 38.18146548086544 Test RE 0.10401966897333205 Lambda1 0.88842523\n",
      "70 Train Loss 36.51159 Test MSE 38.1663945654529 Test RE 0.10399913772420083 Lambda1 0.8965341\n",
      "71 Train Loss 36.35203 Test MSE 38.117920841764374 Test RE 0.10393307401541974 Lambda1 0.91113997\n",
      "72 Train Loss 36.20861 Test MSE 38.129513779931536 Test RE 0.1039488775807253 Lambda1 0.92223674\n",
      "73 Train Loss 36.1337 Test MSE 38.12007510947586 Test RE 0.10393601090831922 Lambda1 0.9283546\n",
      "74 Train Loss 36.031082 Test MSE 38.088166728653746 Test RE 0.1038925020209146 Lambda1 0.9376091\n",
      "Training time: 143.13\n",
      "Training time: 143.13\n",
      "inv_HT_stan_tune7\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 2950.7126 Test MSE 2903.4465589025513 Test RE 0.907081039370736 Lambda1 0.021228008\n",
      "1 Train Loss 844.0061 Test MSE 851.8710875094336 Test RE 0.4913331988197401 Lambda1 0.060077738\n",
      "2 Train Loss 832.4733 Test MSE 850.5495336951424 Test RE 0.49095193497557366 Lambda1 0.059526727\n",
      "3 Train Loss 829.12933 Test MSE 846.4787876582792 Test RE 0.489775673295875 Lambda1 0.05946179\n",
      "4 Train Loss 825.55334 Test MSE 844.1969804714914 Test RE 0.48911509689980254 Lambda1 0.06077342\n",
      "5 Train Loss 814.32776 Test MSE 832.4916151956425 Test RE 0.4857123034806149 Lambda1 0.06591119\n",
      "6 Train Loss 798.6076 Test MSE 814.5153971345127 Test RE 0.4804396255580895 Lambda1 0.080458686\n",
      "7 Train Loss 769.9583 Test MSE 786.0088765304064 Test RE 0.4719575043230495 Lambda1 0.103433736\n",
      "8 Train Loss 722.0007 Test MSE 734.2471130166061 Test RE 0.4561527468484893 Lambda1 0.119794406\n",
      "9 Train Loss 658.87146 Test MSE 672.560237279959 Test RE 0.43657088026899565 Lambda1 0.11994059\n",
      "10 Train Loss 574.6468 Test MSE 580.0396347828519 Test RE 0.4054319832631341 Lambda1 0.11275314\n",
      "11 Train Loss 438.30408 Test MSE 395.20028958409245 Test RE 0.3346553695875266 Lambda1 0.095142424\n",
      "12 Train Loss 378.808 Test MSE 351.65711799338953 Test RE 0.31568131491650303 Lambda1 0.082770795\n",
      "13 Train Loss 310.18195 Test MSE 294.9679976606597 Test RE 0.2891189917299986 Lambda1 0.045895036\n",
      "14 Train Loss 286.0405 Test MSE 291.6501513794064 Test RE 0.2874883655050455 Lambda1 0.035266276\n",
      "15 Train Loss 253.25716 Test MSE 270.8034149117851 Test RE 0.27702326205402994 Lambda1 0.029642912\n",
      "16 Train Loss 247.44499 Test MSE 266.07885575441804 Test RE 0.27459609238215454 Lambda1 0.03776717\n",
      "17 Train Loss 239.5769 Test MSE 255.2865900092966 Test RE 0.26896958430753604 Lambda1 0.06269287\n",
      "18 Train Loss 230.75133 Test MSE 247.0159717915172 Test RE 0.2645767566088349 Lambda1 0.0907273\n",
      "19 Train Loss 223.09386 Test MSE 239.9559434542479 Test RE 0.26076837840732553 Lambda1 0.115991056\n",
      "20 Train Loss 214.40654 Test MSE 227.51055288905437 Test RE 0.25391592729047485 Lambda1 0.15557602\n",
      "21 Train Loss 207.62463 Test MSE 223.5565288727901 Test RE 0.2516997878637913 Lambda1 0.17305075\n",
      "22 Train Loss 200.62177 Test MSE 214.21230729774354 Test RE 0.24638336376411582 Lambda1 0.22359739\n",
      "23 Train Loss 194.10559 Test MSE 210.9229282264167 Test RE 0.2444843513792597 Lambda1 0.23940057\n",
      "24 Train Loss 187.95459 Test MSE 201.45998496964464 Test RE 0.23893708950213072 Lambda1 0.26763317\n",
      "25 Train Loss 177.04251 Test MSE 184.8611532294989 Test RE 0.228882189671462 Lambda1 0.31623828\n",
      "26 Train Loss 169.71756 Test MSE 176.84500367671362 Test RE 0.22386467441893537 Lambda1 0.33341616\n",
      "27 Train Loss 157.0329 Test MSE 163.0659729455001 Test RE 0.2149665287867761 Lambda1 0.3736888\n",
      "28 Train Loss 143.85059 Test MSE 147.64664037155814 Test RE 0.20455069303229298 Lambda1 0.42334825\n",
      "29 Train Loss 134.36348 Test MSE 137.13935740092023 Test RE 0.19713794528263753 Lambda1 0.4531993\n",
      "30 Train Loss 127.93661 Test MSE 135.0180012858881 Test RE 0.1956072772555535 Lambda1 0.46374068\n",
      "31 Train Loss 118.45854 Test MSE 123.03156188316024 Test RE 0.1867228378884239 Lambda1 0.49536198\n",
      "32 Train Loss 111.764725 Test MSE 113.21461273802046 Test RE 0.17911848748347078 Lambda1 0.511831\n",
      "33 Train Loss 103.87421 Test MSE 105.13897083469615 Test RE 0.1726120185993626 Lambda1 0.51914126\n",
      "34 Train Loss 97.13676 Test MSE 93.3677439198181 Test RE 0.16266256003701476 Lambda1 0.5422233\n",
      "35 Train Loss 90.3467 Test MSE 81.16179923319618 Test RE 0.1516578880056623 Lambda1 0.5729326\n",
      "36 Train Loss 82.086174 Test MSE 74.78891168694139 Test RE 0.14558203217326673 Lambda1 0.58339673\n",
      "37 Train Loss 74.639915 Test MSE 66.06362120210166 Test RE 0.13682654423215534 Lambda1 0.60439014\n",
      "38 Train Loss 68.553375 Test MSE 59.05606476395044 Test RE 0.12936637911738907 Lambda1 0.6327301\n",
      "39 Train Loss 63.846542 Test MSE 55.311236514708774 Test RE 0.12519755696362692 Lambda1 0.65058345\n",
      "40 Train Loss 57.024597 Test MSE 53.29621803203287 Test RE 0.12289589215084139 Lambda1 0.64947295\n",
      "41 Train Loss 53.93153 Test MSE 51.10238203945763 Test RE 0.12033992675918082 Lambda1 0.65886974\n",
      "42 Train Loss 52.803574 Test MSE 49.688753500569995 Test RE 0.11866379178722043 Lambda1 0.66410935\n",
      "43 Train Loss 50.913925 Test MSE 47.23349947472611 Test RE 0.11569490457194814 Lambda1 0.6797997\n",
      "44 Train Loss 48.41797 Test MSE 45.519992573617294 Test RE 0.11357696567983133 Lambda1 0.6975196\n",
      "45 Train Loss 45.567535 Test MSE 43.04549440432032 Test RE 0.11044677032588632 Lambda1 0.7285888\n",
      "46 Train Loss 44.79155 Test MSE 42.705911384365784 Test RE 0.11001025412562822 Lambda1 0.7348415\n",
      "47 Train Loss 43.958134 Test MSE 42.27914595922129 Test RE 0.1094592009833508 Lambda1 0.7373927\n",
      "48 Train Loss 42.630047 Test MSE 41.41986610621002 Test RE 0.10834116866395671 Lambda1 0.75039554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 Train Loss 41.870743 Test MSE 40.93202124058851 Test RE 0.10770125550925731 Lambda1 0.76015794\n",
      "50 Train Loss 41.08147 Test MSE 40.53991871196017 Test RE 0.10718415966859043 Lambda1 0.7705804\n",
      "51 Train Loss 39.995804 Test MSE 39.97290785480641 Test RE 0.10643195555641222 Lambda1 0.7869294\n",
      "52 Train Loss 38.90323 Test MSE 39.9607901106287 Test RE 0.10641582196696688 Lambda1 0.8006656\n",
      "53 Train Loss 38.38377 Test MSE 40.07137759716582 Test RE 0.10656296780196933 Lambda1 0.80102277\n",
      "54 Train Loss 37.654217 Test MSE 39.37032098098455 Test RE 0.10562668459892796 Lambda1 0.8121784\n",
      "55 Train Loss 37.08357 Test MSE 38.7830138602272 Test RE 0.10483588079834612 Lambda1 0.81698555\n",
      "56 Train Loss 36.500008 Test MSE 37.96835331943205 Test RE 0.10372896672232856 Lambda1 0.8247579\n",
      "57 Train Loss 36.119926 Test MSE 37.6497297597855 Test RE 0.10329281239071977 Lambda1 0.83041555\n",
      "58 Train Loss 35.860428 Test MSE 37.375795525479084 Test RE 0.10291635425714594 Lambda1 0.83205944\n",
      "59 Train Loss 35.55209 Test MSE 37.17222403641016 Test RE 0.10263569880996916 Lambda1 0.84115195\n",
      "60 Train Loss 35.276443 Test MSE 37.08242974174272 Test RE 0.10251165898153422 Lambda1 0.8494563\n",
      "61 Train Loss 34.983658 Test MSE 36.89218124536436 Test RE 0.10224835682272566 Lambda1 0.8612534\n",
      "62 Train Loss 34.405437 Test MSE 36.95644884727426 Test RE 0.10233737835946986 Lambda1 0.86162233\n",
      "63 Train Loss 34.09157 Test MSE 36.6904284422894 Test RE 0.1019683900023219 Lambda1 0.8744355\n",
      "64 Train Loss 33.886158 Test MSE 36.55164948538808 Test RE 0.10177536318139443 Lambda1 0.88123727\n",
      "65 Train Loss 33.677444 Test MSE 36.37435194827309 Test RE 0.10152822719138671 Lambda1 0.9016382\n",
      "66 Train Loss 33.51924 Test MSE 36.08341591914961 Test RE 0.10112138116418029 Lambda1 0.91745543\n",
      "67 Train Loss 33.30682 Test MSE 35.90155002724799 Test RE 0.10086622569934771 Lambda1 0.93211657\n",
      "68 Train Loss 33.140778 Test MSE 35.743371489588675 Test RE 0.10064377729849949 Lambda1 0.9435487\n",
      "69 Train Loss 33.082935 Test MSE 35.717631862015686 Test RE 0.10060753281762161 Lambda1 0.94171304\n",
      "70 Train Loss 32.955383 Test MSE 35.68232680349186 Test RE 0.10055779781540436 Lambda1 0.947925\n",
      "71 Train Loss 32.869778 Test MSE 35.619513356049126 Test RE 0.10046925028043331 Lambda1 0.957741\n",
      "72 Train Loss 32.78651 Test MSE 35.59677611213649 Test RE 0.1004371785548031 Lambda1 0.9635498\n",
      "73 Train Loss 32.68859 Test MSE 35.67386682065093 Test RE 0.1005458764000886 Lambda1 0.9655607\n",
      "74 Train Loss 32.63566 Test MSE 35.52958627831532 Test RE 0.10034234489979939 Lambda1 0.9696592\n",
      "Training time: 142.94\n",
      "Training time: 142.94\n",
      "inv_HT_stan_tune7\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 844.2119 Test MSE 855.2845756801804 Test RE 0.49231661246603076 Lambda1 0.08379849\n",
      "1 Train Loss 828.701 Test MSE 845.82462181103 Test RE 0.4895863853610955 Lambda1 0.084073834\n",
      "2 Train Loss 816.51685 Test MSE 832.7563740192801 Test RE 0.4857895333246879 Lambda1 0.0827413\n",
      "3 Train Loss 797.3233 Test MSE 812.8557464508152 Test RE 0.4799499058149673 Lambda1 0.08226911\n",
      "4 Train Loss 760.5171 Test MSE 782.7809674759914 Test RE 0.47098741146305473 Lambda1 0.082064025\n",
      "5 Train Loss 684.27466 Test MSE 682.6942589848508 Test RE 0.43984767027196814 Lambda1 0.081809886\n",
      "6 Train Loss 584.25775 Test MSE 583.5157113074213 Test RE 0.40664501025029925 Lambda1 0.076916955\n",
      "7 Train Loss 462.6828 Test MSE 464.2221223875359 Test RE 0.36270378445337054 Lambda1 0.04206862\n",
      "8 Train Loss 365.30203 Test MSE 358.1217444212864 Test RE 0.3185697364813201 Lambda1 0.013082534\n",
      "9 Train Loss 293.27747 Test MSE 304.01012132281016 Test RE 0.29351695409400513 Lambda1 0.007340388\n",
      "10 Train Loss 273.48682 Test MSE 290.1262329090834 Test RE 0.2867362955587598 Lambda1 0.0063061644\n",
      "11 Train Loss 263.5122 Test MSE 281.96948456935763 Test RE 0.28267683876008676 Lambda1 0.006180271\n",
      "12 Train Loss 258.71808 Test MSE 279.3855897016998 Test RE 0.2813786694555552 Lambda1 0.005815775\n",
      "13 Train Loss 255.30228 Test MSE 277.4559532997883 Test RE 0.2804052848040281 Lambda1 0.006760552\n",
      "14 Train Loss 253.54709 Test MSE 276.9660964373702 Test RE 0.2801576434530528 Lambda1 0.008627798\n",
      "15 Train Loss 250.94621 Test MSE 271.82447986606206 Test RE 0.2775450291375003 Lambda1 0.019333642\n",
      "16 Train Loss 248.14952 Test MSE 266.61445808865193 Test RE 0.27487232697598946 Lambda1 0.030252412\n",
      "17 Train Loss 243.28114 Test MSE 258.0951855888608 Test RE 0.2704451032243194 Lambda1 0.047475968\n",
      "18 Train Loss 238.68901 Test MSE 254.07075363954525 Test RE 0.26832831814157443 Lambda1 0.05809417\n",
      "19 Train Loss 233.41437 Test MSE 244.43649898484637 Test RE 0.26319170530406927 Lambda1 0.077115655\n",
      "20 Train Loss 225.76332 Test MSE 235.0783143408324 Test RE 0.25810442757166974 Lambda1 0.10156777\n",
      "21 Train Loss 217.75444 Test MSE 230.42477842236693 Test RE 0.25553698118640034 Lambda1 0.11125254\n",
      "22 Train Loss 207.8857 Test MSE 220.34881547880406 Test RE 0.2498874991816791 Lambda1 0.13093793\n",
      "23 Train Loss 201.54904 Test MSE 213.5843164241094 Test RE 0.2460219464569298 Lambda1 0.14741191\n",
      "24 Train Loss 195.8147 Test MSE 206.24696081391406 Test RE 0.2417591663628455 Lambda1 0.16568692\n",
      "25 Train Loss 187.55682 Test MSE 197.7552756867452 Test RE 0.23672995187672202 Lambda1 0.18372416\n",
      "26 Train Loss 179.57208 Test MSE 190.41597763438907 Test RE 0.23229553598101232 Lambda1 0.20183022\n",
      "27 Train Loss 171.70425 Test MSE 179.8508158976253 Test RE 0.22575915807967178 Lambda1 0.23168448\n",
      "28 Train Loss 163.72618 Test MSE 170.1014409169217 Test RE 0.2195549165836743 Lambda1 0.25553814\n",
      "29 Train Loss 154.26907 Test MSE 162.38159415244098 Test RE 0.21451495321369524 Lambda1 0.27786225\n",
      "30 Train Loss 147.06712 Test MSE 157.20686218263327 Test RE 0.21106922733264824 Lambda1 0.29366136\n",
      "31 Train Loss 137.08748 Test MSE 142.38660850473147 Test RE 0.20087400727988663 Lambda1 0.31702253\n",
      "32 Train Loss 126.85642 Test MSE 128.56860485032277 Test RE 0.19087833420028713 Lambda1 0.33294833\n",
      "33 Train Loss 118.59698 Test MSE 115.19287910824083 Test RE 0.18067663237821385 Lambda1 0.34914577\n",
      "34 Train Loss 111.88997 Test MSE 108.28175989202624 Test RE 0.17517286114691805 Lambda1 0.35717136\n",
      "35 Train Loss 107.59975 Test MSE 101.31969807391287 Test RE 0.1694478700330575 Lambda1 0.36572623\n",
      "36 Train Loss 101.81182 Test MSE 97.85238970704327 Test RE 0.16652325400016818 Lambda1 0.37076715\n",
      "37 Train Loss 97.85903 Test MSE 96.68305914652737 Test RE 0.16552529187608536 Lambda1 0.36682868\n",
      "38 Train Loss 94.83708 Test MSE 90.69236225062262 Test RE 0.16031513595171473 Lambda1 0.37244505\n",
      "39 Train Loss 90.95102 Test MSE 84.27384306085331 Test RE 0.15453810062613652 Lambda1 0.3858554\n",
      "40 Train Loss 87.474556 Test MSE 79.90710069078203 Test RE 0.15048106536723693 Lambda1 0.3976926\n",
      "41 Train Loss 81.651245 Test MSE 73.4672570962212 Test RE 0.14428995019367766 Lambda1 0.42001152\n",
      "42 Train Loss 78.035805 Test MSE 69.1450293640154 Test RE 0.1399811810192184 Lambda1 0.43036902\n",
      "43 Train Loss 75.06253 Test MSE 67.96617034327642 Test RE 0.13878277607641168 Lambda1 0.43768635\n",
      "44 Train Loss 72.65047 Test MSE 66.31508020096219 Test RE 0.13708669942111779 Lambda1 0.44947484\n",
      "45 Train Loss 70.059326 Test MSE 64.12701062089106 Test RE 0.13480613837439212 Lambda1 0.45932284\n",
      "46 Train Loss 68.56886 Test MSE 63.156440096418535 Test RE 0.13378209475253305 Lambda1 0.4646489\n",
      "47 Train Loss 66.52813 Test MSE 62.17479135821723 Test RE 0.13273832679004485 Lambda1 0.471732\n",
      "48 Train Loss 64.89397 Test MSE 60.66880925047349 Test RE 0.13112089536942736 Lambda1 0.48270258\n",
      "49 Train Loss 64.116875 Test MSE 59.842483241120746 Test RE 0.13022488244150762 Lambda1 0.4882955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Train Loss 62.6232 Test MSE 59.340692842686 Test RE 0.129677753101275 Lambda1 0.49271524\n",
      "51 Train Loss 61.217793 Test MSE 58.00571363089987 Test RE 0.12821078454622953 Lambda1 0.50395924\n",
      "52 Train Loss 59.31668 Test MSE 56.89529331808247 Test RE 0.1269776663014177 Lambda1 0.5150548\n",
      "53 Train Loss 58.18498 Test MSE 55.92948601561184 Test RE 0.12589531962145878 Lambda1 0.5246451\n",
      "54 Train Loss 57.089634 Test MSE 55.10164194550174 Test RE 0.12496012207859387 Lambda1 0.53292644\n",
      "55 Train Loss 56.068737 Test MSE 54.61440803164312 Test RE 0.12440641806535495 Lambda1 0.53812206\n",
      "56 Train Loss 55.163788 Test MSE 54.20167757574872 Test RE 0.12393544624649962 Lambda1 0.54126567\n",
      "57 Train Loss 54.59652 Test MSE 53.989103871801916 Test RE 0.12369217611302308 Lambda1 0.5417064\n",
      "58 Train Loss 54.232365 Test MSE 54.13925819030367 Test RE 0.12386406282044865 Lambda1 0.5406621\n",
      "59 Train Loss 53.573124 Test MSE 52.93602156748295 Test RE 0.12247989910176546 Lambda1 0.5494469\n",
      "60 Train Loss 53.13026 Test MSE 52.619028936822595 Test RE 0.12211263006204436 Lambda1 0.553979\n",
      "61 Train Loss 52.756207 Test MSE 52.31276564507723 Test RE 0.12175673986046558 Lambda1 0.5591114\n",
      "62 Train Loss 52.4399 Test MSE 51.9024237397903 Test RE 0.12127826913194657 Lambda1 0.56521267\n",
      "63 Train Loss 52.155434 Test MSE 51.390334717117064 Test RE 0.12067849734123541 Lambda1 0.5735907\n",
      "64 Train Loss 51.924778 Test MSE 50.98654853764152 Test RE 0.12020346244360701 Lambda1 0.58073694\n",
      "65 Train Loss 51.144005 Test MSE 50.35511677121825 Test RE 0.11945682685152645 Lambda1 0.5857492\n",
      "66 Train Loss 50.728065 Test MSE 49.26347017890121 Test RE 0.1181548820649252 Lambda1 0.60110664\n",
      "67 Train Loss 50.372494 Test MSE 48.832111464191755 Test RE 0.11763645331740329 Lambda1 0.6082464\n",
      "68 Train Loss 50.043354 Test MSE 48.53338527560116 Test RE 0.11727608596447169 Lambda1 0.6121375\n",
      "69 Train Loss 49.363693 Test MSE 48.334910523700344 Test RE 0.11703604307890636 Lambda1 0.6119898\n",
      "70 Train Loss 48.812477 Test MSE 48.12369535413011 Test RE 0.11678004951361488 Lambda1 0.6185621\n",
      "71 Train Loss 48.42858 Test MSE 47.88069262527022 Test RE 0.11648483335179488 Lambda1 0.61994064\n",
      "72 Train Loss 47.79673 Test MSE 47.619353689333046 Test RE 0.11616650383740934 Lambda1 0.62427557\n",
      "73 Train Loss 47.40118 Test MSE 47.1795992073259 Test RE 0.11562887340318619 Lambda1 0.6312988\n",
      "74 Train Loss 46.873352 Test MSE 46.75006088690799 Test RE 0.11510130855906325 Lambda1 0.63758516\n",
      "Training time: 137.25\n",
      "Training time: 137.25\n",
      "inv_HT_stan_tune7\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 2268.1243 Test MSE 2206.0947407405583 Test RE 0.7906809042556858 Lambda1 0.035126954\n",
      "1 Train Loss 858.21564 Test MSE 863.7816409523967 Test RE 0.49475609745867727 Lambda1 0.08064393\n",
      "2 Train Loss 835.6287 Test MSE 850.3218477726765 Test RE 0.4908862184466488 Lambda1 0.07883188\n",
      "3 Train Loss 826.74554 Test MSE 843.4270634646148 Test RE 0.4888920068271378 Lambda1 0.074531004\n",
      "4 Train Loss 805.73346 Test MSE 821.5502330153217 Test RE 0.48250990412448097 Lambda1 0.06582754\n",
      "5 Train Loss 782.20654 Test MSE 800.6608376570115 Test RE 0.47633606420835006 Lambda1 0.061357215\n",
      "6 Train Loss 741.147 Test MSE 755.7260428769986 Test RE 0.4627765721980147 Lambda1 0.0529461\n",
      "7 Train Loss 675.5712 Test MSE 677.1874976515206 Test RE 0.4380701247012939 Lambda1 0.037958045\n",
      "8 Train Loss 535.99054 Test MSE 536.8559072062328 Test RE 0.390047985042861 Lambda1 0.022470811\n",
      "9 Train Loss 432.3509 Test MSE 419.5657392961144 Test RE 0.34481740520506826 Lambda1 -0.00050118775\n",
      "10 Train Loss 332.4641 Test MSE 323.30625816300056 Test RE 0.30268871261876895 Lambda1 -0.0016903527\n",
      "11 Train Loss 287.43216 Test MSE 303.44937144496697 Test RE 0.2932461315925609 Lambda1 -0.0006270548\n",
      "12 Train Loss 270.66632 Test MSE 291.78444012601193 Test RE 0.28755454412534476 Lambda1 -0.0010757635\n",
      "13 Train Loss 267.08493 Test MSE 290.25171647893865 Test RE 0.2867982975430657 Lambda1 -0.00070835644\n",
      "14 Train Loss 263.4581 Test MSE 287.61330571325317 Test RE 0.28549181229289267 Lambda1 -0.000106638414\n",
      "15 Train Loss 260.81006 Test MSE 285.29345893682745 Test RE 0.28433811363101763 Lambda1 0.00038657806\n",
      "16 Train Loss 259.2155 Test MSE 284.3109489544971 Test RE 0.2838480814122051 Lambda1 7.742051e-05\n",
      "17 Train Loss 257.45682 Test MSE 282.8071972061827 Test RE 0.28309643441310595 Lambda1 -3.6976988e-05\n",
      "18 Train Loss 256.75308 Test MSE 282.6234565354425 Test RE 0.28300445517637957 Lambda1 0.00016879826\n",
      "19 Train Loss 256.4088 Test MSE 282.66884351046787 Test RE 0.2830271783442464 Lambda1 -0.0001273389\n",
      "20 Train Loss 255.89366 Test MSE 282.5764752015873 Test RE 0.2829809318645579 Lambda1 3.5802037e-05\n",
      "21 Train Loss 255.60048 Test MSE 282.39844000952144 Test RE 0.2828917728103678 Lambda1 0.000104532126\n",
      "22 Train Loss 255.10518 Test MSE 282.1032155886189 Test RE 0.28274386406368807 Lambda1 3.6387668e-05\n",
      "23 Train Loss 254.82993 Test MSE 281.65152023602934 Test RE 0.2825174128105851 Lambda1 0.00013061696\n",
      "24 Train Loss 254.50685 Test MSE 281.63820006295884 Test RE 0.28251073216969047 Lambda1 8.019715e-05\n",
      "25 Train Loss 254.3844 Test MSE 281.53479964932154 Test RE 0.2824588670316493 Lambda1 0.00023285468\n",
      "26 Train Loss 254.29678 Test MSE 281.5655808649291 Test RE 0.2824743077332285 Lambda1 0.00026223337\n",
      "27 Train Loss 254.20221 Test MSE 281.4219784912033 Test RE 0.2824022656278853 Lambda1 0.00026716155\n",
      "28 Train Loss 254.1227 Test MSE 281.49144992260125 Test RE 0.2824371201888278 Lambda1 0.0002550731\n",
      "29 Train Loss 254.0493 Test MSE 281.60344841282443 Test RE 0.2824933019755713 Lambda1 0.00029277732\n",
      "30 Train Loss 253.96075 Test MSE 281.65959475115386 Test RE 0.28252146245102633 Lambda1 0.00034304336\n",
      "31 Train Loss 253.89633 Test MSE 281.6722138912327 Test RE 0.2825277912572249 Lambda1 0.00037418737\n",
      "32 Train Loss 253.81273 Test MSE 281.6210363482191 Test RE 0.2825021235940507 Lambda1 0.0004916869\n",
      "33 Train Loss 253.66214 Test MSE 281.4796378383602 Test RE 0.2824311942431967 Lambda1 0.0007412326\n",
      "34 Train Loss 253.59457 Test MSE 281.4576101970592 Test RE 0.2824201429735457 Lambda1 0.00081701274\n",
      "35 Train Loss 253.52827 Test MSE 281.36584272735905 Test RE 0.28237409857141293 Lambda1 0.0011112969\n",
      "36 Train Loss 253.46101 Test MSE 281.5372725195233 Test RE 0.2824601075223144 Lambda1 0.0011626459\n",
      "37 Train Loss 253.41217 Test MSE 281.5884224816455 Test RE 0.2824857651688269 Lambda1 0.0011633079\n",
      "38 Train Loss 253.33281 Test MSE 281.50153834501674 Test RE 0.2824421813007123 Lambda1 0.0012725926\n",
      "39 Train Loss 253.24992 Test MSE 281.54702700236317 Test RE 0.282465000708183 Lambda1 0.001380862\n",
      "40 Train Loss 253.1591 Test MSE 281.3814975805011 Test RE 0.2823819539377839 Lambda1 0.0014852722\n",
      "41 Train Loss 253.12926 Test MSE 281.32758631657555 Test RE 0.28235490116879086 Lambda1 0.0015677735\n",
      "42 Train Loss 253.05919 Test MSE 281.4596649807489 Test RE 0.2824211738769983 Lambda1 0.0016096476\n",
      "43 Train Loss 252.94528 Test MSE 281.2550878927713 Test RE 0.28231851724261814 Lambda1 0.001720237\n",
      "44 Train Loss 252.89053 Test MSE 281.1478797852007 Test RE 0.28226470538271314 Lambda1 0.0018646754\n",
      "45 Train Loss 252.79779 Test MSE 280.9005272084541 Test RE 0.2821405104828644 Lambda1 0.0020943405\n",
      "46 Train Loss 252.71461 Test MSE 280.7945008637726 Test RE 0.2820872582682003 Lambda1 0.0022319283\n",
      "47 Train Loss 252.67392 Test MSE 280.7902365844961 Test RE 0.28208511630421446 Lambda1 0.0022915436\n",
      "48 Train Loss 252.61227 Test MSE 280.8668630102869 Test RE 0.28212360357503924 Lambda1 0.0024496347\n",
      "49 Train Loss 252.53699 Test MSE 280.7588190294759 Test RE 0.28206933464334333 Lambda1 0.0027164547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Train Loss 252.4681 Test MSE 280.6029570040712 Test RE 0.28199102899787953 Lambda1 0.0029274572\n",
      "51 Train Loss 252.42754 Test MSE 280.5210533679942 Test RE 0.2819498715980147 Lambda1 0.0030140511\n",
      "52 Train Loss 252.29802 Test MSE 280.3791824908987 Test RE 0.2818785658371006 Lambda1 0.0033219669\n",
      "53 Train Loss 252.23119 Test MSE 280.2516554420568 Test RE 0.2818144540336259 Lambda1 0.0035922776\n",
      "54 Train Loss 252.16838 Test MSE 280.0589800451813 Test RE 0.2817175624602173 Lambda1 0.0038812428\n",
      "55 Train Loss 252.04483 Test MSE 279.55431468763896 Test RE 0.2814636209494029 Lambda1 0.004562055\n",
      "56 Train Loss 251.95547 Test MSE 279.26115679179895 Test RE 0.2813160021826448 Lambda1 0.004870923\n",
      "57 Train Loss 251.8548 Test MSE 279.01920638944034 Test RE 0.2811941104205109 Lambda1 0.00517445\n",
      "58 Train Loss 251.71004 Test MSE 278.85441218733763 Test RE 0.2811110587842324 Lambda1 0.0055012736\n",
      "59 Train Loss 251.58229 Test MSE 278.83570148598915 Test RE 0.2811016275665959 Lambda1 0.005568367\n",
      "60 Train Loss 251.43484 Test MSE 278.78124065132465 Test RE 0.28107417452386224 Lambda1 0.005622234\n",
      "61 Train Loss 251.3486 Test MSE 278.5567439699192 Test RE 0.2809609801645504 Lambda1 0.005958813\n",
      "62 Train Loss 251.19833 Test MSE 278.06378703295087 Test RE 0.28071226424319096 Lambda1 0.0068109264\n",
      "63 Train Loss 250.99527 Test MSE 278.0365525146756 Test RE 0.2806985169469729 Lambda1 0.007046569\n",
      "64 Train Loss 250.81625 Test MSE 278.01395893931345 Test RE 0.28068711177041644 Lambda1 0.007401975\n",
      "65 Train Loss 250.56276 Test MSE 277.636553849225 Test RE 0.2804965301165939 Lambda1 0.008370807\n",
      "66 Train Loss 250.28711 Test MSE 277.0534393043114 Test RE 0.2802018146408306 Lambda1 0.009722906\n",
      "67 Train Loss 249.9299 Test MSE 276.2519500812094 Test RE 0.27979622250691333 Lambda1 0.011584421\n",
      "68 Train Loss 249.46552 Test MSE 275.42506842794995 Test RE 0.2793771634759653 Lambda1 0.013213738\n",
      "69 Train Loss 248.87436 Test MSE 274.22090251409134 Test RE 0.2787657721985019 Lambda1 0.015363008\n",
      "70 Train Loss 248.38934 Test MSE 272.99243304247784 Test RE 0.2781406563948382 Lambda1 0.017364409\n",
      "71 Train Loss 247.84592 Test MSE 271.78407118007885 Test RE 0.2775243988271406 Lambda1 0.019693721\n",
      "72 Train Loss 247.30861 Test MSE 270.9464989411033 Test RE 0.27709643759010755 Lambda1 0.02185259\n",
      "73 Train Loss 246.45403 Test MSE 269.74218619138196 Test RE 0.2764799276465764 Lambda1 0.023880783\n",
      "74 Train Loss 245.79292 Test MSE 268.6217256528999 Test RE 0.27590510614663766 Lambda1 0.025133347\n",
      "Training time: 138.40\n",
      "Training time: 138.40\n",
      "inv_HT_stan_tune7\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 986.29254 Test MSE 842.697567555069 Test RE 0.48868053515093135 Lambda1 0.17436394\n",
      "1 Train Loss 822.6219 Test MSE 837.4823795860306 Test RE 0.48716604391875207 Lambda1 0.1704623\n",
      "2 Train Loss 801.18866 Test MSE 818.5195479715514 Test RE 0.4816190963459619 Lambda1 0.16076449\n",
      "3 Train Loss 721.60187 Test MSE 710.2030722977046 Test RE 0.4486218727584368 Lambda1 0.13231207\n",
      "4 Train Loss 576.52264 Test MSE 555.1530343790954 Test RE 0.3966391051335335 Lambda1 0.08008635\n",
      "5 Train Loss 400.32205 Test MSE 408.3601175028285 Test RE 0.3401816087912054 Lambda1 0.012596424\n",
      "6 Train Loss 320.5043 Test MSE 326.21066515506885 Test RE 0.3040452678994946 Lambda1 -0.0009032972\n",
      "7 Train Loss 286.78925 Test MSE 303.52214685695935 Test RE 0.29328129368327316 Lambda1 0.0030943812\n",
      "8 Train Loss 268.63934 Test MSE 285.4349453719004 Test RE 0.28440861121549643 Lambda1 0.0012076099\n",
      "9 Train Loss 264.97406 Test MSE 285.0887274482589 Test RE 0.2842360723629101 Lambda1 0.0008517173\n",
      "10 Train Loss 260.5986 Test MSE 282.86645913667826 Test RE 0.28312609412931095 Lambda1 0.0007181219\n",
      "11 Train Loss 259.18173 Test MSE 282.1817479053423 Test RE 0.2827832166558242 Lambda1 0.00059524976\n",
      "12 Train Loss 257.72702 Test MSE 281.05464188386156 Test RE 0.2822178973628664 Lambda1 0.00052371284\n",
      "13 Train Loss 256.62082 Test MSE 280.70080127966514 Test RE 0.2820401888560919 Lambda1 0.0009143033\n",
      "14 Train Loss 256.0421 Test MSE 280.810120990535 Test RE 0.2820951041794316 Lambda1 0.00127743\n",
      "15 Train Loss 255.15384 Test MSE 281.2454757420456 Test RE 0.28231369295419934 Lambda1 0.0013595726\n",
      "16 Train Loss 254.74162 Test MSE 280.8311566312731 Test RE 0.28210566993133174 Lambda1 0.0019271903\n",
      "17 Train Loss 254.26814 Test MSE 280.3516050907764 Test RE 0.28186470305834266 Lambda1 0.0029439\n",
      "18 Train Loss 253.67366 Test MSE 279.09984662248377 Test RE 0.2812347418894713 Lambda1 0.0046477104\n",
      "19 Train Loss 253.3677 Test MSE 278.4129809619065 Test RE 0.2808884688909783 Lambda1 0.0058474867\n",
      "20 Train Loss 252.6075 Test MSE 276.71490320193453 Test RE 0.2800305707367165 Lambda1 0.008531089\n",
      "21 Train Loss 251.29274 Test MSE 273.9197663165458 Test RE 0.27861266657629985 Lambda1 0.013395522\n",
      "22 Train Loss 249.7882 Test MSE 271.24998238898434 Test RE 0.27725158011381307 Lambda1 0.019915076\n",
      "23 Train Loss 247.43059 Test MSE 268.47740984099653 Test RE 0.2758309818019274 Lambda1 0.0256736\n",
      "24 Train Loss 244.04878 Test MSE 263.41714261789326 Test RE 0.27321918270744533 Lambda1 0.03503118\n",
      "25 Train Loss 240.4185 Test MSE 257.0444208459971 Test RE 0.26989401970923965 Lambda1 0.047762543\n",
      "26 Train Loss 236.67995 Test MSE 252.86533289125347 Test RE 0.26769102897140007 Lambda1 0.056578908\n",
      "27 Train Loss 232.81389 Test MSE 249.19190630960497 Test RE 0.26573951427315207 Lambda1 0.062839694\n",
      "28 Train Loss 228.0292 Test MSE 243.97344724144173 Test RE 0.2629422966633317 Lambda1 0.07195104\n",
      "29 Train Loss 220.77466 Test MSE 234.3225769116679 Test RE 0.25768921232680636 Lambda1 0.09189244\n",
      "30 Train Loss 212.15256 Test MSE 219.91982997268886 Test RE 0.2496441342683787 Lambda1 0.1197631\n",
      "31 Train Loss 205.26772 Test MSE 212.28672496724212 Test RE 0.24527347783264034 Lambda1 0.1322568\n",
      "32 Train Loss 193.49007 Test MSE 198.4190263954137 Test RE 0.23712690221117 Lambda1 0.1597336\n",
      "33 Train Loss 186.71445 Test MSE 190.88837925808556 Test RE 0.23258350764371524 Lambda1 0.17657425\n",
      "34 Train Loss 178.69197 Test MSE 179.6783758298183 Test RE 0.22565090374603522 Lambda1 0.20376979\n",
      "35 Train Loss 170.60638 Test MSE 167.05808901179222 Test RE 0.21758198034807164 Lambda1 0.2340525\n",
      "36 Train Loss 161.15118 Test MSE 154.78379288276335 Test RE 0.20943627882747354 Lambda1 0.26182938\n",
      "37 Train Loss 150.22124 Test MSE 146.77496235147052 Test RE 0.2039459847630699 Lambda1 0.27873278\n",
      "38 Train Loss 140.02737 Test MSE 138.27857163107905 Test RE 0.197955062569735 Lambda1 0.29519206\n",
      "39 Train Loss 135.07635 Test MSE 131.11160294996012 Test RE 0.1927568116662123 Lambda1 0.308248\n",
      "40 Train Loss 131.06381 Test MSE 124.60646851570233 Test RE 0.18791414164130094 Lambda1 0.32141063\n",
      "41 Train Loss 126.13939 Test MSE 120.19654841607878 Test RE 0.18455897473450297 Lambda1 0.33361894\n",
      "42 Train Loss 122.79962 Test MSE 118.44818388793072 Test RE 0.18321177140624822 Lambda1 0.3402575\n",
      "43 Train Loss 118.065186 Test MSE 117.63031728377221 Test RE 0.18257815111059808 Lambda1 0.3464791\n",
      "44 Train Loss 113.35133 Test MSE 109.65691990569908 Test RE 0.17628168462222016 Lambda1 0.36423388\n",
      "45 Train Loss 109.101036 Test MSE 104.38256950371105 Test RE 0.17198998645850738 Lambda1 0.37890247\n",
      "46 Train Loss 104.29457 Test MSE 100.73302189227016 Test RE 0.16895657685167206 Lambda1 0.38975003\n",
      "47 Train Loss 99.376366 Test MSE 97.3177833243088 Test RE 0.1660677397203841 Lambda1 0.40135476\n",
      "48 Train Loss 96.84702 Test MSE 95.21519944797495 Test RE 0.1642639686904356 Lambda1 0.4092535\n",
      "49 Train Loss 92.460175 Test MSE 89.15433415007283 Test RE 0.15894995197861073 Lambda1 0.42815056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Train Loss 86.93807 Test MSE 84.64235920825921 Test RE 0.15487561735173602 Lambda1 0.4425739\n",
      "51 Train Loss 82.86962 Test MSE 79.68415584825001 Test RE 0.15027099385501322 Lambda1 0.4587062\n",
      "52 Train Loss 80.33585 Test MSE 77.95488130453697 Test RE 0.14863148877221563 Lambda1 0.46324545\n",
      "53 Train Loss 76.17035 Test MSE 74.16932619447321 Test RE 0.1449777440075105 Lambda1 0.46951234\n",
      "54 Train Loss 73.93929 Test MSE 72.2356235783631 Test RE 0.14307537199291953 Lambda1 0.47459894\n",
      "55 Train Loss 71.37857 Test MSE 68.35746354828505 Test RE 0.1391817011640788 Lambda1 0.4882769\n",
      "56 Train Loss 68.37781 Test MSE 65.92505283587019 Test RE 0.13668297215086372 Lambda1 0.49935699\n",
      "57 Train Loss 65.982864 Test MSE 63.735524092529204 Test RE 0.13439402202047074 Lambda1 0.5109473\n",
      "58 Train Loss 64.369125 Test MSE 61.17145915680048 Test RE 0.13166295277132867 Lambda1 0.52409494\n",
      "59 Train Loss 62.27588 Test MSE 59.591241155135016 Test RE 0.12995122748672835 Lambda1 0.5336602\n",
      "60 Train Loss 61.311687 Test MSE 58.72094623153417 Test RE 0.1289988068023514 Lambda1 0.5380981\n",
      "61 Train Loss 60.062607 Test MSE 57.89719042568733 Test RE 0.12809079326970207 Lambda1 0.5436497\n",
      "62 Train Loss 58.835266 Test MSE 56.728767847845496 Test RE 0.12679170618722968 Lambda1 0.5513399\n",
      "63 Train Loss 58.117157 Test MSE 56.11221067279576 Test RE 0.12610080533580773 Lambda1 0.55616623\n",
      "64 Train Loss 57.37105 Test MSE 55.29511700783088 Test RE 0.12517931229883583 Lambda1 0.5630405\n",
      "65 Train Loss 56.256886 Test MSE 54.29472547963299 Test RE 0.12404178048778117 Lambda1 0.571489\n",
      "66 Train Loss 55.527283 Test MSE 54.54904816752026 Test RE 0.12433195400831641 Lambda1 0.56963116\n",
      "67 Train Loss 54.756535 Test MSE 53.30086054880645 Test RE 0.12290124463022514 Lambda1 0.5804243\n",
      "68 Train Loss 53.725086 Test MSE 52.822362376524765 Test RE 0.12234833985057283 Lambda1 0.5859203\n",
      "69 Train Loss 52.32252 Test MSE 51.79367965102746 Test RE 0.12115115358445475 Lambda1 0.5955281\n",
      "70 Train Loss 51.317444 Test MSE 51.32790503690008 Test RE 0.12060517412289454 Lambda1 0.6023678\n",
      "71 Train Loss 50.36942 Test MSE 50.035848825877764 Test RE 0.1190775269556551 Lambda1 0.61823297\n",
      "72 Train Loss 49.61844 Test MSE 48.92597615109072 Test RE 0.11774945895503003 Lambda1 0.62975335\n",
      "73 Train Loss 48.938633 Test MSE 47.94480016870626 Test RE 0.11656278814103506 Lambda1 0.6413485\n",
      "74 Train Loss 48.180576 Test MSE 47.20480525063108 Test RE 0.1156597570612063 Lambda1 0.65126795\n",
      "Training time: 137.51\n",
      "Training time: 137.51\n",
      "inv_HT_stan_tune7\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 2300.8528 Test MSE 1367.8689618954454 Test RE 0.6226032999341695 Lambda1 -0.028734805\n",
      "1 Train Loss 829.20984 Test MSE 847.5117196064651 Test RE 0.49007441126002205 Lambda1 -0.049280066\n",
      "2 Train Loss 814.6915 Test MSE 832.3266504078183 Test RE 0.48566417723202004 Lambda1 -0.050389927\n",
      "3 Train Loss 789.1774 Test MSE 797.6558959580552 Test RE 0.47544136100581486 Lambda1 -0.036364973\n",
      "4 Train Loss 654.33624 Test MSE 669.1082480575654 Test RE 0.4354490649817587 Lambda1 0.034428578\n",
      "5 Train Loss 531.3512 Test MSE 547.9519638386269 Test RE 0.39405824110080884 Lambda1 0.026721563\n",
      "6 Train Loss 403.9597 Test MSE 406.0874860153402 Test RE 0.3392336880191457 Lambda1 0.0073522944\n",
      "7 Train Loss 354.4455 Test MSE 352.8460197820234 Test RE 0.3162145010447303 Lambda1 0.0038157438\n",
      "8 Train Loss 293.81903 Test MSE 303.78977590403787 Test RE 0.2934105648080076 Lambda1 -0.001050445\n",
      "9 Train Loss 274.06076 Test MSE 291.351754632758 Test RE 0.28734125852378845 Lambda1 -0.00060691073\n",
      "10 Train Loss 262.54868 Test MSE 281.0634461836966 Test RE 0.28222231769820005 Lambda1 0.0010550944\n",
      "11 Train Loss 259.3278 Test MSE 279.80030720563724 Test RE 0.28158743002066605 Lambda1 0.003533995\n",
      "12 Train Loss 256.74713 Test MSE 278.9224517650354 Test RE 0.281145351788611 Lambda1 0.005311176\n",
      "13 Train Loss 255.04172 Test MSE 277.8812603088544 Test RE 0.280620116499599 Lambda1 0.0069728773\n",
      "14 Train Loss 253.66621 Test MSE 277.00432541702054 Test RE 0.28017697753661 Lambda1 0.00897405\n",
      "15 Train Loss 251.82738 Test MSE 273.96488389753716 Test RE 0.27863561090803507 Lambda1 0.014048739\n",
      "16 Train Loss 249.93443 Test MSE 271.201392241475 Test RE 0.2772267463821745 Lambda1 0.019659957\n",
      "17 Train Loss 246.393 Test MSE 266.7028870311206 Test RE 0.27491790712611974 Lambda1 0.029570658\n",
      "18 Train Loss 241.40533 Test MSE 259.43775524846313 Test RE 0.27114759681739026 Lambda1 0.04422358\n",
      "19 Train Loss 238.83382 Test MSE 256.2524210383455 Test RE 0.26947790307020164 Lambda1 0.049463484\n",
      "20 Train Loss 234.66966 Test MSE 250.6773106887361 Test RE 0.2665303588770719 Lambda1 0.06182389\n",
      "21 Train Loss 230.47438 Test MSE 245.5788762862366 Test RE 0.26380600341461613 Lambda1 0.07563958\n",
      "22 Train Loss 224.82382 Test MSE 241.3676080989947 Test RE 0.26153430583430015 Lambda1 0.08868499\n",
      "23 Train Loss 218.58403 Test MSE 231.067521049314 Test RE 0.2558931280730431 Lambda1 0.1127597\n",
      "24 Train Loss 214.50793 Test MSE 223.6113518400969 Test RE 0.2517306482538537 Lambda1 0.13312253\n",
      "25 Train Loss 208.54938 Test MSE 217.15241979406014 Test RE 0.248068435242883 Lambda1 0.15429655\n",
      "26 Train Loss 198.98796 Test MSE 208.21837128378547 Test RE 0.24291184529304236 Lambda1 0.18424451\n",
      "27 Train Loss 190.30696 Test MSE 198.59440534694167 Test RE 0.2372316751323735 Lambda1 0.21789743\n",
      "28 Train Loss 179.77353 Test MSE 189.96856622978575 Test RE 0.2320224685735662 Lambda1 0.24814618\n",
      "29 Train Loss 169.74432 Test MSE 180.325163009021 Test RE 0.22605667601710935 Lambda1 0.26928538\n",
      "30 Train Loss 161.59978 Test MSE 173.31411969836157 Test RE 0.2216185675447691 Lambda1 0.2917846\n",
      "31 Train Loss 155.8054 Test MSE 162.68237476853372 Test RE 0.21471353510701305 Lambda1 0.31080815\n",
      "32 Train Loss 147.64961 Test MSE 152.84999317795814 Test RE 0.20812386501535696 Lambda1 0.35257316\n",
      "33 Train Loss 141.562 Test MSE 143.66876364917465 Test RE 0.20177639003780334 Lambda1 0.38848096\n",
      "34 Train Loss 131.87036 Test MSE 136.52596945153 Test RE 0.1966965783454563 Lambda1 0.40590185\n",
      "35 Train Loss 126.49718 Test MSE 130.85861320064186 Test RE 0.19257075244922928 Lambda1 0.41484427\n",
      "36 Train Loss 118.912895 Test MSE 118.58172541203683 Test RE 0.1833150211407039 Lambda1 0.44096178\n",
      "37 Train Loss 113.253975 Test MSE 111.59501034013967 Test RE 0.17783267406746525 Lambda1 0.47094387\n",
      "38 Train Loss 104.88757 Test MSE 102.91545005185502 Test RE 0.1707770311745491 Lambda1 0.4911377\n",
      "39 Train Loss 98.96305 Test MSE 97.59780114100697 Test RE 0.16630648603022324 Lambda1 0.49144524\n",
      "40 Train Loss 94.86464 Test MSE 92.41324916222274 Test RE 0.16182897768169988 Lambda1 0.49881464\n",
      "41 Train Loss 91.83601 Test MSE 89.6718316298664 Test RE 0.15941059801016466 Lambda1 0.5037526\n",
      "42 Train Loss 90.1922 Test MSE 88.75042222621025 Test RE 0.15858948345555876 Lambda1 0.50729775\n",
      "43 Train Loss 87.47481 Test MSE 85.97525757174527 Test RE 0.15609029932160517 Lambda1 0.51189846\n",
      "44 Train Loss 83.81687 Test MSE 82.76613046578663 Test RE 0.1531494693192547 Lambda1 0.5189227\n",
      "45 Train Loss 81.6465 Test MSE 78.79462637858138 Test RE 0.1494298879904914 Lambda1 0.5292355\n",
      "46 Train Loss 80.03177 Test MSE 77.40974238712516 Test RE 0.14811088661927896 Lambda1 0.53253293\n",
      "47 Train Loss 76.4547 Test MSE 72.44748634899001 Test RE 0.14328503413521435 Lambda1 0.54971164\n",
      "48 Train Loss 73.99126 Test MSE 67.47007798343222 Test RE 0.13827535327072804 Lambda1 0.569014\n",
      "49 Train Loss 72.13226 Test MSE 66.53592460478862 Test RE 0.1373147747447685 Lambda1 0.57248515\n",
      "50 Train Loss 68.020325 Test MSE 65.225670782869 Test RE 0.13595602162573917 Lambda1 0.5875479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 Train Loss 65.90798 Test MSE 62.91932313304831 Test RE 0.13353072027183924 Lambda1 0.59019727\n",
      "52 Train Loss 63.97744 Test MSE 61.68735406326337 Test RE 0.13221698268235854 Lambda1 0.5983824\n",
      "53 Train Loss 62.353718 Test MSE 58.78610464643873 Test RE 0.1290703573128801 Lambda1 0.605027\n",
      "54 Train Loss 61.00564 Test MSE 57.038005333699935 Test RE 0.127136817334572 Lambda1 0.60687673\n",
      "55 Train Loss 59.3328 Test MSE 56.03827957334893 Test RE 0.12601770538380358 Lambda1 0.609573\n",
      "56 Train Loss 56.921974 Test MSE 52.31941743102865 Test RE 0.1217644805526065 Lambda1 0.621877\n",
      "57 Train Loss 55.29398 Test MSE 51.476242219010835 Test RE 0.12077932232411526 Lambda1 0.62407035\n",
      "58 Train Loss 53.933243 Test MSE 49.860780947000634 Test RE 0.11886902727646362 Lambda1 0.63287866\n",
      "59 Train Loss 52.581284 Test MSE 49.11869008547747 Test RE 0.11798113200396239 Lambda1 0.6465294\n",
      "60 Train Loss 51.410797 Test MSE 48.68106985671349 Test RE 0.11745438296223633 Lambda1 0.6498711\n",
      "61 Train Loss 50.21405 Test MSE 48.06300525308757 Test RE 0.11670638903421171 Lambda1 0.65202737\n",
      "62 Train Loss 49.470596 Test MSE 47.67076033096188 Test RE 0.11622918968433588 Lambda1 0.65115047\n",
      "63 Train Loss 48.43554 Test MSE 47.07603430196509 Test RE 0.1155018940259952 Lambda1 0.6620438\n",
      "64 Train Loss 47.650604 Test MSE 46.405668025161795 Test RE 0.11467656747437799 Lambda1 0.673313\n",
      "65 Train Loss 46.940407 Test MSE 45.964405541511965 Test RE 0.11413004657794713 Lambda1 0.6745312\n",
      "66 Train Loss 45.82861 Test MSE 45.10222484181465 Test RE 0.11305457805944091 Lambda1 0.6770655\n",
      "67 Train Loss 45.00199 Test MSE 45.17509912188557 Test RE 0.1131458756144347 Lambda1 0.68334603\n",
      "68 Train Loss 44.07709 Test MSE 44.444225891651335 Test RE 0.11222686822297055 Lambda1 0.6859163\n",
      "69 Train Loss 43.752796 Test MSE 43.70264823152275 Test RE 0.11128664451987681 Lambda1 0.6942354\n",
      "70 Train Loss 43.41683 Test MSE 43.142610890636526 Test RE 0.11057129159437155 Lambda1 0.700792\n",
      "71 Train Loss 42.917088 Test MSE 42.58935183636824 Test RE 0.10986002308625466 Lambda1 0.7083579\n",
      "72 Train Loss 42.381466 Test MSE 42.50485587711139 Test RE 0.1097509895324602 Lambda1 0.71095186\n",
      "73 Train Loss 41.956783 Test MSE 42.15254618375322 Test RE 0.1092951969670475 Lambda1 0.71612275\n",
      "74 Train Loss 41.67174 Test MSE 41.89577161861825 Test RE 0.10896179959743979 Lambda1 0.7191556\n",
      "Training time: 144.34\n",
      "Training time: 144.34\n",
      "inv_HT_stan_tune7\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 2425.6797 Test MSE 2378.414030640793 Test RE 0.8209806094119463 Lambda1 0.043210603\n",
      "1 Train Loss 847.77405 Test MSE 848.1395691046462 Test RE 0.4902559049203812 Lambda1 0.091773205\n",
      "2 Train Loss 812.1019 Test MSE 829.9767345311308 Test RE 0.4849781024565467 Lambda1 0.09059872\n",
      "3 Train Loss 740.36456 Test MSE 740.4881352965965 Test RE 0.45808726962666263 Lambda1 0.093463495\n",
      "4 Train Loss 640.1222 Test MSE 649.4356983238565 Test RE 0.42899995764022913 Lambda1 0.08649281\n",
      "5 Train Loss 547.46075 Test MSE 562.1259440930467 Test RE 0.3991222926450543 Lambda1 0.077601455\n",
      "6 Train Loss 450.90323 Test MSE 449.35019642268077 Test RE 0.3568466619286891 Lambda1 0.05062374\n",
      "7 Train Loss 380.78265 Test MSE 375.4898846006099 Test RE 0.32620325597549593 Lambda1 0.02298111\n",
      "8 Train Loss 315.19595 Test MSE 328.31700830092166 Test RE 0.30502529898716557 Lambda1 0.0020550673\n",
      "9 Train Loss 294.26056 Test MSE 311.9069919142477 Test RE 0.2973046663826075 Lambda1 0.0029339439\n",
      "10 Train Loss 271.86487 Test MSE 287.43897647933125 Test RE 0.28540527750494277 Lambda1 0.00306205\n",
      "11 Train Loss 265.17966 Test MSE 284.2820214274326 Test RE 0.2838336408288263 Lambda1 0.0013138459\n",
      "12 Train Loss 261.40225 Test MSE 281.07469873646124 Test RE 0.2822279671161153 Lambda1 0.0014372228\n",
      "13 Train Loss 258.53564 Test MSE 279.7135973343659 Test RE 0.2815437947902372 Lambda1 0.0021982857\n",
      "14 Train Loss 257.49066 Test MSE 279.8179983552958 Test RE 0.28159633195268097 Lambda1 0.0030195632\n",
      "15 Train Loss 256.69232 Test MSE 279.1977866602106 Test RE 0.28128408216246303 Lambda1 0.0038864329\n",
      "16 Train Loss 255.42424 Test MSE 278.4081980476234 Test RE 0.28088605616001444 Lambda1 0.005353785\n",
      "17 Train Loss 254.7979 Test MSE 278.10859135993263 Test RE 0.28073487886987963 Lambda1 0.0061133616\n",
      "18 Train Loss 253.83325 Test MSE 277.63996494673216 Test RE 0.2804982532291713 Lambda1 0.006949376\n",
      "19 Train Loss 252.76724 Test MSE 276.0280432433245 Test RE 0.27968280975646176 Lambda1 0.009763322\n",
      "20 Train Loss 251.35715 Test MSE 274.1694797761356 Test RE 0.27873963347016767 Lambda1 0.012909111\n",
      "21 Train Loss 249.79276 Test MSE 271.1931832618108 Test RE 0.2772225506700396 Lambda1 0.019079562\n",
      "22 Train Loss 248.0979 Test MSE 268.3385127549488 Test RE 0.2757596218388615 Lambda1 0.025916794\n",
      "23 Train Loss 244.36906 Test MSE 263.24802169728815 Test RE 0.27313146157780044 Lambda1 0.035653595\n",
      "24 Train Loss 240.37321 Test MSE 259.166317701636 Test RE 0.27100571519442285 Lambda1 0.04436928\n",
      "25 Train Loss 237.06577 Test MSE 254.9484784010882 Test RE 0.2687914083399544 Lambda1 0.05543458\n",
      "26 Train Loss 232.31677 Test MSE 249.53041168578127 Test RE 0.2659199449451657 Lambda1 0.07310519\n",
      "27 Train Loss 227.989 Test MSE 244.08564810628556 Test RE 0.26300275193468636 Lambda1 0.09095081\n",
      "28 Train Loss 222.22026 Test MSE 238.19947425011654 Test RE 0.25981221767366186 Lambda1 0.11075641\n",
      "29 Train Loss 214.87709 Test MSE 228.275254485968 Test RE 0.2543422965879269 Lambda1 0.13732962\n",
      "30 Train Loss 209.3527 Test MSE 224.5792616587306 Test RE 0.25227487257086706 Lambda1 0.14794214\n",
      "31 Train Loss 203.36984 Test MSE 217.84568844313586 Test RE 0.24846410439067854 Lambda1 0.16417184\n",
      "32 Train Loss 196.47559 Test MSE 208.19556803727158 Test RE 0.24289854356005716 Lambda1 0.18334484\n",
      "33 Train Loss 189.00426 Test MSE 197.17427179902427 Test RE 0.23638194044120786 Lambda1 0.20477253\n",
      "34 Train Loss 180.10126 Test MSE 185.50300005874942 Test RE 0.22927919036287256 Lambda1 0.22944371\n",
      "35 Train Loss 170.44656 Test MSE 177.1851352689855 Test RE 0.2240798540097404 Lambda1 0.24534951\n",
      "36 Train Loss 163.10553 Test MSE 167.68739076686097 Test RE 0.21799140684636253 Lambda1 0.2604981\n",
      "37 Train Loss 157.27841 Test MSE 159.02522368838257 Test RE 0.21228640291184842 Lambda1 0.2726063\n",
      "38 Train Loss 150.74077 Test MSE 151.04109817379697 Test RE 0.2068886843889505 Lambda1 0.2850554\n",
      "39 Train Loss 143.24216 Test MSE 143.12846398216396 Test RE 0.20139661921901256 Lambda1 0.30277714\n",
      "40 Train Loss 134.91617 Test MSE 132.73031494197605 Test RE 0.19394305532185782 Lambda1 0.3202895\n",
      "41 Train Loss 127.53986 Test MSE 118.82098160257263 Test RE 0.18349986055263406 Lambda1 0.3458782\n",
      "42 Train Loss 120.789345 Test MSE 109.40571800007227 Test RE 0.1760796559463946 Lambda1 0.36190632\n",
      "43 Train Loss 116.04729 Test MSE 105.85814526615287 Test RE 0.173201365199465 Lambda1 0.36830783\n",
      "44 Train Loss 111.62832 Test MSE 105.04407295079795 Test RE 0.17253410166199265 Lambda1 0.37067443\n",
      "45 Train Loss 106.96089 Test MSE 100.80475007520207 Test RE 0.16901671994790152 Lambda1 0.3801916\n",
      "46 Train Loss 103.47164 Test MSE 96.32277367342435 Test RE 0.16521659239711894 Lambda1 0.39155823\n",
      "47 Train Loss 98.13455 Test MSE 89.7299791027854 Test RE 0.15946227433416746 Lambda1 0.40839022\n",
      "48 Train Loss 93.138405 Test MSE 85.13573862336656 Test RE 0.15532634564987133 Lambda1 0.42098248\n",
      "49 Train Loss 90.5582 Test MSE 81.61059636730158 Test RE 0.15207661820400867 Lambda1 0.43153062\n",
      "50 Train Loss 86.5446 Test MSE 78.15703930318286 Test RE 0.14882408472645126 Lambda1 0.44208854\n",
      "51 Train Loss 85.02006 Test MSE 76.10760292749912 Test RE 0.14685988793625548 Lambda1 0.44941905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 Train Loss 80.725685 Test MSE 71.38805108279091 Test RE 0.14223351194175238 Lambda1 0.4664627\n",
      "53 Train Loss 78.08232 Test MSE 68.22029079254445 Test RE 0.1390419832454783 Lambda1 0.48442057\n",
      "54 Train Loss 76.79736 Test MSE 67.47694421726777 Test RE 0.13828238903223633 Lambda1 0.49066198\n",
      "55 Train Loss 74.96903 Test MSE 66.25168700425317 Test RE 0.13702116046727167 Lambda1 0.50024617\n",
      "56 Train Loss 72.80273 Test MSE 63.94784445012363 Test RE 0.13461768741432711 Lambda1 0.51150686\n",
      "57 Train Loss 71.45803 Test MSE 62.63165101915994 Test RE 0.13322511405894272 Lambda1 0.52025104\n",
      "58 Train Loss 69.2615 Test MSE 62.42124060547887 Test RE 0.13300114156419568 Lambda1 0.52250373\n",
      "59 Train Loss 67.69728 Test MSE 62.33162134514924 Test RE 0.1329056312448424 Lambda1 0.52048415\n",
      "60 Train Loss 65.96568 Test MSE 61.44146060040055 Test RE 0.1319532028893886 Lambda1 0.5312323\n",
      "61 Train Loss 65.03984 Test MSE 59.90019913890038 Test RE 0.13028766588757762 Lambda1 0.541453\n",
      "62 Train Loss 64.69061 Test MSE 59.34233104176246 Test RE 0.12967954307455504 Lambda1 0.5448735\n",
      "63 Train Loss 64.0068 Test MSE 58.916182629040655 Test RE 0.12921307755725026 Lambda1 0.54632205\n",
      "64 Train Loss 62.441456 Test MSE 57.56399481385818 Test RE 0.12772168320085192 Lambda1 0.5494062\n",
      "65 Train Loss 61.442356 Test MSE 55.98177658159225 Test RE 0.12595415799098206 Lambda1 0.5565599\n",
      "66 Train Loss 59.758286 Test MSE 53.24772064091791 Test RE 0.12283996429278271 Lambda1 0.5669916\n",
      "67 Train Loss 58.0637 Test MSE 50.86010398574785 Test RE 0.12005432008789421 Lambda1 0.5851991\n",
      "68 Train Loss 57.110687 Test MSE 49.886743290158556 Test RE 0.11889997060296228 Lambda1 0.59222704\n",
      "69 Train Loss 55.51579 Test MSE 48.42465877526633 Test RE 0.1171446489373847 Lambda1 0.6019979\n",
      "70 Train Loss 54.342697 Test MSE 47.86369732110984 Test RE 0.11646415830754159 Lambda1 0.6042417\n",
      "71 Train Loss 53.289738 Test MSE 47.53354927086636 Test RE 0.11606179752903018 Lambda1 0.6035025\n",
      "72 Train Loss 52.46855 Test MSE 47.980326314507614 Test RE 0.11660596550095079 Lambda1 0.60112995\n",
      "73 Train Loss 51.729324 Test MSE 47.74248664032461 Test RE 0.11631659711428342 Lambda1 0.6037997\n",
      "74 Train Loss 51.17236 Test MSE 47.18792911244411 Test RE 0.11563908051653907 Lambda1 0.6075333\n",
      "Training time: 140.14\n",
      "Training time: 140.14\n",
      "inv_HT_stan_tune8\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.0575 Test MSE 858.2635432652587 Test RE 0.49317323986716705 Lambda1 -0.052728985\n",
      "1 Train Loss 837.8802 Test MSE 857.8604884982334 Test RE 0.4930574251098914 Lambda1 -0.063958816\n",
      "2 Train Loss 837.70056 Test MSE 857.6616409131525 Test RE 0.493000277728853 Lambda1 -0.2416392\n",
      "3 Train Loss 835.88153 Test MSE 854.4870646067243 Test RE 0.49208702834248497 Lambda1 -0.6759409\n",
      "4 Train Loss 828.6671 Test MSE 845.2813613829331 Test RE 0.4894291331224148 Lambda1 -0.7129287\n",
      "5 Train Loss 816.06226 Test MSE 806.7913224813353 Test RE 0.47815618728432085 Lambda1 -0.89606297\n",
      "6 Train Loss 783.95337 Test MSE 790.2962130582007 Test RE 0.4732429153156791 Lambda1 -1.0061036\n",
      "7 Train Loss 766.12573 Test MSE 762.5646698016784 Test RE 0.46486570822368806 Lambda1 -0.9853913\n",
      "8 Train Loss 745.5779 Test MSE 747.5702214970765 Test RE 0.4602726479418268 Lambda1 -0.9982729\n",
      "9 Train Loss 729.5665 Test MSE 729.3028318358403 Test RE 0.45461432946152025 Lambda1 -1.0326823\n",
      "10 Train Loss 705.0762 Test MSE 695.55138790654 Test RE 0.44397015999002387 Lambda1 -1.1546578\n",
      "11 Train Loss 690.9821 Test MSE 689.4883246970743 Test RE 0.4420308992913763 Lambda1 -1.1874326\n",
      "12 Train Loss 671.3074 Test MSE 660.5350994777069 Test RE 0.4326504110586677 Lambda1 -1.3220161\n",
      "13 Train Loss 649.39984 Test MSE 633.1274908307457 Test RE 0.4235793272785842 Lambda1 -1.4494213\n",
      "14 Train Loss 636.55225 Test MSE 611.3663964031296 Test RE 0.4162363003162956 Lambda1 -1.4900832\n",
      "15 Train Loss 617.41565 Test MSE 601.789996814053 Test RE 0.41296348560001983 Lambda1 -1.4500886\n",
      "16 Train Loss 597.03735 Test MSE 591.3268651503349 Test RE 0.40935771123799725 Lambda1 -1.4256372\n",
      "17 Train Loss 577.5777 Test MSE 566.3513932580377 Test RE 0.40061956702800494 Lambda1 -1.4440161\n",
      "18 Train Loss 543.80994 Test MSE 527.9560720107803 Test RE 0.38680142480235824 Lambda1 -1.4797864\n",
      "19 Train Loss 523.6392 Test MSE 516.6957372220302 Test RE 0.38265431052243487 Lambda1 -1.4932221\n",
      "20 Train Loss 511.04364 Test MSE 507.4593832410234 Test RE 0.37921876041796065 Lambda1 -1.4869683\n",
      "21 Train Loss 489.98233 Test MSE 491.8852337955311 Test RE 0.3733542190003628 Lambda1 -1.4124752\n",
      "22 Train Loss 430.3725 Test MSE 425.128283396916 Test RE 0.347095649459559 Lambda1 -1.2133098\n",
      "23 Train Loss 377.98886 Test MSE 366.46163161072286 Test RE 0.32225779121883447 Lambda1 -1.1874539\n",
      "24 Train Loss 346.76727 Test MSE 354.14471687432115 Test RE 0.31679590149317194 Lambda1 -1.1924605\n",
      "25 Train Loss 304.5137 Test MSE 301.5378551510565 Test RE 0.2923210509008664 Lambda1 -1.1494263\n",
      "26 Train Loss 284.03598 Test MSE 277.5852024585148 Test RE 0.2804705887324262 Lambda1 -1.1695149\n",
      "27 Train Loss 258.0705 Test MSE 254.37103754511978 Test RE 0.26848683871238815 Lambda1 -1.1904556\n",
      "28 Train Loss 237.43224 Test MSE 236.25629587796334 Test RE 0.258750302356573 Lambda1 -1.2104597\n",
      "29 Train Loss 223.23181 Test MSE 221.85375505957325 Test RE 0.2507393886215303 Lambda1 -1.2309259\n",
      "30 Train Loss 211.24358 Test MSE 211.12447617548486 Test RE 0.24460113231518008 Lambda1 -1.2564238\n",
      "31 Train Loss 204.24786 Test MSE 204.90444561336773 Test RE 0.2409710450480661 Lambda1 -1.2748718\n",
      "32 Train Loss 192.79738 Test MSE 191.8478574321427 Test RE 0.2331673019066795 Lambda1 -1.3448985\n",
      "33 Train Loss 186.41965 Test MSE 188.6422491885179 Test RE 0.23121108599093684 Lambda1 -1.3505359\n",
      "34 Train Loss 182.05936 Test MSE 186.29222313377988 Test RE 0.2297664072010803 Lambda1 -1.3687577\n",
      "35 Train Loss 178.52388 Test MSE 186.13816304235706 Test RE 0.22967138134744153 Lambda1 -1.3872771\n",
      "36 Train Loss 173.20169 Test MSE 177.96668828606164 Test RE 0.22457351164122455 Lambda1 -1.459814\n",
      "37 Train Loss 167.96011 Test MSE 170.61692797991526 Test RE 0.21988734205522156 Lambda1 -1.506619\n",
      "38 Train Loss 164.74376 Test MSE 168.39208388752508 Test RE 0.21844897253271628 Lambda1 -1.5412331\n",
      "39 Train Loss 162.98091 Test MSE 169.95379543748695 Test RE 0.2194596107237898 Lambda1 -1.5540347\n",
      "40 Train Loss 157.07452 Test MSE 164.96188374467766 Test RE 0.2162125875489868 Lambda1 -1.5845435\n",
      "41 Train Loss 154.7928 Test MSE 160.26040711214435 Test RE 0.213109245483982 Lambda1 -1.6030911\n",
      "42 Train Loss 148.5936 Test MSE 151.49852715500813 Test RE 0.2072017294391227 Lambda1 -1.6729608\n",
      "43 Train Loss 144.87207 Test MSE 148.34453929910748 Test RE 0.2050335601377142 Lambda1 -1.6871729\n",
      "44 Train Loss 142.6488 Test MSE 148.3969952549382 Test RE 0.20506980778392614 Lambda1 -1.6614083\n",
      "45 Train Loss 139.36816 Test MSE 149.24897015640133 Test RE 0.20565763733986772 Lambda1 -1.6355082\n",
      "46 Train Loss 134.03157 Test MSE 144.90920860459272 Test RE 0.2026455927732622 Lambda1 -1.6448553\n",
      "47 Train Loss 130.87468 Test MSE 138.48304671440235 Test RE 0.19810136841095088 Lambda1 -1.6989636\n",
      "48 Train Loss 126.92149 Test MSE 130.6080805553031 Test RE 0.19238632296187105 Lambda1 -1.7654241\n",
      "49 Train Loss 123.56892 Test MSE 126.07946498454753 Test RE 0.1890215626803901 Lambda1 -1.7841425\n",
      "50 Train Loss 118.13419 Test MSE 125.83844723102993 Test RE 0.18884080625989558 Lambda1 -1.7793559\n",
      "51 Train Loss 115.04237 Test MSE 124.48041664612838 Test RE 0.18781907064454584 Lambda1 -1.7753693\n",
      "52 Train Loss 113.50519 Test MSE 121.88868322227448 Test RE 0.185853550986072 Lambda1 -1.7656693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 Train Loss 112.43891 Test MSE 121.79693409685241 Test RE 0.18578358915540297 Lambda1 -1.7639254\n",
      "54 Train Loss 110.93756 Test MSE 120.38237328508743 Test RE 0.18470158449598667 Lambda1 -1.7705065\n",
      "55 Train Loss 109.862526 Test MSE 118.8525037986811 Test RE 0.18352419941488904 Lambda1 -1.7770222\n",
      "56 Train Loss 108.87399 Test MSE 118.42655332449986 Test RE 0.18319504191891048 Lambda1 -1.7785705\n",
      "57 Train Loss 108.364334 Test MSE 117.79459907781508 Test RE 0.1827056004015776 Lambda1 -1.7788651\n",
      "58 Train Loss 107.13974 Test MSE 117.01021787029633 Test RE 0.18209627615844215 Lambda1 -1.7889682\n",
      "59 Train Loss 106.45892 Test MSE 118.14179489611601 Test RE 0.1829746617457155 Lambda1 -1.795286\n",
      "60 Train Loss 105.898415 Test MSE 118.25495348218001 Test RE 0.18306226917730517 Lambda1 -1.7988342\n",
      "61 Train Loss 104.952866 Test MSE 116.87065185385188 Test RE 0.18198764447121316 Lambda1 -1.8140693\n",
      "62 Train Loss 103.08311 Test MSE 113.9053027672553 Test RE 0.17966403207235898 Lambda1 -1.8434769\n",
      "63 Train Loss 101.77793 Test MSE 111.58040607533543 Test RE 0.177821037344503 Lambda1 -1.8653882\n",
      "64 Train Loss 100.888435 Test MSE 110.2725535319578 Test RE 0.17677583054255397 Lambda1 -1.8832041\n",
      "65 Train Loss 100.22002 Test MSE 109.81965639467309 Test RE 0.17641244165481218 Lambda1 -1.8931243\n",
      "66 Train Loss 98.59801 Test MSE 107.54941797008142 Test RE 0.17457948292498468 Lambda1 -1.9049953\n",
      "67 Train Loss 96.19325 Test MSE 103.85135449927404 Test RE 0.17155178974306856 Lambda1 -1.9098841\n",
      "68 Train Loss 93.075714 Test MSE 101.83611924088144 Test RE 0.16987915460922343 Lambda1 -1.912789\n",
      "69 Train Loss 90.64733 Test MSE 98.98092054928411 Test RE 0.16748075688660902 Lambda1 -1.9200484\n",
      "70 Train Loss 89.68129 Test MSE 98.10623382015405 Test RE 0.16673910753742122 Lambda1 -1.9242244\n",
      "71 Train Loss 89.155426 Test MSE 97.5021803127865 Test RE 0.16622499720201175 Lambda1 -1.9248644\n",
      "72 Train Loss 88.580086 Test MSE 96.38032886975995 Test RE 0.1652659454888094 Lambda1 -1.9285066\n",
      "73 Train Loss 88.032646 Test MSE 95.9757073099618 Test RE 0.1649186728849778 Lambda1 -1.9348077\n",
      "74 Train Loss 87.46368 Test MSE 96.0052231802972 Test RE 0.16494403005144484 Lambda1 -1.941193\n",
      "Training time: 125.13\n",
      "Training time: 125.13\n",
      "inv_HT_stan_tune8\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 838.60406 Test MSE 859.5437329056443 Test RE 0.4935409124893469 Lambda1 -0.047213353\n",
      "1 Train Loss 838.0213 Test MSE 858.2610540677344 Test RE 0.49317252469843614 Lambda1 -0.04703832\n",
      "2 Train Loss 837.9947 Test MSE 858.083095744853 Test RE 0.49312139299156055 Lambda1 -0.046966344\n",
      "3 Train Loss 837.85565 Test MSE 857.798503791105 Test RE 0.49303961185669504 Lambda1 -0.053254526\n",
      "4 Train Loss 837.83795 Test MSE 857.8827620866612 Test RE 0.4930638259682016 Lambda1 -0.06400302\n",
      "5 Train Loss 837.7232 Test MSE 857.5733217630126 Test RE 0.49297489330889954 Lambda1 -0.14605257\n",
      "6 Train Loss 835.011 Test MSE 853.0744224596488 Test RE 0.4916800997614725 Lambda1 -0.63575435\n",
      "7 Train Loss 830.1144 Test MSE 844.7342585371124 Test RE 0.48927071757842805 Lambda1 -0.63218176\n",
      "8 Train Loss 814.61475 Test MSE 821.8685451369428 Test RE 0.4826033700325218 Lambda1 -0.5912895\n",
      "9 Train Loss 796.7197 Test MSE 801.502839799945 Test RE 0.4765864639888105 Lambda1 -0.53419757\n",
      "10 Train Loss 774.9363 Test MSE 775.1622746736822 Test RE 0.46868978133335504 Lambda1 -0.5195193\n",
      "11 Train Loss 745.48376 Test MSE 745.9202291540271 Test RE 0.45976442420764563 Lambda1 -0.4704568\n",
      "12 Train Loss 722.619 Test MSE 724.3740530151781 Test RE 0.4530755365249694 Lambda1 -0.4741431\n",
      "13 Train Loss 717.9931 Test MSE 718.0136950239475 Test RE 0.4510820386524174 Lambda1 -0.47551006\n",
      "14 Train Loss 708.5562 Test MSE 706.357550359172 Test RE 0.44740565235862795 Lambda1 -0.45543364\n",
      "15 Train Loss 704.0479 Test MSE 702.8428681766858 Test RE 0.4462911674590306 Lambda1 -0.43172908\n",
      "16 Train Loss 700.3966 Test MSE 702.4854149999187 Test RE 0.44617766521603724 Lambda1 -0.4024405\n",
      "17 Train Loss 696.21564 Test MSE 700.0814808855461 Test RE 0.4454135917807027 Lambda1 -0.4056636\n",
      "18 Train Loss 693.4843 Test MSE 697.8084494454555 Test RE 0.4446899172786918 Lambda1 -0.4230405\n",
      "19 Train Loss 689.96344 Test MSE 693.0127920867255 Test RE 0.44315922706093663 Lambda1 -0.4608041\n",
      "20 Train Loss 674.8515 Test MSE 676.8044081110652 Test RE 0.4379461975582977 Lambda1 -0.56015795\n",
      "21 Train Loss 650.93805 Test MSE 652.5194223389101 Test RE 0.4300172645006501 Lambda1 -0.6769458\n",
      "22 Train Loss 626.645 Test MSE 622.1956435766574 Test RE 0.41990655435208857 Lambda1 -0.7135559\n",
      "23 Train Loss 583.94507 Test MSE 573.2818026808266 Test RE 0.40306329312490496 Lambda1 -0.76721287\n",
      "24 Train Loss 542.46204 Test MSE 518.4197226594628 Test RE 0.383292153118777 Lambda1 -0.860376\n",
      "25 Train Loss 494.92236 Test MSE 474.72853687249255 Test RE 0.36678523101383625 Lambda1 -0.89807594\n",
      "26 Train Loss 418.76755 Test MSE 396.48545731954334 Test RE 0.33519906756402096 Lambda1 -0.96615106\n",
      "27 Train Loss 353.76965 Test MSE 342.69330759613973 Test RE 0.3116319536024793 Lambda1 -0.98635465\n",
      "28 Train Loss 312.1605 Test MSE 299.15631972117416 Test RE 0.291164391844217 Lambda1 -1.018149\n",
      "29 Train Loss 273.0765 Test MSE 256.7870273382336 Test RE 0.26975885558513696 Lambda1 -1.0342212\n",
      "30 Train Loss 253.60184 Test MSE 252.72680029578433 Test RE 0.2676176914895181 Lambda1 -1.0232003\n",
      "31 Train Loss 239.83304 Test MSE 252.40375958574126 Test RE 0.2674465995185636 Lambda1 -0.9946103\n",
      "32 Train Loss 235.7016 Test MSE 254.3344678286918 Test RE 0.26846753847961424 Lambda1 -0.9707158\n",
      "33 Train Loss 231.07002 Test MSE 250.29098689158383 Test RE 0.26632490206539483 Lambda1 -0.9715158\n",
      "34 Train Loss 226.75668 Test MSE 243.49384993394958 Test RE 0.2626837266136321 Lambda1 -0.98784494\n",
      "35 Train Loss 220.9264 Test MSE 237.0524705725686 Test RE 0.2591859249760578 Lambda1 -0.9972386\n",
      "36 Train Loss 216.17459 Test MSE 227.86360873298187 Test RE 0.2541128670564935 Lambda1 -1.0382975\n",
      "37 Train Loss 212.68782 Test MSE 221.1213110170525 Test RE 0.2503251418509899 Lambda1 -1.0834597\n",
      "38 Train Loss 206.69257 Test MSE 209.8792996108874 Test RE 0.24387875750971902 Lambda1 -1.1364654\n",
      "39 Train Loss 200.24371 Test MSE 206.09233000919113 Test RE 0.2416685215750421 Lambda1 -1.1467063\n",
      "40 Train Loss 193.24281 Test MSE 199.0219314081117 Test RE 0.2374868892645648 Lambda1 -1.200487\n",
      "41 Train Loss 188.98654 Test MSE 193.41856463980065 Test RE 0.23411985618489822 Lambda1 -1.236578\n",
      "42 Train Loss 183.83405 Test MSE 189.48848038805963 Test RE 0.23172910117691925 Lambda1 -1.2591659\n",
      "43 Train Loss 179.29968 Test MSE 184.2006582354642 Test RE 0.22847293436189248 Lambda1 -1.2978022\n",
      "44 Train Loss 173.913 Test MSE 175.6625675702194 Test RE 0.2231150076815292 Lambda1 -1.3517855\n",
      "45 Train Loss 171.19656 Test MSE 175.14762665656448 Test RE 0.22278774567678014 Lambda1 -1.36411\n",
      "46 Train Loss 165.89308 Test MSE 171.70395315990214 Test RE 0.2205866969659891 Lambda1 -1.4188546\n",
      "47 Train Loss 162.79405 Test MSE 166.79743647504515 Test RE 0.21741217284434325 Lambda1 -1.4320668\n",
      "48 Train Loss 159.34276 Test MSE 162.26692641584557 Test RE 0.21443919866667419 Lambda1 -1.4471154\n",
      "49 Train Loss 156.57872 Test MSE 160.86236135832354 Test RE 0.21350910026777847 Lambda1 -1.4691263\n",
      "50 Train Loss 154.37982 Test MSE 159.97923143731938 Test RE 0.21292221413023843 Lambda1 -1.483815\n",
      "51 Train Loss 152.59122 Test MSE 160.2222298430056 Test RE 0.2130838605066167 Lambda1 -1.481079\n",
      "52 Train Loss 148.64574 Test MSE 157.33921949543708 Test RE 0.21115806148934252 Lambda1 -1.5195576\n",
      "53 Train Loss 146.1571 Test MSE 152.70083600540798 Test RE 0.20802229240946207 Lambda1 -1.5687286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 Train Loss 144.81406 Test MSE 148.73974532592675 Test RE 0.20530649435658718 Lambda1 -1.5880212\n",
      "55 Train Loss 142.85652 Test MSE 147.34042006270025 Test RE 0.2043384630497104 Lambda1 -1.5842435\n",
      "56 Train Loss 140.76387 Test MSE 145.53639889199985 Test RE 0.2030836605854556 Lambda1 -1.5967084\n",
      "57 Train Loss 138.04153 Test MSE 140.49502081530056 Test RE 0.1995352535100637 Lambda1 -1.6239558\n",
      "58 Train Loss 135.82915 Test MSE 135.9215320304823 Test RE 0.19626068085455545 Lambda1 -1.6451457\n",
      "59 Train Loss 133.13257 Test MSE 135.56706807614887 Test RE 0.19600460387163 Lambda1 -1.6588273\n",
      "60 Train Loss 131.5082 Test MSE 135.36748513698203 Test RE 0.1958602709448131 Lambda1 -1.6811707\n",
      "61 Train Loss 130.24617 Test MSE 132.685077261217 Test RE 0.1939100022785077 Lambda1 -1.7201377\n",
      "62 Train Loss 129.1581 Test MSE 131.7494608719551 Test RE 0.19322512369780814 Lambda1 -1.7340873\n",
      "63 Train Loss 127.748795 Test MSE 131.07083368013542 Test RE 0.1927268403849158 Lambda1 -1.7418087\n",
      "64 Train Loss 126.52369 Test MSE 129.31845779573646 Test RE 0.19143415648584083 Lambda1 -1.7617915\n",
      "65 Train Loss 125.62329 Test MSE 129.73855017984386 Test RE 0.19174484231949254 Lambda1 -1.7616248\n",
      "66 Train Loss 124.81652 Test MSE 129.36503523376936 Test RE 0.19146862840105922 Lambda1 -1.7668793\n",
      "67 Train Loss 123.77786 Test MSE 128.14155944818714 Test RE 0.1905610657837909 Lambda1 -1.7943057\n",
      "68 Train Loss 123.253685 Test MSE 128.07437580168136 Test RE 0.19051110437450502 Lambda1 -1.8047463\n",
      "69 Train Loss 122.63834 Test MSE 127.8944833581537 Test RE 0.19037726202425145 Lambda1 -1.806208\n",
      "70 Train Loss 121.553055 Test MSE 126.55032641787182 Test RE 0.18937419750574877 Lambda1 -1.8136051\n",
      "71 Train Loss 121.11356 Test MSE 125.66473834907964 Test RE 0.188710422206417 Lambda1 -1.8212664\n",
      "72 Train Loss 120.82299 Test MSE 125.12494465375832 Test RE 0.18830468260444572 Lambda1 -1.8228601\n",
      "73 Train Loss 120.19556 Test MSE 124.82247273882945 Test RE 0.18807694487769494 Lambda1 -1.8221619\n",
      "74 Train Loss 118.9176 Test MSE 122.4738670330861 Test RE 0.1862991553778109 Lambda1 -1.8432937\n",
      "Training time: 123.63\n",
      "Training time: 123.63\n",
      "inv_HT_stan_tune8\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 838.0483 Test MSE 858.1578767672023 Test RE 0.4931428800239351 Lambda1 -0.12880373\n",
      "1 Train Loss 838.0021 Test MSE 858.3085084444124 Test RE 0.493186158587073 Lambda1 -0.12906721\n",
      "2 Train Loss 837.8214 Test MSE 857.7671087186698 Test RE 0.49303058925063237 Lambda1 -0.13016799\n",
      "3 Train Loss 837.79486 Test MSE 857.6735999655482 Test RE 0.49300371486263556 Lambda1 -0.13042876\n",
      "4 Train Loss 836.7249 Test MSE 855.4486105009391 Test RE 0.4923638208443036 Lambda1 -0.119959645\n",
      "5 Train Loss 814.7303 Test MSE 831.9190347667319 Test RE 0.48554524041912384 Lambda1 0.15414834\n",
      "6 Train Loss 771.4279 Test MSE 780.733290357954 Test RE 0.47037097996749583 Lambda1 0.18799612\n",
      "7 Train Loss 710.98114 Test MSE 715.5417372372706 Test RE 0.45030488276942865 Lambda1 0.20141374\n",
      "8 Train Loss 661.8818 Test MSE 660.7073371214083 Test RE 0.4327068151999953 Lambda1 0.1017743\n",
      "9 Train Loss 649.1397 Test MSE 655.0019298946012 Test RE 0.43083448744727065 Lambda1 0.09421888\n",
      "10 Train Loss 637.1784 Test MSE 644.7415524095292 Test RE 0.42744673185569326 Lambda1 0.08544375\n",
      "11 Train Loss 632.7509 Test MSE 639.7916716264594 Test RE 0.42580274989685374 Lambda1 0.07811101\n",
      "12 Train Loss 627.03864 Test MSE 633.417932564308 Test RE 0.42367647280611 Lambda1 0.104950204\n",
      "13 Train Loss 622.17145 Test MSE 626.2247067163865 Test RE 0.4212639248067013 Lambda1 0.12682335\n",
      "14 Train Loss 616.1752 Test MSE 621.2915561554613 Test RE 0.41960136880060755 Lambda1 0.12813279\n",
      "15 Train Loss 601.2879 Test MSE 608.1261101776334 Test RE 0.41513179359599045 Lambda1 0.15635733\n",
      "16 Train Loss 576.2283 Test MSE 562.0022891339457 Test RE 0.39907839130430456 Lambda1 0.21153305\n",
      "17 Train Loss 519.9475 Test MSE 516.9852143498568 Test RE 0.38276148593780834 Lambda1 0.20172223\n",
      "18 Train Loss 429.78534 Test MSE 407.61124389787324 Test RE 0.3398695436321811 Lambda1 0.18465145\n",
      "19 Train Loss 355.85565 Test MSE 358.4398798535232 Test RE 0.31871120488463694 Lambda1 0.18917489\n",
      "20 Train Loss 288.78058 Test MSE 292.9741243612568 Test RE 0.2881401666985705 Lambda1 0.19319592\n",
      "21 Train Loss 266.58798 Test MSE 272.0707914455819 Test RE 0.2776707482688592 Lambda1 0.18500042\n",
      "22 Train Loss 253.02295 Test MSE 263.95227611271997 Test RE 0.2734965651017612 Lambda1 0.16227278\n",
      "23 Train Loss 239.32098 Test MSE 256.31577729317235 Test RE 0.26951121408334766 Lambda1 0.14472161\n",
      "24 Train Loss 231.27599 Test MSE 251.71694406622927 Test RE 0.2670824773615778 Lambda1 0.13127917\n",
      "25 Train Loss 223.91847 Test MSE 246.5726300195726 Test RE 0.264339220133344 Lambda1 0.13586242\n",
      "26 Train Loss 217.16255 Test MSE 237.08757296070368 Test RE 0.2592051142047848 Lambda1 0.1613202\n",
      "27 Train Loss 212.15009 Test MSE 231.9772541294411 Test RE 0.2563963700679572 Lambda1 0.20178165\n",
      "28 Train Loss 209.5028 Test MSE 230.5940976427597 Test RE 0.2556308499476047 Lambda1 0.2224725\n",
      "29 Train Loss 204.43924 Test MSE 221.32531508189746 Test RE 0.2504405888269604 Lambda1 0.28574857\n",
      "30 Train Loss 191.5381 Test MSE 207.77577373652855 Test RE 0.2426535362395485 Lambda1 0.3703396\n",
      "31 Train Loss 178.90927 Test MSE 187.21646543493713 Test RE 0.23033566631494354 Lambda1 0.41008052\n",
      "32 Train Loss 166.37209 Test MSE 169.5390119516465 Test RE 0.21919164427419915 Lambda1 0.41286343\n",
      "33 Train Loss 154.53441 Test MSE 158.83883140625602 Test RE 0.21216195677844574 Lambda1 0.4473859\n",
      "34 Train Loss 143.40991 Test MSE 140.54184393903063 Test RE 0.19956850054440423 Lambda1 0.5138656\n",
      "35 Train Loss 134.0853 Test MSE 134.48485175720305 Test RE 0.19522069516410875 Lambda1 0.56312215\n",
      "36 Train Loss 129.32405 Test MSE 135.48154204330942 Test RE 0.1959427668002002 Lambda1 0.5783184\n",
      "37 Train Loss 126.34838 Test MSE 131.50338737367284 Test RE 0.19304459239166172 Lambda1 0.5678578\n",
      "38 Train Loss 123.2037 Test MSE 126.06847484584647 Test RE 0.18901332415215757 Lambda1 0.56694\n",
      "39 Train Loss 119.21441 Test MSE 119.22184123003792 Test RE 0.18380913148200878 Lambda1 0.6125798\n",
      "40 Train Loss 116.265335 Test MSE 115.6614658941451 Test RE 0.18104374169570803 Lambda1 0.6400952\n",
      "41 Train Loss 110.20381 Test MSE 111.33132934760901 Test RE 0.17762245486656017 Lambda1 0.6796053\n",
      "42 Train Loss 107.33491 Test MSE 106.70312217356924 Test RE 0.1738912519516884 Lambda1 0.709804\n",
      "43 Train Loss 105.88956 Test MSE 105.2360988286796 Test RE 0.17269173018846784 Lambda1 0.729659\n",
      "44 Train Loss 103.66589 Test MSE 103.98468227891186 Test RE 0.1716618763321327 Lambda1 0.7700416\n",
      "45 Train Loss 99.296165 Test MSE 97.26927468644975 Test RE 0.16602634582670509 Lambda1 0.8334591\n",
      "46 Train Loss 90.6629 Test MSE 86.69420397736361 Test RE 0.15674157350555482 Lambda1 0.9125899\n",
      "47 Train Loss 88.10081 Test MSE 84.69467163842769 Test RE 0.15492346968330348 Lambda1 0.9321857\n",
      "48 Train Loss 85.10617 Test MSE 83.65401660601945 Test RE 0.15396874496335275 Lambda1 0.9611008\n",
      "49 Train Loss 83.00977 Test MSE 81.49484236861112 Test RE 0.1519687294987513 Lambda1 0.98465514\n",
      "50 Train Loss 81.48525 Test MSE 80.7167739761862 Test RE 0.15124153226329043 Lambda1 1.0286713\n",
      "51 Train Loss 79.49538 Test MSE 78.76370936295893 Test RE 0.1494005688626 Lambda1 1.0941383\n",
      "52 Train Loss 78.34122 Test MSE 77.10695197443017 Test RE 0.147820932824542 Lambda1 1.109753\n",
      "53 Train Loss 77.246544 Test MSE 77.08115902389142 Test RE 0.1477962070584911 Lambda1 1.1274856\n",
      "54 Train Loss 76.150505 Test MSE 76.18080942098062 Test RE 0.1469305018627078 Lambda1 1.1387013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 Train Loss 74.5823 Test MSE 75.44352469146172 Test RE 0.14621776988888377 Lambda1 1.1311692\n",
      "56 Train Loss 73.91852 Test MSE 75.30839502527466 Test RE 0.1460867631926219 Lambda1 1.141224\n",
      "57 Train Loss 72.55174 Test MSE 73.74897249583401 Test RE 0.14456633051286233 Lambda1 1.1879267\n",
      "58 Train Loss 71.649536 Test MSE 72.4976607869823 Test RE 0.1433346424936429 Lambda1 1.2027551\n",
      "59 Train Loss 70.58639 Test MSE 71.22665504804623 Test RE 0.1420726382799959 Lambda1 1.1987919\n",
      "60 Train Loss 69.76384 Test MSE 70.25859658514362 Test RE 0.14110386374510364 Lambda1 1.2106165\n",
      "61 Train Loss 69.10952 Test MSE 69.67666508691548 Test RE 0.14051828757564405 Lambda1 1.2283553\n",
      "62 Train Loss 67.98471 Test MSE 68.77663138538078 Test RE 0.13960777994782128 Lambda1 1.2427264\n",
      "63 Train Loss 67.19558 Test MSE 67.12002393644485 Test RE 0.1379161807671063 Lambda1 1.2657313\n",
      "64 Train Loss 64.59497 Test MSE 63.72830566877516 Test RE 0.13438641134742713 Lambda1 1.3430935\n",
      "65 Train Loss 62.311558 Test MSE 62.35412355662267 Test RE 0.13292961907550113 Lambda1 1.3731581\n",
      "66 Train Loss 60.970417 Test MSE 61.56669320605799 Test RE 0.13208761075845304 Lambda1 1.3913858\n",
      "67 Train Loss 60.134598 Test MSE 60.378436432188366 Test RE 0.1308067338498069 Lambda1 1.4185615\n",
      "68 Train Loss 58.580875 Test MSE 58.844569704324435 Test RE 0.12913452409740986 Lambda1 1.470269\n",
      "69 Train Loss 57.412804 Test MSE 58.931271233262 Test RE 0.12922962241885746 Lambda1 1.5166255\n",
      "70 Train Loss 57.13 Test MSE 58.87369536305263 Test RE 0.12916647830161024 Lambda1 1.5321227\n",
      "71 Train Loss 56.62416 Test MSE 57.884926824903246 Test RE 0.12807722665676807 Lambda1 1.5729607\n",
      "72 Train Loss 56.32543 Test MSE 57.78857048572831 Test RE 0.1279705823761606 Lambda1 1.5867194\n",
      "73 Train Loss 56.126247 Test MSE 57.837929578406175 Test RE 0.1280252226239965 Lambda1 1.5880464\n",
      "74 Train Loss 55.06806 Test MSE 56.424835467840396 Test RE 0.1264515978006078 Lambda1 1.6120063\n",
      "Training time: 123.08\n",
      "Training time: 123.08\n",
      "inv_HT_stan_tune8\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 838.0474 Test MSE 858.2666150333937 Test RE 0.4931741224122196 Lambda1 -0.03633155\n",
      "1 Train Loss 837.97064 Test MSE 857.9559063715134 Test RE 0.49308484517608736 Lambda1 -0.036433287\n",
      "2 Train Loss 837.8884 Test MSE 857.9078829198247 Test RE 0.49307104495195186 Lambda1 -0.045424383\n",
      "3 Train Loss 837.85956 Test MSE 857.9987334010083 Test RE 0.49309715180835473 Lambda1 -0.07854089\n",
      "4 Train Loss 837.7174 Test MSE 857.2190336264699 Test RE 0.49287305175531415 Lambda1 -0.28676778\n",
      "5 Train Loss 835.5249 Test MSE 854.2904897076016 Test RE 0.49203042273316455 Lambda1 -0.75375634\n",
      "6 Train Loss 826.91797 Test MSE 841.8347413219052 Test RE 0.4884302945033084 Lambda1 -0.72422665\n",
      "7 Train Loss 813.0741 Test MSE 827.8029438669153 Test RE 0.4843425833973059 Lambda1 -0.7326326\n",
      "8 Train Loss 791.8431 Test MSE 799.4164386452021 Test RE 0.4759657559611475 Lambda1 -0.8129153\n",
      "9 Train Loss 738.2827 Test MSE 729.2899227917171 Test RE 0.45461030598721613 Lambda1 -0.71690613\n",
      "10 Train Loss 673.2313 Test MSE 660.0799566099387 Test RE 0.43250132611048864 Lambda1 -0.6787812\n",
      "11 Train Loss 605.6666 Test MSE 594.6405419982783 Test RE 0.41050308801436297 Lambda1 -0.62340206\n",
      "12 Train Loss 579.88403 Test MSE 555.0464147705247 Test RE 0.3966010151527705 Lambda1 -0.7089788\n",
      "13 Train Loss 512.8875 Test MSE 480.8897125899785 Test RE 0.36915768504949265 Lambda1 -0.8241531\n",
      "14 Train Loss 477.84622 Test MSE 459.0889638850899 Test RE 0.36069290285309497 Lambda1 -0.7693373\n",
      "15 Train Loss 450.49924 Test MSE 446.94383713124154 Test RE 0.3558898870069942 Lambda1 -0.72515243\n",
      "16 Train Loss 425.20825 Test MSE 398.91096818941253 Test RE 0.33622279910822117 Lambda1 -0.7353592\n",
      "17 Train Loss 386.2084 Test MSE 379.2761887011302 Test RE 0.32784378850229584 Lambda1 -0.8291798\n",
      "18 Train Loss 345.69754 Test MSE 342.7473073548195 Test RE 0.3116565052745609 Lambda1 -0.8708686\n",
      "19 Train Loss 323.36334 Test MSE 321.92583765378566 Test RE 0.302041826483252 Lambda1 -0.9015493\n",
      "20 Train Loss 305.75037 Test MSE 299.7830620561869 Test RE 0.2914692317568902 Lambda1 -0.9121952\n",
      "21 Train Loss 294.7892 Test MSE 300.4886235228794 Test RE 0.2918120273048631 Lambda1 -0.8837244\n",
      "22 Train Loss 284.84418 Test MSE 289.753167458523 Test RE 0.2865518830458759 Lambda1 -0.8647643\n",
      "23 Train Loss 264.2068 Test MSE 264.3779357531494 Test RE 0.2737170018312022 Lambda1 -0.89953303\n",
      "24 Train Loss 251.76926 Test MSE 250.39524045885108 Test RE 0.26638036237335627 Lambda1 -0.90632653\n",
      "25 Train Loss 243.09544 Test MSE 244.77197039348275 Test RE 0.2633722491655996 Lambda1 -0.90513194\n",
      "26 Train Loss 232.09972 Test MSE 239.05421034045452 Test RE 0.2602779450248657 Lambda1 -0.89775515\n",
      "27 Train Loss 213.95172 Test MSE 222.6856316268227 Test RE 0.2512090427355154 Lambda1 -0.8652312\n",
      "28 Train Loss 205.55118 Test MSE 214.6173525203388 Test RE 0.24661619182847677 Lambda1 -0.8538531\n",
      "29 Train Loss 192.79706 Test MSE 205.54056491851657 Test RE 0.2413447986517418 Lambda1 -0.8916437\n",
      "30 Train Loss 188.93935 Test MSE 203.69526210530475 Test RE 0.24025898300234294 Lambda1 -0.8981431\n",
      "31 Train Loss 185.33308 Test MSE 199.21833290124914 Test RE 0.23760404036905136 Lambda1 -0.9090438\n",
      "32 Train Loss 181.44424 Test MSE 193.81461470784737 Test RE 0.2343594292784546 Lambda1 -0.9232271\n",
      "33 Train Loss 179.66425 Test MSE 191.23637817658403 Test RE 0.23279541670324747 Lambda1 -0.9484159\n",
      "34 Train Loss 177.70325 Test MSE 189.44602873991317 Test RE 0.23170314225509275 Lambda1 -0.9825208\n",
      "35 Train Loss 174.07645 Test MSE 185.63952156100015 Test RE 0.22936354420754215 Lambda1 -1.0330298\n",
      "36 Train Loss 172.97394 Test MSE 183.71253548037316 Test RE 0.22817001248357746 Lambda1 -1.0541337\n",
      "37 Train Loss 170.57349 Test MSE 178.95041155577934 Test RE 0.2251933291670669 Lambda1 -1.1118796\n",
      "38 Train Loss 168.06664 Test MSE 177.09401470246132 Test RE 0.22402222810053699 Lambda1 -1.146791\n",
      "39 Train Loss 166.83246 Test MSE 175.30714264403213 Test RE 0.22288917473976 Lambda1 -1.2022115\n",
      "40 Train Loss 164.00076 Test MSE 169.93677966439765 Test RE 0.21944862430126744 Lambda1 -1.2823257\n",
      "41 Train Loss 161.78651 Test MSE 169.54751437808085 Test RE 0.21919714046485117 Lambda1 -1.3206179\n",
      "42 Train Loss 159.57039 Test MSE 167.8811950424997 Test RE 0.21811734197674645 Lambda1 -1.3752635\n",
      "43 Train Loss 157.52086 Test MSE 166.43310505114658 Test RE 0.2171745990853228 Lambda1 -1.3910857\n",
      "44 Train Loss 155.30264 Test MSE 166.72621454681638 Test RE 0.21736575076553574 Lambda1 -1.4038786\n",
      "45 Train Loss 153.65027 Test MSE 162.81318831045354 Test RE 0.2147998437622357 Lambda1 -1.4564362\n",
      "46 Train Loss 152.50618 Test MSE 161.79924052928274 Test RE 0.21412994724556114 Lambda1 -1.4843946\n",
      "47 Train Loss 150.39655 Test MSE 160.46336717462844 Test RE 0.21324414773777386 Lambda1 -1.4980478\n",
      "48 Train Loss 149.1746 Test MSE 158.56072536824325 Test RE 0.21197614147688357 Lambda1 -1.53705\n",
      "49 Train Loss 148.21336 Test MSE 158.03300229861503 Test RE 0.21162309713924002 Lambda1 -1.562031\n",
      "50 Train Loss 145.65762 Test MSE 156.43565895342974 Test RE 0.2105508740271707 Lambda1 -1.6169143\n",
      "51 Train Loss 142.74054 Test MSE 151.68057133798408 Test RE 0.20732618129284025 Lambda1 -1.6696676\n",
      "52 Train Loss 142.23724 Test MSE 151.33715230578193 Test RE 0.20709134534015664 Lambda1 -1.6813657\n",
      "53 Train Loss 141.37459 Test MSE 149.1326547191312 Test RE 0.20557748328427974 Lambda1 -1.7142137\n",
      "54 Train Loss 140.22417 Test MSE 147.04245616116134 Test RE 0.2041317434722489 Lambda1 -1.7199593\n",
      "55 Train Loss 139.3506 Test MSE 147.35152802310142 Test RE 0.2043461654194235 Lambda1 -1.7167138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 Train Loss 138.59985 Test MSE 147.0654709062866 Test RE 0.20414771796136388 Lambda1 -1.7479503\n",
      "57 Train Loss 138.28575 Test MSE 146.29656701361617 Test RE 0.20361334478698798 Lambda1 -1.765637\n",
      "58 Train Loss 137.05067 Test MSE 144.0080044076918 Test RE 0.20201447385579327 Lambda1 -1.8045199\n",
      "59 Train Loss 136.00394 Test MSE 142.52549538686637 Test RE 0.2009719517597855 Lambda1 -1.8191547\n",
      "60 Train Loss 134.05067 Test MSE 141.08694629300604 Test RE 0.19995514688917973 Lambda1 -1.8306078\n",
      "61 Train Loss 132.79044 Test MSE 140.89659015529404 Test RE 0.199820210462383 Lambda1 -1.848667\n",
      "62 Train Loss 130.0021 Test MSE 136.9960529518987 Test RE 0.1970349182260259 Lambda1 -1.9916073\n",
      "63 Train Loss 128.77386 Test MSE 134.17038807585712 Test RE 0.19499232094058722 Lambda1 -2.0719936\n",
      "64 Train Loss 127.21072 Test MSE 132.13714804007208 Test RE 0.19350920787195539 Lambda1 -2.0933654\n",
      "65 Train Loss 126.07252 Test MSE 131.95405411376507 Test RE 0.19337509478160295 Lambda1 -2.0889335\n",
      "66 Train Loss 125.32277 Test MSE 131.35118379071596 Test RE 0.19293284399515742 Lambda1 -2.117346\n",
      "67 Train Loss 124.85072 Test MSE 130.3281864754919 Test RE 0.192180069747628 Lambda1 -2.146312\n",
      "68 Train Loss 124.41347 Test MSE 130.23984553551864 Test RE 0.19211492556661197 Lambda1 -2.1461718\n",
      "69 Train Loss 123.64941 Test MSE 129.77773408535 Test RE 0.19177379572062694 Lambda1 -2.1480742\n",
      "70 Train Loss 120.85856 Test MSE 126.74440979236911 Test RE 0.18951935834048983 Lambda1 -2.2545993\n",
      "71 Train Loss 119.680954 Test MSE 125.70593398040477 Test RE 0.18874135135990008 Lambda1 -2.2992496\n",
      "72 Train Loss 119.32329 Test MSE 124.99955224731548 Test RE 0.18821030535640826 Lambda1 -2.3107812\n",
      "73 Train Loss 118.92901 Test MSE 124.50825057924148 Test RE 0.1878400677274251 Lambda1 -2.3214195\n",
      "74 Train Loss 118.52351 Test MSE 123.77557693993735 Test RE 0.18728657617607422 Lambda1 -2.3407187\n",
      "Training time: 123.70\n",
      "Training time: 123.70\n",
      "inv_HT_stan_tune8\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 840.74475 Test MSE 862.5406790538514 Test RE 0.4944005711812768 Lambda1 -0.04709664\n",
      "1 Train Loss 838.0742 Test MSE 858.3867728794023 Test RE 0.49320864354195854 Lambda1 -0.046553925\n",
      "2 Train Loss 838.0639 Test MSE 858.2808240121424 Test RE 0.49317820475184404 Lambda1 -0.046518408\n",
      "3 Train Loss 838.05774 Test MSE 858.2137082070926 Test RE 0.4931589216113886 Lambda1 -0.046500556\n",
      "4 Train Loss 837.85876 Test MSE 857.8339456462884 Test RE 0.493049797266178 Lambda1 -0.048853427\n",
      "5 Train Loss 837.85315 Test MSE 857.8599596665659 Test RE 0.4930572731362174 Lambda1 -0.050961096\n",
      "6 Train Loss 837.8395 Test MSE 857.8759267414525 Test RE 0.49306186167405625 Lambda1 -0.067900226\n",
      "7 Train Loss 836.5205 Test MSE 853.698250385923 Test RE 0.4918598424163643 Lambda1 -0.36940947\n",
      "8 Train Loss 827.83746 Test MSE 846.0442166532948 Test RE 0.4896499349818825 Lambda1 -0.844509\n",
      "9 Train Loss 813.6992 Test MSE 823.5392476499226 Test RE 0.4830936414554444 Lambda1 -0.98050547\n",
      "10 Train Loss 781.11163 Test MSE 788.156212591952 Test RE 0.4726017464991502 Lambda1 -0.9797551\n",
      "11 Train Loss 729.5282 Test MSE 723.769807629076 Test RE 0.4528865278787625 Lambda1 -1.032681\n",
      "12 Train Loss 703.26025 Test MSE 694.8246840060596 Test RE 0.44373817198173526 Lambda1 -1.0803274\n",
      "13 Train Loss 681.84796 Test MSE 677.6929795934815 Test RE 0.43823359141552565 Lambda1 -1.0895268\n",
      "14 Train Loss 666.2172 Test MSE 657.841702448963 Test RE 0.4317674226554862 Lambda1 -1.1044139\n",
      "15 Train Loss 654.79 Test MSE 652.2509940235303 Test RE 0.42992880683990187 Lambda1 -1.0783973\n",
      "16 Train Loss 642.95636 Test MSE 645.060683189838 Test RE 0.427552506435208 Lambda1 -1.0814387\n",
      "17 Train Loss 636.1485 Test MSE 639.3916914147543 Test RE 0.4256696289852898 Lambda1 -1.1143239\n",
      "18 Train Loss 630.44543 Test MSE 634.5001249017436 Test RE 0.4240382432421864 Lambda1 -1.1152987\n",
      "19 Train Loss 624.9538 Test MSE 629.8201230870856 Test RE 0.42247151962005985 Lambda1 -1.1448879\n",
      "20 Train Loss 623.0877 Test MSE 628.2841425221419 Test RE 0.4219560516827873 Lambda1 -1.153899\n",
      "21 Train Loss 622.07336 Test MSE 627.2212942416966 Test RE 0.4215989958047079 Lambda1 -1.1493459\n",
      "22 Train Loss 619.5697 Test MSE 625.4441340221218 Test RE 0.4210012957170436 Lambda1 -1.1221222\n",
      "23 Train Loss 618.67224 Test MSE 625.4223717823505 Test RE 0.4209939713131988 Lambda1 -1.1119924\n",
      "24 Train Loss 618.01434 Test MSE 625.3272552716259 Test RE 0.42096195694818794 Lambda1 -1.1044792\n",
      "25 Train Loss 617.3159 Test MSE 624.6019493756751 Test RE 0.4207177529943784 Lambda1 -1.1122224\n",
      "26 Train Loss 616.8553 Test MSE 624.4818909757282 Test RE 0.4206773167391146 Lambda1 -1.1205305\n",
      "27 Train Loss 616.49335 Test MSE 624.350376712107 Test RE 0.4206330176317519 Lambda1 -1.1222883\n",
      "28 Train Loss 616.2963 Test MSE 623.9542133150054 Test RE 0.420499546223827 Lambda1 -1.1258335\n",
      "29 Train Loss 615.9261 Test MSE 623.5664731126922 Test RE 0.4203688716378735 Lambda1 -1.1397128\n",
      "30 Train Loss 615.5675 Test MSE 623.2951613691569 Test RE 0.4202774111241064 Lambda1 -1.1497352\n",
      "31 Train Loss 614.96344 Test MSE 622.7352948535441 Test RE 0.4200886144458342 Lambda1 -1.1397268\n",
      "32 Train Loss 614.4425 Test MSE 622.1857138214774 Test RE 0.4199032036488475 Lambda1 -1.1176927\n",
      "33 Train Loss 614.2351 Test MSE 621.8554672861483 Test RE 0.41979174980061196 Lambda1 -1.1022363\n",
      "34 Train Loss 613.90906 Test MSE 621.6636857488862 Test RE 0.41972701247857125 Lambda1 -1.0788113\n",
      "35 Train Loss 613.02454 Test MSE 621.0411603789296 Test RE 0.41951680544403663 Lambda1 -1.0403167\n",
      "36 Train Loss 611.9854 Test MSE 619.9097781568628 Test RE 0.41913450371501554 Lambda1 -1.0187551\n",
      "37 Train Loss 611.26746 Test MSE 619.0238596905156 Test RE 0.4188349022500147 Lambda1 -1.0008688\n",
      "38 Train Loss 610.6449 Test MSE 618.3985331796607 Test RE 0.41862329914200797 Lambda1 -0.9751352\n",
      "39 Train Loss 608.3227 Test MSE 616.4226580557279 Test RE 0.41795398227677594 Lambda1 -0.9164176\n",
      "40 Train Loss 607.19183 Test MSE 614.5182157666673 Test RE 0.4173078468954412 Lambda1 -0.89360654\n",
      "41 Train Loss 601.1642 Test MSE 602.6867136862394 Test RE 0.41327104594594594 Lambda1 -0.90607184\n",
      "42 Train Loss 590.253 Test MSE 573.3601307213376 Test RE 0.4030908276458674 Lambda1 -0.97255594\n",
      "43 Train Loss 577.4492 Test MSE 560.6986441926938 Test RE 0.3986152627825047 Lambda1 -0.9749296\n",
      "44 Train Loss 555.1789 Test MSE 543.9922400975372 Test RE 0.39263184694612485 Lambda1 -1.010051\n",
      "45 Train Loss 515.78595 Test MSE 504.4329457533367 Test RE 0.37808625780053123 Lambda1 -1.0615455\n",
      "46 Train Loss 477.062 Test MSE 460.6676557884863 Test RE 0.3613125368720487 Lambda1 -1.1301991\n",
      "47 Train Loss 429.1245 Test MSE 423.89808927816824 Test RE 0.346593090126276 Lambda1 -1.0425936\n",
      "48 Train Loss 387.13986 Test MSE 378.27417752536337 Test RE 0.32741043623121474 Lambda1 -0.975475\n",
      "49 Train Loss 329.0738 Test MSE 321.8985492076432 Test RE 0.30202902473542026 Lambda1 -1.003537\n",
      "50 Train Loss 306.25482 Test MSE 319.11660399463284 Test RE 0.30072107931951386 Lambda1 -0.9773731\n",
      "51 Train Loss 287.57556 Test MSE 312.43344700639005 Test RE 0.2975554648102947 Lambda1 -1.0086677\n",
      "52 Train Loss 282.989 Test MSE 310.56924497193296 Test RE 0.2966664219985884 Lambda1 -1.0348352\n",
      "53 Train Loss 279.15836 Test MSE 306.7491740407696 Test RE 0.2948362451344007 Lambda1 -1.0175107\n",
      "54 Train Loss 274.24313 Test MSE 300.66666569120025 Test RE 0.29189846510711176 Lambda1 -0.9790266\n",
      "55 Train Loss 270.71097 Test MSE 299.2383636304371 Test RE 0.291204315164819 Lambda1 -0.97851086\n",
      "56 Train Loss 268.84995 Test MSE 298.73637643429294 Test RE 0.29095995780071293 Lambda1 -0.97402984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 Train Loss 265.07794 Test MSE 295.7858770830353 Test RE 0.28951954498845983 Lambda1 -0.937655\n",
      "58 Train Loss 262.52866 Test MSE 293.7789094926545 Test RE 0.28853564853951275 Lambda1 -0.91749245\n",
      "59 Train Loss 260.77563 Test MSE 293.3689270203511 Test RE 0.288334245622286 Lambda1 -0.90803146\n",
      "60 Train Loss 259.1466 Test MSE 292.579104848381 Test RE 0.2879458502536 Lambda1 -0.8878235\n",
      "61 Train Loss 257.50507 Test MSE 290.5693960279633 Test RE 0.28695520451794393 Lambda1 -0.8811013\n",
      "62 Train Loss 256.35703 Test MSE 288.79325317877783 Test RE 0.28607683485441215 Lambda1 -0.8684472\n",
      "63 Train Loss 255.33708 Test MSE 287.59090729905785 Test RE 0.28548069547695265 Lambda1 -0.8437222\n",
      "64 Train Loss 254.26944 Test MSE 285.86097725201745 Test RE 0.2846207820768396 Lambda1 -0.82252485\n",
      "65 Train Loss 253.6158 Test MSE 284.5960922864658 Test RE 0.28399038529565923 Lambda1 -0.81142193\n",
      "66 Train Loss 251.95421 Test MSE 282.3080763937058 Test RE 0.28284650845132253 Lambda1 -0.7843661\n",
      "67 Train Loss 250.3101 Test MSE 280.9782907822953 Test RE 0.2821795612037403 Lambda1 -0.7538522\n",
      "68 Train Loss 247.57677 Test MSE 276.5258925600259 Test RE 0.2799349166922888 Lambda1 -0.70915157\n",
      "69 Train Loss 244.10115 Test MSE 271.49490220995267 Test RE 0.27737672129437047 Lambda1 -0.66170084\n",
      "70 Train Loss 241.84705 Test MSE 269.76916585723865 Test RE 0.2764937540888871 Lambda1 -0.63024\n",
      "71 Train Loss 237.7799 Test MSE 263.58657961003735 Test RE 0.27330703954575003 Lambda1 -0.5960314\n",
      "72 Train Loss 235.6671 Test MSE 259.14240674702677 Test RE 0.27099321327117476 Lambda1 -0.59986794\n",
      "73 Train Loss 232.24959 Test MSE 256.1238489293467 Test RE 0.26941029065649924 Lambda1 -0.62766546\n",
      "74 Train Loss 228.7695 Test MSE 252.44265051867637 Test RE 0.26746720310852534 Lambda1 -0.6541602\n",
      "Training time: 124.05\n",
      "Training time: 124.05\n",
      "inv_HT_stan_tune8\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 838.7447 Test MSE 858.0975106596932 Test RE 0.4931255349406726 Lambda1 0.0040909736\n",
      "1 Train Loss 837.9584 Test MSE 858.1198943315081 Test RE 0.4931319665454843 Lambda1 0.003950028\n",
      "2 Train Loss 837.9133 Test MSE 858.0620083527932 Test RE 0.4931153337252226 Lambda1 0.0016625449\n",
      "3 Train Loss 837.9041 Test MSE 857.9323313192144 Test RE 0.4930780705966582 Lambda1 -0.0029940433\n",
      "4 Train Loss 837.8948 Test MSE 857.8403236298833 Test RE 0.493051630172004 Lambda1 -0.03270859\n",
      "5 Train Loss 837.8672 Test MSE 857.8721254818109 Test RE 0.49306076929101267 Lambda1 -0.1610758\n",
      "6 Train Loss 837.37616 Test MSE 856.971496869627 Test RE 0.49280188382684104 Lambda1 -1.3102896\n",
      "7 Train Loss 832.7674 Test MSE 850.0725039519816 Test RE 0.49081424074796354 Lambda1 -1.348879\n",
      "8 Train Loss 822.4089 Test MSE 835.6264904103151 Test RE 0.48662595635450656 Lambda1 -1.8329414\n",
      "9 Train Loss 807.33777 Test MSE 818.8399595801116 Test RE 0.4817133526473702 Lambda1 -1.9759054\n",
      "10 Train Loss 791.07367 Test MSE 798.2688772178 Test RE 0.47562400911052927 Lambda1 -1.873021\n",
      "11 Train Loss 766.04034 Test MSE 770.6740544747056 Test RE 0.46733094537294095 Lambda1 -2.011613\n",
      "12 Train Loss 742.9045 Test MSE 743.8160978730643 Test RE 0.4591155023393627 Lambda1 -2.143369\n",
      "13 Train Loss 715.07434 Test MSE 711.0418191298492 Test RE 0.44888670485964044 Lambda1 -1.9904717\n",
      "14 Train Loss 692.2634 Test MSE 684.7053536297725 Test RE 0.4404950499667724 Lambda1 -2.085904\n",
      "15 Train Loss 670.685 Test MSE 654.2383939117715 Test RE 0.43058330226657415 Lambda1 -2.1894152\n",
      "16 Train Loss 657.5192 Test MSE 647.0557336923017 Test RE 0.4282131654628893 Lambda1 -2.2915618\n",
      "17 Train Loss 653.8328 Test MSE 643.2073383507744 Test RE 0.42693785711602433 Lambda1 -2.3580391\n",
      "18 Train Loss 651.7666 Test MSE 640.2042796280572 Test RE 0.42594002997772595 Lambda1 -2.4378524\n",
      "19 Train Loss 647.4038 Test MSE 636.0031385508814 Test RE 0.4245401803233073 Lambda1 -2.5543437\n",
      "20 Train Loss 644.2525 Test MSE 633.7393953486765 Test RE 0.42378396813891933 Lambda1 -2.6515002\n",
      "21 Train Loss 641.5046 Test MSE 630.2466039177516 Test RE 0.4226145330828943 Lambda1 -2.8061774\n",
      "22 Train Loss 638.6225 Test MSE 627.1886439548068 Test RE 0.42158802239911997 Lambda1 -3.0293689\n",
      "23 Train Loss 636.7228 Test MSE 626.8385864723912 Test RE 0.4214703539421121 Lambda1 -3.2170901\n",
      "24 Train Loss 635.71765 Test MSE 625.7607962640301 Test RE 0.42110785866711453 Lambda1 -3.3931828\n",
      "25 Train Loss 634.93085 Test MSE 624.6173762850573 Test RE 0.42072294857100273 Lambda1 -3.4944577\n",
      "26 Train Loss 634.1618 Test MSE 625.0879224519999 Test RE 0.4208813914097648 Lambda1 -3.4657228\n",
      "27 Train Loss 632.9658 Test MSE 623.9548621939203 Test RE 0.420499764872257 Lambda1 -3.5484602\n",
      "28 Train Loss 632.3354 Test MSE 622.5976020173998 Test RE 0.42004216903715896 Lambda1 -3.6331623\n",
      "29 Train Loss 630.96356 Test MSE 620.9515171683497 Test RE 0.41948652710428697 Lambda1 -3.8168151\n",
      "30 Train Loss 628.3911 Test MSE 620.347838824968 Test RE 0.41928256874172315 Lambda1 -4.040779\n",
      "31 Train Loss 627.1447 Test MSE 620.1568751128825 Test RE 0.41921802920957946 Lambda1 -4.1776643\n",
      "32 Train Loss 626.1269 Test MSE 618.2511440044146 Test RE 0.41857340880725175 Lambda1 -4.258967\n",
      "33 Train Loss 624.8173 Test MSE 616.2892286510773 Test RE 0.41790874515867055 Lambda1 -4.305395\n",
      "34 Train Loss 624.2858 Test MSE 614.9101512343613 Test RE 0.41744090371236137 Lambda1 -4.3574533\n",
      "35 Train Loss 623.5175 Test MSE 613.2727276441635 Test RE 0.41688473853570895 Lambda1 -4.397455\n",
      "36 Train Loss 622.33405 Test MSE 612.750282496901 Test RE 0.416707129289263 Lambda1 -4.4086432\n",
      "37 Train Loss 621.01117 Test MSE 611.6838022031629 Test RE 0.41634433591764985 Lambda1 -4.4547024\n",
      "38 Train Loss 619.3609 Test MSE 609.9918650495628 Test RE 0.4157681262477913 Lambda1 -4.53511\n",
      "39 Train Loss 615.0444 Test MSE 605.1399994046837 Test RE 0.4141113185653926 Lambda1 -4.619826\n",
      "40 Train Loss 611.32404 Test MSE 599.1226354152475 Test RE 0.4120472638526103 Lambda1 -4.702247\n",
      "41 Train Loss 607.48413 Test MSE 592.34612157551 Test RE 0.40971035954454765 Lambda1 -4.815719\n",
      "42 Train Loss 604.6142 Test MSE 590.0726785096354 Test RE 0.4089233630590148 Lambda1 -4.835051\n",
      "43 Train Loss 600.2746 Test MSE 589.5793046820569 Test RE 0.40875237202499526 Lambda1 -4.7678657\n",
      "44 Train Loss 598.1518 Test MSE 586.0337890707057 Test RE 0.4075214746053108 Lambda1 -4.739512\n",
      "45 Train Loss 596.77594 Test MSE 584.7007533244621 Test RE 0.4070577215087738 Lambda1 -4.7636943\n",
      "46 Train Loss 590.23584 Test MSE 578.0792255086814 Test RE 0.40474626690551685 Lambda1 -4.7988925\n",
      "47 Train Loss 586.333 Test MSE 572.1708009238462 Test RE 0.4026725418463133 Lambda1 -4.874136\n",
      "48 Train Loss 583.5471 Test MSE 571.4653522683427 Test RE 0.40242423101405866 Lambda1 -4.950396\n",
      "49 Train Loss 581.8991 Test MSE 570.3302225505494 Test RE 0.4020243548269397 Lambda1 -5.009832\n",
      "50 Train Loss 575.4695 Test MSE 560.6506491916957 Test RE 0.39859820196957163 Lambda1 -5.085894\n",
      "51 Train Loss 572.49774 Test MSE 558.4915227127357 Test RE 0.397829939128925 Lambda1 -5.081868\n",
      "52 Train Loss 571.0347 Test MSE 556.6963534068491 Test RE 0.3971900485622326 Lambda1 -5.0739613\n",
      "53 Train Loss 567.9176 Test MSE 551.7613154844455 Test RE 0.39542561143773636 Lambda1 -5.0297756\n",
      "54 Train Loss 556.92584 Test MSE 539.0938999828961 Test RE 0.3908601366014698 Lambda1 -5.015657\n",
      "55 Train Loss 552.679 Test MSE 534.6971590955579 Test RE 0.3892629852865633 Lambda1 -4.9455366\n",
      "56 Train Loss 550.1681 Test MSE 531.2358823612384 Test RE 0.3880010237851147 Lambda1 -4.9242587\n",
      "57 Train Loss 547.4318 Test MSE 525.1388765432882 Test RE 0.3857680502142836 Lambda1 -4.9540586\n",
      "58 Train Loss 540.1623 Test MSE 511.2429247811188 Test RE 0.3806298343963584 Lambda1 -5.0574718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 Train Loss 537.54877 Test MSE 507.9094350602201 Test RE 0.3793868825132373 Lambda1 -5.112694\n",
      "60 Train Loss 532.93866 Test MSE 509.2357414386251 Test RE 0.37988190696487556 Lambda1 -5.1284547\n",
      "61 Train Loss 518.70996 Test MSE 501.677079162568 Test RE 0.37705204470691756 Lambda1 -5.2049317\n",
      "62 Train Loss 513.8079 Test MSE 499.49181250011884 Test RE 0.3762299436653456 Lambda1 -5.1991735\n",
      "63 Train Loss 507.83432 Test MSE 483.76057935741807 Test RE 0.3702579638405686 Lambda1 -5.2324233\n",
      "64 Train Loss 495.0804 Test MSE 475.4552375397157 Test RE 0.3670658557554608 Lambda1 -5.275214\n",
      "65 Train Loss 490.67822 Test MSE 474.5390946076315 Test RE 0.366712040176448 Lambda1 -5.2893224\n",
      "66 Train Loss 481.78516 Test MSE 462.54175841580854 Test RE 0.3620467424063208 Lambda1 -5.3848753\n",
      "67 Train Loss 477.23724 Test MSE 452.45628084979217 Test RE 0.35807786984112133 Lambda1 -5.450117\n",
      "68 Train Loss 468.8955 Test MSE 433.7257884462314 Test RE 0.3505877952234892 Lambda1 -5.4961796\n",
      "69 Train Loss 454.9416 Test MSE 421.6908812642377 Test RE 0.345689569331975 Lambda1 -5.496301\n",
      "70 Train Loss 445.01602 Test MSE 420.1362288222763 Test RE 0.3450517521454402 Lambda1 -5.4637346\n",
      "71 Train Loss 432.37466 Test MSE 414.568226040386 Test RE 0.3427576662944539 Lambda1 -5.443222\n",
      "72 Train Loss 423.05414 Test MSE 409.4458674235161 Test RE 0.3406335468673594 Lambda1 -5.443601\n",
      "73 Train Loss 404.56897 Test MSE 385.03427298416915 Test RE 0.330323038773638 Lambda1 -5.5460167\n",
      "74 Train Loss 390.58447 Test MSE 370.62042054021487 Test RE 0.324081203443485 Lambda1 -5.6160474\n",
      "Training time: 123.47\n",
      "Training time: 123.47\n",
      "inv_HT_stan_tune8\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 838.39624 Test MSE 859.1540618577139 Test RE 0.4934290273118502 Lambda1 -0.00038150477\n",
      "1 Train Loss 838.0895 Test MSE 858.3329953102501 Test RE 0.49319319364326025 Lambda1 -0.00048005817\n",
      "2 Train Loss 838.0354 Test MSE 858.089065474837 Test RE 0.49312310832452777 Lambda1 -0.00062737643\n",
      "3 Train Loss 837.86334 Test MSE 857.8459775934538 Test RE 0.4930532550031556 Lambda1 -0.008305193\n",
      "4 Train Loss 837.82495 Test MSE 857.9326720649341 Test RE 0.4930781685147684 Lambda1 -0.029738791\n",
      "5 Train Loss 837.63336 Test MSE 857.3427472892238 Test RE 0.4929086161375157 Lambda1 -0.062425375\n",
      "6 Train Loss 835.9374 Test MSE 855.8589643779845 Test RE 0.49248189876059195 Lambda1 -0.11266216\n",
      "7 Train Loss 819.39746 Test MSE 820.1566489709422 Test RE 0.4821004930671607 Lambda1 -0.027670717\n",
      "8 Train Loss 725.42084 Test MSE 726.3521522316609 Test RE 0.45369373740170926 Lambda1 0.0025406645\n",
      "9 Train Loss 660.6419 Test MSE 658.0705196352155 Test RE 0.4318425069932936 Lambda1 0.00036200625\n",
      "10 Train Loss 650.19904 Test MSE 655.0462115777219 Test RE 0.4308490505752138 Lambda1 0.00051214657\n",
      "11 Train Loss 647.95557 Test MSE 653.677681920718 Test RE 0.4303987480009111 Lambda1 0.00091866613\n",
      "12 Train Loss 644.4597 Test MSE 650.9153192583614 Test RE 0.4294883787313432 Lambda1 0.00084852235\n",
      "13 Train Loss 639.23737 Test MSE 646.2693559899749 Test RE 0.42795287902969636 Lambda1 -0.00036067393\n",
      "14 Train Loss 638.0479 Test MSE 644.3199420852116 Test RE 0.4273069507092955 Lambda1 0.00078495697\n",
      "15 Train Loss 634.8118 Test MSE 638.3175741164702 Test RE 0.4253119364310894 Lambda1 0.0026922491\n",
      "16 Train Loss 611.2305 Test MSE 618.1964912037599 Test RE 0.4185549076569718 Lambda1 0.00021962449\n",
      "17 Train Loss 567.2244 Test MSE 545.0447108661741 Test RE 0.39301147907318285 Lambda1 0.00016489052\n",
      "18 Train Loss 399.12695 Test MSE 387.1633735937316 Test RE 0.3312350633321514 Lambda1 0.0020094106\n",
      "19 Train Loss 337.74017 Test MSE 337.872311408295 Test RE 0.30943217647396093 Lambda1 -0.0008305942\n",
      "20 Train Loss 291.52917 Test MSE 310.2741904636575 Test RE 0.29652546540587593 Lambda1 -0.00047797296\n",
      "21 Train Loss 274.39368 Test MSE 302.08607134044126 Test RE 0.2925866599402276 Lambda1 0.00030024518\n",
      "22 Train Loss 269.9191 Test MSE 301.31310266743367 Test RE 0.2922120892430401 Lambda1 0.0003751561\n",
      "23 Train Loss 264.479 Test MSE 290.6987828242918 Test RE 0.2870190861291034 Lambda1 0.00038576423\n",
      "24 Train Loss 260.52527 Test MSE 283.7439811352127 Test RE 0.2835649177832446 Lambda1 0.00055921776\n",
      "25 Train Loss 257.9574 Test MSE 283.5469610037418 Test RE 0.2834664527891396 Lambda1 0.00018114263\n",
      "26 Train Loss 256.2387 Test MSE 283.23156940422 Test RE 0.28330875790082216 Lambda1 0.00028848098\n",
      "27 Train Loss 255.83641 Test MSE 283.633863013243 Test RE 0.28350988812895384 Lambda1 0.00044884306\n",
      "28 Train Loss 255.55472 Test MSE 283.23695481952507 Test RE 0.2833114513295093 Lambda1 0.00023969106\n",
      "29 Train Loss 255.38742 Test MSE 282.7825961617454 Test RE 0.28308412104290887 Lambda1 0.00026232383\n",
      "30 Train Loss 255.1189 Test MSE 282.967609601698 Test RE 0.2831767112587217 Lambda1 0.00057493715\n",
      "31 Train Loss 254.6684 Test MSE 282.6203391611518 Test RE 0.2830028943836974 Lambda1 0.00084878475\n",
      "32 Train Loss 254.54196 Test MSE 282.695431297437 Test RE 0.283040488777203 Lambda1 0.0009038175\n",
      "33 Train Loss 254.37837 Test MSE 282.609068997676 Test RE 0.282997251618051 Lambda1 0.0010977393\n",
      "34 Train Loss 254.29518 Test MSE 282.4659211893614 Test RE 0.28292557032411414 Lambda1 0.0010890302\n",
      "35 Train Loss 254.17595 Test MSE 282.5644331682313 Test RE 0.28297490216565335 Lambda1 0.0011235194\n",
      "36 Train Loss 254.10112 Test MSE 282.52076032221737 Test RE 0.28295303317678516 Lambda1 0.0014066751\n",
      "37 Train Loss 254.0251 Test MSE 282.4165853598278 Test RE 0.2829008611889397 Lambda1 0.0016221881\n",
      "38 Train Loss 253.90872 Test MSE 282.418705354091 Test RE 0.2829019230017163 Lambda1 0.0015744258\n",
      "39 Train Loss 253.88367 Test MSE 282.6794628752633 Test RE 0.28303249470771363 Lambda1 0.0016775972\n",
      "40 Train Loss 253.73357 Test MSE 282.5075757495892 Test RE 0.2829464307268664 Lambda1 0.0020200207\n",
      "41 Train Loss 253.62074 Test MSE 282.0718822413171 Test RE 0.28272816137542306 Lambda1 0.002395002\n",
      "42 Train Loss 253.56192 Test MSE 281.8413338326074 Test RE 0.2826125953507466 Lambda1 0.0022924575\n",
      "43 Train Loss 253.4103 Test MSE 281.2848505235394 Test RE 0.28233345442930424 Lambda1 0.0026676934\n",
      "44 Train Loss 253.18001 Test MSE 280.3271878450378 Test RE 0.2818524282765043 Lambda1 0.0039080633\n",
      "45 Train Loss 253.12056 Test MSE 280.0629176801759 Test RE 0.2817195429306046 Lambda1 0.004236528\n",
      "46 Train Loss 252.9557 Test MSE 279.73335800177193 Test RE 0.28155373959643515 Lambda1 0.0045495937\n",
      "47 Train Loss 252.75685 Test MSE 280.0760103673114 Test RE 0.28172612792007595 Lambda1 0.0046001645\n",
      "48 Train Loss 251.85942 Test MSE 278.06034654531067 Test RE 0.28071052760917375 Lambda1 0.008233971\n",
      "49 Train Loss 251.6599 Test MSE 276.7968638453677 Test RE 0.2800720390245373 Lambda1 0.009735635\n",
      "50 Train Loss 250.89919 Test MSE 275.1394160362598 Test RE 0.27923225027297166 Lambda1 0.012934733\n",
      "51 Train Loss 249.96968 Test MSE 273.19679320887633 Test RE 0.2782447439633158 Lambda1 0.01682937\n",
      "52 Train Loss 248.62555 Test MSE 268.58591922226884 Test RE 0.27588671689264543 Lambda1 0.026475213\n",
      "53 Train Loss 247.52112 Test MSE 266.6821102894132 Test RE 0.2749071985626367 Lambda1 0.031027999\n",
      "54 Train Loss 245.82396 Test MSE 266.55132061727585 Test RE 0.2748397785324898 Lambda1 0.031449527\n",
      "55 Train Loss 244.10336 Test MSE 264.5005326671172 Test RE 0.27378045828095815 Lambda1 0.037129637\n",
      "56 Train Loss 241.6632 Test MSE 260.70201911857197 Test RE 0.2718074574027871 Lambda1 0.045046054\n",
      "57 Train Loss 238.00943 Test MSE 255.80561825795928 Test RE 0.26924286918123064 Lambda1 0.060434464\n",
      "58 Train Loss 235.66588 Test MSE 252.58773665087992 Test RE 0.26754405265793674 Lambda1 0.073781826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 Train Loss 230.57153 Test MSE 248.66358034615706 Test RE 0.2654576600549679 Lambda1 0.099278174\n",
      "60 Train Loss 226.7959 Test MSE 245.32810451257686 Test RE 0.26367127685037844 Lambda1 0.10924971\n",
      "61 Train Loss 222.16516 Test MSE 241.48808821050747 Test RE 0.2615995709117024 Lambda1 0.12546454\n",
      "62 Train Loss 219.47191 Test MSE 240.4680248651506 Test RE 0.26104647835820244 Lambda1 0.13754208\n",
      "63 Train Loss 217.28104 Test MSE 239.17098761721496 Test RE 0.26034150976729775 Lambda1 0.14579171\n",
      "64 Train Loss 214.2965 Test MSE 236.87092995614395 Test RE 0.25908666048733703 Lambda1 0.16702674\n",
      "65 Train Loss 213.09442 Test MSE 236.0388868229161 Test RE 0.2586312206486768 Lambda1 0.17860386\n",
      "66 Train Loss 211.86671 Test MSE 235.5068934453035 Test RE 0.2583395998220978 Lambda1 0.19285725\n",
      "67 Train Loss 209.8015 Test MSE 233.45892567311805 Test RE 0.25721388669325057 Lambda1 0.20717956\n",
      "68 Train Loss 209.1357 Test MSE 232.62407798123834 Test RE 0.2567535771650036 Lambda1 0.21059246\n",
      "69 Train Loss 207.33594 Test MSE 231.08281573872756 Test RE 0.25590159689881364 Lambda1 0.22410803\n",
      "70 Train Loss 204.69409 Test MSE 225.2834237238898 Test RE 0.252670063479456 Lambda1 0.24513134\n",
      "71 Train Loss 197.41612 Test MSE 207.4302114952339 Test RE 0.24245166767729215 Lambda1 0.27294022\n",
      "72 Train Loss 185.99767 Test MSE 199.2389045268059 Test RE 0.23761630775203094 Lambda1 0.31673607\n",
      "73 Train Loss 175.27444 Test MSE 184.852791432273 Test RE 0.22887701311579278 Lambda1 0.3381355\n",
      "74 Train Loss 165.86458 Test MSE 173.24665481930208 Test RE 0.22157542932503893 Lambda1 0.35798126\n",
      "Training time: 125.22\n",
      "Training time: 125.22\n",
      "inv_HT_stan_tune8\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 838.10724 Test MSE 858.6324088096533 Test RE 0.49327920677870596 Lambda1 -0.14952444\n",
      "1 Train Loss 837.99854 Test MSE 858.216205116883 Test RE 0.49315963901570886 Lambda1 -0.14932674\n",
      "2 Train Loss 837.9719 Test MSE 858.0232431345985 Test RE 0.49310419470541583 Lambda1 -0.14954911\n",
      "3 Train Loss 837.8738 Test MSE 857.8198155894129 Test RE 0.4930457365441686 Lambda1 -0.15273878\n",
      "4 Train Loss 837.86554 Test MSE 857.9005510353107 Test RE 0.4930689379963248 Lambda1 -0.15569389\n",
      "5 Train Loss 837.8315 Test MSE 857.8211350973479 Test RE 0.49304611574822743 Lambda1 -0.17731154\n",
      "6 Train Loss 837.7745 Test MSE 857.4885480594052 Test RE 0.4929505266808335 Lambda1 -0.24253145\n",
      "7 Train Loss 836.25275 Test MSE 855.0613836600567 Test RE 0.4922523716788796 Lambda1 -0.73332185\n",
      "8 Train Loss 827.80005 Test MSE 843.4565900752991 Test RE 0.48890056429359935 Lambda1 -0.85272056\n",
      "9 Train Loss 809.10364 Test MSE 821.2748034683942 Test RE 0.4824290152028585 Lambda1 -1.1043116\n",
      "10 Train Loss 794.8249 Test MSE 802.2170171834097 Test RE 0.4767987473830965 Lambda1 -0.9233653\n",
      "11 Train Loss 778.6145 Test MSE 784.2729349112982 Test RE 0.4714360448583692 Lambda1 -0.64383024\n",
      "12 Train Loss 748.5986 Test MSE 749.0929102962505 Test RE 0.460741162782136 Lambda1 -0.5246035\n",
      "13 Train Loss 735.12085 Test MSE 733.1891214438008 Test RE 0.45582398849997585 Lambda1 -0.484498\n",
      "14 Train Loss 721.022 Test MSE 720.7446266001601 Test RE 0.4519390592563787 Lambda1 -0.44254383\n",
      "15 Train Loss 714.0589 Test MSE 714.9396928529346 Test RE 0.4501154035530177 Lambda1 -0.38124794\n",
      "16 Train Loss 708.6893 Test MSE 712.9806653516754 Test RE 0.4494982932248163 Lambda1 -0.37858918\n",
      "17 Train Loss 702.07 Test MSE 705.2965651589425 Test RE 0.44706951298992625 Lambda1 -0.45016778\n",
      "18 Train Loss 695.46185 Test MSE 697.4545458537322 Test RE 0.4445771375336468 Lambda1 -0.52207565\n",
      "19 Train Loss 686.91614 Test MSE 688.5788722352248 Test RE 0.44173927815240394 Lambda1 -0.5042308\n",
      "20 Train Loss 675.3975 Test MSE 675.4127120118212 Test RE 0.4374956969049113 Lambda1 -0.53843963\n",
      "21 Train Loss 663.12006 Test MSE 664.129859995671 Test RE 0.43382609750487455 Lambda1 -0.5737169\n",
      "22 Train Loss 641.0813 Test MSE 635.8743762368761 Test RE 0.42449720290718407 Lambda1 -0.6872287\n",
      "23 Train Loss 621.57886 Test MSE 612.7136507627271 Test RE 0.41669467320937864 Lambda1 -0.66454947\n",
      "24 Train Loss 593.279 Test MSE 581.7925053051612 Test RE 0.40604412563344444 Lambda1 -0.68505514\n",
      "25 Train Loss 534.1365 Test MSE 502.8381203851184 Test RE 0.3774881020744227 Lambda1 -0.80200946\n",
      "26 Train Loss 501.03165 Test MSE 492.7958819846114 Test RE 0.3736996625144648 Lambda1 -0.77343076\n",
      "27 Train Loss 457.63272 Test MSE 448.59080996448563 Test RE 0.3565450051024928 Lambda1 -0.76598424\n",
      "28 Train Loss 425.5449 Test MSE 417.2066398358031 Test RE 0.3438466333258539 Lambda1 -0.7776821\n",
      "29 Train Loss 381.30762 Test MSE 375.9825242073465 Test RE 0.3264171738137655 Lambda1 -0.7447681\n",
      "30 Train Loss 340.79034 Test MSE 335.9618875980328 Test RE 0.30855612866662274 Lambda1 -0.74196106\n",
      "31 Train Loss 321.40598 Test MSE 321.69825444054396 Test RE 0.3019350444322136 Lambda1 -0.7097083\n",
      "32 Train Loss 303.52118 Test MSE 305.5888368842256 Test RE 0.2942780796751539 Lambda1 -0.68332547\n",
      "33 Train Loss 287.67593 Test MSE 301.28125008127216 Test RE 0.29219664358756375 Lambda1 -0.67875147\n",
      "34 Train Loss 271.9264 Test MSE 283.818121619274 Test RE 0.2836019622115121 Lambda1 -0.7051411\n",
      "35 Train Loss 258.87668 Test MSE 265.8038061242844 Test RE 0.27445412867366575 Lambda1 -0.739107\n",
      "36 Train Loss 241.65015 Test MSE 254.36397385639785 Test RE 0.2684831108496581 Lambda1 -0.7488837\n",
      "37 Train Loss 232.88666 Test MSE 248.3495746087568 Test RE 0.26529000068262293 Lambda1 -0.75012624\n",
      "38 Train Loss 222.58835 Test MSE 242.45740189294227 Test RE 0.26212406489452666 Lambda1 -0.78295106\n",
      "39 Train Loss 215.64473 Test MSE 239.01488206628673 Test RE 0.2602565341837635 Lambda1 -0.8173842\n",
      "40 Train Loss 210.03577 Test MSE 235.41306937001627 Test RE 0.2582881344736878 Lambda1 -0.82863635\n",
      "41 Train Loss 204.99832 Test MSE 226.92952113408526 Test RE 0.25359148622497213 Lambda1 -0.8279167\n",
      "42 Train Loss 197.53394 Test MSE 211.93269189687288 Test RE 0.24506886975699033 Lambda1 -0.8626806\n",
      "43 Train Loss 194.21588 Test MSE 208.00596588045224 Test RE 0.24278791541787917 Lambda1 -0.8748972\n",
      "44 Train Loss 190.75871 Test MSE 202.8463571817238 Test RE 0.23975781774593316 Lambda1 -0.88482875\n",
      "45 Train Loss 185.87038 Test MSE 198.50603283050964 Test RE 0.2371788864023279 Lambda1 -0.9119279\n",
      "46 Train Loss 183.63805 Test MSE 195.40132627664923 Test RE 0.23531679473570447 Lambda1 -0.9358651\n",
      "47 Train Loss 180.93585 Test MSE 189.80830523406652 Test RE 0.23192457870104627 Lambda1 -0.9590908\n",
      "48 Train Loss 177.81801 Test MSE 187.14458299946833 Test RE 0.2302914429657173 Lambda1 -0.97671455\n",
      "49 Train Loss 174.3376 Test MSE 180.7114575039531 Test RE 0.2262986769980437 Lambda1 -1.0124521\n",
      "50 Train Loss 171.36029 Test MSE 176.94371355025322 Test RE 0.22392714316425366 Lambda1 -1.030007\n",
      "51 Train Loss 169.43416 Test MSE 176.84332009378525 Test RE 0.22386360880884373 Lambda1 -1.0338691\n",
      "52 Train Loss 166.565 Test MSE 176.9209283785421 Test RE 0.22391272506888857 Lambda1 -1.0591404\n",
      "53 Train Loss 164.18661 Test MSE 173.9181421894932 Test RE 0.22200441657452735 Lambda1 -1.084211\n",
      "54 Train Loss 162.12025 Test MSE 172.41096614312283 Test RE 0.22104037738975 Lambda1 -1.0843824\n",
      "55 Train Loss 161.06978 Test MSE 171.1601643163828 Test RE 0.22023711937560586 Lambda1 -1.0961845\n",
      "56 Train Loss 159.27931 Test MSE 168.35880932992092 Test RE 0.21842738852511578 Lambda1 -1.1239709\n",
      "57 Train Loss 155.02403 Test MSE 166.33341236442973 Test RE 0.21710954603471352 Lambda1 -1.146897\n",
      "58 Train Loss 153.66379 Test MSE 165.48576269623254 Test RE 0.2165556347873052 Lambda1 -1.1491053\n",
      "59 Train Loss 151.97673 Test MSE 163.36785196528533 Test RE 0.2151654172414925 Lambda1 -1.1670588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 Train Loss 147.29652 Test MSE 158.34421570885866 Test RE 0.21183136867835312 Lambda1 -1.201329\n",
      "61 Train Loss 145.5047 Test MSE 157.80881828002123 Test RE 0.21147294067518874 Lambda1 -1.1996181\n",
      "62 Train Loss 144.07971 Test MSE 154.9116684251113 Test RE 0.20952277447422896 Lambda1 -1.2140615\n",
      "63 Train Loss 142.68643 Test MSE 153.2012020173221 Test RE 0.20836283460743385 Lambda1 -1.2341883\n",
      "64 Train Loss 140.40211 Test MSE 152.10272968792287 Test RE 0.20761449660820702 Lambda1 -1.2552594\n",
      "65 Train Loss 137.09631 Test MSE 148.7099654450221 Test RE 0.20528594064044223 Lambda1 -1.2703129\n",
      "66 Train Loss 136.51155 Test MSE 147.610440227173 Test RE 0.2045256155278686 Lambda1 -1.2748947\n",
      "67 Train Loss 134.97835 Test MSE 145.13798142583383 Test RE 0.20280549124147676 Lambda1 -1.2958962\n",
      "68 Train Loss 133.29839 Test MSE 142.41311801564805 Test RE 0.2008927057512971 Lambda1 -1.3332517\n",
      "69 Train Loss 131.21944 Test MSE 140.45232711453653 Test RE 0.19950493376764156 Lambda1 -1.370164\n",
      "70 Train Loss 129.68576 Test MSE 139.28610630608964 Test RE 0.1986749304406278 Lambda1 -1.3965904\n",
      "71 Train Loss 128.03511 Test MSE 137.31671128447977 Test RE 0.19726537727673046 Lambda1 -1.4229357\n",
      "72 Train Loss 126.73136 Test MSE 135.0428457297433 Test RE 0.19562527311696776 Lambda1 -1.4547726\n",
      "73 Train Loss 125.27112 Test MSE 133.3981837691862 Test RE 0.19443038178560154 Lambda1 -1.4812604\n",
      "74 Train Loss 124.23057 Test MSE 132.75859031554356 Test RE 0.19396371194548262 Lambda1 -1.480987\n",
      "Training time: 133.53\n",
      "Training time: 133.53\n",
      "inv_HT_stan_tune8\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 837.96436 Test MSE 857.9894979268792 Test RE 0.4930944979593069 Lambda1 0.015444501\n",
      "1 Train Loss 837.8983 Test MSE 857.9404627476944 Test RE 0.4930804072725354 Lambda1 0.012147668\n",
      "2 Train Loss 837.7715 Test MSE 857.7968443442438 Test RE 0.49303913495365287 Lambda1 -0.1819869\n",
      "3 Train Loss 837.60895 Test MSE 857.2029863896668 Test RE 0.4928684384142187 Lambda1 -0.42360643\n",
      "4 Train Loss 835.55865 Test MSE 852.0694586289122 Test RE 0.4913904026881707 Lambda1 -0.2964022\n",
      "5 Train Loss 822.80347 Test MSE 836.7081019801911 Test RE 0.4869407920261504 Lambda1 -1.1403793\n",
      "6 Train Loss 810.21136 Test MSE 822.7501309610481 Test RE 0.4828621354158833 Lambda1 -1.2786436\n",
      "7 Train Loss 798.5583 Test MSE 805.3385011742675 Test RE 0.477725475840118 Lambda1 -0.73607045\n",
      "8 Train Loss 770.67 Test MSE 773.2618171021528 Test RE 0.4681148877346608 Lambda1 -0.6198664\n",
      "9 Train Loss 740.01953 Test MSE 736.5215180769891 Test RE 0.4568586903362944 Lambda1 -0.49419314\n",
      "10 Train Loss 721.0417 Test MSE 718.486014843915 Test RE 0.45123037841433106 Lambda1 -0.52189\n",
      "11 Train Loss 712.66077 Test MSE 712.153119852585 Test RE 0.449237354669186 Lambda1 -0.54033065\n",
      "12 Train Loss 694.8917 Test MSE 695.1001118064057 Test RE 0.4438261119485324 Lambda1 -0.5202525\n",
      "13 Train Loss 681.2245 Test MSE 676.1889692562896 Test RE 0.4377470333657499 Lambda1 -0.491112\n",
      "14 Train Loss 653.5138 Test MSE 633.6597133903418 Test RE 0.423757325485612 Lambda1 -0.64209217\n",
      "15 Train Loss 626.88135 Test MSE 624.7464111058986 Test RE 0.4207664032593563 Lambda1 -0.7349334\n",
      "16 Train Loss 600.44684 Test MSE 591.4885172737673 Test RE 0.4094136608534187 Lambda1 -0.8215301\n",
      "17 Train Loss 574.0118 Test MSE 563.4020931003231 Test RE 0.3995750833163408 Lambda1 -0.85127825\n",
      "18 Train Loss 553.8181 Test MSE 546.2425722258457 Test RE 0.3934431087000414 Lambda1 -0.86634904\n",
      "19 Train Loss 541.0894 Test MSE 531.0518147206727 Test RE 0.38793379882642637 Lambda1 -0.96054244\n",
      "20 Train Loss 525.9482 Test MSE 514.7127741277071 Test RE 0.3819193336904149 Lambda1 -1.0872731\n",
      "21 Train Loss 492.2677 Test MSE 480.6270431596812 Test RE 0.36905685144768746 Lambda1 -1.2053964\n",
      "22 Train Loss 462.5854 Test MSE 451.3333270416948 Test RE 0.3576332360375141 Lambda1 -1.341301\n",
      "23 Train Loss 433.77957 Test MSE 424.54880902252904 Test RE 0.34685901309835465 Lambda1 -1.5761274\n",
      "24 Train Loss 409.91104 Test MSE 403.8902810209469 Test RE 0.33831470264250324 Lambda1 -1.7607428\n",
      "25 Train Loss 394.59464 Test MSE 383.6273394315633 Test RE 0.32971897840241254 Lambda1 -1.8295567\n",
      "26 Train Loss 378.63562 Test MSE 365.2271029482312 Test RE 0.32171452547545537 Lambda1 -1.8145758\n",
      "27 Train Loss 343.43167 Test MSE 333.65638323043726 Test RE 0.3074955885083563 Lambda1 -1.8110592\n",
      "28 Train Loss 317.9412 Test MSE 308.5823251467389 Test RE 0.2957159122275453 Lambda1 -1.9055992\n",
      "29 Train Loss 304.4459 Test MSE 287.090871114047 Test RE 0.28523240393048527 Lambda1 -1.9475937\n",
      "30 Train Loss 291.92398 Test MSE 262.52432496286855 Test RE 0.27275576951627173 Lambda1 -1.9665501\n",
      "31 Train Loss 280.32816 Test MSE 251.57926597303273 Test RE 0.2670094261893442 Lambda1 -1.9009664\n",
      "32 Train Loss 266.47565 Test MSE 238.99347049742528 Test RE 0.2602448766972914 Lambda1 -1.7887245\n",
      "33 Train Loss 256.7456 Test MSE 223.8861750021529 Test RE 0.25188529194448367 Lambda1 -1.7549955\n",
      "34 Train Loss 226.25627 Test MSE 210.72501771362755 Test RE 0.24436962374076962 Lambda1 -1.7547191\n",
      "35 Train Loss 204.23598 Test MSE 200.98140191416542 Test RE 0.23865311440996326 Lambda1 -1.7712473\n",
      "36 Train Loss 186.78104 Test MSE 175.21930613414528 Test RE 0.22283332915396886 Lambda1 -1.7609627\n",
      "37 Train Loss 173.59789 Test MSE 162.59442878403993 Test RE 0.21465549026347516 Lambda1 -1.7631716\n",
      "38 Train Loss 155.51297 Test MSE 155.08768161334098 Test RE 0.2096417722950417 Lambda1 -1.7489008\n",
      "39 Train Loss 150.84789 Test MSE 151.2548812599526 Test RE 0.20703504740715253 Lambda1 -1.741296\n",
      "40 Train Loss 148.85147 Test MSE 153.0667744289965 Test RE 0.2082713997491673 Lambda1 -1.7413515\n",
      "41 Train Loss 141.16464 Test MSE 148.0437971779926 Test RE 0.2048256201889928 Lambda1 -1.7448972\n",
      "42 Train Loss 138.91049 Test MSE 143.8628490157378 Test RE 0.20191263617232053 Lambda1 -1.7463801\n",
      "43 Train Loss 138.15073 Test MSE 142.31522353506088 Test RE 0.20082364726791924 Lambda1 -1.7407324\n",
      "44 Train Loss 136.88203 Test MSE 140.5268081919869 Test RE 0.19955782492756283 Lambda1 -1.7237918\n",
      "45 Train Loss 135.21239 Test MSE 140.6152098757299 Test RE 0.19962058332368948 Lambda1 -1.7112375\n",
      "46 Train Loss 133.89972 Test MSE 140.75223624080846 Test RE 0.19971782252849074 Lambda1 -1.7117162\n",
      "47 Train Loss 132.61688 Test MSE 140.9923542924835 Test RE 0.199888105503789 Lambda1 -1.722418\n",
      "48 Train Loss 131.72488 Test MSE 139.07271271740242 Test RE 0.19852268192076714 Lambda1 -1.7341655\n",
      "49 Train Loss 130.8694 Test MSE 137.88628838291882 Test RE 0.1976740732347059 Lambda1 -1.7431122\n",
      "50 Train Loss 129.97955 Test MSE 136.90560566879796 Test RE 0.19696986447046533 Lambda1 -1.7614012\n",
      "51 Train Loss 128.7423 Test MSE 135.6878684313098 Test RE 0.19609191177707505 Lambda1 -1.7967457\n",
      "52 Train Loss 127.50888 Test MSE 135.54252188488945 Test RE 0.19598685846943342 Lambda1 -1.8334806\n",
      "53 Train Loss 126.802895 Test MSE 134.51871361659676 Test RE 0.19524527086754562 Lambda1 -1.8662188\n",
      "54 Train Loss 125.4039 Test MSE 132.3803319120006 Test RE 0.19368719221477268 Lambda1 -1.870415\n",
      "55 Train Loss 122.957375 Test MSE 130.91995857910504 Test RE 0.19261588490642645 Lambda1 -1.8807085\n",
      "56 Train Loss 121.49417 Test MSE 130.22823583502324 Test RE 0.19210636272455522 Lambda1 -1.9021184\n",
      "57 Train Loss 120.80639 Test MSE 130.0803083723525 Test RE 0.19199722402196193 Lambda1 -1.9096586\n",
      "58 Train Loss 120.496346 Test MSE 129.53834507744114 Test RE 0.1915968403692962 Lambda1 -1.9220153\n",
      "59 Train Loss 119.79527 Test MSE 129.11406736157545 Test RE 0.19128281388699434 Lambda1 -1.9524018\n",
      "60 Train Loss 118.37638 Test MSE 127.56269404954021 Test RE 0.19013015926371332 Lambda1 -1.9863098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 Train Loss 116.463104 Test MSE 123.736818713013 Test RE 0.1872572510695473 Lambda1 -2.0237432\n",
      "62 Train Loss 115.40418 Test MSE 122.25734662651888 Test RE 0.18613440426101133 Lambda1 -2.048559\n",
      "63 Train Loss 113.635864 Test MSE 121.60759435695697 Test RE 0.18563912781011366 Lambda1 -2.0953553\n",
      "64 Train Loss 111.151924 Test MSE 118.80940253681233 Test RE 0.18349091933412856 Lambda1 -2.1458216\n",
      "65 Train Loss 110.560844 Test MSE 118.10950435079722 Test RE 0.1829496546964079 Lambda1 -2.16461\n",
      "66 Train Loss 109.371735 Test MSE 117.72943868021771 Test RE 0.1826550598143655 Lambda1 -2.1924834\n",
      "67 Train Loss 108.456 Test MSE 116.02802165773274 Test RE 0.18133039780154026 Lambda1 -2.195788\n",
      "68 Train Loss 107.88105 Test MSE 115.3633231601259 Test RE 0.1808102512029364 Lambda1 -2.1945553\n",
      "69 Train Loss 107.13538 Test MSE 115.24835346218705 Test RE 0.1807201320854792 Lambda1 -2.1969347\n",
      "70 Train Loss 106.69133 Test MSE 114.38937688215682 Test RE 0.1800453949198882 Lambda1 -2.1985927\n",
      "71 Train Loss 106.24723 Test MSE 112.53524016076148 Test RE 0.17858025619679238 Lambda1 -2.2078104\n",
      "72 Train Loss 105.451385 Test MSE 111.64178850840548 Test RE 0.17786994193076608 Lambda1 -2.2375174\n",
      "73 Train Loss 105.01567 Test MSE 110.75630752297283 Test RE 0.1771631546252023 Lambda1 -2.2633245\n",
      "74 Train Loss 104.56339 Test MSE 109.49109383354038 Test RE 0.17614834529839757 Lambda1 -2.2860122\n",
      "Training time: 135.06\n",
      "Training time: 135.06\n",
      "inv_HT_stan_tune8\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 840.51624 Test MSE 862.2500285402976 Test RE 0.49431726503067275 Lambda1 0.0010975841\n",
      "1 Train Loss 837.9837 Test MSE 858.2617185908055 Test RE 0.4931727156219646 Lambda1 0.0010562157\n",
      "2 Train Loss 837.9344 Test MSE 857.975204514081 Test RE 0.49309039066395033 Lambda1 0.0009406131\n",
      "3 Train Loss 837.84625 Test MSE 857.822628573123 Test RE 0.4930465449473222 Lambda1 -0.0035051273\n",
      "4 Train Loss 836.4898 Test MSE 856.1621297983196 Test RE 0.49256911538663495 Lambda1 -0.17012626\n",
      "5 Train Loss 822.73846 Test MSE 833.1148582068975 Test RE 0.48589408318700034 Lambda1 -0.32056585\n",
      "6 Train Loss 786.1989 Test MSE 782.5390515203752 Test RE 0.4709146272622553 Lambda1 -0.16905281\n",
      "7 Train Loss 686.10187 Test MSE 681.3305191867983 Test RE 0.4394081341095991 Lambda1 -0.008649298\n",
      "8 Train Loss 663.564 Test MSE 658.6641515831676 Test RE 0.43203724123057097 Lambda1 -0.0056233006\n",
      "9 Train Loss 649.8954 Test MSE 653.2795565955829 Test RE 0.4302676598936162 Lambda1 -0.0027940492\n",
      "10 Train Loss 643.3218 Test MSE 650.9631768940168 Test RE 0.4295041672063929 Lambda1 -0.0003314172\n",
      "11 Train Loss 639.3281 Test MSE 647.1922875817323 Test RE 0.42825834788339034 Lambda1 0.0038533525\n",
      "12 Train Loss 632.82465 Test MSE 640.9063140666856 Test RE 0.42617350439227036 Lambda1 0.0031546142\n",
      "13 Train Loss 614.1094 Test MSE 620.1213550608076 Test RE 0.41920602348969566 Lambda1 0.0015457314\n",
      "14 Train Loss 581.1201 Test MSE 588.6099871677183 Test RE 0.4084162223204522 Lambda1 0.00021261637\n",
      "15 Train Loss 480.08002 Test MSE 428.7330814485812 Test RE 0.34856411039725016 Lambda1 0.00021571721\n",
      "16 Train Loss 358.3784 Test MSE 334.18246117179143 Test RE 0.30773790806213075 Lambda1 -2.2399221e-05\n",
      "17 Train Loss 313.6814 Test MSE 320.422223144012 Test RE 0.30133562964382377 Lambda1 2.9784485e-05\n",
      "18 Train Loss 283.57217 Test MSE 297.0298626425668 Test RE 0.29012772184261515 Lambda1 -3.4940036e-05\n",
      "19 Train Loss 262.94205 Test MSE 282.3599919086322 Test RE 0.28287251452116424 Lambda1 5.496788e-05\n",
      "20 Train Loss 258.5238 Test MSE 281.7670244454503 Test RE 0.2825753365269445 Lambda1 0.000120222714\n",
      "21 Train Loss 256.0914 Test MSE 282.4839510508192 Test RE 0.28293459978050967 Lambda1 0.00024456056\n",
      "22 Train Loss 255.74962 Test MSE 282.20865954039397 Test RE 0.28279670083285824 Lambda1 0.0002137935\n",
      "23 Train Loss 255.43594 Test MSE 282.27962778289617 Test RE 0.28283225665755957 Lambda1 3.863546e-05\n",
      "24 Train Loss 255.30707 Test MSE 282.37660889426485 Test RE 0.2828808379725916 Lambda1 4.3000615e-05\n",
      "25 Train Loss 255.2624 Test MSE 282.3400922198382 Test RE 0.282862546440786 Lambda1 9.373503e-05\n",
      "26 Train Loss 255.18822 Test MSE 282.37858688324786 Test RE 0.28288182873135764 Lambda1 8.024845e-05\n",
      "27 Train Loss 255.13283 Test MSE 282.42817261422107 Test RE 0.2829066646912721 Lambda1 7.36401e-05\n",
      "28 Train Loss 255.0494 Test MSE 282.40367705438797 Test RE 0.2828943958950385 Lambda1 8.512053e-05\n",
      "29 Train Loss 255.00835 Test MSE 282.45246842285536 Test RE 0.2829188329149798 Lambda1 9.762953e-05\n",
      "30 Train Loss 254.9307 Test MSE 282.9218686601691 Test RE 0.2831538229626062 Lambda1 8.875103e-05\n",
      "31 Train Loss 254.88875 Test MSE 283.22299418913167 Test RE 0.2833044690923229 Lambda1 7.68882e-05\n",
      "32 Train Loss 254.81285 Test MSE 283.3139421646766 Test RE 0.2833499525105286 Lambda1 6.0334904e-05\n",
      "33 Train Loss 254.78038 Test MSE 283.2529908966712 Test RE 0.2833194713634572 Lambda1 6.7343084e-05\n",
      "34 Train Loss 254.7746 Test MSE 283.29077973436364 Test RE 0.2833383695866066 Lambda1 5.7232548e-05\n",
      "35 Train Loss 254.7498 Test MSE 283.1924554336338 Test RE 0.2832891949102752 Lambda1 6.725281e-05\n",
      "36 Train Loss 254.70674 Test MSE 283.0485033260418 Test RE 0.28321718511707206 Lambda1 9.057276e-05\n",
      "37 Train Loss 254.66878 Test MSE 283.08701612959396 Test RE 0.28323645233926203 Lambda1 9.8219614e-05\n",
      "38 Train Loss 254.59203 Test MSE 283.08913556675884 Test RE 0.28323751261526514 Lambda1 9.733342e-05\n",
      "39 Train Loss 254.5698 Test MSE 283.1325311136851 Test RE 0.2832592209293997 Lambda1 9.0500245e-05\n",
      "40 Train Loss 254.54942 Test MSE 283.1550026775535 Test RE 0.2832704615158402 Lambda1 9.526802e-05\n",
      "41 Train Loss 254.4945 Test MSE 283.0317257822979 Test RE 0.2832087912215524 Lambda1 9.245189e-05\n",
      "42 Train Loss 254.46832 Test MSE 283.0759977074683 Test RE 0.2832309401663551 Lambda1 8.405884e-05\n",
      "43 Train Loss 254.44191 Test MSE 283.1541629856934 Test RE 0.28327004149840296 Lambda1 0.000101519836\n",
      "44 Train Loss 254.43965 Test MSE 283.12537301792594 Test RE 0.2832556402574162 Lambda1 0.00010253231\n",
      "45 Train Loss 254.42493 Test MSE 283.05278268099966 Test RE 0.28321932606159245 Lambda1 0.00012251403\n",
      "46 Train Loss 254.41083 Test MSE 283.00157905113866 Test RE 0.283193708024381 Lambda1 0.00013472646\n",
      "47 Train Loss 254.40898 Test MSE 282.990110192875 Test RE 0.28318796964402276 Lambda1 0.00013918601\n",
      "48 Train Loss 254.40083 Test MSE 283.02938318484945 Test RE 0.2832076191876352 Lambda1 0.0001425777\n",
      "49 Train Loss 254.39052 Test MSE 283.0246028871321 Test RE 0.2832052275234964 Lambda1 0.0001533097\n",
      "50 Train Loss 254.38579 Test MSE 283.09460204160035 Test RE 0.283240247272071 Lambda1 0.00015373944\n",
      "51 Train Loss 254.3813 Test MSE 283.1414687253349 Test RE 0.2832636916995442 Lambda1 0.00015088732\n",
      "52 Train Loss 254.33086 Test MSE 283.30650246621354 Test RE 0.28334623216406535 Lambda1 0.0002028387\n",
      "53 Train Loss 254.2565 Test MSE 283.30067129360174 Test RE 0.28334331615388525 Lambda1 0.00033070846\n",
      "54 Train Loss 254.23225 Test MSE 283.36846070706724 Test RE 0.2833772139350608 Lambda1 0.0004227846\n",
      "55 Train Loss 254.18419 Test MSE 283.3099659357702 Test RE 0.2833479641364015 Lambda1 0.00065888476\n",
      "56 Train Loss 254.14139 Test MSE 283.1815252293221 Test RE 0.2832837278884702 Lambda1 0.0008567001\n",
      "57 Train Loss 254.05556 Test MSE 282.78663026106443 Test RE 0.2830861402361029 Lambda1 0.0010623534\n",
      "58 Train Loss 254.01952 Test MSE 282.5698531336707 Test RE 0.2829776160720324 Lambda1 0.0011440532\n",
      "59 Train Loss 254.00186 Test MSE 282.2976158684202 Test RE 0.2828412681647184 Lambda1 0.0013877741\n",
      "60 Train Loss 253.97379 Test MSE 282.0023544113623 Test RE 0.2826933144299172 Lambda1 0.0016781044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 Train Loss 253.89615 Test MSE 281.526750563208 Test RE 0.2824548292501789 Lambda1 0.0024030642\n",
      "62 Train Loss 253.66125 Test MSE 280.5950412522182 Test RE 0.2819870515152888 Lambda1 0.0037233206\n",
      "63 Train Loss 253.54146 Test MSE 280.4953756162008 Test RE 0.2819369670327101 Lambda1 0.00401371\n",
      "64 Train Loss 253.46858 Test MSE 280.28726984086296 Test RE 0.28183235996444916 Lambda1 0.0038854503\n",
      "65 Train Loss 253.19664 Test MSE 279.780086833632 Test RE 0.28157725507569137 Lambda1 0.004228674\n",
      "66 Train Loss 252.93517 Test MSE 279.36837218730193 Test RE 0.2813699991516582 Lambda1 0.0055998634\n",
      "67 Train Loss 252.56786 Test MSE 278.26882361442705 Test RE 0.28081573991950654 Lambda1 0.008015716\n",
      "68 Train Loss 251.80682 Test MSE 277.55502923394425 Test RE 0.28045534488662943 Lambda1 0.010960516\n",
      "69 Train Loss 250.94041 Test MSE 276.4276859556868 Test RE 0.2798852036280941 Lambda1 0.013595356\n",
      "70 Train Loss 249.91814 Test MSE 273.9485501670239 Test RE 0.2786273046850768 Lambda1 0.017517371\n",
      "71 Train Loss 248.66843 Test MSE 271.46018048340056 Test RE 0.2773589837483995 Lambda1 0.023874981\n",
      "72 Train Loss 247.27193 Test MSE 269.7811949393784 Test RE 0.2764999184868695 Lambda1 0.02821013\n",
      "73 Train Loss 244.89572 Test MSE 268.2720264900017 Test RE 0.27572545722547787 Lambda1 0.036231983\n",
      "74 Train Loss 240.07874 Test MSE 262.361953240891 Test RE 0.27267140652605487 Lambda1 0.057459805\n",
      "Training time: 133.26\n",
      "Training time: 133.26\n",
      "inv_HT_stan_tune9\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 837.94037 Test MSE 858.1409410846497 Test RE 0.4931380139307017 Lambda1 -0.011002288\n",
      "1 Train Loss 837.1726 Test MSE 856.7997627525384 Test RE 0.49275250345824806 Lambda1 -0.08675717\n",
      "2 Train Loss 803.42 Test MSE 808.216913649756 Test RE 0.4785784491289865 Lambda1 0.03866087\n",
      "3 Train Loss 735.1332 Test MSE 735.659765253635 Test RE 0.45659134275439284 Lambda1 0.0052525112\n",
      "4 Train Loss 646.739 Test MSE 650.9533816650533 Test RE 0.42950093575826004 Lambda1 0.00010566271\n",
      "5 Train Loss 624.2849 Test MSE 624.6387536597784 Test RE 0.42073014807866466 Lambda1 -0.0009907647\n",
      "6 Train Loss 517.82526 Test MSE 512.7963850122461 Test RE 0.38120768573451747 Lambda1 -0.00027108775\n",
      "7 Train Loss 368.32153 Test MSE 343.6060961255866 Test RE 0.3120467047840392 Lambda1 -0.002023065\n",
      "8 Train Loss 297.67154 Test MSE 294.78647265927447 Test RE 0.28903001529398303 Lambda1 -0.0014101226\n",
      "9 Train Loss 281.94962 Test MSE 289.73522823986355 Test RE 0.28654301239831786 Lambda1 0.00032817438\n",
      "10 Train Loss 269.55496 Test MSE 288.8168721078298 Test RE 0.28608853299875153 Lambda1 0.00013643719\n",
      "11 Train Loss 264.66272 Test MSE 288.45490898496377 Test RE 0.28590920489489163 Lambda1 0.0002540103\n",
      "12 Train Loss 262.02686 Test MSE 286.6817443177236 Test RE 0.28502909230284634 Lambda1 0.00046018095\n",
      "13 Train Loss 260.60986 Test MSE 285.13622733093337 Test RE 0.2842597502857169 Lambda1 0.00070614065\n",
      "14 Train Loss 258.18936 Test MSE 284.1689286317993 Test RE 0.28377717800211993 Lambda1 0.0005015433\n",
      "15 Train Loss 256.74734 Test MSE 283.8984271150714 Test RE 0.2836420815408686 Lambda1 0.0006500115\n",
      "16 Train Loss 256.22977 Test MSE 283.10406654287993 Test RE 0.2832449819177654 Lambda1 0.0011542636\n",
      "17 Train Loss 255.85603 Test MSE 282.1357318396898 Test RE 0.28276015864144005 Lambda1 0.0016650871\n",
      "18 Train Loss 255.34126 Test MSE 282.00948038312674 Test RE 0.2826968861232369 Lambda1 0.0017342416\n",
      "19 Train Loss 255.00003 Test MSE 282.2344889331911 Test RE 0.2828096421436425 Lambda1 0.001756781\n",
      "20 Train Loss 254.72844 Test MSE 281.6898123070934 Test RE 0.28253661705483424 Lambda1 0.0018600007\n",
      "21 Train Loss 254.54837 Test MSE 281.45323630943074 Test RE 0.28241794854226404 Lambda1 0.0017369913\n",
      "22 Train Loss 254.42674 Test MSE 281.47457040392675 Test RE 0.28242865194917627 Lambda1 0.0017723087\n",
      "23 Train Loss 254.18936 Test MSE 281.4170462202912 Test RE 0.28239979089120903 Lambda1 0.001881078\n",
      "24 Train Loss 253.98077 Test MSE 281.12855555985647 Test RE 0.2822550047217866 Lambda1 0.0018090403\n",
      "25 Train Loss 253.92496 Test MSE 281.1548171156718 Test RE 0.2822681878053154 Lambda1 0.0018175739\n",
      "26 Train Loss 253.8542 Test MSE 281.12042550475866 Test RE 0.28225092337664126 Lambda1 0.0018745152\n",
      "27 Train Loss 253.6788 Test MSE 281.57306240925004 Test RE 0.28247806055345603 Lambda1 0.0017367902\n",
      "28 Train Loss 253.63097 Test MSE 281.8040799019386 Test RE 0.28259391679528695 Lambda1 0.0016893676\n",
      "29 Train Loss 253.54579 Test MSE 281.837455826268 Test RE 0.28261065103478783 Lambda1 0.0016845631\n",
      "30 Train Loss 253.50192 Test MSE 281.7739098244165 Test RE 0.28257878907146483 Lambda1 0.0017078627\n",
      "31 Train Loss 253.47635 Test MSE 281.7831704179351 Test RE 0.28258343255638446 Lambda1 0.0016301614\n",
      "32 Train Loss 253.44966 Test MSE 281.8211437070148 Test RE 0.282602472481419 Lambda1 0.001567703\n",
      "33 Train Loss 253.36288 Test MSE 281.7032882100654 Test RE 0.2825433751808575 Lambda1 0.0015233639\n",
      "34 Train Loss 253.29918 Test MSE 281.6678291308735 Test RE 0.2825255922091479 Lambda1 0.0015177309\n",
      "35 Train Loss 253.26967 Test MSE 281.77753225949533 Test RE 0.2825806054568585 Lambda1 0.0015050696\n",
      "36 Train Loss 253.2264 Test MSE 282.03004613444836 Test RE 0.28270719387576365 Lambda1 0.0013274618\n",
      "37 Train Loss 253.07837 Test MSE 282.4711784206979 Test RE 0.2829282032050067 Lambda1 0.0009711776\n",
      "38 Train Loss 253.0081 Test MSE 282.6997013676757 Test RE 0.28304262641026995 Lambda1 0.0008627174\n",
      "39 Train Loss 252.95705 Test MSE 282.78923270356955 Test RE 0.28308744283253107 Lambda1 0.00080785365\n",
      "40 Train Loss 252.91046 Test MSE 282.8439162162479 Test RE 0.2831148120985628 Lambda1 0.00068497687\n",
      "41 Train Loss 252.68808 Test MSE 283.13371742390746 Test RE 0.28325981434930103 Lambda1 0.00039874556\n",
      "42 Train Loss 252.48355 Test MSE 283.28638515735224 Test RE 0.28333617192045674 Lambda1 0.00029552908\n",
      "43 Train Loss 252.36693 Test MSE 283.32654648172746 Test RE 0.28335625539998244 Lambda1 0.0002317643\n",
      "44 Train Loss 252.27852 Test MSE 283.37050312938726 Test RE 0.28337823517592586 Lambda1 0.00018696798\n",
      "45 Train Loss 252.03555 Test MSE 284.36756778084293 Test RE 0.28387634332974765 Lambda1 0.00011563458\n",
      "46 Train Loss 251.89996 Test MSE 285.0694563152835 Test RE 0.28422646545251845 Lambda1 7.765052e-05\n",
      "47 Train Loss 251.79025 Test MSE 284.54704782448744 Test RE 0.28396591420110967 Lambda1 7.94304e-05\n",
      "48 Train Loss 251.53952 Test MSE 284.0644814896257 Test RE 0.2837250216325948 Lambda1 6.312142e-05\n",
      "49 Train Loss 251.34459 Test MSE 284.50683190540946 Test RE 0.2839458465992025 Lambda1 3.5388264e-05\n",
      "50 Train Loss 251.22992 Test MSE 284.8655431838623 Test RE 0.284124792201616 Lambda1 3.08062e-05\n",
      "51 Train Loss 251.15086 Test MSE 285.00161363216483 Test RE 0.2841926424087063 Lambda1 3.0793617e-05\n",
      "52 Train Loss 251.05464 Test MSE 284.8237362561244 Test RE 0.28410394232912195 Lambda1 3.313616e-05\n",
      "53 Train Loss 250.97145 Test MSE 285.1569415949598 Test RE 0.28427007539354 Lambda1 3.8423983e-05\n",
      "54 Train Loss 250.71092 Test MSE 286.62985744574735 Test RE 0.2850032972573693 Lambda1 1.3898634e-05\n",
      "55 Train Loss 250.4989 Test MSE 287.1663329178619 Test RE 0.2852698881204287 Lambda1 5.711323e-06\n",
      "56 Train Loss 250.34366 Test MSE 287.050102768492 Test RE 0.2852121509945145 Lambda1 5.127686e-06\n",
      "57 Train Loss 250.243 Test MSE 287.0112134433605 Test RE 0.2851928301796743 Lambda1 5.817982e-06\n",
      "58 Train Loss 250.17046 Test MSE 286.86058413406084 Test RE 0.28511798286560736 Lambda1 6.906613e-06\n",
      "59 Train Loss 249.98358 Test MSE 286.2575741292016 Test RE 0.28481815177364267 Lambda1 7.1117965e-06\n",
      "60 Train Loss 249.87398 Test MSE 286.4488264958892 Test RE 0.28491328122204385 Lambda1 6.0765833e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 Train Loss 249.81732 Test MSE 286.8462853680259 Test RE 0.2851108768246257 Lambda1 8.017381e-06\n",
      "62 Train Loss 249.78217 Test MSE 286.6662763612882 Test RE 0.28502140280544863 Lambda1 1.09650455e-05\n",
      "63 Train Loss 249.7174 Test MSE 286.5723660495798 Test RE 0.2849747132512819 Lambda1 9.093225e-06\n",
      "64 Train Loss 249.67139 Test MSE 286.8999802768125 Test RE 0.28513756060431544 Lambda1 6.060963e-06\n",
      "65 Train Loss 249.65959 Test MSE 286.96566832161255 Test RE 0.2851702009984636 Lambda1 7.0547026e-06\n",
      "66 Train Loss 249.62949 Test MSE 287.02189100159353 Test RE 0.28519813508517583 Lambda1 7.1588756e-06\n",
      "67 Train Loss 249.54684 Test MSE 287.55052665281147 Test RE 0.28546065259769593 Lambda1 1.0135161e-06\n",
      "68 Train Loss 249.52573 Test MSE 287.6274116328625 Test RE 0.2854988131428137 Lambda1 1.1207411e-06\n",
      "69 Train Loss 249.50392 Test MSE 287.49065734753015 Test RE 0.28543093395864744 Lambda1 1.8375904e-06\n",
      "70 Train Loss 249.49522 Test MSE 287.48004548971437 Test RE 0.28542566599539065 Lambda1 1.1516378e-06\n",
      "71 Train Loss 249.46832 Test MSE 287.7310360740497 Test RE 0.28555023729366935 Lambda1 3.939111e-06\n",
      "72 Train Loss 249.3763 Test MSE 288.29882172450266 Test RE 0.2858318395566164 Lambda1 7.5167422e-06\n",
      "73 Train Loss 249.33647 Test MSE 288.3349350378247 Test RE 0.2858497411413072 Lambda1 5.901759e-06\n",
      "74 Train Loss 249.32088 Test MSE 288.57180463378944 Test RE 0.28596713103110355 Lambda1 6.0063244e-06\n",
      "Training time: 122.71\n",
      "Training time: 122.71\n",
      "inv_HT_stan_tune9\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 837.8664 Test MSE 858.0186379173283 Test RE 0.49310287139918924 Lambda1 -0.046975575\n",
      "1 Train Loss 833.9226 Test MSE 851.4459840145971 Test RE 0.4912105901721732 Lambda1 -0.035085645\n",
      "2 Train Loss 809.88464 Test MSE 771.3652089650176 Test RE 0.46754045385084037 Lambda1 0.025841027\n",
      "3 Train Loss 691.1731 Test MSE 685.8383830798248 Test RE 0.4408593581768034 Lambda1 0.051176403\n",
      "4 Train Loss 663.8942 Test MSE 665.7719907736551 Test RE 0.43436210665236463 Lambda1 0.034589\n",
      "5 Train Loss 633.1428 Test MSE 633.6548606917735 Test RE 0.4237557028714074 Lambda1 0.0090649845\n",
      "6 Train Loss 599.8646 Test MSE 596.1088198351922 Test RE 0.4110095796993091 Lambda1 0.0026682462\n",
      "7 Train Loss 400.70312 Test MSE 404.55323198194594 Test RE 0.3385922459589544 Lambda1 0.0053418735\n",
      "8 Train Loss 295.21924 Test MSE 307.39751859023676 Test RE 0.2951476633692589 Lambda1 0.0053655882\n",
      "9 Train Loss 265.40833 Test MSE 285.4623293775025 Test RE 0.28442225365896934 Lambda1 0.0058063604\n",
      "10 Train Loss 260.60013 Test MSE 282.48329506219727 Test RE 0.2829342712627562 Lambda1 0.0048856465\n",
      "11 Train Loss 256.76636 Test MSE 280.3071601570137 Test RE 0.28184235976803734 Lambda1 0.004787089\n",
      "12 Train Loss 255.14098 Test MSE 279.5660038981977 Test RE 0.28146950541078497 Lambda1 0.0049201306\n",
      "13 Train Loss 254.53627 Test MSE 278.8555126501536 Test RE 0.28111161346787494 Lambda1 0.0055790655\n",
      "14 Train Loss 253.85405 Test MSE 278.55321370513144 Test RE 0.28095919979157596 Lambda1 0.006884293\n",
      "15 Train Loss 251.79337 Test MSE 277.59128751062127 Test RE 0.280473662867464 Lambda1 0.011931757\n",
      "16 Train Loss 249.60083 Test MSE 274.72565619433925 Test RE 0.2790222139174939 Lambda1 0.018156668\n",
      "17 Train Loss 248.36353 Test MSE 272.89605181729894 Test RE 0.27809155264590774 Lambda1 0.022117702\n",
      "18 Train Loss 246.5528 Test MSE 271.3902079120637 Test RE 0.277323234910317 Lambda1 0.026915057\n",
      "19 Train Loss 243.30656 Test MSE 266.91853057381934 Test RE 0.27502902758177056 Lambda1 0.0362278\n",
      "20 Train Loss 240.4625 Test MSE 262.53692140426386 Test RE 0.2727623131210479 Lambda1 0.048977982\n",
      "21 Train Loss 233.73642 Test MSE 255.8619411241014 Test RE 0.26927250827897675 Lambda1 0.077689\n",
      "22 Train Loss 228.61037 Test MSE 250.0226073062753 Test RE 0.26618207763055746 Lambda1 0.10744607\n",
      "23 Train Loss 222.85391 Test MSE 245.70693245692485 Test RE 0.26387477476903465 Lambda1 0.1379929\n",
      "24 Train Loss 220.3788 Test MSE 243.48363745056636 Test RE 0.2626782178886213 Lambda1 0.15089239\n",
      "25 Train Loss 218.4571 Test MSE 242.00770879100702 Test RE 0.2618808673463756 Lambda1 0.17394108\n",
      "26 Train Loss 216.38622 Test MSE 241.1432635123315 Test RE 0.261412733087926 Lambda1 0.20399773\n",
      "27 Train Loss 214.68735 Test MSE 239.5647190022227 Test RE 0.26055571315433734 Lambda1 0.22404234\n",
      "28 Train Loss 208.83685 Test MSE 226.320727041629 Test RE 0.2532510971011414 Lambda1 0.25406474\n",
      "29 Train Loss 193.09596 Test MSE 208.3027450116495 Test RE 0.2429610563735836 Lambda1 0.2629668\n",
      "30 Train Loss 185.35202 Test MSE 197.40677140079362 Test RE 0.23652126519897956 Lambda1 0.27587876\n",
      "31 Train Loss 169.44284 Test MSE 178.21861136270354 Test RE 0.22473240441837528 Lambda1 0.34027556\n",
      "32 Train Loss 161.09888 Test MSE 168.00920113594677 Test RE 0.21820048121310331 Lambda1 0.38818774\n",
      "33 Train Loss 155.47664 Test MSE 156.6558996512393 Test RE 0.21069903577573032 Lambda1 0.4246793\n",
      "34 Train Loss 151.85968 Test MSE 154.50510207668358 Test RE 0.2092476471203524 Lambda1 0.42868346\n",
      "35 Train Loss 145.79102 Test MSE 149.45540425199263 Test RE 0.2057998161372039 Lambda1 0.46069494\n",
      "36 Train Loss 141.01392 Test MSE 145.007188101704 Test RE 0.20271409000117466 Lambda1 0.4954826\n",
      "37 Train Loss 136.82169 Test MSE 140.56927828021762 Test RE 0.19958797788659527 Lambda1 0.5274139\n",
      "38 Train Loss 132.89702 Test MSE 138.2136219317399 Test RE 0.19790856718092312 Lambda1 0.5404617\n",
      "39 Train Loss 130.63506 Test MSE 136.7524302779767 Test RE 0.19685964480613752 Lambda1 0.5465167\n",
      "40 Train Loss 129.00728 Test MSE 135.71596120958165 Test RE 0.19611221013373095 Lambda1 0.54751\n",
      "41 Train Loss 127.801315 Test MSE 134.80342777905818 Test RE 0.1954517838100036 Lambda1 0.54747117\n",
      "42 Train Loss 126.38036 Test MSE 132.9486920014831 Test RE 0.19410253399131827 Lambda1 0.54768586\n",
      "43 Train Loss 124.75638 Test MSE 131.2516314438517 Test RE 0.19285971729195645 Lambda1 0.5458111\n",
      "44 Train Loss 121.83426 Test MSE 128.3400247091372 Test RE 0.1907085788910152 Lambda1 0.54579633\n",
      "45 Train Loss 118.69739 Test MSE 123.35609310264178 Test RE 0.18696894336413672 Lambda1 0.5539561\n",
      "46 Train Loss 115.75827 Test MSE 119.90209417837802 Test RE 0.18433277233317794 Lambda1 0.5633781\n",
      "47 Train Loss 112.82849 Test MSE 116.55193152404614 Test RE 0.1817393239537827 Lambda1 0.5842194\n",
      "48 Train Loss 110.68105 Test MSE 111.9500529628176 Test RE 0.1781153392133676 Lambda1 0.61523277\n",
      "49 Train Loss 107.0876 Test MSE 106.9291126114048 Test RE 0.17407529986813838 Lambda1 0.65188617\n",
      "50 Train Loss 103.011894 Test MSE 100.99084359304635 Test RE 0.16917265710963686 Lambda1 0.695042\n",
      "51 Train Loss 99.11773 Test MSE 95.35516969266273 Test RE 0.16438466173318955 Lambda1 0.7475958\n",
      "52 Train Loss 92.319 Test MSE 89.03367348818497 Test RE 0.15884235485412068 Lambda1 0.8095696\n",
      "53 Train Loss 89.52071 Test MSE 87.4429789961137 Test RE 0.15741700415798712 Lambda1 0.8294836\n",
      "54 Train Loss 86.44398 Test MSE 86.12785918894984 Test RE 0.15622876399444416 Lambda1 0.8798648\n",
      "55 Train Loss 83.73396 Test MSE 82.66630637031477 Test RE 0.15305708479159297 Lambda1 0.93027925\n",
      "56 Train Loss 80.450836 Test MSE 79.16065996549023 Test RE 0.14977656761653374 Lambda1 0.95887893\n",
      "57 Train Loss 76.77896 Test MSE 74.33824488269924 Test RE 0.1451427416145491 Lambda1 1.0259638\n",
      "58 Train Loss 74.54701 Test MSE 71.94480837284426 Test RE 0.1427870761748135 Lambda1 1.0680405\n",
      "59 Train Loss 72.44301 Test MSE 70.4379454578862 Test RE 0.14128384662659432 Lambda1 1.0988065\n",
      "60 Train Loss 69.574234 Test MSE 69.19100524145273 Test RE 0.14002771139328588 Lambda1 1.1370472\n",
      "61 Train Loss 68.49751 Test MSE 67.36433259069834 Test RE 0.13816695177112046 Lambda1 1.1595751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 Train Loss 67.7699 Test MSE 66.43761325463058 Test RE 0.13721329128817034 Lambda1 1.1656474\n",
      "63 Train Loss 66.06177 Test MSE 64.95378098839157 Test RE 0.13567236293311616 Lambda1 1.1875769\n",
      "64 Train Loss 63.93947 Test MSE 63.497080332591956 Test RE 0.13414239277443388 Lambda1 1.2160252\n",
      "65 Train Loss 62.828426 Test MSE 62.258995235372545 Test RE 0.13282818073063427 Lambda1 1.227092\n",
      "66 Train Loss 60.80851 Test MSE 61.56545356620051 Test RE 0.13208628096564878 Lambda1 1.2387894\n",
      "67 Train Loss 59.900795 Test MSE 60.98864746476961 Test RE 0.1314660673491864 Lambda1 1.2543219\n",
      "68 Train Loss 59.15852 Test MSE 60.3931479373073 Test RE 0.13082266873353698 Lambda1 1.278018\n",
      "69 Train Loss 58.234 Test MSE 59.694590181833064 Test RE 0.13006386580915374 Lambda1 1.3030827\n",
      "70 Train Loss 57.51485 Test MSE 57.7125048880109 Test RE 0.12788633246244946 Lambda1 1.3256867\n",
      "71 Train Loss 57.118137 Test MSE 56.2156142603716 Test RE 0.12621694114515908 Lambda1 1.3462166\n",
      "72 Train Loss 56.670834 Test MSE 55.616643742857 Test RE 0.125542727357942 Lambda1 1.3607337\n",
      "73 Train Loss 56.298275 Test MSE 55.32345716366302 Test RE 0.12521138698252318 Lambda1 1.3696475\n",
      "74 Train Loss 55.874386 Test MSE 54.65383693329976 Test RE 0.12445131760368258 Lambda1 1.3701718\n",
      "Training time: 122.65\n",
      "Training time: 122.65\n",
      "inv_HT_stan_tune9\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 837.9375 Test MSE 858.1272004588492 Test RE 0.4931340658309678 Lambda1 -0.14181668\n",
      "1 Train Loss 837.2129 Test MSE 856.5022779338278 Test RE 0.49266695306385605 Lambda1 -0.2032033\n",
      "2 Train Loss 836.5513 Test MSE 855.0287330974048 Test RE 0.49224297324816874 Lambda1 -0.28982064\n",
      "3 Train Loss 830.70984 Test MSE 848.1484669596354 Test RE 0.49025847655756677 Lambda1 -0.3365966\n",
      "4 Train Loss 813.32996 Test MSE 828.0754932432496 Test RE 0.4844223103456963 Lambda1 -0.5616936\n",
      "5 Train Loss 783.2951 Test MSE 786.4413053334937 Test RE 0.47208731198958337 Lambda1 -0.76519775\n",
      "6 Train Loss 750.8993 Test MSE 748.5497437616652 Test RE 0.46057409100811764 Lambda1 -0.79706407\n",
      "7 Train Loss 722.5885 Test MSE 720.4154432258083 Test RE 0.45183584113318953 Lambda1 -0.8241331\n",
      "8 Train Loss 696.5072 Test MSE 698.4092210281964 Test RE 0.44488130188344777 Lambda1 -0.82381976\n",
      "9 Train Loss 679.10986 Test MSE 680.2025084237853 Test RE 0.4390442413929691 Lambda1 -0.84673744\n",
      "10 Train Loss 655.29376 Test MSE 651.4734873254993 Test RE 0.429672485112759 Lambda1 -0.835954\n",
      "11 Train Loss 619.06836 Test MSE 595.7457414623384 Test RE 0.41088439163426177 Lambda1 -0.89139795\n",
      "12 Train Loss 582.4698 Test MSE 526.5044543937779 Test RE 0.3862693026457018 Lambda1 -0.9417182\n",
      "13 Train Loss 519.817 Test MSE 507.86703230139074 Test RE 0.37937104564829427 Lambda1 -0.97254586\n",
      "14 Train Loss 447.15674 Test MSE 454.5959899314543 Test RE 0.35892356344782933 Lambda1 -0.9832216\n",
      "15 Train Loss 417.46017 Test MSE 418.75974938785345 Test RE 0.3444860471811044 Lambda1 -1.0071929\n",
      "16 Train Loss 383.78314 Test MSE 382.19945380856535 Test RE 0.32910478873768 Lambda1 -0.9550441\n",
      "17 Train Loss 357.44272 Test MSE 369.0725906464242 Test RE 0.3234037618109957 Lambda1 -0.87993985\n",
      "18 Train Loss 341.85452 Test MSE 353.66846535190064 Test RE 0.3165828172918561 Lambda1 -0.8517437\n",
      "19 Train Loss 326.98645 Test MSE 337.4735670718273 Test RE 0.3092495323971631 Lambda1 -0.841443\n",
      "20 Train Loss 302.98026 Test MSE 317.5923541933118 Test RE 0.30000202756404115 Lambda1 -0.8255687\n",
      "21 Train Loss 278.89407 Test MSE 291.7225559798726 Test RE 0.28752404899157963 Lambda1 -0.83367753\n",
      "22 Train Loss 268.4454 Test MSE 282.72982655412534 Test RE 0.2830577068732271 Lambda1 -0.83632505\n",
      "23 Train Loss 263.116 Test MSE 276.429804644484 Test RE 0.27988627622056467 Lambda1 -0.834565\n",
      "24 Train Loss 246.05627 Test MSE 249.25371428938462 Test RE 0.265772468401593 Lambda1 -0.8397246\n",
      "25 Train Loss 232.93074 Test MSE 238.10617747058552 Test RE 0.25976133171505816 Lambda1 -0.8425067\n",
      "26 Train Loss 227.84286 Test MSE 236.44678823122666 Test RE 0.258854595921023 Lambda1 -0.85361224\n",
      "27 Train Loss 222.67441 Test MSE 231.63987001624034 Test RE 0.2562098527783598 Lambda1 -0.861938\n",
      "28 Train Loss 220.09752 Test MSE 228.4596757936811 Test RE 0.2544450161514715 Lambda1 -0.86369413\n",
      "29 Train Loss 215.02228 Test MSE 222.62280282969726 Test RE 0.2511736020218644 Lambda1 -0.8702349\n",
      "30 Train Loss 213.2746 Test MSE 219.76683202897465 Test RE 0.24955728060770632 Lambda1 -0.8741656\n",
      "31 Train Loss 209.52116 Test MSE 214.2233843727883 Test RE 0.24638973401371347 Lambda1 -0.8962467\n",
      "32 Train Loss 204.73967 Test MSE 209.6111414083373 Test RE 0.243722908428464 Lambda1 -0.9223881\n",
      "33 Train Loss 196.40842 Test MSE 202.6776787126648 Test RE 0.23965811077272695 Lambda1 -0.946093\n",
      "34 Train Loss 189.1196 Test MSE 203.13952364191195 Test RE 0.23993101181637935 Lambda1 -0.94982314\n",
      "35 Train Loss 184.9509 Test MSE 197.09901582718635 Test RE 0.23633682590820732 Lambda1 -0.97380537\n",
      "36 Train Loss 183.01099 Test MSE 192.48986873097053 Test RE 0.23355711864865547 Lambda1 -0.9958688\n",
      "37 Train Loss 179.73616 Test MSE 190.2554772275907 Test RE 0.23219761513342851 Lambda1 -1.0195776\n",
      "38 Train Loss 176.25069 Test MSE 185.97001940248302 Test RE 0.22956762372335612 Lambda1 -1.040872\n",
      "39 Train Loss 171.93004 Test MSE 178.69480533848238 Test RE 0.22503244274143286 Lambda1 -1.0793743\n",
      "40 Train Loss 168.82947 Test MSE 178.1560275133547 Test RE 0.22469294205864757 Lambda1 -1.0964065\n",
      "41 Train Loss 166.5599 Test MSE 176.16765056769506 Test RE 0.22343553899637952 Lambda1 -1.1053047\n",
      "42 Train Loss 165.01414 Test MSE 173.44828343028902 Test RE 0.22170432923053654 Lambda1 -1.1168486\n",
      "43 Train Loss 161.264 Test MSE 169.97247666579074 Test RE 0.21947167183259228 Lambda1 -1.1375892\n",
      "44 Train Loss 159.9009 Test MSE 170.3820443593029 Test RE 0.21973593350667187 Lambda1 -1.134893\n",
      "45 Train Loss 158.35292 Test MSE 170.03305808685118 Test RE 0.21951078028563603 Lambda1 -1.1304514\n",
      "46 Train Loss 155.72427 Test MSE 167.6496581899049 Test RE 0.21796687953914443 Lambda1 -1.1330678\n",
      "47 Train Loss 153.74327 Test MSE 165.92055425916539 Test RE 0.21683993355889572 Lambda1 -1.1366028\n",
      "48 Train Loss 152.96594 Test MSE 164.6899024235206 Test RE 0.21603427349568485 Lambda1 -1.1404662\n",
      "49 Train Loss 151.31706 Test MSE 160.90212689632304 Test RE 0.21353548860227609 Lambda1 -1.1648613\n",
      "50 Train Loss 149.30333 Test MSE 159.21091413815498 Test RE 0.21241030796315785 Lambda1 -1.1756648\n",
      "51 Train Loss 148.06285 Test MSE 157.52151237524347 Test RE 0.21128034971581508 Lambda1 -1.1738325\n",
      "52 Train Loss 146.08594 Test MSE 154.19608722705934 Test RE 0.20903829159580178 Lambda1 -1.1821916\n",
      "53 Train Loss 144.65674 Test MSE 153.0382289613663 Test RE 0.2082519785464648 Lambda1 -1.19685\n",
      "54 Train Loss 143.45409 Test MSE 150.5143391378043 Test RE 0.20652760495675074 Lambda1 -1.2068367\n",
      "55 Train Loss 140.44662 Test MSE 148.6672073308118 Test RE 0.2052564259044526 Lambda1 -1.2195703\n",
      "56 Train Loss 139.128 Test MSE 147.70980464718278 Test RE 0.20459444246935565 Lambda1 -1.2285086\n",
      "57 Train Loss 138.17508 Test MSE 146.96192924279887 Test RE 0.2040758400554937 Lambda1 -1.2342837\n",
      "58 Train Loss 136.14377 Test MSE 146.3636294897922 Test RE 0.20366000770901113 Lambda1 -1.2542925\n",
      "59 Train Loss 133.4981 Test MSE 141.18942064330793 Test RE 0.20002774947584362 Lambda1 -1.2832081\n",
      "60 Train Loss 131.13678 Test MSE 136.65385685262092 Test RE 0.19678868216361156 Lambda1 -1.316991\n",
      "61 Train Loss 130.1983 Test MSE 136.5139507377596 Test RE 0.19668792031535923 Lambda1 -1.3323228\n",
      "62 Train Loss 129.30774 Test MSE 134.99865788203186 Test RE 0.19559326487922762 Lambda1 -1.3511227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 Train Loss 128.12393 Test MSE 134.5688296510477 Test RE 0.19528163758287173 Lambda1 -1.3594\n",
      "64 Train Loss 127.20357 Test MSE 133.5643436170355 Test RE 0.19455143466581307 Lambda1 -1.3537806\n",
      "65 Train Loss 125.98141 Test MSE 132.69164638548818 Test RE 0.19391480237797118 Lambda1 -1.3596622\n",
      "66 Train Loss 125.17181 Test MSE 132.75174473284503 Test RE 0.193958711094899 Lambda1 -1.3652635\n",
      "67 Train Loss 123.99325 Test MSE 130.68879700331252 Test RE 0.19244576163557434 Lambda1 -1.382125\n",
      "68 Train Loss 122.74096 Test MSE 128.53259267421097 Test RE 0.19085159973578866 Lambda1 -1.3970034\n",
      "69 Train Loss 121.63952 Test MSE 127.33958633620269 Test RE 0.18996381725596848 Lambda1 -1.3927083\n",
      "70 Train Loss 120.75688 Test MSE 126.69999391430369 Test RE 0.1894861481724087 Lambda1 -1.3892562\n",
      "71 Train Loss 120.11214 Test MSE 126.54438231346528 Test RE 0.18936974997387668 Lambda1 -1.3910754\n",
      "72 Train Loss 119.53853 Test MSE 126.46288619195074 Test RE 0.18930876194178417 Lambda1 -1.3955215\n",
      "73 Train Loss 118.89932 Test MSE 125.5808271261336 Test RE 0.1886474070487879 Lambda1 -1.4046681\n",
      "74 Train Loss 117.97591 Test MSE 124.59092572073122 Test RE 0.1879024215352144 Lambda1 -1.4143085\n",
      "Training time: 121.72\n",
      "Training time: 121.72\n",
      "inv_HT_stan_tune9\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 837.9859 Test MSE 858.1921007781657 Test RE 0.49315271338882194 Lambda1 -0.03334339\n",
      "1 Train Loss 837.70557 Test MSE 857.2129816903738 Test RE 0.4928713119190759 Lambda1 -0.051384185\n",
      "2 Train Loss 834.01605 Test MSE 852.5473073380022 Test RE 0.49152817159625223 Lambda1 -0.22131711\n",
      "3 Train Loss 817.9737 Test MSE 820.1955439910695 Test RE 0.48211192447242646 Lambda1 -0.22284596\n",
      "4 Train Loss 753.8664 Test MSE 761.6014643469397 Test RE 0.46457202646380125 Lambda1 0.009653484\n",
      "5 Train Loss 672.93695 Test MSE 676.103077080085 Test RE 0.43771923030987137 Lambda1 0.0026295828\n",
      "6 Train Loss 646.52686 Test MSE 644.6106137190069 Test RE 0.4274033251936921 Lambda1 0.0009862236\n",
      "7 Train Loss 538.7431 Test MSE 513.0436518517446 Test RE 0.3812995825012035 Lambda1 0.0009957323\n",
      "8 Train Loss 427.42892 Test MSE 437.0198655933071 Test RE 0.35191660574458783 Lambda1 0.0010654131\n",
      "9 Train Loss 377.66592 Test MSE 395.21353671724796 Test RE 0.3346609783727542 Lambda1 8.072626e-05\n",
      "10 Train Loss 349.42123 Test MSE 379.44356372948704 Test RE 0.32791611944805826 Lambda1 1.5609372e-05\n",
      "11 Train Loss 344.07547 Test MSE 374.29710767361354 Test RE 0.3256847370780918 Lambda1 1.2690192e-05\n",
      "12 Train Loss 324.39542 Test MSE 361.5099356092268 Test RE 0.32007318356810477 Lambda1 9.82711e-06\n",
      "13 Train Loss 316.28293 Test MSE 351.14404159523485 Test RE 0.3154509374466243 Lambda1 1.6108295e-05\n",
      "14 Train Loss 314.96323 Test MSE 349.47616693516113 Test RE 0.31470087648726275 Lambda1 4.1505537e-05\n",
      "15 Train Loss 314.2547 Test MSE 349.21466195419293 Test RE 0.3145831127378439 Lambda1 3.9969236e-05\n",
      "16 Train Loss 313.84912 Test MSE 346.7346832718567 Test RE 0.3134641026684281 Lambda1 4.072582e-05\n",
      "17 Train Loss 299.1528 Test MSE 326.0470413077007 Test RE 0.30396900537685395 Lambda1 5.3437103e-05\n",
      "18 Train Loss 296.90088 Test MSE 326.6127021484471 Test RE 0.30423256987769726 Lambda1 4.344925e-05\n",
      "19 Train Loss 295.9767 Test MSE 325.3857120241916 Test RE 0.30366057496980825 Lambda1 2.3123148e-05\n",
      "20 Train Loss 294.8358 Test MSE 323.1212953904434 Test RE 0.3026021164707336 Lambda1 3.8495884e-05\n",
      "21 Train Loss 292.51935 Test MSE 319.6201324937119 Test RE 0.3009582370661004 Lambda1 4.4219174e-05\n",
      "22 Train Loss 290.9617 Test MSE 317.0001403775714 Test RE 0.2997221904654954 Lambda1 3.596571e-05\n",
      "23 Train Loss 289.77896 Test MSE 317.75110465574954 Test RE 0.30007699711226477 Lambda1 2.8847242e-05\n",
      "24 Train Loss 288.10687 Test MSE 316.00335674752694 Test RE 0.29925059225357054 Lambda1 4.371509e-05\n",
      "25 Train Loss 286.8958 Test MSE 313.60431211551906 Test RE 0.2981124978597093 Lambda1 0.00010102321\n",
      "26 Train Loss 284.65277 Test MSE 309.1862193204627 Test RE 0.2960051281299319 Lambda1 0.00012675917\n",
      "27 Train Loss 284.00012 Test MSE 308.12062800780234 Test RE 0.29549460613383527 Lambda1 7.228919e-05\n",
      "28 Train Loss 280.483 Test MSE 297.8761601378243 Test RE 0.2905407438140405 Lambda1 0.00010063492\n",
      "29 Train Loss 273.40024 Test MSE 294.00605773984387 Test RE 0.2886471740791823 Lambda1 0.00027566488\n",
      "30 Train Loss 268.4311 Test MSE 289.43408392234676 Test RE 0.28639406048405747 Lambda1 0.0003926174\n",
      "31 Train Loss 263.09778 Test MSE 286.2167335276871 Test RE 0.2847978334299315 Lambda1 -9.1854145e-06\n",
      "32 Train Loss 260.6425 Test MSE 285.50211329266745 Test RE 0.28444207245014663 Lambda1 0.00010134891\n",
      "33 Train Loss 260.01764 Test MSE 285.035457851297 Test RE 0.28420951598428856 Lambda1 0.000114906914\n",
      "34 Train Loss 258.9512 Test MSE 284.31950521317924 Test RE 0.28385235254447705 Lambda1 0.00013931286\n",
      "35 Train Loss 258.20657 Test MSE 284.0665748826203 Test RE 0.28372606707635334 Lambda1 0.00017229037\n",
      "36 Train Loss 257.86035 Test MSE 284.05594720922227 Test RE 0.2837207595596556 Lambda1 0.0001335726\n",
      "37 Train Loss 257.347 Test MSE 283.6200617623692 Test RE 0.2835029904358352 Lambda1 0.00012685485\n",
      "38 Train Loss 256.7223 Test MSE 283.33121675261856 Test RE 0.2833585907706607 Lambda1 8.1858634e-05\n",
      "39 Train Loss 256.30585 Test MSE 283.2802461334246 Test RE 0.28333310185240235 Lambda1 4.417505e-05\n",
      "40 Train Loss 256.09424 Test MSE 283.098884395723 Test RE 0.28324238954261394 Lambda1 5.2462532e-05\n",
      "41 Train Loss 255.99461 Test MSE 283.4253479293621 Test RE 0.2834056569973893 Lambda1 5.2376057e-05\n",
      "42 Train Loss 255.90129 Test MSE 283.4362152060451 Test RE 0.28341109020614924 Lambda1 5.78215e-05\n",
      "43 Train Loss 255.61868 Test MSE 283.1148065250868 Test RE 0.2832503545308341 Lambda1 3.993827e-05\n",
      "44 Train Loss 255.51877 Test MSE 283.3992171937585 Test RE 0.28339259223616464 Lambda1 2.9635452e-05\n",
      "45 Train Loss 255.48625 Test MSE 283.46595907322586 Test RE 0.2834259604313167 Lambda1 2.3440156e-05\n",
      "46 Train Loss 255.41841 Test MSE 283.45779573778907 Test RE 0.283421879310162 Lambda1 2.8843222e-05\n",
      "47 Train Loss 255.19633 Test MSE 283.0582916562575 Test RE 0.28322208215650424 Lambda1 2.8879589e-05\n",
      "48 Train Loss 254.95197 Test MSE 283.1093466418638 Test RE 0.2832476232690293 Lambda1 3.14526e-05\n",
      "49 Train Loss 254.83981 Test MSE 282.88500734747754 Test RE 0.2831353765949797 Lambda1 6.166711e-05\n",
      "50 Train Loss 254.79907 Test MSE 282.8265519502157 Test RE 0.2831061215167843 Lambda1 6.92103e-05\n",
      "51 Train Loss 254.74997 Test MSE 282.8564894281371 Test RE 0.2831211046556217 Lambda1 7.791202e-05\n",
      "52 Train Loss 254.71461 Test MSE 282.8322014615042 Test RE 0.2831089490505347 Lambda1 6.118279e-05\n",
      "53 Train Loss 254.67273 Test MSE 282.60291254025424 Test RE 0.28299416914439784 Lambda1 6.0807753e-05\n",
      "54 Train Loss 254.64572 Test MSE 282.5729983126603 Test RE 0.2829791909264731 Lambda1 6.633424e-05\n",
      "55 Train Loss 254.52527 Test MSE 282.81894350044587 Test RE 0.2831023135058729 Lambda1 0.000101761645\n",
      "56 Train Loss 254.48547 Test MSE 283.12818378315364 Test RE 0.28325704628316317 Lambda1 8.6477114e-05\n",
      "57 Train Loss 254.46388 Test MSE 283.03528437074107 Test RE 0.2832105716233092 Lambda1 9.446483e-05\n",
      "58 Train Loss 254.44525 Test MSE 282.98916887321883 Test RE 0.2831874986547297 Lambda1 0.000104693594\n",
      "59 Train Loss 254.4207 Test MSE 283.0012855340622 Test RE 0.2831935611661707 Lambda1 0.0001056404\n",
      "60 Train Loss 254.40405 Test MSE 283.11362563639807 Test RE 0.2832497638031843 Lambda1 0.00011844154\n",
      "61 Train Loss 254.38335 Test MSE 283.32780680832064 Test RE 0.28335688562865563 Lambda1 0.00013875321\n",
      "62 Train Loss 254.34781 Test MSE 283.23912448066 Test RE 0.28331253644332904 Lambda1 0.00015972482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 Train Loss 254.32138 Test MSE 282.9829121129304 Test RE 0.28318436806479125 Lambda1 0.00018481373\n",
      "64 Train Loss 254.29851 Test MSE 283.1055738585121 Test RE 0.2832457359497119 Lambda1 0.00023187046\n",
      "65 Train Loss 254.26503 Test MSE 283.06980431952167 Test RE 0.2832278417604499 Lambda1 0.00041274904\n",
      "66 Train Loss 254.1653 Test MSE 282.49887478226 Test RE 0.2829420734514915 Lambda1 0.0009466041\n",
      "67 Train Loss 254.13156 Test MSE 282.2447202226551 Test RE 0.28281476816686574 Lambda1 0.0011359745\n",
      "68 Train Loss 253.96971 Test MSE 281.59357734131373 Test RE 0.28248835080036777 Lambda1 0.0019449289\n",
      "69 Train Loss 253.68463 Test MSE 280.77196036131045 Test RE 0.28207593590232705 Lambda1 0.002625065\n",
      "70 Train Loss 253.48888 Test MSE 280.8125769312318 Test RE 0.2820963377662579 Lambda1 0.002825218\n",
      "71 Train Loss 253.42279 Test MSE 280.8121048793176 Test RE 0.2820961006611865 Lambda1 0.002933434\n",
      "72 Train Loss 253.22826 Test MSE 280.2302648922407 Test RE 0.281803698912234 Lambda1 0.0040219827\n",
      "73 Train Loss 252.87024 Test MSE 278.7970160352383 Test RE 0.2810821269784021 Lambda1 0.0068121403\n",
      "74 Train Loss 252.33919 Test MSE 277.6037788341487 Test RE 0.28047997331020663 Lambda1 0.008950378\n",
      "Training time: 123.89\n",
      "Training time: 123.89\n",
      "inv_HT_stan_tune9\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 838.0503 Test MSE 858.2675270323415 Test RE 0.4931743844369525 Lambda1 -0.057578344\n",
      "1 Train Loss 837.9783 Test MSE 857.9778458091926 Test RE 0.49309114965805023 Lambda1 -0.057938766\n",
      "2 Train Loss 837.87646 Test MSE 857.9041064320803 Test RE 0.49306995970792705 Lambda1 -0.064443976\n",
      "3 Train Loss 837.8655 Test MSE 857.9664726953071 Test RE 0.49308788150817134 Lambda1 -0.07402739\n",
      "4 Train Loss 837.8171 Test MSE 857.7815181608781 Test RE 0.49303473038980333 Lambda1 -0.12866037\n",
      "5 Train Loss 837.42285 Test MSE 855.7599911457963 Test RE 0.492453422144353 Lambda1 -0.16712722\n",
      "6 Train Loss 819.13 Test MSE 802.9535033826647 Test RE 0.4770175629472851 Lambda1 0.06138585\n",
      "7 Train Loss 755.72253 Test MSE 763.5394707352938 Test RE 0.4651627366717359 Lambda1 0.19701244\n",
      "8 Train Loss 661.35034 Test MSE 657.2361794179977 Test RE 0.4315686625980018 Lambda1 0.12576315\n",
      "9 Train Loss 633.4842 Test MSE 639.2119509184805 Test RE 0.4256097944197841 Lambda1 0.04399523\n",
      "10 Train Loss 602.9178 Test MSE 605.7872734441899 Test RE 0.4143327316829412 Lambda1 0.007396493\n",
      "11 Train Loss 549.01514 Test MSE 526.5640878207507 Test RE 0.386291177019086 Lambda1 0.010119822\n",
      "12 Train Loss 432.92883 Test MSE 416.1856066708246 Test RE 0.3434256262690441 Lambda1 0.003917657\n",
      "13 Train Loss 333.2582 Test MSE 336.9837680013582 Test RE 0.30902503313555535 Lambda1 0.0027357412\n",
      "14 Train Loss 277.5061 Test MSE 296.80411623631085 Test RE 0.29001745054022937 Lambda1 0.00076184515\n",
      "15 Train Loss 267.33237 Test MSE 290.82519751756905 Test RE 0.2870814866070919 Lambda1 0.0009672057\n",
      "16 Train Loss 260.02158 Test MSE 286.12270794697764 Test RE 0.2847510498597505 Lambda1 0.00025922369\n",
      "17 Train Loss 257.86987 Test MSE 284.1215253424908 Test RE 0.28375350804500493 Lambda1 0.00030433558\n",
      "18 Train Loss 256.9253 Test MSE 283.7825975672074 Test RE 0.28358421315785176 Lambda1 0.00029846578\n",
      "19 Train Loss 256.27072 Test MSE 283.83747243716005 Test RE 0.2836116300867565 Lambda1 0.0002464135\n",
      "20 Train Loss 255.54486 Test MSE 283.5440606623725 Test RE 0.28346500302649824 Lambda1 0.00022371105\n",
      "21 Train Loss 255.2231 Test MSE 283.3375959151682 Test RE 0.28336178064214185 Lambda1 0.00017002333\n",
      "22 Train Loss 255.14238 Test MSE 283.1595134138553 Test RE 0.28327271779465063 Lambda1 0.00017987115\n",
      "23 Train Loss 254.98358 Test MSE 282.851740374337 Test RE 0.28311872789735043 Lambda1 0.00018368178\n",
      "24 Train Loss 254.88739 Test MSE 282.79638307718665 Test RE 0.28309102176686296 Lambda1 0.00013292141\n",
      "25 Train Loss 254.82913 Test MSE 282.8422941133179 Test RE 0.2831140002691432 Lambda1 0.00010135246\n",
      "26 Train Loss 254.80928 Test MSE 282.8589680684335 Test RE 0.28312234513244944 Lambda1 9.188439e-05\n",
      "27 Train Loss 254.73155 Test MSE 282.7715707550378 Test RE 0.2830786024076883 Lambda1 5.9379476e-05\n",
      "28 Train Loss 254.63828 Test MSE 282.8005775270663 Test RE 0.2830931211690661 Lambda1 5.6868823e-05\n",
      "29 Train Loss 254.6196 Test MSE 282.82017800530593 Test RE 0.2831029313760838 Lambda1 7.5369244e-05\n",
      "30 Train Loss 254.59116 Test MSE 282.8450651468613 Test RE 0.283115387113484 Lambda1 7.325693e-05\n",
      "31 Train Loss 254.5093 Test MSE 282.9301951600027 Test RE 0.28315798959511923 Lambda1 5.355628e-05\n",
      "32 Train Loss 254.4952 Test MSE 282.95085873874405 Test RE 0.28316832951409276 Lambda1 4.836487e-05\n",
      "33 Train Loss 254.46968 Test MSE 282.9612864768815 Test RE 0.28317354734237116 Lambda1 5.2131873e-05\n",
      "34 Train Loss 254.45087 Test MSE 282.9555900073932 Test RE 0.28317069695670344 Lambda1 6.4130036e-05\n",
      "35 Train Loss 254.4237 Test MSE 282.91587420002037 Test RE 0.28315082325935487 Lambda1 4.6169545e-05\n",
      "36 Train Loss 254.40018 Test MSE 282.9005246175438 Test RE 0.2831431419903555 Lambda1 4.5231765e-05\n",
      "37 Train Loss 254.3775 Test MSE 283.07253723938265 Test RE 0.28322920897997295 Lambda1 4.1931162e-05\n",
      "38 Train Loss 254.36765 Test MSE 283.25561673700173 Test RE 0.2833207845887259 Lambda1 3.786088e-05\n",
      "39 Train Loss 254.36482 Test MSE 283.2755143597805 Test RE 0.2833307355142608 Lambda1 3.3953223e-05\n",
      "40 Train Loss 254.3566 Test MSE 283.18440802900517 Test RE 0.2832851698048557 Lambda1 3.2423883e-05\n",
      "41 Train Loss 254.33943 Test MSE 283.00282455762766 Test RE 0.28319433119972526 Lambda1 3.4367735e-05\n",
      "42 Train Loss 254.27138 Test MSE 282.59542644829156 Test RE 0.282990420891441 Lambda1 5.1355873e-05\n",
      "43 Train Loss 254.23778 Test MSE 282.6372097002691 Test RE 0.283011340945198 Lambda1 7.739008e-05\n",
      "44 Train Loss 254.22389 Test MSE 282.53389620162255 Test RE 0.28295961108934997 Lambda1 9.748144e-05\n",
      "45 Train Loss 254.21042 Test MSE 282.4247576578684 Test RE 0.2829049543151023 Lambda1 0.0001015508\n",
      "46 Train Loss 254.1718 Test MSE 282.10472629820055 Test RE 0.2827446211328548 Lambda1 0.00011741989\n",
      "47 Train Loss 254.14548 Test MSE 282.2218869095921 Test RE 0.2828033282208077 Lambda1 0.00014052799\n",
      "48 Train Loss 254.12624 Test MSE 282.3327335732385 Test RE 0.2828588602850901 Lambda1 0.0001241541\n",
      "49 Train Loss 254.08104 Test MSE 282.34838962769743 Test RE 0.28286670279111886 Lambda1 0.000116026495\n",
      "50 Train Loss 254.05992 Test MSE 282.40283020174974 Test RE 0.2828939717326345 Lambda1 0.000115552226\n",
      "51 Train Loss 254.04407 Test MSE 282.626969097839 Test RE 0.28300621381971863 Lambda1 9.461986e-05\n",
      "52 Train Loss 254.01369 Test MSE 283.0061583484844 Test RE 0.2831959992182214 Lambda1 8.4022155e-05\n",
      "53 Train Loss 254.00362 Test MSE 282.9702042236398 Test RE 0.28317800952537703 Lambda1 7.846798e-05\n",
      "54 Train Loss 253.99219 Test MSE 282.93596468785154 Test RE 0.2831608766669236 Lambda1 8.21338e-05\n",
      "55 Train Loss 253.97586 Test MSE 283.0518182897713 Test RE 0.2832188435818517 Lambda1 0.00010144848\n",
      "56 Train Loss 253.95766 Test MSE 283.0223064445449 Test RE 0.28320407856708407 Lambda1 0.0001045835\n",
      "57 Train Loss 253.92049 Test MSE 283.2321302792959 Test RE 0.2833090384146482 Lambda1 0.00010228868\n",
      "58 Train Loss 253.9146 Test MSE 283.29673949232665 Test RE 0.28334134995050136 Lambda1 0.00010005464\n",
      "59 Train Loss 253.901 Test MSE 283.28150564595626 Test RE 0.2833337317254708 Lambda1 9.9595454e-05\n",
      "60 Train Loss 253.8646 Test MSE 283.2860338901409 Test RE 0.2833359962559296 Lambda1 7.337435e-05\n",
      "61 Train Loss 253.82597 Test MSE 283.3698771513657 Test RE 0.28337792217820196 Lambda1 5.758927e-05\n",
      "62 Train Loss 253.77448 Test MSE 283.1878436402351 Test RE 0.28328688821648285 Lambda1 4.8838956e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 Train Loss 253.73494 Test MSE 283.1011634886053 Test RE 0.28324352966440586 Lambda1 4.2331627e-05\n",
      "64 Train Loss 253.69221 Test MSE 283.04998755044153 Test RE 0.28321792767055776 Lambda1 5.1255254e-05\n",
      "65 Train Loss 253.66835 Test MSE 282.8949612334769 Test RE 0.28314035789903674 Lambda1 4.4628272e-05\n",
      "66 Train Loss 253.62424 Test MSE 282.59944376378377 Test RE 0.28299243234961624 Lambda1 5.1162362e-05\n",
      "67 Train Loss 253.55959 Test MSE 282.3161350468952 Test RE 0.28285054543404076 Lambda1 4.9761842e-05\n",
      "68 Train Loss 253.4567 Test MSE 282.38538277900466 Test RE 0.28288523271441857 Lambda1 2.7559394e-05\n",
      "69 Train Loss 253.346 Test MSE 282.46379503308464 Test RE 0.2829245055140538 Lambda1 4.8461927e-05\n",
      "70 Train Loss 253.17505 Test MSE 282.87600028784317 Test RE 0.2831308690434033 Lambda1 8.494282e-05\n",
      "71 Train Loss 253.11627 Test MSE 283.373357518706 Test RE 0.2833796624059335 Lambda1 5.9389076e-05\n",
      "72 Train Loss 252.95511 Test MSE 283.7097624211368 Test RE 0.2835478187079307 Lambda1 6.0339724e-05\n",
      "73 Train Loss 252.64803 Test MSE 284.21116138751574 Test RE 0.2837982644858544 Lambda1 4.3357475e-05\n",
      "74 Train Loss 252.24509 Test MSE 284.2178556152562 Test RE 0.283801606717417 Lambda1 5.7501857e-05\n",
      "Training time: 123.34\n",
      "Training time: 123.34\n",
      "inv_HT_stan_tune9\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 837.97394 Test MSE 858.152168323197 Test RE 0.4931412398343645 Lambda1 0.028783558\n",
      "1 Train Loss 837.8219 Test MSE 857.5597124065429 Test RE 0.4929709816330689 Lambda1 0.013884544\n",
      "2 Train Loss 837.10406 Test MSE 856.7283011148676 Test RE 0.49273195394559616 Lambda1 -0.029579611\n",
      "3 Train Loss 804.97925 Test MSE 804.5815281211248 Test RE 0.47750090547105906 Lambda1 0.014003354\n",
      "4 Train Loss 733.31665 Test MSE 732.0337687394787 Test RE 0.4554647052716434 Lambda1 -0.012769551\n",
      "5 Train Loss 622.2518 Test MSE 623.5702976121573 Test RE 0.42037016075309136 Lambda1 -0.0013345398\n",
      "6 Train Loss 522.93976 Test MSE 501.1748537967642 Test RE 0.3768632653848201 Lambda1 0.00045848373\n",
      "7 Train Loss 397.2927 Test MSE 372.1992330415227 Test RE 0.32477074950410273 Lambda1 0.00035150233\n",
      "8 Train Loss 303.1735 Test MSE 306.9104795738631 Test RE 0.2949137554769199 Lambda1 0.00020830674\n",
      "9 Train Loss 276.45505 Test MSE 295.3519844539971 Test RE 0.2893071168289549 Lambda1 -0.00010202105\n",
      "10 Train Loss 269.2463 Test MSE 290.9000464837976 Test RE 0.2871184269587097 Lambda1 0.00037244594\n",
      "11 Train Loss 261.0822 Test MSE 285.09305589779734 Test RE 0.28423823010663674 Lambda1 0.0006200266\n",
      "12 Train Loss 257.49533 Test MSE 282.5775113098613 Test RE 0.28298145065971925 Lambda1 0.0009590097\n",
      "13 Train Loss 256.45862 Test MSE 283.14095454150083 Test RE 0.2832634344965322 Lambda1 0.00083387597\n",
      "14 Train Loss 256.05493 Test MSE 282.87444522325296 Test RE 0.28313009080948887 Lambda1 0.0006913032\n",
      "15 Train Loss 255.59601 Test MSE 282.42403421862485 Test RE 0.28290459198022966 Lambda1 0.000762765\n",
      "16 Train Loss 255.37874 Test MSE 282.07352271033665 Test RE 0.2827289835171263 Lambda1 0.0008182065\n",
      "17 Train Loss 255.09254 Test MSE 281.7424915357815 Test RE 0.28256303461524385 Lambda1 0.00083543995\n",
      "18 Train Loss 254.75314 Test MSE 281.97195842431285 Test RE 0.28267807878782514 Lambda1 0.00078889215\n",
      "19 Train Loss 254.58197 Test MSE 281.99961530223044 Test RE 0.2826919415163452 Lambda1 0.00095996703\n",
      "20 Train Loss 254.05722 Test MSE 281.0639325163595 Test RE 0.2822225618670412 Lambda1 0.0019623744\n",
      "21 Train Loss 253.77132 Test MSE 280.4744093915775 Test RE 0.28192642984607735 Lambda1 0.0030620005\n",
      "22 Train Loss 253.35051 Test MSE 279.89043578818524 Test RE 0.2816327784921391 Lambda1 0.0040301606\n",
      "23 Train Loss 252.87555 Test MSE 278.65210428049727 Test RE 0.28100906773428647 Lambda1 0.005724625\n",
      "24 Train Loss 252.43915 Test MSE 277.89698506805684 Test RE 0.2806280562601462 Lambda1 0.006727321\n",
      "25 Train Loss 251.82372 Test MSE 277.08641762348185 Test RE 0.2802184906834252 Lambda1 0.008731471\n",
      "26 Train Loss 251.23367 Test MSE 275.7166720138257 Test RE 0.27952501826521126 Lambda1 0.010440807\n",
      "27 Train Loss 249.9721 Test MSE 274.23302217803644 Test RE 0.27877193239657244 Lambda1 0.01465068\n",
      "28 Train Loss 248.96632 Test MSE 272.07277332584715 Test RE 0.277671759603393 Lambda1 0.020552626\n",
      "29 Train Loss 247.53279 Test MSE 269.5766305986031 Test RE 0.2763950691666414 Lambda1 0.023617646\n",
      "30 Train Loss 244.78685 Test MSE 264.5077607768341 Test RE 0.2737841991083421 Lambda1 0.036432106\n",
      "31 Train Loss 242.64687 Test MSE 261.19228621178496 Test RE 0.27206291315480485 Lambda1 0.047239766\n",
      "32 Train Loss 238.36583 Test MSE 257.2788361561324 Test RE 0.2700170585060421 Lambda1 0.059947915\n",
      "33 Train Loss 233.65955 Test MSE 252.00131146297943 Test RE 0.2672332977819798 Lambda1 0.07741414\n",
      "34 Train Loss 228.13406 Test MSE 246.28373274763916 Test RE 0.2641843179823677 Lambda1 0.10036499\n",
      "35 Train Loss 221.40083 Test MSE 241.09511513547562 Test RE 0.2613866340235162 Lambda1 0.11998014\n",
      "36 Train Loss 217.85287 Test MSE 238.08537988535775 Test RE 0.25974998693083784 Lambda1 0.13742639\n",
      "37 Train Loss 215.31273 Test MSE 234.4602587979888 Test RE 0.2577649069678861 Lambda1 0.15875241\n",
      "38 Train Loss 211.21906 Test MSE 229.03027399877882 Test RE 0.2547625674401363 Lambda1 0.18992795\n",
      "39 Train Loss 206.14198 Test MSE 220.25785567928227 Test RE 0.24983591718663795 Lambda1 0.2197484\n",
      "40 Train Loss 192.50723 Test MSE 197.3643266763117 Test RE 0.23649583643695776 Lambda1 0.2819479\n",
      "41 Train Loss 174.72035 Test MSE 177.6848615587086 Test RE 0.2243956247146803 Lambda1 0.32901216\n",
      "42 Train Loss 166.04042 Test MSE 164.58941445262963 Test RE 0.21596835519112983 Lambda1 0.35956544\n",
      "43 Train Loss 160.0881 Test MSE 159.5145369624809 Test RE 0.21261274979833641 Lambda1 0.38106763\n",
      "44 Train Loss 151.66986 Test MSE 149.63625570655873 Test RE 0.2059242945514708 Lambda1 0.4223612\n",
      "45 Train Loss 143.99556 Test MSE 139.68486164349207 Test RE 0.1989591155304208 Lambda1 0.46175894\n",
      "46 Train Loss 138.98447 Test MSE 136.96833072246284 Test RE 0.19701498143422652 Lambda1 0.4820899\n",
      "47 Train Loss 135.50801 Test MSE 132.57393455482062 Test RE 0.19382877160080114 Lambda1 0.5099255\n",
      "48 Train Loss 132.37766 Test MSE 126.80597479659058 Test RE 0.18956538144944174 Lambda1 0.5340403\n",
      "49 Train Loss 127.08053 Test MSE 123.64159981370611 Test RE 0.18718518738998943 Lambda1 0.56734043\n",
      "50 Train Loss 122.24293 Test MSE 121.05419224188489 Test RE 0.18521625031389738 Lambda1 0.58069736\n",
      "51 Train Loss 118.01192 Test MSE 116.96563012191882 Test RE 0.18206157817771984 Lambda1 0.5908803\n",
      "52 Train Loss 114.58154 Test MSE 113.86295913872388 Test RE 0.17963063444290792 Lambda1 0.61909103\n",
      "53 Train Loss 111.91359 Test MSE 108.37555918574478 Test RE 0.17524871664577865 Lambda1 0.64362335\n",
      "54 Train Loss 108.14397 Test MSE 103.70760445563442 Test RE 0.17143301846162712 Lambda1 0.660014\n",
      "55 Train Loss 104.325584 Test MSE 100.80364348034824 Test RE 0.1690157922458554 Lambda1 0.67506\n",
      "56 Train Loss 100.41279 Test MSE 100.49671928005934 Test RE 0.16875828873589155 Lambda1 0.67181885\n",
      "57 Train Loss 96.99164 Test MSE 94.5861811600635 Test RE 0.1637204826949863 Lambda1 0.69776624\n",
      "58 Train Loss 92.29004 Test MSE 88.08939624957796 Test RE 0.15799778092370279 Lambda1 0.7578094\n",
      "59 Train Loss 90.72522 Test MSE 88.65224331489748 Test RE 0.15850174048551333 Lambda1 0.77421767\n",
      "60 Train Loss 88.94866 Test MSE 86.85157539287296 Test RE 0.15688377135901513 Lambda1 0.79041654\n",
      "61 Train Loss 86.34822 Test MSE 82.22064803321851 Test RE 0.15264395790825808 Lambda1 0.8313945\n",
      "62 Train Loss 85.39046 Test MSE 80.87959454885754 Test RE 0.1513939964029368 Lambda1 0.85186946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 Train Loss 81.57213 Test MSE 77.98008307613308 Test RE 0.14865551211801759 Lambda1 0.88430834\n",
      "64 Train Loss 77.967285 Test MSE 75.31263682659683 Test RE 0.1460908773574467 Lambda1 0.90727866\n",
      "65 Train Loss 76.72206 Test MSE 73.90902260398073 Test RE 0.14472311450728645 Lambda1 0.9253748\n",
      "66 Train Loss 74.684 Test MSE 72.83001457170941 Test RE 0.14366281403896927 Lambda1 0.9286578\n",
      "67 Train Loss 72.12526 Test MSE 72.66857177520713 Test RE 0.14350349652368585 Lambda1 0.92977685\n",
      "68 Train Loss 70.05061 Test MSE 69.41161724447993 Test RE 0.1402507693524234 Lambda1 0.9556046\n",
      "69 Train Loss 67.317924 Test MSE 64.886012538322 Test RE 0.13560156870800114 Lambda1 0.9910606\n",
      "70 Train Loss 64.505196 Test MSE 63.21974679538023 Test RE 0.13384912814996594 Lambda1 1.0131861\n",
      "71 Train Loss 63.638718 Test MSE 62.56554961319431 Test RE 0.1331547926480306 Lambda1 1.0270373\n",
      "72 Train Loss 63.102295 Test MSE 60.939963057552184 Test RE 0.13141358524301275 Lambda1 1.044605\n",
      "73 Train Loss 61.41677 Test MSE 58.058147654941095 Test RE 0.1282687192599546 Lambda1 1.0760932\n",
      "74 Train Loss 60.293762 Test MSE 57.94994799341457 Test RE 0.12814913996914445 Lambda1 1.0842578\n",
      "Training time: 123.59\n",
      "Training time: 123.59\n",
      "inv_HT_stan_tune9\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 838.12946 Test MSE 858.6780924990751 Test RE 0.49329232910795623 Lambda1 0.0052949223\n",
      "1 Train Loss 837.992 Test MSE 858.1718568822178 Test RE 0.49314689686412877 Lambda1 0.0052108117\n",
      "2 Train Loss 837.8772 Test MSE 857.8664910064481 Test RE 0.49305915008505175 Lambda1 0.0035543737\n",
      "3 Train Loss 837.85895 Test MSE 857.8816541178402 Test RE 0.4930635075683301 Lambda1 0.0008409632\n",
      "4 Train Loss 837.743 Test MSE 857.7209575822501 Test RE 0.4930173256107572 Lambda1 -0.04049907\n",
      "5 Train Loss 837.68256 Test MSE 857.329232619647 Test RE 0.492904731154904 Lambda1 -0.057028376\n",
      "6 Train Loss 828.23987 Test MSE 845.309947638848 Test RE 0.4894374089633385 Lambda1 0.0030242912\n",
      "7 Train Loss 777.84753 Test MSE 779.7558129742009 Test RE 0.47007643572247176 Lambda1 0.09612997\n",
      "8 Train Loss 701.2511 Test MSE 696.4957296432406 Test RE 0.44427144423000003 Lambda1 0.0010197293\n",
      "9 Train Loss 654.4631 Test MSE 660.3266179349436 Test RE 0.43258212797750456 Lambda1 0.00075346045\n",
      "10 Train Loss 646.3326 Test MSE 653.377693635548 Test RE 0.4302999765400181 Lambda1 -0.00013189395\n",
      "11 Train Loss 643.704 Test MSE 651.3209677281817 Test RE 0.42962218582067635 Lambda1 0.00022045698\n",
      "12 Train Loss 636.59534 Test MSE 645.1126449567903 Test RE 0.42756972647116387 Lambda1 -6.022795e-05\n",
      "13 Train Loss 623.51355 Test MSE 626.9017073682646 Test RE 0.4214915738519948 Lambda1 0.0004075944\n",
      "14 Train Loss 547.1427 Test MSE 529.5278014078973 Test RE 0.38737675238052166 Lambda1 -0.0010155814\n",
      "15 Train Loss 461.18896 Test MSE 467.8856581021513 Test RE 0.3641321598946052 Lambda1 -0.0022441875\n",
      "16 Train Loss 392.9956 Test MSE 398.04210435951103 Test RE 0.33585643781705704 Lambda1 0.0017951756\n",
      "17 Train Loss 295.0288 Test MSE 301.6486134464939 Test RE 0.2923747324008878 Lambda1 0.0007818387\n",
      "18 Train Loss 273.7326 Test MSE 295.34198184532823 Test RE 0.2893022178431201 Lambda1 0.00040876205\n",
      "19 Train Loss 260.77426 Test MSE 288.31026887635926 Test RE 0.2858375140993962 Lambda1 0.00049174303\n",
      "20 Train Loss 257.36606 Test MSE 283.95102488693493 Test RE 0.2836683554633846 Lambda1 0.00030251624\n",
      "21 Train Loss 256.29593 Test MSE 282.3743366251005 Test RE 0.2828796998069678 Lambda1 8.331141e-05\n",
      "22 Train Loss 255.49113 Test MSE 281.44005582728164 Test RE 0.28241133563498444 Lambda1 3.7400292e-05\n",
      "23 Train Loss 254.89322 Test MSE 281.9664562769754 Test RE 0.2826753208114032 Lambda1 0.0001042564\n",
      "24 Train Loss 254.55127 Test MSE 282.8828985511257 Test RE 0.2831343212616349 Lambda1 0.00019685192\n",
      "25 Train Loss 254.19699 Test MSE 283.2148437807201 Test RE 0.2833003926864738 Lambda1 0.00044312823\n",
      "26 Train Loss 254.12505 Test MSE 283.1380477463766 Test RE 0.2832619804665334 Lambda1 0.0005132575\n",
      "27 Train Loss 254.01729 Test MSE 283.22313272798635 Test RE 0.28330453838166847 Lambda1 0.0005504734\n",
      "28 Train Loss 253.88423 Test MSE 283.33819460237834 Test RE 0.2833620800111397 Lambda1 0.0005462298\n",
      "29 Train Loss 253.7623 Test MSE 283.5547271868655 Test RE 0.28347033475158434 Lambda1 0.00046984057\n",
      "30 Train Loss 253.62724 Test MSE 283.2964330407675 Test RE 0.2833411967005521 Lambda1 0.00039390972\n",
      "31 Train Loss 253.54109 Test MSE 283.1449173813483 Test RE 0.28326541676663447 Lambda1 0.0005293778\n",
      "32 Train Loss 253.36525 Test MSE 283.0888628599821 Test RE 0.2832373761903763 Lambda1 0.00067097525\n",
      "33 Train Loss 253.3035 Test MSE 282.75871500853793 Test RE 0.2830721674819536 Lambda1 0.0006922126\n",
      "34 Train Loss 253.09483 Test MSE 282.9852469510718 Test RE 0.28318553631253957 Lambda1 0.0006318585\n",
      "35 Train Loss 252.84296 Test MSE 283.4146850262169 Test RE 0.2834003258660766 Lambda1 0.00045637236\n",
      "36 Train Loss 252.78838 Test MSE 283.2676527254779 Test RE 0.2833268039035656 Lambda1 0.00043006465\n",
      "37 Train Loss 252.6093 Test MSE 282.898199044071 Test RE 0.2831419782040138 Lambda1 0.00036449436\n",
      "38 Train Loss 252.49011 Test MSE 283.0693043160804 Test RE 0.28322759161904326 Lambda1 0.0003759468\n",
      "39 Train Loss 252.41241 Test MSE 282.8362138754619 Test RE 0.28311095721343843 Lambda1 0.00036241944\n",
      "40 Train Loss 252.33897 Test MSE 282.4967629438395 Test RE 0.2829410158737248 Lambda1 0.00027592448\n",
      "41 Train Loss 252.2083 Test MSE 283.00904426422625 Test RE 0.2831974431403491 Lambda1 0.00021771244\n",
      "42 Train Loss 252.15157 Test MSE 283.3815277322418 Test RE 0.28338374757414697 Lambda1 0.00022567366\n",
      "43 Train Loss 252.00365 Test MSE 283.69739532589443 Test RE 0.2835416386225768 Lambda1 0.0001079903\n",
      "44 Train Loss 251.80655 Test MSE 283.70155288440014 Test RE 0.28354371625291797 Lambda1 5.0858445e-05\n",
      "45 Train Loss 251.48914 Test MSE 284.4134765301079 Test RE 0.2838992571275819 Lambda1 3.127904e-05\n",
      "46 Train Loss 251.30153 Test MSE 285.283417909977 Test RE 0.28433310988546073 Lambda1 3.0944702e-05\n",
      "47 Train Loss 251.04863 Test MSE 285.7779079790506 Test RE 0.284579424631768 Lambda1 1.7230986e-05\n",
      "48 Train Loss 250.76068 Test MSE 286.0041901168911 Test RE 0.2846920689206022 Lambda1 1.4136906e-05\n",
      "49 Train Loss 250.48473 Test MSE 286.35567232650595 Test RE 0.28486695005265233 Lambda1 8.407219e-07\n",
      "50 Train Loss 250.28511 Test MSE 286.69026571366675 Test RE 0.2850333284074297 Lambda1 -3.2516464e-06\n",
      "51 Train Loss 249.98007 Test MSE 287.72551226229734 Test RE 0.28554749630794085 Lambda1 6.3420275e-06\n",
      "52 Train Loss 249.91934 Test MSE 287.94127116956247 Test RE 0.2856545390754385 Lambda1 5.2825594e-06\n",
      "53 Train Loss 249.70847 Test MSE 288.21342490355624 Test RE 0.2857895033824714 Lambda1 2.9781063e-06\n",
      "54 Train Loss 249.31744 Test MSE 288.5089144796513 Test RE 0.28593596808199534 Lambda1 1.0399724e-05\n",
      "55 Train Loss 249.08875 Test MSE 288.4074130353524 Test RE 0.28588566553431694 Lambda1 1.0118039e-05\n",
      "56 Train Loss 249.03096 Test MSE 288.3941825762435 Test RE 0.28587910807113065 Lambda1 1.2670356e-05\n",
      "57 Train Loss 248.94719 Test MSE 288.3977580475507 Test RE 0.2858808802105181 Lambda1 1.3784173e-05\n",
      "58 Train Loss 248.86574 Test MSE 288.28410014740916 Test RE 0.2858245416614055 Lambda1 1.2715131e-05\n",
      "59 Train Loss 248.8138 Test MSE 287.9219675668862 Test RE 0.28564496376495396 Lambda1 1.181399e-05\n",
      "60 Train Loss 248.7985 Test MSE 287.7465806644809 Test RE 0.2855579505761809 Lambda1 1.1764739e-05\n",
      "61 Train Loss 248.7856 Test MSE 287.7592386444589 Test RE 0.28556423135814896 Lambda1 1.238098e-05\n",
      "62 Train Loss 248.7597 Test MSE 287.9452977186329 Test RE 0.2856565363542465 Lambda1 1.0943177e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 Train Loss 248.72284 Test MSE 288.152559188599 Test RE 0.28575932487686734 Lambda1 1.0709559e-05\n",
      "64 Train Loss 248.63739 Test MSE 288.73998561912197 Test RE 0.28605045037788823 Lambda1 9.153524e-06\n",
      "65 Train Loss 248.49574 Test MSE 289.2159820393567 Test RE 0.286286134605635 Lambda1 7.573229e-06\n",
      "66 Train Loss 248.4598 Test MSE 289.15845376586356 Test RE 0.28625766044247125 Lambda1 8.922061e-06\n",
      "67 Train Loss 248.39891 Test MSE 289.3670617619028 Test RE 0.28636089946214566 Lambda1 8.617717e-06\n",
      "68 Train Loss 248.33064 Test MSE 289.91000067489693 Test RE 0.2866294227916687 Lambda1 6.990693e-06\n",
      "69 Train Loss 248.22493 Test MSE 290.36956164659534 Test RE 0.2868565131567428 Lambda1 6.2547388e-06\n",
      "70 Train Loss 248.16142 Test MSE 290.34062123171435 Test RE 0.28684221766151446 Lambda1 7.459673e-06\n",
      "71 Train Loss 248.14519 Test MSE 290.16773068971787 Test RE 0.2867568012784908 Lambda1 5.759029e-06\n",
      "72 Train Loss 248.10672 Test MSE 289.89241465397316 Test RE 0.2866207291497466 Lambda1 5.414758e-06\n",
      "73 Train Loss 248.04927 Test MSE 290.11384156213586 Test RE 0.2867301722123395 Lambda1 6.023237e-06\n",
      "74 Train Loss 247.98503 Test MSE 290.19553010863325 Test RE 0.2867705372675082 Lambda1 4.757178e-06\n",
      "Training time: 123.39\n",
      "Training time: 123.39\n",
      "inv_HT_stan_tune9\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 837.87964 Test MSE 857.9805159964766 Test RE 0.4930919169533246 Lambda1 -0.14961892\n",
      "1 Train Loss 837.5905 Test MSE 856.8358791581327 Test RE 0.4927628887679249 Lambda1 -0.11315149\n",
      "2 Train Loss 826.14966 Test MSE 844.5477385121865 Test RE 0.4892166983270303 Lambda1 0.09990013\n",
      "3 Train Loss 755.997 Test MSE 759.5983890021428 Test RE 0.46396069265429757 Lambda1 0.015472421\n",
      "4 Train Loss 658.89197 Test MSE 664.9380809926555 Test RE 0.4340899922428981 Lambda1 0.0005125429\n",
      "5 Train Loss 652.1929 Test MSE 659.0580696001527 Test RE 0.4321664131425016 Lambda1 -0.000528075\n",
      "6 Train Loss 638.77716 Test MSE 643.2303805683371 Test RE 0.4269455043444521 Lambda1 0.00023623266\n",
      "7 Train Loss 615.1173 Test MSE 615.5971586188936 Test RE 0.41767403120674446 Lambda1 0.0022282905\n",
      "8 Train Loss 488.706 Test MSE 463.87289025610016 Test RE 0.3625673286044556 Lambda1 0.00027292274\n",
      "9 Train Loss 327.69254 Test MSE 336.12578932637047 Test RE 0.3086313852925241 Lambda1 -0.00059522863\n",
      "10 Train Loss 285.64694 Test MSE 300.70037587479527 Test RE 0.29191482820315623 Lambda1 0.0008135162\n",
      "11 Train Loss 268.5555 Test MSE 292.7495041117819 Test RE 0.28802968846696586 Lambda1 0.0011636596\n",
      "12 Train Loss 265.39032 Test MSE 291.52971854814103 Test RE 0.2874290022359673 Lambda1 0.00023871925\n",
      "13 Train Loss 261.4277 Test MSE 286.09285627246936 Test RE 0.2847361951887573 Lambda1 0.00020006763\n",
      "14 Train Loss 260.603 Test MSE 283.64757650749664 Test RE 0.28351674179619074 Lambda1 0.0004896318\n",
      "15 Train Loss 259.463 Test MSE 283.45328700840753 Test RE 0.28341962522215425 Lambda1 0.00028546673\n",
      "16 Train Loss 257.90964 Test MSE 283.7902144277297 Test RE 0.28358801890020996 Lambda1 0.00017883652\n",
      "17 Train Loss 256.46655 Test MSE 282.99250272594946 Test RE 0.28318916674442873 Lambda1 0.00040998653\n",
      "18 Train Loss 255.98445 Test MSE 283.34463322121945 Test RE 0.2833652995736577 Lambda1 0.0004739996\n",
      "19 Train Loss 255.49564 Test MSE 283.47061956491274 Test RE 0.2834282903387728 Lambda1 0.0010018685\n",
      "20 Train Loss 255.24876 Test MSE 282.61888924123656 Test RE 0.2830021684415021 Lambda1 0.0014386595\n",
      "21 Train Loss 254.906 Test MSE 282.0468090186373 Test RE 0.2827155953166742 Lambda1 0.0020044737\n",
      "22 Train Loss 254.57947 Test MSE 281.43960259277776 Test RE 0.2824111082355605 Lambda1 0.0024204014\n",
      "23 Train Loss 254.36351 Test MSE 280.5372339122661 Test RE 0.28195800296024415 Lambda1 0.0029819356\n",
      "24 Train Loss 254.06802 Test MSE 279.87910738313224 Test RE 0.2816270789717794 Lambda1 0.0035972584\n",
      "25 Train Loss 253.64822 Test MSE 279.7872914320171 Test RE 0.2815808804909244 Lambda1 0.004606409\n",
      "26 Train Loss 253.12021 Test MSE 278.94958578288646 Test RE 0.28115902658858544 Lambda1 0.0067864046\n",
      "27 Train Loss 252.38338 Test MSE 277.2121156341188 Test RE 0.2802820429018699 Lambda1 0.008797182\n",
      "28 Train Loss 250.92671 Test MSE 274.25425932342193 Test RE 0.27878272651136354 Lambda1 0.014070278\n",
      "29 Train Loss 247.98369 Test MSE 269.1060656709175 Test RE 0.2761537303067986 Lambda1 0.027212892\n",
      "30 Train Loss 245.45654 Test MSE 264.9688532195293 Test RE 0.2740227268117477 Lambda1 0.034224585\n",
      "31 Train Loss 242.77441 Test MSE 263.0422170340755 Test RE 0.27302467498385313 Lambda1 0.0403947\n",
      "32 Train Loss 240.61879 Test MSE 259.8471153711547 Test RE 0.27136143088921766 Lambda1 0.04924982\n",
      "33 Train Loss 238.85045 Test MSE 257.7721652629059 Test RE 0.2702758117746415 Lambda1 0.059783623\n",
      "34 Train Loss 235.25798 Test MSE 252.4772413903209 Test RE 0.26748552728385605 Lambda1 0.069985844\n",
      "35 Train Loss 230.8253 Test MSE 249.7400462348225 Test RE 0.26603162332536234 Lambda1 0.0731925\n",
      "36 Train Loss 226.2377 Test MSE 242.39208769072522 Test RE 0.2620887564708712 Lambda1 0.09345609\n",
      "37 Train Loss 222.64717 Test MSE 240.0978254306561 Test RE 0.26084546102689216 Lambda1 0.107205056\n",
      "38 Train Loss 217.48633 Test MSE 234.52236705010426 Test RE 0.25779904552048744 Lambda1 0.122746445\n",
      "39 Train Loss 213.54373 Test MSE 233.3564854753612 Test RE 0.2571574486444111 Lambda1 0.14055413\n",
      "40 Train Loss 209.99872 Test MSE 226.94639751693924 Test RE 0.2536009156447825 Lambda1 0.16796021\n",
      "41 Train Loss 207.75671 Test MSE 224.4169803293518 Test RE 0.252183708988358 Lambda1 0.17832084\n",
      "42 Train Loss 204.11807 Test MSE 221.5501521963518 Test RE 0.250567763719097 Lambda1 0.20491543\n",
      "43 Train Loss 198.19193 Test MSE 212.44032986212164 Test RE 0.24536219839220683 Lambda1 0.2454361\n",
      "44 Train Loss 193.48091 Test MSE 203.34441947834847 Test RE 0.24005198402859165 Lambda1 0.27724048\n",
      "45 Train Loss 187.49933 Test MSE 193.35737610783983 Test RE 0.23408282100297728 Lambda1 0.30776578\n",
      "46 Train Loss 180.87181 Test MSE 184.6675748529297 Test RE 0.22876232065898627 Lambda1 0.31470275\n",
      "47 Train Loss 171.48982 Test MSE 173.08231906312875 Test RE 0.22147031501609474 Lambda1 0.32291704\n",
      "48 Train Loss 162.31187 Test MSE 159.72878374768746 Test RE 0.21275548385248566 Lambda1 0.3398803\n",
      "49 Train Loss 156.92587 Test MSE 150.93911512887905 Test RE 0.2068188269097978 Lambda1 0.35423088\n",
      "50 Train Loss 152.55028 Test MSE 147.36955135230787 Test RE 0.20435866235666622 Lambda1 0.36036462\n",
      "51 Train Loss 147.99963 Test MSE 142.1916224578551 Test RE 0.20073642044021273 Lambda1 0.36161757\n",
      "52 Train Loss 143.06445 Test MSE 135.36071627193076 Test RE 0.19585537402176284 Lambda1 0.37060913\n",
      "53 Train Loss 136.78236 Test MSE 130.6741009067983 Test RE 0.19243494096438962 Lambda1 0.38629812\n",
      "54 Train Loss 133.41603 Test MSE 128.05273898460732 Test RE 0.19049501127235544 Lambda1 0.39716926\n",
      "55 Train Loss 126.731155 Test MSE 121.20510008113652 Test RE 0.18533166093347328 Lambda1 0.41570678\n",
      "56 Train Loss 122.66264 Test MSE 120.98895551891182 Test RE 0.1851663365962841 Lambda1 0.42958906\n",
      "57 Train Loss 119.9317 Test MSE 119.25511386659537 Test RE 0.1838347785770426 Lambda1 0.44749674\n",
      "58 Train Loss 116.0042 Test MSE 113.62987846326216 Test RE 0.17944668578064032 Lambda1 0.45053148\n",
      "59 Train Loss 112.896484 Test MSE 109.88945518369866 Test RE 0.17646849454779795 Lambda1 0.46088812\n",
      "60 Train Loss 110.203255 Test MSE 105.76031063726367 Test RE 0.17312130991302657 Lambda1 0.47341222\n",
      "61 Train Loss 107.65703 Test MSE 102.20074521709113 Test RE 0.1701830104792014 Lambda1 0.48239827\n",
      "62 Train Loss 104.08627 Test MSE 101.74486703336181 Test RE 0.16980302581413576 Lambda1 0.4784114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 Train Loss 102.67605 Test MSE 98.78321439187239 Test RE 0.16731340883690632 Lambda1 0.47661772\n",
      "64 Train Loss 101.645744 Test MSE 96.09371472797831 Test RE 0.16502003003411808 Lambda1 0.47615466\n",
      "65 Train Loss 99.546524 Test MSE 93.8614732002908 Test RE 0.16309207334383974 Lambda1 0.47724903\n",
      "66 Train Loss 96.69663 Test MSE 91.62931684932997 Test RE 0.16114112634772249 Lambda1 0.48584047\n",
      "67 Train Loss 93.89928 Test MSE 89.9739748210499 Test RE 0.15967893379683895 Lambda1 0.49990347\n",
      "68 Train Loss 92.30533 Test MSE 88.41597603211137 Test RE 0.15829038790915828 Lambda1 0.5062987\n",
      "69 Train Loss 90.319984 Test MSE 87.50920497357731 Test RE 0.15747660369327404 Lambda1 0.5131262\n",
      "70 Train Loss 88.89341 Test MSE 85.93543270010248 Test RE 0.1560541435929527 Lambda1 0.51837844\n",
      "71 Train Loss 87.74931 Test MSE 84.25955018019889 Test RE 0.15452499520570198 Lambda1 0.52271414\n",
      "72 Train Loss 86.31668 Test MSE 83.65835839319871 Test RE 0.15397274053269994 Lambda1 0.5273471\n",
      "73 Train Loss 84.695595 Test MSE 82.32299124812572 Test RE 0.1527389292775905 Lambda1 0.52657443\n",
      "74 Train Loss 82.04926 Test MSE 80.83598869039591 Test RE 0.15135317921492938 Lambda1 0.52159214\n",
      "Training time: 122.87\n",
      "Training time: 122.87\n",
      "inv_HT_stan_tune9\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 839.30145 Test MSE 858.4763770745617 Test RE 0.49323438509093226 Lambda1 0.0065360353\n",
      "1 Train Loss 838.1044 Test MSE 858.2910470437188 Test RE 0.4931811418798828 Lambda1 0.0066239885\n",
      "2 Train Loss 838.0253 Test MSE 858.3934277617559 Test RE 0.49321055540688974 Lambda1 0.006706376\n",
      "3 Train Loss 837.9215 Test MSE 858.0042135730677 Test RE 0.49309872655014275 Lambda1 0.0033878752\n",
      "4 Train Loss 837.9168 Test MSE 857.9226680151038 Test RE 0.4930752937012145 Lambda1 -0.0009528694\n",
      "5 Train Loss 837.8592 Test MSE 857.7059020780868 Test RE 0.49301299864584786 Lambda1 -0.05969772\n",
      "6 Train Loss 837.7646 Test MSE 857.7854882235584 Test RE 0.49303587134264343 Lambda1 -0.12562093\n",
      "7 Train Loss 836.58044 Test MSE 854.8305822250616 Test RE 0.4921859318748766 Lambda1 -0.10669501\n",
      "8 Train Loss 802.66125 Test MSE 799.320461652707 Test RE 0.4759371831605479 Lambda1 0.27522442\n",
      "9 Train Loss 722.9292 Test MSE 687.2459975147259 Test RE 0.4413115360722749 Lambda1 -0.0033968717\n",
      "10 Train Loss 483.2623 Test MSE 471.5201480478119 Test RE 0.36554369549875765 Lambda1 0.002508221\n",
      "11 Train Loss 369.05692 Test MSE 362.6868233594872 Test RE 0.32059375588599626 Lambda1 0.0010609737\n",
      "12 Train Loss 322.35138 Test MSE 321.8566613027882 Test RE 0.30200937293102004 Lambda1 0.0007852757\n",
      "13 Train Loss 288.42645 Test MSE 308.8963650054844 Test RE 0.295866346909804 Lambda1 0.0013210969\n",
      "14 Train Loss 276.7895 Test MSE 298.3248053107585 Test RE 0.29075945997057817 Lambda1 0.0030423787\n",
      "15 Train Loss 270.7652 Test MSE 291.09613034804954 Test RE 0.28721517807179525 Lambda1 0.0038355747\n",
      "16 Train Loss 266.11026 Test MSE 288.51068107949766 Test RE 0.2859368435032365 Lambda1 0.0018144636\n",
      "17 Train Loss 261.81818 Test MSE 284.50270713740133 Test RE 0.28394378827430855 Lambda1 0.00068786956\n",
      "18 Train Loss 259.60983 Test MSE 281.90632669845496 Test RE 0.2826451788321345 Lambda1 0.00093256694\n",
      "19 Train Loss 257.8076 Test MSE 280.7919313689819 Test RE 0.2820859676029691 Lambda1 5.6084427e-05\n",
      "20 Train Loss 257.55344 Test MSE 281.0354696928515 Test RE 0.28220827142736304 Lambda1 -7.988054e-05\n",
      "21 Train Loss 257.27744 Test MSE 281.0770864366963 Test RE 0.2822291658621342 Lambda1 2.1098404e-05\n",
      "22 Train Loss 257.04913 Test MSE 280.92136200054233 Test RE 0.2821509736705573 Lambda1 -3.8984675e-05\n",
      "23 Train Loss 256.6914 Test MSE 280.86831432914533 Test RE 0.2821243324804945 Lambda1 -0.00011406881\n",
      "24 Train Loss 255.78752 Test MSE 281.35018723084255 Test RE 0.28236624266365845 Lambda1 -2.0051732e-05\n",
      "25 Train Loss 255.4439 Test MSE 281.3699127581743 Test RE 0.2823761408717778 Lambda1 0.00019404588\n",
      "26 Train Loss 255.09618 Test MSE 282.041281448488 Test RE 0.2827128249645693 Lambda1 0.00036344884\n",
      "27 Train Loss 254.94055 Test MSE 281.97918384237516 Test RE 0.28268170052068026 Lambda1 0.00032355095\n",
      "28 Train Loss 254.69028 Test MSE 282.22493631903393 Test RE 0.2828048560626403 Lambda1 0.0003838363\n",
      "29 Train Loss 254.47327 Test MSE 282.571948219355 Test RE 0.2829786651245887 Lambda1 0.0003806624\n",
      "30 Train Loss 254.38927 Test MSE 282.48422184459935 Test RE 0.2829347353933669 Lambda1 0.00038797598\n",
      "31 Train Loss 254.36168 Test MSE 282.58117456189217 Test RE 0.2829832848980658 Lambda1 0.00040215917\n",
      "32 Train Loss 254.3342 Test MSE 282.7406621464254 Test RE 0.28306313090039903 Lambda1 0.0004390441\n",
      "33 Train Loss 254.30574 Test MSE 282.7177004847819 Test RE 0.2830516367421989 Lambda1 0.0004984806\n",
      "34 Train Loss 254.29083 Test MSE 282.57348662495184 Test RE 0.2829794354334825 Lambda1 0.00054954365\n",
      "35 Train Loss 254.26596 Test MSE 282.43057249190474 Test RE 0.2829078666605066 Lambda1 0.00057857746\n",
      "36 Train Loss 254.24738 Test MSE 282.28391734042737 Test RE 0.2828344056270709 Lambda1 0.0006304695\n",
      "37 Train Loss 254.21182 Test MSE 282.5172634837846 Test RE 0.28295128207696935 Lambda1 0.0006504291\n",
      "38 Train Loss 254.19499 Test MSE 282.6815569351513 Test RE 0.28303354304332756 Lambda1 0.0006400612\n",
      "39 Train Loss 254.16205 Test MSE 282.7236541905031 Test RE 0.2830546170955233 Lambda1 0.0006964036\n",
      "40 Train Loss 254.07515 Test MSE 282.5769419671537 Test RE 0.2829811655812895 Lambda1 0.0008425248\n",
      "41 Train Loss 254.06139 Test MSE 282.5510822055017 Test RE 0.28296821690813934 Lambda1 0.00086546224\n",
      "42 Train Loss 254.05403 Test MSE 282.425449658107 Test RE 0.28290530090330096 Lambda1 0.000900912\n",
      "43 Train Loss 254.0191 Test MSE 282.33415328267813 Test RE 0.2828595714617293 Lambda1 0.0009871102\n",
      "44 Train Loss 253.97705 Test MSE 282.42918374368355 Test RE 0.28290717111208197 Lambda1 0.0010687418\n",
      "45 Train Loss 253.93692 Test MSE 282.04068795321206 Test RE 0.2827125275102153 Lambda1 0.0013350776\n",
      "46 Train Loss 253.84795 Test MSE 281.34496242061766 Test RE 0.28236362081205113 Lambda1 0.0017792318\n",
      "47 Train Loss 253.68376 Test MSE 280.5538206041104 Test RE 0.281966338184266 Lambda1 0.0025092787\n",
      "48 Train Loss 253.56355 Test MSE 280.1300131208004 Test RE 0.28175328707111924 Lambda1 0.0027731268\n",
      "49 Train Loss 253.52104 Test MSE 280.1923600653468 Test RE 0.28178463944043747 Lambda1 0.0026329213\n",
      "50 Train Loss 253.46172 Test MSE 279.9011447002494 Test RE 0.2816381662285985 Lambda1 0.0029469845\n",
      "51 Train Loss 252.95865 Test MSE 278.190179977179 Test RE 0.28077605539524897 Lambda1 0.0066538253\n",
      "52 Train Loss 251.18657 Test MSE 273.76862596044566 Test RE 0.278535791094654 Lambda1 0.01324765\n",
      "53 Train Loss 249.97484 Test MSE 271.57532467799496 Test RE 0.27741780064538835 Lambda1 0.01872487\n",
      "54 Train Loss 248.05942 Test MSE 269.11127936646955 Test RE 0.27615640541279857 Lambda1 0.026920006\n",
      "55 Train Loss 244.99706 Test MSE 263.50099405042334 Test RE 0.27326266506541397 Lambda1 0.04223932\n",
      "56 Train Loss 240.849 Test MSE 257.3841295774159 Test RE 0.2700723061743098 Lambda1 0.05555726\n",
      "57 Train Loss 234.13594 Test MSE 251.7075683633273 Test RE 0.2670775033037423 Lambda1 0.08525249\n",
      "58 Train Loss 229.1851 Test MSE 243.53463361432114 Test RE 0.2627057246242323 Lambda1 0.10759757\n",
      "59 Train Loss 223.14532 Test MSE 238.5611908769105 Test RE 0.2600094111156884 Lambda1 0.13016248\n",
      "60 Train Loss 219.57497 Test MSE 238.87001891613406 Test RE 0.26017765353964006 Lambda1 0.14328049\n",
      "61 Train Loss 215.2102 Test MSE 235.0793729331954 Test RE 0.2581050087121911 Lambda1 0.15439124\n",
      "62 Train Loss 212.61656 Test MSE 233.36190378850512 Test RE 0.25716043410138945 Lambda1 0.1798766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 Train Loss 208.31013 Test MSE 226.5599651209371 Test RE 0.2533849145025872 Lambda1 0.2097642\n",
      "64 Train Loss 205.21704 Test MSE 220.0566157896045 Test RE 0.24972175907530783 Lambda1 0.23081143\n",
      "65 Train Loss 199.8204 Test MSE 207.82405592240485 Test RE 0.2426817280792113 Lambda1 0.24963884\n",
      "66 Train Loss 190.46414 Test MSE 198.7201679964375 Test RE 0.23730677836127298 Lambda1 0.26559028\n",
      "67 Train Loss 182.88623 Test MSE 186.24627336536284 Test RE 0.2297380690232171 Lambda1 0.27945772\n",
      "68 Train Loss 177.91661 Test MSE 183.21943942024762 Test RE 0.22786359536512105 Lambda1 0.287454\n",
      "69 Train Loss 170.59854 Test MSE 177.28571081034517 Test RE 0.22414344217512788 Lambda1 0.31558508\n",
      "70 Train Loss 164.43457 Test MSE 166.54192583502314 Test RE 0.21724558630956445 Lambda1 0.34152392\n",
      "71 Train Loss 155.55026 Test MSE 154.03255574004152 Test RE 0.2089274152059444 Lambda1 0.3835809\n",
      "72 Train Loss 147.99915 Test MSE 146.17236710965028 Test RE 0.20352689666332638 Lambda1 0.40588486\n",
      "73 Train Loss 141.2041 Test MSE 135.1561111466359 Test RE 0.19570729498879466 Lambda1 0.42309752\n",
      "74 Train Loss 137.29411 Test MSE 129.28405755943297 Test RE 0.1914086929196283 Lambda1 0.4402593\n",
      "Training time: 123.83\n",
      "Training time: 123.83\n",
      "inv_HT_stan_tune9\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 837.87946 Test MSE 857.9678490822831 Test RE 0.493088277024481 Lambda1 -0.0150214005\n",
      "1 Train Loss 836.577 Test MSE 853.599829258804 Test RE 0.49183148884097105 Lambda1 -0.06361353\n",
      "2 Train Loss 809.9883 Test MSE 811.8448790002606 Test RE 0.4796513801099437 Lambda1 -0.071178205\n",
      "3 Train Loss 757.4099 Test MSE 747.276548771602 Test RE 0.46018223315765183 Lambda1 -0.025859153\n",
      "4 Train Loss 695.00616 Test MSE 692.8458444917733 Test RE 0.4431058450568448 Lambda1 -0.00014183899\n",
      "5 Train Loss 670.8986 Test MSE 672.1353330121212 Test RE 0.43643295200046245 Lambda1 0.00035263426\n",
      "6 Train Loss 662.50824 Test MSE 665.9905668313014 Test RE 0.43443340235648786 Lambda1 0.00035237358\n",
      "7 Train Loss 648.89276 Test MSE 655.5362442218425 Test RE 0.43101017709155226 Lambda1 -0.00013749528\n",
      "8 Train Loss 642.9902 Test MSE 649.5475443168573 Test RE 0.42903689729468314 Lambda1 -0.0004744909\n",
      "9 Train Loss 626.58716 Test MSE 631.3918066490127 Test RE 0.42299831909450725 Lambda1 0.0008756268\n",
      "10 Train Loss 590.8993 Test MSE 584.282444258887 Test RE 0.40691208598501993 Lambda1 0.0017092328\n",
      "11 Train Loss 508.62863 Test MSE 505.2630826698757 Test RE 0.3783972350464594 Lambda1 -0.0009911451\n",
      "12 Train Loss 313.90363 Test MSE 315.5934688603744 Test RE 0.29905645033784634 Lambda1 -0.00030437284\n",
      "13 Train Loss 299.4847 Test MSE 307.29160899034633 Test RE 0.29509681444893293 Lambda1 -0.00091412093\n",
      "14 Train Loss 278.35028 Test MSE 293.0136276531432 Test RE 0.2881595917953592 Lambda1 -0.00019829064\n",
      "15 Train Loss 269.9397 Test MSE 287.9613025729964 Test RE 0.2856644750630403 Lambda1 -0.0004893178\n",
      "16 Train Loss 263.87085 Test MSE 283.3313161295493 Test RE 0.2833586404639224 Lambda1 -4.1470303e-06\n",
      "17 Train Loss 259.8422 Test MSE 282.4063366293349 Test RE 0.2828957279900813 Lambda1 0.00025148073\n",
      "18 Train Loss 258.52887 Test MSE 282.6283919303083 Test RE 0.28300692618967493 Lambda1 0.00028304252\n",
      "19 Train Loss 256.17767 Test MSE 281.0980194579482 Test RE 0.28223967507725567 Lambda1 0.0009391784\n",
      "20 Train Loss 255.41699 Test MSE 281.51682748236504 Test RE 0.2824498513097203 Lambda1 0.00090142747\n",
      "21 Train Loss 255.26755 Test MSE 281.95378617927406 Test RE 0.2826689697648738 Lambda1 0.00077114196\n",
      "22 Train Loss 255.03668 Test MSE 281.95994487424804 Test RE 0.28267205690635294 Lambda1 0.00042068737\n",
      "23 Train Loss 254.74475 Test MSE 282.2144773384032 Test RE 0.2827996157781447 Lambda1 0.00034166678\n",
      "24 Train Loss 254.58899 Test MSE 282.41077350722804 Test RE 0.2828979502647062 Lambda1 0.0003847074\n",
      "25 Train Loss 254.5462 Test MSE 282.2519022333987 Test RE 0.28281836640207914 Lambda1 0.00034509954\n",
      "26 Train Loss 254.44797 Test MSE 282.34145988634526 Test RE 0.2828632315386042 Lambda1 0.00030388733\n",
      "27 Train Loss 254.23405 Test MSE 282.6405164882276 Test RE 0.28301299652295797 Lambda1 0.00022959187\n",
      "28 Train Loss 254.01746 Test MSE 282.622187599664 Test RE 0.28300381985246453 Lambda1 0.00014242624\n",
      "29 Train Loss 253.87178 Test MSE 282.6117534054146 Test RE 0.28299859566233626 Lambda1 0.00013772752\n",
      "30 Train Loss 253.58472 Test MSE 282.4633082570021 Test RE 0.2829242617289312 Lambda1 0.0001543672\n",
      "31 Train Loss 253.45761 Test MSE 282.54828542344484 Test RE 0.2829668164491782 Lambda1 0.00015587141\n",
      "32 Train Loss 253.30121 Test MSE 282.98171769370634 Test RE 0.2831837704293942 Lambda1 0.00019012998\n",
      "33 Train Loss 253.13359 Test MSE 283.164627905254 Test RE 0.2832752760511597 Lambda1 0.00022238152\n",
      "34 Train Loss 252.95367 Test MSE 282.7883102614104 Test RE 0.28308698112470276 Lambda1 0.00018703159\n",
      "35 Train Loss 252.8608 Test MSE 282.946637688489 Test RE 0.28316621735897757 Lambda1 0.00015897038\n",
      "36 Train Loss 252.68161 Test MSE 283.3858464138416 Test RE 0.2833859069236348 Lambda1 0.00011397778\n",
      "37 Train Loss 252.60515 Test MSE 283.571211424377 Test RE 0.283478574297547 Lambda1 8.837729e-05\n",
      "38 Train Loss 252.31668 Test MSE 284.211074932866 Test RE 0.28379822132132615 Lambda1 6.334395e-05\n",
      "39 Train Loss 252.05301 Test MSE 285.1260546487977 Test RE 0.2842546795342685 Lambda1 3.343263e-06\n",
      "40 Train Loss 251.94843 Test MSE 285.11917287328816 Test RE 0.2842512491415284 Lambda1 -9.1661786e-07\n",
      "41 Train Loss 251.85297 Test MSE 285.0342195420545 Test RE 0.28420889862310944 Lambda1 4.571412e-06\n",
      "42 Train Loss 251.69911 Test MSE 284.9688025638525 Test RE 0.2841762829704593 Lambda1 1.1074956e-05\n",
      "43 Train Loss 251.54456 Test MSE 284.94515150655855 Test RE 0.2841644900848002 Lambda1 1.5410267e-05\n",
      "44 Train Loss 251.43689 Test MSE 284.44483056275516 Test RE 0.2839149053686114 Lambda1 2.1530415e-05\n",
      "45 Train Loss 251.37788 Test MSE 284.451716357603 Test RE 0.283918341831066 Lambda1 1.3991985e-05\n",
      "46 Train Loss 251.17757 Test MSE 285.24058506609066 Test RE 0.28431176400253794 Lambda1 9.369929e-06\n",
      "47 Train Loss 251.01886 Test MSE 285.3634801235662 Test RE 0.2843730048457313 Lambda1 -6.168429e-06\n",
      "48 Train Loss 250.85043 Test MSE 285.9563075377623 Test RE 0.2846682364706033 Lambda1 -1.2406914e-05\n",
      "49 Train Loss 250.56903 Test MSE 286.8536927475228 Test RE 0.28511455808343383 Lambda1 -2.4210898e-05\n",
      "50 Train Loss 250.44214 Test MSE 286.95475584123244 Test RE 0.2851647788449592 Lambda1 -2.669586e-05\n",
      "51 Train Loss 250.32521 Test MSE 286.8062675050499 Test RE 0.2850909882526854 Lambda1 -3.1011736e-05\n",
      "52 Train Loss 250.1703 Test MSE 287.4817569748945 Test RE 0.2854265156214395 Lambda1 -2.6088963e-05\n",
      "53 Train Loss 249.82968 Test MSE 288.8732357292426 Test RE 0.28611644722518115 Lambda1 -1.2361585e-05\n",
      "54 Train Loss 249.6444 Test MSE 288.3852555815408 Test RE 0.2858746834655267 Lambda1 -3.2487933e-06\n",
      "55 Train Loss 249.557 Test MSE 288.6574851445482 Test RE 0.28600958145592237 Lambda1 -1.2053841e-06\n",
      "56 Train Loss 249.37946 Test MSE 288.9199441359372 Test RE 0.28613957761864806 Lambda1 1.5868093e-06\n",
      "57 Train Loss 249.28215 Test MSE 288.693818957007 Test RE 0.2860275811476685 Lambda1 -2.8426034e-06\n",
      "58 Train Loss 249.20993 Test MSE 288.84057408312634 Test RE 0.286100271793959 Lambda1 -3.8035514e-06\n",
      "59 Train Loss 249.05034 Test MSE 289.27343446824517 Test RE 0.2863145684026186 Lambda1 3.853146e-06\n",
      "60 Train Loss 248.95474 Test MSE 289.4599823396108 Test RE 0.2864068733961832 Lambda1 1.0887185e-06\n",
      "61 Train Loss 248.84239 Test MSE 289.91495754976256 Test RE 0.2866318731730029 Lambda1 2.1393619e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 Train Loss 248.61638 Test MSE 290.19069113961586 Test RE 0.2867681463286655 Lambda1 1.7912855e-06\n",
      "63 Train Loss 248.50801 Test MSE 290.0089699592625 Test RE 0.28667834329834074 Lambda1 1.7422583e-06\n",
      "64 Train Loss 248.3787 Test MSE 289.6138626767895 Test RE 0.2864829919153975 Lambda1 1.9095312e-06\n",
      "65 Train Loss 248.14543 Test MSE 288.3101323565368 Test RE 0.2858374464249228 Lambda1 2.5066192e-06\n",
      "66 Train Loss 248.0335 Test MSE 288.002675731191 Test RE 0.28568499590546076 Lambda1 2.7075544e-07\n",
      "67 Train Loss 247.94806 Test MSE 288.7210827382237 Test RE 0.2860410868218096 Lambda1 4.3378536e-06\n",
      "68 Train Loss 247.88391 Test MSE 288.8982907873466 Test RE 0.2861288549318991 Lambda1 6.7009785e-07\n",
      "69 Train Loss 247.78262 Test MSE 288.48864413322553 Test RE 0.28592592311865633 Lambda1 2.3073885e-06\n",
      "70 Train Loss 247.60587 Test MSE 288.4565617788502 Test RE 0.285910023997452 Lambda1 -1.4601665e-06\n",
      "71 Train Loss 247.45464 Test MSE 288.1608145937534 Test RE 0.2857634182677328 Lambda1 -5.2957154e-07\n",
      "72 Train Loss 247.39453 Test MSE 288.04381428733666 Test RE 0.2857053989252484 Lambda1 -1.7354156e-06\n",
      "73 Train Loss 247.30145 Test MSE 288.7525452090499 Test RE 0.2860566716106006 Lambda1 -1.6561936e-06\n",
      "74 Train Loss 247.26468 Test MSE 289.01505434596385 Test RE 0.2861866712106968 Lambda1 6.7226074e-07\n",
      "Training time: 124.68\n",
      "Training time: 124.68\n",
      "inv_HT_stan_tune10\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 840.59296 Test MSE 858.9102995041661 Test RE 0.4933590235959352 Lambda1 -0.004677297\n",
      "1 Train Loss 837.8479 Test MSE 857.8718829342157 Test RE 0.4930606995890671 Lambda1 -0.004378478\n",
      "2 Train Loss 837.6336 Test MSE 857.8457961062577 Test RE 0.49305320284760557 Lambda1 -0.0022568258\n",
      "3 Train Loss 835.2033 Test MSE 854.463218980282 Test RE 0.49208016211489286 Lambda1 0.0047987653\n",
      "4 Train Loss 797.8561 Test MSE 774.0671160353929 Test RE 0.4683585790140962 Lambda1 0.1294842\n",
      "5 Train Loss 729.33716 Test MSE 731.3558655960262 Test RE 0.455253763861488 Lambda1 0.11291805\n",
      "6 Train Loss 665.04706 Test MSE 670.3265475116706 Test RE 0.43584531334283405 Lambda1 0.05628384\n",
      "7 Train Loss 589.7301 Test MSE 578.2379668815178 Test RE 0.4048018350396461 Lambda1 0.026869027\n",
      "8 Train Loss 492.18143 Test MSE 492.4076361605325 Test RE 0.3735524251627424 Lambda1 0.0102631\n",
      "9 Train Loss 369.5632 Test MSE 368.62462243335574 Test RE 0.3232074337759938 Lambda1 0.00034493007\n",
      "10 Train Loss 276.36642 Test MSE 303.59225900454743 Test RE 0.29331516500738153 Lambda1 0.00014720894\n",
      "11 Train Loss 270.65866 Test MSE 297.62386831845384 Test RE 0.29041767827511483 Lambda1 0.00027714393\n",
      "12 Train Loss 264.61014 Test MSE 288.3459977008766 Test RE 0.28585522474501607 Lambda1 0.00019208099\n",
      "13 Train Loss 259.3241 Test MSE 283.2507721754335 Test RE 0.28331836174029457 Lambda1 8.893663e-05\n",
      "14 Train Loss 257.98517 Test MSE 281.9486902490736 Test RE 0.28266641532524744 Lambda1 0.0002101321\n",
      "15 Train Loss 255.91959 Test MSE 282.0282894884167 Test RE 0.28270631344254327 Lambda1 0.00015618822\n",
      "16 Train Loss 255.15636 Test MSE 282.82841273324004 Test RE 0.2831070528264419 Lambda1 7.770665e-05\n",
      "17 Train Loss 254.73454 Test MSE 282.8534968526151 Test RE 0.2831196069641064 Lambda1 6.1142004e-05\n",
      "18 Train Loss 254.61761 Test MSE 282.8262502043332 Test RE 0.28310597049466724 Lambda1 6.0963255e-05\n",
      "19 Train Loss 254.45671 Test MSE 283.03053443447243 Test RE 0.2832081951743572 Lambda1 4.585293e-05\n",
      "20 Train Loss 254.41277 Test MSE 283.09860711099 Test RE 0.2832422508299344 Lambda1 3.9692073e-05\n",
      "21 Train Loss 254.37845 Test MSE 283.0073526633255 Test RE 0.28319659677558534 Lambda1 3.59145e-05\n",
      "22 Train Loss 254.33757 Test MSE 282.9902696516706 Test RE 0.28318804942915404 Lambda1 2.9826211e-05\n",
      "23 Train Loss 254.29768 Test MSE 282.9401938617399 Test RE 0.2831629929268909 Lambda1 3.3751046e-05\n",
      "24 Train Loss 254.27853 Test MSE 282.9953996399418 Test RE 0.28319061620431235 Lambda1 3.159536e-05\n",
      "25 Train Loss 254.22809 Test MSE 283.12618146055325 Test RE 0.2832560446644252 Lambda1 2.6425505e-05\n",
      "26 Train Loss 254.16737 Test MSE 283.480609908757 Test RE 0.2834332847207875 Lambda1 2.5410773e-05\n",
      "27 Train Loss 254.13637 Test MSE 283.7772703286128 Test RE 0.2835815513881237 Lambda1 2.1501462e-05\n",
      "28 Train Loss 254.12735 Test MSE 283.844011178056 Test RE 0.2836148968370347 Lambda1 1.8617035e-05\n",
      "29 Train Loss 254.1222 Test MSE 283.9250743534851 Test RE 0.28365539281718943 Lambda1 1.861073e-05\n",
      "30 Train Loss 254.1022 Test MSE 283.90607442121905 Test RE 0.2836459017156561 Lambda1 2.1608967e-05\n",
      "31 Train Loss 254.07152 Test MSE 283.81010846639595 Test RE 0.2835979586582728 Lambda1 2.354477e-05\n",
      "32 Train Loss 254.04028 Test MSE 283.847222332943 Test RE 0.28361650111394443 Lambda1 2.401304e-05\n",
      "33 Train Loss 254.0221 Test MSE 283.82748593866245 Test RE 0.28360664076662456 Lambda1 2.4348346e-05\n",
      "34 Train Loss 254.01349 Test MSE 283.6461476466024 Test RE 0.28351602769438106 Lambda1 2.3760254e-05\n",
      "35 Train Loss 254.00539 Test MSE 283.61248765423846 Test RE 0.2834992049196985 Lambda1 2.2282753e-05\n",
      "36 Train Loss 253.97609 Test MSE 283.86811353504635 Test RE 0.2836269380324002 Lambda1 2.3664385e-05\n",
      "37 Train Loss 253.91621 Test MSE 283.714132292 Test RE 0.2835500023877758 Lambda1 2.0798983e-05\n",
      "38 Train Loss 253.88484 Test MSE 283.29269849772106 Test RE 0.28333932912782656 Lambda1 2.7214886e-05\n",
      "39 Train Loss 253.80324 Test MSE 283.4516410659565 Test RE 0.283418802347467 Lambda1 2.6920792e-05\n",
      "40 Train Loss 253.71971 Test MSE 283.5507336756434 Test RE 0.2834683385832255 Lambda1 2.1094334e-05\n",
      "41 Train Loss 253.69446 Test MSE 283.3282828003706 Test RE 0.2833571236490069 Lambda1 2.4759072e-05\n",
      "42 Train Loss 253.65964 Test MSE 283.18244740005673 Test RE 0.2832841891398717 Lambda1 2.8774888e-05\n",
      "43 Train Loss 253.57639 Test MSE 283.33614913284873 Test RE 0.2833610571883124 Lambda1 2.45105e-05\n",
      "44 Train Loss 253.42345 Test MSE 283.199811285441 Test RE 0.28329287406867937 Lambda1 1.1543648e-05\n",
      "45 Train Loss 253.39986 Test MSE 283.1252209338829 Test RE 0.28325556418039716 Lambda1 1.0420575e-05\n",
      "46 Train Loss 253.36456 Test MSE 283.0695439011048 Test RE 0.28322771147851594 Lambda1 1.1587374e-05\n",
      "47 Train Loss 253.29492 Test MSE 282.9113549754023 Test RE 0.2831485617615071 Lambda1 7.312704e-06\n",
      "48 Train Loss 253.17923 Test MSE 282.8078272504449 Test RE 0.28309674975724597 Lambda1 9.010553e-06\n",
      "49 Train Loss 253.0836 Test MSE 283.2994181388015 Test RE 0.28334268948147434 Lambda1 9.135338e-06\n",
      "50 Train Loss 252.94604 Test MSE 283.31432537630985 Test RE 0.2833501441406343 Lambda1 1.8518816e-05\n",
      "51 Train Loss 252.91426 Test MSE 283.2206175011429 Test RE 0.2833032804039831 Lambda1 2.3862738e-05\n",
      "52 Train Loss 252.90758 Test MSE 283.1989904236842 Test RE 0.28329246350263104 Lambda1 2.070286e-05\n",
      "53 Train Loss 252.88452 Test MSE 283.36540932184596 Test RE 0.2833756881912143 Lambda1 1.254824e-05\n",
      "54 Train Loss 252.69904 Test MSE 283.906425448362 Test RE 0.283646077068332 Lambda1 1.5028453e-05\n",
      "55 Train Loss 252.5921 Test MSE 283.64270626368597 Test RE 0.2835143077870567 Lambda1 1.4483619e-05\n",
      "56 Train Loss 252.55817 Test MSE 283.7167898238964 Test RE 0.28355133038191715 Lambda1 1.15706125e-05\n",
      "57 Train Loss 252.54161 Test MSE 283.7873227802745 Test RE 0.28358657410292587 Lambda1 1.4007687e-05\n",
      "58 Train Loss 252.44469 Test MSE 283.752926466175 Test RE 0.28356938759091577 Lambda1 1.5497226e-05\n",
      "59 Train Loss 252.2101 Test MSE 283.7165147171306 Test RE 0.28355119290872005 Lambda1 1.1792464e-05\n",
      "60 Train Loss 251.97855 Test MSE 283.42275227726395 Test RE 0.2834043592585353 Lambda1 -1.2016775e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 Train Loss 251.74724 Test MSE 283.568730874636 Test RE 0.28347733442513845 Lambda1 1.3744449e-06\n",
      "62 Train Loss 251.64668 Test MSE 283.939428711227 Test RE 0.2836625630883103 Lambda1 -9.0228605e-07\n",
      "63 Train Loss 251.5393 Test MSE 284.24085642723173 Test RE 0.28381309004825844 Lambda1 -4.482202e-06\n",
      "64 Train Loss 250.89458 Test MSE 285.69793951652076 Test RE 0.2845396052988581 Lambda1 6.507286e-06\n",
      "65 Train Loss 250.66154 Test MSE 285.7532660415543 Test RE 0.28456715506968006 Lambda1 6.7260617e-06\n",
      "66 Train Loss 250.62057 Test MSE 285.659042013786 Test RE 0.2845202347411985 Lambda1 8.861288e-06\n",
      "67 Train Loss 250.49889 Test MSE 285.94200209549115 Test RE 0.2846611158793659 Lambda1 5.1250217e-06\n",
      "68 Train Loss 250.40224 Test MSE 286.3767509989689 Test RE 0.28487743440344376 Lambda1 2.8104994e-06\n",
      "69 Train Loss 250.34679 Test MSE 286.1860207168742 Test RE 0.2847825527429273 Lambda1 3.9050587e-06\n",
      "70 Train Loss 250.25539 Test MSE 286.1663844924462 Test RE 0.2847727826115444 Lambda1 1.0902453e-06\n",
      "71 Train Loss 250.13225 Test MSE 286.73002658278676 Test RE 0.2850530932569918 Lambda1 2.2978477e-06\n",
      "72 Train Loss 250.02129 Test MSE 287.0424149251116 Test RE 0.28520833165947995 Lambda1 7.155816e-07\n",
      "73 Train Loss 249.82481 Test MSE 287.0656199147874 Test RE 0.28521985978615244 Lambda1 5.948782e-06\n",
      "74 Train Loss 249.659 Test MSE 287.1420711277028 Test RE 0.2852578370831658 Lambda1 -6.848358e-06\n",
      "Training time: 124.45\n",
      "Training time: 124.45\n",
      "inv_HT_stan_tune10\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 837.48065 Test MSE 857.5483750153262 Test RE 0.4929677229542451 Lambda1 -0.05958458\n",
      "1 Train Loss 834.1431 Test MSE 851.8266555538382 Test RE 0.4913203851558745 Lambda1 -0.03724324\n",
      "2 Train Loss 766.0874 Test MSE 718.3402827347506 Test RE 0.4511846140652391 Lambda1 0.09490024\n",
      "3 Train Loss 574.38293 Test MSE 576.5317229159809 Test RE 0.40420415640442603 Lambda1 0.03409278\n",
      "4 Train Loss 439.8695 Test MSE 434.2978960764259 Test RE 0.3508189410991459 Lambda1 0.009845228\n",
      "5 Train Loss 342.8925 Test MSE 355.1299966302307 Test RE 0.3172362804908867 Lambda1 0.008442984\n",
      "6 Train Loss 274.31406 Test MSE 296.56896581808644 Test RE 0.28990254101869045 Lambda1 0.0069788755\n",
      "7 Train Loss 261.78442 Test MSE 286.72596892026985 Test RE 0.2850510762841659 Lambda1 0.003905214\n",
      "8 Train Loss 258.98883 Test MSE 283.5075684117365 Test RE 0.283446761401612 Lambda1 0.0032835905\n",
      "9 Train Loss 257.32538 Test MSE 281.5550567981095 Test RE 0.28246902866776324 Lambda1 0.0040929033\n",
      "10 Train Loss 255.6856 Test MSE 280.15215197863756 Test RE 0.2817644204242732 Lambda1 0.005566507\n",
      "11 Train Loss 254.26834 Test MSE 279.3537739448534 Test RE 0.28136264763743085 Lambda1 0.0068538254\n",
      "12 Train Loss 253.04527 Test MSE 278.1965147644277 Test RE 0.2807792522127105 Lambda1 0.009728142\n",
      "13 Train Loss 251.36766 Test MSE 275.40780060586974 Test RE 0.27936840553901326 Lambda1 0.015293908\n",
      "14 Train Loss 249.68723 Test MSE 272.61006495239127 Test RE 0.27794579865259084 Lambda1 0.022067027\n",
      "15 Train Loss 248.30309 Test MSE 269.5141188312223 Test RE 0.276363020865595 Lambda1 0.033092678\n",
      "16 Train Loss 245.39352 Test MSE 265.6733698944139 Test RE 0.27438677983611176 Lambda1 0.04694532\n",
      "17 Train Loss 241.75252 Test MSE 262.9848505396 Test RE 0.2729949015833508 Lambda1 0.06533647\n",
      "18 Train Loss 237.52417 Test MSE 257.1463328819325 Test RE 0.26994751770614245 Lambda1 0.082151935\n",
      "19 Train Loss 233.22803 Test MSE 250.00139092747298 Test RE 0.2661707835726675 Lambda1 0.10698516\n",
      "20 Train Loss 227.08696 Test MSE 247.33410537249185 Test RE 0.2647470769059938 Lambda1 0.12639825\n",
      "21 Train Loss 223.73152 Test MSE 246.72913060772967 Test RE 0.26442309538178255 Lambda1 0.13266923\n",
      "22 Train Loss 220.86751 Test MSE 244.18184162570705 Test RE 0.26305457118016456 Lambda1 0.14392275\n",
      "23 Train Loss 218.19475 Test MSE 241.8783747202864 Test RE 0.2618108806397028 Lambda1 0.16720606\n",
      "24 Train Loss 215.54422 Test MSE 238.42469927408644 Test RE 0.2599350189255273 Lambda1 0.2041726\n",
      "25 Train Loss 213.57393 Test MSE 237.3854629267608 Test RE 0.2593679029219601 Lambda1 0.22877929\n",
      "26 Train Loss 211.84866 Test MSE 236.50140679087974 Test RE 0.25888449154556015 Lambda1 0.24604814\n",
      "27 Train Loss 210.18745 Test MSE 234.57112036424866 Test RE 0.25782584020297256 Lambda1 0.2732849\n",
      "28 Train Loss 204.30627 Test MSE 218.87233015434862 Test RE 0.24904888481019455 Lambda1 0.33897653\n",
      "29 Train Loss 194.93584 Test MSE 204.9152298981559 Test RE 0.24097738621400738 Lambda1 0.36082748\n",
      "30 Train Loss 178.96852 Test MSE 185.35616175780027 Test RE 0.22918842732451306 Lambda1 0.38848507\n",
      "31 Train Loss 170.28844 Test MSE 177.7443031050318 Test RE 0.22443315550783088 Lambda1 0.40535426\n",
      "32 Train Loss 162.0783 Test MSE 168.53317858862272 Test RE 0.21854047190159018 Lambda1 0.42284024\n",
      "33 Train Loss 155.84293 Test MSE 159.17154804320586 Test RE 0.2123840463172842 Lambda1 0.45788518\n",
      "34 Train Loss 151.09492 Test MSE 154.40943803481875 Test RE 0.2091828577545279 Lambda1 0.48064247\n",
      "35 Train Loss 146.82626 Test MSE 149.80957537533428 Test RE 0.20604351833661766 Lambda1 0.50215197\n",
      "36 Train Loss 143.5134 Test MSE 144.86563807590946 Test RE 0.20261512528437833 Lambda1 0.52391905\n",
      "37 Train Loss 140.84338 Test MSE 144.80914542440237 Test RE 0.20257561494243104 Lambda1 0.52605784\n",
      "38 Train Loss 138.22244 Test MSE 142.0715207255864 Test RE 0.20065162682011936 Lambda1 0.53424865\n",
      "39 Train Loss 134.445 Test MSE 137.3231282201898 Test RE 0.19726998641877824 Lambda1 0.54661024\n",
      "40 Train Loss 132.12256 Test MSE 136.76360515383556 Test RE 0.1968676879434632 Lambda1 0.5482897\n",
      "41 Train Loss 130.30943 Test MSE 134.99930715955983 Test RE 0.19559373523264012 Lambda1 0.55849653\n",
      "42 Train Loss 128.43318 Test MSE 132.52836610409832 Test RE 0.1937954572243198 Lambda1 0.57233894\n",
      "43 Train Loss 126.74114 Test MSE 130.69279341085036 Test RE 0.19244870406763637 Lambda1 0.5812749\n",
      "44 Train Loss 123.8401 Test MSE 127.34972927268592 Test RE 0.18997138266677663 Lambda1 0.60008854\n",
      "45 Train Loss 121.08827 Test MSE 122.53879181165951 Test RE 0.18634852847734576 Lambda1 0.6301222\n",
      "46 Train Loss 119.29405 Test MSE 120.02029540656142 Test RE 0.18442360891555012 Lambda1 0.6433821\n",
      "47 Train Loss 116.152145 Test MSE 116.27978932393049 Test RE 0.18152702440826474 Lambda1 0.6695359\n",
      "48 Train Loss 114.50078 Test MSE 115.89716160597429 Test RE 0.18122811389102647 Lambda1 0.67987263\n",
      "49 Train Loss 112.73129 Test MSE 113.78466047345756 Test RE 0.1795688616864193 Lambda1 0.7001444\n",
      "50 Train Loss 111.387245 Test MSE 110.55318902854388 Test RE 0.17700062832008814 Lambda1 0.7234781\n",
      "51 Train Loss 109.7314 Test MSE 108.37726114604641 Test RE 0.17525009271797556 Lambda1 0.75615907\n",
      "52 Train Loss 106.44165 Test MSE 105.17411777662784 Test RE 0.17264086745347507 Lambda1 0.8040937\n",
      "53 Train Loss 105.19016 Test MSE 103.25398786474042 Test RE 0.17105768396190285 Lambda1 0.8215963\n",
      "54 Train Loss 104.04222 Test MSE 103.32411646308789 Test RE 0.17111576404030598 Lambda1 0.82026273\n",
      "55 Train Loss 102.47255 Test MSE 102.71130468483294 Test RE 0.17060756854390222 Lambda1 0.8454394\n",
      "56 Train Loss 101.57358 Test MSE 101.5549730316981 Test RE 0.16964449381062965 Lambda1 0.874376\n",
      "57 Train Loss 98.99236 Test MSE 99.62708048134144 Test RE 0.16802653535907422 Lambda1 0.8969712\n",
      "58 Train Loss 96.36152 Test MSE 97.47752077170364 Test RE 0.16620397566515238 Lambda1 0.9191272\n",
      "59 Train Loss 94.13706 Test MSE 95.56557344441855 Test RE 0.16456592139285126 Lambda1 0.95461404\n",
      "60 Train Loss 93.03326 Test MSE 95.53162283288628 Test RE 0.16453668696368012 Lambda1 0.97271395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 Train Loss 91.777435 Test MSE 94.50695947360185 Test RE 0.16365190539562824 Lambda1 1.0092642\n",
      "62 Train Loss 90.28856 Test MSE 92.04978554470318 Test RE 0.16151042544871982 Lambda1 1.0559496\n",
      "63 Train Loss 89.35045 Test MSE 91.60970791559701 Test RE 0.16112388309607117 Lambda1 1.0859257\n",
      "64 Train Loss 87.519615 Test MSE 90.16195468622854 Test RE 0.1598456529106968 Lambda1 1.1153922\n",
      "65 Train Loss 86.328064 Test MSE 88.93202783958093 Test RE 0.15875165745634703 Lambda1 1.1306256\n",
      "66 Train Loss 85.340965 Test MSE 87.64833315419015 Test RE 0.1576017375691928 Lambda1 1.1582751\n",
      "67 Train Loss 84.76321 Test MSE 86.21442337220877 Test RE 0.15630725438715135 Lambda1 1.1765031\n",
      "68 Train Loss 83.78884 Test MSE 84.37230014656339 Test RE 0.15462834766503367 Lambda1 1.2062435\n",
      "69 Train Loss 82.87528 Test MSE 83.66934824059882 Test RE 0.15398285357690827 Lambda1 1.2228321\n",
      "70 Train Loss 82.29073 Test MSE 82.97657351345022 Test RE 0.15334404637748777 Lambda1 1.2300572\n",
      "71 Train Loss 81.78406 Test MSE 82.09945574919904 Test RE 0.152531418708559 Lambda1 1.2421072\n",
      "72 Train Loss 80.998405 Test MSE 80.91266948126531 Test RE 0.1514249487983094 Lambda1 1.2566646\n",
      "73 Train Loss 80.30961 Test MSE 80.04706898864565 Test RE 0.15061280186477757 Lambda1 1.2641838\n",
      "74 Train Loss 79.62628 Test MSE 79.88599098583578 Test RE 0.1504611871543629 Lambda1 1.267013\n",
      "Training time: 123.06\n",
      "Training time: 123.06\n",
      "inv_HT_stan_tune10\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 837.31244 Test MSE 857.0138699847313 Test RE 0.4928140670166592 Lambda1 -0.111402445\n",
      "1 Train Loss 825.0342 Test MSE 841.8846106963247 Test RE 0.4884447613287544 Lambda1 -0.10084628\n",
      "2 Train Loss 753.70667 Test MSE 758.5569137536753 Test RE 0.46364251838826276 Lambda1 0.002947648\n",
      "3 Train Loss 610.3619 Test MSE 610.9996060792471 Test RE 0.41611142073994883 Lambda1 -0.0021148194\n",
      "4 Train Loss 571.53253 Test MSE 584.0487709365417 Test RE 0.40683070923931486 Lambda1 -0.0007236238\n",
      "5 Train Loss 412.66516 Test MSE 403.13084851415454 Test RE 0.33799648742028016 Lambda1 0.0010836909\n",
      "6 Train Loss 320.94473 Test MSE 338.5979670678365 Test RE 0.30976428533405076 Lambda1 -0.0016588719\n",
      "7 Train Loss 274.42368 Test MSE 292.87557268911314 Test RE 0.28809169981817834 Lambda1 0.00072893896\n",
      "8 Train Loss 261.93802 Test MSE 283.2976285288613 Test RE 0.28334179453841773 Lambda1 0.00056796893\n",
      "9 Train Loss 258.41635 Test MSE 282.42319164407195 Test RE 0.28290416997579837 Lambda1 0.0005260733\n",
      "10 Train Loss 257.06247 Test MSE 282.79009491913564 Test RE 0.2830878743946018 Lambda1 0.0010483672\n",
      "11 Train Loss 256.1324 Test MSE 282.4123139829532 Test RE 0.2828987218302266 Lambda1 0.0015582031\n",
      "12 Train Loss 255.3611 Test MSE 282.22963108423437 Test RE 0.282807208258893 Lambda1 0.001338292\n",
      "13 Train Loss 254.7851 Test MSE 282.31192090076365 Test RE 0.2828484343645125 Lambda1 0.0012580794\n",
      "14 Train Loss 254.54141 Test MSE 282.2065159902492 Test RE 0.2827956268224769 Lambda1 0.0015014404\n",
      "15 Train Loss 254.43028 Test MSE 282.0154111336915 Test RE 0.2826998587110733 Lambda1 0.0016268672\n",
      "16 Train Loss 254.28357 Test MSE 281.9872341669822 Test RE 0.2826857356823837 Lambda1 0.0016801478\n",
      "17 Train Loss 254.2255 Test MSE 282.05700375089316 Test RE 0.2827207047234506 Lambda1 0.001846581\n",
      "18 Train Loss 254.04663 Test MSE 281.88817091399113 Test RE 0.28263607700098115 Lambda1 0.0021306358\n",
      "19 Train Loss 253.78174 Test MSE 281.3315353195891 Test RE 0.2823568828735666 Lambda1 0.0025590225\n",
      "20 Train Loss 253.43192 Test MSE 280.26524739413645 Test RE 0.2818212878239683 Lambda1 0.0036444631\n",
      "21 Train Loss 252.84178 Test MSE 278.85283563520454 Test RE 0.2811102641278698 Lambda1 0.005105393\n",
      "22 Train Loss 252.41447 Test MSE 278.18299373392875 Test RE 0.280772428850876 Lambda1 0.006551169\n",
      "23 Train Loss 251.13121 Test MSE 276.00365971351073 Test RE 0.27967045629290815 Lambda1 0.010531199\n",
      "24 Train Loss 250.15637 Test MSE 273.3746906896723 Test RE 0.2783353214971296 Lambda1 0.016284049\n",
      "25 Train Loss 249.0478 Test MSE 271.5872408564104 Test RE 0.2774238868458749 Lambda1 0.019659778\n",
      "26 Train Loss 246.93198 Test MSE 267.78402149250803 Test RE 0.27547456142785987 Lambda1 0.025493482\n",
      "27 Train Loss 242.45294 Test MSE 260.7423698019621 Test RE 0.27182849136397946 Lambda1 0.041790385\n",
      "28 Train Loss 237.90802 Test MSE 255.7571935251854 Test RE 0.26921738375371757 Lambda1 0.05658168\n",
      "29 Train Loss 232.25189 Test MSE 249.38806559916694 Test RE 0.2658440863291383 Lambda1 0.073165365\n",
      "30 Train Loss 226.04274 Test MSE 241.1948270375193 Test RE 0.26144068046111124 Lambda1 0.08630124\n",
      "31 Train Loss 221.14127 Test MSE 231.0331281735345 Test RE 0.25587408336256184 Lambda1 0.1059507\n",
      "32 Train Loss 212.16484 Test MSE 213.7738349567796 Test RE 0.24613107286806296 Lambda1 0.14234243\n",
      "33 Train Loss 201.4864 Test MSE 204.4661748369393 Test RE 0.24071320022711906 Lambda1 0.1748505\n",
      "34 Train Loss 194.05872 Test MSE 195.42476709877434 Test RE 0.23533090890221164 Lambda1 0.19399242\n",
      "35 Train Loss 187.93613 Test MSE 187.7577905253044 Test RE 0.2306684267757049 Lambda1 0.20597513\n",
      "36 Train Loss 173.64716 Test MSE 172.69457145941763 Test RE 0.22122210152588861 Lambda1 0.2585372\n",
      "37 Train Loss 164.87047 Test MSE 164.91863747118535 Test RE 0.21618424463308328 Lambda1 0.2829766\n",
      "38 Train Loss 160.534 Test MSE 160.9532183433074 Test RE 0.213569388002457 Lambda1 0.296395\n",
      "39 Train Loss 149.90305 Test MSE 151.80874681402085 Test RE 0.20741376179209903 Lambda1 0.34161016\n",
      "40 Train Loss 143.40382 Test MSE 146.70408609738442 Test RE 0.20389673701446553 Lambda1 0.36165354\n",
      "41 Train Loss 136.63847 Test MSE 140.14427726726785 Test RE 0.19928602959978173 Lambda1 0.387825\n",
      "42 Train Loss 133.35208 Test MSE 134.53889203805272 Test RE 0.19525991415963198 Lambda1 0.40745533\n",
      "43 Train Loss 128.52522 Test MSE 126.04921789744904 Test RE 0.18899888771656087 Lambda1 0.44169036\n",
      "44 Train Loss 123.36549 Test MSE 120.26251713895213 Test RE 0.1846096144988194 Lambda1 0.47853985\n",
      "45 Train Loss 116.78341 Test MSE 111.63018806749228 Test RE 0.1778607006628594 Lambda1 0.5318428\n",
      "46 Train Loss 112.7197 Test MSE 104.67676504248834 Test RE 0.1722321872629171 Lambda1 0.5639971\n",
      "47 Train Loss 107.72913 Test MSE 101.42749718824274 Test RE 0.16953798811783388 Lambda1 0.57348895\n",
      "48 Train Loss 102.53229 Test MSE 98.97140581401754 Test RE 0.16747270698490857 Lambda1 0.5928773\n",
      "49 Train Loss 100.17524 Test MSE 97.60108472803819 Test RE 0.1663092836200323 Lambda1 0.61328864\n",
      "50 Train Loss 97.0724 Test MSE 95.15181650018923 Test RE 0.16420928588833025 Lambda1 0.63678455\n",
      "51 Train Loss 92.064514 Test MSE 90.21595663467453 Test RE 0.15989351503063093 Lambda1 0.68471694\n",
      "52 Train Loss 90.21626 Test MSE 87.02624691456377 Test RE 0.15704145049631055 Lambda1 0.7112988\n",
      "53 Train Loss 86.5526 Test MSE 84.49997633513624 Test RE 0.15474529893290925 Lambda1 0.73202926\n",
      "54 Train Loss 84.73428 Test MSE 82.90832569824089 Test RE 0.15328097105082159 Lambda1 0.7527229\n",
      "55 Train Loss 82.661514 Test MSE 79.20890831329044 Test RE 0.1498222050021576 Lambda1 0.7839964\n",
      "56 Train Loss 79.951996 Test MSE 77.37671039757848 Test RE 0.1480792825915731 Lambda1 0.7946669\n",
      "57 Train Loss 78.032524 Test MSE 75.51152854339877 Test RE 0.146283654482136 Lambda1 0.81246245\n",
      "58 Train Loss 76.519516 Test MSE 72.82268212353 Test RE 0.14365558194749864 Lambda1 0.8400335\n",
      "59 Train Loss 74.203186 Test MSE 72.02656698960213 Test RE 0.1428681852872008 Lambda1 0.87897384\n",
      "60 Train Loss 71.64755 Test MSE 70.75595352399796 Test RE 0.1416024164379752 Lambda1 0.8989584\n",
      "61 Train Loss 69.057625 Test MSE 66.19967882766906 Test RE 0.13696736848009483 Lambda1 0.91985756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 Train Loss 66.249344 Test MSE 63.28257231273835 Test RE 0.1339156188669643 Lambda1 0.95621884\n",
      "63 Train Loss 64.62599 Test MSE 61.942375826892324 Test RE 0.13248999999558334 Lambda1 0.9760597\n",
      "64 Train Loss 62.5003 Test MSE 60.38351446643036 Test RE 0.13081223438234107 Lambda1 0.9848982\n",
      "65 Train Loss 61.345234 Test MSE 59.528710191099634 Test RE 0.12988302863565243 Lambda1 0.99615675\n",
      "66 Train Loss 60.88802 Test MSE 59.95392600192881 Test RE 0.13034608287742874 Lambda1 1.0010111\n",
      "67 Train Loss 60.43688 Test MSE 59.21477170439219 Test RE 0.1295400917314772 Lambda1 1.0109575\n",
      "68 Train Loss 59.62316 Test MSE 58.389132269468696 Test RE 0.12863382422902048 Lambda1 1.0265307\n",
      "69 Train Loss 57.50972 Test MSE 58.41480532043765 Test RE 0.1286621005508939 Lambda1 1.0434849\n",
      "70 Train Loss 56.864605 Test MSE 57.6473526114069 Test RE 0.1278141259398203 Lambda1 1.0456368\n",
      "71 Train Loss 55.72948 Test MSE 56.44688179912457 Test RE 0.12647629899410406 Lambda1 1.0476314\n",
      "72 Train Loss 54.63869 Test MSE 54.91557316584402 Test RE 0.12474895922918403 Lambda1 1.0632294\n",
      "73 Train Loss 53.59582 Test MSE 53.186630689213644 Test RE 0.12276947826063261 Lambda1 1.086004\n",
      "74 Train Loss 53.255108 Test MSE 53.30900801759223 Test RE 0.12291063749725828 Lambda1 1.0951377\n",
      "Training time: 123.21\n",
      "Training time: 123.21\n",
      "inv_HT_stan_tune10\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 838.27155 Test MSE 859.1120580297675 Test RE 0.49341696535334095 Lambda1 -0.0021879675\n",
      "1 Train Loss 837.7322 Test MSE 857.8572767363855 Test RE 0.4930565021250179 Lambda1 -0.001922643\n",
      "2 Train Loss 837.2866 Test MSE 856.5252755775074 Test RE 0.4926735672336466 Lambda1 0.008365611\n",
      "3 Train Loss 829.4907 Test MSE 850.2262187679441 Test RE 0.4908586146160932 Lambda1 0.09403488\n",
      "4 Train Loss 773.3935 Test MSE 777.4434094547876 Test RE 0.4693789010097256 Lambda1 0.19366933\n",
      "5 Train Loss 624.5772 Test MSE 601.3024586675876 Test RE 0.41279617121482726 Lambda1 0.16048086\n",
      "6 Train Loss 413.7918 Test MSE 400.14574108165095 Test RE 0.33674276228420236 Lambda1 0.041436717\n",
      "7 Train Loss 328.6293 Test MSE 324.4844288538767 Test RE 0.30323972989673265 Lambda1 0.016297773\n",
      "8 Train Loss 283.88818 Test MSE 303.2891150782194 Test RE 0.29316868742837016 Lambda1 0.004772088\n",
      "9 Train Loss 267.64615 Test MSE 289.1027663960324 Test RE 0.286230094753895 Lambda1 0.00046823604\n",
      "10 Train Loss 262.91046 Test MSE 286.0692097222327 Test RE 0.2847244277368115 Lambda1 0.00039993547\n",
      "11 Train Loss 259.1028 Test MSE 284.15611743215464 Test RE 0.2837707811608546 Lambda1 0.0006908768\n",
      "12 Train Loss 257.31613 Test MSE 283.24062436153434 Test RE 0.28331328657714816 Lambda1 0.00051968714\n",
      "13 Train Loss 256.21243 Test MSE 282.8742876135958 Test RE 0.28313001193343007 Lambda1 0.00025879135\n",
      "14 Train Loss 255.78845 Test MSE 283.46653209797574 Test RE 0.2834262469031184 Lambda1 0.00019863853\n",
      "15 Train Loss 255.64514 Test MSE 283.5261267991043 Test RE 0.28345603845326817 Lambda1 0.00016925325\n",
      "16 Train Loss 255.1327 Test MSE 283.44985932915984 Test RE 0.2834179115808786 Lambda1 9.8469354e-05\n",
      "17 Train Loss 254.95215 Test MSE 283.29789350509026 Test RE 0.2833419270471561 Lambda1 0.00012547425\n",
      "18 Train Loss 254.78221 Test MSE 283.33427313588174 Test RE 0.28336011910581677 Lambda1 0.00013108534\n",
      "19 Train Loss 254.58954 Test MSE 283.3568698935838 Test RE 0.2833714182900331 Lambda1 7.4275966e-05\n",
      "20 Train Loss 254.54707 Test MSE 283.3157721313482 Test RE 0.2833508676086848 Lambda1 6.832735e-05\n",
      "21 Train Loss 254.48361 Test MSE 283.3795322140988 Test RE 0.28338274980550304 Lambda1 5.751673e-05\n",
      "22 Train Loss 254.36917 Test MSE 283.4103383711283 Test RE 0.28339815264031104 Lambda1 5.0007064e-05\n",
      "23 Train Loss 254.31146 Test MSE 283.1013582242689 Test RE 0.2832436270811853 Lambda1 3.166631e-05\n",
      "24 Train Loss 254.2896 Test MSE 282.8804509673752 Test RE 0.2831330963794195 Lambda1 3.8066035e-05\n",
      "25 Train Loss 254.25761 Test MSE 282.7722166027612 Test RE 0.2830789256819942 Lambda1 4.505114e-05\n",
      "26 Train Loss 254.22992 Test MSE 282.522098691459 Test RE 0.2829537033845029 Lambda1 4.505472e-05\n",
      "27 Train Loss 254.21349 Test MSE 282.4571764148009 Test RE 0.28292119078787026 Lambda1 5.684783e-05\n",
      "28 Train Loss 254.1594 Test MSE 282.47688126770123 Test RE 0.28293105922755946 Lambda1 7.615756e-05\n",
      "29 Train Loss 254.14145 Test MSE 282.46688058003434 Test RE 0.28292605079963784 Lambda1 7.6175646e-05\n",
      "30 Train Loss 254.13083 Test MSE 282.3553436534042 Test RE 0.2828701861653533 Lambda1 8.523237e-05\n",
      "31 Train Loss 254.10608 Test MSE 282.3591091916429 Test RE 0.28287207236119716 Lambda1 0.00010284795\n",
      "32 Train Loss 254.0491 Test MSE 282.94921601970793 Test RE 0.2831675075221 Lambda1 0.000102163\n",
      "33 Train Loss 254.04018 Test MSE 282.9943168841754 Test RE 0.2831900744524564 Lambda1 0.00010013803\n",
      "34 Train Loss 254.0276 Test MSE 283.13434641666055 Test RE 0.28326012898556757 Lambda1 0.0001136282\n",
      "35 Train Loss 253.99466 Test MSE 283.1451331019014 Test RE 0.2832655246727931 Lambda1 0.000121458725\n",
      "36 Train Loss 253.9448 Test MSE 282.78603325956055 Test RE 0.2830858414190398 Lambda1 0.00013384792\n",
      "37 Train Loss 253.89719 Test MSE 282.88323048865806 Test RE 0.2831344873778633 Lambda1 0.00016514171\n",
      "38 Train Loss 253.80013 Test MSE 282.6544930278231 Test RE 0.28301999391579813 Lambda1 0.0002084681\n",
      "39 Train Loss 253.76353 Test MSE 282.37664018354656 Test RE 0.282880853645168 Lambda1 0.00018963464\n",
      "40 Train Loss 253.706 Test MSE 282.50070246180763 Test RE 0.28294298872346363 Lambda1 0.00017913249\n",
      "41 Train Loss 253.6384 Test MSE 282.69009370763604 Test RE 0.283037816712164 Lambda1 0.00017538524\n",
      "42 Train Loss 253.57672 Test MSE 282.5401497171709 Test RE 0.28296274254111564 Lambda1 0.0001972282\n",
      "43 Train Loss 253.42976 Test MSE 282.8834104290663 Test RE 0.28313457742796455 Lambda1 0.00021181918\n",
      "44 Train Loss 253.31142 Test MSE 283.2088905996696 Test RE 0.2832974151811818 Lambda1 0.0002578204\n",
      "45 Train Loss 253.23209 Test MSE 283.05929010108446 Test RE 0.2832225816673502 Lambda1 0.0002606988\n",
      "46 Train Loss 253.1697 Test MSE 283.2042623551488 Test RE 0.2832951003261226 Lambda1 0.0002982019\n",
      "47 Train Loss 253.00241 Test MSE 283.30790767122426 Test RE 0.28334693486422935 Lambda1 0.00030945317\n",
      "48 Train Loss 252.8903 Test MSE 283.25708266355576 Test RE 0.2833215177196853 Lambda1 0.00025462746\n",
      "49 Train Loss 252.71143 Test MSE 283.53140428499495 Test RE 0.28345867653163376 Lambda1 0.0002629283\n",
      "50 Train Loss 252.56073 Test MSE 283.8155753517958 Test RE 0.28360069004455424 Lambda1 0.00020523273\n",
      "51 Train Loss 252.30038 Test MSE 283.7069070566117 Test RE 0.28354639183701 Lambda1 0.00016091169\n",
      "52 Train Loss 252.1141 Test MSE 283.8884577406515 Test RE 0.2836371013108512 Lambda1 0.00015769573\n",
      "53 Train Loss 251.80754 Test MSE 283.9043539278939 Test RE 0.28364504225602505 Lambda1 0.00013661373\n",
      "54 Train Loss 251.10765 Test MSE 284.0270036663895 Test RE 0.2837063044967828 Lambda1 5.779038e-05\n",
      "55 Train Loss 250.72694 Test MSE 284.2760743788489 Test RE 0.2838306719789651 Lambda1 4.544728e-05\n",
      "56 Train Loss 250.48872 Test MSE 284.5950635144709 Test RE 0.2839898720039676 Lambda1 1.4475223e-05\n",
      "57 Train Loss 250.20708 Test MSE 286.0614037084353 Test RE 0.28472054305104694 Lambda1 6.553878e-06\n",
      "58 Train Loss 250.02753 Test MSE 285.95757480676286 Test RE 0.2846688672502405 Lambda1 2.6162527e-06\n",
      "59 Train Loss 249.80006 Test MSE 285.8060210239937 Test RE 0.2845934218603564 Lambda1 7.8204785e-06\n",
      "60 Train Loss 249.61139 Test MSE 286.2861145522367 Test RE 0.2848323498747052 Lambda1 6.564609e-06\n",
      "61 Train Loss 249.32382 Test MSE 286.93095932713163 Test RE 0.2851529545621517 Lambda1 2.209315e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 Train Loss 249.14932 Test MSE 287.3509836551266 Test RE 0.28536158903408654 Lambda1 6.371757e-06\n",
      "63 Train Loss 249.09085 Test MSE 287.4335305516934 Test RE 0.28540257379393263 Lambda1 9.91373e-06\n",
      "64 Train Loss 249.02496 Test MSE 287.94102045076363 Test RE 0.285654414711574 Lambda1 7.450001e-06\n",
      "65 Train Loss 248.98506 Test MSE 288.2419298827239 Test RE 0.28580363565689254 Lambda1 6.6713415e-06\n",
      "66 Train Loss 248.95676 Test MSE 288.4572135787343 Test RE 0.285910347020118 Lambda1 6.9222665e-06\n",
      "67 Train Loss 248.9161 Test MSE 288.7081281470921 Test RE 0.2860346695784648 Lambda1 6.907635e-06\n",
      "68 Train Loss 248.8629 Test MSE 288.71363481101173 Test RE 0.28603739740136036 Lambda1 9.19515e-06\n",
      "69 Train Loss 248.77501 Test MSE 288.63619364668705 Test RE 0.2859990331677231 Lambda1 9.521203e-06\n",
      "70 Train Loss 248.73132 Test MSE 288.81919249239087 Test RE 0.2860896822288448 Lambda1 1.0482634e-05\n",
      "71 Train Loss 248.68498 Test MSE 288.3472273269879 Test RE 0.28585583424658095 Lambda1 8.746669e-06\n",
      "72 Train Loss 248.65552 Test MSE 288.1551132616824 Test RE 0.28576059130425 Lambda1 8.924416e-06\n",
      "73 Train Loss 248.60193 Test MSE 288.10096270318047 Test RE 0.28573373975459476 Lambda1 8.454957e-06\n",
      "74 Train Loss 248.55447 Test MSE 288.11645846560725 Test RE 0.2857414238709063 Lambda1 1.009498e-05\n",
      "Training time: 122.91\n",
      "Training time: 122.91\n",
      "inv_HT_stan_tune10\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 837.7652 Test MSE 857.3428925271995 Test RE 0.49290865788805643 Lambda1 -0.08811952\n",
      "1 Train Loss 837.45337 Test MSE 857.698426794912 Test RE 0.49301085022889346 Lambda1 -0.07766578\n",
      "2 Train Loss 825.64984 Test MSE 842.8106687598988 Test RE 0.48871332775948445 Lambda1 0.20169301\n",
      "3 Train Loss 729.1226 Test MSE 726.7856485466249 Test RE 0.4538291023393345 Lambda1 0.46296963\n",
      "4 Train Loss 599.26495 Test MSE 563.2345305299523 Test RE 0.399515659675751 Lambda1 0.5409018\n",
      "5 Train Loss 493.1698 Test MSE 465.0478896438961 Test RE 0.36302623336284606 Lambda1 0.54582584\n",
      "6 Train Loss 406.58206 Test MSE 396.459969937654 Test RE 0.33518829354471047 Lambda1 0.5319905\n",
      "7 Train Loss 332.5163 Test MSE 328.7855102771872 Test RE 0.3052428539835097 Lambda1 0.52305675\n",
      "8 Train Loss 285.47897 Test MSE 286.3415244002953 Test RE 0.2848599127791482 Lambda1 0.5315951\n",
      "9 Train Loss 253.32372 Test MSE 260.50062309161683 Test RE 0.27170244955066564 Lambda1 0.52845114\n",
      "10 Train Loss 230.25317 Test MSE 243.34073886942147 Test RE 0.2626011247128986 Lambda1 0.5133132\n",
      "11 Train Loss 212.86212 Test MSE 220.7670468233931 Test RE 0.25012453526487766 Lambda1 0.49586836\n",
      "12 Train Loss 199.34396 Test MSE 210.83888897512685 Test RE 0.2444356408644373 Lambda1 0.4844365\n",
      "13 Train Loss 187.02878 Test MSE 204.150811911277 Test RE 0.24052749392907186 Lambda1 0.47403038\n",
      "14 Train Loss 176.66649 Test MSE 185.7421171125073 Test RE 0.229426915489581 Lambda1 0.45752263\n",
      "15 Train Loss 170.85333 Test MSE 179.48611652090935 Test RE 0.2255301460362876 Lambda1 0.45900717\n",
      "16 Train Loss 164.74146 Test MSE 173.63542742247367 Test RE 0.22182390218557763 Lambda1 0.4572021\n",
      "17 Train Loss 161.01872 Test MSE 171.2085658156444 Test RE 0.22026825703345984 Lambda1 0.45426938\n",
      "18 Train Loss 157.69972 Test MSE 166.09353967598918 Test RE 0.21695294057819267 Lambda1 0.45793825\n",
      "19 Train Loss 154.77351 Test MSE 161.3840942160218 Test RE 0.21385506229167672 Lambda1 0.46840733\n",
      "20 Train Loss 150.24997 Test MSE 155.15788237599492 Test RE 0.20968921431977694 Lambda1 0.4942484\n",
      "21 Train Loss 146.7816 Test MSE 149.201526777662 Test RE 0.2056249474369153 Lambda1 0.511988\n",
      "22 Train Loss 142.53026 Test MSE 145.7540975475665 Test RE 0.20323549380190764 Lambda1 0.53076375\n",
      "23 Train Loss 138.05656 Test MSE 141.0272849335322 Test RE 0.1999128649572122 Lambda1 0.55554354\n",
      "24 Train Loss 135.08957 Test MSE 136.15476189562793 Test RE 0.19642899205683034 Lambda1 0.5675951\n",
      "25 Train Loss 129.71211 Test MSE 132.7182741418142 Test RE 0.19393425822627272 Lambda1 0.5779772\n",
      "26 Train Loss 127.662056 Test MSE 131.37109437166922 Test RE 0.192947466091953 Lambda1 0.5808658\n",
      "27 Train Loss 124.32865 Test MSE 127.80416509064432 Test RE 0.1903100285487747 Lambda1 0.5883305\n",
      "28 Train Loss 120.967636 Test MSE 124.55362299669348 Test RE 0.18787429028544075 Lambda1 0.5997501\n",
      "29 Train Loss 117.80174 Test MSE 119.71085283466223 Test RE 0.18418571020259156 Lambda1 0.61159766\n",
      "30 Train Loss 115.066826 Test MSE 115.91040984709109 Test RE 0.1812384717164779 Lambda1 0.6190528\n",
      "31 Train Loss 112.73161 Test MSE 114.64211088366118 Test RE 0.18024418298901668 Lambda1 0.62828076\n",
      "32 Train Loss 109.83176 Test MSE 113.03876063286233 Test RE 0.17897932433502156 Lambda1 0.63726765\n",
      "33 Train Loss 107.74765 Test MSE 111.48697818726468 Test RE 0.17774657569086647 Lambda1 0.6443445\n",
      "34 Train Loss 106.6923 Test MSE 110.0985245492896 Test RE 0.17663628418984273 Lambda1 0.647396\n",
      "35 Train Loss 104.64035 Test MSE 106.73110458068714 Test RE 0.17391405155086748 Lambda1 0.652412\n",
      "36 Train Loss 102.84666 Test MSE 104.31322584066665 Test RE 0.1719328485803697 Lambda1 0.6572459\n",
      "37 Train Loss 101.515274 Test MSE 102.78852669927768 Test RE 0.17067169091484646 Lambda1 0.6610533\n",
      "38 Train Loss 100.21317 Test MSE 101.70181902863467 Test RE 0.16976710039100734 Lambda1 0.6664843\n",
      "39 Train Loss 98.01311 Test MSE 98.76258259271567 Test RE 0.16729593543855703 Lambda1 0.6754927\n",
      "40 Train Loss 96.19441 Test MSE 96.31934999666382 Test RE 0.16521365615881992 Lambda1 0.68066454\n",
      "41 Train Loss 94.3249 Test MSE 94.03512317382926 Test RE 0.16324286922661704 Lambda1 0.68683517\n",
      "42 Train Loss 92.12033 Test MSE 92.02443154892141 Test RE 0.1614881808740304 Lambda1 0.695076\n",
      "43 Train Loss 90.378944 Test MSE 89.676123043279 Test RE 0.1594144124108605 Lambda1 0.70205784\n",
      "44 Train Loss 87.20678 Test MSE 85.82509431022574 Test RE 0.1559539271104438 Lambda1 0.71552813\n",
      "45 Train Loss 84.162254 Test MSE 82.56495706072417 Test RE 0.15296323163673625 Lambda1 0.7318199\n",
      "46 Train Loss 82.015495 Test MSE 79.4265539909579 Test RE 0.15002790021403364 Lambda1 0.74194664\n",
      "47 Train Loss 80.25119 Test MSE 77.48844531770463 Test RE 0.14818616008680102 Lambda1 0.74915135\n",
      "48 Train Loss 78.4839 Test MSE 75.58023385839351 Test RE 0.1463501885574684 Lambda1 0.75435007\n",
      "49 Train Loss 77.13443 Test MSE 74.31841969287679 Test RE 0.1451233863400051 Lambda1 0.7573009\n",
      "50 Train Loss 74.89033 Test MSE 72.73132603493116 Test RE 0.14356544567243879 Lambda1 0.7657392\n",
      "51 Train Loss 72.98504 Test MSE 70.78633988540913 Test RE 0.1416328189703892 Lambda1 0.77479064\n",
      "52 Train Loss 71.7879 Test MSE 69.68534481330094 Test RE 0.14052703958971807 Lambda1 0.7835344\n",
      "53 Train Loss 69.47685 Test MSE 66.84653000800412 Test RE 0.13763491045474907 Lambda1 0.80227375\n",
      "54 Train Loss 67.762146 Test MSE 63.99957703839804 Test RE 0.13467212797887118 Lambda1 0.81344706\n",
      "55 Train Loss 66.55623 Test MSE 62.384022173044414 Test RE 0.13296148493416216 Lambda1 0.8255147\n",
      "56 Train Loss 64.98926 Test MSE 61.062889331644634 Test RE 0.13154606025088417 Lambda1 0.83933914\n",
      "57 Train Loss 63.652336 Test MSE 58.92658283614636 Test RE 0.12922448175391588 Lambda1 0.8546064\n",
      "58 Train Loss 62.53912 Test MSE 57.55554298875933 Test RE 0.12771230649795093 Lambda1 0.86351806\n",
      "59 Train Loss 59.929413 Test MSE 55.59013892469647 Test RE 0.12551280930221623 Lambda1 0.8729429\n",
      "60 Train Loss 58.436966 Test MSE 54.284831400149365 Test RE 0.12403047796123563 Lambda1 0.88211054\n",
      "61 Train Loss 57.53678 Test MSE 52.94195913781915 Test RE 0.1224867678898116 Lambda1 0.89197797\n",
      "62 Train Loss 56.52938 Test MSE 51.59676959605124 Test RE 0.1209206370658761 Lambda1 0.9005349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 Train Loss 54.93101 Test MSE 50.280940293077805 Test RE 0.1193688104507075 Lambda1 0.91101915\n",
      "64 Train Loss 53.3089 Test MSE 49.213190596496744 Test RE 0.1180945706951617 Lambda1 0.9195012\n",
      "65 Train Loss 52.21225 Test MSE 48.35132004209649 Test RE 0.11705590803890864 Lambda1 0.923683\n",
      "66 Train Loss 50.739773 Test MSE 47.45658531077923 Test RE 0.11596779871750086 Lambda1 0.92873377\n",
      "67 Train Loss 49.951035 Test MSE 46.763880715291144 Test RE 0.11511831990367481 Lambda1 0.9340662\n",
      "68 Train Loss 48.05138 Test MSE 44.33947379565615 Test RE 0.11209453455486453 Lambda1 0.94478446\n",
      "69 Train Loss 47.389076 Test MSE 44.24363939574582 Test RE 0.1119733296501436 Lambda1 0.9477852\n",
      "70 Train Loss 46.79519 Test MSE 43.953001900733966 Test RE 0.11160494597123982 Lambda1 0.95218766\n",
      "71 Train Loss 45.373833 Test MSE 42.87901976469994 Test RE 0.11023299187909726 Lambda1 0.9567342\n",
      "72 Train Loss 44.69937 Test MSE 42.22974387835354 Test RE 0.10939523217870901 Lambda1 0.95797217\n",
      "73 Train Loss 43.61187 Test MSE 41.94756035531978 Test RE 0.10902912442446264 Lambda1 0.96126884\n",
      "74 Train Loss 42.70489 Test MSE 41.557901026015074 Test RE 0.10852154618673876 Lambda1 0.96667725\n",
      "Training time: 123.02\n",
      "Training time: 123.02\n",
      "inv_HT_stan_tune10\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 838.8707 Test MSE 859.921400381729 Test RE 0.4936493269152665 Lambda1 0.024191087\n",
      "1 Train Loss 837.81177 Test MSE 857.8737773504012 Test RE 0.4930612439953314 Lambda1 0.024075681\n",
      "2 Train Loss 837.2289 Test MSE 856.229417754245 Test RE 0.492588471135144 Lambda1 0.035056524\n",
      "3 Train Loss 816.87976 Test MSE 830.6618046752334 Test RE 0.4851782137811826 Lambda1 0.30755064\n",
      "4 Train Loss 742.42456 Test MSE 738.2912676801443 Test RE 0.4574072421149442 Lambda1 0.44911987\n",
      "5 Train Loss 650.0792 Test MSE 651.7617709970601 Test RE 0.4297675418294577 Lambda1 0.47888297\n",
      "6 Train Loss 623.27795 Test MSE 624.3825401309175 Test RE 0.4206438519502565 Lambda1 0.49423477\n",
      "7 Train Loss 591.5066 Test MSE 585.1560221271731 Test RE 0.40721616548218337 Lambda1 0.5115662\n",
      "8 Train Loss 570.3634 Test MSE 573.9628188160833 Test RE 0.40330262667260247 Lambda1 0.4881398\n",
      "9 Train Loss 541.8933 Test MSE 551.0936468322769 Test RE 0.3951862930998339 Lambda1 0.4819682\n",
      "10 Train Loss 519.0469 Test MSE 523.9528608748469 Test RE 0.3853321792538713 Lambda1 0.4796108\n",
      "11 Train Loss 479.77173 Test MSE 486.83627441166493 Test RE 0.3714331280403537 Lambda1 0.46114796\n",
      "12 Train Loss 414.84326 Test MSE 404.6811139821151 Test RE 0.3386457573746707 Lambda1 0.46796715\n",
      "13 Train Loss 360.66537 Test MSE 355.5428381750988 Test RE 0.31742062182580666 Lambda1 0.47109315\n",
      "14 Train Loss 317.3938 Test MSE 333.33750770163124 Test RE 0.30734861656086704 Lambda1 0.46078223\n",
      "15 Train Loss 279.60138 Test MSE 300.2155830670265 Test RE 0.2916794189620581 Lambda1 0.4547299\n",
      "16 Train Loss 264.14188 Test MSE 290.7862234061147 Test RE 0.2870622497582336 Lambda1 0.45296156\n",
      "17 Train Loss 248.14029 Test MSE 277.20058969655105 Test RE 0.28027621605173253 Lambda1 0.44027945\n",
      "18 Train Loss 240.6098 Test MSE 269.311726028571 Test RE 0.2762592333692693 Lambda1 0.4255352\n",
      "19 Train Loss 236.97491 Test MSE 266.75607317873346 Test RE 0.2749453179570461 Lambda1 0.41570607\n",
      "20 Train Loss 231.01593 Test MSE 260.5736780534342 Test RE 0.2717405450845759 Lambda1 0.39205387\n",
      "21 Train Loss 227.6042 Test MSE 255.45178362813982 Test RE 0.26905659411368915 Lambda1 0.3593799\n",
      "22 Train Loss 223.4418 Test MSE 252.10237889051032 Test RE 0.26728688058743016 Lambda1 0.3518307\n",
      "23 Train Loss 221.38405 Test MSE 250.30961851426554 Test RE 0.26633481447336294 Lambda1 0.35570607\n",
      "24 Train Loss 219.57207 Test MSE 248.8479036381962 Test RE 0.2655560278289572 Lambda1 0.35343733\n",
      "25 Train Loss 217.73456 Test MSE 247.49745113650542 Test RE 0.2648344853443927 Lambda1 0.33681333\n",
      "26 Train Loss 216.25987 Test MSE 245.66038145002756 Test RE 0.2638497770650644 Lambda1 0.3210515\n",
      "27 Train Loss 214.97441 Test MSE 244.19195938071255 Test RE 0.2630600210000956 Lambda1 0.3092987\n",
      "28 Train Loss 213.57927 Test MSE 242.14098075884982 Test RE 0.2619529654110235 Lambda1 0.29107344\n",
      "29 Train Loss 212.1791 Test MSE 240.56017793405675 Test RE 0.26109649317791184 Lambda1 0.2749655\n",
      "30 Train Loss 211.3174 Test MSE 239.72942691560945 Test RE 0.26064526768914176 Lambda1 0.26904184\n",
      "31 Train Loss 209.22032 Test MSE 236.05531169972866 Test RE 0.2586402189796283 Lambda1 0.2665907\n",
      "32 Train Loss 206.83212 Test MSE 229.8115245493411 Test RE 0.25519671081690887 Lambda1 0.26848236\n",
      "33 Train Loss 201.37328 Test MSE 219.28292923759463 Test RE 0.24928238013071546 Lambda1 0.27719358\n",
      "34 Train Loss 195.90741 Test MSE 210.64460320877026 Test RE 0.24432299250162817 Lambda1 0.29359987\n",
      "35 Train Loss 187.78508 Test MSE 200.14284959686492 Test RE 0.2381547292368834 Lambda1 0.30359256\n",
      "36 Train Loss 178.56914 Test MSE 186.66836540849715 Test RE 0.22999825069009988 Lambda1 0.3097758\n",
      "37 Train Loss 173.81073 Test MSE 178.30792016198026 Test RE 0.22478870624688313 Lambda1 0.32201898\n",
      "38 Train Loss 168.51822 Test MSE 172.9790930491821 Test RE 0.2214042628982145 Lambda1 0.33641884\n",
      "39 Train Loss 164.01326 Test MSE 167.21068900527294 Test RE 0.21768133330391812 Lambda1 0.36097524\n",
      "40 Train Loss 159.55746 Test MSE 160.44681952165382 Test RE 0.21323315214047406 Lambda1 0.38727143\n",
      "41 Train Loss 153.30019 Test MSE 153.7487038897773 Test RE 0.20873482027070187 Lambda1 0.42428067\n",
      "42 Train Loss 145.93259 Test MSE 145.41664981365363 Test RE 0.20300009356739754 Lambda1 0.4714344\n",
      "43 Train Loss 141.25458 Test MSE 139.83425047419325 Test RE 0.19906547754685983 Lambda1 0.49255866\n",
      "44 Train Loss 136.64981 Test MSE 136.61912434467155 Test RE 0.19676367226275365 Lambda1 0.5040982\n",
      "45 Train Loss 133.43555 Test MSE 134.39778446963822 Test RE 0.1951574906910814 Lambda1 0.5178379\n",
      "46 Train Loss 131.09009 Test MSE 132.12492329686754 Test RE 0.19350025635784568 Lambda1 0.5273216\n",
      "47 Train Loss 128.98378 Test MSE 129.228186083 Test RE 0.19136732880196577 Lambda1 0.5351064\n",
      "48 Train Loss 125.6993 Test MSE 124.60306707455993 Test RE 0.18791157683362167 Lambda1 0.55268353\n",
      "49 Train Loss 123.58563 Test MSE 122.71978646195444 Test RE 0.18648609977621994 Lambda1 0.5609284\n",
      "50 Train Loss 118.13402 Test MSE 118.98013334079548 Test RE 0.18362271153683885 Lambda1 0.5902019\n",
      "51 Train Loss 113.272675 Test MSE 113.3478484973691 Test RE 0.1792238536102994 Lambda1 0.6036114\n",
      "52 Train Loss 111.404526 Test MSE 110.82968732699152 Test RE 0.17722183320207827 Lambda1 0.6055363\n",
      "53 Train Loss 108.626076 Test MSE 109.27248127414734 Test RE 0.17597240641568435 Lambda1 0.6318191\n",
      "54 Train Loss 106.21492 Test MSE 105.21698895925404 Test RE 0.17267604989300597 Lambda1 0.6574058\n",
      "55 Train Loss 103.46384 Test MSE 103.6451340596507 Test RE 0.17138137759179906 Lambda1 0.6765851\n",
      "56 Train Loss 100.0879 Test MSE 99.97838219049902 Test RE 0.1683225194663057 Lambda1 0.708009\n",
      "57 Train Loss 98.20099 Test MSE 96.73209820591583 Test RE 0.16556726497695753 Lambda1 0.7276333\n",
      "58 Train Loss 96.28317 Test MSE 93.80002952151378 Test RE 0.16303868286771547 Lambda1 0.7422097\n",
      "59 Train Loss 92.6381 Test MSE 90.4243561090779 Test RE 0.160078086056515 Lambda1 0.76985914\n",
      "60 Train Loss 91.00706 Test MSE 90.8348016615771 Test RE 0.16044098025595194 Lambda1 0.78285486\n",
      "61 Train Loss 89.81666 Test MSE 89.026625959326 Test RE 0.1588360680853649 Lambda1 0.80237794\n",
      "62 Train Loss 88.23934 Test MSE 86.29175906527024 Test RE 0.15637734370504253 Lambda1 0.8210654\n",
      "63 Train Loss 85.001434 Test MSE 82.84674083124548 Test RE 0.15322403140229035 Lambda1 0.842546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 Train Loss 83.76014 Test MSE 81.50981763870895 Test RE 0.151982691537125 Lambda1 0.8501354\n",
      "65 Train Loss 81.4149 Test MSE 78.89691607584757 Test RE 0.1495268500626863 Lambda1 0.8642334\n",
      "66 Train Loss 79.058784 Test MSE 77.08028545132277 Test RE 0.14779536955760447 Lambda1 0.88536596\n",
      "67 Train Loss 78.331345 Test MSE 76.28342752620726 Test RE 0.14702942871633903 Lambda1 0.8947696\n",
      "68 Train Loss 77.18234 Test MSE 74.02150188371543 Test RE 0.14483319689281518 Lambda1 0.90442723\n",
      "69 Train Loss 75.58123 Test MSE 72.54845932792384 Test RE 0.14338485042866045 Lambda1 0.91173285\n",
      "70 Train Loss 74.40555 Test MSE 71.39917400111983 Test RE 0.1422445921582115 Lambda1 0.92624724\n",
      "71 Train Loss 71.24882 Test MSE 68.56914785041947 Test RE 0.13939703834614914 Lambda1 0.9519418\n",
      "72 Train Loss 69.218735 Test MSE 65.78597830391051 Test RE 0.13653872383030774 Lambda1 0.9641694\n",
      "73 Train Loss 68.07426 Test MSE 65.14480718200709 Test RE 0.1358717196716743 Lambda1 0.9771986\n",
      "74 Train Loss 67.009575 Test MSE 64.53206825427614 Test RE 0.13523121900182253 Lambda1 0.9825982\n",
      "Training time: 122.75\n",
      "Training time: 122.75\n",
      "inv_HT_stan_tune10\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 837.80176 Test MSE 857.4309450655692 Test RE 0.49293396908183945 Lambda1 -0.020464204\n",
      "1 Train Loss 837.45416 Test MSE 857.3046932397883 Test RE 0.49289767688536207 Lambda1 -0.018989427\n",
      "2 Train Loss 834.3189 Test MSE 851.2209934611255 Test RE 0.49114568585261525 Lambda1 0.03329571\n",
      "3 Train Loss 808.5717 Test MSE 822.5974119489564 Test RE 0.4828173188623207 Lambda1 0.086794905\n",
      "4 Train Loss 741.54364 Test MSE 748.2185765681935 Test RE 0.46047219804506073 Lambda1 0.083530724\n",
      "5 Train Loss 706.4631 Test MSE 710.4639961329228 Test RE 0.4487042755209234 Lambda1 0.03567461\n",
      "6 Train Loss 637.8708 Test MSE 639.8590898140097 Test RE 0.42582518383489737 Lambda1 0.01092257\n",
      "7 Train Loss 567.53827 Test MSE 560.9036474196848 Test RE 0.3986881271883739 Lambda1 0.0052005914\n",
      "8 Train Loss 425.3316 Test MSE 410.41396568981366 Test RE 0.3410360079452064 Lambda1 0.0009521114\n",
      "9 Train Loss 308.41553 Test MSE 317.92265400519153 Test RE 0.3001579898557897 Lambda1 0.0008334162\n",
      "10 Train Loss 266.16455 Test MSE 289.57747435411375 Test RE 0.2864649938766916 Lambda1 0.0004166247\n",
      "11 Train Loss 256.7201 Test MSE 284.3238003183751 Test RE 0.2838544965604301 Lambda1 0.00038995026\n",
      "12 Train Loss 255.85258 Test MSE 284.0555607455127 Test RE 0.2837205665557498 Lambda1 0.0003137441\n",
      "13 Train Loss 254.9978 Test MSE 282.8165878425216 Test RE 0.2831011344943318 Lambda1 0.000283662\n",
      "14 Train Loss 254.61623 Test MSE 282.50386851559017 Test RE 0.2829445742243172 Lambda1 0.00021436437\n",
      "15 Train Loss 254.32297 Test MSE 282.7257143345314 Test RE 0.2830556483714851 Lambda1 9.613188e-05\n",
      "16 Train Loss 254.19795 Test MSE 282.82367272805965 Test RE 0.28310468047900594 Lambda1 9.405402e-05\n",
      "17 Train Loss 254.13995 Test MSE 282.67562873691776 Test RE 0.28303057523784886 Lambda1 0.00011402331\n",
      "18 Train Loss 254.0909 Test MSE 282.70990906943035 Test RE 0.283047736406177 Lambda1 9.349615e-05\n",
      "19 Train Loss 254.08029 Test MSE 282.77381724188984 Test RE 0.2830797268684919 Lambda1 8.581993e-05\n",
      "20 Train Loss 254.0473 Test MSE 282.90092181756137 Test RE 0.2831433407606152 Lambda1 8.140057e-05\n",
      "21 Train Loss 254.00253 Test MSE 283.1181257627643 Test RE 0.2832520149393885 Lambda1 7.312572e-05\n",
      "22 Train Loss 253.98117 Test MSE 283.1088841250662 Test RE 0.2832473918975845 Lambda1 7.706098e-05\n",
      "23 Train Loss 253.96283 Test MSE 283.002361525047 Test RE 0.2831940995266742 Lambda1 8.757914e-05\n",
      "24 Train Loss 253.94942 Test MSE 282.9167156222881 Test RE 0.28315124431955757 Lambda1 0.00010167523\n",
      "25 Train Loss 253.8962 Test MSE 282.9843987782048 Test RE 0.2831851119256311 Lambda1 0.00020547623\n",
      "26 Train Loss 253.84177 Test MSE 283.14941676353027 Test RE 0.2832676674061965 Lambda1 0.00031269173\n",
      "27 Train Loss 253.74323 Test MSE 283.2130394008624 Test RE 0.28329949022258866 Lambda1 0.00041285623\n",
      "28 Train Loss 253.70389 Test MSE 283.04328512199527 Test RE 0.2832145744481309 Lambda1 0.0004498289\n",
      "29 Train Loss 253.6776 Test MSE 282.92687261918684 Test RE 0.28315632698229826 Lambda1 0.0004608218\n",
      "30 Train Loss 253.65251 Test MSE 282.9324263117167 Test RE 0.2831591060669541 Lambda1 0.0004114113\n",
      "31 Train Loss 253.58551 Test MSE 282.9322253145575 Test RE 0.28315900548783923 Lambda1 0.00041855563\n",
      "32 Train Loss 253.54778 Test MSE 283.10032543028314 Test RE 0.28324311042421274 Lambda1 0.00040034653\n",
      "33 Train Loss 253.50531 Test MSE 283.0410759867995 Test RE 0.28321346920992524 Lambda1 0.0004043059\n",
      "34 Train Loss 253.46452 Test MSE 283.03140554568574 Test RE 0.2832086310030305 Lambda1 0.0005190203\n",
      "35 Train Loss 253.44987 Test MSE 283.0147218443147 Test RE 0.2832002838059265 Lambda1 0.00053580006\n",
      "36 Train Loss 253.42029 Test MSE 283.08431202216013 Test RE 0.28323509956859144 Lambda1 0.00052955473\n",
      "37 Train Loss 253.36853 Test MSE 283.1095699065449 Test RE 0.28324773495587147 Lambda1 0.000531169\n",
      "38 Train Loss 253.29765 Test MSE 282.9751358434457 Test RE 0.28318047713534955 Lambda1 0.0005997391\n",
      "39 Train Loss 253.24228 Test MSE 282.81369575052486 Test RE 0.283099686989741 Lambda1 0.0006653532\n",
      "40 Train Loss 253.21602 Test MSE 282.65141133693425 Test RE 0.2830184510736792 Lambda1 0.00069958525\n",
      "41 Train Loss 253.12598 Test MSE 282.99356811364237 Test RE 0.28318969980796604 Lambda1 0.0006087578\n",
      "42 Train Loss 253.05948 Test MSE 283.3016128938017 Test RE 0.2833437870244629 Lambda1 0.0005409729\n",
      "43 Train Loss 253.01947 Test MSE 283.08056931509805 Test RE 0.2832332272120774 Lambda1 0.00053657417\n",
      "44 Train Loss 252.99161 Test MSE 282.9705320195492 Test RE 0.28317817354364627 Lambda1 0.00058059976\n",
      "45 Train Loss 252.87329 Test MSE 282.8881616461895 Test RE 0.2831369551358018 Lambda1 0.0005500518\n",
      "46 Train Loss 252.749 Test MSE 283.100366078321 Test RE 0.2832431307584815 Lambda1 0.00040451981\n",
      "47 Train Loss 252.62456 Test MSE 283.4171635069236 Test RE 0.2834015650409349 Lambda1 0.0003184697\n",
      "48 Train Loss 252.40318 Test MSE 283.7145521249388 Test RE 0.2835502121827299 Lambda1 0.00020872666\n",
      "49 Train Loss 252.06651 Test MSE 283.8175070211636 Test RE 0.28360165514633345 Lambda1 0.00014722117\n",
      "50 Train Loss 251.9722 Test MSE 283.82407307510107 Test RE 0.2836049356574866 Lambda1 0.00012442465\n",
      "51 Train Loss 251.7815 Test MSE 284.0287169799937 Test RE 0.28370716018503045 Lambda1 8.2216466e-05\n",
      "52 Train Loss 251.61324 Test MSE 284.2971660865362 Test RE 0.28384120111431793 Lambda1 6.8638714e-05\n",
      "53 Train Loss 251.4961 Test MSE 283.88298160763446 Test RE 0.28363436565544947 Lambda1 0.00011936834\n",
      "54 Train Loss 251.3046 Test MSE 284.0343979048314 Test RE 0.283709997417496 Lambda1 0.00011264617\n",
      "55 Train Loss 250.91544 Test MSE 284.55726073705375 Test RE 0.2839710101826943 Lambda1 3.2174827e-05\n",
      "56 Train Loss 250.66595 Test MSE 284.36420047678797 Test RE 0.283874662581132 Lambda1 4.989523e-05\n",
      "57 Train Loss 250.40643 Test MSE 284.6426743843181 Test RE 0.28401362582320216 Lambda1 3.0962245e-05\n",
      "58 Train Loss 250.10307 Test MSE 284.9876118428238 Test RE 0.28418566130018696 Lambda1 3.753551e-05\n",
      "59 Train Loss 249.83325 Test MSE 284.50360552488274 Test RE 0.2839442365852333 Lambda1 7.973808e-06\n",
      "60 Train Loss 249.54097 Test MSE 285.1212770914916 Test RE 0.2842522980459974 Lambda1 1.9075387e-05\n",
      "61 Train Loss 249.37097 Test MSE 285.26342997097566 Test RE 0.28432314903241096 Lambda1 1.3470255e-06\n",
      "62 Train Loss 249.29733 Test MSE 285.30472024431464 Test RE 0.2843437253743474 Lambda1 7.046385e-06\n",
      "63 Train Loss 249.21677 Test MSE 285.24245812303116 Test RE 0.28431269747988386 Lambda1 4.0617733e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 Train Loss 249.10083 Test MSE 285.28047141697294 Test RE 0.28433164154270096 Lambda1 4.6945015e-06\n",
      "65 Train Loss 249.02376 Test MSE 285.575882630268 Test RE 0.2844788177963999 Lambda1 8.1897715e-06\n",
      "66 Train Loss 248.95827 Test MSE 285.8378941331121 Test RE 0.2846092903579387 Lambda1 2.2015365e-06\n",
      "67 Train Loss 248.8439 Test MSE 286.35836305659245 Test RE 0.28486828842019074 Lambda1 4.7606172e-06\n",
      "68 Train Loss 248.70557 Test MSE 286.6591618474829 Test RE 0.2850178659379238 Lambda1 6.155772e-06\n",
      "69 Train Loss 248.65457 Test MSE 286.8794625971143 Test RE 0.28512736460197724 Lambda1 1.7071602e-07\n",
      "70 Train Loss 248.63054 Test MSE 286.9139198434678 Test RE 0.28514448748829396 Lambda1 4.2365227e-06\n",
      "71 Train Loss 248.59393 Test MSE 286.83385667726 Test RE 0.2851047000088744 Lambda1 3.5373473e-06\n",
      "72 Train Loss 248.50862 Test MSE 286.787567570416 Test RE 0.2850816940524692 Lambda1 3.176609e-06\n",
      "73 Train Loss 248.4649 Test MSE 286.5709735971435 Test RE 0.28497402090571267 Lambda1 5.9246013e-06\n",
      "74 Train Loss 248.43065 Test MSE 286.5566412948119 Test RE 0.2849668945997198 Lambda1 2.0860898e-06\n",
      "Training time: 123.77\n",
      "Training time: 123.77\n",
      "inv_HT_stan_tune10\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 837.4489 Test MSE 857.2721595745215 Test RE 0.49288832436443547 Lambda1 -0.088097975\n",
      "1 Train Loss 836.31256 Test MSE 855.9252341683379 Test RE 0.49250096500970325 Lambda1 -0.077685386\n",
      "2 Train Loss 823.0983 Test MSE 838.7300881799048 Test RE 0.4875288067944269 Lambda1 0.008641731\n",
      "3 Train Loss 724.20514 Test MSE 717.0466941587603 Test RE 0.4507781839432017 Lambda1 0.13802074\n",
      "4 Train Loss 513.94763 Test MSE 509.19201583593497 Test RE 0.3798655973065348 Lambda1 0.015880983\n",
      "5 Train Loss 385.9648 Test MSE 400.6852324117729 Test RE 0.3369696903629515 Lambda1 0.008063346\n",
      "6 Train Loss 275.16327 Test MSE 292.7235383024506 Test RE 0.2880169145942299 Lambda1 0.00185709\n",
      "7 Train Loss 268.49298 Test MSE 288.0330669261079 Test RE 0.2857000688198659 Lambda1 0.00071544864\n",
      "8 Train Loss 262.9184 Test MSE 284.30077134764224 Test RE 0.28384300084804814 Lambda1 0.00039120202\n",
      "9 Train Loss 259.94525 Test MSE 282.4264591644802 Test RE 0.28290580651362995 Lambda1 0.00036989088\n",
      "10 Train Loss 257.38855 Test MSE 280.32418019296705 Test RE 0.28185091626420194 Lambda1 0.0012013906\n",
      "11 Train Loss 256.68356 Test MSE 279.97136606208636 Test RE 0.2816734925852385 Lambda1 0.0015034826\n",
      "12 Train Loss 255.64696 Test MSE 280.14134383219874 Test RE 0.2817589851983284 Lambda1 0.0019480197\n",
      "13 Train Loss 254.70161 Test MSE 280.0063448774379 Test RE 0.28169108777274876 Lambda1 0.0032994899\n",
      "14 Train Loss 254.07942 Test MSE 279.2830266920168 Test RE 0.28132701737809807 Lambda1 0.0047052274\n",
      "15 Train Loss 252.84837 Test MSE 277.7161003149225 Test RE 0.28053671018172804 Lambda1 0.007140667\n",
      "16 Train Loss 251.3637 Test MSE 275.5010318469813 Test RE 0.2794156875317992 Lambda1 0.0115180835\n",
      "17 Train Loss 249.31247 Test MSE 271.98042757158896 Test RE 0.2776246325352555 Lambda1 0.019092007\n",
      "18 Train Loss 245.86777 Test MSE 268.4579105584363 Test RE 0.27582096493697256 Lambda1 0.029684894\n",
      "19 Train Loss 241.75531 Test MSE 260.3662346083524 Test RE 0.2716323568382276 Lambda1 0.04541122\n",
      "20 Train Loss 238.5315 Test MSE 257.7810589142651 Test RE 0.27028047426041685 Lambda1 0.054968297\n",
      "21 Train Loss 231.7259 Test MSE 249.896668571574 Test RE 0.2661150299805508 Lambda1 0.074302934\n",
      "22 Train Loss 223.97697 Test MSE 239.7771472426651 Test RE 0.26067120830615964 Lambda1 0.10671699\n",
      "23 Train Loss 218.44026 Test MSE 236.07497297406454 Test RE 0.258650989951863 Lambda1 0.12449357\n",
      "24 Train Loss 211.44278 Test MSE 229.42822477813866 Test RE 0.2549838022998515 Lambda1 0.14997979\n",
      "25 Train Loss 200.87431 Test MSE 216.0202982758749 Test RE 0.2474209393163183 Lambda1 0.20303643\n",
      "26 Train Loss 187.07971 Test MSE 200.774494216607 Test RE 0.23853023766292808 Lambda1 0.24604194\n",
      "27 Train Loss 169.62393 Test MSE 175.74384911573975 Test RE 0.2231666209408132 Lambda1 0.29346207\n",
      "28 Train Loss 161.3331 Test MSE 161.41292994799173 Test RE 0.21387416699914424 Lambda1 0.32050458\n",
      "29 Train Loss 153.79117 Test MSE 154.2130764507528 Test RE 0.20904980712950627 Lambda1 0.34228945\n",
      "30 Train Loss 144.37198 Test MSE 142.2424371754279 Test RE 0.20077228560996432 Lambda1 0.3838159\n",
      "31 Train Loss 135.97404 Test MSE 128.658026462525 Test RE 0.19094470219715415 Lambda1 0.42626336\n",
      "32 Train Loss 130.8583 Test MSE 125.7880709683257 Test RE 0.1888030036385685 Lambda1 0.44574285\n",
      "33 Train Loss 119.34883 Test MSE 115.77663087858528 Test RE 0.18113385257237888 Lambda1 0.49569494\n",
      "34 Train Loss 110.6717 Test MSE 105.53149641158197 Test RE 0.17293393306489868 Lambda1 0.54031605\n",
      "35 Train Loss 107.90217 Test MSE 104.24083825608214 Test RE 0.17187318230390883 Lambda1 0.53918546\n",
      "36 Train Loss 103.46161 Test MSE 100.02953729788608 Test RE 0.16836557605128466 Lambda1 0.5483618\n",
      "37 Train Loss 98.63937 Test MSE 97.03305648793608 Test RE 0.16582462599341957 Lambda1 0.5654714\n",
      "38 Train Loss 96.028854 Test MSE 96.52374478694821 Test RE 0.16538885934895337 Lambda1 0.5750662\n",
      "39 Train Loss 91.325516 Test MSE 94.10955669301904 Test RE 0.16330746390836967 Lambda1 0.5851983\n",
      "40 Train Loss 89.50029 Test MSE 92.26608570479922 Test RE 0.16170007408393883 Lambda1 0.5900993\n",
      "41 Train Loss 87.90102 Test MSE 89.24519852820667 Test RE 0.15903093071382482 Lambda1 0.60209596\n",
      "42 Train Loss 86.41499 Test MSE 87.20858023677569 Test RE 0.15720587736753208 Lambda1 0.6122319\n",
      "43 Train Loss 83.962395 Test MSE 84.0841216789817 Test RE 0.15436405076244084 Lambda1 0.62729555\n",
      "44 Train Loss 81.529945 Test MSE 81.41200312522064 Test RE 0.15189147200002695 Lambda1 0.64502573\n",
      "45 Train Loss 78.56507 Test MSE 79.1056326685566 Test RE 0.14972450114531463 Lambda1 0.66673696\n",
      "46 Train Loss 76.18713 Test MSE 76.23612970092677 Test RE 0.14698384050186744 Lambda1 0.6846407\n",
      "47 Train Loss 73.72992 Test MSE 73.39424503517309 Test RE 0.14421823439960546 Lambda1 0.7099279\n",
      "48 Train Loss 70.4432 Test MSE 71.02966009346346 Test RE 0.14187603371272617 Lambda1 0.735566\n",
      "49 Train Loss 68.759514 Test MSE 69.52234837548814 Test RE 0.14036259456362443 Lambda1 0.74753463\n",
      "50 Train Loss 67.70622 Test MSE 68.33185461458065 Test RE 0.13915562772117476 Lambda1 0.7626004\n",
      "51 Train Loss 66.04979 Test MSE 66.23309119367102 Test RE 0.13700192927118138 Lambda1 0.78229296\n",
      "52 Train Loss 64.11332 Test MSE 64.41099040942404 Test RE 0.13510429612162866 Lambda1 0.8008408\n",
      "53 Train Loss 62.85942 Test MSE 63.32692794022635 Test RE 0.13396254229794555 Lambda1 0.8139979\n",
      "54 Train Loss 62.128975 Test MSE 62.06167891930755 Test RE 0.1326175287097884 Lambda1 0.82593554\n",
      "55 Train Loss 60.359673 Test MSE 59.578376292574944 Test RE 0.12993719946106633 Lambda1 0.8595093\n",
      "56 Train Loss 59.162827 Test MSE 58.64973207625804 Test RE 0.12892056106045496 Lambda1 0.8796828\n",
      "57 Train Loss 57.986824 Test MSE 57.925421271903 Test RE 0.12812201819409436 Lambda1 0.8918859\n",
      "58 Train Loss 57.246864 Test MSE 57.796692798982164 Test RE 0.12797957533642998 Lambda1 0.9010596\n",
      "59 Train Loss 56.747944 Test MSE 56.954321562556785 Test RE 0.12704351819561846 Lambda1 0.9110219\n",
      "60 Train Loss 56.079243 Test MSE 55.83126083812367 Test RE 0.12578472031864785 Lambda1 0.92107826\n",
      "61 Train Loss 55.766773 Test MSE 55.56145188121786 Test RE 0.12548041996010972 Lambda1 0.92773503\n",
      "62 Train Loss 54.619816 Test MSE 54.57439264105653 Test RE 0.12436083409047347 Lambda1 0.95170933\n",
      "63 Train Loss 53.939495 Test MSE 54.36441511697579 Test RE 0.12412136146480734 Lambda1 0.9612363\n",
      "64 Train Loss 53.58153 Test MSE 54.536101091561385 Test RE 0.1243171981983187 Lambda1 0.96723986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Train Loss 53.113773 Test MSE 53.65926225539769 Test RE 0.12331375408287024 Lambda1 0.97931015\n",
      "66 Train Loss 52.145485 Test MSE 52.30731177983911 Test RE 0.12175039282307194 Lambda1 1.0005283\n",
      "67 Train Loss 51.350048 Test MSE 51.51877177747367 Test RE 0.12082920582848349 Lambda1 1.010122\n",
      "68 Train Loss 50.73449 Test MSE 51.29676285927988 Test RE 0.12056858118548479 Lambda1 1.0095246\n",
      "69 Train Loss 49.96522 Test MSE 50.899639264621285 Test RE 0.12010097216516549 Lambda1 1.0154445\n",
      "70 Train Loss 49.42219 Test MSE 50.328931239426694 Test RE 0.1194257630043705 Lambda1 1.0215093\n",
      "71 Train Loss 49.038586 Test MSE 50.08361176519213 Test RE 0.11913434757717792 Lambda1 1.0263913\n",
      "72 Train Loss 48.5176 Test MSE 50.0505939580938 Test RE 0.11909507122224978 Lambda1 1.0353953\n",
      "73 Train Loss 48.034943 Test MSE 49.69108963288255 Test RE 0.11866658126210702 Lambda1 1.0442263\n",
      "74 Train Loss 47.21273 Test MSE 48.6127498612098 Test RE 0.11737193509957487 Lambda1 1.0593714\n",
      "Training time: 124.11\n",
      "Training time: 124.11\n",
      "inv_HT_stan_tune10\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 838.5875 Test MSE 858.2743182994409 Test RE 0.4931763356189794 Lambda1 -0.033618923\n",
      "1 Train Loss 837.8185 Test MSE 857.9669358102624 Test RE 0.49308801458816276 Lambda1 -0.03499529\n",
      "2 Train Loss 837.1046 Test MSE 856.4285455676593 Test RE 0.49264574688441787 Lambda1 -0.012180459\n",
      "3 Train Loss 818.59247 Test MSE 832.3186868386531 Test RE 0.48566185384761573 Lambda1 0.0819257\n",
      "4 Train Loss 694.09686 Test MSE 657.4097289175672 Test RE 0.431625638753609 Lambda1 0.030723762\n",
      "5 Train Loss 576.9159 Test MSE 586.7546467377537 Test RE 0.4077720358384636 Lambda1 0.013337757\n",
      "6 Train Loss 483.08826 Test MSE 510.25740334726044 Test RE 0.3802627879343952 Lambda1 0.007117888\n",
      "7 Train Loss 320.34244 Test MSE 329.4026089766517 Test RE 0.3055291753989263 Lambda1 0.00015087602\n",
      "8 Train Loss 265.1825 Test MSE 285.8883738588261 Test RE 0.2846344206239939 Lambda1 -5.9951915e-05\n",
      "9 Train Loss 260.74692 Test MSE 283.5906584794981 Test RE 0.2834882944819752 Lambda1 0.00022731864\n",
      "10 Train Loss 258.18036 Test MSE 281.56068513459485 Test RE 0.28247185195712793 Lambda1 0.0010503691\n",
      "11 Train Loss 256.40576 Test MSE 280.7517159649532 Test RE 0.28206576651080306 Lambda1 0.0011008916\n",
      "12 Train Loss 255.9105 Test MSE 281.1044794058911 Test RE 0.2822429181508585 Lambda1 0.0007003094\n",
      "13 Train Loss 255.37573 Test MSE 281.87666326872375 Test RE 0.28263030785341753 Lambda1 0.0005547846\n",
      "14 Train Loss 254.9616 Test MSE 282.6988894164855 Test RE 0.2830422199419198 Lambda1 0.0006527357\n",
      "15 Train Loss 254.8778 Test MSE 282.7867964838134 Test RE 0.28308622343549156 Lambda1 0.0005833309\n",
      "16 Train Loss 254.6869 Test MSE 282.9391226836908 Test RE 0.2831624569156137 Lambda1 0.00042421423\n",
      "17 Train Loss 254.49251 Test MSE 282.9942025109558 Test RE 0.2831900172262827 Lambda1 0.00035409204\n",
      "18 Train Loss 254.42876 Test MSE 283.0146065271171 Test RE 0.2832002261095173 Lambda1 0.00031780137\n",
      "19 Train Loss 254.3051 Test MSE 283.1943753766047 Test RE 0.2832901552080625 Lambda1 0.00023586299\n",
      "20 Train Loss 254.2346 Test MSE 283.01916734681765 Test RE 0.28320250800581703 Lambda1 0.00023007278\n",
      "21 Train Loss 254.22186 Test MSE 282.8503503800506 Test RE 0.2831180322433356 Lambda1 0.00021413909\n",
      "22 Train Loss 254.16771 Test MSE 282.697078164661 Test RE 0.2830413132146958 Lambda1 0.00021118685\n",
      "23 Train Loss 254.14052 Test MSE 282.83264930131446 Test RE 0.2831091731894537 Lambda1 0.0002530556\n",
      "24 Train Loss 254.07402 Test MSE 282.89793295152566 Test RE 0.28314184504306106 Lambda1 0.0002875404\n",
      "25 Train Loss 254.01732 Test MSE 282.8739873846813 Test RE 0.28312986168322635 Lambda1 0.00030166167\n",
      "26 Train Loss 253.86424 Test MSE 282.8644272547705 Test RE 0.28312507725402797 Lambda1 0.00039470088\n",
      "27 Train Loss 253.80272 Test MSE 282.58644381371545 Test RE 0.28298592326072597 Lambda1 0.000461166\n",
      "28 Train Loss 253.78075 Test MSE 282.5230108440832 Test RE 0.282954160157187 Lambda1 0.0004975711\n",
      "29 Train Loss 253.75185 Test MSE 282.5769463316454 Test RE 0.282981167766657 Lambda1 0.00050845585\n",
      "30 Train Loss 253.636 Test MSE 282.6393861997989 Test RE 0.28301243063339804 Lambda1 0.00047910295\n",
      "31 Train Loss 253.56532 Test MSE 282.4868664571554 Test RE 0.2829360598053953 Lambda1 0.00048101117\n",
      "32 Train Loss 253.51958 Test MSE 282.3020147469494 Test RE 0.28284347183110625 Lambda1 0.00048254346\n",
      "33 Train Loss 253.44771 Test MSE 282.3083957574165 Test RE 0.2828466684376858 Lambda1 0.0004660159\n",
      "34 Train Loss 253.35507 Test MSE 282.66566346797345 Test RE 0.28302558630287666 Lambda1 0.0004358476\n",
      "35 Train Loss 253.08957 Test MSE 283.48143269634096 Test RE 0.2834336960456016 Lambda1 0.00039533008\n",
      "36 Train Loss 252.96362 Test MSE 283.29116752056217 Test RE 0.2833385635122115 Lambda1 0.00039868304\n",
      "37 Train Loss 252.78848 Test MSE 283.5644456920557 Test RE 0.28347519251633047 Lambda1 0.00038194112\n",
      "38 Train Loss 252.54836 Test MSE 283.8417234664536 Test RE 0.2836137539022326 Lambda1 0.0003269536\n",
      "39 Train Loss 252.24425 Test MSE 282.96295200624803 Test RE 0.283174380730519 Lambda1 0.00033454\n",
      "40 Train Loss 252.01857 Test MSE 283.0513109469507 Test RE 0.28321858976063935 Lambda1 0.00022499649\n",
      "41 Train Loss 251.82564 Test MSE 283.45599452803634 Test RE 0.2834209788179691 Lambda1 0.00013837327\n",
      "42 Train Loss 251.58675 Test MSE 283.8531245328064 Test RE 0.28361944979978776 Lambda1 0.00010908814\n",
      "43 Train Loss 251.17908 Test MSE 283.38603810755416 Test RE 0.28338600277049536 Lambda1 4.5387653e-05\n",
      "44 Train Loss 250.91013 Test MSE 283.9380709658141 Test RE 0.283661884876753 Lambda1 1.6293397e-05\n",
      "45 Train Loss 250.64626 Test MSE 284.8113898379368 Test RE 0.2840977846539579 Lambda1 1.2241347e-05\n",
      "46 Train Loss 250.46275 Test MSE 285.2873804522898 Test RE 0.2843350845499449 Lambda1 4.8965776e-06\n",
      "47 Train Loss 250.20442 Test MSE 285.65836552744696 Test RE 0.28451989784626847 Lambda1 -2.6826901e-06\n",
      "48 Train Loss 249.7892 Test MSE 285.61916747930843 Test RE 0.2845003762627702 Lambda1 -2.8774898e-06\n",
      "49 Train Loss 249.55522 Test MSE 286.1636724262499 Test RE 0.28477143317899545 Lambda1 -5.1786196e-07\n",
      "50 Train Loss 249.48091 Test MSE 285.9218323506726 Test RE 0.28465107600540956 Lambda1 5.7960773e-07\n",
      "51 Train Loss 249.40979 Test MSE 285.7544596826839 Test RE 0.28456774941232815 Lambda1 4.5923016e-06\n",
      "52 Train Loss 249.21455 Test MSE 286.4086687654205 Test RE 0.2848933092921148 Lambda1 4.4498715e-06\n",
      "53 Train Loss 248.86214 Test MSE 287.6644900906598 Test RE 0.28551721457836676 Lambda1 2.800071e-06\n",
      "54 Train Loss 248.7952 Test MSE 287.4770003000823 Test RE 0.28542415427728335 Lambda1 3.042923e-06\n",
      "55 Train Loss 248.75763 Test MSE 287.4650590495132 Test RE 0.2854182262261076 Lambda1 2.294578e-06\n",
      "56 Train Loss 248.70769 Test MSE 287.79069177427067 Test RE 0.2855798375354796 Lambda1 1.1214677e-06\n",
      "57 Train Loss 248.62463 Test MSE 287.7668699081775 Test RE 0.28556801785953206 Lambda1 1.0155306e-06\n",
      "58 Train Loss 248.58438 Test MSE 287.88570949058686 Test RE 0.28562697753232713 Lambda1 4.0426752e-07\n",
      "59 Train Loss 248.50116 Test MSE 288.20874985956607 Test RE 0.285787185510015 Lambda1 4.9405065e-07\n",
      "60 Train Loss 248.38985 Test MSE 287.9932271850364 Test RE 0.2856803096121787 Lambda1 8.2095016e-07\n",
      "61 Train Loss 248.33968 Test MSE 287.9415841707512 Test RE 0.2856546943331159 Lambda1 -3.916512e-07\n",
      "62 Train Loss 248.26804 Test MSE 288.18821209122723 Test RE 0.28577700272482753 Lambda1 5.1395534e-07\n",
      "63 Train Loss 248.18053 Test MSE 288.0771002288217 Test RE 0.28572190630710304 Lambda1 7.187783e-07\n",
      "64 Train Loss 248.1402 Test MSE 287.97373593197136 Test RE 0.2856706420906845 Lambda1 3.540094e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Train Loss 247.99219 Test MSE 287.6613988453544 Test RE 0.28551568048891407 Lambda1 4.7084893e-07\n",
      "66 Train Loss 247.87263 Test MSE 287.4558384766348 Test RE 0.2854136487295099 Lambda1 1.2037541e-06\n",
      "67 Train Loss 247.66374 Test MSE 288.2366526281086 Test RE 0.2858010193382683 Lambda1 -1.0436367e-07\n",
      "68 Train Loss 247.6128 Test MSE 288.8683215073196 Test RE 0.286114013552685 Lambda1 1.012204e-06\n",
      "69 Train Loss 247.58093 Test MSE 289.33479038961326 Test RE 0.2863449309603933 Lambda1 6.4577773e-07\n",
      "70 Train Loss 247.5712 Test MSE 289.48994672699155 Test RE 0.28642169718002125 Lambda1 1.442617e-07\n",
      "71 Train Loss 247.56549 Test MSE 289.44157562805646 Test RE 0.2863977669689767 Lambda1 4.664074e-07\n",
      "72 Train Loss 247.55858 Test MSE 289.4344726436293 Test RE 0.2863942528031991 Lambda1 8.2871924e-07\n",
      "73 Train Loss 247.53319 Test MSE 289.59103142048417 Test RE 0.28647169947333695 Lambda1 2.2400059e-08\n",
      "74 Train Loss 247.47034 Test MSE 290.0667306156428 Test RE 0.2867068905271885 Lambda1 1.3714488e-06\n",
      "Training time: 124.01\n",
      "Training time: 124.01\n",
      "inv_HT_stan_tune10\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 838.3441 Test MSE 857.8813495246333 Test RE 0.49306342003655484 Lambda1 -0.028321305\n",
      "1 Train Loss 813.2587 Test MSE 816.5049072105022 Test RE 0.48102602117141247 Lambda1 0.031951696\n",
      "2 Train Loss 652.61725 Test MSE 646.4292252514698 Test RE 0.4280058076418327 Lambda1 0.014804824\n",
      "3 Train Loss 606.9868 Test MSE 610.3205879944103 Test RE 0.41588013931100687 Lambda1 0.0062984843\n",
      "4 Train Loss 517.7675 Test MSE 517.9302044365098 Test RE 0.38311114842463173 Lambda1 0.0035722228\n",
      "5 Train Loss 369.08774 Test MSE 343.5124684668323 Test RE 0.3120041877954888 Lambda1 0.0057089003\n",
      "6 Train Loss 271.42017 Test MSE 285.6339951214689 Test RE 0.28450776094752883 Lambda1 0.0026886947\n",
      "7 Train Loss 261.09616 Test MSE 279.96841325635535 Test RE 0.28167200720245106 Lambda1 0.0021812622\n",
      "8 Train Loss 258.13968 Test MSE 280.405415421155 Test RE 0.28189175213670215 Lambda1 0.001984604\n",
      "9 Train Loss 256.8282 Test MSE 280.66195632076443 Test RE 0.2820206730254397 Lambda1 0.0020374027\n",
      "10 Train Loss 255.81232 Test MSE 281.55354073566224 Test RE 0.2824682681748276 Lambda1 0.0026763668\n",
      "11 Train Loss 254.93279 Test MSE 280.5765258536599 Test RE 0.2819777477357979 Lambda1 0.0032892565\n",
      "12 Train Loss 254.28856 Test MSE 279.69660918637607 Test RE 0.2815352450087453 Lambda1 0.004017881\n",
      "13 Train Loss 253.71632 Test MSE 279.6197796389453 Test RE 0.28149657505289133 Lambda1 0.004634647\n",
      "14 Train Loss 253.36523 Test MSE 279.1423220211569 Test RE 0.28125614122643194 Lambda1 0.005505741\n",
      "15 Train Loss 252.8286 Test MSE 277.7576732907943 Test RE 0.2805577070008474 Lambda1 0.0075713363\n",
      "16 Train Loss 252.3142 Test MSE 276.9047858789455 Test RE 0.28012663320943765 Lambda1 0.0095627755\n",
      "17 Train Loss 251.00848 Test MSE 275.7495279035529 Test RE 0.27954167262550134 Lambda1 0.012160311\n",
      "18 Train Loss 250.29051 Test MSE 274.06955758985333 Test RE 0.2786888349425856 Lambda1 0.01502035\n",
      "19 Train Loss 249.00858 Test MSE 270.7779646900941 Test RE 0.2770102443653672 Lambda1 0.021215243\n",
      "20 Train Loss 247.23114 Test MSE 269.3763667989693 Test RE 0.2762923855429812 Lambda1 0.026630297\n",
      "21 Train Loss 244.84059 Test MSE 264.70398797750164 Test RE 0.27388573478093786 Lambda1 0.038278222\n",
      "22 Train Loss 242.8871 Test MSE 262.6694915059397 Test RE 0.27283117117672007 Lambda1 0.044640295\n",
      "23 Train Loss 240.64043 Test MSE 260.88271812558685 Test RE 0.2719016393159209 Lambda1 0.050499894\n",
      "24 Train Loss 235.29951 Test MSE 253.44384363706322 Test RE 0.2679970686763922 Lambda1 0.0723911\n",
      "25 Train Loss 230.4968 Test MSE 250.1938782147301 Test RE 0.2662732322705491 Lambda1 0.09144625\n",
      "26 Train Loss 223.98352 Test MSE 243.949695129778 Test RE 0.2629294969368121 Lambda1 0.12547599\n",
      "27 Train Loss 219.45189 Test MSE 241.09973362549505 Test RE 0.2613891376117336 Lambda1 0.15117532\n",
      "28 Train Loss 217.7745 Test MSE 237.5387268142208 Test RE 0.25945161764991137 Lambda1 0.16680633\n",
      "29 Train Loss 215.14674 Test MSE 235.6748711854049 Test RE 0.2584317151021282 Lambda1 0.18102622\n",
      "30 Train Loss 212.16476 Test MSE 233.08430438689714 Test RE 0.2570074339240627 Lambda1 0.20379338\n",
      "31 Train Loss 208.97542 Test MSE 228.0826373350676 Test RE 0.2542349677679465 Lambda1 0.24211803\n",
      "32 Train Loss 203.13962 Test MSE 224.09480430574231 Test RE 0.2520026248176602 Lambda1 0.2694839\n",
      "33 Train Loss 196.67159 Test MSE 214.48076331503503 Test RE 0.246537702203728 Lambda1 0.29281807\n",
      "34 Train Loss 182.6628 Test MSE 191.45472784464437 Test RE 0.2329282792405437 Lambda1 0.30366775\n",
      "35 Train Loss 174.90756 Test MSE 184.09688690962847 Test RE 0.22840856901256978 Lambda1 0.29837474\n",
      "36 Train Loss 169.04375 Test MSE 173.6420069550192 Test RE 0.22182810491015878 Lambda1 0.31301078\n",
      "37 Train Loss 155.10623 Test MSE 154.0104865674279 Test RE 0.20891244752480767 Lambda1 0.37894127\n",
      "38 Train Loss 148.5964 Test MSE 146.9159231572707 Test RE 0.20404389482486412 Lambda1 0.40370837\n",
      "39 Train Loss 140.94788 Test MSE 139.22509536623002 Test RE 0.19863141327940093 Lambda1 0.44930324\n",
      "40 Train Loss 134.26663 Test MSE 136.24950105286135 Test RE 0.19649731974810433 Lambda1 0.4703864\n",
      "41 Train Loss 128.47157 Test MSE 132.28876474940708 Test RE 0.19362019414070838 Lambda1 0.4762497\n",
      "42 Train Loss 119.06154 Test MSE 120.58399213996127 Test RE 0.18485619077966894 Lambda1 0.50783634\n",
      "43 Train Loss 114.61056 Test MSE 114.4747573599764 Test RE 0.1801125755219921 Lambda1 0.5339954\n",
      "44 Train Loss 110.571175 Test MSE 113.44105503967528 Test RE 0.17929752683483563 Lambda1 0.54284894\n",
      "45 Train Loss 103.18295 Test MSE 105.03255010564924 Test RE 0.17252463830911446 Lambda1 0.5999399\n",
      "46 Train Loss 96.55813 Test MSE 93.01997872122293 Test RE 0.16235934423540843 Lambda1 0.6547584\n",
      "47 Train Loss 92.4944 Test MSE 88.41287400054668 Test RE 0.15828761111412304 Lambda1 0.6795861\n",
      "48 Train Loss 89.81612 Test MSE 85.64768009387669 Test RE 0.15579265284933402 Lambda1 0.68851095\n",
      "49 Train Loss 85.29021 Test MSE 83.79534754180044 Test RE 0.15409875284879668 Lambda1 0.7008689\n",
      "50 Train Loss 82.10009 Test MSE 82.57273970636147 Test RE 0.15297044069138635 Lambda1 0.7142874\n",
      "51 Train Loss 77.86467 Test MSE 77.04088627068037 Test RE 0.14775759231845398 Lambda1 0.7302163\n",
      "52 Train Loss 72.78223 Test MSE 72.12175607275337 Test RE 0.1429625601955836 Lambda1 0.75680435\n",
      "53 Train Loss 70.45342 Test MSE 69.64689630186089 Test RE 0.14048826672444711 Lambda1 0.7904082\n",
      "54 Train Loss 68.302 Test MSE 65.67011000967193 Test RE 0.13641842858115388 Lambda1 0.82998914\n",
      "55 Train Loss 66.58801 Test MSE 63.99133598231355 Test RE 0.1346634570130527 Lambda1 0.84179956\n",
      "56 Train Loss 65.093185 Test MSE 63.952767125170816 Test RE 0.13462286871780774 Lambda1 0.83880705\n",
      "57 Train Loss 63.49716 Test MSE 62.23747533283895 Test RE 0.1328052226300287 Lambda1 0.862822\n",
      "58 Train Loss 61.968853 Test MSE 59.640494307613416 Test RE 0.1300049198206424 Lambda1 0.8860367\n",
      "59 Train Loss 60.38649 Test MSE 58.18597828795718 Test RE 0.12840985066650432 Lambda1 0.89612156\n",
      "60 Train Loss 57.845505 Test MSE 56.612797006730574 Test RE 0.12666203949433844 Lambda1 0.9234283\n",
      "61 Train Loss 56.11785 Test MSE 54.81582855324176 Test RE 0.1246356153161128 Lambda1 0.9464338\n",
      "62 Train Loss 55.05336 Test MSE 53.10305118228628 Test RE 0.12267297800640416 Lambda1 0.958748\n",
      "63 Train Loss 53.546448 Test MSE 51.72425467242786 Test RE 0.12106992999964791 Lambda1 0.97989327\n",
      "64 Train Loss 52.763996 Test MSE 50.599420627099335 Test RE 0.1197462557493225 Lambda1 0.99264514\n",
      "65 Train Loss 51.072464 Test MSE 48.058894412133284 Test RE 0.11670139796405173 Lambda1 1.0207663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 Train Loss 50.088608 Test MSE 47.42972669839819 Test RE 0.11593497740341037 Lambda1 1.0343313\n",
      "67 Train Loss 48.744453 Test MSE 46.771698438341936 Test RE 0.11512794191878306 Lambda1 1.0525485\n",
      "68 Train Loss 47.70318 Test MSE 46.39765145975678 Test RE 0.11466666187503141 Lambda1 1.0609076\n",
      "69 Train Loss 47.038353 Test MSE 45.27102892077588 Test RE 0.1132659451354261 Lambda1 1.0740889\n",
      "70 Train Loss 46.60617 Test MSE 44.76162301988371 Test RE 0.11262688774613604 Lambda1 1.0815258\n",
      "71 Train Loss 45.452213 Test MSE 45.05786410620694 Test RE 0.11299896641218433 Lambda1 1.086868\n",
      "72 Train Loss 44.493797 Test MSE 44.74326335951944 Test RE 0.11260378756181626 Lambda1 1.0968359\n",
      "73 Train Loss 43.692924 Test MSE 43.99735176679202 Test RE 0.11166123810410562 Lambda1 1.1067561\n",
      "74 Train Loss 41.93157 Test MSE 42.41622559870889 Test RE 0.10963650453277504 Lambda1 1.1266716\n",
      "Training time: 123.33\n",
      "Training time: 123.33\n",
      "inv_HT_stan_tune11\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1214.2499 Test MSE 1028.7544553353882 Test RE 0.5399394193580254 Lambda1 -0.0067128087\n",
      "1 Train Loss 838.7882 Test MSE 854.1323535708964 Test RE 0.49198488120356954 Lambda1 -0.0062970296\n",
      "2 Train Loss 826.7622 Test MSE 843.1579416483313 Test RE 0.4888140024520585 Lambda1 -0.0048759375\n",
      "3 Train Loss 793.63184 Test MSE 813.6291466536625 Test RE 0.4801781782385505 Lambda1 0.0043646586\n",
      "4 Train Loss 755.7789 Test MSE 767.4416708615638 Test RE 0.4663498690316537 Lambda1 0.008085511\n",
      "5 Train Loss 632.41046 Test MSE 648.4785035892179 Test RE 0.4286836923646859 Lambda1 -0.016254025\n",
      "6 Train Loss 493.59073 Test MSE 511.8187663510722 Test RE 0.3808441364298378 Lambda1 -0.0066143693\n",
      "7 Train Loss 361.90805 Test MSE 368.30105501262057 Test RE 0.323065551833538 Lambda1 0.0067098336\n",
      "8 Train Loss 308.69565 Test MSE 314.15847564147515 Test RE 0.29837577573770047 Lambda1 0.0031933337\n",
      "9 Train Loss 277.79904 Test MSE 292.6420512418557 Test RE 0.28797682337799313 Lambda1 0.0025515708\n",
      "10 Train Loss 264.30463 Test MSE 281.79547728561306 Test RE 0.2825896033985533 Lambda1 0.005106471\n",
      "11 Train Loss 259.82916 Test MSE 277.76872122017767 Test RE 0.2805632865963218 Lambda1 0.008708819\n",
      "12 Train Loss 254.58685 Test MSE 274.1076022462944 Test RE 0.27870817720947294 Lambda1 0.012060848\n",
      "13 Train Loss 250.1768 Test MSE 271.1248154573714 Test RE 0.2771876045634082 Lambda1 0.017776288\n",
      "14 Train Loss 243.35217 Test MSE 260.3906732742887 Test RE 0.2716451046061374 Lambda1 0.04369311\n",
      "15 Train Loss 231.49034 Test MSE 247.33234703428772 Test RE 0.2647461358394021 Lambda1 0.07249518\n",
      "16 Train Loss 223.0635 Test MSE 237.62871464177525 Test RE 0.25950075758795876 Lambda1 0.101051465\n",
      "17 Train Loss 211.5204 Test MSE 225.68768666122983 Test RE 0.25289666550254847 Lambda1 0.14181888\n",
      "18 Train Loss 202.8627 Test MSE 219.12319687388688 Test RE 0.24919157115202506 Lambda1 0.15845531\n",
      "19 Train Loss 193.92322 Test MSE 209.20840958207808 Test RE 0.24348865999185415 Lambda1 0.19083948\n",
      "20 Train Loss 185.83923 Test MSE 199.21033555005454 Test RE 0.23759927117437168 Lambda1 0.22644268\n",
      "21 Train Loss 173.78247 Test MSE 182.49069255225936 Test RE 0.22740998539316754 Lambda1 0.28326848\n",
      "22 Train Loss 163.51566 Test MSE 174.6143455021422 Test RE 0.22244832035302414 Lambda1 0.31530356\n",
      "23 Train Loss 153.56894 Test MSE 162.3972462100261 Test RE 0.21452529157597006 Lambda1 0.3441442\n",
      "24 Train Loss 141.04558 Test MSE 143.66794599197667 Test RE 0.20177581585539842 Lambda1 0.39546132\n",
      "25 Train Loss 123.47405 Test MSE 126.30078996420694 Test RE 0.18918739797252515 Lambda1 0.43459862\n",
      "26 Train Loss 112.54744 Test MSE 112.37156881091431 Test RE 0.178450345305058 Lambda1 0.45692548\n",
      "27 Train Loss 100.80774 Test MSE 100.30443731530364 Test RE 0.16859676748521682 Lambda1 0.48023725\n",
      "28 Train Loss 93.44151 Test MSE 90.97275590368805 Test RE 0.16056276791913163 Lambda1 0.5053838\n",
      "29 Train Loss 83.58528 Test MSE 77.4374972617981 Test RE 0.14813743644723668 Lambda1 0.536077\n",
      "30 Train Loss 75.42005 Test MSE 68.87387785031376 Test RE 0.13970644403714458 Lambda1 0.56696564\n",
      "31 Train Loss 65.94113 Test MSE 59.96381944603317 Test RE 0.13035683712302276 Lambda1 0.60855883\n",
      "32 Train Loss 60.410675 Test MSE 55.43789220258418 Test RE 0.12534081822893978 Lambda1 0.62932694\n",
      "33 Train Loss 55.514606 Test MSE 50.05964124732121 Test RE 0.11910583471955982 Lambda1 0.6451264\n",
      "34 Train Loss 51.338478 Test MSE 45.05888611374694 Test RE 0.11300024793254843 Lambda1 0.6711655\n",
      "35 Train Loss 48.568466 Test MSE 43.05593946810914 Test RE 0.11046016956317685 Lambda1 0.687184\n",
      "36 Train Loss 45.760384 Test MSE 42.52998694413301 Test RE 0.10978342996597777 Lambda1 0.68719476\n",
      "37 Train Loss 44.692657 Test MSE 41.543094492121355 Test RE 0.10850221206421862 Lambda1 0.6946746\n",
      "38 Train Loss 42.2135 Test MSE 40.6188201312867 Test RE 0.10728841334883095 Lambda1 0.71210086\n",
      "39 Train Loss 41.642673 Test MSE 40.19699162501016 Test RE 0.10672986161130085 Lambda1 0.7211346\n",
      "40 Train Loss 40.30889 Test MSE 39.36637301066791 Test RE 0.10562138845884306 Lambda1 0.7396735\n",
      "41 Train Loss 39.58943 Test MSE 39.298529774286926 Test RE 0.1055303362962432 Lambda1 0.7444863\n",
      "42 Train Loss 38.939285 Test MSE 39.25975746012648 Test RE 0.10547826481811404 Lambda1 0.74113715\n",
      "43 Train Loss 38.622936 Test MSE 39.24904378210476 Test RE 0.10546387174302506 Lambda1 0.74129814\n",
      "44 Train Loss 37.978584 Test MSE 38.92290411071541 Test RE 0.10502478201231431 Lambda1 0.74953127\n",
      "45 Train Loss 37.683533 Test MSE 38.8038649808092 Test RE 0.10486405875104796 Lambda1 0.7508813\n",
      "46 Train Loss 37.371834 Test MSE 38.53098408692571 Test RE 0.10449468982506221 Lambda1 0.7550009\n",
      "47 Train Loss 37.048786 Test MSE 38.345357852051755 Test RE 0.10424267998994068 Lambda1 0.76223075\n",
      "48 Train Loss 36.521168 Test MSE 38.006590097894524 Test RE 0.10378118472891933 Lambda1 0.768415\n",
      "49 Train Loss 36.21991 Test MSE 37.94792450933556 Test RE 0.10370105736765292 Lambda1 0.7716481\n",
      "50 Train Loss 35.765728 Test MSE 37.77096704755603 Test RE 0.10345898722522022 Lambda1 0.776799\n",
      "51 Train Loss 35.353966 Test MSE 37.48454645001976 Test RE 0.10306597139474603 Lambda1 0.78197736\n",
      "52 Train Loss 34.91556 Test MSE 37.09700436482446 Test RE 0.10253180224110157 Lambda1 0.79503644\n",
      "53 Train Loss 34.470234 Test MSE 36.907360142173765 Test RE 0.10226938916070795 Lambda1 0.8071146\n",
      "54 Train Loss 34.30264 Test MSE 36.710764227262516 Test RE 0.10199664423775787 Lambda1 0.81105864\n",
      "55 Train Loss 34.12705 Test MSE 36.562945312386354 Test RE 0.10179108816378084 Lambda1 0.81764764\n",
      "56 Train Loss 33.862373 Test MSE 36.18330659831946 Test RE 0.10126125293173875 Lambda1 0.83310753\n",
      "57 Train Loss 33.65345 Test MSE 35.81852588346511 Test RE 0.10074952907843047 Lambda1 0.84406865\n",
      "58 Train Loss 33.536156 Test MSE 35.69545751093603 Test RE 0.10057629820049996 Lambda1 0.8506846\n",
      "59 Train Loss 33.412056 Test MSE 35.51331120627424 Test RE 0.10031936030973722 Lambda1 0.86114883\n",
      "60 Train Loss 33.15603 Test MSE 35.157539147330915 Test RE 0.0998155961954115 Lambda1 0.87472534\n",
      "61 Train Loss 32.813313 Test MSE 35.120640704918245 Test RE 0.0997632033519651 Lambda1 0.8753735\n",
      "62 Train Loss 32.526237 Test MSE 35.01996106536727 Test RE 0.09962010613320253 Lambda1 0.88012767\n",
      "63 Train Loss 32.26779 Test MSE 34.79732100907719 Test RE 0.09930293287838651 Lambda1 0.8924882\n",
      "64 Train Loss 32.09373 Test MSE 34.592662407916016 Test RE 0.09901047976131469 Lambda1 0.9051324\n",
      "65 Train Loss 31.944498 Test MSE 34.50231198788027 Test RE 0.0988810955712762 Lambda1 0.9127734\n",
      "66 Train Loss 31.758186 Test MSE 34.54894596242907 Test RE 0.09894789778145181 Lambda1 0.91608137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 Train Loss 31.465513 Test MSE 34.56073728841588 Test RE 0.09896478147144436 Lambda1 0.9306556\n",
      "68 Train Loss 31.299114 Test MSE 34.367099175962764 Test RE 0.09868715036853185 Lambda1 0.9433612\n",
      "69 Train Loss 31.165258 Test MSE 34.223385092671506 Test RE 0.09848059247046557 Lambda1 0.9514988\n",
      "70 Train Loss 30.985228 Test MSE 34.113935169333736 Test RE 0.09832299080542314 Lambda1 0.96250105\n",
      "71 Train Loss 30.843452 Test MSE 34.001249511764655 Test RE 0.09816046549827094 Lambda1 0.97143006\n",
      "72 Train Loss 30.590664 Test MSE 33.90590543567517 Test RE 0.09802274130830851 Lambda1 0.9890879\n",
      "73 Train Loss 30.391409 Test MSE 33.85533372158208 Test RE 0.0979496120435019 Lambda1 1.003313\n",
      "74 Train Loss 30.308914 Test MSE 33.87052226636744 Test RE 0.09797158118521125 Lambda1 1.0088094\n",
      "Training time: 123.63\n",
      "Training time: 123.63\n",
      "inv_HT_stan_tune11\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 865.4368 Test MSE 854.6591167428767 Test RE 0.4921365670461709 Lambda1 0.05260767\n",
      "1 Train Loss 832.6287 Test MSE 846.4819233723681 Test RE 0.48977658046276396 Lambda1 0.049804863\n",
      "2 Train Loss 800.58057 Test MSE 815.6563848160324 Test RE 0.4807760119893195 Lambda1 0.03684222\n",
      "3 Train Loss 750.9249 Test MSE 768.200775111778 Test RE 0.4665804537678046 Lambda1 0.028482417\n",
      "4 Train Loss 638.4757 Test MSE 639.0374493055374 Test RE 0.42555169579928614 Lambda1 0.034860406\n",
      "5 Train Loss 451.6673 Test MSE 433.15162140294973 Test RE 0.3503556639711116 Lambda1 0.03740879\n",
      "6 Train Loss 329.76883 Test MSE 331.3414971143465 Test RE 0.3064270397836878 Lambda1 0.019218296\n",
      "7 Train Loss 290.20044 Test MSE 306.27827687195196 Test RE 0.2946098535270085 Lambda1 0.0022686273\n",
      "8 Train Loss 271.3234 Test MSE 290.83217128929886 Test RE 0.2870849285866225 Lambda1 0.0010216468\n",
      "9 Train Loss 263.40933 Test MSE 282.6686007632257 Test RE 0.2830270568167358 Lambda1 0.0013236409\n",
      "10 Train Loss 259.41333 Test MSE 279.6718444935244 Test RE 0.2815227809888434 Lambda1 0.0024218152\n",
      "11 Train Loss 256.72635 Test MSE 279.28293975750586 Test RE 0.281326973592715 Lambda1 0.0049707666\n",
      "12 Train Loss 255.4149 Test MSE 278.9267606685071 Test RE 0.28114752340207577 Lambda1 0.0058768983\n",
      "13 Train Loss 254.08432 Test MSE 278.04671152669044 Test RE 0.28070364503810435 Lambda1 0.0073473495\n",
      "14 Train Loss 251.90182 Test MSE 275.2282206380738 Test RE 0.2792773094444035 Lambda1 0.01306212\n",
      "15 Train Loss 248.46024 Test MSE 270.49544708130963 Test RE 0.27686569660663646 Lambda1 0.020927496\n",
      "16 Train Loss 245.13882 Test MSE 265.08168526120846 Test RE 0.27408106433432156 Lambda1 0.03237228\n",
      "17 Train Loss 238.59843 Test MSE 258.90235769932445 Test RE 0.2708676708420956 Lambda1 0.04577023\n",
      "18 Train Loss 233.5957 Test MSE 252.5150107450699 Test RE 0.26750553379595227 Lambda1 0.058280095\n",
      "19 Train Loss 227.78427 Test MSE 244.90869731841983 Test RE 0.2634457973140664 Lambda1 0.07845838\n",
      "20 Train Loss 218.73529 Test MSE 235.2390293202953 Test RE 0.25819264106870954 Lambda1 0.102827236\n",
      "21 Train Loss 212.12085 Test MSE 227.61581506916778 Test RE 0.2539746600677463 Lambda1 0.119031735\n",
      "22 Train Loss 199.93121 Test MSE 211.92573311137286 Test RE 0.24506484631992326 Lambda1 0.16435285\n",
      "23 Train Loss 192.25894 Test MSE 200.44385255718714 Test RE 0.23833374723904396 Lambda1 0.18816687\n",
      "24 Train Loss 185.8222 Test MSE 197.14050870412848 Test RE 0.2363617011688656 Lambda1 0.19827396\n",
      "25 Train Loss 175.86284 Test MSE 182.3614664893385 Test RE 0.22732945388502174 Lambda1 0.23659162\n",
      "26 Train Loss 166.27037 Test MSE 171.4739370389101 Test RE 0.22043889751446427 Lambda1 0.26497245\n",
      "27 Train Loss 155.34064 Test MSE 161.87409037627353 Test RE 0.21417947078020463 Lambda1 0.2889579\n",
      "28 Train Loss 148.49008 Test MSE 152.4238104345682 Test RE 0.207833512648686 Lambda1 0.30788797\n",
      "29 Train Loss 140.7247 Test MSE 142.1081720121524 Test RE 0.20067750697539988 Lambda1 0.3278393\n",
      "30 Train Loss 132.63269 Test MSE 133.04997155635377 Test RE 0.19417645301758168 Lambda1 0.34097028\n",
      "31 Train Loss 123.08481 Test MSE 124.15857224972521 Test RE 0.18757611018502102 Lambda1 0.35600403\n",
      "32 Train Loss 114.73913 Test MSE 113.53327983432254 Test RE 0.17937039427324974 Lambda1 0.37831345\n",
      "33 Train Loss 106.96981 Test MSE 105.33960133613557 Test RE 0.17277663277681327 Lambda1 0.40123367\n",
      "34 Train Loss 98.03521 Test MSE 92.85828503127686 Test RE 0.16221817080031015 Lambda1 0.42610228\n",
      "35 Train Loss 92.653046 Test MSE 88.96022895527281 Test RE 0.15877682622344608 Lambda1 0.4349193\n",
      "36 Train Loss 88.618484 Test MSE 87.10482747858671 Test RE 0.1571123349808737 Lambda1 0.43964437\n",
      "37 Train Loss 84.906845 Test MSE 82.29107867404551 Test RE 0.15270932172328508 Lambda1 0.45121196\n",
      "38 Train Loss 81.78061 Test MSE 77.50884613141554 Test RE 0.14820566570007002 Lambda1 0.46402532\n",
      "39 Train Loss 76.708885 Test MSE 73.41870998925016 Test RE 0.14424226897140413 Lambda1 0.47282198\n",
      "40 Train Loss 73.912315 Test MSE 69.77252796272856 Test RE 0.14061491861461445 Lambda1 0.48528463\n",
      "41 Train Loss 71.56895 Test MSE 67.54223729439651 Test RE 0.1383492763200863 Lambda1 0.49372956\n",
      "42 Train Loss 69.364494 Test MSE 67.67355705516313 Test RE 0.1384837045855561 Lambda1 0.49447042\n",
      "43 Train Loss 67.462875 Test MSE 65.93890487939687 Test RE 0.13669733117639685 Lambda1 0.50215805\n",
      "44 Train Loss 65.62785 Test MSE 63.07508504659329 Test RE 0.13369590119699035 Lambda1 0.5141801\n",
      "45 Train Loss 63.86393 Test MSE 60.86111931693763 Test RE 0.13132854669382607 Lambda1 0.5212572\n",
      "46 Train Loss 62.431477 Test MSE 60.088285917822844 Test RE 0.13049205736840627 Lambda1 0.52456886\n",
      "47 Train Loss 61.43321 Test MSE 59.035355748448346 Test RE 0.1293436948658549 Lambda1 0.52983767\n",
      "48 Train Loss 60.293983 Test MSE 57.10952611110698 Test RE 0.12721650170737053 Lambda1 0.5359778\n",
      "49 Train Loss 59.211823 Test MSE 56.459833344461515 Test RE 0.126490807940542 Lambda1 0.53707457\n",
      "50 Train Loss 58.07962 Test MSE 55.57083671690299 Test RE 0.12549101690601336 Lambda1 0.5392274\n",
      "51 Train Loss 57.02059 Test MSE 54.36903763634383 Test RE 0.12412663827317834 Lambda1 0.5445265\n",
      "52 Train Loss 55.273552 Test MSE 52.87482783450679 Test RE 0.12240908560543347 Lambda1 0.552326\n",
      "53 Train Loss 53.924843 Test MSE 52.35668072976785 Test RE 0.12180783480634183 Lambda1 0.5552265\n",
      "54 Train Loss 51.926495 Test MSE 51.235409045794235 Test RE 0.12049645621050634 Lambda1 0.5617992\n",
      "55 Train Loss 50.724506 Test MSE 49.798154041451916 Test RE 0.11879435196774453 Lambda1 0.5704773\n",
      "56 Train Loss 48.99437 Test MSE 48.599283557412434 Test RE 0.11735567726995333 Lambda1 0.5803226\n",
      "57 Train Loss 48.26953 Test MSE 47.85762798683453 Test RE 0.1164567739812452 Lambda1 0.58752\n",
      "58 Train Loss 46.54143 Test MSE 46.2767364032982 Test RE 0.11451715031082026 Lambda1 0.6079999\n",
      "59 Train Loss 45.87367 Test MSE 45.80727006821837 Test RE 0.11393479514076009 Lambda1 0.6139099\n",
      "60 Train Loss 45.220627 Test MSE 45.3331026962514 Test RE 0.11334357134101931 Lambda1 0.61772686\n",
      "61 Train Loss 44.278183 Test MSE 44.88055256184582 Test RE 0.11277641068364462 Lambda1 0.6240051\n",
      "62 Train Loss 43.391167 Test MSE 43.60682516622469 Test RE 0.11116457320761639 Lambda1 0.6391068\n",
      "63 Train Loss 42.856266 Test MSE 42.866315444234644 Test RE 0.11021666059390244 Lambda1 0.6495907\n",
      "64 Train Loss 42.281532 Test MSE 42.7128367259974 Test RE 0.11001917358998557 Lambda1 0.6533476\n",
      "65 Train Loss 41.64795 Test MSE 42.15299536496269 Test RE 0.10929577929507299 Lambda1 0.66288495\n",
      "66 Train Loss 41.114105 Test MSE 41.68456756868595 Test RE 0.10868680466136037 Lambda1 0.6704723\n",
      "67 Train Loss 40.457222 Test MSE 41.37335578451307 Test RE 0.10828032348936803 Lambda1 0.67889434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 Train Loss 39.751984 Test MSE 40.63451895475768 Test RE 0.10730914436890501 Lambda1 0.6967932\n",
      "69 Train Loss 39.154305 Test MSE 39.84653182298626 Test RE 0.10626357781255229 Lambda1 0.71358037\n",
      "70 Train Loss 38.473724 Test MSE 39.33868275279411 Test RE 0.10558423494754213 Lambda1 0.7262622\n",
      "71 Train Loss 37.935658 Test MSE 38.97528454766063 Test RE 0.10509542672034997 Lambda1 0.73527634\n",
      "72 Train Loss 37.363728 Test MSE 38.48530968636272 Test RE 0.10443273776668635 Lambda1 0.7482243\n",
      "73 Train Loss 36.77089 Test MSE 38.12227888080416 Test RE 0.10393901520316672 Lambda1 0.7561495\n",
      "74 Train Loss 36.19506 Test MSE 37.549917730276206 Test RE 0.10315580334154953 Lambda1 0.7648261\n",
      "Training time: 123.55\n",
      "Training time: 123.55\n",
      "inv_HT_stan_tune11\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 1538.4207 Test MSE 1317.113771888158 Test RE 0.6109431728617926 Lambda1 0.09451594\n",
      "1 Train Loss 848.4591 Test MSE 846.7116568040352 Test RE 0.4898430381260749 Lambda1 0.10890503\n",
      "2 Train Loss 787.6462 Test MSE 797.5266411780683 Test RE 0.47540283840569314 Lambda1 0.10835552\n",
      "3 Train Loss 731.51514 Test MSE 741.3692234740423 Test RE 0.45835972185562984 Lambda1 0.1010207\n",
      "4 Train Loss 664.1608 Test MSE 675.7199438705234 Test RE 0.4375951896534269 Lambda1 0.06617051\n",
      "5 Train Loss 458.5922 Test MSE 471.9675083075693 Test RE 0.3657170613177502 Lambda1 0.00047299976\n",
      "6 Train Loss 299.82675 Test MSE 309.048723924354 Test RE 0.2959393039347316 Lambda1 0.002597868\n",
      "7 Train Loss 274.60208 Test MSE 290.49904369590615 Test RE 0.28692046378091235 Lambda1 -0.0003838838\n",
      "8 Train Loss 260.89114 Test MSE 281.5441855572946 Test RE 0.28246357534983896 Lambda1 0.001652468\n",
      "9 Train Loss 257.52042 Test MSE 279.5630964554484 Test RE 0.28146804178754575 Lambda1 0.0030047775\n",
      "10 Train Loss 256.31412 Test MSE 279.3920761783007 Test RE 0.28138193581149695 Lambda1 0.0035639256\n",
      "11 Train Loss 255.297 Test MSE 279.09500023417263 Test RE 0.2812323001491782 Lambda1 0.0045082984\n",
      "12 Train Loss 254.34468 Test MSE 277.5569498015034 Test RE 0.28045631520323094 Lambda1 0.0069818795\n",
      "13 Train Loss 252.85805 Test MSE 275.38244239445817 Test RE 0.27935554380061506 Lambda1 0.010505308\n",
      "14 Train Loss 250.89601 Test MSE 272.53101004522574 Test RE 0.27790549461512826 Lambda1 0.016599724\n",
      "15 Train Loss 247.69324 Test MSE 268.52563430816724 Test RE 0.2758557533550793 Lambda1 0.022658631\n",
      "16 Train Loss 242.22491 Test MSE 261.56990620009856 Test RE 0.27225951024709744 Lambda1 0.03596592\n",
      "17 Train Loss 234.56984 Test MSE 252.18675366362902 Test RE 0.26733160524125527 Lambda1 0.0549645\n",
      "18 Train Loss 224.92555 Test MSE 240.61001658955232 Test RE 0.2611235384361329 Lambda1 0.07997977\n",
      "19 Train Loss 214.84642 Test MSE 220.11142949810787 Test RE 0.24975285862523963 Lambda1 0.1210502\n",
      "20 Train Loss 200.06432 Test MSE 204.41266267530114 Test RE 0.2406816988636358 Lambda1 0.16536449\n",
      "21 Train Loss 191.55342 Test MSE 196.29298964145173 Test RE 0.23585308725972873 Lambda1 0.18326242\n",
      "22 Train Loss 182.28105 Test MSE 189.15723418462508 Test RE 0.23152646891110298 Lambda1 0.2113042\n",
      "23 Train Loss 171.97865 Test MSE 174.7475020530937 Test RE 0.22253312096380884 Lambda1 0.24238351\n",
      "24 Train Loss 163.60948 Test MSE 162.74413410060706 Test RE 0.2147542872359436 Lambda1 0.2683638\n",
      "25 Train Loss 151.62825 Test MSE 148.3006202184452 Test RE 0.20500320663731925 Lambda1 0.30761188\n",
      "26 Train Loss 140.5516 Test MSE 135.1414238623702 Test RE 0.1956966610383797 Lambda1 0.3589939\n",
      "27 Train Loss 132.94249 Test MSE 128.92396563206813 Test RE 0.19114194392126319 Lambda1 0.40032464\n",
      "28 Train Loss 126.71284 Test MSE 124.10276736299014 Test RE 0.18753395103374806 Lambda1 0.4196517\n",
      "29 Train Loss 121.99831 Test MSE 115.4525440357617 Test RE 0.18088015613722327 Lambda1 0.44453138\n",
      "30 Train Loss 110.94975 Test MSE 106.59658448062649 Test RE 0.17380441944364608 Lambda1 0.47869074\n",
      "31 Train Loss 104.22117 Test MSE 101.30667135234869 Test RE 0.16943697668645707 Lambda1 0.48451963\n",
      "32 Train Loss 97.58041 Test MSE 89.32595094355754 Test RE 0.15910286303222876 Lambda1 0.5066074\n",
      "33 Train Loss 89.16309 Test MSE 80.2049459977534 Test RE 0.15076125568102783 Lambda1 0.5385309\n",
      "34 Train Loss 80.80025 Test MSE 68.96646537502365 Test RE 0.13980031656175168 Lambda1 0.5658455\n",
      "35 Train Loss 77.4303 Test MSE 65.28023525175348 Test RE 0.13601287667386786 Lambda1 0.56841236\n",
      "36 Train Loss 70.91527 Test MSE 63.61159119029325 Test RE 0.13426329472138615 Lambda1 0.56926596\n",
      "37 Train Loss 64.816925 Test MSE 61.81443696519027 Test RE 0.13235310355535265 Lambda1 0.5767239\n",
      "38 Train Loss 61.42361 Test MSE 57.99168770503909 Test RE 0.12819528276574954 Lambda1 0.5875694\n",
      "39 Train Loss 58.2129 Test MSE 56.40304743936225 Test RE 0.12642718127364044 Lambda1 0.5906226\n",
      "40 Train Loss 55.554684 Test MSE 53.57716362201071 Test RE 0.12321938299376142 Lambda1 0.60024697\n",
      "41 Train Loss 53.053757 Test MSE 51.674413158153904 Test RE 0.12101158442223116 Lambda1 0.61316174\n",
      "42 Train Loss 51.956875 Test MSE 51.211525067657824 Test RE 0.12046836752874227 Lambda1 0.6158654\n",
      "43 Train Loss 51.38732 Test MSE 50.38425688309818 Test RE 0.11949138621771772 Lambda1 0.6169049\n",
      "44 Train Loss 50.6856 Test MSE 49.24157673868013 Test RE 0.11812862422816449 Lambda1 0.62615585\n",
      "45 Train Loss 50.13242 Test MSE 48.59587644334646 Test RE 0.11735156351397157 Lambda1 0.63319355\n",
      "46 Train Loss 49.34601 Test MSE 48.550687818079616 Test RE 0.11729698903588903 Lambda1 0.63554084\n",
      "47 Train Loss 48.896866 Test MSE 48.19591665101382 Test RE 0.11686764507976123 Lambda1 0.64005095\n",
      "48 Train Loss 48.378674 Test MSE 47.94515881865775 Test RE 0.11656322411281915 Lambda1 0.64390993\n",
      "49 Train Loss 47.73085 Test MSE 47.91443219878734 Test RE 0.11652586718268766 Lambda1 0.6496107\n",
      "50 Train Loss 47.48587 Test MSE 47.493045580978425 Test RE 0.11601233843141892 Lambda1 0.655808\n",
      "51 Train Loss 47.128452 Test MSE 47.11304580461722 Test RE 0.11554728929783456 Lambda1 0.66230166\n",
      "52 Train Loss 46.871696 Test MSE 47.07207139966056 Test RE 0.11549703239775826 Lambda1 0.6621039\n",
      "53 Train Loss 46.293842 Test MSE 46.66922320467733 Test RE 0.11500175201806569 Lambda1 0.66872805\n",
      "54 Train Loss 46.10733 Test MSE 46.6699847029269 Test RE 0.1150026902517625 Lambda1 0.67255545\n",
      "55 Train Loss 45.809788 Test MSE 46.32527579493091 Test RE 0.11457719274693134 Lambda1 0.6798835\n",
      "56 Train Loss 45.30829 Test MSE 46.0442071564513 Test RE 0.11422907768925002 Lambda1 0.68635553\n",
      "57 Train Loss 44.84203 Test MSE 45.87800655319167 Test RE 0.11402273137841405 Lambda1 0.69396263\n",
      "58 Train Loss 44.521286 Test MSE 45.527914683875174 Test RE 0.11358684848131463 Lambda1 0.7013922\n",
      "59 Train Loss 43.89633 Test MSE 45.138351955309034 Test RE 0.11309984764448802 Lambda1 0.71876305\n",
      "60 Train Loss 43.02558 Test MSE 44.65393030744838 Test RE 0.11249132073134221 Lambda1 0.7356952\n",
      "61 Train Loss 42.510227 Test MSE 43.90033769728317 Test RE 0.11153806376748411 Lambda1 0.74524885\n",
      "62 Train Loss 42.01981 Test MSE 43.78804021044087 Test RE 0.11139531469972244 Lambda1 0.75108165\n",
      "63 Train Loss 41.458584 Test MSE 43.37749961009321 Test RE 0.11087188413592496 Lambda1 0.7606711\n",
      "64 Train Loss 41.08479 Test MSE 43.09904940819305 Test RE 0.11051545508209457 Lambda1 0.7683955\n",
      "65 Train Loss 40.75222 Test MSE 42.947298585301354 Test RE 0.11032072225220643 Lambda1 0.7768232\n",
      "66 Train Loss 40.415325 Test MSE 42.48929189782311 Test RE 0.10973089396348902 Lambda1 0.7902317\n",
      "67 Train Loss 40.14519 Test MSE 42.03665524855125 Test RE 0.10914484969652359 Lambda1 0.80336696\n",
      "68 Train Loss 39.40416 Test MSE 41.22017578218633 Test RE 0.10807969000608471 Lambda1 0.8258041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 Train Loss 38.91423 Test MSE 40.60308759922001 Test RE 0.10726763379551965 Lambda1 0.8404574\n",
      "70 Train Loss 38.423656 Test MSE 40.289576786334386 Test RE 0.10685270560486185 Lambda1 0.8501664\n",
      "71 Train Loss 38.156048 Test MSE 40.14975560467474 Test RE 0.10666713333659515 Lambda1 0.85398173\n",
      "72 Train Loss 37.86875 Test MSE 39.71555008409865 Test RE 0.10608878160641479 Lambda1 0.8617695\n",
      "73 Train Loss 37.61631 Test MSE 39.46493029183204 Test RE 0.10575352216566657 Lambda1 0.86832976\n",
      "74 Train Loss 37.233723 Test MSE 39.267867643753725 Test RE 0.10548915897509784 Lambda1 0.87519634\n",
      "Training time: 123.50\n",
      "Training time: 123.50\n",
      "inv_HT_stan_tune11\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 1588.2816 Test MSE 1143.087776993874 Test RE 0.5691529105124126 Lambda1 0.18068703\n",
      "1 Train Loss 856.2275 Test MSE 852.1519240456864 Test RE 0.49141418110994206 Lambda1 0.1663805\n",
      "2 Train Loss 834.0085 Test MSE 848.8959157698814 Test RE 0.49047445432024755 Lambda1 0.16459432\n",
      "3 Train Loss 820.42944 Test MSE 839.3080951366918 Test RE 0.48769676697209086 Lambda1 0.16597007\n",
      "4 Train Loss 801.48846 Test MSE 817.3676912104019 Test RE 0.4812800992379571 Lambda1 0.1646843\n",
      "5 Train Loss 777.317 Test MSE 791.448124749794 Test RE 0.4735876819614326 Lambda1 0.16623028\n",
      "6 Train Loss 753.2522 Test MSE 766.1850959052421 Test RE 0.4659679223253365 Lambda1 0.16697736\n",
      "7 Train Loss 713.2467 Test MSE 723.54886725308 Test RE 0.45281739777959545 Lambda1 0.16183583\n",
      "8 Train Loss 653.0434 Test MSE 658.8675317255247 Test RE 0.43210393759895444 Lambda1 0.16084121\n",
      "9 Train Loss 545.2549 Test MSE 547.7333787857243 Test RE 0.3939796358224366 Lambda1 0.16433424\n",
      "10 Train Loss 475.36398 Test MSE 459.9425764305113 Test RE 0.36102807645055784 Lambda1 0.16596743\n",
      "11 Train Loss 401.39954 Test MSE 392.1852828624437 Test RE 0.33337637258379793 Lambda1 0.16521561\n",
      "12 Train Loss 331.03384 Test MSE 326.32424677030923 Test RE 0.3040981952725024 Lambda1 0.164328\n",
      "13 Train Loss 292.59595 Test MSE 288.2910292437022 Test RE 0.2858279766303658 Lambda1 0.16524497\n",
      "14 Train Loss 262.62863 Test MSE 248.6425699878127 Test RE 0.2654464451469135 Lambda1 0.1643828\n",
      "15 Train Loss 242.52818 Test MSE 225.3880193768287 Test RE 0.25272871209858555 Lambda1 0.16323441\n",
      "16 Train Loss 225.16869 Test MSE 219.92196257770098 Test RE 0.24964534468910649 Lambda1 0.16446902\n",
      "17 Train Loss 210.80482 Test MSE 213.73257895331588 Test RE 0.24610732142447475 Lambda1 0.1656208\n",
      "18 Train Loss 201.40887 Test MSE 203.85174976081476 Test RE 0.24035125404076468 Lambda1 0.1696129\n",
      "19 Train Loss 196.75836 Test MSE 200.11495438786653 Test RE 0.2381381320727625 Lambda1 0.17321472\n",
      "20 Train Loss 188.97662 Test MSE 195.5121881784817 Test RE 0.23538353933949588 Lambda1 0.17935854\n",
      "21 Train Loss 183.38364 Test MSE 190.9684939572775 Test RE 0.2326323094700955 Lambda1 0.18650462\n",
      "22 Train Loss 176.44402 Test MSE 180.07114214184526 Test RE 0.2258973988843419 Lambda1 0.20060292\n",
      "23 Train Loss 168.75761 Test MSE 174.46874796465613 Test RE 0.22235555969857204 Lambda1 0.21103638\n",
      "24 Train Loss 162.49936 Test MSE 167.58027303475848 Test RE 0.21792176990084125 Lambda1 0.22886637\n",
      "25 Train Loss 154.96118 Test MSE 156.06947160112955 Test RE 0.21030429904429448 Lambda1 0.24451634\n",
      "26 Train Loss 148.64525 Test MSE 149.41789795224074 Test RE 0.20577399146409853 Lambda1 0.24822916\n",
      "27 Train Loss 142.18726 Test MSE 144.31798956590387 Test RE 0.20223178061616062 Lambda1 0.25347692\n",
      "28 Train Loss 137.53008 Test MSE 141.1258449333889 Test RE 0.19998270949337144 Lambda1 0.25983328\n",
      "29 Train Loss 130.06194 Test MSE 130.35393187958033 Test RE 0.19219905071161963 Lambda1 0.2762776\n",
      "30 Train Loss 118.825005 Test MSE 115.3226762485373 Test RE 0.18077839521277217 Lambda1 0.30821744\n",
      "31 Train Loss 110.84143 Test MSE 108.28494597865357 Test RE 0.17517543827442997 Lambda1 0.3287131\n",
      "32 Train Loss 105.29162 Test MSE 98.42444991297863 Test RE 0.16700930500332517 Lambda1 0.35480443\n",
      "33 Train Loss 99.35207 Test MSE 90.67319714301128 Test RE 0.16029819616157684 Lambda1 0.36372244\n",
      "34 Train Loss 91.06421 Test MSE 85.53783102287166 Test RE 0.1556927133701982 Lambda1 0.37524468\n",
      "35 Train Loss 86.42253 Test MSE 80.14004406576467 Test RE 0.15070024524776102 Lambda1 0.38524044\n",
      "36 Train Loss 83.66036 Test MSE 77.11463373494541 Test RE 0.14782829595261174 Lambda1 0.39180136\n",
      "37 Train Loss 81.4654 Test MSE 75.73793339413503 Test RE 0.1465027901656022 Lambda1 0.39773828\n",
      "38 Train Loss 79.53239 Test MSE 73.7579987557884 Test RE 0.14457517709954096 Lambda1 0.40833336\n",
      "39 Train Loss 77.37607 Test MSE 70.36987175371071 Test RE 0.14121555928824725 Lambda1 0.42328304\n",
      "40 Train Loss 73.25635 Test MSE 65.16172746852423 Test RE 0.1358893637423546 Lambda1 0.44873923\n",
      "41 Train Loss 69.01078 Test MSE 62.73443112514644 Test RE 0.13333438212174004 Lambda1 0.45794797\n",
      "42 Train Loss 64.50978 Test MSE 59.52824539764513 Test RE 0.12988252157864408 Lambda1 0.4766694\n",
      "43 Train Loss 62.138462 Test MSE 57.33164236551256 Test RE 0.1274636533930313 Lambda1 0.49022672\n",
      "44 Train Loss 60.785633 Test MSE 56.205268651383825 Test RE 0.12620532647883312 Lambda1 0.49759534\n",
      "45 Train Loss 58.92116 Test MSE 54.36743775964182 Test RE 0.124124811969217 Lambda1 0.5106525\n",
      "46 Train Loss 57.692947 Test MSE 53.07378066001587 Test RE 0.12263916453437232 Lambda1 0.52164334\n",
      "47 Train Loss 55.276825 Test MSE 51.020425778839176 Test RE 0.12024338949897583 Lambda1 0.53757507\n",
      "48 Train Loss 53.696106 Test MSE 50.337905992730406 Test RE 0.11943641064735457 Lambda1 0.5451973\n",
      "49 Train Loss 52.262486 Test MSE 49.65312162910694 Test RE 0.11862123717511457 Lambda1 0.55066335\n",
      "50 Train Loss 51.21621 Test MSE 48.36810084064392 Test RE 0.11707621897569166 Lambda1 0.5579461\n",
      "51 Train Loss 49.98898 Test MSE 47.14472165359905 Test RE 0.11558612613625158 Lambda1 0.57097596\n",
      "52 Train Loss 48.696484 Test MSE 46.588405844956434 Test RE 0.11490213427763103 Lambda1 0.5798683\n",
      "53 Train Loss 47.552505 Test MSE 46.19636273647265 Test RE 0.11441766011408143 Lambda1 0.5837425\n",
      "54 Train Loss 46.828205 Test MSE 45.5426973869042 Test RE 0.11360528754647069 Lambda1 0.5921741\n",
      "55 Train Loss 45.255436 Test MSE 43.500271026702784 Test RE 0.1110286737222374 Lambda1 0.62314534\n",
      "56 Train Loss 44.426395 Test MSE 42.51581398111148 Test RE 0.10976513597794833 Lambda1 0.64369327\n",
      "57 Train Loss 43.667572 Test MSE 41.472247730851194 Test RE 0.10840965383602418 Lambda1 0.65884584\n",
      "58 Train Loss 42.688595 Test MSE 40.90618384687781 Test RE 0.1076672581774871 Lambda1 0.67043376\n",
      "59 Train Loss 41.592484 Test MSE 40.73042439874252 Test RE 0.10743570506220115 Lambda1 0.6744971\n",
      "60 Train Loss 40.775337 Test MSE 40.29078506789037 Test RE 0.10685430784538802 Lambda1 0.68150127\n",
      "61 Train Loss 40.12289 Test MSE 39.65874441615128 Test RE 0.10601288437528056 Lambda1 0.69296014\n",
      "62 Train Loss 39.405983 Test MSE 38.959798087065295 Test RE 0.10507454530910197 Lambda1 0.71383536\n",
      "63 Train Loss 38.81738 Test MSE 38.351717072628404 Test RE 0.10425132347054597 Lambda1 0.7287042\n",
      "64 Train Loss 38.434044 Test MSE 38.16224195026718 Test RE 0.10399347986535266 Lambda1 0.73548454\n",
      "65 Train Loss 38.134357 Test MSE 37.82798968032465 Test RE 0.10353705352008537 Lambda1 0.7443663\n",
      "66 Train Loss 37.73368 Test MSE 37.64468083935237 Test RE 0.10328588624968559 Lambda1 0.7481316\n",
      "67 Train Loss 37.373528 Test MSE 37.46510310139703 Test RE 0.10303923761064523 Lambda1 0.7553613\n",
      "68 Train Loss 36.72527 Test MSE 36.819072993199484 Test RE 0.10214699515982012 Lambda1 0.773184\n",
      "69 Train Loss 36.36113 Test MSE 36.64100560458298 Test RE 0.1018996899945671 Lambda1 0.77796036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 Train Loss 35.80189 Test MSE 36.41010586940972 Test RE 0.1015781131748207 Lambda1 0.7873064\n",
      "71 Train Loss 35.194176 Test MSE 36.153031142209414 Test RE 0.10121888018490588 Lambda1 0.798397\n",
      "72 Train Loss 34.857166 Test MSE 35.8972536295474 Test RE 0.1008601901049809 Lambda1 0.80384487\n",
      "73 Train Loss 34.687088 Test MSE 36.03969245685761 Test RE 0.10106009654106944 Lambda1 0.8042384\n",
      "74 Train Loss 34.518738 Test MSE 35.88691213622972 Test RE 0.1008456608581864 Lambda1 0.8071591\n",
      "Training time: 124.86\n",
      "Training time: 124.86\n",
      "inv_HT_stan_tune11\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 1164.142 Test MSE 870.7740683184668 Test RE 0.49675461927030123 Lambda1 0.024133716\n",
      "1 Train Loss 839.20825 Test MSE 853.8211774674975 Test RE 0.49189525347556107 Lambda1 0.025141867\n",
      "2 Train Loss 831.89197 Test MSE 850.8285152695754 Test RE 0.4910324448747015 Lambda1 0.02629098\n",
      "3 Train Loss 820.99524 Test MSE 838.3550189910851 Test RE 0.48741978633357647 Lambda1 0.035439614\n",
      "4 Train Loss 790.98724 Test MSE 801.0662542207328 Test RE 0.4764566459087597 Lambda1 0.06228962\n",
      "5 Train Loss 743.996 Test MSE 756.892967223565 Test RE 0.46313372341065573 Lambda1 0.072693564\n",
      "6 Train Loss 658.4574 Test MSE 666.9014396346421 Test RE 0.43473038731335273 Lambda1 0.052885436\n",
      "7 Train Loss 490.0555 Test MSE 501.51781021741266 Test RE 0.3769921880272353 Lambda1 -0.0069207167\n",
      "8 Train Loss 350.56345 Test MSE 353.03049039594885 Test RE 0.3162971499308175 Lambda1 0.0017701623\n",
      "9 Train Loss 279.0482 Test MSE 290.3647173635411 Test RE 0.28685412030961793 Lambda1 0.0036539289\n",
      "10 Train Loss 261.77905 Test MSE 280.25590038566605 Test RE 0.28181658833312495 Lambda1 0.0026572302\n",
      "11 Train Loss 256.8421 Test MSE 280.69388829884986 Test RE 0.2820367158513653 Lambda1 0.0015774687\n",
      "12 Train Loss 255.50366 Test MSE 280.1143804439393 Test RE 0.28174542532954233 Lambda1 0.0025133295\n",
      "13 Train Loss 254.44801 Test MSE 280.04032212534946 Test RE 0.2817081780954621 Lambda1 0.0028216708\n",
      "14 Train Loss 253.81223 Test MSE 280.2099745893492 Test RE 0.28179349661321607 Lambda1 0.0029088063\n",
      "15 Train Loss 253.20908 Test MSE 279.4112352882924 Test RE 0.28139158342769516 Lambda1 0.0038373936\n",
      "16 Train Loss 252.60446 Test MSE 278.3098628972915 Test RE 0.2808364466080164 Lambda1 0.0054525128\n",
      "17 Train Loss 251.60812 Test MSE 275.95654431153577 Test RE 0.2796465846015534 Lambda1 0.009118617\n",
      "18 Train Loss 250.19756 Test MSE 272.8683597504541 Test RE 0.27807744264837203 Lambda1 0.014221226\n",
      "19 Train Loss 246.79572 Test MSE 266.32439614068176 Test RE 0.2747227632568041 Lambda1 0.027136598\n",
      "20 Train Loss 241.26857 Test MSE 259.1426411717374 Test RE 0.27099333584372487 Lambda1 0.044265613\n",
      "21 Train Loss 235.50348 Test MSE 252.34059837865303 Test RE 0.2674131346704346 Lambda1 0.05734897\n",
      "22 Train Loss 226.86668 Test MSE 242.3439761057897 Test RE 0.2620627446250342 Lambda1 0.09305855\n",
      "23 Train Loss 221.26903 Test MSE 238.39218928535794 Test RE 0.25991729682575404 Lambda1 0.10755897\n",
      "24 Train Loss 215.86618 Test MSE 231.9927138334679 Test RE 0.25640491346151445 Lambda1 0.11767894\n",
      "25 Train Loss 204.56845 Test MSE 216.13230645828844 Test RE 0.24748507583145898 Lambda1 0.1631757\n",
      "26 Train Loss 198.55005 Test MSE 205.95144245584154 Test RE 0.24158590348913428 Lambda1 0.18744612\n",
      "27 Train Loss 190.277 Test MSE 192.65421779156975 Test RE 0.23365680364371336 Lambda1 0.21650904\n",
      "28 Train Loss 180.6921 Test MSE 183.81086942911273 Test RE 0.22823106944049273 Lambda1 0.24151418\n",
      "29 Train Loss 171.30038 Test MSE 168.9210742282889 Test RE 0.2187918235219949 Lambda1 0.25959736\n",
      "30 Train Loss 160.7821 Test MSE 154.10700402486862 Test RE 0.20897789936703012 Lambda1 0.2894777\n",
      "31 Train Loss 151.60562 Test MSE 141.40701093727102 Test RE 0.20018182384105135 Lambda1 0.31721374\n",
      "32 Train Loss 139.03523 Test MSE 132.30734148932694 Test RE 0.1936337882873223 Lambda1 0.34165475\n",
      "33 Train Loss 131.06679 Test MSE 122.39361851407753 Test RE 0.18623811100387425 Lambda1 0.36495242\n",
      "34 Train Loss 115.91987 Test MSE 105.67869912843261 Test RE 0.17305450121166574 Lambda1 0.39632386\n",
      "35 Train Loss 108.34149 Test MSE 103.25343684700992 Test RE 0.17105722753428929 Lambda1 0.40504012\n",
      "36 Train Loss 102.00886 Test MSE 95.5164799347576 Test RE 0.16452364593622443 Lambda1 0.4232087\n",
      "37 Train Loss 97.3527 Test MSE 86.25692422030268 Test RE 0.156345776783396 Lambda1 0.44611353\n",
      "38 Train Loss 92.27553 Test MSE 82.32773719450063 Test RE 0.15274333193782844 Lambda1 0.46344584\n",
      "39 Train Loss 87.9618 Test MSE 77.2274121435088 Test RE 0.14793635422934184 Lambda1 0.4809027\n",
      "40 Train Loss 84.545555 Test MSE 69.87472061784982 Test RE 0.1407178570808651 Lambda1 0.502225\n",
      "41 Train Loss 80.41493 Test MSE 67.17232054940413 Test RE 0.13796989904918425 Lambda1 0.5096704\n",
      "42 Train Loss 75.85 Test MSE 66.02775550773023 Test RE 0.13678939787869834 Lambda1 0.5181106\n",
      "43 Train Loss 71.139465 Test MSE 61.6516263581157 Test RE 0.13217868882440786 Lambda1 0.5444536\n",
      "44 Train Loss 65.65816 Test MSE 55.33865561526576 Test RE 0.12522858482789825 Lambda1 0.5788908\n",
      "45 Train Loss 61.404068 Test MSE 52.84010918685629 Test RE 0.12236889090453473 Lambda1 0.5982986\n",
      "46 Train Loss 59.36475 Test MSE 51.51428487270287 Test RE 0.12082394404791119 Lambda1 0.6096669\n",
      "47 Train Loss 56.977524 Test MSE 49.99129436715197 Test RE 0.11902449881215753 Lambda1 0.62805134\n",
      "48 Train Loss 54.678288 Test MSE 49.578706629740026 Test RE 0.11853231518184242 Lambda1 0.6440246\n",
      "49 Train Loss 52.81947 Test MSE 47.927056859557865 Test RE 0.11654121749142377 Lambda1 0.65913886\n",
      "50 Train Loss 51.021862 Test MSE 46.37450350105965 Test RE 0.11463805449715762 Lambda1 0.6750375\n",
      "51 Train Loss 49.50672 Test MSE 45.87986465084485 Test RE 0.11402504036304921 Lambda1 0.68350476\n",
      "52 Train Loss 48.691998 Test MSE 45.77167590364518 Test RE 0.1138905204878984 Lambda1 0.68831116\n",
      "53 Train Loss 48.096687 Test MSE 45.650537659857015 Test RE 0.11373971064933876 Lambda1 0.6927421\n",
      "54 Train Loss 46.345844 Test MSE 44.566362021239065 Test RE 0.11238096643283413 Lambda1 0.7113582\n",
      "55 Train Loss 44.759808 Test MSE 43.667998159948574 Test RE 0.11124251842006609 Lambda1 0.72869724\n",
      "56 Train Loss 44.20939 Test MSE 43.33578532888635 Test RE 0.11081856094497157 Lambda1 0.732775\n",
      "57 Train Loss 43.740257 Test MSE 43.40200214807647 Test RE 0.1109031936748484 Lambda1 0.735955\n",
      "58 Train Loss 43.172382 Test MSE 43.07551535647967 Test RE 0.11048527771603796 Lambda1 0.7429708\n",
      "59 Train Loss 42.57066 Test MSE 42.33721925344006 Test RE 0.10953435003038005 Lambda1 0.7543213\n",
      "60 Train Loss 41.779243 Test MSE 42.14995553528295 Test RE 0.10929183833499498 Lambda1 0.7642623\n",
      "61 Train Loss 41.31935 Test MSE 41.84725569299201 Test RE 0.10889869163122716 Lambda1 0.77081877\n",
      "62 Train Loss 41.029503 Test MSE 41.502839098713494 Test RE 0.10844962982520209 Lambda1 0.77509314\n",
      "63 Train Loss 40.45357 Test MSE 41.29324545320209 Test RE 0.10817544227421587 Lambda1 0.7814809\n",
      "64 Train Loss 39.9616 Test MSE 40.784772482655605 Test RE 0.10750735884836043 Lambda1 0.78968364\n",
      "65 Train Loss 39.545975 Test MSE 40.35378078746365 Test RE 0.10693781000213343 Lambda1 0.79921263\n",
      "66 Train Loss 39.157326 Test MSE 39.92680554779524 Test RE 0.10637056179549317 Lambda1 0.8116509\n",
      "67 Train Loss 38.842445 Test MSE 39.54440961928364 Test RE 0.10585995832487825 Lambda1 0.81912535\n",
      "68 Train Loss 38.26277 Test MSE 39.18376970055585 Test RE 0.1053761381161844 Lambda1 0.8354699\n",
      "69 Train Loss 37.9701 Test MSE 39.12282760193001 Test RE 0.10529416103803982 Lambda1 0.8416738\n",
      "70 Train Loss 37.563766 Test MSE 38.99415018128627 Test RE 0.10512085888808509 Lambda1 0.8494534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 Train Loss 37.354034 Test MSE 38.912614649251545 Test RE 0.105010899185479 Lambda1 0.8595839\n",
      "72 Train Loss 36.961784 Test MSE 38.733341430748716 Test RE 0.10476872354516162 Lambda1 0.8737468\n",
      "73 Train Loss 36.666115 Test MSE 38.728203101266246 Test RE 0.10476177405337847 Lambda1 0.87852407\n",
      "74 Train Loss 36.42942 Test MSE 38.76392949725792 Test RE 0.1048100837806274 Lambda1 0.8848151\n",
      "Training time: 123.13\n",
      "Training time: 123.13\n",
      "inv_HT_stan_tune11\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 901.30835 Test MSE 850.3306617832603 Test RE 0.4908887625804519 Lambda1 0.047920898\n",
      "1 Train Loss 837.2658 Test MSE 854.3155790073797 Test RE 0.4920376477978759 Lambda1 0.048929997\n",
      "2 Train Loss 829.94763 Test MSE 846.3798470011253 Test RE 0.489747048749231 Lambda1 0.05002892\n",
      "3 Train Loss 804.6384 Test MSE 825.059500003223 Test RE 0.48353933097377344 Lambda1 0.051611047\n",
      "4 Train Loss 715.08826 Test MSE 737.3172223652847 Test RE 0.45710540837461516 Lambda1 0.05830693\n",
      "5 Train Loss 593.0559 Test MSE 582.277068353683 Test RE 0.4062131833529241 Lambda1 0.049607027\n",
      "6 Train Loss 471.7442 Test MSE 472.7370021180896 Test RE 0.36601507166620817 Lambda1 0.031241387\n",
      "7 Train Loss 378.3829 Test MSE 377.58558852780254 Test RE 0.3271123007376465 Lambda1 0.0036434247\n",
      "8 Train Loss 331.51962 Test MSE 336.30545821856236 Test RE 0.3087138604442858 Lambda1 0.0020777262\n",
      "9 Train Loss 283.93948 Test MSE 295.93070032599877 Test RE 0.28959041386955275 Lambda1 0.003326342\n",
      "10 Train Loss 271.3786 Test MSE 290.9251352086661 Test RE 0.28713080798098356 Lambda1 0.0035102458\n",
      "11 Train Loss 263.69617 Test MSE 284.71458460001236 Test RE 0.28404949920618544 Lambda1 0.005594586\n",
      "12 Train Loss 259.31244 Test MSE 281.46574999955055 Test RE 0.2824242267635722 Lambda1 0.008815168\n",
      "13 Train Loss 256.28568 Test MSE 278.6072870078703 Test RE 0.2809864686470359 Lambda1 0.01045607\n",
      "14 Train Loss 253.01982 Test MSE 275.6612996215575 Test RE 0.2794969482431153 Lambda1 0.013918187\n",
      "15 Train Loss 249.82103 Test MSE 273.28959339854975 Test RE 0.2782919973961518 Lambda1 0.01848843\n",
      "16 Train Loss 246.23453 Test MSE 266.1193064292436 Test RE 0.2746169643433696 Lambda1 0.030791624\n",
      "17 Train Loss 242.27348 Test MSE 260.7160407826036 Test RE 0.27181476678538746 Lambda1 0.04288495\n",
      "18 Train Loss 238.00502 Test MSE 255.5552658670207 Test RE 0.26911108533547234 Lambda1 0.057995856\n",
      "19 Train Loss 233.99016 Test MSE 250.37775971137023 Test RE 0.2663710638557129 Lambda1 0.0761638\n",
      "20 Train Loss 229.7657 Test MSE 246.82830144317396 Test RE 0.26447623143569415 Lambda1 0.08336221\n",
      "21 Train Loss 223.79051 Test MSE 239.05806920513328 Test RE 0.2602800457477814 Lambda1 0.101568356\n",
      "22 Train Loss 214.50803 Test MSE 226.98338725838553 Test RE 0.25362158186861183 Lambda1 0.12671754\n",
      "23 Train Loss 206.51064 Test MSE 218.15734536075908 Test RE 0.2486417711644003 Lambda1 0.14190647\n",
      "24 Train Loss 197.79117 Test MSE 205.10821519973067 Test RE 0.24109083349003976 Lambda1 0.16658825\n",
      "25 Train Loss 188.85529 Test MSE 192.1311169151011 Test RE 0.23333937181935985 Lambda1 0.19413254\n",
      "26 Train Loss 177.22855 Test MSE 179.6971080196776 Test RE 0.22566266594443107 Lambda1 0.21324077\n",
      "27 Train Loss 166.34543 Test MSE 168.17864613247818 Test RE 0.21831048608781636 Lambda1 0.23302567\n",
      "28 Train Loss 156.26094 Test MSE 161.3691175680249 Test RE 0.21384513905135982 Lambda1 0.2469021\n",
      "29 Train Loss 147.70114 Test MSE 151.78536118830695 Test RE 0.20739778548138507 Lambda1 0.26237687\n",
      "30 Train Loss 137.39468 Test MSE 136.0654776840535 Test RE 0.19636457681513936 Lambda1 0.28713375\n",
      "31 Train Loss 131.23228 Test MSE 128.73825758633825 Test RE 0.19100422946050172 Lambda1 0.3047475\n",
      "32 Train Loss 124.36957 Test MSE 121.2547896782009 Test RE 0.18536964659537813 Lambda1 0.32251477\n",
      "33 Train Loss 120.20559 Test MSE 115.8339479924995 Test RE 0.18117868366198425 Lambda1 0.33486065\n",
      "34 Train Loss 115.39014 Test MSE 110.73026976751844 Test RE 0.17714232871404315 Lambda1 0.3494403\n",
      "35 Train Loss 110.03648 Test MSE 104.87411918098942 Test RE 0.1723944712652777 Lambda1 0.36399928\n",
      "36 Train Loss 106.0914 Test MSE 99.07373783544648 Test RE 0.16755926427310758 Lambda1 0.37977958\n",
      "37 Train Loss 100.78347 Test MSE 94.53815026349113 Test RE 0.16367890875671018 Lambda1 0.3973843\n",
      "38 Train Loss 95.858696 Test MSE 89.5358220734068 Test RE 0.15928965929469754 Lambda1 0.41716608\n",
      "39 Train Loss 89.49234 Test MSE 82.45843841723172 Test RE 0.1528645293844129 Lambda1 0.44243732\n",
      "40 Train Loss 82.66307 Test MSE 76.83176751148899 Test RE 0.14755692042687976 Lambda1 0.46496373\n",
      "41 Train Loss 78.96 Test MSE 74.51780994738742 Test RE 0.14531793303980647 Lambda1 0.47166243\n",
      "42 Train Loss 74.43475 Test MSE 72.63241288813528 Test RE 0.1434677893930528 Lambda1 0.4801219\n",
      "43 Train Loss 71.32738 Test MSE 70.35107907378365 Test RE 0.14119670181456323 Lambda1 0.48432893\n",
      "44 Train Loss 69.294174 Test MSE 67.43720249716856 Test RE 0.1382416611252374 Lambda1 0.49106392\n",
      "45 Train Loss 66.95686 Test MSE 63.37501523904719 Test RE 0.13401339487016384 Lambda1 0.50724924\n",
      "46 Train Loss 65.128296 Test MSE 61.24792667529796 Test RE 0.13174521985321427 Lambda1 0.5196405\n",
      "47 Train Loss 62.702744 Test MSE 57.31994266515293 Test RE 0.12745064693987915 Lambda1 0.5402652\n",
      "48 Train Loss 60.338387 Test MSE 53.53625240783087 Test RE 0.12317232920325098 Lambda1 0.5662788\n",
      "49 Train Loss 57.18133 Test MSE 51.101670723436236 Test RE 0.12033908922468263 Lambda1 0.5898953\n",
      "50 Train Loss 54.607994 Test MSE 50.2553251228744 Test RE 0.11933840089689435 Lambda1 0.6028526\n",
      "51 Train Loss 53.29236 Test MSE 49.355390439265165 Test RE 0.11826506275179648 Lambda1 0.6085672\n",
      "52 Train Loss 51.025925 Test MSE 48.09244437460296 Test RE 0.1167421255388808 Lambda1 0.61625606\n",
      "53 Train Loss 48.593502 Test MSE 46.11679028608104 Test RE 0.114319076405236 Lambda1 0.6344673\n",
      "54 Train Loss 45.90233 Test MSE 44.56069675337732 Test RE 0.11237382328177688 Lambda1 0.6487898\n",
      "55 Train Loss 44.62178 Test MSE 43.56492993106828 Test RE 0.11111115972163126 Lambda1 0.6575183\n",
      "56 Train Loss 43.573296 Test MSE 42.5145496010129 Test RE 0.10976350381014652 Lambda1 0.6706008\n",
      "57 Train Loss 42.79142 Test MSE 42.143026587520794 Test RE 0.10928285483129462 Lambda1 0.67571336\n",
      "58 Train Loss 41.706062 Test MSE 41.35157595346058 Test RE 0.10825181918318436 Lambda1 0.6804821\n",
      "59 Train Loss 40.982845 Test MSE 40.330390344977104 Test RE 0.10690681308915558 Lambda1 0.6915441\n",
      "60 Train Loss 40.344646 Test MSE 40.101385531377396 Test RE 0.1066028608163532 Lambda1 0.69523555\n",
      "61 Train Loss 39.601414 Test MSE 39.42118468465065 Test RE 0.10569489372354786 Lambda1 0.6997677\n",
      "62 Train Loss 39.16013 Test MSE 38.967581783952916 Test RE 0.10508504109713822 Lambda1 0.70650893\n",
      "63 Train Loss 38.2986 Test MSE 38.237766488821585 Test RE 0.10409633254312428 Lambda1 0.7214828\n",
      "64 Train Loss 37.42359 Test MSE 37.73497601426765 Test RE 0.10340968370457833 Lambda1 0.7323423\n",
      "65 Train Loss 36.943596 Test MSE 37.763146900541315 Test RE 0.10344827653344953 Lambda1 0.734124\n",
      "66 Train Loss 36.660625 Test MSE 37.5738880464935 Test RE 0.10318872329034487 Lambda1 0.73908645\n",
      "67 Train Loss 36.19917 Test MSE 37.00840303318503 Test RE 0.10240928716051992 Lambda1 0.7526224\n",
      "68 Train Loss 35.698666 Test MSE 36.60329221231518 Test RE 0.10184723547959403 Lambda1 0.7666908\n",
      "69 Train Loss 35.274513 Test MSE 36.466712548018144 Test RE 0.10165704409698977 Lambda1 0.77369297\n",
      "70 Train Loss 34.812824 Test MSE 36.262726948406545 Test RE 0.10137232343223121 Lambda1 0.7821507\n",
      "71 Train Loss 34.47321 Test MSE 36.018058887743365 Test RE 0.10102976028367978 Lambda1 0.7920222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 Train Loss 34.242035 Test MSE 35.90087595113685 Test RE 0.10086527877877932 Lambda1 0.7976092\n",
      "73 Train Loss 34.05714 Test MSE 35.813276922738126 Test RE 0.1007421467297172 Lambda1 0.8015669\n",
      "74 Train Loss 33.71967 Test MSE 35.710864605223634 Test RE 0.10059800154096463 Lambda1 0.8042247\n",
      "Training time: 122.46\n",
      "Training time: 122.46\n",
      "inv_HT_stan_tune11\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 1124.4885 Test MSE 928.0372139867818 Test RE 0.5128281619605922 Lambda1 0.027403623\n",
      "1 Train Loss 837.0384 Test MSE 852.9100538396156 Test RE 0.49163272952586023 Lambda1 0.026479058\n",
      "2 Train Loss 825.0098 Test MSE 842.3074956513728 Test RE 0.48856742066126685 Lambda1 0.022423713\n",
      "3 Train Loss 801.4443 Test MSE 820.2798042470691 Test RE 0.48213668797621 Lambda1 0.012505891\n",
      "4 Train Loss 772.3062 Test MSE 789.2458118882097 Test RE 0.47292831163373145 Lambda1 0.0033439812\n",
      "5 Train Loss 716.1792 Test MSE 731.1029130274547 Test RE 0.45517502820529604 Lambda1 -0.007144932\n",
      "6 Train Loss 605.40985 Test MSE 616.0063474015438 Test RE 0.417812822569541 Lambda1 -0.005486002\n",
      "7 Train Loss 438.09277 Test MSE 434.3698895389715 Test RE 0.3508480174818697 Lambda1 0.014215764\n",
      "8 Train Loss 315.44537 Test MSE 309.90398373849035 Test RE 0.2963485114577251 Lambda1 0.004431183\n",
      "9 Train Loss 278.6506 Test MSE 288.37649750448077 Test RE 0.2858703425162788 Lambda1 0.004413533\n",
      "10 Train Loss 263.6486 Test MSE 282.5706185627566 Test RE 0.28297799933859275 Lambda1 0.0023977188\n",
      "11 Train Loss 259.8777 Test MSE 281.1638001428925 Test RE 0.28227269706926356 Lambda1 0.002781776\n",
      "12 Train Loss 257.6193 Test MSE 279.5269994430502 Test RE 0.2814498697120602 Lambda1 0.0055008754\n",
      "13 Train Loss 255.64171 Test MSE 277.9865131388711 Test RE 0.28067325658015213 Lambda1 0.0073278244\n",
      "14 Train Loss 253.53473 Test MSE 277.68141495913335 Test RE 0.2805191908167439 Lambda1 0.009125223\n",
      "15 Train Loss 251.52017 Test MSE 273.5081755305339 Test RE 0.2784032667259415 Lambda1 0.017382517\n",
      "16 Train Loss 249.14072 Test MSE 270.449860666895 Test RE 0.27684236562948117 Lambda1 0.022670526\n",
      "17 Train Loss 246.93192 Test MSE 268.3429111170744 Test RE 0.2757618818307164 Lambda1 0.02879883\n",
      "18 Train Loss 244.88081 Test MSE 265.9853879028651 Test RE 0.2745478582579229 Lambda1 0.033944786\n",
      "19 Train Loss 241.9955 Test MSE 261.1540463423941 Test RE 0.27204299673378296 Lambda1 0.042479757\n",
      "20 Train Loss 238.78734 Test MSE 256.02064557315595 Test RE 0.26935600666902326 Lambda1 0.052737534\n",
      "21 Train Loss 236.83577 Test MSE 254.61508807199277 Test RE 0.26861560464059503 Lambda1 0.056537926\n",
      "22 Train Loss 231.83545 Test MSE 247.02343753606598 Test RE 0.2645807548271269 Lambda1 0.07212622\n",
      "23 Train Loss 226.14577 Test MSE 240.99204700618043 Test RE 0.26133075667554273 Lambda1 0.08878799\n",
      "24 Train Loss 219.64627 Test MSE 236.00919070578334 Test RE 0.25861495090450953 Lambda1 0.10804147\n",
      "25 Train Loss 212.58507 Test MSE 227.6477327198529 Test RE 0.2539924663663175 Lambda1 0.12851141\n",
      "26 Train Loss 203.06665 Test MSE 218.45896214879264 Test RE 0.24881359354553167 Lambda1 0.15163727\n",
      "27 Train Loss 192.98918 Test MSE 204.75280488034193 Test RE 0.24088186252959412 Lambda1 0.19116409\n",
      "28 Train Loss 187.10223 Test MSE 196.67265559077316 Test RE 0.23608106822084485 Lambda1 0.2092344\n",
      "29 Train Loss 178.8977 Test MSE 188.01506297582878 Test RE 0.23082640774717386 Lambda1 0.2324795\n",
      "30 Train Loss 170.05531 Test MSE 173.89384170797652 Test RE 0.22198890639654173 Lambda1 0.2638467\n",
      "31 Train Loss 162.75383 Test MSE 166.59698542642928 Test RE 0.217281494579183 Lambda1 0.26760226\n",
      "32 Train Loss 149.6241 Test MSE 145.7254537055464 Test RE 0.20321552272900656 Lambda1 0.29884553\n",
      "33 Train Loss 140.07169 Test MSE 132.61294319184307 Test RE 0.1938572856526584 Lambda1 0.32363904\n",
      "34 Train Loss 134.09537 Test MSE 122.52944917998796 Test RE 0.18634142452755242 Lambda1 0.33752564\n",
      "35 Train Loss 124.012024 Test MSE 115.36599264122073 Test RE 0.1808123431449069 Lambda1 0.35561016\n",
      "36 Train Loss 114.4974 Test MSE 103.52369641123204 Test RE 0.17128094715928804 Lambda1 0.3828565\n",
      "37 Train Loss 110.18477 Test MSE 96.89563870157161 Test RE 0.16570716433976546 Lambda1 0.39485276\n",
      "38 Train Loss 107.03894 Test MSE 95.03523450327923 Test RE 0.16410865872943975 Lambda1 0.3985\n",
      "39 Train Loss 100.689 Test MSE 90.32236056196605 Test RE 0.15998777931056116 Lambda1 0.4084317\n",
      "40 Train Loss 96.05358 Test MSE 82.02352842786321 Test RE 0.15246087024490698 Lambda1 0.42432564\n",
      "41 Train Loss 88.2976 Test MSE 75.1307719022792 Test RE 0.145914380646832 Lambda1 0.44303975\n",
      "42 Train Loss 82.722084 Test MSE 73.6067541093648 Test RE 0.14442687154029804 Lambda1 0.45168954\n",
      "43 Train Loss 78.350296 Test MSE 67.93698200611821 Test RE 0.13875297247378915 Lambda1 0.4664711\n",
      "44 Train Loss 74.99575 Test MSE 66.30528044577397 Test RE 0.1370765700076832 Lambda1 0.4709752\n",
      "45 Train Loss 72.450836 Test MSE 64.89689285373915 Test RE 0.13561293730607427 Lambda1 0.47812158\n",
      "46 Train Loss 69.20485 Test MSE 63.335861361793974 Test RE 0.1339719908982556 Lambda1 0.48367622\n",
      "47 Train Loss 66.51207 Test MSE 61.455839792505884 Test RE 0.13196864254069537 Lambda1 0.49327776\n",
      "48 Train Loss 65.46341 Test MSE 59.523444688908825 Test RE 0.12987728422683215 Lambda1 0.5048818\n",
      "49 Train Loss 64.108665 Test MSE 59.215980985937335 Test RE 0.12954141445584147 Lambda1 0.51334995\n",
      "50 Train Loss 62.743378 Test MSE 58.96535636959185 Test RE 0.12926698944376477 Lambda1 0.51189524\n",
      "51 Train Loss 61.610023 Test MSE 57.333581841995404 Test RE 0.1274658093636024 Lambda1 0.52076083\n",
      "52 Train Loss 60.863205 Test MSE 56.850430116419865 Test RE 0.12692759407453083 Lambda1 0.5267153\n",
      "53 Train Loss 59.87719 Test MSE 56.751228613228186 Test RE 0.12681680418460342 Lambda1 0.52421504\n",
      "54 Train Loss 59.16837 Test MSE 55.989168332157305 Test RE 0.12596247311640946 Lambda1 0.5248206\n",
      "55 Train Loss 58.485542 Test MSE 55.4114477410492 Test RE 0.1253109202144202 Lambda1 0.5302635\n",
      "56 Train Loss 57.4172 Test MSE 54.25517903252782 Test RE 0.12399659833401656 Lambda1 0.53775233\n",
      "57 Train Loss 56.56263 Test MSE 54.02998682926487 Test RE 0.12373899986676509 Lambda1 0.5389945\n",
      "58 Train Loss 55.733562 Test MSE 53.647326165759864 Test RE 0.12330003822262672 Lambda1 0.54328996\n",
      "59 Train Loss 54.82099 Test MSE 52.980341757950676 Test RE 0.12253116095112777 Lambda1 0.548998\n",
      "60 Train Loss 53.442177 Test MSE 52.48466937334413 Test RE 0.12195662673995793 Lambda1 0.55863106\n",
      "61 Train Loss 52.68879 Test MSE 51.947670157172425 Test RE 0.12133112034182983 Lambda1 0.56790996\n",
      "62 Train Loss 51.328537 Test MSE 50.367155941764146 Test RE 0.11947110618650778 Lambda1 0.5817142\n",
      "63 Train Loss 50.868732 Test MSE 49.726334534753846 Test RE 0.1187086577257537 Lambda1 0.58734405\n",
      "64 Train Loss 50.15258 Test MSE 49.605471727475816 Test RE 0.11856430573947177 Lambda1 0.5902619\n",
      "65 Train Loss 49.562027 Test MSE 49.24405614791459 Test RE 0.11813159819380867 Lambda1 0.5901977\n",
      "66 Train Loss 49.152794 Test MSE 48.66057537129074 Test RE 0.11742965650746451 Lambda1 0.59876645\n",
      "67 Train Loss 48.552753 Test MSE 47.90034631373283 Test RE 0.11650873778611644 Lambda1 0.61454326\n",
      "68 Train Loss 48.1052 Test MSE 47.26757197975045 Test RE 0.11573662606846367 Lambda1 0.6212649\n",
      "69 Train Loss 47.530014 Test MSE 46.822455447835694 Test RE 0.11519039384729138 Lambda1 0.6286505\n",
      "70 Train Loss 46.98462 Test MSE 46.67830347820725 Test RE 0.11501293922565195 Lambda1 0.6297853\n",
      "71 Train Loss 46.610577 Test MSE 46.338753983284775 Test RE 0.11459385946556375 Lambda1 0.6343512\n",
      "72 Train Loss 46.07773 Test MSE 45.70135795126789 Test RE 0.11380300319430076 Lambda1 0.64398676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 Train Loss 45.636993 Test MSE 45.1849943677629 Test RE 0.11315826678764158 Lambda1 0.6541511\n",
      "74 Train Loss 45.303886 Test MSE 44.82292805522706 Test RE 0.11270398764076647 Lambda1 0.66309017\n",
      "Training time: 125.63\n",
      "Training time: 125.63\n",
      "inv_HT_stan_tune11\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 1006.8478 Test MSE 839.5992154987619 Test RE 0.48778134029550024 Lambda1 0.137688\n",
      "1 Train Loss 811.22546 Test MSE 830.0592144534547 Test RE 0.48500219949494494 Lambda1 0.14356554\n",
      "2 Train Loss 754.2583 Test MSE 770.1802818328313 Test RE 0.46718121137879365 Lambda1 0.15019794\n",
      "3 Train Loss 645.7927 Test MSE 653.9950235649653 Test RE 0.43050320840803696 Lambda1 0.13649575\n",
      "4 Train Loss 546.6664 Test MSE 545.0399341432893 Test RE 0.3930097569107605 Lambda1 0.10288248\n",
      "5 Train Loss 416.9309 Test MSE 414.3713777526116 Test RE 0.34267628129351413 Lambda1 0.06469657\n",
      "6 Train Loss 342.62936 Test MSE 345.91665987794806 Test RE 0.31309411893367917 Lambda1 0.041861083\n",
      "7 Train Loss 294.47095 Test MSE 305.22662742799565 Test RE 0.2941036264654501 Lambda1 0.0446273\n",
      "8 Train Loss 268.57236 Test MSE 283.0809970818265 Test RE 0.28323344121070254 Lambda1 0.048954397\n",
      "9 Train Loss 253.20375 Test MSE 268.6527928808851 Test RE 0.2759210604760816 Lambda1 0.04816654\n",
      "10 Train Loss 241.66331 Test MSE 259.91288570609674 Test RE 0.27139577108761215 Lambda1 0.04893249\n",
      "11 Train Loss 233.77722 Test MSE 248.29648873763225 Test RE 0.26526164568448335 Lambda1 0.06470641\n",
      "12 Train Loss 221.02979 Test MSE 235.68702277578157 Test RE 0.25843837749224147 Lambda1 0.09665043\n",
      "13 Train Loss 209.61824 Test MSE 216.0355521565021 Test RE 0.247429674752015 Lambda1 0.12610927\n",
      "14 Train Loss 203.90494 Test MSE 209.01287745157106 Test RE 0.24337484767876222 Lambda1 0.13940758\n",
      "15 Train Loss 193.40295 Test MSE 192.69577727925622 Test RE 0.2336820045802006 Lambda1 0.16888939\n",
      "16 Train Loss 186.8666 Test MSE 181.35443320157913 Test RE 0.22670090758788952 Lambda1 0.1902361\n",
      "17 Train Loss 176.08731 Test MSE 170.99808355850172 Test RE 0.22013281749546204 Lambda1 0.21918993\n",
      "18 Train Loss 168.2169 Test MSE 163.81282338262125 Test RE 0.21545824521769297 Lambda1 0.23597752\n",
      "19 Train Loss 159.9017 Test MSE 160.1669506360278 Test RE 0.21304709868208696 Lambda1 0.24641766\n",
      "20 Train Loss 150.69498 Test MSE 144.94420290382192 Test RE 0.2026700598602777 Lambda1 0.272624\n",
      "21 Train Loss 143.71341 Test MSE 138.08468171937116 Test RE 0.1978162306707485 Lambda1 0.28499603\n",
      "22 Train Loss 134.43762 Test MSE 130.51551126505228 Test RE 0.19231813337518908 Lambda1 0.30231178\n",
      "23 Train Loss 126.85036 Test MSE 117.18737127478663 Test RE 0.18223407085308513 Lambda1 0.33306828\n",
      "24 Train Loss 120.91998 Test MSE 112.41899458412786 Test RE 0.17848799830273696 Lambda1 0.3492468\n",
      "25 Train Loss 110.83347 Test MSE 106.03548995178743 Test RE 0.1733463870470515 Lambda1 0.36634934\n",
      "26 Train Loss 103.638985 Test MSE 98.08748468704181 Test RE 0.1667231739775553 Lambda1 0.38161618\n",
      "27 Train Loss 100.18072 Test MSE 95.374146976187 Test RE 0.16440101857648554 Lambda1 0.38822696\n",
      "28 Train Loss 95.34536 Test MSE 89.54776571322967 Test RE 0.15930028317032802 Lambda1 0.402432\n",
      "29 Train Loss 91.55449 Test MSE 89.00988093250177 Test RE 0.15882112963706188 Lambda1 0.4051796\n",
      "30 Train Loss 89.11804 Test MSE 86.57127178652354 Test RE 0.15663040447871965 Lambda1 0.413338\n",
      "31 Train Loss 86.66886 Test MSE 82.13988188057913 Test RE 0.1525689676559309 Lambda1 0.42656064\n",
      "32 Train Loss 83.71853 Test MSE 77.93307298944559 Test RE 0.14861069707182925 Lambda1 0.44495267\n",
      "33 Train Loss 81.01984 Test MSE 75.93068688926103 Test RE 0.14668909694315704 Lambda1 0.45167145\n",
      "34 Train Loss 77.813995 Test MSE 73.19828157849615 Test RE 0.14402557356104348 Lambda1 0.46401298\n",
      "35 Train Loss 75.20532 Test MSE 69.7031452175913 Test RE 0.1405449865368407 Lambda1 0.48059654\n",
      "36 Train Loss 72.46294 Test MSE 67.28417605997957 Test RE 0.13808472517161077 Lambda1 0.49014503\n",
      "37 Train Loss 70.53175 Test MSE 65.9618568839086 Test RE 0.1367211199001552 Lambda1 0.4990429\n",
      "38 Train Loss 68.678116 Test MSE 64.63298729733454 Test RE 0.1353369189591977 Lambda1 0.5080263\n",
      "39 Train Loss 67.01745 Test MSE 62.44417669169906 Test RE 0.1330255743169781 Lambda1 0.51883465\n",
      "40 Train Loss 65.25712 Test MSE 61.643462833893786 Test RE 0.13216993739549987 Lambda1 0.5236555\n",
      "41 Train Loss 63.13278 Test MSE 59.59158755953271 Test RE 0.12995160518998133 Lambda1 0.53225905\n",
      "42 Train Loss 60.588905 Test MSE 57.3585884724346 Test RE 0.12749360409399957 Lambda1 0.5411902\n",
      "43 Train Loss 59.182747 Test MSE 56.56980519508062 Test RE 0.12661393672392157 Lambda1 0.547474\n",
      "44 Train Loss 58.249107 Test MSE 54.39805488981761 Test RE 0.12415975761674308 Lambda1 0.5607204\n",
      "45 Train Loss 57.467426 Test MSE 53.312707472923314 Test RE 0.12291490220355873 Lambda1 0.56780005\n",
      "46 Train Loss 56.657276 Test MSE 52.790743013351005 Test RE 0.12231171563005987 Lambda1 0.5716607\n",
      "47 Train Loss 55.598637 Test MSE 51.231303084912355 Test RE 0.12049162787344903 Lambda1 0.5845313\n",
      "48 Train Loss 54.230026 Test MSE 50.39321440640534 Test RE 0.11950200758414056 Lambda1 0.59134585\n",
      "49 Train Loss 52.354866 Test MSE 49.459650883737524 Test RE 0.11838991094940565 Lambda1 0.60128343\n",
      "50 Train Loss 50.64313 Test MSE 48.249158657496515 Test RE 0.11693217907831972 Lambda1 0.6129899\n",
      "51 Train Loss 49.039288 Test MSE 47.33851388121256 Test RE 0.1158234456105015 Lambda1 0.6254533\n",
      "52 Train Loss 47.688072 Test MSE 46.49733861156518 Test RE 0.11478977866307909 Lambda1 0.63761187\n",
      "53 Train Loss 46.70982 Test MSE 45.257222932035546 Test RE 0.1132486728573267 Lambda1 0.6508576\n",
      "54 Train Loss 45.417885 Test MSE 44.373698042902596 Test RE 0.1121377873354354 Lambda1 0.6633883\n",
      "55 Train Loss 44.881523 Test MSE 43.92212143511971 Test RE 0.1115657334248311 Lambda1 0.6695192\n",
      "56 Train Loss 44.145355 Test MSE 43.5590599936024 Test RE 0.11110367391215428 Lambda1 0.67444086\n",
      "57 Train Loss 43.512444 Test MSE 43.09705427347083 Test RE 0.11051289707006617 Lambda1 0.6812969\n",
      "58 Train Loss 42.537632 Test MSE 41.75839200207194 Test RE 0.1087830056395392 Lambda1 0.697598\n",
      "59 Train Loss 41.83119 Test MSE 41.437266473963284 Test RE 0.10836392318258589 Lambda1 0.7041514\n",
      "60 Train Loss 41.509884 Test MSE 41.40660452317729 Test RE 0.1083238232381709 Lambda1 0.70476514\n",
      "61 Train Loss 41.244595 Test MSE 41.2439997115413 Test RE 0.10811091877826075 Lambda1 0.7054953\n",
      "62 Train Loss 40.974586 Test MSE 41.153206462937746 Test RE 0.10799185722306913 Lambda1 0.7074871\n",
      "63 Train Loss 40.719864 Test MSE 41.052800753138925 Test RE 0.1078600373506359 Lambda1 0.70952445\n",
      "64 Train Loss 40.267975 Test MSE 40.869118112956734 Test RE 0.10761846762796712 Lambda1 0.7147393\n",
      "65 Train Loss 39.88298 Test MSE 40.41412154215515 Test RE 0.1070177318541012 Lambda1 0.7226494\n",
      "66 Train Loss 39.551777 Test MSE 39.95617120092248 Test RE 0.10640967169721749 Lambda1 0.7346124\n",
      "67 Train Loss 39.054016 Test MSE 39.38022972214869 Test RE 0.10563997584987539 Lambda1 0.7530612\n",
      "68 Train Loss 38.81431 Test MSE 39.013307025836504 Test RE 0.10514667733359213 Lambda1 0.76134443\n",
      "69 Train Loss 38.509666 Test MSE 38.90218453309585 Test RE 0.10499682471001497 Lambda1 0.7683386\n",
      "70 Train Loss 38.23409 Test MSE 38.77696531400193 Test RE 0.10482770544870959 Lambda1 0.7730002\n",
      "71 Train Loss 37.973434 Test MSE 38.380419176330555 Test RE 0.10429032658298383 Lambda1 0.77654564\n",
      "72 Train Loss 37.70838 Test MSE 38.223301842110686 Test RE 0.10407664181278602 Lambda1 0.7816849\n",
      "73 Train Loss 37.434444 Test MSE 38.01287765726418 Test RE 0.10378976881096152 Lambda1 0.7887827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 Train Loss 37.226646 Test MSE 37.710245173969284 Test RE 0.10337579170568673 Lambda1 0.79537916\n",
      "Training time: 117.58\n",
      "Training time: 117.58\n",
      "inv_HT_stan_tune11\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 936.2925 Test MSE 871.3692157637583 Test RE 0.49692434856920775 Lambda1 -0.05407849\n",
      "1 Train Loss 838.94183 Test MSE 857.8847337269897 Test RE 0.4930643925630838 Lambda1 -0.055477932\n",
      "2 Train Loss 822.1691 Test MSE 838.5578921583589 Test RE 0.48747875801745816 Lambda1 -0.054262225\n",
      "3 Train Loss 789.95715 Test MSE 804.8473123158391 Test RE 0.47757976740731906 Lambda1 -0.039585855\n",
      "4 Train Loss 724.88196 Test MSE 731.8565039884608 Test RE 0.45540955568062674 Lambda1 -0.013985332\n",
      "5 Train Loss 528.92395 Test MSE 533.6974025726165 Test RE 0.38889890041597636 Lambda1 0.028372318\n",
      "6 Train Loss 372.2951 Test MSE 375.59712859693485 Test RE 0.3262498362492538 Lambda1 0.0030610675\n",
      "7 Train Loss 310.1039 Test MSE 321.3325296972625 Test RE 0.30176336722023367 Lambda1 -0.0057296036\n",
      "8 Train Loss 272.13043 Test MSE 287.9751066237459 Test RE 0.28567132195465217 Lambda1 0.002108497\n",
      "9 Train Loss 262.33002 Test MSE 280.7315137604754 Test RE 0.2820556179480684 Lambda1 0.0017754716\n",
      "10 Train Loss 257.99738 Test MSE 280.15887894942034 Test RE 0.2817678032461299 Lambda1 0.0016171539\n",
      "11 Train Loss 256.8193 Test MSE 280.4232936396737 Test RE 0.2819007384859976 Lambda1 0.0020667987\n",
      "12 Train Loss 255.8221 Test MSE 280.6177312440469 Test RE 0.28199845256237055 Lambda1 0.0025043427\n",
      "13 Train Loss 254.81868 Test MSE 280.495772200326 Test RE 0.28193716634381405 Lambda1 0.0026110832\n",
      "14 Train Loss 254.2835 Test MSE 280.91362918291304 Test RE 0.28214709031148755 Lambda1 0.0027431003\n",
      "15 Train Loss 253.86249 Test MSE 281.02292968527894 Test RE 0.28220197518770596 Lambda1 0.0027672022\n",
      "16 Train Loss 253.52588 Test MSE 280.6979310079127 Test RE 0.2820387468686574 Lambda1 0.0030988825\n",
      "17 Train Loss 253.27788 Test MSE 280.7180517226548 Test RE 0.2820488551004156 Lambda1 0.0032343855\n",
      "18 Train Loss 253.04678 Test MSE 280.78369827624334 Test RE 0.28208183205508397 Lambda1 0.0030255963\n",
      "19 Train Loss 252.81883 Test MSE 280.5816768861143 Test RE 0.2819803361024819 Lambda1 0.0031219122\n",
      "20 Train Loss 252.41597 Test MSE 279.805902269111 Test RE 0.28159024540654615 Lambda1 0.0042301477\n",
      "21 Train Loss 252.15726 Test MSE 279.36252911300267 Test RE 0.281367056666805 Lambda1 0.004946646\n",
      "22 Train Loss 251.4628 Test MSE 277.75275714167014 Test RE 0.2805552241358157 Lambda1 0.006797176\n",
      "23 Train Loss 250.34573 Test MSE 275.01048093227064 Test RE 0.27916681606761107 Lambda1 0.011419255\n",
      "24 Train Loss 248.89754 Test MSE 272.96022166180035 Test RE 0.27812424649152 Lambda1 0.016122844\n",
      "25 Train Loss 246.67499 Test MSE 268.20112926518163 Test RE 0.2756890213330136 Lambda1 0.025094125\n",
      "26 Train Loss 242.67987 Test MSE 261.7511852276076 Test RE 0.27235383759472787 Lambda1 0.041470043\n",
      "27 Train Loss 237.64476 Test MSE 252.99118831051257 Test RE 0.26775763789574786 Lambda1 0.058280353\n",
      "28 Train Loss 231.52452 Test MSE 247.34951090607572 Test RE 0.2647553218394424 Lambda1 0.07703868\n",
      "29 Train Loss 225.31197 Test MSE 240.69006888064277 Test RE 0.2611669734506308 Lambda1 0.08890946\n",
      "30 Train Loss 214.74263 Test MSE 229.20382732157015 Test RE 0.25485907546126446 Lambda1 0.113341376\n",
      "31 Train Loss 208.88708 Test MSE 223.60875736955308 Test RE 0.2517291878860304 Lambda1 0.12868863\n",
      "32 Train Loss 201.2195 Test MSE 212.0239550312635 Test RE 0.24512163024833 Lambda1 0.15123789\n",
      "33 Train Loss 196.28062 Test MSE 206.6539128865414 Test RE 0.24199755996137348 Lambda1 0.1654299\n",
      "34 Train Loss 186.29839 Test MSE 192.06452460340054 Test RE 0.23329893080419073 Lambda1 0.19505824\n",
      "35 Train Loss 178.9327 Test MSE 184.68854814206472 Test RE 0.22877531092562553 Lambda1 0.21227993\n",
      "36 Train Loss 169.96986 Test MSE 172.52905883275048 Test RE 0.2211160651054103 Lambda1 0.23666814\n",
      "37 Train Loss 159.54329 Test MSE 157.80763669596683 Test RE 0.211472148978232 Lambda1 0.2657248\n",
      "38 Train Loss 150.66855 Test MSE 144.51593736961738 Test RE 0.2023704245215678 Lambda1 0.29936332\n",
      "39 Train Loss 144.55206 Test MSE 137.74627320750542 Test RE 0.19757368471853734 Lambda1 0.31852353\n",
      "40 Train Loss 137.87602 Test MSE 133.5710296581653 Test RE 0.19455630408897062 Lambda1 0.3303731\n",
      "41 Train Loss 132.2329 Test MSE 128.44696492692867 Test RE 0.19078801697094852 Lambda1 0.34458193\n",
      "42 Train Loss 126.65972 Test MSE 124.99581075150573 Test RE 0.188207488572965 Lambda1 0.35221505\n",
      "43 Train Loss 116.13553 Test MSE 114.95901487019545 Test RE 0.1804931346178731 Lambda1 0.37536663\n",
      "44 Train Loss 109.46661 Test MSE 109.53322113490613 Test RE 0.1761822290624579 Lambda1 0.3891159\n",
      "45 Train Loss 105.102005 Test MSE 104.88286062519602 Test RE 0.17240165580834174 Lambda1 0.39704454\n",
      "46 Train Loss 101.41562 Test MSE 100.99984614395964 Test RE 0.16918019715715757 Lambda1 0.4100666\n",
      "47 Train Loss 95.37427 Test MSE 96.16222827385168 Test RE 0.16507884809975154 Lambda1 0.43071476\n",
      "48 Train Loss 92.94734 Test MSE 94.15270516116843 Test RE 0.16334489719171902 Lambda1 0.4362725\n",
      "49 Train Loss 89.34373 Test MSE 89.71029864771913 Test RE 0.15944478596375236 Lambda1 0.4500157\n",
      "50 Train Loss 85.30052 Test MSE 83.73970153597013 Test RE 0.15404757814506043 Lambda1 0.46979076\n",
      "51 Train Loss 81.43568 Test MSE 78.82606600695155 Test RE 0.14945969682276303 Lambda1 0.48594585\n",
      "52 Train Loss 79.0633 Test MSE 75.47851635117448 Test RE 0.1462516747822683 Lambda1 0.49891663\n",
      "53 Train Loss 75.832214 Test MSE 69.59284766510902 Test RE 0.14043374402907943 Lambda1 0.51712364\n",
      "54 Train Loss 73.30032 Test MSE 68.56023965458351 Test RE 0.13938798313334874 Lambda1 0.521183\n",
      "55 Train Loss 71.0392 Test MSE 65.51858557035568 Test RE 0.1362609546301725 Lambda1 0.52980417\n",
      "56 Train Loss 69.019005 Test MSE 62.034677069143825 Test RE 0.13258867589526796 Lambda1 0.5394877\n",
      "57 Train Loss 67.99965 Test MSE 60.78249882245003 Test RE 0.1312436940649797 Lambda1 0.54407376\n",
      "58 Train Loss 66.70516 Test MSE 59.29973944223302 Test RE 0.12963299745970616 Lambda1 0.5487971\n",
      "59 Train Loss 64.83465 Test MSE 57.94737323329736 Test RE 0.12814629305580655 Lambda1 0.55193055\n",
      "60 Train Loss 63.33259 Test MSE 57.22813867580984 Test RE 0.1273485431524817 Lambda1 0.5548484\n",
      "61 Train Loss 62.095436 Test MSE 56.550360406620385 Test RE 0.1265921742942267 Lambda1 0.5586931\n",
      "62 Train Loss 60.44609 Test MSE 54.814470066331836 Test RE 0.12463407089997035 Lambda1 0.56465375\n",
      "63 Train Loss 58.697598 Test MSE 53.492444492233034 Test RE 0.12312192385116105 Lambda1 0.5708886\n",
      "64 Train Loss 57.198357 Test MSE 52.557900206735546 Test RE 0.12204167893124053 Lambda1 0.5750394\n",
      "65 Train Loss 54.964172 Test MSE 50.744262308737476 Test RE 0.11991752109703516 Lambda1 0.58452797\n",
      "66 Train Loss 53.653442 Test MSE 49.96998714624346 Test RE 0.11899913087949343 Lambda1 0.5885921\n",
      "67 Train Loss 53.058743 Test MSE 49.50072487648875 Test RE 0.11843905946848937 Lambda1 0.5904811\n",
      "68 Train Loss 51.958588 Test MSE 48.714312157911465 Test RE 0.11749447850313807 Lambda1 0.59749883\n",
      "69 Train Loss 51.190018 Test MSE 48.20670867936425 Test RE 0.11688072884733264 Lambda1 0.60085124\n",
      "70 Train Loss 50.611736 Test MSE 48.02299077203104 Test RE 0.11665779742040061 Lambda1 0.6011377\n",
      "71 Train Loss 50.028656 Test MSE 48.01031426244407 Test RE 0.11664239946968785 Lambda1 0.60233885\n",
      "72 Train Loss 49.06642 Test MSE 47.57726302254118 Test RE 0.11611515279875453 Lambda1 0.60610276\n",
      "73 Train Loss 48.609016 Test MSE 47.07533456411152 Test RE 0.11550103561312186 Lambda1 0.61030453\n",
      "74 Train Loss 48.074318 Test MSE 46.79414099656847 Test RE 0.1151555596422917 Lambda1 0.6143992\n",
      "Training time: 66.86\n",
      "Training time: 66.86\n",
      "inv_HT_stan_tune11\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 1352.2903 Test MSE 887.5313849561084 Test RE 0.5015116550523313 Lambda1 0.09592885\n",
      "1 Train Loss 827.06995 Test MSE 841.4198120505865 Test RE 0.4883099092334903 Lambda1 0.09254618\n",
      "2 Train Loss 790.9115 Test MSE 806.478341950163 Test RE 0.47806343213810487 Lambda1 0.08729699\n",
      "3 Train Loss 687.6124 Test MSE 678.5794163370408 Test RE 0.4385201071548395 Lambda1 0.05858414\n",
      "4 Train Loss 500.25375 Test MSE 486.81245082125 Test RE 0.3714240397909434 Lambda1 0.028377302\n",
      "5 Train Loss 362.93567 Test MSE 357.23408506226974 Test RE 0.31817467968958724 Lambda1 0.020909008\n",
      "6 Train Loss 306.60672 Test MSE 312.6696793571727 Test RE 0.29766793507486855 Lambda1 0.013938154\n",
      "7 Train Loss 275.63696 Test MSE 292.54511061120866 Test RE 0.2879291218138248 Lambda1 0.011308147\n",
      "8 Train Loss 262.2383 Test MSE 281.6194745313243 Test RE 0.28250134024134577 Lambda1 0.011577689\n",
      "9 Train Loss 258.25085 Test MSE 279.040861803551 Test RE 0.28120502231585487 Lambda1 0.011588687\n",
      "10 Train Loss 256.0218 Test MSE 277.68341481937773 Test RE 0.2805202009638715 Lambda1 0.012557819\n",
      "11 Train Loss 254.47931 Test MSE 276.36747524216094 Test RE 0.27985472005781376 Lambda1 0.013892905\n",
      "12 Train Loss 251.86894 Test MSE 272.9252779798652 Test RE 0.27810644353801817 Lambda1 0.021278948\n",
      "13 Train Loss 249.69044 Test MSE 269.35103499979164 Test RE 0.27627939415130937 Lambda1 0.028384356\n",
      "14 Train Loss 244.11562 Test MSE 263.3685842952003 Test RE 0.2731939989312926 Lambda1 0.043953653\n",
      "15 Train Loss 236.9602 Test MSE 253.8621685516918 Test RE 0.2682181504480493 Lambda1 0.06440279\n",
      "16 Train Loss 228.28139 Test MSE 244.31605329629312 Test RE 0.2631268536711716 Lambda1 0.102597944\n",
      "17 Train Loss 219.27942 Test MSE 240.0332277541061 Test RE 0.2608103687802094 Lambda1 0.1249895\n",
      "18 Train Loss 211.21463 Test MSE 230.07558214972187 Test RE 0.2553432815452779 Lambda1 0.14002585\n",
      "19 Train Loss 202.45679 Test MSE 216.89390968037526 Test RE 0.24792073415750143 Lambda1 0.16556264\n",
      "20 Train Loss 191.97144 Test MSE 199.71871474263784 Test RE 0.23790225133845597 Lambda1 0.19278309\n",
      "21 Train Loss 182.90419 Test MSE 187.08058559605817 Test RE 0.23025206348077232 Lambda1 0.22080497\n",
      "22 Train Loss 170.42049 Test MSE 173.09595954240297 Test RE 0.22147904179344743 Lambda1 0.2565794\n",
      "23 Train Loss 158.59299 Test MSE 158.69010992047058 Test RE 0.2120626094386553 Lambda1 0.27580696\n",
      "24 Train Loss 149.22697 Test MSE 146.66704792902414 Test RE 0.2038709966326092 Lambda1 0.2961968\n",
      "25 Train Loss 142.82065 Test MSE 141.26893477596272 Test RE 0.20008406670641687 Lambda1 0.29985687\n",
      "26 Train Loss 127.51255 Test MSE 118.14571016373948 Test RE 0.18297769364833033 Lambda1 0.34688026\n",
      "27 Train Loss 115.624596 Test MSE 103.08702465473756 Test RE 0.17091932661819667 Lambda1 0.38731578\n",
      "28 Train Loss 106.1958 Test MSE 95.32161201618925 Test RE 0.16435573381699498 Lambda1 0.40405372\n",
      "29 Train Loss 97.38682 Test MSE 91.51669198128229 Test RE 0.16104206373933463 Lambda1 0.42378226\n",
      "30 Train Loss 86.66232 Test MSE 81.5845961720184 Test RE 0.1520523913454637 Lambda1 0.4539567\n",
      "31 Train Loss 81.013725 Test MSE 75.96809314986677 Test RE 0.14672522472754557 Lambda1 0.4685779\n",
      "32 Train Loss 77.96103 Test MSE 75.19693333275112 Test RE 0.14597861384644545 Lambda1 0.46959317\n",
      "33 Train Loss 74.4054 Test MSE 71.49197699716949 Test RE 0.14233700525538742 Lambda1 0.48409316\n",
      "34 Train Loss 69.71971 Test MSE 65.18696002891043 Test RE 0.13591567139898852 Lambda1 0.50421953\n",
      "35 Train Loss 65.9707 Test MSE 61.54037176863193 Test RE 0.13205937221517972 Lambda1 0.5192481\n",
      "36 Train Loss 62.364937 Test MSE 59.280518154322365 Test RE 0.12961198627873233 Lambda1 0.53212285\n",
      "37 Train Loss 59.630875 Test MSE 56.681804077625266 Test RE 0.12673921210268982 Lambda1 0.5445149\n",
      "38 Train Loss 58.02735 Test MSE 55.020786885563886 Test RE 0.1248684064145719 Lambda1 0.55707073\n",
      "39 Train Loss 55.902752 Test MSE 53.5325673808376 Test RE 0.12316809000879461 Lambda1 0.56972957\n",
      "40 Train Loss 54.784214 Test MSE 52.722455448635806 Test RE 0.12223258175616744 Lambda1 0.5758562\n",
      "41 Train Loss 52.68612 Test MSE 51.03328778968854 Test RE 0.12025854494209591 Lambda1 0.59529465\n",
      "42 Train Loss 50.472042 Test MSE 48.888208196353865 Test RE 0.1177040023780467 Lambda1 0.6107918\n",
      "43 Train Loss 49.250175 Test MSE 48.18769728840843 Test RE 0.11685767931319456 Lambda1 0.6169242\n",
      "44 Train Loss 48.69572 Test MSE 47.73726817616846 Test RE 0.11631023998226946 Lambda1 0.6213205\n",
      "45 Train Loss 47.82332 Test MSE 46.90099287864396 Test RE 0.1152869604096814 Lambda1 0.63353705\n",
      "46 Train Loss 47.298573 Test MSE 46.6461487379141 Test RE 0.11497331858800094 Lambda1 0.6390526\n",
      "47 Train Loss 46.55203 Test MSE 46.04223036532153 Test RE 0.1142266255952175 Lambda1 0.6493008\n",
      "48 Train Loss 45.670757 Test MSE 44.82018771697703 Test RE 0.11270054239764762 Lambda1 0.6657389\n",
      "49 Train Loss 45.19675 Test MSE 44.41184506458157 Test RE 0.11218597808608051 Lambda1 0.67135686\n",
      "50 Train Loss 43.645985 Test MSE 42.795863326820815 Test RE 0.11012605110063846 Lambda1 0.6978966\n",
      "51 Train Loss 42.90489 Test MSE 41.75414028278684 Test RE 0.10877746751229851 Lambda1 0.7142347\n",
      "52 Train Loss 42.12069 Test MSE 40.87599564610571 Test RE 0.10762752236754679 Lambda1 0.7326942\n",
      "53 Train Loss 41.303642 Test MSE 40.26205345975842 Test RE 0.1068162018167214 Lambda1 0.74684185\n",
      "54 Train Loss 40.66057 Test MSE 39.76513959000829 Test RE 0.10615499306598969 Lambda1 0.75456154\n",
      "55 Train Loss 39.63765 Test MSE 38.73780418678353 Test RE 0.10477475896282867 Lambda1 0.7713004\n",
      "56 Train Loss 38.791557 Test MSE 38.0400986536507 Test RE 0.10382692405261913 Lambda1 0.7904084\n",
      "57 Train Loss 38.135227 Test MSE 37.84299762876505 Test RE 0.10355759022693119 Lambda1 0.80230975\n",
      "58 Train Loss 37.796814 Test MSE 37.744457025268254 Test RE 0.10342267386485218 Lambda1 0.8098306\n",
      "59 Train Loss 37.163754 Test MSE 37.325173036208206 Test RE 0.10284663470666629 Lambda1 0.8212944\n",
      "60 Train Loss 36.436123 Test MSE 36.83372059888094 Test RE 0.10216731153400749 Lambda1 0.82334894\n",
      "61 Train Loss 35.98902 Test MSE 36.78559514934964 Test RE 0.10210054588009476 Lambda1 0.82687527\n",
      "62 Train Loss 35.549965 Test MSE 36.48319272068997 Test RE 0.10168001211749238 Lambda1 0.83628994\n",
      "63 Train Loss 35.306904 Test MSE 36.13326616142573 Test RE 0.1011912080552735 Lambda1 0.8465267\n",
      "64 Train Loss 34.999012 Test MSE 35.93135747505932 Test RE 0.10090808936485958 Lambda1 0.86168325\n",
      "65 Train Loss 34.72602 Test MSE 35.66768206790023 Test RE 0.10053716023898975 Lambda1 0.87448215\n",
      "66 Train Loss 34.466347 Test MSE 35.462445116277486 Test RE 0.10024749032684856 Lambda1 0.8827939\n",
      "67 Train Loss 34.166615 Test MSE 35.529521254066935 Test RE 0.10034225307930233 Lambda1 0.8846149\n",
      "68 Train Loss 33.872128 Test MSE 35.44844387951676 Test RE 0.10022769858081594 Lambda1 0.8902238\n",
      "69 Train Loss 33.5081 Test MSE 35.30890542850268 Test RE 0.1000302370479652 Lambda1 0.89666086\n",
      "70 Train Loss 33.198273 Test MSE 34.94009927682469 Test RE 0.09950645122696036 Lambda1 0.90412277\n",
      "71 Train Loss 32.943752 Test MSE 34.7342515148717 Test RE 0.09921289970874946 Lambda1 0.9108012\n",
      "72 Train Loss 32.70574 Test MSE 34.66693504671781 Test RE 0.0991167136570134 Lambda1 0.9169597\n",
      "73 Train Loss 32.48164 Test MSE 34.47118701326804 Test RE 0.09883648456078369 Lambda1 0.92185557\n",
      "74 Train Loss 32.311375 Test MSE 34.39617520558349 Test RE 0.09872888830856148 Lambda1 0.92440695\n",
      "Training time: 66.48\n",
      "Training time: 66.48\n",
      "inv_HT_stan_tune12\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.06244 Test MSE 858.2640707653803 Test RE 0.49317339142254063 Lambda1 -0.040824976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 837.8955 Test MSE 857.9471747307763 Test RE 0.4930823360432499 Lambda1 -0.044602256\n",
      "2 Train Loss 837.68164 Test MSE 857.2302183770101 Test RE 0.4928762671786387 Lambda1 -0.38037527\n",
      "3 Train Loss 832.5995 Test MSE 848.9169091488283 Test RE 0.4904805190522324 Lambda1 -1.2442106\n",
      "4 Train Loss 823.5485 Test MSE 833.6704463451157 Test RE 0.48605607285392405 Lambda1 -1.3118916\n",
      "5 Train Loss 814.3784 Test MSE 828.1186492686637 Test RE 0.48443493327037707 Lambda1 -1.2026173\n",
      "6 Train Loss 781.63477 Test MSE 781.9606163033342 Test RE 0.4707405503522828 Lambda1 -1.2314917\n",
      "7 Train Loss 755.6834 Test MSE 755.275416106677 Test RE 0.4626385786681492 Lambda1 -1.0440043\n",
      "8 Train Loss 739.2751 Test MSE 735.8581873212424 Test RE 0.45665291448522305 Lambda1 -1.0871958\n",
      "9 Train Loss 721.97095 Test MSE 705.0401807562431 Test RE 0.4469882478355063 Lambda1 -1.2738541\n",
      "10 Train Loss 701.95807 Test MSE 682.6762730187222 Test RE 0.43984187621606297 Lambda1 -1.3804815\n",
      "11 Train Loss 690.359 Test MSE 670.0213878911977 Test RE 0.4357460948862811 Lambda1 -1.5496995\n",
      "12 Train Loss 670.91266 Test MSE 657.6249104480589 Test RE 0.43169627222811763 Lambda1 -1.8557328\n",
      "13 Train Loss 653.19775 Test MSE 633.6788311122164 Test RE 0.4237637178863093 Lambda1 -2.0049543\n",
      "14 Train Loss 640.7044 Test MSE 617.5330946944393 Test RE 0.41833026840302917 Lambda1 -2.131136\n",
      "15 Train Loss 634.1 Test MSE 617.918580310667 Test RE 0.4184608161780943 Lambda1 -2.1333644\n",
      "16 Train Loss 606.56433 Test MSE 589.5593673243884 Test RE 0.40874546073157697 Lambda1 -2.3889403\n",
      "17 Train Loss 597.5309 Test MSE 578.9001408416668 Test RE 0.4050335497952866 Lambda1 -2.4867556\n",
      "18 Train Loss 590.63904 Test MSE 573.1882080492755 Test RE 0.4030303894987131 Lambda1 -2.46025\n",
      "19 Train Loss 574.1572 Test MSE 561.7993937157008 Test RE 0.39900634668090734 Lambda1 -2.3753734\n",
      "20 Train Loss 563.5922 Test MSE 551.5311500169416 Test RE 0.3953431275705059 Lambda1 -2.2217586\n",
      "21 Train Loss 549.0063 Test MSE 532.6950866366926 Test RE 0.3885335409891658 Lambda1 -2.1031425\n",
      "22 Train Loss 520.5224 Test MSE 506.5975975087522 Test RE 0.3788966221404918 Lambda1 -1.979133\n",
      "23 Train Loss 491.1712 Test MSE 475.2113257541383 Test RE 0.36697169003094027 Lambda1 -1.9807564\n",
      "24 Train Loss 472.50394 Test MSE 462.89895930581457 Test RE 0.36218651190624324 Lambda1 -2.0549536\n",
      "25 Train Loss 456.51578 Test MSE 438.56686467865427 Test RE 0.35253892725656755 Lambda1 -2.1607208\n",
      "26 Train Loss 445.53793 Test MSE 414.654209279215 Test RE 0.34279320916106093 Lambda1 -2.2033718\n",
      "27 Train Loss 428.2452 Test MSE 409.72969936154277 Test RE 0.34075159169075603 Lambda1 -2.1455998\n",
      "28 Train Loss 407.16263 Test MSE 383.9623737812861 Test RE 0.3298629241852594 Lambda1 -2.12907\n",
      "29 Train Loss 384.22955 Test MSE 361.21090255256144 Test RE 0.31994077743758137 Lambda1 -2.1130562\n",
      "30 Train Loss 360.8465 Test MSE 339.6279432203844 Test RE 0.3102350612953642 Lambda1 -2.0918903\n",
      "31 Train Loss 349.15216 Test MSE 325.7448512812673 Test RE 0.3038281089971833 Lambda1 -2.0816286\n",
      "32 Train Loss 341.04352 Test MSE 326.8723752318791 Test RE 0.3043534857404343 Lambda1 -2.1001906\n",
      "33 Train Loss 328.8487 Test MSE 317.58791376934994 Test RE 0.29999993031448435 Lambda1 -2.0995452\n",
      "34 Train Loss 315.8127 Test MSE 302.36094830928135 Test RE 0.2927197462603408 Lambda1 -2.1822133\n",
      "35 Train Loss 308.81882 Test MSE 301.0582463556296 Test RE 0.2920884838491673 Lambda1 -2.1901534\n",
      "36 Train Loss 302.7317 Test MSE 295.58444386813875 Test RE 0.28942094530950613 Lambda1 -2.2243254\n",
      "37 Train Loss 300.09058 Test MSE 291.6836712393012 Test RE 0.2875048857996801 Lambda1 -2.2193987\n",
      "38 Train Loss 293.403 Test MSE 282.1094014382381 Test RE 0.2827469639954495 Lambda1 -2.213791\n",
      "39 Train Loss 281.81293 Test MSE 270.1084577535238 Test RE 0.27666757419769505 Lambda1 -2.2629187\n",
      "40 Train Loss 271.70544 Test MSE 247.84004827617557 Test RE 0.265017719878303 Lambda1 -2.2887175\n",
      "41 Train Loss 264.5501 Test MSE 247.7964610885782 Test RE 0.2649944147565563 Lambda1 -2.30986\n",
      "42 Train Loss 257.82294 Test MSE 247.60553383938418 Test RE 0.26489230594516605 Lambda1 -2.2999487\n",
      "43 Train Loss 254.03122 Test MSE 249.38773723382283 Test RE 0.2658439113127168 Lambda1 -2.2534177\n",
      "44 Train Loss 251.86339 Test MSE 246.66410431100866 Test RE 0.2643882482851278 Lambda1 -2.2655334\n",
      "45 Train Loss 244.10056 Test MSE 242.80232688394202 Test RE 0.26231045022853455 Lambda1 -2.2902563\n",
      "46 Train Loss 238.05328 Test MSE 238.1758083060125 Test RE 0.2597993107304966 Lambda1 -2.280882\n",
      "47 Train Loss 235.70682 Test MSE 236.35435455780885 Test RE 0.2588039942185421 Lambda1 -2.2735107\n",
      "48 Train Loss 226.84708 Test MSE 228.10948214980894 Test RE 0.25424992877042235 Lambda1 -2.3419528\n",
      "49 Train Loss 223.72935 Test MSE 225.59295856330087 Test RE 0.2528435856584722 Lambda1 -2.3766115\n",
      "50 Train Loss 220.47684 Test MSE 219.730596232309 Test RE 0.24953670589318053 Lambda1 -2.4351852\n",
      "51 Train Loss 216.85254 Test MSE 215.1089854273695 Test RE 0.24689849723917126 Lambda1 -2.4553275\n",
      "52 Train Loss 207.20764 Test MSE 203.761115520206 Test RE 0.2402978169834481 Lambda1 -2.4417737\n",
      "53 Train Loss 203.70291 Test MSE 198.0126225819076 Test RE 0.236883934901476 Lambda1 -2.3998408\n",
      "54 Train Loss 200.58575 Test MSE 194.41756269232238 Test RE 0.23472368668205476 Lambda1 -2.3489616\n",
      "55 Train Loss 198.4585 Test MSE 192.7560187378262 Test RE 0.23371852910849272 Lambda1 -2.3470495\n",
      "56 Train Loss 196.53107 Test MSE 190.0186451424403 Test RE 0.232053049073013 Lambda1 -2.319598\n",
      "57 Train Loss 194.88347 Test MSE 188.4520450128345 Test RE 0.2310944938564579 Lambda1 -2.3420413\n",
      "58 Train Loss 193.08615 Test MSE 186.23836966582934 Test RE 0.22973319429498615 Lambda1 -2.3433018\n",
      "59 Train Loss 189.96841 Test MSE 182.4503545511119 Test RE 0.2273848504914986 Lambda1 -2.3382404\n",
      "60 Train Loss 183.43732 Test MSE 178.6325204483471 Test RE 0.22499322128083937 Lambda1 -2.359514\n",
      "61 Train Loss 177.60136 Test MSE 170.18489649869963 Test RE 0.21960876926216766 Lambda1 -2.3463874\n",
      "62 Train Loss 174.3284 Test MSE 168.99157733136497 Test RE 0.21883747766356662 Lambda1 -2.3635664\n",
      "63 Train Loss 171.566 Test MSE 162.18129737661712 Test RE 0.21438261090089192 Lambda1 -2.3604615\n",
      "64 Train Loss 167.8444 Test MSE 157.63775963438383 Test RE 0.21135829536128947 Lambda1 -2.384653\n",
      "65 Train Loss 163.13544 Test MSE 149.3732298635858 Test RE 0.2057432313675228 Lambda1 -2.3889866\n",
      "66 Train Loss 152.91891 Test MSE 145.054496407643 Test RE 0.20274715483836142 Lambda1 -2.3550572\n",
      "67 Train Loss 145.66023 Test MSE 139.71340188508125 Test RE 0.1989794400349465 Lambda1 -2.3678102\n",
      "68 Train Loss 141.29181 Test MSE 137.31093214096242 Test RE 0.19726122615444272 Lambda1 -2.4042385\n",
      "69 Train Loss 139.50607 Test MSE 135.33334467257149 Test RE 0.19583557083951833 Lambda1 -2.3956985\n",
      "70 Train Loss 135.74583 Test MSE 131.4453822192015 Test RE 0.19300201243729775 Lambda1 -2.4260736\n",
      "71 Train Loss 133.70502 Test MSE 130.84989084907522 Test RE 0.19256433446208396 Lambda1 -2.4348977\n",
      "72 Train Loss 132.0004 Test MSE 128.15922069854508 Test RE 0.1905741974748236 Lambda1 -2.4154296\n",
      "73 Train Loss 130.85208 Test MSE 128.06185568885232 Test RE 0.1905017922900126 Lambda1 -2.4045186\n",
      "74 Train Loss 128.808 Test MSE 124.25258867237112 Test RE 0.18764711574396575 Lambda1 -2.394923\n",
      "Training time: 69.96\n",
      "Training time: 69.96\n",
      "inv_HT_stan_tune12\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 838.03076 Test MSE 858.207889683146 Test RE 0.49315724984721676 Lambda1 -0.057010498\n",
      "1 Train Loss 837.86365 Test MSE 857.8587490297427 Test RE 0.4930569252277419 Lambda1 -0.061310682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Train Loss 837.764 Test MSE 857.7450999090694 Test RE 0.49302426405707966 Lambda1 -0.18708177\n",
      "3 Train Loss 834.92676 Test MSE 853.2204533816403 Test RE 0.4917221813336828 Lambda1 -0.68449926\n",
      "4 Train Loss 828.3768 Test MSE 843.0249382541206 Test RE 0.4887754471100822 Lambda1 -0.7415294\n",
      "5 Train Loss 810.0847 Test MSE 820.4180178785915 Test RE 0.4821773052481324 Lambda1 -0.7555709\n",
      "6 Train Loss 787.722 Test MSE 795.5667743534005 Test RE 0.4748183442202295 Lambda1 -0.675159\n",
      "7 Train Loss 763.81 Test MSE 765.9061048236268 Test RE 0.46588307812005997 Lambda1 -0.56792325\n",
      "8 Train Loss 741.2916 Test MSE 735.3336146314168 Test RE 0.45649011793099437 Lambda1 -0.5477684\n",
      "9 Train Loss 729.5008 Test MSE 726.4472092701875 Test RE 0.45372342367168506 Lambda1 -0.5586787\n",
      "10 Train Loss 718.85394 Test MSE 710.7295493790187 Test RE 0.44878812476970337 Lambda1 -0.6441242\n",
      "11 Train Loss 714.05194 Test MSE 709.2941137752239 Test RE 0.4483346948731907 Lambda1 -0.6082077\n",
      "12 Train Loss 707.6789 Test MSE 704.898981652914 Test RE 0.4469434862021801 Lambda1 -0.65580565\n",
      "13 Train Loss 704.6941 Test MSE 699.958520198196 Test RE 0.4453744743579783 Lambda1 -0.6492581\n",
      "14 Train Loss 702.0698 Test MSE 696.0502502799019 Test RE 0.4441293432749729 Lambda1 -0.631604\n",
      "15 Train Loss 695.979 Test MSE 688.1244251056883 Test RE 0.4415934849192817 Lambda1 -0.71444833\n",
      "16 Train Loss 688.976 Test MSE 680.7460023570037 Test RE 0.4392196084584418 Lambda1 -0.83857465\n",
      "17 Train Loss 685.20154 Test MSE 677.4026513762064 Test RE 0.43813971025487314 Lambda1 -0.9162247\n",
      "18 Train Loss 681.2528 Test MSE 674.2737171352888 Test RE 0.4371266517215876 Lambda1 -1.0184479\n",
      "19 Train Loss 677.99774 Test MSE 671.0867767471939 Test RE 0.43609239326121374 Lambda1 -1.0687816\n",
      "20 Train Loss 673.83966 Test MSE 665.8558514818283 Test RE 0.43438946193870254 Lambda1 -1.0733762\n",
      "21 Train Loss 672.2553 Test MSE 664.8041356236683 Test RE 0.4340462684178562 Lambda1 -1.0614102\n",
      "22 Train Loss 670.1094 Test MSE 663.4111779527636 Test RE 0.43359130349939284 Lambda1 -1.0735087\n",
      "23 Train Loss 668.20197 Test MSE 661.4247974890495 Test RE 0.4329416890539672 Lambda1 -1.1272182\n",
      "24 Train Loss 667.195 Test MSE 660.6556396141896 Test RE 0.432689886145292 Lambda1 -1.1693332\n",
      "25 Train Loss 665.93176 Test MSE 660.199483717457 Test RE 0.4325404829519773 Lambda1 -1.1824458\n",
      "26 Train Loss 664.16925 Test MSE 656.8992392310314 Test RE 0.43145802393254573 Lambda1 -1.2651347\n",
      "27 Train Loss 659.5734 Test MSE 653.0354370920812 Test RE 0.43018726051413875 Lambda1 -1.3747222\n",
      "28 Train Loss 657.1899 Test MSE 650.9511631403029 Test RE 0.4295002038630929 Lambda1 -1.4642395\n",
      "29 Train Loss 654.76935 Test MSE 646.7737181040876 Test RE 0.4281198381475103 Lambda1 -1.5546398\n",
      "30 Train Loss 652.7721 Test MSE 646.4366822246634 Test RE 0.42800827629462973 Lambda1 -1.5938252\n",
      "31 Train Loss 651.7291 Test MSE 646.5192590806195 Test RE 0.42803561265263057 Lambda1 -1.6123794\n",
      "32 Train Loss 650.7223 Test MSE 644.754306974498 Test RE 0.42745095980573716 Lambda1 -1.6444436\n",
      "33 Train Loss 649.5663 Test MSE 643.7881256310986 Test RE 0.42713056645963715 Lambda1 -1.6425518\n",
      "34 Train Loss 648.1357 Test MSE 641.1234882559537 Test RE 0.4262457037481032 Lambda1 -1.6860926\n",
      "35 Train Loss 643.30865 Test MSE 633.1500201310967 Test RE 0.42358686356561326 Lambda1 -1.8453362\n",
      "36 Train Loss 641.4876 Test MSE 629.0337433674889 Test RE 0.4222076928627363 Lambda1 -1.9345113\n",
      "37 Train Loss 637.1278 Test MSE 623.2075347393471 Test RE 0.42024786750668897 Lambda1 -1.9943292\n",
      "38 Train Loss 626.50824 Test MSE 617.3669341465882 Test RE 0.41827398424304174 Lambda1 -2.1118279\n",
      "39 Train Loss 622.673 Test MSE 613.5539336418823 Test RE 0.4169803053594953 Lambda1 -2.171601\n",
      "40 Train Loss 617.5686 Test MSE 605.2856966422052 Test RE 0.4141611675623097 Lambda1 -2.3419323\n",
      "41 Train Loss 605.4678 Test MSE 592.1055518490457 Test RE 0.40962715319355153 Lambda1 -2.4709268\n",
      "42 Train Loss 596.71906 Test MSE 583.3991470962659 Test RE 0.4066043921303847 Lambda1 -2.4683583\n",
      "43 Train Loss 592.42194 Test MSE 578.8928624974998 Test RE 0.40503100360240063 Lambda1 -2.4624329\n",
      "44 Train Loss 585.61707 Test MSE 573.7972429260412 Test RE 0.40324445042335966 Lambda1 -2.446039\n",
      "45 Train Loss 578.43146 Test MSE 568.6457540860114 Test RE 0.40143022699914227 Lambda1 -2.453406\n",
      "46 Train Loss 565.54913 Test MSE 555.5800893847106 Test RE 0.3967916343838964 Lambda1 -2.532877\n",
      "47 Train Loss 558.9142 Test MSE 541.5086953394541 Test RE 0.39173456000510304 Lambda1 -2.6026528\n",
      "48 Train Loss 550.7226 Test MSE 527.7848435629408 Test RE 0.3867386953613399 Lambda1 -2.7455652\n",
      "49 Train Loss 535.8165 Test MSE 525.4563214717426 Test RE 0.3858846304328367 Lambda1 -2.853258\n",
      "50 Train Loss 526.5564 Test MSE 507.44291544203963 Test RE 0.3792126072664096 Lambda1 -2.9452748\n",
      "51 Train Loss 519.2513 Test MSE 501.7089276127397 Test RE 0.3770640128963919 Lambda1 -3.021659\n",
      "52 Train Loss 506.12933 Test MSE 484.583134671277 Test RE 0.3705726115361863 Lambda1 -3.1539984\n",
      "53 Train Loss 488.5324 Test MSE 474.7567053269471 Test RE 0.36679611262197137 Lambda1 -3.082722\n",
      "54 Train Loss 476.68155 Test MSE 456.0918044097213 Test RE 0.35951358420943236 Lambda1 -3.1604927\n",
      "55 Train Loss 465.24615 Test MSE 442.47351514812186 Test RE 0.3541056129940979 Lambda1 -3.1671596\n",
      "56 Train Loss 449.01367 Test MSE 428.96769079637954 Test RE 0.348659467179684 Lambda1 -3.1171074\n",
      "57 Train Loss 430.09763 Test MSE 410.08356730717765 Test RE 0.34089870702292363 Lambda1 -3.1670656\n",
      "58 Train Loss 418.76865 Test MSE 401.146843919738 Test RE 0.33716373833356766 Lambda1 -3.2205358\n",
      "59 Train Loss 410.45462 Test MSE 403.40375653675204 Test RE 0.33811087502837256 Lambda1 -3.2682953\n",
      "60 Train Loss 385.32166 Test MSE 388.8857217047754 Test RE 0.33197101750960073 Lambda1 -3.3985775\n",
      "61 Train Loss 368.7295 Test MSE 374.08439051634974 Test RE 0.3255921788191862 Lambda1 -3.5371072\n",
      "62 Train Loss 360.98898 Test MSE 362.4283559688454 Test RE 0.32047950056367536 Lambda1 -3.6961799\n",
      "63 Train Loss 354.3595 Test MSE 347.59609274532454 Test RE 0.3138532380249053 Lambda1 -3.7663772\n",
      "64 Train Loss 347.63074 Test MSE 344.1800946123501 Test RE 0.3123072350795832 Lambda1 -3.7621033\n",
      "65 Train Loss 327.72122 Test MSE 315.37203383835515 Test RE 0.29895151598667996 Lambda1 -4.013549\n",
      "66 Train Loss 320.45914 Test MSE 303.2487221212874 Test RE 0.2931491642343463 Lambda1 -4.060267\n",
      "67 Train Loss 307.88098 Test MSE 300.46882601395987 Test RE 0.2918024142181766 Lambda1 -4.0328445\n",
      "68 Train Loss 300.154 Test MSE 286.8545775096249 Test RE 0.28511499778208094 Lambda1 -4.0246305\n",
      "69 Train Loss 289.71387 Test MSE 276.32283568190354 Test RE 0.2798321177307351 Lambda1 -3.935177\n",
      "70 Train Loss 279.33078 Test MSE 263.58307271500064 Test RE 0.2733052214291184 Lambda1 -3.9594839\n",
      "71 Train Loss 270.94797 Test MSE 259.94053555632365 Test RE 0.2714102064105098 Lambda1 -3.970013\n",
      "72 Train Loss 266.37323 Test MSE 256.859747629277 Test RE 0.26979704979230623 Lambda1 -3.9608297\n",
      "73 Train Loss 261.492 Test MSE 245.5146048123029 Test RE 0.2637714802698984 Lambda1 -3.981896\n",
      "74 Train Loss 257.60703 Test MSE 235.67877227922514 Test RE 0.25843385398564533 Lambda1 -3.974197\n",
      "Training time: 69.26\n",
      "Training time: 69.26\n",
      "inv_HT_stan_tune12\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 838.0653 Test MSE 858.2698566544802 Test RE 0.49317505375575377 Lambda1 -0.1389299\n",
      "1 Train Loss 837.79706 Test MSE 857.6877611137094 Test RE 0.4930077848665583 Lambda1 -0.1397396\n",
      "2 Train Loss 837.4857 Test MSE 856.9182193798646 Test RE 0.49278656496530476 Lambda1 -0.17405254\n",
      "3 Train Loss 831.2852 Test MSE 850.3797554890098 Test RE 0.4909029330683064 Lambda1 -0.35589308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 817.2495 Test MSE 820.7871847458715 Test RE 0.48228577669902234 Lambda1 -0.265743\n",
      "5 Train Loss 796.90625 Test MSE 805.9969574182643 Test RE 0.47792073351800163 Lambda1 -0.33801183\n",
      "6 Train Loss 773.902 Test MSE 778.2137060772595 Test RE 0.4696113754655733 Lambda1 -0.41961724\n",
      "7 Train Loss 731.13403 Test MSE 732.3270973791964 Test RE 0.45555594932843024 Lambda1 -0.48999286\n",
      "8 Train Loss 718.0106 Test MSE 720.7555747980203 Test RE 0.45194249174776957 Lambda1 -0.4835569\n",
      "9 Train Loss 705.4817 Test MSE 706.6552352397268 Test RE 0.4474999189724126 Lambda1 -0.49569198\n",
      "10 Train Loss 703.59644 Test MSE 704.9375818520888 Test RE 0.4469557233254147 Lambda1 -0.50751776\n",
      "11 Train Loss 696.9176 Test MSE 699.3995168303229 Test RE 0.44519659556137914 Lambda1 -0.52491593\n",
      "12 Train Loss 691.7654 Test MSE 695.8513803976203 Test RE 0.4440658922095627 Lambda1 -0.54789346\n",
      "13 Train Loss 678.85834 Test MSE 679.5761176296405 Test RE 0.43884203939360406 Lambda1 -0.60428834\n",
      "14 Train Loss 664.24243 Test MSE 662.6015769089669 Test RE 0.4333266538020377 Lambda1 -0.6897392\n",
      "15 Train Loss 652.38763 Test MSE 654.1819396603474 Test RE 0.43056472434249937 Lambda1 -0.6998252\n",
      "16 Train Loss 631.75946 Test MSE 628.7608554461674 Test RE 0.4221161016887298 Lambda1 -0.8463081\n",
      "17 Train Loss 615.4293 Test MSE 610.7450586866211 Test RE 0.41602473401361817 Lambda1 -0.98978764\n",
      "18 Train Loss 601.7083 Test MSE 597.5092179946568 Test RE 0.4114920750048713 Lambda1 -1.0601684\n",
      "19 Train Loss 586.31244 Test MSE 587.7468436916062 Test RE 0.4081166596764916 Lambda1 -1.0570419\n",
      "20 Train Loss 573.5466 Test MSE 578.6375235742454 Test RE 0.4049416679120315 Lambda1 -1.0394312\n",
      "21 Train Loss 554.5288 Test MSE 555.8008548374348 Test RE 0.39687046116893093 Lambda1 -1.0849637\n",
      "22 Train Loss 520.914 Test MSE 511.13578836196626 Test RE 0.380589949781665 Lambda1 -1.1269807\n",
      "23 Train Loss 472.6879 Test MSE 450.2486028919574 Test RE 0.3572032137538694 Lambda1 -1.0998207\n",
      "24 Train Loss 423.8276 Test MSE 403.62338313009786 Test RE 0.3382029019790947 Lambda1 -1.0282193\n",
      "25 Train Loss 380.6219 Test MSE 380.9794175658527 Test RE 0.3285790937036744 Lambda1 -0.96966815\n",
      "26 Train Loss 342.17258 Test MSE 364.36040069339657 Test RE 0.3213325763829966 Lambda1 -0.933743\n",
      "27 Train Loss 320.44647 Test MSE 348.611103078523 Test RE 0.31431114314313807 Lambda1 -0.85940796\n",
      "28 Train Loss 316.6224 Test MSE 346.8104499324588 Test RE 0.31349834907083124 Lambda1 -0.8321103\n",
      "29 Train Loss 305.87906 Test MSE 334.4318591658142 Test RE 0.3078527179517318 Lambda1 -0.7819183\n",
      "30 Train Loss 301.69965 Test MSE 329.942077149803 Test RE 0.3057792581750698 Lambda1 -0.77583116\n",
      "31 Train Loss 294.825 Test MSE 325.0370432241377 Test RE 0.30349783680576276 Lambda1 -0.7355779\n",
      "32 Train Loss 282.4424 Test MSE 307.31719406719077 Test RE 0.2951090990628065 Lambda1 -0.74101883\n",
      "33 Train Loss 265.19528 Test MSE 287.3972338059444 Test RE 0.2853845530854681 Lambda1 -0.7610098\n",
      "34 Train Loss 241.38911 Test MSE 255.25893515122175 Test RE 0.26895501535360694 Lambda1 -0.7806241\n",
      "35 Train Loss 211.90884 Test MSE 224.89928420563479 Test RE 0.25245455280050355 Lambda1 -0.8399546\n",
      "36 Train Loss 196.55702 Test MSE 196.83948439573058 Test RE 0.23618117561436563 Lambda1 -0.90879655\n",
      "37 Train Loss 179.57271 Test MSE 177.73074273244376 Test RE 0.22442459417786095 Lambda1 -0.93288565\n",
      "38 Train Loss 160.57556 Test MSE 166.152088662048 Test RE 0.2169911758265329 Lambda1 -0.9113301\n",
      "39 Train Loss 153.75876 Test MSE 161.92704130343037 Test RE 0.21421449823344674 Lambda1 -0.91876155\n",
      "40 Train Loss 146.7482 Test MSE 153.70581145647063 Test RE 0.20870570207767203 Lambda1 -0.9370962\n",
      "41 Train Loss 139.63132 Test MSE 148.25969266843785 Test RE 0.2049749166074471 Lambda1 -0.9662085\n",
      "42 Train Loss 133.21327 Test MSE 142.1353319166785 Test RE 0.20069668293641818 Lambda1 -0.9906266\n",
      "43 Train Loss 124.59895 Test MSE 135.85613794020387 Test RE 0.1962134630496992 Lambda1 -1.0450938\n",
      "44 Train Loss 119.38679 Test MSE 132.5334139861838 Test RE 0.1937991479334505 Lambda1 -1.0950751\n",
      "45 Train Loss 114.74252 Test MSE 130.35872644844005 Test RE 0.19220258533116164 Lambda1 -1.1312906\n",
      "46 Train Loss 110.60477 Test MSE 127.26514755697907 Test RE 0.18990828565650877 Lambda1 -1.1647472\n",
      "47 Train Loss 108.39081 Test MSE 122.44021079577743 Test RE 0.1862735557966182 Lambda1 -1.2123029\n",
      "48 Train Loss 106.41202 Test MSE 120.33720011960631 Test RE 0.18466692685515237 Lambda1 -1.2350867\n",
      "49 Train Loss 103.931854 Test MSE 118.27008731875434 Test RE 0.18307398262249808 Lambda1 -1.2670597\n",
      "50 Train Loss 102.268616 Test MSE 116.3621527887876 Test RE 0.18159130276719873 Lambda1 -1.3064907\n",
      "51 Train Loss 100.58369 Test MSE 116.48439537904389 Test RE 0.1816866617982448 Lambda1 -1.341811\n",
      "52 Train Loss 98.517334 Test MSE 114.69507181011213 Test RE 0.18028581165681506 Lambda1 -1.3481668\n",
      "53 Train Loss 97.731224 Test MSE 112.95060595961355 Test RE 0.17890952110510627 Lambda1 -1.3706039\n",
      "54 Train Loss 96.507904 Test MSE 112.4735898296013 Test RE 0.17853133356521367 Lambda1 -1.3919725\n",
      "55 Train Loss 94.8761 Test MSE 110.33320729913153 Test RE 0.17682444030873895 Lambda1 -1.4392908\n",
      "56 Train Loss 93.77455 Test MSE 108.21298275960955 Test RE 0.17511722019319007 Lambda1 -1.4615343\n",
      "57 Train Loss 92.45859 Test MSE 107.61373290927546 Test RE 0.17463167470312752 Lambda1 -1.4703196\n",
      "58 Train Loss 90.08953 Test MSE 105.08690296454367 Test RE 0.17256927206542694 Lambda1 -1.4994155\n",
      "59 Train Loss 89.28272 Test MSE 104.93250145827862 Test RE 0.17244244964905028 Lambda1 -1.5009139\n",
      "60 Train Loss 88.766045 Test MSE 104.77322106427059 Test RE 0.17231152199553024 Lambda1 -1.5087827\n",
      "61 Train Loss 87.97639 Test MSE 104.20339806846772 Test RE 0.17184231368171424 Lambda1 -1.5255879\n",
      "62 Train Loss 87.58645 Test MSE 103.95284027993767 Test RE 0.17163559132704412 Lambda1 -1.5385226\n",
      "63 Train Loss 87.035805 Test MSE 103.49457567318613 Test RE 0.17125685519493614 Lambda1 -1.5522778\n",
      "64 Train Loss 86.7977 Test MSE 102.73712500217351 Test RE 0.1706290114841884 Lambda1 -1.5562618\n",
      "65 Train Loss 85.920265 Test MSE 101.65470063320164 Test RE 0.1697277693340034 Lambda1 -1.5993718\n",
      "66 Train Loss 85.676636 Test MSE 100.69380413682649 Test RE 0.16892368424785195 Lambda1 -1.6177663\n",
      "67 Train Loss 85.08949 Test MSE 99.84764176507228 Test RE 0.16821242688203109 Lambda1 -1.6249695\n",
      "68 Train Loss 84.88363 Test MSE 99.95913570281111 Test RE 0.16830631709758173 Lambda1 -1.6212834\n",
      "69 Train Loss 84.195045 Test MSE 100.07474652284985 Test RE 0.16840361890124034 Lambda1 -1.6300898\n",
      "70 Train Loss 83.74653 Test MSE 99.72753029650518 Test RE 0.1681112210794476 Lambda1 -1.6442564\n",
      "71 Train Loss 83.357 Test MSE 98.60158972801159 Test RE 0.16715952529110567 Lambda1 -1.6760868\n",
      "72 Train Loss 82.689865 Test MSE 98.14138391716214 Test RE 0.16676897501220586 Lambda1 -1.6983893\n",
      "73 Train Loss 81.81204 Test MSE 98.02532110281493 Test RE 0.16667033465405565 Lambda1 -1.7063032\n",
      "74 Train Loss 81.360596 Test MSE 97.06235578248946 Test RE 0.16584965961614037 Lambda1 -1.7314382\n",
      "Training time: 68.25\n",
      "Training time: 68.25\n",
      "inv_HT_stan_tune12\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 838.0613 Test MSE 858.2877516304105 Test RE 0.49318019509308847 Lambda1 -0.04917307\n",
      "1 Train Loss 837.90186 Test MSE 857.9074568817232 Test RE 0.4930709225220953 Lambda1 -0.05275001\n",
      "2 Train Loss 837.8531 Test MSE 857.8514916272414 Test RE 0.4930548396162503 Lambda1 -0.12289257\n",
      "3 Train Loss 834.28424 Test MSE 853.8406729445621 Test RE 0.4919008692169745 Lambda1 -0.9721924\n",
      "4 Train Loss 825.59937 Test MSE 835.9238845379397 Test RE 0.4867125421853273 Lambda1 -1.3760272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 807.8445 Test MSE 819.0025320166288 Test RE 0.4817611699441644 Lambda1 -1.5516454\n",
      "6 Train Loss 775.4775 Test MSE 776.6426759454765 Test RE 0.46913711862621277 Lambda1 -1.7647641\n",
      "7 Train Loss 755.753 Test MSE 758.6049294843943 Test RE 0.4636571921600185 Lambda1 -1.8731643\n",
      "8 Train Loss 740.3891 Test MSE 734.8132439442942 Test RE 0.45632856804944705 Lambda1 -1.9144489\n",
      "9 Train Loss 721.8643 Test MSE 713.7941269462456 Test RE 0.44975464334787046 Lambda1 -1.7736051\n",
      "10 Train Loss 689.0307 Test MSE 669.3113262304556 Test RE 0.43551514060704827 Lambda1 -1.6730844\n",
      "11 Train Loss 661.12885 Test MSE 653.3231066351267 Test RE 0.4302820012740673 Lambda1 -1.646775\n",
      "12 Train Loss 629.0083 Test MSE 620.9308250373537 Test RE 0.4194795377160311 Lambda1 -1.6016494\n",
      "13 Train Loss 606.9441 Test MSE 590.7479459525149 Test RE 0.4091572780399629 Lambda1 -1.4447265\n",
      "14 Train Loss 579.85077 Test MSE 574.7947214839218 Test RE 0.40359479459397946 Lambda1 -1.4223537\n",
      "15 Train Loss 559.7969 Test MSE 558.904705397054 Test RE 0.39797707301055224 Lambda1 -1.340888\n",
      "16 Train Loss 542.45544 Test MSE 538.7755331677644 Test RE 0.39074470654919297 Lambda1 -1.30342\n",
      "17 Train Loss 525.1551 Test MSE 517.8725104744088 Test RE 0.3830898098200671 Lambda1 -1.2694963\n",
      "18 Train Loss 520.7719 Test MSE 515.2483815796639 Test RE 0.3821179936648729 Lambda1 -1.2497602\n",
      "19 Train Loss 513.4886 Test MSE 509.4844103049835 Test RE 0.3799746471891696 Lambda1 -1.170508\n",
      "20 Train Loss 504.19388 Test MSE 502.90529764842614 Test RE 0.37751331672075067 Lambda1 -1.1263093\n",
      "21 Train Loss 491.29703 Test MSE 486.1392842995585 Test RE 0.37116714750719926 Lambda1 -1.1080792\n",
      "22 Train Loss 477.22473 Test MSE 473.3553983795698 Test RE 0.3662543890653547 Lambda1 -1.1245952\n",
      "23 Train Loss 458.46054 Test MSE 452.47981407319077 Test RE 0.35808718191975913 Lambda1 -1.2289035\n",
      "24 Train Loss 444.3512 Test MSE 435.0206494198566 Test RE 0.35111073408451904 Lambda1 -1.2527801\n",
      "25 Train Loss 418.61792 Test MSE 413.7159272720516 Test RE 0.34240515225769225 Lambda1 -1.3085779\n",
      "26 Train Loss 408.25122 Test MSE 400.2787344239357 Test RE 0.33679871792772664 Lambda1 -1.3627555\n",
      "27 Train Loss 389.4028 Test MSE 393.4345157812902 Test RE 0.333906904505415 Lambda1 -1.3290071\n",
      "28 Train Loss 379.19757 Test MSE 384.1358253221793 Test RE 0.3299374220738973 Lambda1 -1.3531923\n",
      "29 Train Loss 365.78818 Test MSE 373.36718361186786 Test RE 0.32527991105469206 Lambda1 -1.301387\n",
      "30 Train Loss 346.4636 Test MSE 351.8739854351815 Test RE 0.3157786404702025 Lambda1 -1.2782642\n",
      "31 Train Loss 327.1206 Test MSE 330.7499240795629 Test RE 0.3061533720438566 Lambda1 -1.1924918\n",
      "32 Train Loss 319.70004 Test MSE 321.9925237385138 Test RE 0.30207310844557805 Lambda1 -1.1937863\n",
      "33 Train Loss 310.32172 Test MSE 304.05544941500847 Test RE 0.2935388350557416 Lambda1 -1.1727232\n",
      "34 Train Loss 296.5052 Test MSE 283.3002043247912 Test RE 0.28334308263423763 Lambda1 -1.1247231\n",
      "35 Train Loss 284.3479 Test MSE 272.9803337689281 Test RE 0.2781344926027898 Lambda1 -1.0741965\n",
      "36 Train Loss 269.51132 Test MSE 266.0140637807447 Test RE 0.27456265735785595 Lambda1 -1.0666711\n",
      "37 Train Loss 259.0878 Test MSE 258.794138353098 Test RE 0.2708110545441165 Lambda1 -1.080032\n",
      "38 Train Loss 252.09929 Test MSE 251.2914073648654 Test RE 0.26685662551977846 Lambda1 -1.1133039\n",
      "39 Train Loss 236.82158 Test MSE 239.56293815401253 Test RE 0.2605547447082328 Lambda1 -1.2016916\n",
      "40 Train Loss 228.7415 Test MSE 234.86755790341502 Test RE 0.25798870152622766 Lambda1 -1.2206479\n",
      "41 Train Loss 222.9661 Test MSE 227.62852098817984 Test RE 0.2539817486270985 Lambda1 -1.2300436\n",
      "42 Train Loss 214.89287 Test MSE 213.75541315082208 Test RE 0.2461204675557898 Lambda1 -1.2607543\n",
      "43 Train Loss 209.34265 Test MSE 205.32447590890797 Test RE 0.24121789992289155 Lambda1 -1.2718666\n",
      "44 Train Loss 199.74911 Test MSE 195.73859173012207 Test RE 0.23551978723984096 Lambda1 -1.3191885\n",
      "45 Train Loss 188.84451 Test MSE 178.70406461588541 Test RE 0.22503827282367064 Lambda1 -1.3352795\n",
      "46 Train Loss 181.16898 Test MSE 170.139725428476 Test RE 0.21957962266432807 Lambda1 -1.3547955\n",
      "47 Train Loss 172.18132 Test MSE 168.17272973871553 Test RE 0.2183066460576235 Lambda1 -1.398624\n",
      "48 Train Loss 163.43672 Test MSE 164.04043387907765 Test RE 0.2156078779938799 Lambda1 -1.4446547\n",
      "49 Train Loss 160.23296 Test MSE 162.13375398737114 Test RE 0.21435118550483154 Lambda1 -1.4855506\n",
      "50 Train Loss 155.40108 Test MSE 158.42420041590924 Test RE 0.21188486330960923 Lambda1 -1.5255729\n",
      "51 Train Loss 152.34348 Test MSE 151.296369204626 Test RE 0.20706343944844535 Lambda1 -1.5498489\n",
      "52 Train Loss 149.1013 Test MSE 147.3286513477374 Test RE 0.20433030218947631 Lambda1 -1.542413\n",
      "53 Train Loss 144.23334 Test MSE 141.56819154789846 Test RE 0.2002958784388832 Lambda1 -1.5493424\n",
      "54 Train Loss 141.86847 Test MSE 141.8947685043517 Test RE 0.20052677189430307 Lambda1 -1.5589901\n",
      "55 Train Loss 135.46683 Test MSE 139.25691783884076 Test RE 0.19865411242548964 Lambda1 -1.522483\n",
      "56 Train Loss 129.69986 Test MSE 134.25917334154764 Test RE 0.1950568269100277 Lambda1 -1.529931\n",
      "57 Train Loss 125.2764 Test MSE 131.68922253951703 Test RE 0.19318094556637494 Lambda1 -1.5186701\n",
      "58 Train Loss 121.61917 Test MSE 128.09166325489502 Test RE 0.19052396151572892 Lambda1 -1.4961742\n",
      "59 Train Loss 119.14169 Test MSE 128.30774845384627 Test RE 0.19068459671614751 Lambda1 -1.4990331\n",
      "60 Train Loss 117.34408 Test MSE 127.09227750826567 Test RE 0.18977926128794795 Lambda1 -1.4830619\n",
      "61 Train Loss 116.09414 Test MSE 125.40689159133234 Test RE 0.18851671887774402 Lambda1 -1.4840109\n",
      "62 Train Loss 114.9301 Test MSE 123.23700880326867 Test RE 0.18687867444766873 Lambda1 -1.4890971\n",
      "63 Train Loss 112.76297 Test MSE 120.25296714051366 Test RE 0.18460228446543192 Lambda1 -1.5365962\n",
      "64 Train Loss 112.05768 Test MSE 119.1275336026165 Test RE 0.18373641815910427 Lambda1 -1.5473486\n",
      "65 Train Loss 110.99009 Test MSE 118.02685724388077 Test RE 0.18288563399891689 Lambda1 -1.5630687\n",
      "66 Train Loss 109.95919 Test MSE 116.74499389852383 Test RE 0.18188978266781322 Lambda1 -1.590605\n",
      "67 Train Loss 108.98026 Test MSE 115.2907683721543 Test RE 0.18075338428723184 Lambda1 -1.6003417\n",
      "68 Train Loss 108.47569 Test MSE 114.58819016118983 Test RE 0.1802017900168472 Lambda1 -1.6040126\n",
      "69 Train Loss 107.8352 Test MSE 114.84224888127493 Test RE 0.18040144622882626 Lambda1 -1.6053818\n",
      "70 Train Loss 107.321754 Test MSE 114.45387546034692 Test RE 0.18009614716544323 Lambda1 -1.607327\n",
      "71 Train Loss 106.184265 Test MSE 113.49262147561984 Test RE 0.17933827347620432 Lambda1 -1.6420002\n",
      "72 Train Loss 104.83891 Test MSE 112.69544249020662 Test RE 0.17870732216511695 Lambda1 -1.6749492\n",
      "73 Train Loss 104.14354 Test MSE 112.08547316606634 Test RE 0.17822303512285023 Lambda1 -1.6790035\n",
      "74 Train Loss 103.24298 Test MSE 109.86260591976715 Test RE 0.17644693497857108 Lambda1 -1.723518\n",
      "Training time: 69.18\n",
      "Training time: 69.18\n",
      "inv_HT_stan_tune12\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 838.07526 Test MSE 858.4037080053847 Test RE 0.4932135087784589 Lambda1 -0.0510197\n",
      "1 Train Loss 838.0603 Test MSE 858.2480726027214 Test RE 0.49316879499052685 Lambda1 -0.050958615\n",
      "2 Train Loss 837.8622 Test MSE 857.89298889673 Test RE 0.4930667648634068 Lambda1 -0.053867534\n",
      "3 Train Loss 836.51965 Test MSE 854.5741699686502 Test RE 0.4921121090769823 Lambda1 -0.56640226\n",
      "4 Train Loss 832.7408 Test MSE 850.0813334393794 Test RE 0.49081678972283344 Lambda1 -0.80214196\n",
      "5 Train Loss 820.64276 Test MSE 833.827780355596 Test RE 0.48610193602674456 Lambda1 -1.3671513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 804.22327 Test MSE 814.3212217898358 Test RE 0.4803823552530779 Lambda1 -1.3919952\n",
      "7 Train Loss 784.2277 Test MSE 791.3190361743117 Test RE 0.4735490582978235 Lambda1 -1.5926565\n",
      "8 Train Loss 769.4 Test MSE 768.2052988730317 Test RE 0.4665818275593571 Lambda1 -1.6101154\n",
      "9 Train Loss 742.63525 Test MSE 739.2992770420116 Test RE 0.4577193909453629 Lambda1 -1.5569185\n",
      "10 Train Loss 701.0022 Test MSE 693.9901774693386 Test RE 0.4434716200969956 Lambda1 -1.4331841\n",
      "11 Train Loss 676.48987 Test MSE 671.6902722395203 Test RE 0.4362884339679747 Lambda1 -1.2695453\n",
      "12 Train Loss 669.1736 Test MSE 664.1850613663397 Test RE 0.4338441265825084 Lambda1 -1.2735522\n",
      "13 Train Loss 654.2001 Test MSE 653.9034827203426 Test RE 0.43047307820443387 Lambda1 -1.3976259\n",
      "14 Train Loss 647.108 Test MSE 646.8917020195865 Test RE 0.4281588849965463 Lambda1 -1.3745524\n",
      "15 Train Loss 635.39404 Test MSE 639.2219397015753 Test RE 0.4256131198483096 Lambda1 -1.4031234\n",
      "16 Train Loss 630.67566 Test MSE 635.0976966895718 Test RE 0.4242378757149393 Lambda1 -1.503357\n",
      "17 Train Loss 624.9857 Test MSE 630.3066092335574 Test RE 0.422634651013548 Lambda1 -1.5341687\n",
      "18 Train Loss 623.7297 Test MSE 629.0084273969377 Test RE 0.42219919673261874 Lambda1 -1.5694503\n",
      "19 Train Loss 623.23444 Test MSE 628.4671721743115 Test RE 0.422017508625858 Lambda1 -1.5839436\n",
      "20 Train Loss 621.88007 Test MSE 628.2603068742274 Test RE 0.421948047588306 Lambda1 -1.5693389\n",
      "21 Train Loss 620.3929 Test MSE 626.6962202194848 Test RE 0.42142248949691735 Lambda1 -1.6013879\n",
      "22 Train Loss 619.48376 Test MSE 625.4877213057694 Test RE 0.4210159652791969 Lambda1 -1.6153303\n",
      "23 Train Loss 619.18646 Test MSE 624.9579802004325 Test RE 0.42083764306956756 Lambda1 -1.6317075\n",
      "24 Train Loss 618.5918 Test MSE 624.7804509103095 Test RE 0.420777865999056 Lambda1 -1.6868843\n",
      "25 Train Loss 618.357 Test MSE 624.7777202060358 Test RE 0.42077694645910335 Lambda1 -1.7021146\n",
      "26 Train Loss 617.77893 Test MSE 624.6700750518493 Test RE 0.4207406963264767 Lambda1 -1.6897671\n",
      "27 Train Loss 617.47205 Test MSE 624.7467820113732 Test RE 0.42076652816166604 Lambda1 -1.6774784\n",
      "28 Train Loss 616.9293 Test MSE 624.4643485666184 Test RE 0.4206714080446217 Lambda1 -1.6393921\n",
      "29 Train Loss 616.72876 Test MSE 624.3164369418269 Test RE 0.42062158464272664 Lambda1 -1.6718408\n",
      "30 Train Loss 616.6985 Test MSE 624.385969868526 Test RE 0.42064500724846304 Lambda1 -1.6699103\n",
      "31 Train Loss 616.5399 Test MSE 624.3298657647716 Test RE 0.420626108328216 Lambda1 -1.6680001\n",
      "32 Train Loss 616.1336 Test MSE 624.4002788064143 Test RE 0.42064982714279825 Lambda1 -1.7411507\n",
      "33 Train Loss 615.99316 Test MSE 624.4204659825392 Test RE 0.42065662699843664 Lambda1 -1.7734448\n",
      "34 Train Loss 615.9154 Test MSE 624.4093013607677 Test RE 0.42065286631682447 Lambda1 -1.7761551\n",
      "35 Train Loss 615.78723 Test MSE 624.1577376311441 Test RE 0.4205681208908607 Lambda1 -1.8301854\n",
      "36 Train Loss 615.67773 Test MSE 623.9575907616492 Test RE 0.4205006842984074 Lambda1 -1.8344247\n",
      "37 Train Loss 615.62494 Test MSE 623.8706159891884 Test RE 0.4204713760357451 Lambda1 -1.8567598\n",
      "38 Train Loss 615.4601 Test MSE 623.4622749058814 Test RE 0.4203337482673228 Lambda1 -1.897388\n",
      "39 Train Loss 615.31116 Test MSE 623.3248741939458 Test RE 0.42028742843294653 Lambda1 -1.9018203\n",
      "40 Train Loss 615.1901 Test MSE 623.061156659708 Test RE 0.4201985110042753 Lambda1 -1.9573033\n",
      "41 Train Loss 615.0576 Test MSE 622.8327364483947 Test RE 0.4201214795361524 Lambda1 -1.9815549\n",
      "42 Train Loss 614.95844 Test MSE 622.8907922743814 Test RE 0.42014105937656 Lambda1 -1.9630599\n",
      "43 Train Loss 614.7528 Test MSE 622.7519145542169 Test RE 0.4200942201185042 Lambda1 -1.9242593\n",
      "44 Train Loss 614.5665 Test MSE 622.5026156233743 Test RE 0.4200101260187394 Lambda1 -1.9374638\n",
      "45 Train Loss 614.39075 Test MSE 622.4376923084885 Test RE 0.41998822317045925 Lambda1 -1.9718926\n",
      "46 Train Loss 614.17786 Test MSE 622.6911782932918 Test RE 0.4200737339716848 Lambda1 -1.9444423\n",
      "47 Train Loss 613.4723 Test MSE 622.6618699967478 Test RE 0.4200638480194931 Lambda1 -1.9794253\n",
      "48 Train Loss 613.09546 Test MSE 622.2468410382093 Test RE 0.4199238300335138 Lambda1 -2.0555234\n",
      "49 Train Loss 612.6617 Test MSE 621.7240088216765 Test RE 0.41974737606866347 Lambda1 -2.1151965\n",
      "50 Train Loss 612.4931 Test MSE 621.6507341568118 Test RE 0.4197226402098616 Lambda1 -2.123269\n",
      "51 Train Loss 612.44543 Test MSE 621.545024404286 Test RE 0.41968695243508614 Lambda1 -2.1067586\n",
      "52 Train Loss 612.31793 Test MSE 621.5887312863555 Test RE 0.41970170831354625 Lambda1 -2.1287909\n",
      "53 Train Loss 612.0385 Test MSE 621.1413607295752 Test RE 0.41955064702997974 Lambda1 -2.1748636\n",
      "54 Train Loss 611.885 Test MSE 620.9207310518891 Test RE 0.41947612812723806 Lambda1 -2.2617073\n",
      "55 Train Loss 611.79663 Test MSE 620.7925925963043 Test RE 0.4194328425724644 Lambda1 -2.2498987\n",
      "56 Train Loss 611.6592 Test MSE 620.7350902535499 Test RE 0.41941341665638987 Lambda1 -2.2665734\n",
      "57 Train Loss 611.37616 Test MSE 620.5837508703108 Test RE 0.4193622856360078 Lambda1 -2.3621943\n",
      "58 Train Loss 611.1194 Test MSE 620.4512165015009 Test RE 0.41931750289432607 Lambda1 -2.3930113\n",
      "59 Train Loss 610.96967 Test MSE 619.9780885180982 Test RE 0.41915759614044534 Lambda1 -2.3933172\n",
      "60 Train Loss 610.8673 Test MSE 619.8540895793817 Test RE 0.41911567716207876 Lambda1 -2.415895\n",
      "61 Train Loss 610.6688 Test MSE 619.6887903459851 Test RE 0.4190597897196243 Lambda1 -2.4506462\n",
      "62 Train Loss 610.28467 Test MSE 619.4075518035684 Test RE 0.41896468622478605 Lambda1 -2.5169811\n",
      "63 Train Loss 609.88776 Test MSE 618.6493852227266 Test RE 0.4187081973563998 Lambda1 -2.5076034\n",
      "64 Train Loss 608.8823 Test MSE 617.811028321184 Test RE 0.41842439693670785 Lambda1 -2.5472033\n",
      "65 Train Loss 607.92633 Test MSE 616.8102405285832 Test RE 0.41808535819261355 Lambda1 -2.5450234\n",
      "66 Train Loss 606.1872 Test MSE 614.6086971806293 Test RE 0.41733856788408547 Lambda1 -2.5683002\n",
      "67 Train Loss 603.2806 Test MSE 612.1996360526491 Test RE 0.4165198508176776 Lambda1 -2.5880568\n",
      "68 Train Loss 600.96893 Test MSE 610.2576578330642 Test RE 0.41585869805582837 Lambda1 -2.6944444\n",
      "69 Train Loss 598.5899 Test MSE 606.8581469845745 Test RE 0.4146987859616772 Lambda1 -2.7129254\n",
      "70 Train Loss 597.01025 Test MSE 605.1002033040945 Test RE 0.4140977016447962 Lambda1 -2.6344092\n",
      "71 Train Loss 593.5592 Test MSE 601.7727708829182 Test RE 0.412957575123351 Lambda1 -2.7201414\n",
      "72 Train Loss 590.6519 Test MSE 597.9415174005369 Test RE 0.41164090585991425 Lambda1 -2.8220165\n",
      "73 Train Loss 586.16895 Test MSE 591.618021095522 Test RE 0.4094584780648855 Lambda1 -2.8843114\n",
      "74 Train Loss 577.98267 Test MSE 580.4644569121199 Test RE 0.40558042566520397 Lambda1 -2.8587146\n",
      "Training time: 69.61\n",
      "Training time: 69.61\n",
      "inv_HT_stan_tune12\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 837.9341 Test MSE 858.1224487876918 Test RE 0.49313270052395985 Lambda1 0.004305562\n",
      "1 Train Loss 837.9014 Test MSE 857.8874648570455 Test RE 0.4930651774132259 Lambda1 -0.012768248\n",
      "2 Train Loss 837.79535 Test MSE 857.8487398460018 Test RE 0.49305404881495235 Lambda1 -0.86078966\n",
      "3 Train Loss 834.73956 Test MSE 854.3324797590573 Test RE 0.49204251471425825 Lambda1 -1.5856776\n",
      "4 Train Loss 820.3639 Test MSE 833.8740576370914 Test RE 0.4861154251215817 Lambda1 -1.792695\n",
      "5 Train Loss 789.6187 Test MSE 794.4559914034633 Test RE 0.47448675398493884 Lambda1 -1.6013236\n",
      "6 Train Loss 750.47125 Test MSE 748.5064364764454 Test RE 0.46056076757691267 Lambda1 -1.593938\n",
      "7 Train Loss 736.3651 Test MSE 726.9862304743522 Test RE 0.4538917230284826 Lambda1 -1.6387687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 706.1985 Test MSE 697.7101046153773 Test RE 0.44465858024389454 Lambda1 -1.7358607\n",
      "9 Train Loss 688.3045 Test MSE 672.234734880185 Test RE 0.4364652227685315 Lambda1 -1.7974058\n",
      "10 Train Loss 664.5984 Test MSE 646.5662815197082 Test RE 0.428051178246603 Lambda1 -1.864812\n",
      "11 Train Loss 641.31146 Test MSE 637.5362577589588 Test RE 0.42505156061226046 Lambda1 -2.1118524\n",
      "12 Train Loss 620.34436 Test MSE 611.7068347507923 Test RE 0.4163521744286935 Lambda1 -2.2405667\n",
      "13 Train Loss 611.6058 Test MSE 603.1925337623546 Test RE 0.41344443366536376 Lambda1 -2.268081\n",
      "14 Train Loss 601.816 Test MSE 591.779890824371 Test RE 0.40951448920510797 Lambda1 -2.284221\n",
      "15 Train Loss 595.7413 Test MSE 589.0635190532845 Test RE 0.4085735371101048 Lambda1 -2.3232956\n",
      "16 Train Loss 590.2669 Test MSE 582.6631076930297 Test RE 0.4063478170992979 Lambda1 -2.443342\n",
      "17 Train Loss 586.86334 Test MSE 578.0286943040393 Test RE 0.40472857662838546 Lambda1 -2.537586\n",
      "18 Train Loss 581.2289 Test MSE 572.3276493657961 Test RE 0.40272773011605906 Lambda1 -2.663582\n",
      "19 Train Loss 575.9563 Test MSE 568.7288707156822 Test RE 0.4014595636363543 Lambda1 -2.6941068\n",
      "20 Train Loss 569.77313 Test MSE 564.3864497071108 Test RE 0.39992399284935964 Lambda1 -2.8938713\n",
      "21 Train Loss 561.2638 Test MSE 557.1849553306365 Test RE 0.39736431345269413 Lambda1 -3.1083746\n",
      "22 Train Loss 556.81036 Test MSE 549.1415253835337 Test RE 0.3944857442654781 Lambda1 -3.206963\n",
      "23 Train Loss 551.2537 Test MSE 543.5585137717306 Test RE 0.392475292575068 Lambda1 -3.2334087\n",
      "24 Train Loss 538.41046 Test MSE 529.358760460965 Test RE 0.3873149163827773 Lambda1 -3.2758143\n",
      "25 Train Loss 529.41187 Test MSE 519.1529805877799 Test RE 0.38356312342275867 Lambda1 -3.364387\n",
      "26 Train Loss 504.99866 Test MSE 495.6121335771973 Test RE 0.37476595889371517 Lambda1 -3.5980263\n",
      "27 Train Loss 492.03333 Test MSE 479.19408368422444 Test RE 0.36850628080568076 Lambda1 -3.7461553\n",
      "28 Train Loss 479.4937 Test MSE 471.0249143619351 Test RE 0.36535168131775525 Lambda1 -3.786431\n",
      "29 Train Loss 468.16534 Test MSE 453.3428140642604 Test RE 0.35842850325078635 Lambda1 -3.8563697\n",
      "30 Train Loss 460.69678 Test MSE 443.91334457349313 Test RE 0.3546812831382194 Lambda1 -3.889423\n",
      "31 Train Loss 443.7146 Test MSE 428.5315117238642 Test RE 0.3484821617035544 Lambda1 -4.006434\n",
      "32 Train Loss 438.12115 Test MSE 427.811607523738 Test RE 0.34818932533518315 Lambda1 -3.9755557\n",
      "33 Train Loss 425.66425 Test MSE 408.6763815922617 Test RE 0.34031331411615556 Lambda1 -4.0159364\n",
      "34 Train Loss 409.26508 Test MSE 396.7220909332271 Test RE 0.3352990807343946 Lambda1 -3.9959464\n",
      "35 Train Loss 397.76477 Test MSE 384.804437910698 Test RE 0.3302244356646016 Lambda1 -3.9773498\n",
      "36 Train Loss 390.3396 Test MSE 378.722740988715 Test RE 0.3276045029362866 Lambda1 -3.9462488\n",
      "37 Train Loss 385.10532 Test MSE 376.14039441415645 Test RE 0.3264856957997575 Lambda1 -3.9594207\n",
      "38 Train Loss 380.17947 Test MSE 367.99339425492775 Test RE 0.3229305870307865 Lambda1 -3.9507463\n",
      "39 Train Loss 377.23904 Test MSE 368.7814814205542 Test RE 0.3232761928814142 Lambda1 -3.9382792\n",
      "40 Train Loss 369.01965 Test MSE 359.43437496625677 Test RE 0.3191530323820328 Lambda1 -3.9229815\n",
      "41 Train Loss 352.9909 Test MSE 352.09980982719634 Test RE 0.3158799538448279 Lambda1 -3.867052\n",
      "42 Train Loss 344.92563 Test MSE 344.63159948411726 Test RE 0.3125120145337467 Lambda1 -3.829196\n",
      "43 Train Loss 342.8481 Test MSE 343.98028614290206 Test RE 0.31221656933391306 Lambda1 -3.7802029\n",
      "44 Train Loss 339.26086 Test MSE 341.24124970553714 Test RE 0.310971030284173 Lambda1 -3.7393303\n",
      "45 Train Loss 331.2937 Test MSE 332.6695115661815 Test RE 0.3070405044465512 Lambda1 -3.7335079\n",
      "46 Train Loss 324.12006 Test MSE 324.7294073193747 Test RE 0.3033541778965454 Lambda1 -3.7249897\n",
      "47 Train Loss 312.57605 Test MSE 314.30566445990496 Test RE 0.298445664724386 Lambda1 -3.61136\n",
      "48 Train Loss 299.66385 Test MSE 300.29148044050436 Test RE 0.2917162863068371 Lambda1 -3.4184933\n",
      "49 Train Loss 289.50797 Test MSE 286.6993442706721 Test RE 0.28503784141527166 Lambda1 -3.3078606\n",
      "50 Train Loss 280.31995 Test MSE 270.09856264184543 Test RE 0.27666250645261137 Lambda1 -3.2307954\n",
      "51 Train Loss 269.76828 Test MSE 267.584963845675 Test RE 0.2753721551828524 Lambda1 -3.2122698\n",
      "52 Train Loss 261.17847 Test MSE 260.9845129312947 Test RE 0.27195468130219846 Lambda1 -3.2088313\n",
      "53 Train Loss 259.67102 Test MSE 259.062350278106 Test RE 0.27095135127757164 Lambda1 -3.2110507\n",
      "54 Train Loss 253.71982 Test MSE 255.0556816613172 Test RE 0.2688479144367464 Lambda1 -3.269901\n",
      "55 Train Loss 247.10364 Test MSE 250.88455089095714 Test RE 0.2666405092410721 Lambda1 -3.3166687\n",
      "56 Train Loss 239.31671 Test MSE 235.38276058781273 Test RE 0.25827150699667073 Lambda1 -3.3070602\n",
      "57 Train Loss 232.88324 Test MSE 230.84262214118232 Test RE 0.25576856687316213 Lambda1 -3.28652\n",
      "58 Train Loss 228.70767 Test MSE 227.0102343268409 Test RE 0.253636580311287 Lambda1 -3.2816372\n",
      "59 Train Loss 221.83273 Test MSE 215.20645357412803 Test RE 0.24695442705897008 Lambda1 -3.1994088\n",
      "60 Train Loss 218.59427 Test MSE 216.0206013096969 Test RE 0.24742111285761795 Lambda1 -3.1374018\n",
      "61 Train Loss 213.93245 Test MSE 211.4729330986199 Test RE 0.24480290384744158 Lambda1 -3.0536485\n",
      "62 Train Loss 210.65375 Test MSE 205.08799100495372 Test RE 0.2410789471105257 Lambda1 -3.0393522\n",
      "63 Train Loss 202.05612 Test MSE 198.4786463814979 Test RE 0.23716252490581352 Lambda1 -3.0047097\n",
      "64 Train Loss 198.75665 Test MSE 193.38775622861408 Test RE 0.23410120971201373 Lambda1 -2.9663334\n",
      "65 Train Loss 191.01051 Test MSE 193.53164082853445 Test RE 0.23418828165639594 Lambda1 -2.930346\n",
      "66 Train Loss 187.55629 Test MSE 188.85045838490842 Test RE 0.2313386475486879 Lambda1 -2.9330523\n",
      "67 Train Loss 185.75786 Test MSE 186.95588966983436 Test RE 0.23017531505673164 Lambda1 -2.9391081\n",
      "68 Train Loss 181.38106 Test MSE 185.11853137455012 Test RE 0.2290414680869704 Lambda1 -2.9369755\n",
      "69 Train Loss 178.68234 Test MSE 183.65283393443607 Test RE 0.22813293496644285 Lambda1 -2.9243853\n",
      "70 Train Loss 176.30151 Test MSE 180.9100071283468 Test RE 0.22642296126598394 Lambda1 -2.9118712\n",
      "71 Train Loss 173.93532 Test MSE 177.82887655115434 Test RE 0.22448654351441827 Lambda1 -2.9175858\n",
      "72 Train Loss 171.82425 Test MSE 178.18784462078955 Test RE 0.2247130052594689 Lambda1 -2.9007626\n",
      "73 Train Loss 169.81084 Test MSE 178.2704654009745 Test RE 0.2247650958277662 Lambda1 -2.888849\n",
      "74 Train Loss 167.32588 Test MSE 175.2681556253295 Test RE 0.22286438890535856 Lambda1 -2.8411546\n",
      "Training time: 69.89\n",
      "Training time: 69.89\n",
      "inv_HT_stan_tune12\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 838.08606 Test MSE 858.3090210668767 Test RE 0.49318630586410905 Lambda1 -0.0016887379\n",
      "1 Train Loss 837.8803 Test MSE 857.8448025487158 Test RE 0.4930529173202578 Lambda1 -0.0033566887\n",
      "2 Train Loss 837.7873 Test MSE 857.8458737550607 Test RE 0.4930532251622164 Lambda1 -0.0412718\n",
      "3 Train Loss 834.6981 Test MSE 853.8633527283777 Test RE 0.49190740212858425 Lambda1 0.05089172\n",
      "4 Train Loss 811.7939 Test MSE 818.9567029552842 Test RE 0.48174769076051044 Lambda1 0.3301263\n",
      "5 Train Loss 762.458 Test MSE 758.7970420253882 Test RE 0.46371589776499794 Lambda1 0.12221637\n",
      "6 Train Loss 706.91187 Test MSE 709.9557325598186 Test RE 0.44854374603418723 Lambda1 0.124889016\n",
      "7 Train Loss 665.592 Test MSE 669.2179869578061 Test RE 0.43548477201608077 Lambda1 0.048036553\n",
      "8 Train Loss 648.47375 Test MSE 653.9638061285539 Test RE 0.43049293358450785 Lambda1 0.022756208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 642.559 Test MSE 647.4868669785182 Test RE 0.42835580094024056 Lambda1 0.013967641\n",
      "10 Train Loss 638.9756 Test MSE 645.9923879703108 Test RE 0.42786116652438233 Lambda1 0.012890615\n",
      "11 Train Loss 634.2808 Test MSE 640.1709933513755 Test RE 0.42592895683870385 Lambda1 0.009676718\n",
      "12 Train Loss 624.4064 Test MSE 633.4029782570049 Test RE 0.42367147150661927 Lambda1 0.0025798557\n",
      "13 Train Loss 558.73834 Test MSE 534.3019347858295 Test RE 0.3891190957674033 Lambda1 0.0016015696\n",
      "14 Train Loss 388.8938 Test MSE 346.08777273150673 Test RE 0.31317154770573086 Lambda1 0.00094894523\n",
      "15 Train Loss 284.40167 Test MSE 292.08474218074747 Test RE 0.28770248041238417 Lambda1 0.0012223207\n",
      "16 Train Loss 262.14804 Test MSE 282.3697256862277 Test RE 0.2828773902020219 Lambda1 0.002435038\n",
      "17 Train Loss 256.0316 Test MSE 282.86279815299577 Test RE 0.2831242619513856 Lambda1 0.0028689536\n",
      "18 Train Loss 254.51837 Test MSE 281.57827142573814 Test RE 0.2824806734207118 Lambda1 0.0033030773\n",
      "19 Train Loss 253.88644 Test MSE 280.801265907908 Test RE 0.28209065634268604 Lambda1 0.0034360827\n",
      "20 Train Loss 253.50674 Test MSE 280.3628335842514 Test RE 0.28187034754946094 Lambda1 0.0039277496\n",
      "21 Train Loss 252.83331 Test MSE 279.045831443874 Test RE 0.2812075263964162 Lambda1 0.005357911\n",
      "22 Train Loss 251.7737 Test MSE 277.6559058032368 Test RE 0.2805063055969662 Lambda1 0.0071235565\n",
      "23 Train Loss 250.37955 Test MSE 275.55737076899027 Test RE 0.2794442557985146 Lambda1 0.011025\n",
      "24 Train Loss 249.12149 Test MSE 273.84168820248686 Test RE 0.2785729558569325 Lambda1 0.014690513\n",
      "25 Train Loss 246.94484 Test MSE 269.1333560054173 Test RE 0.27616773247467535 Lambda1 0.02265534\n",
      "26 Train Loss 240.94852 Test MSE 259.2632575080884 Test RE 0.27105639459137454 Lambda1 0.042756964\n",
      "27 Train Loss 234.2357 Test MSE 250.16856245627488 Test RE 0.26625976055931666 Lambda1 0.081786774\n",
      "28 Train Loss 229.43279 Test MSE 245.75065071033143 Test RE 0.26389824913959714 Lambda1 0.10031499\n",
      "29 Train Loss 217.04262 Test MSE 237.57465882489282 Test RE 0.25947124027290946 Lambda1 0.13440728\n",
      "30 Train Loss 213.86366 Test MSE 236.26541555326912 Test RE 0.25875529628903654 Lambda1 0.15142536\n",
      "31 Train Loss 206.2068 Test MSE 223.92887161462278 Test RE 0.2519093089165001 Lambda1 0.19231729\n",
      "32 Train Loss 201.34903 Test MSE 223.21029847975825 Test RE 0.2515048038821394 Lambda1 0.20660053\n",
      "33 Train Loss 198.5892 Test MSE 218.64683798979627 Test RE 0.24892056104307908 Lambda1 0.22052899\n",
      "34 Train Loss 191.27095 Test MSE 209.99847756132823 Test RE 0.24394798979140356 Lambda1 0.23765804\n",
      "35 Train Loss 182.32596 Test MSE 191.13679012804766 Test RE 0.23273479366557837 Lambda1 0.24399956\n",
      "36 Train Loss 177.27327 Test MSE 187.3422787265636 Test RE 0.23041304845453645 Lambda1 0.24959742\n",
      "37 Train Loss 173.98021 Test MSE 180.46158568731389 Test RE 0.22614216998289424 Lambda1 0.2562765\n",
      "38 Train Loss 165.35352 Test MSE 164.67407150938647 Test RE 0.21602389003508643 Lambda1 0.27979362\n",
      "39 Train Loss 163.43372 Test MSE 162.23783745287312 Test RE 0.21441997696318268 Lambda1 0.2878336\n",
      "40 Train Loss 159.28767 Test MSE 157.0139637506527 Test RE 0.2109396928461733 Lambda1 0.3018545\n",
      "41 Train Loss 148.38925 Test MSE 144.70795645518493 Test RE 0.20250482521502747 Lambda1 0.3487442\n",
      "42 Train Loss 142.63333 Test MSE 135.6392433617184 Test RE 0.19605677290519827 Lambda1 0.37910432\n",
      "43 Train Loss 138.21872 Test MSE 131.29682242143267 Test RE 0.19289291599961464 Lambda1 0.4213217\n",
      "44 Train Loss 129.66985 Test MSE 119.00895946294533 Test RE 0.1836449539482474 Lambda1 0.433975\n",
      "45 Train Loss 126.34318 Test MSE 120.4309426599905 Test RE 0.18473884051444803 Lambda1 0.43442827\n",
      "46 Train Loss 122.83506 Test MSE 115.56263181377174 Test RE 0.18096637316280678 Lambda1 0.44660246\n",
      "47 Train Loss 120.75292 Test MSE 115.81675133561527 Test RE 0.18116523430901854 Lambda1 0.4548682\n",
      "48 Train Loss 111.40362 Test MSE 109.49178196766016 Test RE 0.17614889882970247 Lambda1 0.50621015\n",
      "49 Train Loss 105.12675 Test MSE 101.060207814669 Test RE 0.1692307441352917 Lambda1 0.56099296\n",
      "50 Train Loss 102.15292 Test MSE 99.54949245591091 Test RE 0.16796109438535567 Lambda1 0.5754068\n",
      "51 Train Loss 97.16133 Test MSE 94.02090876222294 Test RE 0.16323053081012093 Lambda1 0.577001\n",
      "52 Train Loss 94.68513 Test MSE 90.26562351311142 Test RE 0.15993752231807842 Lambda1 0.57251126\n",
      "53 Train Loss 90.33621 Test MSE 85.89298060682636 Test RE 0.1560155934674713 Lambda1 0.5838604\n",
      "54 Train Loss 87.255424 Test MSE 86.15628859698575 Test RE 0.15625454615636483 Lambda1 0.59748787\n",
      "55 Train Loss 85.35811 Test MSE 83.78459540431263 Test RE 0.15408886599839253 Lambda1 0.6022325\n",
      "56 Train Loss 84.84733 Test MSE 82.44548394275145 Test RE 0.15285252116872278 Lambda1 0.60646224\n",
      "57 Train Loss 83.66542 Test MSE 82.06012154146428 Test RE 0.15249487509661958 Lambda1 0.62332666\n",
      "58 Train Loss 81.08428 Test MSE 76.03161113743793 Test RE 0.14678655141976063 Lambda1 0.640999\n",
      "59 Train Loss 79.4504 Test MSE 75.1200200733164 Test RE 0.14590393950163333 Lambda1 0.6564478\n",
      "60 Train Loss 77.007 Test MSE 70.37276140958919 Test RE 0.14121845868376823 Lambda1 0.6861992\n",
      "61 Train Loss 76.06524 Test MSE 70.22190027202659 Test RE 0.14106700940901412 Lambda1 0.69589466\n",
      "62 Train Loss 74.7857 Test MSE 68.63524489148531 Test RE 0.13946420785852082 Lambda1 0.7072702\n",
      "63 Train Loss 72.381744 Test MSE 66.61188042847579 Test RE 0.1373931300239728 Lambda1 0.7243011\n",
      "64 Train Loss 70.10844 Test MSE 64.92666995962861 Test RE 0.13564404586542336 Lambda1 0.730588\n",
      "65 Train Loss 67.94153 Test MSE 63.87860863316392 Test RE 0.13454479293458857 Lambda1 0.7339904\n",
      "66 Train Loss 66.25224 Test MSE 62.60446278702392 Test RE 0.13319619458735155 Lambda1 0.7423552\n",
      "67 Train Loss 64.78551 Test MSE 62.823377047907805 Test RE 0.13342887049719482 Lambda1 0.752246\n",
      "68 Train Loss 63.960037 Test MSE 60.54105945367571 Test RE 0.1309827725417559 Lambda1 0.7703041\n",
      "69 Train Loss 63.292416 Test MSE 59.74498925887543 Test RE 0.13011875952539262 Lambda1 0.77919716\n",
      "70 Train Loss 61.255398 Test MSE 56.60084387333608 Test RE 0.12664866718048987 Lambda1 0.8033544\n",
      "71 Train Loss 60.351402 Test MSE 55.44030978207055 Test RE 0.1253435511799539 Lambda1 0.81644833\n",
      "72 Train Loss 59.856667 Test MSE 54.262814649418814 Test RE 0.1240053233739868 Lambda1 0.82826966\n",
      "73 Train Loss 58.584618 Test MSE 55.05644816750124 Test RE 0.1249088660880261 Lambda1 0.8295407\n",
      "74 Train Loss 58.03742 Test MSE 54.81467633391642 Test RE 0.12463430539958828 Lambda1 0.840487\n",
      "Training time: 68.61\n",
      "Training time: 68.61\n",
      "inv_HT_stan_tune12\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 838.0608 Test MSE 858.313219215514 Test RE 0.493187511995488 Lambda1 -0.19105083\n",
      "1 Train Loss 837.9368 Test MSE 857.8406092302322 Test RE 0.49305171224771316 Lambda1 -0.19137442\n",
      "2 Train Loss 837.8693 Test MSE 857.8782348540415 Test RE 0.4930625249643348 Lambda1 -0.19396956\n",
      "3 Train Loss 837.68677 Test MSE 857.440360856637 Test RE 0.49293667562663335 Lambda1 -0.32505906\n",
      "4 Train Loss 834.63617 Test MSE 852.7145472804751 Test RE 0.4915763795440603 Lambda1 -0.6777781\n",
      "5 Train Loss 825.71027 Test MSE 843.4126288085712 Test RE 0.4888878232892476 Lambda1 -1.0608456\n",
      "6 Train Loss 810.1017 Test MSE 819.164010366246 Test RE 0.4818086607402319 Lambda1 -1.1242776\n",
      "7 Train Loss 799.83093 Test MSE 809.7703939909718 Test RE 0.479038168103639 Lambda1 -1.1226847\n",
      "8 Train Loss 780.83716 Test MSE 784.9321134246717 Test RE 0.47163412337576466 Lambda1 -0.8575022\n",
      "9 Train Loss 749.4127 Test MSE 743.8965451747017 Test RE 0.45914032944703 Lambda1 -0.7262875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Train Loss 718.08734 Test MSE 706.2662092916839 Test RE 0.44737672378619897 Lambda1 -0.6917414\n",
      "11 Train Loss 689.70435 Test MSE 673.8576356741645 Test RE 0.43699175964534426 Lambda1 -0.7127984\n",
      "12 Train Loss 672.4526 Test MSE 661.656386806738 Test RE 0.43301747688025527 Lambda1 -0.74236596\n",
      "13 Train Loss 659.49426 Test MSE 648.9111692026403 Test RE 0.42882667764827265 Lambda1 -0.77855563\n",
      "14 Train Loss 641.2724 Test MSE 625.5124931791952 Test RE 0.4210243021742211 Lambda1 -0.8492232\n",
      "15 Train Loss 619.3789 Test MSE 595.7264302332525 Test RE 0.4108777321263839 Lambda1 -0.947519\n",
      "16 Train Loss 603.80695 Test MSE 587.0448615256018 Test RE 0.40787286746027007 Lambda1 -1.0024966\n",
      "17 Train Loss 585.1647 Test MSE 577.2239583579749 Test RE 0.40444674540047293 Lambda1 -1.0767783\n",
      "18 Train Loss 569.7854 Test MSE 560.5746064998914 Test RE 0.3985711695313551 Lambda1 -1.0924911\n",
      "19 Train Loss 563.1651 Test MSE 551.5702139050186 Test RE 0.39535712801838235 Lambda1 -1.1522938\n",
      "20 Train Loss 555.11035 Test MSE 540.9124643751734 Test RE 0.39151883992186787 Lambda1 -1.1903187\n",
      "21 Train Loss 547.953 Test MSE 536.8138603412266 Test RE 0.3900327103519377 Lambda1 -1.2234764\n",
      "22 Train Loss 532.6354 Test MSE 521.088300300638 Test RE 0.3842773895234323 Lambda1 -1.3002708\n",
      "23 Train Loss 523.4804 Test MSE 517.5679033423047 Test RE 0.3829771285614282 Lambda1 -1.323097\n",
      "24 Train Loss 516.43134 Test MSE 509.9703765876654 Test RE 0.38015582138340553 Lambda1 -1.4034853\n",
      "25 Train Loss 511.22235 Test MSE 505.97898051135206 Test RE 0.3786652121507805 Lambda1 -1.4664547\n",
      "26 Train Loss 506.31506 Test MSE 501.1908975214921 Test RE 0.3768692974533248 Lambda1 -1.4372952\n",
      "27 Train Loss 492.2001 Test MSE 482.33904875672573 Test RE 0.36971356204885203 Lambda1 -1.4955893\n",
      "28 Train Loss 479.29007 Test MSE 475.3858384768834 Test RE 0.36703906568774 Lambda1 -1.5486721\n",
      "29 Train Loss 467.67682 Test MSE 459.60751997761923 Test RE 0.3608965526138233 Lambda1 -1.5677398\n",
      "30 Train Loss 453.92258 Test MSE 443.42544761290503 Test RE 0.35448631772804673 Lambda1 -1.5773077\n",
      "31 Train Loss 442.72507 Test MSE 430.1893462828857 Test RE 0.3491555872042364 Lambda1 -1.5730406\n",
      "32 Train Loss 428.21515 Test MSE 415.8243627492486 Test RE 0.34327654933025925 Lambda1 -1.5780474\n",
      "33 Train Loss 409.54526 Test MSE 402.829230258951 Test RE 0.3378700210545487 Lambda1 -1.587888\n",
      "34 Train Loss 387.48788 Test MSE 377.5885216090783 Test RE 0.3271135712378063 Lambda1 -1.5776775\n",
      "35 Train Loss 371.93323 Test MSE 361.17399676436037 Test RE 0.3199244324600276 Lambda1 -1.5961475\n",
      "36 Train Loss 353.09238 Test MSE 336.03480932627406 Test RE 0.3085896134548894 Lambda1 -1.6403548\n",
      "37 Train Loss 341.6731 Test MSE 321.8278683117237 Test RE 0.30199586389313327 Lambda1 -1.6707507\n",
      "38 Train Loss 329.93274 Test MSE 317.3952302347689 Test RE 0.29990891013579396 Lambda1 -1.6876866\n",
      "39 Train Loss 318.4394 Test MSE 306.3624477585237 Test RE 0.294650332841024 Lambda1 -1.7129564\n",
      "40 Train Loss 305.71497 Test MSE 295.83097578033374 Test RE 0.289541615779647 Lambda1 -1.6941092\n",
      "41 Train Loss 298.5721 Test MSE 287.46720548604645 Test RE 0.28541929180076636 Lambda1 -1.7003136\n",
      "42 Train Loss 293.39694 Test MSE 283.7682280043977 Test RE 0.2835770333082344 Lambda1 -1.7077159\n",
      "43 Train Loss 285.0264 Test MSE 273.03406629024335 Test RE 0.27816186478010757 Lambda1 -1.7269803\n",
      "44 Train Loss 278.07083 Test MSE 267.7939930204298 Test RE 0.2754796903311724 Lambda1 -1.7094439\n",
      "45 Train Loss 274.70398 Test MSE 265.9820924294451 Test RE 0.27454615747281774 Lambda1 -1.7002631\n",
      "46 Train Loss 272.538 Test MSE 262.77587792998645 Test RE 0.27288641663769375 Lambda1 -1.6868871\n",
      "47 Train Loss 268.5274 Test MSE 261.2993778492485 Test RE 0.27211868179448007 Lambda1 -1.696436\n",
      "48 Train Loss 262.7587 Test MSE 249.40147717068592 Test RE 0.2658512345040814 Lambda1 -1.6988215\n",
      "49 Train Loss 259.12808 Test MSE 245.978876836313 Test RE 0.2640207605103238 Lambda1 -1.6988393\n",
      "50 Train Loss 251.8294 Test MSE 238.70159231927525 Test RE 0.2600859120892568 Lambda1 -1.7115601\n",
      "51 Train Loss 246.26863 Test MSE 237.9166785414774 Test RE 0.25965794445198004 Lambda1 -1.7251523\n",
      "52 Train Loss 237.70299 Test MSE 231.96807672450515 Test RE 0.25639129828496243 Lambda1 -1.6996608\n",
      "53 Train Loss 235.12898 Test MSE 231.02377514925738 Test RE 0.25586890397388673 Lambda1 -1.6919373\n",
      "54 Train Loss 230.77916 Test MSE 227.55752970410893 Test RE 0.25394214046109576 Lambda1 -1.6966351\n",
      "55 Train Loss 222.26117 Test MSE 222.506565899221 Test RE 0.25110802145220973 Lambda1 -1.6854259\n",
      "56 Train Loss 215.9994 Test MSE 216.8811467899275 Test RE 0.2479134397347948 Lambda1 -1.6753166\n",
      "57 Train Loss 211.93138 Test MSE 211.17704212585855 Test RE 0.2446315809182636 Lambda1 -1.6605959\n",
      "58 Train Loss 208.48883 Test MSE 206.67624146868104 Test RE 0.24201063330793882 Lambda1 -1.6630675\n",
      "59 Train Loss 206.24141 Test MSE 203.39798186803915 Test RE 0.240083597660656 Lambda1 -1.6643153\n",
      "60 Train Loss 204.46579 Test MSE 202.10561748776695 Test RE 0.23931965221331866 Lambda1 -1.6576035\n",
      "61 Train Loss 203.87305 Test MSE 201.1754247541061 Test RE 0.2387682817456619 Lambda1 -1.6553054\n",
      "62 Train Loss 199.9105 Test MSE 196.02430280596275 Test RE 0.23569161353585225 Lambda1 -1.6471167\n",
      "63 Train Loss 197.65106 Test MSE 194.82995399502744 Test RE 0.234972498386287 Lambda1 -1.6415321\n",
      "64 Train Loss 196.61093 Test MSE 194.4485144374717 Test RE 0.23474237022714564 Lambda1 -1.6314377\n",
      "65 Train Loss 195.16353 Test MSE 194.37456213751318 Test RE 0.23469772758974605 Lambda1 -1.6324502\n",
      "66 Train Loss 191.43922 Test MSE 192.91207622300846 Test RE 0.2338131205686519 Lambda1 -1.6219712\n",
      "67 Train Loss 189.3121 Test MSE 192.77396320155546 Test RE 0.23372940777285886 Lambda1 -1.622124\n",
      "68 Train Loss 184.83147 Test MSE 189.38919004896832 Test RE 0.23166838119057742 Lambda1 -1.6297314\n",
      "69 Train Loss 181.66522 Test MSE 185.71605792158465 Test RE 0.22941082089132853 Lambda1 -1.6125447\n",
      "70 Train Loss 178.0799 Test MSE 181.00595298446237 Test RE 0.22648299516236048 Lambda1 -1.5913439\n",
      "71 Train Loss 175.31094 Test MSE 176.5060641474287 Test RE 0.22365004302769106 Lambda1 -1.5792185\n",
      "72 Train Loss 173.66423 Test MSE 173.90405142781745 Test RE 0.22199542304846492 Lambda1 -1.5767438\n",
      "73 Train Loss 172.43423 Test MSE 173.24564079925685 Test RE 0.22157478087888025 Lambda1 -1.5788116\n",
      "74 Train Loss 170.62682 Test MSE 169.6012317798797 Test RE 0.21923186161195748 Lambda1 -1.569408\n",
      "Training time: 68.91\n",
      "Training time: 68.91\n",
      "inv_HT_stan_tune12\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 838.0514 Test MSE 858.2964792811475 Test RE 0.493182702581715 Lambda1 -0.0075172866\n",
      "1 Train Loss 838.01373 Test MSE 858.0595309129337 Test RE 0.4931146218509914 Lambda1 -0.007594569\n",
      "2 Train Loss 837.8874 Test MSE 857.909625527795 Test RE 0.49307154572197254 Lambda1 -0.013316241\n",
      "3 Train Loss 837.7055 Test MSE 857.5712524263414 Test RE 0.4929742985308123 Lambda1 -0.21532987\n",
      "4 Train Loss 837.1691 Test MSE 856.6187762648248 Test RE 0.49270045730997414 Lambda1 -0.25937742\n",
      "5 Train Loss 829.7357 Test MSE 841.6005697134219 Test RE 0.4883623569039419 Lambda1 -0.6482798\n",
      "6 Train Loss 810.376 Test MSE 821.3878687731919 Test RE 0.4824622221795021 Lambda1 -0.34148893\n",
      "7 Train Loss 779.8371 Test MSE 776.3375862640485 Test RE 0.4690449636638046 Lambda1 -0.39584547\n",
      "8 Train Loss 743.161 Test MSE 745.9702093727705 Test RE 0.4597798271560302 Lambda1 -0.50706357\n",
      "9 Train Loss 721.39984 Test MSE 719.7062751755208 Test RE 0.45161339557029667 Lambda1 -0.6168117\n",
      "10 Train Loss 699.47125 Test MSE 698.2653976131338 Test RE 0.44483549232035685 Lambda1 -0.5934621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Train Loss 674.313 Test MSE 674.3854078506786 Test RE 0.4371628543537982 Lambda1 -0.71477824\n",
      "12 Train Loss 656.27795 Test MSE 649.5029365043418 Test RE 0.42902216494272605 Lambda1 -0.89988244\n",
      "13 Train Loss 637.31354 Test MSE 619.2255560667653 Test RE 0.41890313112399946 Lambda1 -1.1284533\n",
      "14 Train Loss 599.68506 Test MSE 589.6774082967062 Test RE 0.40878637798220024 Lambda1 -1.1406994\n",
      "15 Train Loss 556.57025 Test MSE 525.126244949355 Test RE 0.38576341058952573 Lambda1 -0.9966599\n",
      "16 Train Loss 504.78418 Test MSE 470.2771154456338 Test RE 0.3650615500465215 Lambda1 -0.9795771\n",
      "17 Train Loss 452.1822 Test MSE 432.3074744739441 Test RE 0.35001410240518344 Lambda1 -0.94855684\n",
      "18 Train Loss 391.61298 Test MSE 373.3553637993342 Test RE 0.32527476226528645 Lambda1 -0.88020355\n",
      "19 Train Loss 354.3109 Test MSE 326.430738908776 Test RE 0.3041478106869692 Lambda1 -0.8078208\n",
      "20 Train Loss 304.98523 Test MSE 294.63476617340405 Test RE 0.2889556337079721 Lambda1 -0.8319632\n",
      "21 Train Loss 262.98465 Test MSE 258.0624123502077 Test RE 0.2704279319561335 Lambda1 -0.75678074\n",
      "22 Train Loss 246.58246 Test MSE 254.0146067263892 Test RE 0.2682986676623327 Lambda1 -0.74915624\n",
      "23 Train Loss 220.70752 Test MSE 241.65288231682936 Test RE 0.26168881488981177 Lambda1 -0.67425394\n",
      "24 Train Loss 200.32986 Test MSE 219.7057134156229 Test RE 0.24952257642734577 Lambda1 -0.71480304\n",
      "25 Train Loss 192.6122 Test MSE 209.62796323138684 Test RE 0.24373268792147754 Lambda1 -0.7313878\n",
      "26 Train Loss 181.37796 Test MSE 195.45459995548072 Test RE 0.2353488706106322 Lambda1 -0.87229884\n",
      "27 Train Loss 173.08382 Test MSE 185.8720796414471 Test RE 0.22950716570089283 Lambda1 -0.9483806\n",
      "28 Train Loss 167.68903 Test MSE 178.39892586343942 Test RE 0.2248460633297757 Lambda1 -1.0151521\n",
      "29 Train Loss 160.59193 Test MSE 171.85312077544495 Test RE 0.22068249338677828 Lambda1 -1.1474541\n",
      "30 Train Loss 157.47559 Test MSE 165.81471455023325 Test RE 0.216770762091011 Lambda1 -1.1706268\n",
      "31 Train Loss 153.03201 Test MSE 160.26915603433326 Test RE 0.2131150624252792 Lambda1 -1.268877\n",
      "32 Train Loss 151.66971 Test MSE 161.64566371955254 Test RE 0.2140282990508173 Lambda1 -1.2686063\n",
      "33 Train Loss 146.85866 Test MSE 151.6149080316242 Test RE 0.2072813001451747 Lambda1 -1.3878655\n",
      "34 Train Loss 142.59549 Test MSE 150.5585793998571 Test RE 0.2065579547696797 Lambda1 -1.4085959\n",
      "35 Train Loss 140.91965 Test MSE 149.20166237959532 Test RE 0.20562504087809746 Lambda1 -1.4147782\n",
      "36 Train Loss 135.70505 Test MSE 145.89424666017052 Test RE 0.20333318035373737 Lambda1 -1.4460481\n",
      "37 Train Loss 131.18945 Test MSE 140.46019384167627 Test RE 0.19951052081961013 Lambda1 -1.4940076\n",
      "38 Train Loss 128.32925 Test MSE 136.3372946194331 Test RE 0.1965606169429322 Lambda1 -1.5786427\n",
      "39 Train Loss 126.97962 Test MSE 134.98974334378573 Test RE 0.19558680684307578 Lambda1 -1.6198694\n",
      "40 Train Loss 123.36248 Test MSE 132.54614646339454 Test RE 0.1938084568461955 Lambda1 -1.6591425\n",
      "41 Train Loss 121.7091 Test MSE 129.5813464503614 Test RE 0.19162863884389394 Lambda1 -1.6903651\n",
      "42 Train Loss 120.9375 Test MSE 129.81823216527113 Test RE 0.19180371558615436 Lambda1 -1.7105744\n",
      "43 Train Loss 119.448746 Test MSE 130.15820183769964 Test RE 0.19205470040374845 Lambda1 -1.7004564\n",
      "44 Train Loss 118.39117 Test MSE 129.0964083809626 Test RE 0.19126973252770985 Lambda1 -1.7148924\n",
      "45 Train Loss 116.04192 Test MSE 126.50222138813392 Test RE 0.18933820108619304 Lambda1 -1.778131\n",
      "46 Train Loss 114.423676 Test MSE 123.74559851722806 Test RE 0.1872638944149001 Lambda1 -1.8770158\n",
      "47 Train Loss 111.60095 Test MSE 118.5073382745939 Test RE 0.18325751472970442 Lambda1 -2.0449786\n",
      "48 Train Loss 110.516235 Test MSE 117.7698942725625 Test RE 0.18268644017267452 Lambda1 -2.0672326\n",
      "49 Train Loss 108.835396 Test MSE 117.65047103001548 Test RE 0.1825937911097772 Lambda1 -2.0494099\n",
      "50 Train Loss 107.97915 Test MSE 116.34215824727791 Test RE 0.18157570065457362 Lambda1 -2.1285143\n",
      "51 Train Loss 107.17538 Test MSE 114.48745387890905 Test RE 0.18012256348516162 Lambda1 -2.193124\n",
      "52 Train Loss 106.606094 Test MSE 113.86588410295951 Test RE 0.17963294164540436 Lambda1 -2.1988397\n",
      "53 Train Loss 104.51012 Test MSE 111.76500973295704 Test RE 0.17796807412674323 Lambda1 -2.2546372\n",
      "54 Train Loss 103.37092 Test MSE 111.27878222491083 Test RE 0.17758053202877147 Lambda1 -2.253354\n",
      "55 Train Loss 102.54871 Test MSE 111.24071033225765 Test RE 0.1775501515501747 Lambda1 -2.2845724\n",
      "56 Train Loss 101.49329 Test MSE 112.11106802731962 Test RE 0.17824338269040527 Lambda1 -2.3044353\n",
      "57 Train Loss 100.57619 Test MSE 110.83543160216117 Test RE 0.1772264258243714 Lambda1 -2.3144572\n",
      "58 Train Loss 99.42093 Test MSE 109.17389680003745 Test RE 0.17589300828389187 Lambda1 -2.3872302\n",
      "59 Train Loss 99.13895 Test MSE 108.18744257552525 Test RE 0.1750965535880563 Lambda1 -2.4051392\n",
      "60 Train Loss 98.70033 Test MSE 107.96753739826651 Test RE 0.1749185097038446 Lambda1 -2.3762488\n",
      "61 Train Loss 98.4305 Test MSE 107.45493946685693 Test RE 0.17450278501020286 Lambda1 -2.408219\n",
      "62 Train Loss 98.01378 Test MSE 106.8667043904972 Test RE 0.17402449370742212 Lambda1 -2.4256027\n",
      "63 Train Loss 97.1514 Test MSE 106.31714661839958 Test RE 0.17357645995127954 Lambda1 -2.483315\n",
      "64 Train Loss 96.563385 Test MSE 106.51832525787836 Test RE 0.17374060737948913 Lambda1 -2.5368671\n",
      "65 Train Loss 96.16354 Test MSE 105.53685033215324 Test RE 0.1729383197306355 Lambda1 -2.5480518\n",
      "66 Train Loss 95.53936 Test MSE 104.24692368295602 Test RE 0.17187819908253224 Lambda1 -2.542427\n",
      "67 Train Loss 94.334236 Test MSE 102.51030257375334 Test RE 0.1704405505352463 Lambda1 -2.594766\n",
      "68 Train Loss 93.165924 Test MSE 100.86942748303798 Test RE 0.16907093272274223 Lambda1 -2.6495087\n",
      "69 Train Loss 92.5907 Test MSE 99.60598313548974 Test RE 0.1680087435016695 Lambda1 -2.7279072\n",
      "70 Train Loss 92.290146 Test MSE 99.21624866071419 Test RE 0.16767973226312516 Lambda1 -2.741528\n",
      "71 Train Loss 91.95433 Test MSE 98.5160643698645 Test RE 0.16708701388554764 Lambda1 -2.7808223\n",
      "72 Train Loss 91.428474 Test MSE 98.23888760773579 Test RE 0.16685179712638185 Lambda1 -2.7971468\n",
      "73 Train Loss 90.659836 Test MSE 97.88828490433751 Test RE 0.1665537940667098 Lambda1 -2.8619184\n",
      "74 Train Loss 89.89472 Test MSE 96.7361373190404 Test RE 0.16557072162653172 Lambda1 -2.864761\n",
      "Training time: 68.09\n",
      "Training time: 68.09\n",
      "inv_HT_stan_tune12\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 838.0287 Test MSE 858.2500165718287 Test RE 0.4931693535145655 Lambda1 -0.011042447\n",
      "1 Train Loss 837.862 Test MSE 857.7824779883742 Test RE 0.4930350062340297 Lambda1 -0.012338254\n",
      "2 Train Loss 837.84644 Test MSE 857.8993537399225 Test RE 0.4930685939300046 Lambda1 -0.020024143\n",
      "3 Train Loss 836.1815 Test MSE 853.7946984914364 Test RE 0.49188762600978153 Lambda1 -0.18517359\n",
      "4 Train Loss 820.85236 Test MSE 815.2346233312708 Test RE 0.48065169553218995 Lambda1 -0.46833566\n",
      "5 Train Loss 756.6365 Test MSE 749.4050983745253 Test RE 0.4608371608306103 Lambda1 -0.5417033\n",
      "6 Train Loss 701.2269 Test MSE 690.9008305376034 Test RE 0.44248344631462383 Lambda1 -0.60340565\n",
      "7 Train Loss 660.05426 Test MSE 654.1586712446087 Test RE 0.430557066955854 Lambda1 -0.67244935\n",
      "8 Train Loss 640.93024 Test MSE 642.9355598675489 Test RE 0.4268476492033503 Lambda1 -0.75627583\n",
      "9 Train Loss 631.8955 Test MSE 632.7715420987149 Test RE 0.42346024089871126 Lambda1 -0.880617\n",
      "10 Train Loss 619.7123 Test MSE 620.7073266891593 Test RE 0.41940403701780493 Lambda1 -1.0176218\n",
      "11 Train Loss 610.1322 Test MSE 611.396882345667 Test RE 0.4162466780519894 Lambda1 -0.9891887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 Train Loss 592.6366 Test MSE 589.016387896209 Test RE 0.40855719173466776 Lambda1 -0.84717065\n",
      "13 Train Loss 555.6677 Test MSE 554.026864827445 Test RE 0.39623659481835066 Lambda1 -0.7883082\n",
      "14 Train Loss 528.0373 Test MSE 524.6976293295165 Test RE 0.3856059456201084 Lambda1 -0.85203063\n",
      "15 Train Loss 483.3325 Test MSE 471.0786511859685 Test RE 0.36537252127599323 Lambda1 -0.84764713\n",
      "16 Train Loss 447.68884 Test MSE 428.92649205229645 Test RE 0.3486427238708305 Lambda1 -0.7293239\n",
      "17 Train Loss 389.7827 Test MSE 397.782543310114 Test RE 0.33574691489881375 Lambda1 -0.6310126\n",
      "18 Train Loss 339.4873 Test MSE 349.230781816687 Test RE 0.314590373283511 Lambda1 -0.62211215\n",
      "19 Train Loss 307.1445 Test MSE 322.77599448929556 Test RE 0.30244038662625733 Lambda1 -0.7294802\n",
      "20 Train Loss 284.8108 Test MSE 300.58938660285077 Test RE 0.2918609499790499 Lambda1 -0.83474797\n",
      "21 Train Loss 276.34085 Test MSE 296.9868782249511 Test RE 0.29010672829290823 Lambda1 -0.84218943\n",
      "22 Train Loss 260.47784 Test MSE 261.96959000389654 Test RE 0.272467439708667 Lambda1 -0.9446272\n",
      "23 Train Loss 238.9361 Test MSE 243.33366103412396 Test RE 0.2625973056626366 Lambda1 -0.92704237\n",
      "24 Train Loss 224.91374 Test MSE 229.2630304932021 Test RE 0.2548919882872715 Lambda1 -0.9090914\n",
      "25 Train Loss 215.6087 Test MSE 224.65897316521938 Test RE 0.2523196394516866 Lambda1 -0.90451974\n",
      "26 Train Loss 208.40848 Test MSE 219.19910324042453 Test RE 0.2492347285839805 Lambda1 -0.8959967\n",
      "27 Train Loss 205.67558 Test MSE 217.3861082306214 Test RE 0.2482018786945928 Lambda1 -0.89498574\n",
      "28 Train Loss 200.38493 Test MSE 208.30455706615365 Test RE 0.24296211314726904 Lambda1 -0.9375616\n",
      "29 Train Loss 193.91176 Test MSE 201.78442756421543 Test RE 0.23912941102735205 Lambda1 -0.9915116\n",
      "30 Train Loss 190.33652 Test MSE 203.8141768626763 Test RE 0.2403291028712086 Lambda1 -1.0139532\n",
      "31 Train Loss 187.78851 Test MSE 202.20409210715812 Test RE 0.2393779485683416 Lambda1 -1.0139807\n",
      "32 Train Loss 181.50749 Test MSE 196.1114084528374 Test RE 0.23574397385541382 Lambda1 -1.0883005\n",
      "33 Train Loss 177.2893 Test MSE 188.85603651613872 Test RE 0.23134206408211025 Lambda1 -1.1261146\n",
      "34 Train Loss 172.1715 Test MSE 181.16386445027493 Test RE 0.22658176667865826 Lambda1 -1.1893497\n",
      "35 Train Loss 167.67969 Test MSE 181.293531628594 Test RE 0.22666283958777675 Lambda1 -1.2003078\n",
      "36 Train Loss 166.02005 Test MSE 176.63155266724604 Test RE 0.2237295318724783 Lambda1 -1.2444639\n",
      "37 Train Loss 164.3771 Test MSE 175.79512206230794 Test RE 0.22319917279350965 Lambda1 -1.2698445\n",
      "38 Train Loss 162.458 Test MSE 172.72736724372996 Test RE 0.2212431062623097 Lambda1 -1.3494843\n",
      "39 Train Loss 161.82727 Test MSE 172.18196916278066 Test RE 0.22089353522860172 Lambda1 -1.3695753\n",
      "40 Train Loss 160.83112 Test MSE 171.4807033306222 Test RE 0.22044324668611476 Lambda1 -1.3649881\n",
      "41 Train Loss 159.56175 Test MSE 168.4474200132066 Test RE 0.21848486236375672 Lambda1 -1.4485912\n",
      "42 Train Loss 158.5264 Test MSE 168.67614202394788 Test RE 0.2186331440802079 Lambda1 -1.4540225\n",
      "43 Train Loss 158.08722 Test MSE 168.07048707698652 Test RE 0.21824027487455533 Lambda1 -1.4569956\n",
      "44 Train Loss 156.2005 Test MSE 167.59365628690037 Test RE 0.21793047153294293 Lambda1 -1.4874485\n",
      "45 Train Loss 154.30528 Test MSE 163.71461829360487 Test RE 0.21539365238588096 Lambda1 -1.5260633\n",
      "46 Train Loss 153.50351 Test MSE 162.01609423311697 Test RE 0.2142733945314709 Lambda1 -1.5692663\n",
      "47 Train Loss 151.94685 Test MSE 157.88657585280342 Test RE 0.21152503408060253 Lambda1 -1.6573457\n",
      "48 Train Loss 151.3079 Test MSE 157.6624658047136 Test RE 0.21137485753989424 Lambda1 -1.6720847\n",
      "49 Train Loss 148.54414 Test MSE 158.46340872853094 Test RE 0.2119110813179328 Lambda1 -1.6418046\n",
      "50 Train Loss 147.31683 Test MSE 156.49211770738805 Test RE 0.2105888652617134 Lambda1 -1.6557845\n",
      "51 Train Loss 146.47906 Test MSE 156.06336510861965 Test RE 0.210300184741299 Lambda1 -1.6680796\n",
      "52 Train Loss 145.36337 Test MSE 155.69378474526155 Test RE 0.21005102668973047 Lambda1 -1.7057714\n",
      "53 Train Loss 144.67616 Test MSE 153.33730671943414 Test RE 0.208455369343956 Lambda1 -1.735782\n",
      "54 Train Loss 143.2704 Test MSE 152.89878067000754 Test RE 0.20815707741886555 Lambda1 -1.7590884\n",
      "55 Train Loss 142.81007 Test MSE 152.39523003169649 Test RE 0.2078140267036229 Lambda1 -1.7714169\n",
      "56 Train Loss 140.86157 Test MSE 147.39988755628428 Test RE 0.20437969501571834 Lambda1 -1.8775382\n",
      "57 Train Loss 139.52052 Test MSE 145.80785507055066 Test RE 0.2032729693504843 Lambda1 -1.9297304\n",
      "58 Train Loss 138.41566 Test MSE 144.0401131166081 Test RE 0.2020369936540639 Lambda1 -1.9582831\n",
      "59 Train Loss 137.36226 Test MSE 142.24287937646224 Test RE 0.20077259768857153 Lambda1 -2.0074816\n",
      "60 Train Loss 135.46945 Test MSE 139.6721896980527 Test RE 0.19895009072913175 Lambda1 -2.0836704\n",
      "61 Train Loss 133.71434 Test MSE 138.51682084442683 Test RE 0.19812552405180542 Lambda1 -2.1043062\n",
      "62 Train Loss 132.48364 Test MSE 139.07311159595716 Test RE 0.1985229666149467 Lambda1 -2.103124\n",
      "63 Train Loss 131.6967 Test MSE 139.28902378146154 Test RE 0.19867701114414077 Lambda1 -2.0810335\n",
      "64 Train Loss 130.92488 Test MSE 138.72695828772638 Test RE 0.19827575061884384 Lambda1 -2.0841477\n",
      "65 Train Loss 130.24355 Test MSE 137.2766983297327 Test RE 0.19723663443245004 Lambda1 -2.0943344\n",
      "66 Train Loss 128.87474 Test MSE 135.28139702104482 Test RE 0.19579798152829925 Lambda1 -2.1408029\n",
      "67 Train Loss 128.07726 Test MSE 133.24849429039264 Test RE 0.19432126353935708 Lambda1 -2.1793134\n",
      "68 Train Loss 127.632675 Test MSE 132.24902037613765 Test RE 0.19359110666922852 Lambda1 -2.214086\n",
      "69 Train Loss 126.57538 Test MSE 129.45100767655228 Test RE 0.19153224022512128 Lambda1 -2.2562644\n",
      "70 Train Loss 125.7616 Test MSE 128.7057625090902 Test RE 0.19098012206221157 Lambda1 -2.2846024\n",
      "71 Train Loss 125.221054 Test MSE 129.37713291447375 Test RE 0.1914775808671423 Lambda1 -2.2761137\n",
      "72 Train Loss 124.581055 Test MSE 128.24834211211834 Test RE 0.19064044822958623 Lambda1 -2.2898307\n",
      "73 Train Loss 123.69299 Test MSE 128.14605378699213 Test RE 0.19056440755084494 Lambda1 -2.2966282\n",
      "74 Train Loss 122.370804 Test MSE 127.40131378725735 Test RE 0.1900098538494604 Lambda1 -2.3050263\n",
      "Training time: 67.83\n",
      "Training time: 67.83\n",
      "inv_HT_stan_tune13\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.1335 Test MSE 858.3646973119542 Test RE 0.493202301452924 Lambda1 -0.02223753\n",
      "1 Train Loss 837.9076 Test MSE 857.9740598180471 Test RE 0.4930900617274031 Lambda1 -0.024141742\n",
      "2 Train Loss 837.76105 Test MSE 857.7721906499837 Test RE 0.49303204975421994 Lambda1 -0.08356533\n",
      "3 Train Loss 835.9299 Test MSE 853.2423425404517 Test RE 0.4917284887984439 Lambda1 -0.22384328\n",
      "4 Train Loss 815.20734 Test MSE 819.7137219979098 Test RE 0.4819702958967953 Lambda1 0.29290524\n",
      "5 Train Loss 765.29425 Test MSE 758.2260932525699 Test RE 0.4635414058848011 Lambda1 0.41924733\n",
      "6 Train Loss 724.5271 Test MSE 694.1072734407996 Test RE 0.443509031685499 Lambda1 0.18065725\n",
      "7 Train Loss 671.1747 Test MSE 675.0255224241614 Test RE 0.4373702787278016 Lambda1 0.10628143\n",
      "8 Train Loss 657.02704 Test MSE 661.5399132129137 Test RE 0.43297936244314944 Lambda1 0.10478426\n",
      "9 Train Loss 631.39264 Test MSE 638.5528869775154 Test RE 0.42539032385979336 Lambda1 0.13544713\n",
      "10 Train Loss 623.08154 Test MSE 631.1529927803149 Test RE 0.42291831534891117 Lambda1 0.16635588\n",
      "11 Train Loss 616.8365 Test MSE 623.5986011503279 Test RE 0.4203797008385707 Lambda1 0.1849941\n",
      "12 Train Loss 610.1836 Test MSE 613.6365994692007 Test RE 0.41700839487131 Lambda1 0.17578836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Train Loss 574.7507 Test MSE 574.3885185468812 Test RE 0.403452160746561 Lambda1 0.25662163\n",
      "14 Train Loss 525.2134 Test MSE 519.4436659276902 Test RE 0.38367049117190605 Lambda1 0.18949777\n",
      "15 Train Loss 431.11343 Test MSE 417.1697607956549 Test RE 0.34383143580401987 Lambda1 0.096099265\n",
      "16 Train Loss 370.7097 Test MSE 353.60428851304545 Test RE 0.31655409235845133 Lambda1 0.056165874\n",
      "17 Train Loss 290.10522 Test MSE 298.5796932873364 Test RE 0.29088364553238794 Lambda1 0.028165136\n",
      "18 Train Loss 268.07352 Test MSE 282.74406292068437 Test RE 0.28306483322171944 Lambda1 0.020146105\n",
      "19 Train Loss 262.53717 Test MSE 280.8570058834493 Test RE 0.28211865291529536 Lambda1 0.01887961\n",
      "20 Train Loss 255.40656 Test MSE 275.06230803722514 Test RE 0.2791931200219737 Lambda1 0.021617135\n",
      "21 Train Loss 252.3093 Test MSE 272.6494048374444 Test RE 0.2779658528668784 Lambda1 0.026198031\n",
      "22 Train Loss 248.92834 Test MSE 268.2799351738352 Test RE 0.2757295214013511 Lambda1 0.03500046\n",
      "23 Train Loss 246.39796 Test MSE 265.9930328247824 Test RE 0.2745518037413085 Lambda1 0.044719223\n",
      "24 Train Loss 241.84938 Test MSE 260.58101653890384 Test RE 0.27174437154551473 Lambda1 0.06252278\n",
      "25 Train Loss 234.82805 Test MSE 256.5441783311084 Test RE 0.2696312670403386 Lambda1 0.09085243\n",
      "26 Train Loss 226.46461 Test MSE 248.8907483110144 Test RE 0.2655788875180968 Lambda1 0.12960774\n",
      "27 Train Loss 221.89622 Test MSE 245.36682447971734 Test RE 0.26369208355818347 Lambda1 0.15329337\n",
      "28 Train Loss 219.70636 Test MSE 244.33741916737645 Test RE 0.2631383588740667 Lambda1 0.16073956\n",
      "29 Train Loss 216.9198 Test MSE 242.97706112048658 Test RE 0.26240481994384823 Lambda1 0.18308297\n",
      "30 Train Loss 215.14842 Test MSE 241.67000792536214 Test RE 0.2616980874893359 Lambda1 0.19500436\n",
      "31 Train Loss 213.6054 Test MSE 241.12233008513226 Test RE 0.2614013863409693 Lambda1 0.21863641\n",
      "32 Train Loss 211.21892 Test MSE 239.781734954987 Test RE 0.26067370203587453 Lambda1 0.2566682\n",
      "33 Train Loss 210.86572 Test MSE 239.79759019820196 Test RE 0.2606823202415639 Lambda1 0.2591758\n",
      "34 Train Loss 209.76189 Test MSE 238.95143970361576 Test RE 0.260221991595681 Lambda1 0.2841654\n",
      "35 Train Loss 208.67831 Test MSE 236.57118667158392 Test RE 0.2589226807404171 Lambda1 0.29638687\n",
      "36 Train Loss 207.53453 Test MSE 235.2801918936056 Test RE 0.25821522960060866 Lambda1 0.2989921\n",
      "37 Train Loss 202.83987 Test MSE 226.601238966126 Test RE 0.2534079938084668 Lambda1 0.33794805\n",
      "38 Train Loss 195.15671 Test MSE 210.11607121720854 Test RE 0.24401628247967883 Lambda1 0.32527104\n",
      "39 Train Loss 186.43877 Test MSE 196.46398063715347 Test RE 0.23595579081405174 Lambda1 0.31391147\n",
      "40 Train Loss 179.45496 Test MSE 184.87513072217877 Test RE 0.22889084248630068 Lambda1 0.31599164\n",
      "41 Train Loss 167.06131 Test MSE 169.70791501690675 Test RE 0.21930080181836617 Lambda1 0.37346166\n",
      "42 Train Loss 162.12683 Test MSE 165.99589238918418 Test RE 0.21688915730088107 Lambda1 0.4031271\n",
      "43 Train Loss 157.97368 Test MSE 159.30926382063524 Test RE 0.21247590416073062 Lambda1 0.433409\n",
      "44 Train Loss 148.82474 Test MSE 149.4712354364907 Test RE 0.20581071560451247 Lambda1 0.4669183\n",
      "45 Train Loss 141.01508 Test MSE 139.71759133780927 Test RE 0.19898242331606883 Lambda1 0.5155631\n",
      "46 Train Loss 136.95148 Test MSE 135.23641720282865 Test RE 0.19576542830963806 Lambda1 0.5466821\n",
      "47 Train Loss 134.3 Test MSE 132.15625859732535 Test RE 0.19352320066334952 Lambda1 0.5514334\n",
      "48 Train Loss 129.6259 Test MSE 131.24985092579942 Test RE 0.19285840915060604 Lambda1 0.5551757\n",
      "49 Train Loss 126.00362 Test MSE 126.76775016152189 Test RE 0.18953680782075974 Lambda1 0.56838894\n",
      "50 Train Loss 123.09701 Test MSE 123.02063824705989 Test RE 0.18671454839921403 Lambda1 0.55967253\n",
      "51 Train Loss 120.42761 Test MSE 118.82292633709459 Test RE 0.18350136221104743 Lambda1 0.539886\n",
      "52 Train Loss 116.66909 Test MSE 116.83189367021238 Test RE 0.18195746540006003 Lambda1 0.54068464\n",
      "53 Train Loss 114.95085 Test MSE 114.3801785268079 Test RE 0.18003815580878724 Lambda1 0.5431572\n",
      "54 Train Loss 113.31133 Test MSE 112.58999287740528 Test RE 0.17862369399071257 Lambda1 0.5426926\n",
      "55 Train Loss 111.727615 Test MSE 112.44043457376296 Test RE 0.17850501766147173 Lambda1 0.54442185\n",
      "56 Train Loss 110.6112 Test MSE 110.02380310981073 Test RE 0.1765763344420201 Lambda1 0.5453694\n",
      "57 Train Loss 109.57545 Test MSE 109.04250696413347 Test RE 0.17578713356613648 Lambda1 0.5442691\n",
      "58 Train Loss 108.11921 Test MSE 108.48139263350706 Test RE 0.17533426476297595 Lambda1 0.5473295\n",
      "59 Train Loss 105.9121 Test MSE 103.51167992484186 Test RE 0.1712710061748981 Lambda1 0.5500311\n",
      "60 Train Loss 104.21168 Test MSE 101.90643511874764 Test RE 0.16993779362996178 Lambda1 0.55791223\n",
      "61 Train Loss 101.73993 Test MSE 99.88153126955669 Test RE 0.1682409711323517 Lambda1 0.5616844\n",
      "62 Train Loss 99.2983 Test MSE 95.89377488163608 Test RE 0.16484826406631356 Lambda1 0.5815229\n",
      "63 Train Loss 97.65511 Test MSE 94.68988383356016 Test RE 0.1638102082787687 Lambda1 0.6011719\n",
      "64 Train Loss 94.650894 Test MSE 95.13005378563459 Test RE 0.16419050619337688 Lambda1 0.61969644\n",
      "65 Train Loss 91.52744 Test MSE 90.23220657931283 Test RE 0.15990791461066792 Lambda1 0.63320446\n",
      "66 Train Loss 88.80713 Test MSE 88.07773570006505 Test RE 0.15798732335446997 Lambda1 0.62964714\n",
      "67 Train Loss 87.488815 Test MSE 86.69859821356104 Test RE 0.156745545805529 Lambda1 0.6473025\n",
      "68 Train Loss 84.86835 Test MSE 82.88936742090713 Test RE 0.15326344501060815 Lambda1 0.6838634\n",
      "69 Train Loss 83.00879 Test MSE 79.90595335779628 Test RE 0.1504799850345238 Lambda1 0.70836115\n",
      "70 Train Loss 81.70208 Test MSE 79.50144070366214 Test RE 0.15009860987417453 Lambda1 0.72005904\n",
      "71 Train Loss 79.04654 Test MSE 75.39421547899136 Test RE 0.1461699787658166 Lambda1 0.74771553\n",
      "72 Train Loss 76.54965 Test MSE 72.67525296657296 Test RE 0.14351009327013317 Lambda1 0.7584127\n",
      "73 Train Loss 75.86989 Test MSE 71.60525503886545 Test RE 0.14244972612296294 Lambda1 0.7672346\n",
      "74 Train Loss 73.64837 Test MSE 70.49308801986663 Test RE 0.14133913805411652 Lambda1 0.76844156\n",
      "Training time: 68.01\n",
      "Training time: 68.01\n",
      "inv_HT_stan_tune13\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 837.8372 Test MSE 858.0300212197974 Test RE 0.4931061423773873 Lambda1 -0.044740073\n",
      "1 Train Loss 834.2008 Test MSE 852.1303353747916 Test RE 0.49140795625383177 Lambda1 -0.0031981696\n",
      "2 Train Loss 744.5663 Test MSE 753.568683190503 Test RE 0.46211555947996913 Lambda1 0.019133164\n",
      "3 Train Loss 669.27795 Test MSE 671.4173109471418 Test RE 0.4361997755780202 Lambda1 0.0065573435\n",
      "4 Train Loss 646.50055 Test MSE 650.3339354865855 Test RE 0.42929653092316383 Lambda1 0.004306023\n",
      "5 Train Loss 629.83185 Test MSE 630.5712462101723 Test RE 0.42272336420285567 Lambda1 0.0025860267\n",
      "6 Train Loss 538.35767 Test MSE 560.9605083124422 Test RE 0.3987083349267685 Lambda1 0.0016613666\n",
      "7 Train Loss 396.3153 Test MSE 388.9333174169902 Test RE 0.3319913318496604 Lambda1 0.0026655449\n",
      "8 Train Loss 302.0063 Test MSE 307.52454652371927 Test RE 0.2952086399923864 Lambda1 0.003074028\n",
      "9 Train Loss 266.84622 Test MSE 284.76767764816213 Test RE 0.2840759824837339 Lambda1 0.0008498106\n",
      "10 Train Loss 263.0664 Test MSE 282.83371953888144 Test RE 0.2831097088309132 Lambda1 0.0008943374\n",
      "11 Train Loss 257.611 Test MSE 281.4755354782944 Test RE 0.28242913612154513 Lambda1 0.0022633148\n",
      "12 Train Loss 256.1678 Test MSE 282.39741755516866 Test RE 0.2828912606896453 Lambda1 0.0017954482\n",
      "13 Train Loss 254.92616 Test MSE 282.2193885368217 Test RE 0.28280207645804406 Lambda1 0.0019260133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 Train Loss 254.67738 Test MSE 281.82909641740497 Test RE 0.28260645983264226 Lambda1 0.0021208627\n",
      "15 Train Loss 254.57147 Test MSE 281.55992509651276 Test RE 0.2824714707080388 Lambda1 0.00225841\n",
      "16 Train Loss 254.3118 Test MSE 281.11830214106203 Test RE 0.282249857423332 Lambda1 0.0028012262\n",
      "17 Train Loss 254.15753 Test MSE 280.7099735519185 Test RE 0.28204479683771166 Lambda1 0.0032601363\n",
      "18 Train Loss 253.9394 Test MSE 280.38961190927387 Test RE 0.2818838083842289 Lambda1 0.003529216\n",
      "19 Train Loss 253.53802 Test MSE 278.7352571043262 Test RE 0.28105099269050515 Lambda1 0.005699254\n",
      "20 Train Loss 253.17073 Test MSE 278.5068539965711 Test RE 0.2809358187498746 Lambda1 0.007265118\n",
      "21 Train Loss 252.3106 Test MSE 275.9160430174139 Test RE 0.2796260624135325 Lambda1 0.011424605\n",
      "22 Train Loss 249.94533 Test MSE 270.69789395018483 Test RE 0.276969284505102 Lambda1 0.022492394\n",
      "23 Train Loss 246.95465 Test MSE 267.50486147959 Test RE 0.2753309353551508 Lambda1 0.032193977\n",
      "24 Train Loss 243.81317 Test MSE 261.89267735771506 Test RE 0.27242743939763137 Lambda1 0.053428926\n",
      "25 Train Loss 237.39117 Test MSE 256.48418734829977 Test RE 0.26959973954564165 Lambda1 0.06437121\n",
      "26 Train Loss 231.8182 Test MSE 250.79426669924416 Test RE 0.266592527831006 Lambda1 0.08812568\n",
      "27 Train Loss 227.19394 Test MSE 246.05791645993386 Test RE 0.26406317558617515 Lambda1 0.12240362\n",
      "28 Train Loss 216.37108 Test MSE 235.8396500007588 Test RE 0.2585220442721068 Lambda1 0.15486209\n",
      "29 Train Loss 204.20656 Test MSE 222.61894701118828 Test RE 0.2511714268538114 Lambda1 0.18491034\n",
      "30 Train Loss 194.39511 Test MSE 202.08495848950736 Test RE 0.239307420414194 Lambda1 0.21745569\n",
      "31 Train Loss 183.00822 Test MSE 183.8498640386258 Test RE 0.22825527722944883 Lambda1 0.25634366\n",
      "32 Train Loss 175.58438 Test MSE 180.60039281397735 Test RE 0.2262291250843897 Lambda1 0.28280345\n",
      "33 Train Loss 167.949 Test MSE 172.23068900232073 Test RE 0.22092478454137 Lambda1 0.318752\n",
      "34 Train Loss 160.29381 Test MSE 161.52881213009792 Test RE 0.2139509259009232 Lambda1 0.3495567\n",
      "35 Train Loss 148.0492 Test MSE 145.97601321410693 Test RE 0.20339015149892692 Lambda1 0.43393582\n",
      "36 Train Loss 142.4394 Test MSE 141.05677675373764 Test RE 0.19993376689144043 Lambda1 0.46299225\n",
      "37 Train Loss 134.81126 Test MSE 135.47817911684163 Test RE 0.1959403349368046 Lambda1 0.49285907\n",
      "38 Train Loss 126.509926 Test MSE 125.33824342429303 Test RE 0.18846511446207065 Lambda1 0.53154856\n",
      "39 Train Loss 123.09687 Test MSE 122.28424776377729 Test RE 0.18615488136067934 Lambda1 0.55113035\n",
      "40 Train Loss 116.49154 Test MSE 116.2247064980282 Test RE 0.1814840237889039 Lambda1 0.5829876\n",
      "41 Train Loss 110.37506 Test MSE 109.81748548071906 Test RE 0.17641069798643572 Lambda1 0.6108653\n",
      "42 Train Loss 106.3961 Test MSE 105.55557055972375 Test RE 0.17295365703290622 Lambda1 0.6404042\n",
      "43 Train Loss 103.75116 Test MSE 105.22746307889614 Test RE 0.17268464443945117 Lambda1 0.6471372\n",
      "44 Train Loss 100.39966 Test MSE 103.29446701512299 Test RE 0.17109121095369653 Lambda1 0.6655478\n",
      "45 Train Loss 95.42147 Test MSE 98.29911756469481 Test RE 0.1669029374484601 Lambda1 0.7264759\n",
      "46 Train Loss 92.71826 Test MSE 97.46139326719316 Test RE 0.16619022600151487 Lambda1 0.7533826\n",
      "47 Train Loss 89.934166 Test MSE 92.04361819894216 Test RE 0.16150501474994977 Lambda1 0.7955885\n",
      "48 Train Loss 87.68392 Test MSE 87.3223438828243 Test RE 0.1573083815306412 Lambda1 0.8426033\n",
      "49 Train Loss 85.96469 Test MSE 85.63980282821245 Test RE 0.15578548833341993 Lambda1 0.86514986\n",
      "50 Train Loss 84.472374 Test MSE 83.26765394560155 Test RE 0.1536127750849104 Lambda1 0.88055694\n",
      "51 Train Loss 82.75051 Test MSE 81.21874976906766 Test RE 0.1517110871922082 Lambda1 0.89355224\n",
      "52 Train Loss 79.881676 Test MSE 75.68119482672081 Test RE 0.1464479040904615 Lambda1 0.96669257\n",
      "53 Train Loss 78.66291 Test MSE 75.17407413262634 Test RE 0.1459564240586468 Lambda1 0.9751668\n",
      "54 Train Loss 76.6435 Test MSE 72.05304260518392 Test RE 0.14289444070779306 Lambda1 1.0181606\n",
      "55 Train Loss 74.96788 Test MSE 71.95105159782823 Test RE 0.14279327142727694 Lambda1 1.0301293\n",
      "56 Train Loss 73.77053 Test MSE 71.68311145583445 Test RE 0.14252714790014856 Lambda1 1.0435724\n",
      "57 Train Loss 72.128685 Test MSE 71.29539133339064 Test RE 0.14214117435338244 Lambda1 1.0908529\n",
      "58 Train Loss 68.91111 Test MSE 66.16518492487143 Test RE 0.13693167982737675 Lambda1 1.1509843\n",
      "59 Train Loss 67.32565 Test MSE 64.7394982131809 Test RE 0.13544838622995028 Lambda1 1.1664611\n",
      "60 Train Loss 66.7224 Test MSE 63.982517152539806 Test RE 0.13465417751447303 Lambda1 1.1838149\n",
      "61 Train Loss 65.69496 Test MSE 62.91525541764018 Test RE 0.13352640384152492 Lambda1 1.2161696\n",
      "62 Train Loss 64.50586 Test MSE 61.47673885161278 Test RE 0.13199107967630727 Lambda1 1.237018\n",
      "63 Train Loss 63.187153 Test MSE 60.09525258179943 Test RE 0.1304996218042421 Lambda1 1.2641906\n",
      "64 Train Loss 62.207203 Test MSE 58.648629493007626 Test RE 0.12891934923634005 Lambda1 1.2866471\n",
      "65 Train Loss 60.309944 Test MSE 57.08861460804151 Test RE 0.12719320846701962 Lambda1 1.3310478\n",
      "66 Train Loss 58.435837 Test MSE 55.22628082150088 Test RE 0.12510137096911414 Lambda1 1.4140074\n",
      "67 Train Loss 57.192123 Test MSE 53.83954489474294 Test RE 0.12352073314295421 Lambda1 1.4524449\n",
      "68 Train Loss 56.04562 Test MSE 53.28637412138802 Test RE 0.12288454207673893 Lambda1 1.4499421\n",
      "69 Train Loss 54.635437 Test MSE 52.13120029811622 Test RE 0.1215452616515681 Lambda1 1.4937454\n",
      "70 Train Loss 53.560417 Test MSE 51.83489475078466 Test RE 0.12119934734049398 Lambda1 1.5087457\n",
      "71 Train Loss 52.646492 Test MSE 51.98590757512109 Test RE 0.12137576657257879 Lambda1 1.5055825\n",
      "72 Train Loss 51.749317 Test MSE 50.78514358760189 Test RE 0.11996581615951592 Lambda1 1.531197\n",
      "73 Train Loss 50.824413 Test MSE 50.305262209115035 Test RE 0.11939767752399112 Lambda1 1.5704669\n",
      "74 Train Loss 50.053482 Test MSE 49.94457351006655 Test RE 0.118968866861044 Lambda1 1.5773845\n",
      "Training time: 68.25\n",
      "Training time: 68.25\n",
      "inv_HT_stan_tune13\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 837.9603 Test MSE 858.1372227196624 Test RE 0.4931369455344608 Lambda1 -0.12975955\n",
      "1 Train Loss 837.811 Test MSE 857.7590446736535 Test RE 0.4930282717041969 Lambda1 -0.1318885\n",
      "2 Train Loss 837.2541 Test MSE 856.1596902619839 Test RE 0.49256841362637893 Lambda1 -0.17806509\n",
      "3 Train Loss 821.73816 Test MSE 834.7533529568569 Test RE 0.4863716547003819 Lambda1 -0.101147205\n",
      "4 Train Loss 788.72406 Test MSE 784.3539106384948 Test RE 0.47146038197975076 Lambda1 0.016308444\n",
      "5 Train Loss 682.9506 Test MSE 681.5853459833514 Test RE 0.43949029870970835 Lambda1 -0.001703852\n",
      "6 Train Loss 644.8041 Test MSE 649.670638422764 Test RE 0.42907754821539673 Lambda1 0.00084799447\n",
      "7 Train Loss 639.2271 Test MSE 647.3073051113886 Test RE 0.4282964007386992 Lambda1 0.0022603464\n",
      "8 Train Loss 628.72784 Test MSE 639.6765367718132 Test RE 0.4257644351252854 Lambda1 0.0008649094\n",
      "9 Train Loss 596.1333 Test MSE 603.3675969359762 Test RE 0.4135044258235842 Lambda1 0.0024640122\n",
      "10 Train Loss 524.06586 Test MSE 527.9957067351668 Test RE 0.3868159435104099 Lambda1 0.009700373\n",
      "11 Train Loss 434.41776 Test MSE 394.5310480481439 Test RE 0.3343718928583152 Lambda1 0.0064301384\n",
      "12 Train Loss 303.27673 Test MSE 310.6377969322009 Test RE 0.2966991617868587 Lambda1 0.0051193964\n",
      "13 Train Loss 268.31656 Test MSE 282.147210694629 Test RE 0.2827659107129859 Lambda1 0.007642002\n",
      "14 Train Loss 261.1411 Test MSE 281.55070661521984 Test RE 0.2824668465072016 Lambda1 0.002344656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Train Loss 258.30325 Test MSE 281.4416712632894 Test RE 0.28241214613934673 Lambda1 0.0027210643\n",
      "16 Train Loss 256.746 Test MSE 281.22865048217375 Test RE 0.282305248245391 Lambda1 0.0031476251\n",
      "17 Train Loss 255.96028 Test MSE 279.99079511393273 Test RE 0.28168326600247334 Lambda1 0.004775119\n",
      "18 Train Loss 255.35172 Test MSE 280.64470686976483 Test RE 0.2820120064134908 Lambda1 0.0037291045\n",
      "19 Train Loss 255.03526 Test MSE 281.0065723996736 Test RE 0.282193762111491 Lambda1 0.0034172672\n",
      "20 Train Loss 254.84052 Test MSE 281.2046758782361 Test RE 0.2822932147972408 Lambda1 0.0030946825\n",
      "21 Train Loss 254.6336 Test MSE 280.9452713917914 Test RE 0.28216298043721716 Lambda1 0.0033977898\n",
      "22 Train Loss 254.17586 Test MSE 280.0916713269874 Test RE 0.2817340044244708 Lambda1 0.0041998005\n",
      "23 Train Loss 253.87051 Test MSE 279.66197870388 Test RE 0.28151781540312754 Lambda1 0.0052495548\n",
      "24 Train Loss 253.3898 Test MSE 278.90416635179804 Test RE 0.281136136067247 Lambda1 0.0063395873\n",
      "25 Train Loss 253.28293 Test MSE 278.14790411391056 Test RE 0.280754720168521 Lambda1 0.0074272617\n",
      "26 Train Loss 252.47456 Test MSE 275.67442881521623 Test RE 0.2795036041028117 Lambda1 0.012693842\n",
      "27 Train Loss 251.76797 Test MSE 274.7108940396491 Test RE 0.2790147173034631 Lambda1 0.014197763\n",
      "28 Train Loss 251.04605 Test MSE 273.0198892184377 Test RE 0.2781546430220748 Lambda1 0.019860476\n",
      "29 Train Loss 248.3639 Test MSE 269.13579979159493 Test RE 0.27616898630170983 Lambda1 0.03003562\n",
      "30 Train Loss 247.31952 Test MSE 268.30051453423016 Test RE 0.27574009660216775 Lambda1 0.031217467\n",
      "31 Train Loss 244.79906 Test MSE 264.8022774396005 Test RE 0.2739365794683135 Lambda1 0.044929624\n",
      "32 Train Loss 239.51254 Test MSE 260.6320996489565 Test RE 0.27177100600179654 Lambda1 0.05723239\n",
      "33 Train Loss 234.05882 Test MSE 253.2084912945792 Test RE 0.2678726064155802 Lambda1 0.08267791\n",
      "34 Train Loss 231.47523 Test MSE 247.56748237800733 Test RE 0.2648719511363047 Lambda1 0.11044159\n",
      "35 Train Loss 227.78424 Test MSE 245.83903342564008 Test RE 0.2639456995675949 Lambda1 0.117981434\n",
      "36 Train Loss 224.7946 Test MSE 245.4072892404354 Test RE 0.26371382610060295 Lambda1 0.12364993\n",
      "37 Train Loss 223.05577 Test MSE 242.899044003666 Test RE 0.2623626889891869 Lambda1 0.14093688\n",
      "38 Train Loss 219.00217 Test MSE 240.48841280083036 Test RE 0.26105754445734 Lambda1 0.1745907\n",
      "39 Train Loss 215.5101 Test MSE 241.32449467343608 Test RE 0.26151094697625554 Lambda1 0.19797444\n",
      "40 Train Loss 212.91907 Test MSE 239.95358595420922 Test RE 0.26076709741598325 Lambda1 0.2160582\n",
      "41 Train Loss 211.87119 Test MSE 239.24764890259235 Test RE 0.2603832299523865 Lambda1 0.22476192\n",
      "42 Train Loss 210.72484 Test MSE 240.00602362118246 Test RE 0.26079558890775373 Lambda1 0.24367754\n",
      "43 Train Loss 209.75447 Test MSE 239.5530066311379 Test RE 0.2605493437638148 Lambda1 0.24105854\n",
      "44 Train Loss 208.66747 Test MSE 238.34431018850236 Test RE 0.25989119439685693 Lambda1 0.26279852\n",
      "45 Train Loss 207.90016 Test MSE 237.54942193007827 Test RE 0.25945745845303786 Lambda1 0.29372367\n",
      "46 Train Loss 207.16727 Test MSE 235.8608731155681 Test RE 0.25853367615757783 Lambda1 0.324803\n",
      "47 Train Loss 206.62245 Test MSE 235.10153790691533 Test RE 0.25811717642350146 Lambda1 0.3397853\n",
      "48 Train Loss 205.92792 Test MSE 234.14407157342407 Test RE 0.25759104068588956 Lambda1 0.33551943\n",
      "49 Train Loss 205.02534 Test MSE 233.50267217539275 Test RE 0.2572379844671414 Lambda1 0.3321954\n",
      "50 Train Loss 203.07822 Test MSE 228.4640426678983 Test RE 0.2544474479243156 Lambda1 0.3300787\n",
      "51 Train Loss 200.00876 Test MSE 218.76340022151922 Test RE 0.24898690289010034 Lambda1 0.354596\n",
      "52 Train Loss 194.65578 Test MSE 209.05601758397984 Test RE 0.24339996259411908 Lambda1 0.36133364\n",
      "53 Train Loss 183.47453 Test MSE 194.2773950416415 Test RE 0.23463905801107443 Lambda1 0.3604708\n",
      "54 Train Loss 176.00668 Test MSE 185.26512207679957 Test RE 0.22913213622582138 Lambda1 0.38404888\n",
      "55 Train Loss 169.00323 Test MSE 172.97199969717173 Test RE 0.2213997232904321 Lambda1 0.44470927\n",
      "56 Train Loss 160.343 Test MSE 154.685682964012 Test RE 0.20936989257020477 Lambda1 0.5587161\n",
      "57 Train Loss 150.3707 Test MSE 148.90126401666262 Test RE 0.2054179367908853 Lambda1 0.60658926\n",
      "58 Train Loss 146.74345 Test MSE 146.39694899698597 Test RE 0.20368318786982634 Lambda1 0.60453355\n",
      "59 Train Loss 140.28995 Test MSE 143.0653047474366 Test RE 0.20135217851320203 Lambda1 0.6009692\n",
      "60 Train Loss 135.819 Test MSE 137.8754433450635 Test RE 0.19766629934729743 Lambda1 0.6277519\n",
      "61 Train Loss 131.11247 Test MSE 133.03182115057257 Test RE 0.1941632079879901 Lambda1 0.62484556\n",
      "62 Train Loss 129.11336 Test MSE 132.2427212263721 Test RE 0.193586496147759 Lambda1 0.6236021\n",
      "63 Train Loss 123.98723 Test MSE 122.48380002509542 Test RE 0.18630670993025392 Lambda1 0.6551381\n",
      "64 Train Loss 122.27562 Test MSE 120.18114241303857 Test RE 0.18454714657771082 Lambda1 0.66004467\n",
      "65 Train Loss 120.425705 Test MSE 118.9078806886693 Test RE 0.18356694910690952 Lambda1 0.66068393\n",
      "66 Train Loss 118.12962 Test MSE 117.62655228201021 Test RE 0.18257522919141492 Lambda1 0.65820384\n",
      "67 Train Loss 116.84051 Test MSE 116.12946385701169 Test RE 0.18140964820716143 Lambda1 0.67131525\n",
      "68 Train Loss 110.610374 Test MSE 109.0557971122604 Test RE 0.17579784574585366 Lambda1 0.7197437\n",
      "69 Train Loss 108.65408 Test MSE 105.47189256331482 Test RE 0.1728850899039328 Lambda1 0.76069146\n",
      "70 Train Loss 106.69655 Test MSE 102.9399103022059 Test RE 0.1707973245355508 Lambda1 0.79560274\n",
      "71 Train Loss 105.05175 Test MSE 102.42552964040341 Test RE 0.17037006135770538 Lambda1 0.8154105\n",
      "72 Train Loss 103.9669 Test MSE 103.40667367718551 Test RE 0.17118411216898413 Lambda1 0.80680054\n",
      "73 Train Loss 102.35792 Test MSE 99.77754508226144 Test RE 0.16815337088881221 Lambda1 0.8246325\n",
      "74 Train Loss 101.44131 Test MSE 97.40970939041576 Test RE 0.16614615473435823 Lambda1 0.85170007\n",
      "Training time: 68.29\n",
      "Training time: 68.29\n",
      "inv_HT_stan_tune13\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 837.97906 Test MSE 858.1363901054763 Test RE 0.4931367062994612 Lambda1 -0.03740084\n",
      "1 Train Loss 837.82336 Test MSE 857.8471485499158 Test RE 0.49305359151093975 Lambda1 -0.037846986\n",
      "2 Train Loss 836.5903 Test MSE 855.7508523493592 Test RE 0.49245079264330044 Lambda1 -0.21389958\n",
      "3 Train Loss 829.0491 Test MSE 839.5500185836853 Test RE 0.4877670491383167 Lambda1 -0.20880702\n",
      "4 Train Loss 755.86096 Test MSE 764.6789729601117 Test RE 0.465509710455444 Lambda1 0.0067936857\n",
      "5 Train Loss 670.6803 Test MSE 669.4055706671961 Test RE 0.4355458015511779 Lambda1 0.004622204\n",
      "6 Train Loss 645.7095 Test MSE 648.8896726229266 Test RE 0.4288195746859241 Lambda1 0.005157256\n",
      "7 Train Loss 603.35095 Test MSE 584.3583770668065 Test RE 0.40693852608583575 Lambda1 0.008107505\n",
      "8 Train Loss 501.96732 Test MSE 491.96277755013796 Test RE 0.3733836467459288 Lambda1 0.005594281\n",
      "9 Train Loss 392.80408 Test MSE 389.2002611017483 Test RE 0.33210524313486345 Lambda1 -8.231984e-06\n",
      "10 Train Loss 282.95343 Test MSE 301.13695024187575 Test RE 0.29212666084123495 Lambda1 0.0006103013\n",
      "11 Train Loss 268.9646 Test MSE 283.9687714773663 Test RE 0.28367721978685423 Lambda1 0.0022934668\n",
      "12 Train Loss 262.16388 Test MSE 284.5985739673605 Test RE 0.28399162349252166 Lambda1 8.847192e-05\n",
      "13 Train Loss 258.77466 Test MSE 283.6977605995821 Test RE 0.2835418211590899 Lambda1 -0.00023194861\n",
      "14 Train Loss 257.47342 Test MSE 281.89160213412043 Test RE 0.2826377971576702 Lambda1 7.2291914e-05\n",
      "15 Train Loss 256.56784 Test MSE 282.01987450756434 Test RE 0.28270209580543215 Lambda1 0.0006768622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 Train Loss 255.41357 Test MSE 281.6009388406659 Test RE 0.2824920432216231 Lambda1 0.0007962906\n",
      "17 Train Loss 255.22679 Test MSE 281.68305449084386 Test RE 0.28253322796884345 Lambda1 0.0006557592\n",
      "18 Train Loss 255.00842 Test MSE 281.9709560928916 Test RE 0.2826775763666378 Lambda1 0.0007420849\n",
      "19 Train Loss 254.92078 Test MSE 282.1930789435646 Test RE 0.2827888941939613 Lambda1 0.00054543314\n",
      "20 Train Loss 254.67622 Test MSE 282.4600761696134 Test RE 0.28292264304337555 Lambda1 0.00075954996\n",
      "21 Train Loss 254.47371 Test MSE 282.50078157795247 Test RE 0.2829430283434658 Lambda1 0.0008291307\n",
      "22 Train Loss 254.14607 Test MSE 283.2795807384886 Test RE 0.28333276909266275 Lambda1 0.0006575965\n",
      "23 Train Loss 253.88484 Test MSE 283.5173199666746 Test RE 0.2834516360914018 Lambda1 0.0005247977\n",
      "24 Train Loss 253.78127 Test MSE 283.8480404023537 Test RE 0.28361690981587717 Lambda1 0.00048959686\n",
      "25 Train Loss 253.55904 Test MSE 284.0792255587239 Test RE 0.28373238476194723 Lambda1 0.0003168501\n",
      "26 Train Loss 253.18893 Test MSE 284.04124455747404 Test RE 0.28371341681337237 Lambda1 0.00022737219\n",
      "27 Train Loss 253.15213 Test MSE 284.3415409252636 Test RE 0.28386335208445135 Lambda1 0.00024180532\n",
      "28 Train Loss 253.0704 Test MSE 284.50929608025194 Test RE 0.283947076254554 Lambda1 0.00024272257\n",
      "29 Train Loss 252.82158 Test MSE 285.3388156241297 Test RE 0.28436071513476724 Lambda1 0.00015261746\n",
      "30 Train Loss 252.52283 Test MSE 286.0542502641012 Test RE 0.2847169830718567 Lambda1 0.00013816959\n",
      "31 Train Loss 252.37057 Test MSE 286.19957292560537 Test RE 0.2847892955371748 Lambda1 0.00012451885\n",
      "32 Train Loss 252.17838 Test MSE 285.8677289824513 Test RE 0.28462414327616814 Lambda1 0.00012934844\n",
      "33 Train Loss 251.89027 Test MSE 285.20242641759205 Test RE 0.2842927461697082 Lambda1 0.000115851006\n",
      "34 Train Loss 251.65797 Test MSE 285.3866793937041 Test RE 0.2843845639844084 Lambda1 6.275484e-05\n",
      "35 Train Loss 251.00381 Test MSE 285.80533245317986 Test RE 0.2845930790354547 Lambda1 -4.942237e-06\n",
      "36 Train Loss 250.84882 Test MSE 285.97239274609507 Test RE 0.2846762427348093 Lambda1 -3.5468005e-07\n",
      "37 Train Loss 250.6911 Test MSE 285.7616168941124 Test RE 0.28457131313412876 Lambda1 2.3915252e-06\n",
      "38 Train Loss 250.41052 Test MSE 286.9385729812234 Test RE 0.2851567377746052 Lambda1 -3.2843538e-05\n",
      "39 Train Loss 250.2489 Test MSE 287.282845201408 Test RE 0.2853277536701126 Lambda1 -2.7999082e-05\n",
      "40 Train Loss 250.1224 Test MSE 287.3893018942749 Test RE 0.28538061487657207 Lambda1 -1.3371025e-05\n",
      "41 Train Loss 250.01938 Test MSE 287.82380635136246 Test RE 0.28559626715567593 Lambda1 3.8358075e-06\n",
      "42 Train Loss 249.98029 Test MSE 287.4748892257544 Test RE 0.28542310627568623 Lambda1 1.6968968e-05\n",
      "43 Train Loss 249.89488 Test MSE 287.3732214710594 Test RE 0.2853726307497632 Lambda1 1.4126173e-05\n",
      "44 Train Loss 249.82771 Test MSE 287.5208901653203 Test RE 0.2854459416716647 Lambda1 1.4159065e-05\n",
      "45 Train Loss 249.76555 Test MSE 287.9109157788035 Test RE 0.2856394815193169 Lambda1 4.211149e-06\n",
      "46 Train Loss 249.70024 Test MSE 288.1409857831031 Test RE 0.28575358617761915 Lambda1 1.9539038e-06\n",
      "47 Train Loss 249.67137 Test MSE 288.2673850448886 Test RE 0.28581625529429566 Lambda1 5.8205865e-06\n",
      "48 Train Loss 249.66077 Test MSE 288.2473016646308 Test RE 0.2858062988149672 Lambda1 1.0954946e-05\n",
      "49 Train Loss 249.64093 Test MSE 288.1335276055822 Test RE 0.28574988796238915 Lambda1 1.8435927e-05\n",
      "50 Train Loss 249.61475 Test MSE 288.2864758853606 Test RE 0.2858257193933923 Lambda1 2.2368573e-05\n",
      "51 Train Loss 249.60349 Test MSE 288.19266622966154 Test RE 0.285779211152098 Lambda1 2.023861e-05\n",
      "52 Train Loss 249.59372 Test MSE 288.17995519811745 Test RE 0.28577290879078127 Lambda1 1.9101537e-05\n",
      "53 Train Loss 249.59074 Test MSE 288.23990043787427 Test RE 0.28580262951642993 Lambda1 1.633371e-05\n",
      "54 Train Loss 249.58095 Test MSE 288.0751824862566 Test RE 0.2857209552735551 Lambda1 1.547412e-05\n",
      "55 Train Loss 249.5505 Test MSE 288.4433124957512 Test RE 0.2859034577638312 Lambda1 8.997913e-06\n",
      "56 Train Loss 249.5038 Test MSE 288.80678799049264 Test RE 0.28608353852682683 Lambda1 7.1115473e-06\n",
      "57 Train Loss 249.46745 Test MSE 288.5805940022732 Test RE 0.28597148601539146 Lambda1 1.000428e-05\n",
      "58 Train Loss 249.29796 Test MSE 288.84581491522465 Test RE 0.2861028673376965 Lambda1 -5.29562e-06\n",
      "59 Train Loss 249.2141 Test MSE 288.97814991785606 Test RE 0.286168398991918 Lambda1 -1.2327271e-05\n",
      "60 Train Loss 249.19652 Test MSE 288.72078277837716 Test RE 0.28604093823400656 Lambda1 -1.3949694e-05\n",
      "61 Train Loss 249.08208 Test MSE 288.05157787937435 Test RE 0.2857092491818836 Lambda1 -1.2743857e-05\n",
      "62 Train Loss 248.92876 Test MSE 288.4442641139965 Test RE 0.2859039293828818 Lambda1 -1.098786e-05\n",
      "63 Train Loss 248.83624 Test MSE 288.1364130175669 Test RE 0.28575131872973863 Lambda1 -1.049585e-05\n",
      "64 Train Loss 248.65616 Test MSE 288.25258031735893 Test RE 0.2858089157783928 Lambda1 -5.2274945e-06\n",
      "65 Train Loss 248.22066 Test MSE 289.2237063743982 Test RE 0.28628995762267717 Lambda1 2.6509879e-06\n",
      "66 Train Loss 248.01593 Test MSE 289.113502581999 Test RE 0.28623540945744363 Lambda1 2.229544e-06\n",
      "67 Train Loss 247.84114 Test MSE 288.9459253694995 Test RE 0.28615244293345893 Lambda1 1.9461545e-06\n",
      "68 Train Loss 247.75334 Test MSE 288.88284682167273 Test RE 0.2861212068712947 Lambda1 1.6040443e-06\n",
      "69 Train Loss 247.70447 Test MSE 288.6026379493979 Test RE 0.28598240812877496 Lambda1 1.891847e-06\n",
      "70 Train Loss 247.47478 Test MSE 289.01635365604324 Test RE 0.28618731450730545 Lambda1 1.6934354e-06\n",
      "71 Train Loss 246.78116 Test MSE 290.19403115356175 Test RE 0.28676979663462154 Lambda1 7.9089006e-08\n",
      "72 Train Loss 246.3757 Test MSE 290.0415554789474 Test RE 0.2866944484905516 Lambda1 4.9402325e-07\n",
      "73 Train Loss 246.30328 Test MSE 290.0187031347443 Test RE 0.2866831539550788 Lambda1 6.990703e-07\n",
      "74 Train Loss 246.20058 Test MSE 290.3276583897919 Test RE 0.2868358142657167 Lambda1 4.8327126e-07\n",
      "Training time: 100.37\n",
      "Training time: 100.37\n",
      "inv_HT_stan_tune13\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 837.8812 Test MSE 857.8772135685492 Test RE 0.4930622314741091 Lambda1 -0.080607034\n",
      "1 Train Loss 837.8076 Test MSE 857.7892356440414 Test RE 0.4930369483081147 Lambda1 -0.16233957\n",
      "2 Train Loss 837.1714 Test MSE 853.3067493179298 Test RE 0.49174704744710585 Lambda1 -0.5522324\n",
      "3 Train Loss 808.354 Test MSE 819.4729411578162 Test RE 0.48189950426985806 Lambda1 -0.49658212\n",
      "4 Train Loss 782.0213 Test MSE 787.976088758681 Test RE 0.4725477396277521 Lambda1 -0.56339794\n",
      "5 Train Loss 716.044 Test MSE 718.1355393137641 Test RE 0.45112031051235874 Lambda1 -0.4070579\n",
      "6 Train Loss 703.85156 Test MSE 703.9395841258591 Test RE 0.44663922808257744 Lambda1 -0.40634564\n",
      "7 Train Loss 696.3396 Test MSE 694.6176337298131 Test RE 0.443672052456373 Lambda1 -0.43247327\n",
      "8 Train Loss 677.1176 Test MSE 673.6429990077323 Test RE 0.4369221589466145 Lambda1 -0.56361234\n",
      "9 Train Loss 661.9743 Test MSE 663.4823563143627 Test RE 0.43361456320004366 Lambda1 -0.6197643\n",
      "10 Train Loss 648.6565 Test MSE 644.7367945930641 Test RE 0.42744515469904576 Lambda1 -0.73756343\n",
      "11 Train Loss 634.6399 Test MSE 621.1576830011278 Test RE 0.4195561594423197 Lambda1 -0.88058126\n",
      "12 Train Loss 587.76306 Test MSE 559.9113916471773 Test RE 0.3983353253214427 Lambda1 -1.0558889\n",
      "13 Train Loss 549.5622 Test MSE 539.8026159139501 Test RE 0.39111697295550457 Lambda1 -0.96082395\n",
      "14 Train Loss 474.99936 Test MSE 474.7348179907996 Test RE 0.3667876574677251 Lambda1 -0.9085652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Train Loss 441.16586 Test MSE 443.3996766935116 Test RE 0.354476016592753 Lambda1 -0.79449904\n",
      "16 Train Loss 375.2488 Test MSE 379.82337732247834 Test RE 0.3280801963489528 Lambda1 -0.73062867\n",
      "17 Train Loss 342.40106 Test MSE 348.68629988625474 Test RE 0.31434504039996114 Lambda1 -0.603589\n",
      "18 Train Loss 321.6291 Test MSE 328.00469323572906 Test RE 0.30488018517987703 Lambda1 -0.5818616\n",
      "19 Train Loss 301.68597 Test MSE 317.2117780055519 Test RE 0.29982222497904526 Lambda1 -0.56995755\n",
      "20 Train Loss 283.33408 Test MSE 302.0875104382706 Test RE 0.2925873568613467 Lambda1 -0.54309666\n",
      "21 Train Loss 263.22147 Test MSE 278.132512084096 Test RE 0.2807469519187069 Lambda1 -0.5854194\n",
      "22 Train Loss 248.17216 Test MSE 258.57814836617814 Test RE 0.2706980212929109 Lambda1 -0.6499372\n",
      "23 Train Loss 235.93765 Test MSE 255.51664431837378 Test RE 0.2690907494610485 Lambda1 -0.6718992\n",
      "24 Train Loss 226.00891 Test MSE 245.46336928375402 Test RE 0.2637439560898952 Lambda1 -0.73963535\n",
      "25 Train Loss 212.26721 Test MSE 229.28709054478244 Test RE 0.2549053627800564 Lambda1 -0.8561937\n",
      "26 Train Loss 201.995 Test MSE 214.35016265627848 Test RE 0.24646263046122205 Lambda1 -0.9753211\n",
      "27 Train Loss 195.10544 Test MSE 210.59814449970116 Test RE 0.2442960476936445 Lambda1 -1.0231628\n",
      "28 Train Loss 190.70944 Test MSE 203.17617848297422 Test RE 0.23995265761984622 Lambda1 -1.0428882\n",
      "29 Train Loss 182.41478 Test MSE 190.8739475538536 Test RE 0.23257471549028857 Lambda1 -1.0750856\n",
      "30 Train Loss 174.54941 Test MSE 187.3754652767521 Test RE 0.23043345568893311 Lambda1 -1.134622\n",
      "31 Train Loss 168.52065 Test MSE 181.21627542976333 Test RE 0.22661453953210586 Lambda1 -1.16615\n",
      "32 Train Loss 164.93869 Test MSE 176.7819521059826 Test RE 0.22382476297837578 Lambda1 -1.224125\n",
      "33 Train Loss 162.62053 Test MSE 175.0843108812584 Test RE 0.2227474732101416 Lambda1 -1.2552454\n",
      "34 Train Loss 159.35782 Test MSE 169.54826100390045 Test RE 0.2191976230967112 Lambda1 -1.3191037\n",
      "35 Train Loss 157.14401 Test MSE 168.22729503598367 Test RE 0.21834205905524276 Lambda1 -1.3129115\n",
      "36 Train Loss 151.6983 Test MSE 163.397997263403 Test RE 0.21518526792319265 Lambda1 -1.3300216\n",
      "37 Train Loss 150.5463 Test MSE 159.32698511462524 Test RE 0.21248772156282705 Lambda1 -1.3667341\n",
      "38 Train Loss 146.95973 Test MSE 155.12159297371497 Test RE 0.2096646911014648 Lambda1 -1.42001\n",
      "39 Train Loss 144.76309 Test MSE 152.78826272441464 Test RE 0.20808183400920172 Lambda1 -1.466972\n",
      "40 Train Loss 142.51627 Test MSE 153.03010930203718 Test RE 0.20824645392179678 Lambda1 -1.4434224\n",
      "41 Train Loss 138.29326 Test MSE 148.60716125750653 Test RE 0.20521497060520671 Lambda1 -1.5182389\n",
      "42 Train Loss 137.64536 Test MSE 146.73792235760203 Test RE 0.2039202493291978 Lambda1 -1.549834\n",
      "43 Train Loss 136.42677 Test MSE 145.06191073063738 Test RE 0.202752336386403 Lambda1 -1.5723453\n",
      "44 Train Loss 133.47565 Test MSE 142.04126507817813 Test RE 0.20063026022923197 Lambda1 -1.6075674\n",
      "45 Train Loss 131.75833 Test MSE 140.07089008793534 Test RE 0.19923384425551988 Lambda1 -1.6478876\n",
      "46 Train Loss 130.9166 Test MSE 138.4789331492611 Test RE 0.19809842614128506 Lambda1 -1.6706755\n",
      "47 Train Loss 127.893074 Test MSE 134.4316003325837 Test RE 0.1951820409614443 Lambda1 -1.7653261\n",
      "48 Train Loss 126.20098 Test MSE 132.39162046410112 Test RE 0.1936954502441919 Lambda1 -1.7917471\n",
      "49 Train Loss 125.463524 Test MSE 133.07887920472672 Test RE 0.19419754614409912 Lambda1 -1.7824415\n",
      "50 Train Loss 123.06983 Test MSE 133.3974107142049 Test RE 0.19442981841363996 Lambda1 -1.7708486\n",
      "51 Train Loss 122.26436 Test MSE 132.5546004573771 Test RE 0.19381463744610994 Lambda1 -1.7767556\n",
      "52 Train Loss 121.09832 Test MSE 131.29532662573595 Test RE 0.19289181723267523 Lambda1 -1.8135843\n",
      "53 Train Loss 119.98737 Test MSE 129.11471736174425 Test RE 0.19128329537482666 Lambda1 -1.8429754\n",
      "54 Train Loss 119.39465 Test MSE 127.85161152279633 Test RE 0.1903453509264466 Lambda1 -1.8544652\n",
      "55 Train Loss 118.340515 Test MSE 126.65754245977611 Test RE 0.18945440138119196 Lambda1 -1.872134\n",
      "56 Train Loss 117.27475 Test MSE 126.21542104380359 Test RE 0.18912344962356561 Lambda1 -1.8822256\n",
      "57 Train Loss 114.822395 Test MSE 124.64789750155747 Test RE 0.1879453777617935 Lambda1 -1.922543\n",
      "58 Train Loss 113.58014 Test MSE 122.98280383956498 Test RE 0.18668583460924995 Lambda1 -1.9385566\n",
      "59 Train Loss 113.269585 Test MSE 122.35836268783761 Test RE 0.186211285865988 Lambda1 -1.9508786\n",
      "60 Train Loss 112.28891 Test MSE 121.5156380702863 Test RE 0.18556892695933755 Lambda1 -1.9798826\n",
      "61 Train Loss 111.97008 Test MSE 120.95245758441791 Test RE 0.18513840554032138 Lambda1 -1.9861279\n",
      "62 Train Loss 111.357285 Test MSE 119.89976515214443 Test RE 0.18433098204773612 Lambda1 -2.0060484\n",
      "63 Train Loss 110.38906 Test MSE 118.67108416204489 Test RE 0.18338407780294444 Lambda1 -2.0339437\n",
      "64 Train Loss 109.661674 Test MSE 118.22726028704015 Test RE 0.18304083296753246 Lambda1 -2.0533054\n",
      "65 Train Loss 108.74067 Test MSE 115.88433532353784 Test RE 0.18121808540000095 Lambda1 -2.1069193\n",
      "66 Train Loss 108.36854 Test MSE 116.01790412134847 Test RE 0.18132249170753345 Lambda1 -2.101943\n",
      "67 Train Loss 107.70775 Test MSE 115.47729024711877 Test RE 0.1808995401134895 Lambda1 -2.1437762\n",
      "68 Train Loss 106.36273 Test MSE 114.56922245772155 Test RE 0.18018687506108352 Lambda1 -2.1658413\n",
      "69 Train Loss 105.15193 Test MSE 113.01996365212202 Test RE 0.1789644426662708 Lambda1 -2.1863215\n",
      "70 Train Loss 104.61174 Test MSE 113.83362728277882 Test RE 0.1796074959292847 Lambda1 -2.1740422\n",
      "71 Train Loss 104.165794 Test MSE 112.91034845010307 Test RE 0.1788776350715186 Lambda1 -2.1895542\n",
      "72 Train Loss 103.10659 Test MSE 112.00248746372216 Test RE 0.17815704663262338 Lambda1 -2.241038\n",
      "73 Train Loss 102.467964 Test MSE 111.32798265441097 Test RE 0.17761978512241697 Lambda1 -2.2606137\n",
      "74 Train Loss 102.23339 Test MSE 110.43937727500354 Test RE 0.17690949599109607 Lambda1 -2.2977076\n",
      "Training time: 121.93\n",
      "Training time: 121.93\n",
      "inv_HT_stan_tune13\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 837.9347 Test MSE 858.0997958600528 Test RE 0.49312619156179405 Lambda1 0.024837788\n",
      "1 Train Loss 837.8434 Test MSE 857.8377189522823 Test RE 0.4930508816402104 Lambda1 -0.055185225\n",
      "2 Train Loss 837.73975 Test MSE 857.395921100493 Test RE 0.49292390140319775 Lambda1 -0.23473802\n",
      "3 Train Loss 836.0506 Test MSE 852.7283875706374 Test RE 0.49158036888149154 Lambda1 -0.8651727\n",
      "4 Train Loss 816.2003 Test MSE 815.6131963561919 Test RE 0.4807632834367455 Lambda1 -0.82302815\n",
      "5 Train Loss 778.07715 Test MSE 780.2309130705239 Test RE 0.470219621157994 Lambda1 -0.8942153\n",
      "6 Train Loss 752.8336 Test MSE 757.983991073228 Test RE 0.46346739541131415 Lambda1 -0.87985635\n",
      "7 Train Loss 730.25195 Test MSE 729.4687997374598 Test RE 0.45466605495254536 Lambda1 -0.9066404\n",
      "8 Train Loss 685.7176 Test MSE 677.4677014502089 Test RE 0.43816074673614586 Lambda1 -0.7970369\n",
      "9 Train Loss 654.4366 Test MSE 654.5640123790342 Test RE 0.43069044091452435 Lambda1 -0.79466206\n",
      "10 Train Loss 629.3749 Test MSE 612.6004896586053 Test RE 0.41665619209619753 Lambda1 -0.98722106\n",
      "11 Train Loss 602.05804 Test MSE 588.0391480951693 Test RE 0.40821813148326236 Lambda1 -0.9139705\n",
      "12 Train Loss 544.74695 Test MSE 546.3529886297789 Test RE 0.3934828716005331 Lambda1 -0.97978014\n",
      "13 Train Loss 484.8262 Test MSE 478.7123135785446 Test RE 0.3683209905853271 Lambda1 -0.8689619\n",
      "14 Train Loss 396.71393 Test MSE 369.1892662227422 Test RE 0.323454876875545 Lambda1 -0.8862811\n",
      "15 Train Loss 328.62006 Test MSE 336.4548019615302 Test RE 0.30878239838354615 Lambda1 -0.91264904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 Train Loss 303.74097 Test MSE 324.47353983593365 Test RE 0.3032346418088791 Lambda1 -0.86008465\n",
      "17 Train Loss 286.56638 Test MSE 311.5761185108533 Test RE 0.2971469329697929 Lambda1 -0.76162094\n",
      "18 Train Loss 275.56503 Test MSE 302.39713063059463 Test RE 0.29273726003508277 Lambda1 -0.71483874\n",
      "19 Train Loss 268.51334 Test MSE 296.2206317429441 Test RE 0.2897322389728476 Lambda1 -0.7310652\n",
      "20 Train Loss 258.438 Test MSE 282.59637731130107 Test RE 0.2829908969870741 Lambda1 -0.73090565\n",
      "21 Train Loss 247.05052 Test MSE 268.637855279996 Test RE 0.27591338950432687 Lambda1 -0.73750097\n",
      "22 Train Loss 238.53496 Test MSE 259.23749815389414 Test RE 0.27104292872046326 Lambda1 -0.7269455\n",
      "23 Train Loss 228.58849 Test MSE 247.9517959954869 Test RE 0.26507745959443674 Lambda1 -0.69027376\n",
      "24 Train Loss 221.20297 Test MSE 241.95628463343098 Test RE 0.2618530423681303 Lambda1 -0.6793659\n",
      "25 Train Loss 210.48112 Test MSE 226.17550567249052 Test RE 0.2531698332834037 Lambda1 -0.7021222\n",
      "26 Train Loss 200.51196 Test MSE 217.77204037976153 Test RE 0.24842210115642507 Lambda1 -0.75595766\n",
      "27 Train Loss 193.26013 Test MSE 208.96977055065634 Test RE 0.24334974952081198 Lambda1 -0.80218494\n",
      "28 Train Loss 188.242 Test MSE 199.8890357347399 Test RE 0.23800367175954856 Lambda1 -0.8792128\n",
      "29 Train Loss 184.30222 Test MSE 195.04723845091536 Test RE 0.23510348862497488 Lambda1 -0.9270955\n",
      "30 Train Loss 181.67075 Test MSE 191.48786266805922 Test RE 0.23294843466730972 Lambda1 -0.97498137\n",
      "31 Train Loss 173.9706 Test MSE 185.02655652657046 Test RE 0.22898456219177837 Lambda1 -1.0173604\n",
      "32 Train Loss 166.20512 Test MSE 178.48176838153088 Test RE 0.22489826278126618 Lambda1 -1.1699467\n",
      "33 Train Loss 162.6861 Test MSE 172.8578881648862 Test RE 0.2213266813244798 Lambda1 -1.280102\n",
      "34 Train Loss 160.21034 Test MSE 171.36331052674535 Test RE 0.22036777789917214 Lambda1 -1.3411821\n",
      "35 Train Loss 156.82999 Test MSE 167.51818385048756 Test RE 0.2178813957022569 Lambda1 -1.3705219\n",
      "36 Train Loss 154.55402 Test MSE 161.74152773250879 Test RE 0.21409175442001435 Lambda1 -1.4570372\n",
      "37 Train Loss 153.4374 Test MSE 160.0035354317777 Test RE 0.21293838705387333 Lambda1 -1.483411\n",
      "38 Train Loss 150.40651 Test MSE 159.80107735634704 Test RE 0.21280362521238813 Lambda1 -1.4909902\n",
      "39 Train Loss 148.65398 Test MSE 160.6681036073477 Test RE 0.21338014428822696 Lambda1 -1.5407548\n",
      "40 Train Loss 144.489 Test MSE 156.61167145909846 Test RE 0.21066929066146498 Lambda1 -1.6070846\n",
      "41 Train Loss 142.37617 Test MSE 154.69188094352387 Test RE 0.2093740870673331 Lambda1 -1.6627961\n",
      "42 Train Loss 138.56482 Test MSE 149.00768303342116 Test RE 0.2054913292853048 Lambda1 -1.7614695\n",
      "43 Train Loss 134.3257 Test MSE 144.87789757150583 Test RE 0.20262369842401 Lambda1 -1.8498592\n",
      "44 Train Loss 133.3991 Test MSE 143.70205871078204 Test RE 0.20179976940009645 Lambda1 -1.8781078\n",
      "45 Train Loss 131.83212 Test MSE 142.41339241071847 Test RE 0.20089289928663406 Lambda1 -1.9375918\n",
      "46 Train Loss 129.99026 Test MSE 142.04685521407265 Test RE 0.20063420816430558 Lambda1 -1.9596211\n",
      "47 Train Loss 129.36285 Test MSE 140.8788213274843 Test RE 0.19980761014729145 Lambda1 -1.9911954\n",
      "48 Train Loss 127.16998 Test MSE 135.7875198761404 Test RE 0.1961639051545611 Lambda1 -2.0925913\n",
      "49 Train Loss 124.81009 Test MSE 133.02784395366726 Test RE 0.19416030555804864 Lambda1 -2.1876385\n",
      "50 Train Loss 123.778015 Test MSE 131.70882259431372 Test RE 0.1931953211400552 Lambda1 -2.244838\n",
      "51 Train Loss 123.07269 Test MSE 129.62139336976335 Test RE 0.19165824782711807 Lambda1 -2.3024998\n",
      "52 Train Loss 122.48341 Test MSE 129.59447739094023 Test RE 0.19163834780508512 Lambda1 -2.3147547\n",
      "53 Train Loss 121.81722 Test MSE 129.03104782664803 Test RE 0.19122130717392674 Lambda1 -2.3461776\n",
      "54 Train Loss 120.687164 Test MSE 128.19843693800905 Test RE 0.190603352738436 Lambda1 -2.365482\n",
      "55 Train Loss 120.231 Test MSE 128.59537448120616 Test RE 0.19089820482346662 Lambda1 -2.3792078\n",
      "56 Train Loss 119.12979 Test MSE 127.58453617262374 Test RE 0.1901464362348814 Lambda1 -2.402381\n",
      "57 Train Loss 118.67837 Test MSE 127.69173125531194 Test RE 0.19022629890485052 Lambda1 -2.3992302\n",
      "58 Train Loss 118.2085 Test MSE 126.46971107820377 Test RE 0.18931387013350226 Lambda1 -2.3974905\n",
      "59 Train Loss 117.76465 Test MSE 125.25994485594775 Test RE 0.18840623836184997 Lambda1 -2.4284341\n",
      "60 Train Loss 116.952896 Test MSE 122.94584896426932 Test RE 0.18665778406127392 Lambda1 -2.506183\n",
      "61 Train Loss 116.08607 Test MSE 122.66529261656301 Test RE 0.18644469050807222 Lambda1 -2.5460563\n",
      "62 Train Loss 115.14955 Test MSE 122.7019927780336 Test RE 0.1864725795644339 Lambda1 -2.5248826\n",
      "63 Train Loss 114.58797 Test MSE 123.01568818440342 Test RE 0.1867107918831359 Lambda1 -2.5254107\n",
      "64 Train Loss 114.02026 Test MSE 122.5593036504791 Test RE 0.18636412431838872 Lambda1 -2.550328\n",
      "65 Train Loss 113.53793 Test MSE 121.58858478820761 Test RE 0.18562461778834505 Lambda1 -2.585536\n",
      "66 Train Loss 113.01611 Test MSE 120.00906284811087 Test RE 0.1844149787191716 Lambda1 -2.6359856\n",
      "67 Train Loss 111.70476 Test MSE 117.57734821377431 Test RE 0.18253703890394138 Lambda1 -2.7180455\n",
      "68 Train Loss 110.70517 Test MSE 114.78672068738238 Test RE 0.1803578273613486 Lambda1 -2.7812202\n",
      "69 Train Loss 109.546425 Test MSE 114.22996293202375 Test RE 0.17991989482927578 Lambda1 -2.8034527\n",
      "70 Train Loss 108.21136 Test MSE 113.31753712254505 Test RE 0.1791998880709675 Lambda1 -2.7943096\n",
      "71 Train Loss 107.51767 Test MSE 112.85287498730813 Test RE 0.17883210325868085 Lambda1 -2.7797763\n",
      "72 Train Loss 107.08581 Test MSE 112.37821089292115 Test RE 0.17845561916714675 Lambda1 -2.7899542\n",
      "73 Train Loss 106.39981 Test MSE 112.134902348136 Test RE 0.1782623285637797 Lambda1 -2.788567\n",
      "74 Train Loss 106.05815 Test MSE 111.08639833556775 Test RE 0.17742696091919472 Lambda1 -2.8295588\n",
      "Training time: 105.92\n",
      "Training time: 105.92\n",
      "inv_HT_stan_tune13\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 838.0227 Test MSE 858.2586736069073 Test RE 0.4931718407697619 Lambda1 0.002168259\n",
      "1 Train Loss 837.88446 Test MSE 857.890703188441 Test RE 0.49306610801718187 Lambda1 0.00047718827\n",
      "2 Train Loss 837.82074 Test MSE 858.0125138953633 Test RE 0.4931011116599072 Lambda1 -0.019912751\n",
      "3 Train Loss 837.10266 Test MSE 855.3595464521248 Test RE 0.4923381892304971 Lambda1 0.023317842\n",
      "4 Train Loss 824.8091 Test MSE 823.3845021272294 Test RE 0.4830482519428334 Lambda1 0.5755206\n",
      "5 Train Loss 770.1335 Test MSE 777.1194774407905 Test RE 0.46928110437767306 Lambda1 0.7517131\n",
      "6 Train Loss 717.32733 Test MSE 717.3237616504956 Test RE 0.45086526608103306 Lambda1 0.8167052\n",
      "7 Train Loss 680.67676 Test MSE 672.6663835313227 Test RE 0.4366053296250263 Lambda1 0.95523405\n",
      "8 Train Loss 655.23224 Test MSE 652.3145414973741 Test RE 0.4299497498698363 Lambda1 0.91608965\n",
      "9 Train Loss 632.9115 Test MSE 634.6164285244115 Test RE 0.42407710448283514 Lambda1 0.96009314\n",
      "10 Train Loss 620.61523 Test MSE 623.3648875444488 Test RE 0.42030091805850456 Lambda1 1.0032617\n",
      "11 Train Loss 615.19073 Test MSE 620.6047644188346 Test RE 0.4193693855762891 Lambda1 0.9827995\n",
      "12 Train Loss 611.08484 Test MSE 616.8570426810197 Test RE 0.41810121957283514 Lambda1 0.9822729\n",
      "13 Train Loss 602.0119 Test MSE 602.1277705577736 Test RE 0.4130793637765697 Lambda1 0.9704048\n",
      "14 Train Loss 594.14197 Test MSE 593.4099997220765 Test RE 0.41007812286919004 Lambda1 0.95146066\n",
      "15 Train Loss 582.26355 Test MSE 586.3255613516245 Test RE 0.4076229096046804 Lambda1 0.89671564\n",
      "16 Train Loss 548.21783 Test MSE 533.5455534069805 Test RE 0.38884357114660867 Lambda1 0.7476776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 Train Loss 495.61557 Test MSE 458.04450260911204 Test RE 0.36028236773321626 Lambda1 0.5564871\n",
      "18 Train Loss 399.8843 Test MSE 395.16209355259167 Test RE 0.33463919700839745 Lambda1 0.47216907\n",
      "19 Train Loss 306.3233 Test MSE 308.721427971989 Test RE 0.295782556167999 Lambda1 0.49042994\n",
      "20 Train Loss 258.59393 Test MSE 272.95823949886534 Test RE 0.2781232366583469 Lambda1 0.4931404\n",
      "21 Train Loss 239.37373 Test MSE 261.6488943364627 Test RE 0.27230061522043736 Lambda1 0.43772733\n",
      "22 Train Loss 229.18883 Test MSE 255.04666792386504 Test RE 0.26884316381541123 Lambda1 0.41933915\n",
      "23 Train Loss 219.05124 Test MSE 242.4311156319277 Test RE 0.2621098552884309 Lambda1 0.36873096\n",
      "24 Train Loss 213.81775 Test MSE 232.20158421728235 Test RE 0.25652031222301325 Lambda1 0.30635002\n",
      "25 Train Loss 209.71252 Test MSE 228.39439876352563 Test RE 0.25440866268659335 Lambda1 0.28008813\n",
      "26 Train Loss 200.81578 Test MSE 213.732954948748 Test RE 0.24610753789869783 Lambda1 0.28298786\n",
      "27 Train Loss 188.95953 Test MSE 202.4957013822239 Test RE 0.23955049621596422 Lambda1 0.28983584\n",
      "28 Train Loss 182.05312 Test MSE 188.7396505749736 Test RE 0.2312707687349149 Lambda1 0.32783633\n",
      "29 Train Loss 173.53415 Test MSE 177.08900391571487 Test RE 0.22401905877955844 Lambda1 0.39758953\n",
      "30 Train Loss 167.59317 Test MSE 175.0182006624974 Test RE 0.22270541554529363 Lambda1 0.43725392\n",
      "31 Train Loss 161.47232 Test MSE 169.1730063815997 Test RE 0.2189549179102249 Lambda1 0.5032844\n",
      "32 Train Loss 155.73578 Test MSE 161.8608584387003 Test RE 0.2141707168551721 Lambda1 0.5247611\n",
      "33 Train Loss 150.53947 Test MSE 154.46900650433125 Test RE 0.2092232034137524 Lambda1 0.5391582\n",
      "34 Train Loss 145.84172 Test MSE 151.94236624219195 Test RE 0.20750502271431834 Lambda1 0.5487989\n",
      "35 Train Loss 142.5628 Test MSE 148.11812230554088 Test RE 0.20487702990813886 Lambda1 0.5433694\n",
      "36 Train Loss 137.98135 Test MSE 143.08546971538664 Test RE 0.2013663682469422 Lambda1 0.5440829\n",
      "37 Train Loss 134.67802 Test MSE 139.36566709346576 Test RE 0.19873166430211528 Lambda1 0.571545\n",
      "38 Train Loss 130.51016 Test MSE 135.01425147007137 Test RE 0.19560456096458498 Lambda1 0.59934235\n",
      "39 Train Loss 127.31356 Test MSE 131.42732476463308 Test RE 0.192988755049652 Lambda1 0.61385965\n",
      "40 Train Loss 122.758286 Test MSE 126.48078548114043 Test RE 0.18932215864846486 Lambda1 0.67570555\n",
      "41 Train Loss 119.761116 Test MSE 120.8302204524519 Test RE 0.18504482948224743 Lambda1 0.7065952\n",
      "42 Train Loss 116.24864 Test MSE 117.15198946143198 Test RE 0.18220655825266052 Lambda1 0.7344751\n",
      "43 Train Loss 113.3744 Test MSE 115.09859768543117 Test RE 0.18060267842786362 Lambda1 0.7647289\n",
      "44 Train Loss 111.64724 Test MSE 113.95868301184804 Test RE 0.179706125751888 Lambda1 0.7772782\n",
      "45 Train Loss 109.62905 Test MSE 110.83812305746204 Test RE 0.1772285776367504 Lambda1 0.798157\n",
      "46 Train Loss 107.30533 Test MSE 109.60348358165513 Test RE 0.17623872794837858 Lambda1 0.8531435\n",
      "47 Train Loss 105.66453 Test MSE 106.94040161328921 Test RE 0.17408448859362133 Lambda1 0.87814325\n",
      "48 Train Loss 103.082405 Test MSE 104.14495405744306 Test RE 0.17179411677609852 Lambda1 0.91577196\n",
      "49 Train Loss 102.32183 Test MSE 104.77390785077978 Test RE 0.17231208674401055 Lambda1 0.90557444\n",
      "50 Train Loss 101.11203 Test MSE 103.24565142367885 Test RE 0.17105077846065198 Lambda1 0.9014123\n",
      "51 Train Loss 99.18622 Test MSE 100.27991228926027 Test RE 0.16857615477354035 Lambda1 0.92691207\n",
      "52 Train Loss 96.292 Test MSE 97.07828040282158 Test RE 0.16586326419291414 Lambda1 0.94449145\n",
      "53 Train Loss 94.07765 Test MSE 94.51293238757033 Test RE 0.16365707677831415 Lambda1 0.9664202\n",
      "54 Train Loss 93.01539 Test MSE 93.65575106345634 Test RE 0.16291324568868884 Lambda1 0.9743656\n",
      "55 Train Loss 91.440254 Test MSE 93.05937134261707 Test RE 0.16239371901854877 Lambda1 0.9559748\n",
      "56 Train Loss 89.86913 Test MSE 91.94542356268121 Test RE 0.1614188427891487 Lambda1 0.9676423\n",
      "57 Train Loss 87.04862 Test MSE 89.37443591240678 Test RE 0.1591460366608464 Lambda1 1.0134689\n",
      "58 Train Loss 82.64735 Test MSE 83.59145051181773 Test RE 0.15391115642621905 Lambda1 1.0404234\n",
      "59 Train Loss 81.67521 Test MSE 80.88421842848325 Test RE 0.1513983239321145 Lambda1 1.0615691\n",
      "60 Train Loss 80.52722 Test MSE 78.33165188547922 Test RE 0.14899023776820428 Lambda1 1.0842147\n",
      "61 Train Loss 79.34331 Test MSE 77.3974950256332 Test RE 0.1480991694938361 Lambda1 1.0878667\n",
      "62 Train Loss 77.92351 Test MSE 76.66326203995008 Test RE 0.14739502257469433 Lambda1 1.0968782\n",
      "63 Train Loss 75.26728 Test MSE 74.15221723376042 Test RE 0.14496102172006056 Lambda1 1.1281793\n",
      "64 Train Loss 73.48763 Test MSE 73.20753099825357 Test RE 0.14403467289286184 Lambda1 1.1384245\n",
      "65 Train Loss 71.58641 Test MSE 70.55387064514028 Test RE 0.1414000597204504 Lambda1 1.1520414\n",
      "66 Train Loss 70.80829 Test MSE 69.96076521617368 Test RE 0.1408044712825882 Lambda1 1.1716168\n",
      "67 Train Loss 68.45536 Test MSE 68.40282607102579 Test RE 0.13922787451268168 Lambda1 1.2038572\n",
      "68 Train Loss 67.62408 Test MSE 67.77686043726949 Test RE 0.13858936165744085 Lambda1 1.2056804\n",
      "69 Train Loss 65.829285 Test MSE 66.24872551064898 Test RE 0.137018097965443 Lambda1 1.231252\n",
      "70 Train Loss 64.31749 Test MSE 63.80048449604813 Test RE 0.13446249299204166 Lambda1 1.2467914\n",
      "71 Train Loss 62.957073 Test MSE 61.77747777134711 Test RE 0.13231353031278054 Lambda1 1.2574646\n",
      "72 Train Loss 61.80253 Test MSE 60.51020320016253 Test RE 0.1309493889770859 Lambda1 1.2737273\n",
      "73 Train Loss 61.279995 Test MSE 60.27573513301802 Test RE 0.13069543799778233 Lambda1 1.2717375\n",
      "74 Train Loss 60.823395 Test MSE 59.41672176099606 Test RE 0.1297607998509698 Lambda1 1.2753397\n",
      "Training time: 120.29\n",
      "Training time: 120.29\n",
      "inv_HT_stan_tune13\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 838.0212 Test MSE 857.6741477972658 Test RE 0.4930038723135755 Lambda1 -0.18955736\n",
      "1 Train Loss 837.87366 Test MSE 857.9164696480002 Test RE 0.49307351249918174 Lambda1 -0.18956451\n",
      "2 Train Loss 837.5807 Test MSE 857.163156964319 Test RE 0.49285698786223575 Lambda1 -0.1664935\n",
      "3 Train Loss 835.64905 Test MSE 853.104954846468 Test RE 0.4916888985440167 Lambda1 -0.17884617\n",
      "4 Train Loss 818.4463 Test MSE 825.635799743463 Test RE 0.4837081763398093 Lambda1 -0.31468162\n",
      "5 Train Loss 795.0461 Test MSE 798.6148377339179 Test RE 0.47572706292361255 Lambda1 -0.20265675\n",
      "6 Train Loss 709.2538 Test MSE 699.5249427499152 Test RE 0.44523651315341056 Lambda1 -0.024730785\n",
      "7 Train Loss 642.7554 Test MSE 642.3798192807369 Test RE 0.42666313005296175 Lambda1 -0.0020267454\n",
      "8 Train Loss 628.1415 Test MSE 642.1899172939859 Test RE 0.42660005976131515 Lambda1 0.0008166302\n",
      "9 Train Loss 430.08655 Test MSE 384.0764147291383 Test RE 0.3299119069636383 Lambda1 0.0007729687\n",
      "10 Train Loss 314.5125 Test MSE 324.7581087391933 Test RE 0.30336758367846567 Lambda1 0.00047583965\n",
      "11 Train Loss 284.0654 Test MSE 296.99106560473825 Test RE 0.2901087734721018 Lambda1 0.0010355015\n",
      "12 Train Loss 271.1108 Test MSE 286.3200342206036 Test RE 0.28484922308684935 Lambda1 0.0009800756\n",
      "13 Train Loss 263.29514 Test MSE 282.06292742186474 Test RE 0.2827236735127669 Lambda1 0.00028458433\n",
      "14 Train Loss 260.56808 Test MSE 283.19624789464194 Test RE 0.2832910917821873 Lambda1 0.0006069557\n",
      "15 Train Loss 257.65897 Test MSE 283.59038474062726 Test RE 0.2834881576619109 Lambda1 0.0005377977\n",
      "16 Train Loss 256.76486 Test MSE 282.84126977099066 Test RE 0.2831134876055096 Lambda1 0.00052025774\n",
      "17 Train Loss 256.38196 Test MSE 283.44853666394187 Test RE 0.28341725032203724 Lambda1 0.0004074722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 Train Loss 255.9648 Test MSE 283.0114176260758 Test RE 0.28319863060875683 Lambda1 0.00040525963\n",
      "19 Train Loss 255.60858 Test MSE 283.2209083710603 Test RE 0.2833034258813523 Lambda1 0.00042879544\n",
      "20 Train Loss 255.35316 Test MSE 282.3599904165679 Test RE 0.2828725137737779 Lambda1 0.00042909052\n",
      "21 Train Loss 255.29814 Test MSE 282.18064862866856 Test RE 0.2827826658453999 Lambda1 0.0004694286\n",
      "22 Train Loss 254.98433 Test MSE 282.74032563675956 Test RE 0.28306296245361956 Lambda1 0.00029141988\n",
      "23 Train Loss 254.72032 Test MSE 282.6476984429208 Test RE 0.28301659220986075 Lambda1 0.0004603058\n",
      "24 Train Loss 254.69342 Test MSE 282.82530840293884 Test RE 0.28310549912784827 Lambda1 0.00048012432\n",
      "25 Train Loss 254.6408 Test MSE 282.3686004760581 Test RE 0.28287682658488394 Lambda1 0.00048600673\n",
      "26 Train Loss 254.59291 Test MSE 282.11711423254104 Test RE 0.28275082908157706 Lambda1 0.000517845\n",
      "27 Train Loss 254.56969 Test MSE 282.13832977023657 Test RE 0.282761460478605 Lambda1 0.00054938294\n",
      "28 Train Loss 254.46819 Test MSE 282.0236144183655 Test RE 0.28270397027814137 Lambda1 0.00055776065\n",
      "29 Train Loss 254.3602 Test MSE 282.85289336791175 Test RE 0.2831193049377127 Lambda1 0.00054320035\n",
      "30 Train Loss 254.29236 Test MSE 282.82110136617126 Test RE 0.2831033935177107 Lambda1 0.0006010748\n",
      "31 Train Loss 254.27899 Test MSE 282.8084455815676 Test RE 0.2830970592384883 Lambda1 0.00066067075\n",
      "32 Train Loss 254.2506 Test MSE 282.7869323686633 Test RE 0.2830862914498487 Lambda1 0.0008084282\n",
      "33 Train Loss 254.20341 Test MSE 282.456413287259 Test RE 0.2829208085970167 Lambda1 0.00090423174\n",
      "34 Train Loss 254.16896 Test MSE 282.59073686122014 Test RE 0.28298807281075616 Lambda1 0.0008630092\n",
      "35 Train Loss 254.11809 Test MSE 282.5319847993667 Test RE 0.28295865394657704 Lambda1 0.00096796395\n",
      "36 Train Loss 254.06334 Test MSE 282.64477590810213 Test RE 0.283015129031516 Lambda1 0.0009828742\n",
      "37 Train Loss 254.03017 Test MSE 282.901091068972 Test RE 0.283143425458824 Lambda1 0.0011341395\n",
      "38 Train Loss 253.94788 Test MSE 282.2282631568917 Test RE 0.28280652289467356 Lambda1 0.0014382288\n",
      "39 Train Loss 253.92323 Test MSE 282.0627529540134 Test RE 0.28272358607447795 Lambda1 0.0015290716\n",
      "40 Train Loss 253.88837 Test MSE 281.8895268027837 Test RE 0.28263675674328304 Lambda1 0.0016125584\n",
      "41 Train Loss 253.82523 Test MSE 281.3508329863811 Test RE 0.28236656670726806 Lambda1 0.001830057\n",
      "42 Train Loss 253.76422 Test MSE 281.16504745842775 Test RE 0.28227332318601395 Lambda1 0.0017432105\n",
      "43 Train Loss 253.73083 Test MSE 281.4836506987673 Test RE 0.28243320744906564 Lambda1 0.0016623944\n",
      "44 Train Loss 253.66457 Test MSE 281.1935596481652 Test RE 0.2822876351116858 Lambda1 0.0019856223\n",
      "45 Train Loss 253.59103 Test MSE 280.84066606336637 Test RE 0.28211044618556574 Lambda1 0.002441275\n",
      "46 Train Loss 253.42928 Test MSE 279.6458827223041 Test RE 0.2815097138893057 Lambda1 0.0038469709\n",
      "47 Train Loss 253.29318 Test MSE 279.90931630418874 Test RE 0.2816422773563933 Lambda1 0.0040892935\n",
      "48 Train Loss 253.16391 Test MSE 279.10032051611046 Test RE 0.2812349806486409 Lambda1 0.004692556\n",
      "49 Train Loss 252.8433 Test MSE 278.09932445251735 Test RE 0.28073020162170476 Lambda1 0.006031477\n",
      "50 Train Loss 252.44212 Test MSE 278.00678016044174 Test RE 0.2806834878449794 Lambda1 0.0061445106\n",
      "51 Train Loss 251.58371 Test MSE 276.8018215488888 Test RE 0.2800745471958681 Lambda1 0.008647284\n",
      "52 Train Loss 250.95203 Test MSE 274.6401185659451 Test RE 0.27897877284095524 Lambda1 0.011732137\n",
      "53 Train Loss 249.72269 Test MSE 271.4189428574539 Test RE 0.27733791609050035 Lambda1 0.017855816\n",
      "54 Train Loss 248.7509 Test MSE 271.54550692094887 Test RE 0.27740257060993007 Lambda1 0.018393483\n",
      "55 Train Loss 247.0975 Test MSE 268.73559160242166 Test RE 0.27596357660033755 Lambda1 0.02342481\n",
      "56 Train Loss 245.83525 Test MSE 267.8709690784446 Test RE 0.27551928012323995 Lambda1 0.026710335\n",
      "57 Train Loss 243.84865 Test MSE 265.5826871442763 Test RE 0.27433994738272005 Lambda1 0.03347834\n",
      "58 Train Loss 239.99464 Test MSE 257.52826510216767 Test RE 0.27014791605236094 Lambda1 0.05064738\n",
      "59 Train Loss 236.5271 Test MSE 251.14357269656747 Test RE 0.2667781181306058 Lambda1 0.06670648\n",
      "60 Train Loss 230.19623 Test MSE 246.16050765191014 Test RE 0.2641182189945319 Lambda1 0.09141944\n",
      "61 Train Loss 224.1701 Test MSE 240.42091896818602 Test RE 0.2610209085744595 Lambda1 0.10730148\n",
      "62 Train Loss 220.50894 Test MSE 235.8291234898433 Test RE 0.25851627474716105 Lambda1 0.12665312\n",
      "63 Train Loss 215.0705 Test MSE 233.52689347595484 Test RE 0.2572513258069034 Lambda1 0.14174545\n",
      "64 Train Loss 213.613 Test MSE 233.96178721944668 Test RE 0.2574907520934291 Lambda1 0.14129885\n",
      "65 Train Loss 211.69902 Test MSE 232.6600750892805 Test RE 0.25677344189443113 Lambda1 0.16206457\n",
      "66 Train Loss 207.86337 Test MSE 227.68317169146198 Test RE 0.25401223568580156 Lambda1 0.18303335\n",
      "67 Train Loss 206.09157 Test MSE 223.92421594305682 Test RE 0.25190669019856915 Lambda1 0.20394097\n",
      "68 Train Loss 202.73976 Test MSE 217.78339857425345 Test RE 0.24842857946683078 Lambda1 0.21308489\n",
      "69 Train Loss 198.06255 Test MSE 211.2968192397063 Test RE 0.24470094714948096 Lambda1 0.22861615\n",
      "70 Train Loss 191.0676 Test MSE 196.17519021775692 Test RE 0.2357823065166221 Lambda1 0.24890488\n",
      "71 Train Loss 182.81737 Test MSE 184.86212869716823 Test RE 0.22888279354869007 Lambda1 0.24514508\n",
      "72 Train Loss 174.78737 Test MSE 176.31896149821557 Test RE 0.22353147313305002 Lambda1 0.26328355\n",
      "73 Train Loss 170.44766 Test MSE 170.07184705664272 Test RE 0.21953581692138277 Lambda1 0.2852181\n",
      "74 Train Loss 164.32764 Test MSE 163.13422666922952 Test RE 0.21501151282237393 Lambda1 0.3070958\n",
      "Training time: 121.22\n",
      "Training time: 121.22\n",
      "inv_HT_stan_tune13\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 838.13214 Test MSE 858.362232256103 Test RE 0.49320159326204904 Lambda1 -0.016070183\n",
      "1 Train Loss 837.8928 Test MSE 857.9508701936726 Test RE 0.49308339797664696 Lambda1 -0.019217527\n",
      "2 Train Loss 837.81177 Test MSE 857.7408225254452 Test RE 0.4930230347544917 Lambda1 -0.04545031\n",
      "3 Train Loss 835.8291 Test MSE 853.4947761319788 Test RE 0.49180122288740685 Lambda1 -0.15680307\n",
      "4 Train Loss 814.7598 Test MSE 810.586721886611 Test RE 0.47927956598475885 Lambda1 -0.08822151\n",
      "5 Train Loss 758.2611 Test MSE 766.3246694903219 Test RE 0.46601036236507304 Lambda1 -0.020531513\n",
      "6 Train Loss 651.83124 Test MSE 642.7917915470053 Test RE 0.42679992233577413 Lambda1 -0.0035974937\n",
      "7 Train Loss 632.2167 Test MSE 634.5993907893575 Test RE 0.4240714117828675 Lambda1 -0.0022367688\n",
      "8 Train Loss 567.3918 Test MSE 573.9089915603975 Test RE 0.4032837150097853 Lambda1 0.0017364852\n",
      "9 Train Loss 396.4907 Test MSE 410.9091701109497 Test RE 0.34124169249838715 Lambda1 0.0007501335\n",
      "10 Train Loss 271.94608 Test MSE 284.7436473872771 Test RE 0.28406399628493945 Lambda1 0.0039327294\n",
      "11 Train Loss 261.792 Test MSE 282.4577717468659 Test RE 0.2829214889427447 Lambda1 0.0002887434\n",
      "12 Train Loss 259.26755 Test MSE 282.6797340892952 Test RE 0.2830326304840534 Lambda1 9.82778e-05\n",
      "13 Train Loss 257.71973 Test MSE 282.08297750379273 Test RE 0.28273372185931034 Lambda1 0.00021885794\n",
      "14 Train Loss 256.0611 Test MSE 282.2788659191884 Test RE 0.2828318749796843 Lambda1 0.0002132689\n",
      "15 Train Loss 255.06387 Test MSE 282.19395407121834 Test RE 0.2827893326813093 Lambda1 0.00023193928\n",
      "16 Train Loss 254.8572 Test MSE 282.0966242556803 Test RE 0.28274056089355815 Lambda1 0.00021463624\n",
      "17 Train Loss 254.71791 Test MSE 282.0658156466357 Test RE 0.28272512100435027 Lambda1 0.00019468335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 Train Loss 254.18565 Test MSE 283.12530391406966 Test RE 0.2832556056895884 Lambda1 0.00011829312\n",
      "19 Train Loss 254.07285 Test MSE 282.8670742114592 Test RE 0.28312640194881383 Lambda1 0.00012460756\n",
      "20 Train Loss 253.73773 Test MSE 283.71324901894184 Test RE 0.28354956100639195 Lambda1 8.244638e-05\n",
      "21 Train Loss 253.59396 Test MSE 283.8200015943358 Test RE 0.28360290148158146 Lambda1 6.383975e-05\n",
      "22 Train Loss 253.23734 Test MSE 283.80182983553664 Test RE 0.28359382240684355 Lambda1 5.8745598e-05\n",
      "23 Train Loss 252.9614 Test MSE 285.46809093614837 Test RE 0.28442512392775343 Lambda1 3.6668847e-05\n",
      "24 Train Loss 252.67654 Test MSE 285.96803913022336 Test RE 0.2846740757848569 Lambda1 3.8519454e-05\n",
      "25 Train Loss 252.36604 Test MSE 287.8399758788919 Test RE 0.28560428923680065 Lambda1 2.4707548e-05\n",
      "26 Train Loss 251.91171 Test MSE 287.32761730079665 Test RE 0.28534998650627574 Lambda1 5.0915725e-05\n",
      "27 Train Loss 251.5919 Test MSE 287.21271528959835 Test RE 0.285292925221654 Lambda1 9.290057e-05\n",
      "28 Train Loss 251.37907 Test MSE 287.56358395122413 Test RE 0.28546713372408594 Lambda1 2.7978118e-05\n",
      "29 Train Loss 250.97395 Test MSE 286.8604043493818 Test RE 0.2851178935193258 Lambda1 -1.1225266e-06\n",
      "30 Train Loss 250.79541 Test MSE 287.3215492678065 Test RE 0.2853469733565303 Lambda1 7.4207064e-06\n",
      "31 Train Loss 250.69131 Test MSE 287.0784530238497 Test RE 0.28522623501259364 Lambda1 7.535437e-06\n",
      "32 Train Loss 250.40294 Test MSE 288.3415489652174 Test RE 0.28585301958308046 Lambda1 1.3793574e-06\n",
      "33 Train Loss 250.15619 Test MSE 287.5769875572133 Test RE 0.2854737865908103 Lambda1 6.4144015e-06\n",
      "34 Train Loss 250.0607 Test MSE 287.2600795830761 Test RE 0.2853164481022033 Lambda1 1.8372108e-06\n",
      "35 Train Loss 249.91078 Test MSE 287.72939353335823 Test RE 0.28554942224681173 Lambda1 2.1348304e-05\n",
      "36 Train Loss 249.82184 Test MSE 288.0483766887375 Test RE 0.28570766159760236 Lambda1 1.8941822e-05\n",
      "37 Train Loss 249.76544 Test MSE 287.99132227317216 Test RE 0.2856793648040032 Lambda1 1.2976263e-05\n",
      "38 Train Loss 249.6862 Test MSE 288.271753710158 Test RE 0.28581842104540384 Lambda1 7.0221026e-06\n",
      "39 Train Loss 249.66228 Test MSE 288.33947414009793 Test RE 0.2858519911220513 Lambda1 5.544151e-06\n",
      "40 Train Loss 249.64342 Test MSE 288.0081728297065 Test RE 0.28568772232296585 Lambda1 7.3779433e-06\n",
      "41 Train Loss 249.57973 Test MSE 288.1940942325038 Test RE 0.28577991917327283 Lambda1 6.8112995e-06\n",
      "42 Train Loss 249.54048 Test MSE 288.2139162477022 Test RE 0.2857897469882987 Lambda1 7.735604e-06\n",
      "43 Train Loss 249.43356 Test MSE 288.1584195517255 Test RE 0.2857622307072037 Lambda1 1.3182284e-06\n",
      "44 Train Loss 249.39246 Test MSE 288.02629814668904 Test RE 0.2856967118231713 Lambda1 5.4836123e-06\n",
      "45 Train Loss 249.3854 Test MSE 287.9178820868445 Test RE 0.28564293717256134 Lambda1 4.954501e-06\n",
      "46 Train Loss 249.32785 Test MSE 288.7768863581177 Test RE 0.2860687283032034 Lambda1 6.0761386e-06\n",
      "47 Train Loss 249.30176 Test MSE 288.45567860580456 Test RE 0.2859095863089862 Lambda1 6.6817697e-06\n",
      "48 Train Loss 249.29195 Test MSE 288.70315545524323 Test RE 0.28603220624568 Lambda1 8.686695e-06\n",
      "49 Train Loss 249.26526 Test MSE 288.64249729931413 Test RE 0.28600215617993247 Lambda1 8.570124e-06\n",
      "50 Train Loss 249.08421 Test MSE 288.508194808081 Test RE 0.2859356114550332 Lambda1 1.0412623e-05\n",
      "51 Train Loss 249.04547 Test MSE 288.40342458947 Test RE 0.2858836887414258 Lambda1 1.1915239e-05\n",
      "52 Train Loss 249.02986 Test MSE 288.36492009569963 Test RE 0.28586460406141007 Lambda1 8.6085465e-06\n",
      "53 Train Loss 249.01118 Test MSE 288.70185362333905 Test RE 0.28603156135097685 Lambda1 4.9222585e-06\n",
      "54 Train Loss 248.97935 Test MSE 288.4402590763083 Test RE 0.2859019444933734 Lambda1 2.227978e-06\n",
      "55 Train Loss 248.94682 Test MSE 288.24353294211005 Test RE 0.2858044304051859 Lambda1 1.1801224e-06\n",
      "56 Train Loss 248.83919 Test MSE 288.51221282808797 Test RE 0.2859376025440699 Lambda1 2.0491134e-06\n",
      "57 Train Loss 248.75876 Test MSE 288.232414966881 Test RE 0.285798918404298 Lambda1 4.073878e-06\n",
      "58 Train Loss 248.71562 Test MSE 288.19951204714255 Test RE 0.2857826053757036 Lambda1 4.349203e-06\n",
      "59 Train Loss 248.65349 Test MSE 288.5021651118238 Test RE 0.28593262347423126 Lambda1 3.7847013e-07\n",
      "60 Train Loss 248.27388 Test MSE 288.53604749114874 Test RE 0.28594941328441376 Lambda1 -3.585055e-06\n",
      "61 Train Loss 248.17468 Test MSE 288.7462145277121 Test RE 0.28605353580523973 Lambda1 -7.509486e-07\n",
      "62 Train Loss 248.13316 Test MSE 288.8148024905497 Test RE 0.28608750796370436 Lambda1 -4.967205e-06\n",
      "63 Train Loss 247.97902 Test MSE 288.8643105880734 Test RE 0.2861120272078668 Lambda1 -1.1252058e-05\n",
      "64 Train Loss 247.85388 Test MSE 289.1962106817657 Test RE 0.286276348904581 Lambda1 -1.2143798e-05\n",
      "65 Train Loss 247.75963 Test MSE 289.98577389716127 Test RE 0.2866668782363636 Lambda1 -9.293666e-06\n",
      "66 Train Loss 247.70396 Test MSE 289.553561949456 Test RE 0.2864531659396443 Lambda1 -3.2493556e-06\n",
      "67 Train Loss 247.67163 Test MSE 289.84822552560814 Test RE 0.28659888310919956 Lambda1 -2.6095868e-06\n",
      "68 Train Loss 247.4881 Test MSE 289.96450026992795 Test RE 0.28665636296853375 Lambda1 7.674141e-06\n",
      "69 Train Loss 247.2644 Test MSE 289.7733428050413 Test RE 0.2865618590936846 Lambda1 3.71166e-06\n",
      "70 Train Loss 247.1928 Test MSE 289.8022430229529 Test RE 0.28657614869950476 Lambda1 3.7066068e-06\n",
      "71 Train Loss 247.13438 Test MSE 290.02433080273505 Test RE 0.28668593541324866 Lambda1 9.666748e-08\n",
      "72 Train Loss 246.88318 Test MSE 290.6968289277625 Test RE 0.2870181215455349 Lambda1 -3.1913248e-06\n",
      "73 Train Loss 246.68193 Test MSE 290.7898615652742 Test RE 0.2870640455360294 Lambda1 1.8560432e-06\n",
      "74 Train Loss 246.645 Test MSE 290.79270494519767 Test RE 0.28706544900681097 Lambda1 1.5824207e-06\n",
      "Training time: 123.03\n",
      "Training time: 123.03\n",
      "inv_HT_stan_tune13\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 837.88074 Test MSE 857.9156871770477 Test RE 0.4930732876429151 Lambda1 -0.0117812\n",
      "1 Train Loss 836.0539 Test MSE 853.9224212109788 Test RE 0.4919244163974578 Lambda1 -0.07846724\n",
      "2 Train Loss 791.7892 Test MSE 788.7839706816046 Test RE 0.4727899201840046 Lambda1 -0.022321615\n",
      "3 Train Loss 702.6529 Test MSE 682.9565183105801 Test RE 0.4399321466503878 Lambda1 0.004921395\n",
      "4 Train Loss 669.14374 Test MSE 660.6147408942259 Test RE 0.4326764928316561 Lambda1 0.0016755359\n",
      "5 Train Loss 645.17004 Test MSE 651.524090577764 Test RE 0.4296891722100667 Lambda1 -0.0005290624\n",
      "6 Train Loss 641.55524 Test MSE 649.9766742965512 Test RE 0.4291785976195676 Lambda1 -0.00087627006\n",
      "7 Train Loss 628.59 Test MSE 634.2068992621741 Test RE 0.4239402501536909 Lambda1 0.0013228308\n",
      "8 Train Loss 580.90717 Test MSE 567.2187701470976 Test RE 0.40092622753387164 Lambda1 -0.000816006\n",
      "9 Train Loss 340.04156 Test MSE 319.88319618959815 Test RE 0.3010820635937824 Lambda1 0.0029666203\n",
      "10 Train Loss 282.75983 Test MSE 292.37668891760956 Test RE 0.28784622777431534 Lambda1 0.00011631593\n",
      "11 Train Loss 267.71704 Test MSE 288.1992444056035 Test RE 0.28578247267715456 Lambda1 0.0009710769\n",
      "12 Train Loss 260.14462 Test MSE 282.45850848048167 Test RE 0.282921857914068 Lambda1 0.0019643528\n",
      "13 Train Loss 258.614 Test MSE 281.4095061615459 Test RE 0.2823960076710656 Lambda1 0.0019633905\n",
      "14 Train Loss 257.60837 Test MSE 281.836179125735 Test RE 0.28261001093254107 Lambda1 0.0019205647\n",
      "15 Train Loss 256.6456 Test MSE 281.5938145432342 Test RE 0.2824884697781617 Lambda1 0.0031882368\n",
      "16 Train Loss 256.09357 Test MSE 281.153800480536 Test RE 0.282267677474353 Lambda1 0.0035143984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 Train Loss 255.60724 Test MSE 280.72780309339544 Test RE 0.28205375385746306 Lambda1 0.0046326304\n",
      "18 Train Loss 255.04977 Test MSE 279.79421037296015 Test RE 0.2815843621169631 Lambda1 0.0060593267\n",
      "19 Train Loss 253.76091 Test MSE 277.7625308598616 Test RE 0.28056016025887015 Lambda1 0.01098608\n",
      "20 Train Loss 252.38666 Test MSE 276.02850484605364 Test RE 0.27968304361366847 Lambda1 0.015071161\n",
      "21 Train Loss 250.10583 Test MSE 271.7011044476371 Test RE 0.277482036061126 Lambda1 0.024901643\n",
      "22 Train Loss 245.20294 Test MSE 266.2680822719952 Test RE 0.27469371687626276 Lambda1 0.04450974\n",
      "23 Train Loss 242.76688 Test MSE 262.409740975152 Test RE 0.2726962381665763 Lambda1 0.05972589\n",
      "24 Train Loss 241.05353 Test MSE 260.6267736513211 Test RE 0.2717682291774299 Lambda1 0.06800091\n",
      "25 Train Loss 238.09099 Test MSE 257.8693377525479 Test RE 0.2703267499720043 Lambda1 0.073957615\n",
      "26 Train Loss 232.41687 Test MSE 248.2301289915129 Test RE 0.2652261963878902 Lambda1 0.12224009\n",
      "27 Train Loss 224.29993 Test MSE 243.53764386075906 Test RE 0.2627073482260099 Lambda1 0.15607238\n",
      "28 Train Loss 220.68152 Test MSE 243.00107664333527 Test RE 0.26241778749150996 Lambda1 0.17230539\n",
      "29 Train Loss 214.96678 Test MSE 236.0458158595495 Test RE 0.2586350167436233 Lambda1 0.21479793\n",
      "30 Train Loss 209.36868 Test MSE 232.8569925999834 Test RE 0.2568820821341728 Lambda1 0.2348605\n",
      "31 Train Loss 206.98622 Test MSE 231.06402621254819 Test RE 0.2558911929068855 Lambda1 0.24213704\n",
      "32 Train Loss 204.70105 Test MSE 226.68083745862506 Test RE 0.25345249736752457 Lambda1 0.2509035\n",
      "33 Train Loss 201.30353 Test MSE 218.9104864585328 Test RE 0.2490705923756887 Lambda1 0.24973765\n",
      "34 Train Loss 198.20923 Test MSE 216.53071265925342 Test RE 0.2477130708824121 Lambda1 0.24793078\n",
      "35 Train Loss 188.5564 Test MSE 205.66356285257913 Test RE 0.24141699965989644 Lambda1 0.24579795\n",
      "36 Train Loss 184.26965 Test MSE 196.228357288808 Test RE 0.23581425501475467 Lambda1 0.25699362\n",
      "37 Train Loss 177.57684 Test MSE 190.2808117087556 Test RE 0.2322130743737443 Lambda1 0.26598945\n",
      "38 Train Loss 171.56831 Test MSE 181.2411070883225 Test RE 0.22663006524054158 Lambda1 0.27637193\n",
      "39 Train Loss 165.44113 Test MSE 168.6247009583625 Test RE 0.2185998033262158 Lambda1 0.30000153\n",
      "40 Train Loss 162.20186 Test MSE 164.48305537033932 Test RE 0.2158985636185008 Lambda1 0.3119394\n",
      "41 Train Loss 157.45169 Test MSE 156.4694370003405 Test RE 0.21057360418178564 Lambda1 0.33572373\n",
      "42 Train Loss 150.84543 Test MSE 146.71845805916888 Test RE 0.20390672420849762 Lambda1 0.37731233\n",
      "43 Train Loss 146.94417 Test MSE 145.1728323366089 Test RE 0.20282983887180564 Lambda1 0.39733875\n",
      "44 Train Loss 143.67137 Test MSE 143.9024707686948 Test RE 0.2019404389785489 Lambda1 0.41129968\n",
      "45 Train Loss 135.42506 Test MSE 129.37450208470068 Test RE 0.19147563404906806 Lambda1 0.508773\n",
      "46 Train Loss 129.04648 Test MSE 126.88396042168206 Test RE 0.18962366380777687 Lambda1 0.5456082\n",
      "47 Train Loss 128.20488 Test MSE 124.28232664733163 Test RE 0.18766956964791334 Lambda1 0.55012536\n",
      "48 Train Loss 125.95114 Test MSE 120.88220494013042 Test RE 0.18508463089204982 Lambda1 0.58570457\n",
      "49 Train Loss 122.976456 Test MSE 116.95629424415229 Test RE 0.18205431220327983 Lambda1 0.59734464\n",
      "50 Train Loss 121.24824 Test MSE 119.62431903670132 Test RE 0.18411912822621715 Lambda1 0.5900412\n",
      "51 Train Loss 117.793 Test MSE 114.52557065312308 Test RE 0.1801525454591252 Lambda1 0.5960116\n",
      "52 Train Loss 115.66435 Test MSE 112.8769048027179 Test RE 0.17885114164693117 Lambda1 0.6076351\n",
      "53 Train Loss 114.59808 Test MSE 111.19529793575474 Test RE 0.17751390672162434 Lambda1 0.628807\n",
      "54 Train Loss 113.097244 Test MSE 108.84010216248066 Test RE 0.17562390966741723 Lambda1 0.65279055\n",
      "55 Train Loss 109.248474 Test MSE 103.73101872033627 Test RE 0.1714523697500821 Lambda1 0.6684781\n",
      "56 Train Loss 107.56438 Test MSE 102.75138906505616 Test RE 0.17064085617263772 Lambda1 0.6765027\n",
      "57 Train Loss 106.6409 Test MSE 101.02692313789693 Test RE 0.16920287335106576 Lambda1 0.6905416\n",
      "58 Train Loss 105.28987 Test MSE 101.4933714356059 Test RE 0.16959303420942956 Lambda1 0.6967887\n",
      "59 Train Loss 103.081924 Test MSE 97.32161909334864 Test RE 0.1660710124583982 Lambda1 0.7159902\n",
      "60 Train Loss 101.012924 Test MSE 97.06682242475091 Test RE 0.16585347562993083 Lambda1 0.7252168\n",
      "61 Train Loss 98.01403 Test MSE 94.61186373941636 Test RE 0.16374270834597612 Lambda1 0.747762\n",
      "62 Train Loss 95.88366 Test MSE 95.50612721959801 Test RE 0.16451472960785934 Lambda1 0.761654\n",
      "63 Train Loss 94.00327 Test MSE 92.30857685480828 Test RE 0.16173730353386007 Lambda1 0.7872708\n",
      "64 Train Loss 93.31646 Test MSE 90.64729273907749 Test RE 0.16027529674933005 Lambda1 0.79576784\n",
      "65 Train Loss 91.91146 Test MSE 89.7862608699086 Test RE 0.15951227663991993 Lambda1 0.80559325\n",
      "66 Train Loss 89.65514 Test MSE 86.35568818862235 Test RE 0.15643525894347896 Lambda1 0.829648\n",
      "67 Train Loss 88.95943 Test MSE 86.16373448505867 Test RE 0.15626129800666094 Lambda1 0.83033663\n",
      "68 Train Loss 87.27612 Test MSE 87.78562732074231 Test RE 0.15772512458400553 Lambda1 0.8194098\n",
      "69 Train Loss 86.57654 Test MSE 87.48289097509705 Test RE 0.15745292532326008 Lambda1 0.81648517\n",
      "70 Train Loss 86.33996 Test MSE 86.76504917879916 Test RE 0.1568056038569543 Lambda1 0.8256093\n",
      "71 Train Loss 85.246544 Test MSE 83.57826008881243 Test RE 0.1538990126403411 Lambda1 0.8427553\n",
      "72 Train Loss 82.89015 Test MSE 79.33659780484462 Test RE 0.14994291754465663 Lambda1 0.86770546\n",
      "73 Train Loss 81.17763 Test MSE 74.04753648033773 Test RE 0.14485866477896142 Lambda1 0.89296407\n",
      "74 Train Loss 79.702446 Test MSE 71.74306038515184 Test RE 0.14258673351291146 Lambda1 0.9111361\n",
      "Training time: 121.78\n",
      "Training time: 121.78\n",
      "inv_HT_stan_tune14\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 837.9847 Test MSE 858.2088064656798 Test RE 0.49315751325532436 Lambda1 -0.00834937\n",
      "1 Train Loss 837.29614 Test MSE 856.9356363102155 Test RE 0.4927915729027453 Lambda1 -0.0065615615\n",
      "2 Train Loss 828.49445 Test MSE 845.3440388213187 Test RE 0.48944727832062945 Lambda1 0.11534543\n",
      "3 Train Loss 729.15106 Test MSE 730.0133976767667 Test RE 0.45483574281272554 Lambda1 0.21163979\n",
      "4 Train Loss 636.4635 Test MSE 622.6630252848055 Test RE 0.4200642377129571 Lambda1 0.102363676\n",
      "5 Train Loss 502.352 Test MSE 493.4215212391367 Test RE 0.37393680635748183 Lambda1 0.052050382\n",
      "6 Train Loss 333.24493 Test MSE 327.6207551759217 Test RE 0.3047016978239588 Lambda1 0.004801337\n",
      "7 Train Loss 276.9709 Test MSE 296.22544353727085 Test RE 0.28973459216187775 Lambda1 0.0005510672\n",
      "8 Train Loss 264.30957 Test MSE 286.7606922111724 Test RE 0.285068335990068 Lambda1 -1.8939616e-05\n",
      "9 Train Loss 262.3787 Test MSE 286.1786445834156 Test RE 0.2847788827389806 Lambda1 8.090094e-05\n",
      "10 Train Loss 258.78558 Test MSE 282.73790640169 Test RE 0.28306175145319196 Lambda1 0.00034477227\n",
      "11 Train Loss 256.95578 Test MSE 282.50024026459283 Test RE 0.28294275726295093 Lambda1 0.000114667106\n",
      "12 Train Loss 256.087 Test MSE 282.00744210754556 Test RE 0.2826958644994243 Lambda1 0.00017292405\n",
      "13 Train Loss 255.38219 Test MSE 282.54833031367787 Test RE 0.28296683892754115 Lambda1 0.00012805678\n",
      "14 Train Loss 255.0559 Test MSE 282.31709809379987 Test RE 0.2828510278685816 Lambda1 0.00010167678\n",
      "15 Train Loss 254.74031 Test MSE 282.02733851626135 Test RE 0.2827058368129481 Lambda1 6.464088e-05\n",
      "16 Train Loss 254.52565 Test MSE 282.3698517603517 Test RE 0.2828774533524094 Lambda1 8.079624e-05\n",
      "17 Train Loss 254.36058 Test MSE 282.13231434124936 Test RE 0.2827584461052059 Lambda1 7.973236e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 Train Loss 254.1815 Test MSE 282.83570025184827 Test RE 0.2831107001520513 Lambda1 9.123112e-05\n",
      "19 Train Loss 254.09084 Test MSE 283.124084088417 Test RE 0.2832549954954068 Lambda1 0.000104980674\n",
      "20 Train Loss 254.04271 Test MSE 283.27460484452035 Test RE 0.2833302806676161 Lambda1 0.0001268821\n",
      "21 Train Loss 253.96165 Test MSE 283.1602617511652 Test RE 0.28327309211264723 Lambda1 0.00013308307\n",
      "22 Train Loss 253.94095 Test MSE 283.18336707092755 Test RE 0.2832846491401432 Lambda1 0.00014547426\n",
      "23 Train Loss 253.91357 Test MSE 282.95003659119357 Test RE 0.28316791812407516 Lambda1 0.00020146782\n",
      "24 Train Loss 253.8992 Test MSE 282.78950400131424 Test RE 0.2830875786244168 Lambda1 0.00021817145\n",
      "25 Train Loss 253.8526 Test MSE 282.8223014340822 Test RE 0.2831039941499407 Lambda1 0.0002550678\n",
      "26 Train Loss 253.80809 Test MSE 282.7570610139017 Test RE 0.2830713395666559 Lambda1 0.00029471796\n",
      "27 Train Loss 253.79181 Test MSE 282.784315424892 Test RE 0.28308498158978046 Lambda1 0.00030293158\n",
      "28 Train Loss 253.74675 Test MSE 282.6528880382272 Test RE 0.2830191903821525 Lambda1 0.00033653073\n",
      "29 Train Loss 253.71895 Test MSE 282.7226384138493 Test RE 0.28305410861219377 Lambda1 0.0003476179\n",
      "30 Train Loss 253.68925 Test MSE 282.76641241587447 Test RE 0.2830760204258848 Lambda1 0.00036510426\n",
      "31 Train Loss 253.53569 Test MSE 282.55661910120216 Test RE 0.28297098942951104 Lambda1 0.0005149505\n",
      "32 Train Loss 253.50073 Test MSE 282.3132252781751 Test RE 0.28284908779190115 Lambda1 0.00061942375\n",
      "33 Train Loss 253.4065 Test MSE 282.30744064383873 Test RE 0.28284618996994276 Lambda1 0.00076811586\n",
      "34 Train Loss 253.37025 Test MSE 282.41222891294177 Test RE 0.28289867922195816 Lambda1 0.00067078904\n",
      "35 Train Loss 253.33365 Test MSE 282.34042411644197 Test RE 0.28286271269611685 Lambda1 0.0008509885\n",
      "36 Train Loss 253.02518 Test MSE 282.30023467243524 Test RE 0.2828425800853462 Lambda1 0.0010496164\n",
      "37 Train Loss 252.90726 Test MSE 282.1088562052262 Test RE 0.2827466907627028 Lambda1 0.001255395\n",
      "38 Train Loss 252.7529 Test MSE 282.00310040162213 Test RE 0.2826936883387096 Lambda1 0.0012768591\n",
      "39 Train Loss 252.5783 Test MSE 281.745644879565 Test RE 0.28256461587458936 Lambda1 0.0017057575\n",
      "40 Train Loss 252.4509 Test MSE 281.7807781296502 Test RE 0.28258223301265545 Lambda1 0.001805379\n",
      "41 Train Loss 252.33661 Test MSE 281.85763023118915 Test RE 0.2826207657295596 Lambda1 0.0021056398\n",
      "42 Train Loss 252.0986 Test MSE 281.24222551689905 Test RE 0.2823120616644645 Lambda1 0.002980551\n",
      "43 Train Loss 251.90176 Test MSE 280.7566194780174 Test RE 0.2820682297319576 Lambda1 0.0035019955\n",
      "44 Train Loss 251.70003 Test MSE 280.47975906012095 Test RE 0.28192911851518776 Lambda1 0.0041108113\n",
      "45 Train Loss 251.248 Test MSE 279.4630721326987 Test RE 0.2814176843381494 Lambda1 0.005636521\n",
      "46 Train Loss 250.9283 Test MSE 277.94326958312917 Test RE 0.2806514249784396 Lambda1 0.008139398\n",
      "47 Train Loss 250.1818 Test MSE 277.5145347779518 Test RE 0.28043488533863103 Lambda1 0.009453606\n",
      "48 Train Loss 249.38177 Test MSE 275.17017546283074 Test RE 0.27924785833084553 Lambda1 0.012293767\n",
      "49 Train Loss 248.15157 Test MSE 273.1948093915861 Test RE 0.2782437337250007 Lambda1 0.016170401\n",
      "50 Train Loss 246.8665 Test MSE 270.0884697015558 Test RE 0.2766573372946993 Lambda1 0.020964723\n",
      "51 Train Loss 245.3788 Test MSE 266.5566942822576 Test RE 0.27484254889852394 Lambda1 0.026900284\n",
      "52 Train Loss 243.28474 Test MSE 263.07218178425114 Test RE 0.2730402254975557 Lambda1 0.03230174\n",
      "53 Train Loss 239.70285 Test MSE 256.2400498377395 Test RE 0.26947139814544496 Lambda1 0.04486454\n",
      "54 Train Loss 236.43274 Test MSE 251.91608754819265 Test RE 0.2671881063630864 Lambda1 0.054127783\n",
      "55 Train Loss 231.9398 Test MSE 246.45810366991824 Test RE 0.26427782377700754 Lambda1 0.06872732\n",
      "56 Train Loss 223.62236 Test MSE 237.73324892887413 Test RE 0.25955782927664506 Lambda1 0.09195256\n",
      "57 Train Loss 215.38202 Test MSE 230.9124067969736 Test RE 0.25580722388655225 Lambda1 0.11029244\n",
      "58 Train Loss 206.83224 Test MSE 217.6423347695104 Test RE 0.24834810970315543 Lambda1 0.14160629\n",
      "59 Train Loss 200.22607 Test MSE 207.46007989304053 Test RE 0.24246912266164583 Lambda1 0.16933887\n",
      "60 Train Loss 187.70084 Test MSE 190.31175297728782 Test RE 0.23223195351075773 Lambda1 0.22263429\n",
      "61 Train Loss 178.91347 Test MSE 182.4414939458254 Test RE 0.22737932901176394 Lambda1 0.24158002\n",
      "62 Train Loss 171.10042 Test MSE 177.8698641011157 Test RE 0.22451241283178383 Lambda1 0.2551215\n",
      "63 Train Loss 165.96286 Test MSE 173.42809742039833 Test RE 0.22169142781718157 Lambda1 0.27490816\n",
      "64 Train Loss 161.34639 Test MSE 166.1849906299921 Test RE 0.2170126594063066 Lambda1 0.29923937\n",
      "65 Train Loss 155.41339 Test MSE 161.36996892985152 Test RE 0.2138457031597555 Lambda1 0.31178913\n",
      "66 Train Loss 151.15079 Test MSE 153.48092029838895 Test RE 0.20855296469082812 Lambda1 0.33202282\n",
      "67 Train Loss 146.90694 Test MSE 149.30259641069782 Test RE 0.20569458117356956 Lambda1 0.3440799\n",
      "68 Train Loss 141.66246 Test MSE 143.05906725095417 Test RE 0.20134778909398862 Lambda1 0.3554446\n",
      "69 Train Loss 136.58618 Test MSE 137.59437412221496 Test RE 0.19746471792201942 Lambda1 0.35835448\n",
      "70 Train Loss 132.43954 Test MSE 132.63673032513816 Test RE 0.1938746712193155 Lambda1 0.3736797\n",
      "71 Train Loss 128.1887 Test MSE 127.95948148096011 Test RE 0.1904256323391453 Lambda1 0.3851847\n",
      "72 Train Loss 124.30713 Test MSE 121.33528560371376 Test RE 0.1854311659204603 Lambda1 0.40182757\n",
      "73 Train Loss 120.71717 Test MSE 117.65019651337262 Test RE 0.18259357808443472 Lambda1 0.41381434\n",
      "74 Train Loss 117.73901 Test MSE 111.29753383584858 Test RE 0.1775954934648627 Lambda1 0.43207914\n",
      "Training time: 121.27\n",
      "Training time: 121.27\n",
      "inv_HT_stan_tune14\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 837.47736 Test MSE 857.3276184440983 Test RE 0.49290426713530155 Lambda1 -0.061089184\n",
      "1 Train Loss 826.7232 Test MSE 839.916301766213 Test RE 0.4878734402887046 Lambda1 -0.026424322\n",
      "2 Train Loss 730.3296 Test MSE 720.0609130128339 Test RE 0.4517246489235988 Lambda1 0.024486141\n",
      "3 Train Loss 538.6536 Test MSE 535.6348463181885 Test RE 0.38960415695285167 Lambda1 0.0004513656\n",
      "4 Train Loss 379.8789 Test MSE 379.20889694599526 Test RE 0.32781470394173734 Lambda1 0.00013548344\n",
      "5 Train Loss 278.0111 Test MSE 303.80500120510055 Test RE 0.29341791727436334 Lambda1 0.00038117994\n",
      "6 Train Loss 261.99518 Test MSE 285.84654721929616 Test RE 0.2846135982718482 Lambda1 0.0014034678\n",
      "7 Train Loss 257.6368 Test MSE 281.36475868229604 Test RE 0.28237355460605573 Lambda1 0.0012774388\n",
      "8 Train Loss 256.37973 Test MSE 280.3929379791797 Test RE 0.28188548027653826 Lambda1 0.0013799904\n",
      "9 Train Loss 255.62183 Test MSE 280.32045607524105 Test RE 0.2818490440577466 Lambda1 0.0017775148\n",
      "10 Train Loss 255.3881 Test MSE 280.645282168736 Test RE 0.282012295464255 Lambda1 0.001645574\n",
      "11 Train Loss 255.2638 Test MSE 280.8190475840263 Test RE 0.28209958786452843 Lambda1 0.0018126847\n",
      "12 Train Loss 255.02632 Test MSE 281.5625609350684 Test RE 0.2824727928909642 Lambda1 0.0013942674\n",
      "13 Train Loss 254.75566 Test MSE 281.2625116153446 Test RE 0.28232224311391735 Lambda1 0.0018350021\n",
      "14 Train Loss 254.4227 Test MSE 281.30975552189045 Test RE 0.2823459530735932 Lambda1 0.0028609084\n",
      "15 Train Loss 253.9098 Test MSE 280.99880874062023 Test RE 0.28218986385518013 Lambda1 0.004088438\n",
      "16 Train Loss 253.41058 Test MSE 279.75015154233944 Test RE 0.2815621908823654 Lambda1 0.005677569\n",
      "17 Train Loss 252.88129 Test MSE 278.17804119019627 Test RE 0.28076992951778745 Lambda1 0.008834103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 Train Loss 251.79155 Test MSE 277.6545557059909 Test RE 0.28050562361750336 Lambda1 0.010822369\n",
      "19 Train Loss 250.44264 Test MSE 274.86808376859636 Test RE 0.2790945320771078 Lambda1 0.017056705\n",
      "20 Train Loss 249.34312 Test MSE 273.49550866481906 Test RE 0.278396819866321 Lambda1 0.02037838\n",
      "21 Train Loss 247.67049 Test MSE 269.7835790418154 Test RE 0.2765011402226418 Lambda1 0.030592384\n",
      "22 Train Loss 245.19164 Test MSE 267.37580410849347 Test RE 0.2752645108174674 Lambda1 0.046097107\n",
      "23 Train Loss 238.83849 Test MSE 258.8944602886022 Test RE 0.27086353961366616 Lambda1 0.06947986\n",
      "24 Train Loss 235.9978 Test MSE 256.3339644227659 Test RE 0.2695207756259149 Lambda1 0.079210274\n",
      "25 Train Loss 231.43413 Test MSE 249.72153943162243 Test RE 0.2660217661034497 Lambda1 0.10575976\n",
      "26 Train Loss 226.49844 Test MSE 247.5612199532601 Test RE 0.2648686010373021 Lambda1 0.14030895\n",
      "27 Train Loss 224.08446 Test MSE 244.58704541128964 Test RE 0.263272741630358 Lambda1 0.15605117\n",
      "28 Train Loss 219.927 Test MSE 243.4676038299691 Test RE 0.26266956894558136 Lambda1 0.17998827\n",
      "29 Train Loss 216.6188 Test MSE 243.23368059000885 Test RE 0.2625433523942213 Lambda1 0.21869479\n",
      "30 Train Loss 214.25977 Test MSE 241.26516611396437 Test RE 0.26147879934556 Lambda1 0.23223904\n",
      "31 Train Loss 212.98811 Test MSE 240.31784817834637 Test RE 0.26096495147345644 Lambda1 0.24134311\n",
      "32 Train Loss 211.72601 Test MSE 239.80697919003592 Test RE 0.26068742354601493 Lambda1 0.25224924\n",
      "33 Train Loss 210.11307 Test MSE 237.43241799686086 Test RE 0.2593935532624155 Lambda1 0.27941358\n",
      "34 Train Loss 207.92937 Test MSE 235.57549643418324 Test RE 0.2583772241522347 Lambda1 0.30071256\n",
      "35 Train Loss 202.7636 Test MSE 224.60639170481664 Test RE 0.25229011000236184 Lambda1 0.3280853\n",
      "36 Train Loss 184.78453 Test MSE 196.3093350597871 Test RE 0.23586290685933062 Lambda1 0.37840554\n",
      "37 Train Loss 176.4773 Test MSE 184.39643338234913 Test RE 0.22859431678266423 Lambda1 0.38070047\n",
      "38 Train Loss 169.63893 Test MSE 176.99054306531045 Test RE 0.22395677322450666 Lambda1 0.40448475\n",
      "39 Train Loss 163.05211 Test MSE 171.07878426756974 Test RE 0.22018475602775545 Lambda1 0.44152418\n",
      "40 Train Loss 159.01482 Test MSE 166.0736925191493 Test RE 0.2169399779196608 Lambda1 0.47540182\n",
      "41 Train Loss 151.06602 Test MSE 154.74451983517702 Test RE 0.20940970717355703 Lambda1 0.53120357\n",
      "42 Train Loss 143.64914 Test MSE 149.31335072460547 Test RE 0.20570198916350074 Lambda1 0.5647253\n",
      "43 Train Loss 138.42334 Test MSE 140.29041419551885 Test RE 0.19938990633538908 Lambda1 0.6233904\n",
      "44 Train Loss 134.40396 Test MSE 135.08502847423313 Test RE 0.19565582403788667 Lambda1 0.6494751\n",
      "45 Train Loss 128.91577 Test MSE 130.96327849416886 Test RE 0.19264774946602725 Lambda1 0.6895908\n",
      "46 Train Loss 125.43686 Test MSE 127.37779097625797 Test RE 0.18999231175324302 Lambda1 0.711162\n",
      "47 Train Loss 122.01784 Test MSE 124.2613302987434 Test RE 0.1876537164600684 Lambda1 0.74702007\n",
      "48 Train Loss 120.19328 Test MSE 120.91088309743694 Test RE 0.18510658437721456 Lambda1 0.7834485\n",
      "49 Train Loss 118.0747 Test MSE 116.61635375121618 Test RE 0.18178954377617204 Lambda1 0.8475287\n",
      "50 Train Loss 114.17921 Test MSE 111.57401344414615 Test RE 0.17781594343674895 Lambda1 0.9400663\n",
      "51 Train Loss 111.50058 Test MSE 110.35192060235599 Test RE 0.17683943501955926 Lambda1 0.9816039\n",
      "52 Train Loss 109.024506 Test MSE 108.93157553348911 Test RE 0.17569769468210664 Lambda1 1.0284555\n",
      "53 Train Loss 107.17074 Test MSE 106.75157505002339 Test RE 0.17393072865538872 Lambda1 1.0322654\n",
      "54 Train Loss 104.93146 Test MSE 106.14616306699041 Test RE 0.17343682743318056 Lambda1 1.053413\n",
      "55 Train Loss 103.16077 Test MSE 104.26850672147285 Test RE 0.171895990791248 Lambda1 1.1055746\n",
      "56 Train Loss 99.467964 Test MSE 98.35564899149006 Test RE 0.16695092315407348 Lambda1 1.1713506\n",
      "57 Train Loss 98.53588 Test MSE 97.85594082884955 Test RE 0.16652627558704688 Lambda1 1.177722\n",
      "58 Train Loss 96.78141 Test MSE 95.82754845689651 Test RE 0.16479133025202677 Lambda1 1.215738\n",
      "59 Train Loss 95.044235 Test MSE 92.17904033231676 Test RE 0.1616237808048672 Lambda1 1.2753465\n",
      "60 Train Loss 93.82275 Test MSE 91.54059872389992 Test RE 0.1610630967317119 Lambda1 1.2930257\n",
      "61 Train Loss 92.72905 Test MSE 90.64845590197612 Test RE 0.16027632505185885 Lambda1 1.3058\n",
      "62 Train Loss 91.23876 Test MSE 90.36822847859929 Test RE 0.16002839701957303 Lambda1 1.3305174\n",
      "63 Train Loss 90.63766 Test MSE 88.61475943083158 Test RE 0.1584682281411256 Lambda1 1.336597\n",
      "64 Train Loss 88.07893 Test MSE 86.37885877331097 Test RE 0.1564562445518596 Lambda1 1.3329287\n",
      "65 Train Loss 86.41169 Test MSE 83.79742071830564 Test RE 0.15410065911177706 Lambda1 1.3320194\n",
      "66 Train Loss 85.80768 Test MSE 83.21807277437075 Test RE 0.15356703442181707 Lambda1 1.3468579\n",
      "67 Train Loss 83.76299 Test MSE 79.09557080909954 Test RE 0.1497149787210248 Lambda1 1.3634535\n",
      "68 Train Loss 82.12321 Test MSE 76.86439031002415 Test RE 0.14758824346535127 Lambda1 1.3661461\n",
      "69 Train Loss 81.28249 Test MSE 76.50408119051899 Test RE 0.1472419201884495 Lambda1 1.3451566\n",
      "70 Train Loss 80.591995 Test MSE 77.1357641226233 Test RE 0.14784854797936742 Lambda1 1.337651\n",
      "71 Train Loss 79.58131 Test MSE 76.87954998306634 Test RE 0.1476027968833885 Lambda1 1.326007\n",
      "72 Train Loss 78.985535 Test MSE 76.1643647916745 Test RE 0.1469146425664991 Lambda1 1.3172734\n",
      "73 Train Loss 77.22277 Test MSE 74.39444135198045 Test RE 0.14519759203995555 Lambda1 1.2934718\n",
      "74 Train Loss 76.47883 Test MSE 74.11303847446155 Test RE 0.144922721156888 Lambda1 1.2966273\n",
      "Training time: 121.46\n",
      "Training time: 121.46\n",
      "inv_HT_stan_tune14\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 837.9155 Test MSE 857.1378453305157 Test RE 0.49284971088842405 Lambda1 -0.18276495\n",
      "1 Train Loss 830.5162 Test MSE 846.467724414732 Test RE 0.48977247266914875 Lambda1 -0.19856937\n",
      "2 Train Loss 744.6811 Test MSE 732.3102582435329 Test RE 0.4555507117695861 Lambda1 -0.00415246\n",
      "3 Train Loss 585.89124 Test MSE 572.2388263946618 Test RE 0.4026964780338016 Lambda1 0.0018715978\n",
      "4 Train Loss 453.05267 Test MSE 455.3057031391291 Test RE 0.35920362910478115 Lambda1 0.0045163794\n",
      "5 Train Loss 363.1849 Test MSE 387.7758583012809 Test RE 0.33149696341300233 Lambda1 0.0032118063\n",
      "6 Train Loss 287.15256 Test MSE 301.88476008487515 Test RE 0.29248915328617675 Lambda1 0.0032301727\n",
      "7 Train Loss 272.3412 Test MSE 296.538741163463 Test RE 0.2898877680173876 Lambda1 0.0025954184\n",
      "8 Train Loss 263.3807 Test MSE 287.6942429548967 Test RE 0.2855319795833819 Lambda1 0.002126368\n",
      "9 Train Loss 258.13174 Test MSE 283.38233245014754 Test RE 0.28338414993596595 Lambda1 0.0011003435\n",
      "10 Train Loss 256.2217 Test MSE 281.499833263373 Test RE 0.28244132590981313 Lambda1 0.001086175\n",
      "11 Train Loss 254.93805 Test MSE 281.5922133963589 Test RE 0.28248766666007047 Lambda1 0.0006795483\n",
      "12 Train Loss 254.6948 Test MSE 281.7318572110943 Test RE 0.2825577019169465 Lambda1 0.00048187745\n",
      "13 Train Loss 254.44743 Test MSE 282.2871944455626 Test RE 0.2828360473702523 Lambda1 0.00038961123\n",
      "14 Train Loss 254.30475 Test MSE 282.32876958625496 Test RE 0.2828568745911859 Lambda1 0.00056701375\n",
      "15 Train Loss 254.20639 Test MSE 282.6242171403087 Test RE 0.28300483599123405 Lambda1 0.0006850164\n",
      "16 Train Loss 254.09433 Test MSE 282.8354919926801 Test RE 0.28311059592120336 Lambda1 0.00081443985\n",
      "17 Train Loss 253.99672 Test MSE 282.689937137794 Test RE 0.28303773833093776 Lambda1 0.0010400923\n",
      "18 Train Loss 253.95131 Test MSE 282.38073188038305 Test RE 0.28288290313924147 Lambda1 0.0012107698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 Train Loss 253.83734 Test MSE 282.1918840958236 Test RE 0.2827882955080716 Lambda1 0.0014534892\n",
      "20 Train Loss 253.79561 Test MSE 282.0024145829618 Test RE 0.28269334458943124 Lambda1 0.001584036\n",
      "21 Train Loss 253.60193 Test MSE 280.83071015163375 Test RE 0.28210544567828 Lambda1 0.002305619\n",
      "22 Train Loss 253.5054 Test MSE 280.7982296438972 Test RE 0.2820891312355607 Lambda1 0.0026514586\n",
      "23 Train Loss 253.26834 Test MSE 280.7040506206867 Test RE 0.282041821274131 Lambda1 0.0032311024\n",
      "24 Train Loss 252.97942 Test MSE 279.4566428490321 Test RE 0.28141444719394665 Lambda1 0.0046566105\n",
      "25 Train Loss 252.60237 Test MSE 279.05004127053076 Test RE 0.2812096476079563 Lambda1 0.006692909\n",
      "26 Train Loss 252.0145 Test MSE 277.94769199628286 Test RE 0.2806536577211469 Lambda1 0.008365622\n",
      "27 Train Loss 250.86801 Test MSE 275.9156513933683 Test RE 0.2796258639685137 Lambda1 0.012442813\n",
      "28 Train Loss 249.15996 Test MSE 270.6296918144088 Test RE 0.27693439120311547 Lambda1 0.021434043\n",
      "29 Train Loss 247.43376 Test MSE 267.97942757403877 Test RE 0.2755750520916737 Lambda1 0.026355226\n",
      "30 Train Loss 244.20634 Test MSE 264.1343980434653 Test RE 0.27359090248397694 Lambda1 0.038316116\n",
      "31 Train Loss 241.62027 Test MSE 262.4238820797833 Test RE 0.27270358578650666 Lambda1 0.04420412\n",
      "32 Train Loss 238.02425 Test MSE 259.50687822739195 Test RE 0.27118371584979367 Lambda1 0.05242156\n",
      "33 Train Loss 233.3717 Test MSE 252.05959137899856 Test RE 0.267264197291398 Lambda1 0.0777052\n",
      "34 Train Loss 229.97112 Test MSE 251.05484884834593 Test RE 0.2667309903624714 Lambda1 0.085228294\n",
      "35 Train Loss 225.82672 Test MSE 245.59440366050327 Test RE 0.2638143431990966 Lambda1 0.1053955\n",
      "36 Train Loss 222.82079 Test MSE 241.583437571025 Test RE 0.26165121091246873 Lambda1 0.119408675\n",
      "37 Train Loss 218.36385 Test MSE 235.6792381300063 Test RE 0.258434109400149 Lambda1 0.1502528\n",
      "38 Train Loss 213.09035 Test MSE 225.75548677012586 Test RE 0.2529346497057283 Lambda1 0.1714304\n",
      "39 Train Loss 203.03636 Test MSE 211.60412834396823 Test RE 0.24487882845126782 Lambda1 0.18389541\n",
      "40 Train Loss 197.5531 Test MSE 200.28057493963115 Test RE 0.23823665647291384 Lambda1 0.20819154\n",
      "41 Train Loss 192.1666 Test MSE 189.47024860368575 Test RE 0.23171795290915645 Lambda1 0.24057351\n",
      "42 Train Loss 184.3976 Test MSE 182.1974191205626 Test RE 0.22722718121157237 Lambda1 0.25855377\n",
      "43 Train Loss 174.17004 Test MSE 181.76297859819246 Test RE 0.22695611367317828 Lambda1 0.27436912\n",
      "44 Train Loss 163.52289 Test MSE 171.3113229132731 Test RE 0.22033434815316583 Lambda1 0.30998942\n",
      "45 Train Loss 154.71048 Test MSE 160.57324269342385 Test RE 0.21331714353187983 Lambda1 0.3374629\n",
      "46 Train Loss 149.52995 Test MSE 154.73371520020956 Test RE 0.20940239630095428 Lambda1 0.36050734\n",
      "47 Train Loss 143.4693 Test MSE 145.41790634052933 Test RE 0.2030009706145673 Lambda1 0.40148836\n",
      "48 Train Loss 138.15913 Test MSE 137.9765432912683 Test RE 0.19773875746478767 Lambda1 0.44059902\n",
      "49 Train Loss 134.81859 Test MSE 135.55238940566386 Test RE 0.19599399227945374 Lambda1 0.44969684\n",
      "50 Train Loss 127.894295 Test MSE 130.82270556636888 Test RE 0.19254432990661047 Lambda1 0.49785566\n",
      "51 Train Loss 123.6943 Test MSE 125.96491320495049 Test RE 0.18893567368618958 Lambda1 0.512191\n",
      "52 Train Loss 121.10498 Test MSE 122.56830823744838 Test RE 0.18637097039654404 Lambda1 0.5114524\n",
      "53 Train Loss 117.19266 Test MSE 113.85455074628237 Test RE 0.17962400176461674 Lambda1 0.54762256\n",
      "54 Train Loss 115.29904 Test MSE 113.54724477897713 Test RE 0.17938142549074626 Lambda1 0.55063075\n",
      "55 Train Loss 110.55096 Test MSE 112.45909977886488 Test RE 0.17851983303691354 Lambda1 0.58566564\n",
      "56 Train Loss 105.30575 Test MSE 102.19675174406962 Test RE 0.17017968551371918 Lambda1 0.6411299\n",
      "57 Train Loss 103.4227 Test MSE 99.48027925954415 Test RE 0.16790269556655363 Lambda1 0.65938836\n",
      "58 Train Loss 100.39086 Test MSE 96.46355230234705 Test RE 0.16533728282292218 Lambda1 0.670506\n",
      "59 Train Loss 98.373276 Test MSE 94.36388229573254 Test RE 0.1635279794871554 Lambda1 0.69445723\n",
      "60 Train Loss 96.56548 Test MSE 91.53230866235342 Test RE 0.16105580350206988 Lambda1 0.7046698\n",
      "61 Train Loss 93.66764 Test MSE 90.40380068517574 Test RE 0.16005989040614924 Lambda1 0.7368136\n",
      "62 Train Loss 91.203125 Test MSE 87.8293096504017 Test RE 0.15776436189548748 Lambda1 0.74170256\n",
      "63 Train Loss 86.514275 Test MSE 84.085310489182 Test RE 0.1543651419845753 Lambda1 0.78389615\n",
      "64 Train Loss 84.07014 Test MSE 80.77795455066986 Test RE 0.15129883937967303 Lambda1 0.7959081\n",
      "65 Train Loss 81.127525 Test MSE 78.23789733364605 Test RE 0.1489010484377243 Lambda1 0.81534284\n",
      "66 Train Loss 77.68713 Test MSE 75.77631955532028 Test RE 0.14653991137478561 Lambda1 0.8575006\n",
      "67 Train Loss 72.521774 Test MSE 68.75147474696716 Test RE 0.13958224523070645 Lambda1 0.89318806\n",
      "68 Train Loss 70.595215 Test MSE 68.10034605686525 Test RE 0.13891969784691038 Lambda1 0.88795555\n",
      "69 Train Loss 68.55691 Test MSE 66.61463647150738 Test RE 0.1373959722906938 Lambda1 0.90369755\n",
      "70 Train Loss 67.041794 Test MSE 65.4312330633975 Test RE 0.136170089527891 Lambda1 0.9051019\n",
      "71 Train Loss 65.61418 Test MSE 64.02813537673127 Test RE 0.1347021717965019 Lambda1 0.9028707\n",
      "72 Train Loss 64.292206 Test MSE 62.90557616664687 Test RE 0.13351613220485092 Lambda1 0.908296\n",
      "73 Train Loss 62.89359 Test MSE 61.48187699514981 Test RE 0.13199659537993527 Lambda1 0.9117725\n",
      "74 Train Loss 62.24154 Test MSE 61.68293766034213 Test RE 0.1322122496707952 Lambda1 0.91408145\n",
      "Training time: 120.72\n",
      "Training time: 120.72\n",
      "inv_HT_stan_tune14\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 837.8901 Test MSE 857.9966347257191 Test RE 0.4930965487471846 Lambda1 -0.04346912\n",
      "1 Train Loss 836.7108 Test MSE 856.5263954408724 Test RE 0.49267388930639466 Lambda1 -0.032103617\n",
      "2 Train Loss 826.7302 Test MSE 847.9750249181457 Test RE 0.49020834631391674 Lambda1 0.03290103\n",
      "3 Train Loss 756.40924 Test MSE 747.7856208080185 Test RE 0.4603389529308774 Lambda1 0.08273828\n",
      "4 Train Loss 617.08765 Test MSE 620.9853174629122 Test RE 0.4194979439177144 Lambda1 0.10460803\n",
      "5 Train Loss 520.02875 Test MSE 518.9269805941315 Test RE 0.3834796271289811 Lambda1 0.04268059\n",
      "6 Train Loss 417.35855 Test MSE 424.2316592281238 Test RE 0.3467294322180563 Lambda1 -0.00080598483\n",
      "7 Train Loss 355.3702 Test MSE 368.2385265116854 Test RE 0.32303812635938 Lambda1 -0.00026865897\n",
      "8 Train Loss 284.56735 Test MSE 304.61229928148913 Test RE 0.2938075069224911 Lambda1 0.0006105262\n",
      "9 Train Loss 270.10236 Test MSE 287.1884684619875 Test RE 0.28528088258912226 Lambda1 0.0015550362\n",
      "10 Train Loss 265.6456 Test MSE 285.99061628937665 Test RE 0.28468531306347894 Lambda1 7.832671e-05\n",
      "11 Train Loss 262.005 Test MSE 283.11107602231107 Test RE 0.2832484883802649 Lambda1 -4.4707913e-06\n",
      "12 Train Loss 258.95325 Test MSE 281.92242435447326 Test RE 0.2826532486402261 Lambda1 0.00022506199\n",
      "13 Train Loss 256.70535 Test MSE 281.6187560790641 Test RE 0.28250097989010203 Lambda1 9.913501e-06\n",
      "14 Train Loss 255.96454 Test MSE 281.61972345521133 Test RE 0.28250146509300295 Lambda1 3.234647e-06\n",
      "15 Train Loss 255.60309 Test MSE 281.70997895263775 Test RE 0.2825467305086851 Lambda1 -8.3760915e-06\n",
      "16 Train Loss 255.15433 Test MSE 282.12207367190456 Test RE 0.2827533143604884 Lambda1 7.631515e-05\n",
      "17 Train Loss 254.92899 Test MSE 282.3148604321566 Test RE 0.2828499069195596 Lambda1 7.0427915e-05\n",
      "18 Train Loss 254.82744 Test MSE 282.21024215786616 Test RE 0.2827974937893414 Lambda1 7.614601e-05\n",
      "19 Train Loss 254.79823 Test MSE 282.09151945096875 Test RE 0.2827380026533082 Lambda1 6.685201e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Train Loss 254.7056 Test MSE 282.2904124989816 Test RE 0.2828376595208089 Lambda1 5.2954685e-05\n",
      "21 Train Loss 254.63754 Test MSE 282.6167420842267 Test RE 0.2830010934050005 Lambda1 4.7907197e-05\n",
      "22 Train Loss 254.57967 Test MSE 283.0469770550879 Test RE 0.28321642152577775 Lambda1 4.1247815e-05\n",
      "23 Train Loss 254.53249 Test MSE 283.1128685010786 Test RE 0.2832493850532331 Lambda1 4.62533e-05\n",
      "24 Train Loss 254.4873 Test MSE 282.8922365488804 Test RE 0.28313899437169426 Lambda1 6.394227e-05\n",
      "25 Train Loss 254.46309 Test MSE 282.8645738270674 Test RE 0.2831251506076976 Lambda1 8.321507e-05\n",
      "26 Train Loss 254.41861 Test MSE 283.08018181967253 Test RE 0.2832330333598153 Lambda1 6.364629e-05\n",
      "27 Train Loss 254.34796 Test MSE 283.21641442555136 Test RE 0.28330117824501755 Lambda1 6.903369e-05\n",
      "28 Train Loss 254.31633 Test MSE 283.4744312786365 Test RE 0.2834301959046369 Lambda1 7.529796e-05\n",
      "29 Train Loss 254.27316 Test MSE 283.1671076317041 Test RE 0.28327651639615264 Lambda1 7.160412e-05\n",
      "30 Train Loss 254.2381 Test MSE 283.00997807446 Test RE 0.2831979103558989 Lambda1 7.423599e-05\n",
      "31 Train Loss 254.2155 Test MSE 282.9865524140918 Test RE 0.28318618950528207 Lambda1 6.1158134e-05\n",
      "32 Train Loss 254.1806 Test MSE 282.9073081257532 Test RE 0.28314653663290174 Lambda1 6.5155284e-05\n",
      "33 Train Loss 254.16862 Test MSE 282.68688286668197 Test RE 0.2830362093123749 Lambda1 7.7523604e-05\n",
      "34 Train Loss 254.0673 Test MSE 282.6416151655478 Test RE 0.2830135465850338 Lambda1 0.0001625555\n",
      "35 Train Loss 253.94678 Test MSE 282.8825116461417 Test RE 0.28313412763713863 Lambda1 0.00021619664\n",
      "36 Train Loss 253.8997 Test MSE 283.2438215270127 Test RE 0.2833148855654736 Lambda1 0.00026367203\n",
      "37 Train Loss 253.86963 Test MSE 283.41293064437315 Test RE 0.2833994487182401 Lambda1 0.000287752\n",
      "38 Train Loss 253.75537 Test MSE 283.5977438322805 Test RE 0.2834918358574933 Lambda1 0.0002913421\n",
      "39 Train Loss 253.62622 Test MSE 283.30567862401773 Test RE 0.2833458201849019 Lambda1 0.00030460066\n",
      "40 Train Loss 253.59302 Test MSE 283.192129359796 Test RE 0.2832890318176155 Lambda1 0.00034629894\n",
      "41 Train Loss 253.47012 Test MSE 282.82972048948636 Test RE 0.2831077073480035 Lambda1 0.00036669834\n",
      "42 Train Loss 253.38014 Test MSE 282.8423392339204 Test RE 0.28311402285111564 Lambda1 0.00037955714\n",
      "43 Train Loss 253.15372 Test MSE 283.0086844650282 Test RE 0.28319726312093385 Lambda1 0.00033156105\n",
      "44 Train Loss 252.97745 Test MSE 283.3017665193924 Test RE 0.2833438638486827 Lambda1 0.00028431517\n",
      "45 Train Loss 252.76584 Test MSE 283.02085392524833 Test RE 0.28320335184006046 Lambda1 0.00033270798\n",
      "46 Train Loss 252.59424 Test MSE 283.59547253119774 Test RE 0.2834907006287813 Lambda1 0.0002863153\n",
      "47 Train Loss 252.45468 Test MSE 283.55415058317936 Test RE 0.28347004653540003 Lambda1 0.00023663504\n",
      "48 Train Loss 252.15108 Test MSE 283.1716470078644 Test RE 0.2832787869520846 Lambda1 0.00020015583\n",
      "49 Train Loss 251.72804 Test MSE 283.6148602672864 Test RE 0.28350039074989847 Lambda1 8.862566e-05\n",
      "50 Train Loss 251.35043 Test MSE 283.9894554035025 Test RE 0.28368755094367043 Lambda1 6.5412976e-05\n",
      "51 Train Loss 250.97017 Test MSE 284.4655800496181 Test RE 0.28392526059488954 Lambda1 2.1813521e-05\n",
      "52 Train Loss 250.31061 Test MSE 285.20722131087587 Test RE 0.2842951359594217 Lambda1 5.748948e-07\n",
      "53 Train Loss 250.15547 Test MSE 285.21545215233306 Test RE 0.28429923818954417 Lambda1 -9.740324e-07\n",
      "54 Train Loss 249.72226 Test MSE 286.258286697171 Test RE 0.2848185062658624 Lambda1 1.0601809e-07\n",
      "55 Train Loss 249.53928 Test MSE 286.7224189817338 Test RE 0.2850493116778131 Lambda1 9.706591e-07\n",
      "56 Train Loss 249.41205 Test MSE 287.2758525006708 Test RE 0.28532428109288044 Lambda1 1.6600818e-06\n",
      "57 Train Loss 249.30075 Test MSE 288.01003648154875 Test RE 0.2856886466397716 Lambda1 1.173659e-07\n",
      "58 Train Loss 249.21191 Test MSE 287.58030516524997 Test RE 0.2854754332583779 Lambda1 1.7890002e-06\n",
      "59 Train Loss 249.03165 Test MSE 288.1347957742484 Test RE 0.2857505168004466 Lambda1 4.510387e-06\n",
      "60 Train Loss 248.81009 Test MSE 288.41989992285977 Test RE 0.28589185432029335 Lambda1 -4.148807e-07\n",
      "61 Train Loss 248.66092 Test MSE 288.2520457489011 Test RE 0.2858086507599461 Lambda1 2.531218e-07\n",
      "62 Train Loss 248.5396 Test MSE 288.34997082966214 Test RE 0.2858571941424044 Lambda1 6.0559023e-07\n",
      "63 Train Loss 248.37645 Test MSE 287.75512390412956 Test RE 0.28556218967416086 Lambda1 -6.3960047e-06\n",
      "64 Train Loss 248.21478 Test MSE 288.54219380756234 Test RE 0.2859524588765124 Lambda1 -2.3926145e-06\n",
      "65 Train Loss 248.13844 Test MSE 288.75646380482686 Test RE 0.2860586126093571 Lambda1 -8.245365e-07\n",
      "66 Train Loss 248.00668 Test MSE 289.27451486405965 Test RE 0.2863151030745511 Lambda1 -9.528695e-07\n",
      "67 Train Loss 247.95624 Test MSE 289.1135709346327 Test RE 0.286235443293537 Lambda1 -3.231853e-07\n",
      "68 Train Loss 247.7941 Test MSE 289.48271515546764 Test RE 0.28641811969495123 Lambda1 -1.2550474e-06\n",
      "69 Train Loss 247.70538 Test MSE 289.22350524020817 Test RE 0.28628985807567003 Lambda1 -6.0294036e-07\n",
      "70 Train Loss 247.67625 Test MSE 289.163411703224 Test RE 0.28626011453199784 Lambda1 -1.2142993e-06\n",
      "71 Train Loss 247.58624 Test MSE 288.8278404012234 Test RE 0.2860939652873023 Lambda1 5.6788684e-07\n",
      "72 Train Loss 247.53165 Test MSE 289.31668844767904 Test RE 0.28633597337810174 Lambda1 5.477561e-07\n",
      "73 Train Loss 247.50385 Test MSE 289.3773040775329 Test RE 0.2863659673726832 Lambda1 5.302749e-07\n",
      "74 Train Loss 247.44695 Test MSE 288.83392844497035 Test RE 0.2860969804803595 Lambda1 5.9963577e-07\n",
      "Training time: 121.83\n",
      "Training time: 121.83\n",
      "inv_HT_stan_tune14\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 837.6704 Test MSE 857.6470084978699 Test RE 0.4929960722151501 Lambda1 -0.063359655\n",
      "1 Train Loss 836.74854 Test MSE 856.3870423367632 Test RE 0.4926338097309876 Lambda1 -0.058690507\n",
      "2 Train Loss 825.5204 Test MSE 841.2205701178474 Test RE 0.48825209173925405 Lambda1 0.026849002\n",
      "3 Train Loss 731.92596 Test MSE 738.9313080848995 Test RE 0.4576054671969045 Lambda1 0.17373936\n",
      "4 Train Loss 562.2924 Test MSE 505.7329945087481 Test RE 0.3785731552974572 Lambda1 0.22069788\n",
      "5 Train Loss 442.8262 Test MSE 442.19350351112115 Test RE 0.3539935504854315 Lambda1 0.19515017\n",
      "6 Train Loss 310.83093 Test MSE 305.49289926499245 Test RE 0.29423188270861567 Lambda1 0.14308004\n",
      "7 Train Loss 263.10043 Test MSE 273.74334315725173 Test RE 0.2785229292698937 Lambda1 0.098894216\n",
      "8 Train Loss 245.92151 Test MSE 261.83213378393 Test RE 0.27239594809306755 Lambda1 0.092503235\n",
      "9 Train Loss 231.88747 Test MSE 245.3295848836526 Test RE 0.26367207237835855 Lambda1 0.13171436\n",
      "10 Train Loss 221.6347 Test MSE 235.1478163709349 Test RE 0.25814257965732523 Lambda1 0.15897907\n",
      "11 Train Loss 207.37305 Test MSE 220.07251317443107 Test RE 0.24973077914315017 Lambda1 0.20606796\n",
      "12 Train Loss 198.31586 Test MSE 209.24792608004387 Test RE 0.2435116546813199 Lambda1 0.2394457\n",
      "13 Train Loss 191.91629 Test MSE 196.216947020517 Test RE 0.2358073988623882 Lambda1 0.27633983\n",
      "14 Train Loss 182.17804 Test MSE 184.5816787317783 Test RE 0.2287091113136807 Lambda1 0.30394948\n",
      "15 Train Loss 174.02747 Test MSE 175.1443994272558 Test RE 0.22278569314981128 Lambda1 0.33832785\n",
      "16 Train Loss 163.90317 Test MSE 168.78325038121793 Test RE 0.21870254844386736 Lambda1 0.37344658\n",
      "17 Train Loss 155.37611 Test MSE 158.1493106752321 Test RE 0.21170095749578569 Lambda1 0.42094815\n",
      "18 Train Loss 148.27383 Test MSE 149.4562180490823 Test RE 0.20580037643498736 Lambda1 0.44815344\n",
      "19 Train Loss 141.53659 Test MSE 140.8826064859154 Test RE 0.19981029436331826 Lambda1 0.48648506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Train Loss 137.2119 Test MSE 135.6038104969054 Test RE 0.1960311634034919 Lambda1 0.52665216\n",
      "21 Train Loss 129.7072 Test MSE 128.52303093166185 Test RE 0.1908445007278099 Lambda1 0.5351988\n",
      "22 Train Loss 122.7088 Test MSE 124.13698204634879 Test RE 0.18755980046767018 Lambda1 0.54985046\n",
      "23 Train Loss 117.65915 Test MSE 119.70713274925004 Test RE 0.18418284834052698 Lambda1 0.56763834\n",
      "24 Train Loss 114.02104 Test MSE 116.4816951601915 Test RE 0.18168455595140176 Lambda1 0.5656168\n",
      "25 Train Loss 110.491165 Test MSE 113.54087782799053 Test RE 0.17937639618000809 Lambda1 0.56836957\n",
      "26 Train Loss 108.340324 Test MSE 111.75215043422477 Test RE 0.17795783563409115 Lambda1 0.5662327\n",
      "27 Train Loss 105.1411 Test MSE 107.2520807465876 Test RE 0.17433798972576586 Lambda1 0.5717185\n",
      "28 Train Loss 102.4132 Test MSE 103.99009770329816 Test RE 0.17166634626841457 Lambda1 0.5892203\n",
      "29 Train Loss 100.04679 Test MSE 101.85404253557674 Test RE 0.1698941034319651 Lambda1 0.6088936\n",
      "30 Train Loss 99.00489 Test MSE 101.4863588348262 Test RE 0.16958717516281874 Lambda1 0.60972905\n",
      "31 Train Loss 97.0134 Test MSE 97.63460922836664 Test RE 0.1663378435328733 Lambda1 0.6396969\n",
      "32 Train Loss 94.72182 Test MSE 94.98775853599184 Test RE 0.16406766240504597 Lambda1 0.65769297\n",
      "33 Train Loss 92.23501 Test MSE 92.5554862302188 Test RE 0.1619534686464698 Lambda1 0.676245\n",
      "34 Train Loss 90.5803 Test MSE 92.36801383394953 Test RE 0.1617893660295362 Lambda1 0.6807522\n",
      "35 Train Loss 88.21033 Test MSE 89.67646736797397 Test RE 0.1594147184581358 Lambda1 0.68522775\n",
      "36 Train Loss 84.17603 Test MSE 84.1165385520497 Test RE 0.1543938038093658 Lambda1 0.7108917\n",
      "37 Train Loss 82.105316 Test MSE 81.92569164234413 Test RE 0.15236991626866325 Lambda1 0.72686404\n",
      "38 Train Loss 80.39552 Test MSE 79.57017282086822 Test RE 0.15016347892776002 Lambda1 0.7511168\n",
      "39 Train Loss 79.026306 Test MSE 77.6745074302495 Test RE 0.14836396271681762 Lambda1 0.7696356\n",
      "40 Train Loss 77.56755 Test MSE 75.31887270458309 Test RE 0.14609692538648605 Lambda1 0.7910136\n",
      "41 Train Loss 75.875824 Test MSE 74.27002840058003 Test RE 0.14507613121946525 Lambda1 0.8014374\n",
      "42 Train Loss 72.620186 Test MSE 72.95848109342352 Test RE 0.14378946326881614 Lambda1 0.82437915\n",
      "43 Train Loss 71.463684 Test MSE 71.86358045702728 Test RE 0.14270644789583123 Lambda1 0.8276923\n",
      "44 Train Loss 70.68453 Test MSE 71.17833405254011 Test RE 0.1420244382343395 Lambda1 0.8367927\n",
      "45 Train Loss 69.391365 Test MSE 70.18401840420715 Test RE 0.14102895431025025 Lambda1 0.8452887\n",
      "46 Train Loss 68.51256 Test MSE 68.29458687268675 Test RE 0.1391176752658824 Lambda1 0.86226714\n",
      "47 Train Loss 65.51823 Test MSE 62.928543702893705 Test RE 0.13354050410426832 Lambda1 0.893325\n",
      "48 Train Loss 63.847473 Test MSE 61.559059822600815 Test RE 0.13207942202372328 Lambda1 0.91437477\n",
      "49 Train Loss 62.293587 Test MSE 61.49114835211381 Test RE 0.13200654742987944 Lambda1 0.91722137\n",
      "50 Train Loss 61.125763 Test MSE 60.98759934326415 Test RE 0.13146493768809234 Lambda1 0.9141003\n",
      "51 Train Loss 59.391193 Test MSE 58.6006778519184 Test RE 0.12886663565609724 Lambda1 0.9251248\n",
      "52 Train Loss 57.36323 Test MSE 56.73680738815017 Test RE 0.12680069026143245 Lambda1 0.9386874\n",
      "53 Train Loss 56.196285 Test MSE 55.58738486267257 Test RE 0.12550970016856655 Lambda1 0.952678\n",
      "54 Train Loss 55.73133 Test MSE 54.65715401345889 Test RE 0.12445509417963826 Lambda1 0.96261597\n",
      "55 Train Loss 54.249718 Test MSE 51.947771993368995 Test RE 0.12133123926818142 Lambda1 0.9834104\n",
      "56 Train Loss 52.78988 Test MSE 50.984797014999586 Test RE 0.12020139777262688 Lambda1 0.992596\n",
      "57 Train Loss 51.648518 Test MSE 49.061126517045814 Test RE 0.11791197904117917 Lambda1 1.0118397\n",
      "58 Train Loss 50.639202 Test MSE 47.5076408949112 Test RE 0.11603016321407514 Lambda1 1.0223647\n",
      "59 Train Loss 49.561497 Test MSE 47.553274312613134 Test RE 0.11608587616806651 Lambda1 1.0275761\n",
      "60 Train Loss 48.89429 Test MSE 47.13732064652327 Test RE 0.11557705314479734 Lambda1 1.0360277\n",
      "61 Train Loss 48.516846 Test MSE 46.44116773373458 Test RE 0.11472042210070131 Lambda1 1.0469508\n",
      "62 Train Loss 48.05694 Test MSE 45.88606479497856 Test RE 0.11403274469922468 Lambda1 1.0602804\n",
      "63 Train Loss 47.501328 Test MSE 45.600607845798265 Test RE 0.11367749280281696 Lambda1 1.0721912\n",
      "64 Train Loss 46.690285 Test MSE 44.440187924803304 Test RE 0.11222176993789708 Lambda1 1.0876824\n",
      "65 Train Loss 46.058403 Test MSE 43.5907010300661 Test RE 0.11114401910362594 Lambda1 1.0906377\n",
      "66 Train Loss 44.771965 Test MSE 42.9509546582748 Test RE 0.11032541791457244 Lambda1 1.0843722\n",
      "67 Train Loss 43.763027 Test MSE 42.25791610828917 Test RE 0.10943171586916066 Lambda1 1.0976555\n",
      "68 Train Loss 43.28683 Test MSE 42.40366183102862 Test RE 0.10962026605836404 Lambda1 1.1043562\n",
      "69 Train Loss 42.025486 Test MSE 41.5349198582312 Test RE 0.10849153628875984 Lambda1 1.1133602\n",
      "70 Train Loss 41.58607 Test MSE 41.51289339471518 Test RE 0.10846276529585008 Lambda1 1.1190177\n",
      "71 Train Loss 41.37961 Test MSE 41.42369509227142 Test RE 0.10834617625182355 Lambda1 1.1188426\n",
      "72 Train Loss 41.09187 Test MSE 41.325763524356475 Test RE 0.10821802750131496 Lambda1 1.1239195\n",
      "73 Train Loss 40.56317 Test MSE 41.85600618309996 Test RE 0.10891007669218612 Lambda1 1.1310364\n",
      "74 Train Loss 40.336857 Test MSE 41.61127646947077 Test RE 0.10859121437041468 Lambda1 1.1326733\n",
      "Training time: 120.29\n",
      "Training time: 120.29\n",
      "inv_HT_stan_tune14\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 837.9482 Test MSE 858.315649485996 Test RE 0.49318821021274545 Lambda1 0.02906001\n",
      "1 Train Loss 837.5263 Test MSE 856.7813679716425 Test RE 0.4927472139358992 Lambda1 0.035422277\n",
      "2 Train Loss 831.11096 Test MSE 849.0115758655809 Test RE 0.4905078661844864 Lambda1 0.23589054\n",
      "3 Train Loss 780.73157 Test MSE 775.1471059759073 Test RE 0.46868519555263805 Lambda1 0.40749675\n",
      "4 Train Loss 715.5888 Test MSE 709.1383321528442 Test RE 0.44828545849856716 Lambda1 0.49403\n",
      "5 Train Loss 683.9516 Test MSE 682.5203828390069 Test RE 0.4397916540634604 Lambda1 0.5646671\n",
      "6 Train Loss 643.8754 Test MSE 641.575503867889 Test RE 0.42639593671281517 Lambda1 0.6610454\n",
      "7 Train Loss 610.8774 Test MSE 607.166510528934 Test RE 0.4148041332718715 Lambda1 0.70612705\n",
      "8 Train Loss 587.3177 Test MSE 581.2850541027699 Test RE 0.4058670070497397 Lambda1 0.718695\n",
      "9 Train Loss 563.9275 Test MSE 560.6098574666936 Test RE 0.3985837011355613 Lambda1 0.72184956\n",
      "10 Train Loss 511.34305 Test MSE 505.0755396832399 Test RE 0.3783270019970848 Lambda1 0.72944635\n",
      "11 Train Loss 456.84314 Test MSE 445.6648768574597 Test RE 0.35538032061350355 Lambda1 0.72080255\n",
      "12 Train Loss 380.58533 Test MSE 384.43800568144803 Test RE 0.3300671691613994 Lambda1 0.70928305\n",
      "13 Train Loss 330.66104 Test MSE 342.51813925493184 Test RE 0.31155229778684174 Lambda1 0.7012636\n",
      "14 Train Loss 295.83847 Test MSE 319.37243586382846 Test RE 0.30084159737118404 Lambda1 0.6941052\n",
      "15 Train Loss 265.31693 Test MSE 292.34099172503414 Test RE 0.2878286552107875 Lambda1 0.6587964\n",
      "16 Train Loss 246.06361 Test MSE 274.44744343081425 Test RE 0.27888089620172235 Lambda1 0.6536964\n",
      "17 Train Loss 234.78825 Test MSE 252.82083200249082 Test RE 0.2676674729292921 Lambda1 0.6281458\n",
      "18 Train Loss 218.39987 Test MSE 233.35906057475364 Test RE 0.2571588675125467 Lambda1 0.6056054\n",
      "19 Train Loss 202.57672 Test MSE 208.75164097320888 Test RE 0.24322270808974877 Lambda1 0.5829763\n",
      "20 Train Loss 182.84544 Test MSE 188.67199474974464 Test RE 0.2312293142310323 Lambda1 0.57315135\n",
      "21 Train Loss 172.80676 Test MSE 176.82997803634228 Test RE 0.22385516388295126 Lambda1 0.5606876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 Train Loss 166.44241 Test MSE 169.7893328526576 Test RE 0.21935340058984476 Lambda1 0.5478133\n",
      "23 Train Loss 159.94608 Test MSE 165.23034437609763 Test RE 0.2163884492998206 Lambda1 0.53995514\n",
      "24 Train Loss 156.62195 Test MSE 161.2973233346543 Test RE 0.21379756317043355 Lambda1 0.5301513\n",
      "25 Train Loss 153.64285 Test MSE 156.6254843407486 Test RE 0.21067858079224455 Lambda1 0.5213002\n",
      "26 Train Loss 148.83693 Test MSE 152.59491920785618 Test RE 0.20795013538140844 Lambda1 0.5234265\n",
      "27 Train Loss 140.32022 Test MSE 141.3420550746958 Test RE 0.2001358414087126 Lambda1 0.520542\n",
      "28 Train Loss 135.73195 Test MSE 137.93897849713576 Test RE 0.19771183795663688 Lambda1 0.52545345\n",
      "29 Train Loss 131.91144 Test MSE 135.6298700259637 Test RE 0.1960499985467472 Lambda1 0.5243346\n",
      "30 Train Loss 127.71097 Test MSE 132.927400537546 Test RE 0.19408699080501238 Lambda1 0.5235691\n",
      "31 Train Loss 123.62212 Test MSE 126.35233264098305 Test RE 0.1892259972174461 Lambda1 0.5217819\n",
      "32 Train Loss 120.018845 Test MSE 122.82696159950679 Test RE 0.18656751415985134 Lambda1 0.52512753\n",
      "33 Train Loss 116.01703 Test MSE 116.16685374260307 Test RE 0.18143884984078412 Lambda1 0.52104235\n",
      "34 Train Loss 111.67363 Test MSE 111.99046033016667 Test RE 0.17814748088025456 Lambda1 0.5211899\n",
      "35 Train Loss 109.02358 Test MSE 106.3261389094837 Test RE 0.17358380033380558 Lambda1 0.52407813\n",
      "36 Train Loss 107.15677 Test MSE 104.7369438708391 Test RE 0.17228168842004 Lambda1 0.5246908\n",
      "37 Train Loss 105.08747 Test MSE 102.4833233797446 Test RE 0.17041812034453116 Lambda1 0.52345365\n",
      "38 Train Loss 102.67135 Test MSE 99.57411608854224 Test RE 0.1679818657447324 Lambda1 0.5196895\n",
      "39 Train Loss 100.4087 Test MSE 97.58165723283706 Test RE 0.16629273086544086 Lambda1 0.52497363\n",
      "40 Train Loss 96.96298 Test MSE 96.78260660503572 Test RE 0.16561048458204022 Lambda1 0.5289414\n",
      "41 Train Loss 95.63858 Test MSE 94.79148290937555 Test RE 0.16389806615309369 Lambda1 0.53273696\n",
      "42 Train Loss 92.98151 Test MSE 89.55203951047643 Test RE 0.1593040845437244 Lambda1 0.55438924\n",
      "43 Train Loss 91.122246 Test MSE 88.10806042883978 Test RE 0.1580145181409317 Lambda1 0.56552434\n",
      "44 Train Loss 87.716576 Test MSE 83.06973855842959 Test RE 0.15343010860265852 Lambda1 0.59061915\n",
      "45 Train Loss 85.0636 Test MSE 81.09459867465701 Test RE 0.15159508995691223 Lambda1 0.6085655\n",
      "46 Train Loss 82.0068 Test MSE 77.75285202238315 Test RE 0.14843876579610943 Lambda1 0.62839675\n",
      "47 Train Loss 79.67551 Test MSE 75.0358393558448 Test RE 0.14582216542088633 Lambda1 0.6436614\n",
      "48 Train Loss 78.402596 Test MSE 73.44322478367371 Test RE 0.14426634849039827 Lambda1 0.6527947\n",
      "49 Train Loss 76.14113 Test MSE 71.56080658504317 Test RE 0.14240550693150814 Lambda1 0.66692436\n",
      "50 Train Loss 75.339264 Test MSE 71.06756505425969 Test RE 0.14191388471900973 Lambda1 0.66731715\n",
      "51 Train Loss 73.6988 Test MSE 70.15155330061322 Test RE 0.14099633257305977 Lambda1 0.6711483\n",
      "52 Train Loss 72.11809 Test MSE 66.81472049288116 Test RE 0.13760215916063995 Lambda1 0.6913667\n",
      "53 Train Loss 71.042015 Test MSE 65.71234475236612 Test RE 0.1364622892595939 Lambda1 0.7012498\n",
      "54 Train Loss 69.31128 Test MSE 64.56654105811019 Test RE 0.13526733420003348 Lambda1 0.70531565\n",
      "55 Train Loss 67.74956 Test MSE 62.6247322290054 Test RE 0.13321775530302588 Lambda1 0.7207606\n",
      "56 Train Loss 66.940765 Test MSE 62.48599542040485 Test RE 0.13307011033005492 Lambda1 0.72620803\n",
      "57 Train Loss 65.0897 Test MSE 59.733044780444374 Test RE 0.13010575192070212 Lambda1 0.74356097\n",
      "58 Train Loss 64.03789 Test MSE 58.20570483809812 Test RE 0.12843161595062855 Lambda1 0.75814396\n",
      "59 Train Loss 62.868362 Test MSE 57.07324053536207 Test RE 0.12717608062711735 Lambda1 0.765505\n",
      "60 Train Loss 61.93182 Test MSE 56.20242661405928 Test RE 0.12620213563221566 Lambda1 0.7719758\n",
      "61 Train Loss 60.753338 Test MSE 56.36568519321683 Test RE 0.12638530068219378 Lambda1 0.7742312\n",
      "62 Train Loss 58.916138 Test MSE 54.45661982761852 Test RE 0.12422657482602013 Lambda1 0.7849695\n",
      "63 Train Loss 57.704735 Test MSE 53.009750969836944 Test RE 0.12256516455336332 Lambda1 0.7988364\n",
      "64 Train Loss 56.83717 Test MSE 51.44476781916978 Test RE 0.12074239229791736 Lambda1 0.8139775\n",
      "65 Train Loss 55.202995 Test MSE 50.34905844386084 Test RE 0.11944964058747254 Lambda1 0.82690924\n",
      "66 Train Loss 54.21751 Test MSE 49.59685491776193 Test RE 0.11855400757682542 Lambda1 0.8375479\n",
      "67 Train Loss 53.064068 Test MSE 48.96763898556136 Test RE 0.11779958296553848 Lambda1 0.8435344\n",
      "68 Train Loss 52.371674 Test MSE 48.17118761770092 Test RE 0.1168376591919744 Lambda1 0.8506908\n",
      "69 Train Loss 51.74328 Test MSE 48.25520933254958 Test RE 0.11693951077547644 Lambda1 0.85546136\n",
      "70 Train Loss 50.8709 Test MSE 46.915412347520096 Test RE 0.11530468123905804 Lambda1 0.86136305\n",
      "71 Train Loss 49.83707 Test MSE 46.43138681721672 Test RE 0.11470834090192952 Lambda1 0.8627513\n",
      "72 Train Loss 49.364456 Test MSE 46.35491675540971 Test RE 0.11461384266321588 Lambda1 0.8603728\n",
      "73 Train Loss 48.91172 Test MSE 45.73860399495283 Test RE 0.11384936777235419 Lambda1 0.86135656\n",
      "74 Train Loss 48.356316 Test MSE 44.76169606240263 Test RE 0.1126269796390084 Lambda1 0.86908346\n",
      "Training time: 119.84\n",
      "Training time: 119.84\n",
      "inv_HT_stan_tune14\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 837.6714 Test MSE 857.6392776001293 Test RE 0.4929938502574146 Lambda1 -0.022911593\n",
      "1 Train Loss 834.8327 Test MSE 852.8986497233344 Test RE 0.4916294427457764 Lambda1 0.023605013\n",
      "2 Train Loss 809.91614 Test MSE 821.3624423600342 Test RE 0.4824547547093843 Lambda1 0.18295527\n",
      "3 Train Loss 745.0587 Test MSE 748.368275495851 Test RE 0.46051825995055573 Lambda1 0.29053655\n",
      "4 Train Loss 705.6737 Test MSE 705.9297128797297 Test RE 0.44727013607487076 Lambda1 0.31737196\n",
      "5 Train Loss 667.40265 Test MSE 662.0836336298803 Test RE 0.4331572590157661 Lambda1 0.34786165\n",
      "6 Train Loss 625.3017 Test MSE 625.5300727962624 Test RE 0.4210302184381539 Lambda1 0.3246749\n",
      "7 Train Loss 592.40894 Test MSE 592.1674823098175 Test RE 0.409648574826024 Lambda1 0.33560973\n",
      "8 Train Loss 536.03656 Test MSE 531.5246009147371 Test RE 0.3881064457657157 Lambda1 0.2839801\n",
      "9 Train Loss 452.5488 Test MSE 454.7892606704826 Test RE 0.35899985322203687 Lambda1 0.30559766\n",
      "10 Train Loss 385.98145 Test MSE 379.12671140627833 Test RE 0.32777917855124233 Lambda1 0.2597014\n",
      "11 Train Loss 318.38876 Test MSE 320.45345187879644 Test RE 0.3013503135522814 Lambda1 0.22115475\n",
      "12 Train Loss 283.5983 Test MSE 287.7209839330644 Test RE 0.2855452492736887 Lambda1 0.19413152\n",
      "13 Train Loss 255.94107 Test MSE 268.19007709692795 Test RE 0.27568334090793745 Lambda1 0.19982395\n",
      "14 Train Loss 236.2845 Test MSE 252.54013750482633 Test RE 0.2675188426682865 Lambda1 0.20030022\n",
      "15 Train Loss 220.51346 Test MSE 230.43181162607362 Test RE 0.2555408810055551 Lambda1 0.21672618\n",
      "16 Train Loss 201.11151 Test MSE 206.60982200523114 Test RE 0.24197174275151107 Lambda1 0.25549623\n",
      "17 Train Loss 189.30049 Test MSE 200.89003373230142 Test RE 0.2385988611811985 Lambda1 0.26647577\n",
      "18 Train Loss 182.88512 Test MSE 189.48199297812377 Test RE 0.23172513435387637 Lambda1 0.30115777\n",
      "19 Train Loss 174.17412 Test MSE 183.76020230066385 Test RE 0.22819961153502966 Lambda1 0.35119784\n",
      "20 Train Loss 163.83928 Test MSE 169.6286643224989 Test RE 0.21924959097675362 Lambda1 0.39797613\n",
      "21 Train Loss 155.43152 Test MSE 156.75131829018036 Test RE 0.21076319408314628 Lambda1 0.46811858\n",
      "22 Train Loss 147.72214 Test MSE 156.22766121101296 Test RE 0.21041085264821008 Lambda1 0.47513834\n",
      "23 Train Loss 138.44609 Test MSE 143.59648676742023 Test RE 0.20172562881951486 Lambda1 0.51368153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 Train Loss 131.21391 Test MSE 135.73040604985957 Test RE 0.1961226463939675 Lambda1 0.54806674\n",
      "25 Train Loss 127.83885 Test MSE 129.3651988937093 Test RE 0.19146874951467488 Lambda1 0.58041227\n",
      "26 Train Loss 122.81194 Test MSE 121.47170567131205 Test RE 0.18553537891047886 Lambda1 0.63138473\n",
      "27 Train Loss 117.9068 Test MSE 118.12241761556375 Test RE 0.18295965563906014 Lambda1 0.65859973\n",
      "28 Train Loss 113.2953 Test MSE 112.36889663180531 Test RE 0.17844822353117573 Lambda1 0.68740135\n",
      "29 Train Loss 109.91011 Test MSE 112.28417857437437 Test RE 0.17838094229140153 Lambda1 0.69906425\n",
      "30 Train Loss 103.5207 Test MSE 104.11534735894566 Test RE 0.17176969591864444 Lambda1 0.7392049\n",
      "31 Train Loss 101.47756 Test MSE 100.53942631623576 Test RE 0.16879414264668555 Lambda1 0.76066333\n",
      "32 Train Loss 96.80774 Test MSE 95.84439712352578 Test RE 0.16480581665076405 Lambda1 0.7840902\n",
      "33 Train Loss 93.23813 Test MSE 92.46140968751217 Test RE 0.16187114021515805 Lambda1 0.8026802\n",
      "34 Train Loss 89.22845 Test MSE 88.59944954357348 Test RE 0.1584545383476785 Lambda1 0.81232923\n",
      "35 Train Loss 86.92719 Test MSE 85.65426429119267 Test RE 0.15579864104336494 Lambda1 0.8275097\n",
      "36 Train Loss 83.640305 Test MSE 85.7885772607753 Test RE 0.15592074577099513 Lambda1 0.8404832\n",
      "37 Train Loss 81.61825 Test MSE 81.9727513280573 Test RE 0.15241367208450296 Lambda1 0.85511565\n",
      "38 Train Loss 79.62425 Test MSE 77.94608605356791 Test RE 0.14862310386995886 Lambda1 0.8980363\n",
      "39 Train Loss 77.56435 Test MSE 76.84177674840261 Test RE 0.14756653158203104 Lambda1 0.90759736\n",
      "40 Train Loss 75.31311 Test MSE 74.0867998372071 Test RE 0.1448970650010366 Lambda1 0.9422368\n",
      "41 Train Loss 73.03218 Test MSE 71.55841528231213 Test RE 0.14240312757373677 Lambda1 0.9711499\n",
      "42 Train Loss 71.97317 Test MSE 71.67862508514912 Test RE 0.14252268771691715 Lambda1 0.98398954\n",
      "43 Train Loss 70.761475 Test MSE 71.50567400586176 Test RE 0.1423506396372401 Lambda1 0.9947707\n",
      "44 Train Loss 69.152245 Test MSE 71.03891854452338 Test RE 0.14188527991713998 Lambda1 0.9969112\n",
      "45 Train Loss 67.74102 Test MSE 68.99921437579965 Test RE 0.13983350499087613 Lambda1 1.0105658\n",
      "46 Train Loss 66.87917 Test MSE 67.6652591831332 Test RE 0.13847521415490474 Lambda1 1.0198191\n",
      "47 Train Loss 64.47723 Test MSE 63.90952505091973 Test RE 0.13457734796304546 Lambda1 1.043662\n",
      "48 Train Loss 62.677017 Test MSE 62.67177668576809 Test RE 0.13326778330440164 Lambda1 1.0518273\n",
      "49 Train Loss 62.022545 Test MSE 61.948599086516445 Test RE 0.13249665536638588 Lambda1 1.0600045\n",
      "50 Train Loss 60.232574 Test MSE 59.895244046597846 Test RE 0.13028227691749625 Lambda1 1.0916442\n",
      "51 Train Loss 59.3274 Test MSE 58.835238936147306 Test RE 0.12912428549694965 Lambda1 1.1125146\n",
      "52 Train Loss 58.19191 Test MSE 58.157438695244004 Test RE 0.1283783549828504 Lambda1 1.1183484\n",
      "53 Train Loss 57.156742 Test MSE 56.39667613538589 Test RE 0.12642004044714109 Lambda1 1.1303502\n",
      "54 Train Loss 56.112328 Test MSE 55.335477212649536 Test RE 0.1252249884939017 Lambda1 1.1495686\n",
      "55 Train Loss 54.724228 Test MSE 54.38778877738287 Test RE 0.12414804122193909 Lambda1 1.1628186\n",
      "56 Train Loss 53.133217 Test MSE 52.56736278358207 Test RE 0.12205266468967428 Lambda1 1.1913643\n",
      "57 Train Loss 52.39842 Test MSE 51.74597288100332 Test RE 0.12109534502063954 Lambda1 1.2085944\n",
      "58 Train Loss 50.264267 Test MSE 49.28925960387444 Test RE 0.11818580505679177 Lambda1 1.2206974\n",
      "59 Train Loss 49.873184 Test MSE 48.54704994977494 Test RE 0.1172925944638303 Lambda1 1.2310343\n",
      "60 Train Loss 48.945236 Test MSE 46.850244402485046 Test RE 0.11522457131398393 Lambda1 1.2494334\n",
      "61 Train Loss 47.932125 Test MSE 46.334164571186705 Test RE 0.11458818461004612 Lambda1 1.2601649\n",
      "62 Train Loss 46.862198 Test MSE 45.67614621981663 Test RE 0.11377160843169107 Lambda1 1.2679492\n",
      "63 Train Loss 46.16983 Test MSE 45.39780033798385 Test RE 0.11342442227373145 Lambda1 1.2687167\n",
      "64 Train Loss 45.40313 Test MSE 44.49302234019528 Test RE 0.11228845969160829 Lambda1 1.2798581\n",
      "65 Train Loss 44.58964 Test MSE 44.078489323264236 Test RE 0.11176415051293428 Lambda1 1.2930909\n",
      "66 Train Loss 43.290573 Test MSE 42.40087260381911 Test RE 0.10961666069925391 Lambda1 1.3161174\n",
      "67 Train Loss 42.806477 Test MSE 41.68024813963759 Test RE 0.10868117335544863 Lambda1 1.3252025\n",
      "68 Train Loss 42.40815 Test MSE 41.50219825126892 Test RE 0.10844879253385123 Lambda1 1.3370373\n",
      "69 Train Loss 41.734734 Test MSE 40.871659373929226 Test RE 0.10762181345940516 Lambda1 1.3392876\n",
      "70 Train Loss 41.16474 Test MSE 40.66197700574252 Test RE 0.10734539436570475 Lambda1 1.3375189\n",
      "71 Train Loss 40.87567 Test MSE 40.928250429229536 Test RE 0.10769629447314033 Lambda1 1.3298178\n",
      "72 Train Loss 40.587967 Test MSE 40.72393999310326 Test RE 0.1074271526786732 Lambda1 1.3362149\n",
      "73 Train Loss 40.346535 Test MSE 40.60554710211001 Test RE 0.10727088257628722 Lambda1 1.3385353\n",
      "74 Train Loss 39.906277 Test MSE 40.05937491920272 Test RE 0.10654700707320304 Lambda1 1.3416636\n",
      "Training time: 120.45\n",
      "Training time: 120.45\n",
      "inv_HT_stan_tune14\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 837.3841 Test MSE 857.2804161147305 Test RE 0.49289069790615414 Lambda1 -0.110125706\n",
      "1 Train Loss 828.4515 Test MSE 844.689561751108 Test RE 0.4892577732011695 Lambda1 -0.028765101\n",
      "2 Train Loss 749.4626 Test MSE 744.0603244228944 Test RE 0.4591908697539857 Lambda1 0.11843086\n",
      "3 Train Loss 615.37366 Test MSE 601.4022743798918 Test RE 0.41283043170509764 Lambda1 0.06861455\n",
      "4 Train Loss 442.25067 Test MSE 431.16220902504364 Test RE 0.34955016768026526 Lambda1 0.009992381\n",
      "5 Train Loss 360.98065 Test MSE 347.28610302845294 Test RE 0.3137132580604584 Lambda1 -0.0010795118\n",
      "6 Train Loss 279.33093 Test MSE 290.49461808399064 Test RE 0.2869182782255945 Lambda1 -0.0003202036\n",
      "7 Train Loss 267.981 Test MSE 288.4261415372228 Test RE 0.2858949477563751 Lambda1 4.14347e-05\n",
      "8 Train Loss 261.3676 Test MSE 283.8676906843722 Test RE 0.28362672678661227 Lambda1 0.00030695766\n",
      "9 Train Loss 257.63885 Test MSE 282.3485766306988 Test RE 0.28286679646424673 Lambda1 0.00038410266\n",
      "10 Train Loss 255.92029 Test MSE 281.74364116729015 Test RE 0.28256361110449585 Lambda1 0.00034967164\n",
      "11 Train Loss 255.10362 Test MSE 282.1226118240049 Test RE 0.2827535840384605 Lambda1 0.00033176123\n",
      "12 Train Loss 254.8411 Test MSE 282.34598600745386 Test RE 0.28286549877224526 Lambda1 0.00038538585\n",
      "13 Train Loss 254.50856 Test MSE 282.61394809358205 Test RE 0.282999694506344 Lambda1 0.00031513683\n",
      "14 Train Loss 254.36095 Test MSE 283.2156452057512 Test RE 0.28330079351974635 Lambda1 0.00027811626\n",
      "15 Train Loss 254.26108 Test MSE 283.0517263855678 Test RE 0.28321879760263047 Lambda1 0.0003935321\n",
      "16 Train Loss 254.22238 Test MSE 283.1126119853205 Test RE 0.2832492567334792 Lambda1 0.00041075825\n",
      "17 Train Loss 254.20166 Test MSE 282.95863173315956 Test RE 0.2831722189716784 Lambda1 0.00042988462\n",
      "18 Train Loss 254.16017 Test MSE 283.1140353527802 Test RE 0.28324996875980946 Lambda1 0.0005499935\n",
      "19 Train Loss 254.12764 Test MSE 282.9353462203075 Test RE 0.28316056718716576 Lambda1 0.0005947246\n",
      "20 Train Loss 253.94102 Test MSE 282.85144347328753 Test RE 0.28311857930666234 Lambda1 0.0009811945\n",
      "21 Train Loss 253.90291 Test MSE 282.78607671893604 Test RE 0.28308586317176443 Lambda1 0.000983457\n",
      "22 Train Loss 253.79118 Test MSE 281.92875099282566 Test RE 0.2826564201418457 Lambda1 0.0013974496\n",
      "23 Train Loss 253.76791 Test MSE 281.98265163912146 Test RE 0.2826834387335052 Lambda1 0.001354981\n",
      "24 Train Loss 253.58765 Test MSE 281.4864448929347 Test RE 0.28243460925563685 Lambda1 0.002061674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 Train Loss 253.11133 Test MSE 279.3530910629249 Test RE 0.28136230374090465 Lambda1 0.0045199026\n",
      "26 Train Loss 252.82776 Test MSE 277.72655163333553 Test RE 0.28054198886542153 Lambda1 0.0073783216\n",
      "27 Train Loss 251.775 Test MSE 275.24004968908133 Test RE 0.27928331091844477 Lambda1 0.011705177\n",
      "28 Train Loss 250.73795 Test MSE 273.8326775434307 Test RE 0.27856837264939927 Lambda1 0.0150694875\n",
      "29 Train Loss 249.08243 Test MSE 271.8961501852139 Test RE 0.2775816160369145 Lambda1 0.020292742\n",
      "30 Train Loss 247.16354 Test MSE 269.4075963280534 Test RE 0.276308400739723 Lambda1 0.026125042\n",
      "31 Train Loss 244.94719 Test MSE 265.9306426910542 Test RE 0.2745196030326452 Lambda1 0.034473293\n",
      "32 Train Loss 240.84792 Test MSE 261.3680424241779 Test RE 0.27215443328845534 Lambda1 0.04821108\n",
      "33 Train Loss 238.81837 Test MSE 259.82897740747285 Test RE 0.2713519598784177 Lambda1 0.056668427\n",
      "34 Train Loss 233.82645 Test MSE 253.3157727692968 Test RE 0.2679293476504149 Lambda1 0.07587697\n",
      "35 Train Loss 227.98239 Test MSE 246.5553439528197 Test RE 0.2643299541708328 Lambda1 0.102779776\n",
      "36 Train Loss 224.26375 Test MSE 241.29754651992252 Test RE 0.2614963454044128 Lambda1 0.12479561\n",
      "37 Train Loss 216.20146 Test MSE 236.8632631343298 Test RE 0.2590824675132683 Lambda1 0.15275191\n",
      "38 Train Loss 211.01306 Test MSE 228.36534593215816 Test RE 0.2543924811885309 Lambda1 0.17179933\n",
      "39 Train Loss 206.24428 Test MSE 221.15891891625492 Test RE 0.2503464283620136 Lambda1 0.1967617\n",
      "40 Train Loss 187.94757 Test MSE 197.6941089353811 Test RE 0.23669333813300503 Lambda1 0.26227483\n",
      "41 Train Loss 174.65146 Test MSE 183.5345346536295 Test RE 0.22805944764526898 Lambda1 0.29231018\n",
      "42 Train Loss 164.17441 Test MSE 169.83587826388654 Test RE 0.2193834648890376 Lambda1 0.3348002\n",
      "43 Train Loss 158.53925 Test MSE 166.14967449750722 Test RE 0.21698959939643975 Lambda1 0.3399957\n",
      "44 Train Loss 151.05058 Test MSE 155.1149565755152 Test RE 0.20966020612540795 Lambda1 0.37461206\n",
      "45 Train Loss 146.62006 Test MSE 152.12810788078977 Test RE 0.2076318160244631 Lambda1 0.38252833\n",
      "46 Train Loss 141.00655 Test MSE 142.09779633569372 Test RE 0.20067018085592744 Lambda1 0.4231458\n",
      "47 Train Loss 136.7486 Test MSE 136.97759829656368 Test RE 0.1970216465517963 Lambda1 0.44557744\n",
      "48 Train Loss 130.86671 Test MSE 127.86652688597508 Test RE 0.19035645359150852 Lambda1 0.46042424\n",
      "49 Train Loss 125.494545 Test MSE 121.12530840362382 Test RE 0.18527064716957956 Lambda1 0.4936638\n",
      "50 Train Loss 122.16303 Test MSE 117.87297848864652 Test RE 0.182766375581687 Lambda1 0.5083921\n",
      "51 Train Loss 117.639084 Test MSE 114.80204155109983 Test RE 0.18036986335941077 Lambda1 0.5252977\n",
      "52 Train Loss 114.3769 Test MSE 112.43295715538692 Test RE 0.17849908216817795 Lambda1 0.5277033\n",
      "53 Train Loss 110.45443 Test MSE 111.30002442663053 Test RE 0.17759748054943317 Lambda1 0.53436255\n",
      "54 Train Loss 107.962975 Test MSE 107.51549021562336 Test RE 0.17455194415656075 Lambda1 0.5422209\n",
      "55 Train Loss 105.051926 Test MSE 104.48397655466684 Test RE 0.17207350980681838 Lambda1 0.5542359\n",
      "56 Train Loss 102.476524 Test MSE 102.51600816185639 Test RE 0.17044529371725758 Lambda1 0.56894463\n",
      "57 Train Loss 101.26632 Test MSE 101.77512615405398 Test RE 0.16982827381159793 Lambda1 0.57079357\n",
      "58 Train Loss 98.900764 Test MSE 98.5928381793736 Test RE 0.16715210686518578 Lambda1 0.58617586\n",
      "59 Train Loss 97.23987 Test MSE 97.12842502700582 Test RE 0.16590609600483186 Lambda1 0.6036105\n",
      "60 Train Loss 96.27633 Test MSE 96.00952989385569 Test RE 0.16494772963517915 Lambda1 0.61046773\n",
      "61 Train Loss 93.80693 Test MSE 92.64285116835528 Test RE 0.1620298861449228 Lambda1 0.62690556\n",
      "62 Train Loss 91.80951 Test MSE 90.24021856033424 Test RE 0.1599150137983469 Lambda1 0.6431686\n",
      "63 Train Loss 89.46203 Test MSE 88.94921504867365 Test RE 0.15876699707122247 Lambda1 0.6595638\n",
      "64 Train Loss 87.142876 Test MSE 84.66158166201376 Test RE 0.154893202638899 Lambda1 0.68806505\n",
      "65 Train Loss 85.116135 Test MSE 81.80746193497318 Test RE 0.15225993151094205 Lambda1 0.7093734\n",
      "66 Train Loss 84.15589 Test MSE 81.02080085953328 Test RE 0.15152609687380922 Lambda1 0.7221619\n",
      "67 Train Loss 83.10631 Test MSE 79.44932171848961 Test RE 0.15004940152204832 Lambda1 0.73623115\n",
      "68 Train Loss 80.080956 Test MSE 77.81228729079042 Test RE 0.14849548919785238 Lambda1 0.756478\n",
      "69 Train Loss 77.941124 Test MSE 76.10434437765848 Test RE 0.1468567439968301 Lambda1 0.7752185\n",
      "70 Train Loss 77.21847 Test MSE 76.08916047351704 Test RE 0.1468420932566203 Lambda1 0.783231\n",
      "71 Train Loss 76.414246 Test MSE 76.27207860050217 Test RE 0.1470184912939959 Lambda1 0.7967625\n",
      "72 Train Loss 75.37454 Test MSE 74.94514013647532 Test RE 0.14573400784309018 Lambda1 0.7983975\n",
      "73 Train Loss 74.26028 Test MSE 74.91675561122567 Test RE 0.14570640777214247 Lambda1 0.8036085\n",
      "74 Train Loss 73.630135 Test MSE 74.40081295312827 Test RE 0.14520380971777821 Lambda1 0.8034317\n",
      "Training time: 120.16\n",
      "Training time: 120.16\n",
      "inv_HT_stan_tune14\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 837.9189 Test MSE 858.2216915193435 Test RE 0.4931612153480576 Lambda1 -0.009938668\n",
      "1 Train Loss 836.8015 Test MSE 855.591509371547 Test RE 0.4924049427159457 Lambda1 0.0005648305\n",
      "2 Train Loss 798.0753 Test MSE 820.3567001948112 Test RE 0.4821592860510019 Lambda1 0.105202615\n",
      "3 Train Loss 594.81726 Test MSE 559.6922826571567 Test RE 0.39825737781719656 Lambda1 0.037831783\n",
      "4 Train Loss 497.0746 Test MSE 498.5068032207406 Test RE 0.37585879356862406 Lambda1 0.027942646\n",
      "5 Train Loss 340.58264 Test MSE 332.98633321266817 Test RE 0.3071866764356587 Lambda1 0.0080527915\n",
      "6 Train Loss 271.61356 Test MSE 289.75778365275346 Test RE 0.2865541656333148 Lambda1 0.005259453\n",
      "7 Train Loss 262.8996 Test MSE 283.7561010861261 Test RE 0.2835709738697406 Lambda1 0.0019723175\n",
      "8 Train Loss 260.11823 Test MSE 281.5928168550897 Test RE 0.2824879693487624 Lambda1 0.0015972772\n",
      "9 Train Loss 256.31686 Test MSE 280.2974585452071 Test RE 0.2818374823528062 Lambda1 0.0014506218\n",
      "10 Train Loss 255.82735 Test MSE 280.31615352699555 Test RE 0.28184688104442207 Lambda1 0.0010826524\n",
      "11 Train Loss 255.19533 Test MSE 280.77385342209755 Test RE 0.28207688682704274 Lambda1 0.0010928909\n",
      "12 Train Loss 254.73228 Test MSE 281.09520920136634 Test RE 0.2822382642386299 Lambda1 0.0010671559\n",
      "13 Train Loss 254.45439 Test MSE 281.1078833591841 Test RE 0.2822446270151289 Lambda1 0.0015325429\n",
      "14 Train Loss 254.22592 Test MSE 281.48460172987376 Test RE 0.28243368456831963 Lambda1 0.0017343671\n",
      "15 Train Loss 253.9755 Test MSE 281.0391123871595 Test RE 0.282210100369336 Lambda1 0.002283826\n",
      "16 Train Loss 253.88107 Test MSE 281.5462995828882 Test RE 0.2824646358123476 Lambda1 0.002195657\n",
      "17 Train Loss 253.7124 Test MSE 281.2347519160684 Test RE 0.2823083106244639 Lambda1 0.00269059\n",
      "18 Train Loss 253.5543 Test MSE 280.48873440502 Test RE 0.28193362934128324 Lambda1 0.0032006034\n",
      "19 Train Loss 253.06581 Test MSE 279.6423926388352 Test RE 0.28150795721144095 Lambda1 0.0045244014\n",
      "20 Train Loss 252.23872 Test MSE 276.66786332469167 Test RE 0.2800067679646525 Lambda1 0.007742714\n",
      "21 Train Loss 249.99554 Test MSE 272.5144204761959 Test RE 0.27789703612347194 Lambda1 0.015699238\n",
      "22 Train Loss 246.07597 Test MSE 267.1010170854378 Test RE 0.27512302726091986 Lambda1 0.02905289\n",
      "23 Train Loss 243.08139 Test MSE 263.52314389153344 Test RE 0.2732741500266794 Lambda1 0.036355525\n",
      "24 Train Loss 238.64539 Test MSE 256.2921033946755 Test RE 0.26949876746666507 Lambda1 0.053247716\n",
      "25 Train Loss 230.861 Test MSE 246.58371517195087 Test RE 0.2643451620085666 Lambda1 0.07232245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 Train Loss 224.33818 Test MSE 235.47983581041657 Test RE 0.25832475894153606 Lambda1 0.09519163\n",
      "27 Train Loss 214.043 Test MSE 228.00436365255007 Test RE 0.25419133968867613 Lambda1 0.12311718\n",
      "28 Train Loss 207.31966 Test MSE 221.9916118770484 Test RE 0.25081727949622273 Lambda1 0.14440809\n",
      "29 Train Loss 197.41139 Test MSE 211.53139235785855 Test RE 0.24483673798242475 Lambda1 0.16802703\n",
      "30 Train Loss 188.5833 Test MSE 196.40327120149544 Test RE 0.2359193315871513 Lambda1 0.18909296\n",
      "31 Train Loss 183.54813 Test MSE 190.37470369479803 Test RE 0.23227035880930869 Lambda1 0.20305873\n",
      "32 Train Loss 175.74532 Test MSE 180.3775251067563 Test RE 0.2260894943504185 Lambda1 0.23148832\n",
      "33 Train Loss 168.8416 Test MSE 170.52478058860822 Test RE 0.21982795527065707 Lambda1 0.26258484\n",
      "34 Train Loss 160.07755 Test MSE 158.03258879658856 Test RE 0.2116228202773331 Lambda1 0.29947165\n",
      "35 Train Loss 150.14655 Test MSE 146.9185218155019 Test RE 0.2040456993876056 Lambda1 0.3397566\n",
      "36 Train Loss 143.32037 Test MSE 137.48376754751115 Test RE 0.19738533499945163 Lambda1 0.3749569\n",
      "37 Train Loss 139.85385 Test MSE 133.91028358376442 Test RE 0.19480322190013052 Lambda1 0.40257064\n",
      "38 Train Loss 132.70105 Test MSE 129.22648054610795 Test RE 0.19136606597716727 Lambda1 0.41501114\n",
      "39 Train Loss 126.56497 Test MSE 122.57287681524511 Test RE 0.1863744437346178 Lambda1 0.4564556\n",
      "40 Train Loss 120.66048 Test MSE 115.88465526772426 Test RE 0.18121833556165018 Lambda1 0.47745287\n",
      "41 Train Loss 115.42338 Test MSE 114.86987171468674 Test RE 0.18042314076864352 Lambda1 0.49831313\n",
      "42 Train Loss 112.813416 Test MSE 111.85384928835035 Test RE 0.1780387915363769 Lambda1 0.51677567\n",
      "43 Train Loss 109.68518 Test MSE 108.60078845530889 Test RE 0.17543072564531606 Lambda1 0.53600764\n",
      "44 Train Loss 106.95966 Test MSE 104.99122157423815 Test RE 0.17249069220637459 Lambda1 0.5634938\n",
      "45 Train Loss 104.78794 Test MSE 104.85838983871567 Test RE 0.17238154265453515 Lambda1 0.5638496\n",
      "46 Train Loss 101.31386 Test MSE 100.1762582963767 Test RE 0.168489008161307 Lambda1 0.60086876\n",
      "47 Train Loss 99.62259 Test MSE 97.27015430920648 Test RE 0.16602709652738784 Lambda1 0.6305617\n",
      "48 Train Loss 96.811455 Test MSE 94.14252513856714 Test RE 0.1633360663265155 Lambda1 0.66473603\n",
      "49 Train Loss 94.17464 Test MSE 92.42565653525529 Test RE 0.1618398408702698 Lambda1 0.65958166\n",
      "50 Train Loss 90.80484 Test MSE 88.31146203283977 Test RE 0.15819680497291305 Lambda1 0.7031313\n",
      "51 Train Loss 86.60831 Test MSE 83.6785983892359 Test RE 0.15399136520600107 Lambda1 0.7415053\n",
      "52 Train Loss 83.25402 Test MSE 78.9872106857322 Test RE 0.14961238957735165 Lambda1 0.7925566\n",
      "53 Train Loss 81.626396 Test MSE 76.12401243976346 Test RE 0.14687571926686066 Lambda1 0.82859963\n",
      "54 Train Loss 80.02078 Test MSE 75.47379993949433 Test RE 0.1462471053106063 Lambda1 0.8411458\n",
      "55 Train Loss 78.07134 Test MSE 74.4662746457854 Test RE 0.14526767459300852 Lambda1 0.86017823\n",
      "56 Train Loss 76.83156 Test MSE 73.08138605993322 Test RE 0.14391052529252668 Lambda1 0.8812008\n",
      "57 Train Loss 74.4009 Test MSE 71.51274640042624 Test RE 0.14235767918348174 Lambda1 0.9087896\n",
      "58 Train Loss 72.788475 Test MSE 70.16624334952577 Test RE 0.14101109443152193 Lambda1 0.93479306\n",
      "59 Train Loss 69.2217 Test MSE 65.1435973832409 Test RE 0.13587045803464523 Lambda1 0.97392964\n",
      "60 Train Loss 66.003654 Test MSE 62.46992173044708 Test RE 0.1330529939726051 Lambda1 1.0054327\n",
      "61 Train Loss 65.46179 Test MSE 62.03739688239317 Test RE 0.1325915824348187 Lambda1 1.0137041\n",
      "62 Train Loss 63.947636 Test MSE 60.63327351601566 Test RE 0.13108248881522097 Lambda1 1.0541316\n",
      "63 Train Loss 62.13453 Test MSE 58.55723093432381 Test RE 0.12881885552413036 Lambda1 1.1043061\n",
      "64 Train Loss 60.14972 Test MSE 56.57844646881723 Test RE 0.12662360675816706 Lambda1 1.141096\n",
      "65 Train Loss 58.637173 Test MSE 56.201265627243835 Test RE 0.12620083213180602 Lambda1 1.1468452\n",
      "66 Train Loss 57.854378 Test MSE 55.63125636058573 Test RE 0.1255592187108631 Lambda1 1.146686\n",
      "67 Train Loss 55.360054 Test MSE 54.05804633422957 Test RE 0.1237711265117492 Lambda1 1.1728753\n",
      "68 Train Loss 53.60562 Test MSE 52.3924129121547 Test RE 0.1218493931838943 Lambda1 1.215097\n",
      "69 Train Loss 52.82271 Test MSE 51.232668807472734 Test RE 0.12049323389386948 Lambda1 1.2458526\n",
      "70 Train Loss 52.06304 Test MSE 50.39351564602485 Test RE 0.1195023647620456 Lambda1 1.2647532\n",
      "71 Train Loss 51.31129 Test MSE 50.29269221059111 Test RE 0.11938275937894317 Lambda1 1.2646397\n",
      "72 Train Loss 50.321613 Test MSE 49.79383388180118 Test RE 0.11878919894844935 Lambda1 1.273828\n",
      "73 Train Loss 49.169044 Test MSE 49.10354965675039 Test RE 0.11796294724994888 Lambda1 1.3015956\n",
      "74 Train Loss 48.16909 Test MSE 48.20921411447687 Test RE 0.11688376611419109 Lambda1 1.326028\n",
      "Training time: 120.22\n",
      "Training time: 120.22\n",
      "inv_HT_stan_tune14\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 838.2088 Test MSE 857.5459085576765 Test RE 0.49296701402346 Lambda1 -0.01508229\n",
      "1 Train Loss 801.42865 Test MSE 784.5335485402143 Test RE 0.4715143673707716 Lambda1 0.05198483\n",
      "2 Train Loss 653.4197 Test MSE 640.6962815014111 Test RE 0.4261036676259495 Lambda1 0.01952597\n",
      "3 Train Loss 590.00854 Test MSE 589.0540351981358 Test RE 0.4085702481033046 Lambda1 0.003981309\n",
      "4 Train Loss 507.98584 Test MSE 484.15645457913365 Test RE 0.3704094292402024 Lambda1 0.0008258304\n",
      "5 Train Loss 290.5578 Test MSE 303.86030333977425 Test RE 0.2934446217373233 Lambda1 0.0009429368\n",
      "6 Train Loss 265.07236 Test MSE 283.4195122835403 Test RE 0.2834027393621753 Lambda1 0.00085358834\n",
      "7 Train Loss 260.4844 Test MSE 283.02057775040043 Test RE 0.2832032136635611 Lambda1 0.00011075524\n",
      "8 Train Loss 258.5871 Test MSE 282.84364727420467 Test RE 0.28311467749871855 Lambda1 0.00011414345\n",
      "9 Train Loss 257.839 Test MSE 282.609767324155 Test RE 0.28299760126067375 Lambda1 0.00014231942\n",
      "10 Train Loss 257.33533 Test MSE 281.7567102029556 Test RE 0.28257016456400386 Lambda1 0.00021824258\n",
      "11 Train Loss 256.96945 Test MSE 281.2725514749447 Test RE 0.2823272819126758 Lambda1 0.00033010056\n",
      "12 Train Loss 256.47223 Test MSE 281.6561294661895 Test RE 0.28251972450137813 Lambda1 0.00011728304\n",
      "13 Train Loss 256.12466 Test MSE 281.40369889496134 Test RE 0.2823930938438427 Lambda1 0.00019582904\n",
      "14 Train Loss 255.82747 Test MSE 281.8066193122921 Test RE 0.28259519005630884 Lambda1 0.00025707184\n",
      "15 Train Loss 255.28825 Test MSE 282.09470193220375 Test RE 0.2827395975361529 Lambda1 0.00042092692\n",
      "16 Train Loss 254.98648 Test MSE 282.06737425974006 Test RE 0.2827259021313875 Lambda1 0.00041138675\n",
      "17 Train Loss 254.8822 Test MSE 282.58608873059626 Test RE 0.2829857454681281 Lambda1 0.0004477261\n",
      "18 Train Loss 254.76303 Test MSE 282.629996686 Test RE 0.2830077296410335 Lambda1 0.0004950996\n",
      "19 Train Loss 254.69029 Test MSE 282.6619972803708 Test RE 0.28302375086901543 Lambda1 0.00049859803\n",
      "20 Train Loss 254.60901 Test MSE 282.397333211328 Test RE 0.2828912184439736 Lambda1 0.00068364094\n",
      "21 Train Loss 254.27917 Test MSE 281.8180771440495 Test RE 0.2826009349448415 Lambda1 0.001574233\n",
      "22 Train Loss 254.17975 Test MSE 281.16920178211717 Test RE 0.28227540852775973 Lambda1 0.002055511\n",
      "23 Train Loss 253.67746 Test MSE 279.20118314505726 Test RE 0.28128579308973883 Lambda1 0.004305537\n",
      "24 Train Loss 253.0528 Test MSE 279.0496820091338 Test RE 0.2812094665870131 Lambda1 0.0049323123\n",
      "25 Train Loss 252.92738 Test MSE 279.04606012280016 Test RE 0.28120764162161355 Lambda1 0.0049740872\n",
      "26 Train Loss 252.33995 Test MSE 277.7429475083565 Test RE 0.2805502697868441 Lambda1 0.0071944427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 Train Loss 251.76009 Test MSE 277.2187907725317 Test RE 0.2802854174119705 Lambda1 0.008660435\n",
      "28 Train Loss 251.19702 Test MSE 276.0974660177704 Test RE 0.27971797853081853 Lambda1 0.0108121745\n",
      "29 Train Loss 250.05258 Test MSE 273.8718944483192 Test RE 0.27858831949862173 Lambda1 0.015218918\n",
      "30 Train Loss 248.91707 Test MSE 271.6812484339624 Test RE 0.27747189663276434 Lambda1 0.020519912\n",
      "31 Train Loss 246.85835 Test MSE 270.22822619168153 Test RE 0.27672890580340964 Lambda1 0.024616161\n",
      "32 Train Loss 245.17737 Test MSE 268.1490028024123 Test RE 0.2756622291418867 Lambda1 0.031627905\n",
      "33 Train Loss 243.35402 Test MSE 264.6318424883303 Test RE 0.27384840824304496 Lambda1 0.04126246\n",
      "34 Train Loss 241.17847 Test MSE 261.16106576050856 Test RE 0.2720466527573721 Lambda1 0.04725633\n",
      "35 Train Loss 238.40186 Test MSE 258.3344266242872 Test RE 0.2705704185795019 Lambda1 0.055467717\n",
      "36 Train Loss 233.848 Test MSE 250.62865715605025 Test RE 0.26650449240999957 Lambda1 0.07571146\n",
      "37 Train Loss 229.27362 Test MSE 245.95716415268228 Test RE 0.26400910762811486 Lambda1 0.09405119\n",
      "38 Train Loss 222.91064 Test MSE 239.1919338565195 Test RE 0.260352909678777 Lambda1 0.11316845\n",
      "39 Train Loss 216.63861 Test MSE 230.54022493153246 Test RE 0.25560098723311975 Lambda1 0.13828734\n",
      "40 Train Loss 211.0535 Test MSE 221.63482113628666 Test RE 0.2506156383880388 Lambda1 0.15913114\n",
      "41 Train Loss 198.99823 Test MSE 207.82871166057382 Test RE 0.24268444637919703 Lambda1 0.19919063\n",
      "42 Train Loss 191.9983 Test MSE 194.2958473532619 Test RE 0.23465020066154704 Lambda1 0.22371095\n",
      "43 Train Loss 185.53387 Test MSE 191.11134176974747 Test RE 0.23271929974740574 Lambda1 0.23499325\n",
      "44 Train Loss 176.0095 Test MSE 177.34306138101743 Test RE 0.2241796935876837 Lambda1 0.28793955\n",
      "45 Train Loss 168.53773 Test MSE 168.16146403012468 Test RE 0.21829933387255365 Lambda1 0.3462299\n",
      "46 Train Loss 163.43828 Test MSE 164.03541386206675 Test RE 0.21560457891855322 Lambda1 0.38497624\n",
      "47 Train Loss 156.68262 Test MSE 162.53227150137246 Test RE 0.2146144566392747 Lambda1 0.4066373\n",
      "48 Train Loss 151.69527 Test MSE 156.10818267207253 Test RE 0.2103303790963198 Lambda1 0.4466232\n",
      "49 Train Loss 145.2625 Test MSE 144.60365731221896 Test RE 0.20243183376592952 Lambda1 0.4955959\n",
      "50 Train Loss 140.27133 Test MSE 138.69779200616318 Test RE 0.1982549065434415 Lambda1 0.53077894\n",
      "51 Train Loss 134.18532 Test MSE 130.37189556597917 Test RE 0.192212293444245 Lambda1 0.58888936\n",
      "52 Train Loss 129.91635 Test MSE 128.01287753582577 Test RE 0.19046535943427767 Lambda1 0.600272\n",
      "53 Train Loss 125.664024 Test MSE 126.80968015961768 Test RE 0.18956815104853045 Lambda1 0.61897457\n",
      "54 Train Loss 122.493484 Test MSE 123.00258218880883 Test RE 0.18670084560704842 Lambda1 0.6495967\n",
      "55 Train Loss 118.29348 Test MSE 117.55213522212402 Test RE 0.1825174664622219 Lambda1 0.6877409\n",
      "56 Train Loss 114.872116 Test MSE 113.6186503390895 Test RE 0.17943781971709513 Lambda1 0.70120895\n",
      "57 Train Loss 110.53693 Test MSE 107.42094659512158 Test RE 0.17447518125359412 Lambda1 0.7458044\n",
      "58 Train Loss 106.713196 Test MSE 106.53226117518241 Test RE 0.17375197235139825 Lambda1 0.74501526\n",
      "59 Train Loss 104.53885 Test MSE 104.67936175555027 Test RE 0.17223432352876633 Lambda1 0.75002366\n",
      "60 Train Loss 102.15996 Test MSE 100.89964323196125 Test RE 0.16909625368700038 Lambda1 0.7708875\n",
      "61 Train Loss 99.66585 Test MSE 96.08556198413393 Test RE 0.16501302960451228 Lambda1 0.79487646\n",
      "62 Train Loss 97.22934 Test MSE 93.32270236579157 Test RE 0.16262332026321724 Lambda1 0.8196119\n",
      "63 Train Loss 93.607185 Test MSE 90.75069873234867 Test RE 0.16036668779870517 Lambda1 0.8398281\n",
      "64 Train Loss 91.151375 Test MSE 87.78915701114126 Test RE 0.1577282954633331 Lambda1 0.86803293\n",
      "65 Train Loss 88.52879 Test MSE 85.79775535215754 Test RE 0.15592908613882417 Lambda1 0.8877924\n",
      "66 Train Loss 87.02022 Test MSE 85.80528545681516 Test RE 0.15593592860557226 Lambda1 0.8950656\n",
      "67 Train Loss 85.06717 Test MSE 82.52524830535886 Test RE 0.1529264441772549 Lambda1 0.91620636\n",
      "68 Train Loss 82.40289 Test MSE 77.15615009843326 Test RE 0.14786808390955872 Lambda1 0.95782655\n",
      "69 Train Loss 79.44365 Test MSE 73.03736922838543 Test RE 0.14386718019610806 Lambda1 0.99310255\n",
      "70 Train Loss 76.755875 Test MSE 71.32770472239044 Test RE 0.14217338220483267 Lambda1 1.0079796\n",
      "71 Train Loss 74.917564 Test MSE 70.08629199386627 Test RE 0.14093073355554644 Lambda1 1.0099771\n",
      "72 Train Loss 73.52414 Test MSE 70.95813655613007 Test RE 0.14180458446479483 Lambda1 1.0198041\n",
      "73 Train Loss 72.09706 Test MSE 69.35348122934401 Test RE 0.14019202321478438 Lambda1 1.026441\n",
      "74 Train Loss 69.20655 Test MSE 66.21685913255016 Test RE 0.13698514038274956 Lambda1 1.0379729\n",
      "Training time: 121.08\n",
      "Training time: 121.08\n",
      "inv_HT_stan_tune15\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1083.3274 Test MSE 924.5353439797444 Test RE 0.5118596906185414 Lambda1 0.0016165808\n",
      "1 Train Loss 830.44604 Test MSE 845.985554483545 Test RE 0.48963295925829514 Lambda1 0.0017888524\n",
      "2 Train Loss 813.0479 Test MSE 832.4801408233384 Test RE 0.48570895614179155 Lambda1 -0.0038702826\n",
      "3 Train Loss 758.8097 Test MSE 739.3310292944278 Test RE 0.45772922016205514 Lambda1 -0.009054784\n",
      "4 Train Loss 582.7007 Test MSE 586.7402538839905 Test RE 0.4077670345663776 Lambda1 -0.03208144\n",
      "5 Train Loss 367.12952 Test MSE 368.80614833947953 Test RE 0.3232870042896629 Lambda1 0.0008259645\n",
      "6 Train Loss 292.23355 Test MSE 300.8343214468667 Test RE 0.29197983701002894 Lambda1 0.0035639908\n",
      "7 Train Loss 274.68625 Test MSE 291.39551490373236 Test RE 0.2873628366660516 Lambda1 0.00047004665\n",
      "8 Train Loss 264.23837 Test MSE 284.5192681544119 Test RE 0.2839520523951053 Lambda1 -0.00012423261\n",
      "9 Train Loss 259.82553 Test MSE 280.03053240586723 Test RE 0.2817032540400239 Lambda1 0.0015952529\n",
      "10 Train Loss 258.54968 Test MSE 279.461500235924 Test RE 0.2814168928915926 Lambda1 0.0025566246\n",
      "11 Train Loss 256.52148 Test MSE 278.03866755381415 Test RE 0.2806995845893657 Lambda1 0.005149538\n",
      "12 Train Loss 254.54152 Test MSE 276.9593908838985 Test RE 0.2801542520209359 Lambda1 0.008372217\n",
      "13 Train Loss 251.50395 Test MSE 272.6944614221474 Test RE 0.27798881949433935 Lambda1 0.015288358\n",
      "14 Train Loss 246.82085 Test MSE 266.8058125703894 Test RE 0.2749709499422168 Lambda1 0.027140828\n",
      "15 Train Loss 240.78336 Test MSE 257.6766084559797 Test RE 0.27022571116062555 Lambda1 0.044586908\n",
      "16 Train Loss 231.90157 Test MSE 249.37828098744527 Test RE 0.2658388711503938 Lambda1 0.063905075\n",
      "17 Train Loss 223.03548 Test MSE 237.3605348762362 Test RE 0.25935428434174973 Lambda1 0.091528326\n",
      "18 Train Loss 206.68742 Test MSE 222.9966401765272 Test RE 0.2513844040723929 Lambda1 0.12759645\n",
      "19 Train Loss 194.90225 Test MSE 207.04960594663694 Test RE 0.24222913302883373 Lambda1 0.16350634\n",
      "20 Train Loss 178.87343 Test MSE 188.19688972059183 Test RE 0.2309379952745639 Lambda1 0.21241587\n",
      "21 Train Loss 159.99483 Test MSE 169.00798536103255 Test RE 0.21884810128377408 Lambda1 0.280088\n",
      "22 Train Loss 140.83147 Test MSE 142.76702369911115 Test RE 0.20114216646085323 Lambda1 0.33172685\n",
      "23 Train Loss 127.436295 Test MSE 128.18753565994552 Test RE 0.19059524864461438 Lambda1 0.36201736\n",
      "24 Train Loss 110.3156 Test MSE 106.90153015771921 Test RE 0.17405284698579457 Lambda1 0.42070708\n",
      "25 Train Loss 94.0653 Test MSE 87.66900440883707 Test RE 0.15762032111477534 Lambda1 0.48205367\n",
      "26 Train Loss 77.445694 Test MSE 70.22968857383407 Test RE 0.14107483205393243 Lambda1 0.5352111\n",
      "27 Train Loss 63.489483 Test MSE 62.657984935073976 Test RE 0.13325311883125254 Lambda1 0.54746515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 Train Loss 55.41427 Test MSE 52.051923502365796 Test RE 0.1214528085245022 Lambda1 0.58444417\n",
      "29 Train Loss 51.427284 Test MSE 49.6064995356571 Test RE 0.11856553403876999 Lambda1 0.5933245\n",
      "30 Train Loss 47.80583 Test MSE 46.7314140119015 Test RE 0.11507835143629479 Lambda1 0.61243516\n",
      "31 Train Loss 45.763203 Test MSE 44.37799606560337 Test RE 0.11214321801967538 Lambda1 0.63274866\n",
      "32 Train Loss 44.35904 Test MSE 44.06141843689715 Test RE 0.11174250619271857 Lambda1 0.63983387\n",
      "33 Train Loss 43.264908 Test MSE 43.49408828697671 Test RE 0.1110207831301887 Lambda1 0.65121794\n",
      "34 Train Loss 41.856865 Test MSE 42.42352608650963 Test RE 0.10964593919504431 Lambda1 0.66964966\n",
      "35 Train Loss 40.628216 Test MSE 41.01364101512105 Test RE 0.10780858192666264 Lambda1 0.6876296\n",
      "36 Train Loss 39.56602 Test MSE 40.09498067890559 Test RE 0.10659434735912024 Lambda1 0.7020155\n",
      "37 Train Loss 38.690895 Test MSE 39.32835850355852 Test RE 0.10557037899950975 Lambda1 0.70854956\n",
      "38 Train Loss 37.57131 Test MSE 37.85892576223839 Test RE 0.10357938165061012 Lambda1 0.7407254\n",
      "39 Train Loss 36.72819 Test MSE 37.68406603359027 Test RE 0.10333990279536096 Lambda1 0.7501477\n",
      "40 Train Loss 35.61381 Test MSE 36.429347436990554 Test RE 0.10160495000934015 Lambda1 0.7837675\n",
      "41 Train Loss 34.909214 Test MSE 35.75388203233984 Test RE 0.10065857364991691 Lambda1 0.81492954\n",
      "42 Train Loss 34.366974 Test MSE 35.14147772918176 Test RE 0.09979279364425514 Lambda1 0.84025913\n",
      "43 Train Loss 33.4721 Test MSE 34.609647154248265 Test RE 0.09903478349214824 Lambda1 0.88093877\n",
      "44 Train Loss 32.86328 Test MSE 34.57323610189367 Test RE 0.09898267504992936 Lambda1 0.8937679\n",
      "45 Train Loss 32.464806 Test MSE 34.2382529879218 Test RE 0.09850198194053317 Lambda1 0.89242315\n",
      "46 Train Loss 31.962194 Test MSE 34.45242647576898 Test RE 0.0988095856041573 Lambda1 0.90094954\n",
      "47 Train Loss 31.827583 Test MSE 34.30779172079688 Test RE 0.0986019611737161 Lambda1 0.90887463\n",
      "48 Train Loss 31.65603 Test MSE 34.22246973859578 Test RE 0.09847927545851269 Lambda1 0.9183474\n",
      "49 Train Loss 31.431103 Test MSE 34.07370192868773 Test RE 0.09826499369103871 Lambda1 0.94508487\n",
      "50 Train Loss 31.275068 Test MSE 33.954266987238164 Test RE 0.09809262356911368 Lambda1 0.96167153\n",
      "51 Train Loss 31.112898 Test MSE 33.8558103551244 Test RE 0.097950301534656 Lambda1 0.97530794\n",
      "52 Train Loss 31.023129 Test MSE 33.915537484988555 Test RE 0.098036663556083 Lambda1 0.9674445\n",
      "53 Train Loss 30.861639 Test MSE 33.571682236168066 Test RE 0.09753842139826005 Lambda1 0.96267676\n",
      "54 Train Loss 30.775211 Test MSE 33.53779975553735 Test RE 0.0974891882688125 Lambda1 0.9656414\n",
      "55 Train Loss 30.668205 Test MSE 33.44528427521653 Test RE 0.09735463132495899 Lambda1 0.9689295\n",
      "56 Train Loss 30.62905 Test MSE 33.35835304507784 Test RE 0.09722802656469817 Lambda1 0.96687347\n",
      "57 Train Loss 30.464828 Test MSE 33.340769955915675 Test RE 0.09720239888466214 Lambda1 0.9899618\n",
      "58 Train Loss 30.423029 Test MSE 33.320107437082214 Test RE 0.09717227424036003 Lambda1 0.9962824\n",
      "59 Train Loss 30.381557 Test MSE 33.255939525341 Test RE 0.09707866189486114 Lambda1 0.99740344\n",
      "60 Train Loss 30.311886 Test MSE 33.197328067668536 Test RE 0.09699307671510154 Lambda1 1.0023447\n",
      "61 Train Loss 30.270021 Test MSE 33.06067603474595 Test RE 0.09679324182043943 Lambda1 1.0015479\n",
      "62 Train Loss 30.165613 Test MSE 32.88943125343165 Test RE 0.0965422358042809 Lambda1 1.0020171\n",
      "63 Train Loss 30.08462 Test MSE 32.81563791957737 Test RE 0.09643387008092613 Lambda1 1.0105239\n",
      "64 Train Loss 30.059198 Test MSE 32.78587503777767 Test RE 0.09639012872557297 Lambda1 1.0145407\n",
      "65 Train Loss 29.993427 Test MSE 32.75989402600978 Test RE 0.09635192920527219 Lambda1 1.0204631\n",
      "66 Train Loss 29.963938 Test MSE 32.65355504847955 Test RE 0.09619542238357551 Lambda1 1.0211433\n",
      "67 Train Loss 29.904894 Test MSE 32.57247514811189 Test RE 0.09607591988396157 Lambda1 1.0182614\n",
      "68 Train Loss 29.848577 Test MSE 32.51890381513719 Test RE 0.09599688024893134 Lambda1 1.0227447\n",
      "69 Train Loss 29.798317 Test MSE 32.40103230923255 Test RE 0.09582274201249953 Lambda1 1.0290114\n",
      "70 Train Loss 29.756178 Test MSE 32.47270831451605 Test RE 0.09592867069282819 Lambda1 1.029985\n",
      "71 Train Loss 29.71091 Test MSE 32.54347046970645 Test RE 0.09603313419179538 Lambda1 1.0308145\n",
      "72 Train Loss 29.669233 Test MSE 32.49145545441565 Test RE 0.09595635746145292 Lambda1 1.0323833\n",
      "73 Train Loss 29.600021 Test MSE 32.348011312565625 Test RE 0.09574430782223715 Lambda1 1.0234232\n",
      "74 Train Loss 29.53155 Test MSE 32.27640315315567 Test RE 0.09563827545929915 Lambda1 1.0143021\n",
      "Training time: 119.05\n",
      "Training time: 119.05\n",
      "inv_HT_stan_tune15\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 853.3165 Test MSE 858.9098046952972 Test RE 0.49335888148653473 Lambda1 0.042258393\n",
      "1 Train Loss 821.587 Test MSE 833.1565259919784 Test RE 0.48590623389880644 Lambda1 0.03536026\n",
      "2 Train Loss 786.5335 Test MSE 800.5937072842871 Test RE 0.4763160948990521 Lambda1 0.0145348\n",
      "3 Train Loss 638.0055 Test MSE 642.6244284361542 Test RE 0.4267443559658893 Lambda1 0.01421469\n",
      "4 Train Loss 441.15353 Test MSE 438.5561395540242 Test RE 0.35253461657175206 Lambda1 -0.008111046\n",
      "5 Train Loss 333.26508 Test MSE 340.36859715674524 Test RE 0.3105731543279737 Lambda1 -0.0013543025\n",
      "6 Train Loss 270.18732 Test MSE 290.6085098197912 Test RE 0.28697451750925923 Lambda1 -1.1176351e-05\n",
      "7 Train Loss 259.42493 Test MSE 282.9157731966032 Test RE 0.2831507727157024 Lambda1 0.0021449917\n",
      "8 Train Loss 256.2502 Test MSE 281.10447244292726 Test RE 0.282242914655277 Lambda1 0.0037879304\n",
      "9 Train Loss 254.51797 Test MSE 280.3299319878354 Test RE 0.2818538078098705 Lambda1 0.0049337484\n",
      "10 Train Loss 252.49832 Test MSE 276.76876174554565 Test RE 0.2800578213559067 Lambda1 0.009841047\n",
      "11 Train Loss 249.72847 Test MSE 271.9856303497814 Test RE 0.27762728789632163 Lambda1 0.017798927\n",
      "12 Train Loss 244.86964 Test MSE 264.2816783249192 Test RE 0.2736671684492692 Lambda1 0.032361165\n",
      "13 Train Loss 241.28407 Test MSE 260.1704452827779 Test RE 0.2715302070379079 Lambda1 0.041646782\n",
      "14 Train Loss 237.00688 Test MSE 252.63121386968965 Test RE 0.26756707747095215 Lambda1 0.058022097\n",
      "15 Train Loss 230.89075 Test MSE 244.0883981609782 Test RE 0.26300423352510455 Lambda1 0.075857416\n",
      "16 Train Loss 225.00526 Test MSE 237.10111288053548 Test RE 0.2592125156183552 Lambda1 0.08874923\n",
      "17 Train Loss 214.34596 Test MSE 222.82726721952744 Test RE 0.2512889187599717 Lambda1 0.11749422\n",
      "18 Train Loss 207.02805 Test MSE 217.7339167041014 Test RE 0.248400355531678 Lambda1 0.13172372\n",
      "19 Train Loss 197.04889 Test MSE 203.0551944699987 Test RE 0.23988120544816774 Lambda1 0.15877707\n",
      "20 Train Loss 190.21082 Test MSE 190.30746813017376 Test RE 0.23222933915825114 Lambda1 0.18374477\n",
      "21 Train Loss 181.04353 Test MSE 184.39307066661812 Test RE 0.2285922324115201 Lambda1 0.19559537\n",
      "22 Train Loss 174.8462 Test MSE 175.022681912703 Test RE 0.2227082666553416 Lambda1 0.2108678\n",
      "23 Train Loss 168.34299 Test MSE 162.01428725001932 Test RE 0.21427219962091704 Lambda1 0.24139649\n",
      "24 Train Loss 162.02693 Test MSE 150.704766896836 Test RE 0.20665821097621814 Lambda1 0.26459467\n",
      "25 Train Loss 152.91583 Test MSE 136.796752165184 Test RE 0.19689154363274042 Lambda1 0.3027297\n",
      "26 Train Loss 145.08524 Test MSE 125.36021347158226 Test RE 0.18848163139231106 Lambda1 0.3269999\n",
      "27 Train Loss 139.06068 Test MSE 120.413152977885 Test RE 0.18472519548872826 Lambda1 0.33969307\n",
      "28 Train Loss 130.82771 Test MSE 111.77955990061744 Test RE 0.17797965816834446 Lambda1 0.35531545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 Train Loss 121.775604 Test MSE 106.67080880733377 Test RE 0.17386491983943708 Lambda1 0.36446586\n",
      "30 Train Loss 115.17944 Test MSE 99.86190859915861 Test RE 0.16822444405651166 Lambda1 0.37633246\n",
      "31 Train Loss 108.03251 Test MSE 96.44977908207603 Test RE 0.16532547884072726 Lambda1 0.38269597\n",
      "32 Train Loss 104.4077 Test MSE 93.99412004901437 Test RE 0.1632072750925795 Lambda1 0.3871164\n",
      "33 Train Loss 98.24612 Test MSE 92.71487533537355 Test RE 0.16209285808568405 Lambda1 0.3889106\n",
      "34 Train Loss 93.35621 Test MSE 88.27126029302201 Test RE 0.15816079316539583 Lambda1 0.40269485\n",
      "35 Train Loss 88.16985 Test MSE 83.60860847737679 Test RE 0.15392695150338429 Lambda1 0.41914877\n",
      "36 Train Loss 83.80971 Test MSE 79.73678586417559 Test RE 0.15032061136834335 Lambda1 0.4278985\n",
      "37 Train Loss 80.72973 Test MSE 77.11132393806975 Test RE 0.14782512348807067 Lambda1 0.43085608\n",
      "38 Train Loss 76.99684 Test MSE 72.4402093197921 Test RE 0.14327783778090125 Lambda1 0.44591343\n",
      "39 Train Loss 74.654785 Test MSE 70.26600954616202 Test RE 0.1411113074594893 Lambda1 0.4565598\n",
      "40 Train Loss 69.373245 Test MSE 65.86958691132608 Test RE 0.13662546106530532 Lambda1 0.47531947\n",
      "41 Train Loss 67.05852 Test MSE 64.27393592410499 Test RE 0.13496048129955482 Lambda1 0.4829365\n",
      "42 Train Loss 65.20252 Test MSE 63.71855437372731 Test RE 0.13437612948235644 Lambda1 0.48539466\n",
      "43 Train Loss 63.32289 Test MSE 61.548302052578364 Test RE 0.1320678807320522 Lambda1 0.49194673\n",
      "44 Train Loss 61.410835 Test MSE 58.72113383363399 Test RE 0.12899901286534302 Lambda1 0.5108206\n",
      "45 Train Loss 59.744637 Test MSE 55.9033160883718 Test RE 0.1258658623791244 Lambda1 0.5291518\n",
      "46 Train Loss 58.451385 Test MSE 54.63583055743476 Test RE 0.12443081490952396 Lambda1 0.5358114\n",
      "47 Train Loss 57.599243 Test MSE 54.310616307315534 Test RE 0.12405993125967395 Lambda1 0.5390033\n",
      "48 Train Loss 56.11757 Test MSE 51.79965528965153 Test RE 0.12115814222319794 Lambda1 0.5599241\n",
      "49 Train Loss 54.823345 Test MSE 50.67484195300296 Test RE 0.11983546683427432 Lambda1 0.57858706\n",
      "50 Train Loss 53.071087 Test MSE 48.796367197159 Test RE 0.11759339150396307 Lambda1 0.59808016\n",
      "51 Train Loss 52.20421 Test MSE 48.66602879989905 Test RE 0.11743623653967751 Lambda1 0.6009513\n",
      "52 Train Loss 50.702065 Test MSE 47.18700814718025 Test RE 0.1156379520489633 Lambda1 0.6113881\n",
      "53 Train Loss 49.42106 Test MSE 46.0477025161559 Test RE 0.11423341335015834 Lambda1 0.623054\n",
      "54 Train Loss 48.412888 Test MSE 45.43086443786819 Test RE 0.11346571936079541 Lambda1 0.6292032\n",
      "55 Train Loss 46.81835 Test MSE 44.898879911927445 Test RE 0.11279943492996516 Lambda1 0.6368442\n",
      "56 Train Loss 45.951504 Test MSE 44.497096763875334 Test RE 0.11229360095041147 Lambda1 0.64729404\n",
      "57 Train Loss 45.47323 Test MSE 43.96725987692706 Test RE 0.11162304634626528 Lambda1 0.6571383\n",
      "58 Train Loss 44.718357 Test MSE 43.212828479783 Test RE 0.11066123623147654 Lambda1 0.6688774\n",
      "59 Train Loss 44.40256 Test MSE 43.06947086784919 Test RE 0.11047752562730505 Lambda1 0.67347777\n",
      "60 Train Loss 43.188946 Test MSE 42.066873241458225 Test RE 0.10918407196304125 Lambda1 0.7013497\n",
      "61 Train Loss 42.825417 Test MSE 41.72063084300562 Test RE 0.10873380952297497 Lambda1 0.7118786\n",
      "62 Train Loss 42.10952 Test MSE 41.21678739603813 Test RE 0.10807524772455863 Lambda1 0.73049575\n",
      "63 Train Loss 41.399483 Test MSE 40.78395066443535 Test RE 0.10750627569960157 Lambda1 0.74671924\n",
      "64 Train Loss 41.1438 Test MSE 40.70289185036377 Test RE 0.10739938726247025 Lambda1 0.7522967\n",
      "65 Train Loss 40.48769 Test MSE 39.98403354273777 Test RE 0.10644676616694325 Lambda1 0.7736038\n",
      "66 Train Loss 40.18058 Test MSE 39.6556152498319 Test RE 0.106008701962348 Lambda1 0.7931738\n",
      "67 Train Loss 39.329815 Test MSE 38.9066423826277 Test RE 0.10500284039580962 Lambda1 0.8157768\n",
      "68 Train Loss 38.766537 Test MSE 38.66480944936722 Test RE 0.10467599739882386 Lambda1 0.82020015\n",
      "69 Train Loss 38.34004 Test MSE 38.27899903227545 Test RE 0.10415244199319156 Lambda1 0.83369285\n",
      "70 Train Loss 37.888954 Test MSE 37.796644688180315 Test RE 0.10349414824374374 Lambda1 0.84652567\n",
      "71 Train Loss 37.07457 Test MSE 37.100729288437805 Test RE 0.10253694973954582 Lambda1 0.858388\n",
      "72 Train Loss 36.645164 Test MSE 36.783065933913875 Test RE 0.10209703582793296 Lambda1 0.8668557\n",
      "73 Train Loss 36.202568 Test MSE 36.70044796823994 Test RE 0.10198231196006657 Lambda1 0.8713657\n",
      "74 Train Loss 35.752125 Test MSE 36.580002279839476 Test RE 0.1018148286586477 Lambda1 0.88646704\n",
      "Training time: 118.72\n",
      "Training time: 118.72\n",
      "inv_HT_stan_tune15\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 1239.3206 Test MSE 856.9343584799441 Test RE 0.49279120548645555 Lambda1 0.094144285\n",
      "1 Train Loss 787.5059 Test MSE 801.1276591612872 Test RE 0.4764749067150434 Lambda1 0.08307259\n",
      "2 Train Loss 659.4644 Test MSE 637.5678609502118 Test RE 0.4250620955576318 Lambda1 0.07845356\n",
      "3 Train Loss 484.6551 Test MSE 488.0108251565104 Test RE 0.3718809215393679 Lambda1 0.022408428\n",
      "4 Train Loss 326.22525 Test MSE 332.7724565932204 Test RE 0.30708800782002627 Lambda1 0.0017120319\n",
      "5 Train Loss 275.0819 Test MSE 294.03266812051584 Test RE 0.2886602364586578 Lambda1 0.0007634522\n",
      "6 Train Loss 259.87692 Test MSE 283.7589373699946 Test RE 0.28357239108291393 Lambda1 0.0009858778\n",
      "7 Train Loss 257.5145 Test MSE 281.917491161937 Test RE 0.28265077563901014 Lambda1 0.0007842775\n",
      "8 Train Loss 255.90404 Test MSE 281.20695592315525 Test RE 0.2822943592303898 Lambda1 0.0016891368\n",
      "9 Train Loss 254.9588 Test MSE 280.6962858033629 Test RE 0.28203792033582936 Lambda1 0.0022164902\n",
      "10 Train Loss 254.15073 Test MSE 279.88615146761316 Test RE 0.2816306229884672 Lambda1 0.0028555572\n",
      "11 Train Loss 253.27226 Test MSE 276.8305654291253 Test RE 0.28008908868301013 Lambda1 0.0066435905\n",
      "12 Train Loss 251.24605 Test MSE 271.9266980689548 Test RE 0.2775972089255437 Lambda1 0.016700948\n",
      "13 Train Loss 246.05562 Test MSE 268.5429441643759 Test RE 0.27586464439910774 Lambda1 0.02443013\n",
      "14 Train Loss 240.27054 Test MSE 258.3247458632956 Test RE 0.27056534888722866 Lambda1 0.04551827\n",
      "15 Train Loss 232.80318 Test MSE 247.00337891606878 Test RE 0.2645700124607075 Lambda1 0.068313956\n",
      "16 Train Loss 225.5192 Test MSE 237.02060297691855 Test RE 0.2591685028630439 Lambda1 0.0928273\n",
      "17 Train Loss 218.07619 Test MSE 229.20343501096693 Test RE 0.25485885734980973 Lambda1 0.108769335\n",
      "18 Train Loss 204.9732 Test MSE 217.24622559718293 Test RE 0.2481220099308428 Lambda1 0.14395146\n",
      "19 Train Loss 196.85759 Test MSE 208.65025649186074 Test RE 0.2431636378885247 Lambda1 0.16457823\n",
      "20 Train Loss 188.79948 Test MSE 196.94299433511688 Test RE 0.23624326652604463 Lambda1 0.18405555\n",
      "21 Train Loss 179.2052 Test MSE 185.99452696399732 Test RE 0.2295827497026238 Lambda1 0.20434695\n",
      "22 Train Loss 170.2515 Test MSE 174.1172230762726 Test RE 0.22213144241258093 Lambda1 0.22846143\n",
      "23 Train Loss 164.9276 Test MSE 167.71536283850864 Test RE 0.21800958775118434 Lambda1 0.23432024\n",
      "24 Train Loss 159.73032 Test MSE 157.92599851838924 Test RE 0.2115514402527808 Lambda1 0.24816786\n",
      "25 Train Loss 151.37741 Test MSE 145.42632243611564 Test RE 0.20300684489407284 Lambda1 0.26984435\n",
      "26 Train Loss 143.93652 Test MSE 137.90080849845307 Test RE 0.19768448099481103 Lambda1 0.2872468\n",
      "27 Train Loss 135.93846 Test MSE 130.6765424373195 Test RE 0.19243673869471203 Lambda1 0.2991916\n",
      "28 Train Loss 130.38191 Test MSE 123.9911120153857 Test RE 0.18744956983580469 Lambda1 0.3074915\n",
      "29 Train Loss 125.154305 Test MSE 117.40566634262886 Test RE 0.1824037234780958 Lambda1 0.31409255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Train Loss 120.185936 Test MSE 105.74372368028502 Test RE 0.17310773360873288 Lambda1 0.3346005\n",
      "31 Train Loss 117.4886 Test MSE 100.85402767463687 Test RE 0.1690580261393584 Lambda1 0.3447059\n",
      "32 Train Loss 111.30617 Test MSE 97.65742004750057 Test RE 0.16635727353264376 Lambda1 0.35907537\n",
      "33 Train Loss 106.949005 Test MSE 91.79448740891233 Test RE 0.1612862970603251 Lambda1 0.37151897\n",
      "34 Train Loss 100.01754 Test MSE 84.08080424147825 Test RE 0.15436100560877763 Lambda1 0.3870953\n",
      "35 Train Loss 97.12831 Test MSE 80.9944833619688 Test RE 0.1515014852214435 Lambda1 0.38561946\n",
      "36 Train Loss 93.166115 Test MSE 80.78294427555613 Test RE 0.15130351223831592 Lambda1 0.38101602\n",
      "37 Train Loss 88.42285 Test MSE 77.25275944861244 Test RE 0.1479606298082295 Lambda1 0.39170548\n",
      "38 Train Loss 86.69093 Test MSE 77.68504542525618 Test RE 0.14837402654422718 Lambda1 0.39051843\n",
      "39 Train Loss 84.31941 Test MSE 78.75622053052852 Test RE 0.14939346621069227 Lambda1 0.38807833\n",
      "40 Train Loss 81.97407 Test MSE 76.77925637271714 Test RE 0.1475064874708995 Lambda1 0.39483374\n",
      "41 Train Loss 79.677734 Test MSE 73.67871173576319 Test RE 0.14449744982001891 Lambda1 0.40996844\n",
      "42 Train Loss 76.91063 Test MSE 73.17835329417319 Test RE 0.14400596669393734 Lambda1 0.41589814\n",
      "43 Train Loss 75.06928 Test MSE 70.97134727351025 Test RE 0.14181778417095087 Lambda1 0.42505682\n",
      "44 Train Loss 73.56897 Test MSE 69.86913986645534 Test RE 0.14071223754456413 Lambda1 0.43575966\n",
      "45 Train Loss 70.752686 Test MSE 66.52991268463134 Test RE 0.1373085709975769 Lambda1 0.45492402\n",
      "46 Train Loss 68.8669 Test MSE 63.70506107414757 Test RE 0.13436190071186008 Lambda1 0.4761478\n",
      "47 Train Loss 65.47446 Test MSE 61.250984229037 Test RE 0.131748508234424 Lambda1 0.48538655\n",
      "48 Train Loss 64.25215 Test MSE 60.1761232363436 Test RE 0.13058739946765613 Lambda1 0.49642918\n",
      "49 Train Loss 63.094433 Test MSE 58.841117063923086 Test RE 0.12913073562875801 Lambda1 0.51112443\n",
      "50 Train Loss 62.63439 Test MSE 58.58016566254243 Test RE 0.12884407987443902 Lambda1 0.5147453\n",
      "51 Train Loss 62.12595 Test MSE 58.25628724334689 Test RE 0.12848740918680043 Lambda1 0.5186406\n",
      "52 Train Loss 61.08803 Test MSE 57.71660481367713 Test RE 0.12789087493678933 Lambda1 0.5191669\n",
      "53 Train Loss 59.747993 Test MSE 56.91994851205363 Test RE 0.1270051757817372 Lambda1 0.5228137\n",
      "54 Train Loss 58.055218 Test MSE 55.9420655389589 Test RE 0.1259094768593009 Lambda1 0.5332128\n",
      "55 Train Loss 57.67657 Test MSE 55.834285634429705 Test RE 0.12578812762128205 Lambda1 0.5348981\n",
      "56 Train Loss 56.858517 Test MSE 55.58071615994421 Test RE 0.12550217137416098 Lambda1 0.5377021\n",
      "57 Train Loss 55.950165 Test MSE 54.53802794938737 Test RE 0.12431939435321097 Lambda1 0.5448422\n",
      "58 Train Loss 54.778503 Test MSE 53.575376194688204 Test RE 0.12321732757020117 Lambda1 0.55670935\n",
      "59 Train Loss 53.290615 Test MSE 51.821268005629875 Test RE 0.12118341539750001 Lambda1 0.5828278\n",
      "60 Train Loss 52.535522 Test MSE 50.4090201951195 Test RE 0.11952074696623094 Lambda1 0.60094357\n",
      "61 Train Loss 52.04133 Test MSE 49.66627151801608 Test RE 0.118636943668256 Lambda1 0.60860157\n",
      "62 Train Loss 51.50874 Test MSE 49.446648863827505 Test RE 0.11837434867663438 Lambda1 0.6084514\n",
      "63 Train Loss 50.068336 Test MSE 47.545307314021116 Test RE 0.11607615134085092 Lambda1 0.6370371\n",
      "64 Train Loss 49.157326 Test MSE 46.69315980353234 Test RE 0.11503124038037783 Lambda1 0.6478835\n",
      "65 Train Loss 48.434845 Test MSE 45.59865487752552 Test RE 0.1136750585050703 Lambda1 0.6696635\n",
      "66 Train Loss 47.879635 Test MSE 44.29290057723978 Test RE 0.11203564825766087 Lambda1 0.690618\n",
      "67 Train Loss 47.19146 Test MSE 43.48057595818183 Test RE 0.11100353635120176 Lambda1 0.7012185\n",
      "68 Train Loss 45.647953 Test MSE 42.61961459654564 Test RE 0.10989904783064541 Lambda1 0.70966333\n",
      "69 Train Loss 45.04969 Test MSE 42.44252786395388 Test RE 0.10967049201607214 Lambda1 0.7110007\n",
      "70 Train Loss 44.75734 Test MSE 42.50622615564568 Test RE 0.10975275860342776 Lambda1 0.7095739\n",
      "71 Train Loss 44.277622 Test MSE 42.662662012271596 Test RE 0.10995453491163289 Lambda1 0.703324\n",
      "72 Train Loss 43.967102 Test MSE 42.36106896273972 Test RE 0.10956519752878058 Lambda1 0.7080226\n",
      "73 Train Loss 43.384106 Test MSE 41.73332856421377 Test RE 0.10875035489315599 Lambda1 0.72043586\n",
      "74 Train Loss 42.847588 Test MSE 41.478678375878324 Test RE 0.10841805845618536 Lambda1 0.72385144\n",
      "Training time: 120.37\n",
      "Training time: 120.37\n",
      "inv_HT_stan_tune15\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 952.05054 Test MSE 858.3495498313114 Test RE 0.49319794968653785 Lambda1 0.1440584\n",
      "1 Train Loss 841.87744 Test MSE 854.7082495607402 Test RE 0.49215071286661083 Lambda1 0.14746259\n",
      "2 Train Loss 829.19006 Test MSE 845.467602955021 Test RE 0.4894830483244961 Lambda1 0.15259962\n",
      "3 Train Loss 819.6691 Test MSE 834.3338456278468 Test RE 0.48624942571247376 Lambda1 0.16192873\n",
      "4 Train Loss 796.8085 Test MSE 809.976107504959 Test RE 0.4790990115031189 Lambda1 0.17925726\n",
      "5 Train Loss 751.9599 Test MSE 767.7059769562594 Test RE 0.4664301673178789 Lambda1 0.19229314\n",
      "6 Train Loss 717.4679 Test MSE 741.818787525283 Test RE 0.45849867476706474 Lambda1 0.19629328\n",
      "7 Train Loss 678.24286 Test MSE 697.5233754039822 Test RE 0.4445990739377408 Lambda1 0.1984896\n",
      "8 Train Loss 599.7079 Test MSE 594.124233819366 Test RE 0.41032483568089434 Lambda1 0.16971046\n",
      "9 Train Loss 502.72455 Test MSE 484.7155794312586 Test RE 0.3706232499549753 Lambda1 0.15576005\n",
      "10 Train Loss 458.23175 Test MSE 415.0299757009222 Test RE 0.34294849639250985 Lambda1 0.15116382\n",
      "11 Train Loss 411.64737 Test MSE 386.4469442878412 Test RE 0.3309284532473854 Lambda1 0.14582194\n",
      "12 Train Loss 364.36243 Test MSE 333.558335758362 Test RE 0.3074504052272039 Lambda1 0.13503288\n",
      "13 Train Loss 315.09457 Test MSE 287.99928875128774 Test RE 0.2856833160388029 Lambda1 0.117191896\n",
      "14 Train Loss 288.81787 Test MSE 273.4558533033118 Test RE 0.27837663612217195 Lambda1 0.09853803\n",
      "15 Train Loss 277.01517 Test MSE 266.4441507425928 Test RE 0.27478452180785445 Lambda1 0.09228995\n",
      "16 Train Loss 255.77332 Test MSE 256.68506798400904 Test RE 0.2697052953065011 Lambda1 0.099844806\n",
      "17 Train Loss 249.58751 Test MSE 256.5657061028184 Test RE 0.2696425797870045 Lambda1 0.10541696\n",
      "18 Train Loss 243.70206 Test MSE 256.18187199485044 Test RE 0.2694408054363159 Lambda1 0.1080398\n",
      "19 Train Loss 236.30432 Test MSE 251.2216292189069 Test RE 0.26681957281354973 Lambda1 0.12569496\n",
      "20 Train Loss 232.65883 Test MSE 246.68785917927195 Test RE 0.26440097887035596 Lambda1 0.13311839\n",
      "21 Train Loss 220.96767 Test MSE 233.16494604614203 Test RE 0.25705188932752865 Lambda1 0.16143797\n",
      "22 Train Loss 214.29143 Test MSE 226.9751862174375 Test RE 0.25361700008006943 Lambda1 0.18002698\n",
      "23 Train Loss 210.42967 Test MSE 214.63998437650469 Test RE 0.24662919458659882 Lambda1 0.21330762\n",
      "24 Train Loss 202.44414 Test MSE 205.0995860392088 Test RE 0.24108576193943818 Lambda1 0.23106347\n",
      "25 Train Loss 198.62216 Test MSE 204.10137474709143 Test RE 0.24049836909467762 Lambda1 0.2319561\n",
      "26 Train Loss 197.43332 Test MSE 202.9925114320167 Test RE 0.23984417698561258 Lambda1 0.2309106\n",
      "27 Train Loss 193.79182 Test MSE 201.08831474453797 Test RE 0.23871658219202363 Lambda1 0.23645484\n",
      "28 Train Loss 189.82301 Test MSE 196.6465750052273 Test RE 0.2360654144518613 Lambda1 0.24637339\n",
      "29 Train Loss 186.9005 Test MSE 195.37618187909797 Test RE 0.23530165387276838 Lambda1 0.24739759\n",
      "30 Train Loss 180.17653 Test MSE 188.54686381523115 Test RE 0.23115262362521552 Lambda1 0.26162598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 Train Loss 177.91624 Test MSE 185.89946308763822 Test RE 0.2295240710523574 Lambda1 0.27106443\n",
      "32 Train Loss 171.59796 Test MSE 179.05750657885866 Test RE 0.22526070391453007 Lambda1 0.28606707\n",
      "33 Train Loss 167.55765 Test MSE 171.35994817492085 Test RE 0.22036561594970813 Lambda1 0.3015469\n",
      "34 Train Loss 166.38255 Test MSE 172.55401524479026 Test RE 0.22113205680016093 Lambda1 0.30109432\n",
      "35 Train Loss 164.51205 Test MSE 169.8470075746573 Test RE 0.21939065284833634 Lambda1 0.31316316\n",
      "36 Train Loss 156.65405 Test MSE 162.0948397033739 Test RE 0.21432546037582456 Lambda1 0.32122436\n",
      "37 Train Loss 147.90918 Test MSE 153.32105549835927 Test RE 0.20844432263872964 Lambda1 0.34530988\n",
      "38 Train Loss 141.383 Test MSE 145.5081102328251 Test RE 0.20306392241789506 Lambda1 0.35976416\n",
      "39 Train Loss 132.80173 Test MSE 127.12852518330365 Test RE 0.189806322596106 Lambda1 0.4118569\n",
      "40 Train Loss 125.1564 Test MSE 121.4052144520468 Test RE 0.18548459275356988 Lambda1 0.4324694\n",
      "41 Train Loss 117.14472 Test MSE 108.05038267077005 Test RE 0.17498560576433042 Lambda1 0.46498817\n",
      "42 Train Loss 113.27621 Test MSE 105.1552567615517 Test RE 0.17262538680067951 Lambda1 0.48239568\n",
      "43 Train Loss 110.71207 Test MSE 104.7209158212514 Test RE 0.1722685056547518 Lambda1 0.48966643\n",
      "44 Train Loss 106.032104 Test MSE 94.74165746841759 Test RE 0.1638549854534039 Lambda1 0.52405053\n",
      "45 Train Loss 100.11609 Test MSE 91.69800609787598 Test RE 0.16120151416709505 Lambda1 0.53483987\n",
      "46 Train Loss 97.13296 Test MSE 88.74642923854807 Test RE 0.15858591585014525 Lambda1 0.54207003\n",
      "47 Train Loss 90.096436 Test MSE 77.78548250516585 Test RE 0.14846991012324154 Lambda1 0.566265\n",
      "48 Train Loss 82.433304 Test MSE 75.82974703966387 Test RE 0.14659156259849423 Lambda1 0.57224536\n",
      "49 Train Loss 75.73265 Test MSE 67.7768648028437 Test RE 0.13858936612077924 Lambda1 0.5891426\n",
      "50 Train Loss 74.6108 Test MSE 66.97561662342615 Test RE 0.13776773900765324 Lambda1 0.59094644\n",
      "51 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "52 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "53 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "54 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "55 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "56 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "57 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "58 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "59 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "60 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "61 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "62 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "63 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "64 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "65 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "66 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "67 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "68 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "69 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "70 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "71 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "72 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "73 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "74 Train Loss 71.57378 Test MSE 64.49985335945514 Test RE 0.1351974605663297 Lambda1 0.5984813\n",
      "Training time: 124.52\n",
      "Training time: 124.52\n",
      "inv_HT_stan_tune15\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 1292.9366 Test MSE 883.5143130657807 Test RE 0.5003754174112057 Lambda1 0.035876743\n",
      "1 Train Loss 835.4511 Test MSE 853.4350804351258 Test RE 0.49178402364325 Lambda1 0.03461919\n",
      "2 Train Loss 829.72833 Test MSE 848.2830623912229 Test RE 0.49029737536839685 Lambda1 0.036620595\n",
      "3 Train Loss 820.06604 Test MSE 839.0322903178303 Test RE 0.4876166294330148 Lambda1 0.043243565\n",
      "4 Train Loss 798.5097 Test MSE 818.1361808964365 Test RE 0.481506296029073 Lambda1 0.055566777\n",
      "5 Train Loss 761.4323 Test MSE 762.786114391476 Test RE 0.4649332005478031 Lambda1 0.07586082\n",
      "6 Train Loss 674.77155 Test MSE 686.6565133113162 Test RE 0.4411222283213585 Lambda1 0.089075975\n",
      "7 Train Loss 499.0278 Test MSE 461.83263740315783 Test RE 0.3617691097134075 Lambda1 0.055712976\n",
      "8 Train Loss 312.25018 Test MSE 299.2042694404901 Test RE 0.2911877252832084 Lambda1 0.0056070224\n",
      "9 Train Loss 280.062 Test MSE 280.33363308129094 Test RE 0.2818556684100444 Lambda1 0.01254137\n",
      "10 Train Loss 263.38626 Test MSE 277.69620795548155 Test RE 0.2805266628042015 Lambda1 0.014405152\n",
      "11 Train Loss 247.84789 Test MSE 265.76930685525974 Test RE 0.27443631708885186 Lambda1 0.029702656\n",
      "12 Train Loss 233.74768 Test MSE 245.759235529926 Test RE 0.2639028584846154 Lambda1 0.0664063\n",
      "13 Train Loss 212.75592 Test MSE 211.01863962852852 Test RE 0.24453981544447767 Lambda1 0.1377044\n",
      "14 Train Loss 200.18585 Test MSE 189.26683765610696 Test RE 0.2315935359465887 Lambda1 0.18057689\n",
      "15 Train Loss 184.19363 Test MSE 178.24173491514466 Test RE 0.22474698326327586 Lambda1 0.21350633\n",
      "16 Train Loss 165.13321 Test MSE 156.8417065873235 Test RE 0.2108239520433116 Lambda1 0.27929217\n",
      "17 Train Loss 148.65569 Test MSE 146.0021604110379 Test RE 0.203408366286774 Lambda1 0.31880236\n",
      "18 Train Loss 138.5505 Test MSE 135.5877987064671 Test RE 0.19601958960420982 Lambda1 0.3454995\n",
      "19 Train Loss 123.66282 Test MSE 121.33552106268648 Test RE 0.18543134584096094 Lambda1 0.3873658\n",
      "20 Train Loss 116.92213 Test MSE 113.17804082961366 Test RE 0.1790895546722967 Lambda1 0.40346852\n",
      "21 Train Loss 105.31093 Test MSE 99.79502388732571 Test RE 0.1681680986077715 Lambda1 0.42000782\n",
      "22 Train Loss 95.24683 Test MSE 89.1279253462598 Test RE 0.15892640859714077 Lambda1 0.44499093\n",
      "23 Train Loss 85.35634 Test MSE 75.11388945897264 Test RE 0.1458979857024061 Lambda1 0.48524353\n",
      "24 Train Loss 79.25923 Test MSE 66.38441890758729 Test RE 0.137158349270591 Lambda1 0.5141408\n",
      "25 Train Loss 73.517235 Test MSE 62.53182816274956 Test RE 0.13311890406430815 Lambda1 0.5388936\n",
      "26 Train Loss 68.52195 Test MSE 61.75913328759977 Test RE 0.13229388396504688 Lambda1 0.53972167\n",
      "27 Train Loss 63.72775 Test MSE 58.707075908867644 Test RE 0.12898357066550342 Lambda1 0.5645843\n",
      "28 Train Loss 59.299633 Test MSE 53.17160209006957 Test RE 0.12275213195004121 Lambda1 0.59318244\n",
      "29 Train Loss 56.78843 Test MSE 51.20230941675338 Test RE 0.12045752773858942 Lambda1 0.5955229\n",
      "30 Train Loss 54.97805 Test MSE 50.90423596116821 Test RE 0.12010639514328873 Lambda1 0.60027754\n",
      "31 Train Loss 52.04376 Test MSE 47.514120310301884 Test RE 0.11603807543596975 Lambda1 0.64167625\n",
      "32 Train Loss 50.808613 Test MSE 46.056859103065285 Test RE 0.11424477044428703 Lambda1 0.64936936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 Train Loss 48.88448 Test MSE 45.3465381104124 Test RE 0.11336036596701962 Lambda1 0.6654334\n",
      "34 Train Loss 46.833412 Test MSE 44.09082392270727 Test RE 0.11177978704700552 Lambda1 0.6894078\n",
      "35 Train Loss 45.430305 Test MSE 43.25286289647877 Test RE 0.11071248527688565 Lambda1 0.70519185\n",
      "36 Train Loss 43.64657 Test MSE 42.739542776213305 Test RE 0.11005356276513788 Lambda1 0.7101178\n",
      "37 Train Loss 42.74237 Test MSE 42.704222793452665 Test RE 0.11000807920246485 Lambda1 0.7103111\n",
      "38 Train Loss 41.897327 Test MSE 41.97620990384208 Test RE 0.1090663506897956 Lambda1 0.71987474\n",
      "39 Train Loss 40.82419 Test MSE 41.39589574475982 Test RE 0.10830981471409977 Lambda1 0.7378901\n",
      "40 Train Loss 40.34397 Test MSE 41.21677251149426 Test RE 0.10807522821004754 Lambda1 0.74234927\n",
      "41 Train Loss 39.870724 Test MSE 40.76775149106488 Test RE 0.10748492311194871 Lambda1 0.7516559\n",
      "42 Train Loss 39.352062 Test MSE 40.20399824906348 Test RE 0.10673916309632196 Lambda1 0.7720149\n",
      "43 Train Loss 38.889362 Test MSE 39.919207320549 Test RE 0.10636043994695527 Lambda1 0.7874949\n",
      "44 Train Loss 38.366547 Test MSE 39.63052176833319 Test RE 0.10597515629245914 Lambda1 0.80084276\n",
      "45 Train Loss 38.14155 Test MSE 39.33011820897043 Test RE 0.10557274079003859 Lambda1 0.81251806\n",
      "46 Train Loss 37.741257 Test MSE 38.885201679686176 Test RE 0.10497390388492908 Lambda1 0.82828474\n",
      "47 Train Loss 37.409138 Test MSE 38.535343383543335 Test RE 0.10450060078834607 Lambda1 0.8437866\n",
      "48 Train Loss 36.779213 Test MSE 38.05686493300742 Test RE 0.1038498025348344 Lambda1 0.86687195\n",
      "49 Train Loss 36.161842 Test MSE 37.760910697746645 Test RE 0.10344521356374448 Lambda1 0.870684\n",
      "50 Train Loss 35.756893 Test MSE 37.517130455540226 Test RE 0.10311075748664576 Lambda1 0.87272877\n",
      "51 Train Loss 35.53551 Test MSE 37.55738034985987 Test RE 0.10316605335434104 Lambda1 0.86930054\n",
      "52 Train Loss 35.114094 Test MSE 37.5924887037189 Test RE 0.10321426151232241 Lambda1 0.87495387\n",
      "53 Train Loss 34.94875 Test MSE 37.5875505635126 Test RE 0.10320748218949241 Lambda1 0.8844223\n",
      "54 Train Loss 34.711254 Test MSE 37.51797512171255 Test RE 0.10311191820546392 Lambda1 0.90218717\n",
      "55 Train Loss 34.618717 Test MSE 37.50447270950964 Test RE 0.10309336196800196 Lambda1 0.9066668\n",
      "56 Train Loss 34.517532 Test MSE 37.4749744233202 Test RE 0.10305281112854732 Lambda1 0.9097828\n",
      "57 Train Loss 34.168255 Test MSE 37.54692248754307 Test RE 0.10315168904715433 Lambda1 0.93687713\n",
      "58 Train Loss 34.01106 Test MSE 37.35757534446015 Test RE 0.1028912660528458 Lambda1 0.94746643\n",
      "59 Train Loss 33.774437 Test MSE 37.28659876731707 Test RE 0.10279347675790859 Lambda1 0.97105825\n",
      "60 Train Loss 33.699203 Test MSE 37.235709155465024 Test RE 0.10272330535030348 Lambda1 0.9829748\n",
      "61 Train Loss 33.642212 Test MSE 37.320523030593854 Test RE 0.10284022814126216 Lambda1 0.98464775\n",
      "62 Train Loss 33.56152 Test MSE 37.19988465500098 Test RE 0.10267387837892297 Lambda1 0.9865548\n",
      "63 Train Loss 33.41162 Test MSE 37.01555561551194 Test RE 0.10241918295997186 Lambda1 1.0049561\n",
      "64 Train Loss 33.283073 Test MSE 36.803754988854834 Test RE 0.10212574461270751 Lambda1 1.0203424\n",
      "65 Train Loss 33.218193 Test MSE 36.766645534459336 Test RE 0.10207424461667675 Lambda1 1.0233246\n",
      "66 Train Loss 33.15003 Test MSE 36.65133234170683 Test RE 0.10191404845937167 Lambda1 1.0269231\n",
      "67 Train Loss 33.09545 Test MSE 36.572634757553516 Test RE 0.1018045749579025 Lambda1 1.0359279\n",
      "68 Train Loss 33.015663 Test MSE 36.47971722976071 Test RE 0.10167516884104057 Lambda1 1.0414072\n",
      "69 Train Loss 32.92327 Test MSE 36.35515947077261 Test RE 0.10150143859976528 Lambda1 1.0558693\n",
      "70 Train Loss 32.810875 Test MSE 36.31399779499535 Test RE 0.10144396185338288 Lambda1 1.070638\n",
      "71 Train Loss 32.73939 Test MSE 36.25319334531052 Test RE 0.10135899697960911 Lambda1 1.0810089\n",
      "72 Train Loss 32.67655 Test MSE 36.321687446211065 Test RE 0.10145470189084185 Lambda1 1.0835822\n",
      "73 Train Loss 32.65717 Test MSE 36.283389755788455 Test RE 0.10140120072123758 Lambda1 1.0843377\n",
      "74 Train Loss 32.593536 Test MSE 36.32616596922032 Test RE 0.10146095646274601 Lambda1 1.0851002\n",
      "Training time: 120.46\n",
      "Training time: 120.46\n",
      "inv_HT_stan_tune15\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 1046.806 Test MSE 845.2105186347975 Test RE 0.4894086232467123 Lambda1 0.06599717\n",
      "1 Train Loss 833.2749 Test MSE 851.2369402026545 Test RE 0.4911502863833141 Lambda1 0.06487096\n",
      "2 Train Loss 809.0516 Test MSE 825.6140785882227 Test RE 0.4837018135043453 Lambda1 0.07008468\n",
      "3 Train Loss 728.05695 Test MSE 725.8000376482978 Test RE 0.4535212738474246 Lambda1 0.06729342\n",
      "4 Train Loss 543.49854 Test MSE 518.4521267534101 Test RE 0.38330413186912055 Lambda1 0.03994762\n",
      "5 Train Loss 405.92462 Test MSE 383.2493126531721 Test RE 0.32955648565067025 Lambda1 0.005918081\n",
      "6 Train Loss 311.22952 Test MSE 321.3788553873697 Test RE 0.3017851186632662 Lambda1 -0.00021174768\n",
      "7 Train Loss 284.93942 Test MSE 302.8424068443813 Test RE 0.29295270683758273 Lambda1 -0.00062192895\n",
      "8 Train Loss 264.69553 Test MSE 285.3975296994797 Test RE 0.2843899700358668 Lambda1 -1.1346478e-05\n",
      "9 Train Loss 260.95456 Test MSE 283.84844933461363 Test RE 0.2836171141154419 Lambda1 0.00016037705\n",
      "10 Train Loss 258.38348 Test MSE 282.6085165540741 Test RE 0.2829969750167056 Lambda1 4.6668814e-05\n",
      "11 Train Loss 256.93744 Test MSE 282.19080257085096 Test RE 0.2827877536021667 Lambda1 0.00010193368\n",
      "12 Train Loss 256.43036 Test MSE 281.8945190530901 Test RE 0.28263925947405905 Lambda1 0.00013246712\n",
      "13 Train Loss 255.83102 Test MSE 281.3641150901048 Test RE 0.2823732316560183 Lambda1 0.0003016556\n",
      "14 Train Loss 255.3526 Test MSE 281.23531173565334 Test RE 0.28230859160261745 Lambda1 0.0005021881\n",
      "15 Train Loss 254.9183 Test MSE 281.1934521000224 Test RE 0.2822875811283837 Lambda1 0.0005649007\n",
      "16 Train Loss 254.62436 Test MSE 281.19330719177566 Test RE 0.2822875083923306 Lambda1 0.0006941018\n",
      "17 Train Loss 254.33183 Test MSE 281.12053700850714 Test RE 0.28225097935271054 Lambda1 0.0015772237\n",
      "18 Train Loss 253.86949 Test MSE 280.00036604773237 Test RE 0.28168808035512305 Lambda1 0.0029429495\n",
      "19 Train Loss 253.08788 Test MSE 277.9862055872879 Test RE 0.28067310131807255 Lambda1 0.0057111746\n",
      "20 Train Loss 251.51863 Test MSE 275.6086189782333 Test RE 0.2794702401368368 Lambda1 0.010579417\n",
      "21 Train Loss 248.77742 Test MSE 270.94026053355526 Test RE 0.27709324756834125 Lambda1 0.01933921\n",
      "22 Train Loss 246.5959 Test MSE 266.4703734455351 Test RE 0.2747980432448769 Lambda1 0.027107472\n",
      "23 Train Loss 241.8307 Test MSE 257.29764090024605 Test RE 0.2700269262219202 Lambda1 0.043896478\n",
      "24 Train Loss 237.62717 Test MSE 251.2903599472814 Test RE 0.2668560693714084 Lambda1 0.0570907\n",
      "25 Train Loss 230.27185 Test MSE 244.97020155706932 Test RE 0.26347887497978867 Lambda1 0.071941435\n",
      "26 Train Loss 221.71284 Test MSE 236.90149956592606 Test RE 0.2591033782892898 Lambda1 0.09567325\n",
      "27 Train Loss 215.50066 Test MSE 221.31444457457837 Test RE 0.25043443849191654 Lambda1 0.12797739\n",
      "28 Train Loss 208.17273 Test MSE 214.30382606365262 Test RE 0.24643598981020623 Lambda1 0.14034986\n",
      "29 Train Loss 195.89546 Test MSE 199.2533168471698 Test RE 0.2376249018075697 Lambda1 0.17725308\n",
      "30 Train Loss 188.38939 Test MSE 190.63573084285085 Test RE 0.23242953988742981 Lambda1 0.19507788\n",
      "31 Train Loss 176.37375 Test MSE 177.82114338153536 Test RE 0.22448166238536507 Lambda1 0.22191273\n",
      "32 Train Loss 166.97661 Test MSE 170.02231383475814 Test RE 0.21950384482130386 Lambda1 0.23994473\n",
      "33 Train Loss 160.09781 Test MSE 162.8717372933173 Test RE 0.2148384621999328 Lambda1 0.24432136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 Train Loss 151.78319 Test MSE 153.7108890186872 Test RE 0.20870914927145612 Lambda1 0.26314932\n",
      "35 Train Loss 144.88962 Test MSE 147.376663282432 Test RE 0.2043635933854278 Lambda1 0.27820662\n",
      "36 Train Loss 137.15121 Test MSE 137.82455051081075 Test RE 0.19762981450830044 Lambda1 0.2865861\n",
      "37 Train Loss 132.86786 Test MSE 129.60308301666646 Test RE 0.19164471050105866 Lambda1 0.30128822\n",
      "38 Train Loss 129.34546 Test MSE 128.05999963501986 Test RE 0.1905004117740857 Lambda1 0.30840233\n",
      "39 Train Loss 125.05443 Test MSE 123.73825498691627 Test RE 0.1872583378597414 Lambda1 0.32278207\n",
      "40 Train Loss 121.403046 Test MSE 119.92841163653044 Test RE 0.18435300093671958 Lambda1 0.3298518\n",
      "41 Train Loss 117.1653 Test MSE 113.1858365071527 Test RE 0.17909572238985144 Lambda1 0.34106869\n",
      "42 Train Loss 112.93906 Test MSE 109.81481655387496 Test RE 0.17640855429271474 Lambda1 0.34687713\n",
      "43 Train Loss 109.29322 Test MSE 103.09250409684076 Test RE 0.1709238690429724 Lambda1 0.35664898\n",
      "44 Train Loss 105.02849 Test MSE 100.49184250420353 Test RE 0.16875419404335615 Lambda1 0.3648694\n",
      "45 Train Loss 99.97007 Test MSE 92.16715644330769 Test RE 0.1616133620536249 Lambda1 0.38790834\n",
      "46 Train Loss 96.37705 Test MSE 86.5247285082937 Test RE 0.1565882942471711 Lambda1 0.39933816\n",
      "47 Train Loss 94.65756 Test MSE 83.92368007679424 Test RE 0.154216708787138 Lambda1 0.40334737\n",
      "48 Train Loss 91.99311 Test MSE 82.63235372543964 Test RE 0.15302564981657815 Lambda1 0.40753135\n",
      "49 Train Loss 89.2602 Test MSE 77.66665644686238 Test RE 0.1483564645524919 Lambda1 0.41884306\n",
      "50 Train Loss 85.12567 Test MSE 74.54758509754816 Test RE 0.14534696255063417 Lambda1 0.43388712\n",
      "51 Train Loss 82.80096 Test MSE 72.73295901735888 Test RE 0.14356705734753727 Lambda1 0.43962887\n",
      "52 Train Loss 78.61411 Test MSE 70.64919763053165 Test RE 0.1414955519404217 Lambda1 0.45365572\n",
      "53 Train Loss 76.81559 Test MSE 67.5244061171531 Test RE 0.13833101298290992 Lambda1 0.46476465\n",
      "54 Train Loss 75.302246 Test MSE 65.89097665276329 Test RE 0.13664764236487068 Lambda1 0.46742645\n",
      "55 Train Loss 73.74894 Test MSE 65.55103736280554 Test RE 0.13629469593246937 Lambda1 0.4736504\n",
      "56 Train Loss 72.07237 Test MSE 63.719844097660236 Test RE 0.13437748942558597 Lambda1 0.48508814\n",
      "57 Train Loss 69.92745 Test MSE 60.873818482361266 Test RE 0.13134224736158442 Lambda1 0.49813932\n",
      "58 Train Loss 68.371826 Test MSE 59.33484487505987 Test RE 0.12967136313555117 Lambda1 0.5085942\n",
      "59 Train Loss 65.943275 Test MSE 58.58604005204473 Test RE 0.12885053992220216 Lambda1 0.5190761\n",
      "60 Train Loss 63.762154 Test MSE 57.7184106894962 Test RE 0.12789287568884963 Lambda1 0.52499187\n",
      "61 Train Loss 62.091503 Test MSE 57.86972814195951 Test RE 0.12806041111373734 Lambda1 0.53153986\n",
      "62 Train Loss 60.682655 Test MSE 56.647803774497376 Test RE 0.12670119445220585 Lambda1 0.5394748\n",
      "63 Train Loss 59.485954 Test MSE 56.2574160050669 Test RE 0.12626385967449844 Lambda1 0.549467\n",
      "64 Train Loss 58.414738 Test MSE 55.90605732753551 Test RE 0.12586894828015788 Lambda1 0.552823\n",
      "65 Train Loss 57.42331 Test MSE 54.79254428677719 Test RE 0.12460914160224368 Lambda1 0.5581489\n",
      "66 Train Loss 56.806873 Test MSE 54.54382724196109 Test RE 0.12432600391907404 Lambda1 0.5595555\n",
      "67 Train Loss 56.132103 Test MSE 55.004342756503775 Test RE 0.12484974523439125 Lambda1 0.559284\n",
      "68 Train Loss 55.36326 Test MSE 54.349154631431986 Test RE 0.12410393935898262 Lambda1 0.5627085\n",
      "69 Train Loss 54.845577 Test MSE 53.291366826092194 Test RE 0.12289029881916033 Lambda1 0.5701865\n",
      "70 Train Loss 54.24918 Test MSE 52.50602755690103 Test RE 0.12198143881467914 Lambda1 0.5742116\n",
      "71 Train Loss 52.80589 Test MSE 51.56327616690774 Test RE 0.12088138359812416 Lambda1 0.5852783\n",
      "72 Train Loss 52.14528 Test MSE 50.75163171462398 Test RE 0.11992622837509076 Lambda1 0.59558177\n",
      "73 Train Loss 51.26898 Test MSE 50.29344105786158 Test RE 0.11938364816732164 Lambda1 0.6020851\n",
      "74 Train Loss 50.91889 Test MSE 50.09980428226159 Test RE 0.11915360466538455 Lambda1 0.60537386\n",
      "Training time: 119.93\n",
      "Training time: 119.93\n",
      "inv_HT_stan_tune15\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 1517.7524 Test MSE 1101.7221857278705 Test RE 0.5587598835326116 Lambda1 0.0009587325\n",
      "1 Train Loss 839.76746 Test MSE 856.4543155824667 Test RE 0.4926531587068968 Lambda1 -0.0057603787\n",
      "2 Train Loss 827.1356 Test MSE 845.0357506867036 Test RE 0.4893580220290616 Lambda1 -0.015494057\n",
      "3 Train Loss 812.9592 Test MSE 832.0417562266924 Test RE 0.48558105197105916 Lambda1 -0.03128262\n",
      "4 Train Loss 797.2352 Test MSE 814.7462696949519 Test RE 0.48050771050035307 Lambda1 -0.035088483\n",
      "5 Train Loss 738.3805 Test MSE 751.4883072237205 Test RE 0.46147723774582083 Lambda1 -0.014650866\n",
      "6 Train Loss 604.6592 Test MSE 595.0787797863281 Test RE 0.41065432629994314 Lambda1 0.026765386\n",
      "7 Train Loss 387.4974 Test MSE 370.9289186650147 Test RE 0.32421605522386654 Lambda1 0.0059396336\n",
      "8 Train Loss 295.14886 Test MSE 305.3920842591562 Test RE 0.2941833293106459 Lambda1 0.002198374\n",
      "9 Train Loss 268.45203 Test MSE 283.9446227287971 Test RE 0.28366515755292115 Lambda1 0.0019667095\n",
      "10 Train Loss 261.41623 Test MSE 280.72184554122555 Test RE 0.2820507609959619 Lambda1 0.0010291397\n",
      "11 Train Loss 258.0597 Test MSE 280.6919031919508 Test RE 0.2820357185478193 Lambda1 0.00047284798\n",
      "12 Train Loss 256.45346 Test MSE 280.4081695513042 Test RE 0.2818931364977457 Lambda1 0.00072338665\n",
      "13 Train Loss 255.19057 Test MSE 280.5084634172617 Test RE 0.2819435444885531 Lambda1 0.0013854337\n",
      "14 Train Loss 254.69327 Test MSE 280.3651346617866 Test RE 0.2818715042723144 Lambda1 0.0014235897\n",
      "15 Train Loss 254.38258 Test MSE 280.2115871636186 Test RE 0.2817943074556724 Lambda1 0.0019589122\n",
      "16 Train Loss 253.98302 Test MSE 280.4078844132809 Test RE 0.2818929931736901 Lambda1 0.0022491422\n",
      "17 Train Loss 253.70763 Test MSE 280.2227488422454 Test RE 0.2817999197613758 Lambda1 0.0025835056\n",
      "18 Train Loss 253.54398 Test MSE 280.25128118411016 Test RE 0.2818142658610815 Lambda1 0.0029420464\n",
      "19 Train Loss 253.33615 Test MSE 279.97433336688164 Test RE 0.281674985253761 Lambda1 0.003394245\n",
      "20 Train Loss 252.97804 Test MSE 279.481950868548 Test RE 0.2814271895660917 Lambda1 0.004028058\n",
      "21 Train Loss 252.72195 Test MSE 279.10274344895856 Test RE 0.2812362013781487 Lambda1 0.0046283086\n",
      "22 Train Loss 252.24966 Test MSE 278.270089656288 Test RE 0.28081637873357296 Lambda1 0.00638132\n",
      "23 Train Loss 251.27675 Test MSE 276.633128167883 Test RE 0.27998919023775654 Lambda1 0.010302618\n",
      "24 Train Loss 249.71631 Test MSE 273.647406662652 Test RE 0.27847411921531934 Lambda1 0.014133152\n",
      "25 Train Loss 247.76907 Test MSE 269.5939149027633 Test RE 0.27640392976383005 Lambda1 0.021585347\n",
      "26 Train Loss 244.51535 Test MSE 265.10101268199304 Test RE 0.27409105594030514 Lambda1 0.033279005\n",
      "27 Train Loss 240.61424 Test MSE 257.18099963567664 Test RE 0.26996571335496594 Lambda1 0.04850042\n",
      "28 Train Loss 236.8991 Test MSE 252.87909581357903 Test RE 0.26769831379899445 Lambda1 0.058773715\n",
      "29 Train Loss 233.73831 Test MSE 249.589717263596 Test RE 0.26595154349652333 Lambda1 0.0691897\n",
      "30 Train Loss 226.71973 Test MSE 241.84294723787224 Test RE 0.26179170645755573 Lambda1 0.08387857\n",
      "31 Train Loss 218.6227 Test MSE 234.02010219036117 Test RE 0.2575228398765597 Lambda1 0.105248585\n",
      "32 Train Loss 211.95506 Test MSE 226.48334084947402 Test RE 0.2533420625442723 Lambda1 0.12885934\n",
      "33 Train Loss 208.03706 Test MSE 223.53642203160211 Test RE 0.25168846857580807 Lambda1 0.14090867\n",
      "34 Train Loss 203.40822 Test MSE 216.58051445060056 Test RE 0.2477415560918591 Lambda1 0.1606265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 Train Loss 198.17638 Test MSE 204.82225589260548 Test RE 0.24092271195971918 Lambda1 0.18413064\n",
      "36 Train Loss 192.21967 Test MSE 195.23748422890114 Test RE 0.2352181186632491 Lambda1 0.20902166\n",
      "37 Train Loss 186.08742 Test MSE 187.03791827460063 Test RE 0.23022580528034767 Lambda1 0.22986552\n",
      "38 Train Loss 179.02257 Test MSE 181.79165980327227 Test RE 0.22697401918431384 Lambda1 0.24205111\n",
      "39 Train Loss 172.46968 Test MSE 171.03144796595538 Test RE 0.22015429213025686 Lambda1 0.26670074\n",
      "40 Train Loss 161.75887 Test MSE 158.02725603312112 Test RE 0.2116192496721366 Lambda1 0.30974373\n",
      "41 Train Loss 155.6357 Test MSE 153.0880122269922 Test RE 0.20828584792877464 Lambda1 0.3239099\n",
      "42 Train Loss 148.84265 Test MSE 145.93604168104753 Test RE 0.20336230318126963 Lambda1 0.34375617\n",
      "43 Train Loss 138.15129 Test MSE 135.5497993370777 Test RE 0.19599211979196146 Lambda1 0.3826301\n",
      "44 Train Loss 130.3849 Test MSE 127.53942495579273 Test RE 0.1901128173664313 Lambda1 0.405985\n",
      "45 Train Loss 122.799324 Test MSE 114.41047455496877 Test RE 0.1800619977015664 Lambda1 0.4299586\n",
      "46 Train Loss 115.47974 Test MSE 110.46322911789021 Test RE 0.17692859873235633 Lambda1 0.43420738\n",
      "47 Train Loss 111.19604 Test MSE 104.2739338406219 Test RE 0.17190046427952207 Lambda1 0.4465803\n",
      "48 Train Loss 103.898926 Test MSE 94.31503297041799 Test RE 0.163485647265559 Lambda1 0.4634624\n",
      "49 Train Loss 97.77294 Test MSE 90.0092963632579 Test RE 0.15971027370806531 Lambda1 0.47345683\n",
      "50 Train Loss 90.62224 Test MSE 81.51762933267351 Test RE 0.15198997418027393 Lambda1 0.48882827\n",
      "51 Train Loss 86.249435 Test MSE 78.02758171237407 Test RE 0.14870077918441785 Lambda1 0.49614453\n",
      "52 Train Loss 81.95787 Test MSE 74.21862033186659 Test RE 0.1450259132855962 Lambda1 0.5083368\n",
      "53 Train Loss 78.8956 Test MSE 71.45032345600347 Test RE 0.14229553413938126 Lambda1 0.5184233\n",
      "54 Train Loss 76.5561 Test MSE 68.61360342318781 Test RE 0.13944221880468816 Lambda1 0.5240055\n",
      "55 Train Loss 74.137276 Test MSE 66.4298287785119 Test RE 0.13720525243166326 Lambda1 0.53098595\n",
      "56 Train Loss 72.212685 Test MSE 63.66617388191454 Test RE 0.1343208854881177 Lambda1 0.5390196\n",
      "57 Train Loss 69.5596 Test MSE 62.03363248688597 Test RE 0.13258755958123186 Lambda1 0.54589164\n",
      "58 Train Loss 67.94601 Test MSE 60.643227959707694 Test RE 0.13109324858149052 Lambda1 0.5503255\n",
      "59 Train Loss 65.50783 Test MSE 58.85853841757671 Test RE 0.1291498503722839 Lambda1 0.55588347\n",
      "60 Train Loss 63.40584 Test MSE 57.55761979024968 Test RE 0.12771461062629472 Lambda1 0.5578802\n",
      "61 Train Loss 62.24248 Test MSE 57.31970399775406 Test RE 0.12745038160163352 Lambda1 0.55878043\n",
      "62 Train Loss 60.644516 Test MSE 57.061139197985014 Test RE 0.12716259723077072 Lambda1 0.56595093\n",
      "63 Train Loss 57.72613 Test MSE 53.91229814825559 Test RE 0.12360416160996976 Lambda1 0.57823884\n",
      "64 Train Loss 55.97437 Test MSE 52.20131103013894 Test RE 0.12162696669558883 Lambda1 0.5848023\n",
      "65 Train Loss 54.456207 Test MSE 50.9515075198713 Test RE 0.12016214982560428 Lambda1 0.59649605\n",
      "66 Train Loss 53.669495 Test MSE 50.95726582949606 Test RE 0.12016893972593233 Lambda1 0.59691626\n",
      "67 Train Loss 52.66932 Test MSE 50.274171460522325 Test RE 0.1193607754509613 Lambda1 0.5993662\n",
      "68 Train Loss 51.597736 Test MSE 48.922610201481966 Test RE 0.11774540849356334 Lambda1 0.60738057\n",
      "69 Train Loss 50.553814 Test MSE 47.88562898718076 Test RE 0.11649083782294227 Lambda1 0.6134221\n",
      "70 Train Loss 50.14881 Test MSE 47.96936174492645 Test RE 0.11659264121397489 Lambda1 0.6120479\n",
      "71 Train Loss 49.425392 Test MSE 47.25821399765284 Test RE 0.11572516879616739 Lambda1 0.6170597\n",
      "72 Train Loss 48.984413 Test MSE 47.00144479899787 Test RE 0.11541035441039718 Lambda1 0.6198297\n",
      "73 Train Loss 48.12811 Test MSE 45.912891845856876 Test RE 0.1140660741561885 Lambda1 0.6242831\n",
      "74 Train Loss 47.20735 Test MSE 44.96498627792781 Test RE 0.11288244388301472 Lambda1 0.63322526\n",
      "Training time: 119.12\n",
      "Training time: 119.12\n",
      "inv_HT_stan_tune15\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 932.00604 Test MSE 844.9257894218467 Test RE 0.4893261818508937 Lambda1 0.14883973\n",
      "1 Train Loss 794.63074 Test MSE 808.5914904613801 Test RE 0.47868933744428244 Lambda1 0.14165664\n",
      "2 Train Loss 698.76 Test MSE 696.7107361616702 Test RE 0.4443400116897417 Lambda1 0.10727686\n",
      "3 Train Loss 514.13153 Test MSE 531.5954536329065 Test RE 0.38813231237682344 Lambda1 0.011623484\n",
      "4 Train Loss 352.99057 Test MSE 365.46345645433155 Test RE 0.321818606001704 Lambda1 0.0014449976\n",
      "5 Train Loss 296.88327 Test MSE 313.917429611666 Test RE 0.29826128559489756 Lambda1 -0.0015028229\n",
      "6 Train Loss 271.49808 Test MSE 288.5635431725955 Test RE 0.28596303755553354 Lambda1 -0.0019439992\n",
      "7 Train Loss 259.75397 Test MSE 281.8191865255779 Test RE 0.28260149117592465 Lambda1 0.000870132\n",
      "8 Train Loss 256.92346 Test MSE 281.16201218226615 Test RE 0.2822717995617068 Lambda1 0.001220131\n",
      "9 Train Loss 255.55798 Test MSE 281.32279516235127 Test RE 0.28235249683359037 Lambda1 0.0010681448\n",
      "10 Train Loss 254.70323 Test MSE 281.6118614455458 Test RE 0.28249752175280785 Lambda1 0.0011466868\n",
      "11 Train Loss 254.16444 Test MSE 281.2542281915179 Test RE 0.28231808576639056 Lambda1 0.0013611845\n",
      "12 Train Loss 253.5726 Test MSE 281.5697311393681 Test RE 0.28247638956008997 Lambda1 0.0013064384\n",
      "13 Train Loss 253.16566 Test MSE 281.65703004879657 Test RE 0.2825201761729735 Lambda1 0.0012224112\n",
      "14 Train Loss 252.95628 Test MSE 281.52293366928814 Test RE 0.282452914505359 Lambda1 0.0014165923\n",
      "15 Train Loss 252.80623 Test MSE 281.73189694233474 Test RE 0.2825577218407984 Lambda1 0.001463388\n",
      "16 Train Loss 252.67175 Test MSE 281.70933983550685 Test RE 0.28254641000076625 Lambda1 0.0015623751\n",
      "17 Train Loss 252.56525 Test MSE 281.98445907768956 Test RE 0.2826843446972729 Lambda1 0.0015187445\n",
      "18 Train Loss 252.4608 Test MSE 282.2535914779102 Test RE 0.2828192127181794 Lambda1 0.0014163852\n",
      "19 Train Loss 252.31151 Test MSE 282.61289792849516 Test RE 0.28299916870661457 Lambda1 0.001256987\n",
      "20 Train Loss 252.189 Test MSE 282.4158467181027 Test RE 0.28290049123453975 Lambda1 0.0012720708\n",
      "21 Train Loss 251.98543 Test MSE 282.1865424748074 Test RE 0.28278561904015037 Lambda1 0.001392204\n",
      "22 Train Loss 251.82463 Test MSE 282.4660131868389 Test RE 0.2829256163977017 Lambda1 0.0013357231\n",
      "23 Train Loss 251.60133 Test MSE 282.3879273587489 Test RE 0.28288650725349784 Lambda1 0.0013107313\n",
      "24 Train Loss 251.4514 Test MSE 282.9349873212131 Test RE 0.283160387594717 Lambda1 0.0013156756\n",
      "25 Train Loss 251.30809 Test MSE 283.26206451693906 Test RE 0.2833240092020557 Lambda1 0.0011249647\n",
      "26 Train Loss 251.09314 Test MSE 283.6025988365016 Test RE 0.2834942624426816 Lambda1 0.00094646367\n",
      "27 Train Loss 250.93866 Test MSE 283.7155128847158 Test RE 0.2835506922839549 Lambda1 0.0008827511\n",
      "28 Train Loss 250.75069 Test MSE 284.15325170507504 Test RE 0.28376935023677374 Lambda1 0.00079404865\n",
      "29 Train Loss 250.5227 Test MSE 284.36735811101033 Test RE 0.28387623867590683 Lambda1 0.0006508394\n",
      "30 Train Loss 250.26643 Test MSE 284.9235451678841 Test RE 0.2841537163085617 Lambda1 0.0005055172\n",
      "31 Train Loss 250.07712 Test MSE 284.99612556073254 Test RE 0.2841899061487065 Lambda1 0.00055435806\n",
      "32 Train Loss 249.76698 Test MSE 285.935457123917 Test RE 0.2846578580341396 Lambda1 0.00037731905\n",
      "33 Train Loss 249.36856 Test MSE 285.70042283122183 Test RE 0.2845408419193437 Lambda1 0.0003612553\n",
      "34 Train Loss 249.16576 Test MSE 285.6996303125752 Test RE 0.28454044726805633 Lambda1 0.00028263647\n",
      "35 Train Loss 248.87837 Test MSE 285.86437161321845 Test RE 0.2846224718892811 Lambda1 0.00021174824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 Train Loss 248.69601 Test MSE 286.3367262420245 Test RE 0.28485752610352505 Lambda1 0.00016279779\n",
      "37 Train Loss 248.4329 Test MSE 286.74876969503924 Test RE 0.28506240985155806 Lambda1 0.00015255185\n",
      "38 Train Loss 248.24567 Test MSE 286.8566679084806 Test RE 0.28511603664118595 Lambda1 9.983399e-05\n",
      "39 Train Loss 248.01166 Test MSE 287.54762530391736 Test RE 0.28545921246290024 Lambda1 0.00010442429\n",
      "40 Train Loss 247.79459 Test MSE 288.46149585361195 Test RE 0.28591246924500047 Lambda1 8.369415e-05\n",
      "41 Train Loss 247.61815 Test MSE 289.0590525297617 Test RE 0.28620845418425417 Lambda1 6.4927735e-05\n",
      "42 Train Loss 247.4819 Test MSE 288.86991480960995 Test RE 0.2861148026068155 Lambda1 4.664388e-05\n",
      "43 Train Loss 247.3197 Test MSE 288.918291305109 Test RE 0.28613875915491604 Lambda1 4.161998e-05\n",
      "44 Train Loss 247.17877 Test MSE 289.0523503894437 Test RE 0.2862051361419331 Lambda1 3.233832e-05\n",
      "45 Train Loss 247.06784 Test MSE 289.3065736694467 Test RE 0.28633096804951125 Lambda1 3.5399804e-05\n",
      "46 Train Loss 246.95216 Test MSE 289.861236737036 Test RE 0.286605315711657 Lambda1 1.9097717e-05\n",
      "47 Train Loss 246.85683 Test MSE 290.03647026862365 Test RE 0.28669193521627767 Lambda1 2.7157877e-05\n",
      "48 Train Loss 246.7427 Test MSE 290.39725024600216 Test RE 0.2868701896345704 Lambda1 1.9473193e-05\n",
      "49 Train Loss 246.63629 Test MSE 290.63714667895636 Test RE 0.2869886565418761 Lambda1 2.6078658e-05\n",
      "50 Train Loss 246.58217 Test MSE 290.8561680243085 Test RE 0.28709677211621887 Lambda1 2.1517433e-05\n",
      "51 Train Loss 246.48865 Test MSE 290.91114224070793 Test RE 0.28712390266407706 Lambda1 2.5387742e-05\n",
      "52 Train Loss 246.40685 Test MSE 290.9960678791091 Test RE 0.2871658096209376 Lambda1 1.9994979e-05\n",
      "53 Train Loss 246.30963 Test MSE 290.8684935230794 Test RE 0.287102855145931 Lambda1 1.5114478e-05\n",
      "54 Train Loss 246.08147 Test MSE 291.0783204623934 Test RE 0.2872063917170655 Lambda1 2.1007409e-05\n",
      "55 Train Loss 245.94205 Test MSE 291.1541339716669 Test RE 0.2872437918002868 Lambda1 1.7350718e-05\n",
      "56 Train Loss 245.75018 Test MSE 291.19726938439743 Test RE 0.28726506905378135 Lambda1 2.2035754e-06\n",
      "57 Train Loss 245.59422 Test MSE 291.4621862951612 Test RE 0.28739570914249923 Lambda1 3.36182e-06\n",
      "58 Train Loss 245.49097 Test MSE 291.1500272743362 Test RE 0.28724176602195317 Lambda1 -2.297038e-06\n",
      "59 Train Loss 245.46132 Test MSE 291.1268744938423 Test RE 0.2872303448006694 Lambda1 -2.286917e-06\n",
      "60 Train Loss 245.40814 Test MSE 291.0336284955913 Test RE 0.2871843421339119 Lambda1 -1.4999783e-06\n",
      "61 Train Loss 245.34671 Test MSE 291.1506468339153 Test RE 0.28724207164324167 Lambda1 -2.1349813e-06\n",
      "62 Train Loss 245.26675 Test MSE 291.37617230771053 Test RE 0.28735329905270535 Lambda1 -1.4580122e-06\n",
      "63 Train Loss 245.18645 Test MSE 291.3595291163835 Test RE 0.2873450922316199 Lambda1 3.521409e-07\n",
      "64 Train Loss 245.1465 Test MSE 291.64537601886605 Test RE 0.2874860118868326 Lambda1 -1.3619339e-06\n",
      "65 Train Loss 245.04668 Test MSE 291.87376087367613 Test RE 0.28759855370572285 Lambda1 -2.2088199e-07\n",
      "66 Train Loss 244.94453 Test MSE 292.1760360070588 Test RE 0.2877474389542472 Lambda1 -2.2025508e-06\n",
      "67 Train Loss 244.81406 Test MSE 292.17880079722397 Test RE 0.28774880039274525 Lambda1 -1.5290465e-06\n",
      "68 Train Loss 244.7047 Test MSE 291.7766473709098 Test RE 0.28755070420656464 Lambda1 -7.5042766e-07\n",
      "69 Train Loss 244.59178 Test MSE 292.0883588471835 Test RE 0.28770426160895696 Lambda1 2.6684763e-06\n",
      "70 Train Loss 244.51369 Test MSE 292.34156116620136 Test RE 0.2878289355365204 Lambda1 4.2811566e-06\n",
      "71 Train Loss 244.32668 Test MSE 292.08240281722345 Test RE 0.28770132827750877 Lambda1 4.0901778e-06\n",
      "72 Train Loss 244.05627 Test MSE 292.19476467060025 Test RE 0.28775666120033544 Lambda1 3.0134433e-06\n",
      "73 Train Loss 243.97237 Test MSE 292.28425376332893 Test RE 0.28780072275496865 Lambda1 2.3653467e-06\n",
      "74 Train Loss 243.8507 Test MSE 292.61223073765706 Test RE 0.2879621504476728 Lambda1 2.1733906e-06\n",
      "Training time: 119.63\n",
      "Training time: 119.63\n",
      "inv_HT_stan_tune15\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 947.4923 Test MSE 855.5256566432649 Test RE 0.49238599276581935 Lambda1 -0.055532582\n",
      "1 Train Loss 824.01984 Test MSE 842.5602360671932 Test RE 0.4886407142460092 Lambda1 -0.06023709\n",
      "2 Train Loss 809.68713 Test MSE 821.9500464585744 Test RE 0.48262729833446516 Lambda1 -0.051394917\n",
      "3 Train Loss 738.7171 Test MSE 745.5069018083202 Test RE 0.4596370248329198 Lambda1 -0.016616251\n",
      "4 Train Loss 625.20264 Test MSE 625.9385153836037 Test RE 0.4211676527653023 Lambda1 0.03718424\n",
      "5 Train Loss 395.8755 Test MSE 403.105460394737 Test RE 0.3379858441882813 Lambda1 0.00096334354\n",
      "6 Train Loss 317.6157 Test MSE 329.54106280261175 Test RE 0.3055933783499938 Lambda1 0.00012637662\n",
      "7 Train Loss 276.07703 Test MSE 292.9643097165647 Test RE 0.28813534030468896 Lambda1 -0.0018036265\n",
      "8 Train Loss 260.84488 Test MSE 281.9239836871935 Test RE 0.28265403032662556 Lambda1 0.00025821998\n",
      "9 Train Loss 258.4514 Test MSE 281.1227132740115 Test RE 0.28225207185894946 Lambda1 0.0003053547\n",
      "10 Train Loss 257.51068 Test MSE 281.31349249306027 Test RE 0.28234782843547057 Lambda1 0.00045779522\n",
      "11 Train Loss 256.26862 Test MSE 281.44779171348887 Test RE 0.28241521690010374 Lambda1 0.00013144773\n",
      "12 Train Loss 255.71248 Test MSE 281.2856901404924 Test RE 0.2823338758024735 Lambda1 0.0001514871\n",
      "13 Train Loss 255.3794 Test MSE 281.5712039628461 Test RE 0.2824771283421188 Lambda1 0.00013074519\n",
      "14 Train Loss 255.09113 Test MSE 281.49992340673333 Test RE 0.2824413711322345 Lambda1 0.00012281991\n",
      "15 Train Loss 254.7187 Test MSE 281.3247102880436 Test RE 0.28235345789968597 Lambda1 0.0002995143\n",
      "16 Train Loss 254.56105 Test MSE 281.7701482229538 Test RE 0.2825769028919841 Lambda1 0.00032895117\n",
      "17 Train Loss 254.42204 Test MSE 281.6659596385661 Test RE 0.2825246546148558 Lambda1 0.0002897261\n",
      "18 Train Loss 254.30464 Test MSE 281.8986211176511 Test RE 0.28264131591750447 Lambda1 0.00026255168\n",
      "19 Train Loss 254.12848 Test MSE 282.3223014060591 Test RE 0.2828536344330607 Lambda1 0.00022061213\n",
      "20 Train Loss 254.01617 Test MSE 282.43500094729893 Test RE 0.2829100846214484 Lambda1 0.00020649658\n",
      "21 Train Loss 253.83318 Test MSE 282.3089119361353 Test RE 0.2828469270190217 Lambda1 0.00025464452\n",
      "22 Train Loss 253.68768 Test MSE 282.51229245939555 Test RE 0.2829487927354436 Lambda1 0.00021394911\n",
      "23 Train Loss 253.54211 Test MSE 282.8305049052332 Test RE 0.28310809994109687 Lambda1 0.00022680443\n",
      "24 Train Loss 253.35759 Test MSE 282.60242248167003 Test RE 0.28299392377576243 Lambda1 0.00027980428\n",
      "25 Train Loss 253.12714 Test MSE 282.9185844202865 Test RE 0.28315217949159865 Lambda1 0.000256465\n",
      "26 Train Loss 252.93059 Test MSE 282.8203632330556 Test RE 0.28310302408253546 Lambda1 0.0002435381\n",
      "27 Train Loss 252.74495 Test MSE 283.22713293259574 Test RE 0.28330653905179004 Lambda1 0.00022174937\n",
      "28 Train Loss 252.49193 Test MSE 283.16674020026943 Test RE 0.2832763326093925 Lambda1 0.00021239877\n",
      "29 Train Loss 252.27388 Test MSE 283.34888004404945 Test RE 0.28336742313199126 Lambda1 0.0001613776\n",
      "30 Train Loss 252.1281 Test MSE 283.8102707807924 Test RE 0.2835980397547939 Lambda1 0.0001500484\n",
      "31 Train Loss 251.91667 Test MSE 283.5292847628026 Test RE 0.28345761704039346 Lambda1 0.0001578636\n",
      "32 Train Loss 251.57532 Test MSE 284.73410075158307 Test RE 0.2840592343205519 Lambda1 0.000107813976\n",
      "33 Train Loss 251.24802 Test MSE 284.9538479746957 Test RE 0.2841688263729693 Lambda1 6.5215965e-05\n",
      "34 Train Loss 250.81898 Test MSE 285.8063253694093 Test RE 0.2845935733873955 Lambda1 4.79472e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 Train Loss 250.64555 Test MSE 285.7574418985226 Test RE 0.28456923432394815 Lambda1 5.7958667e-05\n",
      "36 Train Loss 250.39572 Test MSE 285.99989091031864 Test RE 0.28468992917115843 Lambda1 3.4488385e-05\n",
      "37 Train Loss 250.09166 Test MSE 285.9944985821183 Test RE 0.2846872453436146 Lambda1 4.0039497e-05\n",
      "38 Train Loss 249.93546 Test MSE 285.9161100893888 Test RE 0.28464822757657854 Lambda1 3.572854e-05\n",
      "39 Train Loss 249.72487 Test MSE 286.93894053355973 Test RE 0.28515692040949914 Lambda1 2.4448747e-05\n",
      "40 Train Loss 249.52791 Test MSE 286.9235416515683 Test RE 0.28514926868284074 Lambda1 2.0899124e-05\n",
      "41 Train Loss 249.4687 Test MSE 287.0191119908729 Test RE 0.2851967544054817 Lambda1 1.3040345e-05\n",
      "42 Train Loss 249.28531 Test MSE 287.209513111093 Test RE 0.28529133482998503 Lambda1 1.7585786e-05\n",
      "43 Train Loss 249.17339 Test MSE 287.12223788449074 Test RE 0.28524798536457885 Lambda1 1.8503815e-05\n",
      "44 Train Loss 249.10342 Test MSE 287.24538135399774 Test RE 0.28530914861916074 Lambda1 2.1593234e-05\n",
      "45 Train Loss 248.97891 Test MSE 286.9656337414605 Test RE 0.2851701838165664 Lambda1 2.7243157e-05\n",
      "46 Train Loss 248.80765 Test MSE 287.51019504691675 Test RE 0.28544063265475683 Lambda1 2.0243706e-05\n",
      "47 Train Loss 248.68657 Test MSE 287.5290423441492 Test RE 0.28544998831653073 Lambda1 2.5846459e-05\n",
      "48 Train Loss 248.62236 Test MSE 287.6372397354147 Test RE 0.2855036907857849 Lambda1 2.9169398e-05\n",
      "49 Train Loss 248.50812 Test MSE 287.70308072804806 Test RE 0.2855363652246405 Lambda1 2.3145432e-05\n",
      "50 Train Loss 248.38802 Test MSE 287.4931470879699 Test RE 0.28543216990733344 Lambda1 1.796473e-05\n",
      "51 Train Loss 248.32462 Test MSE 287.628135016755 Test RE 0.2854991721578181 Lambda1 1.9036233e-05\n",
      "52 Train Loss 248.21739 Test MSE 287.5580359091149 Test RE 0.2854643799134398 Lambda1 1.7355447e-05\n",
      "53 Train Loss 248.17198 Test MSE 287.77490788544657 Test RE 0.28557200611086975 Lambda1 1.3600219e-05\n",
      "54 Train Loss 248.01337 Test MSE 288.37231456079763 Test RE 0.28586826921299296 Lambda1 1.6880856e-05\n",
      "55 Train Loss 247.8804 Test MSE 287.93940706522176 Test RE 0.2856536144238144 Lambda1 1.2803674e-05\n",
      "56 Train Loss 247.82335 Test MSE 288.503105228206 Test RE 0.2859330893454743 Lambda1 1.2819512e-05\n",
      "57 Train Loss 247.6875 Test MSE 289.59863687222787 Test RE 0.2864754612134206 Lambda1 1.0079434e-05\n",
      "58 Train Loss 247.51724 Test MSE 288.7721623044631 Test RE 0.2860663884179631 Lambda1 1.1609578e-05\n",
      "59 Train Loss 247.40501 Test MSE 289.12707165139597 Test RE 0.2862421263740344 Lambda1 1.1088289e-05\n",
      "60 Train Loss 247.3454 Test MSE 288.93171266341415 Test RE 0.28614540519694287 Lambda1 9.304303e-06\n",
      "61 Train Loss 247.22984 Test MSE 289.4024121014549 Test RE 0.28637839047480895 Lambda1 8.500616e-06\n",
      "62 Train Loss 247.16449 Test MSE 289.3986396764972 Test RE 0.28637652396564556 Lambda1 7.553629e-06\n",
      "63 Train Loss 247.00076 Test MSE 290.08005409093596 Test RE 0.2867134750263837 Lambda1 7.558733e-06\n",
      "64 Train Loss 246.89941 Test MSE 289.87954676763474 Test RE 0.28661436774857185 Lambda1 8.9706e-06\n",
      "65 Train Loss 246.86064 Test MSE 290.10432945488094 Test RE 0.2867254715912088 Lambda1 8.227389e-06\n",
      "66 Train Loss 246.76161 Test MSE 290.1306410726246 Test RE 0.2867384738789863 Lambda1 6.731656e-06\n",
      "67 Train Loss 246.65323 Test MSE 290.991372650132 Test RE 0.28716349289776133 Lambda1 5.953025e-06\n",
      "68 Train Loss 246.55734 Test MSE 290.7245031708783 Test RE 0.28703178323608314 Lambda1 6.9368757e-06\n",
      "69 Train Loss 246.49802 Test MSE 290.924039085245 Test RE 0.2871302670667381 Lambda1 5.8725495e-06\n",
      "70 Train Loss 246.4062 Test MSE 291.3373759382133 Test RE 0.2873341680513661 Lambda1 5.8143155e-06\n",
      "71 Train Loss 246.32777 Test MSE 291.25719201755896 Test RE 0.2872946242654802 Lambda1 5.154048e-06\n",
      "72 Train Loss 246.25116 Test MSE 291.0041599880685 Test RE 0.2871698023911543 Lambda1 4.9717387e-06\n",
      "73 Train Loss 246.23767 Test MSE 291.071672581461 Test RE 0.28720311197298515 Lambda1 4.7347803e-06\n",
      "74 Train Loss 246.17729 Test MSE 290.872330409006 Test RE 0.2871047487461251 Lambda1 5.666534e-06\n",
      "Training time: 122.06\n",
      "Training time: 122.06\n",
      "inv_HT_stan_tune15\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 911.5857 Test MSE 867.83826231617 Test RE 0.4959165106570098 Lambda1 0.20666659\n",
      "1 Train Loss 823.6523 Test MSE 837.1186585866677 Test RE 0.4870602436251261 Lambda1 0.19395708\n",
      "2 Train Loss 775.2686 Test MSE 788.3392170920899 Test RE 0.47265661076659377 Lambda1 0.15318167\n",
      "3 Train Loss 699.28156 Test MSE 704.6578377420287 Test RE 0.44686703062329575 Lambda1 0.09740874\n",
      "4 Train Loss 567.39703 Test MSE 548.8786835137008 Test RE 0.39439132436087326 Lambda1 0.062515296\n",
      "5 Train Loss 442.99548 Test MSE 449.22158601157236 Test RE 0.3567955909815509 Lambda1 0.023810143\n",
      "6 Train Loss 329.69693 Test MSE 332.5671342587907 Test RE 0.3069932557540873 Lambda1 0.0034934236\n",
      "7 Train Loss 281.06824 Test MSE 299.2930808587475 Test RE 0.29123093802906863 Lambda1 0.0017268397\n",
      "8 Train Loss 263.93607 Test MSE 284.20266977763146 Test RE 0.2837940248174736 Lambda1 0.001991789\n",
      "9 Train Loss 257.7931 Test MSE 281.3652325845881 Test RE 0.2823737924066585 Lambda1 0.000703363\n",
      "10 Train Loss 256.06387 Test MSE 281.2962490783108 Test RE 0.2823391748951167 Lambda1 0.0010291886\n",
      "11 Train Loss 255.3738 Test MSE 281.2001692247391 Test RE 0.2822909527388364 Lambda1 0.0010446474\n",
      "12 Train Loss 254.26707 Test MSE 280.088847191344 Test RE 0.28173258407333046 Lambda1 0.00205751\n",
      "13 Train Loss 253.75044 Test MSE 280.07899402968434 Test RE 0.2817276285366542 Lambda1 0.002428165\n",
      "14 Train Loss 253.37558 Test MSE 280.0697928683855 Test RE 0.28172300083750856 Lambda1 0.0026908033\n",
      "15 Train Loss 253.0292 Test MSE 279.86481302405696 Test RE 0.28161988706304364 Lambda1 0.0034383654\n",
      "16 Train Loss 252.6066 Test MSE 280.0579637645129 Test RE 0.2817170513100796 Lambda1 0.004189829\n",
      "17 Train Loss 252.1791 Test MSE 279.334835011897 Test RE 0.2813531099129829 Lambda1 0.005533299\n",
      "18 Train Loss 250.6853 Test MSE 274.7595217383507 Test RE 0.27903941097875773 Lambda1 0.012249675\n",
      "19 Train Loss 248.48454 Test MSE 269.8563840465061 Test RE 0.27653844662380905 Lambda1 0.019950313\n",
      "20 Train Loss 245.4761 Test MSE 266.63181233263714 Test RE 0.2748812727095392 Lambda1 0.027378164\n",
      "21 Train Loss 240.58846 Test MSE 256.1863421870401 Test RE 0.269443156201654 Lambda1 0.049492344\n",
      "22 Train Loss 233.7081 Test MSE 248.46306905931098 Test RE 0.26535061182679553 Lambda1 0.06903299\n",
      "23 Train Loss 227.16614 Test MSE 240.2632405462442 Test RE 0.2609353001433323 Lambda1 0.0836484\n",
      "24 Train Loss 220.92642 Test MSE 231.93719548181375 Test RE 0.25637423139897053 Lambda1 0.0985417\n",
      "25 Train Loss 210.18059 Test MSE 223.22814873557797 Test RE 0.2515148601729286 Lambda1 0.122251205\n",
      "26 Train Loss 201.58386 Test MSE 209.46591217334017 Test RE 0.2436384619996814 Lambda1 0.151646\n",
      "27 Train Loss 193.38016 Test MSE 201.98454265929828 Test RE 0.23924795720780223 Lambda1 0.17256169\n",
      "28 Train Loss 184.75534 Test MSE 191.81155553718037 Test RE 0.23314524063513936 Lambda1 0.19054821\n",
      "29 Train Loss 176.82869 Test MSE 183.50432707930312 Test RE 0.22804067895353866 Lambda1 0.21580568\n",
      "30 Train Loss 167.66673 Test MSE 171.49713435088984 Test RE 0.22045380770149953 Lambda1 0.24456051\n",
      "31 Train Loss 159.67992 Test MSE 165.7513854108538 Test RE 0.21672936281936492 Lambda1 0.27080724\n",
      "32 Train Loss 149.14738 Test MSE 154.2449467826307 Test RE 0.209071407575774 Lambda1 0.3019016\n",
      "33 Train Loss 141.22795 Test MSE 144.66170700536279 Test RE 0.2024724618072485 Lambda1 0.32389513\n",
      "34 Train Loss 136.84674 Test MSE 139.29383427882442 Test RE 0.19868044187744813 Lambda1 0.34475797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 Train Loss 128.90512 Test MSE 126.73713031165506 Test RE 0.18951391580331622 Lambda1 0.36685145\n",
      "36 Train Loss 119.04174 Test MSE 117.49362471817341 Test RE 0.18247203760582395 Lambda1 0.37956882\n",
      "37 Train Loss 113.029106 Test MSE 106.92883879665693 Test RE 0.17407507698957106 Lambda1 0.39549404\n",
      "38 Train Loss 106.71691 Test MSE 102.0037967980166 Test RE 0.1700189537595443 Lambda1 0.40389356\n",
      "39 Train Loss 102.60552 Test MSE 97.92868396567772 Test RE 0.16658815937844224 Lambda1 0.41908702\n",
      "40 Train Loss 99.12571 Test MSE 95.99110798226474 Test RE 0.16493190413186284 Lambda1 0.42919582\n",
      "41 Train Loss 94.21749 Test MSE 90.6890179691089 Test RE 0.16031218011356665 Lambda1 0.45143133\n",
      "42 Train Loss 90.04566 Test MSE 86.37702106981308 Test RE 0.1564545802457943 Lambda1 0.46731657\n",
      "43 Train Loss 86.413925 Test MSE 81.9635939986321 Test RE 0.15240515863849274 Lambda1 0.48754442\n",
      "44 Train Loss 83.717 Test MSE 79.10753874813749 Test RE 0.14972630496816303 Lambda1 0.49805644\n",
      "45 Train Loss 79.31349 Test MSE 75.74663140697635 Test RE 0.14651120237405144 Lambda1 0.5113789\n",
      "46 Train Loss 75.59814 Test MSE 71.36484394540295 Test RE 0.1422103911189494 Lambda1 0.5314847\n",
      "47 Train Loss 72.88258 Test MSE 68.0782713180776 Test RE 0.1388971806001464 Lambda1 0.54254496\n",
      "48 Train Loss 70.92047 Test MSE 66.67040208414777 Test RE 0.1374534699227077 Lambda1 0.55350655\n",
      "49 Train Loss 67.73271 Test MSE 63.817220636479476 Test RE 0.13448012793339426 Lambda1 0.5692255\n",
      "50 Train Loss 64.44247 Test MSE 60.136281225341186 Test RE 0.13054416200233843 Lambda1 0.5838873\n",
      "51 Train Loss 63.11785 Test MSE 58.89498997475307 Test RE 0.12918983594299593 Lambda1 0.587113\n",
      "52 Train Loss 61.582355 Test MSE 57.575044645224956 Test RE 0.12773394116995934 Lambda1 0.59394187\n",
      "53 Train Loss 60.06287 Test MSE 57.718553929854416 Test RE 0.12789303438526678 Lambda1 0.6025717\n",
      "54 Train Loss 59.06951 Test MSE 56.07188926602802 Test RE 0.1260554901044541 Lambda1 0.61461407\n",
      "55 Train Loss 57.419865 Test MSE 55.7285310063588 Test RE 0.12566894474324788 Lambda1 0.6239191\n",
      "56 Train Loss 55.448498 Test MSE 53.06220795706366 Test RE 0.12262579311085786 Lambda1 0.64181566\n",
      "57 Train Loss 54.648304 Test MSE 52.51632845980008 Test RE 0.12199340370131818 Lambda1 0.6439544\n",
      "58 Train Loss 53.523342 Test MSE 51.40899765885906 Test RE 0.12070040818629933 Lambda1 0.6485942\n",
      "59 Train Loss 52.71423 Test MSE 50.268187756631434 Test RE 0.1193536719943666 Lambda1 0.657459\n",
      "60 Train Loss 51.96423 Test MSE 49.73286362338633 Test RE 0.1187164507182923 Lambda1 0.66247296\n",
      "61 Train Loss 51.01707 Test MSE 47.92376313539471 Test RE 0.11653721285141026 Lambda1 0.6747917\n",
      "62 Train Loss 49.752098 Test MSE 47.17058646782851 Test RE 0.11561782855852448 Lambda1 0.6868204\n",
      "63 Train Loss 48.23376 Test MSE 45.97896368974015 Test RE 0.1141481191557884 Lambda1 0.69470674\n",
      "64 Train Loss 46.780552 Test MSE 44.856169568063734 Test RE 0.112745771576606 Lambda1 0.7057759\n",
      "65 Train Loss 45.602478 Test MSE 43.45595003471257 Test RE 0.11097209758009574 Lambda1 0.7193961\n",
      "66 Train Loss 44.454933 Test MSE 42.197183878573924 Test RE 0.10935305106121397 Lambda1 0.73358476\n",
      "67 Train Loss 43.543373 Test MSE 41.62407664166277 Test RE 0.10860791512418096 Lambda1 0.74014777\n",
      "68 Train Loss 42.70261 Test MSE 41.13226607920356 Test RE 0.10796437845690036 Lambda1 0.7467501\n",
      "69 Train Loss 41.699696 Test MSE 40.65396239893253 Test RE 0.10733481478264276 Lambda1 0.7524647\n",
      "70 Train Loss 41.09631 Test MSE 40.517992276215836 Test RE 0.10715516991566999 Lambda1 0.7546075\n",
      "71 Train Loss 40.76439 Test MSE 40.2399929803412 Test RE 0.1067869343143105 Lambda1 0.7580949\n",
      "72 Train Loss 40.0811 Test MSE 39.950486827305994 Test RE 0.10640210223013945 Lambda1 0.76276803\n",
      "73 Train Loss 39.542408 Test MSE 39.67174194271877 Test RE 0.10603025497567747 Lambda1 0.7676593\n",
      "74 Train Loss 38.928005 Test MSE 39.14252376430254 Test RE 0.10532066257260884 Lambda1 0.7729792\n",
      "Training time: 119.36\n",
      "Training time: 119.36\n",
      "inv_HT_stan_tune16\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.05743 Test MSE 858.1881245337605 Test RE 0.4931515709301655 Lambda1 -0.032060277\n",
      "1 Train Loss 837.8083 Test MSE 857.6124086755574 Test RE 0.49298612771095945 Lambda1 -0.18136638\n",
      "2 Train Loss 837.1831 Test MSE 856.2362496771253 Test RE 0.4925904363325921 Lambda1 -1.1238446\n",
      "3 Train Loss 829.60046 Test MSE 846.3078637302962 Test RE 0.48972622219944345 Lambda1 -1.5466461\n",
      "4 Train Loss 816.85187 Test MSE 828.843196542852 Test RE 0.4846468106806458 Lambda1 -1.6127617\n",
      "5 Train Loss 801.43646 Test MSE 802.5696048259753 Test RE 0.47690351633876993 Lambda1 -1.42186\n",
      "6 Train Loss 762.45435 Test MSE 760.1194328187269 Test RE 0.4641197914701477 Lambda1 -1.0527594\n",
      "7 Train Loss 739.95734 Test MSE 734.8160419430487 Test RE 0.4563294368454032 Lambda1 -1.0043019\n",
      "8 Train Loss 684.95886 Test MSE 670.4228874356137 Test RE 0.4358766322544284 Lambda1 -1.0425628\n",
      "9 Train Loss 637.6325 Test MSE 627.650892109029 Test RE 0.42174335237295607 Lambda1 -1.126029\n",
      "10 Train Loss 613.3523 Test MSE 599.7713413621271 Test RE 0.4122702776222886 Lambda1 -1.3455316\n",
      "11 Train Loss 596.0166 Test MSE 585.5229513889853 Test RE 0.407343820419374 Lambda1 -1.4444762\n",
      "12 Train Loss 588.31635 Test MSE 577.8253197258648 Test RE 0.40465737017636483 Lambda1 -1.4896669\n",
      "13 Train Loss 573.4276 Test MSE 561.2143519064583 Test RE 0.3987985356700087 Lambda1 -1.4449059\n",
      "14 Train Loss 552.66254 Test MSE 541.395847203208 Test RE 0.3916937399603528 Lambda1 -1.4850829\n",
      "15 Train Loss 538.31335 Test MSE 524.3605943443442 Test RE 0.3854820804035463 Lambda1 -1.5214287\n",
      "16 Train Loss 506.6078 Test MSE 481.8868458874693 Test RE 0.3695402143411052 Lambda1 -1.6183636\n",
      "17 Train Loss 480.97787 Test MSE 465.85584165606554 Test RE 0.36334144873964486 Lambda1 -1.5789857\n",
      "18 Train Loss 467.64102 Test MSE 455.86969168056874 Test RE 0.3594260335597332 Lambda1 -1.6551296\n",
      "19 Train Loss 447.04037 Test MSE 431.6753363541264 Test RE 0.34975810613890007 Lambda1 -1.666301\n",
      "20 Train Loss 422.47028 Test MSE 412.9906377968811 Test RE 0.3421048836731551 Lambda1 -1.7102737\n",
      "21 Train Loss 397.0963 Test MSE 391.28540156616566 Test RE 0.3329936817199544 Lambda1 -1.7473805\n",
      "22 Train Loss 383.6963 Test MSE 376.12592456892156 Test RE 0.32647941590667134 Lambda1 -1.8408877\n",
      "23 Train Loss 363.2847 Test MSE 358.79234845031664 Test RE 0.3188678672637135 Lambda1 -1.9038534\n",
      "24 Train Loss 354.06476 Test MSE 351.0989679367708 Test RE 0.3154306907922 Lambda1 -1.8657103\n",
      "25 Train Loss 317.11475 Test MSE 313.6970551329237 Test RE 0.2981565753926367 Lambda1 -1.8587548\n",
      "26 Train Loss 283.45477 Test MSE 277.15777116865814 Test RE 0.28025456840937746 Lambda1 -1.89399\n",
      "27 Train Loss 261.6326 Test MSE 251.73113293889566 Test RE 0.267090004756833 Lambda1 -1.9129138\n",
      "28 Train Loss 249.64473 Test MSE 235.95121229761997 Test RE 0.25858318306505207 Lambda1 -1.9472673\n",
      "29 Train Loss 217.64906 Test MSE 205.6030677431979 Test RE 0.241381491129062 Lambda1 -1.8814602\n",
      "30 Train Loss 195.914 Test MSE 182.2097658579983 Test RE 0.22723488018675655 Lambda1 -1.9255354\n",
      "31 Train Loss 177.01059 Test MSE 166.88249634582098 Test RE 0.2174676014367845 Lambda1 -1.9308336\n",
      "32 Train Loss 167.05646 Test MSE 160.17048458332414 Test RE 0.21304944902047585 Lambda1 -1.9352837\n",
      "33 Train Loss 153.44571 Test MSE 137.05695511241214 Test RE 0.19707870970252625 Lambda1 -1.9313868\n",
      "34 Train Loss 140.74788 Test MSE 134.4801667134786 Test RE 0.19521729468648405 Lambda1 -1.953505\n",
      "35 Train Loss 133.10355 Test MSE 130.72403667354465 Test RE 0.19247170597138968 Lambda1 -1.966691\n",
      "36 Train Loss 124.781906 Test MSE 125.9302282652074 Test RE 0.18890965980078905 Lambda1 -1.9417559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 Train Loss 121.39367 Test MSE 126.60583478895431 Test RE 0.18941572525634245 Lambda1 -1.9235538\n",
      "38 Train Loss 110.864174 Test MSE 113.91889833428522 Test RE 0.17967475396786614 Lambda1 -1.9509065\n",
      "39 Train Loss 108.73148 Test MSE 109.27079091783813 Test RE 0.17597104533550056 Lambda1 -1.9485034\n",
      "40 Train Loss 107.2813 Test MSE 107.8961542922548 Test RE 0.17486067615807288 Lambda1 -1.9520688\n",
      "41 Train Loss 104.910904 Test MSE 103.28059896278519 Test RE 0.17107972543283473 Lambda1 -1.9763169\n",
      "42 Train Loss 101.66526 Test MSE 103.50088083626474 Test RE 0.1712620718256035 Lambda1 -1.97093\n",
      "43 Train Loss 100.06436 Test MSE 104.96442174898698 Test RE 0.1724686760064806 Lambda1 -1.9728045\n",
      "44 Train Loss 98.771736 Test MSE 102.15763966448924 Test RE 0.17014711736329283 Lambda1 -1.9742384\n",
      "45 Train Loss 97.04307 Test MSE 100.7126557684032 Test RE 0.16893949623377022 Lambda1 -1.9421633\n",
      "46 Train Loss 95.1008 Test MSE 99.44010236977728 Test RE 0.1678687868894624 Lambda1 -1.9186708\n",
      "47 Train Loss 93.395645 Test MSE 99.3251411789545 Test RE 0.16777172355156014 Lambda1 -1.9089028\n",
      "48 Train Loss 92.02385 Test MSE 99.00106039611646 Test RE 0.16749779484306646 Lambda1 -1.9114267\n",
      "49 Train Loss 90.69211 Test MSE 98.52041496204185 Test RE 0.16709070323019853 Lambda1 -1.8936884\n",
      "50 Train Loss 90.1725 Test MSE 98.67309947066104 Test RE 0.16722012962902952 Lambda1 -1.8990908\n",
      "51 Train Loss 89.64655 Test MSE 98.69928857130122 Test RE 0.16724231933565906 Lambda1 -1.911798\n",
      "52 Train Loss 88.769005 Test MSE 97.82476022464661 Test RE 0.1664997426881071 Lambda1 -1.9168025\n",
      "53 Train Loss 87.82816 Test MSE 96.92167817394007 Test RE 0.16572942869205404 Lambda1 -1.9502115\n",
      "54 Train Loss 87.489586 Test MSE 96.30000935395161 Test RE 0.1651970681175032 Lambda1 -1.9700042\n",
      "55 Train Loss 86.99035 Test MSE 95.73095598574193 Test RE 0.1647082559416729 Lambda1 -1.9807608\n",
      "56 Train Loss 85.34839 Test MSE 94.82030232519554 Test RE 0.16392297919070678 Lambda1 -2.0596728\n",
      "57 Train Loss 82.386475 Test MSE 93.09688834389789 Test RE 0.16242645033484704 Lambda1 -2.1321692\n",
      "58 Train Loss 81.189964 Test MSE 93.44188984517977 Test RE 0.16272713465364433 Lambda1 -2.1870627\n",
      "59 Train Loss 79.4143 Test MSE 91.08816087106517 Test RE 0.16066457789500366 Lambda1 -2.2754645\n",
      "60 Train Loss 78.505264 Test MSE 90.02642284654847 Test RE 0.15972546738996 Lambda1 -2.3203375\n",
      "61 Train Loss 78.15972 Test MSE 89.51581273251728 Test RE 0.15927185938449748 Lambda1 -2.360321\n",
      "62 Train Loss 77.60538 Test MSE 88.08889658709332 Test RE 0.15799733282393652 Lambda1 -2.4396574\n",
      "63 Train Loss 77.328896 Test MSE 88.32383744514506 Test RE 0.15820788893676072 Lambda1 -2.426853\n",
      "64 Train Loss 76.44516 Test MSE 88.01402166228984 Test RE 0.15793017025405828 Lambda1 -2.4515038\n",
      "65 Train Loss 76.15596 Test MSE 87.49746236421247 Test RE 0.15746603767178954 Lambda1 -2.4840682\n",
      "66 Train Loss 75.71648 Test MSE 87.0136101817893 Test RE 0.15703004840234452 Lambda1 -2.5417898\n",
      "67 Train Loss 75.11886 Test MSE 87.21391326398341 Test RE 0.15721068406431588 Lambda1 -2.5561025\n",
      "68 Train Loss 74.90828 Test MSE 86.83061659707366 Test RE 0.15686484082548233 Lambda1 -2.5666583\n",
      "69 Train Loss 73.77855 Test MSE 85.79479656003558 Test RE 0.15592639745700831 Lambda1 -2.641871\n",
      "70 Train Loss 73.52516 Test MSE 85.30883713672169 Test RE 0.15548417071397247 Lambda1 -2.680781\n",
      "71 Train Loss 73.42472 Test MSE 84.95316398342746 Test RE 0.15515970663150705 Lambda1 -2.710028\n",
      "72 Train Loss 73.22621 Test MSE 84.86178005644946 Test RE 0.15507623170370377 Lambda1 -2.7137039\n",
      "73 Train Loss 73.128555 Test MSE 84.68195501009194 Test RE 0.15491183862019725 Lambda1 -2.7178588\n",
      "74 Train Loss 72.756325 Test MSE 84.38520302037747 Test RE 0.154640170702819 Lambda1 -2.772028\n",
      "Training time: 131.85\n",
      "Training time: 131.85\n",
      "inv_HT_stan_tune16\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 838.026 Test MSE 858.3423556285342 Test RE 0.493195882828406 Lambda1 -0.05371546\n",
      "1 Train Loss 837.7402 Test MSE 856.9417837817066 Test RE 0.4927933404898219 Lambda1 -0.17722565\n",
      "2 Train Loss 834.05365 Test MSE 853.084253841197 Test RE 0.4916829329730926 Lambda1 -0.7084921\n",
      "3 Train Loss 814.94617 Test MSE 826.0109991888805 Test RE 0.48381807129933063 Lambda1 -1.0152087\n",
      "4 Train Loss 781.9505 Test MSE 775.4814569254959 Test RE 0.46878626568781423 Lambda1 -1.3454175\n",
      "5 Train Loss 727.1907 Test MSE 729.9049106369359 Test RE 0.45480194506539584 Lambda1 -1.1513821\n",
      "6 Train Loss 684.7608 Test MSE 683.0263863629361 Test RE 0.4399546491196882 Lambda1 -0.9807177\n",
      "7 Train Loss 655.7995 Test MSE 653.9713746976504 Test RE 0.4304954247056186 Lambda1 -0.8968941\n",
      "8 Train Loss 641.2339 Test MSE 643.6834782550878 Test RE 0.4270958501393338 Lambda1 -1.0315205\n",
      "9 Train Loss 635.18494 Test MSE 640.5696866504219 Test RE 0.42606156874325807 Lambda1 -1.086681\n",
      "10 Train Loss 629.4944 Test MSE 633.7371787280431 Test RE 0.42378322700690496 Lambda1 -1.1854863\n",
      "11 Train Loss 625.96326 Test MSE 630.7015559608637 Test RE 0.4227670405764218 Lambda1 -1.3601445\n",
      "12 Train Loss 624.4656 Test MSE 629.7996478555367 Test RE 0.42246465236369973 Lambda1 -1.3839335\n",
      "13 Train Loss 621.1117 Test MSE 626.8925071849718 Test RE 0.4214884810115083 Lambda1 -1.4309853\n",
      "14 Train Loss 615.8755 Test MSE 624.5922640999065 Test RE 0.42071449109032993 Lambda1 -1.5324143\n",
      "15 Train Loss 614.259 Test MSE 622.1941672280669 Test RE 0.41990605617372306 Lambda1 -1.5737369\n",
      "16 Train Loss 612.715 Test MSE 620.1286390189932 Test RE 0.41920848548375134 Lambda1 -1.5124525\n",
      "17 Train Loss 611.321 Test MSE 617.9358294566507 Test RE 0.4184666567868445 Lambda1 -1.3828498\n",
      "18 Train Loss 607.00116 Test MSE 611.3315420229213 Test RE 0.416224435201422 Lambda1 -1.276702\n",
      "19 Train Loss 603.714 Test MSE 605.7708613400523 Test RE 0.41432711905430414 Lambda1 -1.2978977\n",
      "20 Train Loss 592.33936 Test MSE 596.9031146583377 Test RE 0.41128331671912877 Lambda1 -1.4444494\n",
      "21 Train Loss 587.5617 Test MSE 590.9857185082723 Test RE 0.40923961144886006 Lambda1 -1.4416202\n",
      "22 Train Loss 582.2299 Test MSE 582.1122862982679 Test RE 0.4061557009418023 Lambda1 -1.5373154\n",
      "23 Train Loss 575.00385 Test MSE 574.8885411927133 Test RE 0.40362773122308715 Lambda1 -1.5338635\n",
      "24 Train Loss 555.84564 Test MSE 549.7363886994127 Test RE 0.3946993518425755 Lambda1 -1.4253491\n",
      "25 Train Loss 538.29816 Test MSE 532.7871433739219 Test RE 0.38856711139919997 Lambda1 -1.3327521\n",
      "26 Train Loss 523.29584 Test MSE 517.4398481486728 Test RE 0.3829297480690477 Lambda1 -1.257654\n",
      "27 Train Loss 500.30157 Test MSE 501.75116775328524 Test RE 0.3770798855476429 Lambda1 -1.2359189\n",
      "28 Train Loss 474.5323 Test MSE 481.18540994443146 Test RE 0.369271164473967 Lambda1 -1.5549425\n",
      "29 Train Loss 422.51587 Test MSE 431.76011718212004 Test RE 0.34979245061440506 Lambda1 -1.735964\n",
      "30 Train Loss 391.1327 Test MSE 401.4977887090815 Test RE 0.33731119055808145 Lambda1 -1.6030802\n",
      "31 Train Loss 374.37958 Test MSE 387.0980747427103 Test RE 0.3312071291542279 Lambda1 -1.6020395\n",
      "32 Train Loss 343.64886 Test MSE 361.6036209685452 Test RE 0.3201146543905894 Lambda1 -1.6796179\n",
      "33 Train Loss 320.51855 Test MSE 343.986267451881 Test RE 0.3122192838145486 Lambda1 -1.6224291\n",
      "34 Train Loss 310.40363 Test MSE 337.2252853623786 Test RE 0.3091357529232681 Lambda1 -1.6718091\n",
      "35 Train Loss 303.11948 Test MSE 331.4353311150182 Test RE 0.30647042589726714 Lambda1 -1.7056605\n",
      "36 Train Loss 297.6562 Test MSE 325.3857158987053 Test RE 0.3036605767777196 Lambda1 -1.6951191\n",
      "37 Train Loss 290.20477 Test MSE 315.46547415856196 Test RE 0.2989958002819994 Lambda1 -1.6999279\n",
      "38 Train Loss 285.99417 Test MSE 310.27329860316587 Test RE 0.29652503923517665 Lambda1 -1.6939963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Train Loss 283.13074 Test MSE 310.2373467335057 Test RE 0.2965078593192066 Lambda1 -1.7164038\n",
      "40 Train Loss 277.50616 Test MSE 304.7354334512852 Test RE 0.293866884183709 Lambda1 -1.6984661\n",
      "41 Train Loss 276.37128 Test MSE 305.2516981649999 Test RE 0.29411570477482024 Lambda1 -1.7009513\n",
      "42 Train Loss 273.6333 Test MSE 303.38889484173217 Test RE 0.29321690857330646 Lambda1 -1.6798615\n",
      "43 Train Loss 269.61572 Test MSE 301.3411480202562 Test RE 0.29222568805501 Lambda1 -1.6280731\n",
      "44 Train Loss 268.0873 Test MSE 301.60723365786066 Test RE 0.29235467790862646 Lambda1 -1.6504848\n",
      "45 Train Loss 266.76495 Test MSE 300.9119250041687 Test RE 0.2920174943039635 Lambda1 -1.6594259\n",
      "46 Train Loss 263.80588 Test MSE 299.02466881964773 Test RE 0.2911003178629666 Lambda1 -1.608504\n",
      "47 Train Loss 261.75696 Test MSE 297.1304491769965 Test RE 0.29017684227718105 Lambda1 -1.635173\n",
      "48 Train Loss 260.42593 Test MSE 296.3057444150771 Test RE 0.2897738601688436 Lambda1 -1.6225069\n",
      "49 Train Loss 258.70386 Test MSE 294.56751110375615 Test RE 0.28892265246705523 Lambda1 -1.6274041\n",
      "50 Train Loss 257.75443 Test MSE 293.1755453948975 Test RE 0.2882391985132482 Lambda1 -1.590859\n",
      "51 Train Loss 257.30228 Test MSE 292.4674716098363 Test RE 0.2878909123000688 Lambda1 -1.5813097\n",
      "52 Train Loss 256.9444 Test MSE 292.4198888769118 Test RE 0.28786749227059827 Lambda1 -1.5996087\n",
      "53 Train Loss 255.2496 Test MSE 291.2783613158265 Test RE 0.2873050647202601 Lambda1 -1.5047243\n",
      "54 Train Loss 254.23064 Test MSE 291.0189847863338 Test RE 0.2871771170291481 Lambda1 -1.4717515\n",
      "55 Train Loss 253.80745 Test MSE 289.9466506583459 Test RE 0.2866475398479834 Lambda1 -1.4156076\n",
      "56 Train Loss 252.66605 Test MSE 288.0632902456498 Test RE 0.28571505768566613 Lambda1 -1.3400061\n",
      "57 Train Loss 251.75662 Test MSE 288.01666075280417 Test RE 0.2856919320603621 Lambda1 -1.3290374\n",
      "58 Train Loss 249.5101 Test MSE 284.3825328515258 Test RE 0.28388381284127834 Lambda1 -1.2415822\n",
      "59 Train Loss 246.0556 Test MSE 278.52891814469734 Test RE 0.28094694681847804 Lambda1 -1.115261\n",
      "60 Train Loss 243.00847 Test MSE 273.0081437605791 Test RE 0.2781486597789025 Lambda1 -1.0591891\n",
      "61 Train Loss 239.46944 Test MSE 268.2037999407406 Test RE 0.27569039394859557 Lambda1 -1.0207572\n",
      "62 Train Loss 238.04445 Test MSE 266.1854487773898 Test RE 0.27465108941798366 Lambda1 -1.0026016\n",
      "63 Train Loss 232.30737 Test MSE 256.88421111659255 Test RE 0.2698098973085529 Lambda1 -1.0108899\n",
      "64 Train Loss 227.5208 Test MSE 248.2862144854323 Test RE 0.26525615750127474 Lambda1 -1.0318364\n",
      "65 Train Loss 223.87668 Test MSE 245.24916084664753 Test RE 0.2636288502967131 Lambda1 -1.0891421\n",
      "66 Train Loss 219.74731 Test MSE 241.63707572432816 Test RE 0.26168025617521545 Lambda1 -1.1154844\n",
      "67 Train Loss 216.03151 Test MSE 233.05967062565767 Test RE 0.2569938525139679 Lambda1 -1.1275045\n",
      "68 Train Loss 202.78758 Test MSE 216.12374860474785 Test RE 0.24748017614285442 Lambda1 -1.1379533\n",
      "69 Train Loss 193.49684 Test MSE 208.12888848238111 Test RE 0.24285964344334018 Lambda1 -1.153393\n",
      "70 Train Loss 189.46048 Test MSE 205.98776096857543 Test RE 0.2416072037864934 Lambda1 -1.1688663\n",
      "71 Train Loss 187.40065 Test MSE 203.69381517433118 Test RE 0.24025812967178933 Lambda1 -1.205104\n",
      "72 Train Loss 183.98874 Test MSE 198.47320337314426 Test RE 0.237159272952825 Lambda1 -1.2415748\n",
      "73 Train Loss 180.31117 Test MSE 191.93228214248268 Test RE 0.23321860015156506 Lambda1 -1.2667528\n",
      "74 Train Loss 176.15288 Test MSE 189.304664012255 Test RE 0.23161667761794566 Lambda1 -1.259857\n",
      "Training time: 128.34\n",
      "Training time: 128.34\n",
      "inv_HT_stan_tune16\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 838.19653 Test MSE 858.4802848622672 Test RE 0.49323550769204516 Lambda1 -0.20484227\n",
      "1 Train Loss 838.0121 Test MSE 858.252302004437 Test RE 0.49317001014407313 Lambda1 -0.21620287\n",
      "2 Train Loss 837.70856 Test MSE 857.4262267752789 Test RE 0.4929326128159075 Lambda1 -0.24316767\n",
      "3 Train Loss 835.94867 Test MSE 855.5508992513521 Test RE 0.4923932567297148 Lambda1 -0.36449277\n",
      "4 Train Loss 828.9249 Test MSE 843.919934777955 Test RE 0.4890348322496232 Lambda1 -0.39488745\n",
      "5 Train Loss 820.9913 Test MSE 835.0039193859701 Test RE 0.4864446458806058 Lambda1 -0.41171157\n",
      "6 Train Loss 806.91376 Test MSE 814.2485450337741 Test RE 0.4803609181289292 Lambda1 -0.55575657\n",
      "7 Train Loss 795.5316 Test MSE 807.6739677105073 Test RE 0.47841767182081374 Lambda1 -0.6127732\n",
      "8 Train Loss 781.63104 Test MSE 788.8244502668059 Test RE 0.47280205157550986 Lambda1 -0.6704603\n",
      "9 Train Loss 761.7496 Test MSE 756.6196624630336 Test RE 0.4630500999095566 Lambda1 -0.64879835\n",
      "10 Train Loss 746.7114 Test MSE 748.9827374176089 Test RE 0.46070727977104503 Lambda1 -0.61814356\n",
      "11 Train Loss 725.4817 Test MSE 725.9463473643897 Test RE 0.45356698287312575 Lambda1 -0.6167729\n",
      "12 Train Loss 720.25397 Test MSE 722.9057850600179 Test RE 0.4526161235113751 Lambda1 -0.615091\n",
      "13 Train Loss 715.7579 Test MSE 718.1218503561122 Test RE 0.45111601090879 Lambda1 -0.6080789\n",
      "14 Train Loss 710.4531 Test MSE 712.5789995619451 Test RE 0.4493716603921695 Lambda1 -0.6495794\n",
      "15 Train Loss 700.0115 Test MSE 699.9967953476902 Test RE 0.44538665118064946 Lambda1 -0.6877001\n",
      "16 Train Loss 683.562 Test MSE 673.2534881816216 Test RE 0.4367958230984902 Lambda1 -0.78478456\n",
      "17 Train Loss 659.2136 Test MSE 655.2451682261243 Test RE 0.43091447632198193 Lambda1 -0.74095535\n",
      "18 Train Loss 647.27716 Test MSE 648.4695883040191 Test RE 0.4286807455819535 Lambda1 -0.7384509\n",
      "19 Train Loss 636.58606 Test MSE 635.2064676567463 Test RE 0.4242742030301698 Lambda1 -0.8355199\n",
      "20 Train Loss 632.89874 Test MSE 635.4389146183555 Test RE 0.424351825215797 Lambda1 -0.8149055\n",
      "21 Train Loss 625.7464 Test MSE 626.2570526329993 Test RE 0.421274804281502 Lambda1 -0.86752445\n",
      "22 Train Loss 615.32666 Test MSE 618.7560700226021 Test RE 0.418744298478999 Lambda1 -0.9040978\n",
      "23 Train Loss 599.89484 Test MSE 596.3840079605716 Test RE 0.4111044381391905 Lambda1 -0.9456277\n",
      "24 Train Loss 572.5313 Test MSE 560.7721631347604 Test RE 0.39864139519082487 Lambda1 -0.9910523\n",
      "25 Train Loss 512.8684 Test MSE 494.9732670623565 Test RE 0.37452433584502465 Lambda1 -0.8187024\n",
      "26 Train Loss 438.99216 Test MSE 446.04274764881393 Test RE 0.35553094880633 Lambda1 -0.8579488\n",
      "27 Train Loss 401.5912 Test MSE 410.3532558586854 Test RE 0.34101078340864216 Lambda1 -0.7893419\n",
      "28 Train Loss 358.27112 Test MSE 366.96513630595194 Test RE 0.322479100355987 Lambda1 -0.7406577\n",
      "29 Train Loss 318.24127 Test MSE 337.26435667383794 Test RE 0.30915366081745277 Lambda1 -0.7102935\n",
      "30 Train Loss 286.40228 Test MSE 311.17619986204795 Test RE 0.29695617259145873 Lambda1 -0.69989175\n",
      "31 Train Loss 275.02185 Test MSE 303.8613601891501 Test RE 0.2934451320482988 Lambda1 -0.7051658\n",
      "32 Train Loss 266.86035 Test MSE 293.34527374716413 Test RE 0.28832262171544515 Lambda1 -0.7148541\n",
      "33 Train Loss 262.1965 Test MSE 289.1589357402599 Test RE 0.28625789901202325 Lambda1 -0.7199724\n",
      "34 Train Loss 253.56485 Test MSE 275.5904759553896 Test RE 0.2794610413708153 Lambda1 -0.71062046\n",
      "35 Train Loss 247.4172 Test MSE 267.0475869473256 Test RE 0.2750955084653943 Lambda1 -0.71466315\n",
      "36 Train Loss 240.29268 Test MSE 255.47570826811 Test RE 0.2690691932260211 Lambda1 -0.73347604\n",
      "37 Train Loss 233.78148 Test MSE 245.64463984040282 Test RE 0.26384132334797694 Lambda1 -0.7535857\n",
      "38 Train Loss 228.0908 Test MSE 240.6451142293382 Test RE 0.26114258270916274 Lambda1 -0.75264275\n",
      "39 Train Loss 222.53333 Test MSE 241.26425209361184 Test RE 0.26147830404577316 Lambda1 -0.7564016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 Train Loss 215.00484 Test MSE 233.4808772896822 Test RE 0.25722597903000605 Lambda1 -0.79029\n",
      "41 Train Loss 209.24292 Test MSE 222.8405532374153 Test RE 0.2512964101665365 Lambda1 -0.84130234\n",
      "42 Train Loss 204.6268 Test MSE 214.91183817094955 Test RE 0.24678533014134701 Lambda1 -0.882293\n",
      "43 Train Loss 196.83275 Test MSE 208.4810896205081 Test RE 0.2430650432984669 Lambda1 -0.94061965\n",
      "44 Train Loss 193.2727 Test MSE 202.0433647634337 Test RE 0.23928279166499467 Lambda1 -0.99485713\n",
      "45 Train Loss 186.55087 Test MSE 191.54534855571302 Test RE 0.232983398352407 Lambda1 -1.0317343\n",
      "46 Train Loss 181.36606 Test MSE 191.18857580080723 Test RE 0.23276631954716626 Lambda1 -1.0300387\n",
      "47 Train Loss 173.51468 Test MSE 185.78459494534115 Test RE 0.22945314809959674 Lambda1 -1.0618411\n",
      "48 Train Loss 169.92392 Test MSE 180.64785292511826 Test RE 0.22625884859151604 Lambda1 -1.0674292\n",
      "49 Train Loss 164.71794 Test MSE 171.4988955331818 Test RE 0.22045493966894314 Lambda1 -1.0603904\n",
      "50 Train Loss 159.41461 Test MSE 168.94703046346362 Test RE 0.21880863253741992 Lambda1 -1.0720012\n",
      "51 Train Loss 155.0903 Test MSE 166.5391089485546 Test RE 0.2172437490582626 Lambda1 -1.1104047\n",
      "52 Train Loss 153.36655 Test MSE 165.91726283268426 Test RE 0.21683778278184987 Lambda1 -1.1300579\n",
      "53 Train Loss 148.9084 Test MSE 161.42377939585398 Test RE 0.21388135470595945 Lambda1 -1.1760776\n",
      "54 Train Loss 147.54533 Test MSE 161.17589326517486 Test RE 0.21371707100720944 Lambda1 -1.1844882\n",
      "55 Train Loss 145.12737 Test MSE 159.89321776689445 Test RE 0.2128649670634349 Lambda1 -1.1986686\n",
      "56 Train Loss 143.84164 Test MSE 157.3005183285726 Test RE 0.21113209032339034 Lambda1 -1.2111905\n",
      "57 Train Loss 141.70592 Test MSE 154.4941304342451 Test RE 0.20924021749151564 Lambda1 -1.2623044\n",
      "58 Train Loss 139.89929 Test MSE 153.5269561113276 Test RE 0.20858423954057506 Lambda1 -1.2969335\n",
      "59 Train Loss 139.34662 Test MSE 153.7839156644211 Test RE 0.20875872129390105 Lambda1 -1.3146011\n",
      "60 Train Loss 138.88048 Test MSE 153.10992290562848 Test RE 0.208300752822142 Lambda1 -1.3246312\n",
      "61 Train Loss 137.67317 Test MSE 152.46038762135564 Test RE 0.20785844808752726 Lambda1 -1.323341\n",
      "62 Train Loss 135.8763 Test MSE 147.1438711231116 Test RE 0.20420212601764104 Lambda1 -1.3920041\n",
      "63 Train Loss 135.06134 Test MSE 145.51220230379127 Test RE 0.20306677774375226 Lambda1 -1.4221016\n",
      "64 Train Loss 133.51685 Test MSE 145.32518229940038 Test RE 0.20293623968714106 Lambda1 -1.424331\n",
      "65 Train Loss 132.09598 Test MSE 145.5890047577945 Test RE 0.20312036077399884 Lambda1 -1.419866\n",
      "66 Train Loss 130.37677 Test MSE 143.43098110025548 Test RE 0.20160934338225373 Lambda1 -1.4425818\n",
      "67 Train Loss 128.6203 Test MSE 139.9508565129621 Test RE 0.19914845936151362 Lambda1 -1.471394\n",
      "68 Train Loss 127.89636 Test MSE 140.53665882812325 Test RE 0.19956481909863558 Lambda1 -1.444927\n",
      "69 Train Loss 127.096504 Test MSE 139.86982277581083 Test RE 0.19909079597527127 Lambda1 -1.4519291\n",
      "70 Train Loss 125.36932 Test MSE 137.92245536695722 Test RE 0.19769999606717434 Lambda1 -1.4735068\n",
      "71 Train Loss 124.59007 Test MSE 137.1467257082991 Test RE 0.19714324118562904 Lambda1 -1.4872738\n",
      "72 Train Loss 124.05944 Test MSE 136.89375767858152 Test RE 0.19696134127118411 Lambda1 -1.5014479\n",
      "73 Train Loss 122.67036 Test MSE 135.87077225362845 Test RE 0.1962240307429832 Lambda1 -1.5370598\n",
      "74 Train Loss 120.748535 Test MSE 133.436893500447 Test RE 0.19445858982296776 Lambda1 -1.5816553\n",
      "Training time: 130.11\n",
      "Training time: 130.11\n",
      "inv_HT_stan_tune16\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 837.97595 Test MSE 857.941545743146 Test RE 0.4930807184850805 Lambda1 -0.029708574\n",
      "1 Train Loss 837.81793 Test MSE 857.8391046057111 Test RE 0.49305127984919195 Lambda1 -0.11214476\n",
      "2 Train Loss 837.1701 Test MSE 856.4343552958546 Test RE 0.4926474178545454 Lambda1 -0.699648\n",
      "3 Train Loss 829.1546 Test MSE 839.6830223002156 Test RE 0.4878056842760056 Lambda1 -0.8971098\n",
      "4 Train Loss 815.4356 Test MSE 828.6547285325512 Test RE 0.48459170640188676 Lambda1 -1.0434734\n",
      "5 Train Loss 790.15906 Test MSE 795.9433590809535 Test RE 0.4749307095096912 Lambda1 -1.0006561\n",
      "6 Train Loss 767.3246 Test MSE 775.3016634761916 Test RE 0.46873191907203915 Lambda1 -0.9970563\n",
      "7 Train Loss 729.1848 Test MSE 736.7019629782278 Test RE 0.4569146512081879 Lambda1 -0.8516212\n",
      "8 Train Loss 692.548 Test MSE 695.7208441329949 Test RE 0.4440242386156852 Lambda1 -0.7966627\n",
      "9 Train Loss 649.40356 Test MSE 634.4734670505254 Test RE 0.4240293353896197 Lambda1 -0.872686\n",
      "10 Train Loss 599.979 Test MSE 574.409438185677 Test RE 0.4034595076871375 Lambda1 -0.9294604\n",
      "11 Train Loss 546.65454 Test MSE 522.3771185933044 Test RE 0.38475231658517256 Lambda1 -1.0297471\n",
      "12 Train Loss 518.889 Test MSE 519.1307239903183 Test RE 0.3835549014709875 Lambda1 -1.0019033\n",
      "13 Train Loss 501.2824 Test MSE 503.736771875285 Test RE 0.37782526706094727 Lambda1 -1.0049531\n",
      "14 Train Loss 485.31946 Test MSE 479.36104459000137 Test RE 0.3685704727335634 Lambda1 -1.1154119\n",
      "15 Train Loss 471.98877 Test MSE 456.9143516131314 Test RE 0.35983762388904067 Lambda1 -1.233254\n",
      "16 Train Loss 454.39255 Test MSE 446.5425841806771 Test RE 0.3557300974310017 Lambda1 -1.2493367\n",
      "17 Train Loss 435.57916 Test MSE 427.1277988389319 Test RE 0.34791094333419137 Lambda1 -1.2855227\n",
      "18 Train Loss 421.8143 Test MSE 419.30478614995246 Test RE 0.344710157193803 Lambda1 -1.3152535\n",
      "19 Train Loss 400.25787 Test MSE 387.23869785400086 Test RE 0.33126728335140787 Lambda1 -1.404518\n",
      "20 Train Loss 379.85748 Test MSE 372.55623526475557 Test RE 0.3249264673027782 Lambda1 -1.3728046\n",
      "21 Train Loss 367.30057 Test MSE 359.70278347497526 Test RE 0.31927217430204813 Lambda1 -1.3758472\n",
      "22 Train Loss 354.42184 Test MSE 347.93723724214965 Test RE 0.31400721421965017 Lambda1 -1.4464755\n",
      "23 Train Loss 342.3644 Test MSE 330.34416745765674 Test RE 0.3059655233535025 Lambda1 -1.5311352\n",
      "24 Train Loss 336.97244 Test MSE 332.2101044570859 Test RE 0.30682842411148253 Lambda1 -1.5493429\n",
      "25 Train Loss 328.70343 Test MSE 321.4184112729989 Test RE 0.3018036902185233 Lambda1 -1.5553817\n",
      "26 Train Loss 324.69537 Test MSE 319.2830629276731 Test RE 0.3007995007856077 Lambda1 -1.545108\n",
      "27 Train Loss 317.5665 Test MSE 321.52033064262815 Test RE 0.30185153627020184 Lambda1 -1.3809634\n",
      "28 Train Loss 309.9642 Test MSE 312.6695367538875 Test RE 0.2976678671942375 Lambda1 -1.3333669\n",
      "29 Train Loss 305.0767 Test MSE 303.5824221628758 Test RE 0.29331041304463396 Lambda1 -1.4100784\n",
      "30 Train Loss 301.16055 Test MSE 294.2035554824772 Test RE 0.2887441067671509 Lambda1 -1.4590759\n",
      "31 Train Loss 290.14093 Test MSE 276.1658578514763 Test RE 0.2797526207193183 Lambda1 -1.4905275\n",
      "32 Train Loss 277.01135 Test MSE 273.02136970397834 Test RE 0.2781553971857549 Lambda1 -1.457427\n",
      "33 Train Loss 271.4677 Test MSE 267.07330499979435 Test RE 0.27510875470077856 Lambda1 -1.4959582\n",
      "34 Train Loss 262.49402 Test MSE 254.556148118649 Test RE 0.2685845123992328 Lambda1 -1.5086772\n",
      "35 Train Loss 257.47812 Test MSE 253.34855999415433 Test RE 0.2679466864355904 Lambda1 -1.4662957\n",
      "36 Train Loss 236.50398 Test MSE 231.1910668491329 Test RE 0.25596152863469707 Lambda1 -1.5084343\n",
      "37 Train Loss 226.91412 Test MSE 229.54168934717669 Test RE 0.2550468460929404 Lambda1 -1.5129223\n",
      "38 Train Loss 217.74883 Test MSE 223.3256559370542 Test RE 0.2515697856667805 Lambda1 -1.5334553\n",
      "39 Train Loss 209.07411 Test MSE 217.36312401806768 Test RE 0.2481887571672317 Lambda1 -1.58891\n",
      "40 Train Loss 206.20294 Test MSE 214.11991537489357 Test RE 0.24633022422204043 Lambda1 -1.5865062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 Train Loss 201.6069 Test MSE 203.31381395764606 Test RE 0.2400339181469364 Lambda1 -1.6435978\n",
      "42 Train Loss 196.98843 Test MSE 191.70102778255838 Test RE 0.23307805820399577 Lambda1 -1.7151515\n",
      "43 Train Loss 192.42368 Test MSE 190.90911314953826 Test RE 0.23259613866472967 Lambda1 -1.7599767\n",
      "44 Train Loss 182.3728 Test MSE 183.85951786885042 Test RE 0.2282612699147847 Lambda1 -1.8086889\n",
      "45 Train Loss 178.10321 Test MSE 178.37156418109376 Test RE 0.22482881994547615 Lambda1 -1.8881856\n",
      "46 Train Loss 176.00766 Test MSE 176.7349632401808 Test RE 0.22379501454932427 Lambda1 -1.9003055\n",
      "47 Train Loss 173.09814 Test MSE 172.2816912320127 Test RE 0.22095749306337922 Lambda1 -1.9269937\n",
      "48 Train Loss 168.97935 Test MSE 169.92910971947677 Test RE 0.2194436719364613 Lambda1 -2.0105538\n",
      "49 Train Loss 165.65732 Test MSE 165.31910976843557 Test RE 0.21644656582390803 Lambda1 -2.006291\n",
      "50 Train Loss 163.5728 Test MSE 162.39533925261662 Test RE 0.21452403203553821 Lambda1 -2.0002847\n",
      "51 Train Loss 159.68483 Test MSE 159.67956212871758 Test RE 0.21272270023012324 Lambda1 -2.0305161\n",
      "52 Train Loss 156.02368 Test MSE 157.48261249172717 Test RE 0.21125426030003228 Lambda1 -2.035816\n",
      "53 Train Loss 151.83702 Test MSE 152.59353998816277 Test RE 0.20794919560709763 Lambda1 -2.0751886\n",
      "54 Train Loss 150.21202 Test MSE 148.04082402590413 Test RE 0.2048235634301499 Lambda1 -2.0786736\n",
      "55 Train Loss 147.22989 Test MSE 147.45664543265246 Test RE 0.2044190405036236 Lambda1 -2.041697\n",
      "56 Train Loss 145.07224 Test MSE 147.79417835234912 Test RE 0.20465286759146809 Lambda1 -2.0672765\n",
      "57 Train Loss 143.44576 Test MSE 146.51695138217363 Test RE 0.2037666508848681 Lambda1 -2.085271\n",
      "58 Train Loss 141.31975 Test MSE 144.49248477991725 Test RE 0.20235400313860508 Lambda1 -2.0975897\n",
      "59 Train Loss 140.01895 Test MSE 141.0365156592855 Test RE 0.19991940734593094 Lambda1 -2.0941172\n",
      "60 Train Loss 139.02293 Test MSE 140.15071013042942 Test RE 0.19929060333291052 Lambda1 -2.1034267\n",
      "61 Train Loss 138.54169 Test MSE 140.38428450702858 Test RE 0.19945660249155883 Lambda1 -2.1032531\n",
      "62 Train Loss 136.75519 Test MSE 138.71928176507794 Test RE 0.1982702647011616 Lambda1 -2.0927835\n",
      "63 Train Loss 134.99191 Test MSE 134.10603764533147 Test RE 0.1949455544935244 Lambda1 -2.1013696\n",
      "64 Train Loss 134.70287 Test MSE 133.55314798562645 Test RE 0.19454328064903256 Lambda1 -2.1006398\n",
      "65 Train Loss 133.95116 Test MSE 131.30917664869602 Test RE 0.19290199080745465 Lambda1 -2.1101449\n",
      "66 Train Loss 133.29884 Test MSE 130.3507799619497 Test RE 0.19219672704086485 Lambda1 -2.1127672\n",
      "67 Train Loss 131.57245 Test MSE 129.29092042835538 Test RE 0.19141377318769284 Lambda1 -2.0924296\n",
      "68 Train Loss 129.35422 Test MSE 129.04333341860047 Test RE 0.19123041045206743 Lambda1 -2.0741546\n",
      "69 Train Loss 127.09792 Test MSE 127.936024308278 Test RE 0.19040817739317178 Lambda1 -2.0659034\n",
      "70 Train Loss 126.11226 Test MSE 128.15561124101413 Test RE 0.19057151380402776 Lambda1 -2.0633204\n",
      "71 Train Loss 124.94346 Test MSE 129.03914117308142 Test RE 0.19122730416474013 Lambda1 -2.0655358\n",
      "72 Train Loss 124.19629 Test MSE 128.57943548239524 Test RE 0.19088637383591395 Lambda1 -2.0553048\n",
      "73 Train Loss 123.84225 Test MSE 128.06095244831792 Test RE 0.19050112046918372 Lambda1 -2.0566468\n",
      "74 Train Loss 122.94289 Test MSE 127.36525921108593 Test RE 0.1899829655494001 Lambda1 -2.059767\n",
      "Training time: 106.17\n",
      "Training time: 106.17\n",
      "inv_HT_stan_tune16\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 838.058 Test MSE 858.3127606506611 Test RE 0.4931873802495913 Lambda1 -0.010122307\n",
      "1 Train Loss 837.8566 Test MSE 857.9376941439343 Test RE 0.493079611678068 Lambda1 -0.02231304\n",
      "2 Train Loss 836.9076 Test MSE 855.7806684534523 Test RE 0.49245937156329805 Lambda1 -0.4828642\n",
      "3 Train Loss 831.1498 Test MSE 846.6909608052209 Test RE 0.489837051523195 Lambda1 -0.6512087\n",
      "4 Train Loss 821.7098 Test MSE 832.8320796554303 Test RE 0.4858116143143951 Lambda1 -0.8625809\n",
      "5 Train Loss 812.99603 Test MSE 825.0029780167898 Test RE 0.48352276788250387 Lambda1 -0.95420444\n",
      "6 Train Loss 804.5006 Test MSE 814.8377500397751 Test RE 0.48053468563289986 Lambda1 -0.9639674\n",
      "7 Train Loss 792.1515 Test MSE 797.681686731714 Test RE 0.4754490472157934 Lambda1 -0.815378\n",
      "8 Train Loss 778.9583 Test MSE 778.7292489457147 Test RE 0.46976690132575133 Lambda1 -0.8195284\n",
      "9 Train Loss 770.62213 Test MSE 768.1076281615221 Test RE 0.4665521656787789 Lambda1 -0.82236636\n",
      "10 Train Loss 756.2305 Test MSE 752.0606578895456 Test RE 0.46165294010182467 Lambda1 -0.72595173\n",
      "11 Train Loss 736.5708 Test MSE 731.6446160894648 Test RE 0.4553436255708226 Lambda1 -0.612173\n",
      "12 Train Loss 714.822 Test MSE 711.7599338133402 Test RE 0.4491133235877726 Lambda1 -0.4564752\n",
      "13 Train Loss 704.9771 Test MSE 700.1121603163741 Test RE 0.4454233512773797 Lambda1 -0.5597838\n",
      "14 Train Loss 697.5324 Test MSE 691.6768135970485 Test RE 0.4427318635161461 Lambda1 -0.58564526\n",
      "15 Train Loss 683.055 Test MSE 676.1199650578196 Test RE 0.4377246970402453 Lambda1 -0.5764587\n",
      "16 Train Loss 667.6913 Test MSE 666.22432781498 Test RE 0.43450963817335747 Lambda1 -0.5467235\n",
      "17 Train Loss 658.2346 Test MSE 660.0197499918082 Test RE 0.43248160120010476 Lambda1 -0.5814264\n",
      "18 Train Loss 644.24066 Test MSE 643.5193885271154 Test RE 0.42704140839025556 Lambda1 -0.59854025\n",
      "19 Train Loss 632.6563 Test MSE 625.9581315333503 Test RE 0.42117425215391263 Lambda1 -0.71756405\n",
      "20 Train Loss 610.1448 Test MSE 596.9313860436727 Test RE 0.4112930565003146 Lambda1 -0.7384532\n",
      "21 Train Loss 579.58 Test MSE 568.7586394217682 Test RE 0.4014700702028302 Lambda1 -0.6873218\n",
      "22 Train Loss 533.51855 Test MSE 503.14474107951446 Test RE 0.37760317690648 Lambda1 -0.76528305\n",
      "23 Train Loss 439.14404 Test MSE 409.33012420650607 Test RE 0.34058539787828734 Lambda1 -0.5311201\n",
      "24 Train Loss 349.81732 Test MSE 344.7629408566714 Test RE 0.3125715590546683 Lambda1 -0.5372908\n",
      "25 Train Loss 318.4492 Test MSE 321.05029507095463 Test RE 0.3016308148440311 Lambda1 -0.6233801\n",
      "26 Train Loss 305.63757 Test MSE 323.4704364960261 Test RE 0.30276555712719994 Lambda1 -0.6390366\n",
      "27 Train Loss 288.872 Test MSE 309.0805429705943 Test RE 0.29595453820574374 Lambda1 -0.6194518\n",
      "28 Train Loss 280.7842 Test MSE 301.47918488205016 Test RE 0.292292611040452 Lambda1 -0.582472\n",
      "29 Train Loss 274.09247 Test MSE 297.127377665095 Test RE 0.29017534245794274 Lambda1 -0.6056032\n",
      "30 Train Loss 268.40884 Test MSE 291.3671478736317 Test RE 0.28734884909895075 Lambda1 -0.6104857\n",
      "31 Train Loss 265.48053 Test MSE 291.2669223186736 Test RE 0.28729942318535595 Lambda1 -0.6420617\n",
      "32 Train Loss 257.8484 Test MSE 284.4295109604588 Test RE 0.2839072597348474 Lambda1 -0.76178634\n",
      "33 Train Loss 254.2771 Test MSE 276.68194710812656 Test RE 0.28001389474917954 Lambda1 -0.807782\n",
      "34 Train Loss 250.3272 Test MSE 268.9045850715211 Test RE 0.2760503323449929 Lambda1 -0.8314904\n",
      "35 Train Loss 249.19061 Test MSE 267.7185238773134 Test RE 0.2754408700407861 Lambda1 -0.8329863\n",
      "36 Train Loss 240.33429 Test MSE 254.90584508956215 Test RE 0.26876893331474233 Lambda1 -0.89739364\n",
      "37 Train Loss 237.13481 Test MSE 249.58015315879607 Test RE 0.26594644790839334 Lambda1 -0.9200295\n",
      "38 Train Loss 228.83075 Test MSE 241.55922703163597 Test RE 0.26163809975772706 Lambda1 -0.9723144\n",
      "39 Train Loss 221.10713 Test MSE 235.06649892879557 Test RE 0.2580979411195073 Lambda1 -1.0214715\n",
      "40 Train Loss 215.61514 Test MSE 229.41781363444733 Test RE 0.25497801682353466 Lambda1 -1.045745\n",
      "41 Train Loss 210.32828 Test MSE 215.17317361527924 Test RE 0.24693533155603647 Lambda1 -1.065908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 Train Loss 204.06122 Test MSE 213.1736233462805 Test RE 0.24578529956707673 Lambda1 -1.1056334\n",
      "43 Train Loss 201.46344 Test MSE 209.93622283456176 Test RE 0.2439118275264542 Lambda1 -1.123475\n",
      "44 Train Loss 190.85652 Test MSE 199.71588555328069 Test RE 0.2379005662863013 Lambda1 -1.1662453\n",
      "45 Train Loss 186.72911 Test MSE 199.2690720256574 Test RE 0.23763429625278198 Lambda1 -1.1953918\n",
      "46 Train Loss 179.78737 Test MSE 195.12731798926043 Test RE 0.23515174628541294 Lambda1 -1.2137574\n",
      "47 Train Loss 176.95865 Test MSE 192.07339711883176 Test RE 0.23330431942153926 Lambda1 -1.2666973\n",
      "48 Train Loss 173.1437 Test MSE 183.21672670073448 Test RE 0.2278619085017853 Lambda1 -1.2940412\n",
      "49 Train Loss 167.46072 Test MSE 177.13033888816517 Test RE 0.22404520179561074 Lambda1 -1.3309907\n",
      "50 Train Loss 165.97638 Test MSE 174.6312523692231 Test RE 0.22245908926534258 Lambda1 -1.3822789\n",
      "51 Train Loss 164.49866 Test MSE 172.4750382592176 Test RE 0.221081445569745 Lambda1 -1.3978068\n",
      "52 Train Loss 162.5473 Test MSE 169.4400056987352 Test RE 0.21912763389601012 Lambda1 -1.4132714\n",
      "53 Train Loss 160.34717 Test MSE 167.62531263788713 Test RE 0.21795105273482995 Lambda1 -1.3947053\n",
      "54 Train Loss 157.33969 Test MSE 165.42507928940196 Test RE 0.21651592581651088 Lambda1 -1.4114101\n",
      "55 Train Loss 154.49063 Test MSE 165.26541600406327 Test RE 0.21641141327943297 Lambda1 -1.428916\n",
      "56 Train Loss 152.41399 Test MSE 160.47817121753403 Test RE 0.21325398425921677 Lambda1 -1.4831219\n",
      "57 Train Loss 151.83218 Test MSE 158.36585840302922 Test RE 0.2118458448777548 Lambda1 -1.5147476\n",
      "58 Train Loss 150.40059 Test MSE 156.0991577789508 Test RE 0.2103242992208302 Lambda1 -1.5498841\n",
      "59 Train Loss 149.0754 Test MSE 155.36797136651404 Test RE 0.20983112942047166 Lambda1 -1.5590061\n",
      "60 Train Loss 147.82967 Test MSE 155.90056957682202 Test RE 0.2101904701287672 Lambda1 -1.5480822\n",
      "61 Train Loss 146.84393 Test MSE 157.37640274211446 Test RE 0.21118301101714818 Lambda1 -1.5663537\n",
      "62 Train Loss 146.31354 Test MSE 157.4240889867126 Test RE 0.21121500362149384 Lambda1 -1.5692097\n",
      "63 Train Loss 145.36562 Test MSE 156.40533794013228 Test RE 0.21053046811309442 Lambda1 -1.6028062\n",
      "64 Train Loss 144.93929 Test MSE 154.66460275587258 Test RE 0.2093556258610735 Lambda1 -1.6150166\n",
      "65 Train Loss 143.70401 Test MSE 152.68138929473153 Test RE 0.2080090459916811 Lambda1 -1.6173388\n",
      "66 Train Loss 141.87003 Test MSE 149.14997126322675 Test RE 0.20558941825662969 Lambda1 -1.672522\n",
      "67 Train Loss 139.82286 Test MSE 149.38657884841362 Test RE 0.20575242445358863 Lambda1 -1.6932287\n",
      "68 Train Loss 138.49406 Test MSE 148.56628638313347 Test RE 0.2051867461476933 Lambda1 -1.7203442\n",
      "69 Train Loss 136.98451 Test MSE 145.14949234639857 Test RE 0.20281353335282387 Lambda1 -1.7636728\n",
      "70 Train Loss 135.93909 Test MSE 144.2935786156041 Test RE 0.2022146764466324 Lambda1 -1.7735963\n",
      "71 Train Loss 135.53217 Test MSE 142.82911564039532 Test RE 0.20118590187539548 Lambda1 -1.7998437\n",
      "72 Train Loss 134.99922 Test MSE 140.625346145339 Test RE 0.19962777803466983 Lambda1 -1.8227907\n",
      "73 Train Loss 133.92326 Test MSE 137.21685198475052 Test RE 0.19719363668199288 Lambda1 -1.8350064\n",
      "74 Train Loss 131.43477 Test MSE 138.63314105790917 Test RE 0.19820869505910244 Lambda1 -1.8310454\n",
      "Training time: 74.75\n",
      "Training time: 74.75\n",
      "inv_HT_stan_tune16\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 837.90924 Test MSE 857.9186943609491 Test RE 0.49307415180752706 Lambda1 -0.0016657626\n",
      "1 Train Loss 837.8788 Test MSE 857.8998669044506 Test RE 0.4930687413979292 Lambda1 -0.11245181\n",
      "2 Train Loss 836.79346 Test MSE 856.8743920590332 Test RE 0.49277396295115955 Lambda1 -2.0213501\n",
      "3 Train Loss 833.45496 Test MSE 849.7345157058487 Test RE 0.49071665734402 Lambda1 -2.2166703\n",
      "4 Train Loss 827.72253 Test MSE 843.4755227237752 Test RE 0.4889060513161034 Lambda1 -1.874279\n",
      "5 Train Loss 804.9191 Test MSE 817.4920870287818 Test RE 0.4813167210391032 Lambda1 -2.1844559\n",
      "6 Train Loss 772.1021 Test MSE 771.7714228514021 Test RE 0.46766354497889595 Lambda1 -2.1502702\n",
      "7 Train Loss 743.7677 Test MSE 743.7644053684731 Test RE 0.4590995486361348 Lambda1 -2.8899035\n",
      "8 Train Loss 701.5088 Test MSE 700.1402375577484 Test RE 0.44543228279883545 Lambda1 -3.505774\n",
      "9 Train Loss 676.7792 Test MSE 663.3024438349886 Test RE 0.43355576890077696 Lambda1 -3.6938117\n",
      "10 Train Loss 663.74756 Test MSE 654.6936190436707 Test RE 0.4307330781414301 Lambda1 -3.438935\n",
      "11 Train Loss 646.8161 Test MSE 635.5051688986836 Test RE 0.42437394724146976 Lambda1 -3.7073812\n",
      "12 Train Loss 639.6134 Test MSE 632.5193408041115 Test RE 0.4233758440371102 Lambda1 -3.9452367\n",
      "13 Train Loss 635.4381 Test MSE 630.5431175439635 Test RE 0.4227139356275044 Lambda1 -4.106535\n",
      "14 Train Loss 633.75415 Test MSE 627.9606706251891 Test RE 0.4218474157266719 Lambda1 -4.184652\n",
      "15 Train Loss 632.48755 Test MSE 625.9211475521948 Test RE 0.42116180968379396 Lambda1 -4.1614747\n",
      "16 Train Loss 629.70435 Test MSE 622.4942767178901 Test RE 0.42000731282857373 Lambda1 -4.1782966\n",
      "17 Train Loss 626.7 Test MSE 620.6076305224602 Test RE 0.41937035395004124 Lambda1 -4.3565583\n",
      "18 Train Loss 625.61084 Test MSE 619.954898537799 Test RE 0.41914975687364414 Lambda1 -4.457873\n",
      "19 Train Loss 624.3956 Test MSE 618.337806196644 Test RE 0.41860274414975773 Lambda1 -4.5516863\n",
      "20 Train Loss 623.0349 Test MSE 616.2096240059487 Test RE 0.4178817541371275 Lambda1 -4.5726166\n",
      "21 Train Loss 621.8014 Test MSE 614.3156582634555 Test RE 0.4172390647176992 Lambda1 -4.663807\n",
      "22 Train Loss 619.6911 Test MSE 611.78464828117 Test RE 0.4163786550894351 Lambda1 -4.706576\n",
      "23 Train Loss 617.2007 Test MSE 608.7896997045477 Test RE 0.41535822853807447 Lambda1 -4.8495617\n",
      "24 Train Loss 613.2753 Test MSE 606.1161668646768 Test RE 0.4144451909770403 Lambda1 -5.1143236\n",
      "25 Train Loss 610.6359 Test MSE 601.4910109706981 Test RE 0.4128608870387817 Lambda1 -5.302795\n",
      "26 Train Loss 608.873 Test MSE 598.8932753895407 Test RE 0.4119683849953139 Lambda1 -5.343585\n",
      "27 Train Loss 604.93805 Test MSE 596.3379282833428 Test RE 0.41108855581714876 Lambda1 -5.3238554\n",
      "28 Train Loss 600.012 Test MSE 592.6617356349456 Test RE 0.4098194960146372 Lambda1 -5.2176313\n",
      "29 Train Loss 591.6211 Test MSE 581.1684412136218 Test RE 0.4058262940657087 Lambda1 -5.0575\n",
      "30 Train Loss 581.8372 Test MSE 571.03715550384 Test RE 0.4022734353060067 Lambda1 -5.124442\n",
      "31 Train Loss 576.96277 Test MSE 561.7240508112842 Test RE 0.398979590417589 Lambda1 -5.238804\n",
      "32 Train Loss 571.45667 Test MSE 553.1442178897345 Test RE 0.3959208371845267 Lambda1 -5.4317236\n",
      "33 Train Loss 563.1175 Test MSE 542.0806939051942 Test RE 0.3919414010656624 Lambda1 -5.554392\n",
      "34 Train Loss 548.9681 Test MSE 524.4352768345775 Test RE 0.38550953072782934 Lambda1 -5.71891\n",
      "35 Train Loss 536.5095 Test MSE 510.6230042200125 Test RE 0.3803989932224562 Lambda1 -5.854204\n",
      "36 Train Loss 529.7389 Test MSE 504.53996570614277 Test RE 0.37812636286097584 Lambda1 -5.874106\n",
      "37 Train Loss 519.0128 Test MSE 487.1528668537424 Test RE 0.37155388096657266 Lambda1 -5.8735104\n",
      "38 Train Loss 506.953 Test MSE 469.9264315376559 Test RE 0.3649254121376631 Lambda1 -5.8317018\n",
      "39 Train Loss 472.38623 Test MSE 422.28029204775896 Test RE 0.3459310756350608 Lambda1 -5.939598\n",
      "40 Train Loss 450.5824 Test MSE 384.4029738173654 Test RE 0.33005213015450424 Lambda1 -6.0593863\n",
      "41 Train Loss 425.02264 Test MSE 356.89163026713936 Test RE 0.3180221374561072 Lambda1 -5.994863\n",
      "42 Train Loss 415.59912 Test MSE 358.02633928316396 Test RE 0.3185272994963113 Lambda1 -6.003641\n",
      "43 Train Loss 390.0406 Test MSE 350.5828342851612 Test RE 0.3151987558025224 Lambda1 -5.9668984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 Train Loss 358.5582 Test MSE 330.68179751913414 Test RE 0.30612184029008627 Lambda1 -5.8446903\n",
      "45 Train Loss 353.42316 Test MSE 323.8511529858821 Test RE 0.30294367841785286 Lambda1 -5.810972\n",
      "46 Train Loss 348.1717 Test MSE 325.1934099994891 Test RE 0.30357083044272365 Lambda1 -5.6627164\n",
      "47 Train Loss 341.62363 Test MSE 327.42081123214314 Test RE 0.30460870530709216 Lambda1 -5.6496964\n",
      "48 Train Loss 326.8769 Test MSE 316.83099560510874 Test RE 0.2996422169896191 Lambda1 -5.6218863\n",
      "49 Train Loss 318.77994 Test MSE 307.7028953212485 Test RE 0.29529423067964405 Lambda1 -5.5187626\n",
      "50 Train Loss 309.64786 Test MSE 290.07396582111704 Test RE 0.28671046620497154 Lambda1 -5.5154004\n",
      "51 Train Loss 304.48547 Test MSE 279.9464082412959 Test RE 0.2816609375276889 Lambda1 -5.508458\n",
      "52 Train Loss 294.64145 Test MSE 277.5988448686718 Test RE 0.28047748075650253 Lambda1 -5.474902\n",
      "53 Train Loss 286.0558 Test MSE 267.6096424967254 Test RE 0.2753848533116007 Lambda1 -5.3950486\n",
      "54 Train Loss 282.27435 Test MSE 263.5152401560732 Test RE 0.2732700518993699 Lambda1 -5.358054\n",
      "55 Train Loss 276.01746 Test MSE 254.731869560631 Test RE 0.26867719905412235 Lambda1 -5.267654\n",
      "56 Train Loss 270.7221 Test MSE 252.07752437692676 Test RE 0.2672737044936935 Lambda1 -5.2389445\n",
      "57 Train Loss 268.0625 Test MSE 248.53578825976672 Test RE 0.2653894398762446 Lambda1 -5.181527\n",
      "58 Train Loss 262.6718 Test MSE 246.49569107578438 Test RE 0.26429797555729734 Lambda1 -5.0872006\n",
      "59 Train Loss 254.7398 Test MSE 231.00400915816232 Test RE 0.25585795789126686 Lambda1 -5.011644\n",
      "60 Train Loss 248.55556 Test MSE 222.8133313438031 Test RE 0.2512810606868959 Lambda1 -4.9397445\n",
      "61 Train Loss 244.87357 Test MSE 214.30209475719366 Test RE 0.24643499436115043 Lambda1 -4.911767\n",
      "62 Train Loss 237.56793 Test MSE 214.32592771329112 Test RE 0.24644869723960955 Lambda1 -4.9261317\n",
      "63 Train Loss 231.956 Test MSE 210.47075671255388 Test RE 0.24422215093524066 Lambda1 -4.9609\n",
      "64 Train Loss 230.73228 Test MSE 210.11521169925928 Test RE 0.2440157833827083 Lambda1 -4.946811\n",
      "65 Train Loss 225.11853 Test MSE 204.41541951078327 Test RE 0.24068332184922162 Lambda1 -4.888307\n",
      "66 Train Loss 222.59021 Test MSE 202.9894930784191 Test RE 0.23984239382321795 Lambda1 -4.8458295\n",
      "67 Train Loss 216.84178 Test MSE 197.05402477284306 Test RE 0.23630985050756004 Lambda1 -4.8335\n",
      "68 Train Loss 214.10289 Test MSE 196.76296355953403 Test RE 0.236135263743875 Lambda1 -4.883659\n",
      "69 Train Loss 210.6691 Test MSE 196.72682760118983 Test RE 0.2361135793635659 Lambda1 -4.882617\n",
      "70 Train Loss 207.4932 Test MSE 195.11011534005905 Test RE 0.23514138043248128 Lambda1 -4.8744183\n",
      "71 Train Loss 205.8736 Test MSE 192.91923680985593 Test RE 0.23381745991239003 Lambda1 -4.8719535\n",
      "72 Train Loss 204.30084 Test MSE 193.09860363798012 Test RE 0.23392613065238255 Lambda1 -4.851009\n",
      "73 Train Loss 202.46753 Test MSE 193.18332019423377 Test RE 0.23397743926551365 Lambda1 -4.79312\n",
      "74 Train Loss 201.84589 Test MSE 194.03572540866156 Test RE 0.23449307401623407 Lambda1 -4.7396474\n",
      "Training time: 77.50\n",
      "Training time: 77.50\n",
      "inv_HT_stan_tune16\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 838.08496 Test MSE 858.3260241996564 Test RE 0.49319119085920277 Lambda1 -0.02001885\n",
      "1 Train Loss 837.8678 Test MSE 857.8702881083354 Test RE 0.49306024127684983 Lambda1 -0.024727516\n",
      "2 Train Loss 837.67773 Test MSE 856.9468127216743 Test RE 0.4927947864599781 Lambda1 0.074108064\n",
      "3 Train Loss 832.3923 Test MSE 850.5665111041521 Test RE 0.49095683477851365 Lambda1 0.69861317\n",
      "4 Train Loss 812.9826 Test MSE 823.5155189488702 Test RE 0.4830866816977858 Lambda1 0.6386689\n",
      "5 Train Loss 781.32886 Test MSE 791.2971470121278 Test RE 0.4735425086867636 Lambda1 0.92948025\n",
      "6 Train Loss 743.1677 Test MSE 738.3853112917184 Test RE 0.4574363734761875 Lambda1 1.2242019\n",
      "7 Train Loss 719.8058 Test MSE 713.6341131462256 Test RE 0.44970422896326956 Lambda1 1.3004087\n",
      "8 Train Loss 704.6496 Test MSE 706.7973486427969 Test RE 0.4475449144214803 Lambda1 1.3851693\n",
      "9 Train Loss 688.8934 Test MSE 687.8221277236091 Test RE 0.4414964768682563 Lambda1 1.3515556\n",
      "10 Train Loss 677.05817 Test MSE 680.256840790647 Test RE 0.43906177575675354 Lambda1 1.2688557\n",
      "11 Train Loss 672.0534 Test MSE 678.1432700900881 Test RE 0.4383791585522249 Lambda1 1.1862587\n",
      "12 Train Loss 663.7952 Test MSE 664.0373689463211 Test RE 0.43379588772925215 Lambda1 1.0028872\n",
      "13 Train Loss 657.14746 Test MSE 658.6580158599567 Test RE 0.4320352289249854 Lambda1 1.0026121\n",
      "14 Train Loss 653.5596 Test MSE 657.4478288432612 Test RE 0.43163814592058797 Lambda1 0.88364327\n",
      "15 Train Loss 641.7852 Test MSE 643.7468123068934 Test RE 0.4271168612777805 Lambda1 0.62299633\n",
      "16 Train Loss 635.4213 Test MSE 639.177124320388 Test RE 0.4255981998750205 Lambda1 0.5149902\n",
      "17 Train Loss 631.68713 Test MSE 636.8702076851494 Test RE 0.42482947159347656 Lambda1 0.4003709\n",
      "18 Train Loss 626.66724 Test MSE 633.8518914565677 Test RE 0.42382157975961515 Lambda1 0.36581686\n",
      "19 Train Loss 624.51447 Test MSE 630.8843625554738 Test RE 0.42282830489622214 Lambda1 0.3872732\n",
      "20 Train Loss 622.60535 Test MSE 628.9616213899741 Test RE 0.42218348801925376 Lambda1 0.40724567\n",
      "21 Train Loss 619.252 Test MSE 625.7205760782925 Test RE 0.4210943252941755 Lambda1 0.41744435\n",
      "22 Train Loss 616.8916 Test MSE 622.8965342391651 Test RE 0.42014299585534354 Lambda1 0.46609038\n",
      "23 Train Loss 615.9887 Test MSE 622.933653962315 Test RE 0.4201555142741304 Lambda1 0.4599195\n",
      "24 Train Loss 615.35754 Test MSE 622.2001919838028 Test RE 0.4199080891607147 Lambda1 0.4798174\n",
      "25 Train Loss 613.63104 Test MSE 618.9922466700006 Test RE 0.4188242073428627 Lambda1 0.45520967\n",
      "26 Train Loss 610.4687 Test MSE 616.7215234545371 Test RE 0.4180552900767906 Lambda1 0.40521592\n",
      "27 Train Loss 608.7158 Test MSE 614.1425366816219 Test RE 0.4171802690678802 Lambda1 0.3464642\n",
      "28 Train Loss 607.5318 Test MSE 613.123562306248 Test RE 0.41683403635067245 Lambda1 0.3012148\n",
      "29 Train Loss 604.70844 Test MSE 606.2584197000874 Test RE 0.4144938223681728 Lambda1 0.27008456\n",
      "30 Train Loss 603.1659 Test MSE 602.732766324738 Test RE 0.41328683512636843 Lambda1 0.2696318\n",
      "31 Train Loss 599.1393 Test MSE 597.113350845649 Test RE 0.4113557397154169 Lambda1 0.31991166\n",
      "32 Train Loss 591.52527 Test MSE 579.1859363306186 Test RE 0.4051335173629746 Lambda1 0.39433518\n",
      "33 Train Loss 583.271 Test MSE 566.1993772870649 Test RE 0.40056579770707673 Lambda1 0.4299218\n",
      "34 Train Loss 564.1611 Test MSE 547.6105976820973 Test RE 0.3939354756821806 Lambda1 0.40363157\n",
      "35 Train Loss 525.24725 Test MSE 519.582343609461 Test RE 0.3837217026766946 Lambda1 0.41802195\n",
      "36 Train Loss 502.875 Test MSE 479.750606648751 Test RE 0.3687202052938468 Lambda1 0.3713478\n",
      "37 Train Loss 440.00793 Test MSE 406.5648614133008 Test RE 0.33943302222390076 Lambda1 0.2863846\n",
      "38 Train Loss 378.68127 Test MSE 335.7343195528988 Test RE 0.30845160876076855 Lambda1 0.2606798\n",
      "39 Train Loss 332.43176 Test MSE 300.9612675850702 Test RE 0.29204143537277344 Lambda1 0.21785077\n",
      "40 Train Loss 275.1322 Test MSE 274.13408088058617 Test RE 0.27872163840796804 Lambda1 0.2125235\n",
      "41 Train Loss 249.69678 Test MSE 265.34284056093134 Test RE 0.27421604178803405 Lambda1 0.18573596\n",
      "42 Train Loss 232.08987 Test MSE 257.081473474649 Test RE 0.2699134714493293 Lambda1 0.19245885\n",
      "43 Train Loss 225.29239 Test MSE 250.79092464226844 Test RE 0.26659075153365025 Lambda1 0.22787988\n",
      "44 Train Loss 220.95477 Test MSE 250.96577569461493 Test RE 0.2666836686748973 Lambda1 0.23882325\n",
      "45 Train Loss 218.04964 Test MSE 247.26591321659077 Test RE 0.2647105778585299 Lambda1 0.25760773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 Train Loss 216.11497 Test MSE 244.7600936449258 Test RE 0.2633658594553723 Lambda1 0.26026678\n",
      "47 Train Loss 214.35411 Test MSE 242.85267793225682 Test RE 0.26233764708793605 Lambda1 0.25800472\n",
      "48 Train Loss 213.02031 Test MSE 242.51091356666913 Test RE 0.26215298940553416 Lambda1 0.27263793\n",
      "49 Train Loss 212.55357 Test MSE 242.4625866010382 Test RE 0.26212686750951275 Lambda1 0.27251598\n",
      "50 Train Loss 211.84796 Test MSE 242.1526385252399 Test RE 0.26195927113777717 Lambda1 0.2751858\n",
      "51 Train Loss 211.26462 Test MSE 241.69440601425012 Test RE 0.26171129717992525 Lambda1 0.28948915\n",
      "52 Train Loss 210.45741 Test MSE 241.3852875008066 Test RE 0.26154388393272626 Lambda1 0.2857228\n",
      "53 Train Loss 209.90001 Test MSE 240.68131822760026 Test RE 0.26116222584647336 Lambda1 0.28355584\n",
      "54 Train Loss 209.51228 Test MSE 240.19814404714327 Test RE 0.2608999490734268 Lambda1 0.2828568\n",
      "55 Train Loss 209.1689 Test MSE 239.95369373507654 Test RE 0.2607671559808525 Lambda1 0.27431035\n",
      "56 Train Loss 208.76523 Test MSE 239.30751277596747 Test RE 0.2604158040943523 Lambda1 0.27002698\n",
      "57 Train Loss 208.5504 Test MSE 238.73193798749028 Test RE 0.26010244367142177 Lambda1 0.26180622\n",
      "58 Train Loss 208.2975 Test MSE 238.756794372047 Test RE 0.26011598404256064 Lambda1 0.26357746\n",
      "59 Train Loss 208.02766 Test MSE 238.41811914073006 Test RE 0.2599314320092074 Lambda1 0.2645144\n",
      "60 Train Loss 207.79834 Test MSE 238.18910973964375 Test RE 0.2598065651512274 Lambda1 0.270429\n",
      "61 Train Loss 207.58737 Test MSE 237.90897338554788 Test RE 0.2596537397842755 Lambda1 0.2773353\n",
      "62 Train Loss 206.8649 Test MSE 236.11063685872875 Test RE 0.2586705264376611 Lambda1 0.2848129\n",
      "63 Train Loss 206.09236 Test MSE 233.16887849096148 Test RE 0.25705405697349254 Lambda1 0.29293916\n",
      "64 Train Loss 204.70526 Test MSE 229.4855156862627 Test RE 0.25501563652982856 Lambda1 0.3019069\n",
      "65 Train Loss 202.87794 Test MSE 224.97998049560093 Test RE 0.2524998404472814 Lambda1 0.30734694\n",
      "66 Train Loss 199.46011 Test MSE 217.74403906631417 Test RE 0.2484061294808814 Lambda1 0.27715325\n",
      "67 Train Loss 191.77724 Test MSE 207.73047422115818 Test RE 0.2426270829948099 Lambda1 0.24441625\n",
      "68 Train Loss 184.22171 Test MSE 192.31221876752647 Test RE 0.23344931819285109 Lambda1 0.26682168\n",
      "69 Train Loss 175.36655 Test MSE 184.54945384067662 Test RE 0.228689146038795 Lambda1 0.3173172\n",
      "70 Train Loss 163.36801 Test MSE 167.75683498506316 Test RE 0.2180365404601124 Lambda1 0.38786018\n",
      "71 Train Loss 156.91296 Test MSE 159.3293628184691 Test RE 0.2124893070788871 Lambda1 0.4360185\n",
      "72 Train Loss 146.82954 Test MSE 143.92093350446748 Test RE 0.20195339307644086 Lambda1 0.46848637\n",
      "73 Train Loss 140.74878 Test MSE 138.42310102718446 Test RE 0.19805848732060335 Lambda1 0.50892764\n",
      "74 Train Loss 136.29355 Test MSE 135.81397815929216 Test RE 0.19618301555347556 Lambda1 0.54768264\n",
      "Training time: 76.20\n",
      "Training time: 76.20\n",
      "inv_HT_stan_tune16\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 838.08344 Test MSE 858.2467049373906 Test RE 0.4931684020446215 Lambda1 -0.14961888\n",
      "1 Train Loss 837.7977 Test MSE 857.4614716475738 Test RE 0.49294274381488734 Lambda1 -0.24959487\n",
      "2 Train Loss 835.74146 Test MSE 856.0271940913123 Test RE 0.49253029810043203 Lambda1 -1.1305369\n",
      "3 Train Loss 828.46643 Test MSE 845.6045959470379 Test RE 0.4895227027305427 Lambda1 -1.4343144\n",
      "4 Train Loss 819.1582 Test MSE 832.078482930589 Test RE 0.4855917687377278 Lambda1 -1.6006579\n",
      "5 Train Loss 811.8899 Test MSE 824.3965569837725 Test RE 0.4833450277489613 Lambda1 -1.5614547\n",
      "6 Train Loss 790.2847 Test MSE 790.1301522508858 Test RE 0.473193192674989 Lambda1 -1.2567273\n",
      "7 Train Loss 770.4003 Test MSE 752.5949932720192 Test RE 0.46181691204843844 Lambda1 -1.1431872\n",
      "8 Train Loss 742.1392 Test MSE 709.7387180461898 Test RE 0.4484751868704551 Lambda1 -1.0778451\n",
      "9 Train Loss 714.34204 Test MSE 694.6249209372461 Test RE 0.44367437972366497 Lambda1 -1.0869561\n",
      "10 Train Loss 694.18604 Test MSE 676.1123119299224 Test RE 0.43772221968952124 Lambda1 -1.1269658\n",
      "11 Train Loss 667.1545 Test MSE 651.9370519623287 Test RE 0.4298253275181094 Lambda1 -1.0939119\n",
      "12 Train Loss 649.09625 Test MSE 635.2131026018088 Test RE 0.4242764188677132 Lambda1 -1.0929304\n",
      "13 Train Loss 628.26935 Test MSE 610.0496801817142 Test RE 0.4157878290676102 Lambda1 -1.0609317\n",
      "14 Train Loss 607.173 Test MSE 600.4793815889216 Test RE 0.4125135518687973 Lambda1 -1.0064132\n",
      "15 Train Loss 590.8532 Test MSE 582.794278988679 Test RE 0.40639355379469483 Lambda1 -0.942584\n",
      "16 Train Loss 583.1938 Test MSE 577.9260508128308 Test RE 0.4046926401784224 Lambda1 -0.9430543\n",
      "17 Train Loss 573.8904 Test MSE 566.221468415057 Test RE 0.4005736119721124 Lambda1 -0.944308\n",
      "18 Train Loss 551.437 Test MSE 549.0379233990701 Test RE 0.39444853033075544 Lambda1 -1.0028162\n",
      "19 Train Loss 533.3763 Test MSE 527.7881668366828 Test RE 0.38673991293753956 Lambda1 -1.02172\n",
      "20 Train Loss 522.0896 Test MSE 521.3376280447579 Test RE 0.3843693120982984 Lambda1 -1.0306505\n",
      "21 Train Loss 508.43625 Test MSE 501.7250578157031 Test RE 0.37707007424973693 Lambda1 -1.0598236\n",
      "22 Train Loss 494.8542 Test MSE 480.60837304263356 Test RE 0.36904968331011856 Lambda1 -1.0883434\n",
      "23 Train Loss 477.26578 Test MSE 467.3963382872209 Test RE 0.36394170343665205 Lambda1 -1.1134896\n",
      "24 Train Loss 466.1853 Test MSE 455.51726204609685 Test RE 0.35928707183471736 Lambda1 -1.1326704\n",
      "25 Train Loss 450.89844 Test MSE 433.05656652984715 Test RE 0.35031721918778846 Lambda1 -1.1661128\n",
      "26 Train Loss 410.858 Test MSE 404.1867759267243 Test RE 0.33843885787509426 Lambda1 -1.2094184\n",
      "27 Train Loss 392.93518 Test MSE 388.5733423597389 Test RE 0.33183765992343345 Lambda1 -1.2401415\n",
      "28 Train Loss 376.927 Test MSE 378.5224324208966 Test RE 0.3275178555542508 Lambda1 -1.2390231\n",
      "29 Train Loss 365.7489 Test MSE 373.2792787012821 Test RE 0.32524161713658334 Lambda1 -1.2518145\n",
      "30 Train Loss 348.03098 Test MSE 362.5326201405691 Test RE 0.320525595365979 Lambda1 -1.2597964\n",
      "31 Train Loss 331.98993 Test MSE 344.4759421893196 Test RE 0.31244143152736065 Lambda1 -1.2656484\n",
      "32 Train Loss 322.9728 Test MSE 330.63668080538883 Test RE 0.30610095664505355 Lambda1 -1.2720457\n",
      "33 Train Loss 297.64557 Test MSE 309.50303261142767 Test RE 0.2961567428240884 Lambda1 -1.3003547\n",
      "34 Train Loss 285.75446 Test MSE 289.95450090215473 Test RE 0.28665142028226137 Lambda1 -1.3249563\n",
      "35 Train Loss 274.5368 Test MSE 276.31892759898136 Test RE 0.2798301388660776 Lambda1 -1.3526044\n",
      "36 Train Loss 254.1895 Test MSE 257.6609432655089 Test RE 0.27021749698550085 Lambda1 -1.4145123\n",
      "37 Train Loss 235.69562 Test MSE 240.1466291541284 Test RE 0.26087197018599456 Lambda1 -1.4629427\n",
      "38 Train Loss 227.96034 Test MSE 234.77491761413043 Test RE 0.2579378164542508 Lambda1 -1.4629214\n",
      "39 Train Loss 218.73633 Test MSE 232.75610639508176 Test RE 0.25682642852091986 Lambda1 -1.4489202\n",
      "40 Train Loss 214.38869 Test MSE 222.39740801884523 Test RE 0.25104641926566124 Lambda1 -1.4652429\n",
      "41 Train Loss 208.2336 Test MSE 219.50193663243112 Test RE 0.24940683363952829 Lambda1 -1.4737117\n",
      "42 Train Loss 202.34152 Test MSE 213.5744662523296 Test RE 0.246016273319464 Lambda1 -1.4842516\n",
      "43 Train Loss 199.29144 Test MSE 209.72598790160163 Test RE 0.24378966749521913 Lambda1 -1.4902661\n",
      "44 Train Loss 196.28868 Test MSE 207.18577789068755 Test RE 0.2423087743066736 Lambda1 -1.5057725\n",
      "45 Train Loss 192.87039 Test MSE 200.82357503676485 Test RE 0.23855939112773533 Lambda1 -1.5403624\n",
      "46 Train Loss 189.07857 Test MSE 197.81261678840312 Test RE 0.23676427048674842 Lambda1 -1.5460984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 Train Loss 181.2094 Test MSE 185.9914719647755 Test RE 0.22958086422240773 Lambda1 -1.5967766\n",
      "48 Train Loss 177.96071 Test MSE 181.92571696506852 Test RE 0.22705769157456065 Lambda1 -1.5940883\n",
      "49 Train Loss 175.71576 Test MSE 183.43318536647865 Test RE 0.2279964707939078 Lambda1 -1.5690498\n",
      "50 Train Loss 173.82883 Test MSE 182.66496925031353 Test RE 0.22751854655653864 Lambda1 -1.5746754\n",
      "51 Train Loss 169.95102 Test MSE 176.02054602802914 Test RE 0.22334223229007885 Lambda1 -1.5720125\n",
      "52 Train Loss 166.72571 Test MSE 168.4418989675254 Test RE 0.2184812817960342 Lambda1 -1.5771738\n",
      "53 Train Loss 163.87218 Test MSE 170.9291704197892 Test RE 0.22008845567636542 Lambda1 -1.5667406\n",
      "54 Train Loss 159.32275 Test MSE 165.26671903756244 Test RE 0.21641226642454975 Lambda1 -1.592689\n",
      "55 Train Loss 158.60135 Test MSE 164.0071254904557 Test RE 0.21558598730689926 Lambda1 -1.6000687\n",
      "56 Train Loss 156.68594 Test MSE 163.87828594667653 Test RE 0.21550129142149704 Lambda1 -1.5914661\n",
      "57 Train Loss 155.20956 Test MSE 161.517990145079 Test RE 0.21394375871982851 Lambda1 -1.5929636\n",
      "58 Train Loss 153.3012 Test MSE 160.96819463250554 Test RE 0.21357932381657876 Lambda1 -1.5828859\n",
      "59 Train Loss 151.75111 Test MSE 160.4229604787834 Test RE 0.21321729726693542 Lambda1 -1.5835239\n",
      "60 Train Loss 150.7134 Test MSE 157.89356006357636 Test RE 0.2115297124995132 Lambda1 -1.5921102\n",
      "61 Train Loss 149.63821 Test MSE 157.22573366134284 Test RE 0.21108189557385584 Lambda1 -1.6058935\n",
      "62 Train Loss 148.41364 Test MSE 156.6621233136098 Test RE 0.210703221084549 Lambda1 -1.6134453\n",
      "63 Train Loss 147.10501 Test MSE 153.000571134081 Test RE 0.2082263548850158 Lambda1 -1.6261708\n",
      "64 Train Loss 144.97537 Test MSE 149.67816553972796 Test RE 0.20595312997169657 Lambda1 -1.6380944\n",
      "65 Train Loss 143.64064 Test MSE 147.7788390017379 Test RE 0.20464224699875197 Lambda1 -1.6442413\n",
      "66 Train Loss 142.10744 Test MSE 146.7320042384481 Test RE 0.20391613711170672 Lambda1 -1.6414618\n",
      "67 Train Loss 140.86838 Test MSE 146.6773844244983 Test RE 0.20387818050429818 Lambda1 -1.6405996\n",
      "68 Train Loss 139.07727 Test MSE 145.9677987886879 Test RE 0.20338442878908447 Lambda1 -1.6393102\n",
      "69 Train Loss 136.60753 Test MSE 142.3027617505786 Test RE 0.20081485454800935 Lambda1 -1.6486435\n",
      "70 Train Loss 135.97388 Test MSE 140.99964742872314 Test RE 0.19989327526048198 Lambda1 -1.6554261\n",
      "71 Train Loss 134.96802 Test MSE 141.74980549660773 Test RE 0.20042431430382457 Lambda1 -1.6645918\n",
      "72 Train Loss 133.96487 Test MSE 139.99954075184743 Test RE 0.19918309490566644 Lambda1 -1.6784405\n",
      "73 Train Loss 132.74664 Test MSE 137.3965963647055 Test RE 0.19732274927862237 Lambda1 -1.6916935\n",
      "74 Train Loss 131.74573 Test MSE 135.02854208083843 Test RE 0.19561491259284267 Lambda1 -1.6991724\n",
      "Training time: 75.43\n",
      "Training time: 75.43\n",
      "inv_HT_stan_tune16\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 838.06714 Test MSE 858.2914400587281 Test RE 0.4931812547947092 Lambda1 -0.033572033\n",
      "1 Train Loss 837.93964 Test MSE 858.1150871519037 Test RE 0.49313058528322273 Lambda1 -0.033997025\n",
      "2 Train Loss 837.67456 Test MSE 857.4188994132403 Test RE 0.4929305065685601 Lambda1 -0.2070159\n",
      "3 Train Loss 835.2494 Test MSE 853.4470222103748 Test RE 0.49178746429964687 Lambda1 -0.3001293\n",
      "4 Train Loss 832.4993 Test MSE 845.2513462016021 Test RE 0.48942044345098557 Lambda1 -0.15061107\n",
      "5 Train Loss 792.46234 Test MSE 800.5581602704948 Test RE 0.47630552037005597 Lambda1 0.10650531\n",
      "6 Train Loss 738.62415 Test MSE 728.7257528292961 Test RE 0.4544344314190912 Lambda1 0.012325874\n",
      "7 Train Loss 682.0394 Test MSE 675.752475705405 Test RE 0.437605723309601 Lambda1 0.002485476\n",
      "8 Train Loss 663.23114 Test MSE 666.272311776759 Test RE 0.43452528539427027 Lambda1 0.0020835686\n",
      "9 Train Loss 654.03546 Test MSE 660.5556272824294 Test RE 0.43265713386125354 Lambda1 0.00032005127\n",
      "10 Train Loss 649.3435 Test MSE 655.4431070022043 Test RE 0.43097955749130373 Lambda1 0.0021684866\n",
      "11 Train Loss 645.58466 Test MSE 652.2701528047851 Test RE 0.42993512101309245 Lambda1 -0.0007392656\n",
      "12 Train Loss 640.8606 Test MSE 648.712384465905 Test RE 0.42876099013077756 Lambda1 0.00069467304\n",
      "13 Train Loss 638.505 Test MSE 646.3528512977028 Test RE 0.4279805230011787 Lambda1 5.1432922e-05\n",
      "14 Train Loss 632.0371 Test MSE 638.6199290404169 Test RE 0.42541265427040825 Lambda1 -0.0023895407\n",
      "15 Train Loss 594.60565 Test MSE 581.3016448208514 Test RE 0.4058727990247775 Lambda1 0.0034432951\n",
      "16 Train Loss 457.5042 Test MSE 409.1202647260937 Test RE 0.3404980793132753 Lambda1 0.0038373743\n",
      "17 Train Loss 301.25623 Test MSE 294.3521387361553 Test RE 0.2888170105825621 Lambda1 -5.9713824e-05\n",
      "18 Train Loss 269.6973 Test MSE 285.7284450785256 Test RE 0.2845547958334752 Lambda1 0.00073999143\n",
      "19 Train Loss 260.0238 Test MSE 283.43422625854083 Test RE 0.28341009581880705 Lambda1 0.00036110895\n",
      "20 Train Loss 257.79388 Test MSE 284.44511018387124 Test RE 0.2839150449186659 Lambda1 0.00057491014\n",
      "21 Train Loss 256.41382 Test MSE 283.2378937275026 Test RE 0.2833119209065839 Lambda1 0.0006432888\n",
      "22 Train Loss 255.0939 Test MSE 284.07425664278065 Test RE 0.28372990332647346 Lambda1 0.0004165009\n",
      "23 Train Loss 254.57112 Test MSE 283.4633116745283 Test RE 0.2834246369156591 Lambda1 0.00027801355\n",
      "24 Train Loss 254.37311 Test MSE 283.6119701614434 Test RE 0.28349894627653355 Lambda1 0.00019379218\n",
      "25 Train Loss 254.24171 Test MSE 283.2339243591365 Test RE 0.28330993569672763 Lambda1 0.00028526975\n",
      "26 Train Loss 254.17181 Test MSE 282.7345853895774 Test RE 0.2830600890403276 Lambda1 0.00037144177\n",
      "27 Train Loss 254.05098 Test MSE 282.6485747176473 Test RE 0.2830170309187129 Lambda1 0.00042545388\n",
      "28 Train Loss 254.006 Test MSE 282.631729348801 Test RE 0.2830085971289501 Lambda1 0.00052875414\n",
      "29 Train Loss 253.99419 Test MSE 282.63329332193194 Test RE 0.2830093801571516 Lambda1 0.0005398518\n",
      "30 Train Loss 253.8909 Test MSE 282.5233752821343 Test RE 0.2829543426542323 Lambda1 0.000646014\n",
      "31 Train Loss 253.87183 Test MSE 282.3672936607751 Test RE 0.2828761720004378 Lambda1 0.00079026073\n",
      "32 Train Loss 253.86179 Test MSE 282.24824973807114 Test RE 0.2828165364833159 Lambda1 0.00089351693\n",
      "33 Train Loss 253.84431 Test MSE 282.154723417935 Test RE 0.2827696752866305 Lambda1 0.0009597112\n",
      "34 Train Loss 253.83525 Test MSE 282.16973114399195 Test RE 0.28277719540421586 Lambda1 0.00094144035\n",
      "35 Train Loss 253.78003 Test MSE 282.50323520642587 Test RE 0.28294425707557647 Lambda1 0.0009098041\n",
      "36 Train Loss 253.75945 Test MSE 282.5892213175777 Test RE 0.2829873139724544 Lambda1 0.00091845775\n",
      "37 Train Loss 253.73338 Test MSE 282.6433594049978 Test RE 0.2830144198510441 Lambda1 0.0010026271\n",
      "38 Train Loss 253.64352 Test MSE 282.537881818108 Test RE 0.28296160689326694 Lambda1 0.001187971\n",
      "39 Train Loss 253.6251 Test MSE 282.2741909519881 Test RE 0.2828295329070516 Lambda1 0.0012647671\n",
      "40 Train Loss 253.55408 Test MSE 282.36893411590023 Test RE 0.2828769937050094 Lambda1 0.0015436045\n",
      "41 Train Loss 253.48555 Test MSE 282.17863768468925 Test RE 0.28278165822649476 Lambda1 0.0019017906\n",
      "42 Train Loss 253.36464 Test MSE 281.2979514002342 Test RE 0.2823400292105256 Lambda1 0.002697142\n",
      "43 Train Loss 253.13354 Test MSE 280.2599460258929 Test RE 0.2818186224105703 Lambda1 0.003908272\n",
      "44 Train Loss 253.01495 Test MSE 280.5033496746872 Test RE 0.2819409745246015 Lambda1 0.0039860434\n",
      "45 Train Loss 252.84952 Test MSE 279.79665697741 Test RE 0.28158559324330845 Lambda1 0.004938784\n",
      "46 Train Loss 252.706 Test MSE 280.16946683144243 Test RE 0.28177312753940836 Lambda1 0.0047330554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 Train Loss 252.48271 Test MSE 279.4249471262279 Test RE 0.2813984878537981 Lambda1 0.005035664\n",
      "48 Train Loss 252.27528 Test MSE 279.80587115549883 Test RE 0.28159022975053266 Lambda1 0.005313857\n",
      "49 Train Loss 251.9834 Test MSE 279.2400660270578 Test RE 0.2813053790051938 Lambda1 0.0063542817\n",
      "50 Train Loss 251.69252 Test MSE 278.7088908770962 Test RE 0.28103769973707665 Lambda1 0.0077019515\n",
      "51 Train Loss 251.26826 Test MSE 278.9674633667467 Test RE 0.28116803603652835 Lambda1 0.009002768\n",
      "52 Train Loss 250.77293 Test MSE 277.31670944286117 Test RE 0.2803349139706356 Lambda1 0.013013914\n",
      "53 Train Loss 250.09496 Test MSE 275.2432563578299 Test RE 0.2792849378009746 Lambda1 0.017286496\n",
      "54 Train Loss 249.30748 Test MSE 274.1288059286304 Test RE 0.27871895678166375 Lambda1 0.018165011\n",
      "55 Train Loss 248.49097 Test MSE 272.75439277400085 Test RE 0.27801936527512067 Lambda1 0.01981712\n",
      "56 Train Loss 247.86249 Test MSE 270.59880738325853 Test RE 0.2769185887885464 Lambda1 0.02276453\n",
      "57 Train Loss 246.94522 Test MSE 267.6627972643871 Test RE 0.27541220152474966 Lambda1 0.030472415\n",
      "58 Train Loss 245.37035 Test MSE 267.2193642442015 Test RE 0.2751839712870139 Lambda1 0.031402543\n",
      "59 Train Loss 243.9304 Test MSE 264.3362219622607 Test RE 0.27369540732075875 Lambda1 0.03757584\n",
      "60 Train Loss 242.55692 Test MSE 261.7889710836708 Test RE 0.2723734951017649 Lambda1 0.043850143\n",
      "61 Train Loss 240.6645 Test MSE 258.6368355669236 Test RE 0.27072873852078655 Lambda1 0.054676104\n",
      "62 Train Loss 237.61697 Test MSE 257.3638541897767 Test RE 0.2700616685165981 Lambda1 0.06141878\n",
      "63 Train Loss 234.3912 Test MSE 255.78414844451933 Test RE 0.2692315701419301 Lambda1 0.06897406\n",
      "64 Train Loss 232.0299 Test MSE 252.94107629748834 Test RE 0.2677311181208108 Lambda1 0.08112035\n",
      "65 Train Loss 228.04468 Test MSE 249.4226477729186 Test RE 0.26586251773978764 Lambda1 0.100405246\n",
      "66 Train Loss 225.10016 Test MSE 246.83084483680832 Test RE 0.264477594053811 Lambda1 0.11880884\n",
      "67 Train Loss 223.47438 Test MSE 245.72695044772644 Test RE 0.2638855236216988 Lambda1 0.13225384\n",
      "68 Train Loss 221.67128 Test MSE 243.10603156546884 Test RE 0.26247445198358826 Lambda1 0.14867418\n",
      "69 Train Loss 220.18062 Test MSE 241.6649445353521 Test RE 0.26169534596912425 Lambda1 0.15505624\n",
      "70 Train Loss 218.09819 Test MSE 239.045899683352 Test RE 0.2602734207381998 Lambda1 0.16338314\n",
      "71 Train Loss 215.80667 Test MSE 236.13095273232906 Test RE 0.258681654705221 Lambda1 0.16846558\n",
      "72 Train Loss 213.16263 Test MSE 233.82425211815965 Test RE 0.2574150576324265 Lambda1 0.16913266\n",
      "73 Train Loss 212.01535 Test MSE 233.36300045538997 Test RE 0.2571610383538323 Lambda1 0.16545366\n",
      "74 Train Loss 210.43745 Test MSE 232.343352392363 Test RE 0.2565986081303612 Lambda1 0.16139127\n",
      "Training time: 75.68\n",
      "Training time: 75.68\n",
      "inv_HT_stan_tune16\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 838.034 Test MSE 858.2256954947254 Test RE 0.49316236575189965 Lambda1 0.0259584\n",
      "1 Train Loss 837.8713 Test MSE 858.0132139900081 Test RE 0.4931013128326008 Lambda1 0.007563606\n",
      "2 Train Loss 837.4397 Test MSE 857.0645869991787 Test RE 0.4928286488625684 Lambda1 -0.47757167\n",
      "3 Train Loss 830.5665 Test MSE 844.8712087183677 Test RE 0.4893103767950514 Lambda1 -0.3942527\n",
      "4 Train Loss 812.7359 Test MSE 827.8808971766526 Test RE 0.4843653878696027 Lambda1 -0.37098134\n",
      "5 Train Loss 789.3056 Test MSE 797.1659206658743 Test RE 0.4752953141288775 Lambda1 -0.51800036\n",
      "6 Train Loss 764.3006 Test MSE 764.3602687723227 Test RE 0.4654126923857251 Lambda1 -0.524909\n",
      "7 Train Loss 731.624 Test MSE 732.2704618959731 Test RE 0.45553833347735184 Lambda1 -0.4592821\n",
      "8 Train Loss 716.54095 Test MSE 717.4320025539412 Test RE 0.45089928156015 Lambda1 -0.4839345\n",
      "9 Train Loss 689.76855 Test MSE 688.2058886203679 Test RE 0.44161962313746184 Lambda1 -0.5988782\n",
      "10 Train Loss 665.0295 Test MSE 662.6611776404271 Test RE 0.4333461421383785 Lambda1 -0.826166\n",
      "11 Train Loss 651.49146 Test MSE 651.6540684804994 Test RE 0.4297320311872087 Lambda1 -0.9225788\n",
      "12 Train Loss 637.2249 Test MSE 638.4192911305425 Test RE 0.4253458221790834 Lambda1 -1.096911\n",
      "13 Train Loss 629.69324 Test MSE 631.611521980828 Test RE 0.4230719113799036 Lambda1 -1.2224213\n",
      "14 Train Loss 627.10626 Test MSE 632.0112126308328 Test RE 0.42320575246583164 Lambda1 -1.1626835\n",
      "15 Train Loss 623.9781 Test MSE 628.7206804399871 Test RE 0.42210261580792996 Lambda1 -1.2331706\n",
      "16 Train Loss 622.4052 Test MSE 628.1638277811558 Test RE 0.4219156480180088 Lambda1 -1.2912586\n",
      "17 Train Loss 621.4763 Test MSE 627.3089115596779 Test RE 0.42162844161887153 Lambda1 -1.3223971\n",
      "18 Train Loss 619.4638 Test MSE 626.0990100129247 Test RE 0.42122164434135184 Lambda1 -1.4051502\n",
      "19 Train Loss 618.94763 Test MSE 625.1709898021501 Test RE 0.4209093557482468 Lambda1 -1.460497\n",
      "20 Train Loss 618.34064 Test MSE 624.7169064756935 Test RE 0.42075646746501816 Lambda1 -1.4438484\n",
      "21 Train Loss 617.73175 Test MSE 624.9708083435245 Test RE 0.42084196219019004 Lambda1 -1.5098165\n",
      "22 Train Loss 617.581 Test MSE 624.590196577213 Test RE 0.4207137947660815 Lambda1 -1.5508355\n",
      "23 Train Loss 617.1834 Test MSE 624.2928066627178 Test RE 0.4206136243369351 Lambda1 -1.5047683\n",
      "24 Train Loss 617.0884 Test MSE 624.0992119153522 Test RE 0.42054840261625076 Lambda1 -1.466753\n",
      "25 Train Loss 616.89734 Test MSE 624.225461405744 Test RE 0.42059093698872413 Lambda1 -1.4692389\n",
      "26 Train Loss 616.395 Test MSE 623.9450235120535 Test RE 0.42049644958458837 Lambda1 -1.565102\n",
      "27 Train Loss 615.8949 Test MSE 622.6945991273083 Test RE 0.4200748878346228 Lambda1 -1.5578941\n",
      "28 Train Loss 615.4933 Test MSE 622.200268935277 Test RE 0.41990811512707227 Lambda1 -1.4970611\n",
      "29 Train Loss 615.0186 Test MSE 621.7075404379513 Test RE 0.4197418168442047 Lambda1 -1.4977571\n",
      "30 Train Loss 613.966 Test MSE 619.2544120040446 Test RE 0.41891289146216637 Lambda1 -1.4897497\n",
      "31 Train Loss 610.5064 Test MSE 614.980173809592 Test RE 0.41746467096919343 Lambda1 -1.5616322\n",
      "32 Train Loss 593.31903 Test MSE 588.2922387072788 Test RE 0.4083059700759309 Lambda1 -1.4258236\n",
      "33 Train Loss 578.50287 Test MSE 571.034233207369 Test RE 0.4022724059826667 Lambda1 -1.4879898\n",
      "34 Train Loss 563.5911 Test MSE 559.72033743982 Test RE 0.3982673590912962 Lambda1 -1.4997069\n",
      "35 Train Loss 548.93115 Test MSE 545.2747824071205 Test RE 0.39309441833928505 Lambda1 -1.5320246\n",
      "36 Train Loss 537.36633 Test MSE 526.9279904203931 Test RE 0.386424634738835 Lambda1 -1.4896818\n",
      "37 Train Loss 521.00116 Test MSE 510.0649911294555 Test RE 0.380191084804931 Lambda1 -1.4762737\n",
      "38 Train Loss 501.83542 Test MSE 493.82935951795105 Test RE 0.37409131344369795 Lambda1 -1.4339076\n",
      "39 Train Loss 477.59628 Test MSE 479.37352737348954 Test RE 0.3685752715751796 Lambda1 -1.4936562\n",
      "40 Train Loss 447.1872 Test MSE 446.6411127789872 Test RE 0.355769340781948 Lambda1 -1.6702467\n",
      "41 Train Loss 420.48813 Test MSE 429.0806252964487 Test RE 0.3487053600198597 Lambda1 -1.8387249\n",
      "42 Train Loss 387.22134 Test MSE 387.86110089667113 Test RE 0.33153339697141004 Lambda1 -1.8889765\n",
      "43 Train Loss 368.48645 Test MSE 375.3907555029337 Test RE 0.3261601944041749 Lambda1 -1.7709444\n",
      "44 Train Loss 343.84277 Test MSE 355.30117350599227 Test RE 0.317312727107238 Lambda1 -1.8206526\n",
      "45 Train Loss 327.96152 Test MSE 337.48846284176716 Test RE 0.30925635731547874 Lambda1 -1.7905183\n",
      "46 Train Loss 312.513 Test MSE 326.55550853274167 Test RE 0.30420593141204577 Lambda1 -1.8451089\n",
      "47 Train Loss 304.68094 Test MSE 320.74087655461204 Test RE 0.3014854285000829 Lambda1 -1.8913246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 Train Loss 299.07196 Test MSE 318.5794280919128 Test RE 0.30046786755694704 Lambda1 -1.8439339\n",
      "49 Train Loss 293.0139 Test MSE 313.62979060096416 Test RE 0.29812460754758524 Lambda1 -1.8060297\n",
      "50 Train Loss 286.4522 Test MSE 310.0793451013936 Test RE 0.2964323450514235 Lambda1 -1.7579325\n",
      "51 Train Loss 280.89487 Test MSE 306.9072549455361 Test RE 0.29491220618191183 Lambda1 -1.7683027\n",
      "52 Train Loss 276.29425 Test MSE 304.006297737802 Test RE 0.29351510828283733 Lambda1 -1.7255847\n",
      "53 Train Loss 273.3251 Test MSE 302.497281920657 Test RE 0.29278573203465996 Lambda1 -1.7253687\n",
      "54 Train Loss 271.27277 Test MSE 299.94221745508855 Test RE 0.29154659227581753 Lambda1 -1.7271053\n",
      "55 Train Loss 268.00436 Test MSE 296.6540946480305 Test RE 0.289944145664167 Lambda1 -1.7731483\n",
      "56 Train Loss 265.4263 Test MSE 293.10199423606235 Test RE 0.28820303987511536 Lambda1 -1.7824625\n",
      "57 Train Loss 262.78848 Test MSE 292.52352623667906 Test RE 0.28791849971758127 Lambda1 -1.8313227\n",
      "58 Train Loss 261.41553 Test MSE 291.8611293647087 Test RE 0.2875923303936227 Lambda1 -1.8375895\n",
      "59 Train Loss 259.7843 Test MSE 289.47311167918735 Test RE 0.2864133687507882 Lambda1 -1.8577719\n",
      "60 Train Loss 253.90729 Test MSE 279.8238855673347 Test RE 0.28159929425068453 Lambda1 -1.8274944\n",
      "61 Train Loss 251.41716 Test MSE 278.3249607285978 Test RE 0.2808440639517741 Lambda1 -1.8241829\n",
      "62 Train Loss 247.58237 Test MSE 273.48569999941395 Test RE 0.2783918275983434 Lambda1 -1.8448783\n",
      "63 Train Loss 244.59505 Test MSE 267.478695306186 Test RE 0.2753174691827202 Lambda1 -1.8243027\n",
      "64 Train Loss 243.47258 Test MSE 267.9307630027494 Test RE 0.27555002899421754 Lambda1 -1.8082334\n",
      "65 Train Loss 241.69463 Test MSE 266.8381269557646 Test RE 0.27498760109409787 Lambda1 -1.777076\n",
      "66 Train Loss 240.71994 Test MSE 264.08892466352705 Test RE 0.2735673507643606 Lambda1 -1.7486383\n",
      "67 Train Loss 236.8611 Test MSE 259.03986567077305 Test RE 0.27093959278211927 Lambda1 -1.7623849\n",
      "68 Train Loss 232.50905 Test MSE 252.67904465183332 Test RE 0.26759240557014585 Lambda1 -1.8179178\n",
      "69 Train Loss 230.892 Test MSE 250.68674444853397 Test RE 0.26653537400931904 Lambda1 -1.8364464\n",
      "70 Train Loss 227.05148 Test MSE 251.17625774644148 Test RE 0.26679547746876564 Lambda1 -1.8480363\n",
      "71 Train Loss 224.11473 Test MSE 244.99793975555022 Test RE 0.26349379153374086 Lambda1 -1.8599322\n",
      "72 Train Loss 222.08923 Test MSE 240.9131830718971 Test RE 0.2612879933182909 Lambda1 -1.8571223\n",
      "73 Train Loss 217.303 Test MSE 228.58366690912294 Test RE 0.254514053809461 Lambda1 -1.8794357\n",
      "74 Train Loss 211.54594 Test MSE 222.8154453181691 Test RE 0.2512822527171434 Lambda1 -1.8886746\n",
      "Training time: 75.42\n",
      "Training time: 75.42\n",
      "inv_HT_stan_tune17\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.11334 Test MSE 858.2282544149989 Test RE 0.4931631009677303 Lambda1 -0.024353944\n",
      "1 Train Loss 837.69257 Test MSE 857.7151463976143 Test RE 0.49301565547577025 Lambda1 -0.094677426\n",
      "2 Train Loss 833.25653 Test MSE 851.8467500561427 Test RE 0.49132618022011865 Lambda1 -0.5908933\n",
      "3 Train Loss 820.5462 Test MSE 832.8190068944907 Test RE 0.4858078014665396 Lambda1 -0.6021315\n",
      "4 Train Loss 794.08466 Test MSE 794.4485663274519 Test RE 0.47448453667618623 Lambda1 -0.609691\n",
      "5 Train Loss 760.3137 Test MSE 764.351189535663 Test RE 0.46540992824118527 Lambda1 -0.4998614\n",
      "6 Train Loss 731.1747 Test MSE 730.5044349208576 Test RE 0.45498868777637363 Lambda1 -0.546292\n",
      "7 Train Loss 711.60077 Test MSE 710.8920157975413 Test RE 0.4488394163144489 Lambda1 -0.5001968\n",
      "8 Train Loss 692.24506 Test MSE 688.2126505559892 Test RE 0.4416217926888817 Lambda1 -0.5107644\n",
      "9 Train Loss 664.9026 Test MSE 659.8439804107294 Test RE 0.4324240103692521 Lambda1 -0.53605455\n",
      "10 Train Loss 649.4961 Test MSE 636.5477233888176 Test RE 0.42472190008297717 Lambda1 -0.60131145\n",
      "11 Train Loss 618.8191 Test MSE 594.614153715943 Test RE 0.4104939794934992 Lambda1 -0.76206625\n",
      "12 Train Loss 554.5043 Test MSE 533.1600671092054 Test RE 0.3887030761579081 Lambda1 -0.8728805\n",
      "13 Train Loss 506.12408 Test MSE 504.4762226286813 Test RE 0.37810247605218983 Lambda1 -0.8529972\n",
      "14 Train Loss 459.90338 Test MSE 445.52078048965615 Test RE 0.3553228635913581 Lambda1 -0.8944806\n",
      "15 Train Loss 398.7865 Test MSE 392.3854538434885 Test RE 0.3334614392163609 Lambda1 -0.9049521\n",
      "16 Train Loss 338.56976 Test MSE 335.6481197613767 Test RE 0.308412008741218 Lambda1 -0.87059045\n",
      "17 Train Loss 311.73557 Test MSE 321.73173425047486 Test RE 0.30195075552912526 Lambda1 -0.90036356\n",
      "18 Train Loss 299.8811 Test MSE 310.685134262008 Test RE 0.29672176755342305 Lambda1 -0.9288037\n",
      "19 Train Loss 288.883 Test MSE 307.41253473623686 Test RE 0.2951548721559492 Lambda1 -0.91985446\n",
      "20 Train Loss 284.94385 Test MSE 306.5886151029892 Test RE 0.29475907330401124 Lambda1 -0.91640943\n",
      "21 Train Loss 282.13617 Test MSE 303.8926868190958 Test RE 0.29346025804217024 Lambda1 -0.9031965\n",
      "22 Train Loss 277.71512 Test MSE 302.9513703280362 Test RE 0.29300540466977265 Lambda1 -0.891832\n",
      "23 Train Loss 274.5919 Test MSE 301.7841307242089 Test RE 0.29244040049442543 Lambda1 -0.87842965\n",
      "24 Train Loss 272.6123 Test MSE 301.22144596657 Test RE 0.292167641734906 Lambda1 -0.85924155\n",
      "25 Train Loss 269.78964 Test MSE 299.99459649418264 Test RE 0.2915720476182665 Lambda1 -0.8254953\n",
      "26 Train Loss 265.4108 Test MSE 295.3728549889122 Test RE 0.28931733834080936 Lambda1 -0.75822586\n",
      "27 Train Loss 263.32568 Test MSE 293.68952378405623 Test RE 0.2884917500078838 Lambda1 -0.74136174\n",
      "28 Train Loss 260.60764 Test MSE 291.2293809841349 Test RE 0.2872809076065293 Lambda1 -0.74049187\n",
      "29 Train Loss 259.25024 Test MSE 288.70601099108796 Test RE 0.28603362080105044 Lambda1 -0.710377\n",
      "30 Train Loss 257.19797 Test MSE 286.8491578937398 Test RE 0.28511230439457597 Lambda1 -0.6689105\n",
      "31 Train Loss 255.43741 Test MSE 285.22162287286784 Test RE 0.2843023136218974 Lambda1 -0.62795234\n",
      "32 Train Loss 253.96504 Test MSE 283.05945457242564 Test RE 0.283222663950449 Lambda1 -0.5983531\n",
      "33 Train Loss 252.70909 Test MSE 282.2650590992025 Test RE 0.2828249579607233 Lambda1 -0.5809925\n",
      "34 Train Loss 249.59966 Test MSE 281.53598777792126 Test RE 0.28245946304518754 Lambda1 -0.57857066\n",
      "35 Train Loss 248.73737 Test MSE 280.4140769582709 Test RE 0.28189610582761687 Lambda1 -0.56323314\n",
      "36 Train Loss 247.28781 Test MSE 277.749160056717 Test RE 0.2805534074405413 Lambda1 -0.5283135\n",
      "37 Train Loss 245.80408 Test MSE 275.7635325806559 Test RE 0.2795487711710291 Lambda1 -0.49448714\n",
      "38 Train Loss 243.74385 Test MSE 271.5617027899133 Test RE 0.2774108430883554 Lambda1 -0.4759576\n",
      "39 Train Loss 241.78627 Test MSE 268.61917443535134 Test RE 0.27590379594803377 Lambda1 -0.46197888\n",
      "40 Train Loss 237.60939 Test MSE 266.48324697718937 Test RE 0.2748046810908347 Lambda1 -0.4824744\n",
      "41 Train Loss 234.10568 Test MSE 261.8110758838806 Test RE 0.27238499412438916 Lambda1 -0.4983223\n",
      "42 Train Loss 231.13423 Test MSE 258.3072715482162 Test RE 0.27055619756860827 Lambda1 -0.5064143\n",
      "43 Train Loss 228.65027 Test MSE 256.21995767309437 Test RE 0.2694608331111316 Lambda1 -0.5160263\n",
      "44 Train Loss 225.07294 Test MSE 251.39905434794215 Test RE 0.2669137767674119 Lambda1 -0.5486037\n",
      "45 Train Loss 222.87148 Test MSE 248.6688432447649 Test RE 0.2654604692105256 Lambda1 -0.57787204\n",
      "46 Train Loss 217.38773 Test MSE 244.03468720370918 Test RE 0.2629752952665711 Lambda1 -0.6279828\n",
      "47 Train Loss 211.706 Test MSE 235.4409839552408 Test RE 0.25830344754113754 Lambda1 -0.6835219\n",
      "48 Train Loss 207.54132 Test MSE 229.0466763578012 Test RE 0.2547716898854143 Lambda1 -0.71594715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 Train Loss 202.90913 Test MSE 221.1876238936049 Test RE 0.2503626744972048 Lambda1 -0.7819367\n",
      "50 Train Loss 197.94331 Test MSE 214.09039775148295 Test RE 0.24631324463985724 Lambda1 -0.83887535\n",
      "51 Train Loss 196.04758 Test MSE 213.30830150377048 Test RE 0.24586292803761636 Lambda1 -0.86465764\n",
      "52 Train Loss 193.21751 Test MSE 211.1888476319899 Test RE 0.2446384186863237 Lambda1 -0.8802112\n",
      "53 Train Loss 186.91728 Test MSE 203.38021317000133 Test RE 0.24007311066839676 Lambda1 -0.89199746\n",
      "54 Train Loss 184.98306 Test MSE 200.67698311011725 Test RE 0.23847230656888624 Lambda1 -0.90491503\n",
      "55 Train Loss 183.53882 Test MSE 199.86711761834684 Test RE 0.237990622681669 Lambda1 -0.8913206\n",
      "56 Train Loss 180.6708 Test MSE 196.26216892010035 Test RE 0.2358345704302235 Lambda1 -0.90084463\n",
      "57 Train Loss 178.7305 Test MSE 193.89808154682882 Test RE 0.23440988763837653 Lambda1 -0.9248738\n",
      "58 Train Loss 176.96869 Test MSE 190.91457308959406 Test RE 0.2325994647288273 Lambda1 -0.9364524\n",
      "59 Train Loss 174.86478 Test MSE 189.3165934761787 Test RE 0.23162397542881946 Lambda1 -0.92270494\n",
      "60 Train Loss 173.23358 Test MSE 187.71230482251715 Test RE 0.23064048452361952 Lambda1 -0.9269539\n",
      "61 Train Loss 172.29634 Test MSE 187.09217413959456 Test RE 0.2302591947518596 Lambda1 -0.9414916\n",
      "62 Train Loss 171.25305 Test MSE 184.85933913610847 Test RE 0.22888106662673668 Lambda1 -0.96739906\n",
      "63 Train Loss 168.8931 Test MSE 181.50976715510552 Test RE 0.22679797389718498 Lambda1 -0.99320817\n",
      "64 Train Loss 167.13766 Test MSE 179.16190006014807 Test RE 0.2253263597002324 Lambda1 -1.0121304\n",
      "65 Train Loss 165.12914 Test MSE 177.64639909929286 Test RE 0.22437133656022168 Lambda1 -1.0379488\n",
      "66 Train Loss 163.45418 Test MSE 177.08641002482406 Test RE 0.22401741812680498 Lambda1 -1.0590047\n",
      "67 Train Loss 158.44006 Test MSE 174.82277506609313 Test RE 0.22258104420975214 Lambda1 -1.0552579\n",
      "68 Train Loss 156.22397 Test MSE 172.44143768907662 Test RE 0.22105990962888863 Lambda1 -1.077846\n",
      "69 Train Loss 154.34148 Test MSE 168.76300268932394 Test RE 0.2186894299779632 Lambda1 -1.1217464\n",
      "70 Train Loss 152.43283 Test MSE 167.1313522735207 Test RE 0.2176296853659695 Lambda1 -1.1334901\n",
      "71 Train Loss 151.62811 Test MSE 165.07045924474184 Test RE 0.21628372982671062 Lambda1 -1.148687\n",
      "72 Train Loss 150.043 Test MSE 163.4928089922044 Test RE 0.2152476895222493 Lambda1 -1.1772027\n",
      "73 Train Loss 149.04167 Test MSE 160.97879396610392 Test RE 0.21358635552008684 Lambda1 -1.2131568\n",
      "74 Train Loss 148.21089 Test MSE 159.83403849121964 Test RE 0.2128255708946369 Lambda1 -1.2376205\n",
      "Training time: 75.23\n",
      "Training time: 75.23\n",
      "inv_HT_stan_tune17\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 837.8667 Test MSE 858.0859478072962 Test RE 0.4931222124995098 Lambda1 -0.052557386\n",
      "1 Train Loss 836.40027 Test MSE 855.7709755678322 Test RE 0.49245658266770054 Lambda1 -0.026873717\n",
      "2 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "3 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "4 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "5 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "6 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "7 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "8 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "9 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "10 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "11 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "12 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "13 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "14 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "15 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "16 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "17 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "18 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "19 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "20 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "21 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "22 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "23 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "24 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "25 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "26 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "27 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "28 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "29 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "30 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "31 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "32 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "33 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "34 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "35 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "36 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "37 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "38 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "39 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "40 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "41 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "42 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "43 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "44 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "45 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "46 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "47 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "48 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "49 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "50 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "52 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "53 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "54 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "55 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "56 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "57 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "58 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "59 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "60 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "61 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "62 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "63 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "64 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "65 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "66 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "67 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "68 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "69 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "70 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "71 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "72 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "73 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "74 Train Loss 822.6211 Test MSE 834.8333367914844 Test RE 0.4863949555580757 Lambda1 0.08197196\n",
      "Training time: 94.18\n",
      "Training time: 94.18\n",
      "inv_HT_stan_tune17\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 838.0158 Test MSE 858.3483921687684 Test RE 0.49319761709651955 Lambda1 -0.1802537\n",
      "1 Train Loss 837.7075 Test MSE 857.1417590202689 Test RE 0.49285083606255575 Lambda1 -0.2505141\n",
      "2 Train Loss 834.7046 Test MSE 853.1736690585615 Test RE 0.49170869993818384 Lambda1 -0.46247935\n",
      "3 Train Loss 823.36127 Test MSE 836.7074219468917 Test RE 0.4869405941458981 Lambda1 -0.4852913\n",
      "4 Train Loss 803.29767 Test MSE 815.7122049147735 Test RE 0.48079246285381616 Lambda1 -0.6315889\n",
      "5 Train Loss 778.5172 Test MSE 785.657081998517 Test RE 0.47185187533068473 Lambda1 -0.65705943\n",
      "6 Train Loss 745.05945 Test MSE 732.4776923956895 Test RE 0.4556027869569795 Lambda1 -0.687364\n",
      "7 Train Loss 701.371 Test MSE 694.9076469707584 Test RE 0.44376466264643516 Lambda1 -0.60247636\n",
      "8 Train Loss 686.56085 Test MSE 685.6937755736678 Test RE 0.4408128786163497 Lambda1 -0.67199147\n",
      "9 Train Loss 673.90173 Test MSE 676.6143987264994 Test RE 0.4378847176626572 Lambda1 -0.737486\n",
      "10 Train Loss 664.0614 Test MSE 664.9953964719994 Test RE 0.4341087004057881 Lambda1 -0.7789891\n",
      "11 Train Loss 653.67053 Test MSE 649.3587791095397 Test RE 0.4289745514938561 Lambda1 -0.8231805\n",
      "12 Train Loss 636.6334 Test MSE 637.3994611907287 Test RE 0.4250059563713063 Lambda1 -0.8869393\n",
      "13 Train Loss 631.5273 Test MSE 632.3408129231947 Test RE 0.42331609113981017 Lambda1 -0.93351877\n",
      "14 Train Loss 626.82 Test MSE 630.0262130702147 Test RE 0.422540634613205 Lambda1 -0.93843895\n",
      "15 Train Loss 622.1462 Test MSE 622.727437854401 Test RE 0.4200859643260143 Lambda1 -1.0170597\n",
      "16 Train Loss 615.57825 Test MSE 614.3269943254573 Test RE 0.4172429143884863 Lambda1 -1.1383911\n",
      "17 Train Loss 609.4975 Test MSE 611.7338122814471 Test RE 0.4163613553214621 Lambda1 -1.146901\n",
      "18 Train Loss 605.5258 Test MSE 609.062318535644 Test RE 0.41545121779274885 Lambda1 -1.156524\n",
      "19 Train Loss 600.92096 Test MSE 604.2410030463711 Test RE 0.41380360222120904 Lambda1 -1.1623392\n",
      "20 Train Loss 586.72925 Test MSE 586.6527826704662 Test RE 0.4077366384868918 Lambda1 -1.1064931\n",
      "21 Train Loss 575.06305 Test MSE 567.7629038189824 Test RE 0.4011184859509245 Lambda1 -1.0776281\n",
      "22 Train Loss 557.595 Test MSE 541.860032459165 Test RE 0.3918616203610099 Lambda1 -1.0793095\n",
      "23 Train Loss 514.283 Test MSE 487.1174108919627 Test RE 0.3715403595025604 Lambda1 -1.0999038\n",
      "24 Train Loss 484.12106 Test MSE 477.6889317887242 Test RE 0.36792708525901513 Lambda1 -1.1324939\n",
      "25 Train Loss 425.88467 Test MSE 413.9461729481287 Test RE 0.34250041851985513 Lambda1 -1.0918658\n",
      "26 Train Loss 374.217 Test MSE 380.7496195738488 Test RE 0.3284799830931099 Lambda1 -1.0605214\n",
      "27 Train Loss 334.2369 Test MSE 334.13188862237337 Test RE 0.3077146218619031 Lambda1 -1.0363097\n",
      "28 Train Loss 316.4857 Test MSE 318.46321122976303 Test RE 0.3004130576494748 Lambda1 -0.97315955\n",
      "29 Train Loss 288.59488 Test MSE 297.33020959681767 Test RE 0.29027436864763295 Lambda1 -0.96390575\n",
      "30 Train Loss 276.91043 Test MSE 288.7179943993519 Test RE 0.28603955698172784 Lambda1 -0.94375\n",
      "31 Train Loss 266.08835 Test MSE 286.07368225223763 Test RE 0.2847266534806623 Lambda1 -0.96754\n",
      "32 Train Loss 257.6882 Test MSE 273.3681344495816 Test RE 0.27833198387262226 Lambda1 -0.9043912\n",
      "33 Train Loss 247.17464 Test MSE 261.50801568643806 Test RE 0.272227298441763 Lambda1 -0.86766773\n",
      "34 Train Loss 236.89758 Test MSE 243.94979687514459 Test RE 0.262929551767492 Lambda1 -0.86559933\n",
      "35 Train Loss 226.96242 Test MSE 233.74406874944196 Test RE 0.25737091726609157 Lambda1 -0.8615452\n",
      "36 Train Loss 216.87125 Test MSE 228.33652318387544 Test RE 0.2543764268222409 Lambda1 -0.8741719\n",
      "37 Train Loss 207.69487 Test MSE 221.14785730425822 Test RE 0.25034016754807803 Lambda1 -0.8825713\n",
      "38 Train Loss 200.79872 Test MSE 213.54441917937672 Test RE 0.24599896710994518 Lambda1 -0.8682012\n",
      "39 Train Loss 193.54742 Test MSE 206.315770042119 Test RE 0.24179949150103935 Lambda1 -0.88845605\n",
      "40 Train Loss 187.58737 Test MSE 203.69060660279374 Test RE 0.24025623739923416 Lambda1 -0.90923834\n",
      "41 Train Loss 181.24026 Test MSE 191.5437090994462 Test RE 0.23298240128588282 Lambda1 -0.9317455\n",
      "42 Train Loss 177.38907 Test MSE 182.74776186934764 Test RE 0.22757010193265978 Lambda1 -0.96749085\n",
      "43 Train Loss 171.98936 Test MSE 177.36108085893773 Test RE 0.22419108252615852 Lambda1 -1.0029345\n",
      "44 Train Loss 168.72142 Test MSE 176.4340894000185 Test RE 0.22360443893818935 Lambda1 -1.001996\n",
      "45 Train Loss 164.23523 Test MSE 171.72800222598997 Test RE 0.22060214424666022 Lambda1 -1.015491\n",
      "46 Train Loss 161.23526 Test MSE 166.35044314698456 Test RE 0.21712066061107552 Lambda1 -1.0322372\n",
      "47 Train Loss 157.13615 Test MSE 165.82646521141572 Test RE 0.21677844281663453 Lambda1 -1.0218484\n",
      "48 Train Loss 153.54135 Test MSE 158.24082886997465 Test RE 0.21176220242712662 Lambda1 -1.0498836\n",
      "49 Train Loss 151.36009 Test MSE 156.70097289108517 Test RE 0.21072934489648462 Lambda1 -1.050766\n",
      "50 Train Loss 148.57214 Test MSE 153.94745075193381 Test RE 0.20886968967576122 Lambda1 -1.0639546\n",
      "51 Train Loss 145.11984 Test MSE 151.6593400977856 Test RE 0.20731167071327225 Lambda1 -1.0827794\n",
      "52 Train Loss 139.76236 Test MSE 151.14704553605117 Test RE 0.20696123241791012 Lambda1 -1.0780022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 Train Loss 137.4716 Test MSE 147.42157308166182 Test RE 0.20439472867054678 Lambda1 -1.0897171\n",
      "54 Train Loss 132.88025 Test MSE 143.4484472666932 Test RE 0.20162161839816878 Lambda1 -1.0913374\n",
      "55 Train Loss 129.94537 Test MSE 140.9513867575252 Test RE 0.1998590630853812 Lambda1 -1.1037916\n",
      "56 Train Loss 127.015144 Test MSE 137.4056532668285 Test RE 0.19732925272712132 Lambda1 -1.127129\n",
      "57 Train Loss 122.99526 Test MSE 133.24354564829133 Test RE 0.19431765511091087 Lambda1 -1.1737745\n",
      "58 Train Loss 121.091286 Test MSE 133.11706495225758 Test RE 0.1942254057362108 Lambda1 -1.1902332\n",
      "59 Train Loss 119.58393 Test MSE 132.62814310394805 Test RE 0.19386839516128762 Lambda1 -1.213178\n",
      "60 Train Loss 117.37689 Test MSE 129.64044773033442 Test RE 0.19167233420231838 Lambda1 -1.2453492\n",
      "61 Train Loss 116.00385 Test MSE 127.35958689855462 Test RE 0.18997873498149812 Lambda1 -1.2668188\n",
      "62 Train Loss 114.77212 Test MSE 126.16377080124758 Test RE 0.18908474883914478 Lambda1 -1.2669895\n",
      "63 Train Loss 113.41182 Test MSE 123.77521441640705 Test RE 0.1872863019061328 Lambda1 -1.2806921\n",
      "64 Train Loss 111.38837 Test MSE 121.04270703173317 Test RE 0.18520746376145394 Lambda1 -1.3213289\n",
      "65 Train Loss 109.940765 Test MSE 119.17394959244018 Test RE 0.1837722095370184 Lambda1 -1.3602409\n",
      "66 Train Loss 108.849556 Test MSE 117.86600264859281 Test RE 0.1827609673537651 Lambda1 -1.3738763\n",
      "67 Train Loss 107.26123 Test MSE 117.53342447859502 Test RE 0.18250294025475844 Lambda1 -1.3822552\n",
      "68 Train Loss 104.92684 Test MSE 116.78841955700037 Test RE 0.1819236083117943 Lambda1 -1.4206219\n",
      "69 Train Loss 103.18382 Test MSE 114.49423924589362 Test RE 0.18012790109878424 Lambda1 -1.4804432\n",
      "70 Train Loss 102.033745 Test MSE 114.22660498103244 Test RE 0.17991725031021172 Lambda1 -1.4912243\n",
      "71 Train Loss 100.81422 Test MSE 112.52656307596094 Test RE 0.17857337130502213 Lambda1 -1.5303766\n",
      "72 Train Loss 99.65425 Test MSE 109.77408150061264 Test RE 0.17637583248611052 Lambda1 -1.5703099\n",
      "73 Train Loss 98.67308 Test MSE 108.24748960090899 Test RE 0.1751451385645486 Lambda1 -1.5757406\n",
      "74 Train Loss 97.804634 Test MSE 107.64085841364735 Test RE 0.1746536824602261 Lambda1 -1.577196\n",
      "Training time: 73.86\n",
      "Training time: 73.86\n",
      "inv_HT_stan_tune17\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 837.99365 Test MSE 858.284951661627 Test RE 0.4931793906482654 Lambda1 -0.025263518\n",
      "1 Train Loss 836.51245 Test MSE 855.9163996646598 Test RE 0.4924984233082552 Lambda1 -0.077539675\n",
      "2 Train Loss 811.2427 Test MSE 807.1395866943899 Test RE 0.47825937798355544 Lambda1 0.028044768\n",
      "3 Train Loss 783.0401 Test MSE 789.3037826651188 Test RE 0.47294567980839947 Lambda1 0.0067730714\n",
      "4 Train Loss 720.6559 Test MSE 707.4286553546667 Test RE 0.44774474188740676 Lambda1 0.008096641\n",
      "5 Train Loss 645.9821 Test MSE 647.6082304014045 Test RE 0.4283959440636205 Lambda1 0.0024446659\n",
      "6 Train Loss 634.24884 Test MSE 638.284402521049 Test RE 0.4253008851461555 Lambda1 0.0034802696\n",
      "7 Train Loss 604.2862 Test MSE 598.6766535443791 Test RE 0.41189387303586933 Lambda1 0.0032033632\n",
      "8 Train Loss 475.29712 Test MSE 499.56013931090536 Test RE 0.37625567553168615 Lambda1 0.004498956\n",
      "9 Train Loss 398.99963 Test MSE 403.4740968861722 Test RE 0.3381403514525024 Lambda1 0.010669011\n",
      "10 Train Loss 330.0409 Test MSE 351.3135014678639 Test RE 0.3155270455696313 Lambda1 0.016448881\n",
      "11 Train Loss 274.9094 Test MSE 290.276507944985 Test RE 0.2868105455299551 Lambda1 0.017642504\n",
      "12 Train Loss 268.54132 Test MSE 286.55462263400966 Test RE 0.28496589086707097 Lambda1 0.01669288\n",
      "13 Train Loss 262.9287 Test MSE 282.4428626509047 Test RE 0.28291402205793387 Lambda1 0.016789762\n",
      "14 Train Loss 258.9355 Test MSE 278.66661066638164 Test RE 0.28101638218276637 Lambda1 0.015345306\n",
      "15 Train Loss 256.26746 Test MSE 277.1124459757333 Test RE 0.28023165165748193 Lambda1 0.013346226\n",
      "16 Train Loss 254.76443 Test MSE 276.5944343609607 Test RE 0.2799696079364441 Lambda1 0.01285535\n",
      "17 Train Loss 254.15764 Test MSE 275.9828882072017 Test RE 0.27965993236370673 Lambda1 0.013835636\n",
      "18 Train Loss 252.80847 Test MSE 274.04495262815146 Test RE 0.278676324830044 Lambda1 0.018336223\n",
      "19 Train Loss 249.3032 Test MSE 271.2855812591989 Test RE 0.2772697727771302 Lambda1 0.033339895\n",
      "20 Train Loss 244.60065 Test MSE 264.6525895308477 Test RE 0.27385914284165247 Lambda1 0.06184421\n",
      "21 Train Loss 238.30121 Test MSE 258.866805380925 Test RE 0.2708490725095393 Lambda1 0.07744365\n",
      "22 Train Loss 232.53609 Test MSE 252.243058581895 Test RE 0.26736144670576656 Lambda1 0.1000282\n",
      "23 Train Loss 230.02168 Test MSE 250.3864683000077 Test RE 0.26637569624768753 Lambda1 0.11425891\n",
      "24 Train Loss 225.37105 Test MSE 247.75499715090757 Test RE 0.26497224298797667 Lambda1 0.13718815\n",
      "25 Train Loss 222.39497 Test MSE 245.59230817096991 Test RE 0.2638132177227708 Lambda1 0.15288769\n",
      "26 Train Loss 217.61409 Test MSE 244.02615906103807 Test RE 0.2629707002017126 Lambda1 0.19026482\n",
      "27 Train Loss 216.04431 Test MSE 243.05832297451562 Test RE 0.2624486959367161 Lambda1 0.22797991\n",
      "28 Train Loss 213.8429 Test MSE 241.83836711422109 Test RE 0.2617892274849888 Lambda1 0.2805163\n",
      "29 Train Loss 212.97958 Test MSE 241.7090274436119 Test RE 0.26171921324103065 Lambda1 0.26153022\n",
      "30 Train Loss 211.973 Test MSE 241.25947866231277 Test RE 0.26147571734906305 Lambda1 0.2673442\n",
      "31 Train Loss 211.26385 Test MSE 240.8015784438794 Test RE 0.2612274646104777 Lambda1 0.26613423\n",
      "32 Train Loss 210.72478 Test MSE 240.21834243232155 Test RE 0.2609109184481191 Lambda1 0.29125893\n",
      "33 Train Loss 209.84216 Test MSE 238.96188951928093 Test RE 0.2602276815428068 Lambda1 0.2810842\n",
      "34 Train Loss 208.79797 Test MSE 237.01274076407944 Test RE 0.2591642043868712 Lambda1 0.2894865\n",
      "35 Train Loss 207.99867 Test MSE 236.08644564299587 Test RE 0.2586572747797032 Lambda1 0.29422593\n",
      "36 Train Loss 207.52603 Test MSE 235.94270331719108 Test RE 0.2585785204505655 Lambda1 0.29413947\n",
      "37 Train Loss 206.51376 Test MSE 233.26871147133852 Test RE 0.2571090808825263 Lambda1 0.3047727\n",
      "38 Train Loss 205.2205 Test MSE 231.2139301048842 Test RE 0.25597418476647327 Lambda1 0.28165704\n",
      "39 Train Loss 203.05031 Test MSE 226.71636301756502 Test RE 0.25347235720433875 Lambda1 0.26791847\n",
      "40 Train Loss 200.61699 Test MSE 220.7270741977037 Test RE 0.25010189015733 Lambda1 0.27932864\n",
      "41 Train Loss 197.83293 Test MSE 216.27932435146423 Test RE 0.24756923388658506 Lambda1 0.2891645\n",
      "42 Train Loss 194.04935 Test MSE 211.38468422357454 Test RE 0.2447518196853909 Lambda1 0.31418908\n",
      "43 Train Loss 184.31961 Test MSE 194.37247675329252 Test RE 0.23469646858690868 Lambda1 0.32734397\n",
      "44 Train Loss 177.37968 Test MSE 185.89128534701132 Test RE 0.2295190226005431 Lambda1 0.360452\n",
      "45 Train Loss 166.96632 Test MSE 174.33097243488814 Test RE 0.2222677468214255 Lambda1 0.43472782\n",
      "46 Train Loss 157.42944 Test MSE 157.02119810201592 Test RE 0.21094455226835743 Lambda1 0.5086088\n",
      "47 Train Loss 148.30711 Test MSE 148.3025761574612 Test RE 0.20500455852798422 Lambda1 0.5621834\n",
      "48 Train Loss 143.57541 Test MSE 142.8936412458507 Test RE 0.2012313414072033 Lambda1 0.6098015\n",
      "49 Train Loss 138.39491 Test MSE 138.58350318248085 Test RE 0.1981732073709327 Lambda1 0.6319064\n",
      "50 Train Loss 132.0568 Test MSE 134.48003641465624 Test RE 0.1952172001127251 Lambda1 0.63378567\n",
      "51 Train Loss 128.4982 Test MSE 129.58446627445431 Test RE 0.19163094567301056 Lambda1 0.6394658\n",
      "52 Train Loss 123.72743 Test MSE 123.99972925669411 Test RE 0.18745608348862144 Lambda1 0.6503792\n",
      "53 Train Loss 121.99633 Test MSE 123.20363713802786 Test RE 0.1868533700570547 Lambda1 0.63397753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 Train Loss 119.782715 Test MSE 119.80190648579183 Test RE 0.18425574392633395 Lambda1 0.6414672\n",
      "55 Train Loss 116.656235 Test MSE 116.70996983200594 Test RE 0.18186249671103363 Lambda1 0.6427074\n",
      "56 Train Loss 113.81526 Test MSE 111.13753909146712 Test RE 0.17746779717347955 Lambda1 0.65635055\n",
      "57 Train Loss 108.666405 Test MSE 107.07998477021069 Test RE 0.1741980627816058 Lambda1 0.65156466\n",
      "58 Train Loss 106.37088 Test MSE 104.44130909550312 Test RE 0.17203837193499646 Lambda1 0.66658664\n",
      "59 Train Loss 105.16804 Test MSE 103.0587923463182 Test RE 0.17089592028946587 Lambda1 0.6801828\n",
      "60 Train Loss 103.716225 Test MSE 102.06085085782506 Test RE 0.17006649569337545 Lambda1 0.676141\n",
      "61 Train Loss 102.881355 Test MSE 102.34984443168649 Test RE 0.17030710402368907 Lambda1 0.6720898\n",
      "62 Train Loss 100.92378 Test MSE 101.0693038064909 Test RE 0.16923835982727667 Lambda1 0.6908902\n",
      "63 Train Loss 98.92759 Test MSE 98.43390115690099 Test RE 0.16701732337573877 Lambda1 0.6897625\n",
      "64 Train Loss 97.845474 Test MSE 96.48692486077807 Test RE 0.16535731174142052 Lambda1 0.6952659\n",
      "65 Train Loss 95.02482 Test MSE 91.97303858457273 Test RE 0.16144308135402502 Lambda1 0.7006183\n",
      "66 Train Loss 93.50071 Test MSE 90.18140172364667 Test RE 0.15986289054048594 Lambda1 0.7279885\n",
      "67 Train Loss 90.97533 Test MSE 87.93194648908798 Test RE 0.15785651625484381 Lambda1 0.7561477\n",
      "68 Train Loss 89.039635 Test MSE 87.08975543970223 Test RE 0.1570987415570914 Lambda1 0.7466304\n",
      "69 Train Loss 87.873116 Test MSE 86.68002674773089 Test RE 0.15672875689248053 Lambda1 0.75182456\n",
      "70 Train Loss 85.72194 Test MSE 84.53325408904408 Test RE 0.15477576680579114 Lambda1 0.80012614\n",
      "71 Train Loss 84.5576 Test MSE 82.8252307535725 Test RE 0.15320413879852962 Lambda1 0.82827204\n",
      "72 Train Loss 83.52246 Test MSE 82.59191963150523 Test RE 0.15298820558121176 Lambda1 0.81504\n",
      "73 Train Loss 82.29714 Test MSE 81.85349222958298 Test RE 0.15230276124642658 Lambda1 0.818542\n",
      "74 Train Loss 81.00864 Test MSE 79.64315580036555 Test RE 0.15023232926350727 Lambda1 0.81558\n",
      "Training time: 74.58\n",
      "Training time: 74.58\n",
      "inv_HT_stan_tune17\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 838.02216 Test MSE 858.2943587846107 Test RE 0.4931820933558333 Lambda1 -0.027393073\n",
      "1 Train Loss 837.7247 Test MSE 857.5793992187293 Test RE 0.49297664011450754 Lambda1 -0.22880514\n",
      "2 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.53\n",
      "Training time: 4.53\n",
      "inv_HT_stan_tune17\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 837.9246 Test MSE 857.9153634573822 Test RE 0.49307319461656235 Lambda1 0.025111033\n",
      "1 Train Loss 837.86005 Test MSE 857.8456855088778 Test RE 0.4930531710642744 Lambda1 -0.096902624\n",
      "2 Train Loss 837.0228 Test MSE 856.0055893053191 Test RE 0.4925240827144604 Lambda1 -0.99045056\n",
      "3 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.30\n",
      "Training time: 6.30\n",
      "inv_HT_stan_tune17\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 837.9889 Test MSE 858.1609522602666 Test RE 0.49314376369358925 Lambda1 -0.013598241\n",
      "1 Train Loss 837.7568 Test MSE 857.6076662544027 Test RE 0.49298476465286384 Lambda1 -0.071889944\n",
      "2 Train Loss 832.5165 Test MSE 851.5101900868822 Test RE 0.4912291104944865 Lambda1 0.10959099\n",
      "3 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "4 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "5 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "6 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "7 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "8 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "9 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "10 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "11 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "12 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "13 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "14 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "15 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "16 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "17 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "18 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "19 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "20 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "21 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "22 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "23 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "24 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "25 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "26 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "27 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "28 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "29 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "30 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "31 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "33 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "34 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "35 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "36 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "37 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "38 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "39 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "40 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "41 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "42 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "43 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "44 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "45 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "46 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "47 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "48 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "49 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "50 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "51 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "52 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "53 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "54 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "55 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "56 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "57 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "58 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "59 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "60 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "61 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "62 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "63 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "64 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "65 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "66 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "67 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "68 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "69 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "70 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "71 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "72 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "73 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "74 Train Loss 811.7987 Test MSE 819.3283164339043 Test RE 0.48185697836415575 Lambda1 0.35483903\n",
      "Training time: 88.66\n",
      "Training time: 88.66\n",
      "inv_HT_stan_tune17\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 837.85236 Test MSE 857.8611687760471 Test RE 0.49305762060552555 Lambda1 -0.20480005\n",
      "1 Train Loss 836.7013 Test MSE 856.3867921710603 Test RE 0.49263373777748626 Lambda1 -0.118954\n",
      "2 Train Loss 825.99817 Test MSE 839.3629578289999 Test RE 0.4877127062443275 Lambda1 -0.20770732\n",
      "3 Train Loss 794.3836 Test MSE 796.6881323727825 Test RE 0.4751528566000654 Lambda1 -0.16581236\n",
      "4 Train Loss 713.4806 Test MSE 707.4198678035191 Test RE 0.44774196097656205 Lambda1 -0.011528214\n",
      "5 Train Loss 658.66156 Test MSE 656.878806660586 Test RE 0.4314513137215302 Lambda1 -0.00087607413\n",
      "6 Train Loss 651.1401 Test MSE 656.690272830637 Test RE 0.4313893928578221 Lambda1 -0.0008877477\n",
      "7 Train Loss 637.3318 Test MSE 640.1884986832498 Test RE 0.4259347802646726 Lambda1 -0.0003165353\n",
      "8 Train Loss 584.1056 Test MSE 569.1612969531602 Test RE 0.40161215714012355 Lambda1 0.0014520197\n",
      "9 Train Loss 457.5178 Test MSE 434.12361400405933 Test RE 0.35074854289111745 Lambda1 0.0023920184\n",
      "10 Train Loss 340.61896 Test MSE 341.9200372993572 Test RE 0.3112801640884888 Lambda1 -9.822598e-05\n",
      "11 Train Loss 297.92645 Test MSE 303.76812055802776 Test RE 0.29340010688439566 Lambda1 0.0018041834\n",
      "12 Train Loss 270.91644 Test MSE 290.26837713226763 Test RE 0.28680652863727235 Lambda1 0.0013027865\n",
      "13 Train Loss 262.54333 Test MSE 284.53445029552114 Test RE 0.2839596282310895 Lambda1 0.00037960403\n",
      "14 Train Loss 257.83252 Test MSE 284.8914416578737 Test RE 0.28413770747249345 Lambda1 0.00050108164\n",
      "15 Train Loss 255.99025 Test MSE 283.9634601079198 Test RE 0.2836745668163259 Lambda1 0.0004362905\n",
      "16 Train Loss 255.13107 Test MSE 282.6304959782258 Test RE 0.2830079796206863 Lambda1 0.00037438585\n",
      "17 Train Loss 254.75536 Test MSE 283.29042757377135 Test RE 0.2833381934766761 Lambda1 0.00036730166\n",
      "18 Train Loss 254.4979 Test MSE 282.7873846975291 Test RE 0.2830865178536094 Lambda1 0.00037717627\n",
      "19 Train Loss 254.35092 Test MSE 282.62039543154344 Test RE 0.2830029225569756 Lambda1 0.00035882613\n",
      "20 Train Loss 254.1229 Test MSE 283.4056051904475 Test RE 0.28339578614184574 Lambda1 0.0002650887\n",
      "21 Train Loss 253.92369 Test MSE 283.49136041670147 Test RE 0.2834386590264448 Lambda1 0.00025443634\n",
      "22 Train Loss 253.77307 Test MSE 283.65440212421623 Test RE 0.2835201530098439 Lambda1 0.0002954572\n",
      "23 Train Loss 253.54375 Test MSE 283.9385851697239 Test RE 0.283662141728509 Lambda1 0.00030136458\n",
      "24 Train Loss 253.41548 Test MSE 284.0004535921094 Test RE 0.2836930441387945 Lambda1 0.00028924638\n",
      "25 Train Loss 253.24089 Test MSE 283.4691203778469 Test RE 0.2834275408561819 Lambda1 0.00024203319\n",
      "26 Train Loss 252.99588 Test MSE 284.10721899706436 Test RE 0.2837463640477311 Lambda1 0.00026938503\n",
      "27 Train Loss 252.5448 Test MSE 283.65037961967437 Test RE 0.28351814270233966 Lambda1 0.0002330579\n",
      "28 Train Loss 252.20386 Test MSE 284.49346278667275 Test RE 0.28393917514194994 Lambda1 0.0001574761\n",
      "29 Train Loss 251.8594 Test MSE 284.66270295041727 Test RE 0.2840236177993341 Lambda1 7.299548e-05\n",
      "30 Train Loss 251.4736 Test MSE 284.48523514292606 Test RE 0.2839350693054805 Lambda1 2.741759e-06\n",
      "31 Train Loss 251.2945 Test MSE 284.88006862721255 Test RE 0.2841320359453114 Lambda1 -3.3880802e-05\n",
      "32 Train Loss 251.04544 Test MSE 286.7255075625843 Test RE 0.2850508469527276 Lambda1 -1.49969665e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 Train Loss 250.7566 Test MSE 287.25522224036985 Test RE 0.285314035853295 Lambda1 -3.8632165e-06\n",
      "34 Train Loss 250.51082 Test MSE 287.2756062456132 Test RE 0.2853241588017765 Lambda1 6.183862e-06\n",
      "35 Train Loss 250.40184 Test MSE 287.1167785769853 Test RE 0.28524527351634077 Lambda1 9.20575e-06\n",
      "36 Train Loss 250.25009 Test MSE 287.29617177039364 Test RE 0.28533437153115127 Lambda1 1.7300947e-05\n",
      "37 Train Loss 250.16481 Test MSE 287.263553576662 Test RE 0.2853181733410798 Lambda1 1.14470695e-05\n",
      "38 Train Loss 250.09048 Test MSE 287.3604373046159 Test RE 0.2853662830953672 Lambda1 7.382084e-06\n",
      "39 Train Loss 249.95412 Test MSE 288.2836896188859 Test RE 0.285824338148328 Lambda1 7.727602e-06\n",
      "40 Train Loss 249.82362 Test MSE 288.6599013289927 Test RE 0.2860107784636065 Lambda1 9.660775e-06\n",
      "41 Train Loss 249.74763 Test MSE 287.9311981154042 Test RE 0.2856495425030532 Lambda1 9.252042e-06\n",
      "42 Train Loss 249.61705 Test MSE 288.50510219721957 Test RE 0.28593407893366407 Lambda1 1.1384042e-05\n",
      "43 Train Loss 249.49294 Test MSE 288.51000814124933 Test RE 0.28593651003559833 Lambda1 1.4371388e-05\n",
      "44 Train Loss 249.44983 Test MSE 288.2728000260771 Test RE 0.2858189397505643 Lambda1 1.5867608e-05\n",
      "45 Train Loss 249.37164 Test MSE 288.3259582512111 Test RE 0.2858452913993746 Lambda1 1.4810234e-05\n",
      "46 Train Loss 249.22871 Test MSE 287.48912235970073 Test RE 0.28543017196241655 Lambda1 1.4575198e-05\n",
      "47 Train Loss 249.19661 Test MSE 287.73733569261475 Test RE 0.2855533632125043 Lambda1 1.0454648e-05\n",
      "48 Train Loss 249.12387 Test MSE 288.2731960141278 Test RE 0.2858191360591406 Lambda1 7.887502e-06\n",
      "49 Train Loss 249.09937 Test MSE 288.42357230793533 Test RE 0.2858936744125222 Lambda1 5.627455e-06\n",
      "50 Train Loss 249.02644 Test MSE 288.37349926556965 Test RE 0.2858688564211571 Lambda1 2.1381006e-06\n",
      "51 Train Loss 248.94014 Test MSE 288.71744649782704 Test RE 0.2860392855722785 Lambda1 7.3772867e-06\n",
      "52 Train Loss 248.87791 Test MSE 288.82429367478125 Test RE 0.2860922087043059 Lambda1 1.024618e-05\n",
      "53 Train Loss 248.75296 Test MSE 288.4472028200165 Test RE 0.2859053857915592 Lambda1 8.109705e-06\n",
      "54 Train Loss 248.50195 Test MSE 287.73073078395214 Test RE 0.28555008580552244 Lambda1 4.620123e-06\n",
      "55 Train Loss 248.30525 Test MSE 288.6504893854574 Test RE 0.28600611564201567 Lambda1 3.5505664e-06\n",
      "56 Train Loss 248.23425 Test MSE 289.1156840277165 Test RE 0.2862364893202394 Lambda1 4.863712e-06\n",
      "57 Train Loss 248.18643 Test MSE 289.60955254997674 Test RE 0.2864808601413733 Lambda1 2.547027e-06\n",
      "58 Train Loss 248.09232 Test MSE 289.1630795146888 Test RE 0.2862599501053249 Lambda1 3.4448785e-08\n",
      "59 Train Loss 247.99817 Test MSE 289.60255559604946 Test RE 0.2864773994380418 Lambda1 1.5193962e-06\n",
      "60 Train Loss 247.8311 Test MSE 289.8630559708991 Test RE 0.2866062151097336 Lambda1 1.5266481e-06\n",
      "61 Train Loss 247.72655 Test MSE 289.1306345378854 Test RE 0.28624389003638717 Lambda1 2.9891437e-06\n",
      "62 Train Loss 247.55148 Test MSE 290.4112160748132 Test RE 0.28687708765407355 Lambda1 4.981274e-06\n",
      "63 Train Loss 247.52339 Test MSE 290.6622387946659 Test RE 0.2870010448364191 Lambda1 3.540387e-06\n",
      "64 Train Loss 247.35385 Test MSE 290.51883849404146 Test RE 0.28693023909005044 Lambda1 1.8403254e-06\n",
      "65 Train Loss 247.21092 Test MSE 290.724380396441 Test RE 0.28703172262858534 Lambda1 2.3928258e-06\n",
      "66 Train Loss 247.1473 Test MSE 290.95593399290743 Test RE 0.2871460061276849 Lambda1 1.6738254e-06\n",
      "67 Train Loss 247.11292 Test MSE 290.8986857987221 Test RE 0.28711775545965007 Lambda1 1.657828e-06\n",
      "68 Train Loss 247.00821 Test MSE 290.98135850627364 Test RE 0.2871585516494281 Lambda1 1.3460497e-06\n",
      "69 Train Loss 246.87445 Test MSE 290.9723150808656 Test RE 0.2871540893066632 Lambda1 1.03157e-06\n",
      "70 Train Loss 246.79628 Test MSE 291.06138913558067 Test RE 0.2871980385425843 Lambda1 3.5344976e-07\n",
      "71 Train Loss 246.65201 Test MSE 291.1130765250346 Test RE 0.28722353807369666 Lambda1 5.596551e-07\n",
      "72 Train Loss 246.50275 Test MSE 290.66385230160955 Test RE 0.2870018414268843 Lambda1 -1.8391142e-07\n",
      "73 Train Loss 246.4816 Test MSE 290.3341356424627 Test RE 0.2868390139224285 Lambda1 -2.0445503e-07\n",
      "74 Train Loss 246.41422 Test MSE 291.03774406956086 Test RE 0.28718637269689296 Lambda1 6.130381e-07\n",
      "Training time: 73.92\n",
      "Training time: 73.92\n",
      "inv_HT_stan_tune17\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 838.0203 Test MSE 858.0532168606721 Test RE 0.49311280754958875 Lambda1 -0.015981352\n",
      "1 Train Loss 837.6729 Test MSE 857.2569637471374 Test RE 0.49288395592729195 Lambda1 -0.19956516\n",
      "2 Train Loss 836.1849 Test MSE 853.8452548630066 Test RE 0.4919021890455522 Lambda1 -0.55381155\n",
      "3 Train Loss 823.06415 Test MSE 833.8719856127169 Test RE 0.48611482116728777 Lambda1 -1.026388\n",
      "4 Train Loss 771.3202 Test MSE 774.2411044077197 Test RE 0.4684112129308151 Lambda1 -1.0458848\n",
      "5 Train Loss 741.7723 Test MSE 736.9120510067753 Test RE 0.456979796586147 Lambda1 -1.1589808\n",
      "6 Train Loss 689.9627 Test MSE 683.1148616106916 Test RE 0.4399831427744783 Lambda1 -1.2493662\n",
      "7 Train Loss 640.6128 Test MSE 645.8826402505775 Test RE 0.42782482028965435 Lambda1 -1.2976065\n",
      "8 Train Loss 621.46545 Test MSE 626.2745178024635 Test RE 0.4212806785343506 Lambda1 -1.3254788\n",
      "9 Train Loss 589.42596 Test MSE 588.4510933720072 Test RE 0.40836109313020064 Lambda1 -1.4144448\n",
      "10 Train Loss 557.79175 Test MSE 558.5787842171725 Test RE 0.39786101738317947 Lambda1 -1.468962\n",
      "11 Train Loss 534.1273 Test MSE 537.5723543343602 Test RE 0.3903081624800014 Lambda1 -1.5271505\n",
      "12 Train Loss 492.95343 Test MSE 500.67398173989494 Test RE 0.37667490052313424 Lambda1 -1.5552115\n",
      "13 Train Loss 452.29193 Test MSE 462.1382243701188 Test RE 0.3618887782193566 Lambda1 -1.5831662\n",
      "14 Train Loss 410.18655 Test MSE 434.4036610401393 Test RE 0.3508616561274378 Lambda1 -1.5757637\n",
      "15 Train Loss 401.82544 Test MSE 424.75114163636135 Test RE 0.3469416567538103 Lambda1 -1.5671782\n",
      "16 Train Loss 390.8293 Test MSE 408.9658243813404 Test RE 0.3404338052977713 Lambda1 -1.5812671\n",
      "17 Train Loss 381.04797 Test MSE 395.37681017023317 Test RE 0.334730100007214 Lambda1 -1.5863992\n",
      "18 Train Loss 370.52087 Test MSE 386.530329649678 Test RE 0.3309641542677104 Lambda1 -1.5671246\n",
      "19 Train Loss 359.02545 Test MSE 371.444342173686 Test RE 0.3244412339000209 Lambda1 -1.5945821\n",
      "20 Train Loss 343.41904 Test MSE 356.3292636667425 Test RE 0.3177714793826618 Lambda1 -1.617424\n",
      "21 Train Loss 330.21756 Test MSE 340.32946739080745 Test RE 0.31055530161736355 Lambda1 -1.6166877\n",
      "22 Train Loss 317.029 Test MSE 328.696523222085 Test RE 0.3052015436155247 Lambda1 -1.6393443\n",
      "23 Train Loss 307.83093 Test MSE 324.7573444316674 Test RE 0.30336722669544125 Lambda1 -1.6544776\n",
      "24 Train Loss 301.8509 Test MSE 324.69561209216266 Test RE 0.3033383921532696 Lambda1 -1.6808892\n",
      "25 Train Loss 300.546 Test MSE 325.9241923773513 Test RE 0.30391173482907596 Lambda1 -1.6833016\n",
      "26 Train Loss 296.371 Test MSE 325.9231427477757 Test RE 0.30391124545914044 Lambda1 -1.6732758\n",
      "27 Train Loss 295.30432 Test MSE 324.5536127515825 Test RE 0.3032720553127597 Lambda1 -1.6743848\n",
      "28 Train Loss 293.53717 Test MSE 324.12914024347396 Test RE 0.3030736708809697 Lambda1 -1.6439322\n",
      "29 Train Loss 290.97058 Test MSE 322.51877849438085 Test RE 0.3023198572027345 Lambda1 -1.6006141\n",
      "30 Train Loss 289.6572 Test MSE 319.3404703179494 Test RE 0.30082654158451866 Lambda1 -1.6006643\n",
      "31 Train Loss 287.73044 Test MSE 318.5962952393 Test RE 0.3004758215682241 Lambda1 -1.6037526\n",
      "32 Train Loss 287.19244 Test MSE 316.51004304358776 Test RE 0.29949040844458974 Lambda1 -1.6190041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 Train Loss 286.0412 Test MSE 315.9143746413464 Test RE 0.29920845690566383 Lambda1 -1.6204787\n",
      "34 Train Loss 284.7093 Test MSE 312.24014586361295 Test RE 0.2974634024667886 Lambda1 -1.6490569\n",
      "35 Train Loss 284.04803 Test MSE 311.8168422030437 Test RE 0.29726169865919505 Lambda1 -1.6489615\n",
      "36 Train Loss 282.31778 Test MSE 309.65650945749263 Test RE 0.2962301630568265 Lambda1 -1.6506413\n",
      "37 Train Loss 281.5562 Test MSE 309.0874659616675 Test RE 0.2959578526802387 Lambda1 -1.6677889\n",
      "38 Train Loss 279.36774 Test MSE 305.0500997347263 Test RE 0.2940185668099773 Lambda1 -1.7237304\n",
      "39 Train Loss 277.5219 Test MSE 302.9099941852897 Test RE 0.29298539510947685 Lambda1 -1.7426088\n",
      "40 Train Loss 274.25363 Test MSE 299.1797200937319 Test RE 0.2911757792386384 Lambda1 -1.7469134\n",
      "41 Train Loss 271.05945 Test MSE 293.1824631805813 Test RE 0.2882425991468787 Lambda1 -1.7877376\n",
      "42 Train Loss 269.30954 Test MSE 288.32539252878615 Test RE 0.2858450109716744 Lambda1 -1.8030521\n",
      "43 Train Loss 264.9713 Test MSE 285.20509254158725 Test RE 0.28429407497667225 Lambda1 -1.8001537\n",
      "44 Train Loss 260.19083 Test MSE 276.5053942145959 Test RE 0.27992454097500674 Lambda1 -1.7995031\n",
      "45 Train Loss 258.04996 Test MSE 271.75784139431266 Test RE 0.2775110066126223 Lambda1 -1.8191483\n",
      "46 Train Loss 254.5687 Test MSE 263.06746412342625 Test RE 0.27303777727794215 Lambda1 -1.8568002\n",
      "47 Train Loss 248.8219 Test MSE 250.94686788158464 Test RE 0.26667362248451715 Lambda1 -1.9135317\n",
      "48 Train Loss 236.77231 Test MSE 234.9973435072992 Test RE 0.25805997274684933 Lambda1 -1.9587964\n",
      "49 Train Loss 227.30821 Test MSE 219.66012220355185 Test RE 0.2494966858240834 Lambda1 -1.9515775\n",
      "50 Train Loss 215.49303 Test MSE 214.6551169271911 Test RE 0.2466378883605486 Lambda1 -1.9282455\n",
      "51 Train Loss 213.59375 Test MSE 217.83168369825893 Test RE 0.24845611769985948 Lambda1 -1.9254974\n",
      "52 Train Loss 208.71814 Test MSE 211.1279728761728 Test RE 0.24460315788185463 Lambda1 -1.9523427\n",
      "53 Train Loss 206.22491 Test MSE 209.52328613371037 Test RE 0.2436718267277429 Lambda1 -1.9553511\n",
      "54 Train Loss 202.23427 Test MSE 204.41748921514468 Test RE 0.2406845403044175 Lambda1 -1.9551574\n",
      "55 Train Loss 200.69536 Test MSE 198.2506029945394 Test RE 0.2370262410027012 Lambda1 -1.9444659\n",
      "56 Train Loss 198.2599 Test MSE 193.91103512008937 Test RE 0.23441771751198007 Lambda1 -1.9254297\n",
      "57 Train Loss 195.33554 Test MSE 193.28878119506194 Test RE 0.23404129604194493 Lambda1 -1.9076451\n",
      "58 Train Loss 189.46852 Test MSE 191.50526094271126 Test RE 0.23295901708421546 Lambda1 -1.8463368\n",
      "59 Train Loss 185.5583 Test MSE 186.68827578398106 Test RE 0.23001051637158784 Lambda1 -1.79644\n",
      "60 Train Loss 182.69844 Test MSE 183.43724717994263 Test RE 0.2279989950754504 Lambda1 -1.7600098\n",
      "61 Train Loss 182.07193 Test MSE 184.35464981646683 Test RE 0.22856841599200156 Lambda1 -1.7661539\n",
      "62 Train Loss 180.36339 Test MSE 182.37600097110058 Test RE 0.22733851295334342 Lambda1 -1.7550658\n",
      "63 Train Loss 178.77289 Test MSE 177.84954202129256 Test RE 0.22449958691119767 Lambda1 -1.74518\n",
      "64 Train Loss 176.23814 Test MSE 174.06931954796647 Test RE 0.22210088365556152 Lambda1 -1.7449423\n",
      "65 Train Loss 174.89185 Test MSE 174.40118434301192 Test RE 0.22231250155261695 Lambda1 -1.7341372\n",
      "66 Train Loss 172.56067 Test MSE 173.90292595235803 Test RE 0.2219947046902584 Lambda1 -1.699361\n",
      "67 Train Loss 170.2167 Test MSE 174.93368599130199 Test RE 0.2226516378591069 Lambda1 -1.6739388\n",
      "68 Train Loss 169.44337 Test MSE 174.75665574828983 Test RE 0.22253894929800921 Lambda1 -1.678149\n",
      "69 Train Loss 168.19342 Test MSE 173.3551332037026 Test RE 0.22164478819156627 Lambda1 -1.6871932\n",
      "70 Train Loss 166.30992 Test MSE 173.23425891383232 Test RE 0.22156750225362826 Lambda1 -1.6766587\n",
      "71 Train Loss 165.61208 Test MSE 172.54262476626062 Test RE 0.22112475809572285 Lambda1 -1.6800686\n",
      "72 Train Loss 165.11575 Test MSE 171.43470763798194 Test RE 0.22041368032782954 Lambda1 -1.6827823\n",
      "73 Train Loss 164.32556 Test MSE 168.73696528508844 Test RE 0.21867255920461928 Lambda1 -1.6910117\n",
      "74 Train Loss 162.84824 Test MSE 166.6082832778226 Test RE 0.21728886197665664 Lambda1 -1.707022\n",
      "Training time: 76.04\n",
      "Training time: 76.04\n",
      "inv_HT_stan_tune17\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 837.9927 Test MSE 858.2194037520727 Test RE 0.49316055803605374 Lambda1 0.014551382\n",
      "1 Train Loss 837.55835 Test MSE 856.8053253799231 Test RE 0.4927541030117493 Lambda1 -0.035249583\n",
      "2 Train Loss 821.74335 Test MSE 835.0451697888426 Test RE 0.48645666126720183 Lambda1 -0.22343533\n",
      "3 Train Loss 739.2921 Test MSE 721.608788218297 Test RE 0.4522099120515992 Lambda1 0.0061594867\n",
      "4 Train Loss 641.9193 Test MSE 644.6216178393271 Test RE 0.4274069732700995 Lambda1 -0.0019281306\n",
      "5 Train Loss 575.95886 Test MSE 578.0722382946408 Test RE 0.40474382082440913 Lambda1 0.0019275171\n",
      "6 Train Loss 422.77094 Test MSE 431.7510659458938 Test RE 0.3497887841447986 Lambda1 -8.791806e-05\n",
      "7 Train Loss 276.72955 Test MSE 296.2379090771945 Test RE 0.28974068829610466 Lambda1 0.0004987176\n",
      "8 Train Loss 262.80157 Test MSE 287.7963406105024 Test RE 0.2855826402422096 Lambda1 -0.00044884917\n",
      "9 Train Loss 259.25833 Test MSE 283.99777012878155 Test RE 0.28369170385628967 Lambda1 -0.00041407967\n",
      "10 Train Loss 257.5245 Test MSE 283.5570932358378 Test RE 0.28347151742150795 Lambda1 -0.0001234946\n",
      "11 Train Loss 256.11884 Test MSE 283.70907759782773 Test RE 0.2835474764915209 Lambda1 0.000104078674\n",
      "12 Train Loss 255.13635 Test MSE 284.021665402081 Test RE 0.28370363836585843 Lambda1 8.812961e-05\n",
      "13 Train Loss 254.90372 Test MSE 283.88782893154956 Test RE 0.2836367871845018 Lambda1 0.000107403735\n",
      "14 Train Loss 254.8679 Test MSE 283.8124928268151 Test RE 0.2835991499448183 Lambda1 0.00011384348\n",
      "15 Train Loss 254.69177 Test MSE 282.8411683928146 Test RE 0.28311343686763185 Lambda1 0.00013653177\n",
      "16 Train Loss 254.57953 Test MSE 282.9317276303834 Test RE 0.283158756446187 Lambda1 0.00020116752\n",
      "17 Train Loss 254.53464 Test MSE 283.23474167530907 Test RE 0.2833103444641903 Lambda1 0.00018423291\n",
      "18 Train Loss 254.46254 Test MSE 283.1410780483228 Test RE 0.2832634962766494 Lambda1 0.0002849486\n",
      "19 Train Loss 254.43782 Test MSE 282.97390413208745 Test RE 0.2831798608321033 Lambda1 0.0002987981\n",
      "20 Train Loss 254.25113 Test MSE 283.0862338585257 Test RE 0.2832360609969846 Lambda1 0.00034269542\n",
      "21 Train Loss 254.22179 Test MSE 282.9780846708332 Test RE 0.2831819526151097 Lambda1 0.00037211235\n",
      "22 Train Loss 254.15483 Test MSE 283.0925068174061 Test RE 0.2832391991190637 Lambda1 0.00039922024\n",
      "23 Train Loss 254.15028 Test MSE 283.1099112722001 Test RE 0.2832479057219454 Lambda1 0.00040174785\n",
      "24 Train Loss 254.11632 Test MSE 282.9364474586653 Test RE 0.28316111824411 Lambda1 0.00044377937\n",
      "25 Train Loss 254.09308 Test MSE 282.7261600018627 Test RE 0.28305587146510436 Lambda1 0.00047834174\n",
      "26 Train Loss 254.08908 Test MSE 282.78688499788956 Test RE 0.28308626773938744 Lambda1 0.00048623775\n",
      "27 Train Loss 254.04544 Test MSE 282.3249991209071 Test RE 0.2828549858258178 Lambda1 0.00058317126\n",
      "28 Train Loss 254.00888 Test MSE 282.4292902205796 Test RE 0.2829072244406265 Lambda1 0.0006117784\n",
      "29 Train Loss 254.00053 Test MSE 282.34524900357843 Test RE 0.2828651295920281 Lambda1 0.0006379449\n",
      "30 Train Loss 253.95699 Test MSE 282.1844000149109 Test RE 0.2827845455339641 Lambda1 0.0007345011\n",
      "31 Train Loss 253.94157 Test MSE 282.34849466070506 Test RE 0.28286675540402345 Lambda1 0.00071306736\n",
      "32 Train Loss 253.93216 Test MSE 282.5204155454908 Test RE 0.28295286052460517 Lambda1 0.0007138719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 Train Loss 253.90198 Test MSE 282.600724318546 Test RE 0.2829930735166539 Lambda1 0.00082281313\n",
      "34 Train Loss 253.82974 Test MSE 282.3590428183911 Test RE 0.28287203911427894 Lambda1 0.00082422135\n",
      "35 Train Loss 253.81589 Test MSE 282.0936006346418 Test RE 0.28273904562799723 Lambda1 0.00089132437\n",
      "36 Train Loss 253.79938 Test MSE 282.0445174714258 Test RE 0.2827144468238899 Lambda1 0.0008952738\n",
      "37 Train Loss 253.7304 Test MSE 282.27758880913933 Test RE 0.2828312351729508 Lambda1 0.00078690314\n",
      "38 Train Loss 253.70758 Test MSE 282.2875718791992 Test RE 0.2828362364539308 Lambda1 0.00076016836\n",
      "39 Train Loss 253.68181 Test MSE 282.1998158065946 Test RE 0.2827922697173504 Lambda1 0.0007906426\n",
      "40 Train Loss 253.66129 Test MSE 282.229955812873 Test RE 0.28280737095544084 Lambda1 0.0008376484\n",
      "41 Train Loss 253.64716 Test MSE 282.2720894951664 Test RE 0.2828284801094942 Lambda1 0.0008494429\n",
      "42 Train Loss 253.6307 Test MSE 282.20437666472026 Test RE 0.282794554924742 Lambda1 0.0008640063\n",
      "43 Train Loss 253.57173 Test MSE 281.9277026913773 Test RE 0.28265589463777946 Lambda1 0.00088023173\n",
      "44 Train Loss 253.44426 Test MSE 281.52081917643045 Test RE 0.2824518537644381 Lambda1 0.0009870752\n",
      "45 Train Loss 253.3184 Test MSE 281.6239266676901 Test RE 0.2825035732714328 Lambda1 0.0010202145\n",
      "46 Train Loss 253.21347 Test MSE 281.6223777196357 Test RE 0.2825027963772835 Lambda1 0.0011142279\n",
      "47 Train Loss 253.12541 Test MSE 281.57607136308536 Test RE 0.2824795698618495 Lambda1 0.0011130569\n",
      "48 Train Loss 253.06386 Test MSE 281.52504587654806 Test RE 0.28245397409572665 Lambda1 0.0011859575\n",
      "49 Train Loss 252.91531 Test MSE 282.03533168485455 Test RE 0.2827098429837983 Lambda1 0.0012856104\n",
      "50 Train Loss 252.85924 Test MSE 282.5504967014577 Test RE 0.2829679237237709 Lambda1 0.0011309793\n",
      "51 Train Loss 252.7179 Test MSE 282.8068404008921 Test RE 0.28309625582794545 Lambda1 0.00095526606\n",
      "52 Train Loss 252.6151 Test MSE 283.2198701635779 Test RE 0.2833029066258923 Lambda1 0.0008870145\n",
      "53 Train Loss 252.56511 Test MSE 283.7172305660924 Test RE 0.28355155062441245 Lambda1 0.00068343984\n",
      "54 Train Loss 252.41797 Test MSE 283.91139284330154 Test RE 0.2836485584773431 Lambda1 0.00043530265\n",
      "55 Train Loss 252.19035 Test MSE 283.9094507068409 Test RE 0.28364758830643166 Lambda1 0.0002601453\n",
      "56 Train Loss 252.00574 Test MSE 284.20525476613017 Test RE 0.2837953154503565 Lambda1 0.00024546968\n",
      "57 Train Loss 251.7731 Test MSE 284.3681953233024 Test RE 0.2838766565587874 Lambda1 0.00026746411\n",
      "58 Train Loss 251.5609 Test MSE 284.5740724226998 Test RE 0.28397939858373683 Lambda1 0.00016663893\n",
      "59 Train Loss 251.32664 Test MSE 284.9710698300789 Test RE 0.28417741344860525 Lambda1 0.0001275514\n",
      "60 Train Loss 251.11357 Test MSE 285.88592398673273 Test RE 0.28463320105810463 Lambda1 0.00012424379\n",
      "61 Train Loss 251.01497 Test MSE 285.98492435916415 Test RE 0.2846824800736799 Lambda1 8.978527e-05\n",
      "62 Train Loss 250.7937 Test MSE 286.3810719288716 Test RE 0.28487958354924126 Lambda1 6.204252e-05\n",
      "63 Train Loss 250.66115 Test MSE 286.2786367088883 Test RE 0.2848286299150331 Lambda1 2.2634798e-05\n",
      "64 Train Loss 250.61029 Test MSE 286.3516902383111 Test RE 0.284864969352823 Lambda1 1.2422928e-05\n",
      "65 Train Loss 250.366 Test MSE 285.8131066878296 Test RE 0.28459694963911303 Lambda1 4.2271795e-06\n",
      "66 Train Loss 249.90631 Test MSE 287.16069599495705 Test RE 0.2852670882583393 Lambda1 1.9690837e-05\n",
      "67 Train Loss 249.53294 Test MSE 287.0704736518394 Test RE 0.28522227104032855 Lambda1 2.022964e-05\n",
      "68 Train Loss 249.31946 Test MSE 286.43542496969854 Test RE 0.28490661630128356 Lambda1 3.349551e-05\n",
      "69 Train Loss 249.0056 Test MSE 287.28786174585565 Test RE 0.28533024486151826 Lambda1 3.7166865e-05\n",
      "70 Train Loss 248.6308 Test MSE 288.04460405260716 Test RE 0.28570579060185414 Lambda1 2.2462418e-05\n",
      "71 Train Loss 248.48494 Test MSE 287.71868105883897 Test RE 0.2855441065414111 Lambda1 2.5133946e-05\n",
      "72 Train Loss 248.30087 Test MSE 288.1532630352517 Test RE 0.2857596738771018 Lambda1 2.0127361e-05\n",
      "73 Train Loss 248.20245 Test MSE 288.6674343607529 Test RE 0.2860145103886362 Lambda1 2.0634907e-05\n",
      "74 Train Loss 247.99269 Test MSE 288.6589765890224 Test RE 0.2860103203365695 Lambda1 1.5769083e-05\n",
      "Training time: 72.09\n",
      "Training time: 72.09\n",
      "inv_HT_stan_tune18\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartlab/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 837.4973 Test MSE 857.2299967751821 Test RE 0.49287620347213 Lambda1 -0.013913003\n",
      "1 Train Loss 833.9308 Test MSE 851.0346256955407 Test RE 0.4910919167753499 Lambda1 -0.011583611\n",
      "2 Train Loss 775.6927 Test MSE 771.4819445515848 Test RE 0.4675758304427028 Lambda1 0.047491662\n",
      "3 Train Loss 680.178 Test MSE 675.3893638698427 Test RE 0.43748813500998124 Lambda1 0.002288541\n",
      "4 Train Loss 621.4258 Test MSE 623.7936870456599 Test RE 0.4204454512565304 Lambda1 0.0008884716\n",
      "5 Train Loss 547.5408 Test MSE 549.2868835424639 Test RE 0.3945379511418147 Lambda1 0.0002452129\n",
      "6 Train Loss 417.1561 Test MSE 420.5771866638568 Test RE 0.3452327807772199 Lambda1 0.00080328237\n",
      "7 Train Loss 300.15253 Test MSE 326.1267751606893 Test RE 0.30400617047168665 Lambda1 -0.00021557824\n",
      "8 Train Loss 273.70486 Test MSE 298.1528057068438 Test RE 0.29067562898913407 Lambda1 0.0020846075\n",
      "9 Train Loss 262.43353 Test MSE 288.10176883743344 Test RE 0.28573413950958265 Lambda1 0.00047599533\n",
      "10 Train Loss 259.5389 Test MSE 285.7301570120439 Test RE 0.2845556482830151 Lambda1 0.0001017865\n",
      "11 Train Loss 256.1731 Test MSE 284.84059913907663 Test RE 0.2841123523385567 Lambda1 -0.000114678114\n",
      "12 Train Loss 255.42715 Test MSE 284.2490303833061 Test RE 0.2838171708462875 Lambda1 -2.7429847e-05\n",
      "13 Train Loss 255.09793 Test MSE 283.51996231343287 Test RE 0.28345295695562056 Lambda1 0.00011951887\n",
      "14 Train Loss 254.65508 Test MSE 282.6547450639623 Test RE 0.2830201200967938 Lambda1 8.933439e-05\n",
      "15 Train Loss 254.38303 Test MSE 283.17536325701025 Test RE 0.2832806457735931 Lambda1 0.00015574605\n",
      "16 Train Loss 254.30423 Test MSE 283.3012895885622 Test RE 0.2833436253477324 Lambda1 0.00012349655\n",
      "17 Train Loss 254.24841 Test MSE 283.4102282778354 Test RE 0.2833980975960261 Lambda1 0.00012729531\n",
      "18 Train Loss 254.22563 Test MSE 283.45966443779884 Test RE 0.2834228135402374 Lambda1 0.00011549647\n",
      "19 Train Loss 254.14944 Test MSE 283.38710832230123 Test RE 0.28338653787729695 Lambda1 9.933854e-05\n",
      "20 Train Loss 254.0707 Test MSE 283.35310985861463 Test RE 0.2833695381697856 Lambda1 9.9701116e-05\n",
      "21 Train Loss 253.9884 Test MSE 283.49581234673093 Test RE 0.2834408845689124 Lambda1 8.502758e-05\n",
      "22 Train Loss 253.92497 Test MSE 283.5185301844648 Test RE 0.28345224105946276 Lambda1 9.038508e-05\n",
      "23 Train Loss 253.8637 Test MSE 283.5873388935426 Test RE 0.2834866352832557 Lambda1 8.2436054e-05\n",
      "24 Train Loss 253.8228 Test MSE 283.5734098335551 Test RE 0.28347967314091793 Lambda1 9.591307e-05\n",
      "25 Train Loss 253.77768 Test MSE 283.553466636718 Test RE 0.2834697046633948 Lambda1 0.000107170534\n",
      "26 Train Loss 253.58574 Test MSE 283.5781240840122 Test RE 0.2834820294771931 Lambda1 6.0115373e-05\n",
      "27 Train Loss 253.53561 Test MSE 283.5957083741229 Test RE 0.28349081850665425 Lambda1 3.8115973e-05\n",
      "28 Train Loss 253.48874 Test MSE 283.57902068775064 Test RE 0.28348247762679435 Lambda1 4.0828218e-05\n",
      "29 Train Loss 253.31943 Test MSE 283.40586946061615 Test RE 0.2833959182723213 Lambda1 2.0039344e-05\n",
      "30 Train Loss 253.17816 Test MSE 283.3177875705585 Test RE 0.2833518754513279 Lambda1 5.371538e-06\n",
      "31 Train Loss 253.11658 Test MSE 283.37787717050486 Test RE 0.2833819222731018 Lambda1 2.1150972e-06\n",
      "32 Train Loss 253.04825 Test MSE 283.62304758055814 Test RE 0.2835044827247663 Lambda1 2.697963e-06\n",
      "33 Train Loss 252.7405 Test MSE 283.40500105928504 Test RE 0.2833954840865691 Lambda1 1.17978725e-05\n",
      "34 Train Loss 252.6311 Test MSE 283.48704616388807 Test RE 0.28343650229284506 Lambda1 4.640362e-06\n",
      "35 Train Loss 252.4213 Test MSE 283.7249583684747 Test RE 0.2835554122429388 Lambda1 1.4039151e-05\n",
      "36 Train Loss 252.09573 Test MSE 284.0919126892009 Test RE 0.2837387205113991 Lambda1 -7.684957e-06\n",
      "37 Train Loss 252.01271 Test MSE 283.91691429702985 Test RE 0.28365131663502136 Lambda1 -8.462992e-06\n",
      "38 Train Loss 251.51874 Test MSE 284.8226681389494 Test RE 0.2841034096196842 Lambda1 1.8057488e-05\n",
      "39 Train Loss 251.44957 Test MSE 284.77258373385916 Test RE 0.28407842955764917 Lambda1 1.9517753e-05\n",
      "40 Train Loss 251.2237 Test MSE 284.8842476092093 Test RE 0.2841341199421212 Lambda1 9.188641e-06\n",
      "41 Train Loss 250.69748 Test MSE 285.03907282927895 Test RE 0.28421131823003853 Lambda1 1.4804923e-05\n",
      "42 Train Loss 250.3618 Test MSE 285.46395685694705 Test RE 0.2844230644326933 Lambda1 2.1476142e-05\n",
      "43 Train Loss 249.98694 Test MSE 285.7662871989393 Test RE 0.28457363855025986 Lambda1 3.029727e-05\n",
      "44 Train Loss 249.77669 Test MSE 286.21463858269675 Test RE 0.28479679114830314 Lambda1 2.1099595e-05\n",
      "45 Train Loss 249.38306 Test MSE 286.91399894110043 Test RE 0.28514452679320534 Lambda1 1.3578368e-05\n",
      "46 Train Loss 248.58408 Test MSE 288.35359433626496 Test RE 0.2858589902275114 Lambda1 1.5060155e-05\n",
      "47 Train Loss 248.44794 Test MSE 288.4656903674974 Test RE 0.2859145479617095 Lambda1 1.0263205e-05\n",
      "48 Train Loss 247.9813 Test MSE 289.5617243884422 Test RE 0.28645720343100833 Lambda1 5.367403e-06\n",
      "49 Train Loss 247.86993 Test MSE 289.5347640665411 Test RE 0.286443867486476 Lambda1 6.242305e-06\n",
      "50 Train Loss 247.74602 Test MSE 289.8521695264034 Test RE 0.2866008329958613 Lambda1 4.5095053e-06\n",
      "51 Train Loss 247.48709 Test MSE 290.67140001879824 Test RE 0.287005567715262 Lambda1 2.0554992e-06\n",
      "52 Train Loss 247.36064 Test MSE 290.2674511827485 Test RE 0.286806071183757 Lambda1 1.1270919e-06\n",
      "53 Train Loss 247.2607 Test MSE 290.2777995185184 Test RE 0.2868111836051826 Lambda1 9.3409005e-07\n",
      "54 Train Loss 247.0645 Test MSE 290.39946249842666 Test RE 0.2868712823240902 Lambda1 -6.7186164e-07\n",
      "55 Train Loss 247.02982 Test MSE 290.41582108760593 Test RE 0.2868793621313466 Lambda1 -3.6858276e-07\n",
      "56 Train Loss 246.94995 Test MSE 290.10120855623927 Test RE 0.28672392931236185 Lambda1 -1.0899814e-06\n",
      "57 Train Loss 246.68414 Test MSE 290.2993433614411 Test RE 0.2868218266864896 Lambda1 9.4897587e-07\n",
      "58 Train Loss 246.57307 Test MSE 290.75613181631917 Test RE 0.28704739626405373 Lambda1 -6.420357e-07\n",
      "59 Train Loss 246.53847 Test MSE 290.8053141602425 Test RE 0.28707167273711637 Lambda1 1.1020662e-06\n",
      "60 Train Loss 246.50371 Test MSE 290.71828856420694 Test RE 0.2870287153846493 Lambda1 1.4298639e-06\n",
      "61 Train Loss 246.36456 Test MSE 290.5125778592148 Test RE 0.2869271474228544 Lambda1 1.4743828e-06\n",
      "62 Train Loss 246.04312 Test MSE 290.82688542018144 Test RE 0.2870823196932417 Lambda1 1.205264e-06\n",
      "63 Train Loss 245.9309 Test MSE 290.97693843139785 Test RE 0.2871563706383727 Lambda1 1.4854347e-06\n",
      "64 Train Loss 245.84357 Test MSE 290.45771619917167 Test RE 0.2869000538578027 Lambda1 5.22683e-07\n",
      "65 Train Loss 245.46509 Test MSE 291.07431055976866 Test RE 0.28720441342883624 Lambda1 -3.962665e-07\n",
      "66 Train Loss 245.36047 Test MSE 291.4082812636629 Test RE 0.2873691314390246 Lambda1 -2.5214453e-07\n",
      "67 Train Loss 245.33711 Test MSE 291.2586839823126 Test RE 0.28729536009778944 Lambda1 -2.6146733e-07\n",
      "68 Train Loss 245.30829 Test MSE 291.225612280359 Test RE 0.2872790487965764 Lambda1 -1.9385425e-07\n",
      "69 Train Loss 245.28845 Test MSE 291.2226204091338 Test RE 0.2872775731294459 Lambda1 -2.4590062e-07\n",
      "70 Train Loss 245.0823 Test MSE 291.5064701278862 Test RE 0.2874175413058474 Lambda1 -2.0994285e-07\n",
      "71 Train Loss 245.0541 Test MSE 291.53997368542895 Test RE 0.28743405763472984 Lambda1 -7.960288e-08\n",
      "72 Train Loss 244.7973 Test MSE 291.38730693622546 Test RE 0.2873587894496134 Lambda1 8.462943e-07\n",
      "73 Train Loss 244.4813 Test MSE 291.8756334452129 Test RE 0.28759947627580973 Lambda1 2.784258e-07\n",
      "74 Train Loss 244.46046 Test MSE 291.79206224581895 Test RE 0.28755829991313103 Lambda1 3.522565e-08\n",
      "Training time: 71.91\n",
      "Training time: 71.91\n",
      "inv_HT_stan_tune18\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 836.9894 Test MSE 857.0660873866439 Test RE 0.4928290802382285 Lambda1 -0.07812525\n",
      "1 Train Loss 757.4107 Test MSE 698.8631568089361 Test RE 0.4450258551993231 Lambda1 -0.045016333\n",
      "2 Train Loss 522.2285 Test MSE 484.6928033204771 Test RE 0.37061454231721663 Lambda1 0.04846552\n",
      "3 Train Loss 349.46057 Test MSE 351.60054803272794 Test RE 0.3156559225713698 Lambda1 0.011077255\n",
      "4 Train Loss 265.18988 Test MSE 287.4344291461007 Test RE 0.28540301991612943 Lambda1 0.00151964\n",
      "5 Train Loss 259.41864 Test MSE 286.07335078742335 Test RE 0.2847264885285879 Lambda1 0.00068951945\n",
      "6 Train Loss 256.7704 Test MSE 282.5706759434263 Test RE 0.2829780280702889 Lambda1 0.00066705944\n",
      "7 Train Loss 255.81807 Test MSE 281.3950345088743 Test RE 0.2823887463856586 Lambda1 0.00042927696\n",
      "8 Train Loss 255.3665 Test MSE 281.50347874173974 Test RE 0.28244315473933335 Lambda1 0.0002774314\n",
      "9 Train Loss 254.90704 Test MSE 282.2039438644161 Test RE 0.28279433807194587 Lambda1 0.0003377226\n",
      "10 Train Loss 254.60992 Test MSE 282.6659600659933 Test RE 0.2830257347906792 Lambda1 0.00043452266\n",
      "11 Train Loss 254.39378 Test MSE 282.79175418927184 Test RE 0.2830887049020618 Lambda1 0.00056376285\n",
      "12 Train Loss 254.25171 Test MSE 282.5949374913923 Test RE 0.2829901760711685 Lambda1 0.0006965035\n",
      "13 Train Loss 254.02838 Test MSE 282.5861359586842 Test RE 0.2829857691155681 Lambda1 0.0009624274\n",
      "14 Train Loss 253.94482 Test MSE 282.4086738600184 Test RE 0.28289689862813344 Lambda1 0.0011413376\n",
      "15 Train Loss 253.80928 Test MSE 282.50408845707943 Test RE 0.28294468436659465 Lambda1 0.0015203727\n",
      "16 Train Loss 253.72646 Test MSE 281.9074583142488 Test RE 0.28264574612239585 Lambda1 0.0020232582\n",
      "17 Train Loss 253.43423 Test MSE 281.59231200342265 Test RE 0.2824877161203816 Lambda1 0.002534905\n",
      "18 Train Loss 253.30644 Test MSE 280.9421863723958 Test RE 0.2821614312375231 Lambda1 0.003685481\n",
      "19 Train Loss 253.14595 Test MSE 281.0221477032385 Test RE 0.2822015825559906 Lambda1 0.0040986636\n",
      "20 Train Loss 253.0423 Test MSE 281.36402983429656 Test RE 0.2823731888752253 Lambda1 0.003665492\n",
      "21 Train Loss 252.67944 Test MSE 280.2394322500062 Test RE 0.28180830828981296 Lambda1 0.005160708\n",
      "22 Train Loss 252.36864 Test MSE 279.567953274987 Test RE 0.2814704867339011 Lambda1 0.00769827\n",
      "23 Train Loss 251.81949 Test MSE 278.8478858535261 Test RE 0.28110776919069885 Lambda1 0.009344524\n",
      "24 Train Loss 250.80263 Test MSE 276.7256102885234 Test RE 0.28003598837510607 Lambda1 0.013299551\n",
      "25 Train Loss 249.9356 Test MSE 274.98473292003195 Test RE 0.2791537471859721 Lambda1 0.01680328\n",
      "26 Train Loss 248.67416 Test MSE 271.6996337192583 Test RE 0.27748128504971925 Lambda1 0.02473038\n",
      "27 Train Loss 246.94693 Test MSE 269.1603203009819 Test RE 0.27618156666075416 Lambda1 0.033662245\n",
      "28 Train Loss 244.95535 Test MSE 265.77264461515267 Test RE 0.27443804038727776 Lambda1 0.044165935\n",
      "29 Train Loss 242.30293 Test MSE 262.02748794101484 Test RE 0.2724975470794531 Lambda1 0.053632826\n",
      "30 Train Loss 239.0541 Test MSE 258.87989715769737 Test RE 0.2708559213033819 Lambda1 0.068323344\n",
      "31 Train Loss 235.97382 Test MSE 256.03311735234365 Test RE 0.26936256728783536 Lambda1 0.078563266\n",
      "32 Train Loss 233.03128 Test MSE 254.21659782930985 Test RE 0.26840532132112616 Lambda1 0.08484459\n",
      "33 Train Loss 229.51291 Test MSE 252.09167076983516 Test RE 0.267281203983786 Lambda1 0.0957773\n",
      "34 Train Loss 225.91563 Test MSE 247.48742544439173 Test RE 0.2648291212974332 Lambda1 0.11654391\n",
      "35 Train Loss 221.53258 Test MSE 243.2522386624871 Test RE 0.2625533678773121 Lambda1 0.14763422\n",
      "36 Train Loss 218.58482 Test MSE 242.15734237699638 Test RE 0.26196181542472197 Lambda1 0.16290823\n",
      "37 Train Loss 216.74586 Test MSE 240.39858117404484 Test RE 0.26100878241092434 Lambda1 0.18297388\n",
      "38 Train Loss 214.35216 Test MSE 240.11129305059123 Test RE 0.2608527766247395 Lambda1 0.20881473\n",
      "39 Train Loss 213.22107 Test MSE 240.47357564518134 Test RE 0.2610494912394744 Lambda1 0.22098792\n",
      "40 Train Loss 211.97092 Test MSE 240.42905621895926 Test RE 0.2610253257745672 Lambda1 0.23766734\n",
      "41 Train Loss 211.00209 Test MSE 240.24098740499164 Test RE 0.26092321597153567 Lambda1 0.24999635\n",
      "42 Train Loss 210.36891 Test MSE 239.54699565278278 Test RE 0.26054607482907527 Lambda1 0.26114064\n",
      "43 Train Loss 209.74384 Test MSE 239.66508602212275 Test RE 0.2606102880977612 Lambda1 0.27805686\n",
      "44 Train Loss 208.62802 Test MSE 238.2784130095492 Test RE 0.2598552646940525 Lambda1 0.28181288\n",
      "45 Train Loss 207.79926 Test MSE 237.0451711711862 Test RE 0.25918193447416854 Lambda1 0.2873105\n",
      "46 Train Loss 206.53384 Test MSE 234.40141448676022 Test RE 0.2577325583082151 Lambda1 0.29117548\n",
      "47 Train Loss 205.15074 Test MSE 232.12638174258367 Test RE 0.2564787696027613 Lambda1 0.30869487\n",
      "48 Train Loss 202.62448 Test MSE 226.29775454981882 Test RE 0.2532382437569724 Lambda1 0.32106885\n",
      "49 Train Loss 199.81711 Test MSE 221.2259250719389 Test RE 0.2503843501456697 Lambda1 0.334917\n",
      "50 Train Loss 196.11021 Test MSE 214.53624068764168 Test RE 0.24656958473583246 Lambda1 0.3436023\n",
      "51 Train Loss 188.08876 Test MSE 201.36722659182445 Test RE 0.23888207617381654 Lambda1 0.3422142\n",
      "52 Train Loss 182.28229 Test MSE 197.57306746325446 Test RE 0.23662086734239413 Lambda1 0.35481548\n",
      "53 Train Loss 175.73396 Test MSE 185.4778538103493 Test RE 0.22926364962532586 Lambda1 0.34589645\n",
      "54 Train Loss 166.66039 Test MSE 178.13690243539614 Test RE 0.22468088132338102 Lambda1 0.37496585\n",
      "55 Train Loss 162.4719 Test MSE 166.21963978367583 Test RE 0.21703528152476742 Lambda1 0.39908743\n",
      "56 Train Loss 157.06963 Test MSE 159.03382422769533 Test RE 0.21229214336240812 Lambda1 0.43249965\n",
      "57 Train Loss 154.85272 Test MSE 157.6442358053908 Test RE 0.21136263689218146 Lambda1 0.44670904\n",
      "58 Train Loss 148.79039 Test MSE 152.1367677779888 Test RE 0.20763772566410493 Lambda1 0.469819\n",
      "59 Train Loss 145.54372 Test MSE 147.99065137312616 Test RE 0.20478885201709207 Lambda1 0.48456115\n",
      "60 Train Loss 142.10625 Test MSE 145.81594855817877 Test RE 0.2032786108998789 Lambda1 0.49022454\n",
      "61 Train Loss 137.42844 Test MSE 140.55358471253348 Test RE 0.19957683628116613 Lambda1 0.49865487\n",
      "62 Train Loss 134.42819 Test MSE 135.8864756477483 Test RE 0.19623536980513445 Lambda1 0.5212501\n",
      "63 Train Loss 132.40509 Test MSE 132.0430462865131 Test RE 0.19344029160121257 Lambda1 0.53340787\n",
      "64 Train Loss 130.86772 Test MSE 130.23127488464078 Test RE 0.19210860424060736 Lambda1 0.5429846\n",
      "65 Train Loss 128.9841 Test MSE 127.03007134924168 Test RE 0.18973281124371685 Lambda1 0.5586786\n",
      "66 Train Loss 127.12636 Test MSE 128.22061633985135 Test RE 0.1906198400124731 Lambda1 0.56105906\n",
      "67 Train Loss 125.31472 Test MSE 124.27431489529307 Test RE 0.1876635205726747 Lambda1 0.5732593\n",
      "68 Train Loss 123.70335 Test MSE 122.45086418596772 Test RE 0.18628165935056876 Lambda1 0.5793258\n",
      "69 Train Loss 120.691956 Test MSE 121.49495812950163 Test RE 0.18555313591479455 Lambda1 0.5875724\n",
      "70 Train Loss 119.38018 Test MSE 120.31295995863117 Test RE 0.18464832669877498 Lambda1 0.5843512\n",
      "71 Train Loss 117.65842 Test MSE 115.71885967196664 Test RE 0.1810886550793143 Lambda1 0.6046944\n",
      "72 Train Loss 115.757774 Test MSE 115.47349503083649 Test RE 0.18089656741418347 Lambda1 0.60593575\n",
      "73 Train Loss 112.344315 Test MSE 111.57053561005525 Test RE 0.17781317209622768 Lambda1 0.6151556\n",
      "74 Train Loss 109.696 Test MSE 109.43614758653634 Test RE 0.17610414122330978 Lambda1 0.6278387\n",
      "Training time: 72.94\n",
      "Training time: 72.94\n",
      "inv_HT_stan_tune18\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 837.83673 Test MSE 857.809888437266 Test RE 0.4930428836407302 Lambda1 -0.16972536\n",
      "1 Train Loss 805.968 Test MSE 819.8860645873648 Test RE 0.48202095970848297 Lambda1 -0.07419894\n",
      "2 Train Loss 680.8022 Test MSE 661.917691421897 Test RE 0.4331029731425359 Lambda1 0.057560638\n",
      "3 Train Loss 464.59332 Test MSE 463.3672941589305 Test RE 0.36236968544877524 Lambda1 0.00938007\n",
      "4 Train Loss 368.2608 Test MSE 366.0935909948404 Test RE 0.32209592741178095 Lambda1 0.004710446\n",
      "5 Train Loss 300.42627 Test MSE 314.5428165495536 Test RE 0.2985582361401785 Lambda1 0.00080716645\n",
      "6 Train Loss 271.61276 Test MSE 288.21258894588516 Test RE 0.28578908891860494 Lambda1 0.00022413982\n",
      "7 Train Loss 263.61728 Test MSE 282.49212928970775 Test RE 0.28293869539371336 Lambda1 0.0007436193\n",
      "8 Train Loss 260.16306 Test MSE 280.8677780588905 Test RE 0.2821240631461622 Lambda1 0.0008770507\n",
      "9 Train Loss 258.3081 Test MSE 280.19700216633555 Test RE 0.281786973671348 Lambda1 0.0008171292\n",
      "10 Train Loss 256.03702 Test MSE 278.98627887118647 Test RE 0.2811775178398283 Lambda1 0.0031747492\n",
      "11 Train Loss 255.25616 Test MSE 278.21639909984515 Test RE 0.28078928650281143 Lambda1 0.0042600287\n",
      "12 Train Loss 253.87914 Test MSE 276.99250414317333 Test RE 0.2801709991387841 Lambda1 0.00779706\n",
      "13 Train Loss 251.90121 Test MSE 273.60680805461027 Test RE 0.27845346109705926 Lambda1 0.011906713\n",
      "14 Train Loss 249.44724 Test MSE 271.5876691237243 Test RE 0.2774241055813642 Lambda1 0.016826374\n",
      "15 Train Loss 244.62549 Test MSE 263.1224811112738 Test RE 0.273066326857058 Lambda1 0.03381241\n",
      "16 Train Loss 239.32451 Test MSE 253.03485028912212 Test RE 0.26778074210734193 Lambda1 0.05563112\n",
      "17 Train Loss 227.77383 Test MSE 237.2624280242228 Test RE 0.25930068010179136 Lambda1 0.08454322\n",
      "18 Train Loss 218.7661 Test MSE 223.21218744240767 Test RE 0.25150586808522235 Lambda1 0.11824637\n",
      "19 Train Loss 207.83257 Test MSE 209.33516916835754 Test RE 0.24356241383394525 Lambda1 0.14078215\n",
      "20 Train Loss 192.80283 Test MSE 193.2129035310118 Test RE 0.2339953537738728 Lambda1 0.17908828\n",
      "21 Train Loss 180.54744 Test MSE 187.01283224845196 Test RE 0.23021036551205803 Lambda1 0.19056693\n",
      "22 Train Loss 163.14908 Test MSE 167.21914886105736 Test RE 0.21768683991777574 Lambda1 0.2457575\n",
      "23 Train Loss 156.52405 Test MSE 156.1635701372971 Test RE 0.21036768858647933 Lambda1 0.2776825\n",
      "24 Train Loss 151.1545 Test MSE 148.27585221853468 Test RE 0.2049860869131546 Lambda1 0.28627548\n",
      "25 Train Loss 144.61694 Test MSE 138.38642619749302 Test RE 0.19803224804973818 Lambda1 0.31851932\n",
      "26 Train Loss 132.32294 Test MSE 121.80502516116056 Test RE 0.1857897599265097 Lambda1 0.37104538\n",
      "27 Train Loss 122.40932 Test MSE 115.14427003506604 Test RE 0.18063850740720971 Lambda1 0.39883083\n",
      "28 Train Loss 116.387825 Test MSE 111.46963012777243 Test RE 0.17773274592265384 Lambda1 0.41996855\n",
      "29 Train Loss 109.777504 Test MSE 105.033289987154 Test RE 0.172525245966284 Lambda1 0.4605755\n",
      "30 Train Loss 104.68327 Test MSE 100.74514790095706 Test RE 0.1689667458470388 Lambda1 0.49204388\n",
      "31 Train Loss 101.01168 Test MSE 95.25314907911147 Test RE 0.16429670052336956 Lambda1 0.51651925\n",
      "32 Train Loss 98.71029 Test MSE 92.05938381521345 Test RE 0.16151884578525422 Lambda1 0.53494215\n",
      "33 Train Loss 93.38975 Test MSE 88.26157387623967 Test RE 0.1581521150668441 Lambda1 0.55649924\n",
      "34 Train Loss 89.624725 Test MSE 86.41059205489623 Test RE 0.15648498083194176 Lambda1 0.57245815\n",
      "35 Train Loss 85.96805 Test MSE 79.91969979912791 Test RE 0.1504929282211162 Lambda1 0.59870934\n",
      "36 Train Loss 81.96026 Test MSE 73.10969429674714 Test RE 0.14393839462439065 Lambda1 0.6363813\n",
      "37 Train Loss 77.15494 Test MSE 72.21103944047127 Test RE 0.14305102331410396 Lambda1 0.65692586\n",
      "38 Train Loss 74.5535 Test MSE 68.66619866922518 Test RE 0.1394956527632859 Lambda1 0.68287814\n",
      "39 Train Loss 71.60488 Test MSE 65.36305193215793 Test RE 0.13609912456072898 Lambda1 0.68879473\n",
      "40 Train Loss 68.55399 Test MSE 64.01509447401617 Test RE 0.13468845339472194 Lambda1 0.68889266\n",
      "41 Train Loss 66.91397 Test MSE 61.32297730386883 Test RE 0.131825912661949 Lambda1 0.7049071\n",
      "42 Train Loss 65.22978 Test MSE 60.946863390514764 Test RE 0.13142102512138149 Lambda1 0.7087559\n",
      "43 Train Loss 63.3534 Test MSE 58.795646918639555 Test RE 0.12908083236063236 Lambda1 0.72321755\n",
      "44 Train Loss 61.608154 Test MSE 57.01603198696741 Test RE 0.12711232585332852 Lambda1 0.7310368\n",
      "45 Train Loss 59.003735 Test MSE 54.37312680590581 Test RE 0.1241313060523927 Lambda1 0.7486911\n",
      "46 Train Loss 56.254063 Test MSE 54.13132173831743 Test RE 0.12385498366744853 Lambda1 0.7543573\n",
      "47 Train Loss 54.05121 Test MSE 52.35522420846448 Test RE 0.1218061404959143 Lambda1 0.7609939\n",
      "48 Train Loss 52.19806 Test MSE 51.33937469134506 Test RE 0.12061864849323563 Lambda1 0.75912327\n",
      "49 Train Loss 49.97247 Test MSE 49.61829379074173 Test RE 0.11857962804919757 Lambda1 0.7731738\n",
      "50 Train Loss 48.677334 Test MSE 47.84518210045187 Test RE 0.11644163008377076 Lambda1 0.7863968\n",
      "51 Train Loss 48.174076 Test MSE 47.857476355033405 Test RE 0.11645658949065171 Lambda1 0.7914171\n",
      "52 Train Loss 47.478752 Test MSE 47.690757143357196 Test RE 0.11625356489460255 Lambda1 0.7966414\n",
      "53 Train Loss 47.056324 Test MSE 46.90207995639393 Test RE 0.11528829647055808 Lambda1 0.809316\n",
      "54 Train Loss 46.466568 Test MSE 45.941002198110716 Test RE 0.11410098751883543 Lambda1 0.81707406\n",
      "55 Train Loss 45.53456 Test MSE 44.69079418201117 Test RE 0.11253774453328105 Lambda1 0.8324571\n",
      "56 Train Loss 45.17251 Test MSE 44.304361581812586 Test RE 0.11205014220599156 Lambda1 0.83225495\n",
      "57 Train Loss 44.660217 Test MSE 44.521623121697324 Test RE 0.11232454424590047 Lambda1 0.82370746\n",
      "58 Train Loss 43.695415 Test MSE 44.04575247862713 Test RE 0.11172263950287616 Lambda1 0.83112824\n",
      "59 Train Loss 43.133434 Test MSE 43.00853751293507 Test RE 0.11039934787398221 Lambda1 0.84846956\n",
      "60 Train Loss 42.61338 Test MSE 42.9005433859962 Test RE 0.11026065477250188 Lambda1 0.85717773\n",
      "61 Train Loss 42.374626 Test MSE 43.1683988068685 Test RE 0.11060433290986854 Lambda1 0.852684\n",
      "62 Train Loss 42.031097 Test MSE 42.91364500224349 Test RE 0.11027749001974982 Lambda1 0.86169404\n",
      "63 Train Loss 41.687286 Test MSE 42.21299549160842 Test RE 0.10937353686176986 Lambda1 0.86678654\n",
      "64 Train Loss 41.171658 Test MSE 41.93988213091581 Test RE 0.10901914543654273 Lambda1 0.8675419\n",
      "65 Train Loss 40.957787 Test MSE 41.410676388482024 Test RE 0.10832914931089938 Lambda1 0.8726771\n",
      "66 Train Loss 40.52404 Test MSE 40.63322946578811 Test RE 0.10730744169025445 Lambda1 0.88785\n",
      "67 Train Loss 40.006184 Test MSE 39.9769791982356 Test RE 0.10643737560255108 Lambda1 0.8999961\n",
      "68 Train Loss 39.51475 Test MSE 39.216261469131304 Test RE 0.10541981879870858 Lambda1 0.9054001\n",
      "69 Train Loss 39.062515 Test MSE 39.39547786971509 Test RE 0.10566042593427268 Lambda1 0.91173744\n",
      "70 Train Loss 38.38078 Test MSE 38.78777199846102 Test RE 0.10484231155583897 Lambda1 0.926101\n",
      "71 Train Loss 37.91034 Test MSE 38.829089632158244 Test RE 0.10489813692325024 Lambda1 0.93654734\n",
      "72 Train Loss 37.45388 Test MSE 38.76784861597654 Test RE 0.10481538191196794 Lambda1 0.9406229\n",
      "73 Train Loss 37.141724 Test MSE 38.44898287684309 Test RE 0.1043834383864892 Lambda1 0.94590247\n",
      "74 Train Loss 36.872643 Test MSE 38.16429284971106 Test RE 0.10399627421513002 Lambda1 0.9557038\n",
      "Training time: 74.85\n",
      "Training time: 74.85\n",
      "inv_HT_stan_tune18\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 837.7962 Test MSE 857.9792943996961 Test RE 0.4930915659198795 Lambda1 -0.019098474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 835.4337 Test MSE 856.3259232198297 Test RE 0.4926162301325302 Lambda1 0.021085186\n",
      "2 Train Loss 784.85187 Test MSE 793.7737983171968 Test RE 0.47428299121878276 Lambda1 0.0796588\n",
      "3 Train Loss 532.1275 Test MSE 488.2251325393175 Test RE 0.3719625673504989 Lambda1 0.0077837985\n",
      "4 Train Loss 360.61093 Test MSE 384.12770070664544 Test RE 0.3299339329058713 Lambda1 -8.195826e-05\n",
      "5 Train Loss 308.89398 Test MSE 325.61376758773713 Test RE 0.3037669707810553 Lambda1 0.0024087422\n",
      "6 Train Loss 279.12473 Test MSE 297.5946943124872 Test RE 0.29040344410974767 Lambda1 0.002147935\n",
      "7 Train Loss 263.89426 Test MSE 286.45373527167635 Test RE 0.2849157224424212 Lambda1 0.000202977\n",
      "8 Train Loss 259.68063 Test MSE 282.6680005796007 Test RE 0.28302675634421315 Lambda1 0.00027773643\n",
      "9 Train Loss 258.14102 Test MSE 282.24057621823516 Test RE 0.28281269197222464 Lambda1 0.00016211954\n",
      "10 Train Loss 256.99878 Test MSE 281.993530414759 Test RE 0.2826888915867724 Lambda1 0.00012808561\n",
      "11 Train Loss 255.85626 Test MSE 282.1279138020302 Test RE 0.28275624094397417 Lambda1 6.399571e-05\n",
      "12 Train Loss 255.54318 Test MSE 281.5842423841018 Test RE 0.2824836684518792 Lambda1 8.88384e-05\n",
      "13 Train Loss 255.09845 Test MSE 282.00394242303014 Test RE 0.2826941103800975 Lambda1 0.00011055521\n",
      "14 Train Loss 254.78163 Test MSE 282.65792101379816 Test RE 0.28302171011994876 Lambda1 6.5977896e-05\n",
      "15 Train Loss 254.63449 Test MSE 282.99660542802036 Test RE 0.2831912195135988 Lambda1 6.831456e-05\n",
      "16 Train Loss 254.53896 Test MSE 282.9690662616143 Test RE 0.2831774401259483 Lambda1 0.000103597326\n",
      "17 Train Loss 254.47908 Test MSE 282.83281580779834 Test RE 0.28310925652407964 Lambda1 9.338769e-05\n",
      "18 Train Loss 254.452 Test MSE 282.6595782900948 Test RE 0.2830225398233628 Lambda1 9.0634334e-05\n",
      "19 Train Loss 254.38617 Test MSE 282.9186940348934 Test RE 0.28315223434414916 Lambda1 7.4924385e-05\n",
      "20 Train Loss 254.34932 Test MSE 282.97724396625796 Test RE 0.2831815319596722 Lambda1 8.958711e-05\n",
      "21 Train Loss 254.31279 Test MSE 282.8996223421384 Test RE 0.2831426904653941 Lambda1 7.1032635e-05\n",
      "22 Train Loss 254.28485 Test MSE 282.67958727862003 Test RE 0.2830325569870674 Lambda1 7.871887e-05\n",
      "23 Train Loss 254.2495 Test MSE 282.75749088769265 Test RE 0.2830715547423675 Lambda1 8.5164844e-05\n",
      "24 Train Loss 254.20729 Test MSE 283.2019639064179 Test RE 0.2832939507308101 Lambda1 9.889647e-05\n",
      "25 Train Loss 254.14575 Test MSE 283.06654376337286 Test RE 0.2832262105674965 Lambda1 0.000119892924\n",
      "26 Train Loss 254.11882 Test MSE 282.99866067339417 Test RE 0.2831922478411051 Lambda1 0.00011793464\n",
      "27 Train Loss 254.01024 Test MSE 282.83931828850314 Test RE 0.28311251092348844 Lambda1 0.00014828872\n",
      "28 Train Loss 253.94495 Test MSE 282.9829701408365 Test RE 0.2831843970993979 Lambda1 0.0001668874\n",
      "29 Train Loss 253.79489 Test MSE 282.97954360915156 Test RE 0.2831826826088647 Lambda1 0.00016837688\n",
      "30 Train Loss 253.7661 Test MSE 282.8430074121017 Test RE 0.2831143572609173 Lambda1 0.00018308703\n",
      "31 Train Loss 253.66937 Test MSE 282.8520735090734 Test RE 0.2831188946218947 Lambda1 0.00024073981\n",
      "32 Train Loss 253.58957 Test MSE 282.9255571398717 Test RE 0.2831556687084471 Lambda1 0.00020850153\n",
      "33 Train Loss 253.50859 Test MSE 283.0757185971005 Test RE 0.2832308005347508 Lambda1 0.00020315149\n",
      "34 Train Loss 253.45393 Test MSE 283.7271642172067 Test RE 0.28355651450608554 Lambda1 0.00018217896\n",
      "35 Train Loss 253.28496 Test MSE 284.0503119253076 Test RE 0.28371794522857374 Lambda1 0.0002087552\n",
      "36 Train Loss 253.02437 Test MSE 283.320842694655 Test RE 0.2833534031930489 Lambda1 0.0002214235\n",
      "37 Train Loss 252.82018 Test MSE 283.7185787911114 Test RE 0.2835522243424658 Lambda1 0.00010804006\n",
      "38 Train Loss 252.5384 Test MSE 283.8874379328706 Test RE 0.28363659185797496 Lambda1 0.00013958398\n",
      "39 Train Loss 252.01054 Test MSE 283.847481068483 Test RE 0.28361663037653023 Lambda1 9.829784e-05\n",
      "40 Train Loss 251.01125 Test MSE 284.4280534222137 Test RE 0.2839065323029121 Lambda1 5.6551977e-05\n",
      "41 Train Loss 250.84451 Test MSE 284.52975028176394 Test RE 0.2839572829620558 Lambda1 5.319122e-05\n",
      "42 Train Loss 250.54 Test MSE 284.84515917323 Test RE 0.28411462651725317 Lambda1 2.3404958e-05\n",
      "43 Train Loss 250.29674 Test MSE 285.46519269356367 Test RE 0.2844236800972085 Lambda1 2.9903098e-05\n",
      "44 Train Loss 250.15038 Test MSE 286.0567431716485 Test RE 0.2847182236959149 Lambda1 1.79841e-05\n",
      "45 Train Loss 249.9451 Test MSE 286.48138974127085 Test RE 0.28492947510481725 Lambda1 6.978038e-06\n",
      "46 Train Loss 249.73506 Test MSE 286.5653241806531 Test RE 0.28497121192466446 Lambda1 8.410277e-06\n",
      "47 Train Loss 249.44719 Test MSE 287.5500906233678 Test RE 0.2854604361673903 Lambda1 8.760563e-06\n",
      "48 Train Loss 249.3816 Test MSE 287.9929912984885 Test RE 0.2856801926161013 Lambda1 9.318222e-06\n",
      "49 Train Loss 249.27307 Test MSE 287.89822051683245 Test RE 0.2856331838976849 Lambda1 8.026563e-06\n",
      "50 Train Loss 249.19751 Test MSE 288.06152984296 Test RE 0.2857141846582074 Lambda1 9.509471e-06\n",
      "51 Train Loss 248.69801 Test MSE 287.9012098248485 Test RE 0.2856346667886669 Lambda1 4.1586823e-06\n",
      "52 Train Loss 248.48225 Test MSE 288.2512708639993 Test RE 0.28580826660142855 Lambda1 4.0866285e-06\n",
      "53 Train Loss 248.31126 Test MSE 288.09896714756053 Test RE 0.28573275017345595 Lambda1 4.2758734e-07\n",
      "54 Train Loss 248.02982 Test MSE 289.13228582489705 Test RE 0.2862447074353177 Lambda1 1.6947322e-06\n",
      "55 Train Loss 247.93924 Test MSE 289.4675209655409 Test RE 0.28641060292764947 Lambda1 9.598479e-07\n",
      "56 Train Loss 247.81963 Test MSE 288.9261634124545 Test RE 0.28614265731531685 Lambda1 9.646694e-07\n",
      "57 Train Loss 247.64226 Test MSE 289.2376603353416 Test RE 0.2862968637486547 Lambda1 1.6171839e-07\n",
      "58 Train Loss 247.5987 Test MSE 289.6457780032126 Test RE 0.28649877663343015 Lambda1 4.565021e-07\n",
      "59 Train Loss 247.52493 Test MSE 289.8475621035113 Test RE 0.28659855511660887 Lambda1 6.22736e-07\n",
      "60 Train Loss 247.38359 Test MSE 289.74544412011716 Test RE 0.2865480640161636 Lambda1 1.9540477e-07\n",
      "61 Train Loss 247.26462 Test MSE 289.88796830710686 Test RE 0.28661853105831125 Lambda1 1.2830328e-06\n",
      "62 Train Loss 247.17407 Test MSE 290.4767064282101 Test RE 0.2869094325184133 Lambda1 1.1698264e-06\n",
      "63 Train Loss 247.00165 Test MSE 291.7302813461692 Test RE 0.28752785605731157 Lambda1 6.8791377e-07\n",
      "64 Train Loss 246.99414 Test MSE 291.6790499461533 Test RE 0.2875026082473994 Lambda1 8.8662256e-07\n",
      "65 Train Loss 246.96439 Test MSE 291.48369848164145 Test RE 0.2874063149711526 Lambda1 2.608483e-07\n",
      "66 Train Loss 246.9192 Test MSE 291.57117009027974 Test RE 0.2874494357478099 Lambda1 5.930244e-07\n",
      "67 Train Loss 246.89671 Test MSE 291.4353885298145 Test RE 0.2873824968971879 Lambda1 5.502652e-07\n",
      "68 Train Loss 246.81537 Test MSE 291.1768467855378 Test RE 0.2872549954666384 Lambda1 3.3640305e-07\n",
      "69 Train Loss 246.71516 Test MSE 291.1906897571318 Test RE 0.2872618236460177 Lambda1 -1.1444173e-07\n",
      "70 Train Loss 246.70422 Test MSE 291.1576968803508 Test RE 0.2872455493234504 Lambda1 -1.1409345e-07\n",
      "71 Train Loss 246.68523 Test MSE 291.3553024652949 Test RE 0.28734300801653484 Lambda1 2.9975013e-08\n",
      "72 Train Loss 246.42691 Test MSE 291.89779028316934 Test RE 0.2876103921819926 Lambda1 6.841957e-07\n",
      "73 Train Loss 246.41199 Test MSE 291.92071756833934 Test RE 0.28762168722467385 Lambda1 6.1034046e-07\n",
      "74 Train Loss 246.40695 Test MSE 291.895608189306 Test RE 0.28760931715836174 Lambda1 7.0772256e-07\n",
      "Training time: 72.72\n",
      "Training time: 72.72\n",
      "inv_HT_stan_tune18\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 837.7151 Test MSE 857.7504328089871 Test RE 0.49302579670644764 Lambda1 -0.04786933\n",
      "1 Train Loss 836.818 Test MSE 856.379443686059 Test RE 0.49263162417681405 Lambda1 -0.015704691\n",
      "2 Train Loss 825.7666 Test MSE 841.7477279952998 Test RE 0.48840505139980783 Lambda1 0.17267573\n",
      "3 Train Loss 757.1916 Test MSE 761.4454179054135 Test RE 0.46452443035957364 Lambda1 0.44169676\n",
      "4 Train Loss 597.12006 Test MSE 573.9796716578625 Test RE 0.40330854756611867 Lambda1 0.56618416\n",
      "5 Train Loss 442.70743 Test MSE 413.1485334830913 Test RE 0.34217027465110583 Lambda1 0.6525626\n",
      "6 Train Loss 366.4512 Test MSE 352.4697991964354 Test RE 0.3160458748921626 Lambda1 0.6281084\n",
      "7 Train Loss 292.82993 Test MSE 297.28325384569683 Test RE 0.2902514470124551 Lambda1 0.6107913\n",
      "8 Train Loss 253.16138 Test MSE 265.37826039163673 Test RE 0.27423434332134183 Lambda1 0.5880758\n",
      "9 Train Loss 227.6696 Test MSE 242.2406083989664 Test RE 0.2620068494578079 Lambda1 0.51316684\n",
      "10 Train Loss 201.14423 Test MSE 221.06590734720993 Test RE 0.2502937794275769 Lambda1 0.47555244\n",
      "11 Train Loss 184.82823 Test MSE 198.04642446409568 Test RE 0.23690415275695706 Lambda1 0.43893397\n",
      "12 Train Loss 175.6739 Test MSE 182.4541734965885 Test RE 0.2273872302232463 Lambda1 0.4323608\n",
      "13 Train Loss 165.43161 Test MSE 173.81357713101852 Test RE 0.22193766852055713 Lambda1 0.42045087\n",
      "14 Train Loss 156.7565 Test MSE 161.24729266649834 Test RE 0.21376440308975395 Lambda1 0.4178005\n",
      "15 Train Loss 150.08829 Test MSE 152.44277488392513 Test RE 0.20784644148678463 Lambda1 0.4293581\n",
      "16 Train Loss 142.9669 Test MSE 141.59928421584658 Test RE 0.20031787275575402 Lambda1 0.43761867\n",
      "17 Train Loss 138.62137 Test MSE 139.86661775740674 Test RE 0.19908851494957214 Lambda1 0.4446919\n",
      "18 Train Loss 125.55789 Test MSE 127.98640638840561 Test RE 0.19044566572308186 Lambda1 0.462491\n",
      "19 Train Loss 121.604515 Test MSE 126.83693169833501 Test RE 0.18958851915562275 Lambda1 0.46851408\n",
      "20 Train Loss 118.89539 Test MSE 123.03340784065045 Test RE 0.18672423867177998 Lambda1 0.47261056\n",
      "21 Train Loss 115.34007 Test MSE 119.78083936489499 Test RE 0.18423954256187877 Lambda1 0.4748008\n",
      "22 Train Loss 112.19281 Test MSE 116.24728361518602 Test RE 0.18150164993409582 Lambda1 0.48469663\n",
      "23 Train Loss 109.49525 Test MSE 110.6320962769854 Test RE 0.17706378407975776 Lambda1 0.49146533\n",
      "24 Train Loss 105.69341 Test MSE 109.40754928418801 Test RE 0.17608112959207609 Lambda1 0.49284896\n",
      "25 Train Loss 103.76895 Test MSE 105.15347677781004 Test RE 0.17262392576253607 Lambda1 0.50063336\n",
      "26 Train Loss 100.922386 Test MSE 103.15194776577144 Test RE 0.17097313973325431 Lambda1 0.5092187\n",
      "27 Train Loss 98.01533 Test MSE 99.77005601392105 Test RE 0.16814706017172468 Lambda1 0.5194918\n",
      "28 Train Loss 96.060486 Test MSE 98.0262684134532 Test RE 0.166671139998011 Lambda1 0.5256575\n",
      "29 Train Loss 92.5304 Test MSE 92.94545534610332 Test RE 0.16229429374379487 Lambda1 0.5478721\n",
      "30 Train Loss 91.549644 Test MSE 90.85253101833217 Test RE 0.16045663712170574 Lambda1 0.56067055\n",
      "31 Train Loss 89.05158 Test MSE 89.79947684236906 Test RE 0.15952401580950634 Lambda1 0.5680435\n",
      "32 Train Loss 86.563614 Test MSE 86.9537599837796 Test RE 0.15697603446060998 Lambda1 0.57480186\n",
      "33 Train Loss 85.143745 Test MSE 84.96013713620572 Test RE 0.1551660744351724 Lambda1 0.5812372\n",
      "34 Train Loss 82.411514 Test MSE 81.24708183843562 Test RE 0.15173754607192325 Lambda1 0.60168916\n",
      "35 Train Loss 81.54917 Test MSE 79.3240101607211 Test RE 0.14993102200710848 Lambda1 0.6106573\n",
      "36 Train Loss 80.10468 Test MSE 76.73513494896636 Test RE 0.14746409886358267 Lambda1 0.63023895\n",
      "37 Train Loss 77.623344 Test MSE 74.71127069260531 Test RE 0.1455064456421283 Lambda1 0.63799876\n",
      "38 Train Loss 76.19257 Test MSE 72.37447739931933 Test RE 0.14321281819479642 Lambda1 0.6472429\n",
      "39 Train Loss 73.69739 Test MSE 69.74624859472287 Test RE 0.1405884352742743 Lambda1 0.66093826\n",
      "40 Train Loss 72.08263 Test MSE 68.42437576156834 Test RE 0.13924980402579146 Lambda1 0.67491865\n",
      "41 Train Loss 70.5406 Test MSE 67.07665698582602 Test RE 0.13787161902338488 Lambda1 0.68442005\n",
      "42 Train Loss 68.62038 Test MSE 65.73711199112012 Test RE 0.13648800342112133 Lambda1 0.6922155\n",
      "43 Train Loss 66.63268 Test MSE 64.65961397091836 Test RE 0.13536479327735676 Lambda1 0.698928\n",
      "44 Train Loss 64.93894 Test MSE 61.945894782467946 Test RE 0.13249376333042934 Lambda1 0.71243477\n",
      "45 Train Loss 62.969017 Test MSE 60.20292751837991 Test RE 0.13061648003648474 Lambda1 0.724004\n",
      "46 Train Loss 62.33174 Test MSE 59.59015838316753 Test RE 0.12995004687542788 Lambda1 0.72893983\n",
      "47 Train Loss 61.745247 Test MSE 58.11391710924786 Test RE 0.12833031061035816 Lambda1 0.7323396\n",
      "48 Train Loss 60.420906 Test MSE 56.42676241992614 Test RE 0.12645375699245828 Lambda1 0.73940796\n",
      "49 Train Loss 58.990128 Test MSE 56.95720598733127 Test RE 0.12704673518444165 Lambda1 0.7390892\n",
      "50 Train Loss 57.65494 Test MSE 56.8470177938582 Test RE 0.12692378474194085 Lambda1 0.7416166\n",
      "51 Train Loss 56.29429 Test MSE 55.02604379367704 Test RE 0.12487437148793203 Lambda1 0.7506231\n",
      "52 Train Loss 55.541584 Test MSE 53.83800859287974 Test RE 0.12351897080927743 Lambda1 0.75844866\n",
      "53 Train Loss 54.40054 Test MSE 53.68692644015885 Test RE 0.1233455373646424 Lambda1 0.76357305\n",
      "54 Train Loss 53.781597 Test MSE 53.73022355543949 Test RE 0.12339526483084057 Lambda1 0.76743424\n",
      "55 Train Loss 53.047535 Test MSE 52.965613291408495 Test RE 0.1225141280150633 Lambda1 0.77420026\n",
      "56 Train Loss 51.966824 Test MSE 51.24408989022533 Test RE 0.1205066636696399 Lambda1 0.78624177\n",
      "57 Train Loss 50.929413 Test MSE 50.48351747480465 Test RE 0.11960903159477776 Lambda1 0.8004068\n",
      "58 Train Loss 50.339645 Test MSE 50.51017583289569 Test RE 0.11964060783713178 Lambda1 0.80377436\n",
      "59 Train Loss 49.888424 Test MSE 50.39288967099121 Test RE 0.11950162254622529 Lambda1 0.8042967\n",
      "60 Train Loss 48.52294 Test MSE 47.82631951211811 Test RE 0.11641867472069266 Lambda1 0.82563156\n",
      "61 Train Loss 48.277805 Test MSE 47.613334768507926 Test RE 0.11615916208421061 Lambda1 0.82892853\n",
      "62 Train Loss 47.543037 Test MSE 48.3611867165051 Test RE 0.11706785076991066 Lambda1 0.8299735\n",
      "63 Train Loss 47.036648 Test MSE 48.2145991140075 Test RE 0.11689029392699182 Lambda1 0.83079123\n",
      "64 Train Loss 46.01848 Test MSE 47.38456331954885 Test RE 0.11587976664509422 Lambda1 0.829414\n",
      "65 Train Loss 45.21334 Test MSE 45.943406667613395 Test RE 0.11410397340009516 Lambda1 0.83724385\n",
      "66 Train Loss 44.672684 Test MSE 45.340141665451846 Test RE 0.11335237055056828 Lambda1 0.84291977\n",
      "67 Train Loss 44.470695 Test MSE 45.16720065787034 Test RE 0.11313598390737996 Lambda1 0.8436361\n",
      "68 Train Loss 43.76153 Test MSE 44.390669423233604 Test RE 0.1121592296671171 Lambda1 0.85190254\n",
      "69 Train Loss 43.53123 Test MSE 44.00221235643857 Test RE 0.11166740579883216 Lambda1 0.85556495\n",
      "70 Train Loss 43.4 Test MSE 43.73968199644385 Test RE 0.11133378686541344 Lambda1 0.85751414\n",
      "71 Train Loss 43.120296 Test MSE 43.47189557530134 Test RE 0.11099245552636582 Lambda1 0.8597054\n",
      "72 Train Loss 42.819885 Test MSE 43.18233977381873 Test RE 0.11062219096023719 Lambda1 0.8630166\n",
      "73 Train Loss 41.824894 Test MSE 42.13722243359363 Test RE 0.10927532907432504 Lambda1 0.8766103\n",
      "74 Train Loss 41.232365 Test MSE 41.655953834077074 Test RE 0.10864949505926241 Lambda1 0.88447785\n",
      "Training time: 72.80\n",
      "Training time: 72.80\n",
      "inv_HT_stan_tune18\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 837.8247 Test MSE 858.0187407285027 Test RE 0.49310290094195297 Lambda1 0.035708003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 837.0439 Test MSE 855.8502717870025 Test RE 0.4924793977909462 Lambda1 0.06561995\n",
      "2 Train Loss 826.42224 Test MSE 841.6668072679465 Test RE 0.48838157462607296 Lambda1 0.30054608\n",
      "3 Train Loss 786.5827 Test MSE 793.3747480687141 Test RE 0.4741637591789518 Lambda1 0.36717117\n",
      "4 Train Loss 722.98413 Test MSE 712.1695984240425 Test RE 0.4492425521097511 Lambda1 0.5011943\n",
      "5 Train Loss 667.0707 Test MSE 660.8949489019664 Test RE 0.43276824567712724 Lambda1 0.5710381\n",
      "6 Train Loss 601.6489 Test MSE 599.0511857683189 Test RE 0.412022693332667 Lambda1 0.6249804\n",
      "7 Train Loss 532.8047 Test MSE 528.1339037417541 Test RE 0.38686656258490054 Lambda1 0.6324048\n",
      "8 Train Loss 460.34824 Test MSE 468.2323973166665 Test RE 0.36426705985754854 Lambda1 0.6943277\n",
      "9 Train Loss 428.9007 Test MSE 432.59100865921573 Test RE 0.3501288641227682 Lambda1 0.7256742\n",
      "10 Train Loss 375.74643 Test MSE 387.5381848051279 Test RE 0.3313953581726829 Lambda1 0.6881991\n",
      "11 Train Loss 341.4378 Test MSE 354.76432047461185 Test RE 0.3170729098623299 Lambda1 0.7121092\n",
      "12 Train Loss 310.7307 Test MSE 327.6705728551911 Test RE 0.3047248632587078 Lambda1 0.72642714\n",
      "13 Train Loss 273.1796 Test MSE 297.2494087442661 Test RE 0.2902349242699992 Lambda1 0.7370678\n",
      "14 Train Loss 255.03952 Test MSE 278.6179324048765 Test RE 0.2809918367477265 Lambda1 0.73365873\n",
      "15 Train Loss 242.79243 Test MSE 266.00497216488566 Test RE 0.2745579654263406 Lambda1 0.70558196\n",
      "16 Train Loss 237.41684 Test MSE 259.33229358992725 Test RE 0.2710924803562701 Lambda1 0.7032237\n",
      "17 Train Loss 224.53914 Test MSE 241.51167847717213 Test RE 0.26161234804903144 Lambda1 0.69595563\n",
      "18 Train Loss 216.9676 Test MSE 230.46749104137254 Test RE 0.25556066385638704 Lambda1 0.70763165\n",
      "19 Train Loss 205.35718 Test MSE 218.1600047623552 Test RE 0.24864328666759553 Lambda1 0.71381783\n",
      "20 Train Loss 192.45805 Test MSE 199.8029557983402 Test RE 0.23795241945601994 Lambda1 0.71981883\n",
      "21 Train Loss 186.26164 Test MSE 190.3769050855814 Test RE 0.2322717017302406 Lambda1 0.7201908\n",
      "22 Train Loss 176.65408 Test MSE 183.60475369814304 Test RE 0.22810307045747305 Lambda1 0.7099912\n",
      "23 Train Loss 162.71683 Test MSE 165.02027864501687 Test RE 0.21625085273809697 Lambda1 0.69683653\n",
      "24 Train Loss 149.94315 Test MSE 148.83123798614534 Test RE 0.2053696286236611 Lambda1 0.6788482\n",
      "25 Train Loss 134.17876 Test MSE 135.08690598387992 Test RE 0.19565718371639745 Lambda1 0.68180597\n",
      "26 Train Loss 125.99774 Test MSE 129.7494233010167 Test RE 0.19175287702167512 Lambda1 0.6819419\n",
      "27 Train Loss 118.48006 Test MSE 123.59081453579383 Test RE 0.18714674066922168 Lambda1 0.67747116\n",
      "28 Train Loss 111.09304 Test MSE 112.27112866533527 Test RE 0.17837057608156778 Lambda1 0.66738486\n",
      "29 Train Loss 105.50144 Test MSE 105.5080051437434 Test RE 0.17291468448235445 Lambda1 0.66575044\n",
      "30 Train Loss 102.783615 Test MSE 100.75287554450023 Test RE 0.16897322600897238 Lambda1 0.6625996\n",
      "31 Train Loss 96.05084 Test MSE 92.17399139687308 Test RE 0.16161935442330086 Lambda1 0.6648147\n",
      "32 Train Loss 91.77718 Test MSE 91.04860211811999 Test RE 0.16062968652993714 Lambda1 0.6641149\n",
      "33 Train Loss 88.82419 Test MSE 88.78381635858086 Test RE 0.1586193168917626 Lambda1 0.6718334\n",
      "34 Train Loss 85.576164 Test MSE 83.3173013447991 Test RE 0.15365856320276922 Lambda1 0.67622113\n",
      "35 Train Loss 82.0443 Test MSE 82.24403036774227 Test RE 0.15266566120574707 Lambda1 0.68429846\n",
      "36 Train Loss 78.35823 Test MSE 79.06634875902088 Test RE 0.14968731988550324 Lambda1 0.6949399\n",
      "37 Train Loss 76.85113 Test MSE 74.12269943832884 Test RE 0.14493216650940183 Lambda1 0.70823663\n",
      "38 Train Loss 72.85258 Test MSE 70.67296950107081 Test RE 0.14151935497823265 Lambda1 0.70487404\n",
      "39 Train Loss 69.84726 Test MSE 66.33273391249728 Test RE 0.13710494510519325 Lambda1 0.7096561\n",
      "40 Train Loss 66.91698 Test MSE 64.72636255977292 Test RE 0.13543464428442564 Lambda1 0.70932657\n",
      "41 Train Loss 64.119736 Test MSE 59.81508192783927 Test RE 0.13019506465005215 Lambda1 0.70885044\n",
      "42 Train Loss 61.729546 Test MSE 59.294403575048364 Test RE 0.12962716505618704 Lambda1 0.7101077\n",
      "43 Train Loss 57.74414 Test MSE 54.92929772508399 Test RE 0.12476454695249942 Lambda1 0.7177405\n",
      "44 Train Loss 56.01768 Test MSE 53.15643411124572 Test RE 0.12273462227883104 Lambda1 0.72413445\n",
      "45 Train Loss 54.06106 Test MSE 51.80860672570275 Test RE 0.12116861036729748 Lambda1 0.73311454\n",
      "46 Train Loss 52.471462 Test MSE 50.40341529014221 Test RE 0.11951410211329222 Lambda1 0.73586327\n",
      "47 Train Loss 50.863174 Test MSE 48.2252647701176 Test RE 0.11690322198987337 Lambda1 0.73580045\n",
      "48 Train Loss 49.44818 Test MSE 45.149008412522406 Test RE 0.11311319740703658 Lambda1 0.7388399\n",
      "49 Train Loss 47.55114 Test MSE 44.784988611266726 Test RE 0.11265627955910607 Lambda1 0.7367179\n",
      "50 Train Loss 46.21109 Test MSE 44.27098456519015 Test RE 0.11200792735443924 Lambda1 0.740414\n",
      "51 Train Loss 44.837734 Test MSE 43.54475593175879 Test RE 0.11108543012643259 Lambda1 0.7385756\n",
      "52 Train Loss 43.58796 Test MSE 41.6745394242539 Test RE 0.1086737303679471 Lambda1 0.74061203\n",
      "53 Train Loss 42.988503 Test MSE 41.18846765330903 Test RE 0.10803811250218123 Lambda1 0.7421222\n",
      "54 Train Loss 42.1394 Test MSE 41.72239941191348 Test RE 0.10873611415251846 Lambda1 0.74262273\n",
      "55 Train Loss 41.65939 Test MSE 41.35974745261537 Test RE 0.10826251449445558 Lambda1 0.74490213\n",
      "56 Train Loss 40.944656 Test MSE 40.868286496430855 Test RE 0.10761737269668914 Lambda1 0.744926\n",
      "57 Train Loss 40.535404 Test MSE 40.395004339826926 Test RE 0.10699241741506685 Lambda1 0.7446589\n",
      "58 Train Loss 39.806534 Test MSE 39.44633927903151 Test RE 0.10572861021642196 Lambda1 0.7484554\n",
      "59 Train Loss 39.544167 Test MSE 39.18476018760416 Test RE 0.1053774699563981 Lambda1 0.75055736\n",
      "60 Train Loss 38.74329 Test MSE 38.12925674448186 Test RE 0.103948527214449 Lambda1 0.75536436\n",
      "61 Train Loss 38.40825 Test MSE 37.910383879899726 Test RE 0.10364975065854481 Lambda1 0.7554889\n",
      "62 Train Loss 38.154995 Test MSE 37.96175238743777 Test RE 0.10371994950723215 Lambda1 0.7567349\n",
      "63 Train Loss 37.823734 Test MSE 37.674258259687626 Test RE 0.1033264541357095 Lambda1 0.759614\n",
      "64 Train Loss 37.15798 Test MSE 37.25643969409566 Test RE 0.10275189636126893 Lambda1 0.76072484\n",
      "65 Train Loss 36.700794 Test MSE 37.441745387788664 Test RE 0.10300711256566251 Lambda1 0.76261765\n",
      "66 Train Loss 36.292473 Test MSE 36.989624679329786 Test RE 0.10238330222625053 Lambda1 0.7650622\n",
      "67 Train Loss 36.07094 Test MSE 37.019211084329015 Test RE 0.10242424003503928 Lambda1 0.76473886\n",
      "68 Train Loss 35.7556 Test MSE 36.743134543057785 Test RE 0.10204160293243886 Lambda1 0.76459837\n",
      "69 Train Loss 35.359154 Test MSE 36.7568623999443 Test RE 0.10206066338715464 Lambda1 0.76920134\n",
      "70 Train Loss 34.953304 Test MSE 36.36118157167421 Test RE 0.10150984492507069 Lambda1 0.77095854\n",
      "71 Train Loss 34.660477 Test MSE 36.20520160337343 Test RE 0.10129188557083312 Lambda1 0.7751194\n",
      "72 Train Loss 34.346016 Test MSE 35.91148335850473 Test RE 0.1008801786949239 Lambda1 0.77992845\n",
      "73 Train Loss 34.13118 Test MSE 35.61427182937569 Test RE 0.10046185782079274 Lambda1 0.78369915\n",
      "74 Train Loss 33.81914 Test MSE 35.31101039639351 Test RE 0.10003321869376328 Lambda1 0.78426504\n",
      "Training time: 72.93\n",
      "Training time: 72.93\n",
      "inv_HT_stan_tune18\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 837.7138 Test MSE 857.6751357777549 Test RE 0.4930041562664426 Lambda1 -0.009700872\n",
      "1 Train Loss 833.44305 Test MSE 850.9802823094456 Test RE 0.49107623702338193 Lambda1 0.06211167\n",
      "2 Train Loss 796.91235 Test MSE 796.7804632670416 Test RE 0.4751803893417697 Lambda1 0.27399814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Train Loss 731.19574 Test MSE 736.0197441498415 Test RE 0.45670304054824995 Lambda1 0.2840815\n",
      "4 Train Loss 685.3003 Test MSE 688.6984176164195 Test RE 0.44177762205221427 Lambda1 0.256422\n",
      "5 Train Loss 652.0711 Test MSE 647.9874024955213 Test RE 0.4285213377894686 Lambda1 0.25823098\n",
      "6 Train Loss 624.5602 Test MSE 623.5264550180901 Test RE 0.42035538259387784 Lambda1 0.27000624\n",
      "7 Train Loss 583.78503 Test MSE 581.5313112220548 Test RE 0.40595296922611535 Lambda1 0.2394622\n",
      "8 Train Loss 513.3042 Test MSE 502.51030165912044 Test RE 0.37736503280105343 Lambda1 0.21820319\n",
      "9 Train Loss 434.22446 Test MSE 434.754300850791 Test RE 0.3510032309572814 Lambda1 0.18180671\n",
      "10 Train Loss 332.7139 Test MSE 341.4970083081553 Test RE 0.3110875440692418 Lambda1 0.110889554\n",
      "11 Train Loss 279.7017 Test MSE 295.4275882785296 Test RE 0.2893441426931594 Lambda1 0.070653036\n",
      "12 Train Loss 260.10376 Test MSE 279.61924294743 Test RE 0.28149630490587935 Lambda1 0.056025658\n",
      "13 Train Loss 246.67874 Test MSE 266.8667543050362 Test RE 0.27500235152550073 Lambda1 0.083289355\n",
      "14 Train Loss 235.1412 Test MSE 252.40569520947142 Test RE 0.2674476250084235 Lambda1 0.12454958\n",
      "15 Train Loss 224.02835 Test MSE 240.72867283732393 Test RE 0.26118791672084174 Lambda1 0.14080822\n",
      "16 Train Loss 211.55351 Test MSE 222.56480740552558 Test RE 0.25114088329710943 Lambda1 0.17798321\n",
      "17 Train Loss 202.32542 Test MSE 208.38920034466472 Test RE 0.24301147121556618 Lambda1 0.21664438\n",
      "18 Train Loss 192.21124 Test MSE 195.3802834630254 Test RE 0.23530412373477302 Lambda1 0.25743482\n",
      "19 Train Loss 181.29364 Test MSE 182.8576973909916 Test RE 0.22763854128375877 Lambda1 0.29945558\n",
      "20 Train Loss 171.53656 Test MSE 171.54254071752337 Test RE 0.2204829899498503 Lambda1 0.32526666\n",
      "21 Train Loss 163.32626 Test MSE 161.83271145501328 Test RE 0.21415209431162666 Lambda1 0.35880715\n",
      "22 Train Loss 158.52406 Test MSE 156.92960609029063 Test RE 0.21088302027627975 Lambda1 0.36930427\n",
      "23 Train Loss 153.67041 Test MSE 153.92862957294085 Test RE 0.20885692137736683 Lambda1 0.3871624\n",
      "24 Train Loss 147.05287 Test MSE 148.37298024069895 Test RE 0.2050532139384635 Lambda1 0.42299154\n",
      "25 Train Loss 141.97969 Test MSE 142.43243447098857 Test RE 0.20090632950820955 Lambda1 0.45209685\n",
      "26 Train Loss 139.31133 Test MSE 140.8603491257593 Test RE 0.19979451020990432 Lambda1 0.46186253\n",
      "27 Train Loss 134.47995 Test MSE 136.8929463399342 Test RE 0.19696075759743709 Lambda1 0.47159007\n",
      "28 Train Loss 129.37762 Test MSE 132.44519000156532 Test RE 0.19373463371452068 Lambda1 0.48476034\n",
      "29 Train Loss 127.52307 Test MSE 131.13115169049604 Test RE 0.1927711811513135 Lambda1 0.48417652\n",
      "30 Train Loss 125.22247 Test MSE 128.5070729246546 Test RE 0.19083265229722346 Lambda1 0.49287423\n",
      "31 Train Loss 122.32734 Test MSE 125.8207128357132 Test RE 0.18882749913641747 Lambda1 0.50477326\n",
      "32 Train Loss 119.59599 Test MSE 124.10949213520782 Test RE 0.18753903192778007 Lambda1 0.52436066\n",
      "33 Train Loss 117.17439 Test MSE 120.42914252604376 Test RE 0.18473745982318432 Lambda1 0.54694986\n",
      "34 Train Loss 114.23096 Test MSE 116.84214668818932 Test RE 0.18196544940244166 Lambda1 0.5626087\n",
      "35 Train Loss 111.63467 Test MSE 112.85743676285455 Test RE 0.17883571762682066 Lambda1 0.5758473\n",
      "36 Train Loss 108.79903 Test MSE 109.54548477426597 Test RE 0.17619209170879566 Lambda1 0.5923855\n",
      "37 Train Loss 105.84959 Test MSE 108.36411822921987 Test RE 0.1752394661012731 Lambda1 0.60730714\n",
      "38 Train Loss 104.16519 Test MSE 103.9834270645391 Test RE 0.1716608402511448 Lambda1 0.626673\n",
      "39 Train Loss 101.53148 Test MSE 102.40967880396897 Test RE 0.17035687806017902 Lambda1 0.639466\n",
      "40 Train Loss 99.710556 Test MSE 99.58586458795408 Test RE 0.16799177533127332 Lambda1 0.6571103\n",
      "41 Train Loss 96.58275 Test MSE 95.37319461064004 Test RE 0.16440019775521578 Lambda1 0.690474\n",
      "42 Train Loss 95.34384 Test MSE 94.01250837620194 Test RE 0.1632232386550226 Lambda1 0.6979928\n",
      "43 Train Loss 93.31716 Test MSE 93.50538097214724 Test RE 0.16278240951351103 Lambda1 0.69954175\n",
      "44 Train Loss 90.6405 Test MSE 91.15409015250903 Test RE 0.1607227115979066 Lambda1 0.722605\n",
      "45 Train Loss 89.27868 Test MSE 88.17678805095754 Test RE 0.15807613478002958 Lambda1 0.7398267\n",
      "46 Train Loss 87.75598 Test MSE 85.4643418580559 Test RE 0.1556258179047021 Lambda1 0.77028507\n",
      "47 Train Loss 86.24183 Test MSE 85.02030299614611 Test RE 0.15522100636204866 Lambda1 0.7811031\n",
      "48 Train Loss 85.26944 Test MSE 84.08946923956532 Test RE 0.15436895928721805 Lambda1 0.79724824\n",
      "49 Train Loss 83.07394 Test MSE 80.33904765723787 Test RE 0.15088723850288885 Lambda1 0.8386857\n",
      "50 Train Loss 81.83477 Test MSE 76.74220584523268 Test RE 0.14747089287865428 Lambda1 0.8754238\n",
      "51 Train Loss 79.7257 Test MSE 75.89985203220269 Test RE 0.14665930927220674 Lambda1 0.89258426\n",
      "52 Train Loss 77.2127 Test MSE 73.01110415002324 Test RE 0.1438413097233825 Lambda1 0.90758777\n",
      "53 Train Loss 76.21928 Test MSE 72.6121312215521 Test RE 0.14344775722758268 Lambda1 0.91166466\n",
      "54 Train Loss 73.99775 Test MSE 70.46174905051521 Test RE 0.14130771713438936 Lambda1 0.935702\n",
      "55 Train Loss 72.58424 Test MSE 71.08730003158509 Test RE 0.1419335876094239 Lambda1 0.93840283\n",
      "56 Train Loss 70.55581 Test MSE 69.60925491491564 Test RE 0.14045029742362375 Lambda1 0.95045435\n",
      "57 Train Loss 68.70141 Test MSE 67.75676560551827 Test RE 0.13856881529146697 Lambda1 0.9628958\n",
      "58 Train Loss 67.74759 Test MSE 67.71477125339001 Test RE 0.13852586748032328 Lambda1 0.9618211\n",
      "59 Train Loss 67.20585 Test MSE 67.36235494319914 Test RE 0.1381649236390095 Lambda1 0.96236336\n",
      "60 Train Loss 66.241745 Test MSE 66.09381748069642 Test RE 0.13685781091386937 Lambda1 0.96930027\n",
      "61 Train Loss 64.96581 Test MSE 64.64526202858566 Test RE 0.13534976956005146 Lambda1 0.98216605\n",
      "62 Train Loss 64.36482 Test MSE 64.55704340493516 Test RE 0.13525738501076454 Lambda1 0.98747516\n",
      "63 Train Loss 63.279934 Test MSE 63.17729439133049 Test RE 0.13380418039401643 Lambda1 1.00151\n",
      "64 Train Loss 62.348667 Test MSE 62.00563052771297 Test RE 0.13255763121531872 Lambda1 1.0100567\n",
      "65 Train Loss 60.194653 Test MSE 59.87673871763688 Test RE 0.13026214925394444 Lambda1 1.0276194\n",
      "66 Train Loss 58.95365 Test MSE 59.40121074890457 Test RE 0.12974386141469674 Lambda1 1.0341555\n",
      "67 Train Loss 58.024014 Test MSE 58.4857328352481 Test RE 0.12874018790016795 Lambda1 1.0466636\n",
      "68 Train Loss 56.974064 Test MSE 57.08242343420512 Test RE 0.12718631132455857 Lambda1 1.0656792\n",
      "69 Train Loss 56.052826 Test MSE 55.538702561511215 Test RE 0.1254547287087736 Lambda1 1.0874815\n",
      "70 Train Loss 54.882248 Test MSE 54.314269316122015 Test RE 0.12406410341267007 Lambda1 1.1038177\n",
      "71 Train Loss 54.101154 Test MSE 53.315350741664155 Test RE 0.12291794925469478 Lambda1 1.1170062\n",
      "72 Train Loss 53.371067 Test MSE 53.05657948004445 Test RE 0.12261928928460497 Lambda1 1.1167868\n",
      "73 Train Loss 52.48953 Test MSE 52.23415709544394 Test RE 0.1216652256873866 Lambda1 1.1167216\n",
      "74 Train Loss 51.3042 Test MSE 50.92370464073196 Test RE 0.12012936071131253 Lambda1 1.1367623\n",
      "Training time: 72.82\n",
      "Training time: 72.82\n",
      "inv_HT_stan_tune18\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 837.2227 Test MSE 856.9197067977962 Test RE 0.492786992648623 Lambda1 -0.15275547\n",
      "1 Train Loss 824.4445 Test MSE 838.710383823098 Test RE 0.4875230799830724 Lambda1 -0.13365954\n",
      "2 Train Loss 769.6484 Test MSE 761.106508212627 Test RE 0.46442104188949435 Lambda1 0.014159168\n",
      "3 Train Loss 662.6877 Test MSE 652.4606997770841 Test RE 0.4299979146677677 Lambda1 0.06628755\n",
      "4 Train Loss 563.3024 Test MSE 535.2607224146512 Test RE 0.3894680701297584 Lambda1 0.018450378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 373.25586 Test MSE 382.4011081713278 Test RE 0.3291915976855297 Lambda1 0.0051577045\n",
      "6 Train Loss 291.9304 Test MSE 303.64384504007825 Test RE 0.2933400838308539 Lambda1 0.0062104766\n",
      "7 Train Loss 270.59097 Test MSE 289.56193119428985 Test RE 0.2864573057252849 Lambda1 0.0048975097\n",
      "8 Train Loss 260.14862 Test MSE 283.1739069280182 Test RE 0.2832799173374343 Lambda1 0.0030993044\n",
      "9 Train Loss 256.41153 Test MSE 280.58734351041375 Test RE 0.2819831835239077 Lambda1 0.0030869683\n",
      "10 Train Loss 255.2323 Test MSE 280.1664618826817 Test RE 0.28177161646097 Lambda1 0.002828732\n",
      "11 Train Loss 254.56898 Test MSE 280.8379548388488 Test RE 0.2821090844408172 Lambda1 0.0029080852\n",
      "12 Train Loss 253.91753 Test MSE 280.1312061316569 Test RE 0.28175388703248844 Lambda1 0.0035048758\n",
      "13 Train Loss 253.40335 Test MSE 279.8131341414428 Test RE 0.28159388437829386 Lambda1 0.0047858963\n",
      "14 Train Loss 252.80634 Test MSE 279.25837026690823 Test RE 0.28131459866486463 Lambda1 0.0059618517\n",
      "15 Train Loss 251.78607 Test MSE 278.00689281681457 Test RE 0.2806835447154997 Lambda1 0.008726104\n",
      "16 Train Loss 250.6859 Test MSE 275.99437866891185 Test RE 0.279665754080261 Lambda1 0.01362383\n",
      "17 Train Loss 249.6928 Test MSE 274.9736063334492 Test RE 0.2791480994910527 Lambda1 0.014440568\n",
      "18 Train Loss 248.04509 Test MSE 271.80488825308925 Test RE 0.2775350269979269 Lambda1 0.022890665\n",
      "19 Train Loss 246.18738 Test MSE 268.87637807445276 Test RE 0.27603585368685063 Lambda1 0.028764049\n",
      "20 Train Loss 244.1099 Test MSE 265.33596434071313 Test RE 0.27421248868273307 Lambda1 0.03757073\n",
      "21 Train Loss 241.50616 Test MSE 262.9688139878467 Test RE 0.27298657797915726 Lambda1 0.046489347\n",
      "22 Train Loss 237.8127 Test MSE 257.5533819119096 Test RE 0.27016108953522705 Lambda1 0.06543045\n",
      "23 Train Loss 234.292 Test MSE 255.49947699970514 Test RE 0.26908170965024664 Lambda1 0.07502474\n",
      "24 Train Loss 231.39757 Test MSE 253.18143496510865 Test RE 0.26785829440914116 Lambda1 0.082695924\n",
      "25 Train Loss 226.71043 Test MSE 246.82307126349724 Test RE 0.26447342935519874 Lambda1 0.10457181\n",
      "26 Train Loss 222.30757 Test MSE 238.49396273234225 Test RE 0.2599727723357579 Lambda1 0.12643297\n",
      "27 Train Loss 218.27278 Test MSE 225.7112442306795 Test RE 0.252909863996873 Lambda1 0.14969347\n",
      "28 Train Loss 209.35213 Test MSE 212.31542734390953 Test RE 0.24529005845916022 Lambda1 0.16713898\n",
      "29 Train Loss 200.68575 Test MSE 204.91706100470526 Test RE 0.2409784628891882 Lambda1 0.18761134\n",
      "30 Train Loss 192.69003 Test MSE 193.7588879917035 Test RE 0.23432573465660278 Lambda1 0.2057542\n",
      "31 Train Loss 184.47171 Test MSE 188.71320508363948 Test RE 0.23125456577134323 Lambda1 0.22629163\n",
      "32 Train Loss 177.12686 Test MSE 181.56646338373133 Test RE 0.2268333923377842 Lambda1 0.2557492\n",
      "33 Train Loss 168.63347 Test MSE 168.34609114550747 Test RE 0.21841913813333266 Lambda1 0.30642837\n",
      "34 Train Loss 159.01721 Test MSE 157.13514062451839 Test RE 0.21102107440648893 Lambda1 0.3516409\n",
      "35 Train Loss 152.34134 Test MSE 151.44671149348903 Test RE 0.20716629274859272 Lambda1 0.3667311\n",
      "36 Train Loss 142.50327 Test MSE 142.0266036375617 Test RE 0.20061990547579342 Lambda1 0.39997807\n",
      "37 Train Loss 136.97041 Test MSE 137.43024894429698 Test RE 0.19734691295203535 Lambda1 0.41313443\n",
      "38 Train Loss 132.0482 Test MSE 133.62044478110298 Test RE 0.1945922891925051 Lambda1 0.42327458\n",
      "39 Train Loss 127.92486 Test MSE 131.30750951920325 Test RE 0.19290076624102656 Lambda1 0.4504184\n",
      "40 Train Loss 124.53355 Test MSE 122.889976823097 Test RE 0.18661536638199566 Lambda1 0.48192737\n",
      "41 Train Loss 119.21717 Test MSE 120.74552705800535 Test RE 0.1849799664790584 Lambda1 0.49727747\n",
      "42 Train Loss 115.35432 Test MSE 117.50716006552128 Test RE 0.18248254775600856 Lambda1 0.5036832\n",
      "43 Train Loss 112.02445 Test MSE 112.53240119475466 Test RE 0.17857800362918064 Lambda1 0.5241964\n",
      "44 Train Loss 106.28497 Test MSE 107.33960846208844 Test RE 0.17440911325947306 Lambda1 0.5612718\n",
      "45 Train Loss 103.65607 Test MSE 103.40920554593438 Test RE 0.17118620784151437 Lambda1 0.580242\n",
      "46 Train Loss 100.77055 Test MSE 102.22150805982915 Test RE 0.1702002965744771 Lambda1 0.5955801\n",
      "47 Train Loss 97.610435 Test MSE 99.38620721685581 Test RE 0.16782328944961458 Lambda1 0.62234354\n",
      "48 Train Loss 95.8821 Test MSE 97.08098677370731 Test RE 0.16586557616413397 Lambda1 0.6384915\n",
      "49 Train Loss 94.227425 Test MSE 94.80875851097638 Test RE 0.16391300055760227 Lambda1 0.66326755\n",
      "50 Train Loss 92.48443 Test MSE 92.88703048092938 Test RE 0.1622432771946428 Lambda1 0.67324144\n",
      "51 Train Loss 90.2224 Test MSE 90.94409476086837 Test RE 0.1605374731212847 Lambda1 0.6802184\n",
      "52 Train Loss 87.24373 Test MSE 87.51633050121903 Test RE 0.15748301491053793 Lambda1 0.719194\n",
      "53 Train Loss 86.44373 Test MSE 86.16765981675921 Test RE 0.15626485733730391 Lambda1 0.72814375\n",
      "54 Train Loss 84.57264 Test MSE 85.40636812865692 Test RE 0.15557302548037064 Lambda1 0.728386\n",
      "55 Train Loss 83.77311 Test MSE 84.27093766933955 Test RE 0.15453543670611275 Lambda1 0.74565303\n",
      "56 Train Loss 81.26986 Test MSE 79.30672948195065 Test RE 0.14991468993472515 Lambda1 0.7949702\n",
      "57 Train Loss 80.02182 Test MSE 77.64316745929865 Test RE 0.1483340288843056 Lambda1 0.80839044\n",
      "58 Train Loss 78.589966 Test MSE 78.13582695892305 Test RE 0.1488038874289457 Lambda1 0.7979788\n",
      "59 Train Loss 76.894135 Test MSE 77.00538481006842 Test RE 0.1477235440679164 Lambda1 0.80029887\n",
      "60 Train Loss 74.8043 Test MSE 73.88052044920316 Test RE 0.14469520642365302 Lambda1 0.83856195\n",
      "61 Train Loss 72.11463 Test MSE 72.09181296364083 Test RE 0.1429328799147769 Lambda1 0.8613419\n",
      "62 Train Loss 70.811295 Test MSE 70.32911784415832 Test RE 0.14117466167484616 Lambda1 0.8906954\n",
      "63 Train Loss 70.195076 Test MSE 68.91829322410929 Test RE 0.1397514837101051 Lambda1 0.9054026\n",
      "64 Train Loss 69.49153 Test MSE 67.33556295377082 Test RE 0.13813744477993545 Lambda1 0.92298585\n",
      "65 Train Loss 68.06878 Test MSE 65.88514894413197 Test RE 0.13664159935037787 Lambda1 0.93232906\n",
      "66 Train Loss 67.13966 Test MSE 65.4230978526621 Test RE 0.1361616240997048 Lambda1 0.9239068\n",
      "67 Train Loss 66.424484 Test MSE 64.72148584397661 Test RE 0.1354295421229248 Lambda1 0.9330857\n",
      "68 Train Loss 64.25691 Test MSE 62.011357812222165 Test RE 0.1325637530604722 Lambda1 0.9489612\n",
      "69 Train Loss 63.061157 Test MSE 59.68176607834238 Test RE 0.13004989434154637 Lambda1 0.9662557\n",
      "70 Train Loss 61.21386 Test MSE 58.22340022929857 Test RE 0.12845113701810584 Lambda1 0.9926217\n",
      "71 Train Loss 59.587543 Test MSE 56.49676257413271 Test RE 0.1265321687118656 Lambda1 0.99901223\n",
      "72 Train Loss 59.0248 Test MSE 57.206755639476725 Test RE 0.12732474932479373 Lambda1 0.9914106\n",
      "73 Train Loss 57.645847 Test MSE 56.014386252336706 Test RE 0.12599083712043851 Lambda1 1.0062166\n",
      "74 Train Loss 56.18194 Test MSE 54.1983997385199 Test RE 0.12393169870232884 Lambda1 1.0266303\n",
      "Training time: 72.48\n",
      "Training time: 72.48\n",
      "inv_HT_stan_tune18\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 837.69415 Test MSE 857.6914633907857 Test RE 0.4930088489189635 Lambda1 -0.01939673\n",
      "1 Train Loss 833.04645 Test MSE 851.9421217838036 Test RE 0.49135368359156684 Lambda1 0.017447779\n",
      "2 Train Loss 763.1739 Test MSE 758.109970588277 Test RE 0.4635059087391404 Lambda1 0.23677433\n",
      "3 Train Loss 670.0012 Test MSE 684.5464647388495 Test RE 0.4404439375938335 Lambda1 0.13861065\n",
      "4 Train Loss 591.3153 Test MSE 614.8344685156721 Test RE 0.4174152137521334 Lambda1 0.07647546\n",
      "5 Train Loss 429.3849 Test MSE 441.5019412950031 Test RE 0.35371663059238073 Lambda1 0.0027151988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 291.43024 Test MSE 301.7663058535818 Test RE 0.29243176387515246 Lambda1 0.0035593524\n",
      "7 Train Loss 264.07578 Test MSE 281.1143279500783 Test RE 0.28224786232234694 Lambda1 0.0021689557\n",
      "8 Train Loss 260.53424 Test MSE 281.1685551218574 Test RE 0.28227508392536227 Lambda1 0.0016592067\n",
      "9 Train Loss 257.95764 Test MSE 280.3495632789771 Test RE 0.2818636766405979 Lambda1 0.0021515207\n",
      "10 Train Loss 257.05704 Test MSE 279.5624159952002 Test RE 0.2814676992388822 Lambda1 0.0029705034\n",
      "11 Train Loss 255.76155 Test MSE 279.03658338552674 Test RE 0.28120286650749055 Lambda1 0.0062670447\n",
      "12 Train Loss 254.38509 Test MSE 276.34577005804607 Test RE 0.27984373031039766 Lambda1 0.013355246\n",
      "13 Train Loss 252.4204 Test MSE 275.6454221660543 Test RE 0.27948889893719664 Lambda1 0.0172044\n",
      "14 Train Loss 249.52089 Test MSE 270.8497481345786 Test RE 0.277046959745494 Lambda1 0.022803662\n",
      "15 Train Loss 246.1035 Test MSE 265.64633470400304 Test RE 0.27437281854302104 Lambda1 0.03751443\n",
      "16 Train Loss 241.42262 Test MSE 260.91961634620975 Test RE 0.27192086698271783 Lambda1 0.061211284\n",
      "17 Train Loss 236.93571 Test MSE 254.57334642716938 Test RE 0.2685935852920392 Lambda1 0.092005566\n",
      "18 Train Loss 227.1802 Test MSE 250.43240867893934 Test RE 0.2664001321511836 Lambda1 0.14039154\n",
      "19 Train Loss 222.47375 Test MSE 248.69087035834835 Test RE 0.2654722262090048 Lambda1 0.16855176\n",
      "20 Train Loss 218.41238 Test MSE 242.80476814689408 Test RE 0.26231176892918834 Lambda1 0.19508962\n",
      "21 Train Loss 212.01363 Test MSE 234.47769071364203 Test RE 0.2577744890877384 Lambda1 0.23616563\n",
      "22 Train Loss 206.69167 Test MSE 226.5944280440955 Test RE 0.2534041854553772 Lambda1 0.27671883\n",
      "23 Train Loss 191.87808 Test MSE 200.33691588246012 Test RE 0.23827016330204423 Lambda1 0.31849545\n",
      "24 Train Loss 182.42531 Test MSE 186.52177942166043 Test RE 0.22990792702474205 Lambda1 0.3039745\n",
      "25 Train Loss 175.06833 Test MSE 176.60967656178406 Test RE 0.22371567681002658 Lambda1 0.32006496\n",
      "26 Train Loss 164.58438 Test MSE 163.11130858969878 Test RE 0.2149964092351924 Lambda1 0.36353904\n",
      "27 Train Loss 157.81636 Test MSE 153.8634461389282 Test RE 0.20881269486485568 Lambda1 0.3966178\n",
      "28 Train Loss 147.68193 Test MSE 142.44500033869542 Test RE 0.2009151916283038 Lambda1 0.44554433\n",
      "29 Train Loss 139.47702 Test MSE 135.10720045883932 Test RE 0.19567188022107831 Lambda1 0.489626\n",
      "30 Train Loss 135.98544 Test MSE 130.55533883091277 Test RE 0.19234747463431617 Lambda1 0.5082664\n",
      "31 Train Loss 130.51547 Test MSE 128.5701086502519 Test RE 0.19087945049919397 Lambda1 0.5132871\n",
      "32 Train Loss 125.32043 Test MSE 127.19961565959491 Test RE 0.18985938517981188 Lambda1 0.5339077\n",
      "33 Train Loss 121.59828 Test MSE 123.12161740602946 Test RE 0.1867911632293606 Lambda1 0.57335985\n",
      "34 Train Loss 116.34317 Test MSE 117.49149472096664 Test RE 0.1824703836152655 Lambda1 0.5978651\n",
      "35 Train Loss 112.68696 Test MSE 112.00874707616069 Test RE 0.17816202499743145 Lambda1 0.6198261\n",
      "36 Train Loss 109.58027 Test MSE 105.63030876096427 Test RE 0.17301487577234198 Lambda1 0.6424106\n",
      "37 Train Loss 107.31912 Test MSE 100.31283749947669 Test RE 0.16860382706446128 Lambda1 0.67026764\n",
      "38 Train Loss 105.01836 Test MSE 98.01684779369496 Test RE 0.1666631310062084 Lambda1 0.68421686\n",
      "39 Train Loss 97.59018 Test MSE 96.66357991049782 Test RE 0.16550861641774472 Lambda1 0.69026065\n",
      "40 Train Loss 91.06587 Test MSE 90.48954470434003 Test RE 0.1601357772911819 Lambda1 0.7367491\n",
      "41 Train Loss 87.58703 Test MSE 86.12842712356768 Test RE 0.15622927908664508 Lambda1 0.7579269\n",
      "42 Train Loss 82.90744 Test MSE 81.50116682108576 Test RE 0.15197462619314991 Lambda1 0.78661746\n",
      "43 Train Loss 79.82859 Test MSE 78.43890644205581 Test RE 0.1490922043116378 Lambda1 0.8282683\n",
      "44 Train Loss 77.90118 Test MSE 76.4564276526945 Test RE 0.14719605537127586 Lambda1 0.8583893\n",
      "45 Train Loss 75.56441 Test MSE 75.54397120507542 Test RE 0.14631507565426846 Lambda1 0.8928811\n",
      "46 Train Loss 74.089966 Test MSE 74.28982670135481 Test RE 0.1450954665391931 Lambda1 0.9093104\n",
      "47 Train Loss 71.79614 Test MSE 73.26868353732705 Test RE 0.14409481866743182 Lambda1 0.9328928\n",
      "48 Train Loss 70.54279 Test MSE 71.87614428734071 Test RE 0.14271892195589778 Lambda1 0.93990076\n",
      "49 Train Loss 67.82781 Test MSE 66.72664490083733 Test RE 0.1375114352292618 Lambda1 0.9809755\n",
      "50 Train Loss 65.41178 Test MSE 63.19088310141936 Test RE 0.1338185694911928 Lambda1 1.0048277\n",
      "51 Train Loss 62.562202 Test MSE 61.840344133247285 Test RE 0.13238083603232456 Lambda1 1.0282179\n",
      "52 Train Loss 59.747036 Test MSE 59.15116625211679 Test RE 0.12947050039747443 Lambda1 1.06547\n",
      "53 Train Loss 56.940044 Test MSE 56.56129833671759 Test RE 0.12660441638600997 Lambda1 1.1276692\n",
      "54 Train Loss 55.65169 Test MSE 54.51659292325996 Test RE 0.12429496138916132 Lambda1 1.1454451\n",
      "55 Train Loss 52.365765 Test MSE 51.233413115864984 Test RE 0.12049410915375301 Lambda1 1.1987153\n",
      "56 Train Loss 51.132496 Test MSE 49.11711596903301 Test RE 0.11797924150639777 Lambda1 1.230194\n",
      "57 Train Loss 49.33024 Test MSE 48.510259133105095 Test RE 0.11724814162656012 Lambda1 1.2459869\n",
      "58 Train Loss 47.428905 Test MSE 47.09848259697317 Test RE 0.1155294293909146 Lambda1 1.2749723\n",
      "59 Train Loss 46.6507 Test MSE 47.009368087469376 Test RE 0.11542008167516705 Lambda1 1.2884145\n",
      "60 Train Loss 45.50825 Test MSE 47.17136637018917 Test RE 0.11561878434741192 Lambda1 1.3204032\n",
      "61 Train Loss 44.313347 Test MSE 46.19837027593077 Test RE 0.11442014619153816 Lambda1 1.3281304\n",
      "62 Train Loss 43.306427 Test MSE 44.925478327411184 Test RE 0.11283284157648207 Lambda1 1.3355383\n",
      "63 Train Loss 41.596786 Test MSE 43.42659246228726 Test RE 0.11093460649077479 Lambda1 1.3637999\n",
      "64 Train Loss 40.69196 Test MSE 42.34626158879201 Test RE 0.10954604651711256 Lambda1 1.3818004\n",
      "65 Train Loss 39.693825 Test MSE 42.123134459778136 Test RE 0.10925706023050934 Lambda1 1.3953748\n",
      "66 Train Loss 39.331936 Test MSE 41.621958962691025 Test RE 0.10860515230465437 Lambda1 1.3939037\n",
      "67 Train Loss 39.041294 Test MSE 41.389921704463426 Test RE 0.10830199907764282 Lambda1 1.3941795\n",
      "68 Train Loss 38.637325 Test MSE 41.058368065807855 Test RE 0.10786735073954146 Lambda1 1.4046667\n",
      "69 Train Loss 38.35047 Test MSE 41.19388837704262 Test RE 0.10804522159759498 Lambda1 1.4002366\n",
      "70 Train Loss 37.971962 Test MSE 40.978060394072195 Test RE 0.107761808115492 Lambda1 1.4056191\n",
      "71 Train Loss 37.85452 Test MSE 41.18571325192426 Test RE 0.10803450001889603 Lambda1 1.4063022\n",
      "72 Train Loss 37.621716 Test MSE 40.95736358770181 Test RE 0.10773459102732078 Lambda1 1.4031631\n",
      "73 Train Loss 37.211296 Test MSE 40.59162368033663 Test RE 0.10725248969765863 Lambda1 1.399455\n",
      "74 Train Loss 37.042423 Test MSE 40.60059193144561 Test RE 0.10726433714363348 Lambda1 1.4018682\n",
      "Training time: 75.23\n",
      "Training time: 75.23\n",
      "inv_HT_stan_tune18\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 834.6288 Test MSE 850.7865908522401 Test RE 0.49102034695933033 Lambda1 -0.024472298\n",
      "1 Train Loss 735.0738 Test MSE 741.8392836304071 Test RE 0.4585050087749682 Lambda1 0.011123579\n",
      "2 Train Loss 645.4199 Test MSE 642.5574367654727 Test RE 0.42672211197609106 Lambda1 0.0001782449\n",
      "3 Train Loss 603.628 Test MSE 607.8423053433565 Test RE 0.4150349138888199 Lambda1 -0.0009217221\n",
      "4 Train Loss 446.8002 Test MSE 437.8048293569947 Test RE 0.35223251569718045 Lambda1 -0.001067458\n",
      "5 Train Loss 312.59692 Test MSE 317.73418559260426 Test RE 0.3000690080145509 Lambda1 -0.0008924077\n",
      "6 Train Loss 275.20148 Test MSE 293.7286702464304 Test RE 0.2885109761876017 Lambda1 -0.0010564624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 260.64676 Test MSE 284.43312109499556 Test RE 0.28390906148204986 Lambda1 -0.00012345913\n",
      "8 Train Loss 257.05743 Test MSE 283.7905371045215 Test RE 0.28358818012360926 Lambda1 0.00015727701\n",
      "9 Train Loss 255.38214 Test MSE 283.76068136553306 Test RE 0.28357326250608317 Lambda1 0.00017305398\n",
      "10 Train Loss 255.07419 Test MSE 283.12835709154166 Test RE 0.2832571329767836 Lambda1 0.00014660711\n",
      "11 Train Loss 254.75592 Test MSE 282.73541980124867 Test RE 0.28306050672617133 Lambda1 0.0002062714\n",
      "12 Train Loss 254.56232 Test MSE 282.8686020650598 Test RE 0.28312716657493675 Lambda1 0.0005279817\n",
      "13 Train Loss 254.3821 Test MSE 282.6296503635161 Test RE 0.2830075562483097 Lambda1 0.0010037544\n",
      "14 Train Loss 254.23271 Test MSE 282.0264893494825 Test RE 0.28270541120778026 Lambda1 0.0013132367\n",
      "15 Train Loss 254.0609 Test MSE 281.62827236066863 Test RE 0.28250575289635554 Lambda1 0.0017324921\n",
      "16 Train Loss 253.83789 Test MSE 281.6390819804689 Test RE 0.282511174494216 Lambda1 0.0021578048\n",
      "17 Train Loss 253.67987 Test MSE 280.81177306569674 Test RE 0.28209593399572835 Lambda1 0.0028901803\n",
      "18 Train Loss 253.34544 Test MSE 280.2325965094001 Test RE 0.2818048712641452 Lambda1 0.004493502\n",
      "19 Train Loss 253.01245 Test MSE 279.63471171881685 Test RE 0.2815040911042116 Lambda1 0.00508145\n",
      "20 Train Loss 252.71373 Test MSE 279.54810857407807 Test RE 0.2814604966819566 Lambda1 0.0058574188\n",
      "21 Train Loss 252.0163 Test MSE 278.50570860732927 Test RE 0.280935241059971 Lambda1 0.008108534\n",
      "22 Train Loss 251.64165 Test MSE 277.9567811352508 Test RE 0.28065824649687077 Lambda1 0.0090562925\n",
      "23 Train Loss 250.7919 Test MSE 275.4387058738159 Test RE 0.27938407995565223 Lambda1 0.012639008\n",
      "24 Train Loss 249.5641 Test MSE 272.48169310153816 Test RE 0.27788034872491674 Lambda1 0.01912221\n",
      "25 Train Loss 248.67287 Test MSE 271.18892740707963 Test RE 0.2772203754244851 Lambda1 0.022861548\n",
      "26 Train Loss 247.15565 Test MSE 270.2991518548728 Test RE 0.2767652193553801 Lambda1 0.027201083\n",
      "27 Train Loss 243.55449 Test MSE 265.4499835809603 Test RE 0.2742713991692926 Lambda1 0.047844347\n",
      "28 Train Loss 239.17801 Test MSE 258.4474122910492 Test RE 0.27062958072385285 Lambda1 0.07230291\n",
      "29 Train Loss 235.19855 Test MSE 255.11273454245543 Test RE 0.2688779817740834 Lambda1 0.080671035\n",
      "30 Train Loss 231.53574 Test MSE 249.6910904428881 Test RE 0.2660055473569464 Lambda1 0.09807981\n",
      "31 Train Loss 227.63086 Test MSE 246.56991257487866 Test RE 0.26433776350535115 Lambda1 0.11201068\n",
      "32 Train Loss 221.89427 Test MSE 243.83616777081215 Test RE 0.26286830980358666 Lambda1 0.14141142\n",
      "33 Train Loss 218.41711 Test MSE 241.51850268074367 Test RE 0.26161604410903133 Lambda1 0.15265483\n",
      "34 Train Loss 215.38004 Test MSE 239.40071605490493 Test RE 0.26046651132818927 Lambda1 0.1707264\n",
      "35 Train Loss 213.7258 Test MSE 236.91060846205121 Test RE 0.25910835952215727 Lambda1 0.19408484\n",
      "36 Train Loss 211.95476 Test MSE 235.35677485516948 Test RE 0.2582572503035428 Lambda1 0.21470805\n",
      "37 Train Loss 210.85722 Test MSE 235.3544065334062 Test RE 0.2582559509192241 Lambda1 0.22901826\n",
      "38 Train Loss 209.42372 Test MSE 233.20537650494086 Test RE 0.2570741745713336 Lambda1 0.23362762\n",
      "39 Train Loss 207.64922 Test MSE 229.90763823369565 Test RE 0.2552500704822496 Lambda1 0.24517617\n",
      "40 Train Loss 203.80647 Test MSE 224.2103465657115 Test RE 0.25206758214834213 Lambda1 0.23776607\n",
      "41 Train Loss 200.81958 Test MSE 217.70433014349254 Test RE 0.2483834781384681 Lambda1 0.2593625\n",
      "42 Train Loss 195.59924 Test MSE 209.09934308074392 Test RE 0.2434251828152412 Lambda1 0.2760824\n",
      "43 Train Loss 184.26009 Test MSE 193.24975074042655 Test RE 0.23401766508075217 Lambda1 0.30372\n",
      "44 Train Loss 180.22488 Test MSE 186.0066258409933 Test RE 0.2295902167187878 Lambda1 0.305195\n",
      "45 Train Loss 174.50635 Test MSE 177.50550032468746 Test RE 0.22428233977348402 Lambda1 0.31221822\n",
      "46 Train Loss 168.57613 Test MSE 167.47765150365245 Test RE 0.2178550350458837 Lambda1 0.33832666\n",
      "47 Train Loss 163.9183 Test MSE 163.95824630720637 Test RE 0.21555385927525877 Lambda1 0.3639431\n",
      "48 Train Loss 153.8595 Test MSE 146.6100118933156 Test RE 0.20383135199427863 Lambda1 0.43281043\n",
      "49 Train Loss 146.77252 Test MSE 144.78953984186197 Test RE 0.20256190121128112 Lambda1 0.45083708\n",
      "50 Train Loss 143.7102 Test MSE 139.38732547535614 Test RE 0.19874710583501265 Lambda1 0.4653206\n",
      "51 Train Loss 136.69154 Test MSE 134.14573306593985 Test RE 0.19497440432518184 Lambda1 0.4982633\n",
      "52 Train Loss 126.64277 Test MSE 127.96031749674982 Test RE 0.19042625440549604 Lambda1 0.53609085\n",
      "53 Train Loss 123.62641 Test MSE 121.6941767417001 Test RE 0.18570520188449985 Lambda1 0.5555113\n",
      "54 Train Loss 121.17455 Test MSE 117.31495010250774 Test RE 0.18233324060413394 Lambda1 0.56676435\n",
      "55 Train Loss 116.51007 Test MSE 114.51868012151085 Test RE 0.18014712585949055 Lambda1 0.5819428\n",
      "56 Train Loss 112.88166 Test MSE 112.12368822924421 Test RE 0.17825341472479533 Lambda1 0.59164834\n",
      "57 Train Loss 111.93403 Test MSE 112.0012651200386 Test RE 0.17815607446789858 Lambda1 0.59154606\n",
      "58 Train Loss 110.07489 Test MSE 111.56098863279996 Test RE 0.1778055642873336 Lambda1 0.5948665\n",
      "59 Train Loss 107.77937 Test MSE 107.48469095424085 Test RE 0.17452694098720684 Lambda1 0.60664237\n",
      "60 Train Loss 105.69497 Test MSE 104.38466416943628 Test RE 0.17199171212843525 Lambda1 0.6268657\n",
      "61 Train Loss 103.32089 Test MSE 99.35591023194375 Test RE 0.16779770779515876 Lambda1 0.63946104\n",
      "62 Train Loss 100.06993 Test MSE 99.1185773026847 Test RE 0.1675971775405171 Lambda1 0.6416328\n",
      "63 Train Loss 98.39923 Test MSE 97.97878136330893 Test RE 0.16663076470025245 Lambda1 0.64357805\n",
      "64 Train Loss 95.60201 Test MSE 94.35385809828962 Test RE 0.16351929353523212 Lambda1 0.6618327\n",
      "65 Train Loss 94.09552 Test MSE 94.33402429118789 Test RE 0.1635021062116571 Lambda1 0.6635537\n",
      "66 Train Loss 91.88112 Test MSE 91.59925838734915 Test RE 0.16111469347730104 Lambda1 0.6694557\n",
      "67 Train Loss 87.9549 Test MSE 85.54430295846502 Test RE 0.15569860324455057 Lambda1 0.6886605\n",
      "68 Train Loss 86.583244 Test MSE 83.44737509842518 Test RE 0.1537784611772678 Lambda1 0.69429046\n",
      "69 Train Loss 82.72376 Test MSE 77.75168357803601 Test RE 0.14843765044738652 Lambda1 0.72713476\n",
      "70 Train Loss 80.48503 Test MSE 74.9243335995139 Test RE 0.14571377684146447 Lambda1 0.7524789\n",
      "71 Train Loss 79.03386 Test MSE 74.27286602720598 Test RE 0.14507890264671192 Lambda1 0.74749714\n",
      "72 Train Loss 77.62433 Test MSE 73.39149048061572 Test RE 0.1442155280517027 Lambda1 0.7532421\n",
      "73 Train Loss 74.85119 Test MSE 68.3005233403259 Test RE 0.1391237214964025 Lambda1 0.7917964\n",
      "74 Train Loss 73.04716 Test MSE 67.81483425237703 Test RE 0.13862818043582736 Lambda1 0.803803\n",
      "Training time: 74.69\n",
      "Training time: 74.69\n",
      "inv_HT_stan_tune19\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1143.3851 Test MSE 871.7755897692235 Test RE 0.49704020852604075 Lambda1 -0.013219707\n",
      "1 Train Loss 829.513 Test MSE 848.0694781475374 Test RE 0.49023564692463284 Lambda1 -0.0128116235\n",
      "2 Train Loss 799.6388 Test MSE 816.5353370486612 Test RE 0.48103498462494976 Lambda1 -0.0050492366\n",
      "3 Train Loss 728.99304 Test MSE 738.1437790683268 Test RE 0.45736155165935616 Lambda1 0.0043021063\n",
      "4 Train Loss 628.7315 Test MSE 626.5856059110992 Test RE 0.4213852965054496 Lambda1 0.02354673\n",
      "5 Train Loss 498.52597 Test MSE 497.2472664812845 Test RE 0.37538366728646144 Lambda1 0.011369062\n",
      "6 Train Loss 382.84003 Test MSE 379.0288468501067 Test RE 0.3277368707512156 Lambda1 -0.0077694263\n",
      "7 Train Loss 299.66522 Test MSE 300.99327713291746 Test RE 0.29205696538736003 Lambda1 -0.00712716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 275.0173 Test MSE 287.5086355107321 Test RE 0.28543985849855896 Lambda1 0.0033134453\n",
      "9 Train Loss 259.4047 Test MSE 274.3944737912435 Test RE 0.27885398224468927 Lambda1 0.023583852\n",
      "10 Train Loss 244.86842 Test MSE 259.5829543517579 Test RE 0.27122346256952706 Lambda1 0.048260987\n",
      "11 Train Loss 230.52422 Test MSE 240.10411985854554 Test RE 0.26084888017944174 Lambda1 0.08599824\n",
      "12 Train Loss 217.40494 Test MSE 223.85884742222092 Test RE 0.25186991889719806 Lambda1 0.11461828\n",
      "13 Train Loss 198.07977 Test MSE 199.36619143989665 Test RE 0.2376921980939124 Lambda1 0.16256681\n",
      "14 Train Loss 178.00179 Test MSE 176.4493554006697 Test RE 0.22361411244200538 Lambda1 0.21390836\n",
      "15 Train Loss 159.22792 Test MSE 152.43460506567735 Test RE 0.20784087188729614 Lambda1 0.2630012\n",
      "16 Train Loss 142.98268 Test MSE 139.2425746051831 Test RE 0.19864388163850627 Lambda1 0.28213277\n",
      "17 Train Loss 132.28702 Test MSE 128.55331840842715 Test RE 0.19086698641711533 Lambda1 0.30888793\n",
      "18 Train Loss 123.13586 Test MSE 116.2596857950131 Test RE 0.18151133169179182 Lambda1 0.33865684\n",
      "19 Train Loss 114.17599 Test MSE 104.98286771333558 Test RE 0.17248382976638874 Lambda1 0.3651124\n",
      "20 Train Loss 95.20445 Test MSE 86.10514053125885 Test RE 0.15620815775894414 Lambda1 0.4256624\n",
      "21 Train Loss 88.37191 Test MSE 79.43256491810472 Test RE 0.15003357709205198 Lambda1 0.45127124\n",
      "22 Train Loss 81.719925 Test MSE 71.13722587239162 Test RE 0.1419834200686084 Lambda1 0.47897607\n",
      "23 Train Loss 75.73029 Test MSE 70.95637048838903 Test RE 0.1418028197759755 Lambda1 0.4847285\n",
      "24 Train Loss 71.06654 Test MSE 64.2955926035802 Test RE 0.1349832164073444 Lambda1 0.51201606\n",
      "25 Train Loss 67.847595 Test MSE 61.92643493223618 Test RE 0.13247295072193335 Lambda1 0.5216109\n",
      "26 Train Loss 63.339325 Test MSE 59.69675455181883 Test RE 0.13006622367587925 Lambda1 0.5296456\n",
      "27 Train Loss 61.617218 Test MSE 58.413833655312224 Test RE 0.12866103047118327 Lambda1 0.5365098\n",
      "28 Train Loss 59.062813 Test MSE 56.48777179129403 Test RE 0.12652210027198924 Lambda1 0.55033517\n",
      "29 Train Loss 55.91126 Test MSE 53.71022455375046 Test RE 0.123372298131436 Lambda1 0.57140446\n",
      "30 Train Loss 52.185295 Test MSE 50.797718291443026 Test RE 0.1199806673653384 Lambda1 0.6095393\n",
      "31 Train Loss 50.520927 Test MSE 49.388246936239604 Test RE 0.11830442146373032 Lambda1 0.62878716\n",
      "32 Train Loss 48.75157 Test MSE 48.3557731029922 Test RE 0.11706129822355213 Lambda1 0.63719916\n",
      "33 Train Loss 47.754242 Test MSE 47.131818124146896 Test RE 0.11557030706893685 Lambda1 0.6603999\n",
      "34 Train Loss 46.60057 Test MSE 46.59840131143656 Test RE 0.11491445964931123 Lambda1 0.66374797\n",
      "35 Train Loss 45.311268 Test MSE 45.16161072275601 Test RE 0.11312898278301929 Lambda1 0.6901608\n",
      "36 Train Loss 44.530556 Test MSE 44.59977842287884 Test RE 0.11242309084740831 Lambda1 0.6939247\n",
      "37 Train Loss 42.802895 Test MSE 43.59500804041453 Test RE 0.11114950980009541 Lambda1 0.7185109\n",
      "38 Train Loss 40.92871 Test MSE 41.982176159532806 Test RE 0.10907410144452415 Lambda1 0.7684214\n",
      "39 Train Loss 40.143986 Test MSE 41.75364496362815 Test RE 0.10877682231013269 Lambda1 0.78035194\n",
      "40 Train Loss 39.501095 Test MSE 41.361847908349844 Test RE 0.1082652635167096 Lambda1 0.79410505\n",
      "41 Train Loss 38.717865 Test MSE 40.49609532947983 Test RE 0.10712621132207323 Lambda1 0.8201078\n",
      "42 Train Loss 38.3062 Test MSE 40.59053409802076 Test RE 0.107251050223332 Lambda1 0.8261459\n",
      "43 Train Loss 37.7202 Test MSE 40.26906014171203 Test RE 0.10682549586074702 Lambda1 0.8393827\n",
      "44 Train Loss 37.287292 Test MSE 40.07903839272787 Test RE 0.10657315360223327 Lambda1 0.8432761\n",
      "45 Train Loss 37.025425 Test MSE 39.9589781076915 Test RE 0.10641340925231105 Lambda1 0.84442514\n",
      "46 Train Loss 36.68736 Test MSE 39.94401751663281 Test RE 0.10639348686423103 Lambda1 0.8526625\n",
      "47 Train Loss 36.445908 Test MSE 39.76353864671506 Test RE 0.10615285614610832 Lambda1 0.8571998\n",
      "48 Train Loss 36.056194 Test MSE 39.268280141843526 Test RE 0.10548971304086338 Lambda1 0.87864774\n",
      "49 Train Loss 35.524 Test MSE 38.848577161999394 Test RE 0.10492445674134261 Lambda1 0.8974833\n",
      "50 Train Loss 35.257004 Test MSE 38.56365405425094 Test RE 0.10453898034672252 Lambda1 0.90330017\n",
      "51 Train Loss 35.012386 Test MSE 38.155752229230885 Test RE 0.10398463712794917 Lambda1 0.91758764\n",
      "52 Train Loss 34.692627 Test MSE 37.90526595044868 Test RE 0.1036427540264623 Lambda1 0.9332466\n",
      "53 Train Loss 34.539524 Test MSE 37.86362930893637 Test RE 0.10358581573871067 Lambda1 0.9316225\n",
      "54 Train Loss 34.228107 Test MSE 37.59060212695939 Test RE 0.10321167157910945 Lambda1 0.9555534\n",
      "55 Train Loss 33.943665 Test MSE 37.533923462633844 Test RE 0.10313383155895654 Lambda1 0.97901356\n",
      "56 Train Loss 33.68201 Test MSE 37.336999948768955 Test RE 0.10286292748869354 Lambda1 0.99065024\n",
      "57 Train Loss 33.504234 Test MSE 37.231509592457044 Test RE 0.10271751245462418 Lambda1 0.9914188\n",
      "58 Train Loss 33.36811 Test MSE 37.131686447538215 Test RE 0.10257971966412661 Lambda1 0.9954835\n",
      "59 Train Loss 33.27504 Test MSE 37.01709797476233 Test RE 0.10242131673219443 Lambda1 0.99892664\n",
      "60 Train Loss 33.105896 Test MSE 36.96392043365601 Test RE 0.10234772275055437 Lambda1 1.0114568\n",
      "61 Train Loss 33.02219 Test MSE 36.990731616586004 Test RE 0.10238483415641693 Lambda1 1.0153975\n",
      "62 Train Loss 32.96019 Test MSE 37.02319296802905 Test RE 0.10242974839723018 Lambda1 1.0154042\n",
      "63 Train Loss 32.866013 Test MSE 36.70553463135864 Test RE 0.10198937906306343 Lambda1 1.0229948\n",
      "64 Train Loss 32.792988 Test MSE 36.49799026244591 Test RE 0.10170063067601667 Lambda1 1.0281775\n",
      "65 Train Loss 32.688457 Test MSE 36.5117242297026 Test RE 0.10171976353441488 Lambda1 1.0318192\n",
      "66 Train Loss 32.572292 Test MSE 36.29210921439798 Test RE 0.10141338412617078 Lambda1 1.0373423\n",
      "67 Train Loss 32.486496 Test MSE 36.28663982701166 Test RE 0.10140574210707462 Lambda1 1.0357784\n",
      "68 Train Loss 32.376728 Test MSE 36.273803754039676 Test RE 0.1013878048353416 Lambda1 1.0419568\n",
      "69 Train Loss 32.26055 Test MSE 36.076778872513145 Test RE 0.10111208079487566 Lambda1 1.0491927\n",
      "70 Train Loss 32.15457 Test MSE 36.0948128510211 Test RE 0.10113734947642049 Lambda1 1.0391897\n",
      "71 Train Loss 32.13062 Test MSE 36.059974284047485 Test RE 0.101088529013483 Lambda1 1.0392597\n",
      "72 Train Loss 32.00228 Test MSE 35.78677147515943 Test RE 0.10070486013995314 Lambda1 1.0506092\n",
      "73 Train Loss 31.861187 Test MSE 35.66525566700401 Test RE 0.10053374051011404 Lambda1 1.0529134\n",
      "74 Train Loss 31.802998 Test MSE 35.71622942779898 Test RE 0.10060555764703828 Lambda1 1.0516764\n",
      "Training time: 71.45\n",
      "Training time: 71.45\n",
      "inv_HT_stan_tune19\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.66\n",
      "Training time: 2.66\n",
      "inv_HT_stan_tune19\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 2223.359 Test MSE 1995.7702153530338 Test RE 0.7520460579330672 Lambda1 0.10343537\n",
      "1 Train Loss 813.5517 Test MSE 829.9149930618786 Test RE 0.4849600635066333 Lambda1 0.10627168\n",
      "2 Train Loss 720.2526 Test MSE 731.8985413456032 Test RE 0.45542263470577304 Lambda1 0.07374333\n",
      "3 Train Loss 486.51288 Test MSE 488.83281870571227 Test RE 0.37219398335026066 Lambda1 0.003939841\n",
      "4 Train Loss 320.6332 Test MSE 330.933382588458 Test RE 0.3062382679889161 Lambda1 -2.5470159e-05\n",
      "5 Train Loss 269.7019 Test MSE 286.7981691290106 Test RE 0.28508696325290694 Lambda1 0.0015710772\n",
      "6 Train Loss 259.41464 Test MSE 280.125957377375 Test RE 0.28175124744108276 Lambda1 0.0031450042\n",
      "7 Train Loss 256.6262 Test MSE 280.2706944500589 Test RE 0.2818240264597259 Lambda1 0.0024946157\n",
      "8 Train Loss 255.13503 Test MSE 280.13254295040065 Test RE 0.2817545593128694 Lambda1 0.0028814469\n",
      "9 Train Loss 254.02516 Test MSE 279.237391742168 Test RE 0.2813040319697622 Lambda1 0.0052949637\n",
      "10 Train Loss 252.81285 Test MSE 277.24627513762414 Test RE 0.2802993112677831 Lambda1 0.008356165\n",
      "11 Train Loss 251.65218 Test MSE 275.3506701823643 Test RE 0.279339428031697 Lambda1 0.010889232\n",
      "12 Train Loss 250.0454 Test MSE 272.9403694748263 Test RE 0.2781141324263556 Lambda1 0.014546411\n",
      "13 Train Loss 248.61433 Test MSE 269.7182241143692 Test RE 0.2764676470675731 Lambda1 0.019472137\n",
      "14 Train Loss 245.98158 Test MSE 266.9867072951533 Test RE 0.27506414949735986 Lambda1 0.025886238\n",
      "15 Train Loss 238.56499 Test MSE 257.646097851945 Test RE 0.2702097124368857 Lambda1 0.047427848\n",
      "16 Train Loss 234.60016 Test MSE 247.83789846322028 Test RE 0.2650165704680947 Lambda1 0.06658804\n",
      "17 Train Loss 230.28127 Test MSE 242.1174316807815 Test RE 0.261940227169021 Lambda1 0.079137\n",
      "18 Train Loss 220.0316 Test MSE 233.06100707217306 Test RE 0.25699458935969793 Lambda1 0.10203389\n",
      "19 Train Loss 209.00443 Test MSE 223.2392361652703 Test RE 0.25152110629089863 Lambda1 0.13276929\n",
      "20 Train Loss 203.47833 Test MSE 217.2396519347135 Test RE 0.2481182559353962 Lambda1 0.15043996\n",
      "21 Train Loss 193.85883 Test MSE 206.4107343313003 Test RE 0.2418551335761193 Lambda1 0.16608556\n",
      "22 Train Loss 185.80827 Test MSE 194.17347798711666 Test RE 0.2345762965631758 Lambda1 0.19429585\n",
      "23 Train Loss 178.47989 Test MSE 181.6234273352446 Test RE 0.22686897245761822 Lambda1 0.21852343\n",
      "24 Train Loss 169.27823 Test MSE 173.32657261831042 Test RE 0.2216265292408886 Lambda1 0.23364976\n",
      "25 Train Loss 158.13594 Test MSE 159.8223983348477 Test RE 0.21281782108097427 Lambda1 0.2613616\n",
      "26 Train Loss 150.18423 Test MSE 152.05759534586977 Test RE 0.2075836909843642 Lambda1 0.2801179\n",
      "27 Train Loss 139.38571 Test MSE 137.44885675046731 Test RE 0.19736027270667117 Lambda1 0.31385997\n",
      "28 Train Loss 134.38353 Test MSE 130.96626866055917 Test RE 0.19264994872960778 Lambda1 0.32942548\n",
      "29 Train Loss 129.36516 Test MSE 123.43527667608741 Test RE 0.18702894240268664 Lambda1 0.34999353\n",
      "30 Train Loss 123.77761 Test MSE 115.88394697031788 Test RE 0.18121778174944395 Lambda1 0.3585045\n",
      "31 Train Loss 119.216644 Test MSE 108.0628855556685 Test RE 0.17499572956744497 Lambda1 0.37264353\n",
      "32 Train Loss 111.54747 Test MSE 99.44433845905651 Test RE 0.16787236240667572 Lambda1 0.3857129\n",
      "33 Train Loss 106.318954 Test MSE 94.74097439618605 Test RE 0.16385439476819058 Lambda1 0.39567262\n",
      "34 Train Loss 99.19987 Test MSE 90.24549945607211 Test RE 0.15991969287699706 Lambda1 0.40166914\n",
      "35 Train Loss 94.120186 Test MSE 86.08473640941536 Test RE 0.15618964853227288 Lambda1 0.41544384\n",
      "36 Train Loss 89.82863 Test MSE 83.01891178296027 Test RE 0.15338316280115524 Lambda1 0.42767304\n",
      "37 Train Loss 84.35573 Test MSE 74.01551974222534 Test RE 0.14482734433538258 Lambda1 0.4517292\n",
      "38 Train Loss 80.253975 Test MSE 70.15042000667658 Test RE 0.1409951936750124 Lambda1 0.4639116\n",
      "39 Train Loss 76.35735 Test MSE 65.795766744901 Test RE 0.1365488814016021 Lambda1 0.48413756\n",
      "40 Train Loss 74.24858 Test MSE 64.99920932415675 Test RE 0.13571979891207803 Lambda1 0.48611712\n",
      "41 Train Loss 71.459946 Test MSE 63.176657862209204 Test RE 0.1338035063348254 Lambda1 0.4997785\n",
      "42 Train Loss 69.11488 Test MSE 60.46529664277366 Test RE 0.1309007890933527 Lambda1 0.5170974\n",
      "43 Train Loss 66.435715 Test MSE 58.560678277699914 Test RE 0.12882264733844154 Lambda1 0.53884906\n",
      "44 Train Loss 63.742393 Test MSE 55.86541008141643 Test RE 0.12582318260951794 Lambda1 0.5617953\n",
      "45 Train Loss 61.658546 Test MSE 55.43323881861182 Test RE 0.12533555764658844 Lambda1 0.5625609\n",
      "46 Train Loss 58.598984 Test MSE 53.21522325922081 Test RE 0.12280247361311919 Lambda1 0.57434165\n",
      "47 Train Loss 57.082397 Test MSE 51.66074665061082 Test RE 0.12099558119169987 Lambda1 0.58363795\n",
      "48 Train Loss 54.936756 Test MSE 50.94749127274691 Test RE 0.12015741384797766 Lambda1 0.59075826\n",
      "49 Train Loss 52.88861 Test MSE 49.40627954362361 Test RE 0.11832601711270772 Lambda1 0.61056656\n",
      "50 Train Loss 50.96275 Test MSE 47.331179596303386 Test RE 0.11581447284197421 Lambda1 0.63692236\n",
      "51 Train Loss 49.578606 Test MSE 46.06440851238724 Test RE 0.11425413327548413 Lambda1 0.6581205\n",
      "52 Train Loss 48.298187 Test MSE 45.51567363254417 Test RE 0.11357157745555571 Lambda1 0.66832966\n",
      "53 Train Loss 46.71484 Test MSE 45.37592391940859 Test RE 0.11339709034224739 Lambda1 0.67612374\n",
      "54 Train Loss 45.835373 Test MSE 44.966535007610176 Test RE 0.11288438787211143 Lambda1 0.680898\n",
      "55 Train Loss 44.835476 Test MSE 44.405059022028404 Test RE 0.11217740686027897 Lambda1 0.69711715\n",
      "56 Train Loss 44.245174 Test MSE 44.335746554833754 Test RE 0.11208982304083367 Lambda1 0.70066416\n",
      "57 Train Loss 43.250793 Test MSE 44.014062217548506 Test RE 0.11168244088573481 Lambda1 0.7067193\n",
      "58 Train Loss 42.583687 Test MSE 43.38663448074432 Test RE 0.11088355778025351 Lambda1 0.717916\n",
      "59 Train Loss 42.182686 Test MSE 43.03527496831441 Test RE 0.110433658957131 Lambda1 0.72581136\n",
      "60 Train Loss 41.52772 Test MSE 42.432466055855585 Test RE 0.10965749150799173 Lambda1 0.7322588\n",
      "61 Train Loss 41.076252 Test MSE 41.92465385296292 Test RE 0.10899935133492325 Lambda1 0.73566014\n",
      "62 Train Loss 40.80863 Test MSE 41.46959532024784 Test RE 0.10840618704161552 Lambda1 0.7438651\n",
      "63 Train Loss 40.265972 Test MSE 41.06709051759377 Test RE 0.10787880781694335 Lambda1 0.75828266\n",
      "64 Train Loss 39.72011 Test MSE 40.37467452744212 Test RE 0.10696549070036321 Lambda1 0.77614456\n",
      "65 Train Loss 39.28059 Test MSE 40.25165662314994 Test RE 0.10680240939674544 Lambda1 0.77609\n",
      "66 Train Loss 38.74389 Test MSE 40.18392851334716 Test RE 0.1067125178085079 Lambda1 0.78077835\n",
      "67 Train Loss 38.325356 Test MSE 39.78759905070518 Test RE 0.10618496715090109 Lambda1 0.78794104\n",
      "68 Train Loss 37.967613 Test MSE 39.49739256439616 Test RE 0.10579700753310466 Lambda1 0.79341686\n",
      "69 Train Loss 37.65157 Test MSE 39.38955114285215 Test RE 0.10565247776264479 Lambda1 0.7949857\n",
      "70 Train Loss 36.90627 Test MSE 38.30225303633902 Test RE 0.10418407283065576 Lambda1 0.8223503\n",
      "71 Train Loss 36.432594 Test MSE 37.87140537746249 Test RE 0.10359645192262244 Lambda1 0.8324688\n",
      "72 Train Loss 35.97775 Test MSE 37.92822451489337 Test RE 0.10367413658760707 Lambda1 0.82677495\n",
      "73 Train Loss 35.77941 Test MSE 37.92982332997184 Test RE 0.10367632168891587 Lambda1 0.8275445\n",
      "74 Train Loss 35.686096 Test MSE 37.79752743285849 Test RE 0.10349535679513662 Lambda1 0.830471\n",
      "Training time: 122.37\n",
      "Training time: 122.37\n",
      "inv_HT_stan_tune19\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 984.66394 Test MSE 855.3934177103423 Test RE 0.4923479371481543 Lambda1 0.15519163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 838.61847 Test MSE 855.413035546412 Test RE 0.4923535829396352 Lambda1 0.15723953\n",
      "2 Train Loss 815.8088 Test MSE 833.5882858624047 Test RE 0.4860321211919037 Lambda1 0.15181033\n",
      "3 Train Loss 784.07184 Test MSE 800.9102154133618 Test RE 0.4764102394179213 Lambda1 0.14162478\n",
      "4 Train Loss 744.21674 Test MSE 753.6379296136015 Test RE 0.46213679119724493 Lambda1 0.13762166\n",
      "5 Train Loss 674.8822 Test MSE 678.8196255126048 Test RE 0.4385977157814385 Lambda1 0.13777725\n",
      "6 Train Loss 589.57764 Test MSE 573.2456544199374 Test RE 0.40305058535475413 Lambda1 0.12456064\n",
      "7 Train Loss 514.36914 Test MSE 509.058720337846 Test RE 0.37981587373801257 Lambda1 0.10806652\n",
      "8 Train Loss 410.47174 Test MSE 393.22695340641394 Test RE 0.333818814047353 Lambda1 0.07701195\n",
      "9 Train Loss 318.38593 Test MSE 309.0070511902005 Test RE 0.2959193507449112 Lambda1 0.047622092\n",
      "10 Train Loss 275.5504 Test MSE 283.5917371983476 Test RE 0.2834888336462018 Lambda1 0.037683215\n",
      "11 Train Loss 264.0443 Test MSE 275.0439652580292 Test RE 0.27918381074370396 Lambda1 0.04075383\n",
      "12 Train Loss 249.74306 Test MSE 262.4337116062627 Test RE 0.27270869302369033 Lambda1 0.056569986\n",
      "13 Train Loss 242.66614 Test MSE 258.0369008069103 Test RE 0.2704145646386176 Lambda1 0.067951016\n",
      "14 Train Loss 236.55995 Test MSE 250.51853884078787 Test RE 0.2664459391497809 Lambda1 0.08200329\n",
      "15 Train Loss 232.67154 Test MSE 244.61516168611328 Test RE 0.26328787333127746 Lambda1 0.09326556\n",
      "16 Train Loss 227.7381 Test MSE 240.0145612447767 Test RE 0.2608002274471093 Lambda1 0.09796639\n",
      "17 Train Loss 221.46829 Test MSE 230.43454108729892 Test RE 0.2555423944399543 Lambda1 0.11867902\n",
      "18 Train Loss 215.09718 Test MSE 223.53172086985512 Test RE 0.2516858219502358 Lambda1 0.1398543\n",
      "19 Train Loss 207.94228 Test MSE 210.60947423337717 Test RE 0.24430261890997795 Lambda1 0.15278538\n",
      "20 Train Loss 197.43974 Test MSE 198.83152891073573 Test RE 0.23737326129250325 Lambda1 0.1811966\n",
      "21 Train Loss 191.04427 Test MSE 190.0738003430577 Test RE 0.23208672472584055 Lambda1 0.19655114\n",
      "22 Train Loss 183.75304 Test MSE 180.20345059956855 Test RE 0.22598037344646335 Lambda1 0.21493399\n",
      "23 Train Loss 178.06604 Test MSE 172.35468225735863 Test RE 0.22100429492022294 Lambda1 0.2355195\n",
      "24 Train Loss 172.74532 Test MSE 165.0415968274875 Test RE 0.21626482049496046 Lambda1 0.25978404\n",
      "25 Train Loss 163.06285 Test MSE 157.8732317904887 Test RE 0.2115160951861383 Lambda1 0.27828234\n",
      "26 Train Loss 158.09572 Test MSE 152.32542402870072 Test RE 0.20776642571186144 Lambda1 0.2880558\n",
      "27 Train Loss 154.1576 Test MSE 145.097258521993 Test RE 0.20277703760089572 Lambda1 0.3006218\n",
      "28 Train Loss 145.80203 Test MSE 137.74939107410455 Test RE 0.19757592073150643 Lambda1 0.31861842\n",
      "29 Train Loss 141.9405 Test MSE 130.8708099685731 Test RE 0.19257972658549005 Lambda1 0.3250146\n",
      "30 Train Loss 137.56042 Test MSE 130.1808647180052 Test RE 0.192071419762113 Lambda1 0.32327166\n",
      "31 Train Loss 131.70445 Test MSE 121.61100263269556 Test RE 0.1856417292303031 Lambda1 0.328456\n",
      "32 Train Loss 125.75661 Test MSE 119.5359312317319 Test RE 0.184051094849125 Lambda1 0.3306703\n",
      "33 Train Loss 119.54722 Test MSE 109.82606870373007 Test RE 0.1764175918921299 Lambda1 0.34694162\n",
      "34 Train Loss 117.12515 Test MSE 107.17668322835428 Test RE 0.1742766997133427 Lambda1 0.35646662\n",
      "35 Train Loss 113.72002 Test MSE 105.76545903866077 Test RE 0.1731255236258156 Lambda1 0.36010474\n",
      "36 Train Loss 110.59269 Test MSE 102.16386782818603 Test RE 0.17015230389634073 Lambda1 0.36028907\n",
      "37 Train Loss 105.09535 Test MSE 99.17224302149758 Test RE 0.1676425424264122 Lambda1 0.3675736\n",
      "38 Train Loss 102.94242 Test MSE 97.01919619692586 Test RE 0.1658127822994375 Lambda1 0.3764416\n",
      "39 Train Loss 101.301605 Test MSE 94.04477776035527 Test RE 0.16325124908459726 Lambda1 0.38213664\n",
      "40 Train Loss 98.41039 Test MSE 89.82318942108392 Test RE 0.15954507648953348 Lambda1 0.38509387\n",
      "41 Train Loss 96.53911 Test MSE 86.68662415380933 Test RE 0.1567347212629236 Lambda1 0.3882317\n",
      "42 Train Loss 91.95388 Test MSE 82.83012155839413 Test RE 0.15320866206078246 Lambda1 0.4017705\n",
      "43 Train Loss 87.1464 Test MSE 80.14207029182114 Test RE 0.15070215035548284 Lambda1 0.4117086\n",
      "44 Train Loss 84.88793 Test MSE 76.50598952446539 Test RE 0.147243756593763 Lambda1 0.41574892\n",
      "45 Train Loss 81.63839 Test MSE 74.65149004329074 Test RE 0.14544822008627617 Lambda1 0.42226645\n",
      "46 Train Loss 79.19843 Test MSE 70.89730454118596 Test RE 0.14174378729181492 Lambda1 0.431421\n",
      "47 Train Loss 77.00138 Test MSE 68.52117832369365 Test RE 0.13934827020433102 Lambda1 0.439951\n",
      "48 Train Loss 75.35695 Test MSE 68.52525578955957 Test RE 0.13935241621711603 Lambda1 0.4412842\n",
      "49 Train Loss 72.94856 Test MSE 67.10236268663768 Test RE 0.13789803467425651 Lambda1 0.44841468\n",
      "50 Train Loss 72.52203 Test MSE 66.64475511293868 Test RE 0.13742702937211768 Lambda1 0.44939253\n",
      "51 Train Loss 71.28695 Test MSE 65.2774336353874 Test RE 0.13600995802637572 Lambda1 0.45739523\n",
      "52 Train Loss 70.71914 Test MSE 64.95073421550681 Test RE 0.13566918091878383 Lambda1 0.4601103\n",
      "53 Train Loss 69.98683 Test MSE 64.28471071269786 Test RE 0.13497179311432728 Lambda1 0.4655618\n",
      "54 Train Loss 69.55537 Test MSE 63.36837835262016 Test RE 0.13400637747564698 Lambda1 0.47209412\n",
      "55 Train Loss 67.62865 Test MSE 62.26997767823319 Test RE 0.13283989561274462 Lambda1 0.48013633\n",
      "56 Train Loss 67.27249 Test MSE 62.002755251690104 Test RE 0.13255455775090896 Lambda1 0.48373547\n",
      "57 Train Loss 66.603966 Test MSE 61.21079657424654 Test RE 0.13170528009371682 Lambda1 0.4891826\n",
      "58 Train Loss 66.09422 Test MSE 61.130841603245074 Test RE 0.13161923373031786 Lambda1 0.49519086\n",
      "59 Train Loss 65.88668 Test MSE 60.748610617793304 Test RE 0.13120710266658875 Lambda1 0.49877512\n",
      "60 Train Loss 65.190865 Test MSE 60.88944572704036 Test RE 0.1313591050663917 Lambda1 0.4964503\n",
      "61 Train Loss 64.52354 Test MSE 60.13756047861018 Test RE 0.13054555049988498 Lambda1 0.49670494\n",
      "62 Train Loss 63.1692 Test MSE 59.28138133970265 Test RE 0.12961292991728246 Lambda1 0.50404\n",
      "63 Train Loss 62.36314 Test MSE 58.066038739883524 Test RE 0.12827743590877372 Lambda1 0.51307106\n",
      "64 Train Loss 61.841137 Test MSE 57.12897847580591 Test RE 0.12723816579097041 Lambda1 0.52320963\n",
      "65 Train Loss 61.47779 Test MSE 57.05986223697089 Test RE 0.12716117434831614 Lambda1 0.5256819\n",
      "66 Train Loss 61.202007 Test MSE 57.196318393661805 Test RE 0.12731313373575784 Lambda1 0.5274269\n",
      "67 Train Loss 60.768085 Test MSE 56.42259485443685 Test RE 0.12644908709804778 Lambda1 0.5384139\n",
      "68 Train Loss 59.987408 Test MSE 55.36244814988554 Test RE 0.1252555025874673 Lambda1 0.54869974\n",
      "69 Train Loss 58.478565 Test MSE 54.519827632668566 Test RE 0.12429864881803109 Lambda1 0.55806553\n",
      "70 Train Loss 57.454624 Test MSE 53.70857224207399 Test RE 0.12337040043822907 Lambda1 0.563837\n",
      "71 Train Loss 56.547455 Test MSE 53.188284235173874 Test RE 0.12277138666688417 Lambda1 0.5690109\n",
      "72 Train Loss 56.379604 Test MSE 53.134320707036586 Test RE 0.12270909044354568 Lambda1 0.56843555\n",
      "73 Train Loss 55.98918 Test MSE 53.125102498811295 Test RE 0.12269844565692897 Lambda1 0.5667763\n",
      "74 Train Loss 55.642925 Test MSE 53.41837349307665 Test RE 0.12303665084460161 Lambda1 0.56813955\n",
      "Training time: 131.38\n",
      "Training time: 131.38\n",
      "inv_HT_stan_tune19\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 845.5042 Test MSE 853.5123386717152 Test RE 0.4918062828094643 Lambda1 0.05127173\n",
      "1 Train Loss 833.7199 Test MSE 850.4473763219133 Test RE 0.4909224505868692 Lambda1 0.053548448\n",
      "2 Train Loss 828.4728 Test MSE 844.831533940963 Test RE 0.4892988877609858 Lambda1 0.059424333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Train Loss 822.39844 Test MSE 841.4660792558697 Test RE 0.4883233344131213 Lambda1 0.06711693\n",
      "4 Train Loss 814.3144 Test MSE 833.4022786177861 Test RE 0.4859778914594114 Lambda1 0.082122244\n",
      "5 Train Loss 791.2946 Test MSE 809.7119253424753 Test RE 0.4790208735593591 Lambda1 0.10715382\n",
      "6 Train Loss 733.476 Test MSE 746.1325120630128 Test RE 0.45982984218569417 Lambda1 0.12551653\n",
      "7 Train Loss 681.1681 Test MSE 685.8183415493033 Test RE 0.44085291674610405 Lambda1 0.13435906\n",
      "8 Train Loss 569.3122 Test MSE 550.6213932757157 Test RE 0.3950169315649493 Lambda1 0.15795135\n",
      "9 Train Loss 466.5489 Test MSE 451.93028229224717 Test RE 0.35786966937679493 Lambda1 0.15407854\n",
      "10 Train Loss 363.94305 Test MSE 316.75386060577847 Test RE 0.29960573964135906 Lambda1 0.14959612\n",
      "11 Train Loss 285.3997 Test MSE 275.11790478179313 Test RE 0.27922133443712666 Lambda1 0.137712\n",
      "12 Train Loss 247.30983 Test MSE 240.2351747071607 Test RE 0.2609200593973764 Lambda1 0.13349836\n",
      "13 Train Loss 235.34468 Test MSE 229.48922803603537 Test RE 0.2550176991946744 Lambda1 0.13263254\n",
      "14 Train Loss 223.60287 Test MSE 220.9618356292559 Test RE 0.2502348567959021 Lambda1 0.13645886\n",
      "15 Train Loss 212.17525 Test MSE 217.81634946079865 Test RE 0.2484473725253879 Lambda1 0.1414902\n",
      "16 Train Loss 202.724 Test MSE 208.61542866608164 Test RE 0.24314334264807566 Lambda1 0.14739579\n",
      "17 Train Loss 190.89302 Test MSE 196.97214770104782 Test RE 0.2362607513610842 Lambda1 0.16308323\n",
      "18 Train Loss 186.34686 Test MSE 192.7084851101943 Test RE 0.23368970984103102 Lambda1 0.16998614\n",
      "19 Train Loss 181.36299 Test MSE 185.46703258763623 Test RE 0.22925696163226278 Lambda1 0.18261097\n",
      "20 Train Loss 174.71109 Test MSE 177.08489937607487 Test RE 0.22401646262617067 Lambda1 0.20105065\n",
      "21 Train Loss 169.45522 Test MSE 168.34229690761012 Test RE 0.21841667671936285 Lambda1 0.23192118\n",
      "22 Train Loss 160.90326 Test MSE 159.30009704273533 Test RE 0.21246979105909758 Lambda1 0.2556364\n",
      "23 Train Loss 157.87839 Test MSE 156.06019326869645 Test RE 0.21029804765451182 Lambda1 0.2635245\n",
      "24 Train Loss 148.81128 Test MSE 143.90190024724114 Test RE 0.20194003866760785 Lambda1 0.2845868\n",
      "25 Train Loss 142.43195 Test MSE 141.55264571338975 Test RE 0.20028488072805717 Lambda1 0.2775611\n",
      "26 Train Loss 131.09134 Test MSE 135.13264732161733 Test RE 0.19569030633395848 Lambda1 0.27796263\n",
      "27 Train Loss 122.87377 Test MSE 124.28410334253512 Test RE 0.1876709110712741 Lambda1 0.30206257\n",
      "28 Train Loss 114.7997 Test MSE 112.09357679479818 Test RE 0.1782294776490931 Lambda1 0.32270914\n",
      "29 Train Loss 110.47314 Test MSE 107.00235852264852 Test RE 0.17413491001979084 Lambda1 0.32991636\n",
      "30 Train Loss 106.6075 Test MSE 104.02517785378025 Test RE 0.17169529889757984 Lambda1 0.33639517\n",
      "31 Train Loss 101.43572 Test MSE 97.85268781074723 Test RE 0.1665235076534574 Lambda1 0.34970886\n",
      "32 Train Loss 95.09009 Test MSE 86.28797217128341 Test RE 0.15637391237552695 Lambda1 0.3788215\n",
      "33 Train Loss 83.648994 Test MSE 77.44729441789356 Test RE 0.1481468070993003 Lambda1 0.40595716\n",
      "34 Train Loss 80.39403 Test MSE 73.43858079573033 Test RE 0.14426178726792815 Lambda1 0.42073298\n",
      "35 Train Loss 77.179 Test MSE 71.8343732660771 Test RE 0.14267744517961542 Lambda1 0.42701456\n",
      "36 Train Loss 74.73387 Test MSE 69.88433780809272 Test RE 0.14072754058176298 Lambda1 0.4384417\n",
      "37 Train Loss 73.47001 Test MSE 68.86136607297266 Test RE 0.13969375377366622 Lambda1 0.44617003\n",
      "38 Train Loss 72.37812 Test MSE 67.32314619209379 Test RE 0.13812470783429542 Lambda1 0.4492568\n",
      "39 Train Loss 70.18639 Test MSE 63.79700923254713 Test RE 0.13445883080231485 Lambda1 0.45811582\n",
      "40 Train Loss 67.79668 Test MSE 62.804834792641145 Test RE 0.13340917834695648 Lambda1 0.45749718\n",
      "41 Train Loss 66.61472 Test MSE 61.150113881637026 Test RE 0.13163997941739736 Lambda1 0.46866184\n",
      "42 Train Loss 64.184875 Test MSE 58.10750673158847 Test RE 0.128323232542967 Lambda1 0.48597762\n",
      "43 Train Loss 59.85586 Test MSE 54.32673192966254 Test RE 0.12407833608404259 Lambda1 0.507919\n",
      "44 Train Loss 58.557133 Test MSE 52.832823221834694 Test RE 0.12236045407303615 Lambda1 0.51910985\n",
      "45 Train Loss 57.551197 Test MSE 52.639377903815635 Test RE 0.12213623963577434 Lambda1 0.52095866\n",
      "46 Train Loss 56.12543 Test MSE 51.66732972795495 Test RE 0.12100329011914966 Lambda1 0.5290088\n",
      "47 Train Loss 55.81911 Test MSE 51.561129438099314 Test RE 0.12087886725055229 Lambda1 0.529669\n",
      "48 Train Loss 55.131554 Test MSE 50.58042749400696 Test RE 0.11972377950343104 Lambda1 0.5416335\n",
      "49 Train Loss 54.87325 Test MSE 50.79153813430262 Test RE 0.1199733685932918 Lambda1 0.5424589\n",
      "50 Train Loss 53.751434 Test MSE 49.70974785245209 Test RE 0.11868885788497398 Lambda1 0.5564581\n",
      "51 Train Loss 52.84086 Test MSE 49.165916630425144 Test RE 0.11803783651467409 Lambda1 0.5694494\n",
      "52 Train Loss 52.66951 Test MSE 48.7448421873858 Test RE 0.11753129055984511 Lambda1 0.5769673\n",
      "53 Train Loss 52.33687 Test MSE 47.75318413108472 Test RE 0.11632962770943046 Lambda1 0.5935888\n",
      "54 Train Loss 51.54991 Test MSE 46.457098091864964 Test RE 0.11474009624762137 Lambda1 0.6130357\n",
      "55 Train Loss 50.87185 Test MSE 45.53321598856528 Test RE 0.11359346135792937 Lambda1 0.6295733\n",
      "56 Train Loss 50.482258 Test MSE 44.95366994506032 Test RE 0.11286823843342517 Lambda1 0.637746\n",
      "57 Train Loss 49.884357 Test MSE 44.79284636055253 Test RE 0.11266616217828075 Lambda1 0.6377864\n",
      "58 Train Loss 49.532482 Test MSE 43.91223232307753 Test RE 0.11155317314642978 Lambda1 0.6461507\n",
      "59 Train Loss 47.950546 Test MSE 42.78581951154827 Test RE 0.11011312753124718 Lambda1 0.6688852\n",
      "60 Train Loss 45.745003 Test MSE 42.1960078955058 Test RE 0.10935152728378796 Lambda1 0.6784022\n",
      "61 Train Loss 44.913486 Test MSE 41.715504221476756 Test RE 0.10872712872497362 Lambda1 0.6856032\n",
      "62 Train Loss 44.533493 Test MSE 41.63393282890109 Test RE 0.10862077302605165 Lambda1 0.68183315\n",
      "63 Train Loss 44.30605 Test MSE 41.61127000341668 Test RE 0.10859120593331839 Lambda1 0.67813367\n",
      "64 Train Loss 44.151894 Test MSE 41.49181306406211 Test RE 0.10843522299385346 Lambda1 0.67771757\n",
      "65 Train Loss 43.568462 Test MSE 41.01137144721699 Test RE 0.10780559898883631 Lambda1 0.68208873\n",
      "66 Train Loss 42.15155 Test MSE 40.3057782808184 Test RE 0.10687418758178935 Lambda1 0.6884032\n",
      "67 Train Loss 41.142353 Test MSE 39.879239387617744 Test RE 0.10630718148069698 Lambda1 0.6906283\n",
      "68 Train Loss 40.905365 Test MSE 39.73012451149697 Test RE 0.10610824553700639 Lambda1 0.692122\n",
      "69 Train Loss 40.73075 Test MSE 39.78374313077131 Test RE 0.10617982169525969 Lambda1 0.69139624\n",
      "70 Train Loss 40.585835 Test MSE 39.80399038398735 Test RE 0.10620683745728257 Lambda1 0.69215196\n",
      "71 Train Loss 40.506996 Test MSE 39.964947568758845 Test RE 0.10642135749086963 Lambda1 0.68887496\n",
      "72 Train Loss 40.37124 Test MSE 40.01479065655676 Test RE 0.10648769958006864 Lambda1 0.6869346\n",
      "73 Train Loss 40.274952 Test MSE 39.92562421063177 Test RE 0.10636898816061896 Lambda1 0.68886274\n",
      "74 Train Loss 39.94182 Test MSE 39.5422558197779 Test RE 0.10585707543659219 Lambda1 0.7019063\n",
      "Training time: 190.02\n",
      "Training time: 190.02\n",
      "inv_HT_stan_tune19\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 854.058 Test MSE 852.642574745544 Test RE 0.4915556335952279 Lambda1 0.05746383\n",
      "1 Train Loss 826.0612 Test MSE 842.3685027091695 Test RE 0.48858511344053157 Lambda1 0.056399576\n",
      "2 Train Loss 801.8945 Test MSE 812.8820216890718 Test RE 0.4799576628470566 Lambda1 0.0576323\n",
      "3 Train Loss 761.06134 Test MSE 782.5914597296511 Test RE 0.4709303960466301 Lambda1 0.06557126\n",
      "4 Train Loss 707.8523 Test MSE 726.9495580653805 Test RE 0.45388027472855685 Lambda1 0.0714014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 644.27966 Test MSE 646.3401782190261 Test RE 0.42797632726086776 Lambda1 0.071067065\n",
      "6 Train Loss 561.14954 Test MSE 560.2435736633107 Test RE 0.39845346920430436 Lambda1 0.057910666\n",
      "7 Train Loss 469.30032 Test MSE 469.95026267910293 Test RE 0.36493466515928197 Lambda1 0.054904133\n",
      "8 Train Loss 386.30353 Test MSE 385.0510301877527 Test RE 0.3303302267443889 Lambda1 0.045112655\n",
      "9 Train Loss 316.90707 Test MSE 317.1933469723819 Test RE 0.29981351453064875 Lambda1 0.024322832\n",
      "10 Train Loss 284.6562 Test MSE 295.82186126295505 Test RE 0.28953715537382013 Lambda1 0.017776133\n",
      "11 Train Loss 270.18848 Test MSE 282.4062151166179 Test RE 0.2828956671284295 Lambda1 0.015861861\n",
      "12 Train Loss 264.2025 Test MSE 280.10621605077705 Test RE 0.2817413193333024 Lambda1 0.010592601\n",
      "13 Train Loss 259.03616 Test MSE 278.25727525906257 Test RE 0.28080991283194173 Lambda1 0.0065539535\n",
      "14 Train Loss 255.9763 Test MSE 275.6889146210112 Test RE 0.2795109475146735 Lambda1 0.008826247\n",
      "15 Train Loss 252.93982 Test MSE 271.73739959706694 Test RE 0.27750056913934856 Lambda1 0.017619072\n",
      "16 Train Loss 250.96832 Test MSE 268.8655081980837 Test RE 0.27603027397406554 Lambda1 0.024816563\n",
      "17 Train Loss 248.38809 Test MSE 265.1671720596585 Test RE 0.27412525528618487 Lambda1 0.031057943\n",
      "18 Train Loss 246.28377 Test MSE 261.905255226537 Test RE 0.27243398122898405 Lambda1 0.035599206\n",
      "19 Train Loss 242.5965 Test MSE 256.3203447455732 Test RE 0.2695136153677534 Lambda1 0.04759129\n",
      "20 Train Loss 239.55907 Test MSE 252.56346743947887 Test RE 0.2675311992248401 Lambda1 0.05250017\n",
      "21 Train Loss 234.28018 Test MSE 245.97158478854703 Test RE 0.2640168470314708 Lambda1 0.063276745\n",
      "22 Train Loss 230.88065 Test MSE 240.6511166028569 Test RE 0.26114583950819203 Lambda1 0.07197838\n",
      "23 Train Loss 226.16379 Test MSE 238.50877841591858 Test RE 0.25998084720371867 Lambda1 0.07948425\n",
      "24 Train Loss 222.05444 Test MSE 232.5323208773714 Test RE 0.2567029347541185 Lambda1 0.09496633\n",
      "25 Train Loss 210.86687 Test MSE 214.88398293697654 Test RE 0.2467693364065846 Lambda1 0.122563995\n",
      "26 Train Loss 207.39697 Test MSE 209.39500415180436 Test RE 0.24359722048350324 Lambda1 0.13344169\n",
      "27 Train Loss 203.54321 Test MSE 204.30032429087964 Test RE 0.2406155544559342 Lambda1 0.14368384\n",
      "28 Train Loss 196.87103 Test MSE 198.15537846122734 Test RE 0.23696930946300734 Lambda1 0.1564472\n",
      "29 Train Loss 189.34569 Test MSE 188.80338077601752 Test RE 0.23130981111037213 Lambda1 0.16834795\n",
      "30 Train Loss 185.01804 Test MSE 183.5407785435338 Test RE 0.22806332693146059 Lambda1 0.1751944\n",
      "31 Train Loss 178.07233 Test MSE 175.2090977118154 Test RE 0.222826837831988 Lambda1 0.19227651\n",
      "32 Train Loss 174.72345 Test MSE 172.20366002103324 Test RE 0.22090744847252178 Lambda1 0.1998632\n",
      "33 Train Loss 170.42075 Test MSE 166.42270424545768 Test RE 0.21716781309735672 Lambda1 0.20829922\n",
      "34 Train Loss 163.93025 Test MSE 159.57483151735335 Test RE 0.21265292851868636 Lambda1 0.22717598\n",
      "35 Train Loss 159.53685 Test MSE 153.02143026553088 Test RE 0.2082405485342523 Lambda1 0.23951198\n",
      "36 Train Loss 154.16223 Test MSE 149.65502668531633 Test RE 0.20593721013574937 Lambda1 0.25269595\n",
      "37 Train Loss 150.82466 Test MSE 147.79900245820792 Test RE 0.20465620757093292 Lambda1 0.25955254\n",
      "38 Train Loss 146.40163 Test MSE 142.39866380176585 Test RE 0.20088251069391966 Lambda1 0.27050114\n",
      "39 Train Loss 141.9007 Test MSE 133.42739460476744 Test RE 0.19445166829368038 Lambda1 0.28721046\n",
      "40 Train Loss 137.04308 Test MSE 125.69048412570828 Test RE 0.1887297524004665 Lambda1 0.30453512\n",
      "41 Train Loss 132.45474 Test MSE 121.25150933056202 Test RE 0.18536713914393296 Lambda1 0.31261772\n",
      "42 Train Loss 130.4885 Test MSE 120.8408861511705 Test RE 0.1850529962671876 Lambda1 0.31289777\n",
      "43 Train Loss 127.91614 Test MSE 121.52295843453507 Test RE 0.18557451641145278 Lambda1 0.31283957\n",
      "44 Train Loss 125.16243 Test MSE 116.06824503152585 Test RE 0.18136182593522993 Lambda1 0.32841906\n",
      "45 Train Loss 122.8237 Test MSE 110.49652187327172 Test RE 0.1769552591732935 Lambda1 0.33963153\n",
      "46 Train Loss 120.07838 Test MSE 107.14541005151766 Test RE 0.17425127168403123 Lambda1 0.342557\n",
      "47 Train Loss 116.615974 Test MSE 103.80543548271136 Test RE 0.17151385879606867 Lambda1 0.3506533\n",
      "48 Train Loss 112.35051 Test MSE 94.14094326513182 Test RE 0.16333469405575185 Lambda1 0.3723594\n",
      "49 Train Loss 108.91893 Test MSE 89.53606313305627 Test RE 0.15928987372442355 Lambda1 0.38371772\n",
      "50 Train Loss 105.85254 Test MSE 87.2354105249985 Test RE 0.15723005821432515 Lambda1 0.39005256\n",
      "51 Train Loss 99.36287 Test MSE 80.80906510072472 Test RE 0.1513279719383062 Lambda1 0.40868086\n",
      "52 Train Loss 95.91481 Test MSE 78.40704939551523 Test RE 0.14906192520744413 Lambda1 0.418522\n",
      "53 Train Loss 91.42545 Test MSE 76.9592750673303 Test RE 0.14767931005368248 Lambda1 0.42943206\n",
      "54 Train Loss 87.94635 Test MSE 76.42388324188175 Test RE 0.14716472433134578 Lambda1 0.4339695\n",
      "55 Train Loss 85.74591 Test MSE 75.1351869485167 Test RE 0.1459186678999567 Lambda1 0.43951872\n",
      "56 Train Loss 83.66159 Test MSE 74.06542376702038 Test RE 0.1448761601078136 Lambda1 0.44773135\n",
      "57 Train Loss 81.47228 Test MSE 71.70679535434496 Test RE 0.14255069124014383 Lambda1 0.45898843\n",
      "58 Train Loss 79.11226 Test MSE 68.67488999963976 Test RE 0.1395044807199147 Lambda1 0.4669422\n",
      "59 Train Loss 77.08881 Test MSE 65.89172285708791 Test RE 0.1366484161185633 Lambda1 0.47463694\n",
      "60 Train Loss 75.28479 Test MSE 64.58118347750242 Test RE 0.13528267131369257 Lambda1 0.48082113\n",
      "61 Train Loss 74.37834 Test MSE 64.57935387219057 Test RE 0.13528075499973102 Lambda1 0.48310858\n",
      "62 Train Loss 71.57752 Test MSE 63.91236314903032 Test RE 0.1345803360894353 Lambda1 0.49058408\n",
      "63 Train Loss 69.04791 Test MSE 63.33679032422803 Test RE 0.13397297339448866 Lambda1 0.5022643\n",
      "64 Train Loss 67.87244 Test MSE 61.769471259443534 Test RE 0.13230495595671965 Lambda1 0.5091766\n",
      "65 Train Loss 66.84369 Test MSE 61.002543362258955 Test RE 0.13148104337361408 Lambda1 0.51423544\n",
      "66 Train Loss 65.91617 Test MSE 60.27917428195571 Test RE 0.13069916648541038 Lambda1 0.51769596\n",
      "67 Train Loss 65.17488 Test MSE 59.999757466971396 Test RE 0.1303958945502101 Lambda1 0.5224364\n",
      "68 Train Loss 64.824585 Test MSE 60.03230915242778 Test RE 0.13043126161478377 Lambda1 0.5235301\n",
      "69 Train Loss 63.697906 Test MSE 59.177837052650986 Test RE 0.1294996857285145 Lambda1 0.52631474\n",
      "70 Train Loss 63.191208 Test MSE 58.85846760708304 Test RE 0.12914977268459701 Lambda1 0.5280072\n",
      "71 Train Loss 62.64538 Test MSE 58.65128928633218 Test RE 0.12892227253514368 Lambda1 0.530115\n",
      "72 Train Loss 61.825813 Test MSE 58.310928008511034 Test RE 0.12854765165351575 Lambda1 0.531843\n",
      "73 Train Loss 60.95578 Test MSE 57.46631170853493 Test RE 0.1276132686610403 Lambda1 0.5363497\n",
      "74 Train Loss 60.567562 Test MSE 57.20842853569587 Test RE 0.12732661098896433 Lambda1 0.5391443\n",
      "Training time: 219.40\n",
      "Training time: 219.40\n",
      "inv_HT_stan_tune19\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 858.6233 Test MSE 853.8071862039145 Test RE 0.49189122320287476 Lambda1 0.16205984\n",
      "1 Train Loss 815.71967 Test MSE 829.1564354651773 Test RE 0.4847383816224612 Lambda1 0.16489385\n",
      "2 Train Loss 765.53516 Test MSE 777.18991961342 Test RE 0.46930237294218663 Lambda1 0.16398495\n",
      "3 Train Loss 671.954 Test MSE 682.5671311351047 Test RE 0.43980671526818466 Lambda1 0.15543273\n",
      "4 Train Loss 537.6922 Test MSE 548.0658971352568 Test RE 0.39409920638964946 Lambda1 0.13843034\n",
      "5 Train Loss 422.08224 Test MSE 413.53536563957346 Test RE 0.3423304246772586 Lambda1 0.09552245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 323.1372 Test MSE 323.7151090052221 Test RE 0.30288004116855677 Lambda1 0.046953663\n",
      "7 Train Loss 273.829 Test MSE 277.197546499675 Test RE 0.28027467756646623 Lambda1 0.0353465\n",
      "8 Train Loss 255.20036 Test MSE 267.702222826664 Test RE 0.27543248428574424 Lambda1 0.038151246\n",
      "9 Train Loss 244.26453 Test MSE 261.5688114725824 Test RE 0.27225894051357263 Lambda1 0.044589423\n",
      "10 Train Loss 235.33165 Test MSE 251.06521115741535 Test RE 0.2667364949772064 Lambda1 0.05919022\n",
      "11 Train Loss 227.53583 Test MSE 240.79950948015613 Test RE 0.2612263423767444 Lambda1 0.07985425\n",
      "12 Train Loss 216.6905 Test MSE 227.94462418807606 Test RE 0.2541580371477362 Lambda1 0.10803818\n",
      "13 Train Loss 205.34724 Test MSE 211.9911479159761 Test RE 0.2451026653047546 Lambda1 0.1439614\n",
      "14 Train Loss 191.74239 Test MSE 200.27474910326413 Test RE 0.2382331914891755 Lambda1 0.17645156\n",
      "15 Train Loss 178.6019 Test MSE 181.52456149442196 Test RE 0.2268072165342803 Lambda1 0.22312887\n",
      "16 Train Loss 170.95306 Test MSE 173.70002747713158 Test RE 0.22186516249700453 Lambda1 0.23799607\n",
      "17 Train Loss 164.6664 Test MSE 164.95719685355667 Test RE 0.21620951601503047 Lambda1 0.25863752\n",
      "18 Train Loss 155.21416 Test MSE 154.65341514341378 Test RE 0.20934805388945432 Lambda1 0.28703955\n",
      "19 Train Loss 147.15872 Test MSE 145.58282070929806 Test RE 0.20311604685103937 Lambda1 0.29942864\n",
      "20 Train Loss 137.47964 Test MSE 131.425265386538 Test RE 0.19298724304150466 Lambda1 0.33388004\n",
      "21 Train Loss 130.3838 Test MSE 122.1963203436615 Test RE 0.18608794280499524 Lambda1 0.3569171\n",
      "22 Train Loss 125.209335 Test MSE 119.67671954280115 Test RE 0.18415944978984192 Lambda1 0.3673151\n",
      "23 Train Loss 117.49229 Test MSE 115.24865818927032 Test RE 0.18072037100551866 Lambda1 0.38710406\n",
      "24 Train Loss 110.36964 Test MSE 106.1089588147681 Test RE 0.1734064299473298 Lambda1 0.41166097\n",
      "25 Train Loss 103.80761 Test MSE 99.6178277747586 Test RE 0.16801873257934027 Lambda1 0.421526\n",
      "26 Train Loss 98.949875 Test MSE 91.1291596186382 Test RE 0.16070073136088783 Lambda1 0.44287306\n",
      "27 Train Loss 93.55705 Test MSE 86.49864928777522 Test RE 0.15656469400890513 Lambda1 0.45885882\n",
      "28 Train Loss 89.31558 Test MSE 84.45063776548085 Test RE 0.1547001153313787 Lambda1 0.46501917\n",
      "29 Train Loss 85.17189 Test MSE 79.30597232885803 Test RE 0.1499139743041426 Lambda1 0.48368406\n",
      "30 Train Loss 81.737045 Test MSE 72.75785310392817 Test RE 0.1435916243735576 Lambda1 0.5000929\n",
      "31 Train Loss 79.37731 Test MSE 70.30045054716008 Test RE 0.14114588619337698 Lambda1 0.5074137\n",
      "32 Train Loss 76.07408 Test MSE 69.29233193822057 Test RE 0.1401302056019035 Lambda1 0.51246643\n",
      "33 Train Loss 74.48258 Test MSE 68.79331522575629 Test RE 0.13962471195388232 Lambda1 0.5174762\n",
      "34 Train Loss 71.87222 Test MSE 65.14873832608124 Test RE 0.1358758191791484 Lambda1 0.53546894\n",
      "35 Train Loss 70.508286 Test MSE 63.93831999508082 Test RE 0.13460766199542618 Lambda1 0.5429168\n",
      "36 Train Loss 68.56909 Test MSE 63.459831223891 Test RE 0.1341030412203438 Lambda1 0.54800147\n",
      "37 Train Loss 66.87591 Test MSE 62.51540913002288 Test RE 0.1331014263477627 Lambda1 0.55263686\n",
      "38 Train Loss 64.92357 Test MSE 60.01412571394422 Test RE 0.13041150668218746 Lambda1 0.5630848\n",
      "39 Train Loss 63.8197 Test MSE 58.62966916493286 Test RE 0.12889850858953752 Lambda1 0.5689107\n",
      "40 Train Loss 62.08667 Test MSE 57.55680000158974 Test RE 0.1277137011084699 Lambda1 0.57189554\n",
      "41 Train Loss 60.97489 Test MSE 56.39170505108044 Test RE 0.12641446867792344 Lambda1 0.5768363\n",
      "42 Train Loss 59.756256 Test MSE 54.729143352454805 Test RE 0.12453702756906833 Lambda1 0.58578056\n",
      "43 Train Loss 58.025803 Test MSE 54.36241720312631 Test RE 0.12411908068918448 Lambda1 0.5899588\n",
      "44 Train Loss 56.880817 Test MSE 54.33660615425708 Test RE 0.12408961157999493 Lambda1 0.5913505\n",
      "45 Train Loss 55.84724 Test MSE 53.301579258855575 Test RE 0.12290207322905913 Lambda1 0.5990501\n",
      "46 Train Loss 54.244347 Test MSE 51.28532261400568 Test RE 0.1205551357848604 Lambda1 0.6174763\n",
      "47 Train Loss 52.86623 Test MSE 51.244909967313134 Test RE 0.1205076279209188 Lambda1 0.62098885\n",
      "48 Train Loss 51.38517 Test MSE 50.0701767259721 Test RE 0.1191183674798012 Lambda1 0.63292825\n",
      "49 Train Loss 50.41419 Test MSE 49.565405735498715 Test RE 0.11851641428758013 Lambda1 0.6420348\n",
      "50 Train Loss 49.868385 Test MSE 49.14049978999478 Test RE 0.11800732211668252 Lambda1 0.64693284\n",
      "51 Train Loss 49.133358 Test MSE 48.72698936294252 Test RE 0.11750976563986693 Lambda1 0.6529153\n",
      "52 Train Loss 48.669086 Test MSE 48.16264392889691 Test RE 0.11682729751193766 Lambda1 0.6599064\n",
      "53 Train Loss 48.132168 Test MSE 47.745489696943075 Test RE 0.11632025527977319 Lambda1 0.6660066\n",
      "54 Train Loss 47.160206 Test MSE 46.35331806089694 Test RE 0.11461186623747677 Lambda1 0.67884463\n",
      "55 Train Loss 46.678356 Test MSE 45.98951911265329 Test RE 0.1141612209357362 Lambda1 0.6853858\n",
      "56 Train Loss 46.34037 Test MSE 46.16736415015558 Test RE 0.11438174309540748 Lambda1 0.6866185\n",
      "57 Train Loss 45.689285 Test MSE 45.89994074786062 Test RE 0.11404998515550147 Lambda1 0.6949995\n",
      "58 Train Loss 45.47714 Test MSE 45.701408760133624 Test RE 0.11380306645500199 Lambda1 0.6971294\n",
      "59 Train Loss 45.151188 Test MSE 45.09667909965254 Test RE 0.11304762728465588 Lambda1 0.7058249\n",
      "60 Train Loss 44.578266 Test MSE 44.62922151280018 Test RE 0.11246019346509328 Lambda1 0.7174231\n",
      "61 Train Loss 44.2535 Test MSE 44.08098384279732 Test RE 0.11176731298420392 Lambda1 0.7273497\n",
      "62 Train Loss 43.381493 Test MSE 43.319075591513865 Test RE 0.11079719375879298 Lambda1 0.7457321\n",
      "63 Train Loss 42.876144 Test MSE 42.75698904276306 Test RE 0.11007602238646 Lambda1 0.7564169\n",
      "64 Train Loss 42.502148 Test MSE 42.44205611040168 Test RE 0.1096698825143465 Lambda1 0.76332\n",
      "65 Train Loss 42.067413 Test MSE 42.03119704812146 Test RE 0.1091377635736938 Lambda1 0.77030265\n",
      "66 Train Loss 41.68892 Test MSE 41.73859729934932 Test RE 0.10875721941549864 Lambda1 0.77538353\n",
      "67 Train Loss 41.172695 Test MSE 41.611921969859885 Test RE 0.10859205663488282 Lambda1 0.780538\n",
      "68 Train Loss 40.760906 Test MSE 41.19245677010458 Test RE 0.10804334413908283 Lambda1 0.78654385\n",
      "69 Train Loss 40.34498 Test MSE 40.943028013502335 Test RE 0.10771573517021936 Lambda1 0.79308873\n",
      "70 Train Loss 40.02333 Test MSE 40.72408637496724 Test RE 0.10742734575150362 Lambda1 0.7996514\n",
      "71 Train Loss 39.709686 Test MSE 40.39950546089977 Test RE 0.10699837820672639 Lambda1 0.81198955\n",
      "72 Train Loss 39.222816 Test MSE 39.75080883280881 Test RE 0.10613586301221323 Lambda1 0.8290413\n",
      "73 Train Loss 38.88325 Test MSE 39.63172694292258 Test RE 0.10597676764639544 Lambda1 0.83479786\n",
      "74 Train Loss 38.471188 Test MSE 39.52409616286587 Test RE 0.1058327653803211 Lambda1 0.83767706\n",
      "Training time: 213.94\n",
      "Training time: 213.94\n",
      "inv_HT_stan_tune19\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 933.3301 Test MSE 859.6804609727297 Test RE 0.4935801648328096 Lambda1 0.17023544\n",
      "1 Train Loss 816.8405 Test MSE 833.5768032073589 Test RE 0.48602877364101854 Lambda1 0.16187225\n",
      "2 Train Loss 745.2663 Test MSE 752.04328344095 Test RE 0.4616476074125534 Lambda1 0.1646781\n",
      "3 Train Loss 544.3515 Test MSE 537.7576015013673 Test RE 0.3903754066871423 Lambda1 0.12806389\n",
      "4 Train Loss 389.92847 Test MSE 393.3177103989885 Test RE 0.33385733460585126 Lambda1 0.08750245\n",
      "5 Train Loss 317.50098 Test MSE 318.65091081022564 Test RE 0.30050157509321457 Lambda1 0.043268308\n",
      "6 Train Loss 281.10953 Test MSE 294.28481805281194 Test RE 0.2887839813162604 Lambda1 0.0289806\n",
      "7 Train Loss 263.8813 Test MSE 280.7442388283482 Test RE 0.2820620104192276 Lambda1 0.03096984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 255.86833 Test MSE 273.9608978460049 Test RE 0.2786335838965292 Lambda1 0.030513356\n",
      "9 Train Loss 246.55905 Test MSE 264.7176404062359 Test RE 0.2738927976840919 Lambda1 0.047177687\n",
      "10 Train Loss 238.81384 Test MSE 256.236510677867 Test RE 0.2694695371841065 Lambda1 0.06653066\n",
      "11 Train Loss 229.22188 Test MSE 246.17301930315423 Test RE 0.2641249311050275 Lambda1 0.09440562\n",
      "12 Train Loss 213.85452 Test MSE 229.49581744851298 Test RE 0.25502136037903805 Lambda1 0.1473611\n",
      "13 Train Loss 203.82324 Test MSE 217.08355146685815 Test RE 0.24802909556405428 Lambda1 0.18483512\n",
      "14 Train Loss 192.94518 Test MSE 204.90502161956869 Test RE 0.24097138374427698 Lambda1 0.23044221\n",
      "15 Train Loss 184.37244 Test MSE 197.293508888201 Test RE 0.23645340319949093 Lambda1 0.25624272\n",
      "16 Train Loss 174.07663 Test MSE 183.52717946390695 Test RE 0.22805487783143338 Lambda1 0.29358146\n",
      "17 Train Loss 165.2236 Test MSE 170.9870204283827 Test RE 0.22012569637063903 Lambda1 0.32436666\n",
      "18 Train Loss 157.53226 Test MSE 162.69183802539814 Test RE 0.21471977997462674 Lambda1 0.3516444\n",
      "19 Train Loss 147.4527 Test MSE 149.61703283482024 Test RE 0.20591106719750746 Lambda1 0.39515412\n",
      "20 Train Loss 142.33943 Test MSE 145.8000512932808 Test RE 0.20326752959484626 Lambda1 0.39642182\n",
      "21 Train Loss 136.57326 Test MSE 137.43935519842458 Test RE 0.1973534510373048 Lambda1 0.43333974\n",
      "22 Train Loss 129.9316 Test MSE 127.58610826077518 Test RE 0.1901476077171299 Lambda1 0.46508658\n",
      "23 Train Loss 123.711395 Test MSE 122.21527341007373 Test RE 0.18610237368302487 Lambda1 0.47468263\n",
      "24 Train Loss 116.17081 Test MSE 116.30033891487118 Test RE 0.18154306391660308 Lambda1 0.49748197\n",
      "25 Train Loss 110.73708 Test MSE 110.26706460288824 Test RE 0.1767714308889737 Lambda1 0.51122785\n",
      "26 Train Loss 103.98271 Test MSE 100.01580609160743 Test RE 0.16835401975570885 Lambda1 0.52219\n",
      "27 Train Loss 97.59778 Test MSE 93.43083114457475 Test RE 0.16271750511858207 Lambda1 0.53108954\n",
      "28 Train Loss 94.59842 Test MSE 91.8161521464106 Test RE 0.16130532880852566 Lambda1 0.53618014\n",
      "29 Train Loss 89.8735 Test MSE 85.93575020125094 Test RE 0.1560544318752275 Lambda1 0.55731994\n",
      "30 Train Loss 85.22216 Test MSE 81.30731928514498 Test RE 0.15179378556133477 Lambda1 0.5752072\n",
      "31 Train Loss 80.547424 Test MSE 73.31548563147419 Test RE 0.14414083329691088 Lambda1 0.61017436\n",
      "32 Train Loss 77.9461 Test MSE 72.28219311366077 Test RE 0.14312148414450415 Lambda1 0.6171176\n",
      "33 Train Loss 75.406715 Test MSE 71.2562638566815 Test RE 0.1421021648980698 Lambda1 0.6327251\n",
      "34 Train Loss 72.63371 Test MSE 69.41368023619333 Test RE 0.14025285354262554 Lambda1 0.63703215\n",
      "35 Train Loss 69.095436 Test MSE 65.54057155961445 Test RE 0.1362838151709953 Lambda1 0.6507633\n",
      "36 Train Loss 66.989136 Test MSE 63.74809042781266 Test RE 0.13440727018253207 Lambda1 0.66031504\n",
      "37 Train Loss 64.51609 Test MSE 62.5254508234798 Test RE 0.13311211579275517 Lambda1 0.66691303\n",
      "38 Train Loss 63.080566 Test MSE 61.83110092248734 Test RE 0.1323709422509102 Lambda1 0.67239684\n",
      "39 Train Loss 59.973476 Test MSE 60.080442236425085 Test RE 0.13048354013820399 Lambda1 0.68204576\n",
      "40 Train Loss 58.855156 Test MSE 58.502593926970476 Test RE 0.12875874408117016 Lambda1 0.68379116\n",
      "41 Train Loss 57.493668 Test MSE 57.01177533185396 Test RE 0.12710758084203874 Lambda1 0.6873104\n",
      "42 Train Loss 56.45148 Test MSE 55.17180458700238 Test RE 0.12503965458180225 Lambda1 0.6937801\n",
      "43 Train Loss 55.41563 Test MSE 54.15397586707161 Test RE 0.12388089780695533 Lambda1 0.6978262\n",
      "44 Train Loss 54.127323 Test MSE 52.63830886129009 Test RE 0.12213499940932969 Lambda1 0.7074632\n",
      "45 Train Loss 53.027275 Test MSE 51.70675254870497 Test RE 0.12104944483084709 Lambda1 0.7122328\n",
      "46 Train Loss 52.000664 Test MSE 50.66125279008129 Test RE 0.11981939798424666 Lambda1 0.7204737\n",
      "47 Train Loss 51.074818 Test MSE 49.83082229310739 Test RE 0.11883331091716712 Lambda1 0.7251327\n",
      "48 Train Loss 49.990585 Test MSE 50.34152320115529 Test RE 0.11944070183330471 Lambda1 0.724897\n",
      "49 Train Loss 48.457813 Test MSE 49.119012322631704 Test RE 0.11798151900371555 Lambda1 0.7363106\n",
      "50 Train Loss 47.750736 Test MSE 48.08876137853687 Test RE 0.11673765530424432 Lambda1 0.7464858\n",
      "51 Train Loss 47.34826 Test MSE 47.40346914879846 Test RE 0.11590288160539204 Lambda1 0.75113636\n",
      "52 Train Loss 46.766846 Test MSE 46.26002147910212 Test RE 0.11449646693390636 Lambda1 0.76241845\n",
      "53 Train Loss 46.065964 Test MSE 45.51169839370129 Test RE 0.11356661780135052 Lambda1 0.7712052\n",
      "54 Train Loss 45.06184 Test MSE 45.38227990036883 Test RE 0.11340503204841143 Lambda1 0.77647066\n",
      "55 Train Loss 44.456013 Test MSE 45.22453134388288 Test RE 0.11320776283722872 Lambda1 0.78042704\n",
      "56 Train Loss 43.497433 Test MSE 43.956181092368816 Test RE 0.11160898218127636 Lambda1 0.794865\n",
      "57 Train Loss 42.713223 Test MSE 43.286380567846706 Test RE 0.1107553738376217 Lambda1 0.80148345\n",
      "58 Train Loss 42.16495 Test MSE 42.67522291590965 Test RE 0.10997072033710975 Lambda1 0.812516\n",
      "59 Train Loss 41.582375 Test MSE 42.19736114523646 Test RE 0.10935328075225328 Lambda1 0.82311994\n",
      "60 Train Loss 41.107197 Test MSE 41.75640859596132 Test RE 0.1087804221655374 Lambda1 0.8342469\n",
      "61 Train Loss 40.605877 Test MSE 41.543204063749386 Test RE 0.10850235515365772 Lambda1 0.8420327\n",
      "62 Train Loss 39.853985 Test MSE 40.47633615671056 Test RE 0.10710007320226532 Lambda1 0.8716868\n",
      "63 Train Loss 39.190536 Test MSE 40.370811970736945 Test RE 0.10696037400084589 Lambda1 0.88432914\n",
      "64 Train Loss 38.67047 Test MSE 39.88748820521564 Test RE 0.10631817546179487 Lambda1 0.9023722\n",
      "65 Train Loss 38.350807 Test MSE 39.494413316804405 Test RE 0.10579301737826931 Lambda1 0.9152557\n",
      "66 Train Loss 37.921917 Test MSE 38.875923803237264 Test RE 0.10496137993018732 Lambda1 0.9298726\n",
      "67 Train Loss 37.022697 Test MSE 38.16858089575608 Test RE 0.10400211643377932 Lambda1 0.94792867\n",
      "68 Train Loss 36.52144 Test MSE 38.01891464442444 Test RE 0.1037980101315995 Lambda1 0.9519658\n",
      "69 Train Loss 35.826912 Test MSE 37.491262446613426 Test RE 0.1030752039956001 Lambda1 0.97230387\n",
      "70 Train Loss 35.355133 Test MSE 37.088692190482504 Test RE 0.10252031465636939 Lambda1 0.98799336\n",
      "71 Train Loss 34.987465 Test MSE 36.811912945251116 Test RE 0.1021370626318654 Lambda1 0.9977263\n",
      "72 Train Loss 34.627953 Test MSE 36.594127197135066 Test RE 0.10183448403238715 Lambda1 1.0077115\n",
      "73 Train Loss 34.113373 Test MSE 36.39369618913846 Test RE 0.10155522046176522 Lambda1 1.02388\n",
      "74 Train Loss 33.423214 Test MSE 36.0678317806955 Test RE 0.10109954204841384 Lambda1 1.0267162\n",
      "Training time: 219.68\n",
      "Training time: 219.68\n",
      "inv_HT_stan_tune19\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 891.3896 Test MSE 865.3417088376141 Test RE 0.4952026830697247 Lambda1 -0.072675034\n",
      "1 Train Loss 817.206 Test MSE 837.3809259193225 Test RE 0.487136535072004 Lambda1 -0.059996016\n",
      "2 Train Loss 781.12244 Test MSE 790.7527280750949 Test RE 0.4733795800900092 Lambda1 -0.032477364\n",
      "3 Train Loss 724.32135 Test MSE 744.3219635864298 Test RE 0.45927159691504854 Lambda1 0.04345539\n",
      "4 Train Loss 633.72614 Test MSE 643.7629480841211 Test RE 0.42712221417367496 Lambda1 0.04028658\n",
      "5 Train Loss 517.8803 Test MSE 514.4123963268835 Test RE 0.38180787654637055 Lambda1 0.011399412\n",
      "6 Train Loss 377.64383 Test MSE 368.0102484940041 Test RE 0.3229379821186947 Lambda1 0.003650069\n",
      "7 Train Loss 295.8701 Test MSE 298.5819667507527 Test RE 0.29088475296213373 Lambda1 -0.000936102\n",
      "8 Train Loss 266.51 Test MSE 283.9699769780799 Test RE 0.28367782191772617 Lambda1 0.0011505272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 259.3395 Test MSE 283.50926079744244 Test RE 0.2834476074117101 Lambda1 0.00057867594\n",
      "10 Train Loss 256.36237 Test MSE 280.8151777231839 Test RE 0.28209764410409055 Lambda1 0.00159918\n",
      "11 Train Loss 255.12268 Test MSE 279.6553962087572 Test RE 0.281514502295354 Lambda1 0.0030023619\n",
      "12 Train Loss 253.62755 Test MSE 277.89054443608455 Test RE 0.28062480427721537 Lambda1 0.0059987004\n",
      "13 Train Loss 249.7857 Test MSE 270.800465506682 Test RE 0.27702175347621566 Lambda1 0.020461852\n",
      "14 Train Loss 243.8411 Test MSE 263.81182695110954 Test RE 0.27342379157806235 Lambda1 0.034637034\n",
      "15 Train Loss 239.52567 Test MSE 257.81468616908643 Test RE 0.27029810258042053 Lambda1 0.049233116\n",
      "16 Train Loss 233.30986 Test MSE 249.07132402602923 Test RE 0.2656752117135589 Lambda1 0.07289568\n",
      "17 Train Loss 226.96779 Test MSE 241.91290184069797 Test RE 0.2618295661737941 Lambda1 0.09293388\n",
      "18 Train Loss 221.49301 Test MSE 235.51710209817105 Test RE 0.25834519895908786 Lambda1 0.10849855\n",
      "19 Train Loss 214.11511 Test MSE 227.49594168086597 Test RE 0.25390777365101264 Lambda1 0.12511884\n",
      "20 Train Loss 208.74947 Test MSE 222.44992932914323 Test RE 0.25107606104214675 Lambda1 0.1374586\n",
      "21 Train Loss 202.80775 Test MSE 217.76964006351 Test RE 0.24842073207973203 Lambda1 0.15063308\n",
      "22 Train Loss 197.15727 Test MSE 210.32096817031982 Test RE 0.2441352310431332 Lambda1 0.16648965\n",
      "23 Train Loss 191.7459 Test MSE 203.0546312123278 Test RE 0.23988087274300568 Lambda1 0.19145079\n",
      "24 Train Loss 185.75273 Test MSE 194.73035983768182 Test RE 0.23491243349653404 Lambda1 0.21179369\n",
      "25 Train Loss 182.22379 Test MSE 191.27656310603226 Test RE 0.23281987433023532 Lambda1 0.22019511\n",
      "26 Train Loss 177.0629 Test MSE 187.40182369774263 Test RE 0.23044966284934318 Lambda1 0.23146023\n",
      "27 Train Loss 172.15396 Test MSE 178.25208926752248 Test RE 0.2247535111274325 Lambda1 0.25630423\n",
      "28 Train Loss 166.40533 Test MSE 170.44923837526363 Test RE 0.21977925815523924 Lambda1 0.26553893\n",
      "29 Train Loss 160.94464 Test MSE 162.61599948651056 Test RE 0.21466972850087812 Lambda1 0.27359778\n",
      "30 Train Loss 153.39104 Test MSE 151.76328617534216 Test RE 0.20738270341009035 Lambda1 0.28257123\n",
      "31 Train Loss 150.46617 Test MSE 149.61390198482135 Test RE 0.20590891276353387 Lambda1 0.28487644\n",
      "32 Train Loss 144.24121 Test MSE 141.87846505189938 Test RE 0.20051525148153732 Lambda1 0.2953479\n",
      "33 Train Loss 138.42026 Test MSE 138.187541783466 Test RE 0.19788989417238204 Lambda1 0.30584547\n",
      "34 Train Loss 133.36633 Test MSE 133.30278142690136 Test RE 0.19436084398929473 Lambda1 0.32250297\n",
      "35 Train Loss 126.3536 Test MSE 122.121282148624 Test RE 0.18603079768119044 Lambda1 0.34434572\n",
      "36 Train Loss 121.740105 Test MSE 117.61715343443635 Test RE 0.18256793478808156 Lambda1 0.3483387\n",
      "37 Train Loss 115.810776 Test MSE 109.351343971312 Test RE 0.17603589520736168 Lambda1 0.36322027\n",
      "38 Train Loss 109.83332 Test MSE 99.41101130461215 Test RE 0.16784423020197697 Lambda1 0.3766248\n",
      "39 Train Loss 106.202324 Test MSE 96.89818742955998 Test RE 0.1657093436933257 Lambda1 0.383346\n",
      "40 Train Loss 101.46778 Test MSE 96.91590255605013 Test RE 0.1657244906630621 Lambda1 0.38289618\n",
      "41 Train Loss 97.825554 Test MSE 93.95052556200845 Test RE 0.1631694229237512 Lambda1 0.3903724\n",
      "42 Train Loss 94.95048 Test MSE 89.59939538582108 Test RE 0.15934619964988905 Lambda1 0.40124813\n",
      "43 Train Loss 91.71925 Test MSE 84.4396375284884 Test RE 0.15469003966235115 Lambda1 0.41505417\n",
      "44 Train Loss 87.63916 Test MSE 81.54198546481733 Test RE 0.15201267854159678 Lambda1 0.42932308\n",
      "45 Train Loss 84.19973 Test MSE 78.45946745427827 Test RE 0.14911174363155713 Lambda1 0.44140056\n",
      "46 Train Loss 82.0031 Test MSE 76.406795939282 Test RE 0.1471482714330785 Lambda1 0.4488553\n",
      "47 Train Loss 80.10699 Test MSE 75.97054348237243 Test RE 0.1467275910018167 Lambda1 0.45584413\n",
      "48 Train Loss 77.14324 Test MSE 71.78796544569701 Test RE 0.14263135012299114 Lambda1 0.47397655\n",
      "49 Train Loss 75.18956 Test MSE 70.49944473727568 Test RE 0.14134551054205183 Lambda1 0.4817632\n",
      "50 Train Loss 73.42453 Test MSE 67.75260482113478 Test RE 0.13856456063284325 Lambda1 0.49682814\n",
      "51 Train Loss 71.75298 Test MSE 66.7536787665751 Test RE 0.13753928833975781 Lambda1 0.5001208\n",
      "52 Train Loss 70.57679 Test MSE 64.44171881663323 Test RE 0.13513651923120332 Lambda1 0.5125485\n",
      "53 Train Loss 69.441605 Test MSE 63.4044552523846 Test RE 0.13404451831868144 Lambda1 0.5153823\n",
      "54 Train Loss 67.87501 Test MSE 61.10876851719539 Test RE 0.13159546908959902 Lambda1 0.5243534\n",
      "55 Train Loss 66.07615 Test MSE 59.48771586500651 Test RE 0.12983829909087122 Lambda1 0.5306738\n",
      "56 Train Loss 65.05078 Test MSE 58.00766497957982 Test RE 0.12821294107379785 Lambda1 0.5395692\n",
      "57 Train Loss 64.09711 Test MSE 58.29951452495214 Test RE 0.1285350704070819 Lambda1 0.5377444\n",
      "58 Train Loss 63.04118 Test MSE 56.351351811111265 Test RE 0.12636923023099392 Lambda1 0.5483631\n",
      "59 Train Loss 61.69529 Test MSE 55.322014548037735 Test RE 0.12520975446450877 Lambda1 0.5585579\n",
      "60 Train Loss 60.705498 Test MSE 55.29690695960547 Test RE 0.12518133836488876 Lambda1 0.5612453\n",
      "61 Train Loss 59.905277 Test MSE 54.6241974581526 Test RE 0.12441756725621138 Lambda1 0.56372833\n",
      "62 Train Loss 58.98735 Test MSE 53.40657364486082 Test RE 0.12302306100749646 Lambda1 0.57159674\n",
      "63 Train Loss 57.7489 Test MSE 52.39804451988073 Test RE 0.12185594174223476 Lambda1 0.58278936\n",
      "64 Train Loss 55.783585 Test MSE 50.886734969159725 Test RE 0.12008574694260948 Lambda1 0.60304606\n",
      "65 Train Loss 53.82476 Test MSE 50.1207594904348 Test RE 0.11917852120535678 Lambda1 0.6265244\n",
      "66 Train Loss 52.634502 Test MSE 48.971409249912355 Test RE 0.11780411786888169 Lambda1 0.6351512\n",
      "67 Train Loss 51.392597 Test MSE 48.48196613585665 Test RE 0.11721394488918571 Lambda1 0.6394489\n",
      "68 Train Loss 49.837624 Test MSE 47.2818115234059 Test RE 0.11575405781468742 Lambda1 0.6560757\n",
      "69 Train Loss 48.310844 Test MSE 45.82043308817758 Test RE 0.11395116392083256 Lambda1 0.66982764\n",
      "70 Train Loss 47.33559 Test MSE 45.169522963690284 Test RE 0.11313889235613188 Lambda1 0.67420876\n",
      "71 Train Loss 46.243996 Test MSE 44.547593560276134 Test RE 0.11235730015406466 Lambda1 0.6792523\n",
      "72 Train Loss 45.27187 Test MSE 44.489125307490134 Test RE 0.1122835420510534 Lambda1 0.67837065\n",
      "73 Train Loss 44.106243 Test MSE 43.94722239080922 Test RE 0.11159760809816693 Lambda1 0.6887193\n",
      "74 Train Loss 43.64771 Test MSE 43.63361413568916 Test RE 0.11119871381657136 Lambda1 0.69456476\n",
      "Training time: 216.62\n",
      "Training time: 216.62\n",
      "inv_HT_stan_tune19\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 1390.4196 Test MSE 1181.5925807052572 Test RE 0.5786594478580088 Lambda1 0.14124323\n",
      "1 Train Loss 820.0561 Test MSE 835.9318468459418 Test RE 0.4867148601870095 Lambda1 0.16589752\n",
      "2 Train Loss 790.43787 Test MSE 794.0705536822776 Test RE 0.4743716391877369 Lambda1 0.169875\n",
      "3 Train Loss 714.14276 Test MSE 725.6853213754817 Test RE 0.45348543179493467 Lambda1 0.16176756\n",
      "4 Train Loss 604.3062 Test MSE 601.7966363413112 Test RE 0.41296576369933274 Lambda1 0.1481884\n",
      "5 Train Loss 429.8241 Test MSE 407.498670405575 Test RE 0.33982260804826503 Lambda1 0.13496515\n",
      "6 Train Loss 341.37506 Test MSE 335.9937715179461 Test RE 0.30857076982958653 Lambda1 0.10867633\n",
      "7 Train Loss 296.02014 Test MSE 298.5383473345152 Test RE 0.29086350471548034 Lambda1 0.092399955\n",
      "8 Train Loss 267.43518 Test MSE 285.1625668974646 Test RE 0.2842728792833416 Lambda1 0.06821747\n",
      "9 Train Loss 254.31967 Test MSE 273.07520124437167 Test RE 0.2781828177417107 Lambda1 0.074486665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Train Loss 241.68968 Test MSE 260.86196564483316 Test RE 0.2718908245984959 Lambda1 0.09196885\n",
      "11 Train Loss 234.03899 Test MSE 254.68123916270434 Test RE 0.26865049664437723 Lambda1 0.11293759\n",
      "12 Train Loss 227.9635 Test MSE 248.57272135520935 Test RE 0.2654091579407334 Lambda1 0.13416247\n",
      "13 Train Loss 221.65056 Test MSE 242.14362024668623 Test RE 0.2619543931325507 Lambda1 0.153787\n",
      "14 Train Loss 215.86217 Test MSE 236.3503523052192 Test RE 0.2588018030100431 Lambda1 0.16839309\n",
      "15 Train Loss 210.60922 Test MSE 231.6910974976724 Test RE 0.25623818179594854 Lambda1 0.19370836\n",
      "16 Train Loss 206.18742 Test MSE 223.5694260727415 Test RE 0.2517070481654477 Lambda1 0.22285025\n",
      "17 Train Loss 199.9756 Test MSE 217.15349765426032 Test RE 0.24806905089976058 Lambda1 0.25264153\n",
      "18 Train Loss 193.65712 Test MSE 207.58218888365315 Test RE 0.24254046965144874 Lambda1 0.2822878\n",
      "19 Train Loss 188.03427 Test MSE 199.22556928599013 Test RE 0.23760835568133606 Lambda1 0.3081943\n",
      "20 Train Loss 181.83109 Test MSE 193.39939334491052 Test RE 0.23410825313108474 Lambda1 0.3337196\n",
      "21 Train Loss 177.86067 Test MSE 191.71465388550158 Test RE 0.23308634164727493 Lambda1 0.33797514\n",
      "22 Train Loss 170.84848 Test MSE 179.32101687166843 Test RE 0.22542639563131497 Lambda1 0.3706411\n",
      "23 Train Loss 164.66759 Test MSE 168.87283289556618 Test RE 0.21876057945465363 Lambda1 0.3986675\n",
      "24 Train Loss 160.21584 Test MSE 167.95343240028666 Test RE 0.21816426362840058 Lambda1 0.40757516\n",
      "25 Train Loss 154.31143 Test MSE 159.14018910242342 Test RE 0.21236312402658566 Lambda1 0.43516597\n",
      "26 Train Loss 150.8259 Test MSE 155.30305965261215 Test RE 0.2097872918101306 Lambda1 0.44874293\n",
      "27 Train Loss 145.85352 Test MSE 147.55798930457533 Test RE 0.20448927490607036 Lambda1 0.46805945\n",
      "28 Train Loss 142.27725 Test MSE 143.33403820351157 Test RE 0.20154119946662002 Lambda1 0.4753417\n",
      "29 Train Loss 135.24913 Test MSE 137.09636042832807 Test RE 0.19710703876722785 Lambda1 0.4872359\n",
      "30 Train Loss 132.18823 Test MSE 133.12915954623952 Test RE 0.1942342288877323 Lambda1 0.49479145\n",
      "31 Train Loss 127.34761 Test MSE 130.84883833138198 Test RE 0.1925635599953444 Lambda1 0.49152687\n",
      "32 Train Loss 123.700966 Test MSE 126.07845602357872 Test RE 0.18902080634881546 Lambda1 0.4891654\n",
      "33 Train Loss 119.70957 Test MSE 119.2556005847983 Test RE 0.1838351537208787 Lambda1 0.49859732\n",
      "34 Train Loss 114.66126 Test MSE 114.38332689876883 Test RE 0.1800406336121464 Lambda1 0.507362\n",
      "35 Train Loss 109.673004 Test MSE 109.94838692625645 Test RE 0.1765158066487063 Lambda1 0.5170231\n",
      "36 Train Loss 105.597336 Test MSE 106.5271942932127 Test RE 0.17374784031104445 Lambda1 0.5277922\n",
      "37 Train Loss 102.22888 Test MSE 104.11950433145404 Test RE 0.171773124974949 Lambda1 0.53339565\n",
      "38 Train Loss 97.47198 Test MSE 98.68330328397757 Test RE 0.16722877554612917 Lambda1 0.55462843\n",
      "39 Train Loss 94.50088 Test MSE 94.37027858360135 Test RE 0.16353352161981105 Lambda1 0.56689286\n",
      "40 Train Loss 91.65665 Test MSE 90.72672371739137 Test RE 0.16034550312596693 Lambda1 0.57576287\n",
      "41 Train Loss 85.8973 Test MSE 82.08939024098949 Test RE 0.1525220681392174 Lambda1 0.5955352\n",
      "42 Train Loss 81.63573 Test MSE 77.10838840438784 Test RE 0.1478223097005691 Lambda1 0.6107197\n",
      "43 Train Loss 78.150116 Test MSE 76.25567667119438 Test RE 0.14700268264816835 Lambda1 0.61386067\n",
      "44 Train Loss 75.16668 Test MSE 72.25483857955996 Test RE 0.14309440007427163 Lambda1 0.6209693\n",
      "45 Train Loss 71.58006 Test MSE 66.77527801273872 Test RE 0.13756153808487653 Lambda1 0.63748044\n",
      "46 Train Loss 69.17904 Test MSE 65.8175074819364 Test RE 0.1365714392976302 Lambda1 0.6412554\n",
      "47 Train Loss 66.89328 Test MSE 63.76907333440392 Test RE 0.13442938867377213 Lambda1 0.6470465\n",
      "48 Train Loss 65.015915 Test MSE 59.53848096314325 Test RE 0.12989368740342178 Lambda1 0.6614854\n",
      "49 Train Loss 63.641838 Test MSE 57.5906857839081 Test RE 0.12775129042803485 Lambda1 0.67175996\n",
      "50 Train Loss 60.91393 Test MSE 56.81558062269951 Test RE 0.1268886846021203 Lambda1 0.6770544\n",
      "51 Train Loss 58.888664 Test MSE 54.31374315137803 Test RE 0.12406350248112735 Lambda1 0.6864481\n",
      "52 Train Loss 58.04179 Test MSE 53.388639182245754 Test RE 0.12300240308471876 Lambda1 0.69293505\n",
      "53 Train Loss 54.771317 Test MSE 49.83408465292729 Test RE 0.11883720078548546 Lambda1 0.712122\n",
      "54 Train Loss 53.72412 Test MSE 48.82982312246873 Test RE 0.11763369697992411 Lambda1 0.7193334\n",
      "55 Train Loss 52.28501 Test MSE 47.77846829020983 Test RE 0.11636042049988085 Lambda1 0.72742754\n",
      "56 Train Loss 51.483353 Test MSE 47.22028961075247 Test RE 0.11567872515648826 Lambda1 0.7260755\n",
      "57 Train Loss 50.211773 Test MSE 46.28095093199134 Test RE 0.11452236486207389 Lambda1 0.72421396\n",
      "58 Train Loss 49.096718 Test MSE 45.426636025366804 Test RE 0.1134604389084379 Lambda1 0.72629863\n",
      "59 Train Loss 47.511703 Test MSE 44.55759683921345 Test RE 0.11236991450913246 Lambda1 0.73289436\n",
      "60 Train Loss 46.41254 Test MSE 44.27701430367371 Test RE 0.11201555487302144 Lambda1 0.73781276\n",
      "61 Train Loss 44.563908 Test MSE 42.24136904663012 Test RE 0.1094102885152593 Lambda1 0.7492843\n",
      "62 Train Loss 44.067627 Test MSE 41.530829937036245 Test RE 0.10848619460567659 Lambda1 0.7540985\n",
      "63 Train Loss 42.897015 Test MSE 39.92344489175236 Test RE 0.10636608507379082 Lambda1 0.76301074\n",
      "64 Train Loss 42.04571 Test MSE 38.52321421676999 Test RE 0.10448415348494264 Lambda1 0.76997954\n",
      "65 Train Loss 41.0504 Test MSE 37.87103135572937 Test RE 0.1035959403569732 Lambda1 0.78265816\n",
      "66 Train Loss 39.855095 Test MSE 36.71166471703333 Test RE 0.1019978951838155 Lambda1 0.78651166\n",
      "67 Train Loss 39.03248 Test MSE 36.44655946657922 Test RE 0.10162895017689098 Lambda1 0.78857106\n",
      "68 Train Loss 38.37203 Test MSE 35.955387877685155 Test RE 0.1009418267031186 Lambda1 0.7860759\n",
      "69 Train Loss 37.914864 Test MSE 36.25120951831444 Test RE 0.10135622368646091 Lambda1 0.78584665\n",
      "70 Train Loss 37.04373 Test MSE 35.27276286630154 Test RE 0.09997902794545072 Lambda1 0.7876096\n",
      "71 Train Loss 36.42649 Test MSE 34.815517602968136 Test RE 0.09932889377020727 Lambda1 0.7879791\n",
      "72 Train Loss 36.081177 Test MSE 34.70230372908843 Test RE 0.09916726231189822 Lambda1 0.7895481\n",
      "73 Train Loss 35.636658 Test MSE 34.62347187077043 Test RE 0.09905456108631651 Lambda1 0.79130757\n",
      "74 Train Loss 34.941322 Test MSE 34.48572086405337 Test RE 0.0988573182404791 Lambda1 0.7903721\n",
      "Training time: 220.52\n",
      "Training time: 220.52\n"
     ]
    }
   ],
   "source": [
    "nan_tune = []\n",
    "for tune_reps in range(20):\n",
    "    label = \"inv_HT_stan_tune\" + str(tune_reps)\n",
    "    max_reps = 10 #10\n",
    "    max_iter = 75 #75\n",
    "\n",
    "    train_loss_full = []\n",
    "    test_mse_full = []\n",
    "    test_re_full = []\n",
    "    beta_full = []\n",
    "   \n",
    "    lambda1_full = []\n",
    "    elapsed_time= np.zeros((max_reps,1))\n",
    "    \n",
    "    time_threshold = np.empty((max_reps,1))\n",
    "    time_threshold[:] = np.nan\n",
    "    epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "    \n",
    "    beta_init = lrb_tune[tune_reps,1]\n",
    "\n",
    "    for reps in range(max_reps):\n",
    "        print(label)\n",
    "        'Generate Training data'\n",
    "        print(reps)\n",
    "        torch.manual_seed(reps*36)\n",
    "        \n",
    "        train_loss = []\n",
    "        test_mse_loss = []\n",
    "        test_re_loss = []   \n",
    "        beta_val = []\n",
    "        \n",
    "        lambda1_val = []\n",
    "\n",
    "        N_f = 50000 #Total number of collocation points \n",
    "        N_train = 500\n",
    "\n",
    "        layers = np.array([2,50,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "        \n",
    "        PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "        PINN.to(device)\n",
    "\n",
    "        'Neural Network Summary'\n",
    "        print(PINN)\n",
    "\n",
    "        params = list(PINN.parameters())\n",
    "\n",
    "        optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lrb_tune[tune_reps,0], \n",
    "                                  max_iter = 10, \n",
    "                                  max_eval = 15, \n",
    "                                  tolerance_grad = -1, \n",
    "                                  tolerance_change = -1, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "        \n",
    "        \n",
    "        nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "        torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "        train_loss_full.append(train_loss)\n",
    "        test_mse_full.append(test_mse_loss)\n",
    "        test_re_full.append(test_re_loss)\n",
    "        #elapsed_time[reps] = time.time() - start_time\n",
    "        beta_full.append(beta_val)\n",
    "\n",
    "        lambda1_full.append(lambda1_val)\n",
    "        \n",
    "        if(nan_flag == 1):\n",
    "            nan_tune.append(tune_reps)\n",
    "            break\n",
    "\n",
    "        print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "    mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"lambda1\": lambda1_full, \"label\": label}\n",
    "    savemat(label+'.mat', mdic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -0.8243969\n",
      "1   -0.15307483\n",
      "2   0.53936565\n",
      "3   0.5795185\n",
      "4   -1.4561002\n",
      "5   0.12198722\n",
      "6   0.7041\n",
      "7   0.68127084\n",
      "8   -1.4134965\n",
      "9   0.20109919\n",
      "10   0.64974755\n",
      "11   0.8142306\n",
      "12   -2.1422653\n",
      "13   0.05638336\n",
      "14   0.9153641\n",
      "15   0.63197887\n",
      "16   -1.7122774\n",
      "17   [[ 1.45513816e-02 -3.52495834e-02 -2.23435327e-01  6.15948671e-03\n",
      "  -1.92813063e-03  1.92751712e-03 -8.79180589e-05  4.98717593e-04\n",
      "  -4.48849169e-04 -4.14079666e-04 -1.23494596e-04  1.04078674e-04\n",
      "   8.81296073e-05  1.07403735e-04  1.13843482e-04  1.36531773e-04\n",
      "   2.01167524e-04  1.84232907e-04  2.84948590e-04  2.98798113e-04\n",
      "   3.42695421e-04  3.72112350e-04  3.99220240e-04  4.01747850e-04\n",
      "   4.43779369e-04  4.78341739e-04  4.86237754e-04  5.83171262e-04\n",
      "   6.11778407e-04  6.37944904e-04  7.34501111e-04  7.13067362e-04\n",
      "   7.13871908e-04  8.22813134e-04  8.24221352e-04  8.91324366e-04\n",
      "   8.95273814e-04  7.86903140e-04  7.60168361e-04  7.90642574e-04\n",
      "   8.37648404e-04  8.49442906e-04  8.64006288e-04  8.80231732e-04\n",
      "   9.87075153e-04  1.02021452e-03  1.11422793e-03  1.11305690e-03\n",
      "   1.18595746e-03  1.28561037e-03  1.13097928e-03  9.55266063e-04\n",
      "   8.87014496e-04  6.83439837e-04  4.35302645e-04  2.60145287e-04\n",
      "   2.45469681e-04  2.67464115e-04  1.66638929e-04  1.27551393e-04\n",
      "   1.24243787e-04  8.97852733e-05  6.20425199e-05  2.26347984e-05\n",
      "   1.24229282e-05  4.22717949e-06  1.96908368e-05  2.02296396e-05\n",
      "   3.34955112e-05  3.71668648e-05  2.24624182e-05  2.51339461e-05\n",
      "   2.01273615e-05  2.06349068e-05  1.57690829e-05]]\n",
      "18   0.762135\n",
      "19   [[0.14124323 0.16589752 0.169875   0.16176756 0.1481884  0.13496515\n",
      "  0.10867633 0.09239995 0.06821747 0.07448667 0.09196885 0.11293759\n",
      "  0.13416247 0.153787   0.16839309 0.19370836 0.22285025 0.25264153\n",
      "  0.2822878  0.3081943  0.3337196  0.33797514 0.3706411  0.3986675\n",
      "  0.40757516 0.43516597 0.44874293 0.46805945 0.4753417  0.4872359\n",
      "  0.49479145 0.49152687 0.4891654  0.49859732 0.507362   0.5170231\n",
      "  0.5277922  0.53339565 0.55462843 0.56689286 0.57576287 0.5955352\n",
      "  0.6107197  0.61386067 0.6209693  0.63748044 0.6412554  0.6470465\n",
      "  0.6614854  0.67175996 0.6770544  0.6864481  0.69293505 0.712122\n",
      "  0.7193334  0.72742754 0.7260755  0.72421396 0.72629863 0.73289436\n",
      "  0.73781276 0.7492843  0.7540985  0.76301074 0.76997954 0.78265816\n",
      "  0.78651166 0.78857106 0.7860759  0.78584665 0.7876096  0.7879791\n",
      "  0.7895481  0.79130757 0.7903721 ]]\n"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(20):\n",
    "    label = \"inv_HT_stan_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"lambda1\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrb_tune[14]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
