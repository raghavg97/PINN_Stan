{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1]).reshape(-1,1)\n",
    "n_value = np.array([1.0,3.0,5.0,8.0,10.0]).reshape(-1,1)\n",
    "\n",
    "LR_tune,N_value = np.meshgrid(lr_tune,n_value)\n",
    "\n",
    "LR_tune = LR_tune.flatten('F').reshape(-1,1)\n",
    "N_value = N_value.flatten('F').reshape(-1,1)\n",
    "\n",
    "lrn_tune = np.hstack((LR_tune,N_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_2D_4(xt): #True function for 2D_4 Heat Transfer in a rod x \\in [0,1] t \\in [0,0.1]\n",
    "    term1 = 4*u0/np.pi\n",
    "    \n",
    "    resol_n = 10000\n",
    "    \n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "\n",
    "    u = np.zeros((np.shape(xt)[0],1))\n",
    "    \n",
    "    for i in range(resol_n):\n",
    "        j = 2*i-1\n",
    "        term2 = np.sin(j*np.pi*x)/j\n",
    "        term3 = np.exp(-1*np.square(j*np.pi)*t)\n",
    "        \n",
    "        u = u + term2*term3\n",
    "        \n",
    "    u = term1*u\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = 50.0\n",
    "loss_thresh = 0.1\n",
    "\n",
    "x_ll = np.array(0.0)\n",
    "x_ul = np.array(1.0)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,100).reshape(-1,1)\n",
    "t = np.linspace(0,0.1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = true_2D_4(xt)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_f,N_train,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #X_Train\n",
    "    np.random.seed(seed)\n",
    "    x_train = np.random.uniform(x_ll,x_ul,(N_train,1))\n",
    "    t_train = np.random.uniform(0,0.1,(N_train,1))\n",
    "    \n",
    "    xt_train = np.hstack((x_train,t_train))\n",
    "    u_train = true_2D_4(xt_train)\n",
    "    \n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "        \n",
    "    \n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requiresGrad = True\n",
    "        \n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    \n",
    "    def loss_PDE(self, xt_coll,f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_tt[:,[0]]\n",
    "                \n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        f = du_dt - self.lambda1*d2u_dx2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_train = self.loss_function(self.forward(xt_train),u_train)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "        \n",
    "        #print(self.iter,\"train_loss\",loss_train.cpu().detach().numpy(),\"F Loss\",(loss_f).cpu().detach().numpy())\n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                    \n",
    "    \n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_coll,f_hat, xt_train, u_train,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xt_coll, xt_train, u_train = trainingdata(N_f,N_train,123)\n",
    "    \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_train = torch.from_numpy(xt_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    nan_flag = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_coll,f_hat, xt_train, u_train,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_coll,f_hat, xt_train, u_train).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1],\"Lambda1\",lambda1_val[-1])\n",
    "\n",
    "        if(np.isnan(loss_np)):\n",
    "            nan_flag =1\n",
    "            print(\"NAN BREAK!\")\n",
    "            break\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_HT_atanh_tune0\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1023.9092 Test MSE 1022.9456135425443 Test RE 0.5384128827633573 Lambda1 -0.07380828\n",
      "1 Train Loss 854.79126 Test MSE 858.0607044421741 Test RE 0.49311495905616715 Lambda1 -0.091861814\n",
      "2 Train Loss 854.72296 Test MSE 858.0670821434494 Test RE 0.4931167916386656 Lambda1 -0.09217849\n",
      "3 Train Loss 854.7226 Test MSE 858.0684574910181 Test RE 0.49311718683312233 Lambda1 -0.09218619\n",
      "4 Train Loss 854.7218 Test MSE 858.0725913512766 Test RE 0.4931183746608933 Lambda1 -0.09220729\n",
      "5 Train Loss 854.7218 Test MSE 858.0734772772109 Test RE 0.4931186292234144 Lambda1 -0.09221147\n",
      "6 Train Loss 854.72156 Test MSE 858.0742099419773 Test RE 0.49311883974767456 Lambda1 -0.09221516\n",
      "7 Train Loss 854.72156 Test MSE 858.0749053457288 Test RE 0.4931190395652531 Lambda1 -0.09221839\n",
      "8 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "9 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "10 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "11 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "12 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "13 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "14 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "15 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "16 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "17 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "18 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "19 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "20 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "21 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "22 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "23 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "24 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "25 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "26 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "27 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "28 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "29 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "30 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "31 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "32 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "33 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "34 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "35 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "36 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "37 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "38 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "39 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "40 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "41 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "42 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "43 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "44 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "45 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "46 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "47 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "48 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "49 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "50 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "51 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "52 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "53 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "54 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "55 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "56 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "57 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "58 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "59 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "60 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "61 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "62 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "63 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "64 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "65 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "66 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "67 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "68 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "69 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "70 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "71 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "72 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "73 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "74 Train Loss 854.7215 Test MSE 858.075513913333 Test RE 0.49311921443123363 Lambda1 -0.0922214\n",
      "Training time: 239.95\n",
      "Training time: 239.95\n",
      "inv_HT_atanh_tune0\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 871.52655 Test MSE 873.5219062977467 Test RE 0.4975377880025702 Lambda1 -0.07306287\n",
      "1 Train Loss 854.7536 Test MSE 858.0503570337443 Test RE 0.49311198579492777 Lambda1 -0.077772655\n",
      "2 Train Loss 854.7212 Test MSE 858.068895452959 Test RE 0.49311731267770453 Lambda1 -0.07795602\n",
      "3 Train Loss 854.721 Test MSE 858.0701440179273 Test RE 0.4931176714418905 Lambda1 -0.077961385\n",
      "4 Train Loss 854.72076 Test MSE 858.0712037586468 Test RE 0.4931179759488812 Lambda1 -0.077965856\n",
      "5 Train Loss 854.7207 Test MSE 858.0722013358029 Test RE 0.4931182625935576 Lambda1 -0.07796992\n",
      "6 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "7 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "8 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "9 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "10 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "11 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "12 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "13 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "14 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "15 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "16 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "17 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "18 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "19 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "20 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "21 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "22 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "23 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "24 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "25 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "26 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "27 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "28 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "29 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "30 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "31 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "32 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "33 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "34 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "35 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "36 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "37 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "38 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "39 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "40 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "41 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "42 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "43 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "44 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "45 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "46 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "47 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "48 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "49 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "50 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "51 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "52 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "53 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "54 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "55 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "56 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "57 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "58 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "59 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "60 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "61 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "62 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "63 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "64 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "65 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "66 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "67 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "68 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "69 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "70 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "71 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "72 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "73 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "74 Train Loss 854.7206 Test MSE 858.0725144239984 Test RE 0.49311835255655406 Lambda1 -0.07797111\n",
      "Training time: 213.63\n",
      "Training time: 213.63\n",
      "inv_HT_atanh_tune0\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 989.0006 Test MSE 988.5083314898718 Test RE 0.5292725099939292 Lambda1 -0.16474101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 854.73285 Test MSE 858.0515759796931 Test RE 0.4931123360521353 Lambda1 -0.19908522\n",
      "2 Train Loss 854.7196 Test MSE 858.068975572337 Test RE 0.4931173356993125 Lambda1 -0.19935183\n",
      "3 Train Loss 854.7193 Test MSE 858.0707083601378 Test RE 0.4931178336005864 Lambda1 -0.19937186\n",
      "4 Train Loss 854.71875 Test MSE 858.0722286436148 Test RE 0.49311827044020534 Lambda1 -0.19939022\n",
      "5 Train Loss 854.7184 Test MSE 858.0739083880329 Test RE 0.4931187530990243 Lambda1 -0.19941275\n",
      "6 Train Loss 854.71655 Test MSE 858.0809221250258 Test RE 0.4931207684255243 Lambda1 -0.19951549\n",
      "7 Train Loss 854.5664 Test MSE 857.8977311514516 Test RE 0.4930681276469897 Lambda1 -0.20224366\n",
      "8 Train Loss 853.4806 Test MSE 856.7980849913203 Test RE 0.4927520210109592 Lambda1 -0.34621185\n",
      "9 Train Loss 850.8668 Test MSE 852.5609272494061 Test RE 0.49153209779680146 Lambda1 -0.3415112\n",
      "10 Train Loss 840.1542 Test MSE 841.5879388539402 Test RE 0.488358692184791 Lambda1 -0.52874434\n",
      "11 Train Loss 836.5356 Test MSE 838.9585123572853 Test RE 0.4875951903586516 Lambda1 -0.54866093\n",
      "12 Train Loss 822.0327 Test MSE 816.531427829792 Test RE 0.4810338331295986 Lambda1 -0.7969535\n",
      "13 Train Loss 800.5737 Test MSE 800.7972859524042 Test RE 0.4763766509787145 Lambda1 -0.8017154\n",
      "14 Train Loss 790.6949 Test MSE 792.7451584117765 Test RE 0.47397558338452167 Lambda1 -0.83958006\n",
      "15 Train Loss 775.8732 Test MSE 775.0764616647674 Test RE 0.4686638378665015 Lambda1 -0.917482\n",
      "16 Train Loss 758.94196 Test MSE 755.4524876742958 Test RE 0.4626928074539088 Lambda1 -0.95712537\n",
      "17 Train Loss 752.10925 Test MSE 750.3002309933996 Test RE 0.4611123039191199 Lambda1 -0.98310536\n",
      "18 Train Loss 735.6607 Test MSE 725.316345245396 Test RE 0.45337012935641696 Lambda1 -1.0961285\n",
      "19 Train Loss 722.1161 Test MSE 714.3448739243242 Test RE 0.44992812001270155 Lambda1 -1.1700283\n",
      "20 Train Loss 707.6823 Test MSE 704.6662245117681 Test RE 0.4468696898995698 Lambda1 -1.1388079\n",
      "21 Train Loss 688.8905 Test MSE 686.8741761950704 Test RE 0.4411921383321418 Lambda1 -1.1336235\n",
      "22 Train Loss 674.3662 Test MSE 674.636862306537 Test RE 0.437244348037798 Lambda1 -1.1824219\n",
      "23 Train Loss 666.2358 Test MSE 668.0183165540653 Test RE 0.4350942620857375 Lambda1 -1.2155036\n",
      "24 Train Loss 657.5096 Test MSE 660.9868869387154 Test RE 0.43279834613356166 Lambda1 -1.258441\n",
      "25 Train Loss 636.951 Test MSE 649.0881479120619 Test RE 0.4288851509969075 Lambda1 -1.357645\n",
      "26 Train Loss 628.7584 Test MSE 642.0631561897532 Test RE 0.4265579546441512 Lambda1 -1.3878791\n",
      "27 Train Loss 626.73975 Test MSE 640.2898894934355 Test RE 0.42596850795798064 Lambda1 -1.4100647\n",
      "28 Train Loss 624.8714 Test MSE 637.6113059136188 Test RE 0.42507657753965994 Lambda1 -1.4570771\n",
      "29 Train Loss 619.9269 Test MSE 634.4013166270255 Test RE 0.42400522503077526 Lambda1 -1.5119025\n",
      "30 Train Loss 616.9072 Test MSE 631.507996643107 Test RE 0.4230372378061547 Lambda1 -1.5349854\n",
      "31 Train Loss 614.33704 Test MSE 629.665318535695 Test RE 0.422419596435255 Lambda1 -1.5658258\n",
      "32 Train Loss 613.10016 Test MSE 628.0846111087432 Test RE 0.42188904364667695 Lambda1 -1.5981121\n",
      "33 Train Loss 611.3103 Test MSE 627.2674384939706 Test RE 0.4216145038979449 Lambda1 -1.6280687\n",
      "34 Train Loss 610.3541 Test MSE 627.2977346692487 Test RE 0.4216246854823031 Lambda1 -1.6179237\n",
      "35 Train Loss 610.24835 Test MSE 627.4325824123856 Test RE 0.42167000055279724 Lambda1 -1.6104141\n",
      "36 Train Loss 610.1603 Test MSE 627.5692259932464 Test RE 0.4217159141407304 Lambda1 -1.6067903\n",
      "37 Train Loss 609.45386 Test MSE 626.9663537942281 Test RE 0.42151330550552535 Lambda1 -1.618039\n",
      "38 Train Loss 608.99286 Test MSE 626.0972564335076 Test RE 0.42122105446170743 Lambda1 -1.6366569\n",
      "39 Train Loss 608.3736 Test MSE 625.2478536286376 Test RE 0.420935230036919 Lambda1 -1.6513563\n",
      "40 Train Loss 608.0595 Test MSE 625.0938197150216 Test RE 0.4208833767644029 Lambda1 -1.648266\n",
      "41 Train Loss 607.57275 Test MSE 624.760076704338 Test RE 0.4207710051211342 Lambda1 -1.6524647\n",
      "42 Train Loss 607.17615 Test MSE 624.4056793643366 Test RE 0.42065164627943324 Lambda1 -1.6688734\n",
      "43 Train Loss 607.04266 Test MSE 624.2859207792505 Test RE 0.42061130466869806 Lambda1 -1.6776882\n",
      "44 Train Loss 606.801 Test MSE 624.4603046211549 Test RE 0.420670045937249 Lambda1 -1.6790857\n",
      "45 Train Loss 606.71277 Test MSE 624.7077356331243 Test RE 0.42075337910175753 Lambda1 -1.6701143\n",
      "46 Train Loss 606.6293 Test MSE 624.7621572538599 Test RE 0.42077170573743267 Lambda1 -1.6656853\n",
      "47 Train Loss 606.4227 Test MSE 624.4704987277183 Test RE 0.42067347957245627 Lambda1 -1.6665275\n",
      "48 Train Loss 606.29175 Test MSE 624.204097874506 Test RE 0.4205837397618757 Lambda1 -1.6701584\n",
      "49 Train Loss 606.0884 Test MSE 623.9794389504676 Test RE 0.42050804625542426 Lambda1 -1.6609834\n",
      "50 Train Loss 605.543 Test MSE 623.9209106056642 Test RE 0.42048832427778493 Lambda1 -1.6393337\n",
      "51 Train Loss 605.31244 Test MSE 623.8075118774019 Test RE 0.42045011029327284 Lambda1 -1.637731\n",
      "52 Train Loss 605.209 Test MSE 623.699624118606 Test RE 0.42041375021367294 Lambda1 -1.6373899\n",
      "53 Train Loss 604.91113 Test MSE 623.5036804218171 Test RE 0.420347705685005 Lambda1 -1.6359279\n",
      "54 Train Loss 604.1694 Test MSE 622.9810112057013 Test RE 0.42017148469784454 Lambda1 -1.626049\n",
      "55 Train Loss 604.04144 Test MSE 622.7755237808125 Test RE 0.4201021831655882 Lambda1 -1.6238817\n",
      "56 Train Loss 603.99603 Test MSE 622.619293381506 Test RE 0.4200494861294839 Lambda1 -1.6213043\n",
      "57 Train Loss 603.9555 Test MSE 622.4562807716848 Test RE 0.4199944943823733 Lambda1 -1.6153257\n",
      "58 Train Loss 603.8802 Test MSE 622.3650519800392 Test RE 0.4199637155185281 Lambda1 -1.6072153\n",
      "59 Train Loss 603.7657 Test MSE 622.3344794132846 Test RE 0.41995340040976037 Lambda1 -1.5997074\n",
      "60 Train Loss 603.5474 Test MSE 622.2183310992656 Test RE 0.4199142099445867 Lambda1 -1.5828485\n",
      "61 Train Loss 603.23566 Test MSE 622.0452097019047 Test RE 0.4198557889784442 Lambda1 -1.5521506\n",
      "62 Train Loss 603.0821 Test MSE 621.9066919045523 Test RE 0.4198090393723109 Lambda1 -1.5254034\n",
      "63 Train Loss 602.8998 Test MSE 621.7181738203948 Test RE 0.4197454063584791 Lambda1 -1.4934965\n",
      "64 Train Loss 602.7334 Test MSE 621.3556464488416 Test RE 0.4196230105572574 Lambda1 -1.467469\n",
      "65 Train Loss 602.35925 Test MSE 620.3850027924896 Test RE 0.41929512780086975 Lambda1 -1.4079093\n",
      "66 Train Loss 601.6603 Test MSE 619.4924480594777 Test RE 0.41899339697785903 Lambda1 -1.3488957\n",
      "67 Train Loss 600.9856 Test MSE 618.6789551647238 Test RE 0.418718203855607 Lambda1 -1.2850722\n",
      "68 Train Loss 600.63654 Test MSE 617.7127403286762 Test RE 0.418391111896517 Lambda1 -1.2563931\n",
      "69 Train Loss 600.235 Test MSE 616.6338723156546 Test RE 0.41802558110527244 Lambda1 -1.2378407\n",
      "70 Train Loss 598.8203 Test MSE 613.7111378934845 Test RE 0.4170337211025303 Lambda1 -1.1976069\n",
      "71 Train Loss 597.36835 Test MSE 610.7426566478142 Test RE 0.41602391590750004 Lambda1 -1.1643198\n",
      "72 Train Loss 595.4373 Test MSE 608.2764063126284 Test RE 0.4151830895759617 Lambda1 -1.1312271\n",
      "73 Train Loss 591.7787 Test MSE 600.7040890668178 Test RE 0.4125907287148174 Lambda1 -1.0959567\n",
      "74 Train Loss 579.04236 Test MSE 574.2838987806933 Test RE 0.40341541645977663 Lambda1 -1.0422226\n",
      "Training time: 315.10\n",
      "Training time: 315.10\n",
      "inv_HT_atanh_tune0\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 918.0364 Test MSE 918.7503554038407 Test RE 0.5102557773273471 Lambda1 -0.046873987\n",
      "1 Train Loss 854.7547 Test MSE 858.0503853813317 Test RE 0.4931119939404491 Lambda1 -0.053024054\n",
      "2 Train Loss 854.7226 Test MSE 858.063986589958 Test RE 0.4931159021564711 Lambda1 -0.053130046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Train Loss 854.7207 Test MSE 858.0713901406696 Test RE 0.49311802950406425 Lambda1 -0.053151496\n",
      "4 Train Loss 854.72046 Test MSE 858.0722835234079 Test RE 0.4931182862094071 Lambda1 -0.053153727\n",
      "5 Train Loss 854.7204 Test MSE 858.0733006020272 Test RE 0.4931185784574651 Lambda1 -0.053156193\n",
      "6 Train Loss 854.7204 Test MSE 858.0740672913413 Test RE 0.4931187987583766 Lambda1 -0.053157784\n",
      "7 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "8 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "9 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "10 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "11 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "12 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "13 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "14 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "15 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "16 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "17 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "18 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "19 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "20 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "21 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "22 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "23 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "24 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "25 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "26 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "27 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "28 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "29 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "30 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "31 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "32 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "33 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "34 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "35 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "36 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "37 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "38 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "39 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "40 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "41 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "42 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "43 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "44 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "45 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "46 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "47 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "48 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "49 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "50 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "51 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "52 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "53 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "54 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "55 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "56 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "57 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "58 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "59 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "60 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "61 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "62 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "63 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "64 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "65 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "66 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "67 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "68 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "69 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "70 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "71 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "72 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "73 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "74 Train Loss 854.72015 Test MSE 858.0742594941885 Test RE 0.49311885398602845 Lambda1 -0.053158276\n",
      "Training time: 216.16\n",
      "Training time: 216.16\n",
      "inv_HT_atanh_tune0\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 920.14056 Test MSE 920.8108671535242 Test RE 0.5108276406103405 Lambda1 -0.025982924\n",
      "1 Train Loss 854.7527 Test MSE 858.0496127246511 Test RE 0.49311177192180733 Lambda1 -0.02928695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Train Loss 854.7204 Test MSE 858.068764699036 Test RE 0.493117275106697 Lambda1 -0.029345386\n",
      "3 Train Loss 854.72015 Test MSE 858.0699736340491 Test RE 0.4931176224835939 Lambda1 -0.029346816\n",
      "4 Train Loss 854.7201 Test MSE 858.0709652867574 Test RE 0.4931179074261391 Lambda1 -0.029347826\n",
      "5 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "6 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "7 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "8 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "9 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "10 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "11 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "12 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "13 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "14 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "15 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "16 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "17 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "18 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "19 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "20 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "21 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "22 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "23 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "24 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "25 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "26 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "27 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "28 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "29 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "30 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "31 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "32 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "33 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "34 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "35 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "36 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "37 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "38 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "39 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "40 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "41 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "42 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "43 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "44 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "45 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "46 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "47 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "48 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "49 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "50 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "51 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "52 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "53 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "54 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "55 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "56 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "57 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "58 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "59 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "60 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "61 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "62 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "63 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "64 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "65 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "66 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "67 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "68 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "69 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "70 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "71 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "72 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "73 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "74 Train Loss 854.71985 Test MSE 858.0718742951449 Test RE 0.4931181686214326 Lambda1 -0.029348606\n",
      "Training time: 204.42\n",
      "Training time: 204.42\n",
      "inv_HT_atanh_tune0\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 1102.8641 Test MSE 1100.9886593140002 Test RE 0.5585738414506582 Lambda1 0.0024934434\n",
      "1 Train Loss 854.7653 Test MSE 858.0463655445278 Test RE 0.4931108388613901 Lambda1 0.0035422547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Train Loss 854.7112 Test MSE 858.058995838139 Test RE 0.4931144681010261 Lambda1 0.003544459\n",
      "3 Train Loss 854.7077 Test MSE 858.0667868562779 Test RE 0.4931167067903607 Lambda1 0.0035325545\n",
      "4 Train Loss 854.608 Test MSE 857.8797212563575 Test RE 0.4930629521164245 Lambda1 0.0015487312\n",
      "5 Train Loss 854.522 Test MSE 857.668633704956 Test RE 0.4930022875198415 Lambda1 -0.010944569\n",
      "6 Train Loss 854.52014 Test MSE 857.6660211980495 Test RE 0.4930015366629189 Lambda1 -0.015267767\n",
      "7 Train Loss 854.5173 Test MSE 857.6621554604266 Test RE 0.49300042561462676 Lambda1 -0.028633947\n",
      "8 Train Loss 854.4773 Test MSE 857.7036426912084 Test RE 0.49301234929282906 Lambda1 -0.33924747\n",
      "9 Train Loss 853.56433 Test MSE 856.5438406365428 Test RE 0.49267890651817214 Lambda1 -1.3794792\n",
      "10 Train Loss 849.5589 Test MSE 852.3050004944123 Test RE 0.49145831676091145 Lambda1 -1.3737128\n",
      "11 Train Loss 840.22906 Test MSE 840.4459500562122 Test RE 0.48802724169066325 Lambda1 -1.6706905\n",
      "12 Train Loss 832.07404 Test MSE 824.6484398079472 Test RE 0.4834188617621915 Lambda1 -1.3003728\n",
      "13 Train Loss 827.02124 Test MSE 818.056233141017 Test RE 0.4814827692073971 Lambda1 -1.1983795\n",
      "14 Train Loss 818.911 Test MSE 815.8894914027605 Test RE 0.4808447076135386 Lambda1 -1.0761961\n",
      "15 Train Loss 810.09296 Test MSE 805.6045162620807 Test RE 0.4778043691822826 Lambda1 -1.0737664\n",
      "16 Train Loss 803.175 Test MSE 806.0254197940703 Test RE 0.47792917191178863 Lambda1 -1.0534284\n",
      "17 Train Loss 797.4511 Test MSE 800.2123816531046 Test RE 0.4762026458630641 Lambda1 -1.0716662\n",
      "18 Train Loss 790.1134 Test MSE 791.9849879696538 Test RE 0.47374827916156326 Lambda1 -1.0427716\n",
      "19 Train Loss 786.69653 Test MSE 786.1771995061816 Test RE 0.47200803621990656 Lambda1 -1.0039978\n",
      "20 Train Loss 777.34265 Test MSE 777.1737049916665 Test RE 0.46929747735620214 Lambda1 -1.0177156\n",
      "21 Train Loss 769.39124 Test MSE 769.7411833155998 Test RE 0.46704801670412094 Lambda1 -1.0367913\n",
      "22 Train Loss 768.35516 Test MSE 767.1091842625481 Test RE 0.46624883732757966 Lambda1 -1.0324404\n",
      "23 Train Loss 763.36725 Test MSE 764.3473768512488 Test RE 0.4654087674767247 Lambda1 -0.9951825\n",
      "24 Train Loss 756.44147 Test MSE 758.3144690724992 Test RE 0.4635684193696615 Lambda1 -1.0502486\n",
      "25 Train Loss 751.0198 Test MSE 752.597400316652 Test RE 0.46181765056851937 Lambda1 -1.0534877\n",
      "26 Train Loss 743.11017 Test MSE 746.9635216700983 Test RE 0.4600858400624506 Lambda1 -1.0130073\n",
      "27 Train Loss 738.4493 Test MSE 740.3590289937622 Test RE 0.45804733344871706 Lambda1 -0.948987\n",
      "28 Train Loss 732.62177 Test MSE 733.1451223992291 Test RE 0.45581031118241944 Lambda1 -0.924893\n",
      "29 Train Loss 730.1158 Test MSE 731.0903988412728 Test RE 0.4551711326050173 Lambda1 -0.8983164\n",
      "30 Train Loss 723.0516 Test MSE 725.8429400505286 Test RE 0.4535346775837016 Lambda1 -0.8593972\n",
      "31 Train Loss 717.4744 Test MSE 720.5655922895651 Test RE 0.4518829245172125 Lambda1 -0.88644123\n",
      "32 Train Loss 709.0326 Test MSE 712.5973322323134 Test RE 0.44937744089478204 Lambda1 -0.9136121\n",
      "33 Train Loss 700.463 Test MSE 701.829909020813 Test RE 0.44596944709429026 Lambda1 -0.90600646\n",
      "34 Train Loss 697.2433 Test MSE 699.0213455969977 Test RE 0.4450762185049105 Lambda1 -0.89173704\n",
      "35 Train Loss 696.0686 Test MSE 697.9373202800303 Test RE 0.4447309779130072 Lambda1 -0.8594068\n",
      "36 Train Loss 691.9414 Test MSE 693.8890249764501 Test RE 0.44343929983076413 Lambda1 -0.8253598\n",
      "37 Train Loss 689.8136 Test MSE 691.7643087294207 Test RE 0.442759864785882 Lambda1 -0.86202663\n",
      "38 Train Loss 687.0149 Test MSE 692.876745677053 Test RE 0.4431157262903518 Lambda1 -0.90094674\n",
      "39 Train Loss 683.0406 Test MSE 692.4121668474085 Test RE 0.44296714524300557 Lambda1 -0.920472\n",
      "40 Train Loss 680.8622 Test MSE 690.1922346130083 Test RE 0.4422564800213286 Lambda1 -0.95602214\n",
      "41 Train Loss 679.4338 Test MSE 687.6075018243622 Test RE 0.44142758989327846 Lambda1 -0.9681116\n",
      "42 Train Loss 677.5918 Test MSE 685.3544637096622 Test RE 0.4407037981833033 Lambda1 -0.95871115\n",
      "43 Train Loss 676.2713 Test MSE 683.5769789245861 Test RE 0.4401319387272101 Lambda1 -0.9507035\n",
      "44 Train Loss 674.6443 Test MSE 683.2879255966577 Test RE 0.44003887309054635 Lambda1 -0.94258386\n",
      "45 Train Loss 671.3408 Test MSE 681.2245145375972 Test RE 0.4393739501741235 Lambda1 -0.91454756\n",
      "46 Train Loss 670.2275 Test MSE 679.5255126243507 Test RE 0.43882569978381675 Lambda1 -0.93544\n",
      "47 Train Loss 669.88464 Test MSE 679.240379805121 Test RE 0.4387336232281862 Lambda1 -0.93834096\n",
      "48 Train Loss 669.1063 Test MSE 678.991339939794 Test RE 0.4386531861821669 Lambda1 -0.9336242\n",
      "49 Train Loss 667.7363 Test MSE 677.7816044576164 Test RE 0.4382622453369802 Lambda1 -0.9302468\n",
      "50 Train Loss 666.43396 Test MSE 677.4247365977753 Test RE 0.43814685248681684 Lambda1 -0.94505394\n",
      "51 Train Loss 665.3962 Test MSE 678.0958942827062 Test RE 0.43836384546852064 Lambda1 -0.96425766\n",
      "52 Train Loss 664.73157 Test MSE 677.7972520287042 Test RE 0.4382673042674779 Lambda1 -0.9570367\n",
      "53 Train Loss 664.02966 Test MSE 676.8969241452851 Test RE 0.43797612914590683 Lambda1 -0.94885516\n",
      "54 Train Loss 663.08844 Test MSE 676.3744142531423 Test RE 0.43780705536840503 Lambda1 -0.9583176\n",
      "55 Train Loss 662.14105 Test MSE 676.4297169339767 Test RE 0.43782495330218857 Lambda1 -0.9701746\n",
      "56 Train Loss 660.70337 Test MSE 674.7877424448427 Test RE 0.4372932393771476 Lambda1 -0.9720342\n",
      "57 Train Loss 659.2258 Test MSE 673.0446787564257 Test RE 0.4367280817800925 Lambda1 -0.96624386\n",
      "58 Train Loss 658.8936 Test MSE 672.4305551276036 Test RE 0.4365287887375147 Lambda1 -0.9742853\n",
      "59 Train Loss 658.50165 Test MSE 672.0590012727376 Test RE 0.4364081693186172 Lambda1 -0.99789065\n",
      "60 Train Loss 658.01764 Test MSE 671.9560612467362 Test RE 0.4363747454759164 Lambda1 -1.0112127\n",
      "61 Train Loss 657.7505 Test MSE 671.9872563175383 Test RE 0.436384874548247 Lambda1 -1.0145506\n",
      "62 Train Loss 657.25806 Test MSE 671.8528549301514 Test RE 0.43634123260047314 Lambda1 -1.0428166\n",
      "63 Train Loss 656.6003 Test MSE 671.3032573646119 Test RE 0.436162725401301 Lambda1 -1.0939077\n",
      "64 Train Loss 655.9093 Test MSE 670.783929309961 Test RE 0.4359939824657081 Lambda1 -1.1113404\n",
      "65 Train Loss 655.21277 Test MSE 669.8347239940567 Test RE 0.43568539254784255 Lambda1 -1.1480591\n",
      "66 Train Loss 654.6521 Test MSE 667.7959876455458 Test RE 0.4350218523328491 Lambda1 -1.1716932\n",
      "67 Train Loss 652.91205 Test MSE 664.3787025208051 Test RE 0.43390736494790294 Lambda1 -1.189221\n",
      "68 Train Loss 650.7994 Test MSE 663.5542642062015 Test RE 0.43363806003008953 Lambda1 -1.1952666\n",
      "69 Train Loss 648.57477 Test MSE 660.9725758105631 Test RE 0.432793660816618 Lambda1 -1.2257541\n",
      "70 Train Loss 647.1053 Test MSE 658.6430121182921 Test RE 0.4320303081757317 Lambda1 -1.2636217\n",
      "71 Train Loss 646.0104 Test MSE 656.4866211269828 Test RE 0.4313224967877062 Lambda1 -1.3068234\n",
      "72 Train Loss 645.2823 Test MSE 654.495739773242 Test RE 0.43066797930274053 Lambda1 -1.3499061\n",
      "73 Train Loss 644.0987 Test MSE 651.7269080072705 Test RE 0.42975604745953894 Lambda1 -1.4126536\n",
      "74 Train Loss 642.1472 Test MSE 649.6422219900135 Test RE 0.429068164239961 Lambda1 -1.4687192\n",
      "Training time: 284.11\n",
      "Training time: 284.11\n",
      "inv_HT_atanh_tune0\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 1233.1538 Test MSE 1230.048780459435 Test RE 0.5904054231126449 Lambda1 -0.021851521\n",
      "1 Train Loss 854.76935 Test MSE 858.052120120613 Test RE 0.49311249240786326 Lambda1 -0.033323497\n",
      "2 Train Loss 854.7207 Test MSE 858.064582715133 Test RE 0.493116073448331 Lambda1 -0.033434\n",
      "3 Train Loss 854.71967 Test MSE 858.068521268564 Test RE 0.4931172051590516 Lambda1 -0.03344365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 854.719 Test MSE 858.0717191451058 Test RE 0.49311812404048955 Lambda1 -0.03345148\n",
      "5 Train Loss 854.719 Test MSE 858.0727413250618 Test RE 0.49311841775446524 Lambda1 -0.03345444\n",
      "6 Train Loss 854.7187 Test MSE 858.0738845638815 Test RE 0.4931187462533809 Lambda1 -0.033458214\n",
      "7 Train Loss 854.7184 Test MSE 858.0752816707464 Test RE 0.49311914769859294 Lambda1 -0.033463616\n",
      "8 Train Loss 854.71765 Test MSE 858.0790581388293 Test RE 0.493120232828717 Lambda1 -0.033480104\n",
      "9 Train Loss 854.71545 Test MSE 858.0899619917387 Test RE 0.4931233659277904 Lambda1 -0.033539474\n",
      "10 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "11 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "12 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "13 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "14 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "15 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "16 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "17 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "18 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "19 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "20 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "21 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "22 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "23 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "24 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "25 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "26 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "27 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "28 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "29 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "30 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "31 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "32 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "33 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "34 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "35 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "36 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "37 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "38 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "39 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "40 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "41 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "42 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "43 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "44 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "45 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "46 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "47 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "48 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "49 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "50 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "51 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "52 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "53 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "54 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "55 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "56 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "57 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "58 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "59 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "60 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "61 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "62 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "63 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "64 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "65 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "66 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "67 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "68 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "69 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "70 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "71 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "72 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "73 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "74 Train Loss 854.5012 Test MSE 857.7085032660559 Test RE 0.49301374623223226 Lambda1 -0.034295443\n",
      "Training time: 265.87\n",
      "Training time: 265.87\n",
      "inv_HT_atanh_tune0\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 1018.4478 Test MSE 1017.5548425903252 Test RE 0.536992330935966 Lambda1 -0.13158192\n",
      "1 Train Loss 854.781 Test MSE 858.057066567293 Test RE 0.49311391373826874 Lambda1 -0.16196474\n",
      "2 Train Loss 854.7218 Test MSE 858.0701108783556 Test RE 0.4931176619195289 Lambda1 -0.16248193\n",
      "3 Train Loss 854.7215 Test MSE 858.0714124616169 Test RE 0.4931180359177854 Lambda1 -0.1624936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 854.7212 Test MSE 858.0725056457343 Test RE 0.49311835003420135 Lambda1 -0.16250367\n",
      "5 Train Loss 854.721 Test MSE 858.0735676487835 Test RE 0.49311865519083203 Lambda1 -0.16251414\n",
      "6 Train Loss 854.72076 Test MSE 858.0746757680075 Test RE 0.49311897359831225 Lambda1 -0.16252516\n",
      "7 Train Loss 854.7207 Test MSE 858.0757030293299 Test RE 0.49311926877186285 Lambda1 -0.16253589\n",
      "8 Train Loss 854.72046 Test MSE 858.0768107815979 Test RE 0.4931195870735058 Lambda1 -0.16254804\n",
      "9 Train Loss 854.7201 Test MSE 858.0797518788904 Test RE 0.4931204321676869 Lambda1 -0.16258174\n",
      "10 Train Loss 854.71985 Test MSE 858.0809169824447 Test RE 0.49312076694785856 Lambda1 -0.16259724\n",
      "11 Train Loss 854.5946 Test MSE 857.8864403290728 Test RE 0.4930648829927814 Lambda1 -0.1643283\n",
      "12 Train Loss 854.39935 Test MSE 857.5629796779674 Test RE 0.4929719207329613 Lambda1 -0.16938072\n",
      "13 Train Loss 853.6158 Test MSE 856.2158773176069 Test RE 0.49258457621562046 Lambda1 -0.29458708\n",
      "14 Train Loss 847.4815 Test MSE 850.7222465932401 Test RE 0.4910017788847365 Lambda1 -0.45964175\n",
      "15 Train Loss 839.03577 Test MSE 839.5454112489376 Test RE 0.4877657107373234 Lambda1 -0.52480954\n",
      "16 Train Loss 827.36 Test MSE 826.8540494663181 Test RE 0.4840649075633048 Lambda1 -0.55189824\n",
      "17 Train Loss 818.65796 Test MSE 818.0749138061076 Test RE 0.4814882666088267 Lambda1 -0.5611612\n",
      "18 Train Loss 809.83636 Test MSE 810.202007901153 Test RE 0.4791658166288415 Lambda1 -0.58148104\n",
      "19 Train Loss 797.9547 Test MSE 798.4035762987611 Test RE 0.47566413557440246 Lambda1 -0.54932183\n",
      "20 Train Loss 785.0524 Test MSE 783.25055115921 Test RE 0.47112866096537637 Lambda1 -0.5352016\n",
      "21 Train Loss 779.6273 Test MSE 778.9001510089377 Test RE 0.4698184466660723 Lambda1 -0.5473647\n",
      "22 Train Loss 765.8142 Test MSE 768.892818464944 Test RE 0.46679056889927967 Lambda1 -0.59526664\n",
      "23 Train Loss 752.49115 Test MSE 751.5439776337535 Test RE 0.4614943305941228 Lambda1 -0.6253384\n",
      "24 Train Loss 731.8159 Test MSE 732.1703895493755 Test RE 0.4555072053969828 Lambda1 -0.60484016\n",
      "25 Train Loss 712.28644 Test MSE 713.8553296493484 Test RE 0.449773924544541 Lambda1 -0.5940099\n",
      "26 Train Loss 704.7663 Test MSE 702.9789098503937 Test RE 0.4463343572404655 Lambda1 -0.6080362\n",
      "27 Train Loss 700.35156 Test MSE 698.0172884464769 Test RE 0.44475645534570046 Lambda1 -0.6146524\n",
      "28 Train Loss 688.9326 Test MSE 688.5459782437752 Test RE 0.44172872689810105 Lambda1 -0.69281596\n",
      "29 Train Loss 681.1504 Test MSE 675.1690318099616 Test RE 0.43741676838064364 Lambda1 -0.7863507\n",
      "30 Train Loss 670.47034 Test MSE 666.214764123455 Test RE 0.43450651945576946 Lambda1 -0.80639076\n",
      "31 Train Loss 661.9475 Test MSE 660.7637552654871 Test RE 0.43272528933592913 Lambda1 -0.8394995\n",
      "32 Train Loss 652.1999 Test MSE 655.6746770882544 Test RE 0.4310556839762863 Lambda1 -0.8167677\n",
      "33 Train Loss 640.72345 Test MSE 645.0340576953142 Test RE 0.42754368252554564 Lambda1 -0.8244848\n",
      "34 Train Loss 630.2617 Test MSE 637.4232429381311 Test RE 0.42501388490799163 Lambda1 -0.8431463\n",
      "35 Train Loss 626.0002 Test MSE 634.6973900406146 Test RE 0.42410415454713424 Lambda1 -0.8852217\n",
      "36 Train Loss 622.97034 Test MSE 631.849011436365 Test RE 0.4231514426038952 Lambda1 -0.94706726\n",
      "37 Train Loss 619.4015 Test MSE 630.4924844964339 Test RE 0.42269696317638206 Lambda1 -0.9521243\n",
      "38 Train Loss 614.7594 Test MSE 628.7752682298279 Test RE 0.42212093964352343 Lambda1 -0.9508603\n",
      "39 Train Loss 613.4378 Test MSE 627.5886405280136 Test RE 0.42172243720991315 Lambda1 -0.9618821\n",
      "40 Train Loss 611.7871 Test MSE 628.1112896121317 Test RE 0.42189800362480995 Lambda1 -0.9514228\n",
      "41 Train Loss 611.38306 Test MSE 628.1624280584714 Test RE 0.4219151779453889 Lambda1 -0.949296\n",
      "42 Train Loss 610.7963 Test MSE 627.4562592522739 Test RE 0.4216779565620825 Lambda1 -0.94611895\n",
      "43 Train Loss 609.8822 Test MSE 626.4875440583754 Test RE 0.4213523214104595 Lambda1 -0.9453182\n",
      "44 Train Loss 608.1998 Test MSE 625.5392620718408 Test RE 0.4210333109741192 Lambda1 -0.94508237\n",
      "45 Train Loss 607.9858 Test MSE 625.3275762802037 Test RE 0.4209620649975178 Lambda1 -0.94685096\n",
      "46 Train Loss 607.8798 Test MSE 625.1295576490877 Test RE 0.420895407988286 Lambda1 -0.9486466\n",
      "47 Train Loss 607.45667 Test MSE 624.7440435449781 Test RE 0.42076560598303825 Lambda1 -0.9482742\n",
      "48 Train Loss 607.02966 Test MSE 624.5238331910514 Test RE 0.42069144352373866 Lambda1 -0.9434001\n",
      "49 Train Loss 606.889 Test MSE 624.2940119311241 Test RE 0.4206140303580061 Lambda1 -0.9403589\n",
      "50 Train Loss 606.7547 Test MSE 623.9945451604998 Test RE 0.42051313636258525 Lambda1 -0.9376372\n",
      "51 Train Loss 606.4775 Test MSE 623.4792226426695 Test RE 0.4203394612491701 Lambda1 -0.9318328\n",
      "52 Train Loss 606.01874 Test MSE 622.8151354441188 Test RE 0.42011554326163797 Lambda1 -0.92332673\n",
      "53 Train Loss 605.60864 Test MSE 621.9940118723555 Test RE 0.4198385103730417 Lambda1 -0.9170272\n",
      "54 Train Loss 605.16113 Test MSE 619.9984185723819 Test RE 0.4191644685017301 Lambda1 -0.9006106\n",
      "55 Train Loss 604.3069 Test MSE 617.6934210490774 Test RE 0.41838456914812794 Lambda1 -0.8734895\n",
      "56 Train Loss 603.85767 Test MSE 616.6957157890109 Test RE 0.41804654290034166 Lambda1 -0.8681623\n",
      "57 Train Loss 602.0092 Test MSE 613.8379624869377 Test RE 0.4170768092890482 Lambda1 -0.8672666\n",
      "58 Train Loss 598.3533 Test MSE 611.8460826098914 Test RE 0.4163995605674496 Lambda1 -0.8500374\n",
      "59 Train Loss 594.6005 Test MSE 608.6530889316168 Test RE 0.4153116232881503 Lambda1 -0.82089597\n",
      "60 Train Loss 590.66876 Test MSE 603.7010609562716 Test RE 0.4136186760769969 Lambda1 -0.79481316\n",
      "61 Train Loss 586.19824 Test MSE 596.2206670961409 Test RE 0.4110481365349457 Lambda1 -0.77292293\n",
      "62 Train Loss 579.7066 Test MSE 583.5852509062571 Test RE 0.406669240179236 Lambda1 -0.73866415\n",
      "63 Train Loss 574.55426 Test MSE 575.9988545276776 Test RE 0.40401731724132783 Lambda1 -0.70921654\n",
      "64 Train Loss 567.54767 Test MSE 560.4233383067276 Test RE 0.39851738970630957 Lambda1 -0.72041357\n",
      "65 Train Loss 555.3967 Test MSE 539.3559253714828 Test RE 0.39095511341118994 Lambda1 -0.6937354\n",
      "66 Train Loss 542.5161 Test MSE 519.4106027607395 Test RE 0.38365828045089667 Lambda1 -0.6474127\n",
      "67 Train Loss 526.4841 Test MSE 500.9067343709009 Test RE 0.3767624444044594 Lambda1 -0.6263222\n",
      "68 Train Loss 511.66147 Test MSE 489.6084350689961 Test RE 0.3724891408290991 Lambda1 -0.5806163\n",
      "69 Train Loss 490.3562 Test MSE 462.2226355144795 Test RE 0.361921826828537 Lambda1 -0.5861934\n",
      "70 Train Loss 461.1646 Test MSE 426.98077707215685 Test RE 0.34785106092417717 Lambda1 -0.56888616\n",
      "71 Train Loss 435.7608 Test MSE 413.2469980513671 Test RE 0.34221104647826894 Lambda1 -0.50757253\n",
      "72 Train Loss 406.049 Test MSE 383.0438088572091 Test RE 0.32946811733821874 Lambda1 -0.46996725\n",
      "73 Train Loss 386.74002 Test MSE 353.8880720184691 Test RE 0.316681091402864 Lambda1 -0.49103802\n",
      "74 Train Loss 369.39285 Test MSE 335.2171744132033 Test RE 0.30821395697046255 Lambda1 -0.50322366\n",
      "Training time: 290.30\n",
      "Training time: 290.30\n",
      "inv_HT_atanh_tune0\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 870.7496 Test MSE 872.7767587659766 Test RE 0.497325533364535 Lambda1 -0.0035794782\n",
      "1 Train Loss 854.73517 Test MSE 858.0534676354781 Test RE 0.4931128796082077 Lambda1 -0.0030007851\n",
      "2 Train Loss 854.722 Test MSE 858.071234930545 Test RE 0.4931179849058437 Lambda1 -0.0029855752\n",
      "3 Train Loss 854.7218 Test MSE 858.07241845391 Test RE 0.49311832498043884 Lambda1 -0.002984701\n",
      "4 Train Loss 854.72156 Test MSE 858.073523599374 Test RE 0.4931186425336504 Lambda1 -0.002983672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 854.72156 Test MSE 858.074553964792 Test RE 0.49311893859933115 Lambda1 -0.0029825773\n",
      "6 Train Loss 854.7215 Test MSE 858.0752145336068 Test RE 0.4931191284073896 Lambda1 -0.0029816825\n",
      "7 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "8 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "9 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "10 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "11 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "12 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "13 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "14 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "15 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "16 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "17 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "18 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "19 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "20 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "21 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "22 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "23 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "24 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "25 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "26 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "27 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "28 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "29 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "30 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "31 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "32 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "33 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "34 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "35 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "36 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "37 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "38 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "39 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "40 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "41 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "42 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "43 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "44 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "45 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "46 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "47 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "48 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "49 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "50 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "51 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "52 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "53 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "54 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "55 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "56 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "57 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "58 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "59 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "60 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "61 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "62 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "63 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "64 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "65 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "66 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "67 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "68 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "69 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "70 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "71 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "72 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "73 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "74 Train Loss 854.7214 Test MSE 858.0752320393456 Test RE 0.49311913343749336 Lambda1 -0.0029816702\n",
      "Training time: 253.22\n",
      "Training time: 253.22\n",
      "inv_HT_atanh_tune0\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 928.5555 Test MSE 929.0581798736384 Test RE 0.5131101744318103 Lambda1 -0.014192862\n",
      "1 Train Loss 854.7518 Test MSE 858.0513534742227 Test RE 0.49311227211646846 Lambda1 -0.01640318\n",
      "2 Train Loss 854.7228 Test MSE 858.0681637367014 Test RE 0.49311710242533807 Lambda1 -0.01643953\n",
      "3 Train Loss 854.722 Test MSE 858.0721134765924 Test RE 0.49311823734802346 Lambda1 -0.016443485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 854.72156 Test MSE 858.0743174760349 Test RE 0.4931188706465569 Lambda1 -0.016445724\n",
      "5 Train Loss 854.7215 Test MSE 858.0752117579449 Test RE 0.4931191276098301 Lambda1 -0.01644673\n",
      "6 Train Loss 854.7214 Test MSE 858.0760981428941 Test RE 0.4931193823038559 Lambda1 -0.016447792\n",
      "7 Train Loss 854.7212 Test MSE 858.0770825362425 Test RE 0.49311966515948 Lambda1 -0.01644899\n",
      "8 Train Loss 854.721 Test MSE 858.0780686518818 Test RE 0.49311994850982466 Lambda1 -0.016450347\n",
      "9 Train Loss 854.72076 Test MSE 858.0792137741477 Test RE 0.4931202775489092 Lambda1 -0.016452117\n",
      "10 Train Loss 854.7206 Test MSE 858.0803527615554 Test RE 0.4931206048249877 Lambda1 -0.016454045\n",
      "11 Train Loss 854.7203 Test MSE 858.08137656033 Test RE 0.493120899002636 Lambda1 -0.01645615\n",
      "12 Train Loss 854.7201 Test MSE 858.0825465463325 Test RE 0.49312123518540874 Lambda1 -0.016458536\n",
      "13 Train Loss 854.7182 Test MSE 858.0900118794784 Test RE 0.49312338026242364 Lambda1 -0.016477602\n",
      "14 Train Loss 854.6906 Test MSE 858.0735207123131 Test RE 0.49311864170408076 Lambda1 -0.01670371\n",
      "15 Train Loss 854.52686 Test MSE 857.6825535901618 Test RE 0.4930062881951224 Lambda1 -0.018440267\n",
      "16 Train Loss 854.46686 Test MSE 857.6170559311641 Test RE 0.4929874634130896 Lambda1 -0.038011532\n",
      "17 Train Loss 852.46814 Test MSE 854.9526202331022 Test RE 0.49222106354209677 Lambda1 -0.30898213\n",
      "18 Train Loss 846.9793 Test MSE 849.9103134528294 Test RE 0.4907674157984505 Lambda1 -0.3473534\n",
      "19 Train Loss 839.4605 Test MSE 841.1246851257245 Test RE 0.4882242646842758 Lambda1 -0.48773572\n",
      "20 Train Loss 831.62286 Test MSE 835.0267434525482 Test RE 0.48645129409437904 Lambda1 -0.6580083\n",
      "21 Train Loss 820.5634 Test MSE 821.3330895390015 Test RE 0.48244613395083497 Lambda1 -0.9298678\n",
      "22 Train Loss 813.19965 Test MSE 816.7870691902289 Test RE 0.48110912877391737 Lambda1 -0.93056136\n",
      "23 Train Loss 796.6071 Test MSE 798.4091181416194 Test RE 0.4756657864007493 Lambda1 -0.73731863\n",
      "24 Train Loss 779.86444 Test MSE 782.2640268628271 Test RE 0.4708318681237513 Lambda1 -0.73597425\n",
      "25 Train Loss 753.3814 Test MSE 748.9246466478409 Test RE 0.4606894132982071 Lambda1 -0.73503286\n",
      "26 Train Loss 732.6917 Test MSE 724.9843064706032 Test RE 0.4532663445075468 Lambda1 -0.82526535\n",
      "27 Train Loss 711.3016 Test MSE 702.2364515683232 Test RE 0.44609859470157026 Lambda1 -0.86780745\n",
      "28 Train Loss 683.35724 Test MSE 684.0165628237594 Test RE 0.44027343253517814 Lambda1 -0.92135745\n",
      "29 Train Loss 676.2285 Test MSE 673.6466287815947 Test RE 0.4369233360734446 Lambda1 -0.99275917\n",
      "30 Train Loss 664.589 Test MSE 668.8474889060196 Test RE 0.4353642069682119 Lambda1 -1.0857495\n",
      "31 Train Loss 648.8053 Test MSE 655.921252397612 Test RE 0.4311367285106332 Lambda1 -1.2093228\n",
      "32 Train Loss 641.36694 Test MSE 648.7111316184704 Test RE 0.4287605761010946 Lambda1 -1.270592\n",
      "33 Train Loss 634.45807 Test MSE 642.0732982434872 Test RE 0.4265613235935847 Lambda1 -1.3349913\n",
      "34 Train Loss 626.80865 Test MSE 635.9661290901715 Test RE 0.4245278280008495 Lambda1 -1.4344893\n",
      "35 Train Loss 622.9188 Test MSE 634.0614844082353 Test RE 0.4238916455549831 Lambda1 -1.4484674\n",
      "36 Train Loss 620.70593 Test MSE 633.2616113258977 Test RE 0.423624190012857 Lambda1 -1.4525021\n",
      "37 Train Loss 618.822 Test MSE 631.8343903822904 Test RE 0.42314654669062796 Lambda1 -1.480273\n",
      "38 Train Loss 615.06494 Test MSE 628.5785126922343 Test RE 0.42205488970886507 Lambda1 -1.560266\n",
      "39 Train Loss 613.3087 Test MSE 627.6297247923983 Test RE 0.4217362407361716 Lambda1 -1.5704805\n",
      "40 Train Loss 612.05695 Test MSE 626.9145929101373 Test RE 0.42149590556727956 Lambda1 -1.583326\n",
      "41 Train Loss 609.7184 Test MSE 625.6691381413125 Test RE 0.4210770167147847 Lambda1 -1.636458\n",
      "42 Train Loss 609.30176 Test MSE 625.5936328658015 Test RE 0.42105160832120814 Lambda1 -1.6391002\n",
      "43 Train Loss 608.26587 Test MSE 626.124324174439 Test RE 0.4212301595799888 Lambda1 -1.6362333\n",
      "44 Train Loss 607.24493 Test MSE 625.3197648123815 Test RE 0.42095943570207384 Lambda1 -1.6477696\n",
      "45 Train Loss 606.7396 Test MSE 624.3778898821172 Test RE 0.4206422855209569 Lambda1 -1.6663022\n",
      "46 Train Loss 605.9292 Test MSE 623.5903316513586 Test RE 0.4203769135159072 Lambda1 -1.6938002\n",
      "47 Train Loss 605.68823 Test MSE 623.4175844794859 Test RE 0.4203186830165154 Lambda1 -1.7004573\n",
      "48 Train Loss 605.20715 Test MSE 623.3418698077797 Test RE 0.4202931581851501 Lambda1 -1.7076378\n",
      "49 Train Loss 604.46356 Test MSE 623.4320738343874 Test RE 0.42032356747224425 Lambda1 -1.7132177\n",
      "50 Train Loss 604.3944 Test MSE 623.4551639533967 Test RE 0.4203313511841674 Lambda1 -1.709598\n",
      "51 Train Loss 604.26764 Test MSE 623.3263167254426 Test RE 0.42028791475839866 Lambda1 -1.707823\n",
      "52 Train Loss 604.16754 Test MSE 623.2185522473361 Test RE 0.42025158221127473 Lambda1 -1.7107612\n",
      "53 Train Loss 604.13495 Test MSE 623.2029815053675 Test RE 0.42024633231154235 Lambda1 -1.7120168\n",
      "54 Train Loss 604.0597 Test MSE 623.1026342182015 Test RE 0.42021249720617604 Lambda1 -1.7133577\n",
      "55 Train Loss 603.9868 Test MSE 622.9253818286734 Test RE 0.42015272457567643 Lambda1 -1.715986\n",
      "56 Train Loss 603.96497 Test MSE 622.8438791911608 Test RE 0.4201252375953916 Lambda1 -1.7164598\n",
      "57 Train Loss 603.94135 Test MSE 622.7307883882828 Test RE 0.4200870944435247 Lambda1 -1.713355\n",
      "58 Train Loss 603.85333 Test MSE 622.618493062913 Test RE 0.42004921616232627 Lambda1 -1.7001246\n",
      "59 Train Loss 603.54706 Test MSE 622.5331792918574 Test RE 0.42002043673259376 Lambda1 -1.6732115\n",
      "60 Train Loss 603.22833 Test MSE 622.1826057292951 Test RE 0.4199021548466446 Lambda1 -1.6698143\n",
      "61 Train Loss 602.6033 Test MSE 621.7175022084114 Test RE 0.4197451796431177 Lambda1 -1.6959572\n",
      "62 Train Loss 602.3337 Test MSE 621.2721891235586 Test RE 0.419594828798296 Lambda1 -1.6943504\n",
      "63 Train Loss 602.147 Test MSE 620.7162518165104 Test RE 0.41940705230441844 Lambda1 -1.689288\n",
      "64 Train Loss 601.5234 Test MSE 619.410422200538 Test RE 0.4189656569859343 Lambda1 -1.6748879\n",
      "65 Train Loss 600.5826 Test MSE 618.0575556412667 Test RE 0.41850787129389583 Lambda1 -1.6656076\n",
      "66 Train Loss 599.9798 Test MSE 617.1948782012452 Test RE 0.4182156951311424 Lambda1 -1.6538367\n",
      "67 Train Loss 598.9594 Test MSE 616.0916249931048 Test RE 0.4178417417830724 Lambda1 -1.6535581\n",
      "68 Train Loss 598.39453 Test MSE 615.4192243952612 Test RE 0.4176136639076707 Lambda1 -1.6607481\n",
      "69 Train Loss 597.3246 Test MSE 614.4161794442662 Test RE 0.4172731999764327 Lambda1 -1.6928701\n",
      "70 Train Loss 596.92596 Test MSE 614.1940070468816 Test RE 0.41719775032806955 Lambda1 -1.7092521\n",
      "71 Train Loss 596.59564 Test MSE 614.022105257858 Test RE 0.4171393631917468 Lambda1 -1.7104051\n",
      "72 Train Loss 596.01514 Test MSE 613.081437800034 Test RE 0.416819716863835 Lambda1 -1.7165158\n",
      "73 Train Loss 595.50494 Test MSE 612.2459288589265 Test RE 0.416535598546958 Lambda1 -1.735383\n",
      "74 Train Loss 595.15515 Test MSE 611.8584859309279 Test RE 0.4164037811642642 Lambda1 -1.7306124\n",
      "Training time: 294.75\n",
      "Training time: 294.75\n",
      "inv_HT_atanh_tune1\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 30943.309 Test MSE 3591.5186182838866 Test RE 1.008853794262377 Lambda1 3.7291902e-05\n",
      "1 Train Loss 28707.068 Test MSE 3590.5592446608275 Test RE 1.008719041743187 Lambda1 0.000106026935\n",
      "2 Train Loss 27279.955 Test MSE 3589.9279792253865 Test RE 1.0086303651036708 Lambda1 0.00018885522\n",
      "3 Train Loss 26601.377 Test MSE 3589.4944270156407 Test RE 1.008569457596636 Lambda1 0.00023455195\n",
      "4 Train Loss 25910.059 Test MSE 3588.863706913534 Test RE 1.008480844424155 Lambda1 0.00028941248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 25103.77 Test MSE 3588.8204220892517 Test RE 1.0084747628268407 Lambda1 0.00023958812\n",
      "6 Train Loss 23583.39 Test MSE 3589.2196692579046 Test RE 1.0085308563964646 Lambda1 -0.00013535834\n",
      "7 Train Loss 21729.914 Test MSE 3589.102714117006 Test RE 1.0085144247149054 Lambda1 -0.00027977297\n",
      "8 Train Loss 21160.297 Test MSE 3588.9718389262684 Test RE 1.008496037010308 Lambda1 -0.000356481\n",
      "9 Train Loss 20180.543 Test MSE 3589.705175706656 Test RE 1.0085990650493688 Lambda1 -7.654119e-05\n",
      "10 Train Loss 19585.404 Test MSE 3590.7047707614006 Test RE 1.0087394833306027 Lambda1 0.00036549047\n",
      "11 Train Loss 19075.213 Test MSE 3590.9906770753437 Test RE 1.0087796424727569 Lambda1 0.00041920482\n",
      "12 Train Loss 18908.541 Test MSE 3591.5364974191107 Test RE 1.0088563053743713 Lambda1 0.00048596022\n",
      "13 Train Loss 18655.086 Test MSE 3592.417246725661 Test RE 1.0089799982595935 Lambda1 0.00060148374\n",
      "14 Train Loss 18327.695 Test MSE 3592.886547331147 Test RE 1.0090459008856003 Lambda1 0.00060472975\n",
      "15 Train Loss 17891.709 Test MSE 3594.1890538766243 Test RE 1.0092287858416995 Lambda1 0.000530016\n",
      "16 Train Loss 17469.463 Test MSE 3595.4046211410478 Test RE 1.0093994337608476 Lambda1 0.0003488469\n",
      "17 Train Loss 16666.95 Test MSE 3597.8221750711486 Test RE 1.009738737365277 Lambda1 -1.1957309e-05\n",
      "18 Train Loss 16019.646 Test MSE 3598.2715872475587 Test RE 1.0098017997807351 Lambda1 -0.00014918091\n",
      "19 Train Loss 15600.031 Test MSE 3601.2348660773678 Test RE 1.0102175144385357 Lambda1 -0.00017574344\n",
      "20 Train Loss 15124.457 Test MSE 3603.910197955072 Test RE 1.0105926864918138 Lambda1 -0.00028011962\n",
      "21 Train Loss 14784.766 Test MSE 3606.5570137436816 Test RE 1.0109637226150179 Lambda1 -0.0004196064\n",
      "22 Train Loss 14315.516 Test MSE 3606.916808022125 Test RE 1.0110141488096438 Lambda1 -5.4220964e-05\n",
      "23 Train Loss 13746.65 Test MSE 3606.434857461928 Test RE 1.0109466014916575 Lambda1 8.093854e-05\n",
      "24 Train Loss 12888.54 Test MSE 3606.4492744049267 Test RE 1.0109486221499444 Lambda1 -1.6558504e-05\n",
      "25 Train Loss 12242.277 Test MSE 3605.556778822668 Test RE 1.0108235236196588 Lambda1 -0.0008283043\n",
      "26 Train Loss 11755.939 Test MSE 3605.2704948818796 Test RE 1.0107833927461125 Lambda1 -0.0010143893\n",
      "27 Train Loss 11553.501 Test MSE 3606.4893126357056 Test RE 1.0109542338304116 Lambda1 -0.0011132841\n",
      "28 Train Loss 10955.885 Test MSE 3606.443535552611 Test RE 1.0109478178010256 Lambda1 -2.7540198e-05\n",
      "29 Train Loss 10611.096 Test MSE 3604.487948141843 Test RE 1.0106736883318743 Lambda1 0.0005029061\n",
      "30 Train Loss 10139.416 Test MSE 3607.1524792524533 Test RE 1.0110471774417313 Lambda1 0.0006155285\n",
      "31 Train Loss 9720.776 Test MSE 3607.390475308344 Test RE 1.0110805307963533 Lambda1 0.00032840102\n",
      "32 Train Loss 9313.434 Test MSE 3609.2584787314454 Test RE 1.0113422792075193 Lambda1 0.00041630748\n",
      "33 Train Loss 8892.897 Test MSE 3610.229140546579 Test RE 1.0114782635579433 Lambda1 0.00065139064\n",
      "34 Train Loss 8525.456 Test MSE 3611.0988271857923 Test RE 1.0116000863186054 Lambda1 0.00018728644\n",
      "35 Train Loss 8237.588 Test MSE 3610.5630187102956 Test RE 1.0115250338160802 Lambda1 9.637564e-05\n",
      "36 Train Loss 7831.263 Test MSE 3611.383356301371 Test RE 1.011639939009177 Lambda1 -8.9025634e-05\n",
      "37 Train Loss 7515.7217 Test MSE 3613.285733375363 Test RE 1.0119063559272803 Lambda1 -0.00044179364\n",
      "38 Train Loss 7297.6753 Test MSE 3615.658660526295 Test RE 1.0122385723841705 Lambda1 -0.00055381394\n",
      "39 Train Loss 7067.145 Test MSE 3615.8985659893647 Test RE 1.0122721537531734 Lambda1 -0.00041192007\n",
      "40 Train Loss 6602.8965 Test MSE 3621.085844152433 Test RE 1.012997983761407 Lambda1 0.00012701748\n",
      "41 Train Loss 6475.33 Test MSE 3621.881078501448 Test RE 1.0131092109738253 Lambda1 0.00012524718\n",
      "42 Train Loss 6364.2744 Test MSE 3622.828760183817 Test RE 1.0132417446248374 Lambda1 -6.116235e-05\n",
      "43 Train Loss 6251.3687 Test MSE 3624.6390785477893 Test RE 1.0134948701751387 Lambda1 5.855178e-05\n",
      "44 Train Loss 5979.552 Test MSE 3623.934663783251 Test RE 1.0133963837570164 Lambda1 0.0002776375\n",
      "45 Train Loss 5823.1436 Test MSE 3624.245300815798 Test RE 1.0134398160659503 Lambda1 0.0002758266\n",
      "46 Train Loss 5731.0176 Test MSE 3625.3342596203743 Test RE 1.0135920562199159 Lambda1 8.095961e-05\n",
      "47 Train Loss 5606.0713 Test MSE 3627.3583402794943 Test RE 1.0138749688634516 Lambda1 -0.000522587\n",
      "48 Train Loss 5542.8345 Test MSE 3628.566340051203 Test RE 1.0140437774927817 Lambda1 -0.00061403326\n",
      "49 Train Loss 5493.985 Test MSE 3629.2666300017736 Test RE 1.0141416247334276 Lambda1 -0.00047927897\n",
      "50 Train Loss 5398.5957 Test MSE 3628.388765901761 Test RE 1.0140189646400601 Lambda1 -0.0005757153\n",
      "51 Train Loss 5319.513 Test MSE 3628.2576489083467 Test RE 1.0140006429651183 Lambda1 -0.00071086845\n",
      "52 Train Loss 5227.856 Test MSE 3629.373484872502 Test RE 1.0141565540821955 Lambda1 -0.0003649855\n",
      "53 Train Loss 5165.17 Test MSE 3628.606856213578 Test RE 1.0140494388261854 Lambda1 0.0002322231\n",
      "54 Train Loss 5112.842 Test MSE 3629.135703707696 Test RE 1.0141233319189766 Lambda1 0.0004004633\n",
      "55 Train Loss 5080.836 Test MSE 3629.7229007444234 Test RE 1.0142053715805166 Lambda1 0.0004959072\n",
      "56 Train Loss 5055.3994 Test MSE 3629.4212709308 Test RE 1.014163230494102 Lambda1 0.0005652301\n",
      "57 Train Loss 4999.3457 Test MSE 3629.3017450667603 Test RE 1.0141465308985773 Lambda1 0.00037976363\n",
      "58 Train Loss 4941.699 Test MSE 3628.8908404979593 Test RE 1.0140891191334551 Lambda1 0.00026078572\n",
      "59 Train Loss 4898.8027 Test MSE 3629.4111815059437 Test RE 1.0141618208574064 Lambda1 0.0001389789\n",
      "60 Train Loss 4855.436 Test MSE 3628.7765295297236 Test RE 1.0140731469773496 Lambda1 -0.00014181065\n",
      "61 Train Loss 4792.361 Test MSE 3625.8054153276967 Test RE 1.0136579183077734 Lambda1 -0.0003931684\n",
      "62 Train Loss 4707.833 Test MSE 3624.991681110452 Test RE 1.0135441650414947 Lambda1 -0.00041781165\n",
      "63 Train Loss 4669.619 Test MSE 3624.604505539415 Test RE 1.0134900366386914 Lambda1 -0.000593297\n",
      "64 Train Loss 4643.356 Test MSE 3624.540582779786 Test RE 1.0134810997511585 Lambda1 -0.00068993936\n",
      "65 Train Loss 4614.8223 Test MSE 3624.6517462011957 Test RE 1.0134966411915416 Lambda1 -0.0006263672\n",
      "66 Train Loss 4591.0483 Test MSE 3625.214801547274 Test RE 1.0135753566906134 Lambda1 -0.00052065495\n",
      "67 Train Loss 4537.531 Test MSE 3624.9060233828454 Test RE 1.0135321900618919 Lambda1 -0.00045692385\n",
      "68 Train Loss 4512.2764 Test MSE 3624.5769336261937 Test RE 1.0134861818853742 Lambda1 -0.00038931717\n",
      "69 Train Loss 4487.1313 Test MSE 3624.2035740229308 Test RE 1.0134339820630756 Lambda1 -0.00031733752\n",
      "70 Train Loss 4448.0024 Test MSE 3623.9077020084865 Test RE 1.013392613957282 Lambda1 -0.00010455021\n",
      "71 Train Loss 4419.5317 Test MSE 3623.9462743127356 Test RE 1.013398007138818 Lambda1 0.00013368629\n",
      "72 Train Loss 4395.3413 Test MSE 3623.3205658328234 Test RE 1.0133105170035575 Lambda1 0.00017926711\n",
      "73 Train Loss 4372.5903 Test MSE 3623.3349256924203 Test RE 1.0133125249658772 Lambda1 4.2530613e-05\n",
      "74 Train Loss 4342.1616 Test MSE 3623.8300420268183 Test RE 1.0133817554476383 Lambda1 -0.00026431473\n",
      "Training time: 281.64\n",
      "Training time: 281.64\n",
      "inv_HT_atanh_tune1\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 21515.191 Test MSE 3565.229225584484 Test RE 1.0051546812377867 Lambda1 2.7919406e-05\n",
      "1 Train Loss 15719.581 Test MSE 3580.941353449156 Test RE 1.007367127600263 Lambda1 0.0005368615\n",
      "2 Train Loss 13534.383 Test MSE 3576.9414386533595 Test RE 1.0068043553965917 Lambda1 -8.279202e-05\n",
      "3 Train Loss 12060.824 Test MSE 3578.10417052434 Test RE 1.0069679796009752 Lambda1 -0.0010287693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 9955.774 Test MSE 3577.0963499395193 Test RE 1.0068261566577623 Lambda1 0.00033363924\n",
      "5 Train Loss 8904.858 Test MSE 3574.418695093716 Test RE 1.0064492535251395 Lambda1 0.0012812733\n",
      "6 Train Loss 7580.739 Test MSE 3576.193902313322 Test RE 1.0066991450958094 Lambda1 -0.00020376628\n",
      "7 Train Loss 6308.2646 Test MSE 3567.146515467945 Test RE 1.0054249182622794 Lambda1 -0.00050629384\n",
      "8 Train Loss 5307.5005 Test MSE 3563.371098656932 Test RE 1.0048927137347836 Lambda1 -0.000107005515\n",
      "9 Train Loss 4584.074 Test MSE 3557.1507557244586 Test RE 1.0040152430205287 Lambda1 0.00026591713\n",
      "10 Train Loss 4139.579 Test MSE 3538.028969122999 Test RE 1.0013130191016497 Lambda1 -0.0004372634\n",
      "11 Train Loss 3862.2617 Test MSE 3520.4801715070166 Test RE 0.998826651242728 Lambda1 0.00010047055\n",
      "12 Train Loss 3747.6682 Test MSE 3514.706320308912 Test RE 0.9980072398661095 Lambda1 0.00028680876\n",
      "13 Train Loss 3676.9968 Test MSE 3503.297731651549 Test RE 0.9963861785931466 Lambda1 8.997141e-05\n",
      "14 Train Loss 3603.4504 Test MSE 3480.377816723416 Test RE 0.9931214601578461 Lambda1 -0.00020404794\n",
      "15 Train Loss 3541.467 Test MSE 3453.2476011819826 Test RE 0.9892431006826206 Lambda1 -1.6450247e-06\n",
      "16 Train Loss 3470.7668 Test MSE 3403.085893859153 Test RE 0.9820319690775713 Lambda1 -0.00011711126\n",
      "17 Train Loss 3415.9102 Test MSE 3364.266059451126 Test RE 0.9764147643092291 Lambda1 0.00014754667\n",
      "18 Train Loss 3383.181 Test MSE 3332.0624492223533 Test RE 0.971730281879451 Lambda1 -2.4996605e-05\n",
      "19 Train Loss 3336.2046 Test MSE 3297.398390836047 Test RE 0.9666625228074918 Lambda1 -0.00012203131\n",
      "20 Train Loss 3320.158 Test MSE 3277.405602720008 Test RE 0.9637275327594357 Lambda1 -0.00040239305\n",
      "21 Train Loss 3236.1748 Test MSE 3186.6655245473453 Test RE 0.9502927404545628 Lambda1 0.00014550844\n",
      "22 Train Loss 3146.2517 Test MSE 3115.664352362499 Test RE 0.9396465062416461 Lambda1 0.0010169755\n",
      "23 Train Loss 3030.1445 Test MSE 3006.5601065374162 Test RE 0.9230476393111542 Lambda1 -0.0021435255\n",
      "24 Train Loss 1758.247 Test MSE 1750.2375905048452 Test RE 0.7042675434972068 Lambda1 -0.09332991\n",
      "25 Train Loss 856.0752 Test MSE 857.8707515892422 Test RE 0.4930603744694665 Lambda1 -0.23357533\n",
      "26 Train Loss 854.7738 Test MSE 858.0010067846464 Test RE 0.4930978050717052 Lambda1 -0.23390625\n",
      "27 Train Loss 854.22894 Test MSE 856.9103070413066 Test RE 0.49278428989222817 Lambda1 -0.23079163\n",
      "28 Train Loss 853.55786 Test MSE 855.5716490732576 Test RE 0.49239922774233674 Lambda1 -0.22571658\n",
      "29 Train Loss 851.95917 Test MSE 854.071828048653 Test RE 0.491967449379748 Lambda1 -0.2200128\n",
      "30 Train Loss 845.36816 Test MSE 842.3106247308472 Test RE 0.4885683281474733 Lambda1 -0.22501662\n",
      "31 Train Loss 833.9901 Test MSE 828.2593531229019 Test RE 0.48447608616992593 Lambda1 -0.22199543\n",
      "32 Train Loss 824.71027 Test MSE 819.7292808809509 Test RE 0.48197486998386857 Lambda1 -0.1935134\n",
      "33 Train Loss 811.6728 Test MSE 798.1842242483298 Test RE 0.4755987895051075 Lambda1 -0.18785186\n",
      "34 Train Loss 799.6894 Test MSE 788.6132263371866 Test RE 0.47273874611287237 Lambda1 -0.18376356\n",
      "35 Train Loss 790.9249 Test MSE 780.7359702826906 Test RE 0.47037178725838397 Lambda1 -0.17607044\n",
      "36 Train Loss 783.3543 Test MSE 774.8837451476805 Test RE 0.46860556950075116 Lambda1 -0.1717902\n",
      "37 Train Loss 777.0292 Test MSE 767.4645506720098 Test RE 0.466356820646014 Lambda1 -0.15469797\n",
      "38 Train Loss 765.77924 Test MSE 763.3894928117294 Test RE 0.46511704972939416 Lambda1 -0.120675564\n",
      "39 Train Loss 760.03125 Test MSE 759.1194397295959 Test RE 0.4638143991138085 Lambda1 -0.10798843\n",
      "40 Train Loss 743.5256 Test MSE 746.36123329258 Test RE 0.4599003154504614 Lambda1 -0.087329276\n",
      "41 Train Loss 731.3605 Test MSE 736.9781114352711 Test RE 0.45700027908783003 Lambda1 -0.055658292\n",
      "42 Train Loss 708.5025 Test MSE 711.2337835752652 Test RE 0.4489472951581545 Lambda1 -0.018959949\n",
      "43 Train Loss 686.81494 Test MSE 692.5360692486945 Test RE 0.443006776434169 Lambda1 -0.0056696557\n",
      "44 Train Loss 679.9869 Test MSE 688.3812415703071 Test RE 0.4416758812810982 Lambda1 -0.0029067863\n",
      "45 Train Loss 670.96747 Test MSE 677.485336224178 Test RE 0.43816644945483757 Lambda1 -0.00058517104\n",
      "46 Train Loss 647.12787 Test MSE 639.3199122283498 Test RE 0.42564573512052667 Lambda1 6.907599e-06\n",
      "47 Train Loss 630.65497 Test MSE 618.7228949055636 Test RE 0.41873307266805704 Lambda1 0.000118725155\n",
      "48 Train Loss 596.77783 Test MSE 577.4845792072412 Test RE 0.4045380404319692 Lambda1 0.0012191739\n",
      "49 Train Loss 545.7328 Test MSE 536.8636797422176 Test RE 0.39005080856681396 Lambda1 0.00039812434\n",
      "50 Train Loss 472.5156 Test MSE 449.911403563864 Test RE 0.35706943072501024 Lambda1 0.0005670157\n",
      "51 Train Loss 459.83487 Test MSE 435.1031089841727 Test RE 0.3511440095926138 Lambda1 0.00010716946\n",
      "52 Train Loss 436.38794 Test MSE 425.87955986022297 Test RE 0.34740220362156204 Lambda1 0.0010672596\n",
      "53 Train Loss 424.30652 Test MSE 423.19765909693893 Test RE 0.3463066243067747 Lambda1 0.001123201\n",
      "54 Train Loss 411.25888 Test MSE 415.8431804701182 Test RE 0.3432843165629962 Lambda1 0.00055484497\n",
      "55 Train Loss 403.45767 Test MSE 412.61754927762706 Test RE 0.34195032297404815 Lambda1 9.236185e-05\n",
      "56 Train Loss 400.7915 Test MSE 410.7099363369298 Test RE 0.3411589550977195 Lambda1 -7.746442e-05\n",
      "57 Train Loss 397.5963 Test MSE 408.69453298660096 Test RE 0.34032087155384144 Lambda1 -0.00013559326\n",
      "58 Train Loss 395.78238 Test MSE 406.4046380838055 Test RE 0.33936613197668475 Lambda1 -0.00020892007\n",
      "59 Train Loss 393.30475 Test MSE 401.2376264067884 Test RE 0.3372018874946955 Lambda1 -0.0003410543\n",
      "60 Train Loss 389.40494 Test MSE 396.84531989051084 Test RE 0.3353511516285665 Lambda1 -0.00039659097\n",
      "61 Train Loss 384.44806 Test MSE 391.4710282202133 Test RE 0.3330726588213884 Lambda1 -0.0003643217\n",
      "62 Train Loss 381.52686 Test MSE 388.0432064592534 Test RE 0.33161121734409105 Lambda1 -0.0005874561\n",
      "63 Train Loss 376.85022 Test MSE 383.4826250224357 Test RE 0.32965678316379055 Lambda1 -0.0005760482\n",
      "64 Train Loss 366.59976 Test MSE 367.9382509061154 Test RE 0.3229063907524867 Lambda1 -0.00016402587\n",
      "65 Train Loss 357.53577 Test MSE 361.822750855457 Test RE 0.320211633596599 Lambda1 -5.9239746e-06\n",
      "66 Train Loss 355.10916 Test MSE 360.70278632557273 Test RE 0.31971566752164365 Lambda1 0.00016933325\n",
      "67 Train Loss 350.07983 Test MSE 358.7800406964459 Test RE 0.31886239811017486 Lambda1 -0.00038344532\n",
      "68 Train Loss 348.31076 Test MSE 357.30064884161425 Test RE 0.31820432120978026 Lambda1 -0.00023769651\n",
      "69 Train Loss 346.64987 Test MSE 356.98396274873295 Test RE 0.31806327301619275 Lambda1 -2.0091346e-05\n",
      "70 Train Loss 345.26328 Test MSE 354.66195543942575 Test RE 0.3170271618553174 Lambda1 -8.157958e-05\n",
      "71 Train Loss 343.893 Test MSE 352.60684905478996 Test RE 0.3161073125428277 Lambda1 -0.00017285674\n",
      "72 Train Loss 342.65622 Test MSE 351.5628479914186 Test RE 0.31563899916154825 Lambda1 -8.734342e-05\n",
      "73 Train Loss 341.5723 Test MSE 350.94927892550265 Test RE 0.31536344259975996 Lambda1 -0.00018516221\n",
      "74 Train Loss 340.38217 Test MSE 350.25260684724645 Test RE 0.31505027192099294 Lambda1 -0.00030879426\n",
      "Training time: 251.87\n",
      "Training time: 251.87\n",
      "inv_HT_atanh_tune1\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 7517.233 Test MSE 3518.4052951085464 Test RE 0.9985322670686594 Lambda1 -9.033e-05\n",
      "1 Train Loss 5316.346 Test MSE 3515.2175378917723 Test RE 0.9980798178074026 Lambda1 -0.00086225534\n",
      "2 Train Loss 4200.152 Test MSE 3516.578130989959 Test RE 0.998272956513333 Lambda1 0.001471406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Train Loss 3704.231 Test MSE 3516.9667408257314 Test RE 0.9983281135378966 Lambda1 7.5934586e-05\n",
      "4 Train Loss 3604.9375 Test MSE 3510.761581632926 Test RE 0.9974470247783609 Lambda1 -0.000631645\n",
      "5 Train Loss 3539.7112 Test MSE 3486.6844304933666 Test RE 0.9940208450885117 Lambda1 -0.00067847094\n",
      "6 Train Loss 3375.835 Test MSE 3316.7136408595265 Test RE 0.969489609957047 Lambda1 -0.0016875339\n",
      "7 Train Loss 3237.0073 Test MSE 3197.2697237323337 Test RE 0.9518725617463389 Lambda1 0.0008790145\n",
      "8 Train Loss 3082.3315 Test MSE 3055.797110639133 Test RE 0.9305751020088513 Lambda1 0.0016953698\n",
      "9 Train Loss 2748.896 Test MSE 2724.4747423495796 Test RE 0.8786796404883356 Lambda1 0.005494956\n",
      "10 Train Loss 954.187 Test MSE 954.1682254508949 Test RE 0.519997966980306 Lambda1 0.044306908\n",
      "11 Train Loss 854.748 Test MSE 858.0572377461374 Test RE 0.4931139629253619 Lambda1 0.054991465\n",
      "12 Train Loss 854.697 Test MSE 858.0423371848989 Test RE 0.4931096813306279 Lambda1 0.055083323\n",
      "13 Train Loss 854.6246 Test MSE 857.8514783060123 Test RE 0.49305483578802545 Lambda1 0.054402336\n",
      "14 Train Loss 854.47955 Test MSE 857.561130965444 Test RE 0.4929713893644926 Lambda1 0.054064106\n",
      "15 Train Loss 854.322 Test MSE 857.3064233356547 Test RE 0.49289817423467663 Lambda1 0.05388009\n",
      "16 Train Loss 853.8822 Test MSE 856.5349193584304 Test RE 0.4926763407785208 Lambda1 0.050838556\n",
      "17 Train Loss 852.4384 Test MSE 852.7331396024852 Test RE 0.4915817386038721 Lambda1 0.01712263\n",
      "18 Train Loss 845.9194 Test MSE 840.7831512337143 Test RE 0.4881251342781922 Lambda1 -0.0063513746\n",
      "19 Train Loss 814.6617 Test MSE 808.6911049799671 Test RE 0.4787188226292824 Lambda1 -0.003623923\n",
      "20 Train Loss 792.20465 Test MSE 790.589732117515 Test RE 0.4733307892785625 Lambda1 -0.0016278634\n",
      "21 Train Loss 760.5819 Test MSE 758.6065697696393 Test RE 0.46365769342860397 Lambda1 -0.00047886962\n",
      "22 Train Loss 745.44196 Test MSE 749.1600120943217 Test RE 0.4607617983186081 Lambda1 -0.0005317102\n",
      "23 Train Loss 729.38837 Test MSE 730.5538698734434 Test RE 0.45500408259303554 Lambda1 -0.0004642353\n",
      "24 Train Loss 713.76807 Test MSE 716.3631129301186 Test RE 0.45056326280704784 Lambda1 -0.00049921306\n",
      "25 Train Loss 705.2713 Test MSE 709.9576620902449 Test RE 0.4485443555638929 Lambda1 -0.0006698078\n",
      "26 Train Loss 699.72437 Test MSE 706.4852463132866 Test RE 0.4474460917294323 Lambda1 -0.0006328274\n",
      "27 Train Loss 692.8274 Test MSE 703.0452477047162 Test RE 0.4463554163111322 Lambda1 -0.00029716856\n",
      "28 Train Loss 690.5728 Test MSE 702.9980683783667 Test RE 0.44634043925152433 Lambda1 -0.00026635395\n",
      "29 Train Loss 688.2317 Test MSE 702.3382992211704 Test RE 0.44613094309893925 Lambda1 -0.00024635784\n",
      "30 Train Loss 685.99426 Test MSE 700.3339853503636 Test RE 0.44549391013237216 Lambda1 -0.0002501344\n",
      "31 Train Loss 682.51483 Test MSE 693.9935317943039 Test RE 0.4434726918312775 Lambda1 -0.00023942502\n",
      "32 Train Loss 677.9807 Test MSE 691.8990142969438 Test RE 0.442802971458846 Lambda1 -0.00012270964\n",
      "33 Train Loss 674.33734 Test MSE 686.4102628138141 Test RE 0.4410431230432407 Lambda1 -0.00014647223\n",
      "34 Train Loss 671.37823 Test MSE 682.4801164656018 Test RE 0.43977868076873855 Lambda1 -0.00016193822\n",
      "35 Train Loss 667.46985 Test MSE 679.2265078346159 Test RE 0.43872914312722633 Lambda1 -0.00013842249\n",
      "36 Train Loss 663.5733 Test MSE 675.6265445062902 Test RE 0.43756494596645623 Lambda1 -8.363338e-05\n",
      "37 Train Loss 662.74725 Test MSE 674.4146381987265 Test RE 0.4371723283754591 Lambda1 -7.40459e-05\n",
      "38 Train Loss 661.42334 Test MSE 672.3820071551565 Test RE 0.43651303025511495 Lambda1 -3.0946372e-05\n",
      "39 Train Loss 660.7238 Test MSE 671.7577147116559 Test RE 0.4363103366463372 Lambda1 -1.1431027e-05\n",
      "40 Train Loss 659.56396 Test MSE 668.2148135141125 Test RE 0.43515824866484276 Lambda1 -6.393979e-06\n",
      "41 Train Loss 659.08 Test MSE 669.086269250291 Test RE 0.4354419131272568 Lambda1 -3.4529298e-06\n",
      "42 Train Loss 658.119 Test MSE 667.2503867537705 Test RE 0.4348441058247828 Lambda1 -1.9467457e-06\n",
      "43 Train Loss 655.5165 Test MSE 652.462059063131 Test RE 0.42999836257969665 Lambda1 -1.3651136e-05\n",
      "44 Train Loss 645.0183 Test MSE 628.5545988514691 Test RE 0.42204686123730617 Lambda1 6.1113424e-06\n",
      "45 Train Loss 627.5004 Test MSE 598.7173225546704 Test RE 0.4119078630850774 Lambda1 7.5336084e-06\n",
      "46 Train Loss 619.8463 Test MSE 597.3127475899886 Test RE 0.4114244169177563 Lambda1 -1.4417982e-05\n",
      "47 Train Loss 605.41003 Test MSE 581.4368497297635 Test RE 0.4059199972434485 Lambda1 -2.9856377e-05\n",
      "48 Train Loss 569.37384 Test MSE 555.8588625003317 Test RE 0.39689117085935355 Lambda1 2.4594494e-05\n",
      "49 Train Loss 557.1142 Test MSE 544.7635972343749 Test RE 0.3929101157068684 Lambda1 -1.04274195e-05\n",
      "50 Train Loss 550.0479 Test MSE 526.9285860544221 Test RE 0.3864248531440073 Lambda1 -4.7643844e-06\n",
      "51 Train Loss 508.88812 Test MSE 482.3391826437883 Test RE 0.36971361336115977 Lambda1 3.1591208e-07\n",
      "52 Train Loss 501.52112 Test MSE 478.7194493812221 Test RE 0.3683237357164235 Lambda1 5.7769325e-06\n",
      "53 Train Loss 497.85968 Test MSE 478.50397765920815 Test RE 0.36824083509261213 Lambda1 -4.1172189e-07\n",
      "54 Train Loss 493.66034 Test MSE 476.2184542853634 Test RE 0.3673603508509262 Lambda1 8.56805e-07\n",
      "55 Train Loss 488.29086 Test MSE 469.44708811368065 Test RE 0.3647392455219306 Lambda1 1.486142e-06\n",
      "56 Train Loss 485.0958 Test MSE 468.73554649095956 Test RE 0.3644627228027349 Lambda1 -9.096145e-07\n",
      "57 Train Loss 482.96634 Test MSE 466.94251104881204 Test RE 0.3637649725157272 Lambda1 4.991977e-07\n",
      "58 Train Loss 481.55273 Test MSE 467.1900740622863 Test RE 0.36386138997349243 Lambda1 -2.1747146e-06\n",
      "59 Train Loss 479.6127 Test MSE 466.8258875357367 Test RE 0.36371954272803303 Lambda1 -2.2856077e-06\n",
      "60 Train Loss 474.36398 Test MSE 461.1272411381076 Test RE 0.3614927237716358 Lambda1 2.6871055e-07\n",
      "61 Train Loss 472.6151 Test MSE 460.6599426470511 Test RE 0.3613095120597685 Lambda1 2.055709e-06\n",
      "62 Train Loss 472.2419 Test MSE 460.5942443858588 Test RE 0.36128374657540485 Lambda1 7.9130945e-07\n",
      "63 Train Loss 469.0008 Test MSE 459.1567353371414 Test RE 0.36071952490417847 Lambda1 -2.100655e-06\n",
      "64 Train Loss 463.56973 Test MSE 456.5827627306694 Test RE 0.35970703069284593 Lambda1 -5.6246436e-06\n",
      "65 Train Loss 462.4229 Test MSE 455.1723261624979 Test RE 0.3591510128044876 Lambda1 -6.328147e-06\n",
      "66 Train Loss 459.66727 Test MSE 442.7139469647182 Test RE 0.35420180710560145 Lambda1 -4.651856e-06\n",
      "67 Train Loss 445.03925 Test MSE 433.4109746821595 Test RE 0.3504605375168143 Lambda1 -6.5722484e-06\n",
      "68 Train Loss 439.0449 Test MSE 430.1259655535536 Test RE 0.3491298653318515 Lambda1 -7.484365e-06\n",
      "69 Train Loss 437.73004 Test MSE 429.35058193246647 Test RE 0.3488150369881477 Lambda1 -7.0903584e-06\n",
      "70 Train Loss 436.95877 Test MSE 429.29186896067 Test RE 0.348791186237166 Lambda1 -6.982732e-06\n",
      "71 Train Loss 436.31555 Test MSE 430.1703639141892 Test RE 0.349147883767062 Lambda1 -6.4901124e-06\n",
      "72 Train Loss 435.9035 Test MSE 430.418652874582 Test RE 0.3492486311301189 Lambda1 -6.9136054e-06\n",
      "73 Train Loss 435.26538 Test MSE 429.8617359239636 Test RE 0.3490226123010237 Lambda1 -7.6074434e-06\n",
      "74 Train Loss 434.4904 Test MSE 429.37316649561154 Test RE 0.3488242109964828 Lambda1 -7.4485674e-06\n",
      "Training time: 292.41\n",
      "Training time: 292.41\n",
      "inv_HT_atanh_tune1\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 24573.418 Test MSE 3521.571391998186 Test RE 0.9989814391705859 Lambda1 0.0003386539\n",
      "1 Train Loss 21988.389 Test MSE 3518.861406370049 Test RE 0.9985969877324761 Lambda1 0.00017656192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Train Loss 17862.53 Test MSE 3513.4079589337016 Test RE 0.9978228868044561 Lambda1 -0.00052958954\n",
      "3 Train Loss 15446.445 Test MSE 3511.5123986703315 Test RE 0.9975536768774473 Lambda1 0.0002137135\n",
      "4 Train Loss 14445.902 Test MSE 3512.6174956638038 Test RE 0.9977106330155641 Lambda1 0.0005352995\n",
      "5 Train Loss 11307.082 Test MSE 3485.007499282705 Test RE 0.9937817777042535 Lambda1 -0.0003859533\n",
      "6 Train Loss 7313.1357 Test MSE 3464.155096505863 Test RE 0.990804190222669 Lambda1 -0.0002139867\n",
      "7 Train Loss 5248.2646 Test MSE 3465.247549817 Test RE 0.9909604075346558 Lambda1 -7.550513e-05\n",
      "8 Train Loss 4473.662 Test MSE 3464.8276342041486 Test RE 0.9909003638656427 Lambda1 -6.098894e-05\n",
      "9 Train Loss 4045.5762 Test MSE 3465.4503315569045 Test RE 0.9909894019618939 Lambda1 -0.00021769946\n",
      "10 Train Loss 3751.6055 Test MSE 3458.3599574888513 Test RE 0.9899750917503188 Lambda1 0.0002654855\n",
      "11 Train Loss 3596.086 Test MSE 3453.171873324414 Test RE 0.9892322538393027 Lambda1 0.00047671702\n",
      "12 Train Loss 3508.7715 Test MSE 3442.4258502506623 Test RE 0.9876918446009207 Lambda1 -0.00064559304\n",
      "13 Train Loss 3465.0984 Test MSE 3418.6843219379602 Test RE 0.9842800231244796 Lambda1 -0.0011248778\n",
      "14 Train Loss 3430.1458 Test MSE 3396.199661175144 Test RE 0.9810378823364785 Lambda1 -0.00024045128\n",
      "15 Train Loss 3253.2078 Test MSE 3209.4505404907886 Test RE 0.9536840390542153 Lambda1 -0.003965464\n",
      "16 Train Loss 2757.2473 Test MSE 2740.994523938159 Test RE 0.8813395400729388 Lambda1 -0.04850441\n",
      "17 Train Loss 1165.8815 Test MSE 1151.1969555830158 Test RE 0.5711681564691127 Lambda1 -0.366535\n",
      "18 Train Loss 862.2604 Test MSE 864.3501947413375 Test RE 0.49491889854607196 Lambda1 -0.5189419\n",
      "19 Train Loss 854.7315 Test MSE 858.0556119850846 Test RE 0.4931134957736266 Lambda1 -0.545498\n",
      "20 Train Loss 854.72064 Test MSE 858.0730150623053 Test RE 0.49311849641030586 Lambda1 -0.54590315\n",
      "21 Train Loss 854.71844 Test MSE 858.0732818485741 Test RE 0.49311857306883694 Lambda1 -0.54584104\n",
      "22 Train Loss 854.69275 Test MSE 858.0176874247749 Test RE 0.493102598275322 Lambda1 -0.5429251\n",
      "23 Train Loss 854.63385 Test MSE 857.9284315875949 Test RE 0.49307694995214 Lambda1 -0.5252869\n",
      "24 Train Loss 854.5007 Test MSE 857.7039031116464 Test RE 0.4930124241383093 Lambda1 -0.51153713\n",
      "25 Train Loss 854.2998 Test MSE 857.2258648759672 Test RE 0.4928750156244929 Lambda1 -0.48724195\n",
      "26 Train Loss 853.94836 Test MSE 856.6159443470572 Test RE 0.4926996428938237 Lambda1 -0.47452736\n",
      "27 Train Loss 853.7561 Test MSE 856.3208836424625 Test RE 0.49261478057857555 Lambda1 -0.46505123\n",
      "28 Train Loss 853.04205 Test MSE 854.9257852110582 Test RE 0.49221333863054734 Lambda1 -0.43031788\n",
      "29 Train Loss 851.29663 Test MSE 851.8454617181108 Test RE 0.4913258086776768 Lambda1 -0.37471658\n",
      "30 Train Loss 848.5815 Test MSE 847.7392170101662 Test RE 0.4901401821353701 Lambda1 -0.27940187\n",
      "31 Train Loss 844.0286 Test MSE 842.6968198499054 Test RE 0.4886803183536755 Lambda1 -0.25077456\n",
      "32 Train Loss 840.8248 Test MSE 836.2596973815103 Test RE 0.4868102950514255 Lambda1 -0.2113719\n",
      "33 Train Loss 836.1682 Test MSE 834.9604003549156 Test RE 0.4864319693679589 Lambda1 -0.22466363\n",
      "34 Train Loss 833.7647 Test MSE 833.0299933956886 Test RE 0.4858693348828574 Lambda1 -0.2110426\n",
      "35 Train Loss 832.09344 Test MSE 830.6320284871542 Test RE 0.4851695177708857 Lambda1 -0.2128061\n",
      "36 Train Loss 830.5569 Test MSE 828.0770609998714 Test RE 0.4844227689125367 Lambda1 -0.20429295\n",
      "37 Train Loss 828.62335 Test MSE 826.6322204670507 Test RE 0.48399997056684685 Lambda1 -0.19859056\n",
      "38 Train Loss 826.36206 Test MSE 824.0734448837461 Test RE 0.48325029789394724 Lambda1 -0.20086974\n",
      "39 Train Loss 825.4884 Test MSE 823.4481731686353 Test RE 0.48306692826635667 Lambda1 -0.20245388\n",
      "40 Train Loss 824.6395 Test MSE 822.1618689471937 Test RE 0.4826894826060399 Lambda1 -0.20455469\n",
      "41 Train Loss 823.87335 Test MSE 821.3411813494534 Test RE 0.4824485104854611 Lambda1 -0.20123142\n",
      "42 Train Loss 822.43286 Test MSE 820.1625836913629 Test RE 0.4821022373232543 Lambda1 -0.20669983\n",
      "43 Train Loss 821.3104 Test MSE 817.2267180511391 Test RE 0.4812385937426236 Lambda1 -0.21074566\n",
      "44 Train Loss 820.50995 Test MSE 815.6504134839696 Test RE 0.48077425213153924 Lambda1 -0.20663\n",
      "45 Train Loss 817.9767 Test MSE 813.3737950823014 Test RE 0.4801028221166871 Lambda1 -0.18419081\n",
      "46 Train Loss 815.5143 Test MSE 809.5646097609124 Test RE 0.4789772960558488 Lambda1 -0.1847132\n",
      "47 Train Loss 813.5888 Test MSE 806.3323718706627 Test RE 0.47802016618074683 Lambda1 -0.1871455\n",
      "48 Train Loss 812.8642 Test MSE 803.9871812366239 Test RE 0.47732450718439534 Lambda1 -0.18341222\n",
      "49 Train Loss 811.8504 Test MSE 803.2287016282146 Test RE 0.47709930065052314 Lambda1 -0.18306825\n",
      "50 Train Loss 811.20526 Test MSE 803.3910668236969 Test RE 0.47714751880232764 Lambda1 -0.18096003\n",
      "51 Train Loss 808.8444 Test MSE 802.1309529256188 Test RE 0.47677317049424495 Lambda1 -0.19349326\n",
      "52 Train Loss 806.6795 Test MSE 799.707581582843 Test RE 0.4760524200870229 Lambda1 -0.20036925\n",
      "53 Train Loss 803.4374 Test MSE 795.5897384243286 Test RE 0.4748251969972703 Lambda1 -0.20103842\n",
      "54 Train Loss 801.9823 Test MSE 794.4036863196211 Test RE 0.47447113419087744 Lambda1 -0.20930144\n",
      "55 Train Loss 801.135 Test MSE 794.3661300126325 Test RE 0.47445991848625996 Lambda1 -0.21667856\n",
      "56 Train Loss 799.613 Test MSE 797.3171346696921 Test RE 0.4753403912562342 Lambda1 -0.22265957\n",
      "57 Train Loss 797.26404 Test MSE 795.163303806616 Test RE 0.4746979272303693 Lambda1 -0.21994954\n",
      "58 Train Loss 795.588 Test MSE 791.9563712558163 Test RE 0.47373972013480864 Lambda1 -0.22335072\n",
      "59 Train Loss 793.909 Test MSE 790.5887354456969 Test RE 0.4733304909217867 Lambda1 -0.22347997\n",
      "60 Train Loss 792.5502 Test MSE 790.3866919796055 Test RE 0.47327000470466923 Lambda1 -0.22124389\n",
      "61 Train Loss 790.5458 Test MSE 787.4423638623952 Test RE 0.4723876756237618 Lambda1 -0.23306865\n",
      "62 Train Loss 789.224 Test MSE 787.4505499566126 Test RE 0.47239013104154676 Lambda1 -0.23077399\n",
      "63 Train Loss 788.04395 Test MSE 786.4696311375986 Test RE 0.47209581366175013 Lambda1 -0.22530495\n",
      "64 Train Loss 786.44086 Test MSE 785.785192217594 Test RE 0.47189034413882136 Lambda1 -0.22526148\n",
      "65 Train Loss 785.2036 Test MSE 784.3847891801896 Test RE 0.47146966214370967 Lambda1 -0.23006026\n",
      "66 Train Loss 784.09625 Test MSE 783.8952850324683 Test RE 0.47132252621261267 Lambda1 -0.2271842\n",
      "67 Train Loss 783.2563 Test MSE 783.576775693861 Test RE 0.4712267634996776 Lambda1 -0.22385605\n",
      "68 Train Loss 782.5512 Test MSE 782.338472962772 Test RE 0.47085427153308906 Lambda1 -0.22417562\n",
      "69 Train Loss 780.6622 Test MSE 780.7908039006669 Test RE 0.4703883048360414 Lambda1 -0.2281957\n",
      "70 Train Loss 779.29193 Test MSE 779.9451480149322 Test RE 0.4701335026513532 Lambda1 -0.23444636\n",
      "71 Train Loss 778.296 Test MSE 778.0448270821793 Test RE 0.469560417868955 Lambda1 -0.24084009\n",
      "72 Train Loss 777.1215 Test MSE 777.194492524112 Test RE 0.46930375360515453 Lambda1 -0.24033873\n",
      "73 Train Loss 776.1874 Test MSE 777.2194788501536 Test RE 0.4693112974583773 Lambda1 -0.24570249\n",
      "74 Train Loss 775.5331 Test MSE 776.9495913892076 Test RE 0.4692298068141529 Lambda1 -0.24820372\n",
      "Training time: 293.04\n",
      "Training time: 293.04\n",
      "inv_HT_atanh_tune1\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 9429.872 Test MSE 3586.801731033071 Test RE 1.0081910922740527 Lambda1 -3.6126665e-05\n",
      "1 Train Loss 9361.385 Test MSE 3586.5299466109955 Test RE 1.0081528944804772 Lambda1 -7.780707e-05\n",
      "2 Train Loss 8657.615 Test MSE 3585.167539359209 Test RE 1.0079613938798837 Lambda1 -0.000344888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Train Loss 7345.036 Test MSE 3583.354040731445 Test RE 1.0077064312036568 Lambda1 -0.0010730334\n",
      "4 Train Loss 6579.497 Test MSE 3582.9386969879306 Test RE 1.0076480282820175 Lambda1 -0.0020112938\n",
      "5 Train Loss 6229.0356 Test MSE 3583.3851640515313 Test RE 1.0077108074251175 Lambda1 -0.002453768\n",
      "6 Train Loss 6002.348 Test MSE 3584.4286751118266 Test RE 1.0078575235581326 Lambda1 -0.002500451\n",
      "7 Train Loss 5202.616 Test MSE 3589.641774144408 Test RE 1.0085901580450836 Lambda1 -0.00017570061\n",
      "8 Train Loss 4950.2007 Test MSE 3588.6023816600546 Test RE 1.0084441271883557 Lambda1 -0.0001589118\n",
      "9 Train Loss 4352.817 Test MSE 3572.817877372037 Test RE 1.0062238571225428 Lambda1 -0.00053138286\n",
      "10 Train Loss 4084.4988 Test MSE 3560.519818383096 Test RE 1.0044905939322841 Lambda1 -0.0014222804\n",
      "11 Train Loss 3938.0537 Test MSE 3555.1923687794283 Test RE 1.0037388250130335 Lambda1 -0.001390008\n",
      "12 Train Loss 3882.349 Test MSE 3550.604479548001 Test RE 1.003090965670721 Lambda1 -0.0014096149\n",
      "13 Train Loss 3766.81 Test MSE 3539.823408340636 Test RE 1.0015669129499791 Lambda1 -0.0008661987\n",
      "14 Train Loss 3695.181 Test MSE 3532.198943274979 Test RE 1.0004876884336746 Lambda1 -0.0006559813\n",
      "15 Train Loss 3608.65 Test MSE 3520.21568809348 Test RE 0.9987891310705912 Lambda1 -5.7294892e-05\n",
      "16 Train Loss 3568.0627 Test MSE 3502.2492239329144 Test RE 0.9962370624089663 Lambda1 0.00072827464\n",
      "17 Train Loss 3530.8008 Test MSE 3480.6453165889325 Test RE 0.9931596248006813 Lambda1 0.00018725968\n",
      "18 Train Loss 3498.2976 Test MSE 3455.0071269796945 Test RE 0.9894950920386488 Lambda1 -0.001123967\n",
      "19 Train Loss 3427.8228 Test MSE 3361.488944796284 Test RE 0.9760116785356163 Lambda1 -0.00013075453\n",
      "20 Train Loss 3327.5125 Test MSE 3282.3942755718836 Test RE 0.9644607183711148 Lambda1 0.0029524812\n",
      "21 Train Loss 2782.0554 Test MSE 2758.4653824207344 Test RE 0.8841438689653153 Lambda1 0.0623771\n",
      "22 Train Loss 2150.8613 Test MSE 2138.9287713285366 Test RE 0.7785514749418683 Lambda1 0.22995086\n",
      "23 Train Loss 1320.2671 Test MSE 1217.983954505542 Test RE 0.5875028180919316 Lambda1 0.57324225\n",
      "24 Train Loss 937.0744 Test MSE 921.8399404083541 Test RE 0.5111130044632077 Lambda1 0.67507285\n",
      "25 Train Loss 856.0695 Test MSE 858.0770088349085 Test RE 0.49311964398214136 Lambda1 0.78359085\n",
      "26 Train Loss 854.8647 Test MSE 858.0943931666162 Test RE 0.493124639170193 Lambda1 0.7863101\n",
      "27 Train Loss 854.75287 Test MSE 858.084415958853 Test RE 0.4931217723403204 Lambda1 0.7866349\n",
      "28 Train Loss 854.72546 Test MSE 858.0787637844701 Test RE 0.4931201482490378 Lambda1 0.7874207\n",
      "29 Train Loss 854.71234 Test MSE 858.041081239956 Test RE 0.4931093204400225 Lambda1 0.7857648\n",
      "30 Train Loss 854.70746 Test MSE 858.0384355035957 Test RE 0.49310856019771404 Lambda1 0.7843902\n",
      "31 Train Loss 854.69855 Test MSE 858.0360858567692 Test RE 0.4931078850346277 Lambda1 0.77914226\n",
      "32 Train Loss 854.66254 Test MSE 857.9192952844013 Test RE 0.4930743244927543 Lambda1 0.7542846\n",
      "33 Train Loss 854.62134 Test MSE 857.8913181842198 Test RE 0.493066284749206 Lambda1 0.7511452\n",
      "34 Train Loss 854.56415 Test MSE 857.7848683930122 Test RE 0.493035693210259 Lambda1 0.7461814\n",
      "35 Train Loss 854.49524 Test MSE 857.636612957101 Test RE 0.49299308440300427 Lambda1 0.7408\n",
      "36 Train Loss 854.31604 Test MSE 857.2753793663061 Test RE 0.49288924997263917 Lambda1 0.7044938\n",
      "37 Train Loss 854.1615 Test MSE 857.0669061189274 Test RE 0.49282931563138405 Lambda1 0.6871085\n",
      "38 Train Loss 853.75995 Test MSE 856.1686555534881 Test RE 0.4925709925891025 Lambda1 0.63710225\n",
      "39 Train Loss 852.82404 Test MSE 854.7833037872954 Test RE 0.492172320930094 Lambda1 0.6200368\n",
      "40 Train Loss 851.2865 Test MSE 852.1507045356805 Test RE 0.49141382947972895 Lambda1 0.59162414\n",
      "41 Train Loss 848.5751 Test MSE 848.1995466084277 Test RE 0.49027323921690374 Lambda1 0.53712463\n",
      "42 Train Loss 845.57135 Test MSE 843.3978405623425 Test RE 0.4888835372346203 Lambda1 0.47516376\n",
      "43 Train Loss 842.6774 Test MSE 839.1583393090057 Test RE 0.4876532557206219 Lambda1 0.4619104\n",
      "44 Train Loss 840.782 Test MSE 837.5308081611281 Test RE 0.4871801292397432 Lambda1 0.43647426\n",
      "45 Train Loss 839.719 Test MSE 834.7190274586213 Test RE 0.48636165466806947 Lambda1 0.42123196\n",
      "46 Train Loss 838.7464 Test MSE 834.8722834860637 Test RE 0.48640630108898064 Lambda1 0.4037072\n",
      "47 Train Loss 837.2764 Test MSE 833.1242980358234 Test RE 0.4858968359551112 Lambda1 0.37991807\n",
      "48 Train Loss 836.00165 Test MSE 832.1104295005536 Test RE 0.4856010904801084 Lambda1 0.3797484\n",
      "49 Train Loss 835.0187 Test MSE 831.4429381796231 Test RE 0.48540628488075344 Lambda1 0.3683768\n",
      "50 Train Loss 833.94507 Test MSE 830.5381852436614 Test RE 0.4851421102271884 Lambda1 0.3483492\n",
      "51 Train Loss 833.0574 Test MSE 830.0584490639068 Test RE 0.48500197588698213 Lambda1 0.3280477\n",
      "52 Train Loss 832.57697 Test MSE 829.8575512070922 Test RE 0.48494328016868954 Lambda1 0.31508937\n",
      "53 Train Loss 831.7028 Test MSE 829.4683785474105 Test RE 0.48482955655581217 Lambda1 0.30282086\n",
      "54 Train Loss 830.8389 Test MSE 828.0099377672362 Test RE 0.48440313506280225 Lambda1 0.30652198\n",
      "55 Train Loss 829.98315 Test MSE 826.8385616273506 Test RE 0.48406037402171387 Lambda1 0.30630037\n",
      "56 Train Loss 829.19025 Test MSE 825.7980905783564 Test RE 0.4837557139747663 Lambda1 0.2908047\n",
      "57 Train Loss 828.4155 Test MSE 824.9303509941315 Test RE 0.4835014845702958 Lambda1 0.2684863\n",
      "58 Train Loss 827.52563 Test MSE 823.6295206435611 Test RE 0.4831201181014063 Lambda1 0.2586041\n",
      "59 Train Loss 826.453 Test MSE 823.1089602698022 Test RE 0.4829674202357667 Lambda1 0.25641653\n",
      "60 Train Loss 825.6452 Test MSE 822.489631711849 Test RE 0.4827856874293571 Lambda1 0.24793051\n",
      "61 Train Loss 824.50964 Test MSE 822.0073151554615 Test RE 0.48264411137265084 Lambda1 0.22583765\n",
      "62 Train Loss 823.84357 Test MSE 821.6535314187437 Test RE 0.4825402375928911 Lambda1 0.21839505\n",
      "63 Train Loss 823.11487 Test MSE 820.8394777626446 Test RE 0.4823011398631849 Lambda1 0.21697149\n",
      "64 Train Loss 822.91095 Test MSE 820.8375921059345 Test RE 0.48230058588464975 Lambda1 0.21497922\n",
      "65 Train Loss 822.533 Test MSE 820.7852445848117 Test RE 0.482285206689735 Lambda1 0.2147306\n",
      "66 Train Loss 821.8225 Test MSE 820.1394570631084 Test RE 0.4820954402088511 Lambda1 0.21848682\n",
      "67 Train Loss 821.50946 Test MSE 820.0254907707248 Test RE 0.48206194313816564 Lambda1 0.21015862\n",
      "68 Train Loss 821.1965 Test MSE 819.9886000482597 Test RE 0.4820510996866089 Lambda1 0.20136714\n",
      "69 Train Loss 820.9298 Test MSE 819.3613886989237 Test RE 0.48186670336728205 Lambda1 0.20740773\n",
      "70 Train Loss 820.72906 Test MSE 818.8375561415007 Test RE 0.48171264569034694 Lambda1 0.21100302\n",
      "71 Train Loss 820.29364 Test MSE 818.3197401262538 Test RE 0.48156030902220376 Lambda1 0.2029247\n",
      "72 Train Loss 819.8927 Test MSE 817.6941392517738 Test RE 0.4813761987432686 Lambda1 0.20150252\n",
      "73 Train Loss 819.60956 Test MSE 817.5063028447405 Test RE 0.4813209059604542 Lambda1 0.1987094\n",
      "74 Train Loss 819.40326 Test MSE 817.6405106054906 Test RE 0.4813604129035848 Lambda1 0.19203742\n",
      "Training time: 296.05\n",
      "Training time: 296.05\n",
      "inv_HT_atanh_tune1\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 46609.816 Test MSE 3576.6532891630204 Test RE 1.0067638017546423 Lambda1 0.0003840845\n",
      "1 Train Loss 40117.9 Test MSE 3576.994633603525 Test RE 1.0068118417797647 Lambda1 -0.0006105139\n",
      "2 Train Loss 35198.152 Test MSE 3574.5990373241393 Test RE 1.0064746426903897 Lambda1 0.00030421725\n",
      "3 Train Loss 27688.82 Test MSE 3561.9881034928494 Test RE 1.0046976882101046 Lambda1 0.00015511512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 23683.96 Test MSE 3563.5731699552616 Test RE 1.0049212060095185 Lambda1 -0.00028066512\n",
      "5 Train Loss 21012.258 Test MSE 3565.14079362166 Test RE 1.0051422152297154 Lambda1 -5.4757445e-05\n",
      "6 Train Loss 17972.318 Test MSE 3568.243866118317 Test RE 1.0055795543060633 Lambda1 0.00023593611\n",
      "7 Train Loss 15145.136 Test MSE 3573.1673110695588 Test RE 1.0062730619712492 Lambda1 -0.00034825303\n",
      "8 Train Loss 13357.151 Test MSE 3572.3773153619386 Test RE 1.0061618167825297 Lambda1 -0.00051952177\n",
      "9 Train Loss 12341.826 Test MSE 3572.4720345467986 Test RE 1.0061751555464673 Lambda1 0.0002946102\n",
      "10 Train Loss 11220.698 Test MSE 3573.3778482139132 Test RE 1.0063027072016288 Lambda1 0.00039015838\n",
      "11 Train Loss 9970.351 Test MSE 3576.838718063345 Test RE 1.0067898988725352 Lambda1 8.268234e-05\n",
      "12 Train Loss 8503.893 Test MSE 3585.8438798976504 Test RE 1.0080564651695254 Lambda1 -0.0003708537\n",
      "13 Train Loss 7853.449 Test MSE 3588.712093255346 Test RE 1.008459542265953 Lambda1 0.00018245282\n",
      "14 Train Loss 7281.6445 Test MSE 3590.1999891360006 Test RE 1.0086685764912253 Lambda1 9.867657e-05\n",
      "15 Train Loss 6763.788 Test MSE 3592.27302887462 Test RE 1.0089597452687058 Lambda1 -0.00014030706\n",
      "16 Train Loss 6382.997 Test MSE 3593.0681496840284 Test RE 1.0090714016621876 Lambda1 1.6897056e-05\n",
      "17 Train Loss 6094.8 Test MSE 3595.7317613156183 Test RE 1.0094453545445525 Lambda1 0.00012855607\n",
      "18 Train Loss 5719.785 Test MSE 3597.4134579079723 Test RE 1.009681381935574 Lambda1 6.291886e-05\n",
      "19 Train Loss 5454.0986 Test MSE 3596.7537891489706 Test RE 1.0095888035026317 Lambda1 -0.00037153863\n",
      "20 Train Loss 5156.0205 Test MSE 3595.333677601631 Test RE 1.009389475115116 Lambda1 -8.244515e-05\n",
      "21 Train Loss 4956.37 Test MSE 3593.071540245942 Test RE 1.0090718777620156 Lambda1 -1.3029924e-05\n",
      "22 Train Loss 4832.677 Test MSE 3591.83833275363 Test RE 1.0088986969918758 Lambda1 2.5623685e-05\n",
      "23 Train Loss 4692.885 Test MSE 3589.508255308392 Test RE 1.0085714003187338 Lambda1 5.5020053e-05\n",
      "24 Train Loss 4611.999 Test MSE 3589.517815813116 Test RE 1.008572743461656 Lambda1 -0.00015963534\n",
      "25 Train Loss 4517.528 Test MSE 3588.9361700369604 Test RE 1.0084910255440018 Lambda1 5.602258e-06\n",
      "26 Train Loss 4431.773 Test MSE 3588.13815888681 Test RE 1.0083788987483744 Lambda1 4.0835e-05\n",
      "27 Train Loss 4339.135 Test MSE 3588.4039007394495 Test RE 1.0084162389375637 Lambda1 -0.00028020746\n",
      "28 Train Loss 4293.5303 Test MSE 3588.067152120123 Test RE 1.0083689211393378 Lambda1 -0.00023805781\n",
      "29 Train Loss 4219.5093 Test MSE 3588.236066692944 Test RE 1.008392656230372 Lambda1 5.8020978e-05\n",
      "30 Train Loss 4163.2397 Test MSE 3587.309340173452 Test RE 1.00826243004888 Lambda1 0.0002443412\n",
      "31 Train Loss 4112.77 Test MSE 3586.97444664084 Test RE 1.0082153657430273 Lambda1 0.0002915366\n",
      "32 Train Loss 4064.2495 Test MSE 3589.123558254196 Test RE 1.0085173532438692 Lambda1 0.00031339764\n",
      "33 Train Loss 4025.6729 Test MSE 3589.529760739403 Test RE 1.0085744215863615 Lambda1 0.00021065236\n",
      "34 Train Loss 3990.7798 Test MSE 3589.5970183086934 Test RE 1.0085838704488606 Lambda1 0.00018235567\n",
      "35 Train Loss 3968.168 Test MSE 3589.82195419942 Test RE 1.0086154705360255 Lambda1 0.00023797099\n",
      "36 Train Loss 3940.356 Test MSE 3589.4469171223805 Test RE 1.0085627829538961 Lambda1 8.070859e-05\n",
      "37 Train Loss 3911.0325 Test MSE 3589.2229719886236 Test RE 1.0085313204116797 Lambda1 -0.00019366459\n",
      "38 Train Loss 3899.2488 Test MSE 3589.460665781191 Test RE 1.0085647145011003 Lambda1 -0.0002015287\n",
      "39 Train Loss 3874.2192 Test MSE 3590.0010663893177 Test RE 1.0086406323789567 Lambda1 -0.00018302283\n",
      "40 Train Loss 3851.8447 Test MSE 3589.657997548642 Test RE 1.0085924372066797 Lambda1 -0.000178887\n",
      "41 Train Loss 3836.4836 Test MSE 3589.3265906671427 Test RE 1.0085458781487249 Lambda1 -0.00013867946\n",
      "42 Train Loss 3817.643 Test MSE 3589.335853141052 Test RE 1.0085471794546412 Lambda1 -0.00016112516\n",
      "43 Train Loss 3797.6255 Test MSE 3588.9369361020513 Test RE 1.0084911331761373 Lambda1 -0.00014312725\n",
      "44 Train Loss 3782.9182 Test MSE 3588.4169413932996 Test RE 1.008418071283577 Lambda1 -9.329372e-06\n",
      "45 Train Loss 3761.716 Test MSE 3587.2625926475534 Test RE 1.00825586051122 Lambda1 0.0001997127\n",
      "46 Train Loss 3754.9194 Test MSE 3587.1955733863697 Test RE 1.0082464420651596 Lambda1 0.0002635201\n",
      "47 Train Loss 3744.9043 Test MSE 3587.010396575293 Test RE 1.00822041807697 Lambda1 0.00024546328\n",
      "48 Train Loss 3739.2234 Test MSE 3586.2356590701843 Test RE 1.0081115323662895 Lambda1 0.00018403011\n",
      "49 Train Loss 3729.3193 Test MSE 3585.22724905266 Test RE 1.0079697874646454 Lambda1 0.0002642599\n",
      "50 Train Loss 3724.2368 Test MSE 3585.0189546896318 Test RE 1.007940506548846 Lambda1 0.00033479335\n",
      "51 Train Loss 3720.4348 Test MSE 3584.705463267631 Test RE 1.0078964359853837 Lambda1 0.000358101\n",
      "52 Train Loss 3717.533 Test MSE 3584.2585354707753 Test RE 1.0078336036300606 Lambda1 0.00033698414\n",
      "53 Train Loss 3707.0159 Test MSE 3582.7278918852353 Test RE 1.0076183849516986 Lambda1 0.0003070181\n",
      "54 Train Loss 3700.0234 Test MSE 3581.7344964426184 Test RE 1.0074786823373088 Lambda1 0.0003208932\n",
      "55 Train Loss 3693.6685 Test MSE 3581.4013174863467 Test RE 1.0074318225682908 Lambda1 0.00035649384\n",
      "56 Train Loss 3688.961 Test MSE 3580.6313457649426 Test RE 1.0073235219839263 Lambda1 0.00040579133\n",
      "57 Train Loss 3684.896 Test MSE 3579.602438008933 Test RE 1.0071787825020269 Lambda1 0.0004354468\n",
      "58 Train Loss 3681.0044 Test MSE 3578.652669218417 Test RE 1.0070451572669057 Lambda1 0.0003789471\n",
      "59 Train Loss 3674.5884 Test MSE 3576.3391020489407 Test RE 1.0067195817625594 Lambda1 0.00032199814\n",
      "60 Train Loss 3668.7786 Test MSE 3574.007393414159 Test RE 1.0063913467418835 Lambda1 0.00034579152\n",
      "61 Train Loss 3663.1624 Test MSE 3570.7258606941023 Test RE 1.0059292233911907 Lambda1 0.0003257377\n",
      "62 Train Loss 3657.604 Test MSE 3569.1487201973096 Test RE 1.0057070463047089 Lambda1 0.00029568456\n",
      "63 Train Loss 3653.5168 Test MSE 3567.869354433136 Test RE 1.0055267816831934 Lambda1 0.00031241053\n",
      "64 Train Loss 3648.7234 Test MSE 3565.8593497168513 Test RE 1.0052435036104888 Lambda1 0.00032390127\n",
      "65 Train Loss 3645.4272 Test MSE 3564.519015689503 Test RE 1.0050545605568215 Lambda1 0.0003556147\n",
      "66 Train Loss 3641.4165 Test MSE 3562.7034274497255 Test RE 1.004798565617147 Lambda1 0.0003462743\n",
      "67 Train Loss 3637.087 Test MSE 3560.6341737242656 Test RE 1.004506724714883 Lambda1 0.00032990138\n",
      "68 Train Loss 3634.7278 Test MSE 3559.1398108185367 Test RE 1.0042959118306143 Lambda1 0.00033330734\n",
      "69 Train Loss 3629.5872 Test MSE 3555.022921503819 Test RE 1.0037149046708518 Lambda1 0.00029900167\n",
      "70 Train Loss 3626.291 Test MSE 3554.0630913984946 Test RE 1.0035793976937986 Lambda1 0.00026973433\n",
      "71 Train Loss 3619.3682 Test MSE 3550.6363830555215 Test RE 1.0030954722341052 Lambda1 0.00026752843\n",
      "72 Train Loss 3614.926 Test MSE 3547.0422551420666 Test RE 1.0025876525275625 Lambda1 0.00023803186\n",
      "73 Train Loss 3611.6575 Test MSE 3544.510988992256 Test RE 1.0022298517276527 Lambda1 0.00024871124\n",
      "74 Train Loss 3608.0046 Test MSE 3543.0829675382206 Test RE 1.002027940954126 Lambda1 0.00028566952\n",
      "Training time: 277.30\n",
      "Training time: 277.30\n",
      "inv_HT_atanh_tune1\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 46953.15 Test MSE 3526.0096158600777 Test RE 0.9996107473262397 Lambda1 -0.00014390456\n",
      "1 Train Loss 31707.83 Test MSE 3518.0864961796 Test RE 0.9984870280751673 Lambda1 9.1033646e-05\n",
      "2 Train Loss 26460.846 Test MSE 3514.256421631723 Test RE 0.9979433630488697 Lambda1 0.00033840956\n",
      "3 Train Loss 20034.146 Test MSE 3506.4611099642893 Test RE 0.9968359312920896 Lambda1 -0.000656955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 16766.072 Test MSE 3500.609362744248 Test RE 0.9960038006292145 Lambda1 0.00042099765\n",
      "5 Train Loss 15159.187 Test MSE 3500.926790488613 Test RE 0.9960489573491189 Lambda1 6.816397e-05\n",
      "6 Train Loss 12808.189 Test MSE 3500.5538705456056 Test RE 0.995995906195122 Lambda1 -0.00041956585\n",
      "7 Train Loss 10884.543 Test MSE 3496.774742106445 Test RE 0.9954581323132862 Lambda1 7.934516e-05\n",
      "8 Train Loss 9213.753 Test MSE 3488.3093189007627 Test RE 0.9942524382890797 Lambda1 0.00021055549\n",
      "9 Train Loss 8242.506 Test MSE 3487.6391849795023 Test RE 0.9941569315224803 Lambda1 -0.00023643236\n",
      "10 Train Loss 7528.3477 Test MSE 3485.7463608948783 Test RE 0.9938871186959771 Lambda1 3.575952e-05\n",
      "11 Train Loss 6478.77 Test MSE 3474.8381843728157 Test RE 0.9923307818228515 Lambda1 0.0001824433\n",
      "12 Train Loss 5670.6797 Test MSE 3473.6763001956465 Test RE 0.9921648647687333 Lambda1 -0.0007273588\n",
      "13 Train Loss 5156.7393 Test MSE 3472.39200589592 Test RE 0.99198143525923 Lambda1 -0.00028038849\n",
      "14 Train Loss 4756.66 Test MSE 3471.0393907216944 Test RE 0.9917882111314134 Lambda1 0.00020473637\n",
      "15 Train Loss 4527.9736 Test MSE 3471.7127843318763 Test RE 0.9918844116321888 Lambda1 -0.0003243433\n",
      "16 Train Loss 4317.1074 Test MSE 3469.673100633967 Test RE 0.9915929952863439 Lambda1 8.366484e-05\n",
      "17 Train Loss 4168.115 Test MSE 3466.7991723892064 Test RE 0.9911822422566738 Lambda1 0.00013599798\n",
      "18 Train Loss 4078.39 Test MSE 3464.0593526011453 Test RE 0.9907904979781219 Lambda1 -0.0002190337\n",
      "19 Train Loss 4002.3284 Test MSE 3461.4336319410995 Test RE 0.990414922390751 Lambda1 -2.6078458e-05\n",
      "20 Train Loss 3935.0056 Test MSE 3460.32051828292 Test RE 0.9902556627979417 Lambda1 0.00047653192\n",
      "21 Train Loss 3886.207 Test MSE 3458.7892750597953 Test RE 0.9900365371344785 Lambda1 0.00027358133\n",
      "22 Train Loss 3811.2405 Test MSE 3459.0919250362776 Test RE 0.990079851132023 Lambda1 -0.00024404854\n",
      "23 Train Loss 3761.6323 Test MSE 3457.88157148814 Test RE 0.9899066190335497 Lambda1 -6.3263874e-06\n",
      "24 Train Loss 3727.8735 Test MSE 3455.638883464021 Test RE 0.9895855536996624 Lambda1 -0.00016171073\n",
      "25 Train Loss 3690.7983 Test MSE 3449.8978938915975 Test RE 0.9887631931898865 Lambda1 7.3555035e-05\n",
      "26 Train Loss 3650.5413 Test MSE 3446.1714874900476 Test RE 0.9882290427445994 Lambda1 -9.024202e-05\n",
      "27 Train Loss 3624.3835 Test MSE 3443.998027453158 Test RE 0.9879173613459152 Lambda1 -8.2115715e-05\n",
      "28 Train Loss 3597.8333 Test MSE 3437.7211736423537 Test RE 0.9870166870722412 Lambda1 0.00014019935\n",
      "29 Train Loss 3579.2778 Test MSE 3435.5629143140773 Test RE 0.9867068055665507 Lambda1 0.00015849256\n",
      "30 Train Loss 3561.4927 Test MSE 3432.397489410378 Test RE 0.986252139745549 Lambda1 8.895069e-05\n",
      "31 Train Loss 3548.135 Test MSE 3426.291976710772 Test RE 0.9853745818365819 Lambda1 -2.9770574e-05\n",
      "32 Train Loss 3539.4053 Test MSE 3423.366290672882 Test RE 0.9849537898609074 Lambda1 2.3960114e-05\n",
      "33 Train Loss 3527.7188 Test MSE 3419.2989227032235 Test RE 0.9843684945965994 Lambda1 8.125472e-05\n",
      "34 Train Loss 3513.9197 Test MSE 3412.785628299994 Test RE 0.983430504366596 Lambda1 -2.9512443e-05\n",
      "35 Train Loss 3501.675 Test MSE 3407.1812844751826 Test RE 0.9826226969300645 Lambda1 7.63487e-05\n",
      "36 Train Loss 3487.6519 Test MSE 3397.0179092531293 Test RE 0.9811560561920937 Lambda1 0.00013088578\n",
      "37 Train Loss 3475.59 Test MSE 3390.4147506715653 Test RE 0.9802020016913198 Lambda1 -0.00017979405\n",
      "38 Train Loss 3461.296 Test MSE 3377.9117831023304 Test RE 0.9783929673496603 Lambda1 0.00018956095\n",
      "39 Train Loss 3447.9429 Test MSE 3367.3868079808994 Test RE 0.9768675285330521 Lambda1 -1.4798039e-05\n",
      "40 Train Loss 3434.5154 Test MSE 3352.868581546382 Test RE 0.9747594096281965 Lambda1 -0.00035647285\n",
      "41 Train Loss 3421.9163 Test MSE 3338.162368690789 Test RE 0.9726193357636614 Lambda1 -6.173644e-05\n",
      "42 Train Loss 3409.5469 Test MSE 3320.888191009918 Test RE 0.9700995375463465 Lambda1 -0.00025940733\n",
      "43 Train Loss 3396.4048 Test MSE 3314.7915094600935 Test RE 0.9692086456220376 Lambda1 -0.0004990231\n",
      "44 Train Loss 3378.5364 Test MSE 3295.1973098785097 Test RE 0.9663398354326462 Lambda1 5.399706e-05\n",
      "45 Train Loss 3355.6294 Test MSE 3271.6981893440084 Test RE 0.9628880290758053 Lambda1 -9.9225086e-05\n",
      "46 Train Loss 3337.121 Test MSE 3261.5997103869527 Test RE 0.9614008473621382 Lambda1 -0.0003094355\n",
      "47 Train Loss 3318.077 Test MSE 3245.7181685996525 Test RE 0.9590573409473017 Lambda1 0.0001361921\n",
      "48 Train Loss 3299.8767 Test MSE 3229.6747657786054 Test RE 0.9566841213250807 Lambda1 -7.66468e-05\n",
      "49 Train Loss 3284.4397 Test MSE 3218.7027335113453 Test RE 0.9550576883187598 Lambda1 3.170973e-06\n",
      "50 Train Loss 3267.7952 Test MSE 3204.4016393774828 Test RE 0.9529336066171096 Lambda1 0.0001822809\n",
      "51 Train Loss 3243.1846 Test MSE 3182.5709581493506 Test RE 0.9496820256805575 Lambda1 -0.00012702975\n",
      "52 Train Loss 3216.3223 Test MSE 3148.1511164805033 Test RE 0.9445326091620632 Lambda1 -3.5149067e-05\n",
      "53 Train Loss 3194.1562 Test MSE 3127.704095184186 Test RE 0.9414602757733888 Lambda1 -2.1422642e-05\n",
      "54 Train Loss 3150.8499 Test MSE 3081.4351138348507 Test RE 0.9344706901864095 Lambda1 4.5607045e-05\n",
      "55 Train Loss 3088.1606 Test MSE 3020.504837723684 Test RE 0.9251857573350958 Lambda1 -7.0281208e-06\n",
      "56 Train Loss 3051.7466 Test MSE 3002.5615215320645 Test RE 0.9224336298931151 Lambda1 4.7114336e-06\n",
      "57 Train Loss 3028.7097 Test MSE 2986.613783997282 Test RE 0.9199806718231202 Lambda1 0.00016807503\n",
      "58 Train Loss 3007.8972 Test MSE 2965.4420313188853 Test RE 0.9167140551519272 Lambda1 0.00012756427\n",
      "59 Train Loss 2989.6377 Test MSE 2948.2472553352413 Test RE 0.9140524605766149 Lambda1 8.289398e-05\n",
      "60 Train Loss 2940.4817 Test MSE 2881.6118922235864 Test RE 0.9036638610929894 Lambda1 -0.00013910088\n",
      "61 Train Loss 2909.7808 Test MSE 2861.263197402399 Test RE 0.900467567032583 Lambda1 8.918855e-05\n",
      "62 Train Loss 2792.7693 Test MSE 2755.3368604056986 Test RE 0.8836423494780503 Lambda1 -0.00019767808\n",
      "63 Train Loss 2738.879 Test MSE 2703.713462193312 Test RE 0.8753253424564319 Lambda1 0.00024788335\n",
      "64 Train Loss 2675.4983 Test MSE 2644.4169375365072 Test RE 0.8656735251474197 Lambda1 -9.3781455e-05\n",
      "65 Train Loss 2609.656 Test MSE 2548.9608166675625 Test RE 0.8499057165246455 Lambda1 0.000291286\n",
      "66 Train Loss 2517.701 Test MSE 2484.4107019409944 Test RE 0.8390751636602903 Lambda1 -0.0011936561\n",
      "67 Train Loss 2420.717 Test MSE 2383.6644110209895 Test RE 0.8218862726398626 Lambda1 -0.0021825966\n",
      "68 Train Loss 2355.8225 Test MSE 2309.6166145150514 Test RE 0.8090197393788959 Lambda1 -0.0025684633\n",
      "69 Train Loss 2245.965 Test MSE 2178.964604564826 Test RE 0.7858040420281817 Lambda1 -0.0039994786\n",
      "70 Train Loss 2079.4944 Test MSE 1959.7626225777249 Test RE 0.7452309888478109 Lambda1 -0.0055694943\n",
      "71 Train Loss 1328.6444 Test MSE 1288.971771483867 Test RE 0.6043810986921677 Lambda1 0.00044248917\n",
      "72 Train Loss 855.357 Test MSE 858.2422073265583 Test RE 0.4931671098273368 Lambda1 0.010627727\n",
      "73 Train Loss 854.2367 Test MSE 857.3343942383419 Test RE 0.4929062149386997 Lambda1 0.011111915\n",
      "74 Train Loss 848.9344 Test MSE 850.8025122583466 Test RE 0.49102494135342406 Lambda1 0.012798711\n",
      "Training time: 236.44\n",
      "Training time: 236.44\n",
      "inv_HT_atanh_tune1\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 17253.982 Test MSE 3607.8700243151366 Test RE 1.0111477326358684 Lambda1 -0.00043149598\n",
      "1 Train Loss 11983.7 Test MSE 3606.724587480386 Test RE 1.0109872088652294 Lambda1 -0.00010748637\n",
      "2 Train Loss 8922.113 Test MSE 3608.382034555046 Test RE 1.01121947851682 Lambda1 1.0881195e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Train Loss 5982.95 Test MSE 3624.0799019167835 Test RE 1.013416690735246 Lambda1 0.00017986452\n",
      "4 Train Loss 4855.8677 Test MSE 3623.8444179719245 Test RE 1.0133837655179052 Lambda1 -0.0003002441\n",
      "5 Train Loss 4386.6206 Test MSE 3626.5882248488815 Test RE 1.0137673365134259 Lambda1 -0.00013206586\n",
      "6 Train Loss 3985.066 Test MSE 3614.9259191705473 Test RE 1.0121359981763156 Lambda1 -1.8867766e-05\n",
      "7 Train Loss 3855.034 Test MSE 3609.7771777223393 Test RE 1.0114149483411847 Lambda1 -0.0001462088\n",
      "8 Train Loss 3720.7231 Test MSE 3596.779083347082 Test RE 1.0095923534668225 Lambda1 0.00023980431\n",
      "9 Train Loss 3684.8013 Test MSE 3588.7536722757786 Test RE 1.0084653842835585 Lambda1 -8.106087e-06\n",
      "10 Train Loss 3637.5403 Test MSE 3567.3426835896144 Test RE 1.0054525635531744 Lambda1 -3.6170022e-05\n",
      "11 Train Loss 3583.177 Test MSE 3526.1450907907165 Test RE 0.9996299504638734 Lambda1 3.8445287e-05\n",
      "12 Train Loss 3519.9758 Test MSE 3462.581675255397 Test RE 0.990579152748308 Lambda1 -3.3790027e-05\n",
      "13 Train Loss 3439.3394 Test MSE 3385.066826104048 Test RE 0.9794286279981522 Lambda1 0.00064292096\n",
      "14 Train Loss 3306.1248 Test MSE 3249.841526191536 Test RE 0.9596663404043921 Lambda1 0.0012311154\n",
      "15 Train Loss 2962.5554 Test MSE 2916.70979562252 Test RE 0.9091504975278194 Lambda1 -0.021794675\n",
      "16 Train Loss 1226.5751 Test MSE 1179.9743402119273 Test RE 0.5782630629286235 Lambda1 -0.3288806\n",
      "17 Train Loss 856.2981 Test MSE 858.446278404904 Test RE 0.49322573847621815 Lambda1 -0.46637985\n",
      "18 Train Loss 854.8656 Test MSE 858.037638324853 Test RE 0.4931083311311961 Lambda1 -0.472216\n",
      "19 Train Loss 854.64984 Test MSE 857.9441473684807 Test RE 0.4930814660944736 Lambda1 -0.4721719\n",
      "20 Train Loss 854.47046 Test MSE 857.5596373579247 Test RE 0.4929709600620977 Lambda1 -0.47050172\n",
      "21 Train Loss 853.9046 Test MSE 856.3700179362215 Test RE 0.49262891309163864 Lambda1 -0.445613\n",
      "22 Train Loss 852.2829 Test MSE 853.7163727172177 Test RE 0.4918650629961807 Lambda1 -0.4227152\n",
      "23 Train Loss 849.2549 Test MSE 848.9941589215039 Test RE 0.4905028349268849 Lambda1 -0.39564204\n",
      "24 Train Loss 842.49603 Test MSE 840.6549619242903 Test RE 0.48808792206183244 Lambda1 -0.3513322\n",
      "25 Train Loss 829.3004 Test MSE 824.8266511190044 Test RE 0.48347109375043756 Lambda1 -0.3280283\n",
      "26 Train Loss 823.84314 Test MSE 819.3112719962438 Test RE 0.4818519663172276 Lambda1 -0.32288828\n",
      "27 Train Loss 819.6114 Test MSE 812.7322881143706 Test RE 0.4799134565019874 Lambda1 -0.3131138\n",
      "28 Train Loss 816.11993 Test MSE 810.2577894443548 Test RE 0.47918231137237294 Lambda1 -0.30272317\n",
      "29 Train Loss 811.41437 Test MSE 806.0351297462638 Test RE 0.4779320506395068 Lambda1 -0.28728202\n",
      "30 Train Loss 807.0491 Test MSE 799.5127826097186 Test RE 0.47599443628603216 Lambda1 -0.2781515\n",
      "31 Train Loss 797.2481 Test MSE 783.9044405534991 Test RE 0.4713252786151232 Lambda1 -0.2783197\n",
      "32 Train Loss 784.9393 Test MSE 774.5298735112975 Test RE 0.468498556573123 Lambda1 -0.2741943\n",
      "33 Train Loss 780.2377 Test MSE 771.9721202378639 Test RE 0.46772434844174143 Lambda1 -0.28049654\n",
      "34 Train Loss 775.4887 Test MSE 764.3346676716864 Test RE 0.4654048981704623 Lambda1 -0.30091172\n",
      "35 Train Loss 769.21826 Test MSE 762.8695181635411 Test RE 0.46495861797611415 Lambda1 -0.3048105\n",
      "36 Train Loss 765.01074 Test MSE 758.642659722851 Test RE 0.4636687223242893 Lambda1 -0.30387446\n",
      "37 Train Loss 758.3362 Test MSE 752.2871265691267 Test RE 0.4617224438453425 Lambda1 -0.3165009\n",
      "38 Train Loss 753.18097 Test MSE 744.9506696565845 Test RE 0.45946552233091537 Lambda1 -0.31778887\n",
      "39 Train Loss 748.0175 Test MSE 741.2984327675995 Test RE 0.458337837764612 Lambda1 -0.32184047\n",
      "40 Train Loss 743.284 Test MSE 740.1374924928228 Test RE 0.45797879791955404 Lambda1 -0.32500413\n",
      "41 Train Loss 737.30566 Test MSE 732.3715840297892 Test RE 0.4555697859395895 Lambda1 -0.33013877\n",
      "42 Train Loss 732.04034 Test MSE 728.9062992560981 Test RE 0.4544907224388839 Lambda1 -0.34129953\n",
      "43 Train Loss 726.5722 Test MSE 723.8518441238471 Test RE 0.45291219361540347 Lambda1 -0.3452257\n",
      "44 Train Loss 718.2668 Test MSE 713.3720547383582 Test RE 0.4496216519163765 Lambda1 -0.35737884\n",
      "45 Train Loss 715.74274 Test MSE 709.2629043000636 Test RE 0.4483248312325847 Lambda1 -0.36022443\n",
      "46 Train Loss 713.2697 Test MSE 708.4303156246725 Test RE 0.44806161445997106 Lambda1 -0.3580506\n",
      "47 Train Loss 711.3734 Test MSE 707.0414787816176 Test RE 0.44762219949428267 Lambda1 -0.3629908\n",
      "48 Train Loss 710.13007 Test MSE 705.9669666141402 Test RE 0.44728193771937613 Lambda1 -0.3665552\n",
      "49 Train Loss 706.9394 Test MSE 702.180113738984 Test RE 0.4460806999237308 Lambda1 -0.3774874\n",
      "50 Train Loss 704.4494 Test MSE 699.8868690115379 Test RE 0.44535167841695245 Lambda1 -0.37824732\n",
      "51 Train Loss 702.49054 Test MSE 698.0265925161433 Test RE 0.4447594194780539 Lambda1 -0.3810584\n",
      "52 Train Loss 699.14935 Test MSE 699.4482229186546 Test RE 0.4452120970070378 Lambda1 -0.3858049\n",
      "53 Train Loss 695.0509 Test MSE 697.3459911765673 Test RE 0.444542538285599 Lambda1 -0.39418027\n",
      "54 Train Loss 694.01605 Test MSE 695.6760519799838 Test RE 0.4440099447204133 Lambda1 -0.40007687\n",
      "55 Train Loss 691.8956 Test MSE 692.151808973983 Test RE 0.4428838561032659 Lambda1 -0.40896532\n",
      "56 Train Loss 689.5029 Test MSE 690.2756766147947 Test RE 0.44228321290012423 Lambda1 -0.41857418\n",
      "57 Train Loss 687.42365 Test MSE 687.3249962626461 Test RE 0.44133689966631623 Lambda1 -0.43021622\n",
      "58 Train Loss 686.5169 Test MSE 686.359157685227 Test RE 0.44102670430193874 Lambda1 -0.429774\n",
      "59 Train Loss 685.65826 Test MSE 686.6554838146478 Test RE 0.44112189763634657 Lambda1 -0.4254379\n",
      "60 Train Loss 684.60455 Test MSE 686.4512396866863 Test RE 0.44105628739934827 Lambda1 -0.42812556\n",
      "61 Train Loss 683.18036 Test MSE 685.7473615376571 Test RE 0.4408301027201733 Lambda1 -0.4353941\n",
      "62 Train Loss 682.18256 Test MSE 685.4157583902355 Test RE 0.44072350491611234 Lambda1 -0.43820333\n",
      "63 Train Loss 679.81146 Test MSE 683.5665914398415 Test RE 0.4401285946406563 Lambda1 -0.44764265\n",
      "64 Train Loss 679.1501 Test MSE 682.5642303807701 Test RE 0.4398057807281717 Lambda1 -0.45541155\n",
      "65 Train Loss 678.16754 Test MSE 681.952062882423 Test RE 0.43960851343344826 Lambda1 -0.45888644\n",
      "66 Train Loss 676.70447 Test MSE 680.8497596954426 Test RE 0.43925307947482717 Lambda1 -0.45675093\n",
      "67 Train Loss 674.7497 Test MSE 678.3919143609874 Test RE 0.43845951802898797 Lambda1 -0.46678403\n",
      "68 Train Loss 673.62366 Test MSE 676.7868566552972 Test RE 0.43794051892865415 Lambda1 -0.4796803\n",
      "69 Train Loss 672.4164 Test MSE 675.2726771647665 Test RE 0.43745034106708275 Lambda1 -0.48691082\n",
      "70 Train Loss 671.6299 Test MSE 675.1642339098682 Test RE 0.43741521418785384 Lambda1 -0.48649612\n",
      "71 Train Loss 670.94226 Test MSE 675.4146599886329 Test RE 0.4374963278012826 Lambda1 -0.48424596\n",
      "72 Train Loss 670.30853 Test MSE 674.7451719315426 Test RE 0.4372794453422933 Lambda1 -0.48678115\n",
      "73 Train Loss 669.84705 Test MSE 674.1689013302055 Test RE 0.4370926747464436 Lambda1 -0.490501\n",
      "74 Train Loss 668.8542 Test MSE 673.179322480162 Test RE 0.4367717636904074 Lambda1 -0.49600825\n",
      "Training time: 247.70\n",
      "Training time: 247.70\n",
      "inv_HT_atanh_tune1\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 29517.84 Test MSE 3397.858452489601 Test RE 0.9812774351632978 Lambda1 -1.585285e-05\n",
      "1 Train Loss 19152.541 Test MSE 3387.3898842679555 Test RE 0.9797646449160768 Lambda1 -0.00015146393\n",
      "2 Train Loss 13281.166 Test MSE 3361.857153988098 Test RE 0.9760651320254976 Lambda1 0.00023129661\n",
      "3 Train Loss 7906.862 Test MSE 3357.0439521797516 Test RE 0.9753661610416066 Lambda1 -0.00043275612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 4876.8594 Test MSE 3374.0897771843074 Test RE 0.9778392995347742 Lambda1 0.0004976316\n",
      "5 Train Loss 3906.962 Test MSE 3364.6773108597204 Test RE 0.9764744414675328 Lambda1 -0.00049005623\n",
      "6 Train Loss 3534.4668 Test MSE 3367.929737919574 Test RE 0.9769462764288493 Lambda1 1.2980429e-05\n",
      "7 Train Loss 3398.9285 Test MSE 3341.3737039064413 Test RE 0.9730870565567025 Lambda1 -0.0004996588\n",
      "8 Train Loss 3327.3162 Test MSE 3296.4637508766295 Test RE 0.9665255139681742 Lambda1 -0.00040177122\n",
      "9 Train Loss 2917.9546 Test MSE 2903.4701154052636 Test RE 0.9070847190688447 Lambda1 -0.02197645\n",
      "10 Train Loss 867.5567 Test MSE 866.8207177702647 Test RE 0.49562569313613697 Lambda1 -0.4081109\n",
      "11 Train Loss 854.72675 Test MSE 858.0563371798385 Test RE 0.4931137041536194 Lambda1 -0.4228651\n",
      "12 Train Loss 854.72205 Test MSE 858.0679870834446 Test RE 0.49311705166553227 Lambda1 -0.423112\n",
      "13 Train Loss 854.7218 Test MSE 858.0692309032526 Test RE 0.49311740906642804 Lambda1 -0.42313346\n",
      "14 Train Loss 854.7214 Test MSE 858.0718632010646 Test RE 0.4931181654336504 Lambda1 -0.42317474\n",
      "15 Train Loss 854.7213 Test MSE 858.0726691449833 Test RE 0.4931183970141917 Lambda1 -0.4231847\n",
      "16 Train Loss 854.72125 Test MSE 858.0733331774675 Test RE 0.49311858781771134 Lambda1 -0.42319176\n",
      "17 Train Loss 854.7211 Test MSE 858.0739991493351 Test RE 0.4931187791784206 Lambda1 -0.4231963\n",
      "18 Train Loss 854.721 Test MSE 858.0746390796693 Test RE 0.49311896305627173 Lambda1 -0.42319763\n",
      "19 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "20 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "21 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "22 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "23 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "24 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "25 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "26 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "27 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "28 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "29 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "30 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "31 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "32 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "33 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "34 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "35 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "36 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "37 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "38 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "39 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "40 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "41 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "42 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "43 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "44 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "45 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "46 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "47 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "48 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "49 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "50 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "51 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "52 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "53 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "54 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "55 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "56 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "57 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "58 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "59 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "60 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "61 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "62 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "63 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "64 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "65 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "66 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "67 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "68 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "69 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "70 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "71 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "72 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "73 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "74 Train Loss 854.721 Test MSE 858.0748341989586 Test RE 0.49311901912191675 Lambda1 -0.4231973\n",
      "Training time: 238.54\n",
      "Training time: 238.54\n",
      "inv_HT_atanh_tune1\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 24952.504 Test MSE 3673.4845167544386 Test RE 1.0203009181767029 Lambda1 -0.00019151322\n",
      "1 Train Loss 16273.774 Test MSE 3682.7122464668973 Test RE 1.021581603565766 Lambda1 0.00018922672\n",
      "2 Train Loss 11348.261 Test MSE 3688.319721777793 Test RE 1.0223590624432493 Lambda1 -0.0011958987\n",
      "3 Train Loss 7870.6284 Test MSE 3685.0329118694253 Test RE 1.0219034282916302 Lambda1 0.0011707467\n",
      "4 Train Loss 5924.9805 Test MSE 3685.379818425636 Test RE 1.0219515278153988 Lambda1 -0.0010376945\n",
      "5 Train Loss 4826.4697 Test MSE 3679.791931388934 Test RE 1.0211764766338987 Lambda1 0.00049742823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 4285.9634 Test MSE 3670.0135502882003 Test RE 1.0198187782357506 Lambda1 -0.0006124902\n",
      "7 Train Loss 3972.097 Test MSE 3663.7561666983142 Test RE 1.0189490103729613 Lambda1 -3.6957936e-05\n",
      "8 Train Loss 3841.8079 Test MSE 3654.9730660756154 Test RE 1.0177269173373613 Lambda1 0.00048770898\n",
      "9 Train Loss 3760.6348 Test MSE 3638.0717324490934 Test RE 1.015371103337268 Lambda1 -0.0004511309\n",
      "10 Train Loss 3706.0142 Test MSE 3600.4204941052826 Test RE 1.0101032842681055 Lambda1 -0.00033980692\n",
      "11 Train Loss 3628.3237 Test MSE 3545.550590129161 Test RE 1.0023768174034953 Lambda1 0.0014794144\n",
      "12 Train Loss 3535.772 Test MSE 3477.150015181803 Test RE 0.9926608287645378 Lambda1 -0.000499824\n",
      "13 Train Loss 3423.8298 Test MSE 3364.4987850779826 Test RE 0.9764485358368669 Lambda1 -0.00060869067\n",
      "14 Train Loss 3208.3179 Test MSE 3165.9016959065107 Test RE 0.9471916991262318 Lambda1 0.0040661525\n",
      "15 Train Loss 2486.0134 Test MSE 2470.3161174837696 Test RE 0.8366916533703201 Lambda1 -0.013938085\n",
      "16 Train Loss 868.52966 Test MSE 870.2169460734285 Test RE 0.4965956817572954 Lambda1 -0.08289057\n",
      "17 Train Loss 854.76416 Test MSE 858.0770806491315 Test RE 0.4931196646172375 Lambda1 -0.09028903\n",
      "18 Train Loss 854.7199 Test MSE 858.0769516994927 Test RE 0.4931196275648524 Lambda1 -0.09079047\n",
      "19 Train Loss 854.60785 Test MSE 857.8889557145206 Test RE 0.49306560584330344 Lambda1 -0.091011606\n",
      "20 Train Loss 854.3668 Test MSE 857.4205816473527 Test RE 0.4929309901268926 Lambda1 -0.07584656\n",
      "21 Train Loss 853.2016 Test MSE 855.2500069598661 Test RE 0.4923066631884833 Lambda1 -0.010382071\n",
      "22 Train Loss 850.2612 Test MSE 850.4333284982124 Test RE 0.4909183960029479 Lambda1 0.014325352\n",
      "23 Train Loss 842.6023 Test MSE 841.1712792094261 Test RE 0.48823778708329024 Lambda1 0.018369606\n",
      "24 Train Loss 823.13763 Test MSE 807.1285012802795 Test RE 0.47825609371801553 Lambda1 0.0063783564\n",
      "25 Train Loss 800.4542 Test MSE 789.1651532726194 Test RE 0.47290414507056394 Lambda1 0.0069582686\n",
      "26 Train Loss 784.4871 Test MSE 779.7764149098774 Test RE 0.47008264562842117 Lambda1 0.0019192384\n",
      "27 Train Loss 776.7204 Test MSE 773.7545394685014 Test RE 0.46826400536801654 Lambda1 0.0008768461\n",
      "28 Train Loss 772.758 Test MSE 769.4834723548014 Test RE 0.46696982583172203 Lambda1 0.00075855525\n",
      "29 Train Loss 769.5736 Test MSE 767.0737763233332 Test RE 0.46623807673356177 Lambda1 0.00048244765\n",
      "30 Train Loss 762.36615 Test MSE 759.0156695378344 Test RE 0.4637826967547627 Lambda1 0.00010495391\n",
      "31 Train Loss 754.63367 Test MSE 748.6370369947502 Test RE 0.4606009454888928 Lambda1 4.2564723e-05\n",
      "32 Train Loss 749.8451 Test MSE 743.826759979723 Test RE 0.4591187928845166 Lambda1 -4.2129206e-05\n",
      "33 Train Loss 747.1701 Test MSE 740.8057431061684 Test RE 0.4581854997600761 Lambda1 -0.00022913914\n",
      "34 Train Loss 745.0528 Test MSE 736.5306803619253 Test RE 0.4568615319752525 Lambda1 -0.00040375534\n",
      "35 Train Loss 743.07227 Test MSE 734.4784143114756 Test RE 0.4562245894278794 Lambda1 -0.000165573\n",
      "36 Train Loss 740.3213 Test MSE 732.0368707622786 Test RE 0.4554656702957064 Lambda1 -0.00014204325\n",
      "37 Train Loss 739.6926 Test MSE 731.7057682084958 Test RE 0.4553626543760409 Lambda1 -0.00011220731\n",
      "38 Train Loss 738.36426 Test MSE 730.5264412946225 Test RE 0.4549955409693434 Lambda1 -6.505659e-05\n",
      "39 Train Loss 734.6648 Test MSE 726.3234058225581 Test RE 0.45368475952865933 Lambda1 -2.3485673e-06\n",
      "40 Train Loss 732.93774 Test MSE 724.9681524409167 Test RE 0.4532612946611518 Lambda1 2.5513102e-05\n",
      "41 Train Loss 729.0689 Test MSE 721.2065036243256 Test RE 0.45208384482180847 Lambda1 1.5232682e-05\n",
      "42 Train Loss 728.2493 Test MSE 720.8534812441367 Test RE 0.4519731863287174 Lambda1 9.589326e-06\n",
      "43 Train Loss 727.4707 Test MSE 720.2486035061694 Test RE 0.4517835181778776 Lambda1 7.3776923e-06\n",
      "44 Train Loss 726.33405 Test MSE 718.9264748924979 Test RE 0.4513686681648649 Lambda1 -3.8622256e-06\n",
      "45 Train Loss 725.4536 Test MSE 718.1944921047505 Test RE 0.45113882669359506 Lambda1 3.0814854e-06\n",
      "46 Train Loss 724.0872 Test MSE 717.2422257886467 Test RE 0.45083964115772923 Lambda1 1.2385647e-05\n",
      "47 Train Loss 723.43646 Test MSE 716.7233210522262 Test RE 0.4506765267001309 Lambda1 2.7904878e-06\n",
      "48 Train Loss 723.2865 Test MSE 716.4897232263226 Test RE 0.45060307741138406 Lambda1 -5.6567596e-06\n",
      "49 Train Loss 723.15436 Test MSE 716.2429869602835 Test RE 0.45052548404852455 Lambda1 -1.3300969e-05\n",
      "50 Train Loss 722.5538 Test MSE 715.6295866385744 Test RE 0.4503325246232675 Lambda1 -3.865548e-06\n",
      "51 Train Loss 721.102 Test MSE 713.993259106169 Test RE 0.44981737457856963 Lambda1 1.2538121e-05\n",
      "52 Train Loss 720.30066 Test MSE 712.8339441832783 Test RE 0.4494520407026469 Lambda1 1.5097773e-05\n",
      "53 Train Loss 719.3403 Test MSE 711.1659123121343 Test RE 0.44892587368769454 Lambda1 -1.6623288e-05\n",
      "54 Train Loss 718.22266 Test MSE 709.726072416316 Test RE 0.4484711915434859 Lambda1 -0.0001153775\n",
      "55 Train Loss 717.14905 Test MSE 709.2937265732628 Test RE 0.44833457250075487 Lambda1 -0.00022196182\n",
      "56 Train Loss 716.49567 Test MSE 709.7052903645632 Test RE 0.44846462547517935 Lambda1 -0.00017448326\n",
      "57 Train Loss 716.0927 Test MSE 709.7520634937305 Test RE 0.4484794032629055 Lambda1 -0.00012549473\n",
      "58 Train Loss 715.81433 Test MSE 709.3466420814648 Test RE 0.44835129576250654 Lambda1 -0.00014816546\n",
      "59 Train Loss 715.22675 Test MSE 709.0909644333768 Test RE 0.4482704863743141 Lambda1 -0.00016935509\n",
      "60 Train Loss 714.8284 Test MSE 709.1658451630124 Test RE 0.4482941546599442 Lambda1 -0.0001607067\n",
      "61 Train Loss 714.4842 Test MSE 709.2752228507052 Test RE 0.44832872449143435 Lambda1 -0.000113763315\n",
      "62 Train Loss 714.1611 Test MSE 708.8462399610174 Test RE 0.4481931251959471 Lambda1 -0.0001018437\n",
      "63 Train Loss 713.8503 Test MSE 707.9506924914119 Test RE 0.44790991492358506 Lambda1 -0.00015108549\n",
      "64 Train Loss 713.50336 Test MSE 707.4017130725467 Test RE 0.4477362156707195 Lambda1 -0.000213003\n",
      "65 Train Loss 713.15784 Test MSE 706.717059999769 Test RE 0.4475194942678494 Lambda1 -0.00023175038\n",
      "66 Train Loss 712.8235 Test MSE 706.6120488358902 Test RE 0.44748624454779 Lambda1 -0.00020147028\n",
      "67 Train Loss 712.47754 Test MSE 706.4183907902388 Test RE 0.44742492005571877 Lambda1 -0.00017055492\n",
      "68 Train Loss 711.9399 Test MSE 705.8093097439989 Test RE 0.4472319913259028 Lambda1 -0.00013529128\n",
      "69 Train Loss 711.2639 Test MSE 704.9819445288742 Test RE 0.44696978686866906 Lambda1 -9.774768e-05\n",
      "70 Train Loss 710.4581 Test MSE 703.5887127220705 Test RE 0.44652790285713756 Lambda1 -5.4931817e-05\n",
      "71 Train Loss 709.6659 Test MSE 702.6362795998936 Test RE 0.44622557283088293 Lambda1 -2.144934e-05\n",
      "72 Train Loss 708.8601 Test MSE 701.8734431047328 Test RE 0.44598327848706937 Lambda1 -8.720038e-06\n",
      "73 Train Loss 708.3891 Test MSE 701.2909831770643 Test RE 0.44579818721029063 Lambda1 -3.1563277e-06\n",
      "74 Train Loss 707.9199 Test MSE 700.5298226632299 Test RE 0.44555619343848907 Lambda1 -6.1249702e-06\n",
      "Training time: 293.16\n",
      "Training time: 293.16\n",
      "inv_HT_atanh_tune2\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.43\n",
      "Training time: 6.43\n",
      "inv_HT_atanh_tune2\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.80\n",
      "Training time: 6.80\n",
      "inv_HT_atanh_tune2\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 707379.2 Test MSE 3558.377005137906 Test RE 1.004188284184752 Lambda1 7.8350797e-07\n",
      "1 Train Loss 667158.25 Test MSE 3558.385424442093 Test RE 1.0041894721647793 Lambda1 3.109231e-06\n",
      "2 Train Loss 647393.7 Test MSE 3558.3508974783167 Test RE 1.004184600334792 Lambda1 1.1679239e-06\n",
      "3 Train Loss 636127.1 Test MSE 3558.3131214663244 Test RE 1.004179270029929 Lambda1 1.8511438e-06\n",
      "4 Train Loss 629210.5 Test MSE 3558.311653042811 Test RE 1.004179062830541 Lambda1 1.6857177e-05\n",
      "5 Train Loss 620130.8 Test MSE 3558.3607293483124 Test RE 1.0041859876355257 Lambda1 4.4708657e-05\n",
      "6 Train Loss 614052.75 Test MSE 3558.461704101114 Test RE 1.0042002353074537 Lambda1 5.082618e-05\n",
      "7 Train Loss 611093.9 Test MSE 3558.541501401586 Test RE 1.0042114946696927 Lambda1 4.4244724e-05\n",
      "8 Train Loss 606192.6 Test MSE 3558.6790819195176 Test RE 1.00423090692077 Lambda1 2.7300492e-05\n",
      "9 Train Loss 598072.1 Test MSE 3558.89485121869 Test RE 1.0042613506485751 Lambda1 2.8304185e-05\n",
      "10 Train Loss 590569.94 Test MSE 3559.0833579019927 Test RE 1.00428794703278 Lambda1 2.6820246e-05\n",
      "11 Train Loss 584189.7 Test MSE 3559.2606135757 Test RE 1.0043129553709305 Lambda1 3.0956806e-05\n",
      "12 Train Loss 579277.8 Test MSE 3559.3257264412337 Test RE 1.0043221417445123 Lambda1 4.0428673e-05\n",
      "13 Train Loss 572926.0 Test MSE 3559.388016791971 Test RE 1.0043309298282062 Lambda1 3.659233e-05\n",
      "14 Train Loss 569451.94 Test MSE 3559.42923403845 Test RE 1.0043367448216052 Lambda1 3.3122244e-05\n",
      "15 Train Loss 563808.94 Test MSE 3559.4033374941146 Test RE 1.004333091300684 Lambda1 1.8811934e-05\n",
      "16 Train Loss 560326.9 Test MSE 3559.367883331537 Test RE 1.0043280893559765 Lambda1 1.8329802e-05\n",
      "17 Train Loss 557299.5 Test MSE 3559.3728823728943 Test RE 1.0043287946323147 Lambda1 2.6034553e-05\n",
      "18 Train Loss 554972.5 Test MSE 3559.4214625075942 Test RE 1.00433564840449 Lambda1 3.660498e-05\n",
      "19 Train Loss 550695.56 Test MSE 3559.485623413392 Test RE 1.004344700268005 Lambda1 5.167049e-05\n",
      "20 Train Loss 547047.75 Test MSE 3559.4878844799155 Test RE 1.0043450192592946 Lambda1 4.39812e-05\n",
      "21 Train Loss 541892.1 Test MSE 3559.5361371862264 Test RE 1.0043518267274731 Lambda1 1.9944702e-05\n",
      "22 Train Loss 538445.06 Test MSE 3559.5595627699604 Test RE 1.004355131580718 Lambda1 2.3419134e-05\n",
      "23 Train Loss 535508.25 Test MSE 3559.6104752067895 Test RE 1.0043623142087155 Lambda1 2.5611076e-05\n",
      "24 Train Loss 531406.0 Test MSE 3559.6532668862283 Test RE 1.0043683511362562 Lambda1 3.0781895e-05\n",
      "25 Train Loss 527013.7 Test MSE 3559.685296837061 Test RE 1.004372869806212 Lambda1 4.543436e-05\n",
      "26 Train Loss 523636.97 Test MSE 3559.734492098365 Test RE 1.0043798100566756 Lambda1 5.1302795e-05\n",
      "27 Train Loss 520590.62 Test MSE 3559.8098725698933 Test RE 1.0043904443080556 Lambda1 4.5331024e-05\n",
      "28 Train Loss 518006.44 Test MSE 3559.861738227874 Test RE 1.0043977611569017 Lambda1 4.9363676e-05\n",
      "29 Train Loss 514653.0 Test MSE 3559.9238892701937 Test RE 1.0044065289265356 Lambda1 5.335828e-05\n",
      "30 Train Loss 509805.8 Test MSE 3560.1748930772105 Test RE 1.0044419377479876 Lambda1 3.1940897e-05\n",
      "31 Train Loss 503643.62 Test MSE 3560.4581404081555 Test RE 1.0044818936276725 Lambda1 2.1019878e-06\n",
      "32 Train Loss 500230.03 Test MSE 3560.436462780398 Test RE 1.004478835760277 Lambda1 -6.574197e-06\n",
      "33 Train Loss 495545.5 Test MSE 3560.6357275646455 Test RE 1.0045069438953522 Lambda1 -1.8658202e-05\n",
      "34 Train Loss 492625.34 Test MSE 3560.7315543295636 Test RE 1.0045204608640201 Lambda1 -9.852304e-06\n",
      "35 Train Loss 490551.1 Test MSE 3560.818152252973 Test RE 1.004532675890021 Lambda1 6.9107755e-06\n",
      "36 Train Loss 487904.47 Test MSE 3561.041913168527 Test RE 1.0045642376843278 Lambda1 1.8892035e-05\n",
      "37 Train Loss 482703.56 Test MSE 3561.1551006265363 Test RE 1.0045802025580555 Lambda1 1.3346052e-05\n",
      "38 Train Loss 476305.25 Test MSE 3561.52537546671 Test RE 1.0046324273411558 Lambda1 -1.0744296e-05\n",
      "39 Train Loss 470757.06 Test MSE 3561.7018454934573 Test RE 1.0046573162997596 Lambda1 1.0467518e-05\n",
      "40 Train Loss 465507.88 Test MSE 3561.9341654537857 Test RE 1.0046900812744928 Lambda1 3.308719e-05\n",
      "41 Train Loss 461452.25 Test MSE 3562.1241154010327 Test RE 1.0047168698498654 Lambda1 3.967321e-05\n",
      "42 Train Loss 458462.84 Test MSE 3562.2244844982147 Test RE 1.0047310245805177 Lambda1 4.494319e-05\n",
      "43 Train Loss 456433.94 Test MSE 3562.2325999164423 Test RE 1.0047321690631104 Lambda1 3.8909377e-05\n",
      "44 Train Loss 453679.06 Test MSE 3562.184815862523 Test RE 1.0047254302643656 Lambda1 3.518789e-05\n",
      "45 Train Loss 451214.75 Test MSE 3562.2364350707257 Test RE 1.0047327099180008 Lambda1 3.6522248e-05\n",
      "46 Train Loss 448023.56 Test MSE 3562.204592888955 Test RE 1.004728219346746 Lambda1 3.780927e-05\n",
      "47 Train Loss 444495.25 Test MSE 3562.168909064982 Test RE 1.004723186980725 Lambda1 4.6977708e-05\n",
      "48 Train Loss 442002.94 Test MSE 3562.0930323374682 Test RE 1.0047124862650372 Lambda1 5.1449493e-05\n",
      "49 Train Loss 439331.5 Test MSE 3562.1583243615783 Test RE 1.0047216942518415 Lambda1 4.004646e-05\n",
      "50 Train Loss 436591.0 Test MSE 3562.3196279264457 Test RE 1.0047444421677938 Lambda1 6.546449e-07\n",
      "51 Train Loss 434059.75 Test MSE 3562.3098814218492 Test RE 1.0047430676767255 Lambda1 -2.650555e-05\n",
      "52 Train Loss 432374.9 Test MSE 3562.214527082035 Test RE 1.0047296203270808 Lambda1 -1.2322922e-05\n",
      "53 Train Loss 430780.16 Test MSE 3562.2424916569753 Test RE 1.004733564051049 Lambda1 -3.5652895e-06\n",
      "54 Train Loss 428909.4 Test MSE 3562.310590113461 Test RE 1.004743167619315 Lambda1 -8.73769e-06\n",
      "55 Train Loss 426741.2 Test MSE 3562.3292394618884 Test RE 1.0047457976231307 Lambda1 -3.178705e-06\n",
      "56 Train Loss 424804.8 Test MSE 3562.3666584653174 Test RE 1.0047510745761006 Lambda1 1.3420134e-07\n",
      "57 Train Loss 423930.88 Test MSE 3562.3962311315013 Test RE 1.0047552449927515 Lambda1 -3.0851224e-06\n",
      "58 Train Loss 422573.34 Test MSE 3562.381991336731 Test RE 1.0047532368608858 Lambda1 -1.1808474e-05\n",
      "59 Train Loss 420987.5 Test MSE 3562.370538813392 Test RE 1.0047516217941734 Lambda1 -2.1250424e-05\n",
      "60 Train Loss 419314.5 Test MSE 3562.401825122654 Test RE 1.0047560338704309 Lambda1 -2.4894462e-05\n",
      "61 Train Loss 417610.0 Test MSE 3562.4387100543363 Test RE 1.0047612354540194 Lambda1 -2.5487849e-05\n",
      "62 Train Loss 415703.6 Test MSE 3562.4199579546175 Test RE 1.0047585910004289 Lambda1 -2.0388825e-05\n",
      "63 Train Loss 412859.47 Test MSE 3562.3253141924993 Test RE 1.0047452440669116 Lambda1 -1.1066579e-05\n",
      "64 Train Loss 409921.78 Test MSE 3562.3328129968168 Test RE 1.0047463015761708 Lambda1 -1.0233585e-05\n",
      "65 Train Loss 407086.0 Test MSE 3562.476260607031 Test RE 1.0047665308770939 Lambda1 -9.249977e-06\n",
      "66 Train Loss 404577.4 Test MSE 3562.5996979161814 Test RE 1.0047839379548527 Lambda1 -6.059233e-06\n",
      "67 Train Loss 401785.7 Test MSE 3562.549379999748 Test RE 1.0047768421797119 Lambda1 -3.97125e-06\n",
      "68 Train Loss 399763.94 Test MSE 3562.3883889647477 Test RE 1.0047541390710626 Lambda1 -7.186426e-06\n",
      "69 Train Loss 397769.97 Test MSE 3562.292375255954 Test RE 1.0047405988824571 Lambda1 -7.910977e-06\n",
      "70 Train Loss 394807.72 Test MSE 3562.12357072793 Test RE 1.0047167930358274 Lambda1 -3.5359976e-06\n",
      "71 Train Loss 392921.8 Test MSE 3561.9435544093176 Test RE 1.0046914054124252 Lambda1 3.8278667e-06\n",
      "72 Train Loss 391714.44 Test MSE 3561.8033625688076 Test RE 1.0046716337735653 Lambda1 1.150803e-05\n",
      "73 Train Loss 390651.6 Test MSE 3561.7659531143913 Test RE 1.004666357750081 Lambda1 2.2491093e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 Train Loss 390064.94 Test MSE 3561.793542557293 Test RE 1.0046702488158894 Lambda1 2.9383047e-05\n",
      "Training time: 309.25\n",
      "Training time: 309.25\n",
      "inv_HT_atanh_tune2\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 7.32\n",
      "Training time: 7.32\n",
      "inv_HT_atanh_tune2\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 427840.2 Test MSE 3518.35542309412 Test RE 0.9985251901420518 Lambda1 -3.8719336e-06\n",
      "1 Train Loss 419396.03 Test MSE 3518.3894976030588 Test RE 0.9985300253802589 Lambda1 -1.4719999e-05\n",
      "2 Train Loss 411604.0 Test MSE 3518.3493231423527 Test RE 0.9985243245447192 Lambda1 -2.5751295e-05\n",
      "3 Train Loss 400386.72 Test MSE 3518.242944897772 Test RE 0.9985092291039344 Lambda1 -3.1315307e-05\n",
      "4 Train Loss 395262.38 Test MSE 3518.256044330594 Test RE 0.9985110879710734 Lambda1 -2.2547745e-05\n",
      "5 Train Loss 389601.88 Test MSE 3518.19183099294 Test RE 0.998501975782904 Lambda1 7.901444e-06\n",
      "6 Train Loss 384635.78 Test MSE 3518.065268878078 Test RE 0.9984840157533833 Lambda1 2.9055547e-05\n",
      "7 Train Loss 376861.4 Test MSE 3517.9592388231026 Test RE 0.9984689691148625 Lambda1 2.035909e-05\n",
      "8 Train Loss 371099.34 Test MSE 3517.838748779734 Test RE 0.9984518701959498 Lambda1 1.567524e-05\n",
      "9 Train Loss 367209.0 Test MSE 3517.6954096369677 Test RE 0.9984315283464094 Lambda1 3.786403e-05\n",
      "10 Train Loss 362980.34 Test MSE 3517.6844842885175 Test RE 0.998429977868106 Lambda1 4.0318748e-05\n",
      "11 Train Loss 359745.53 Test MSE 3517.7087277289866 Test RE 0.9984334183892973 Lambda1 4.2129963e-05\n",
      "12 Train Loss 354294.72 Test MSE 3517.6567448460505 Test RE 0.9984260411953239 Lambda1 4.699805e-05\n",
      "13 Train Loss 351168.6 Test MSE 3517.7375655876876 Test RE 0.9984375109144409 Lambda1 1.4238185e-05\n",
      "14 Train Loss 347245.53 Test MSE 3517.917163079313 Test RE 0.9984629981175753 Lambda1 -1.6279851e-05\n",
      "15 Train Loss 344263.78 Test MSE 3517.982557288748 Test RE 0.9984722782387312 Lambda1 -5.6999897e-06\n",
      "16 Train Loss 342386.28 Test MSE 3517.9249712451838 Test RE 0.9984641061823786 Lambda1 1.3552384e-05\n",
      "17 Train Loss 339936.62 Test MSE 3517.791316647923 Test RE 0.9984451389521758 Lambda1 4.454106e-05\n",
      "18 Train Loss 334885.88 Test MSE 3517.5513627332075 Test RE 0.9984110856380595 Lambda1 6.6787135e-05\n",
      "19 Train Loss 329496.28 Test MSE 3517.277456601135 Test RE 0.998372212539364 Lambda1 4.8540598e-05\n",
      "20 Train Loss 323852.0 Test MSE 3517.036017244635 Test RE 0.9983379459106654 Lambda1 1.4266349e-05\n",
      "21 Train Loss 320367.94 Test MSE 3516.966502710072 Test RE 0.9983280797420752 Lambda1 3.2037915e-05\n",
      "22 Train Loss 317090.88 Test MSE 3516.969645608296 Test RE 0.9983285258143998 Lambda1 3.164347e-05\n",
      "23 Train Loss 313486.53 Test MSE 3516.9115337354287 Test RE 0.9983202779496373 Lambda1 1.7512448e-05\n",
      "24 Train Loss 308836.72 Test MSE 3516.5631338580056 Test RE 0.9982708278463935 Lambda1 1.3248025e-05\n",
      "25 Train Loss 304413.25 Test MSE 3516.312052538195 Test RE 0.9982351891244895 Lambda1 2.481149e-05\n",
      "26 Train Loss 300952.44 Test MSE 3516.3022226955554 Test RE 0.9982337938413721 Lambda1 5.349814e-05\n",
      "27 Train Loss 297351.78 Test MSE 3516.1642198382506 Test RE 0.9982142050149921 Lambda1 4.557179e-05\n",
      "28 Train Loss 294793.97 Test MSE 3516.153546403916 Test RE 0.9982126899575051 Lambda1 1.7139177e-05\n",
      "29 Train Loss 291983.12 Test MSE 3516.3172098576188 Test RE 0.9982359211721239 Lambda1 2.4710536e-05\n",
      "30 Train Loss 288364.62 Test MSE 3516.0912416270853 Test RE 0.9982038459617054 Lambda1 3.364555e-05\n",
      "31 Train Loss 285555.75 Test MSE 3515.768510168828 Test RE 0.9981580338434495 Lambda1 8.791131e-06\n",
      "32 Train Loss 283858.8 Test MSE 3515.711401410424 Test RE 0.998149926967623 Lambda1 -1.8281726e-05\n",
      "33 Train Loss 282880.84 Test MSE 3515.762509145067 Test RE 0.9981571819710043 Lambda1 -9.539226e-06\n",
      "34 Train Loss 281083.38 Test MSE 3515.847424098357 Test RE 0.9981692359647659 Lambda1 3.3136268e-05\n",
      "35 Train Loss 279218.75 Test MSE 3515.8301553390106 Test RE 0.9981667846118756 Lambda1 2.375535e-05\n",
      "36 Train Loss 277915.72 Test MSE 3515.793067633941 Test RE 0.9981615198790372 Lambda1 -4.3779037e-06\n",
      "37 Train Loss 276214.72 Test MSE 3515.735315500606 Test RE 0.9981533217011567 Lambda1 -3.7720627e-05\n",
      "38 Train Loss 273641.5 Test MSE 3515.4507796154244 Test RE 0.9981129295555494 Lambda1 -2.7157897e-05\n",
      "39 Train Loss 271431.88 Test MSE 3515.0818928090816 Test RE 0.9980605606883143 Lambda1 -1.8947989e-05\n",
      "40 Train Loss 269498.03 Test MSE 3514.898093345329 Test RE 0.9980344666456279 Lambda1 -1.77489e-05\n",
      "41 Train Loss 267877.16 Test MSE 3514.7990145728854 Test RE 0.9980204001316016 Lambda1 -3.7988336e-06\n",
      "42 Train Loss 265437.12 Test MSE 3514.546451828489 Test RE 0.9979845421353898 Lambda1 4.4048084e-06\n",
      "43 Train Loss 262197.47 Test MSE 3513.7942927702074 Test RE 0.997877745527371 Lambda1 -5.215777e-06\n",
      "44 Train Loss 260217.42 Test MSE 3513.482875465169 Test RE 0.9978335250554198 Lambda1 5.467088e-06\n",
      "45 Train Loss 258253.19 Test MSE 3513.385485971029 Test RE 0.9978196955905965 Lambda1 1.0088146e-05\n",
      "46 Train Loss 256193.08 Test MSE 3513.350742024465 Test RE 0.9978147618479097 Lambda1 -6.1410215e-06\n",
      "47 Train Loss 253873.72 Test MSE 3513.029345925029 Test RE 0.9977691214990734 Lambda1 -9.677233e-06\n",
      "48 Train Loss 251231.94 Test MSE 3512.874802163405 Test RE 0.9977471745305191 Lambda1 -1.1878877e-05\n",
      "49 Train Loss 248860.69 Test MSE 3512.8531096543124 Test RE 0.9977440939093343 Lambda1 -2.3190232e-05\n",
      "50 Train Loss 246499.61 Test MSE 3512.744089134341 Test RE 0.9977286114196768 Lambda1 -2.013358e-05\n",
      "51 Train Loss 243546.97 Test MSE 3512.4276070628644 Test RE 0.9976836650298708 Lambda1 -3.7329042e-05\n",
      "52 Train Loss 241391.17 Test MSE 3512.1792955905444 Test RE 0.997648398726038 Lambda1 -1.9682846e-05\n",
      "53 Train Loss 238962.23 Test MSE 3512.090835350828 Test RE 0.9976358349067139 Lambda1 6.967882e-06\n",
      "54 Train Loss 236328.28 Test MSE 3512.132429838968 Test RE 0.9976417425029277 Lambda1 1.693088e-05\n",
      "55 Train Loss 234521.9 Test MSE 3512.042352085151 Test RE 0.9976289488646909 Lambda1 2.3385724e-06\n",
      "56 Train Loss 232296.1 Test MSE 3511.755389210753 Test RE 0.9975881907686605 Lambda1 -1.9512969e-05\n",
      "57 Train Loss 230586.66 Test MSE 3511.6035068285337 Test RE 0.9975666178382613 Lambda1 -2.4910361e-05\n",
      "58 Train Loss 228762.38 Test MSE 3511.4318804552086 Test RE 0.9975422399675242 Lambda1 -2.6406535e-05\n",
      "59 Train Loss 226571.25 Test MSE 3511.2891517679536 Test RE 0.9975219662805965 Lambda1 -8.885744e-06\n",
      "60 Train Loss 225001.88 Test MSE 3511.231492888189 Test RE 0.997513776092784 Lambda1 1.2902643e-05\n",
      "61 Train Loss 222887.14 Test MSE 3511.186720426792 Test RE 0.9975074163170903 Lambda1 -3.900928e-06\n",
      "62 Train Loss 221001.97 Test MSE 3511.1286176080034 Test RE 0.9974991629490646 Lambda1 -3.9855066e-05\n",
      "63 Train Loss 218936.03 Test MSE 3510.8341186476073 Test RE 0.9974573290177804 Lambda1 -4.381077e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 Train Loss 217580.12 Test MSE 3510.626912227528 Test RE 0.9974278940452145 Lambda1 -1.5369938e-06\n",
      "65 Train Loss 216060.3 Test MSE 3510.567216769001 Test RE 0.9974194137694562 Lambda1 2.4237865e-05\n",
      "66 Train Loss 214998.03 Test MSE 3510.512181653353 Test RE 0.9974115954733607 Lambda1 9.781394e-06\n",
      "67 Train Loss 213285.4 Test MSE 3510.384691493064 Test RE 0.9973934839682398 Lambda1 2.163442e-06\n",
      "68 Train Loss 211735.77 Test MSE 3510.2860402127035 Test RE 0.9973794691459114 Lambda1 1.1003313e-05\n",
      "69 Train Loss 210774.56 Test MSE 3510.207293235323 Test RE 0.997368281872555 Lambda1 1.3176748e-05\n",
      "70 Train Loss 209723.2 Test MSE 3510.2843291715385 Test RE 0.9973792260663561 Lambda1 1.4114732e-05\n",
      "71 Train Loss 208166.75 Test MSE 3510.559271611739 Test RE 0.9974182850831083 Lambda1 2.2292315e-05\n",
      "72 Train Loss 206987.72 Test MSE 3510.8520634161064 Test RE 0.9974598781439108 Lambda1 1.6739086e-05\n",
      "73 Train Loss 206056.39 Test MSE 3510.991322748992 Test RE 0.9974796602676365 Lambda1 8.678578e-06\n",
      "74 Train Loss 204951.12 Test MSE 3511.184177286847 Test RE 0.9975070550715032 Lambda1 -4.3133855e-06\n",
      "Training time: 305.63\n",
      "Training time: 305.63\n",
      "inv_HT_atanh_tune2\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.69\n",
      "Training time: 6.69\n",
      "inv_HT_atanh_tune2\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.74\n",
      "Training time: 6.74\n",
      "inv_HT_atanh_tune2\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.39\n",
      "Training time: 6.39\n",
      "inv_HT_atanh_tune2\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.81\n",
      "Training time: 6.81\n",
      "inv_HT_atanh_tune2\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.12\n",
      "Training time: 6.12\n",
      "inv_HT_atanh_tune3\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartlab/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.35\n",
      "Training time: 6.35\n",
      "inv_HT_atanh_tune3\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.51\n",
      "Training time: 6.51\n",
      "inv_HT_atanh_tune3\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.39\n",
      "Training time: 6.39\n",
      "inv_HT_atanh_tune3\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.50\n",
      "Training time: 6.50\n",
      "inv_HT_atanh_tune3\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 7.28\n",
      "Training time: 7.28\n",
      "inv_HT_atanh_tune3\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.60\n",
      "Training time: 6.60\n",
      "inv_HT_atanh_tune3\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 7.04\n",
      "Training time: 7.04\n",
      "inv_HT_atanh_tune3\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.89\n",
      "Training time: 6.89\n",
      "inv_HT_atanh_tune3\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.94\n",
      "Training time: 6.94\n",
      "inv_HT_atanh_tune3\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.71\n",
      "Training time: 6.71\n",
      "inv_HT_atanh_tune4\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 1118437000.0 Test MSE 3552.4578826708494 Test RE 1.0033527365025883 Lambda1 8.2819423e-07\n",
      "1 Train Loss 1048875400.0 Test MSE 3552.455354678734 Test RE 1.0033523795007118 Lambda1 7.247904e-07\n",
      "2 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "3 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "4 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "5 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "6 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "7 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "8 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "9 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "10 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "11 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "12 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "13 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "14 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "15 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "16 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "17 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "18 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "19 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "20 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "21 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "22 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "23 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "24 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "25 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "26 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "27 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "28 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "29 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "30 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "31 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "32 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "33 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "34 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "35 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "36 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "37 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "38 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "39 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "40 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "41 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "42 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "43 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "44 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "45 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "46 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "47 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "48 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "49 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "50 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "51 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "52 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "53 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "54 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "55 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "56 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "57 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "58 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "59 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "60 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "61 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "62 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "63 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "64 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "65 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "66 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "67 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "68 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "69 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "70 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "71 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "72 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "73 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "74 Train Loss 1007447360.0 Test MSE 3552.4606035879256 Test RE 1.003353120749096 Lambda1 6.900507e-07\n",
      "Training time: 236.09\n",
      "Training time: 236.09\n",
      "inv_HT_atanh_tune4\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 7.13\n",
      "Training time: 7.13\n",
      "inv_HT_atanh_tune4\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 7.47\n",
      "Training time: 7.47\n",
      "inv_HT_atanh_tune4\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 7.44\n",
      "Training time: 7.44\n",
      "inv_HT_atanh_tune4\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 7.56\n",
      "Training time: 7.56\n",
      "inv_HT_atanh_tune4\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 7.48\n",
      "Training time: 7.48\n",
      "inv_HT_atanh_tune4\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.97\n",
      "Training time: 6.97\n",
      "inv_HT_atanh_tune4\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 7.00\n",
      "Training time: 7.00\n",
      "inv_HT_atanh_tune4\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.87\n",
      "Training time: 6.87\n",
      "inv_HT_atanh_tune4\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 7.22\n",
      "Training time: 7.22\n",
      "inv_HT_atanh_tune5\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1038.2202 Test MSE 1037.0775474182499 Test RE 0.5421191972976248 Lambda1 -0.059079446\n",
      "1 Train Loss 871.4634 Test MSE 873.461360440876 Test RE 0.4975205449469068 Lambda1 -0.069602564\n",
      "2 Train Loss 856.61816 Test MSE 859.5179821673172 Test RE 0.493533519533298 Lambda1 -0.07239664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Train Loss 854.9474 Test MSE 858.1470493395533 Test RE 0.49313976900797823 Lambda1 -0.07330028\n",
      "4 Train Loss 854.7496 Test MSE 858.0519796520008 Test RE 0.4931124520450294 Lambda1 -0.07361044\n",
      "5 Train Loss 854.72296 Test MSE 858.0711212131054 Test RE 0.49311795223016697 Lambda1 -0.07374924\n",
      "6 Train Loss 854.7225 Test MSE 858.0752068648908 Test RE 0.49311912620385867 Lambda1 -0.07376391\n",
      "7 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "8 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "9 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "10 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "11 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "12 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "13 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "14 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "15 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "16 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "17 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "18 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "19 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "20 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "21 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "22 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "23 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "24 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "25 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "26 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "27 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "28 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "29 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "30 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "31 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "32 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "33 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "34 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "35 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "36 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "37 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "38 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "39 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "40 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "41 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "42 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "43 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "44 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "45 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "46 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "47 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "48 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "49 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "50 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "51 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "52 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "53 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "54 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "55 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "56 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "57 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "58 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "59 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "60 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "61 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "62 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "63 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "64 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "65 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "66 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "67 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "68 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "69 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "70 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "71 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "72 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "73 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "74 Train Loss 854.72235 Test MSE 858.0760738821394 Test RE 0.4931193753327675 Lambda1 -0.07376706\n",
      "Training time: 270.15\n",
      "Training time: 270.15\n",
      "inv_HT_atanh_tune5\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 986.703 Test MSE 986.2440783951771 Test RE 0.5286659931199656 Lambda1 -0.05538197\n",
      "1 Train Loss 867.97955 Test MSE 870.1276099541525 Test RE 0.496570190947267 Lambda1 -0.06261671\n",
      "2 Train Loss 856.2914 Test MSE 859.2327557210241 Test RE 0.49345162450893176 Lambda1 -0.06468126\n",
      "3 Train Loss 854.91077 Test MSE 858.1238325200148 Test RE 0.49313309811487654 Lambda1 -0.06538043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 854.7311 Test MSE 858.0560086491693 Test RE 0.4931136097524807 Lambda1 -0.06566746\n",
      "5 Train Loss 854.7215 Test MSE 858.0743873238495 Test RE 0.4931188907166569 Lambda1 -0.06573904\n",
      "6 Train Loss 854.7214 Test MSE 858.0761666968394 Test RE 0.4931194020021544 Lambda1 -0.06574397\n",
      "7 Train Loss 854.72125 Test MSE 858.0775446704102 Test RE 0.4931197979490743 Lambda1 -0.065747775\n",
      "8 Train Loss 854.7212 Test MSE 858.0785382544672 Test RE 0.4931200834453157 Lambda1 -0.06575047\n",
      "9 Train Loss 854.7212 Test MSE 858.0794746488435 Test RE 0.4931203525085345 Lambda1 -0.065753184\n",
      "10 Train Loss 854.721 Test MSE 858.0805499171097 Test RE 0.49312066147554506 Lambda1 -0.065756276\n",
      "11 Train Loss 854.7209 Test MSE 858.0818426170492 Test RE 0.49312103291900067 Lambda1 -0.06576011\n",
      "12 Train Loss 854.72046 Test MSE 858.0857632358986 Test RE 0.49312215946504345 Lambda1 -0.06577325\n",
      "13 Train Loss 854.476 Test MSE 857.5583715139586 Test RE 0.492970596224693 Lambda1 -0.06682221\n",
      "14 Train Loss 854.20233 Test MSE 857.5043041162705 Test RE 0.4929550555581399 Lambda1 -0.24278744\n",
      "15 Train Loss 851.2641 Test MSE 854.0801150553586 Test RE 0.49196983613899276 Lambda1 -0.45225725\n",
      "16 Train Loss 844.252 Test MSE 845.9287150423568 Test RE 0.4896165104364904 Lambda1 -0.62106186\n",
      "17 Train Loss 839.5272 Test MSE 839.8287976078715 Test RE 0.48784802581190206 Lambda1 -0.67986727\n",
      "18 Train Loss 833.7221 Test MSE 832.5171285669314 Test RE 0.4857197462362943 Lambda1 -0.78605914\n",
      "19 Train Loss 815.1618 Test MSE 815.2395476577174 Test RE 0.48065314718930435 Lambda1 -0.8813382\n",
      "20 Train Loss 804.0834 Test MSE 808.9245395729848 Test RE 0.4787879104859907 Lambda1 -0.87224734\n",
      "21 Train Loss 793.8091 Test MSE 799.4434702167501 Test RE 0.4759738030771453 Lambda1 -0.85905814\n",
      "22 Train Loss 788.0972 Test MSE 793.580764091546 Test RE 0.47422531835473736 Lambda1 -0.7916188\n",
      "23 Train Loss 779.9091 Test MSE 786.0771181967954 Test RE 0.4719779916664443 Lambda1 -0.7552874\n",
      "24 Train Loss 774.76624 Test MSE 778.0202891474266 Test RE 0.46955301332532084 Lambda1 -0.7398209\n",
      "25 Train Loss 766.0406 Test MSE 767.9259712837808 Test RE 0.4664969927979376 Lambda1 -0.748251\n",
      "26 Train Loss 761.7306 Test MSE 765.5385278221387 Test RE 0.4657712703832739 Lambda1 -0.7592045\n",
      "27 Train Loss 758.22565 Test MSE 764.60014251613 Test RE 0.46548571523375476 Lambda1 -0.75880903\n",
      "28 Train Loss 750.9473 Test MSE 757.6612382320876 Test RE 0.4633687117007566 Lambda1 -0.74947333\n",
      "29 Train Loss 747.5737 Test MSE 752.9853172525245 Test RE 0.4619366543074744 Lambda1 -0.7301847\n",
      "30 Train Loss 744.0194 Test MSE 747.5547477156712 Test RE 0.46026788437915944 Lambda1 -0.7234163\n",
      "31 Train Loss 742.2635 Test MSE 748.3103299625163 Test RE 0.46050043083244235 Lambda1 -0.7239035\n",
      "32 Train Loss 739.04175 Test MSE 745.6320594587543 Test RE 0.45967560574725747 Lambda1 -0.7612084\n",
      "33 Train Loss 738.0576 Test MSE 745.4986147390509 Test RE 0.4596344701587513 Lambda1 -0.77173436\n",
      "34 Train Loss 736.5345 Test MSE 743.8328282373781 Test RE 0.4591206656629626 Lambda1 -0.7796558\n",
      "35 Train Loss 735.3571 Test MSE 741.2813520123875 Test RE 0.4583325572991728 Lambda1 -0.7757694\n",
      "36 Train Loss 735.0022 Test MSE 741.1541790049615 Test RE 0.4582932402236134 Lambda1 -0.78048223\n",
      "37 Train Loss 732.7564 Test MSE 739.4214908850063 Test RE 0.45775722226473364 Lambda1 -0.7906843\n",
      "38 Train Loss 732.2361 Test MSE 737.3787731075563 Test RE 0.4571244874013736 Lambda1 -0.79747653\n",
      "39 Train Loss 731.1427 Test MSE 734.2560086227321 Test RE 0.45615551004823995 Lambda1 -0.7978635\n",
      "40 Train Loss 727.45386 Test MSE 733.7572805959106 Test RE 0.45600056669153705 Lambda1 -0.7822628\n",
      "41 Train Loss 726.5914 Test MSE 732.5405594325653 Test RE 0.4556223382572802 Lambda1 -0.7758631\n",
      "42 Train Loss 723.7251 Test MSE 730.4218038889874 Test RE 0.4549629540188517 Lambda1 -0.7763649\n",
      "43 Train Loss 722.96436 Test MSE 728.7330922783651 Test RE 0.4544367198588462 Lambda1 -0.7688006\n",
      "44 Train Loss 721.3421 Test MSE 726.9836205703198 Test RE 0.4538909082847834 Lambda1 -0.7543702\n",
      "45 Train Loss 719.3912 Test MSE 725.6837524401874 Test RE 0.45348494157578395 Lambda1 -0.755124\n",
      "46 Train Loss 717.3135 Test MSE 724.7991742643085 Test RE 0.45320846769875145 Lambda1 -0.7637806\n",
      "47 Train Loss 716.35364 Test MSE 724.6134809084796 Test RE 0.45315040803530415 Lambda1 -0.7680911\n",
      "48 Train Loss 715.6374 Test MSE 723.2195745521499 Test RE 0.45271434569854596 Lambda1 -0.77284\n",
      "49 Train Loss 713.1031 Test MSE 721.4142842727427 Test RE 0.4521489631403953 Lambda1 -0.76896584\n",
      "50 Train Loss 712.6506 Test MSE 720.7870616605576 Test RE 0.45195236339926836 Lambda1 -0.7663808\n",
      "51 Train Loss 711.9952 Test MSE 719.8958332523803 Test RE 0.4516728651995035 Lambda1 -0.7657645\n",
      "52 Train Loss 711.01544 Test MSE 719.725336311434 Test RE 0.4516193759317402 Lambda1 -0.76743513\n",
      "53 Train Loss 710.8235 Test MSE 719.7595777724024 Test RE 0.4516301188655572 Lambda1 -0.77153397\n",
      "54 Train Loss 710.7002 Test MSE 719.6602847898649 Test RE 0.4515989659295086 Lambda1 -0.77158254\n",
      "55 Train Loss 708.74133 Test MSE 717.7111941779986 Test RE 0.450987007684193 Lambda1 -0.77810836\n",
      "56 Train Loss 707.5715 Test MSE 716.361123513177 Test RE 0.45056263717544115 Lambda1 -0.7802254\n",
      "57 Train Loss 707.2568 Test MSE 715.8727594353932 Test RE 0.45040903020681755 Lambda1 -0.7763901\n",
      "58 Train Loss 706.78424 Test MSE 715.1161313889099 Test RE 0.4501709416665029 Lambda1 -0.7721524\n",
      "59 Train Loss 706.19226 Test MSE 714.4641089297166 Test RE 0.4499656683631753 Lambda1 -0.76596594\n",
      "60 Train Loss 705.6342 Test MSE 713.854670234183 Test RE 0.449773716807901 Lambda1 -0.75626165\n",
      "61 Train Loss 705.08 Test MSE 713.2036993291277 Test RE 0.44956859355552614 Lambda1 -0.7513228\n",
      "62 Train Loss 704.3133 Test MSE 712.2316647994194 Test RE 0.44926212767816703 Lambda1 -0.74858975\n",
      "63 Train Loss 703.4907 Test MSE 710.7844004114664 Test RE 0.4488054421987345 Lambda1 -0.75148493\n",
      "64 Train Loss 702.8776 Test MSE 709.5738667386453 Test RE 0.4484231000865349 Lambda1 -0.7541679\n",
      "65 Train Loss 701.7122 Test MSE 708.2297183663615 Test RE 0.4479981739966858 Lambda1 -0.75854677\n",
      "66 Train Loss 700.8224 Test MSE 706.9722545893771 Test RE 0.4476002863218606 Lambda1 -0.7637794\n",
      "67 Train Loss 699.7328 Test MSE 705.719328550005 Test RE 0.44720348238539187 Lambda1 -0.76668614\n",
      "68 Train Loss 698.8967 Test MSE 704.4865208123884 Test RE 0.44681270600133743 Lambda1 -0.7689946\n",
      "69 Train Loss 698.0763 Test MSE 703.2838212987732 Test RE 0.4464311437153999 Lambda1 -0.7759321\n",
      "70 Train Loss 696.76483 Test MSE 701.279906897047 Test RE 0.44579466669942686 Lambda1 -0.78058654\n",
      "71 Train Loss 696.48395 Test MSE 700.5889631141349 Test RE 0.44557500051651283 Lambda1 -0.7840224\n",
      "72 Train Loss 695.83136 Test MSE 700.3832009988145 Test RE 0.4455095632971072 Lambda1 -0.7967977\n",
      "73 Train Loss 695.543 Test MSE 699.7538377904655 Test RE 0.44530935122397125 Lambda1 -0.8067103\n",
      "74 Train Loss 694.1494 Test MSE 698.5741738913861 Test RE 0.44493383563427413 Lambda1 -0.82936835\n",
      "Training time: 285.48\n",
      "Training time: 285.48\n",
      "inv_HT_atanh_tune5\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 1116.9371 Test MSE 1114.9155290759925 Test RE 0.5620955575467278 Lambda1 -0.15410069\n",
      "1 Train Loss 874.52386 Test MSE 876.4028144250111 Test RE 0.49835756182406044 Lambda1 -0.18806198\n",
      "2 Train Loss 856.98193 Test MSE 859.8390277514696 Test RE 0.49362568279656954 Lambda1 -0.19593753\n",
      "3 Train Loss 854.9818 Test MSE 858.1664794926411 Test RE 0.493145351808138 Lambda1 -0.1985227\n",
      "4 Train Loss 854.7083 Test MSE 858.0670246428067 Test RE 0.49311677511633695 Lambda1 -0.19990236\n",
      "5 Train Loss 854.7081 Test MSE 858.0667663030822 Test RE 0.4931167008845707 Lambda1 -0.1999033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 854.7079 Test MSE 858.0665520157214 Test RE 0.4931166393108757 Lambda1 -0.19990441\n",
      "7 Train Loss 854.7078 Test MSE 858.0663722139519 Test RE 0.4931165876463231 Lambda1 -0.19990574\n",
      "8 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "9 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "10 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "11 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "12 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "13 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "14 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "15 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "16 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "17 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "18 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "19 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "20 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "21 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "22 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "23 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "24 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "25 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "26 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "27 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "28 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "29 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "30 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "31 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "32 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "33 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "34 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "35 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "36 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "37 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "38 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "39 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "40 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "41 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "42 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "43 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "44 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "45 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "46 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "47 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "48 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "49 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "50 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "51 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "52 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "53 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "54 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "55 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "56 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "57 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "58 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "59 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "60 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "61 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "62 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "63 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "64 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "65 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "66 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "67 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "68 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "69 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "70 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "71 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "72 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "73 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "74 Train Loss 854.7077 Test MSE 858.0663156512287 Test RE 0.49311657139349097 Lambda1 -0.1999061\n",
      "Training time: 176.36\n",
      "Training time: 176.36\n",
      "inv_HT_atanh_tune5\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 1067.6005 Test MSE 1066.1111627481787 Test RE 0.5496552949321813 Lambda1 -0.0379986\n",
      "1 Train Loss 874.77386 Test MSE 876.6435678321351 Test RE 0.4984260081139555 Lambda1 -0.045252148\n",
      "2 Train Loss 856.94617 Test MSE 859.8077384821875 Test RE 0.49361670127390717 Lambda1 -0.047143817\n",
      "3 Train Loss 854.98517 Test MSE 858.1713770286392 Test RE 0.49314675899061644 Lambda1 -0.047741614\n",
      "4 Train Loss 854.75195 Test MSE 858.049819540669 Test RE 0.4931118313492609 Lambda1 -0.047944736\n",
      "5 Train Loss 854.72 Test MSE 858.0722250042388 Test RE 0.4931182693944642 Lambda1 -0.048043005\n",
      "6 Train Loss 854.7198 Test MSE 858.0742672369123 Test RE 0.493118856210826 Lambda1 -0.048047055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "8 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "9 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "10 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "11 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "12 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "13 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "14 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "15 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "16 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "17 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "18 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "19 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "20 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "21 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "22 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "23 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "24 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "25 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "26 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "27 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "28 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "29 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "30 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "31 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "32 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "33 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "34 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "35 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "36 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "37 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "38 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "39 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "40 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "41 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "42 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "43 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "44 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "45 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "46 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "47 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "48 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "49 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "50 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "51 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "52 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "53 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "54 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "55 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "56 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "57 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "58 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "59 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "60 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "61 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "62 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "63 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "64 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "65 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "66 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "67 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "68 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "69 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "70 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "71 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "72 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "73 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "74 Train Loss 854.7196 Test MSE 858.075474640999 Test RE 0.49311920314671226 Lambda1 -0.048049416\n",
      "Training time: 214.00\n",
      "Training time: 214.00\n",
      "inv_HT_atanh_tune5\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 905.2306 Test MSE 906.2271312468815 Test RE 0.5067662698945437 Lambda1 -0.06755021\n",
      "1 Train Loss 859.544 Test MSE 862.1718068685972 Test RE 0.49429484276421015 Lambda1 -0.07178368\n",
      "2 Train Loss 855.29456 Test MSE 858.4003308467771 Test RE 0.4932125385695296 Lambda1 -0.073018655\n",
      "3 Train Loss 854.7901 Test MSE 858.0599257510902 Test RE 0.49311473530491784 Lambda1 -0.0734378\n",
      "4 Train Loss 854.7217 Test MSE 858.0686995413283 Test RE 0.493117256384193 Lambda1 -0.07363273\n",
      "5 Train Loss 854.72076 Test MSE 858.0752500822382 Test RE 0.4931191386219428 Lambda1 -0.07365113\n",
      "6 Train Loss 854.72076 Test MSE 858.0764113548659 Test RE 0.493119472302214 Lambda1 -0.07365394\n",
      "7 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "9 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "10 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "11 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "12 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "13 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "14 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "15 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "16 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "17 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "18 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "19 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "20 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "21 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "22 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "23 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "24 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "25 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "26 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "27 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "28 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "29 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "30 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "31 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "32 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "33 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "34 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "35 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "36 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "37 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "38 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "39 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "40 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "41 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "42 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "43 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "44 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "45 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "46 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "47 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "48 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "49 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "50 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "51 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "52 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "53 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "54 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "55 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "56 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "57 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "58 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "59 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "60 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "61 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "62 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "63 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "64 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "65 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "66 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "67 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "68 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "69 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "70 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "71 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "72 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "73 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "74 Train Loss 854.7206 Test MSE 858.0768239694938 Test RE 0.4931195908629158 Lambda1 -0.073655\n",
      "Training time: 225.40\n",
      "Training time: 225.40\n",
      "inv_HT_atanh_tune5\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 1262.533 Test MSE 1259.180757747499 Test RE 0.597355971936205 Lambda1 0.004908934\n",
      "1 Train Loss 889.23175 Test MSE 890.6368863187857 Test RE 0.5023882917239977 Lambda1 0.0073185307\n",
      "2 Train Loss 858.8256 Test MSE 861.5091291316058 Test RE 0.4941048451611271 Lambda1 0.008002921\n",
      "3 Train Loss 855.20953 Test MSE 858.3328245938418 Test RE 0.49319314459692554 Lambda1 0.0082314825\n",
      "4 Train Loss 854.7096 Test MSE 858.0683279422839 Test RE 0.4931171496084062 Lambda1 0.008341189\n",
      "5 Train Loss 854.53253 Test MSE 857.7079331777716 Test RE 0.4930135823878689 Lambda1 -0.008122298\n",
      "6 Train Loss 854.5202 Test MSE 857.6636322476959 Test RE 0.4930008500570686 Lambda1 -0.034794368\n",
      "7 Train Loss 854.42535 Test MSE 857.7162701936273 Test RE 0.4930159784552867 Lambda1 -0.60038084\n",
      "8 Train Loss 853.103 Test MSE 856.0030567283604 Test RE 0.49252335412338477 Lambda1 -1.0025151\n",
      "9 Train Loss 851.39484 Test MSE 853.0203926352059 Test RE 0.4916645291328035 Lambda1 -0.8290462\n",
      "10 Train Loss 839.6144 Test MSE 840.2820677042079 Test RE 0.4879796580528015 Lambda1 -1.0191599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Train Loss 828.0008 Test MSE 817.5081657688906 Test RE 0.4813214543744698 Lambda1 -1.279816\n",
      "12 Train Loss 815.9439 Test MSE 790.7658727644271 Test RE 0.47338351457005384 Lambda1 -1.5343034\n",
      "13 Train Loss 795.6615 Test MSE 777.8419602162583 Test RE 0.46949919745001534 Lambda1 -1.4467461\n",
      "14 Train Loss 772.3132 Test MSE 768.4992775357242 Test RE 0.4666710953512781 Lambda1 -1.3662425\n",
      "15 Train Loss 764.1828 Test MSE 759.4493580389138 Test RE 0.46391517656201253 Lambda1 -1.1478239\n",
      "16 Train Loss 754.6448 Test MSE 750.8192437502838 Test RE 0.4612717612837564 Lambda1 -1.0860835\n",
      "17 Train Loss 745.1508 Test MSE 745.6556388845105 Test RE 0.4596828739440784 Lambda1 -1.0880698\n",
      "18 Train Loss 740.39874 Test MSE 740.8110922295004 Test RE 0.45818715396318244 Lambda1 -1.1165826\n",
      "19 Train Loss 734.70746 Test MSE 736.7623917275861 Test RE 0.45693339027510105 Lambda1 -1.1120557\n",
      "20 Train Loss 731.90137 Test MSE 734.3678864669722 Test RE 0.4561902607010525 Lambda1 -1.1102637\n",
      "21 Train Loss 726.89 Test MSE 729.0946399725502 Test RE 0.454549436144362 Lambda1 -1.1438032\n",
      "22 Train Loss 720.99854 Test MSE 722.351297536627 Test RE 0.4524425060482474 Lambda1 -1.1865872\n",
      "23 Train Loss 715.93475 Test MSE 717.6549504727276 Test RE 0.4509693364554979 Lambda1 -1.213873\n",
      "24 Train Loss 709.4769 Test MSE 709.9096263748328 Test RE 0.44852918105798273 Lambda1 -1.2254194\n",
      "25 Train Loss 703.86536 Test MSE 707.4696459946034 Test RE 0.4477577135683702 Lambda1 -1.1786186\n",
      "26 Train Loss 693.8273 Test MSE 698.1085902697723 Test RE 0.4447855418369566 Lambda1 -1.1796881\n",
      "27 Train Loss 684.30804 Test MSE 687.9225894633388 Test RE 0.44152871767800805 Lambda1 -1.197384\n",
      "28 Train Loss 680.5789 Test MSE 683.7785828547552 Test RE 0.4401968368952259 Lambda1 -1.1684192\n",
      "29 Train Loss 678.7464 Test MSE 683.8342394064592 Test RE 0.44021475156826656 Lambda1 -1.1672941\n",
      "30 Train Loss 675.8425 Test MSE 679.6355451122527 Test RE 0.438861226874498 Lambda1 -1.1983551\n",
      "31 Train Loss 667.42053 Test MSE 671.2073142198268 Test RE 0.43613155594222625 Lambda1 -1.2714312\n",
      "32 Train Loss 660.5171 Test MSE 666.4113848783269 Test RE 0.4345706329383105 Lambda1 -1.320379\n",
      "33 Train Loss 657.3833 Test MSE 665.4497635011497 Test RE 0.43425698037927707 Lambda1 -1.3384969\n",
      "34 Train Loss 656.1315 Test MSE 664.9856383834943 Test RE 0.43410551535617564 Lambda1 -1.362819\n",
      "35 Train Loss 654.4935 Test MSE 664.2455379280022 Test RE 0.43386387770713675 Lambda1 -1.3544867\n",
      "36 Train Loss 652.071 Test MSE 660.9753055593563 Test RE 0.4327945545123604 Lambda1 -1.3459159\n",
      "37 Train Loss 648.34424 Test MSE 656.263949681849 Test RE 0.43124934118475095 Lambda1 -1.4280124\n",
      "38 Train Loss 646.4087 Test MSE 654.471426848151 Test RE 0.4306599800943986 Lambda1 -1.4691528\n",
      "39 Train Loss 645.48035 Test MSE 653.4612324671576 Test RE 0.4303274840664423 Lambda1 -1.4840456\n",
      "40 Train Loss 644.4919 Test MSE 653.208069277298 Test RE 0.43024411750479197 Lambda1 -1.4841992\n",
      "41 Train Loss 643.95 Test MSE 652.9555822137609 Test RE 0.4301609574992974 Lambda1 -1.5017788\n",
      "42 Train Loss 642.949 Test MSE 651.7070466238646 Test RE 0.42974949899993425 Lambda1 -1.5299851\n",
      "43 Train Loss 640.22455 Test MSE 649.5156208757885 Test RE 0.42902635418470203 Lambda1 -1.5295352\n",
      "44 Train Loss 638.1824 Test MSE 648.3322068854759 Test RE 0.4286353341320549 Lambda1 -1.546273\n",
      "45 Train Loss 635.97107 Test MSE 645.7372264637916 Test RE 0.42777665741509147 Lambda1 -1.5753659\n",
      "46 Train Loss 634.076 Test MSE 644.1834605461638 Test RE 0.42726169167326067 Lambda1 -1.617484\n",
      "47 Train Loss 632.9064 Test MSE 643.031367995661 Test RE 0.4268794517302272 Lambda1 -1.6572679\n",
      "48 Train Loss 632.20795 Test MSE 642.1893154044591 Test RE 0.42659985984680276 Lambda1 -1.692871\n",
      "49 Train Loss 631.36676 Test MSE 641.5909458027853 Test RE 0.42640106809535405 Lambda1 -1.7084981\n",
      "50 Train Loss 630.44006 Test MSE 641.5518950639001 Test RE 0.42638809134571376 Lambda1 -1.6819808\n",
      "51 Train Loss 629.6916 Test MSE 640.3741814281498 Test RE 0.4259965456707531 Lambda1 -1.6883407\n",
      "52 Train Loss 628.46387 Test MSE 638.9037385431615 Test RE 0.42550717272908106 Lambda1 -1.7088417\n",
      "53 Train Loss 627.14404 Test MSE 636.430900157758 Test RE 0.42468292448100836 Lambda1 -1.7410408\n",
      "54 Train Loss 626.06494 Test MSE 635.3169999461674 Test RE 0.4243111154054959 Lambda1 -1.7507911\n",
      "55 Train Loss 624.9818 Test MSE 634.4487859494858 Test RE 0.4240210879110462 Lambda1 -1.7793941\n",
      "56 Train Loss 624.5957 Test MSE 633.928259480953 Test RE 0.4238471105202997 Lambda1 -1.7966366\n",
      "57 Train Loss 623.7109 Test MSE 633.8844179248415 Test RE 0.42383245394213837 Lambda1 -1.8301189\n",
      "58 Train Loss 622.80414 Test MSE 633.2509502307128 Test RE 0.4236206240957543 Lambda1 -1.8609569\n",
      "59 Train Loss 622.50653 Test MSE 632.5821262219132 Test RE 0.42339685617692047 Lambda1 -1.8776613\n",
      "60 Train Loss 622.0355 Test MSE 631.6679659420298 Test RE 0.4230908148674419 Lambda1 -1.9083151\n",
      "61 Train Loss 619.3493 Test MSE 627.5113219187466 Test RE 0.42169645841218706 Lambda1 -1.9653035\n",
      "62 Train Loss 616.3284 Test MSE 624.2157237838201 Test RE 0.42058765646609664 Lambda1 -1.973804\n",
      "63 Train Loss 614.5178 Test MSE 620.8232157959516 Test RE 0.4194431875873961 Lambda1 -2.0178053\n",
      "64 Train Loss 613.9445 Test MSE 619.1213091054178 Test RE 0.4188678683540877 Lambda1 -2.060936\n",
      "65 Train Loss 613.2504 Test MSE 616.4440991065508 Test RE 0.4179612510675432 Lambda1 -2.105132\n",
      "66 Train Loss 612.5825 Test MSE 614.1589692168669 Test RE 0.41718585025075666 Lambda1 -2.162748\n",
      "67 Train Loss 609.19604 Test MSE 613.0551268895182 Test RE 0.4168107726818214 Lambda1 -2.2326434\n",
      "68 Train Loss 608.0287 Test MSE 612.3841908728983 Test RE 0.4165826285041915 Lambda1 -2.2433128\n",
      "69 Train Loss 607.35986 Test MSE 612.8112421140384 Test RE 0.41672785688093295 Lambda1 -2.2489536\n",
      "70 Train Loss 606.55725 Test MSE 613.2665782822415 Test RE 0.4168826484529059 Lambda1 -2.2761033\n",
      "71 Train Loss 604.1215 Test MSE 612.6877923322877 Test RE 0.4166858802080346 Lambda1 -2.2978861\n",
      "72 Train Loss 602.5838 Test MSE 611.8243488432469 Test RE 0.4163921649074086 Lambda1 -2.2934365\n",
      "73 Train Loss 601.4939 Test MSE 610.2867136062117 Test RE 0.4158685979333756 Lambda1 -2.3110557\n",
      "74 Train Loss 600.37244 Test MSE 608.1034321876086 Test RE 0.41512405306134853 Lambda1 -2.32346\n",
      "Training time: 286.13\n",
      "Training time: 286.13\n",
      "inv_HT_atanh_tune5\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 1171.5226 Test MSE 1168.96724274962 Test RE 0.5755596519776236 Lambda1 -0.02480965\n",
      "1 Train Loss 884.6726 Test MSE 886.2117020947973 Test RE 0.5011386639932809 Lambda1 -0.032067247\n",
      "2 Train Loss 858.2453 Test MSE 860.9787658329034 Test RE 0.4939527310332512 Lambda1 -0.03397866\n",
      "3 Train Loss 855.1446 Test MSE 858.2853725525687 Test RE 0.4931795115723898 Lambda1 -0.034612365\n",
      "4 Train Loss 854.733 Test MSE 858.0525991111209 Test RE 0.49311263004295836 Lambda1 -0.034889463\n",
      "5 Train Loss 854.7204 Test MSE 858.0717160690427 Test RE 0.4931181231566109 Lambda1 -0.034938857\n",
      "6 Train Loss 854.72015 Test MSE 858.0740369385263 Test RE 0.49311879003678455 Lambda1 -0.03494305\n",
      "7 Train Loss 854.72 Test MSE 858.0759022899251 Test RE 0.49311932602743647 Lambda1 -0.034946572\n",
      "8 Train Loss 854.7198 Test MSE 858.0776630827523 Test RE 0.4931198319736613 Lambda1 -0.03495019\n",
      "9 Train Loss 854.7196 Test MSE 858.0791953507936 Test RE 0.4931202722551498 Lambda1 -0.034953576\n",
      "10 Train Loss 854.6135 Test MSE 857.909820231942 Test RE 0.4930716016737106 Lambda1 -0.03522703\n",
      "11 Train Loss 854.4775 Test MSE 857.6081034178293 Test RE 0.49298489030174136 Lambda1 -0.03579341\n",
      "12 Train Loss 854.42255 Test MSE 857.6265727350477 Test RE 0.4929901986968381 Lambda1 -0.03917206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Train Loss 853.6562 Test MSE 854.4160939356841 Test RE 0.492066592414752 Lambda1 -0.04699421\n",
      "14 Train Loss 803.5979 Test MSE 802.3803929864828 Test RE 0.47684729622395783 Lambda1 0.02347763\n",
      "15 Train Loss 731.7671 Test MSE 738.2874001395088 Test RE 0.4574060440488895 Lambda1 0.0046104956\n",
      "16 Train Loss 721.1261 Test MSE 730.7982585729217 Test RE 0.45508018140143314 Lambda1 0.0035537875\n",
      "17 Train Loss 700.63965 Test MSE 713.8489570119133 Test RE 0.44977191695811086 Lambda1 0.0005771772\n",
      "18 Train Loss 684.62976 Test MSE 693.548531533142 Test RE 0.4433304879777182 Lambda1 0.00035706593\n",
      "19 Train Loss 665.807 Test MSE 682.2267281705364 Test RE 0.43969703361696516 Lambda1 0.00046982383\n",
      "20 Train Loss 660.65027 Test MSE 680.5822346105449 Test RE 0.4391667735304236 Lambda1 0.00017294961\n",
      "21 Train Loss 657.42474 Test MSE 678.8767942282833 Test RE 0.4386161842664895 Lambda1 7.6922595e-05\n",
      "22 Train Loss 655.9426 Test MSE 678.4384511126859 Test RE 0.4384745566308422 Lambda1 -5.8131877e-06\n",
      "23 Train Loss 654.4158 Test MSE 675.9335235692085 Test RE 0.43766434113068287 Lambda1 -1.9838571e-05\n",
      "24 Train Loss 653.6749 Test MSE 674.7283382785923 Test RE 0.4372739906486972 Lambda1 9.980296e-06\n",
      "25 Train Loss 653.37164 Test MSE 673.7727436237693 Test RE 0.4369642328407505 Lambda1 8.319542e-07\n",
      "26 Train Loss 651.842 Test MSE 672.9031044485408 Test RE 0.43668214669610145 Lambda1 -3.2490785e-05\n",
      "27 Train Loss 651.51624 Test MSE 671.6650653392177 Test RE 0.4362802474699027 Lambda1 1.6003842e-05\n",
      "28 Train Loss 648.944 Test MSE 669.871180448333 Test RE 0.4356972487027338 Lambda1 0.00034749196\n",
      "29 Train Loss 644.8533 Test MSE 665.911216252666 Test RE 0.43440752093229357 Lambda1 0.0004977866\n",
      "30 Train Loss 638.9292 Test MSE 658.5022847480145 Test RE 0.4319841513533191 Lambda1 0.00014721177\n",
      "31 Train Loss 630.8878 Test MSE 651.6199006476866 Test RE 0.4297207650834753 Lambda1 3.5458757e-05\n",
      "32 Train Loss 627.6087 Test MSE 649.7390711082492 Test RE 0.42910014593951895 Lambda1 1.5847474e-05\n",
      "33 Train Loss 626.20215 Test MSE 648.2295537615846 Test RE 0.42860139898473776 Lambda1 7.5494994e-05\n",
      "34 Train Loss 625.91437 Test MSE 648.1294818017311 Test RE 0.42856831453400557 Lambda1 0.00017569287\n",
      "35 Train Loss 625.6686 Test MSE 648.1117338414731 Test RE 0.4285624466744965 Lambda1 0.00013244964\n",
      "36 Train Loss 625.362 Test MSE 647.9661274290461 Test RE 0.42851430301167376 Lambda1 0.00016135222\n",
      "37 Train Loss 625.2665 Test MSE 647.8662011028673 Test RE 0.42848125999507175 Lambda1 0.00017534634\n",
      "38 Train Loss 625.0144 Test MSE 647.948830884291 Test RE 0.4285085836800844 Lambda1 8.642486e-05\n",
      "39 Train Loss 624.81885 Test MSE 647.6923387109467 Test RE 0.4284237621850935 Lambda1 -3.9121955e-05\n",
      "40 Train Loss 624.6175 Test MSE 647.5273474537537 Test RE 0.4283691909997636 Lambda1 -8.1252154e-05\n",
      "41 Train Loss 624.4581 Test MSE 647.4611345837695 Test RE 0.4283472890069165 Lambda1 -7.187995e-05\n",
      "42 Train Loss 624.3576 Test MSE 647.6730261984268 Test RE 0.4284173748925054 Lambda1 -1.9126395e-05\n",
      "43 Train Loss 624.2866 Test MSE 647.7335175202725 Test RE 0.4284373810740002 Lambda1 -7.3261185e-06\n",
      "44 Train Loss 624.1989 Test MSE 647.6684358527444 Test RE 0.42841585669794413 Lambda1 -1.588405e-05\n",
      "45 Train Loss 624.10724 Test MSE 647.5397878970564 Test RE 0.4283733059433705 Lambda1 -4.753846e-05\n",
      "46 Train Loss 624.00183 Test MSE 647.4621745525975 Test RE 0.42834763301802814 Lambda1 1.1100725e-05\n",
      "47 Train Loss 623.7141 Test MSE 646.6257573979263 Test RE 0.4280708654272011 Lambda1 4.0800165e-05\n",
      "48 Train Loss 622.41034 Test MSE 645.1885011395987 Test RE 0.42759486382834033 Lambda1 9.4877956e-05\n",
      "49 Train Loss 620.1185 Test MSE 642.7096765113777 Test RE 0.42677266015644744 Lambda1 0.00012027374\n",
      "50 Train Loss 614.82574 Test MSE 633.8512453239975 Test RE 0.4238213637430661 Lambda1 0.00013195057\n",
      "51 Train Loss 604.9232 Test MSE 620.3626110762126 Test RE 0.419287560868872 Lambda1 0.00037271823\n",
      "52 Train Loss 585.4815 Test MSE 581.4473244103403 Test RE 0.40592365358477217 Lambda1 -0.0001291055\n",
      "53 Train Loss 559.2109 Test MSE 547.8711985995496 Test RE 0.39402919897394945 Lambda1 -0.00028483156\n",
      "54 Train Loss 506.55273 Test MSE 484.476237767231 Test RE 0.3705317359392857 Lambda1 -0.0005896316\n",
      "55 Train Loss 431.2308 Test MSE 399.588846964835 Test RE 0.3365083534962289 Lambda1 -0.00018078028\n",
      "56 Train Loss 365.57327 Test MSE 357.1388169009133 Test RE 0.3181322510144167 Lambda1 -0.00055143103\n",
      "57 Train Loss 335.6016 Test MSE 333.4376627671634 Test RE 0.30739478629608874 Lambda1 1.9063276e-05\n",
      "58 Train Loss 322.55576 Test MSE 323.0559769197965 Test RE 0.30257152964972084 Lambda1 -0.00038828657\n",
      "59 Train Loss 315.971 Test MSE 321.38353236900355 Test RE 0.30178731457363583 Lambda1 -0.0002813386\n",
      "60 Train Loss 309.73874 Test MSE 311.75647595807305 Test RE 0.2972329230469243 Lambda1 0.00050066685\n",
      "61 Train Loss 301.22308 Test MSE 302.8548109746678 Test RE 0.29295870630504933 Lambda1 -7.1554605e-06\n",
      "62 Train Loss 294.90295 Test MSE 297.42141627448547 Test RE 0.2903188863756012 Lambda1 -0.00012722227\n",
      "63 Train Loss 289.75464 Test MSE 293.8338185508392 Test RE 0.28856261181014375 Lambda1 -1.6268888e-05\n",
      "64 Train Loss 286.53625 Test MSE 291.6088098042389 Test RE 0.28746798896309916 Lambda1 -5.5071312e-05\n",
      "65 Train Loss 284.34735 Test MSE 290.9835108418989 Test RE 0.28715961367697695 Lambda1 -6.352877e-05\n",
      "66 Train Loss 282.7466 Test MSE 289.8357570078654 Test RE 0.2865927186731681 Lambda1 0.00021042493\n",
      "67 Train Loss 280.08023 Test MSE 288.01725256485537 Test RE 0.28569222557782925 Lambda1 -0.000102629696\n",
      "68 Train Loss 278.48203 Test MSE 286.0764769238624 Test RE 0.28472804423347403 Lambda1 -9.900963e-05\n",
      "69 Train Loss 277.48816 Test MSE 284.64537511347976 Test RE 0.28401497320033703 Lambda1 0.0002037478\n",
      "70 Train Loss 276.29422 Test MSE 283.67442370641896 Test RE 0.2835301588864355 Lambda1 -1.16163255e-05\n",
      "71 Train Loss 275.9445 Test MSE 283.5944245169519 Test RE 0.28349017681476435 Lambda1 -5.290791e-05\n",
      "72 Train Loss 275.08246 Test MSE 282.50739047541765 Test RE 0.2829463379458605 Lambda1 -5.628774e-05\n",
      "73 Train Loss 274.65283 Test MSE 281.87261338859844 Test RE 0.2826282774920447 Lambda1 -5.269784e-05\n",
      "74 Train Loss 273.68784 Test MSE 280.7553078227626 Test RE 0.2820675708397317 Lambda1 4.9077782e-05\n",
      "Training time: 311.51\n",
      "Training time: 311.51\n",
      "inv_HT_atanh_tune5\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 942.6896 Test MSE 942.9315379839808 Test RE 0.516927041385042 Lambda1 -0.18213269\n",
      "1 Train Loss 864.5569 Test MSE 866.8724244685292 Test RE 0.4956404751928423 Lambda1 -0.2020513\n",
      "2 Train Loss 855.8628 Test MSE 858.8645447001298 Test RE 0.49334588261596757 Lambda1 -0.20814647\n",
      "3 Train Loss 854.85046 Test MSE 858.0854996246102 Test RE 0.4931220837193643 Lambda1 -0.21016307\n",
      "4 Train Loss 854.7114 Test MSE 858.0672527462414 Test RE 0.49311684065994793 Lambda1 -0.21121015\n",
      "5 Train Loss 854.71063 Test MSE 858.0702911532087 Test RE 0.49311771371990526 Lambda1 -0.2112479\n",
      "6 Train Loss 854.52026 Test MSE 857.7243299441911 Test RE 0.493018294825288 Lambda1 -0.21362348\n",
      "7 Train Loss 854.44244 Test MSE 857.6009926688288 Test RE 0.4929828465366067 Lambda1 -0.20776147\n",
      "8 Train Loss 852.8154 Test MSE 856.1313881997086 Test RE 0.49256027214454595 Lambda1 -0.32219046\n",
      "9 Train Loss 844.27856 Test MSE 844.0374776506377 Test RE 0.4890688879433113 Lambda1 -0.43766034\n",
      "10 Train Loss 838.7507 Test MSE 838.8922982327025 Test RE 0.4875759484502256 Lambda1 -0.49279195\n",
      "11 Train Loss 827.84515 Test MSE 828.347312982059 Test RE 0.4845018107915718 Lambda1 -0.5664542\n",
      "12 Train Loss 816.514 Test MSE 820.561032193695 Test RE 0.4822193297104473 Lambda1 -0.60617524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Train Loss 804.2282 Test MSE 807.1931819730152 Test RE 0.47827525628982775 Lambda1 -0.6278195\n",
      "14 Train Loss 789.33826 Test MSE 792.8726027543025 Test RE 0.4740136807972934 Lambda1 -0.669757\n",
      "15 Train Loss 779.65076 Test MSE 780.8315197222129 Test RE 0.47040056932257557 Lambda1 -0.7262164\n",
      "16 Train Loss 767.10565 Test MSE 767.2138735876385 Test RE 0.46628065131955737 Lambda1 -0.7782993\n",
      "17 Train Loss 758.047 Test MSE 762.2728476064103 Test RE 0.4647767511044269 Lambda1 -0.81173384\n",
      "18 Train Loss 746.1079 Test MSE 748.1858134170615 Test RE 0.46046211630835593 Lambda1 -0.8965533\n",
      "19 Train Loss 735.2716 Test MSE 736.9615219997144 Test RE 0.456995135502677 Lambda1 -1.0003841\n",
      "20 Train Loss 727.7014 Test MSE 728.5788266582504 Test RE 0.4543886174235327 Lambda1 -1.0876882\n",
      "21 Train Loss 720.64514 Test MSE 721.7827395166829 Test RE 0.45226441371690357 Lambda1 -1.1601396\n",
      "22 Train Loss 712.8375 Test MSE 714.0954120065479 Test RE 0.44984955170729823 Lambda1 -1.276028\n",
      "23 Train Loss 704.0403 Test MSE 706.4891381354903 Test RE 0.44744732415307137 Lambda1 -1.3250505\n",
      "24 Train Loss 697.45026 Test MSE 701.4032965589129 Test RE 0.44583388358933895 Lambda1 -1.381913\n",
      "25 Train Loss 685.48517 Test MSE 688.2009605382503 Test RE 0.44161804196698695 Lambda1 -1.6078249\n",
      "26 Train Loss 676.6706 Test MSE 682.3829490457537 Test RE 0.439747373130773 Lambda1 -1.7590033\n",
      "27 Train Loss 673.8276 Test MSE 678.8620853904287 Test RE 0.4386114326169005 Lambda1 -1.9183646\n",
      "28 Train Loss 672.1152 Test MSE 675.1948291203337 Test RE 0.43742512485718354 Lambda1 -2.0202112\n",
      "29 Train Loss 669.84045 Test MSE 674.005230384877 Test RE 0.43703961407155945 Lambda1 -2.0027616\n",
      "30 Train Loss 667.1941 Test MSE 671.6985635387931 Test RE 0.43629112671711934 Lambda1 -2.071401\n",
      "31 Train Loss 665.5893 Test MSE 671.3629324682197 Test RE 0.43618211118345773 Lambda1 -2.0692024\n",
      "32 Train Loss 660.0357 Test MSE 665.3683101876195 Test RE 0.43423040230052107 Lambda1 -2.240548\n",
      "33 Train Loss 655.82965 Test MSE 662.3023130858211 Test RE 0.43322878681734794 Lambda1 -2.3199956\n",
      "34 Train Loss 653.4306 Test MSE 659.5324269076704 Test RE 0.4323219111414032 Lambda1 -2.4483123\n",
      "35 Train Loss 651.75323 Test MSE 656.3267911640995 Test RE 0.43126998813565065 Lambda1 -2.6342487\n",
      "36 Train Loss 650.2702 Test MSE 655.1075389324824 Test RE 0.4308692187814327 Lambda1 -2.7081273\n",
      "37 Train Loss 646.4025 Test MSE 655.2518857729723 Test RE 0.4309166851751743 Lambda1 -2.684299\n",
      "38 Train Loss 644.39026 Test MSE 654.7742867907912 Test RE 0.43075961360455983 Lambda1 -2.7152176\n",
      "39 Train Loss 642.73883 Test MSE 654.244258852492 Test RE 0.4305852322510469 Lambda1 -2.7620044\n",
      "40 Train Loss 638.5002 Test MSE 647.6360066552255 Test RE 0.42840513102753736 Lambda1 -2.9227304\n",
      "41 Train Loss 637.0663 Test MSE 647.6974719486166 Test RE 0.42842545990213127 Lambda1 -2.9438634\n",
      "42 Train Loss 635.3673 Test MSE 645.4529244870007 Test RE 0.4276824773573175 Lambda1 -3.0561666\n",
      "43 Train Loss 634.32825 Test MSE 644.0435536800076 Test RE 0.42721529177790457 Lambda1 -3.1512163\n",
      "44 Train Loss 632.42615 Test MSE 642.1509502956222 Test RE 0.4265871168798096 Lambda1 -3.2679315\n",
      "45 Train Loss 630.24475 Test MSE 642.0428343451657 Test RE 0.4265512041298362 Lambda1 -3.2872841\n",
      "46 Train Loss 629.1146 Test MSE 640.4926066633514 Test RE 0.42603593389907873 Lambda1 -3.3621917\n",
      "47 Train Loss 627.8964 Test MSE 639.9088752437499 Test RE 0.42584174957394105 Lambda1 -3.3687053\n",
      "48 Train Loss 627.131 Test MSE 639.6723862575408 Test RE 0.4257630538457405 Lambda1 -3.3480368\n",
      "49 Train Loss 625.69763 Test MSE 639.7576921899367 Test RE 0.4257914425211371 Lambda1 -3.2862468\n",
      "50 Train Loss 624.5902 Test MSE 639.4363103198751 Test RE 0.4256844810561783 Lambda1 -3.3142915\n",
      "51 Train Loss 623.6263 Test MSE 638.6114856619907 Test RE 0.42540984200936194 Lambda1 -3.3435836\n",
      "52 Train Loss 622.4334 Test MSE 638.2359972431218 Test RE 0.42528475816749994 Lambda1 -3.3810904\n",
      "53 Train Loss 621.4244 Test MSE 638.2426837569056 Test RE 0.42528698592099706 Lambda1 -3.4060705\n",
      "54 Train Loss 621.12427 Test MSE 638.2462712001648 Test RE 0.42528818114878886 Lambda1 -3.410912\n",
      "55 Train Loss 620.9592 Test MSE 638.3264207137593 Test RE 0.42531488367125636 Lambda1 -3.4318368\n",
      "56 Train Loss 620.5931 Test MSE 637.9860081075659 Test RE 0.4252014606237397 Lambda1 -3.452702\n",
      "57 Train Loss 620.03906 Test MSE 637.906609142989 Test RE 0.4251750011044533 Lambda1 -3.4920714\n",
      "58 Train Loss 619.5407 Test MSE 637.9345390029464 Test RE 0.42518430885270936 Lambda1 -3.521537\n",
      "59 Train Loss 619.2463 Test MSE 637.5208889824554 Test RE 0.4250464373257591 Lambda1 -3.5448246\n",
      "60 Train Loss 619.10815 Test MSE 637.0594322300375 Test RE 0.42489257879505 Lambda1 -3.564044\n",
      "61 Train Loss 619.0136 Test MSE 636.8431257094223 Test RE 0.4248204388709435 Lambda1 -3.5645278\n",
      "62 Train Loss 618.9086 Test MSE 636.8712736199611 Test RE 0.4248298271135872 Lambda1 -3.571901\n",
      "63 Train Loss 618.6101 Test MSE 636.4167629225433 Test RE 0.42467820764864284 Lambda1 -3.5673068\n",
      "64 Train Loss 618.40875 Test MSE 636.2941787281767 Test RE 0.4246373057223672 Lambda1 -3.556986\n",
      "65 Train Loss 618.2337 Test MSE 636.0036089424086 Test RE 0.4245403373194405 Lambda1 -3.5716588\n",
      "66 Train Loss 618.0656 Test MSE 635.7885243500708 Test RE 0.42446854542595336 Lambda1 -3.5956068\n",
      "67 Train Loss 617.92194 Test MSE 635.8275397039774 Test RE 0.4244815690460968 Lambda1 -3.6143098\n",
      "68 Train Loss 617.77875 Test MSE 635.9748383604987 Test RE 0.42453073484980586 Lambda1 -3.616616\n",
      "69 Train Loss 617.52795 Test MSE 635.7713364353899 Test RE 0.42446280784327217 Lambda1 -3.6329734\n",
      "70 Train Loss 617.33264 Test MSE 635.5863675201259 Test RE 0.4244010575472185 Lambda1 -3.675223\n",
      "71 Train Loss 617.1135 Test MSE 635.3770941606043 Test RE 0.4243311825847252 Lambda1 -3.7295337\n",
      "72 Train Loss 616.94775 Test MSE 635.4216787941701 Test RE 0.4243460700575264 Lambda1 -3.755737\n",
      "73 Train Loss 616.86444 Test MSE 635.6589924351238 Test RE 0.4244253038274154 Lambda1 -3.7816453\n",
      "74 Train Loss 616.737 Test MSE 635.7164964211768 Test RE 0.42444450090879116 Lambda1 -3.8184607\n",
      "Training time: 329.72\n",
      "Training time: 329.72\n",
      "inv_HT_atanh_tune5\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 1064.993 Test MSE 1063.53322463618 Test RE 0.5489903384952515 Lambda1 -0.00042473912\n",
      "1 Train Loss 875.37036 Test MSE 877.2179194205604 Test RE 0.49858925855351693 Lambda1 0.0010258908\n",
      "2 Train Loss 857.1712 Test MSE 860.0084505047913 Test RE 0.49367431242392457 Lambda1 0.0015184458\n",
      "3 Train Loss 855.01776 Test MSE 858.1942203122344 Test RE 0.493153322374462 Lambda1 0.0016930966\n",
      "4 Train Loss 854.724 Test MSE 858.0655434917599 Test RE 0.49311634951977984 Lambda1 0.0017781651\n",
      "5 Train Loss 854.7218 Test MSE 858.0756404287894 Test RE 0.4931192507842113 Lambda1 0.0017844591\n",
      "6 Train Loss 854.7217 Test MSE 858.0770081808444 Test RE 0.4931196437942026 Lambda1 0.0017851687\n",
      "7 Train Loss 854.72156 Test MSE 858.0783304438423 Test RE 0.49312002373306646 Lambda1 0.0017858247\n",
      "8 Train Loss 854.72125 Test MSE 858.0803465117 Test RE 0.4931206030291579 Lambda1 0.0017867201\n",
      "9 Train Loss 854.72076 Test MSE 858.0841679478094 Test RE 0.4931217010771369 Lambda1 0.0017880694\n",
      "10 Train Loss 854.66376 Test MSE 857.9803895553782 Test RE 0.4930918806196989 Lambda1 0.0017910219\n",
      "11 Train Loss 854.59607 Test MSE 857.7645316966045 Test RE 0.49302984863488203 Lambda1 0.000840645\n",
      "12 Train Loss 854.42896 Test MSE 857.6076458131 Test RE 0.4929847587776534 Lambda1 -0.020160824\n",
      "13 Train Loss 850.9427 Test MSE 851.6355035576016 Test RE 0.49126525531156445 Lambda1 -0.2762285\n",
      "14 Train Loss 844.193 Test MSE 844.9066805719331 Test RE 0.4893206485167519 Lambda1 -0.3625628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Train Loss 831.93823 Test MSE 831.0159769463595 Test RE 0.4852816363510886 Lambda1 -0.5986187\n",
      "16 Train Loss 802.7011 Test MSE 798.8603583206333 Test RE 0.47580018441244637 Lambda1 -0.78075296\n",
      "17 Train Loss 772.59143 Test MSE 772.2720243052194 Test RE 0.4678151929229281 Lambda1 -0.71022826\n",
      "18 Train Loss 749.5143 Test MSE 751.5874720298882 Test RE 0.46150768452111834 Lambda1 -0.7938754\n",
      "19 Train Loss 738.80756 Test MSE 736.2521738103903 Test RE 0.45677514659866447 Lambda1 -0.8684223\n",
      "20 Train Loss 711.21625 Test MSE 708.8366280401409 Test RE 0.44819008644704916 Lambda1 -0.99897355\n",
      "21 Train Loss 698.5495 Test MSE 699.8171350814024 Test RE 0.44532949133371147 Lambda1 -1.0536592\n",
      "22 Train Loss 686.9071 Test MSE 682.8426554620597 Test RE 0.43989547226919207 Lambda1 -1.134695\n",
      "23 Train Loss 676.69385 Test MSE 675.9073507432137 Test RE 0.43765586764687586 Lambda1 -1.1702254\n",
      "24 Train Loss 662.6555 Test MSE 671.7452755741733 Test RE 0.4363062969834729 Lambda1 -1.1877998\n",
      "25 Train Loss 648.464 Test MSE 659.6458631519316 Test RE 0.432359088134825 Lambda1 -1.2562864\n",
      "26 Train Loss 640.8928 Test MSE 653.7697075513834 Test RE 0.43042904299445983 Lambda1 -1.3013058\n",
      "27 Train Loss 635.39465 Test MSE 645.317300548695 Test RE 0.42763754222183126 Lambda1 -1.3677179\n",
      "28 Train Loss 632.09265 Test MSE 638.9460280867501 Test RE 0.4255212548241489 Lambda1 -1.4384221\n",
      "29 Train Loss 628.3057 Test MSE 640.2528997771392 Test RE 0.4259562036235774 Lambda1 -1.4106727\n",
      "30 Train Loss 627.44836 Test MSE 638.8814440095992 Test RE 0.42549974863204565 Lambda1 -1.4213663\n",
      "31 Train Loss 626.6201 Test MSE 637.4307371439627 Test RE 0.42501638335169295 Lambda1 -1.4466269\n",
      "32 Train Loss 622.64746 Test MSE 636.9992452130024 Test RE 0.424872507181929 Lambda1 -1.4478819\n",
      "33 Train Loss 621.5546 Test MSE 636.0252520740595 Test RE 0.42454756078807654 Lambda1 -1.4587255\n",
      "34 Train Loss 621.25653 Test MSE 636.1159224354227 Test RE 0.42457782099301655 Lambda1 -1.456454\n",
      "35 Train Loss 619.7081 Test MSE 634.4260999165161 Test RE 0.42401350696630746 Lambda1 -1.460085\n",
      "36 Train Loss 617.9375 Test MSE 632.7373968068958 Test RE 0.4234488154729437 Lambda1 -1.4843633\n",
      "37 Train Loss 617.5714 Test MSE 632.6525831270192 Test RE 0.4234204344604966 Lambda1 -1.4821928\n",
      "38 Train Loss 616.2147 Test MSE 632.4586184091195 Test RE 0.4233555213286887 Lambda1 -1.4675372\n",
      "39 Train Loss 615.53314 Test MSE 631.444685494732 Test RE 0.42301603170521734 Lambda1 -1.4895103\n",
      "40 Train Loss 615.2484 Test MSE 631.0985041188368 Test RE 0.4229000592758459 Lambda1 -1.5050776\n",
      "41 Train Loss 614.0644 Test MSE 630.7508199095428 Test RE 0.4227835513705587 Lambda1 -1.5000477\n",
      "42 Train Loss 612.7585 Test MSE 630.1067276073693 Test RE 0.4225676331539533 Lambda1 -1.4941645\n",
      "43 Train Loss 611.9762 Test MSE 629.3192286476622 Test RE 0.42230349091001435 Lambda1 -1.4860908\n",
      "44 Train Loss 611.4839 Test MSE 628.9587544949161 Test RE 0.42218252583245774 Lambda1 -1.483395\n",
      "45 Train Loss 610.646 Test MSE 627.9485986705143 Test RE 0.42184336089681107 Lambda1 -1.508794\n",
      "46 Train Loss 609.2641 Test MSE 627.3660449334848 Test RE 0.4216476414950844 Lambda1 -1.523495\n",
      "47 Train Loss 608.6154 Test MSE 627.0405271711476 Test RE 0.4215382383750855 Lambda1 -1.5391903\n",
      "48 Train Loss 608.15765 Test MSE 626.8125705896932 Test RE 0.4214616076417901 Lambda1 -1.5590094\n",
      "49 Train Loss 608.1116 Test MSE 626.7702932379183 Test RE 0.42144739399807535 Lambda1 -1.5588608\n",
      "50 Train Loss 607.86285 Test MSE 626.7393888355441 Test RE 0.4214370036361356 Lambda1 -1.5524735\n",
      "51 Train Loss 607.7145 Test MSE 626.7706173268847 Test RE 0.42144750295859407 Lambda1 -1.5542519\n",
      "52 Train Loss 607.6656 Test MSE 626.7899648408907 Test RE 0.4214540076497176 Lambda1 -1.551323\n",
      "53 Train Loss 607.6354 Test MSE 626.7871062511891 Test RE 0.42145304658977156 Lambda1 -1.5544702\n",
      "54 Train Loss 607.59576 Test MSE 626.7975034568054 Test RE 0.4214565421273851 Lambda1 -1.5543759\n",
      "55 Train Loss 607.4476 Test MSE 626.7637772126429 Test RE 0.4214452032680435 Lambda1 -1.5475898\n",
      "56 Train Loss 607.41943 Test MSE 626.7181112824071 Test RE 0.4214298497660362 Lambda1 -1.5456747\n",
      "57 Train Loss 607.32715 Test MSE 626.4715660914278 Test RE 0.4213469482817719 Lambda1 -1.5326297\n",
      "58 Train Loss 607.0893 Test MSE 626.1570547743617 Test RE 0.42124116932287925 Lambda1 -1.5247291\n",
      "59 Train Loss 606.7524 Test MSE 625.7872458982563 Test RE 0.42111675825878264 Lambda1 -1.5229092\n",
      "60 Train Loss 606.59314 Test MSE 625.5814314836148 Test RE 0.4210475022718708 Lambda1 -1.5206983\n",
      "61 Train Loss 606.3024 Test MSE 625.2837397647893 Test RE 0.42094730966433885 Lambda1 -1.5243833\n",
      "62 Train Loss 606.18414 Test MSE 625.3489996579923 Test RE 0.4209692758998151 Lambda1 -1.5228403\n",
      "63 Train Loss 606.07794 Test MSE 625.4141225221366 Test RE 0.4209911948692897 Lambda1 -1.5142367\n",
      "64 Train Loss 605.6053 Test MSE 624.9508177194576 Test RE 0.4208352315072355 Lambda1 -1.4988366\n",
      "65 Train Loss 605.4969 Test MSE 624.7085282725081 Test RE 0.4207536460310544 Lambda1 -1.4938811\n",
      "66 Train Loss 605.04047 Test MSE 624.2902995812187 Test RE 0.4206127797703445 Lambda1 -1.4883162\n",
      "67 Train Loss 604.93146 Test MSE 624.1338077094831 Test RE 0.42056005861906215 Lambda1 -1.4931229\n",
      "68 Train Loss 604.7279 Test MSE 623.8340104112027 Test RE 0.4204590402860449 Lambda1 -1.5074712\n",
      "69 Train Loss 604.5669 Test MSE 623.8039746814275 Test RE 0.4204489182456318 Lambda1 -1.5045012\n",
      "70 Train Loss 604.4731 Test MSE 623.6259279481638 Test RE 0.42038891149525787 Lambda1 -1.5023102\n",
      "71 Train Loss 604.40344 Test MSE 623.4676717455848 Test RE 0.4203355675184977 Lambda1 -1.5046678\n",
      "72 Train Loss 604.32855 Test MSE 623.3750164340075 Test RE 0.42030433272331186 Lambda1 -1.5046254\n",
      "73 Train Loss 604.17596 Test MSE 623.4517516556896 Test RE 0.4203302009028389 Lambda1 -1.4996372\n",
      "74 Train Loss 604.113 Test MSE 623.6054699031752 Test RE 0.42038201601079084 Lambda1 -1.4983606\n",
      "Training time: 334.31\n",
      "Training time: 334.31\n",
      "inv_HT_atanh_tune5\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 909.1537 Test MSE 910.0598813836062 Test RE 0.5078367847136084 Lambda1 -0.008495265\n",
      "1 Train Loss 859.8113 Test MSE 862.418921053912 Test RE 0.4943656746536588 Lambda1 -0.009314125\n",
      "2 Train Loss 855.31586 Test MSE 858.4168255133228 Test RE 0.4932172772310762 Lambda1 -0.009482119\n",
      "3 Train Loss 854.7933 Test MSE 858.0612347729628 Test RE 0.49311511144282427 Lambda1 -0.009536012\n",
      "4 Train Loss 854.7214 Test MSE 858.0734288793003 Test RE 0.49311861531673123 Lambda1 -0.009563308\n",
      "5 Train Loss 854.7212 Test MSE 858.0752523552054 Test RE 0.4931191392750579 Lambda1 -0.009564043\n",
      "6 Train Loss 854.7212 Test MSE 858.0762695182618 Test RE 0.4931194315468725 Lambda1 -0.009564508\n",
      "7 Train Loss 854.72107 Test MSE 858.0771865610063 Test RE 0.4931196950499503 Lambda1 -0.009564973\n",
      "8 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "9 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "10 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "11 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "12 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "13 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "14 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "15 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "16 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "18 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "19 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "20 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "21 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "22 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "23 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "24 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "25 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "26 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "27 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "28 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "29 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "30 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "31 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "32 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "33 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "34 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "35 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "36 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "37 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "38 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "39 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "40 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "41 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "42 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "43 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "44 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "45 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "46 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "47 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "48 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "49 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "50 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "51 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "52 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "53 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "54 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "55 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "56 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "57 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "58 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "59 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "60 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "61 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "62 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "63 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "64 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "65 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "66 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "67 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "68 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "69 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "70 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "71 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "72 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "73 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "74 Train Loss 854.721 Test MSE 858.0774526234928 Test RE 0.4931197715003233 Lambda1 -0.009565138\n",
      "Training time: 194.90\n",
      "Training time: 194.90\n",
      "inv_HT_atanh_tune6\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 30086.703 Test MSE 3591.287766058234 Test RE 1.0088213706679166 Lambda1 3.1886968e-05\n",
      "1 Train Loss 28288.543 Test MSE 3590.5216627285054 Test RE 1.0087137626616605 Lambda1 8.588861e-05\n",
      "2 Train Loss 26749.514 Test MSE 3589.3960711792856 Test RE 1.0085556395821373 Lambda1 -0.00011091224\n",
      "3 Train Loss 25821.18 Test MSE 3588.682788541033 Test RE 1.0084554248168034 Lambda1 -0.000103436585\n",
      "4 Train Loss 24684.797 Test MSE 3588.1869001657465 Test RE 1.0083857476360272 Lambda1 -0.00013434084\n",
      "5 Train Loss 23208.4 Test MSE 3587.764796129021 Test RE 1.0083264340850462 Lambda1 -0.00036175956\n",
      "6 Train Loss 22274.16 Test MSE 3587.931738401816 Test RE 1.008349893029273 Lambda1 -0.00072661054\n",
      "7 Train Loss 21328.379 Test MSE 3588.094008337509 Test RE 1.0083726948875886 Lambda1 -0.0010401616\n",
      "8 Train Loss 20632.102 Test MSE 3589.5466338901633 Test RE 1.0085767920651434 Lambda1 -0.0010572555\n",
      "9 Train Loss 19972.127 Test MSE 3590.6937234343736 Test RE 1.008737931562323 Lambda1 -0.0010938864\n",
      "10 Train Loss 19323.455 Test MSE 3591.5428395679064 Test RE 1.0088571961232167 Lambda1 -0.0010278365\n",
      "11 Train Loss 18532.266 Test MSE 3592.309388733287 Test RE 1.0089648514425147 Lambda1 -0.0007877117\n",
      "12 Train Loss 17024.918 Test MSE 3593.708267667035 Test RE 1.0091612825046241 Lambda1 -0.00041593815\n",
      "13 Train Loss 16580.016 Test MSE 3594.7466089046434 Test RE 1.0093070620195441 Lambda1 -0.0004298906\n",
      "14 Train Loss 15755.328 Test MSE 3597.5283994592296 Test RE 1.0096975120549878 Lambda1 -0.00031106253\n",
      "15 Train Loss 14292.756 Test MSE 3603.3490150474877 Test RE 1.0105140012041243 Lambda1 0.00013881242\n",
      "16 Train Loss 13578.766 Test MSE 3605.760546255685 Test RE 1.0108520864765527 Lambda1 2.386577e-05\n",
      "17 Train Loss 12896.292 Test MSE 3606.3727710676103 Test RE 1.0109378995045215 Lambda1 0.00046844003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 Train Loss 12092.123 Test MSE 3608.9007457206258 Test RE 1.0112921581804666 Lambda1 0.00039907193\n",
      "19 Train Loss 11434.633 Test MSE 3610.569044794406 Test RE 1.0115258779409932 Lambda1 0.00023261769\n",
      "20 Train Loss 11015.3955 Test MSE 3613.02381656687 Test RE 1.0118696801554368 Lambda1 -0.00014382368\n",
      "21 Train Loss 10227.116 Test MSE 3615.715896551468 Test RE 1.0122465842415616 Lambda1 -0.00016570621\n",
      "22 Train Loss 9702.736 Test MSE 3613.4500804340028 Test RE 1.0119293684918107 Lambda1 -4.5080633e-05\n",
      "23 Train Loss 9186.841 Test MSE 3610.8040785262897 Test RE 1.0115588005676126 Lambda1 0.00040610685\n",
      "24 Train Loss 8802.332 Test MSE 3610.2143740699807 Test RE 1.011476194993207 Lambda1 0.00034677406\n",
      "25 Train Loss 8208.673 Test MSE 3610.027858064352 Test RE 1.0114500664982127 Lambda1 -0.0005147859\n",
      "26 Train Loss 7655.599 Test MSE 3609.619199038025 Test RE 1.0113928162616705 Lambda1 0.00022294531\n",
      "27 Train Loss 7376.7974 Test MSE 3612.3473318372185 Test RE 1.0117749469809476 Lambda1 -0.00020480908\n",
      "28 Train Loss 7186.125 Test MSE 3612.372242713368 Test RE 1.0117784355930088 Lambda1 -0.0004586658\n",
      "29 Train Loss 6841.9746 Test MSE 3614.975478521603 Test RE 1.01214293616515 Lambda1 4.8642134e-05\n",
      "30 Train Loss 6495.001 Test MSE 3619.186238531391 Test RE 1.012732241773715 Lambda1 0.00013884259\n",
      "31 Train Loss 6309.3 Test MSE 3620.952914667471 Test RE 1.0129793900933914 Lambda1 6.0226557e-05\n",
      "32 Train Loss 6052.83 Test MSE 3624.254844333797 Test RE 1.013441150381643 Lambda1 -0.00027451912\n",
      "33 Train Loss 5624.041 Test MSE 3625.736414062004 Test RE 1.013648273000855 Lambda1 -3.5356446e-05\n",
      "34 Train Loss 5443.725 Test MSE 3629.5585879237583 Test RE 1.0141824154436558 Lambda1 0.00028637398\n",
      "35 Train Loss 5234.275 Test MSE 3631.4619403826996 Test RE 1.0144483008872716 Lambda1 0.00036306216\n",
      "36 Train Loss 5119.534 Test MSE 3630.370836768939 Test RE 1.0142958896298044 Lambda1 -0.00010306068\n",
      "37 Train Loss 4923.5044 Test MSE 3631.168933024302 Test RE 1.0144073742828112 Lambda1 0.00015030667\n",
      "38 Train Loss 4772.5776 Test MSE 3631.4020190935066 Test RE 1.0144399313515633 Lambda1 -0.00021624035\n",
      "39 Train Loss 4673.9375 Test MSE 3632.6529292827145 Test RE 1.0146146385277048 Lambda1 0.0006463406\n",
      "40 Train Loss 4540.842 Test MSE 3633.4103918936476 Test RE 1.0147204141976742 Lambda1 0.00025374477\n",
      "41 Train Loss 4432.9395 Test MSE 3634.682946103175 Test RE 1.014898094882081 Lambda1 -3.8815797e-05\n",
      "42 Train Loss 4316.6313 Test MSE 3633.873598305581 Test RE 1.014785093104282 Lambda1 -1.8724488e-05\n",
      "43 Train Loss 4230.718 Test MSE 3635.5394132300653 Test RE 1.0150176617973727 Lambda1 0.0005047775\n",
      "44 Train Loss 4162.978 Test MSE 3636.842632508076 Test RE 1.0151995704374994 Lambda1 0.0004583466\n",
      "45 Train Loss 4104.1846 Test MSE 3636.6097595194474 Test RE 1.0151670674718085 Lambda1 -0.0005230633\n",
      "46 Train Loss 4065.17 Test MSE 3635.2983439510003 Test RE 1.0149840087950845 Lambda1 -0.00046443797\n",
      "47 Train Loss 3986.1667 Test MSE 3628.594245977424 Test RE 1.0140476767981519 Lambda1 0.00041871227\n",
      "48 Train Loss 3951.6702 Test MSE 3625.777027708126 Test RE 1.0136539501698858 Lambda1 2.9401883e-05\n",
      "49 Train Loss 3923.6968 Test MSE 3624.056614709117 Test RE 1.0134134347801516 Lambda1 0.0002787386\n",
      "50 Train Loss 3893.3416 Test MSE 3621.072012762729 Test RE 1.0129960490954164 Lambda1 0.0009422941\n",
      "51 Train Loss 3873.1697 Test MSE 3617.7789542841747 Test RE 1.012535327245506 Lambda1 0.00048094033\n",
      "52 Train Loss 3847.7686 Test MSE 3615.5822594741812 Test RE 1.0122278777213545 Lambda1 0.00031128406\n",
      "53 Train Loss 3821.1052 Test MSE 3613.5903071643434 Test RE 1.0119490032130436 Lambda1 0.00028898573\n",
      "54 Train Loss 3802.722 Test MSE 3610.4166915905034 Test RE 1.0115045363142066 Lambda1 0.00044676726\n",
      "55 Train Loss 3779.9058 Test MSE 3604.4220578675413 Test RE 1.0106644506992553 Lambda1 -0.00019568465\n",
      "56 Train Loss 3753.3015 Test MSE 3598.830900353759 Test RE 1.00988027821426 Lambda1 -0.0001451155\n",
      "57 Train Loss 3740.892 Test MSE 3597.78496986154 Test RE 1.009733516479326 Lambda1 0.0001930541\n",
      "58 Train Loss 3726.7976 Test MSE 3591.5247505062007 Test RE 1.0088546555294013 Lambda1 0.00058260286\n",
      "59 Train Loss 3707.6353 Test MSE 3580.4545483802854 Test RE 1.007298652855354 Lambda1 0.0001265795\n",
      "60 Train Loss 3689.9675 Test MSE 3575.446042506744 Test RE 1.0065938782656296 Lambda1 0.00027223717\n",
      "61 Train Loss 3675.741 Test MSE 3565.4476764456726 Test RE 1.00518547498602 Lambda1 -0.00027128274\n",
      "62 Train Loss 3652.039 Test MSE 3554.133814118618 Test RE 1.0035893828164533 Lambda1 -0.00018070455\n",
      "63 Train Loss 3638.0273 Test MSE 3542.8735324171475 Test RE 1.001998325087605 Lambda1 0.00015516563\n",
      "64 Train Loss 3626.8174 Test MSE 3530.5954129997676 Test RE 1.0002605644175961 Lambda1 0.0008766046\n",
      "65 Train Loss 3612.0051 Test MSE 3522.3492400204873 Test RE 0.9990917610618412 Lambda1 0.000959991\n",
      "66 Train Loss 3603.4707 Test MSE 3514.821334829246 Test RE 0.9980235690234395 Lambda1 0.0010504018\n",
      "67 Train Loss 3578.686 Test MSE 3494.4584173777407 Test RE 0.9951283732587078 Lambda1 0.0001739061\n",
      "68 Train Loss 3554.9348 Test MSE 3473.585328722149 Test RE 0.9921518728715544 Lambda1 -0.0002488249\n",
      "69 Train Loss 3523.193 Test MSE 3442.6166862496966 Test RE 0.9877192213050108 Lambda1 -0.000862215\n",
      "70 Train Loss 3500.979 Test MSE 3424.7400241547048 Test RE 0.9851513918952288 Lambda1 -0.0006782203\n",
      "71 Train Loss 3465.4766 Test MSE 3386.2299860315015 Test RE 0.9795968867568252 Lambda1 0.00022819567\n",
      "72 Train Loss 3413.5388 Test MSE 3345.883856902389 Test RE 0.97374356670954 Lambda1 0.0003390521\n",
      "73 Train Loss 3383.3843 Test MSE 3335.1467597232604 Test RE 0.9721799170095431 Lambda1 0.00016343205\n",
      "74 Train Loss 3348.082 Test MSE 3288.8376762440303 Test RE 0.9654068813790195 Lambda1 -0.00041280373\n",
      "Training time: 252.96\n",
      "Training time: 252.96\n",
      "inv_HT_atanh_tune6\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 15687.087 Test MSE 3575.8397160774775 Test RE 1.0066492921209138 Lambda1 -6.3395615e-05\n",
      "1 Train Loss 12053.142 Test MSE 3578.5915275830953 Test RE 1.0070365544970303 Lambda1 -0.0003505352\n",
      "2 Train Loss 7675.2637 Test MSE 3590.4259424560532 Test RE 1.008700316843924 Lambda1 7.875093e-05\n",
      "3 Train Loss 5309.4844 Test MSE 3567.7988179737345 Test RE 1.0055168420466032 Lambda1 -0.00063254713\n",
      "4 Train Loss 4257.9106 Test MSE 3556.656612590444 Test RE 1.003945503992865 Lambda1 -0.00019104141\n",
      "5 Train Loss 3890.3293 Test MSE 3521.39251253054 Test RE 0.9989560670401588 Lambda1 -4.0065162e-05\n",
      "6 Train Loss 3653.1055 Test MSE 3499.0099166127593 Test RE 0.9957762350440537 Lambda1 0.0009606934\n",
      "7 Train Loss 3520.501 Test MSE 3453.2309153974265 Test RE 0.9892407107124971 Lambda1 -0.00085136143\n",
      "8 Train Loss 3419.532 Test MSE 3376.521142703057 Test RE 0.9781915510698165 Lambda1 -0.0034272943\n",
      "9 Train Loss 3010.5894 Test MSE 2997.7423736746077 Test RE 0.9216930740110821 Lambda1 -0.01881752\n",
      "10 Train Loss 1034.0006 Test MSE 1032.8901138159395 Test RE 0.541023626245811 Lambda1 -0.20590991\n",
      "11 Train Loss 854.16284 Test MSE 856.7247082484685 Test RE 0.49273092075809966 Lambda1 -0.28061768\n",
      "12 Train Loss 851.5343 Test MSE 853.9390092096196 Test RE 0.49192919435018495 Lambda1 -0.2665346\n",
      "13 Train Loss 843.5052 Test MSE 843.4468402410149 Test RE 0.48889773859200747 Lambda1 -0.24428481\n",
      "14 Train Loss 833.61414 Test MSE 832.1746004727435 Test RE 0.4856198144932985 Lambda1 -0.23571077\n",
      "15 Train Loss 824.3841 Test MSE 820.2175780089004 Test RE 0.48211840023998875 Lambda1 -0.23216057\n",
      "16 Train Loss 815.2836 Test MSE 815.027611356417 Test RE 0.4805906658781513 Lambda1 -0.2277247\n",
      "17 Train Loss 810.9821 Test MSE 811.6306123871686 Test RE 0.4795880798038396 Lambda1 -0.22340705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 Train Loss 807.4333 Test MSE 809.4382195053444 Test RE 0.47893990532437775 Lambda1 -0.2202023\n",
      "19 Train Loss 804.62805 Test MSE 806.8129515154775 Test RE 0.4781625966165103 Lambda1 -0.2215046\n",
      "20 Train Loss 803.39484 Test MSE 804.1035108006491 Test RE 0.477359038172389 Lambda1 -0.2184264\n",
      "21 Train Loss 801.65826 Test MSE 802.5998300677485 Test RE 0.4769124964872111 Lambda1 -0.22106542\n",
      "22 Train Loss 799.2843 Test MSE 800.4132233049182 Test RE 0.4762624020776604 Lambda1 -0.22038397\n",
      "23 Train Loss 795.8741 Test MSE 798.3552556012984 Test RE 0.4756497413687366 Lambda1 -0.2247053\n",
      "24 Train Loss 793.38403 Test MSE 795.7769322101661 Test RE 0.4748810543658209 Lambda1 -0.22911659\n",
      "25 Train Loss 792.7492 Test MSE 795.225631566545 Test RE 0.47471653113132095 Lambda1 -0.23005697\n",
      "26 Train Loss 791.8665 Test MSE 794.2585548629022 Test RE 0.47442779109376626 Lambda1 -0.23270258\n",
      "27 Train Loss 791.05444 Test MSE 794.3010725520736 Test RE 0.47444048929090055 Lambda1 -0.233492\n",
      "28 Train Loss 790.0739 Test MSE 792.7630010866689 Test RE 0.4739809173462405 Lambda1 -0.23481958\n",
      "29 Train Loss 789.5437 Test MSE 792.7522800499878 Test RE 0.4739777123631784 Lambda1 -0.2343628\n",
      "30 Train Loss 789.041 Test MSE 792.2528421010741 Test RE 0.47382838466126737 Lambda1 -0.23097295\n",
      "31 Train Loss 787.5513 Test MSE 790.9206139418033 Test RE 0.47342982937807054 Lambda1 -0.22795378\n",
      "32 Train Loss 786.4093 Test MSE 789.5939769143935 Test RE 0.47303261306965233 Lambda1 -0.22829302\n",
      "33 Train Loss 785.2357 Test MSE 788.8095061379561 Test RE 0.47279757298192576 Lambda1 -0.22784308\n",
      "34 Train Loss 784.29205 Test MSE 788.0572985499854 Test RE 0.47257208967793646 Lambda1 -0.22796965\n",
      "35 Train Loss 783.58545 Test MSE 787.1278719147555 Test RE 0.4722933341435051 Lambda1 -0.22799088\n",
      "36 Train Loss 783.0849 Test MSE 786.2462686645821 Test RE 0.472028769765621 Lambda1 -0.22609319\n",
      "37 Train Loss 782.61676 Test MSE 785.9953258447922 Test RE 0.4719534360640773 Lambda1 -0.22553147\n",
      "38 Train Loss 782.323 Test MSE 785.7807048970967 Test RE 0.47188899674372037 Lambda1 -0.2260929\n",
      "39 Train Loss 782.1449 Test MSE 785.7156887807081 Test RE 0.47186947410566127 Lambda1 -0.22483443\n",
      "40 Train Loss 781.56934 Test MSE 784.6231310379366 Test RE 0.47154128669697754 Lambda1 -0.22551689\n",
      "41 Train Loss 781.29236 Test MSE 783.8896769786764 Test RE 0.47132084026880217 Lambda1 -0.22484092\n",
      "42 Train Loss 780.6747 Test MSE 783.2912900345245 Test RE 0.4711409131126501 Lambda1 -0.21760637\n",
      "43 Train Loss 779.90826 Test MSE 783.018722638217 Test RE 0.47105893286650274 Lambda1 -0.21241997\n",
      "44 Train Loss 779.27985 Test MSE 782.1876575557105 Test RE 0.47080888484704914 Lambda1 -0.2057466\n",
      "45 Train Loss 778.88574 Test MSE 781.9299309944416 Test RE 0.47073131397860457 Lambda1 -0.20386021\n",
      "46 Train Loss 777.8228 Test MSE 781.4880376097962 Test RE 0.47059828258899694 Lambda1 -0.20436014\n",
      "47 Train Loss 776.8349 Test MSE 780.5413120066673 Test RE 0.4703131454943362 Lambda1 -0.20201586\n",
      "48 Train Loss 775.9964 Test MSE 780.1802142125883 Test RE 0.4702043436390858 Lambda1 -0.20006633\n",
      "49 Train Loss 775.21045 Test MSE 779.6493069473415 Test RE 0.4700443310030279 Lambda1 -0.19628005\n",
      "50 Train Loss 774.2837 Test MSE 778.0216449627815 Test RE 0.46955342245790843 Lambda1 -0.19056082\n",
      "51 Train Loss 772.85284 Test MSE 775.9157339722907 Test RE 0.46891750971633167 Lambda1 -0.18471198\n",
      "52 Train Loss 771.3607 Test MSE 774.1440053012354 Test RE 0.46838183982205095 Lambda1 -0.17916575\n",
      "53 Train Loss 769.55176 Test MSE 770.8025225433512 Test RE 0.4673698947847242 Lambda1 -0.17168863\n",
      "54 Train Loss 767.628 Test MSE 766.709246515089 Test RE 0.46612728043297685 Lambda1 -0.16912073\n",
      "55 Train Loss 765.6906 Test MSE 760.0212998643035 Test RE 0.4640898311017673 Lambda1 -0.16634142\n",
      "56 Train Loss 764.016 Test MSE 759.282717377455 Test RE 0.463864276938311 Lambda1 -0.16313794\n",
      "57 Train Loss 760.4492 Test MSE 752.5075895737473 Test RE 0.4617900943852771 Lambda1 -0.14371884\n",
      "58 Train Loss 756.23724 Test MSE 746.9066335168583 Test RE 0.46006831984133556 Lambda1 -0.13463603\n",
      "59 Train Loss 752.5077 Test MSE 746.195404576177 Test RE 0.45984922161531133 Lambda1 -0.12530531\n",
      "60 Train Loss 745.6489 Test MSE 743.1665753000723 Test RE 0.4589150018310669 Lambda1 -0.1120409\n",
      "61 Train Loss 731.97546 Test MSE 721.8535737067577 Test RE 0.4522866052959961 Lambda1 -0.07063516\n",
      "62 Train Loss 709.0267 Test MSE 701.5320730296257 Test RE 0.44587480888819886 Lambda1 -0.004164418\n",
      "63 Train Loss 686.77625 Test MSE 674.7139646538533 Test RE 0.43726933303661336 Lambda1 -0.00044166206\n",
      "64 Train Loss 655.5285 Test MSE 628.7465408232068 Test RE 0.422111296628761 Lambda1 -0.0020102374\n",
      "65 Train Loss 632.02716 Test MSE 611.5725966857714 Test RE 0.416306488020039 Lambda1 -0.00055369345\n",
      "66 Train Loss 607.3412 Test MSE 590.4504956140116 Test RE 0.4090542566960175 Lambda1 -8.7571585e-05\n",
      "67 Train Loss 582.45966 Test MSE 568.5788438157361 Test RE 0.4014066089601787 Lambda1 -0.00025859303\n",
      "68 Train Loss 569.78937 Test MSE 557.256137537119 Test RE 0.3973896949395883 Lambda1 -0.00047478973\n",
      "69 Train Loss 555.4896 Test MSE 542.4426424664167 Test RE 0.3920722293308946 Lambda1 -0.00014343242\n",
      "70 Train Loss 543.9413 Test MSE 532.6998369701998 Test RE 0.38853527336837307 Lambda1 -4.2874995e-05\n",
      "71 Train Loss 528.40375 Test MSE 525.0571793878494 Test RE 0.38573804160168 Lambda1 -8.6936496e-05\n",
      "72 Train Loss 523.344 Test MSE 523.7855318414221 Test RE 0.3852706447012552 Lambda1 -5.8717535e-05\n",
      "73 Train Loss 520.4956 Test MSE 521.1374983943413 Test RE 0.38429552970119707 Lambda1 -4.7203797e-05\n",
      "74 Train Loss 517.9083 Test MSE 519.5165460940846 Test RE 0.38369740553273485 Lambda1 -2.5848365e-05\n",
      "Training time: 260.68\n",
      "Training time: 260.68\n",
      "inv_HT_atanh_tune6\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 6674.993 Test MSE 3519.7067373340024 Test RE 0.9987169262816148 Lambda1 -0.00035392767\n",
      "1 Train Loss 4850.6465 Test MSE 3518.968302520447 Test RE 0.9986121553322744 Lambda1 0.0009266208\n",
      "2 Train Loss 4357.7354 Test MSE 3520.6925511377117 Test RE 0.9988567788426275 Lambda1 7.921137e-05\n",
      "3 Train Loss 3913.6492 Test MSE 3511.7896873272316 Test RE 0.9975930623084787 Lambda1 -0.000450058\n",
      "4 Train Loss 3682.5022 Test MSE 3513.3206593074488 Test RE 0.9978104899938341 Lambda1 0.00031167723\n",
      "5 Train Loss 3579.3667 Test MSE 3507.653624976916 Test RE 0.9970054242257157 Lambda1 -6.6590495e-05\n",
      "6 Train Loss 3511.565 Test MSE 3463.434637633728 Test RE 0.9907011534404929 Lambda1 0.0004349301\n",
      "7 Train Loss 3381.298 Test MSE 3318.632803564417 Test RE 0.9697700591348726 Lambda1 -0.0013194713\n",
      "8 Train Loss 3112.1025 Test MSE 3074.864784083058 Test RE 0.9334739051330766 Lambda1 0.0010129431\n",
      "9 Train Loss 2695.8496 Test MSE 2670.5283741212634 Test RE 0.8699369326904154 Lambda1 -0.04657274\n",
      "10 Train Loss 884.97485 Test MSE 886.4985910960967 Test RE 0.5012197730199296 Lambda1 -0.56614137\n",
      "11 Train Loss 857.28143 Test MSE 860.1070935265853 Test RE 0.4937026238490094 Lambda1 -0.61987823\n",
      "12 Train Loss 855.00885 Test MSE 858.1883185697203 Test RE 0.4931516266808396 Lambda1 -0.6346712\n",
      "13 Train Loss 854.7563 Test MSE 858.0527809685001 Test RE 0.4931126822985927 Lambda1 -0.639403\n",
      "14 Train Loss 854.7216 Test MSE 858.0769271128844 Test RE 0.49311962050013686 Lambda1 -0.64175737\n",
      "15 Train Loss 854.71796 Test MSE 858.0778929706996 Test RE 0.49311989802962763 Lambda1 -0.64278126\n",
      "16 Train Loss 854.6656 Test MSE 857.9604856251506 Test RE 0.49308616106979536 Lambda1 -0.64567995\n",
      "17 Train Loss 854.4561 Test MSE 857.644725084032 Test RE 0.49299541593419094 Lambda1 -0.6415673\n",
      "18 Train Loss 853.73065 Test MSE 856.5330702003896 Test RE 0.4926758089631283 Lambda1 -0.6253438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 Train Loss 852.4377 Test MSE 853.45764199237 Test RE 0.4917905240439989 Lambda1 -0.62032837\n",
      "20 Train Loss 851.0582 Test MSE 852.0701795708414 Test RE 0.49139061057255523 Lambda1 -0.6179081\n",
      "21 Train Loss 848.68744 Test MSE 847.9054336258707 Test RE 0.4901882307850104 Lambda1 -0.59201735\n",
      "22 Train Loss 845.95166 Test MSE 846.8554019375168 Test RE 0.48988461636828184 Lambda1 -0.5725255\n",
      "23 Train Loss 843.8881 Test MSE 844.9248725687521 Test RE 0.48932591635991723 Lambda1 -0.5552362\n",
      "24 Train Loss 841.504 Test MSE 842.76370218982 Test RE 0.4886997105205307 Lambda1 -0.5320708\n",
      "25 Train Loss 838.41925 Test MSE 838.0173035297629 Test RE 0.48732160252419077 Lambda1 -0.5205338\n",
      "26 Train Loss 833.3377 Test MSE 832.6151022198752 Test RE 0.4857483260313643 Lambda1 -0.52275336\n",
      "27 Train Loss 829.4266 Test MSE 828.673330497329 Test RE 0.48459714552324495 Lambda1 -0.51727134\n",
      "28 Train Loss 826.9102 Test MSE 824.802888246785 Test RE 0.483464129411595 Lambda1 -0.5163887\n",
      "29 Train Loss 825.57715 Test MSE 824.4086703089731 Test RE 0.4833485787669443 Lambda1 -0.51424307\n",
      "30 Train Loss 823.9432 Test MSE 822.3199658678559 Test RE 0.48273588955794083 Lambda1 -0.50646687\n",
      "31 Train Loss 822.55927 Test MSE 820.3385904309072 Test RE 0.4821539640739748 Lambda1 -0.500144\n",
      "32 Train Loss 820.97284 Test MSE 818.3097880994516 Test RE 0.48155738075624716 Lambda1 -0.4911396\n",
      "33 Train Loss 818.17206 Test MSE 816.509003328098 Test RE 0.4810272277389791 Lambda1 -0.47986335\n",
      "34 Train Loss 815.4237 Test MSE 814.0456545972036 Test RE 0.4803010674181532 Lambda1 -0.47734383\n",
      "35 Train Loss 810.30927 Test MSE 809.3835733623106 Test RE 0.4789237381483756 Lambda1 -0.46180457\n",
      "36 Train Loss 807.30554 Test MSE 807.0933722677412 Test RE 0.47824568592849026 Lambda1 -0.46020633\n",
      "37 Train Loss 803.46625 Test MSE 800.8074115043224 Test RE 0.476379662702998 Lambda1 -0.47186384\n",
      "38 Train Loss 801.6654 Test MSE 798.5893160927676 Test RE 0.47571946136663246 Lambda1 -0.4783591\n",
      "39 Train Loss 797.69965 Test MSE 795.8544520370083 Test RE 0.47490418383787547 Lambda1 -0.48759758\n",
      "40 Train Loss 794.48364 Test MSE 793.6746139070076 Test RE 0.4742533587539471 Lambda1 -0.489151\n",
      "41 Train Loss 791.1787 Test MSE 790.8142448357427 Test RE 0.4733979930596696 Lambda1 -0.4765729\n",
      "42 Train Loss 789.05963 Test MSE 788.8679946945301 Test RE 0.47281510112705577 Lambda1 -0.46897432\n",
      "43 Train Loss 784.81854 Test MSE 783.0986732553855 Test RE 0.47108298113636154 Lambda1 -0.47354394\n",
      "44 Train Loss 781.8003 Test MSE 782.622159557643 Test RE 0.4709396328841856 Lambda1 -0.47568253\n",
      "45 Train Loss 779.42566 Test MSE 782.0993947431173 Test RE 0.47078232083244903 Lambda1 -0.48046872\n",
      "46 Train Loss 776.96533 Test MSE 780.81377100655 Test RE 0.47039522306465276 Lambda1 -0.4895503\n",
      "47 Train Loss 775.6011 Test MSE 780.3319873294046 Test RE 0.47025007724465384 Lambda1 -0.49074665\n",
      "48 Train Loss 774.7552 Test MSE 779.2400802474846 Test RE 0.469920955059301 Lambda1 -0.49609175\n",
      "49 Train Loss 773.77997 Test MSE 778.7094502001805 Test RE 0.4697609295104703 Lambda1 -0.502291\n",
      "50 Train Loss 773.17316 Test MSE 778.0804637852008 Test RE 0.46957117135832505 Lambda1 -0.5047389\n",
      "51 Train Loss 772.38995 Test MSE 777.0351007454751 Test RE 0.46925562730189796 Lambda1 -0.5132585\n",
      "52 Train Loss 770.864 Test MSE 776.3737349691916 Test RE 0.46905588363664963 Lambda1 -0.5174271\n",
      "53 Train Loss 769.83765 Test MSE 775.6449359102904 Test RE 0.4688356754213431 Lambda1 -0.53014314\n",
      "54 Train Loss 768.33295 Test MSE 774.0312164858443 Test RE 0.4683477181619701 Lambda1 -0.5357906\n",
      "55 Train Loss 767.47437 Test MSE 773.0766083471947 Test RE 0.4680588238209308 Lambda1 -0.540413\n",
      "56 Train Loss 766.80536 Test MSE 771.991478710331 Test RE 0.46773021288420075 Lambda1 -0.5485454\n",
      "57 Train Loss 764.5382 Test MSE 771.3021770474644 Test RE 0.4675213509842249 Lambda1 -0.5578253\n",
      "58 Train Loss 764.0988 Test MSE 770.8001919527752 Test RE 0.46736918821678775 Lambda1 -0.565839\n",
      "59 Train Loss 763.34015 Test MSE 770.0850212944823 Test RE 0.4671523185916333 Lambda1 -0.5687841\n",
      "60 Train Loss 762.7267 Test MSE 769.1105635283681 Test RE 0.4668566601311001 Lambda1 -0.5717459\n",
      "61 Train Loss 762.2405 Test MSE 768.2859997220362 Test RE 0.466606334394206 Lambda1 -0.57401633\n",
      "62 Train Loss 761.5277 Test MSE 766.9686767470563 Test RE 0.4662061351495822 Lambda1 -0.578422\n",
      "63 Train Loss 760.91504 Test MSE 766.7614808013028 Test RE 0.46614315829750735 Lambda1 -0.5796577\n",
      "64 Train Loss 760.4461 Test MSE 766.5817102148421 Test RE 0.46608851044197036 Lambda1 -0.57839364\n",
      "65 Train Loss 760.2466 Test MSE 766.1492341042317 Test RE 0.46595701722907606 Lambda1 -0.5806678\n",
      "66 Train Loss 760.08984 Test MSE 765.8542209529916 Test RE 0.4658672979698701 Lambda1 -0.583937\n",
      "67 Train Loss 759.67554 Test MSE 765.9547434929945 Test RE 0.46589787077748634 Lambda1 -0.5876906\n",
      "68 Train Loss 759.54706 Test MSE 765.9346422071654 Test RE 0.46589175735643157 Lambda1 -0.58891785\n",
      "69 Train Loss 759.397 Test MSE 765.7722738157348 Test RE 0.46584237317893046 Lambda1 -0.5910595\n",
      "70 Train Loss 759.087 Test MSE 765.2612871733122 Test RE 0.46568692294820707 Lambda1 -0.59678245\n",
      "71 Train Loss 758.8969 Test MSE 765.0924192028498 Test RE 0.4656355392276146 Lambda1 -0.5997003\n",
      "72 Train Loss 758.78235 Test MSE 764.9621407018952 Test RE 0.4655958937670762 Lambda1 -0.6019476\n",
      "73 Train Loss 758.64795 Test MSE 765.047370496469 Test RE 0.46562183069584695 Lambda1 -0.60183233\n",
      "74 Train Loss 758.4772 Test MSE 764.8706026279696 Test RE 0.46556803550837345 Lambda1 -0.6055181\n",
      "Training time: 265.92\n",
      "Training time: 265.92\n",
      "inv_HT_atanh_tune6\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 21007.115 Test MSE 3516.446208761571 Test RE 0.9982542315450275 Lambda1 -0.0003875978\n",
      "1 Train Loss 15231.959 Test MSE 3504.9355922670097 Test RE 0.9966190664473998 Lambda1 -0.00012491993\n",
      "2 Train Loss 11961.349 Test MSE 3490.6045141926907 Test RE 0.9945794775592506 Lambda1 -0.00023504648\n",
      "3 Train Loss 9614.483 Test MSE 3476.9469480701273 Test RE 0.9926318424237673 Lambda1 -0.00017420802\n",
      "4 Train Loss 7667.998 Test MSE 3470.3640341493824 Test RE 0.9916917208304411 Lambda1 -0.0006729893\n",
      "5 Train Loss 5823.484 Test MSE 3464.952775372496 Test RE 0.9909182581626604 Lambda1 0.0002218842\n",
      "6 Train Loss 4657.4556 Test MSE 3461.945212103262 Test RE 0.9904881085270292 Lambda1 -0.0011408258\n",
      "7 Train Loss 3915.7031 Test MSE 3460.9357788175666 Test RE 0.9903436948379212 Lambda1 0.0010476805\n",
      "8 Train Loss 3648.1077 Test MSE 3459.791331109474 Test RE 0.990179939952477 Lambda1 -0.001578976\n",
      "9 Train Loss 3535.968 Test MSE 3447.401717882636 Test RE 0.9884054183147103 Lambda1 -0.0003471056\n",
      "10 Train Loss 3458.63 Test MSE 3418.999819027294 Test RE 0.9843254397656342 Lambda1 0.001750771\n",
      "11 Train Loss 3367.599 Test MSE 3335.6467007282663 Test RE 0.9722527795290921 Lambda1 0.0014405533\n",
      "12 Train Loss 2792.0596 Test MSE 2779.764549848976 Test RE 0.8875507119010924 Lambda1 -0.04287903\n",
      "13 Train Loss 895.8352 Test MSE 897.0611479144723 Test RE 0.5041969270520159 Lambda1 -0.4413994\n",
      "14 Train Loss 859.4051 Test MSE 862.0433731121078 Test RE 0.49425802498185234 Lambda1 -0.4829692\n",
      "15 Train Loss 855.2848 Test MSE 858.3926482535381 Test RE 0.4932103314642571 Lambda1 -0.49663672\n",
      "16 Train Loss 854.78955 Test MSE 858.0599591552457 Test RE 0.49311474490336243 Lambda1 -0.50135106\n",
      "17 Train Loss 854.7217 Test MSE 858.0722922111571 Test RE 0.4931182887057517 Lambda1 -0.5036878\n",
      "18 Train Loss 854.7214 Test MSE 858.0748684323721 Test RE 0.4931190289585572 Lambda1 -0.5037731\n",
      "19 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "21 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "22 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "23 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "24 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "25 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "26 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "27 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "28 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "29 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "30 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "31 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "32 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "33 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "34 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "35 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "36 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "37 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "38 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "39 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "40 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "41 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "42 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "43 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "44 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "45 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "46 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "47 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "48 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "49 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "50 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "51 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "52 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "53 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "54 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "55 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "56 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "57 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "58 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "59 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "60 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "61 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "62 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "63 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "64 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "65 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "66 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "67 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "68 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "69 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "70 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "71 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "72 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "73 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "74 Train Loss 854.7212 Test MSE 858.0764640106459 Test RE 0.49311948743232914 Lambda1 -0.50382894\n",
      "Training time: 201.97\n",
      "Training time: 201.97\n",
      "inv_HT_atanh_tune6\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 9426.314 Test MSE 3586.942731852535 Test RE 1.008210908587276 Lambda1 -1.6047396e-05\n",
      "1 Train Loss 8906.877 Test MSE 3585.4860865439973 Test RE 1.0080061722799185 Lambda1 -0.0002968232\n",
      "2 Train Loss 7959.109 Test MSE 3583.936117604494 Test RE 1.0077882733525925 Lambda1 -0.0007320702\n",
      "3 Train Loss 7199.581 Test MSE 3583.417205203378 Test RE 1.0077153126822058 Lambda1 -0.0012396532\n",
      "4 Train Loss 6963.583 Test MSE 3582.7625768745966 Test RE 1.0076232624011494 Lambda1 -0.0015564539\n",
      "5 Train Loss 5887.921 Test MSE 3575.009197835181 Test RE 1.0065323840353932 Lambda1 -0.0011687044\n",
      "6 Train Loss 5061.7373 Test MSE 3571.5032426556295 Test RE 1.0060387177497878 Lambda1 -0.00072440016\n",
      "7 Train Loss 4601.582 Test MSE 3567.662833283619 Test RE 1.0054976795032444 Lambda1 -0.00069990184\n",
      "8 Train Loss 4363.4785 Test MSE 3563.6719305981455 Test RE 1.004935131074615 Lambda1 -4.5512847e-05\n",
      "9 Train Loss 4098.647 Test MSE 3552.460456649807 Test RE 1.0033530999985742 Lambda1 1.9909567e-05\n",
      "10 Train Loss 3897.6626 Test MSE 3546.375821836645 Test RE 1.0024934628982585 Lambda1 -5.2568193e-05\n",
      "11 Train Loss 3807.658 Test MSE 3540.4708491634956 Test RE 1.0016585030638403 Lambda1 -0.0004396269\n",
      "12 Train Loss 3742.6284 Test MSE 3536.494354628204 Test RE 1.0010958365757074 Lambda1 -0.00019638412\n",
      "13 Train Loss 3687.401 Test MSE 3533.2164476586954 Test RE 1.0006317810121288 Lambda1 -9.060015e-05\n",
      "14 Train Loss 3633.513 Test MSE 3525.3157022382593 Test RE 0.9995123815076474 Lambda1 -2.0927037e-05\n",
      "15 Train Loss 3584.5696 Test MSE 3517.0293041934947 Test RE 0.9983369931344 Lambda1 -1.3012963e-05\n",
      "16 Train Loss 3548.1619 Test MSE 3487.803574998825 Test RE 0.994180361060736 Lambda1 -0.00018611434\n",
      "17 Train Loss 3499.5276 Test MSE 3450.8083211279854 Test RE 0.9888936517696036 Lambda1 -0.0004483864\n",
      "18 Train Loss 3423.8367 Test MSE 3358.102024134314 Test RE 0.9755198567171754 Lambda1 -0.00083894265\n",
      "19 Train Loss 3259.108 Test MSE 3237.612850906062 Test RE 0.9578590971165617 Lambda1 0.0036247247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Train Loss 2096.3457 Test MSE 2070.4469434886473 Test RE 0.7659866905502609 Lambda1 0.12151788\n",
      "21 Train Loss 979.6523 Test MSE 978.4960906826853 Test RE 0.5265852839644808 Lambda1 0.1530854\n",
      "22 Train Loss 854.8651 Test MSE 857.9137372742425 Test RE 0.4930727273049079 Lambda1 0.15521356\n",
      "23 Train Loss 853.7478 Test MSE 855.4941425888579 Test RE 0.49237692394260885 Lambda1 0.14612845\n",
      "24 Train Loss 853.4585 Test MSE 855.9403140702032 Test RE 0.4925053034923256 Lambda1 0.13979927\n",
      "25 Train Loss 850.16675 Test MSE 847.7601224759568 Test RE 0.4901462255892025 Lambda1 0.085414074\n",
      "26 Train Loss 841.636 Test MSE 832.9611460952328 Test RE 0.4858492566842779 Lambda1 0.04281347\n",
      "27 Train Loss 829.84625 Test MSE 820.4590709320746 Test RE 0.4821893689782346 Lambda1 0.005181662\n",
      "28 Train Loss 808.9722 Test MSE 801.759092313021 Test RE 0.4766626438307107 Lambda1 0.0009973109\n",
      "29 Train Loss 793.21204 Test MSE 777.0017183219497 Test RE 0.46924554728213635 Lambda1 -0.0010525571\n",
      "30 Train Loss 766.27045 Test MSE 753.1873812500843 Test RE 0.46199863061941543 Lambda1 0.00026865755\n",
      "31 Train Loss 756.8267 Test MSE 743.2398722609358 Test RE 0.45893763218521577 Lambda1 0.00059542287\n",
      "32 Train Loss 741.5086 Test MSE 728.8267279037351 Test RE 0.4544659144296185 Lambda1 0.00066878524\n",
      "33 Train Loss 735.56866 Test MSE 725.2100136522041 Test RE 0.45333689604007393 Lambda1 0.00043907258\n",
      "34 Train Loss 714.3717 Test MSE 701.7296552654578 Test RE 0.44593759342999645 Lambda1 2.5196185e-05\n",
      "35 Train Loss 709.31604 Test MSE 698.6926172860143 Test RE 0.44497155334717753 Lambda1 2.3934355e-05\n",
      "36 Train Loss 705.5517 Test MSE 693.3697443196304 Test RE 0.44327334206231717 Lambda1 3.232181e-05\n",
      "37 Train Loss 702.3393 Test MSE 690.8364038084462 Test RE 0.4424628149703609 Lambda1 0.0001409633\n",
      "38 Train Loss 698.7746 Test MSE 687.5871277741782 Test RE 0.4414210500180824 Lambda1 0.0003970044\n",
      "39 Train Loss 696.3478 Test MSE 685.6818688873756 Test RE 0.4408090513660294 Lambda1 0.00024847596\n",
      "40 Train Loss 693.43555 Test MSE 682.6577043394298 Test RE 0.4398358943630239 Lambda1 0.00046681718\n",
      "41 Train Loss 691.83813 Test MSE 680.8672721068634 Test RE 0.4392587285413591 Lambda1 0.00038477383\n",
      "42 Train Loss 686.8912 Test MSE 676.6430050615761 Test RE 0.4378939741495027 Lambda1 -2.3698025e-05\n",
      "43 Train Loss 685.07166 Test MSE 674.2591332536402 Test RE 0.4371219243848519 Lambda1 4.475004e-05\n",
      "44 Train Loss 683.76154 Test MSE 673.053219371681 Test RE 0.43673085270660217 Lambda1 4.2168565e-05\n",
      "45 Train Loss 682.4362 Test MSE 671.4110916287518 Test RE 0.43619775532063026 Lambda1 0.00017687328\n",
      "46 Train Loss 680.7177 Test MSE 669.7483462873731 Test RE 0.4356573000072357 Lambda1 8.4500265e-05\n",
      "47 Train Loss 679.73425 Test MSE 668.2795871888045 Test RE 0.4351793392746489 Lambda1 0.00027531074\n",
      "48 Train Loss 678.41174 Test MSE 667.4248308205883 Test RE 0.43490094432087995 Lambda1 7.7422774e-05\n",
      "49 Train Loss 676.8385 Test MSE 665.5031935007474 Test RE 0.4342744136145413 Lambda1 0.00015835522\n",
      "50 Train Loss 675.1974 Test MSE 663.6115425216418 Test RE 0.4336567755466222 Lambda1 3.4659457e-05\n",
      "51 Train Loss 673.98114 Test MSE 662.2346937546041 Test RE 0.43320667049069894 Lambda1 -8.384959e-06\n",
      "52 Train Loss 673.09204 Test MSE 661.8007985266224 Test RE 0.4330647290402333 Lambda1 0.00010553795\n",
      "53 Train Loss 672.35406 Test MSE 661.6582362094097 Test RE 0.4330180820456667 Lambda1 0.00014002841\n",
      "54 Train Loss 671.16876 Test MSE 660.2854768677888 Test RE 0.43256865194374555 Lambda1 0.00012953274\n",
      "55 Train Loss 670.63153 Test MSE 659.9366607197381 Test RE 0.4324543779901615 Lambda1 8.798161e-05\n",
      "56 Train Loss 670.05237 Test MSE 659.8746272159658 Test RE 0.4324340523344313 Lambda1 6.291334e-05\n",
      "57 Train Loss 669.3659 Test MSE 658.8841719614659 Test RE 0.4321093941328765 Lambda1 4.108254e-05\n",
      "58 Train Loss 668.97955 Test MSE 658.6448975846932 Test RE 0.4320309265517203 Lambda1 2.5748108e-05\n",
      "59 Train Loss 668.1575 Test MSE 657.4643369751469 Test RE 0.43164356497773465 Lambda1 3.3467684e-05\n",
      "60 Train Loss 667.60223 Test MSE 656.7196508768467 Test RE 0.4313990421828574 Lambda1 0.00016123534\n",
      "61 Train Loss 667.0703 Test MSE 655.9663054913393 Test RE 0.43115153494496944 Lambda1 4.3150754e-05\n",
      "62 Train Loss 666.1986 Test MSE 655.1448105799576 Test RE 0.43088147553033623 Lambda1 0.0001260277\n",
      "63 Train Loss 665.4757 Test MSE 654.191421059558 Test RE 0.4305678445304762 Lambda1 0.00017418503\n",
      "64 Train Loss 664.47186 Test MSE 652.82445031671 Test RE 0.4301177611041017 Lambda1 0.0001684668\n",
      "65 Train Loss 663.7802 Test MSE 652.443784308722 Test RE 0.4299923406437171 Lambda1 0.000104111095\n",
      "66 Train Loss 663.53143 Test MSE 652.3650287469536 Test RE 0.4299663879784093 Lambda1 9.9739795e-05\n",
      "67 Train Loss 663.1184 Test MSE 652.1201361285254 Test RE 0.42988567743018535 Lambda1 0.00015096737\n",
      "68 Train Loss 662.94946 Test MSE 652.0290086678671 Test RE 0.4298556402049541 Lambda1 0.00014722733\n",
      "69 Train Loss 662.65436 Test MSE 651.9079911194449 Test RE 0.429815747432519 Lambda1 0.00013067827\n",
      "70 Train Loss 662.523 Test MSE 651.8972537441529 Test RE 0.42981220773669293 Lambda1 0.00012480383\n",
      "71 Train Loss 662.45087 Test MSE 651.8382572379955 Test RE 0.42979275835862746 Lambda1 0.00012072429\n",
      "72 Train Loss 662.3897 Test MSE 651.7783364862948 Test RE 0.42977300338356234 Lambda1 0.00013130666\n",
      "73 Train Loss 662.31335 Test MSE 651.7130855201759 Test RE 0.4297514900837086 Lambda1 0.00013921848\n",
      "74 Train Loss 662.1959 Test MSE 651.6402698929974 Test RE 0.42972748143703315 Lambda1 0.0001056023\n",
      "Training time: 259.02\n",
      "Training time: 259.02\n",
      "inv_HT_atanh_tune6\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 49997.977 Test MSE 3575.900127921914 Test RE 1.0066577954779452 Lambda1 0.0009233646\n",
      "1 Train Loss 45147.69 Test MSE 3576.457549667459 Test RE 1.006736252797696 Lambda1 -0.00010647959\n",
      "2 Train Loss 39817.945 Test MSE 3578.659709877008 Test RE 1.0070461478991781 Lambda1 -0.00025703793\n",
      "3 Train Loss 32355.178 Test MSE 3567.8386183798107 Test RE 1.0055224505280596 Lambda1 0.00018816677\n",
      "4 Train Loss 24575.326 Test MSE 3565.3045228941282 Test RE 1.0051652955684331 Lambda1 9.427608e-05\n",
      "5 Train Loss 18951.738 Test MSE 3564.9639032647765 Test RE 1.0051172790236889 Lambda1 -0.0001847601\n",
      "6 Train Loss 14870.546 Test MSE 3563.7815051474377 Test RE 1.0049505806543975 Lambda1 -1.8337876e-05\n",
      "7 Train Loss 12640.357 Test MSE 3564.421570177363 Test RE 1.0050408225566445 Lambda1 7.5411124e-05\n",
      "8 Train Loss 10141.517 Test MSE 3568.086553831803 Test RE 1.0055573876929542 Lambda1 0.00027302667\n",
      "9 Train Loss 8046.142 Test MSE 3566.424782856205 Test RE 1.0053232004526234 Lambda1 -0.00047112303\n",
      "10 Train Loss 7110.5527 Test MSE 3566.3265018581583 Test RE 1.0053093483662594 Lambda1 0.00025768945\n",
      "11 Train Loss 6405.098 Test MSE 3567.572027899577 Test RE 1.005484883285077 Lambda1 0.00028992203\n",
      "12 Train Loss 5786.2246 Test MSE 3568.6086711444036 Test RE 1.0056309564962802 Lambda1 0.00024462838\n",
      "13 Train Loss 5298.3545 Test MSE 3572.132113449514 Test RE 1.0061272855707435 Lambda1 -8.397765e-06\n",
      "14 Train Loss 4888.354 Test MSE 3573.5146487185984 Test RE 1.006321969284529 Lambda1 -0.0003427806\n",
      "15 Train Loss 4667.247 Test MSE 3574.1388711249015 Test RE 1.006409857729542 Lambda1 0.00048752065\n",
      "16 Train Loss 4510.338 Test MSE 3574.912029831376 Test RE 1.0065187052723912 Lambda1 -0.00017838826\n",
      "17 Train Loss 4371.1113 Test MSE 3576.066072362192 Test RE 1.0066811528593675 Lambda1 -6.521852e-05\n",
      "18 Train Loss 4249.894 Test MSE 3575.6972910934037 Test RE 1.0066292446008305 Lambda1 0.00018223711\n",
      "19 Train Loss 4149.789 Test MSE 3573.804960834465 Test RE 1.0063628452225892 Lambda1 -0.00010859219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Train Loss 4080.2402 Test MSE 3572.158934942909 Test RE 1.0061310628366495 Lambda1 0.0002064733\n",
      "21 Train Loss 4035.7585 Test MSE 3571.3060640025346 Test RE 1.0060109462368052 Lambda1 -8.2662e-05\n",
      "22 Train Loss 3985.35 Test MSE 3569.9189839245632 Test RE 1.005815562078092 Lambda1 7.141656e-05\n",
      "23 Train Loss 3944.712 Test MSE 3568.7679575068705 Test RE 1.0056533996280974 Lambda1 0.00022249967\n",
      "24 Train Loss 3911.1924 Test MSE 3566.7477067122354 Test RE 1.0053687131881783 Lambda1 0.00030984916\n",
      "25 Train Loss 3879.3052 Test MSE 3565.946737010184 Test RE 1.005255821114085 Lambda1 -1.2872621e-05\n",
      "26 Train Loss 3838.171 Test MSE 3564.101012610652 Test RE 1.0049956285920731 Lambda1 -0.00015433884\n",
      "27 Train Loss 3802.7349 Test MSE 3561.191439852029 Test RE 1.0045853280822667 Lambda1 -0.00012395123\n",
      "28 Train Loss 3784.5042 Test MSE 3560.3845801617867 Test RE 1.0044715171094964 Lambda1 -0.00022933667\n",
      "29 Train Loss 3761.4438 Test MSE 3558.5175708701604 Test RE 1.0042081180960143 Lambda1 2.0154466e-06\n",
      "30 Train Loss 3743.3452 Test MSE 3556.3760592474828 Test RE 1.0039059069987466 Lambda1 -5.700441e-05\n",
      "31 Train Loss 3727.7249 Test MSE 3555.851450413981 Test RE 1.003831860086723 Lambda1 -0.00010198257\n",
      "32 Train Loss 3712.5908 Test MSE 3554.206299018459 Test RE 1.0035996166327366 Lambda1 -3.6506754e-06\n",
      "33 Train Loss 3697.1584 Test MSE 3553.2055401502976 Test RE 1.0034583147721454 Lambda1 0.00021717463\n",
      "34 Train Loss 3686.4893 Test MSE 3550.753675374912 Test RE 1.0031120403095386 Lambda1 -0.00019544779\n",
      "35 Train Loss 3669.251 Test MSE 3547.934904351011 Test RE 1.0027138002742277 Lambda1 9.636492e-05\n",
      "36 Train Loss 3658.9644 Test MSE 3545.353575610033 Test RE 1.0023489676344555 Lambda1 0.00020151048\n",
      "37 Train Loss 3648.8027 Test MSE 3543.174129762298 Test RE 1.0020408317767442 Lambda1 7.4383985e-05\n",
      "38 Train Loss 3637.8098 Test MSE 3539.7766598217986 Test RE 1.0015602993539203 Lambda1 4.1017513e-05\n",
      "39 Train Loss 3629.9263 Test MSE 3538.223859565021 Test RE 1.0013405971191405 Lambda1 0.00015104227\n",
      "40 Train Loss 3623.6772 Test MSE 3536.0441863419906 Test RE 1.0010321186824727 Lambda1 -0.00011071547\n",
      "41 Train Loss 3614.7368 Test MSE 3532.558827077627 Test RE 1.0005386552887074 Lambda1 0.0002509357\n",
      "42 Train Loss 3608.9438 Test MSE 3528.2409265652373 Test RE 0.9999269814906235 Lambda1 0.00013965041\n",
      "43 Train Loss 3603.0574 Test MSE 3524.888303162405 Test RE 0.999451790676518 Lambda1 0.00028566993\n",
      "44 Train Loss 3597.2358 Test MSE 3522.0960432080483 Test RE 0.9990558515908944 Lambda1 0.00032559238\n",
      "45 Train Loss 3592.3872 Test MSE 3520.690338208718 Test RE 0.9988564649271916 Lambda1 0.00030560797\n",
      "46 Train Loss 3588.0752 Test MSE 3517.767089367061 Test RE 0.9984417007646486 Lambda1 5.7326004e-05\n",
      "47 Train Loss 3581.6335 Test MSE 3510.968686557236 Test RE 0.9974764447687674 Lambda1 0.00033234415\n",
      "48 Train Loss 3574.7354 Test MSE 3505.387010417074 Test RE 0.9966832441531561 Lambda1 -4.3935524e-05\n",
      "49 Train Loss 3566.9229 Test MSE 3499.012562549581 Test RE 0.9957766115449169 Lambda1 0.00019182646\n",
      "50 Train Loss 3561.9866 Test MSE 3494.7895019224084 Test RE 0.9951755141574349 Lambda1 0.00014583016\n",
      "51 Train Loss 3550.509 Test MSE 3486.522947866012 Test RE 0.9939978262349983 Lambda1 -3.7779075e-05\n",
      "52 Train Loss 3544.5164 Test MSE 3480.896425352161 Test RE 0.993195449563694 Lambda1 -9.905164e-05\n",
      "53 Train Loss 3535.351 Test MSE 3466.6783405941746 Test RE 0.9911649687765582 Lambda1 -9.957236e-05\n",
      "54 Train Loss 3527.259 Test MSE 3458.4610243183515 Test RE 0.9899895571203291 Lambda1 -0.00022893515\n",
      "55 Train Loss 3512.4233 Test MSE 3438.7702638711476 Test RE 0.9871672796484262 Lambda1 0.000101193924\n",
      "56 Train Loss 3499.404 Test MSE 3429.9635958296553 Test RE 0.9859024048381617 Lambda1 0.00025034888\n",
      "57 Train Loss 3487.3276 Test MSE 3417.520520270051 Test RE 0.984112472667817 Lambda1 0.000100603174\n",
      "58 Train Loss 3470.747 Test MSE 3397.90555132495 Test RE 0.9812842360446237 Lambda1 2.0696792e-05\n",
      "59 Train Loss 3461.7026 Test MSE 3391.7439681319097 Test RE 0.9803941277341469 Lambda1 -8.65132e-05\n",
      "60 Train Loss 3453.4604 Test MSE 3375.591197179885 Test RE 0.9780568373426101 Lambda1 -0.0001572974\n",
      "61 Train Loss 3435.7058 Test MSE 3351.6911366298596 Test RE 0.9745882388434316 Lambda1 -0.000105519255\n",
      "62 Train Loss 3426.197 Test MSE 3347.6466980784785 Test RE 0.9740000503903137 Lambda1 -5.308903e-05\n",
      "63 Train Loss 3406.825 Test MSE 3317.3467409398313 Test RE 0.9695821344751595 Lambda1 6.390237e-05\n",
      "64 Train Loss 3391.5703 Test MSE 3301.038974513704 Test RE 0.9671960108106628 Lambda1 -0.0001018758\n",
      "65 Train Loss 3378.006 Test MSE 3289.0596705601174 Test RE 0.9654394629846004 Lambda1 -2.1183414e-05\n",
      "66 Train Loss 3365.0828 Test MSE 3286.2229970540175 Test RE 0.9650230480264056 Lambda1 0.00010133043\n",
      "67 Train Loss 3345.7258 Test MSE 3271.659768640074 Test RE 0.9628823752926081 Lambda1 3.1217736e-05\n",
      "68 Train Loss 3333.4785 Test MSE 3268.7291561953107 Test RE 0.9624510243156807 Lambda1 0.000108743836\n",
      "69 Train Loss 3320.9983 Test MSE 3251.7612067363043 Test RE 0.9599497358890727 Lambda1 -0.00012062151\n",
      "70 Train Loss 3307.688 Test MSE 3243.676199428631 Test RE 0.9587556090140539 Lambda1 0.00037150492\n",
      "71 Train Loss 3300.1377 Test MSE 3236.6966763982637 Test RE 0.9577235608095506 Lambda1 5.634898e-06\n",
      "72 Train Loss 3290.6658 Test MSE 3225.2962080584293 Test RE 0.9560354000663609 Lambda1 0.00026881817\n",
      "73 Train Loss 3283.4785 Test MSE 3219.0450072676485 Test RE 0.9551084669273595 Lambda1 0.00025409245\n",
      "74 Train Loss 3276.8794 Test MSE 3210.6085818166594 Test RE 0.9538560787709128 Lambda1 0.0003938025\n",
      "Training time: 240.07\n",
      "Training time: 240.07\n",
      "inv_HT_atanh_tune6\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 44333.184 Test MSE 3528.983841174036 Test RE 1.00003224942403 Lambda1 -0.0005020822\n",
      "1 Train Loss 33189.324 Test MSE 3518.8283036235457 Test RE 0.9985922907045359 Lambda1 0.0004351862\n",
      "2 Train Loss 25397.61 Test MSE 3510.9590576221317 Test RE 0.997475076963555 Lambda1 0.00012019012\n",
      "3 Train Loss 18673.068 Test MSE 3505.3390979114456 Test RE 0.9966764326720582 Lambda1 5.9747275e-05\n",
      "4 Train Loss 13814.134 Test MSE 3494.7197929738736 Test RE 0.9951655889552731 Lambda1 0.00013057204\n",
      "5 Train Loss 10943.787 Test MSE 3489.9509376083793 Test RE 0.9944863612686675 Lambda1 -0.00019862829\n",
      "6 Train Loss 8943.318 Test MSE 3492.691058907091 Test RE 0.994876693200979 Lambda1 -5.4038297e-05\n",
      "7 Train Loss 6440.4336 Test MSE 3481.025231873012 Test RE 0.9932138254145235 Lambda1 0.00020197332\n",
      "8 Train Loss 5086.4546 Test MSE 3468.0610297191083 Test RE 0.9913626127836196 Lambda1 -0.00021397072\n",
      "9 Train Loss 4408.9604 Test MSE 3473.4308642542915 Test RE 0.9921298129679781 Lambda1 0.0007249059\n",
      "10 Train Loss 4024.0417 Test MSE 3471.8905597591156 Test RE 0.9919098069382961 Lambda1 -6.918375e-05\n",
      "11 Train Loss 3807.6206 Test MSE 3463.325575229042 Test RE 0.9906855548928027 Lambda1 -0.0003965922\n",
      "12 Train Loss 3630.0771 Test MSE 3445.793799732738 Test RE 0.9881748881212347 Lambda1 0.0007483714\n",
      "13 Train Loss 3558.8645 Test MSE 3434.594069583881 Test RE 0.9865676678009405 Lambda1 7.5545475e-05\n",
      "14 Train Loss 3506.0908 Test MSE 3418.9679781909435 Test RE 0.9843208562880277 Lambda1 0.00033439178\n",
      "15 Train Loss 3454.378 Test MSE 3382.1100939448183 Test RE 0.9790007869960873 Lambda1 0.00031246277\n",
      "16 Train Loss 3388.9841 Test MSE 3330.8073275866614 Test RE 0.9715472489089452 Lambda1 -0.0001503996\n",
      "17 Train Loss 3259.7063 Test MSE 3213.2366143004915 Test RE 0.9542463867031925 Lambda1 -0.00060605264\n",
      "18 Train Loss 3099.572 Test MSE 3056.1184155265514 Test RE 0.9306240238545028 Lambda1 0.00068284065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 Train Loss 2897.4216 Test MSE 2876.882910059636 Test RE 0.9029220601247833 Lambda1 6.838335e-05\n",
      "20 Train Loss 1785.1681 Test MSE 1715.1399045440796 Test RE 0.697170410396858 Lambda1 0.0010997053\n",
      "21 Train Loss 857.2995 Test MSE 859.5993172465296 Test RE 0.493556870201432 Lambda1 0.008122048\n",
      "22 Train Loss 854.564 Test MSE 857.7257108226996 Test RE 0.4930186916882833 Lambda1 0.008150071\n",
      "23 Train Loss 853.8593 Test MSE 856.7550782644001 Test RE 0.4927396540846601 Lambda1 0.0033673637\n",
      "24 Train Loss 852.82104 Test MSE 853.866988936026 Test RE 0.4919084495301082 Lambda1 -0.0046011074\n",
      "25 Train Loss 842.7004 Test MSE 842.3503508093346 Test RE 0.48857984923858827 Lambda1 -0.01371255\n",
      "26 Train Loss 815.2732 Test MSE 813.4006653841402 Test RE 0.48011075229668543 Lambda1 -0.026778666\n",
      "27 Train Loss 792.29517 Test MSE 796.6072179398412 Test RE 0.47512872689445323 Lambda1 -0.0057588546\n",
      "28 Train Loss 782.3562 Test MSE 789.1182998019482 Test RE 0.4728901064822642 Lambda1 -0.001820823\n",
      "29 Train Loss 777.8661 Test MSE 786.2915619892758 Test RE 0.4720423656612638 Lambda1 -0.000509467\n",
      "30 Train Loss 776.02124 Test MSE 784.9160205878715 Test RE 0.4716292885819334 Lambda1 -0.00039124844\n",
      "31 Train Loss 773.53625 Test MSE 782.4854692009768 Test RE 0.470898504661173 Lambda1 -0.00012506919\n",
      "32 Train Loss 771.69403 Test MSE 780.304451341051 Test RE 0.4702417801896212 Lambda1 -6.180954e-05\n",
      "33 Train Loss 769.65125 Test MSE 778.9957802943487 Test RE 0.4698472867063107 Lambda1 0.00011012973\n",
      "34 Train Loss 769.15643 Test MSE 778.632478648612 Test RE 0.4697377121723729 Lambda1 0.00040505436\n",
      "35 Train Loss 765.1958 Test MSE 773.2649882286091 Test RE 0.4681158475971229 Lambda1 -5.363205e-05\n",
      "36 Train Loss 763.5037 Test MSE 772.6416746529023 Test RE 0.46792714010934083 Lambda1 0.00036902833\n",
      "37 Train Loss 761.124 Test MSE 769.7142863073169 Test RE 0.4670398566209735 Lambda1 9.140653e-05\n",
      "38 Train Loss 756.8673 Test MSE 765.9221185331116 Test RE 0.46588794848029336 Lambda1 5.3229483e-05\n",
      "39 Train Loss 753.27325 Test MSE 761.6765880968226 Test RE 0.4645949384024976 Lambda1 -0.00017521289\n",
      "40 Train Loss 749.0605 Test MSE 754.8242660644545 Test RE 0.46250038365488144 Lambda1 -0.0007087942\n",
      "41 Train Loss 731.97894 Test MSE 737.9716429424246 Test RE 0.4573082198842317 Lambda1 -0.0017364321\n",
      "42 Train Loss 719.4044 Test MSE 728.6373609421121 Test RE 0.4544068699290644 Lambda1 -0.00042469712\n",
      "43 Train Loss 709.23883 Test MSE 719.621760901794 Test RE 0.45158687857300644 Lambda1 -0.00050342374\n",
      "44 Train Loss 706.48376 Test MSE 718.4811873609278 Test RE 0.45122886251103644 Lambda1 -0.00025508326\n",
      "45 Train Loss 703.7614 Test MSE 716.6434634586379 Test RE 0.45065141872302794 Lambda1 -9.30926e-05\n",
      "46 Train Loss 701.3683 Test MSE 713.6083840769702 Test RE 0.4496961221650312 Lambda1 0.00012309686\n",
      "47 Train Loss 699.1681 Test MSE 711.621558227472 Test RE 0.44906966466772696 Lambda1 -2.2981814e-05\n",
      "48 Train Loss 697.53406 Test MSE 712.1027678719915 Test RE 0.4492214729790144 Lambda1 -3.109533e-05\n",
      "49 Train Loss 695.0253 Test MSE 709.5878036706514 Test RE 0.44842750386453556 Lambda1 1.3985486e-06\n",
      "50 Train Loss 694.41284 Test MSE 709.6545856973506 Test RE 0.4484486049832483 Lambda1 2.433285e-06\n",
      "51 Train Loss 693.4762 Test MSE 708.906680587361 Test RE 0.44821223265131727 Lambda1 3.251761e-06\n",
      "52 Train Loss 692.1425 Test MSE 706.5644638102383 Test RE 0.44747117687171195 Lambda1 6.0512684e-06\n",
      "53 Train Loss 690.6711 Test MSE 705.9408280017201 Test RE 0.44727365727709895 Lambda1 -6.270184e-06\n",
      "54 Train Loss 688.96466 Test MSE 703.814728479228 Test RE 0.4465996167978547 Lambda1 -3.5507073e-05\n",
      "55 Train Loss 684.7086 Test MSE 700.1197498749278 Test RE 0.4454257655744278 Lambda1 -4.1102e-08\n",
      "56 Train Loss 681.38525 Test MSE 696.9348967018198 Test RE 0.4444114871827352 Lambda1 -2.2127731e-05\n",
      "57 Train Loss 676.3659 Test MSE 694.670190716423 Test RE 0.44368883696048345 Lambda1 -4.0345418e-07\n",
      "58 Train Loss 673.92615 Test MSE 693.1804768414089 Test RE 0.44321283830195696 Lambda1 -1.0799553e-06\n",
      "59 Train Loss 672.2219 Test MSE 692.3546473327954 Test RE 0.44294874595312156 Lambda1 -1.4889467e-06\n",
      "60 Train Loss 671.345 Test MSE 691.1335371004384 Test RE 0.44255795782801566 Lambda1 -1.8190485e-06\n",
      "61 Train Loss 670.92773 Test MSE 690.7309460236722 Test RE 0.44242904219223017 Lambda1 -1.5385922e-07\n",
      "62 Train Loss 670.5334 Test MSE 690.1928661073205 Test RE 0.4422566823435033 Lambda1 -3.7067565e-07\n",
      "63 Train Loss 670.0719 Test MSE 690.2072111464126 Test RE 0.4422612782736732 Lambda1 -4.149782e-07\n",
      "64 Train Loss 669.78235 Test MSE 690.2324844735705 Test RE 0.44226937534327454 Lambda1 -1.5260316e-07\n",
      "65 Train Loss 669.4118 Test MSE 689.1030277624628 Test RE 0.441907375118111 Lambda1 -2.0728926e-07\n",
      "66 Train Loss 668.548 Test MSE 688.4194416595594 Test RE 0.4416881359904107 Lambda1 -7.5343064e-07\n",
      "67 Train Loss 667.9594 Test MSE 687.4989160039366 Test RE 0.44139273376576726 Lambda1 -3.8177137e-07\n",
      "68 Train Loss 667.06946 Test MSE 685.6171229952322 Test RE 0.440788239054692 Lambda1 2.1976405e-07\n",
      "69 Train Loss 666.1645 Test MSE 683.9963326330208 Test RE 0.4402669218146353 Lambda1 -1.2276097e-06\n",
      "70 Train Loss 665.75244 Test MSE 683.6014012061671 Test RE 0.4401398009947129 Lambda1 -6.081829e-07\n",
      "71 Train Loss 665.1758 Test MSE 681.7645933318188 Test RE 0.43954808479073576 Lambda1 -1.1084592e-07\n",
      "72 Train Loss 664.8035 Test MSE 680.6035814030203 Test RE 0.439173660816037 Lambda1 -1.4529452e-06\n",
      "73 Train Loss 664.7419 Test MSE 680.4744400959631 Test RE 0.439131993278016 Lambda1 -5.011586e-07\n",
      "74 Train Loss 664.7101 Test MSE 680.6060760892066 Test RE 0.4391744656894547 Lambda1 -4.9562885e-07\n",
      "Training time: 215.57\n",
      "Training time: 215.57\n",
      "inv_HT_atanh_tune6\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 16342.929 Test MSE 3607.721873315613 Test RE 1.011126971898352 Lambda1 -0.00031065388\n",
      "1 Train Loss 12669.766 Test MSE 3606.515066135581 Test RE 1.0109578433743474 Lambda1 0.00015137733\n",
      "2 Train Loss 9490.68 Test MSE 3606.3403085215555 Test RE 1.0109333495461483 Lambda1 -0.00061568356\n",
      "3 Train Loss 6258.8267 Test MSE 3615.771651377701 Test RE 1.0122543887008246 Lambda1 0.0007186528\n",
      "4 Train Loss 4642.859 Test MSE 3611.9717341716596 Test RE 1.0117223454241184 Lambda1 -9.4189054e-05\n",
      "5 Train Loss 3926.0278 Test MSE 3589.7550017571 Test RE 1.0086060648350124 Lambda1 0.00040573932\n",
      "6 Train Loss 3730.03 Test MSE 3582.2147739938678 Test RE 1.007546226871312 Lambda1 -0.00015940487\n",
      "7 Train Loss 3594.0608 Test MSE 3541.1534531920483 Test RE 1.0017550584664392 Lambda1 -0.00037414028\n",
      "8 Train Loss 3460.0535 Test MSE 3427.450767414621 Test RE 0.9855411972892266 Lambda1 -0.0013523502\n",
      "9 Train Loss 3035.9485 Test MSE 3023.781271559271 Test RE 0.9256874099543239 Lambda1 0.020604642\n",
      "10 Train Loss 854.7362 Test MSE 858.0367605079194 Test RE 0.49310807889345354 Lambda1 0.4472622\n",
      "11 Train Loss 854.7136 Test MSE 858.0623099780496 Test RE 0.49311542039496453 Lambda1 0.44795275\n",
      "12 Train Loss 854.7134 Test MSE 858.063709437946 Test RE 0.49311582251900954 Lambda1 0.4479628\n",
      "13 Train Loss 854.71246 Test MSE 858.071343758874 Test RE 0.49311801617667667 Lambda1 0.44786483\n",
      "14 Train Loss 854.70294 Test MSE 858.088680149248 Test RE 0.49312299760584993 Lambda1 0.4451549\n",
      "15 Train Loss 854.6606 Test MSE 857.9666550152195 Test RE 0.49308793389934136 Lambda1 0.44022384\n",
      "16 Train Loss 854.5576 Test MSE 857.7569216945726 Test RE 0.4930276615740203 Lambda1 0.43008378\n",
      "17 Train Loss 854.01624 Test MSE 856.8666607500084 Test RE 0.49277173987364903 Lambda1 0.4005113\n",
      "18 Train Loss 850.3438 Test MSE 849.4803326323638 Test RE 0.49064325724369595 Lambda1 0.30822414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 Train Loss 845.00714 Test MSE 842.1824605863382 Test RE 0.4885311569887177 Lambda1 0.2506268\n",
      "20 Train Loss 835.49097 Test MSE 828.4762320056317 Test RE 0.4845395118073735 Lambda1 0.1339899\n",
      "21 Train Loss 798.21545 Test MSE 790.532746395745 Test RE 0.47331373012535055 Lambda1 0.0114615215\n",
      "22 Train Loss 764.77686 Test MSE 766.0837265548687 Test RE 0.4659370965964902 Lambda1 0.0033686198\n",
      "23 Train Loss 750.27386 Test MSE 753.0823961235492 Test RE 0.4619664310126503 Lambda1 0.0011161286\n",
      "24 Train Loss 739.66986 Test MSE 745.158235512685 Test RE 0.4595295283893714 Lambda1 9.799274e-05\n",
      "25 Train Loss 732.31366 Test MSE 739.9410738571024 Test RE 0.45791802438726326 Lambda1 2.7693295e-05\n",
      "26 Train Loss 718.20306 Test MSE 726.7616170084085 Test RE 0.4538215992319226 Lambda1 0.00016579003\n",
      "27 Train Loss 698.8402 Test MSE 708.5070275267415 Test RE 0.44808587282914564 Lambda1 -2.5555213e-05\n",
      "28 Train Loss 686.4499 Test MSE 701.0306951670904 Test RE 0.44571544930646456 Lambda1 -3.5778269e-06\n",
      "29 Train Loss 680.9495 Test MSE 694.6169780129202 Test RE 0.4436718430438026 Lambda1 -1.3853505e-05\n",
      "30 Train Loss 671.82855 Test MSE 688.9367273525258 Test RE 0.44185404941136874 Lambda1 -2.828544e-05\n",
      "31 Train Loss 668.1504 Test MSE 686.2794322609764 Test RE 0.44100108938748583 Lambda1 1.6991462e-05\n",
      "32 Train Loss 666.1301 Test MSE 685.1335544521419 Test RE 0.4406327667678974 Lambda1 4.7772333e-05\n",
      "33 Train Loss 665.53015 Test MSE 684.7015114973556 Test RE 0.4404938140755411 Lambda1 6.780861e-05\n",
      "34 Train Loss 664.656 Test MSE 684.6324002182984 Test RE 0.44047158259318064 Lambda1 5.562913e-05\n",
      "35 Train Loss 663.59576 Test MSE 684.3126076470269 Test RE 0.4403686981893166 Lambda1 8.552866e-05\n",
      "36 Train Loss 663.3708 Test MSE 684.4215492462943 Test RE 0.44040374983128333 Lambda1 6.7749745e-05\n",
      "37 Train Loss 662.1303 Test MSE 683.3044640531226 Test RE 0.44004419845884 Lambda1 9.3695526e-05\n",
      "38 Train Loss 660.98517 Test MSE 682.1131837974866 Test RE 0.4396604422614744 Lambda1 0.00016914257\n",
      "39 Train Loss 660.133 Test MSE 682.1103993936888 Test RE 0.4396595449078654 Lambda1 0.0001147304\n",
      "40 Train Loss 659.52014 Test MSE 682.0594394471699 Test RE 0.4396431212868532 Lambda1 8.4389496e-05\n",
      "41 Train Loss 658.1082 Test MSE 679.8017618557744 Test RE 0.43891488918292154 Lambda1 0.00013233838\n",
      "42 Train Loss 654.5144 Test MSE 675.858742284763 Test RE 0.43764013016486075 Lambda1 0.00014836599\n",
      "43 Train Loss 651.06085 Test MSE 673.1245988041867 Test RE 0.4367540104399401 Lambda1 0.00019546524\n",
      "44 Train Loss 650.2118 Test MSE 672.4829716867902 Test RE 0.4365458023094212 Lambda1 0.00025976394\n",
      "45 Train Loss 649.95496 Test MSE 672.1963681743911 Test RE 0.4364527673189223 Lambda1 0.00026330797\n",
      "46 Train Loss 649.74976 Test MSE 672.250091474196 Test RE 0.43647020806652326 Lambda1 0.0002410715\n",
      "47 Train Loss 648.93256 Test MSE 672.4445316082148 Test RE 0.4365333253431185 Lambda1 0.00013521787\n",
      "48 Train Loss 648.5225 Test MSE 672.2800298298704 Test RE 0.4364799269605882 Lambda1 0.00012636802\n",
      "49 Train Loss 647.6734 Test MSE 671.5438223642362 Test RE 0.4362408689871226 Lambda1 0.00023116783\n",
      "50 Train Loss 647.089 Test MSE 670.9899072316017 Test RE 0.43606091776138595 Lambda1 0.00028931463\n",
      "51 Train Loss 646.3573 Test MSE 670.224021518768 Test RE 0.43581198094502566 Lambda1 0.0001867735\n",
      "52 Train Loss 645.76294 Test MSE 668.9607538848321 Test RE 0.43540106845833193 Lambda1 0.00011619798\n",
      "53 Train Loss 645.3216 Test MSE 668.2597989919418 Test RE 0.43517289625413047 Lambda1 4.891964e-05\n",
      "54 Train Loss 644.28516 Test MSE 667.9293841186967 Test RE 0.43506529934677535 Lambda1 4.8989205e-05\n",
      "55 Train Loss 643.782 Test MSE 667.4953313111765 Test RE 0.4349239131398996 Lambda1 4.0304712e-05\n",
      "56 Train Loss 643.16473 Test MSE 666.2610881494796 Test RE 0.4345216255015364 Lambda1 5.0202558e-05\n",
      "57 Train Loss 641.8956 Test MSE 664.2779896449706 Test RE 0.43387447578796473 Lambda1 5.744059e-05\n",
      "58 Train Loss 640.4164 Test MSE 662.6509596761698 Test RE 0.43334280111538653 Lambda1 1.5186902e-05\n",
      "59 Train Loss 639.9577 Test MSE 662.272525281505 Test RE 0.4332190442276139 Lambda1 1.1428184e-05\n",
      "60 Train Loss 639.4392 Test MSE 660.6294755807086 Test RE 0.43268131812280247 Lambda1 3.753912e-07\n",
      "61 Train Loss 638.1065 Test MSE 660.214746138928 Test RE 0.43254548263553017 Lambda1 1.49865455e-05\n",
      "62 Train Loss 636.90356 Test MSE 659.6388587070754 Test RE 0.4323567926277965 Lambda1 7.5040307e-06\n",
      "63 Train Loss 636.57166 Test MSE 659.5045529578073 Test RE 0.4323127753915066 Lambda1 -6.4141323e-06\n",
      "64 Train Loss 635.9259 Test MSE 658.8906703505851 Test RE 0.4321115250142827 Lambda1 1.3942518e-05\n",
      "65 Train Loss 635.4541 Test MSE 658.1868804639645 Test RE 0.43188068476406666 Lambda1 9.340576e-06\n",
      "66 Train Loss 635.22595 Test MSE 658.1518332484653 Test RE 0.43186918619323733 Lambda1 1.2734269e-05\n",
      "67 Train Loss 635.0993 Test MSE 658.1353154734567 Test RE 0.4318637668033269 Lambda1 8.298418e-06\n",
      "68 Train Loss 634.9148 Test MSE 657.9719792323147 Test RE 0.431810173431388 Lambda1 4.1705994e-06\n",
      "69 Train Loss 634.64844 Test MSE 657.6242417720127 Test RE 0.4316960527526802 Lambda1 3.7780082e-06\n",
      "70 Train Loss 634.2212 Test MSE 657.3302949159573 Test RE 0.43159956157075463 Lambda1 -1.4567235e-05\n",
      "71 Train Loss 633.8039 Test MSE 656.6842840390709 Test RE 0.431387425791189 Lambda1 3.9683387e-06\n",
      "72 Train Loss 633.1852 Test MSE 656.4592438738716 Test RE 0.4313135030413751 Lambda1 -2.1533654e-06\n",
      "73 Train Loss 632.21643 Test MSE 655.4874297419457 Test RE 0.4309941292150984 Lambda1 2.5009336e-05\n",
      "74 Train Loss 631.8751 Test MSE 655.1448143581457 Test RE 0.4308814767727724 Lambda1 3.1728123e-05\n",
      "Training time: 226.25\n",
      "Training time: 226.25\n",
      "inv_HT_atanh_tune6\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 26095.29 Test MSE 3392.1897724564305 Test RE 0.9804585561793978 Lambda1 0.0003130095\n",
      "1 Train Loss 13803.199 Test MSE 3369.4586240244867 Test RE 0.9771679957387726 Lambda1 -0.00053884723\n",
      "2 Train Loss 8877.141 Test MSE 3359.8617258105382 Test RE 0.9757754176097132 Lambda1 -0.00029346513\n",
      "3 Train Loss 6794.6416 Test MSE 3354.909974960976 Test RE 0.9750561055172247 Lambda1 0.00038926542\n",
      "4 Train Loss 4964.1426 Test MSE 3348.164708097038 Test RE 0.9740754051563891 Lambda1 -0.0005107823\n",
      "5 Train Loss 4183.382 Test MSE 3351.907628909666 Test RE 0.9746197136145446 Lambda1 6.6608773e-06\n",
      "6 Train Loss 3711.2808 Test MSE 3351.7799752524784 Test RE 0.9746011547863865 Lambda1 -9.8085315e-05\n",
      "7 Train Loss 3485.23 Test MSE 3349.935353691989 Test RE 0.9743329364470289 Lambda1 5.602629e-05\n",
      "8 Train Loss 3396.1018 Test MSE 3334.922164547005 Test RE 0.9721471822287909 Lambda1 -0.00041487065\n",
      "9 Train Loss 3271.783 Test MSE 3236.108919724038 Test RE 0.9576365996222075 Lambda1 -0.00096606644\n",
      "10 Train Loss 2812.33 Test MSE 2800.9838047490935 Test RE 0.8909318185328339 Lambda1 -0.0092590675\n",
      "11 Train Loss 854.7553 Test MSE 858.0536078563113 Test RE 0.49311291989980865 Lambda1 -0.10495901\n",
      "12 Train Loss 854.67444 Test MSE 857.9496955281546 Test RE 0.49308306042336536 Lambda1 -0.1051694\n",
      "13 Train Loss 854.6373 Test MSE 857.9136821390556 Test RE 0.49307271146085735 Lambda1 -0.10390016\n",
      "14 Train Loss 854.4772 Test MSE 857.5108617047312 Test RE 0.49295694044092614 Lambda1 -0.093151286\n",
      "15 Train Loss 854.25037 Test MSE 857.2008067293979 Test RE 0.4928678117910891 Lambda1 -0.07761036\n",
      "16 Train Loss 854.04944 Test MSE 856.90454189681 Test RE 0.49278263220565827 Lambda1 -0.08336304\n",
      "17 Train Loss 853.13324 Test MSE 855.1497242965685 Test RE 0.49227779954028333 Lambda1 -0.094511084\n",
      "18 Train Loss 852.21277 Test MSE 853.3161822126305 Test RE 0.49174976545285376 Lambda1 -0.09220605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 Train Loss 849.80725 Test MSE 850.326618785499 Test RE 0.4908875955847075 Lambda1 -0.12319149\n",
      "20 Train Loss 846.3974 Test MSE 845.5708956722282 Test RE 0.48951294804492723 Lambda1 -0.1175583\n",
      "21 Train Loss 842.7431 Test MSE 842.8118946130442 Test RE 0.4887136831717819 Lambda1 -0.11447716\n",
      "22 Train Loss 838.6052 Test MSE 837.918246571795 Test RE 0.48729280000311986 Lambda1 -0.13140054\n",
      "23 Train Loss 833.3754 Test MSE 830.0038755230589 Test RE 0.48498603200361085 Lambda1 -0.13220716\n",
      "24 Train Loss 828.44336 Test MSE 824.2589719445826 Test RE 0.4833046929010818 Lambda1 -0.12242061\n",
      "25 Train Loss 823.68304 Test MSE 815.578142408304 Test RE 0.4807529520491225 Lambda1 -0.10226802\n",
      "26 Train Loss 818.16864 Test MSE 807.9555672744262 Test RE 0.47850106590821495 Lambda1 -0.06853032\n",
      "27 Train Loss 791.65356 Test MSE 782.5778328986147 Test RE 0.47092629600388597 Lambda1 0.009033991\n",
      "28 Train Loss 762.803 Test MSE 767.3639012550545 Test RE 0.46632623937610795 Lambda1 0.006746457\n",
      "29 Train Loss 751.8346 Test MSE 759.7354199079176 Test RE 0.4640025398250065 Lambda1 0.0031164987\n",
      "30 Train Loss 742.3397 Test MSE 749.710244005062 Test RE 0.46093097399950844 Lambda1 0.0020293985\n",
      "31 Train Loss 735.8669 Test MSE 743.3704603957308 Test RE 0.4589779483599256 Lambda1 0.0009038504\n",
      "32 Train Loss 731.9185 Test MSE 739.5127907462104 Test RE 0.4577854821147331 Lambda1 0.00051298895\n",
      "33 Train Loss 728.6127 Test MSE 737.3972321960557 Test RE 0.4571302090527609 Lambda1 0.00040653948\n",
      "34 Train Loss 717.9092 Test MSE 727.3506222799576 Test RE 0.4540054622558203 Lambda1 0.0007485477\n",
      "35 Train Loss 707.2307 Test MSE 717.4053840835276 Test RE 0.4508909167531209 Lambda1 0.000738593\n",
      "36 Train Loss 702.7918 Test MSE 715.6545788207679 Test RE 0.450340388114696 Lambda1 0.00052371644\n",
      "37 Train Loss 699.44403 Test MSE 713.3532647534801 Test RE 0.44961573043429925 Lambda1 0.00026212825\n",
      "38 Train Loss 696.8713 Test MSE 709.7084025775091 Test RE 0.4484656087818434 Lambda1 0.00019675911\n",
      "39 Train Loss 694.55286 Test MSE 707.109110814773 Test RE 0.44764360762692074 Lambda1 -2.6288722e-05\n",
      "40 Train Loss 689.3544 Test MSE 702.4282261342797 Test RE 0.44615950333442267 Lambda1 -1.6132518e-06\n",
      "41 Train Loss 684.49506 Test MSE 698.6035469342438 Test RE 0.44494318963255736 Lambda1 3.146994e-05\n",
      "42 Train Loss 678.21387 Test MSE 693.9452685787174 Test RE 0.4434572710893788 Lambda1 5.5150326e-06\n",
      "43 Train Loss 670.9068 Test MSE 687.0259377749717 Test RE 0.44124087529174727 Lambda1 3.016213e-05\n",
      "44 Train Loss 666.9444 Test MSE 683.8953045927927 Test RE 0.4402344063282837 Lambda1 9.947513e-06\n",
      "45 Train Loss 663.0931 Test MSE 682.2162139100031 Test RE 0.43969364536875616 Lambda1 6.0851416e-05\n",
      "46 Train Loss 660.3775 Test MSE 679.3498968115389 Test RE 0.43876899130834923 Lambda1 3.1992033e-05\n",
      "47 Train Loss 658.1903 Test MSE 678.2649717075907 Test RE 0.43841849320434806 Lambda1 2.0859914e-05\n",
      "48 Train Loss 656.4608 Test MSE 675.9837872254753 Test RE 0.43768061358965016 Lambda1 1.7010898e-05\n",
      "49 Train Loss 654.3605 Test MSE 675.9363829169517 Test RE 0.4376652668380039 Lambda1 6.594842e-05\n",
      "50 Train Loss 653.5565 Test MSE 674.9866409130033 Test RE 0.43735768226907834 Lambda1 0.00015818751\n",
      "51 Train Loss 653.1681 Test MSE 674.9358027778811 Test RE 0.43734121167079143 Lambda1 9.391581e-05\n",
      "52 Train Loss 652.5735 Test MSE 674.2542486406838 Test RE 0.43712034103567865 Lambda1 6.74033e-05\n",
      "53 Train Loss 651.9856 Test MSE 674.8097934282982 Test RE 0.43730038434037394 Lambda1 2.1772545e-05\n",
      "54 Train Loss 651.6106 Test MSE 674.2344587905236 Test RE 0.43711392608942923 Lambda1 6.778249e-05\n",
      "55 Train Loss 651.2944 Test MSE 673.9126125783446 Test RE 0.43700958534237183 Lambda1 4.5825876e-05\n",
      "56 Train Loss 651.1922 Test MSE 673.8406788384992 Test RE 0.43698626141865676 Lambda1 4.9947004e-05\n",
      "57 Train Loss 650.86084 Test MSE 673.1562399404535 Test RE 0.4367642754271266 Lambda1 7.441773e-05\n",
      "58 Train Loss 650.2875 Test MSE 671.7668261329102 Test RE 0.4363132955954549 Lambda1 0.00013079087\n",
      "59 Train Loss 649.5117 Test MSE 669.8128743960997 Test RE 0.43567828659472185 Lambda1 0.00012956682\n",
      "60 Train Loss 645.706 Test MSE 666.8659931124035 Test RE 0.43471883396814187 Lambda1 -0.00019015362\n",
      "61 Train Loss 644.66327 Test MSE 666.3126632706084 Test RE 0.4345384432867752 Lambda1 -0.00031746237\n",
      "62 Train Loss 644.129 Test MSE 665.9075702786356 Test RE 0.4344063317041583 Lambda1 -0.0003462864\n",
      "63 Train Loss 643.58575 Test MSE 665.4782754326849 Test RE 0.4342662833900754 Lambda1 -0.00035397333\n",
      "64 Train Loss 642.75903 Test MSE 664.3779519440351 Test RE 0.43390711984606634 Lambda1 -0.00043817703\n",
      "65 Train Loss 642.3358 Test MSE 664.2883502230542 Test RE 0.4338778592907509 Lambda1 -0.00053294323\n",
      "66 Train Loss 642.13257 Test MSE 664.2631877557492 Test RE 0.4338696418195037 Lambda1 -0.00038161676\n",
      "67 Train Loss 641.50995 Test MSE 664.1524442924799 Test RE 0.4338334737535923 Lambda1 -0.00019777873\n",
      "68 Train Loss 641.1405 Test MSE 663.7472743345663 Test RE 0.43370112228193575 Lambda1 -0.00017678925\n",
      "69 Train Loss 640.39954 Test MSE 662.5149325640172 Test RE 0.4332983211411681 Lambda1 -0.00032899587\n",
      "70 Train Loss 638.8528 Test MSE 661.3867880517864 Test RE 0.43292924916174425 Lambda1 -0.00030780595\n",
      "71 Train Loss 638.38477 Test MSE 661.143897053284 Test RE 0.4328497463083006 Lambda1 -0.00022783752\n",
      "72 Train Loss 637.7506 Test MSE 660.2676949284345 Test RE 0.43256282721974904 Lambda1 -0.00016345023\n",
      "73 Train Loss 636.65076 Test MSE 658.4282924374974 Test RE 0.4319598808199076 Lambda1 -8.2537e-05\n",
      "74 Train Loss 635.48285 Test MSE 657.5622528564872 Test RE 0.4316757060264667 Lambda1 -6.45654e-05\n",
      "Training time: 222.69\n",
      "Training time: 222.69\n",
      "inv_HT_atanh_tune6\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 22701.816 Test MSE 3669.531953388819 Test RE 1.0197518632682243 Lambda1 -0.0003043699\n",
      "1 Train Loss 15669.741 Test MSE 3679.8980570203826 Test RE 1.021191201952485 Lambda1 0.00042858705\n",
      "2 Train Loss 10266.241 Test MSE 3691.1206448389516 Test RE 1.0227471803193753 Lambda1 -7.8265046e-05\n",
      "3 Train Loss 7406.818 Test MSE 3677.886678005452 Test RE 1.0209120796411117 Lambda1 -0.0003387786\n",
      "4 Train Loss 5962.775 Test MSE 3686.5092614314235 Test RE 1.022108112433848 Lambda1 -0.00024457427\n",
      "5 Train Loss 5297.828 Test MSE 3695.90705238861 Test RE 1.0234100818270375 Lambda1 0.0001594233\n",
      "6 Train Loss 4790.1587 Test MSE 3694.8673693308415 Test RE 1.0232661256955948 Lambda1 9.8682845e-05\n",
      "7 Train Loss 4346.3164 Test MSE 3694.390212519762 Test RE 1.0232000510416075 Lambda1 -0.00035701972\n",
      "8 Train Loss 4083.4192 Test MSE 3692.113596650461 Test RE 1.022884736156011 Lambda1 0.00052933866\n",
      "9 Train Loss 4001.8188 Test MSE 3691.162272498908 Test RE 1.0227529474638153 Lambda1 0.00013468305\n",
      "10 Train Loss 3897.9766 Test MSE 3685.835975045453 Test RE 1.0220147717079417 Lambda1 -0.00025244494\n",
      "11 Train Loss 3835.2964 Test MSE 3682.287452747176 Test RE 1.0215226831376674 Lambda1 -0.00039546192\n",
      "12 Train Loss 3796.0178 Test MSE 3671.768517233478 Test RE 1.020062583103537 Lambda1 -7.579584e-05\n",
      "13 Train Loss 3760.8496 Test MSE 3660.1624225304918 Test RE 1.018449148986993 Lambda1 0.00023899798\n",
      "14 Train Loss 3722.0642 Test MSE 3641.78883795688 Test RE 1.0158896854642818 Lambda1 -6.402227e-05\n",
      "15 Train Loss 3705.5403 Test MSE 3633.769314118743 Test RE 1.0147705319487217 Lambda1 -8.5690124e-05\n",
      "16 Train Loss 3683.0583 Test MSE 3607.5937313983604 Test RE 1.0111090147353752 Lambda1 -0.00019866924\n",
      "17 Train Loss 3659.3044 Test MSE 3590.8252547465318 Test RE 1.0087564070184525 Lambda1 6.3726904e-05\n",
      "18 Train Loss 3637.9119 Test MSE 3571.7604074864967 Test RE 1.0060749368292714 Lambda1 0.00053325563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 Train Loss 3619.2366 Test MSE 3550.629072073946 Test RE 1.0030944395155306 Lambda1 0.00034549218\n",
      "20 Train Loss 3592.3982 Test MSE 3523.3252478595473 Test RE 0.9992301706550629 Lambda1 0.00068767276\n",
      "21 Train Loss 3567.4226 Test MSE 3505.6709006023525 Test RE 0.9967236024447599 Lambda1 0.0008088458\n",
      "22 Train Loss 3543.8896 Test MSE 3472.1250131738916 Test RE 0.9919432977283189 Lambda1 0.0003415959\n",
      "23 Train Loss 3507.6765 Test MSE 3436.9071483517446 Test RE 0.9868998212658506 Lambda1 -0.00061330374\n",
      "24 Train Loss 3481.3948 Test MSE 3413.7306755598365 Test RE 0.9835666576550561 Lambda1 -0.00019077731\n",
      "25 Train Loss 3447.5854 Test MSE 3380.186003303694 Test RE 0.9787222694161984 Lambda1 0.0005716401\n",
      "26 Train Loss 3369.6394 Test MSE 3307.6018547484064 Test RE 0.968156987008058 Lambda1 -0.00047338405\n",
      "27 Train Loss 3290.4976 Test MSE 3197.611205784103 Test RE 0.9519233924151353 Lambda1 -0.000770788\n",
      "28 Train Loss 3156.0408 Test MSE 3071.669600366925 Test RE 0.9329887787834706 Lambda1 0.0012439634\n",
      "29 Train Loss 2929.0059 Test MSE 2910.6239234278946 Test RE 0.9082015064816462 Lambda1 0.006392155\n",
      "30 Train Loss 950.3814 Test MSE 950.488633329409 Test RE 0.5189943553551847 Lambda1 0.09580234\n",
      "31 Train Loss 854.72516 Test MSE 858.092331610919 Test RE 0.49312404680848543 Lambda1 0.10987657\n",
      "32 Train Loss 854.7237 Test MSE 858.0817528858962 Test RE 0.4931210071357258 Lambda1 0.10988555\n",
      "33 Train Loss 854.72284 Test MSE 858.0751993689015 Test RE 0.49311912404995906 Lambda1 0.10992683\n",
      "34 Train Loss 854.72235 Test MSE 858.0739196563524 Test RE 0.49311875633686875 Lambda1 0.10995461\n",
      "35 Train Loss 854.7134 Test MSE 858.0371188473903 Test RE 0.4931081818611106 Lambda1 0.11115402\n",
      "36 Train Loss 854.5978 Test MSE 857.86733658996 Test RE 0.493059393084743 Lambda1 0.11255653\n",
      "37 Train Loss 854.1913 Test MSE 857.0096611062792 Test RE 0.4928128568862711 Lambda1 0.10878008\n",
      "38 Train Loss 852.99194 Test MSE 854.5014551741222 Test RE 0.4920911719872667 Lambda1 0.09472691\n",
      "39 Train Loss 850.3702 Test MSE 848.8780715505792 Test RE 0.4904692992832422 Lambda1 0.076837726\n",
      "40 Train Loss 838.43164 Test MSE 826.0713570231104 Test RE 0.4838357476241803 Lambda1 0.014256728\n",
      "41 Train Loss 811.4763 Test MSE 791.8091043660307 Test RE 0.47369567135807134 Lambda1 0.00032294713\n",
      "42 Train Loss 772.0958 Test MSE 744.2615383638409 Test RE 0.4592529543429614 Lambda1 -0.00024087737\n",
      "43 Train Loss 755.6786 Test MSE 733.2139762686724 Test RE 0.4558317145620933 Lambda1 -5.0479957e-05\n",
      "44 Train Loss 727.82715 Test MSE 705.4126782848633 Test RE 0.4471063120506919 Lambda1 0.00017296415\n",
      "45 Train Loss 704.0343 Test MSE 684.1573554234848 Test RE 0.44031874141738364 Lambda1 0.0001041473\n",
      "46 Train Loss 673.9999 Test MSE 652.3660683628725 Test RE 0.4299667305778006 Lambda1 6.6586224e-05\n",
      "47 Train Loss 654.21 Test MSE 629.7982308191401 Test RE 0.42246417709499523 Lambda1 8.158155e-05\n",
      "48 Train Loss 641.9641 Test MSE 615.4901959768158 Test RE 0.41763774330475467 Lambda1 3.9660823e-05\n",
      "49 Train Loss 633.7981 Test MSE 604.9963552559419 Test RE 0.41406216613922947 Lambda1 5.8282374e-05\n",
      "50 Train Loss 624.93506 Test MSE 596.5469336404858 Test RE 0.4111605889541965 Lambda1 5.5970075e-05\n",
      "51 Train Loss 615.5123 Test MSE 582.5391722800288 Test RE 0.40630459867499885 Lambda1 4.0695515e-05\n",
      "52 Train Loss 602.1617 Test MSE 570.970920214447 Test RE 0.40225010453862303 Lambda1 6.010568e-05\n",
      "53 Train Loss 584.30115 Test MSE 560.2713500307947 Test RE 0.3984633465624119 Lambda1 3.1818214e-05\n",
      "54 Train Loss 574.0573 Test MSE 562.3722926735787 Test RE 0.3992097396320188 Lambda1 4.7822814e-05\n",
      "55 Train Loss 566.6353 Test MSE 550.6042417038074 Test RE 0.39501077923037686 Lambda1 7.093888e-05\n",
      "56 Train Loss 559.6482 Test MSE 548.0872507619733 Test RE 0.39410688371952696 Lambda1 0.000100395824\n",
      "57 Train Loss 545.93066 Test MSE 542.2306502528049 Test RE 0.3919956088962667 Lambda1 0.00012450104\n",
      "58 Train Loss 540.5579 Test MSE 539.1857327635453 Test RE 0.39089342601981175 Lambda1 5.200108e-05\n",
      "59 Train Loss 529.61096 Test MSE 524.8417036083257 Test RE 0.38565888285693695 Lambda1 0.00022076556\n",
      "60 Train Loss 515.38245 Test MSE 506.44438709034307 Test RE 0.37883932291518124 Lambda1 0.00036157726\n",
      "61 Train Loss 504.16516 Test MSE 490.9553552099184 Test RE 0.37300115053816035 Lambda1 0.00032275604\n",
      "62 Train Loss 495.52258 Test MSE 487.6444170208243 Test RE 0.3717412875656437 Lambda1 0.00017767146\n",
      "63 Train Loss 488.83887 Test MSE 483.72257445308117 Test RE 0.37024341956445506 Lambda1 1.4843663e-05\n",
      "64 Train Loss 481.7198 Test MSE 482.7561419008145 Test RE 0.36987337874872495 Lambda1 4.432381e-05\n",
      "65 Train Loss 471.5677 Test MSE 473.18340585802395 Test RE 0.36618784420366374 Lambda1 7.992545e-05\n",
      "66 Train Loss 470.4465 Test MSE 472.0742885003319 Test RE 0.3657584297690484 Lambda1 6.554952e-05\n",
      "67 Train Loss 469.2411 Test MSE 470.0271673582347 Test RE 0.3649645236756763 Lambda1 6.759447e-05\n",
      "68 Train Loss 464.35648 Test MSE 466.87473084395657 Test RE 0.3637385699516095 Lambda1 7.651906e-05\n",
      "69 Train Loss 461.54016 Test MSE 466.20927107196803 Test RE 0.36347925017691035 Lambda1 1.5927399e-05\n",
      "70 Train Loss 460.41913 Test MSE 464.24942378894605 Test RE 0.36271444979604545 Lambda1 -1.7524021e-06\n",
      "71 Train Loss 458.04242 Test MSE 461.0105931040852 Test RE 0.3614469987755682 Lambda1 7.027493e-05\n",
      "72 Train Loss 452.7758 Test MSE 455.6186447501299 Test RE 0.3593270521728482 Lambda1 0.0001291367\n",
      "73 Train Loss 448.29382 Test MSE 449.4778611500596 Test RE 0.35689735011894314 Lambda1 8.574419e-05\n",
      "74 Train Loss 442.7946 Test MSE 444.6167026913632 Test RE 0.35496215915116724 Lambda1 5.5972465e-05\n",
      "Training time: 211.86\n",
      "Training time: 211.86\n",
      "inv_HT_atanh_tune7\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 5.24\n",
      "Training time: 5.24\n",
      "inv_HT_atanh_tune7\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.80\n",
      "Training time: 4.80\n",
      "inv_HT_atanh_tune7\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 661287.1 Test MSE 3554.8830772364972 Test RE 1.0036951628634225 Lambda1 -6.533714e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 623100.3 Test MSE 3554.836948557933 Test RE 1.0036886507952707 Lambda1 3.755653e-05\n",
      "2 Train Loss 604721.4 Test MSE 3554.8271974303802 Test RE 1.0036872742057237 Lambda1 -1.875102e-05\n",
      "3 Train Loss 584127.06 Test MSE 3554.783390870816 Test RE 1.0036810899076727 Lambda1 -3.1435753e-05\n",
      "4 Train Loss 563565.6 Test MSE 3554.7177359404095 Test RE 1.0036718211408964 Lambda1 -4.4127948e-05\n",
      "5 Train Loss 559097.6 Test MSE 3554.7047354929814 Test RE 1.0036699858060478 Lambda1 -3.7415128e-05\n",
      "6 Train Loss 547358.4 Test MSE 3554.8179581762283 Test RE 1.0036859698768035 Lambda1 -1.7174574e-05\n",
      "7 Train Loss 525910.94 Test MSE 3554.8118111834415 Test RE 1.0036851020893165 Lambda1 -2.3003604e-05\n",
      "8 Train Loss 513283.75 Test MSE 3554.537734341444 Test RE 1.00364640916262 Lambda1 -1.3357066e-05\n",
      "9 Train Loss 505354.62 Test MSE 3554.252306838432 Test RE 1.0036061122153195 Lambda1 1.7017908e-05\n",
      "10 Train Loss 496620.7 Test MSE 3553.8427293468976 Test RE 1.0035482848185586 Lambda1 3.7842874e-05\n",
      "11 Train Loss 485435.75 Test MSE 3553.5050373044046 Test RE 1.0035006042729195 Lambda1 8.133856e-06\n",
      "12 Train Loss 478412.84 Test MSE 3553.393261413665 Test RE 1.0034848215380148 Lambda1 1.5869895e-05\n",
      "13 Train Loss 472083.72 Test MSE 3553.3331999633865 Test RE 1.00347634076992 Lambda1 2.5482314e-05\n",
      "14 Train Loss 465726.34 Test MSE 3553.316896800861 Test RE 1.0034740387262162 Lambda1 1.9678699e-05\n",
      "15 Train Loss 461177.28 Test MSE 3553.2486491880186 Test RE 1.00346440195042 Lambda1 1.3080258e-05\n",
      "16 Train Loss 456150.22 Test MSE 3553.0957592939385 Test RE 1.0034428130846167 Lambda1 -1.0559167e-05\n",
      "17 Train Loss 451157.72 Test MSE 3552.94582880014 Test RE 1.0034216416507749 Lambda1 -5.240433e-05\n",
      "18 Train Loss 443819.72 Test MSE 3552.456584666537 Test RE 1.003352553199036 Lambda1 -5.5947545e-05\n",
      "19 Train Loss 436019.4 Test MSE 3552.085434575472 Test RE 1.00330013818568 Lambda1 -4.320169e-05\n",
      "20 Train Loss 427196.97 Test MSE 3551.682336700149 Test RE 1.0032432083009761 Lambda1 -4.4530327e-05\n",
      "21 Train Loss 421538.4 Test MSE 3551.674294981167 Test RE 1.0032420725287203 Lambda1 -5.2085285e-05\n",
      "22 Train Loss 414305.1 Test MSE 3551.334881952317 Test RE 1.0031941343539557 Lambda1 -4.6226145e-05\n",
      "23 Train Loss 410336.4 Test MSE 3551.2941010664345 Test RE 1.0031883743701295 Lambda1 -5.1530144e-05\n",
      "24 Train Loss 407060.06 Test MSE 3551.0258684965297 Test RE 1.0031504877759185 Lambda1 -5.20766e-05\n",
      "25 Train Loss 405772.12 Test MSE 3551.042610919059 Test RE 1.0031528526065552 Lambda1 -4.8744198e-05\n",
      "26 Train Loss 403265.25 Test MSE 3551.019037011181 Test RE 1.0031495228419238 Lambda1 -5.9941667e-05\n",
      "27 Train Loss 398866.2 Test MSE 3551.5145725140846 Test RE 1.003219513858192 Lambda1 -8.301125e-05\n",
      "28 Train Loss 395249.75 Test MSE 3551.8370103022307 Test RE 1.0032650533784169 Lambda1 -8.6649874e-05\n",
      "29 Train Loss 393433.22 Test MSE 3551.8731489550164 Test RE 1.003270157294375 Lambda1 -6.706584e-05\n",
      "30 Train Loss 389490.28 Test MSE 3552.0548303686046 Test RE 1.0032958160386367 Lambda1 -4.7027046e-05\n",
      "31 Train Loss 385863.03 Test MSE 3552.228280814136 Test RE 1.0033203117165237 Lambda1 -4.399383e-05\n",
      "32 Train Loss 382535.47 Test MSE 3552.3166893676316 Test RE 1.003332797055097 Lambda1 -5.1248855e-05\n",
      "33 Train Loss 378522.38 Test MSE 3552.0356433239117 Test RE 1.003293106296167 Lambda1 -5.3074156e-05\n",
      "34 Train Loss 371244.84 Test MSE 3552.198334805294 Test RE 1.0033160826101895 Lambda1 -2.6813615e-05\n",
      "35 Train Loss 363732.03 Test MSE 3552.012519259083 Test RE 1.003289840527616 Lambda1 -2.8944947e-05\n",
      "36 Train Loss 358902.3 Test MSE 3551.5263700986943 Test RE 1.003221180127192 Lambda1 -6.128403e-05\n",
      "37 Train Loss 354940.6 Test MSE 3551.476650550198 Test RE 1.0032141578115035 Lambda1 -5.311807e-05\n",
      "38 Train Loss 352320.56 Test MSE 3551.3027426042454 Test RE 1.0031895949230814 Lambda1 -5.816542e-05\n",
      "39 Train Loss 349546.3 Test MSE 3551.3434983585776 Test RE 1.003195351350277 Lambda1 -8.432215e-05\n",
      "40 Train Loss 347046.44 Test MSE 3551.3163673995086 Test RE 1.003191519321073 Lambda1 -0.00010222029\n",
      "41 Train Loss 345485.94 Test MSE 3551.3455402707727 Test RE 1.003195639753318 Lambda1 -8.710946e-05\n",
      "42 Train Loss 342042.1 Test MSE 3551.0118216753967 Test RE 1.0031485036887946 Lambda1 -4.8177524e-05\n",
      "43 Train Loss 339267.56 Test MSE 3550.8971369431674 Test RE 1.0031323045387757 Lambda1 -3.679471e-05\n",
      "44 Train Loss 335937.97 Test MSE 3550.597324092284 Test RE 1.0030899549165462 Lambda1 -3.1437576e-05\n",
      "45 Train Loss 331994.66 Test MSE 3550.666714351545 Test RE 1.003099756694333 Lambda1 -3.4573517e-05\n",
      "46 Train Loss 329210.34 Test MSE 3550.855023619882 Test RE 1.0031263559913146 Lambda1 2.5135316e-06\n",
      "47 Train Loss 326122.25 Test MSE 3551.0128359058954 Test RE 1.0031486469470807 Lambda1 -1.3874227e-05\n",
      "48 Train Loss 322974.9 Test MSE 3550.933720309063 Test RE 1.0031374719458004 Lambda1 2.9497762e-06\n",
      "49 Train Loss 320694.0 Test MSE 3551.0193422088155 Test RE 1.003149565950515 Lambda1 -1.3471085e-05\n",
      "50 Train Loss 317865.62 Test MSE 3551.109162382793 Test RE 1.003162252800415 Lambda1 -4.9776972e-05\n",
      "51 Train Loss 315441.4 Test MSE 3551.0505191087736 Test RE 1.003153969619152 Lambda1 -6.85164e-05\n",
      "52 Train Loss 313003.6 Test MSE 3551.018454818885 Test RE 1.0031494406083532 Lambda1 -3.2889555e-05\n",
      "53 Train Loss 309901.44 Test MSE 3550.644757667839 Test RE 1.0030966551953884 Lambda1 1.7756936e-05\n",
      "54 Train Loss 307403.8 Test MSE 3550.478063057897 Test RE 1.0030731083778848 Lambda1 2.110962e-05\n",
      "55 Train Loss 304522.12 Test MSE 3550.2943641711654 Test RE 1.0030471589432568 Lambda1 1.870355e-05\n",
      "56 Train Loss 301971.72 Test MSE 3550.02849894143 Test RE 1.0030096014439758 Lambda1 1.4323945e-05\n",
      "57 Train Loss 297738.1 Test MSE 3549.6242631425553 Test RE 1.0029524943066186 Lambda1 -2.7102225e-05\n",
      "58 Train Loss 296803.53 Test MSE 3549.6240571631924 Test RE 1.002952465206705 Lambda1 -2.4318326e-05\n",
      "59 Train Loss 294222.84 Test MSE 3549.3873773100263 Test RE 1.0029190274967592 Lambda1 -7.85863e-07\n",
      "60 Train Loss 291275.47 Test MSE 3549.4215977740555 Test RE 1.002923862172174 Lambda1 1.7058775e-06\n",
      "61 Train Loss 289312.62 Test MSE 3549.396938990584 Test RE 1.0029203783756417 Lambda1 -1.650833e-05\n",
      "62 Train Loss 285753.22 Test MSE 3549.260311124929 Test RE 1.002901075351529 Lambda1 -4.538123e-05\n",
      "63 Train Loss 283727.72 Test MSE 3549.417797948794 Test RE 1.002923325333095 Lambda1 -2.5416044e-05\n",
      "64 Train Loss 279800.25 Test MSE 3549.3970088798023 Test RE 1.0029203882496178 Lambda1 -8.726614e-06\n",
      "65 Train Loss 276142.88 Test MSE 3549.4961746084055 Test RE 1.0029343983245869 Lambda1 2.3851902e-05\n",
      "66 Train Loss 271554.53 Test MSE 3550.023503059633 Test RE 1.0030088956863776 Lambda1 1.6476628e-05\n",
      "67 Train Loss 268577.53 Test MSE 3549.690337225061 Test RE 1.002961828936506 Lambda1 1.0756257e-05\n",
      "68 Train Loss 266551.06 Test MSE 3549.749607335896 Test RE 1.0029702022599476 Lambda1 2.5203712e-05\n",
      "69 Train Loss 264370.75 Test MSE 3549.6358624309205 Test RE 1.0029541330048064 Lambda1 -8.52015e-06\n",
      "70 Train Loss 263189.56 Test MSE 3549.553127140228 Test RE 1.0029424444558266 Lambda1 -1.2422023e-05\n",
      "71 Train Loss 260745.17 Test MSE 3549.324411377388 Test RE 1.0029101315937081 Lambda1 5.0151325e-06\n",
      "72 Train Loss 258557.19 Test MSE 3549.4434847183525 Test RE 1.0029269543527257 Lambda1 2.069523e-05\n",
      "73 Train Loss 257161.39 Test MSE 3549.3055404903002 Test RE 1.0029074654766088 Lambda1 -1.3440406e-06\n",
      "74 Train Loss 255046.42 Test MSE 3549.1237556907518 Test RE 1.002881782189422 Lambda1 -3.8652925e-06\n",
      "Training time: 215.32\n",
      "Training time: 215.32\n",
      "inv_HT_atanh_tune7\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 5.19\n",
      "Training time: 5.19\n",
      "inv_HT_atanh_tune7\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 403394.53 Test MSE 3517.2808383578144 Test RE 0.9983726924917035 Lambda1 1.5429243e-05\n",
      "1 Train Loss 393702.0 Test MSE 3517.346673372206 Test RE 0.9983820360124561 Lambda1 3.0280811e-05\n",
      "2 Train Loss 384244.12 Test MSE 3517.5958729127788 Test RE 0.9984174024351073 Lambda1 4.1285646e-05\n",
      "3 Train Loss 378139.34 Test MSE 3517.763505429566 Test RE 0.9984411921531533 Lambda1 4.2929092e-05\n",
      "4 Train Loss 372968.62 Test MSE 3517.6795994927734 Test RE 0.9984292846381955 Lambda1 3.176189e-05\n",
      "5 Train Loss 364993.34 Test MSE 3517.865414035579 Test RE 0.9984556543267782 Lambda1 -1.6184898e-05\n",
      "6 Train Loss 359559.9 Test MSE 3517.827365282364 Test RE 0.9984502547319898 Lambda1 -2.8186572e-05\n",
      "7 Train Loss 351259.0 Test MSE 3517.5485936871974 Test RE 0.9984106926591814 Lambda1 3.431682e-05\n",
      "8 Train Loss 345626.72 Test MSE 3517.2983363630115 Test RE 0.9983751758744126 Lambda1 1.6550968e-05\n",
      "9 Train Loss 339943.8 Test MSE 3517.029117323608 Test RE 0.9983369666121413 Lambda1 -2.4586263e-05\n",
      "10 Train Loss 333029.78 Test MSE 3516.470160325673 Test RE 0.9982576312429162 Lambda1 6.446497e-05\n",
      "11 Train Loss 328297.72 Test MSE 3516.359256789109 Test RE 0.9982418894379844 Lambda1 8.357398e-06\n",
      "12 Train Loss 321883.34 Test MSE 3516.190158003436 Test RE 0.9982178868392149 Lambda1 2.937944e-06\n",
      "13 Train Loss 317875.16 Test MSE 3516.016010055112 Test RE 0.9981931669376097 Lambda1 4.837835e-05\n",
      "14 Train Loss 315298.56 Test MSE 3515.9388773326195 Test RE 0.9981822179290466 Lambda1 1.3112517e-05\n",
      "15 Train Loss 311581.3 Test MSE 3515.5906550037557 Test RE 0.9981327861903962 Lambda1 -2.6369808e-05\n",
      "16 Train Loss 307286.72 Test MSE 3515.1982245244017 Test RE 0.9980770759701686 Lambda1 -1.0447626e-05\n",
      "17 Train Loss 302799.47 Test MSE 3514.8984253107915 Test RE 0.998034513775439 Lambda1 3.7330166e-05\n",
      "18 Train Loss 300309.03 Test MSE 3514.852419184497 Test RE 0.9980279821710255 Lambda1 -1.5197337e-05\n",
      "19 Train Loss 295824.16 Test MSE 3513.9090034022465 Test RE 0.9978940336539869 Lambda1 -1.4312506e-05\n",
      "20 Train Loss 291391.28 Test MSE 3513.2998445142903 Test RE 0.9978075342075945 Lambda1 -8.44548e-06\n",
      "21 Train Loss 288865.66 Test MSE 3512.8601831827546 Test RE 0.9977450984443168 Lambda1 8.588611e-06\n",
      "22 Train Loss 286705.6 Test MSE 3512.3394825290734 Test RE 0.9976711493328713 Lambda1 -5.5718656e-06\n",
      "23 Train Loss 284369.7 Test MSE 3512.0047943420395 Test RE 0.9976236145338091 Lambda1 -3.377028e-06\n",
      "24 Train Loss 281500.8 Test MSE 3511.695710130952 Test RE 0.9975797141819416 Lambda1 -4.0854916e-06\n",
      "25 Train Loss 279565.72 Test MSE 3511.5524669418937 Test RE 0.9975593681769596 Lambda1 4.08446e-06\n",
      "26 Train Loss 277401.47 Test MSE 3511.352127397439 Test RE 0.9975309116122639 Lambda1 2.4735129e-05\n",
      "27 Train Loss 274774.7 Test MSE 3511.274518988117 Test RE 0.997519887765571 Lambda1 2.3044318e-05\n",
      "28 Train Loss 272571.38 Test MSE 3511.129764918702 Test RE 0.9974993259224973 Lambda1 2.5439672e-06\n",
      "29 Train Loss 270000.94 Test MSE 3510.674286598938 Test RE 0.9974346239486304 Lambda1 2.83108e-05\n",
      "30 Train Loss 267737.16 Test MSE 3510.4755196168185 Test RE 0.9974063872253995 Lambda1 2.8489889e-05\n",
      "31 Train Loss 265737.38 Test MSE 3510.363570637688 Test RE 0.9973904834658639 Lambda1 2.412199e-05\n",
      "32 Train Loss 263322.4 Test MSE 3510.2861253271976 Test RE 0.9973794812377247 Lambda1 -9.043301e-06\n",
      "33 Train Loss 259807.42 Test MSE 3509.944421564602 Test RE 0.9973309358195475 Lambda1 8.085479e-06\n",
      "34 Train Loss 257445.86 Test MSE 3509.9297590415717 Test RE 0.9973288526807093 Lambda1 6.6970088e-06\n",
      "35 Train Loss 255379.3 Test MSE 3509.6937585776536 Test RE 0.9972953229458354 Lambda1 1.1021634e-05\n",
      "36 Train Loss 253196.83 Test MSE 3509.4570561525793 Test RE 0.9972616923475576 Lambda1 9.108695e-06\n",
      "37 Train Loss 251786.42 Test MSE 3509.4381606699117 Test RE 0.9972590076350669 Lambda1 -1.275236e-05\n",
      "38 Train Loss 250048.78 Test MSE 3509.0376734143088 Test RE 0.9972021038089044 Lambda1 -7.768235e-06\n",
      "39 Train Loss 248751.36 Test MSE 3509.0663762501767 Test RE 0.9972061822019633 Lambda1 -4.73541e-06\n",
      "40 Train Loss 247199.28 Test MSE 3509.023618411874 Test RE 0.9972001067240429 Lambda1 -1.4576047e-05\n",
      "41 Train Loss 245510.12 Test MSE 3508.9775312462048 Test RE 0.9971935581392476 Lambda1 -8.839565e-06\n",
      "42 Train Loss 244306.81 Test MSE 3508.555620634833 Test RE 0.9971336063174477 Lambda1 1.0918955e-05\n",
      "43 Train Loss 243512.56 Test MSE 3508.3710336086438 Test RE 0.9971073761008122 Lambda1 -2.6182452e-06\n",
      "44 Train Loss 242740.36 Test MSE 3507.9950004122884 Test RE 0.9970539388335571 Lambda1 7.070576e-06\n",
      "45 Train Loss 241852.97 Test MSE 3507.7350790761 Test RE 0.9970170002984896 Lambda1 2.2228436e-05\n",
      "46 Train Loss 241055.73 Test MSE 3507.6186573748737 Test RE 0.9970004546677828 Lambda1 -3.9428767e-07\n",
      "47 Train Loss 239365.55 Test MSE 3507.309873589785 Test RE 0.9969565695736705 Lambda1 -4.922457e-06\n",
      "48 Train Loss 237942.33 Test MSE 3507.411051697574 Test RE 0.9969709494624502 Lambda1 -1.7004582e-05\n",
      "49 Train Loss 237055.8 Test MSE 3507.3738469886653 Test RE 0.9969656617856165 Lambda1 -1.900551e-05\n",
      "50 Train Loss 236398.33 Test MSE 3507.4855167817154 Test RE 0.996981532643393 Lambda1 -1.05116005e-05\n",
      "51 Train Loss 235863.23 Test MSE 3507.461476808619 Test RE 0.9969781160291011 Lambda1 5.261527e-06\n",
      "52 Train Loss 235423.56 Test MSE 3507.482035039294 Test RE 0.9969810378111635 Lambda1 3.3707317e-06\n",
      "53 Train Loss 234962.19 Test MSE 3507.494823785609 Test RE 0.9969828553722834 Lambda1 5.6470158e-06\n",
      "54 Train Loss 234442.02 Test MSE 3507.531390384634 Test RE 0.996988052269089 Lambda1 -9.410082e-06\n",
      "55 Train Loss 233611.45 Test MSE 3507.6840545732593 Test RE 0.9970097488270102 Lambda1 -2.754312e-05\n",
      "56 Train Loss 232766.33 Test MSE 3508.00082159818 Test RE 0.9970547660915654 Lambda1 -3.4057677e-05\n",
      "57 Train Loss 231942.02 Test MSE 3508.069111255572 Test RE 0.9970644707924015 Lambda1 -1.4483863e-05\n",
      "58 Train Loss 231200.03 Test MSE 3508.0478450586156 Test RE 0.9970614486453347 Lambda1 -2.8325421e-06\n",
      "59 Train Loss 230681.28 Test MSE 3508.045269559845 Test RE 0.9970610826396362 Lambda1 -2.462426e-05\n",
      "60 Train Loss 230051.39 Test MSE 3508.051451555331 Test RE 0.9970619611665539 Lambda1 -2.0225474e-05\n",
      "61 Train Loss 229531.19 Test MSE 3508.0578750511177 Test RE 0.9970628740123975 Lambda1 -9.028166e-06\n",
      "62 Train Loss 228486.83 Test MSE 3508.225849355564 Test RE 0.9970867446188579 Lambda1 7.627607e-07\n",
      "63 Train Loss 227729.06 Test MSE 3508.2681229014465 Test RE 0.9970927519666004 Lambda1 1.9724175e-05\n",
      "64 Train Loss 227341.89 Test MSE 3508.206138336181 Test RE 0.9970839435415327 Lambda1 3.041232e-05\n",
      "65 Train Loss 226945.31 Test MSE 3508.2539210240925 Test RE 0.9970907337908497 Lambda1 3.307329e-05\n",
      "66 Train Loss 226220.23 Test MSE 3508.43818222296 Test RE 0.997116918144345 Lambda1 3.0976687e-05\n",
      "67 Train Loss 225386.2 Test MSE 3508.313004881496 Test RE 0.9970991299503097 Lambda1 3.427553e-05\n",
      "68 Train Loss 224546.64 Test MSE 3508.2372563392596 Test RE 0.9970883656295171 Lambda1 2.394335e-05\n",
      "69 Train Loss 223726.12 Test MSE 3508.1451556587294 Test RE 0.9970752774155459 Lambda1 -3.1946593e-06\n",
      "70 Train Loss 222340.66 Test MSE 3508.100314185923 Test RE 0.9970689050357231 Lambda1 2.666625e-06\n",
      "71 Train Loss 220956.72 Test MSE 3508.192999992598 Test RE 0.9970820764842422 Lambda1 1.16186875e-05\n",
      "72 Train Loss 219870.44 Test MSE 3508.340932995116 Test RE 0.9971030986729317 Lambda1 1.666351e-05\n",
      "73 Train Loss 218619.88 Test MSE 3508.331404992997 Test RE 0.9971017446986359 Lambda1 1.0270613e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 Train Loss 217680.77 Test MSE 3508.3794615590596 Test RE 0.9971085737458728 Lambda1 7.892492e-06\n",
      "Training time: 244.98\n",
      "Training time: 244.98\n",
      "inv_HT_atanh_tune7\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.16\n",
      "Training time: 6.16\n",
      "inv_HT_atanh_tune7\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.01\n",
      "Training time: 6.01\n",
      "inv_HT_atanh_tune7\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 5.81\n",
      "Training time: 5.81\n",
      "inv_HT_atanh_tune7\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 5.62\n",
      "Training time: 5.62\n",
      "inv_HT_atanh_tune7\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 5.75\n",
      "Training time: 5.75\n",
      "inv_HT_atanh_tune8\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 5.70\n",
      "Training time: 5.70\n",
      "inv_HT_atanh_tune8\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 5.65\n",
      "Training time: 5.65\n",
      "inv_HT_atanh_tune8\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 5.40\n",
      "Training time: 5.40\n",
      "inv_HT_atanh_tune8\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 5.61\n",
      "Training time: 5.61\n",
      "inv_HT_atanh_tune8\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 5.90\n",
      "Training time: 5.90\n",
      "inv_HT_atanh_tune8\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.05\n",
      "Training time: 6.05\n",
      "inv_HT_atanh_tune8\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 5.10\n",
      "Training time: 5.10\n",
      "inv_HT_atanh_tune8\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 5.98\n",
      "Training time: 5.98\n",
      "inv_HT_atanh_tune8\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 6.19\n",
      "Training time: 6.19\n",
      "inv_HT_atanh_tune8\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 5.66\n",
      "Training time: 5.66\n",
      "inv_HT_atanh_tune9\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1225243400.0 Test MSE 3552.502451018183 Test RE 1.0033590304031046 Lambda1 -1.6318443e-06\n",
      "1 Train Loss 1089988100.0 Test MSE 3552.5066004315577 Test RE 1.003359616377389 Lambda1 -6.8839734e-07\n",
      "2 Train Loss 1018944260.0 Test MSE 3552.535461066871 Test RE 1.0033636920259974 Lambda1 2.5105584e-07\n",
      "3 Train Loss 976949900.0 Test MSE 3552.543654029834 Test RE 1.0033648490189278 Lambda1 2.5666176e-07\n",
      "4 Train Loss 961560300.0 Test MSE 3552.541265864782 Test RE 1.0033645117674397 Lambda1 -2.711898e-08\n",
      "5 Train Loss 944485000.0 Test MSE 3552.5321450998813 Test RE 1.0033632237517613 Lambda1 -3.5889957e-07\n",
      "6 Train Loss 931888960.0 Test MSE 3552.5355906505665 Test RE 1.0033637103255437 Lambda1 -5.0686566e-07\n",
      "7 Train Loss 906555400.0 Test MSE 3552.5301555920296 Test RE 1.003362942797331 Lambda1 -1.6656185e-07\n",
      "8 Train Loss 883513540.0 Test MSE 3552.523223498644 Test RE 1.0033619638599667 Lambda1 6.663754e-07\n",
      "9 Train Loss 866569700.0 Test MSE 3552.5066428474693 Test RE 1.0033596223673027 Lambda1 1.0998e-06\n",
      "10 Train Loss 848338370.0 Test MSE 3552.5021795439793 Test RE 1.0033589920658896 Lambda1 8.70799e-07\n",
      "11 Train Loss 833558460.0 Test MSE 3552.506611814672 Test RE 1.0033596179848958 Lambda1 5.919546e-07\n",
      "12 Train Loss 827108800.0 Test MSE 3552.498596920421 Test RE 1.0033584861325662 Lambda1 4.5229163e-07\n",
      "13 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "14 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "15 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "16 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "17 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "18 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "19 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "20 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "21 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "22 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "23 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "24 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "25 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "26 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "27 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "28 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "30 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "31 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "32 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "33 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "34 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "35 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "36 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "37 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "38 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "39 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "40 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "41 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "42 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "43 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "44 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "45 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "46 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "47 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "48 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "49 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "50 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "51 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "52 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "53 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "54 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "55 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "56 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "57 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "58 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "59 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "60 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "61 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "62 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "63 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "64 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "65 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "66 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "67 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "68 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "69 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "70 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "71 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "72 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "73 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "74 Train Loss 826902400.0 Test MSE 3552.4974577074713 Test RE 1.0033583252543785 Lambda1 4.341732e-07\n",
      "Training time: 210.23\n",
      "Training time: 210.23\n",
      "inv_HT_atanh_tune9\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.22\n",
      "Training time: 4.22\n",
      "inv_HT_atanh_tune9\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.80\n",
      "Training time: 3.80\n",
      "inv_HT_atanh_tune9\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.92\n",
      "Training time: 3.92\n",
      "inv_HT_atanh_tune9\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.99\n",
      "Training time: 3.99\n",
      "inv_HT_atanh_tune9\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.84\n",
      "Training time: 3.84\n",
      "inv_HT_atanh_tune9\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.08\n",
      "Training time: 4.08\n",
      "inv_HT_atanh_tune9\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.72\n",
      "Training time: 3.72\n",
      "inv_HT_atanh_tune9\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.01\n",
      "Training time: 4.01\n",
      "inv_HT_atanh_tune9\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.95\n",
      "Training time: 3.95\n",
      "inv_HT_atanh_tune10\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 875.6589 Test MSE 877.4959915281679 Test RE 0.49866827699200633 Lambda1 -0.059553444\n",
      "1 Train Loss 854.78625 Test MSE 858.0586824730575 Test RE 0.4931143780577597 Lambda1 -0.06347116\n",
      "2 Train Loss 854.7212 Test MSE 858.0738535358215 Test RE 0.4931187373377627 Lambda1 -0.06368982\n",
      "3 Train Loss 854.72076 Test MSE 858.081346293142 Test RE 0.4931208903056852 Lambda1 -0.06371733\n",
      "4 Train Loss 854.6332 Test MSE 857.8860628667794 Test RE 0.4930647745207195 Lambda1 -0.065205365\n",
      "5 Train Loss 854.50415 Test MSE 857.6310001696036 Test RE 0.49299147120814635 Lambda1 -0.09069065\n",
      "6 Train Loss 851.4707 Test MSE 854.3285421960592 Test RE 0.49204138081693033 Lambda1 -0.42366838\n",
      "7 Train Loss 846.6178 Test MSE 848.2032518888489 Test RE 0.49027431007213906 Lambda1 -0.6351756\n",
      "8 Train Loss 826.314 Test MSE 828.239413663106 Test RE 0.4844702545125193 Lambda1 -0.76270825\n",
      "9 Train Loss 806.2394 Test MSE 803.7580588238205 Test RE 0.47725648773192486 Lambda1 -0.74691224\n",
      "10 Train Loss 763.4178 Test MSE 765.503868818681 Test RE 0.4657607265959892 Lambda1 -0.7086668\n",
      "11 Train Loss 732.67053 Test MSE 734.0231992594732 Test RE 0.45608318807070186 Lambda1 -0.7842468\n",
      "12 Train Loss 711.4624 Test MSE 719.0213089363614 Test RE 0.4513984373457553 Lambda1 -0.80109555\n",
      "13 Train Loss 696.30206 Test MSE 696.694202180682 Test RE 0.44433473923410693 Lambda1 -0.88113624\n",
      "14 Train Loss 684.3539 Test MSE 686.0777497765799 Test RE 0.44093628434842413 Lambda1 -1.0168947\n",
      "15 Train Loss 670.1093 Test MSE 676.5689046239282 Test RE 0.43786999620005407 Lambda1 -1.1307454\n",
      "16 Train Loss 663.2677 Test MSE 669.5367312126283 Test RE 0.43558846897836306 Lambda1 -1.2727861\n",
      "17 Train Loss 653.93787 Test MSE 659.8246337474108 Test RE 0.4324176709744016 Lambda1 -1.4173799\n",
      "18 Train Loss 645.9388 Test MSE 653.021066057112 Test RE 0.4301825270266234 Lambda1 -1.4088453\n",
      "19 Train Loss 641.79047 Test MSE 648.6357116387591 Test RE 0.42873565125217683 Lambda1 -1.4053411\n",
      "20 Train Loss 638.3699 Test MSE 646.0972446848907 Test RE 0.4278958900727002 Lambda1 -1.405135\n",
      "21 Train Loss 635.41095 Test MSE 641.2762387987547 Test RE 0.42629647819805344 Lambda1 -1.4280208\n",
      "22 Train Loss 632.9445 Test MSE 639.9121422797976 Test RE 0.4258428366338527 Lambda1 -1.4670278\n",
      "23 Train Loss 626.5124 Test MSE 638.4506716648071 Test RE 0.4253562756657782 Lambda1 -1.5295947\n",
      "24 Train Loss 622.206 Test MSE 637.7990819212664 Test RE 0.4251391652840497 Lambda1 -1.523582\n",
      "25 Train Loss 620.67505 Test MSE 637.2766373102539 Test RE 0.42496500607331933 Lambda1 -1.5139091\n",
      "26 Train Loss 617.36664 Test MSE 634.1800441212227 Test RE 0.4239312743007703 Lambda1 -1.5617702\n",
      "27 Train Loss 614.58826 Test MSE 631.0770237447575 Test RE 0.42289286219907357 Lambda1 -1.6098408\n",
      "28 Train Loss 613.2813 Test MSE 629.8775843320335 Test RE 0.42249079114292204 Lambda1 -1.6461186\n",
      "29 Train Loss 612.1396 Test MSE 629.3839616762623 Test RE 0.422325209840378 Lambda1 -1.691408\n",
      "30 Train Loss 611.5574 Test MSE 628.6867492350441 Test RE 0.42209122550055234 Lambda1 -1.73523\n",
      "31 Train Loss 610.53217 Test MSE 627.5740279523483 Test RE 0.4217175275553264 Lambda1 -1.7755392\n",
      "32 Train Loss 610.0684 Test MSE 627.1598396833149 Test RE 0.4215783413602482 Lambda1 -1.7716475\n",
      "33 Train Loss 609.5895 Test MSE 627.187338700991 Test RE 0.4215875837116002 Lambda1 -1.7573589\n",
      "34 Train Loss 609.5136 Test MSE 627.1663637820343 Test RE 0.42158053411191215 Lambda1 -1.7589557\n",
      "35 Train Loss 609.02356 Test MSE 626.8883088495852 Test RE 0.421487069642775 Lambda1 -1.7344532\n",
      "36 Train Loss 608.70966 Test MSE 626.7999591896291 Test RE 0.42145736773983955 Lambda1 -1.7106408\n",
      "37 Train Loss 608.3513 Test MSE 626.4849039456293 Test RE 0.4213514335884933 Lambda1 -1.7175323\n",
      "38 Train Loss 607.978 Test MSE 626.2159364751011 Test RE 0.421260974907758 Lambda1 -1.7180347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Train Loss 607.6947 Test MSE 626.0983683593709 Test RE 0.42122142849814653 Lambda1 -1.689698\n",
      "40 Train Loss 607.41254 Test MSE 626.0055801287909 Test RE 0.4211902146815971 Lambda1 -1.6694206\n",
      "41 Train Loss 607.2483 Test MSE 625.9567581839132 Test RE 0.421173790126414 Lambda1 -1.6550508\n",
      "42 Train Loss 607.1106 Test MSE 625.992548087763 Test RE 0.42118583053798897 Lambda1 -1.6563041\n",
      "43 Train Loss 606.83466 Test MSE 625.8536015412016 Test RE 0.42113908432304964 Lambda1 -1.6857355\n",
      "44 Train Loss 606.46674 Test MSE 625.4035850210839 Test RE 0.42098764824819046 Lambda1 -1.7047582\n",
      "45 Train Loss 605.60114 Test MSE 624.5536210423837 Test RE 0.42070147624313675 Lambda1 -1.6780974\n",
      "46 Train Loss 605.23755 Test MSE 624.1991365134863 Test RE 0.4205820682958235 Lambda1 -1.6575677\n",
      "47 Train Loss 604.78815 Test MSE 623.8673440776508 Test RE 0.4204702734457889 Lambda1 -1.6224297\n",
      "48 Train Loss 604.50726 Test MSE 623.7034697938874 Test RE 0.4204150463281906 Lambda1 -1.5890145\n",
      "49 Train Loss 604.3194 Test MSE 623.4553346258771 Test RE 0.42033140871756636 Lambda1 -1.5580184\n",
      "50 Train Loss 604.2654 Test MSE 623.2200532169759 Test RE 0.42025208828132343 Lambda1 -1.5501184\n",
      "51 Train Loss 604.21765 Test MSE 623.0625179603026 Test RE 0.42019897004121276 Lambda1 -1.5500109\n",
      "52 Train Loss 604.0051 Test MSE 623.037802528639 Test RE 0.42019063580377836 Lambda1 -1.5674647\n",
      "53 Train Loss 603.7332 Test MSE 622.6416121233299 Test RE 0.4200570147204866 Lambda1 -1.5770699\n",
      "54 Train Loss 603.5237 Test MSE 622.2859403238697 Test RE 0.41993702291999485 Lambda1 -1.5824158\n",
      "55 Train Loss 603.3317 Test MSE 621.921199808971 Test RE 0.41981393601836464 Lambda1 -1.5927511\n",
      "56 Train Loss 602.827 Test MSE 621.0910422382549 Test RE 0.41953365284436006 Lambda1 -1.6072656\n",
      "57 Train Loss 602.0818 Test MSE 620.2007999876316 Test RE 0.41923287527040265 Lambda1 -1.6135635\n",
      "58 Train Loss 601.74445 Test MSE 619.407149791471 Test RE 0.41896455026511054 Lambda1 -1.6120278\n",
      "59 Train Loss 600.8156 Test MSE 617.5044658002441 Test RE 0.4183205713747202 Lambda1 -1.589852\n",
      "60 Train Loss 599.41956 Test MSE 615.0751492145426 Test RE 0.4174969056225821 Lambda1 -1.5635895\n",
      "61 Train Loss 598.1097 Test MSE 612.5810551034531 Test RE 0.41664958290137527 Lambda1 -1.5659045\n",
      "62 Train Loss 596.8231 Test MSE 611.9828436199849 Test RE 0.4164460951814984 Lambda1 -1.5394866\n",
      "63 Train Loss 596.1432 Test MSE 611.4465251340126 Test RE 0.4162635764259443 Lambda1 -1.516435\n",
      "64 Train Loss 595.3471 Test MSE 610.7214269835368 Test RE 0.4160166852640182 Lambda1 -1.4925914\n",
      "65 Train Loss 594.6777 Test MSE 610.5367739546951 Test RE 0.41595378870836525 Lambda1 -1.4777104\n",
      "66 Train Loss 593.6931 Test MSE 608.7903159072532 Test RE 0.4153584387459668 Lambda1 -1.4650328\n",
      "67 Train Loss 591.7932 Test MSE 605.306610393699 Test RE 0.4141683225215025 Lambda1 -1.4479556\n",
      "68 Train Loss 589.66113 Test MSE 600.903031453578 Test RE 0.4126590443721785 Lambda1 -1.4314843\n",
      "69 Train Loss 587.1414 Test MSE 598.3257838150311 Test RE 0.4117731548884585 Lambda1 -1.4163309\n",
      "70 Train Loss 585.7582 Test MSE 596.4708548760628 Test RE 0.4111343700724918 Lambda1 -1.4187734\n",
      "71 Train Loss 585.0274 Test MSE 594.5137170526356 Test RE 0.4104593096269253 Lambda1 -1.4331118\n",
      "72 Train Loss 582.86774 Test MSE 592.2469270214457 Test RE 0.40967605296557785 Lambda1 -1.4679731\n",
      "73 Train Loss 579.6874 Test MSE 588.6204412926218 Test RE 0.4084198491831764 Lambda1 -1.4833201\n",
      "74 Train Loss 575.4987 Test MSE 582.2765926186999 Test RE 0.4062130174097057 Lambda1 -1.5236926\n",
      "Training time: 124.77\n",
      "Training time: 124.77\n",
      "inv_HT_atanh_tune10\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 880.675 Test MSE 882.3398481663162 Test RE 0.5000427296518638 Lambda1 -0.068448775\n",
      "1 Train Loss 854.8003 Test MSE 858.0636652585365 Test RE 0.49311580982440084 Lambda1 -0.07397415\n",
      "2 Train Loss 854.7206 Test MSE 858.0726576577796 Test RE 0.4931183937134506 Lambda1 -0.07427807\n",
      "3 Train Loss 854.72 Test MSE 858.0799688308742 Test RE 0.49312049450655987 Lambda1 -0.074305795\n",
      "4 Train Loss 854.5659 Test MSE 857.7653939899811 Test RE 0.49303009645129925 Lambda1 -0.07682491\n",
      "5 Train Loss 854.24115 Test MSE 857.0773131698539 Test RE 0.4928323077456402 Lambda1 -0.1840593\n",
      "6 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "7 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "8 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "9 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "10 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "11 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "12 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "13 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "14 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "15 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "16 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "17 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "18 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "19 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "20 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "21 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "22 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "23 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "24 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "25 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "26 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "27 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "28 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "29 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "30 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "31 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "32 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "33 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "34 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "35 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "36 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "37 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "38 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "39 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "40 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "42 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "43 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "44 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "45 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "46 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "47 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "48 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "49 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "50 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "51 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "52 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "53 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "54 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "55 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "56 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "57 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "58 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "59 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "60 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "61 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "62 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "63 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "64 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "65 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "66 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "67 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "68 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "69 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "70 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "71 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "72 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "73 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "74 Train Loss 852.9331 Test MSE 856.0794883806575 Test RE 0.4925453420901245 Lambda1 -0.5635063\n",
      "Training time: 136.51\n",
      "Training time: 136.51\n",
      "inv_HT_atanh_tune10\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 904.2389 Test MSE 905.2586563610213 Test RE 0.5064954097623835 Lambda1 -0.21099037\n",
      "1 Train Loss 854.85675 Test MSE 858.0890072843412 Test RE 0.4931230916041896 Lambda1 -0.23441821\n",
      "2 Train Loss 854.70984 Test MSE 858.0652892391181 Test RE 0.4931162764623398 Lambda1 -0.23573351\n",
      "3 Train Loss 853.771 Test MSE 856.9543592851335 Test RE 0.49279695631397946 Lambda1 -0.3382129\n",
      "4 Train Loss 850.21704 Test MSE 850.9121946973275 Test RE 0.4910565909294107 Lambda1 -0.36193496\n",
      "5 Train Loss 843.53754 Test MSE 845.609702905801 Test RE 0.4895241809441747 Lambda1 -0.4492674\n",
      "6 Train Loss 832.3773 Test MSE 827.45435934045 Test RE 0.4842405952919597 Lambda1 -0.58702666\n",
      "7 Train Loss 819.0584 Test MSE 817.8687287471952 Test RE 0.4814275863819837 Lambda1 -0.777046\n",
      "8 Train Loss 810.18756 Test MSE 811.0498918103921 Test RE 0.4794164768001571 Lambda1 -0.8189112\n",
      "9 Train Loss 799.58826 Test MSE 801.0107403438049 Test RE 0.47644013640425575 Lambda1 -0.931918\n",
      "10 Train Loss 787.07947 Test MSE 785.6743506630468 Test RE 0.4718570609308375 Lambda1 -1.0412278\n",
      "11 Train Loss 765.35236 Test MSE 768.1666950090843 Test RE 0.46657010407365196 Lambda1 -0.9007858\n",
      "12 Train Loss 741.9821 Test MSE 742.2751413731576 Test RE 0.4586396832370988 Lambda1 -0.7742638\n",
      "13 Train Loss 732.6072 Test MSE 736.805881781423 Test RE 0.456946876145661 Lambda1 -0.75110877\n",
      "14 Train Loss 716.0166 Test MSE 721.9701333198411 Test RE 0.4523231197815269 Lambda1 -0.689196\n",
      "15 Train Loss 699.75275 Test MSE 706.4221665575943 Test RE 0.44742611578488717 Lambda1 -0.73902506\n",
      "16 Train Loss 689.5405 Test MSE 697.3972098974551 Test RE 0.44455886338276146 Lambda1 -0.7496062\n",
      "17 Train Loss 685.98566 Test MSE 694.5214140460988 Test RE 0.4436413222668682 Lambda1 -0.7494736\n",
      "18 Train Loss 680.3301 Test MSE 688.0254244375292 Test RE 0.4415617176828547 Lambda1 -0.67640907\n",
      "19 Train Loss 669.5577 Test MSE 673.7829104593527 Test RE 0.43696752959532725 Lambda1 -0.57788944\n",
      "20 Train Loss 663.74713 Test MSE 664.9650857072608 Test RE 0.4340988068661477 Lambda1 -0.58995444\n",
      "21 Train Loss 660.2072 Test MSE 662.5749007190237 Test RE 0.4333179308975993 Lambda1 -0.5904418\n",
      "22 Train Loss 650.0361 Test MSE 655.9541862468561 Test RE 0.43114755207652616 Lambda1 -0.52810776\n",
      "23 Train Loss 643.6733 Test MSE 653.7717269904532 Test RE 0.4304297077733146 Lambda1 -0.50616664\n",
      "24 Train Loss 641.12695 Test MSE 651.0935129961728 Test RE 0.42954716280061506 Lambda1 -0.49011707\n",
      "25 Train Loss 639.2294 Test MSE 650.1515882214095 Test RE 0.4292363414326801 Lambda1 -0.50775373\n",
      "26 Train Loss 636.54846 Test MSE 649.6239496259152 Test RE 0.4290621300379153 Lambda1 -0.55918205\n",
      "27 Train Loss 634.88135 Test MSE 649.1259799855553 Test RE 0.42889764959065346 Lambda1 -0.5938225\n",
      "28 Train Loss 632.3283 Test MSE 647.0538679257218 Test RE 0.4282125480922768 Lambda1 -0.6238647\n",
      "29 Train Loss 630.8689 Test MSE 645.9789702756847 Test RE 0.42785672301978944 Lambda1 -0.6210886\n",
      "30 Train Loss 628.77954 Test MSE 643.2876644849207 Test RE 0.426964515080654 Lambda1 -0.63507664\n",
      "31 Train Loss 626.0481 Test MSE 640.4518027466355 Test RE 0.42602236292927925 Lambda1 -0.6290555\n",
      "32 Train Loss 623.4231 Test MSE 638.488882068854 Test RE 0.4253690039725254 Lambda1 -0.60923046\n",
      "33 Train Loss 621.949 Test MSE 636.0502367634742 Test RE 0.424555899360127 Lambda1 -0.6141425\n",
      "34 Train Loss 619.0611 Test MSE 632.435856607621 Test RE 0.4233479031061186 Lambda1 -0.62756526\n",
      "35 Train Loss 615.82654 Test MSE 630.3122518443273 Test RE 0.4226365427575512 Lambda1 -0.63464177\n",
      "36 Train Loss 612.93994 Test MSE 628.5350811479973 Test RE 0.4220403085452638 Lambda1 -0.6502747\n",
      "37 Train Loss 610.8513 Test MSE 627.2331460864951 Test RE 0.42160297900992383 Lambda1 -0.6857311\n",
      "38 Train Loss 608.68976 Test MSE 625.2717785515098 Test RE 0.4209432834404748 Lambda1 -0.7816624\n",
      "39 Train Loss 607.0361 Test MSE 624.7357801951443 Test RE 0.420762823287522 Lambda1 -0.8192598\n",
      "40 Train Loss 605.7687 Test MSE 623.3608009410773 Test RE 0.4202995403694511 Lambda1 -0.81948894\n",
      "41 Train Loss 605.49426 Test MSE 623.1340003628155 Test RE 0.4202230735377183 Lambda1 -0.82435\n",
      "42 Train Loss 605.019 Test MSE 623.2638097817377 Test RE 0.4202668410679694 Lambda1 -0.85814875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 Train Loss 604.59686 Test MSE 623.1078197179345 Test RE 0.4202142457200957 Lambda1 -0.8776908\n",
      "44 Train Loss 603.91815 Test MSE 622.5563306616457 Test RE 0.42002824672438843 Lambda1 -0.93392897\n",
      "45 Train Loss 603.68195 Test MSE 622.4431531966619 Test RE 0.41999006552650453 Lambda1 -0.95694494\n",
      "46 Train Loss 603.5553 Test MSE 622.3693289510117 Test RE 0.41996515853780525 Lambda1 -0.9735167\n",
      "47 Train Loss 603.29956 Test MSE 622.1438393233661 Test RE 0.41988907319601376 Lambda1 -1.000217\n",
      "48 Train Loss 603.19775 Test MSE 622.112617363493 Test RE 0.4198785371081779 Lambda1 -1.0085784\n",
      "49 Train Loss 602.9899 Test MSE 621.9903902376343 Test RE 0.41983728809122645 Lambda1 -1.0128576\n",
      "50 Train Loss 602.81635 Test MSE 621.7966103030594 Test RE 0.4197718832384641 Lambda1 -1.0122241\n",
      "51 Train Loss 602.68427 Test MSE 621.6967167036526 Test RE 0.41973816304126843 Lambda1 -1.029268\n",
      "52 Train Loss 602.2808 Test MSE 621.7040367795883 Test RE 0.41974063410641954 Lambda1 -1.0747613\n",
      "53 Train Loss 601.62335 Test MSE 619.9861886962466 Test RE 0.41916033433408 Lambda1 -1.1116265\n",
      "54 Train Loss 601.38055 Test MSE 619.8122422897442 Test RE 0.41910152935579875 Lambda1 -1.0950675\n",
      "55 Train Loss 600.63745 Test MSE 619.3601169833557 Test RE 0.4189486435603026 Lambda1 -1.0805076\n",
      "56 Train Loss 599.65515 Test MSE 618.124189182084 Test RE 0.4185304306085475 Lambda1 -1.0414553\n",
      "57 Train Loss 598.97046 Test MSE 617.2823671201801 Test RE 0.41824533564160243 Lambda1 -1.0062855\n",
      "58 Train Loss 598.5982 Test MSE 616.903921644708 Test RE 0.41811710638141714 Lambda1 -1.0021565\n",
      "59 Train Loss 597.00824 Test MSE 615.0731472770381 Test RE 0.41749622619065047 Lambda1 -1.0039722\n",
      "60 Train Loss 595.3301 Test MSE 612.8618023063632 Test RE 0.4167450476605462 Lambda1 -0.99870145\n",
      "61 Train Loss 592.7992 Test MSE 610.5608682167029 Test RE 0.41596199624075275 Lambda1 -0.95623684\n",
      "62 Train Loss 586.88824 Test MSE 596.7805910403667 Test RE 0.4112411034143778 Lambda1 -0.8958972\n",
      "63 Train Loss 568.27057 Test MSE 567.7283431360548 Test RE 0.4011062773866739 Lambda1 -0.93818355\n",
      "64 Train Loss 538.9679 Test MSE 531.8543135298779 Test RE 0.3882268012042871 Lambda1 -0.9725329\n",
      "65 Train Loss 477.27618 Test MSE 454.292600100011 Test RE 0.35880377363845406 Lambda1 -1.0888021\n",
      "66 Train Loss 442.47113 Test MSE 424.02773939337953 Test RE 0.34664608918406664 Lambda1 -1.1124113\n",
      "67 Train Loss 397.55667 Test MSE 388.94132270832927 Test RE 0.33199474846831845 Lambda1 -1.1484209\n",
      "68 Train Loss 374.66663 Test MSE 365.85750372700915 Test RE 0.32199205368527684 Lambda1 -1.180544\n",
      "69 Train Loss 352.14316 Test MSE 336.61929018963735 Test RE 0.3088578689416033 Lambda1 -1.187618\n",
      "70 Train Loss 344.30176 Test MSE 330.8705229369168 Test RE 0.3062091821576987 Lambda1 -1.1609169\n",
      "71 Train Loss 335.47003 Test MSE 325.3511927851469 Test RE 0.30364446730188055 Lambda1 -1.1364342\n",
      "72 Train Loss 321.93268 Test MSE 314.0559155134664 Test RE 0.2983270679069847 Lambda1 -1.1587496\n",
      "73 Train Loss 316.43964 Test MSE 307.60245361383824 Test RE 0.29524603113718173 Lambda1 -1.1632332\n",
      "74 Train Loss 312.08377 Test MSE 306.8146035953287 Test RE 0.2948676877220865 Lambda1 -1.1527565\n",
      "Training time: 123.86\n",
      "Training time: 123.86\n",
      "inv_HT_atanh_tune10\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 868.2292 Test MSE 870.3656565892147 Test RE 0.49663811131786556 Lambda1 -0.037690718\n",
      "1 Train Loss 854.7641 Test MSE 858.0530930733171 Test RE 0.49311277198003933 Lambda1 -0.039683796\n",
      "2 Train Loss 854.7218 Test MSE 858.075484347909 Test RE 0.493119205935898 Lambda1 -0.03979393\n",
      "3 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "4 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "5 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "6 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "7 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "8 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "9 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "10 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "11 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "12 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "13 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "14 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "15 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "16 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "17 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "18 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "19 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "20 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "21 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "22 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "23 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "24 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "25 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "26 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "27 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "28 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "29 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "30 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "31 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "32 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "33 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "34 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "35 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "36 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "37 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "38 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "39 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "40 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "41 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "42 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "43 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "45 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "46 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "47 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "48 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "49 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "50 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "51 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "52 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "53 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "54 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "55 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "56 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "57 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "58 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "59 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "60 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "61 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "62 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "63 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "64 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "65 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "66 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "67 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "68 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "69 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "70 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "71 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "72 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "73 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "74 Train Loss 854.72156 Test MSE 858.0777999659432 Test RE 0.4931198713056568 Lambda1 -0.039797924\n",
      "Training time: 121.12\n",
      "Training time: 121.12\n",
      "inv_HT_atanh_tune10\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 894.5287 Test MSE 895.7898904064214 Test RE 0.5038395426223709 Lambda1 -0.04408314\n",
      "1 Train Loss 854.8415 Test MSE 858.0827394409256 Test RE 0.49312129061155324 Lambda1 -0.048155118\n",
      "2 Train Loss 854.71985 Test MSE 858.0707014222148 Test RE 0.4931178316070364 Lambda1 -0.04837196\n",
      "3 Train Loss 854.7192 Test MSE 858.0792357304786 Test RE 0.4931202838578325 Lambda1 -0.048379064\n",
      "4 Train Loss 854.53406 Test MSE 857.5963168285209 Test RE 0.49298150260613843 Lambda1 -0.050600696\n",
      "5 Train Loss 854.50073 Test MSE 857.6888763368369 Test RE 0.49300810538734086 Lambda1 -0.06823355\n",
      "6 Train Loss 852.9784 Test MSE 855.278049178531 Test RE 0.49231473407876397 Lambda1 -0.61452436\n",
      "7 Train Loss 840.70605 Test MSE 837.5876005352508 Test RE 0.48719664663043044 Lambda1 -0.7134789\n",
      "8 Train Loss 818.3739 Test MSE 820.65306713098 Test RE 0.48224637205077375 Lambda1 -0.49643284\n",
      "9 Train Loss 785.7587 Test MSE 784.5162592451687 Test RE 0.4715091718020512 Lambda1 -0.62475276\n",
      "10 Train Loss 748.6634 Test MSE 745.2447106488668 Test RE 0.45955619166961925 Lambda1 -0.74891555\n",
      "11 Train Loss 710.0938 Test MSE 703.8428845923183 Test RE 0.44660854981889664 Lambda1 -0.8244718\n",
      "12 Train Loss 685.96356 Test MSE 687.771329746247 Test RE 0.4414801735674257 Lambda1 -0.8258738\n",
      "13 Train Loss 670.0117 Test MSE 673.1673973358099 Test RE 0.43676789504113056 Lambda1 -0.88660836\n",
      "14 Train Loss 656.8716 Test MSE 659.2054648377366 Test RE 0.4322147364337004 Lambda1 -0.93260276\n",
      "15 Train Loss 646.188 Test MSE 648.2689721137019 Test RE 0.4286144302511568 Lambda1 -0.9793195\n",
      "16 Train Loss 634.2726 Test MSE 639.6836935022054 Test RE 0.42576681685462436 Lambda1 -1.0446486\n",
      "17 Train Loss 627.8019 Test MSE 634.1461339907077 Test RE 0.4239199401716474 Lambda1 -1.1399437\n",
      "18 Train Loss 620.421 Test MSE 627.9413266921803 Test RE 0.4218409183046399 Lambda1 -1.3110449\n",
      "19 Train Loss 614.98553 Test MSE 629.1807738983853 Test RE 0.4222570334544055 Lambda1 -1.2919797\n",
      "20 Train Loss 613.5276 Test MSE 627.7042982229788 Test RE 0.42176129482688185 Lambda1 -1.3148204\n",
      "21 Train Loss 611.9422 Test MSE 626.3635806276103 Test RE 0.4213106327417211 Lambda1 -1.3460617\n",
      "22 Train Loss 608.1325 Test MSE 625.4348637686991 Test RE 0.4209981756916473 Lambda1 -1.418874\n",
      "23 Train Loss 606.9975 Test MSE 624.969565072909 Test RE 0.4208415435940743 Lambda1 -1.4599526\n",
      "24 Train Loss 605.9931 Test MSE 624.4875718120743 Test RE 0.42067923016013653 Lambda1 -1.4947695\n",
      "25 Train Loss 605.5377 Test MSE 623.8584979900911 Test RE 0.42046729241939296 Lambda1 -1.5336542\n",
      "26 Train Loss 605.222 Test MSE 623.5597609734563 Test RE 0.4203666091830179 Lambda1 -1.5605081\n",
      "27 Train Loss 605.02344 Test MSE 623.676208473106 Test RE 0.42040585831238314 Lambda1 -1.5494007\n",
      "28 Train Loss 604.57513 Test MSE 623.3650756989309 Test RE 0.42030098148964834 Lambda1 -1.5881716\n",
      "29 Train Loss 604.3995 Test MSE 623.1126440751308 Test RE 0.4202158724527672 Lambda1 -1.6161987\n",
      "30 Train Loss 604.071 Test MSE 623.1231773776298 Test RE 0.42021942417189523 Lambda1 -1.6371857\n",
      "31 Train Loss 603.85364 Test MSE 623.2735512056205 Test RE 0.4202701253766214 Lambda1 -1.6321361\n",
      "32 Train Loss 603.7127 Test MSE 623.2162952285605 Test RE 0.4202508212289782 Lambda1 -1.6263536\n",
      "33 Train Loss 603.51843 Test MSE 623.1636149494948 Test RE 0.4202330590181868 Lambda1 -1.6200193\n",
      "34 Train Loss 603.4648 Test MSE 623.0943941295586 Test RE 0.42020971869146334 Lambda1 -1.6157418\n",
      "35 Train Loss 603.41156 Test MSE 623.0560733105748 Test RE 0.4201967968707022 Lambda1 -1.6088433\n",
      "36 Train Loss 603.24817 Test MSE 623.0488266982507 Test RE 0.4201943532606715 Lambda1 -1.6197016\n",
      "37 Train Loss 603.13007 Test MSE 622.930391484449 Test RE 0.420154414036688 Lambda1 -1.6257713\n",
      "38 Train Loss 603.0942 Test MSE 622.9971247172138 Test RE 0.42017691856673867 Lambda1 -1.6235214\n",
      "39 Train Loss 602.9264 Test MSE 622.891881829435 Test RE 0.4201414268299082 Lambda1 -1.6084515\n",
      "40 Train Loss 602.8856 Test MSE 622.7275239436136 Test RE 0.42008599336349245 Lambda1 -1.6058114\n",
      "41 Train Loss 602.87476 Test MSE 622.6792974960068 Test RE 0.42006972649987684 Lambda1 -1.6042547\n",
      "42 Train Loss 602.8547 Test MSE 622.6467530665939 Test RE 0.42005874885195227 Lambda1 -1.6024743\n",
      "43 Train Loss 602.70746 Test MSE 622.4260770580235 Test RE 0.41998430447207125 Lambda1 -1.5978023\n",
      "44 Train Loss 602.45905 Test MSE 622.2834627757586 Test RE 0.4199361869576616 Lambda1 -1.6100684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 Train Loss 602.359 Test MSE 622.3812084770776 Test RE 0.41996916657853683 Lambda1 -1.6192329\n",
      "46 Train Loss 602.30255 Test MSE 622.2704905413784 Test RE 0.41993180990173923 Lambda1 -1.6249443\n",
      "47 Train Loss 602.28064 Test MSE 622.2166333320787 Test RE 0.4199136370612292 Lambda1 -1.6334089\n",
      "48 Train Loss 602.2529 Test MSE 622.1301340809836 Test RE 0.4198844482902837 Lambda1 -1.636479\n",
      "49 Train Loss 602.2083 Test MSE 621.99871835715 Test RE 0.41984009878047773 Lambda1 -1.6350412\n",
      "50 Train Loss 602.1747 Test MSE 621.9483403447339 Test RE 0.41982309622293934 Lambda1 -1.6319505\n",
      "51 Train Loss 602.1375 Test MSE 621.8859927667095 Test RE 0.41980205298866724 Lambda1 -1.6366823\n",
      "52 Train Loss 602.0927 Test MSE 621.9018138334725 Test RE 0.41980739293372415 Lambda1 -1.642107\n",
      "53 Train Loss 602.0395 Test MSE 621.9344421689652 Test RE 0.41981840547340177 Lambda1 -1.642096\n",
      "54 Train Loss 602.0105 Test MSE 621.971015650198 Test RE 0.4198307492140032 Lambda1 -1.6403867\n",
      "55 Train Loss 601.97626 Test MSE 622.0556577852707 Test RE 0.41985931498410806 Lambda1 -1.6485145\n",
      "56 Train Loss 601.8542 Test MSE 622.0488576444459 Test RE 0.41985702008474035 Lambda1 -1.6794542\n",
      "57 Train Loss 601.81177 Test MSE 621.8806721900677 Test RE 0.4198002571661205 Lambda1 -1.6927236\n",
      "58 Train Loss 601.7742 Test MSE 621.8431909244517 Test RE 0.41978760612007554 Lambda1 -1.7052653\n",
      "59 Train Loss 601.7557 Test MSE 621.8189568831831 Test RE 0.4197794262047556 Lambda1 -1.7010424\n",
      "60 Train Loss 601.7477 Test MSE 621.7424586522072 Test RE 0.4197536040817301 Lambda1 -1.6997601\n",
      "61 Train Loss 601.73816 Test MSE 621.7200600280123 Test RE 0.4197460430829757 Lambda1 -1.699889\n",
      "62 Train Loss 601.72034 Test MSE 621.7371332497814 Test RE 0.419751806422895 Lambda1 -1.68783\n",
      "63 Train Loss 601.697 Test MSE 621.7115800556359 Test RE 0.41974318050283976 Lambda1 -1.6793532\n",
      "64 Train Loss 601.67145 Test MSE 621.7217732403448 Test RE 0.41974662140885605 Lambda1 -1.6815797\n",
      "65 Train Loss 601.6033 Test MSE 621.6216286580008 Test RE 0.4197128144514994 Lambda1 -1.6794919\n",
      "66 Train Loss 601.5639 Test MSE 621.4149857835185 Test RE 0.41964304703390515 Lambda1 -1.6770132\n",
      "67 Train Loss 601.5182 Test MSE 621.327968351459 Test RE 0.41961366446379283 Lambda1 -1.6682689\n",
      "68 Train Loss 601.40204 Test MSE 621.1584703267533 Test RE 0.41955642533874193 Lambda1 -1.6449194\n",
      "69 Train Loss 601.3431 Test MSE 620.9526579168186 Test RE 0.41948691242293146 Lambda1 -1.637186\n",
      "70 Train Loss 601.29956 Test MSE 620.853869730288 Test RE 0.4194535427294817 Lambda1 -1.6258978\n",
      "71 Train Loss 601.11865 Test MSE 620.6373660487473 Test RE 0.41938040059481424 Lambda1 -1.6050448\n",
      "72 Train Loss 600.9422 Test MSE 620.3070388644662 Test RE 0.41926878051481464 Lambda1 -1.6151677\n",
      "73 Train Loss 600.77905 Test MSE 619.4737437209785 Test RE 0.41898707159527604 Lambda1 -1.6452345\n",
      "74 Train Loss 600.3677 Test MSE 618.2968995568584 Test RE 0.41858889741847827 Lambda1 -1.6710712\n",
      "Training time: 128.08\n",
      "Training time: 128.08\n",
      "inv_HT_atanh_tune10\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 1071.4634 Test MSE 1069.9291205728434 Test RE 0.5506386282479154 Lambda1 0.0050533987\n",
      "1 Train Loss 855.19525 Test MSE 858.3217268486587 Test RE 0.49318995623608236 Lambda1 0.0066684578\n",
      "2 Train Loss 854.705 Test MSE 858.0545751026235 Test RE 0.49311319783204666 Lambda1 0.006700924\n",
      "3 Train Loss 854.5412 Test MSE 857.7007855864654 Test RE 0.4930115281530877 Lambda1 -0.001980118\n",
      "4 Train Loss 854.52277 Test MSE 857.6742308530385 Test RE 0.49300389618442236 Lambda1 -0.040557362\n",
      "5 Train Loss 853.3749 Test MSE 856.2624113487221 Test RE 0.49259796164514175 Lambda1 -1.1599468\n",
      "6 Train Loss 845.6901 Test MSE 845.3522223504283 Test RE 0.48944964741307695 Lambda1 -0.69560677\n",
      "7 Train Loss 825.8413 Test MSE 819.7343269188912 Test RE 0.48197635343687817 Lambda1 -1.2327405\n",
      "8 Train Loss 806.40643 Test MSE 809.1378664865348 Test RE 0.4788510385091768 Lambda1 -1.7704\n",
      "9 Train Loss 799.5667 Test MSE 800.5418677896033 Test RE 0.47630067360285194 Lambda1 -1.8629607\n",
      "10 Train Loss 783.7217 Test MSE 786.7066577734562 Test RE 0.47216694854623165 Lambda1 -1.5639033\n",
      "11 Train Loss 775.2544 Test MSE 779.5833752009689 Test RE 0.4700244557215654 Lambda1 -1.6818317\n",
      "12 Train Loss 771.3139 Test MSE 773.7475032633522 Test RE 0.46826187626315957 Lambda1 -1.56808\n",
      "13 Train Loss 762.5703 Test MSE 761.1539380820171 Test RE 0.4644355123256035 Lambda1 -1.5486244\n",
      "14 Train Loss 757.48676 Test MSE 755.7424866370287 Test RE 0.462781606922951 Lambda1 -1.5943305\n",
      "15 Train Loss 748.1082 Test MSE 749.6308753722914 Test RE 0.46090657495348664 Lambda1 -1.6682271\n",
      "16 Train Loss 734.94885 Test MSE 734.6948760295357 Test RE 0.4562918125629935 Lambda1 -1.7688923\n",
      "17 Train Loss 727.60675 Test MSE 725.8990458054375 Test RE 0.4535522057653182 Lambda1 -1.6982524\n",
      "18 Train Loss 722.054 Test MSE 720.1638796820423 Test RE 0.45175694544114725 Lambda1 -1.677102\n",
      "19 Train Loss 715.8638 Test MSE 711.0533469623762 Test RE 0.44889034365390307 Lambda1 -1.6606691\n",
      "20 Train Loss 702.4452 Test MSE 700.919438375435 Test RE 0.4456800793584192 Lambda1 -1.6718327\n",
      "21 Train Loss 697.39056 Test MSE 693.2296345212989 Test RE 0.4432285534939609 Lambda1 -1.7002839\n",
      "22 Train Loss 687.81604 Test MSE 685.7552698438826 Test RE 0.44083264462524663 Lambda1 -1.7351043\n",
      "23 Train Loss 683.4989 Test MSE 681.0244963103418 Test RE 0.43930944188894694 Lambda1 -1.7178502\n",
      "24 Train Loss 675.56635 Test MSE 671.4968713500626 Test RE 0.4362256188213461 Lambda1 -1.7637218\n",
      "25 Train Loss 668.9638 Test MSE 665.0570446484807 Test RE 0.43412882189061675 Lambda1 -1.8031187\n",
      "26 Train Loss 666.29553 Test MSE 663.212509422374 Test RE 0.43352637589496157 Lambda1 -1.8097938\n",
      "27 Train Loss 661.3102 Test MSE 659.9989466333847 Test RE 0.43247478538841266 Lambda1 -1.8688904\n",
      "28 Train Loss 655.85693 Test MSE 657.2100327377664 Test RE 0.43156007801273194 Lambda1 -1.9209142\n",
      "29 Train Loss 653.6725 Test MSE 655.5157428186457 Test RE 0.4310034372792317 Lambda1 -1.9319628\n",
      "30 Train Loss 649.8718 Test MSE 653.7739983933598 Test RE 0.4304304554948814 Lambda1 -1.9583042\n",
      "31 Train Loss 646.66394 Test MSE 650.280202479001 Test RE 0.4292787955190957 Lambda1 -1.9733399\n",
      "32 Train Loss 645.47363 Test MSE 649.8679285021308 Test RE 0.4291426937774224 Lambda1 -1.9745758\n",
      "33 Train Loss 642.45874 Test MSE 647.7212485086427 Test RE 0.42843332344230406 Lambda1 -2.010796\n",
      "34 Train Loss 639.6989 Test MSE 645.3345625968485 Test RE 0.4276432617728759 Lambda1 -2.0450878\n",
      "35 Train Loss 639.22144 Test MSE 645.3052292656325 Test RE 0.4276335425166034 Lambda1 -2.026997\n",
      "36 Train Loss 638.0724 Test MSE 644.7200004923021 Test RE 0.42743958761817985 Lambda1 -2.0128212\n",
      "37 Train Loss 635.9538 Test MSE 642.2115731474402 Test RE 0.4266072525793947 Lambda1 -2.0385046\n",
      "38 Train Loss 632.1718 Test MSE 638.4899547687618 Test RE 0.42536936129519193 Lambda1 -2.081304\n",
      "39 Train Loss 629.6559 Test MSE 636.096025152745 Test RE 0.42457118068526156 Lambda1 -2.0943472\n",
      "40 Train Loss 628.93884 Test MSE 635.8584224029853 Test RE 0.42449187764122354 Lambda1 -2.1032014\n",
      "41 Train Loss 627.58453 Test MSE 634.8117143378607 Test RE 0.42414234851723515 Lambda1 -2.1701188\n",
      "42 Train Loss 625.6567 Test MSE 633.2050450140453 Test RE 0.42360526940188165 Lambda1 -2.2321825\n",
      "43 Train Loss 624.4022 Test MSE 631.2218320075202 Test RE 0.422941378359413 Lambda1 -2.286801\n",
      "44 Train Loss 622.02795 Test MSE 629.0028218711069 Test RE 0.4221973154750133 Lambda1 -2.2692006\n",
      "45 Train Loss 620.844 Test MSE 628.2487723069582 Test RE 0.42194417418549446 Lambda1 -2.234201\n",
      "46 Train Loss 618.9348 Test MSE 626.9312900474 Test RE 0.4215015185552451 Lambda1 -2.2557762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 Train Loss 617.59686 Test MSE 625.6672362764355 Test RE 0.4210763767342035 Lambda1 -2.2333214\n",
      "48 Train Loss 616.0706 Test MSE 624.6954692649009 Test RE 0.4207492482571078 Lambda1 -2.1281574\n",
      "49 Train Loss 614.1737 Test MSE 623.5695412774905 Test RE 0.42036990581742245 Lambda1 -2.08814\n",
      "50 Train Loss 612.847 Test MSE 622.433837627911 Test RE 0.41998692270057025 Lambda1 -2.077092\n",
      "51 Train Loss 611.8091 Test MSE 621.5501941304921 Test RE 0.41968869781316787 Lambda1 -2.0543036\n",
      "52 Train Loss 611.05835 Test MSE 620.1697447374451 Test RE 0.41922237903691517 Lambda1 -2.0542414\n",
      "53 Train Loss 609.9585 Test MSE 618.4438397247653 Test RE 0.418638633935826 Lambda1 -2.0764797\n",
      "54 Train Loss 609.0369 Test MSE 616.3441787111888 Test RE 0.4179273756958736 Lambda1 -2.0961583\n",
      "55 Train Loss 608.3324 Test MSE 615.1659885524973 Test RE 0.4175277341663648 Lambda1 -2.1063807\n",
      "56 Train Loss 604.8793 Test MSE 613.9845263446861 Test RE 0.4171265982736559 Lambda1 -2.1398861\n",
      "57 Train Loss 602.5618 Test MSE 614.4004575197167 Test RE 0.41726786126632576 Lambda1 -2.134474\n",
      "58 Train Loss 601.41565 Test MSE 613.4922460416717 Test RE 0.41695934293097897 Lambda1 -2.1443613\n",
      "59 Train Loss 599.49603 Test MSE 610.43358492243 Test RE 0.4159186362945359 Lambda1 -2.2147622\n",
      "60 Train Loss 598.40393 Test MSE 609.8992272121352 Test RE 0.415736554250909 Lambda1 -2.245118\n",
      "61 Train Loss 596.74884 Test MSE 607.790952903142 Test RE 0.4150173817874979 Lambda1 -2.3115847\n",
      "62 Train Loss 595.9625 Test MSE 606.5981443693346 Test RE 0.4146099395642053 Lambda1 -2.3364186\n",
      "63 Train Loss 594.7777 Test MSE 604.9456754153583 Test RE 0.4140448230231331 Lambda1 -2.4055812\n",
      "64 Train Loss 593.3824 Test MSE 602.6288412405027 Test RE 0.41325120348068356 Lambda1 -2.5136843\n",
      "65 Train Loss 592.2639 Test MSE 600.089101212723 Test RE 0.41237947389926133 Lambda1 -2.5996616\n",
      "66 Train Loss 590.67175 Test MSE 598.1483047864643 Test RE 0.41171207903167584 Lambda1 -2.6458564\n",
      "67 Train Loss 589.4272 Test MSE 598.14795390376 Test RE 0.41171195827343937 Lambda1 -2.6432798\n",
      "68 Train Loss 587.9107 Test MSE 598.5464135884106 Test RE 0.4118490675824922 Lambda1 -2.6264567\n",
      "69 Train Loss 586.94366 Test MSE 598.335926873855 Test RE 0.4117766451455594 Lambda1 -2.642587\n",
      "70 Train Loss 585.89343 Test MSE 597.7825293297809 Test RE 0.41158617613924947 Lambda1 -2.6531668\n",
      "71 Train Loss 585.45325 Test MSE 597.9487061219779 Test RE 0.4116433803183904 Lambda1 -2.6541822\n",
      "72 Train Loss 584.99786 Test MSE 597.8332211736262 Test RE 0.4116036269837018 Lambda1 -2.6744368\n",
      "73 Train Loss 584.055 Test MSE 597.1372400008779 Test RE 0.4113639683396974 Lambda1 -2.7176514\n",
      "74 Train Loss 583.4323 Test MSE 596.3009806590438 Test RE 0.41107582060439013 Lambda1 -2.738636\n",
      "Training time: 124.48\n",
      "Training time: 124.48\n",
      "inv_HT_atanh_tune10\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 896.88916 Test MSE 898.0893976863548 Test RE 0.5044858103109184 Lambda1 -0.005927385\n",
      "1 Train Loss 854.849 Test MSE 858.0861789276672 Test RE 0.49312227890927995 Lambda1 -0.0062238756\n",
      "2 Train Loss 854.7181 Test MSE 858.0691919803122 Test RE 0.49311739788226067 Lambda1 -0.00623866\n",
      "3 Train Loss 854.71606 Test MSE 858.0839698012551 Test RE 0.49312164414194626 Lambda1 -0.0062386394\n",
      "4 Train Loss 854.47473 Test MSE 857.6125660003277 Test RE 0.4929861729288952 Lambda1 -0.0073390007\n",
      "5 Train Loss 853.26776 Test MSE 853.5183931647722 Test RE 0.4918080271500608 Lambda1 -0.06423776\n",
      "6 Train Loss 826.0009 Test MSE 826.0191475007983 Test RE 0.4838204576421504 Lambda1 -0.045742728\n",
      "7 Train Loss 747.9239 Test MSE 758.228864943903 Test RE 0.4635422531205627 Lambda1 0.01645564\n",
      "8 Train Loss 696.41223 Test MSE 697.4914372287775 Test RE 0.4445888951780265 Lambda1 0.00042298133\n",
      "9 Train Loss 663.5483 Test MSE 676.6753631514999 Test RE 0.4379044443997604 Lambda1 0.00014859626\n",
      "10 Train Loss 648.98737 Test MSE 668.6155092765758 Test RE 0.4352887006773151 Lambda1 7.1469905e-05\n",
      "11 Train Loss 645.3022 Test MSE 666.5949450376194 Test RE 0.43463047912649566 Lambda1 0.000116335745\n",
      "12 Train Loss 636.27765 Test MSE 655.6619996644067 Test RE 0.4310515167404499 Lambda1 0.0001525647\n",
      "13 Train Loss 631.6385 Test MSE 653.0698829711247 Test RE 0.43019860598032655 Lambda1 -6.4023945e-05\n",
      "14 Train Loss 630.2789 Test MSE 652.0522479507828 Test RE 0.4298633004836334 Lambda1 -7.565998e-05\n",
      "15 Train Loss 628.556 Test MSE 649.8132338215981 Test RE 0.4291246344799916 Lambda1 2.193962e-05\n",
      "16 Train Loss 625.8895 Test MSE 647.8448214513264 Test RE 0.4284741899738291 Lambda1 -4.2629613e-06\n",
      "17 Train Loss 624.98596 Test MSE 647.6544018460972 Test RE 0.4284112151072643 Lambda1 4.7261665e-06\n",
      "18 Train Loss 622.8721 Test MSE 645.9419006210752 Test RE 0.4278444465145001 Lambda1 -1.2191022e-05\n",
      "19 Train Loss 621.258 Test MSE 643.5392544174244 Test RE 0.42704799987176745 Lambda1 -6.5746426e-05\n",
      "20 Train Loss 617.72534 Test MSE 638.5745644339414 Test RE 0.42539754432824445 Lambda1 0.00026220153\n",
      "21 Train Loss 612.2857 Test MSE 633.746170545253 Test RE 0.42378623343285343 Lambda1 -0.00018963723\n",
      "22 Train Loss 605.51086 Test MSE 625.7339281802712 Test RE 0.4210988180858424 Lambda1 3.720847e-05\n",
      "23 Train Loss 539.0277 Test MSE 525.1715471996 Test RE 0.38578004999179993 Lambda1 0.0009053216\n",
      "24 Train Loss 503.86493 Test MSE 471.5129653892411 Test RE 0.3655409113276388 Lambda1 -0.00033399445\n",
      "25 Train Loss 407.23856 Test MSE 389.53878296390485 Test RE 0.3322496423821572 Lambda1 0.00014037624\n",
      "26 Train Loss 335.732 Test MSE 340.05896822307903 Test RE 0.31043185998591544 Lambda1 -8.593406e-06\n",
      "27 Train Loss 311.28345 Test MSE 320.10608305328935 Test RE 0.3011869386732861 Lambda1 1.3847757e-05\n",
      "28 Train Loss 294.96295 Test MSE 297.00844050846086 Test RE 0.29011725948207184 Lambda1 6.050057e-05\n",
      "29 Train Loss 283.78714 Test MSE 289.49843358586537 Test RE 0.2864258956033762 Lambda1 0.00014685036\n",
      "30 Train Loss 279.482 Test MSE 284.14434508116057 Test RE 0.28376490290655426 Lambda1 8.3934385e-05\n",
      "31 Train Loss 276.7055 Test MSE 282.5637114488545 Test RE 0.28297454078152917 Lambda1 -0.00041407134\n",
      "32 Train Loss 275.47137 Test MSE 281.6111781777683 Test RE 0.28249717904428356 Lambda1 -0.00030922095\n",
      "33 Train Loss 273.34293 Test MSE 280.43268205755663 Test RE 0.2819054573874639 Lambda1 0.00014282111\n",
      "34 Train Loss 272.57587 Test MSE 280.06403018139235 Test RE 0.281720102470985 Lambda1 -1.8176594e-05\n",
      "35 Train Loss 272.46854 Test MSE 279.9771808127355 Test RE 0.28167641762118545 Lambda1 -7.2034854e-05\n",
      "36 Train Loss 272.2009 Test MSE 279.98148613752613 Test RE 0.28167858334018825 Lambda1 9.658888e-05\n",
      "37 Train Loss 272.02542 Test MSE 280.19452804679975 Test RE 0.2817857295891663 Lambda1 4.6012763e-05\n",
      "38 Train Loss 271.90918 Test MSE 279.9960055662873 Test RE 0.28168588696436575 Lambda1 -2.5503314e-06\n",
      "39 Train Loss 271.71497 Test MSE 279.7190666074093 Test RE 0.2815465473063376 Lambda1 9.647622e-05\n",
      "40 Train Loss 271.55844 Test MSE 279.5664351574731 Test RE 0.28146972250851227 Lambda1 0.00015936504\n",
      "41 Train Loss 271.47296 Test MSE 279.5749174771822 Test RE 0.28147399250978006 Lambda1 0.00011120584\n",
      "42 Train Loss 271.40558 Test MSE 279.56305528588183 Test RE 0.28146802106249685 Lambda1 9.495386e-05\n",
      "43 Train Loss 271.1735 Test MSE 279.42572727002215 Test RE 0.2813988806804463 Lambda1 0.00012253133\n",
      "44 Train Loss 271.0928 Test MSE 279.455702946698 Test RE 0.28141397395001955 Lambda1 0.00013554945\n",
      "45 Train Loss 271.04996 Test MSE 279.5653893375143 Test RE 0.281469196038071 Lambda1 0.00011686522\n",
      "46 Train Loss 271.0002 Test MSE 279.45961249578386 Test RE 0.28141594241565676 Lambda1 0.000107344036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 Train Loss 270.93008 Test MSE 279.4074146608854 Test RE 0.28138965956792306 Lambda1 7.1096576e-05\n",
      "48 Train Loss 270.88385 Test MSE 279.39433421645066 Test RE 0.2813830728692665 Lambda1 7.697031e-05\n",
      "49 Train Loss 270.7587 Test MSE 279.14236262270674 Test RE 0.28125616168093476 Lambda1 0.000109006614\n",
      "50 Train Loss 270.69736 Test MSE 279.2040979110822 Test RE 0.2812872613502311 Lambda1 0.00012248642\n",
      "51 Train Loss 270.68304 Test MSE 279.2747582250178 Test RE 0.2813228528565303 Lambda1 0.000117715506\n",
      "52 Train Loss 270.6434 Test MSE 279.2977087180411 Test RE 0.281334412020651 Lambda1 0.0001470154\n",
      "53 Train Loss 270.57172 Test MSE 279.1970127151842 Test RE 0.2812836922980413 Lambda1 0.00018798043\n",
      "54 Train Loss 270.50757 Test MSE 279.1017990285666 Test RE 0.2812357255579812 Lambda1 0.00013858853\n",
      "55 Train Loss 270.49274 Test MSE 279.08128836619346 Test RE 0.28122539162782734 Lambda1 0.00017500858\n",
      "56 Train Loss 270.47894 Test MSE 279.0842403828415 Test RE 0.2812268789719761 Lambda1 0.00019941495\n",
      "57 Train Loss 270.44675 Test MSE 279.10915647294263 Test RE 0.28123943238211907 Lambda1 0.00019666494\n",
      "58 Train Loss 270.38248 Test MSE 279.0641213190771 Test RE 0.28121674202644803 Lambda1 0.00023500696\n",
      "59 Train Loss 270.37158 Test MSE 279.107469080834 Test RE 0.2812385822453445 Lambda1 0.00022957334\n",
      "60 Train Loss 270.36548 Test MSE 279.1100608639807 Test RE 0.2812398880286112 Lambda1 0.00021381694\n",
      "61 Train Loss 270.3446 Test MSE 279.00258246410385 Test RE 0.28118573354282833 Lambda1 0.00023296384\n",
      "62 Train Loss 270.33572 Test MSE 278.9271502982402 Test RE 0.2811477197679532 Lambda1 0.0002999219\n",
      "63 Train Loss 270.32837 Test MSE 278.9098260520639 Test RE 0.28113898854918157 Lambda1 0.00033535794\n",
      "64 Train Loss 270.30646 Test MSE 278.80691849579785 Test RE 0.28108711874624953 Lambda1 0.00046715583\n",
      "65 Train Loss 270.28207 Test MSE 278.7605198301851 Test RE 0.28106372870636637 Lambda1 0.00062037655\n",
      "66 Train Loss 270.273 Test MSE 278.8024445339582 Test RE 0.28108486346131123 Lambda1 0.0005435127\n",
      "67 Train Loss 270.25296 Test MSE 278.77728366192525 Test RE 0.281072179749262 Lambda1 0.0005840417\n",
      "68 Train Loss 270.22742 Test MSE 278.73289129797746 Test RE 0.2810497999572273 Lambda1 0.0006279132\n",
      "69 Train Loss 270.20758 Test MSE 278.7268729227721 Test RE 0.28104676574000553 Lambda1 0.00068324927\n",
      "70 Train Loss 270.20062 Test MSE 278.71618353595545 Test RE 0.2810413765101025 Lambda1 0.00071405777\n",
      "71 Train Loss 270.18393 Test MSE 278.71842612900974 Test RE 0.28104250715878454 Lambda1 0.00071115786\n",
      "72 Train Loss 270.1368 Test MSE 278.63923183539356 Test RE 0.28100257699633974 Lambda1 0.00090185646\n",
      "73 Train Loss 270.03668 Test MSE 278.54435766146435 Test RE 0.2809547334874792 Lambda1 0.001075578\n",
      "74 Train Loss 270.00302 Test MSE 278.48608013002917 Test RE 0.28092534103280015 Lambda1 0.0011049231\n",
      "Training time: 125.60\n",
      "Training time: 125.60\n",
      "inv_HT_atanh_tune10\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 885.85706 Test MSE 887.360849913415 Test RE 0.5014634711674155 Lambda1 -0.16464286\n",
      "1 Train Loss 854.8191 Test MSE 858.0733531912821 Test RE 0.4931185935684925 Lambda1 -0.1796556\n",
      "2 Train Loss 854.7225 Test MSE 858.0747527240578 Test RE 0.493118995710891 Lambda1 -0.18049392\n",
      "3 Train Loss 854.72015 Test MSE 858.0878870185231 Test RE 0.4931227697091885 Lambda1 -0.18066287\n",
      "4 Train Loss 854.5541 Test MSE 857.7255126460068 Test RE 0.493018634732535 Lambda1 -0.18322566\n",
      "5 Train Loss 854.4454 Test MSE 857.5831435851965 Test RE 0.49297771633158277 Lambda1 -0.18526845\n",
      "6 Train Loss 852.72284 Test MSE 855.7498051412363 Test RE 0.49245049132976587 Lambda1 -0.39775792\n",
      "7 Train Loss 847.4103 Test MSE 850.0864104716813 Test RE 0.49081825539963225 Lambda1 -0.48206916\n",
      "8 Train Loss 837.7467 Test MSE 837.8881535564229 Test RE 0.48728404960208416 Lambda1 -0.70820886\n",
      "9 Train Loss 828.1033 Test MSE 827.768679041031 Test RE 0.48433255922056706 Lambda1 -0.76459527\n",
      "10 Train Loss 807.2358 Test MSE 806.9406179601378 Test RE 0.47820042626703335 Lambda1 -0.7413635\n",
      "11 Train Loss 796.7139 Test MSE 801.8182708194432 Test RE 0.4766802349394998 Lambda1 -0.68607885\n",
      "12 Train Loss 782.4143 Test MSE 785.8575250622133 Test RE 0.4719120627877665 Lambda1 -0.56740063\n",
      "13 Train Loss 774.6769 Test MSE 780.1252165564656 Test RE 0.4701877701653634 Lambda1 -0.520769\n",
      "14 Train Loss 764.0605 Test MSE 767.2924307855135 Test RE 0.466304522605678 Lambda1 -0.43111697\n",
      "15 Train Loss 752.90314 Test MSE 754.9175428377638 Test RE 0.4625289593199066 Lambda1 -0.42189726\n",
      "16 Train Loss 743.51044 Test MSE 744.622894044155 Test RE 0.4593644296218325 Lambda1 -0.41436937\n",
      "17 Train Loss 725.8473 Test MSE 729.8729049124794 Test RE 0.4547919736121987 Lambda1 -0.52376515\n",
      "18 Train Loss 718.35205 Test MSE 723.0178605541072 Test RE 0.4526512077580069 Lambda1 -0.57476044\n",
      "19 Train Loss 700.72363 Test MSE 699.3858547541233 Test RE 0.44519224730307383 Lambda1 -0.7237483\n",
      "20 Train Loss 678.43506 Test MSE 684.4932117051317 Test RE 0.4404268054974423 Lambda1 -0.8690106\n",
      "21 Train Loss 654.49835 Test MSE 659.5977745057057 Test RE 0.43234332820716 Lambda1 -1.0414557\n",
      "22 Train Loss 651.0616 Test MSE 655.3536003565229 Test RE 0.4309501294210525 Lambda1 -1.0631856\n",
      "23 Train Loss 643.4512 Test MSE 645.5804900677834 Test RE 0.4277247382768662 Lambda1 -1.1310731\n",
      "24 Train Loss 634.06085 Test MSE 637.7255893937408 Test RE 0.42511467053891067 Lambda1 -1.1809573\n",
      "25 Train Loss 628.3222 Test MSE 635.2477072630948 Test RE 0.42428797541602253 Lambda1 -1.1613412\n",
      "26 Train Loss 624.7614 Test MSE 632.7874984538327 Test RE 0.42346557998156886 Lambda1 -1.1508055\n",
      "27 Train Loss 620.0172 Test MSE 629.9919112785548 Test RE 0.4225291318393511 Lambda1 -1.1708299\n",
      "28 Train Loss 616.8193 Test MSE 628.596259125225 Test RE 0.4220608475294098 Lambda1 -1.187717\n",
      "29 Train Loss 614.63727 Test MSE 628.5072121164116 Test RE 0.42203095187972184 Lambda1 -1.1605208\n",
      "30 Train Loss 613.76654 Test MSE 628.107686199715 Test RE 0.42189679342949077 Lambda1 -1.1484472\n",
      "31 Train Loss 611.2045 Test MSE 627.0855016343933 Test RE 0.42155335551285456 Lambda1 -1.1404521\n",
      "32 Train Loss 610.251 Test MSE 626.6201998705374 Test RE 0.42139692874254364 Lambda1 -1.1540775\n",
      "33 Train Loss 609.3653 Test MSE 626.5905943500567 Test RE 0.42138697389049135 Lambda1 -1.1548234\n",
      "34 Train Loss 608.73334 Test MSE 625.9692818363586 Test RE 0.4211780033629638 Lambda1 -1.1479484\n",
      "35 Train Loss 607.65454 Test MSE 625.536743214948 Test RE 0.4210324632865404 Lambda1 -1.1467427\n",
      "36 Train Loss 606.6295 Test MSE 624.9354369497344 Test RE 0.42083005285202796 Lambda1 -1.1528687\n",
      "37 Train Loss 605.60785 Test MSE 623.7856907056139 Test RE 0.42044275642677875 Lambda1 -1.1510847\n",
      "38 Train Loss 604.91455 Test MSE 623.1034000409935 Test RE 0.4202127554366756 Lambda1 -1.1672184\n",
      "39 Train Loss 604.749 Test MSE 623.1680611529046 Test RE 0.4202345581736914 Lambda1 -1.1746429\n",
      "40 Train Loss 604.61725 Test MSE 622.9072682303564 Test RE 0.42014661587210417 Lambda1 -1.1696959\n",
      "41 Train Loss 604.53094 Test MSE 622.8561667393672 Test RE 0.4201293817186798 Lambda1 -1.166132\n",
      "42 Train Loss 604.2361 Test MSE 622.8999328044968 Test RE 0.4201441420179837 Lambda1 -1.1675477\n",
      "43 Train Loss 603.6932 Test MSE 622.5656513215903 Test RE 0.4200313909585796 Lambda1 -1.1600925\n",
      "44 Train Loss 603.5666 Test MSE 622.2657130594607 Test RE 0.41993019788533337 Lambda1 -1.15132\n",
      "45 Train Loss 603.3308 Test MSE 621.8815527736346 Test RE 0.4198005543847779 Lambda1 -1.128266\n",
      "46 Train Loss 602.77246 Test MSE 621.8181342853445 Test RE 0.41977914854374276 Lambda1 -1.0911223\n",
      "47 Train Loss 602.5447 Test MSE 621.7308609466039 Test RE 0.41974968911552446 Lambda1 -1.0829859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 Train Loss 602.3885 Test MSE 621.5890313996166 Test RE 0.4197018096329773 Lambda1 -1.0715369\n",
      "49 Train Loss 602.3279 Test MSE 621.6593789477819 Test RE 0.4197255585703463 Lambda1 -1.0682352\n",
      "50 Train Loss 602.26215 Test MSE 621.716385276667 Test RE 0.4197448026013546 Lambda1 -1.0629485\n",
      "51 Train Loss 602.04175 Test MSE 621.5143705442086 Test RE 0.4196766030772464 Lambda1 -1.0424591\n",
      "52 Train Loss 601.84485 Test MSE 621.3419699368302 Test RE 0.4196183924204808 Lambda1 -1.024952\n",
      "53 Train Loss 601.7504 Test MSE 621.1643936299804 Test RE 0.4195584257574131 Lambda1 -1.0179846\n",
      "54 Train Loss 601.6371 Test MSE 620.9224942401107 Test RE 0.41947672370635647 Lambda1 -1.0072577\n",
      "55 Train Loss 601.56335 Test MSE 620.9655035931598 Test RE 0.4194912513730667 Lambda1 -0.99757785\n",
      "56 Train Loss 601.4061 Test MSE 620.9935323292109 Test RE 0.4195007186275408 Lambda1 -0.98702115\n",
      "57 Train Loss 601.25616 Test MSE 620.7356601206019 Test RE 0.4194136091779954 Lambda1 -0.96039915\n",
      "58 Train Loss 601.17084 Test MSE 620.6816863471403 Test RE 0.41939537450203707 Lambda1 -0.93988293\n",
      "59 Train Loss 601.1003 Test MSE 620.6052833031582 Test RE 0.4193695608925022 Lambda1 -0.9182544\n",
      "60 Train Loss 601.0574 Test MSE 620.6112744064767 Test RE 0.4193715851101265 Lambda1 -0.912098\n",
      "61 Train Loss 600.9087 Test MSE 620.3847793379135 Test RE 0.41929505228854824 Lambda1 -0.87324154\n",
      "62 Train Loss 600.60223 Test MSE 619.584259496633 Test RE 0.4190244441365986 Lambda1 -0.81020254\n",
      "63 Train Loss 600.38855 Test MSE 619.0017109864684 Test RE 0.4188274092163209 Lambda1 -0.7695617\n",
      "64 Train Loss 599.9758 Test MSE 618.6120215436595 Test RE 0.41869555310643863 Lambda1 -0.744704\n",
      "65 Train Loss 599.6675 Test MSE 618.1722364253687 Test RE 0.41854669663001404 Lambda1 -0.714164\n",
      "66 Train Loss 599.44147 Test MSE 617.6156003815017 Test RE 0.4183582130376996 Lambda1 -0.67864454\n",
      "67 Train Loss 599.17535 Test MSE 617.164783742627 Test RE 0.4182054988958833 Lambda1 -0.6688078\n",
      "68 Train Loss 598.81036 Test MSE 616.1562779689639 Test RE 0.41786366547287035 Lambda1 -0.6806066\n",
      "69 Train Loss 598.3474 Test MSE 614.7496032276198 Test RE 0.4173864049534712 Lambda1 -0.7092914\n",
      "70 Train Loss 596.64594 Test MSE 611.7140840174739 Test RE 0.41635464149202545 Lambda1 -0.75397956\n",
      "71 Train Loss 593.0615 Test MSE 602.4035323896434 Test RE 0.41317394377133854 Lambda1 -0.8255223\n",
      "72 Train Loss 590.5305 Test MSE 596.1201702750727 Test RE 0.4110134926738891 Lambda1 -0.8419176\n",
      "73 Train Loss 585.1966 Test MSE 591.9223881791484 Test RE 0.4095637906598443 Lambda1 -0.7989103\n",
      "74 Train Loss 583.1225 Test MSE 589.7164014661822 Test RE 0.40879989351865237 Lambda1 -0.7918861\n",
      "Training time: 125.23\n",
      "Training time: 125.23\n",
      "inv_HT_atanh_tune10\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 877.4497 Test MSE 879.2231404410536 Test RE 0.4991587926138897 Lambda1 -0.010154822\n",
      "1 Train Loss 854.7898 Test MSE 858.0589522838296 Test RE 0.49311445558600064 Lambda1 -0.010175699\n",
      "2 Train Loss 854.71936 Test MSE 858.0722266091948 Test RE 0.49311826985563345 Lambda1 -0.010174172\n",
      "3 Train Loss 854.5604 Test MSE 857.8080282640033 Test RE 0.4930423490550958 Lambda1 -0.010069972\n",
      "4 Train Loss 854.377 Test MSE 857.6364547237304 Test RE 0.4929930389245581 Lambda1 -0.041557707\n",
      "5 Train Loss 852.74347 Test MSE 851.7074669599693 Test RE 0.49128601088850105 Lambda1 -0.108291\n",
      "6 Train Loss 789.839 Test MSE 784.7680438841861 Test RE 0.47158482940714835 Lambda1 -0.028439919\n",
      "7 Train Loss 698.7505 Test MSE 707.3254112794024 Test RE 0.44771206814834674 Lambda1 -0.00031370396\n",
      "8 Train Loss 656.8236 Test MSE 672.529951772037 Test RE 0.43656105072587964 Lambda1 5.597262e-05\n",
      "9 Train Loss 649.8261 Test MSE 668.0031504552738 Test RE 0.43508932305947723 Lambda1 3.3852568e-05\n",
      "10 Train Loss 642.5297 Test MSE 664.0188657437377 Test RE 0.4337898438908467 Lambda1 5.0068888e-05\n",
      "11 Train Loss 635.5661 Test MSE 658.2086195932441 Test RE 0.4318878169576693 Lambda1 3.7177997e-05\n",
      "12 Train Loss 634.19446 Test MSE 655.0095113284716 Test RE 0.4308369808244463 Lambda1 5.21439e-05\n",
      "13 Train Loss 630.2541 Test MSE 649.1813571712169 Test RE 0.428915943911764 Lambda1 9.2301816e-05\n",
      "14 Train Loss 626.6598 Test MSE 647.9001187740115 Test RE 0.42849247596546497 Lambda1 3.6950105e-06\n",
      "15 Train Loss 620.2166 Test MSE 640.8126327545035 Test RE 0.42614235635137854 Lambda1 9.1355196e-07\n",
      "16 Train Loss 618.12213 Test MSE 638.8785358250427 Test RE 0.4254987801947733 Lambda1 0.00010285523\n",
      "17 Train Loss 610.43304 Test MSE 618.9660648789713 Test RE 0.4188153496520838 Lambda1 0.00037399595\n",
      "18 Train Loss 601.48474 Test MSE 594.7102814683811 Test RE 0.41052715921913274 Lambda1 -6.933391e-05\n",
      "19 Train Loss 561.54395 Test MSE 540.8486527062165 Test RE 0.3914957454203357 Lambda1 -1.9512758e-05\n",
      "20 Train Loss 451.8347 Test MSE 461.56057647967356 Test RE 0.36166253675882576 Lambda1 6.487247e-05\n",
      "21 Train Loss 383.90466 Test MSE 385.72128230186735 Test RE 0.3306176019887357 Lambda1 1.8163391e-05\n",
      "22 Train Loss 347.49487 Test MSE 335.16222958451135 Test RE 0.30818869654475617 Lambda1 -6.206814e-05\n",
      "23 Train Loss 329.15247 Test MSE 326.5065644952494 Test RE 0.304183133412843 Lambda1 6.538537e-05\n",
      "24 Train Loss 301.29178 Test MSE 311.2613510878498 Test RE 0.2969967998189042 Lambda1 3.420504e-06\n",
      "25 Train Loss 296.69193 Test MSE 304.44009141475044 Test RE 0.29372444540945897 Lambda1 -8.0754864e-05\n",
      "26 Train Loss 291.68076 Test MSE 297.9937519215046 Test RE 0.2905980861557701 Lambda1 -0.00025657416\n",
      "27 Train Loss 288.4226 Test MSE 293.2299929161859 Test RE 0.2882659626509074 Lambda1 -0.000150014\n",
      "28 Train Loss 283.4345 Test MSE 289.915321091123 Test RE 0.28663205288520077 Lambda1 1.46439925e-05\n",
      "29 Train Loss 280.2925 Test MSE 286.43080994325277 Test RE 0.2849043210948418 Lambda1 -0.00013970467\n",
      "30 Train Loss 277.91623 Test MSE 283.3161513094224 Test RE 0.2833510572211437 Lambda1 0.00010384049\n",
      "31 Train Loss 276.82193 Test MSE 283.7399258691638 Test RE 0.2835628914225657 Lambda1 0.000328335\n",
      "32 Train Loss 275.7266 Test MSE 282.0628713485008 Test RE 0.2827236454104062 Lambda1 0.0003805439\n",
      "33 Train Loss 274.38217 Test MSE 280.6257648347504 Test RE 0.28200248909264086 Lambda1 0.0004532243\n",
      "34 Train Loss 273.16064 Test MSE 279.8518982563764 Test RE 0.2816133891090197 Lambda1 0.00031126832\n",
      "35 Train Loss 272.27963 Test MSE 279.3723170007918 Test RE 0.2813719856833501 Lambda1 0.00015040782\n",
      "36 Train Loss 271.90628 Test MSE 279.2131276528686 Test RE 0.2812918098701507 Lambda1 0.00021945481\n",
      "37 Train Loss 271.64575 Test MSE 279.0504296197132 Test RE 0.2812098432852232 Lambda1 7.923032e-05\n",
      "38 Train Loss 271.4271 Test MSE 279.17135354321107 Test RE 0.2812707665285286 Lambda1 2.158479e-05\n",
      "39 Train Loss 271.38092 Test MSE 279.18631387703834 Test RE 0.28127830284652394 Lambda1 0.00010907996\n",
      "40 Train Loss 271.16428 Test MSE 279.38519059553425 Test RE 0.28137846847882975 Lambda1 5.8503258e-05\n",
      "41 Train Loss 270.99222 Test MSE 279.27140159302417 Test RE 0.28132116222732373 Lambda1 7.862529e-05\n",
      "42 Train Loss 270.8737 Test MSE 279.0637983999193 Test RE 0.281216579321369 Lambda1 0.00016970768\n",
      "43 Train Loss 270.79868 Test MSE 279.0080651046834 Test RE 0.2811884962999432 Lambda1 0.00016634741\n",
      "44 Train Loss 270.78128 Test MSE 279.1101074972879 Test RE 0.28123991152318806 Lambda1 0.00013495206\n",
      "45 Train Loss 270.7211 Test MSE 279.0327768809309 Test RE 0.2812009484728707 Lambda1 0.00020201546\n",
      "46 Train Loss 270.69025 Test MSE 278.9933470889564 Test RE 0.28118107968406714 Lambda1 0.00021654557\n",
      "47 Train Loss 270.6441 Test MSE 278.94482317044896 Test RE 0.28115662641081 Lambda1 0.00022412725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 Train Loss 270.59708 Test MSE 278.8397917926548 Test RE 0.2811036893320697 Lambda1 0.0002816944\n",
      "49 Train Loss 270.5798 Test MSE 278.8366119007561 Test RE 0.281102086472795 Lambda1 0.00027803407\n",
      "50 Train Loss 270.5172 Test MSE 278.761724500958 Test RE 0.2810643360177849 Lambda1 0.00030367772\n",
      "51 Train Loss 270.45026 Test MSE 278.6232201937163 Test RE 0.28099450315623065 Lambda1 0.0005519863\n",
      "52 Train Loss 270.19513 Test MSE 278.14024243080684 Test RE 0.2807508533975026 Lambda1 0.0014425184\n",
      "53 Train Loss 269.9768 Test MSE 277.7136511016678 Test RE 0.2805354731347165 Lambda1 0.0025059283\n",
      "54 Train Loss 269.14545 Test MSE 276.46352376382276 Test RE 0.2799033460712269 Lambda1 0.005619646\n",
      "55 Train Loss 266.79044 Test MSE 272.5796110912484 Test RE 0.27793027325522585 Lambda1 0.012369243\n",
      "56 Train Loss 263.5117 Test MSE 270.2364613077562 Test RE 0.2767331223823562 Lambda1 0.02085415\n",
      "57 Train Loss 257.82312 Test MSE 259.53487372297747 Test RE 0.27119834304932966 Lambda1 0.038308192\n",
      "58 Train Loss 251.51553 Test MSE 251.93099947438594 Test RE 0.2671960142152622 Lambda1 0.0495539\n",
      "59 Train Loss 239.17874 Test MSE 232.978535106318 Test RE 0.2569491147335009 Lambda1 0.07839245\n",
      "60 Train Loss 226.6142 Test MSE 217.02373363481004 Test RE 0.24799492073854668 Lambda1 0.10111816\n",
      "61 Train Loss 210.36292 Test MSE 202.57169908610172 Test RE 0.23959544428087698 Lambda1 0.13026847\n",
      "62 Train Loss 199.85226 Test MSE 196.09745945130442 Test RE 0.23573558971408598 Lambda1 0.1436971\n",
      "63 Train Loss 185.44547 Test MSE 182.31832816867885 Test RE 0.2273025644574399 Lambda1 0.16777597\n",
      "64 Train Loss 173.3853 Test MSE 164.2569898431761 Test RE 0.2157501471017189 Lambda1 0.2033929\n",
      "65 Train Loss 165.52028 Test MSE 151.0281747614224 Test RE 0.20687983327156967 Lambda1 0.23055317\n",
      "66 Train Loss 158.77771 Test MSE 142.1426134462752 Test RE 0.20070182367131179 Lambda1 0.24980396\n",
      "67 Train Loss 150.40132 Test MSE 136.58050895688908 Test RE 0.19673586267709242 Lambda1 0.26458645\n",
      "68 Train Loss 144.86586 Test MSE 129.47582966032442 Test RE 0.19155060231578097 Lambda1 0.280554\n",
      "69 Train Loss 137.04567 Test MSE 119.92555310393263 Test RE 0.1843508038668355 Lambda1 0.3038229\n",
      "70 Train Loss 129.62979 Test MSE 117.35966962184627 Test RE 0.18236798927842549 Lambda1 0.33045626\n",
      "71 Train Loss 122.862755 Test MSE 107.25776821239155 Test RE 0.17434261214522498 Lambda1 0.36438498\n",
      "72 Train Loss 114.59326 Test MSE 101.5055020729273 Test RE 0.16960316891129615 Lambda1 0.36774042\n",
      "73 Train Loss 106.61222 Test MSE 98.66270037883422 Test RE 0.16721131778815587 Lambda1 0.36774895\n",
      "74 Train Loss 100.57013 Test MSE 93.0109565787224 Test RE 0.16235147031046873 Lambda1 0.38229015\n",
      "Training time: 124.28\n",
      "Training time: 124.28\n",
      "inv_HT_atanh_tune10\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 878.81 Test MSE 880.536480415335 Test RE 0.4995314628358627 Lambda1 0.0009929856\n",
      "1 Train Loss 854.7882 Test MSE 858.0570674960876 Test RE 0.49311391400515164 Lambda1 0.0010894372\n",
      "2 Train Loss 854.71686 Test MSE 858.0704203857882 Test RE 0.4931177508537346 Lambda1 0.0010907957\n",
      "3 Train Loss 854.55646 Test MSE 857.8282974791402 Test RE 0.4930481740894166 Lambda1 0.00079658243\n",
      "4 Train Loss 854.2095 Test MSE 857.5851217257474 Test RE 0.4929782848937428 Lambda1 -0.15640543\n",
      "5 Train Loss 849.799 Test MSE 851.253675640437 Test RE 0.49115511440138515 Lambda1 -0.5007759\n",
      "6 Train Loss 839.8813 Test MSE 836.4959964360185 Test RE 0.48687906835704214 Lambda1 -0.57134974\n",
      "7 Train Loss 817.50757 Test MSE 820.7629562070781 Test RE 0.48227865843127843 Lambda1 -0.67002153\n",
      "8 Train Loss 796.28296 Test MSE 798.2498644221018 Test RE 0.4756183449814256 Lambda1 -0.63992053\n",
      "9 Train Loss 774.55896 Test MSE 775.1982617589277 Test RE 0.46870066072203853 Lambda1 -0.6100748\n",
      "10 Train Loss 743.9749 Test MSE 734.7250478573659 Test RE 0.45630118176993667 Lambda1 -0.6291441\n",
      "11 Train Loss 725.38153 Test MSE 725.3191459315319 Test RE 0.45337100466152924 Lambda1 -0.6144696\n",
      "12 Train Loss 685.2125 Test MSE 687.6287811418218 Test RE 0.44143442024716767 Lambda1 -0.74518543\n",
      "13 Train Loss 659.8046 Test MSE 662.2013626005297 Test RE 0.43319576841919166 Lambda1 -0.868294\n",
      "14 Train Loss 642.13354 Test MSE 650.4867452165698 Test RE 0.4293469641162353 Lambda1 -1.0233954\n",
      "15 Train Loss 629.05206 Test MSE 634.2690095462051 Test RE 0.42396100868275816 Lambda1 -1.244926\n",
      "16 Train Loss 622.88885 Test MSE 634.4674146649658 Test RE 0.42402731292904405 Lambda1 -1.2374909\n",
      "17 Train Loss 615.76514 Test MSE 631.0467480868773 Test RE 0.4228827180227712 Lambda1 -1.2182164\n",
      "18 Train Loss 611.05756 Test MSE 626.1172922241923 Test RE 0.42122779417289075 Lambda1 -1.2396827\n",
      "19 Train Loss 608.6547 Test MSE 624.2345479511775 Test RE 0.4205939981460995 Lambda1 -1.2780838\n",
      "20 Train Loss 607.3369 Test MSE 623.5625790562798 Test RE 0.42036755907319406 Lambda1 -1.2967895\n",
      "21 Train Loss 605.8788 Test MSE 623.1333620160547 Test RE 0.4202228582966094 Lambda1 -1.2851771\n",
      "22 Train Loss 604.3094 Test MSE 622.8173574050157 Test RE 0.42011629266497097 Lambda1 -1.2577527\n",
      "23 Train Loss 603.5755 Test MSE 622.715142528774 Test RE 0.4200818171510993 Lambda1 -1.2531555\n",
      "24 Train Loss 603.2282 Test MSE 622.6923174023576 Test RE 0.42007411819872736 Lambda1 -1.2619436\n",
      "25 Train Loss 602.9297 Test MSE 622.4304191347743 Test RE 0.419985769385714 Lambda1 -1.2767408\n",
      "26 Train Loss 602.72595 Test MSE 622.344348214485 Test RE 0.41995673014668555 Lambda1 -1.2825451\n",
      "27 Train Loss 602.57996 Test MSE 622.3593241422908 Test RE 0.41996178297945824 Lambda1 -1.2795533\n",
      "28 Train Loss 602.5327 Test MSE 622.4239522490798 Test RE 0.41998358761010085 Lambda1 -1.27811\n",
      "29 Train Loss 602.45844 Test MSE 622.4171360761081 Test RE 0.41998128798088086 Lambda1 -1.2687553\n",
      "30 Train Loss 602.4052 Test MSE 622.3721749716509 Test RE 0.41996611876197354 Lambda1 -1.2623447\n",
      "31 Train Loss 602.3438 Test MSE 622.3935319962044 Test RE 0.41997332437794266 Lambda1 -1.2627221\n",
      "32 Train Loss 602.28345 Test MSE 622.4074350823275 Test RE 0.4199780150538502 Lambda1 -1.2661659\n",
      "33 Train Loss 602.1198 Test MSE 622.2159494365891 Test RE 0.41991340629182666 Lambda1 -1.2510116\n",
      "34 Train Loss 601.9595 Test MSE 621.8468469240511 Test RE 0.4197888401458609 Lambda1 -1.2382399\n",
      "35 Train Loss 601.8413 Test MSE 621.7335514223766 Test RE 0.41975059732614933 Lambda1 -1.2369733\n",
      "36 Train Loss 601.76434 Test MSE 621.6288012036358 Test RE 0.41971523586067544 Lambda1 -1.2281618\n",
      "37 Train Loss 601.72217 Test MSE 621.5524331992241 Test RE 0.41968945375452466 Lambda1 -1.2166284\n",
      "38 Train Loss 601.68365 Test MSE 621.5925501407594 Test RE 0.4197029975721627 Lambda1 -1.2097343\n",
      "39 Train Loss 601.6518 Test MSE 621.5655905071822 Test RE 0.41969389582077055 Lambda1 -1.207961\n",
      "40 Train Loss 601.519 Test MSE 621.3487370126049 Test RE 0.4196206774598504 Lambda1 -1.1875287\n",
      "41 Train Loss 601.3859 Test MSE 621.0607556912358 Test RE 0.4195234237638047 Lambda1 -1.1633515\n",
      "42 Train Loss 601.2745 Test MSE 620.8725307762753 Test RE 0.41945984645360707 Lambda1 -1.1520762\n",
      "43 Train Loss 601.1001 Test MSE 620.6885557019109 Test RE 0.41939769531132537 Lambda1 -1.1360561\n",
      "44 Train Loss 601.0099 Test MSE 620.5148845488657 Test RE 0.4193390166258444 Lambda1 -1.1227679\n",
      "45 Train Loss 600.8684 Test MSE 620.1490145975152 Test RE 0.41921537239778317 Lambda1 -1.1094824\n",
      "46 Train Loss 600.5686 Test MSE 619.8314945461948 Test RE 0.41910803825229714 Lambda1 -1.0932736\n",
      "47 Train Loss 600.3699 Test MSE 619.5838784329808 Test RE 0.41902431528002493 Lambda1 -1.0781857\n",
      "48 Train Loss 600.2104 Test MSE 619.2684147625118 Test RE 0.41891762772464924 Lambda1 -1.0631001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 Train Loss 600.1332 Test MSE 619.1391645423151 Test RE 0.41887390837787497 Lambda1 -1.0574602\n",
      "50 Train Loss 599.96985 Test MSE 618.9024727294604 Test RE 0.41879383469839543 Lambda1 -1.0493649\n",
      "51 Train Loss 599.76996 Test MSE 618.6548921613391 Test RE 0.4187100609282898 Lambda1 -1.0493064\n",
      "52 Train Loss 599.4492 Test MSE 618.003290106288 Test RE 0.4184894983674261 Lambda1 -1.0469973\n",
      "53 Train Loss 599.0536 Test MSE 617.0935653192822 Test RE 0.4181813685531149 Lambda1 -1.046316\n",
      "54 Train Loss 597.4831 Test MSE 610.7451054313188 Test RE 0.41602474993429556 Lambda1 -1.0521282\n",
      "55 Train Loss 595.3038 Test MSE 606.5320509026296 Test RE 0.4145873515007794 Lambda1 -1.0526205\n",
      "56 Train Loss 594.1055 Test MSE 604.7321973713698 Test RE 0.413971760860854 Lambda1 -1.041055\n",
      "57 Train Loss 590.69257 Test MSE 598.9329293247802 Test RE 0.41198202339962303 Lambda1 -1.0608648\n",
      "58 Train Loss 585.3663 Test MSE 594.8444861661808 Test RE 0.4105734772055438 Lambda1 -1.0672628\n",
      "59 Train Loss 579.34204 Test MSE 590.0307699251944 Test RE 0.40890884137023653 Lambda1 -1.0650882\n",
      "60 Train Loss 565.8355 Test MSE 577.9659020320404 Test RE 0.4047065928433724 Lambda1 -1.0707926\n",
      "61 Train Loss 548.69556 Test MSE 556.5886104751895 Test RE 0.39715161065007176 Lambda1 -1.1411678\n",
      "62 Train Loss 528.77106 Test MSE 530.7870139332235 Test RE 0.3878370681672573 Lambda1 -1.1325778\n",
      "63 Train Loss 519.5598 Test MSE 524.228975678022 Test RE 0.3854336978381082 Lambda1 -1.1384468\n",
      "64 Train Loss 494.48044 Test MSE 498.1531495662109 Test RE 0.37572544792685564 Lambda1 -1.1843848\n",
      "65 Train Loss 483.40262 Test MSE 491.9805167950731 Test RE 0.37339037843840595 Lambda1 -1.203568\n",
      "66 Train Loss 476.69492 Test MSE 483.88024457458 Test RE 0.37030375535592047 Lambda1 -1.2128974\n",
      "67 Train Loss 468.96185 Test MSE 472.1972081648652 Test RE 0.36580604513207576 Lambda1 -1.2038769\n",
      "68 Train Loss 463.29474 Test MSE 468.16850579889723 Test RE 0.3642422064199713 Lambda1 -1.1825451\n",
      "69 Train Loss 458.7688 Test MSE 463.064971477387 Test RE 0.36225145262187436 Lambda1 -1.1719948\n",
      "70 Train Loss 448.2556 Test MSE 453.5577649697421 Test RE 0.3585134669895953 Lambda1 -1.1680403\n",
      "71 Train Loss 439.90674 Test MSE 444.69207119958827 Test RE 0.35499224330522483 Lambda1 -1.1511017\n",
      "72 Train Loss 431.69247 Test MSE 433.0253704258332 Test RE 0.35030460105649314 Lambda1 -1.1480447\n",
      "73 Train Loss 427.60294 Test MSE 426.17850854030866 Test RE 0.3475241127490815 Lambda1 -1.1433796\n",
      "74 Train Loss 417.53812 Test MSE 411.7506936610744 Test RE 0.3415909376206195 Lambda1 -1.1219625\n",
      "Training time: 126.33\n",
      "Training time: 126.33\n",
      "inv_HT_atanh_tune11\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 27841.43 Test MSE 3577.4713831524314 Test RE 1.0068789345696443 Lambda1 0.0006018231\n",
      "1 Train Loss 21533.922 Test MSE 3572.492521340622 Test RE 1.0061780405618306 Lambda1 -0.0009544613\n",
      "2 Train Loss 18145.53 Test MSE 3570.4678108992985 Test RE 1.005892874406444 Lambda1 2.8288665e-05\n",
      "3 Train Loss 15444.784 Test MSE 3568.608424139697 Test RE 1.0056309216934192 Lambda1 -0.0002819717\n",
      "4 Train Loss 13962.302 Test MSE 3569.7310096791857 Test RE 1.005789081097568 Lambda1 -0.00014248188\n",
      "5 Train Loss 11820.368 Test MSE 3578.3391977296346 Test RE 1.0070010503239883 Lambda1 0.00039261364\n",
      "6 Train Loss 11038.355 Test MSE 3578.6185422858016 Test RE 1.0070403555374092 Lambda1 0.00016570918\n",
      "7 Train Loss 10003.689 Test MSE 3578.6983778497106 Test RE 1.0070515885293196 Lambda1 -0.00014832718\n",
      "8 Train Loss 8941.611 Test MSE 3582.4183673045554 Test RE 1.0075748581469912 Lambda1 0.00017095337\n",
      "9 Train Loss 8161.5273 Test MSE 3582.904518643727 Test RE 1.0076432221959812 Lambda1 0.0006187218\n",
      "10 Train Loss 7558.925 Test MSE 3580.9741987679713 Test RE 1.0073717475053845 Lambda1 0.00021341915\n",
      "11 Train Loss 6872.5293 Test MSE 3579.0702621070004 Test RE 1.007103911589788 Lambda1 -0.00037645534\n",
      "12 Train Loss 6403.542 Test MSE 3579.0500358258873 Test RE 1.0071010658792945 Lambda1 0.00012266511\n",
      "13 Train Loss 6011.4395 Test MSE 3578.297590436575 Test RE 1.006995195832939 Lambda1 -4.4974354e-07\n",
      "14 Train Loss 5670.043 Test MSE 3579.0258421803014 Test RE 1.0070976619750194 Lambda1 0.0002692324\n",
      "15 Train Loss 5399.13 Test MSE 3578.3811557578147 Test RE 1.007006954131908 Lambda1 0.00019559305\n",
      "16 Train Loss 5179.343 Test MSE 3575.0691559513434 Test RE 1.0065408245077425 Lambda1 -0.00022807738\n",
      "17 Train Loss 5018.295 Test MSE 3573.090596556674 Test RE 1.0062622597675737 Lambda1 -0.00012134598\n",
      "18 Train Loss 4840.4395 Test MSE 3571.3441580136328 Test RE 1.0060163116254432 Lambda1 3.8748025e-05\n",
      "19 Train Loss 4652.4497 Test MSE 3570.326549585341 Test RE 1.0058729757317972 Lambda1 2.1082342e-06\n",
      "20 Train Loss 4516.9785 Test MSE 3570.341472879693 Test RE 1.0058750779096517 Lambda1 -6.226713e-05\n",
      "21 Train Loss 4420.7314 Test MSE 3568.717306381408 Test RE 1.0056462630395235 Lambda1 2.7073631e-05\n",
      "22 Train Loss 4311.373 Test MSE 3565.6812209876575 Test RE 1.0052183953593568 Lambda1 -2.2141885e-05\n",
      "23 Train Loss 4199.1904 Test MSE 3563.0553386466963 Test RE 1.0048481896093961 Lambda1 -0.00029626949\n",
      "24 Train Loss 4116.194 Test MSE 3563.251444250755 Test RE 1.0048758419514354 Lambda1 -0.00024648313\n",
      "25 Train Loss 4040.83 Test MSE 3560.6072957103424 Test RE 1.0045029333682622 Lambda1 0.00013538986\n",
      "26 Train Loss 3990.1099 Test MSE 3559.577781749272 Test RE 1.0043577018849403 Lambda1 0.00048720307\n",
      "27 Train Loss 3944.8418 Test MSE 3558.2531413750007 Test RE 1.0041708066069412 Lambda1 0.00037932888\n",
      "28 Train Loss 3891.3867 Test MSE 3557.2255036489732 Test RE 1.0040257918651658 Lambda1 8.413721e-05\n",
      "29 Train Loss 3853.7568 Test MSE 3555.665520779604 Test RE 1.0038056153837043 Lambda1 -0.00024551453\n",
      "30 Train Loss 3818.977 Test MSE 3553.2869370372996 Test RE 1.0034698083255535 Lambda1 -9.027534e-05\n",
      "31 Train Loss 3798.015 Test MSE 3552.2040655057726 Test RE 1.003316891926153 Lambda1 0.00016995099\n",
      "32 Train Loss 3779.5293 Test MSE 3551.401130693665 Test RE 1.0032034914043313 Lambda1 0.00013236029\n",
      "33 Train Loss 3762.9219 Test MSE 3550.43561830704 Test RE 1.0030671126620032 Lambda1 4.6634046e-05\n",
      "34 Train Loss 3746.563 Test MSE 3548.5362763216467 Test RE 1.002798776244801 Lambda1 5.5508703e-05\n",
      "35 Train Loss 3735.8906 Test MSE 3548.5584269469514 Test RE 1.0028019060684175 Lambda1 4.535864e-05\n",
      "36 Train Loss 3725.0127 Test MSE 3547.787925653134 Test RE 1.0026930305731132 Lambda1 -7.3121264e-05\n",
      "37 Train Loss 3714.5317 Test MSE 3545.6733976716273 Test RE 1.0023941769594384 Lambda1 -8.9823385e-05\n",
      "38 Train Loss 3705.588 Test MSE 3544.048265363672 Test RE 1.002164430775156 Lambda1 8.948899e-05\n",
      "39 Train Loss 3696.071 Test MSE 3541.9686717586324 Test RE 1.0018703602089742 Lambda1 0.00018490669\n",
      "40 Train Loss 3689.1165 Test MSE 3540.1073533198237 Test RE 1.001607082212769 Lambda1 0.00022933367\n",
      "41 Train Loss 3681.9387 Test MSE 3538.985947101067 Test RE 1.0014484292027164 Lambda1 0.00017289833\n",
      "42 Train Loss 3671.5635 Test MSE 3535.446103222654 Test RE 1.0009474582966582 Lambda1 3.9996197e-05\n",
      "43 Train Loss 3663.6077 Test MSE 3533.1419119457714 Test RE 1.000621226437052 Lambda1 6.8926456e-05\n",
      "44 Train Loss 3656.7317 Test MSE 3531.30477745682 Test RE 1.0003610451541647 Lambda1 0.00010313498\n",
      "45 Train Loss 3645.6243 Test MSE 3528.372897731899 Test RE 0.9999456820696248 Lambda1 -7.840281e-05\n",
      "46 Train Loss 3634.7402 Test MSE 3524.423141517155 Test RE 0.9993858422069257 Lambda1 0.00010653742\n",
      "47 Train Loss 3626.0352 Test MSE 3521.614690253588 Test RE 0.9989875804660088 Lambda1 0.00020772014\n",
      "48 Train Loss 3619.2869 Test MSE 3520.967134414125 Test RE 0.9988957291365784 Lambda1 0.00020829782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 Train Loss 3614.082 Test MSE 3520.1828657275564 Test RE 0.9987844747225629 Lambda1 8.925057e-06\n",
      "50 Train Loss 3607.884 Test MSE 3518.282877355425 Test RE 0.9985148956845249 Lambda1 -8.247092e-06\n",
      "51 Train Loss 3602.915 Test MSE 3516.2877847970344 Test RE 0.9982317444707408 Lambda1 0.00022125403\n",
      "52 Train Loss 3597.786 Test MSE 3515.4337456452167 Test RE 0.9981105113953238 Lambda1 0.00027478486\n",
      "53 Train Loss 3593.7717 Test MSE 3513.727537869512 Test RE 0.997868266664831 Lambda1 0.0002460277\n",
      "54 Train Loss 3587.1628 Test MSE 3509.6207226669826 Test RE 0.9972849461500133 Lambda1 8.980237e-05\n",
      "55 Train Loss 3582.1758 Test MSE 3506.5498026475566 Test RE 0.9968485382327745 Lambda1 -0.00014301002\n",
      "56 Train Loss 3579.579 Test MSE 3504.4734155171604 Test RE 0.9965533549193967 Lambda1 -0.00018694489\n",
      "57 Train Loss 3576.318 Test MSE 3501.4855249569 Test RE 0.9961284369719101 Lambda1 -7.1682996e-05\n",
      "58 Train Loss 3572.1733 Test MSE 3499.571311396659 Test RE 0.9958561149495735 Lambda1 0.00015127164\n",
      "59 Train Loss 3568.479 Test MSE 3498.896758059425 Test RE 0.9957601331306652 Lambda1 0.00018929169\n",
      "60 Train Loss 3564.0417 Test MSE 3495.7396532905095 Test RE 0.9953107874142489 Lambda1 0.00012254305\n",
      "61 Train Loss 3559.888 Test MSE 3492.710847991117 Test RE 0.9948795116109469 Lambda1 0.000117139796\n",
      "62 Train Loss 3557.2554 Test MSE 3491.9043502884538 Test RE 0.9947646417578053 Lambda1 8.62894e-05\n",
      "63 Train Loss 3554.2466 Test MSE 3488.9122834839845 Test RE 0.9943383643134387 Lambda1 1.259963e-05\n",
      "64 Train Loss 3549.9446 Test MSE 3484.491106723302 Test RE 0.9937081479448602 Lambda1 0.00015719398\n",
      "65 Train Loss 3546.3784 Test MSE 3481.8666570117607 Test RE 0.9933338568031992 Lambda1 0.00014074265\n",
      "66 Train Loss 3540.4492 Test MSE 3475.9952936623604 Test RE 0.9924959894571844 Lambda1 -4.542958e-05\n",
      "67 Train Loss 3537.3833 Test MSE 3473.169948456843 Test RE 0.9920925490619293 Lambda1 -0.00011716563\n",
      "68 Train Loss 3534.1816 Test MSE 3470.176055726866 Test RE 0.9916648620963234 Lambda1 -4.7020614e-05\n",
      "69 Train Loss 3531.3364 Test MSE 3468.1287152328587 Test RE 0.9913722868579494 Lambda1 8.369136e-06\n",
      "70 Train Loss 3529.0874 Test MSE 3465.7936719784657 Test RE 0.9910384920164883 Lambda1 -5.437471e-05\n",
      "71 Train Loss 3524.8633 Test MSE 3461.288463988611 Test RE 0.9903941538264558 Lambda1 9.539583e-05\n",
      "72 Train Loss 3519.847 Test MSE 3457.951473561508 Test RE 0.9899166246069012 Lambda1 0.00016700085\n",
      "73 Train Loss 3517.162 Test MSE 3454.6285051919726 Test RE 0.9894408729514712 Lambda1 4.72068e-05\n",
      "74 Train Loss 3511.2236 Test MSE 3447.796533650498 Test RE 0.9884620155586206 Lambda1 -7.67455e-05\n",
      "Training time: 123.57\n",
      "Training time: 123.57\n",
      "inv_HT_atanh_tune11\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 10616.787 Test MSE 3560.6704221063465 Test RE 1.0045118377999562 Lambda1 0.0010034253\n",
      "1 Train Loss 6227.9307 Test MSE 3552.7074270212593 Test RE 1.0033879764151006 Lambda1 -0.001636349\n",
      "2 Train Loss 4110.4272 Test MSE 3551.5570035932674 Test RE 1.0032255067323053 Lambda1 0.0012007388\n",
      "3 Train Loss 3670.711 Test MSE 3538.2899404792183 Test RE 1.0013499477419048 Lambda1 5.18687e-05\n",
      "4 Train Loss 3419.835 Test MSE 3373.7376066491024 Test RE 0.9777882672270298 Lambda1 -0.0018938534\n",
      "5 Train Loss 3143.635 Test MSE 3128.679782810727 Test RE 0.9416071086399388 Lambda1 -0.0070490846\n",
      "6 Train Loss 887.3689 Test MSE 888.8269080356207 Test RE 0.501877548027951 Lambda1 -0.080102466\n",
      "7 Train Loss 854.60126 Test MSE 857.7707235370929 Test RE 0.4930316281189693 Lambda1 -0.08958478\n",
      "8 Train Loss 854.4109 Test MSE 857.63418561019 Test RE 0.4929923867496688 Lambda1 -0.08548047\n",
      "9 Train Loss 854.12396 Test MSE 856.741415132954 Test RE 0.4927357250775992 Lambda1 -0.084127896\n",
      "10 Train Loss 853.29944 Test MSE 855.2925156776654 Test RE 0.4923188976607506 Lambda1 -0.066572785\n",
      "11 Train Loss 841.7349 Test MSE 836.1336117152069 Test RE 0.4867735946613487 Lambda1 0.012950495\n",
      "12 Train Loss 828.2749 Test MSE 822.6127832237328 Test RE 0.4828218298678851 Lambda1 0.002464124\n",
      "13 Train Loss 792.8145 Test MSE 795.2175102980859 Test RE 0.47471410709587647 Lambda1 -0.0018049302\n",
      "14 Train Loss 782.08057 Test MSE 786.7977929076901 Test RE 0.47219429657466416 Lambda1 0.0002245235\n",
      "15 Train Loss 763.71313 Test MSE 765.3657678410689 Test RE 0.46571871184233055 Lambda1 -0.00019357109\n",
      "16 Train Loss 736.7027 Test MSE 737.0271247910082 Test RE 0.4570154754311798 Lambda1 1.4745616e-05\n",
      "17 Train Loss 722.08704 Test MSE 727.9631090603791 Test RE 0.45419657629371346 Lambda1 -2.5045574e-05\n",
      "18 Train Loss 714.9449 Test MSE 723.1024225205523 Test RE 0.4526776773372545 Lambda1 -2.3258921e-05\n",
      "19 Train Loss 710.3956 Test MSE 720.5482290002925 Test RE 0.4518774800293775 Lambda1 5.2219624e-05\n",
      "20 Train Loss 705.77875 Test MSE 717.6342048664026 Test RE 0.45096281821165 Lambda1 7.5778364e-05\n",
      "21 Train Loss 702.9261 Test MSE 716.2532870608046 Test RE 0.45052872348065465 Lambda1 0.00011103684\n",
      "22 Train Loss 700.7194 Test MSE 714.3406406148371 Test RE 0.4499267868416589 Lambda1 0.00030603918\n",
      "23 Train Loss 698.3745 Test MSE 711.1246323018022 Test RE 0.44891284442594886 Lambda1 0.00047186285\n",
      "24 Train Loss 695.7888 Test MSE 709.713732341514 Test RE 0.44846729272088287 Lambda1 0.00015009553\n",
      "25 Train Loss 683.9391 Test MSE 698.4680288326218 Test RE 0.44490003154873015 Lambda1 0.0002125861\n",
      "26 Train Loss 681.6474 Test MSE 696.9501926303541 Test RE 0.44441636400055734 Lambda1 0.0001369094\n",
      "27 Train Loss 677.7014 Test MSE 693.3553639973003 Test RE 0.4432687453470418 Lambda1 0.000101147125\n",
      "28 Train Loss 674.8111 Test MSE 690.60788156053 Test RE 0.442389627628395 Lambda1 0.00013617799\n",
      "29 Train Loss 673.3809 Test MSE 689.9377472374813 Test RE 0.4421749381972507 Lambda1 0.00020025439\n",
      "30 Train Loss 672.09283 Test MSE 689.3009949480128 Test RE 0.4419708466699767 Lambda1 8.64568e-05\n",
      "31 Train Loss 670.92566 Test MSE 688.3900036862159 Test RE 0.44167869222561357 Lambda1 0.00013759756\n",
      "32 Train Loss 670.2305 Test MSE 688.0742906607109 Test RE 0.44157739811381225 Lambda1 0.00013371446\n",
      "33 Train Loss 669.05566 Test MSE 687.6099332444901 Test RE 0.4414283703494074 Lambda1 0.00011834378\n",
      "34 Train Loss 668.6062 Test MSE 686.9276989409301 Test RE 0.4412093273279736 Lambda1 0.00014547973\n",
      "35 Train Loss 668.20764 Test MSE 686.0023980085502 Test RE 0.4409120697160755 Lambda1 0.00020735757\n",
      "36 Train Loss 667.42944 Test MSE 685.3609205905703 Test RE 0.44070587416397405 Lambda1 0.00029692292\n",
      "37 Train Loss 665.5569 Test MSE 683.8717383083517 Test RE 0.44022682126478985 Lambda1 0.00039361336\n",
      "38 Train Loss 663.6978 Test MSE 682.3629867681914 Test RE 0.43974094094827715 Lambda1 0.00027374362\n",
      "39 Train Loss 662.3115 Test MSE 681.3791502220164 Test RE 0.43942381555241405 Lambda1 0.00016934244\n",
      "40 Train Loss 661.94617 Test MSE 681.1655623579106 Test RE 0.43935493837117656 Lambda1 0.00025648333\n",
      "41 Train Loss 661.83453 Test MSE 680.9534427725989 Test RE 0.439286523987756 Lambda1 0.00033373397\n",
      "42 Train Loss 661.4443 Test MSE 680.2335457644696 Test RE 0.439054257976328 Lambda1 0.0003331816\n",
      "43 Train Loss 660.1748 Test MSE 679.7561750174053 Test RE 0.4389001723355356 Lambda1 0.00032157585\n",
      "44 Train Loss 659.23267 Test MSE 679.6674279046414 Test RE 0.43887152059544177 Lambda1 0.0005526379\n",
      "45 Train Loss 658.8843 Test MSE 679.4809168417513 Test RE 0.43881129995906115 Lambda1 0.00061970076\n",
      "46 Train Loss 658.1085 Test MSE 679.0640867177752 Test RE 0.4386766840895727 Lambda1 0.00061986217\n",
      "47 Train Loss 657.4232 Test MSE 678.0686535219452 Test RE 0.4383550403093446 Lambda1 0.0007743191\n",
      "48 Train Loss 656.7114 Test MSE 678.066801629879 Test RE 0.4383544417071445 Lambda1 0.00060169847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 Train Loss 656.13605 Test MSE 677.9008101565673 Test RE 0.43830078360656877 Lambda1 0.00039032518\n",
      "50 Train Loss 655.4647 Test MSE 677.0701916242418 Test RE 0.43803218063720045 Lambda1 0.0003275068\n",
      "51 Train Loss 654.34766 Test MSE 674.7936385907544 Test RE 0.43729514985873397 Lambda1 0.00043219543\n",
      "52 Train Loss 653.2873 Test MSE 674.1504538822494 Test RE 0.4370866945688402 Lambda1 0.00036882426\n",
      "53 Train Loss 652.2327 Test MSE 674.5664899382206 Test RE 0.43722154264093976 Lambda1 0.00014260088\n",
      "54 Train Loss 651.444 Test MSE 673.3409002298127 Test RE 0.4368241779298961 Lambda1 0.00017619252\n",
      "55 Train Loss 650.8309 Test MSE 672.8281207755996 Test RE 0.4366578155975423 Lambda1 6.0448685e-05\n",
      "56 Train Loss 649.2953 Test MSE 671.7228896288798 Test RE 0.43629902695819367 Lambda1 2.5244619e-05\n",
      "57 Train Loss 648.2338 Test MSE 669.6102527527584 Test RE 0.4356123842129507 Lambda1 2.4258832e-05\n",
      "58 Train Loss 647.6022 Test MSE 667.9328107055426 Test RE 0.43506641532337065 Lambda1 1.8884293e-05\n",
      "59 Train Loss 647.0095 Test MSE 667.2597869632175 Test RE 0.43484716885131386 Lambda1 8.416382e-06\n",
      "60 Train Loss 645.594 Test MSE 666.2015413691453 Test RE 0.4345022074817857 Lambda1 2.5011598e-06\n",
      "61 Train Loss 643.27435 Test MSE 664.652870798202 Test RE 0.43399688570145556 Lambda1 -1.1434205e-05\n",
      "62 Train Loss 640.8512 Test MSE 662.7318906261996 Test RE 0.43336926283955457 Lambda1 1.9028666e-05\n",
      "63 Train Loss 639.3216 Test MSE 662.5284994257556 Test RE 0.43330275762103526 Lambda1 1.0380403e-05\n",
      "64 Train Loss 638.59784 Test MSE 661.2182829368618 Test RE 0.43287409577698366 Lambda1 4.0182467e-06\n",
      "65 Train Loss 638.38165 Test MSE 661.0892143815609 Test RE 0.4328318456136656 Lambda1 1.5800041e-06\n",
      "66 Train Loss 638.2308 Test MSE 660.8256313757593 Test RE 0.43274554977750385 Lambda1 9.2600726e-07\n",
      "67 Train Loss 637.8712 Test MSE 660.8574087070858 Test RE 0.4327559544385432 Lambda1 -4.3037944e-06\n",
      "68 Train Loss 637.1845 Test MSE 659.89532028065 Test RE 0.43244083264943 Lambda1 3.9147258e-06\n",
      "69 Train Loss 635.0114 Test MSE 656.950651723238 Test RE 0.4314749077230555 Lambda1 -8.2740735e-07\n",
      "70 Train Loss 634.2599 Test MSE 656.51939811769 Test RE 0.43133326416403583 Lambda1 1.5439778e-06\n",
      "71 Train Loss 633.7749 Test MSE 656.986317822409 Test RE 0.43148662003199767 Lambda1 -1.2946134e-07\n",
      "72 Train Loss 633.6398 Test MSE 657.2797254616036 Test RE 0.4315829594345947 Lambda1 -2.6576825e-07\n",
      "73 Train Loss 633.47876 Test MSE 656.841804396664 Test RE 0.431439161631945 Lambda1 -9.830577e-07\n",
      "74 Train Loss 633.3903 Test MSE 656.4902391733555 Test RE 0.43132368534402604 Lambda1 2.063332e-07\n",
      "Training time: 125.75\n",
      "Training time: 125.75\n",
      "inv_HT_atanh_tune11\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 6727.9316 Test MSE 3518.6313150839023 Test RE 0.9985643390721761 Lambda1 -0.0005459001\n",
      "1 Train Loss 4856.808 Test MSE 3517.805497163952 Test RE 0.9984471513587904 Lambda1 0.0007134958\n",
      "2 Train Loss 3931.8286 Test MSE 3520.424776322592 Test RE 0.9988187928822888 Lambda1 9.768537e-06\n",
      "3 Train Loss 3685.713 Test MSE 3512.7913082628475 Test RE 0.9977353172479411 Lambda1 2.3601842e-05\n",
      "4 Train Loss 3586.1375 Test MSE 3497.788252584159 Test RE 0.9956023844064893 Lambda1 0.00030876845\n",
      "5 Train Loss 3483.5999 Test MSE 3422.778780258435 Test RE 0.9848692684586058 Lambda1 8.8835506e-05\n",
      "6 Train Loss 3238.6785 Test MSE 3172.9422953170624 Test RE 0.9482443366395122 Lambda1 0.0001667267\n",
      "7 Train Loss 3095.4524 Test MSE 3063.6034719213258 Test RE 0.9317629707832744 Lambda1 -4.9437866e-05\n",
      "8 Train Loss 2961.8784 Test MSE 2919.4225327771587 Test RE 0.9095731849598698 Lambda1 0.00015558426\n",
      "9 Train Loss 2474.7878 Test MSE 2445.1051708577747 Test RE 0.832411253119538 Lambda1 0.003013473\n",
      "10 Train Loss 858.8985 Test MSE 861.7344866242977 Test RE 0.49416946603648654 Lambda1 0.009097639\n",
      "11 Train Loss 854.7507 Test MSE 858.0923714375413 Test RE 0.4931240582521635 Lambda1 0.009219687\n",
      "12 Train Loss 854.6579 Test MSE 857.953351267394 Test RE 0.4930841109401856 Lambda1 0.0023728504\n",
      "13 Train Loss 854.5378 Test MSE 857.71682088631 Test RE 0.49301613672455225 Lambda1 -0.004469626\n",
      "14 Train Loss 853.90466 Test MSE 856.5514575626831 Test RE 0.4926810971185366 Lambda1 0.008672698\n",
      "15 Train Loss 853.34424 Test MSE 855.340515799281 Test RE 0.4923327122534086 Lambda1 -0.001431321\n",
      "16 Train Loss 850.39984 Test MSE 850.683184276074 Test RE 0.4909905061764792 Lambda1 0.008256158\n",
      "17 Train Loss 839.18445 Test MSE 834.8213107882228 Test RE 0.4863914522190679 Lambda1 0.0032631876\n",
      "18 Train Loss 818.5105 Test MSE 811.8558214278933 Test RE 0.4796546125826525 Lambda1 -0.00055694766\n",
      "19 Train Loss 791.8389 Test MSE 787.2750029917529 Test RE 0.4723374729565535 Lambda1 0.00014542868\n",
      "20 Train Loss 769.9422 Test MSE 763.9290771997063 Test RE 0.465281399362552 Lambda1 0.00020410353\n",
      "21 Train Loss 760.1547 Test MSE 757.7639803257944 Test RE 0.4634001280220369 Lambda1 -6.405386e-05\n",
      "22 Train Loss 753.23065 Test MSE 750.0104558164517 Test RE 0.4610232516985896 Lambda1 -7.467147e-05\n",
      "23 Train Loss 746.9498 Test MSE 746.8333932190023 Test RE 0.46004576255951585 Lambda1 -4.9711864e-05\n",
      "24 Train Loss 742.8447 Test MSE 743.8380421098752 Test RE 0.4591222747559777 Lambda1 -0.00010457187\n",
      "25 Train Loss 737.7742 Test MSE 740.4287770301347 Test RE 0.45806890889280444 Lambda1 -0.0001290306\n",
      "26 Train Loss 726.23535 Test MSE 734.4645602416124 Test RE 0.4562202866485073 Lambda1 -0.000100573845\n",
      "27 Train Loss 721.958 Test MSE 731.6103764259534 Test RE 0.4553329708112654 Lambda1 2.8941304e-05\n",
      "28 Train Loss 718.8805 Test MSE 729.5233180585535 Test RE 0.4546830448271378 Lambda1 -1.8649904e-05\n",
      "29 Train Loss 716.04584 Test MSE 727.9248583398013 Test RE 0.45418464328891067 Lambda1 4.7455978e-07\n",
      "30 Train Loss 713.99457 Test MSE 725.5876166969823 Test RE 0.4534549026258278 Lambda1 5.5061464e-05\n",
      "31 Train Loss 709.59064 Test MSE 721.2280530899994 Test RE 0.45209059884612357 Lambda1 2.9146733e-05\n",
      "32 Train Loss 704.4725 Test MSE 718.4438632520555 Test RE 0.451217141999617 Lambda1 3.1131523e-05\n",
      "33 Train Loss 703.06854 Test MSE 717.5568696652487 Test RE 0.4509385187568235 Lambda1 2.8937628e-05\n",
      "34 Train Loss 701.9608 Test MSE 716.8730954973014 Test RE 0.45072361342014056 Lambda1 2.4818557e-05\n",
      "35 Train Loss 701.29144 Test MSE 716.7136652701555 Test RE 0.4506734909059445 Lambda1 1.7135444e-05\n",
      "36 Train Loss 700.84155 Test MSE 716.7086131196014 Test RE 0.4506719024931309 Lambda1 9.634568e-06\n",
      "37 Train Loss 700.35376 Test MSE 716.5298643736465 Test RE 0.4506156996937391 Lambda1 3.3918245e-06\n",
      "38 Train Loss 700.26447 Test MSE 716.4115974277528 Test RE 0.45057850993761406 Lambda1 2.1465862e-06\n",
      "39 Train Loss 700.1536 Test MSE 716.3600438272828 Test RE 0.4505622976355842 Lambda1 8.8199056e-07\n",
      "40 Train Loss 700.0453 Test MSE 716.4239269637536 Test RE 0.45058238717807003 Lambda1 2.3395298e-06\n",
      "41 Train Loss 699.8448 Test MSE 716.2178780626033 Test RE 0.45051758707945805 Lambda1 5.174112e-06\n",
      "42 Train Loss 699.51135 Test MSE 715.7730946657491 Test RE 0.45037767583960164 Lambda1 7.185734e-06\n",
      "43 Train Loss 699.213 Test MSE 715.4161406082598 Test RE 0.45026536078602625 Lambda1 7.791732e-06\n",
      "44 Train Loss 698.4691 Test MSE 714.5938680404208 Test RE 0.45000652730448176 Lambda1 1.0579491e-05\n",
      "45 Train Loss 697.7255 Test MSE 714.1124890300663 Test RE 0.44985493057216547 Lambda1 -3.4443133e-09\n",
      "46 Train Loss 697.2723 Test MSE 713.853762974169 Test RE 0.4497734309921353 Lambda1 -5.4473044e-06\n",
      "47 Train Loss 697.07855 Test MSE 713.9420220628941 Test RE 0.4498012345622011 Lambda1 -1.0817372e-05\n",
      "48 Train Loss 696.9625 Test MSE 713.838694791256 Test RE 0.44976868400861886 Lambda1 -1.5501791e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 Train Loss 696.83057 Test MSE 713.8254346439596 Test RE 0.44976450657558004 Lambda1 -1.3248302e-05\n",
      "50 Train Loss 696.72815 Test MSE 713.7430550324196 Test RE 0.4497385531053146 Lambda1 -1.5371043e-05\n",
      "51 Train Loss 696.64087 Test MSE 713.6343749495943 Test RE 0.4497043114523695 Lambda1 -1.5929203e-05\n",
      "52 Train Loss 696.59503 Test MSE 713.6945098348813 Test RE 0.4497232583728501 Lambda1 -1.5820131e-05\n",
      "53 Train Loss 696.4571 Test MSE 713.8738357028886 Test RE 0.44977975449778124 Lambda1 -1.4789186e-05\n",
      "54 Train Loss 696.16785 Test MSE 713.6256181767056 Test RE 0.44970155235699716 Lambda1 -1.37625475e-05\n",
      "55 Train Loss 696.043 Test MSE 713.3852093529609 Test RE 0.4496257974195838 Lambda1 -1.5954874e-05\n",
      "56 Train Loss 695.90045 Test MSE 713.2909190775969 Test RE 0.44959608223859454 Lambda1 -2.0308664e-05\n",
      "57 Train Loss 695.8014 Test MSE 713.2503357930256 Test RE 0.44958329198373603 Lambda1 -1.6409862e-05\n",
      "58 Train Loss 695.50836 Test MSE 712.8997784849512 Test RE 0.449472794959877 Lambda1 -1.5312657e-05\n",
      "59 Train Loss 695.2593 Test MSE 712.5516539939135 Test RE 0.4493630378806914 Lambda1 -1.4890579e-05\n",
      "60 Train Loss 695.1498 Test MSE 712.4112554077814 Test RE 0.44931876527049947 Lambda1 -1.1611357e-05\n",
      "61 Train Loss 694.91754 Test MSE 712.1225764216853 Test RE 0.4492277209283214 Lambda1 -8.654773e-06\n",
      "62 Train Loss 694.77795 Test MSE 711.8364648749182 Test RE 0.4491374681018934 Lambda1 -3.3422177e-06\n",
      "63 Train Loss 694.53864 Test MSE 711.6057892791628 Test RE 0.44906468913299435 Lambda1 -5.3747283e-07\n",
      "64 Train Loss 694.26056 Test MSE 711.2325208377194 Test RE 0.44894689662333354 Lambda1 3.0172455e-06\n",
      "65 Train Loss 694.03754 Test MSE 710.7950004927272 Test RE 0.4488087887524984 Lambda1 7.2009752e-06\n",
      "66 Train Loss 693.59467 Test MSE 710.72864328325 Test RE 0.4487878386938318 Lambda1 1.060423e-05\n",
      "67 Train Loss 692.8099 Test MSE 710.2000084331589 Test RE 0.44862090506469793 Lambda1 1.3555351e-05\n",
      "68 Train Loss 691.93256 Test MSE 709.4570766502991 Test RE 0.44838619517001843 Lambda1 2.0991125e-05\n",
      "69 Train Loss 690.9633 Test MSE 708.3292908760396 Test RE 0.448029665710151 Lambda1 1.930159e-05\n",
      "70 Train Loss 690.8311 Test MSE 708.4195280028788 Test RE 0.4480582030181319 Lambda1 1.49015605e-05\n",
      "71 Train Loss 690.741 Test MSE 708.5460096575393 Test RE 0.44809819952540103 Lambda1 1.0584884e-05\n",
      "72 Train Loss 690.65826 Test MSE 708.5206268819146 Test RE 0.4480901731744174 Lambda1 8.425313e-06\n",
      "73 Train Loss 690.5508 Test MSE 708.5644507596425 Test RE 0.4481040307422664 Lambda1 6.2540394e-06\n",
      "74 Train Loss 690.42615 Test MSE 708.4439095963504 Test RE 0.4480659133356285 Lambda1 5.127584e-06\n",
      "Training time: 126.29\n",
      "Training time: 126.29\n",
      "inv_HT_atanh_tune11\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 19951.188 Test MSE 3515.792536336064 Test RE 0.9981614444591951 Lambda1 -0.0013534504\n",
      "1 Train Loss 11390.714 Test MSE 3501.4136701636085 Test RE 0.9961182160285942 Lambda1 -3.9325605e-06\n",
      "2 Train Loss 7937.924 Test MSE 3495.796349777392 Test RE 0.9953188587241568 Lambda1 0.00015285483\n",
      "3 Train Loss 5991.7026 Test MSE 3494.921359296811 Test RE 0.9951942878192798 Lambda1 -5.2394327e-05\n",
      "4 Train Loss 4789.1787 Test MSE 3497.8966532597124 Test RE 0.9956178117461535 Lambda1 6.489967e-05\n",
      "5 Train Loss 4190.8965 Test MSE 3504.530047750357 Test RE 0.9965614070298415 Lambda1 -3.6814294e-05\n",
      "6 Train Loss 3891.002 Test MSE 3498.1168278961586 Test RE 0.9956491457680021 Lambda1 0.00051471667\n",
      "7 Train Loss 3680.4739 Test MSE 3482.7991250466966 Test RE 0.9934668587465311 Lambda1 -0.00038623964\n",
      "8 Train Loss 3580.9346 Test MSE 3473.9715411462985 Test RE 0.9922070278041704 Lambda1 0.0003572722\n",
      "9 Train Loss 3534.8447 Test MSE 3467.5223198575864 Test RE 0.9912856133374937 Lambda1 0.00014102171\n",
      "10 Train Loss 3496.4946 Test MSE 3447.625670822462 Test RE 0.9884375225953277 Lambda1 -0.00030950477\n",
      "11 Train Loss 3464.4602 Test MSE 3415.3940033436065 Test RE 0.9838062481016782 Lambda1 -8.618226e-07\n",
      "12 Train Loss 3409.1 Test MSE 3350.3558277785864 Test RE 0.9743940822384449 Lambda1 4.265765e-05\n",
      "13 Train Loss 3041.3909 Test MSE 2948.474631686683 Test RE 0.9140877069261404 Lambda1 -0.0029110943\n",
      "14 Train Loss 2091.4836 Test MSE 2059.255818904312 Test RE 0.763913740178262 Lambda1 0.020030243\n",
      "15 Train Loss 855.3938 Test MSE 858.4432736685173 Test RE 0.49322487528031567 Lambda1 0.09019238\n",
      "16 Train Loss 854.68616 Test MSE 857.9890884148377 Test RE 0.49309438028411445 Lambda1 0.091009356\n",
      "17 Train Loss 854.18787 Test MSE 857.1731189260482 Test RE 0.4928598518491826 Lambda1 0.08667058\n",
      "18 Train Loss 849.44666 Test MSE 849.6832506562108 Test RE 0.49070185448926235 Lambda1 0.075837485\n",
      "19 Train Loss 831.8015 Test MSE 828.9810316807851 Test RE 0.48468710695337386 Lambda1 0.06338071\n",
      "20 Train Loss 804.797 Test MSE 800.7913993700431 Test RE 0.4763749000764616 Lambda1 0.016941667\n",
      "21 Train Loss 767.13983 Test MSE 770.7979779097165 Test RE 0.4673685169817123 Lambda1 0.0012401877\n",
      "22 Train Loss 748.61975 Test MSE 752.3252895230651 Test RE 0.461734155111074 Lambda1 0.00052985875\n",
      "23 Train Loss 741.4936 Test MSE 747.7208515371877 Test RE 0.4603190164255896 Lambda1 0.000236503\n",
      "24 Train Loss 735.38354 Test MSE 742.8462757436077 Test RE 0.45881609660726913 Lambda1 0.0002179267\n",
      "25 Train Loss 725.37006 Test MSE 729.2610360713935 Test RE 0.45460130248230185 Lambda1 0.0001425973\n",
      "26 Train Loss 720.7576 Test MSE 726.8790544568999 Test RE 0.4538582642790294 Lambda1 0.0007460032\n",
      "27 Train Loss 709.61053 Test MSE 714.8515192051552 Test RE 0.4500876462881623 Lambda1 0.00022735028\n",
      "28 Train Loss 696.9751 Test MSE 681.8113831509804 Test RE 0.43956316772606535 Lambda1 -9.225997e-05\n",
      "29 Train Loss 617.07074 Test MSE 583.9952509661784 Test RE 0.40681206861594654 Lambda1 -0.0012574706\n",
      "30 Train Loss 563.0213 Test MSE 540.6842994512319 Test RE 0.3914362569768668 Lambda1 -0.00010400774\n",
      "31 Train Loss 543.0741 Test MSE 533.3391276156464 Test RE 0.38876834316784 Lambda1 0.00024427514\n",
      "32 Train Loss 500.3348 Test MSE 486.80020897354444 Test RE 0.37141936967095446 Lambda1 0.00016279292\n",
      "33 Train Loss 470.42953 Test MSE 455.27492733987737 Test RE 0.3591914889476096 Lambda1 0.0003414185\n",
      "34 Train Loss 447.92294 Test MSE 442.67806743340856 Test RE 0.3541874537602687 Lambda1 3.18991e-05\n",
      "35 Train Loss 433.67245 Test MSE 432.3721275752671 Test RE 0.35004027434592755 Lambda1 3.2756816e-05\n",
      "36 Train Loss 423.69254 Test MSE 422.62136150793566 Test RE 0.34607074910914437 Lambda1 1.6216534e-05\n",
      "37 Train Loss 420.7797 Test MSE 418.3790718157291 Test RE 0.3443294324050813 Lambda1 1.8829669e-05\n",
      "38 Train Loss 415.62582 Test MSE 411.409978368253 Test RE 0.34144957860811365 Lambda1 -8.097742e-06\n",
      "39 Train Loss 409.618 Test MSE 409.31659716466453 Test RE 0.34057977020673513 Lambda1 -2.3616109e-05\n",
      "40 Train Loss 399.94278 Test MSE 401.99894161701894 Test RE 0.33752164223551395 Lambda1 8.0752434e-05\n",
      "41 Train Loss 395.0551 Test MSE 397.3487554153105 Test RE 0.33556379641111683 Lambda1 0.00019536178\n",
      "42 Train Loss 385.51788 Test MSE 387.1654334420362 Test RE 0.3312359444757753 Lambda1 0.00037275316\n",
      "43 Train Loss 381.72357 Test MSE 384.0051892452193 Test RE 0.3298813151024442 Lambda1 0.00040523906\n",
      "44 Train Loss 376.24173 Test MSE 377.0155338313756 Test RE 0.3268652807996585 Lambda1 0.0002493518\n",
      "45 Train Loss 373.04218 Test MSE 376.9297592872808 Test RE 0.3268280962372729 Lambda1 9.707301e-06\n",
      "46 Train Loss 370.59894 Test MSE 374.36760635620135 Test RE 0.32571540691682627 Lambda1 9.509309e-05\n",
      "47 Train Loss 368.6094 Test MSE 371.47852678118056 Test RE 0.32445616297526886 Lambda1 0.00021653241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 Train Loss 366.81262 Test MSE 370.1543929946606 Test RE 0.3238773853768274 Lambda1 0.000109642\n",
      "49 Train Loss 366.05795 Test MSE 369.09134112789496 Test RE 0.3234119768605279 Lambda1 0.00022184444\n",
      "50 Train Loss 364.5098 Test MSE 366.12901590376424 Test RE 0.32211151078057104 Lambda1 0.0004988134\n",
      "51 Train Loss 360.4706 Test MSE 363.441778768473 Test RE 0.3209272504509929 Lambda1 0.0003494917\n",
      "52 Train Loss 357.04956 Test MSE 361.4547150755705 Test RE 0.32004873709338283 Lambda1 0.00020850528\n",
      "53 Train Loss 354.71722 Test MSE 360.2500322061843 Test RE 0.3195149509720598 Lambda1 0.00027047016\n",
      "54 Train Loss 352.89462 Test MSE 357.5493149281148 Test RE 0.3183150303018503 Lambda1 0.0004534512\n",
      "55 Train Loss 350.50204 Test MSE 353.6043865766258 Test RE 0.3165541362527514 Lambda1 0.00046545122\n",
      "56 Train Loss 348.4084 Test MSE 352.90834343475933 Test RE 0.31624242650155643 Lambda1 0.000203712\n",
      "57 Train Loss 346.58368 Test MSE 351.0242650517616 Test RE 0.3153971321117331 Lambda1 8.823388e-05\n",
      "58 Train Loss 344.42636 Test MSE 348.41572710404705 Test RE 0.3142230543648987 Lambda1 0.00049207977\n",
      "59 Train Loss 343.4765 Test MSE 347.03640252713217 Test RE 0.31360045705446987 Lambda1 0.00056708395\n",
      "60 Train Loss 341.88907 Test MSE 344.32117523177396 Test RE 0.31237123643331116 Lambda1 0.0005240612\n",
      "61 Train Loss 338.81046 Test MSE 341.24673118765276 Test RE 0.3109735278944373 Lambda1 0.0006878319\n",
      "62 Train Loss 334.59943 Test MSE 337.0792548593281 Test RE 0.3090688123056969 Lambda1 0.00081722933\n",
      "63 Train Loss 332.84418 Test MSE 334.3956879131624 Test RE 0.30783606923952644 Lambda1 0.0008478949\n",
      "64 Train Loss 331.6447 Test MSE 333.46791362724895 Test RE 0.30740873005051284 Lambda1 0.00080382265\n",
      "65 Train Loss 330.03052 Test MSE 332.48097606178163 Test RE 0.30695348679316664 Lambda1 0.0007045859\n",
      "66 Train Loss 328.60217 Test MSE 331.7396170346101 Test RE 0.30661107662534337 Lambda1 0.00037038987\n",
      "67 Train Loss 327.15186 Test MSE 331.0242786297945 Test RE 0.3062803216719305 Lambda1 0.00034475425\n",
      "68 Train Loss 325.6846 Test MSE 329.7974646372073 Test RE 0.3057122398159927 Lambda1 0.0005550813\n",
      "69 Train Loss 324.54288 Test MSE 328.6194194976776 Test RE 0.3051657453118963 Lambda1 0.0006299466\n",
      "70 Train Loss 323.0853 Test MSE 327.1867799287697 Test RE 0.3044998229253126 Lambda1 0.00077823154\n",
      "71 Train Loss 321.3202 Test MSE 324.9169384744698 Test RE 0.30344175873649865 Lambda1 0.0006816386\n",
      "72 Train Loss 319.4577 Test MSE 324.2633677698845 Test RE 0.30313641843020833 Lambda1 0.00055052934\n",
      "73 Train Loss 318.98148 Test MSE 323.6063394198355 Test RE 0.3028291524343133 Lambda1 0.0005267363\n",
      "74 Train Loss 318.25992 Test MSE 322.0884992842622 Test RE 0.30211812419789674 Lambda1 0.00028167656\n",
      "Training time: 124.01\n",
      "Training time: 124.01\n",
      "inv_HT_atanh_tune11\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 9407.965 Test MSE 3587.0411384053887 Test RE 1.008224738453963 Lambda1 -7.371095e-05\n",
      "1 Train Loss 8146.838 Test MSE 3587.3823617328353 Test RE 1.0082726918514267 Lambda1 -0.0010298395\n",
      "2 Train Loss 5319.2725 Test MSE 3586.997625563919 Test RE 1.0082186232667432 Lambda1 -0.002158645\n",
      "3 Train Loss 4417.6484 Test MSE 3576.4432410457134 Test RE 1.0067342389302802 Lambda1 0.00061526574\n",
      "4 Train Loss 4001.3372 Test MSE 3561.686065012148 Test RE 1.0046550906792207 Lambda1 0.0005366339\n",
      "5 Train Loss 3729.7036 Test MSE 3545.474721281609 Test RE 1.0023660927531408 Lambda1 0.00029854022\n",
      "6 Train Loss 3602.6147 Test MSE 3521.7594808118874 Test RE 0.9990081168527957 Lambda1 -0.00015730865\n",
      "7 Train Loss 3503.8767 Test MSE 3462.0638132592944 Test RE 0.9905050747153157 Lambda1 -0.0004260869\n",
      "8 Train Loss 3387.4893 Test MSE 3352.721913990233 Test RE 0.9747380895047757 Lambda1 0.0031129883\n",
      "9 Train Loss 2544.2683 Test MSE 2469.5048370417253 Test RE 0.8365542524705922 Lambda1 0.110673845\n",
      "10 Train Loss 942.2094 Test MSE 938.5840062754668 Test RE 0.5157339785547661 Lambda1 0.49445102\n",
      "11 Train Loss 854.7284 Test MSE 858.0562013515565 Test RE 0.4931136651242533 Lambda1 0.5937187\n",
      "12 Train Loss 854.72064 Test MSE 858.0761532432704 Test RE 0.49311939813640443 Lambda1 0.5941794\n",
      "13 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "14 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "15 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "16 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "17 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "18 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "19 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "20 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "21 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "22 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "23 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "24 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "25 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "26 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "27 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "28 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "29 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "30 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "31 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "32 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "33 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "34 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "35 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "36 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "37 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "38 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "39 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "40 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "41 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "42 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "43 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "44 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "45 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "46 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "47 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "48 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "50 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "51 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "52 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "53 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "54 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "55 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "56 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "57 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "58 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "59 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "60 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "61 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "62 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "63 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "64 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "65 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "66 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "67 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "68 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "69 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "70 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "71 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "72 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "73 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "74 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.4931196249617329 Lambda1 0.5941898\n",
      "Training time: 111.78\n",
      "Training time: 111.78\n",
      "inv_HT_atanh_tune11\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 40396.727 Test MSE 3575.657100939162 Test RE 1.0066235874247618 Lambda1 0.00041480272\n",
      "1 Train Loss 30371.723 Test MSE 3576.210461739662 Test RE 1.0067014758336688 Lambda1 -0.0004633428\n",
      "2 Train Loss 22765.125 Test MSE 3571.3784563813897 Test RE 1.0060211423884635 Lambda1 0.0002308748\n",
      "3 Train Loss 16225.311 Test MSE 3565.8296300906964 Test RE 1.0052393145048004 Lambda1 -0.0003919829\n",
      "4 Train Loss 12793.997 Test MSE 3555.896017014735 Test RE 1.0038381507366814 Lambda1 0.00018467978\n",
      "5 Train Loss 10320.791 Test MSE 3557.053048436948 Test RE 1.0040014538552098 Lambda1 0.00021334736\n",
      "6 Train Loss 8262.929 Test MSE 3559.8870246004494 Test RE 1.0044013283643904 Lambda1 -1.4857361e-05\n",
      "7 Train Loss 6815.412 Test MSE 3559.834373229631 Test RE 1.0043939006985734 Lambda1 0.00019354594\n",
      "8 Train Loss 5564.29 Test MSE 3556.654034445002 Test RE 1.0039451401233275 Lambda1 3.8729904e-05\n",
      "9 Train Loss 4842.374 Test MSE 3553.331873975765 Test RE 1.0034761535376433 Lambda1 4.6586974e-05\n",
      "10 Train Loss 4441.8066 Test MSE 3553.7007702099845 Test RE 1.0035282411247433 Lambda1 0.00031244525\n",
      "11 Train Loss 4295.905 Test MSE 3552.218710024008 Test RE 1.0033189600896233 Lambda1 0.00030518897\n",
      "12 Train Loss 4090.4116 Test MSE 3552.726766452541 Test RE 1.0033907074205108 Lambda1 0.00016694769\n",
      "13 Train Loss 3979.8809 Test MSE 3550.9404753771314 Test RE 1.003138426097471 Lambda1 0.0001749257\n",
      "14 Train Loss 3860.941 Test MSE 3548.9353856180833 Test RE 1.002855167814452 Lambda1 7.142483e-05\n",
      "15 Train Loss 3812.3271 Test MSE 3548.1538763728836 Test RE 1.0027447426232878 Lambda1 0.00014945643\n",
      "16 Train Loss 3774.6072 Test MSE 3548.3021374931322 Test RE 1.0027656924624204 Lambda1 0.00021605579\n",
      "17 Train Loss 3742.825 Test MSE 3547.7635700117776 Test RE 1.002689588812463 Lambda1 0.00022764888\n",
      "18 Train Loss 3712.949 Test MSE 3547.371667156982 Test RE 1.0026342063500824 Lambda1 0.0001237459\n",
      "19 Train Loss 3677.72 Test MSE 3545.6871387374745 Test RE 1.0023961193198168 Lambda1 0.00010315607\n",
      "20 Train Loss 3650.3503 Test MSE 3541.0461273725628 Test RE 1.001739877678788 Lambda1 0.00021603213\n",
      "21 Train Loss 3635.0742 Test MSE 3537.721035054948 Test RE 1.001269443271231 Lambda1 0.00014883197\n",
      "22 Train Loss 3616.0488 Test MSE 3532.541296942093 Test RE 1.000536172725663 Lambda1 0.00019099751\n",
      "23 Train Loss 3597.711 Test MSE 3522.855588392342 Test RE 0.9991635697146228 Lambda1 0.00029067634\n",
      "24 Train Loss 3578.069 Test MSE 3511.463902438321 Test RE 0.9975467884265325 Lambda1 0.00043936807\n",
      "25 Train Loss 3564.17 Test MSE 3500.172117620054 Test RE 0.9959415955442942 Lambda1 0.00038348514\n",
      "26 Train Loss 3551.9712 Test MSE 3487.7160165344126 Test RE 0.9941678819388159 Lambda1 0.00018326358\n",
      "27 Train Loss 3544.1218 Test MSE 3483.6747525914343 Test RE 0.9935917370764396 Lambda1 0.00012859279\n",
      "28 Train Loss 3525.6873 Test MSE 3464.0460319539643 Test RE 0.990788592990072 Lambda1 7.1851755e-05\n",
      "29 Train Loss 3509.647 Test MSE 3437.2644716079167 Test RE 0.9869511221996786 Lambda1 0.00027786652\n",
      "30 Train Loss 3489.9604 Test MSE 3414.914913897881 Test RE 0.983737244683679 Lambda1 0.00022051645\n",
      "31 Train Loss 3461.708 Test MSE 3372.3923545211987 Test RE 0.9775933049848131 Lambda1 0.00018393648\n",
      "32 Train Loss 3428.3125 Test MSE 3306.5130018654304 Test RE 0.9679976166628128 Lambda1 0.00034323917\n",
      "33 Train Loss 3386.6094 Test MSE 3263.2909253868165 Test RE 0.9616500693638572 Lambda1 7.974231e-05\n",
      "34 Train Loss 3348.493 Test MSE 3245.398766718053 Test RE 0.9590101507354818 Lambda1 9.720236e-05\n",
      "35 Train Loss 3285.06 Test MSE 3188.733202697849 Test RE 0.9506009907072811 Lambda1 0.00021816944\n",
      "36 Train Loss 3172.5957 Test MSE 3061.43944730407 Test RE 0.9314338299166307 Lambda1 -6.490012e-05\n",
      "37 Train Loss 3110.0522 Test MSE 3026.712076577266 Test RE 0.9261359133320161 Lambda1 0.00016407132\n",
      "38 Train Loss 3015.8857 Test MSE 2938.2313183750193 Test RE 0.9124985066842244 Lambda1 -0.00031047635\n",
      "39 Train Loss 2941.527 Test MSE 2897.6607433947197 Test RE 0.9061768001117754 Lambda1 0.00042907704\n",
      "40 Train Loss 2729.2002 Test MSE 2603.202831815227 Test RE 0.8589011310502689 Lambda1 0.0040283464\n",
      "41 Train Loss 1734.0073 Test MSE 1695.9637425459628 Test RE 0.6932620903478185 Lambda1 -0.018393014\n",
      "42 Train Loss 854.7929 Test MSE 857.9827173744436 Test RE 0.4930925495323666 Lambda1 -0.05972506\n",
      "43 Train Loss 852.37787 Test MSE 853.174129084307 Test RE 0.4917088325012474 Lambda1 -0.052922815\n",
      "44 Train Loss 842.6258 Test MSE 837.5404051778345 Test RE 0.48718292045792894 Lambda1 -0.042519815\n",
      "45 Train Loss 816.3592 Test MSE 801.4524572166267 Test RE 0.47657148460670856 Lambda1 -0.037879854\n",
      "46 Train Loss 735.2311 Test MSE 715.0726999331099 Test RE 0.45015727124615784 Lambda1 -0.0163601\n",
      "47 Train Loss 615.28827 Test MSE 581.5269260424395 Test RE 0.405951438629218 Lambda1 -0.002174729\n",
      "48 Train Loss 586.6566 Test MSE 557.0242570828033 Test RE 0.39730700720891393 Lambda1 -0.0013200697\n",
      "49 Train Loss 546.26324 Test MSE 525.6235656513112 Test RE 0.3859460359401449 Lambda1 -0.0014032624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Train Loss 480.1 Test MSE 446.6772016312915 Test RE 0.35578371367271117 Lambda1 -0.0014019822\n",
      "51 Train Loss 448.37332 Test MSE 429.3269030454931 Test RE 0.3488054181986716 Lambda1 -0.00079458614\n",
      "52 Train Loss 427.56992 Test MSE 422.8992526123412 Test RE 0.34618450835996034 Lambda1 -0.0006057603\n",
      "53 Train Loss 411.57025 Test MSE 411.92626343778187 Test RE 0.3416637567485879 Lambda1 -0.00037495463\n",
      "54 Train Loss 406.45923 Test MSE 406.28543135512064 Test RE 0.33931635683928313 Lambda1 -0.00036554039\n",
      "55 Train Loss 397.93915 Test MSE 397.8342355422763 Test RE 0.335768729510792 Lambda1 -0.0003333182\n",
      "56 Train Loss 381.19577 Test MSE 380.2924930180102 Test RE 0.32828273795858337 Lambda1 -0.00037061493\n",
      "57 Train Loss 368.97638 Test MSE 374.162592902525 Test RE 0.3256262095825315 Lambda1 -0.0002698013\n",
      "58 Train Loss 359.49652 Test MSE 367.94249607128876 Test RE 0.32290825354740815 Lambda1 -8.043984e-05\n",
      "59 Train Loss 354.47372 Test MSE 359.886486586091 Test RE 0.3193536913320738 Lambda1 -3.6972388e-06\n",
      "60 Train Loss 347.61548 Test MSE 353.6504522365557 Test RE 0.31657475505846994 Lambda1 -0.00031945368\n",
      "61 Train Loss 343.19666 Test MSE 351.08605786337426 Test RE 0.31542489147181346 Lambda1 -5.398333e-05\n",
      "62 Train Loss 340.1704 Test MSE 348.6474117878441 Test RE 0.31432751085875926 Lambda1 -9.561958e-06\n",
      "63 Train Loss 337.5822 Test MSE 347.125003534369 Test RE 0.31364048677892936 Lambda1 2.0048677e-05\n",
      "64 Train Loss 335.6478 Test MSE 343.7315597482204 Test RE 0.3121036696302288 Lambda1 3.95672e-05\n",
      "65 Train Loss 333.8253 Test MSE 341.39051053557563 Test RE 0.31103903308221686 Lambda1 7.787853e-06\n",
      "66 Train Loss 332.2625 Test MSE 341.0805655679218 Test RE 0.31089780643518644 Lambda1 1.965605e-05\n",
      "67 Train Loss 330.12912 Test MSE 339.1629724954993 Test RE 0.31002262349421955 Lambda1 1.717703e-05\n",
      "68 Train Loss 327.82648 Test MSE 336.7547912877298 Test RE 0.30892002576220245 Lambda1 8.857886e-05\n",
      "69 Train Loss 324.55234 Test MSE 334.1088399521551 Test RE 0.30770400848527496 Lambda1 0.000116441384\n",
      "70 Train Loss 318.86105 Test MSE 324.82703860794726 Test RE 0.30339977683652436 Lambda1 5.5536624e-05\n",
      "71 Train Loss 312.17255 Test MSE 316.3285663663723 Test RE 0.29940453705073977 Lambda1 0.00034554556\n",
      "72 Train Loss 306.77383 Test MSE 311.8609180416058 Test RE 0.2972827071390152 Lambda1 0.00027535233\n",
      "73 Train Loss 304.0947 Test MSE 310.4106388807623 Test RE 0.2965906593284039 Lambda1 0.00028153165\n",
      "74 Train Loss 301.11688 Test MSE 307.1847241903416 Test RE 0.295045488434334 Lambda1 0.00058043597\n",
      "Training time: 125.85\n",
      "Training time: 125.85\n",
      "inv_HT_atanh_tune11\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 34478.617 Test MSE 3521.9294496050998 Test RE 0.9990322238590043 Lambda1 -0.00081904966\n",
      "1 Train Loss 25089.242 Test MSE 3511.826068018725 Test RE 0.9975982296212553 Lambda1 0.000808949\n",
      "2 Train Loss 19271.23 Test MSE 3511.339008969538 Test RE 0.9975290482204577 Lambda1 -0.00018102487\n",
      "3 Train Loss 14682.768 Test MSE 3512.8636035083337 Test RE 0.9977455841756118 Lambda1 -2.1337486e-05\n",
      "4 Train Loss 11272.502 Test MSE 3513.4005453436084 Test RE 0.9978218340582538 Lambda1 0.00017905245\n",
      "5 Train Loss 9233.391 Test MSE 3514.2680787939807 Test RE 0.9979450181896257 Lambda1 0.0001900797\n",
      "6 Train Loss 7696.25 Test MSE 3511.070356106878 Test RE 0.9974908869720184 Lambda1 -0.0003104767\n",
      "7 Train Loss 6243.0967 Test MSE 3501.4438288463175 Test RE 0.9961225059456621 Lambda1 0.00012253504\n",
      "8 Train Loss 5396.624 Test MSE 3502.4413128611877 Test RE 0.9962643824929297 Lambda1 0.0002851383\n",
      "9 Train Loss 4929.1562 Test MSE 3499.9454681192906 Test RE 0.9959093495129325 Lambda1 -3.1916643e-06\n",
      "10 Train Loss 4588.69 Test MSE 3498.5801042155085 Test RE 0.99571507344044 Lambda1 0.00025140136\n",
      "11 Train Loss 4275.9453 Test MSE 3497.8857799333746 Test RE 0.9956162642896585 Lambda1 0.00029342182\n",
      "12 Train Loss 4093.8608 Test MSE 3495.8406328302094 Test RE 0.9953251628124958 Lambda1 -0.00014228342\n",
      "13 Train Loss 3992.5903 Test MSE 3493.3507956218377 Test RE 0.9949706502212391 Lambda1 -3.906371e-05\n",
      "14 Train Loss 3893.0112 Test MSE 3489.932655024285 Test RE 0.9944837563890193 Lambda1 0.0001882338\n",
      "15 Train Loss 3813.8228 Test MSE 3487.198585323541 Test RE 0.9940941327331322 Lambda1 0.00016824689\n",
      "16 Train Loss 3734.154 Test MSE 3476.3227550334864 Test RE 0.9925427381458439 Lambda1 8.697287e-05\n",
      "17 Train Loss 3676.4924 Test MSE 3468.030365857313 Test RE 0.9913582300647051 Lambda1 0.00011091709\n",
      "18 Train Loss 3630.1333 Test MSE 3464.655447970573 Test RE 0.9908757419306836 Lambda1 -4.443458e-05\n",
      "19 Train Loss 3582.9158 Test MSE 3451.2463109827304 Test RE 0.9889564068698553 Lambda1 0.00017583233\n",
      "20 Train Loss 3540.9014 Test MSE 3436.534772791362 Test RE 0.986846356433533 Lambda1 0.00011488116\n",
      "21 Train Loss 3505.5222 Test MSE 3415.7496355788307 Test RE 0.9838574668054971 Lambda1 -0.00032478265\n",
      "22 Train Loss 3480.0198 Test MSE 3398.32569013042 Test RE 0.9813449003034177 Lambda1 -0.00037282368\n",
      "23 Train Loss 3454.6675 Test MSE 3381.682436166685 Test RE 0.97893888916815 Lambda1 -0.00010596966\n",
      "24 Train Loss 3430.0732 Test MSE 3357.0554381665575 Test RE 0.9753678296276351 Lambda1 -5.7746096e-05\n",
      "25 Train Loss 3388.6807 Test MSE 3301.4723996733733 Test RE 0.9672595049604024 Lambda1 0.0002707681\n",
      "26 Train Loss 3349.1458 Test MSE 3268.286124931672 Test RE 0.9623857986083615 Lambda1 0.00027032226\n",
      "27 Train Loss 3302.6086 Test MSE 3222.532248396661 Test RE 0.9556256687622772 Lambda1 0.0002003659\n",
      "28 Train Loss 3246.863 Test MSE 3172.6861887643445 Test RE 0.9482060667205732 Lambda1 -0.00018020008\n",
      "29 Train Loss 3205.8215 Test MSE 3134.5777229054606 Test RE 0.942494212449539 Lambda1 0.00012620856\n",
      "30 Train Loss 3151.2056 Test MSE 3096.7980232179625 Test RE 0.9367972586187419 Lambda1 5.7604768e-05\n",
      "31 Train Loss 3085.0237 Test MSE 3034.9228802886482 Test RE 0.9273912639854245 Lambda1 -1.8024311e-05\n",
      "32 Train Loss 3038.895 Test MSE 2990.5537489533544 Test RE 0.9205872947791807 Lambda1 2.2580363e-05\n",
      "33 Train Loss 2986.8286 Test MSE 2944.4383039849445 Test RE 0.9134618203969193 Lambda1 0.00022417745\n",
      "34 Train Loss 2944.3853 Test MSE 2906.114124076586 Test RE 0.9074976377508813 Lambda1 -5.0000464e-05\n",
      "35 Train Loss 2899.9402 Test MSE 2860.411496201428 Test RE 0.9003335377007667 Lambda1 7.339617e-05\n",
      "36 Train Loss 2849.0054 Test MSE 2808.2521458313004 Test RE 0.8920870199388844 Lambda1 1.4692141e-06\n",
      "37 Train Loss 2776.1663 Test MSE 2715.6856570442164 Test RE 0.8772611966607182 Lambda1 -4.5478107e-05\n",
      "38 Train Loss 2697.2317 Test MSE 2636.0752602894127 Test RE 0.8643070853952747 Lambda1 7.334058e-05\n",
      "39 Train Loss 2597.223 Test MSE 2526.81723751064 Test RE 0.8462059721955687 Lambda1 8.2875085e-05\n",
      "40 Train Loss 2463.6143 Test MSE 2410.23231568515 Test RE 0.826453880455394 Lambda1 -0.00026358562\n",
      "41 Train Loss 1974.6688 Test MSE 1801.8541618258216 Test RE 0.7145769271554899 Lambda1 0.0009219352\n",
      "42 Train Loss 1713.8805 Test MSE 1638.1482616261403 Test RE 0.6813429605714976 Lambda1 -6.0446626e-05\n",
      "43 Train Loss 885.30725 Test MSE 864.0855229632134 Test RE 0.4948431184427656 Lambda1 -0.0019506125\n",
      "44 Train Loss 854.0082 Test MSE 856.451786712933 Test RE 0.4926524313730154 Lambda1 -0.0025255175\n",
      "45 Train Loss 832.31165 Test MSE 834.3504593492136 Test RE 0.48625426692339946 Lambda1 -0.00030640844\n",
      "46 Train Loss 801.4656 Test MSE 803.2740134033883 Test RE 0.47711275753480115 Lambda1 -0.00040773008\n",
      "47 Train Loss 767.1731 Test MSE 737.3862651558783 Test RE 0.45712680967465735 Lambda1 -9.069868e-05\n",
      "48 Train Loss 717.7206 Test MSE 702.3556216630645 Test RE 0.4461364447423045 Lambda1 -0.000109261186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 Train Loss 676.2595 Test MSE 678.9128880680855 Test RE 0.4386278440639694 Lambda1 -0.000112982874\n",
      "50 Train Loss 668.6046 Test MSE 671.7839526661851 Test RE 0.4363188574111458 Lambda1 -0.00014470101\n",
      "51 Train Loss 658.99866 Test MSE 663.7428566412818 Test RE 0.4336996789903416 Lambda1 -4.2892378e-05\n",
      "52 Train Loss 649.05035 Test MSE 650.1956857518163 Test RE 0.4292508979930362 Lambda1 -1.7897044e-05\n",
      "53 Train Loss 625.2015 Test MSE 626.8845217941796 Test RE 0.4214857965313821 Lambda1 9.0034075e-07\n",
      "54 Train Loss 614.01917 Test MSE 618.9973586690486 Test RE 0.4188259367865999 Lambda1 5.136046e-07\n",
      "55 Train Loss 602.9373 Test MSE 609.5497853111593 Test RE 0.41561743901002174 Lambda1 3.1207145e-07\n",
      "56 Train Loss 597.8174 Test MSE 603.3382132005418 Test RE 0.413494356959559 Lambda1 1.7443773e-07\n",
      "57 Train Loss 592.898 Test MSE 598.9040990141186 Test RE 0.41197210767110104 Lambda1 1.1509576e-06\n",
      "58 Train Loss 572.31525 Test MSE 574.7272036340707 Test RE 0.4035710899112539 Lambda1 1.2278078e-06\n",
      "59 Train Loss 557.3767 Test MSE 559.3055263468158 Test RE 0.3981197529320321 Lambda1 -2.253606e-05\n",
      "60 Train Loss 536.494 Test MSE 537.8714540369471 Test RE 0.39041672909481195 Lambda1 -5.560514e-05\n",
      "61 Train Loss 516.67163 Test MSE 521.5531830593283 Test RE 0.38444876557125046 Lambda1 -0.00012071299\n",
      "62 Train Loss 512.102 Test MSE 518.8023989640793 Test RE 0.3834335923432187 Lambda1 -0.00012453209\n",
      "63 Train Loss 511.25842 Test MSE 518.5804940711449 Test RE 0.3833515814555396 Lambda1 -0.00010593151\n",
      "64 Train Loss 509.80194 Test MSE 517.5229373768753 Test RE 0.3829604917971851 Lambda1 -7.102777e-05\n",
      "65 Train Loss 507.48276 Test MSE 515.2730393138686 Test RE 0.3821271368776759 Lambda1 -6.275218e-05\n",
      "66 Train Loss 503.65768 Test MSE 512.4978015754666 Test RE 0.3810966876095557 Lambda1 -5.8150417e-05\n",
      "67 Train Loss 497.49994 Test MSE 504.9040419888885 Test RE 0.37826276634117173 Lambda1 -6.5573226e-06\n",
      "68 Train Loss 492.64194 Test MSE 500.36725216675177 Test RE 0.3765595010450372 Lambda1 -1.1013231e-05\n",
      "69 Train Loss 490.37067 Test MSE 497.81295603672993 Test RE 0.3755971327730947 Lambda1 3.4551567e-07\n",
      "70 Train Loss 486.96835 Test MSE 494.37407171391027 Test RE 0.3742975749142061 Lambda1 -1.2616924e-06\n",
      "71 Train Loss 479.14648 Test MSE 484.20960541842317 Test RE 0.37042976051076754 Lambda1 -1.16412175e-05\n",
      "72 Train Loss 472.03302 Test MSE 474.6951476167446 Test RE 0.3667723321671515 Lambda1 -2.050424e-06\n",
      "73 Train Loss 457.14148 Test MSE 459.01740382813335 Test RE 0.3606647904254337 Lambda1 -1.7989518e-05\n",
      "74 Train Loss 446.35486 Test MSE 451.55826883801166 Test RE 0.35772234605519093 Lambda1 -7.4774753e-06\n",
      "Training time: 126.16\n",
      "Training time: 126.16\n",
      "inv_HT_atanh_tune11\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 14821.799 Test MSE 3606.9632457854473 Test RE 1.0110206570114049 Lambda1 -0.0003852384\n",
      "1 Train Loss 10448.691 Test MSE 3602.78976312184 Test RE 1.010435580569875 Lambda1 0.0009813812\n",
      "2 Train Loss 7193.2773 Test MSE 3605.754400606253 Test RE 1.010851225029277 Lambda1 -0.0008761586\n",
      "3 Train Loss 5819.8047 Test MSE 3605.2873893496067 Test RE 1.0107857610326845 Lambda1 0.0009466496\n",
      "4 Train Loss 4857.6694 Test MSE 3598.914623952663 Test RE 1.0098920251290153 Lambda1 0.0004679955\n",
      "5 Train Loss 4313.618 Test MSE 3595.0597212239595 Test RE 1.0093510177733256 Lambda1 -0.0010600984\n",
      "6 Train Loss 3992.0486 Test MSE 3593.003781486758 Test RE 1.0090623630919133 Lambda1 7.9854566e-05\n",
      "7 Train Loss 3803.3276 Test MSE 3588.4018084167005 Test RE 1.0084159449443844 Lambda1 0.0005685287\n",
      "8 Train Loss 3689.4043 Test MSE 3580.4108669978104 Test RE 1.0072925083377615 Lambda1 -0.00067044894\n",
      "9 Train Loss 3630.762 Test MSE 3572.295814117969 Test RE 1.0061503392844688 Lambda1 -0.0002490607\n",
      "10 Train Loss 3504.4075 Test MSE 3397.583294242204 Test RE 0.9812377024832079 Lambda1 -0.00062082155\n",
      "11 Train Loss 3122.2263 Test MSE 3087.0419854155257 Test RE 0.9353204689007021 Lambda1 0.0022842335\n",
      "12 Train Loss 1450.7367 Test MSE 1353.8271813337037 Test RE 0.6193994071033166 Lambda1 0.086865425\n",
      "13 Train Loss 861.0162 Test MSE 862.5539418946978 Test RE 0.4944043722372982 Lambda1 0.08624782\n",
      "14 Train Loss 854.7736 Test MSE 857.9932433398537 Test RE 0.49309557421986683 Lambda1 0.08823073\n",
      "15 Train Loss 854.6892 Test MSE 858.0112632557616 Test RE 0.49310075228752476 Lambda1 0.08655444\n",
      "16 Train Loss 854.48975 Test MSE 857.611343698377 Test RE 0.49298582161747595 Lambda1 0.072408445\n",
      "17 Train Loss 854.20856 Test MSE 856.9511553811349 Test RE 0.4927960351006163 Lambda1 0.015485461\n",
      "18 Train Loss 853.1906 Test MSE 854.5275018624001 Test RE 0.49209867182685807 Lambda1 -0.038950503\n",
      "19 Train Loss 849.8604 Test MSE 850.1907444344874 Test RE 0.49084837436271966 Lambda1 -0.0061420184\n",
      "20 Train Loss 841.9994 Test MSE 839.3212748098924 Test RE 0.48770059611350725 Lambda1 -0.0050680437\n",
      "21 Train Loss 828.96576 Test MSE 823.921695418195 Test RE 0.48320580164717186 Lambda1 -0.0038415997\n",
      "22 Train Loss 811.7932 Test MSE 814.6346127007084 Test RE 0.48047478375556474 Lambda1 -0.00041111777\n",
      "23 Train Loss 802.4898 Test MSE 806.3806572970676 Test RE 0.4780344785554108 Lambda1 0.00025526667\n",
      "24 Train Loss 793.0124 Test MSE 797.8846269820282 Test RE 0.4755095234772614 Lambda1 0.0004320765\n",
      "25 Train Loss 780.86725 Test MSE 784.7642428794048 Test RE 0.4715836873509854 Lambda1 0.00018056163\n",
      "26 Train Loss 765.3419 Test MSE 772.1964574583803 Test RE 0.4677923044958297 Lambda1 -3.8220354e-05\n",
      "27 Train Loss 756.1362 Test MSE 764.0679622522788 Test RE 0.465323692355934 Lambda1 -0.0001731992\n",
      "28 Train Loss 751.12854 Test MSE 759.4024195209842 Test RE 0.4639008399725634 Lambda1 -0.00015991762\n",
      "29 Train Loss 744.99506 Test MSE 753.9261349846498 Test RE 0.4622251476672163 Lambda1 -7.615655e-05\n",
      "30 Train Loss 739.3119 Test MSE 748.2976413537028 Test RE 0.46049652661359036 Lambda1 6.3909145e-05\n",
      "31 Train Loss 730.0037 Test MSE 738.937662114989 Test RE 0.4576074346549248 Lambda1 0.0005195683\n",
      "32 Train Loss 720.0938 Test MSE 725.6512788666234 Test RE 0.45347479497865334 Lambda1 0.00069063\n",
      "33 Train Loss 698.1036 Test MSE 707.2302276183075 Test RE 0.4476819431826425 Lambda1 0.00011711243\n",
      "34 Train Loss 687.0919 Test MSE 699.295525532468 Test RE 0.44516349695914526 Lambda1 0.00010599204\n",
      "35 Train Loss 683.98706 Test MSE 698.8333365589633 Test RE 0.44501636054812777 Lambda1 0.00010489604\n",
      "36 Train Loss 678.2748 Test MSE 693.9669506018422 Test RE 0.4434641988516525 Lambda1 2.4311823e-05\n",
      "37 Train Loss 676.2439 Test MSE 693.0322921066949 Test RE 0.4431654618325651 Lambda1 2.0808475e-05\n",
      "38 Train Loss 675.1596 Test MSE 692.3780473668967 Test RE 0.44295623122690664 Lambda1 1.6830418e-05\n",
      "39 Train Loss 673.6612 Test MSE 691.5415917733508 Test RE 0.4426885846790638 Lambda1 1.4586596e-05\n",
      "40 Train Loss 671.86646 Test MSE 690.5338734081427 Test RE 0.4423659229205249 Lambda1 1.12979715e-05\n",
      "41 Train Loss 670.57697 Test MSE 689.292733455533 Test RE 0.44196819808119353 Lambda1 4.094799e-06\n",
      "42 Train Loss 668.52203 Test MSE 688.1237020343422 Test RE 0.44159325290914825 Lambda1 -2.0984662e-06\n",
      "43 Train Loss 667.3966 Test MSE 688.1896215127005 Test RE 0.44161440382989114 Lambda1 5.7904475e-07\n",
      "44 Train Loss 666.92773 Test MSE 688.0653951633118 Test RE 0.441574543724871 Lambda1 -4.488824e-06\n",
      "45 Train Loss 666.84283 Test MSE 687.9826611946448 Test RE 0.4415479951489732 Lambda1 -5.602547e-06\n",
      "46 Train Loss 665.9787 Test MSE 687.3560629527952 Test RE 0.4413468736392261 Lambda1 -5.011717e-06\n",
      "47 Train Loss 665.14655 Test MSE 686.7441715820396 Test RE 0.4411503841583427 Lambda1 -7.6580443e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 Train Loss 664.61523 Test MSE 686.5124810304995 Test RE 0.4410759612492614 Lambda1 -3.6126598e-06\n",
      "49 Train Loss 664.0042 Test MSE 686.18503076243 Test RE 0.4409707572855106 Lambda1 -8.023748e-06\n",
      "50 Train Loss 663.81055 Test MSE 686.1099801718041 Test RE 0.44094664132605166 Lambda1 -7.530349e-06\n",
      "51 Train Loss 663.63214 Test MSE 686.2282084323741 Test RE 0.44098463094162976 Lambda1 -6.0590532e-06\n",
      "52 Train Loss 663.41315 Test MSE 686.1250482561081 Test RE 0.4409514832499983 Lambda1 -5.487097e-06\n",
      "53 Train Loss 663.21625 Test MSE 685.9965586499384 Test RE 0.44091019315618446 Lambda1 -6.2761583e-06\n",
      "54 Train Loss 662.97284 Test MSE 685.7709491121093 Test RE 0.4408376842465762 Lambda1 -6.579906e-06\n",
      "55 Train Loss 662.74426 Test MSE 685.4548559309919 Test RE 0.4407360746296518 Lambda1 -1.6359853e-06\n",
      "56 Train Loss 662.67865 Test MSE 685.3992583033291 Test RE 0.4407182000921358 Lambda1 4.528789e-08\n",
      "57 Train Loss 662.27747 Test MSE 684.5856568401083 Test RE 0.440456545705353 Lambda1 7.185164e-06\n",
      "58 Train Loss 661.9008 Test MSE 684.2801946508407 Test RE 0.44035826886374213 Lambda1 7.3057186e-06\n",
      "59 Train Loss 661.36255 Test MSE 683.2686439356238 Test RE 0.44003266433144717 Lambda1 7.38265e-06\n",
      "60 Train Loss 661.0471 Test MSE 683.0426756753136 Test RE 0.4399598952683924 Lambda1 7.681709e-06\n",
      "61 Train Loss 660.83655 Test MSE 682.8622226056272 Test RE 0.4399017749189532 Lambda1 7.2420257e-06\n",
      "62 Train Loss 660.344 Test MSE 682.5066510999843 Test RE 0.43978722992115976 Lambda1 4.3317987e-06\n",
      "63 Train Loss 660.13245 Test MSE 682.643003606964 Test RE 0.4398311585015657 Lambda1 6.2769795e-06\n",
      "64 Train Loss 659.8372 Test MSE 682.2132617623049 Test RE 0.43969269402667366 Lambda1 1.8374178e-05\n",
      "65 Train Loss 659.5521 Test MSE 681.7043883999461 Test RE 0.4395286766643797 Lambda1 2.2843973e-05\n",
      "66 Train Loss 659.40326 Test MSE 681.562323288363 Test RE 0.43948287606146424 Lambda1 1.6045076e-05\n",
      "67 Train Loss 658.98456 Test MSE 681.3940110038953 Test RE 0.4394286074115926 Lambda1 8.775505e-06\n",
      "68 Train Loss 658.3339 Test MSE 680.9665884235603 Test RE 0.4392907641303883 Lambda1 3.1261561e-06\n",
      "69 Train Loss 658.02954 Test MSE 680.5439768334816 Test RE 0.43915442984895126 Lambda1 1.20661025e-05\n",
      "70 Train Loss 657.86804 Test MSE 680.6417740391609 Test RE 0.4391859829408278 Lambda1 4.2458723e-06\n",
      "71 Train Loss 657.7172 Test MSE 679.8846977195398 Test RE 0.4389416621903694 Lambda1 -1.1037464e-05\n",
      "72 Train Loss 656.28876 Test MSE 677.2086331915019 Test RE 0.4380769608998538 Lambda1 -4.0793708e-05\n",
      "73 Train Loss 655.761 Test MSE 677.5398731806231 Test RE 0.43818408510125256 Lambda1 -1.7946168e-05\n",
      "74 Train Loss 655.38293 Test MSE 676.957462443862 Test RE 0.43799571391275177 Lambda1 5.1851188e-05\n",
      "Training time: 125.83\n",
      "Training time: 125.83\n",
      "inv_HT_atanh_tune11\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 26439.357 Test MSE 3387.365642200287 Test RE 0.9797611390363099 Lambda1 0.0004397675\n",
      "1 Train Loss 19044.213 Test MSE 3384.13308716904 Test RE 0.9792935355810919 Lambda1 -0.00045813428\n",
      "2 Train Loss 13453.207 Test MSE 3360.1078945447357 Test RE 0.9758111632771487 Lambda1 0.00014438442\n",
      "3 Train Loss 10022.018 Test MSE 3346.309706152402 Test RE 0.9738055316180494 Lambda1 0.0010663478\n",
      "4 Train Loss 8095.09 Test MSE 3344.1303749364133 Test RE 0.9734883777040748 Lambda1 0.0008172648\n",
      "5 Train Loss 5745.2676 Test MSE 3347.9181301930657 Test RE 0.9740395362677631 Lambda1 -0.00091783423\n",
      "6 Train Loss 4677.9336 Test MSE 3346.3641086663 Test RE 0.9738134473906919 Lambda1 -0.0009789724\n",
      "7 Train Loss 4197.958 Test MSE 3348.0325523776455 Test RE 0.974056181056407 Lambda1 -0.0007977967\n",
      "8 Train Loss 3923.753 Test MSE 3352.7135870835164 Test RE 0.9747368790616802 Lambda1 0.00018525077\n",
      "9 Train Loss 3697.1465 Test MSE 3352.644156546906 Test RE 0.9747267862140228 Lambda1 0.0012967251\n",
      "10 Train Loss 3555.4187 Test MSE 3344.6181234485066 Test RE 0.9735593677885672 Lambda1 0.0012915102\n",
      "11 Train Loss 3423.8862 Test MSE 3331.8754845106455 Test RE 0.9717030192120503 Lambda1 -0.000833879\n",
      "12 Train Loss 3359.3948 Test MSE 3312.8586194364448 Test RE 0.9689260265052283 Lambda1 -0.00051200757\n",
      "13 Train Loss 3291.2546 Test MSE 3245.777261921509 Test RE 0.9590660714690579 Lambda1 0.0013609227\n",
      "14 Train Loss 3181.944 Test MSE 3134.3492721614593 Test RE 0.9424598669257895 Lambda1 -0.0007035561\n",
      "15 Train Loss 865.1009 Test MSE 867.3575942706622 Test RE 0.49577915543428563 Lambda1 0.024406807\n",
      "16 Train Loss 854.2687 Test MSE 857.1394489450065 Test RE 0.4928501719231182 Lambda1 0.024734383\n",
      "17 Train Loss 853.5797 Test MSE 855.9581295660972 Test RE 0.4925104289551715 Lambda1 0.022798695\n",
      "18 Train Loss 849.2392 Test MSE 849.227599409673 Test RE 0.49057026492632927 Lambda1 0.018015265\n",
      "19 Train Loss 840.46875 Test MSE 838.5440143781328 Test RE 0.4874747242166186 Lambda1 0.018034637\n",
      "20 Train Loss 820.5602 Test MSE 820.5410133591963 Test RE 0.4822134474375552 Lambda1 0.016014658\n",
      "21 Train Loss 804.1755 Test MSE 807.9059134823477 Test RE 0.47848636227942065 Lambda1 0.006287872\n",
      "22 Train Loss 781.4967 Test MSE 773.5464142074505 Test RE 0.46820102407496683 Lambda1 0.002144827\n",
      "23 Train Loss 748.48376 Test MSE 734.0520986421009 Test RE 0.4560921662561629 Lambda1 0.0016878525\n",
      "24 Train Loss 709.16614 Test MSE 697.0241962035155 Test RE 0.44443995788593293 Lambda1 -0.0013945024\n",
      "25 Train Loss 688.29645 Test MSE 678.8092616895093 Test RE 0.438594367641288 Lambda1 -0.00067702984\n",
      "26 Train Loss 671.13873 Test MSE 666.1253770890472 Test RE 0.4344773692821587 Lambda1 -0.00026980683\n",
      "27 Train Loss 655.253 Test MSE 652.0976429774819 Test RE 0.429878263484257 Lambda1 0.0005012809\n",
      "28 Train Loss 645.8447 Test MSE 645.3055544040551 Test RE 0.4276336502486282 Lambda1 0.00046939435\n",
      "29 Train Loss 636.71515 Test MSE 634.6544975812985 Test RE 0.42408982395603495 Lambda1 0.00035634247\n",
      "30 Train Loss 630.434 Test MSE 627.618548788236 Test RE 0.4217324858574763 Lambda1 0.00050258345\n",
      "31 Train Loss 622.2445 Test MSE 622.5205071720269 Test RE 0.4200161617986875 Lambda1 0.00027536566\n",
      "32 Train Loss 616.1152 Test MSE 615.6144844323082 Test RE 0.4176799088259916 Lambda1 0.00019949136\n",
      "33 Train Loss 612.6696 Test MSE 610.6545054717375 Test RE 0.4159938915425294 Lambda1 0.00015212683\n",
      "34 Train Loss 607.53754 Test MSE 608.0443832731106 Test RE 0.41510389759230215 Lambda1 4.73087e-05\n",
      "35 Train Loss 600.7732 Test MSE 605.2932579465725 Test RE 0.4141637544306946 Lambda1 6.0567513e-06\n",
      "36 Train Loss 597.2153 Test MSE 600.6501756117855 Test RE 0.41257221320007975 Lambda1 4.2575884e-06\n",
      "37 Train Loss 595.4266 Test MSE 599.2129241507247 Test RE 0.41207831077244794 Lambda1 3.533287e-07\n",
      "38 Train Loss 593.4681 Test MSE 597.5251763837833 Test RE 0.4114975700722373 Lambda1 -3.768658e-05\n",
      "39 Train Loss 589.90375 Test MSE 594.652216688825 Test RE 0.41050711773707754 Lambda1 1.0914455e-06\n",
      "40 Train Loss 586.5099 Test MSE 591.3411181653089 Test RE 0.4093626446739477 Lambda1 -3.1107254e-05\n",
      "41 Train Loss 582.2002 Test MSE 584.7019600981743 Test RE 0.40705814157519354 Lambda1 -8.101228e-05\n",
      "42 Train Loss 577.2398 Test MSE 577.8697462848946 Test RE 0.4046729260791509 Lambda1 -7.961391e-05\n",
      "43 Train Loss 574.9905 Test MSE 575.6847862053244 Test RE 0.4039071552665407 Lambda1 -6.3503154e-05\n",
      "44 Train Loss 571.2791 Test MSE 570.1313918431591 Test RE 0.4019542710827288 Lambda1 -5.105051e-05\n",
      "45 Train Loss 556.7855 Test MSE 556.1577953892723 Test RE 0.39699787769210887 Lambda1 -2.671725e-05\n",
      "46 Train Loss 554.70074 Test MSE 554.2273588309029 Test RE 0.39630828437010535 Lambda1 -2.1914455e-05\n",
      "47 Train Loss 552.0483 Test MSE 553.6758765403216 Test RE 0.3961110626048425 Lambda1 -2.851613e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 Train Loss 550.13464 Test MSE 553.0825820352755 Test RE 0.3958987782005042 Lambda1 -3.5810623e-05\n",
      "49 Train Loss 547.2988 Test MSE 551.5302302719487 Test RE 0.3953427979290942 Lambda1 -3.7071568e-05\n",
      "50 Train Loss 545.8312 Test MSE 550.610736574692 Test RE 0.3950131089767257 Lambda1 -2.9922698e-05\n",
      "51 Train Loss 543.66956 Test MSE 548.7689882515629 Test RE 0.3943519121712734 Lambda1 -1.9594641e-05\n",
      "52 Train Loss 540.0847 Test MSE 545.0543401691754 Test RE 0.3930149507240628 Lambda1 -7.1464783e-06\n",
      "53 Train Loss 537.20953 Test MSE 542.5835187732735 Test RE 0.3921231380332008 Lambda1 -3.5476821e-06\n",
      "54 Train Loss 527.7868 Test MSE 533.3233313178757 Test RE 0.38876258590602797 Lambda1 1.2325602e-06\n",
      "55 Train Loss 524.7031 Test MSE 528.2431669715959 Test RE 0.38690657905022807 Lambda1 6.0804714e-06\n",
      "56 Train Loss 523.4534 Test MSE 523.6737874869046 Test RE 0.3852295457081538 Lambda1 7.0660176e-06\n",
      "57 Train Loss 522.0227 Test MSE 523.192572007364 Test RE 0.38505250703286825 Lambda1 2.4326803e-06\n",
      "58 Train Loss 521.0376 Test MSE 523.7862098375986 Test RE 0.3852708940513455 Lambda1 2.7941549e-06\n",
      "59 Train Loss 520.7785 Test MSE 523.0442986325087 Test RE 0.38499794100555795 Lambda1 4.378894e-06\n",
      "60 Train Loss 519.41187 Test MSE 521.0693149637618 Test RE 0.38427038907634553 Lambda1 -4.257737e-08\n",
      "61 Train Loss 517.07715 Test MSE 517.2181950344195 Test RE 0.38284772243544446 Lambda1 8.774059e-06\n",
      "62 Train Loss 514.46436 Test MSE 514.9440037358473 Test RE 0.3820051107939952 Lambda1 2.3394095e-06\n",
      "63 Train Loss 512.424 Test MSE 513.7325950793229 Test RE 0.3815555116344751 Lambda1 3.3258702e-06\n",
      "64 Train Loss 509.88998 Test MSE 511.7403933700048 Test RE 0.3808149766618016 Lambda1 1.197732e-05\n",
      "65 Train Loss 508.81863 Test MSE 511.646566917272 Test RE 0.3807800642757574 Lambda1 4.8329825e-06\n",
      "66 Train Loss 508.14127 Test MSE 509.7936838634192 Test RE 0.38008995815871377 Lambda1 4.502957e-06\n",
      "67 Train Loss 506.9206 Test MSE 508.02543114855996 Test RE 0.3794302021279087 Lambda1 5.710752e-06\n",
      "68 Train Loss 505.65665 Test MSE 503.91360930471177 Test RE 0.3778915792601004 Lambda1 5.381767e-07\n",
      "69 Train Loss 504.32782 Test MSE 501.3532665128854 Test RE 0.37693033899736667 Lambda1 1.3285817e-06\n",
      "70 Train Loss 502.92783 Test MSE 500.06851146104185 Test RE 0.376447073176812 Lambda1 -7.778456e-07\n",
      "71 Train Loss 501.62372 Test MSE 501.4818331945328 Test RE 0.3769786657758342 Lambda1 -3.0410913e-06\n",
      "72 Train Loss 501.04205 Test MSE 501.7040870334694 Test RE 0.3770621939008086 Lambda1 1.9416078e-07\n",
      "73 Train Loss 499.35233 Test MSE 501.1231381208856 Test RE 0.3768438208325419 Lambda1 4.791476e-06\n",
      "74 Train Loss 497.70734 Test MSE 499.9687846383094 Test RE 0.3764095345780298 Lambda1 2.8953907e-06\n",
      "Training time: 129.35\n",
      "Training time: 129.35\n",
      "inv_HT_atanh_tune11\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 27475.646 Test MSE 3669.8348503417865 Test RE 1.01979394947463 Lambda1 -0.0024005603\n",
      "1 Train Loss 19021.38 Test MSE 3669.717516114691 Test RE 1.0197776466032218 Lambda1 0.0008492633\n",
      "2 Train Loss 11503.555 Test MSE 3670.826740252106 Test RE 1.0199317560831647 Lambda1 -0.0010388606\n",
      "3 Train Loss 8552.929 Test MSE 3679.0731954827493 Test RE 1.0210767438152335 Lambda1 0.0005004265\n",
      "4 Train Loss 6526.6836 Test MSE 3674.7678703607007 Test RE 1.0204791267024977 Lambda1 7.681534e-06\n",
      "5 Train Loss 5240.8086 Test MSE 3661.520650353954 Test RE 1.0186380965672142 Lambda1 -0.00034086424\n",
      "6 Train Loss 4532.827 Test MSE 3660.8044369289196 Test RE 1.0185384661090298 Lambda1 0.0002605856\n",
      "7 Train Loss 4168.03 Test MSE 3659.021092837073 Test RE 1.0182903477167013 Lambda1 0.00010760907\n",
      "8 Train Loss 3967.9294 Test MSE 3647.9760619116314 Test RE 1.0167522930777495 Lambda1 -0.0004662251\n",
      "9 Train Loss 3897.5103 Test MSE 3638.3287361645466 Test RE 1.01540696705256 Lambda1 -0.0006420538\n",
      "10 Train Loss 3828.8215 Test MSE 3625.6690495786697 Test RE 1.013638856402188 Lambda1 5.1187286e-05\n",
      "11 Train Loss 3752.0637 Test MSE 3596.8381844586975 Test RE 1.009600648080403 Lambda1 0.00018990935\n",
      "12 Train Loss 3688.6643 Test MSE 3567.5146497092633 Test RE 1.0054767975149068 Lambda1 0.00013257946\n",
      "13 Train Loss 3604.9082 Test MSE 3513.3526715654784 Test RE 0.9978150358490531 Lambda1 1.9430374e-05\n",
      "14 Train Loss 3513.9966 Test MSE 3429.588502086576 Test RE 0.9858484952376861 Lambda1 -0.00012726086\n",
      "15 Train Loss 3400.2676 Test MSE 3334.605040759591 Test RE 0.9721009595119663 Lambda1 0.0003548948\n",
      "16 Train Loss 3293.1675 Test MSE 3253.3498990375188 Test RE 0.9601842055242495 Lambda1 -0.0008065137\n",
      "17 Train Loss 2999.096 Test MSE 2979.411961260071 Test RE 0.9188707966980182 Lambda1 3.7486065e-05\n",
      "18 Train Loss 1048.1079 Test MSE 946.5353762181029 Test RE 0.5179139342589105 Lambda1 -0.11579732\n",
      "19 Train Loss 855.862 Test MSE 858.8127624789797 Test RE 0.4933310101131811 Lambda1 -0.12787277\n",
      "20 Train Loss 854.7019 Test MSE 858.0126199547486 Test RE 0.4931011421361522 Lambda1 -0.13018417\n",
      "21 Train Loss 854.6865 Test MSE 858.026230129249 Test RE 0.49310505301451274 Lambda1 -0.12931909\n",
      "22 Train Loss 854.6076 Test MSE 857.8070283058247 Test RE 0.49304206168201425 Lambda1 -0.11702634\n",
      "23 Train Loss 854.5149 Test MSE 857.7211326661173 Test RE 0.49301737592978856 Lambda1 -0.10515917\n",
      "24 Train Loss 854.4749 Test MSE 857.5174223808647 Test RE 0.49295882620400655 Lambda1 -0.10037294\n",
      "25 Train Loss 854.4164 Test MSE 857.5117400847188 Test RE 0.4929571929178449 Lambda1 -0.08866099\n",
      "26 Train Loss 854.2343 Test MSE 857.0701676640037 Test RE 0.4928302533548495 Lambda1 -0.06474598\n",
      "27 Train Loss 853.8497 Test MSE 856.5657043897595 Test RE 0.49268519442824615 Lambda1 -0.065564156\n",
      "28 Train Loss 853.5551 Test MSE 855.8727005498594 Test RE 0.4924858508071004 Lambda1 -0.058676228\n",
      "29 Train Loss 853.14453 Test MSE 854.7881929581521 Test RE 0.4921737284863229 Lambda1 -0.054955136\n",
      "30 Train Loss 852.59845 Test MSE 854.7819281091924 Test RE 0.4921719248817892 Lambda1 -0.055036787\n",
      "31 Train Loss 851.04565 Test MSE 851.7262002365724 Test RE 0.49129141376829466 Lambda1 -0.058792222\n",
      "32 Train Loss 850.4554 Test MSE 850.5595522579365 Test RE 0.4909548264111166 Lambda1 -0.0597552\n",
      "33 Train Loss 850.2268 Test MSE 850.489878930948 Test RE 0.49093471779168035 Lambda1 -0.060927033\n",
      "34 Train Loss 849.5256 Test MSE 849.2821851833241 Test RE 0.4905860308571403 Lambda1 -0.06583196\n",
      "35 Train Loss 848.6353 Test MSE 847.9781259384114 Test RE 0.4902092426520057 Lambda1 -0.062106717\n",
      "36 Train Loss 847.96765 Test MSE 847.4419227674458 Test RE 0.4900542308021715 Lambda1 -0.058401737\n",
      "37 Train Loss 847.26105 Test MSE 845.8439232118347 Test RE 0.48959197141812166 Lambda1 -0.06185923\n",
      "38 Train Loss 846.1678 Test MSE 845.1186119326981 Test RE 0.4893820138077762 Lambda1 -0.06083123\n",
      "39 Train Loss 843.8424 Test MSE 841.4485801878773 Test RE 0.48831825681827307 Lambda1 -0.063174985\n",
      "40 Train Loss 842.4915 Test MSE 840.7949989418557 Test RE 0.48812857341927524 Lambda1 -0.06470532\n",
      "41 Train Loss 841.2998 Test MSE 838.5150100115109 Test RE 0.4874662935214838 Lambda1 -0.058418266\n",
      "42 Train Loss 839.689 Test MSE 836.5319345356685 Test RE 0.48688952705655764 Lambda1 -0.056529444\n",
      "43 Train Loss 838.1687 Test MSE 835.0169716033574 Test RE 0.4864484477528523 Lambda1 -0.063242376\n",
      "44 Train Loss 836.0925 Test MSE 832.0294401506465 Test RE 0.4855774581157935 Lambda1 -0.073826246\n",
      "45 Train Loss 833.156 Test MSE 829.4796557610346 Test RE 0.48483285234624773 Lambda1 -0.068966314\n",
      "46 Train Loss 830.8006 Test MSE 825.5644962462578 Test RE 0.4836872889041504 Lambda1 -0.07077615\n",
      "47 Train Loss 825.7446 Test MSE 819.3217202633836 Test RE 0.4818550387160748 Lambda1 -0.06285414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 Train Loss 819.1841 Test MSE 812.6818349321678 Test RE 0.4798985601226955 Lambda1 -0.063009575\n",
      "49 Train Loss 805.2595 Test MSE 794.6936948616426 Test RE 0.47455773255908745 Lambda1 -0.053441774\n",
      "50 Train Loss 790.04065 Test MSE 767.8946248229945 Test RE 0.46648747158139575 Lambda1 -0.04532594\n",
      "51 Train Loss 777.1603 Test MSE 763.564200920885 Test RE 0.46517026965994646 Lambda1 -0.048458412\n",
      "52 Train Loss 759.59827 Test MSE 735.149158249311 Test RE 0.45643285970486475 Lambda1 -0.018640177\n",
      "53 Train Loss 723.97906 Test MSE 711.3541554537153 Test RE 0.44898528431458584 Lambda1 0.0056895902\n",
      "54 Train Loss 686.6486 Test MSE 691.4336106851638 Test RE 0.44265402142080706 Lambda1 0.0016864572\n",
      "55 Train Loss 670.2547 Test MSE 677.8382569787625 Test RE 0.43828056107626306 Lambda1 0.0003768858\n",
      "56 Train Loss 655.97906 Test MSE 669.560387322684 Test RE 0.4355961640287346 Lambda1 -0.0004990207\n",
      "57 Train Loss 650.5603 Test MSE 666.4393815846083 Test RE 0.434579761247689 Lambda1 -0.00033359043\n",
      "58 Train Loss 648.15027 Test MSE 666.3268170527919 Test RE 0.4345430584848305 Lambda1 -0.00013322517\n",
      "59 Train Loss 646.0963 Test MSE 666.3056093801822 Test RE 0.4345361431693932 Lambda1 -5.294488e-05\n",
      "60 Train Loss 644.5783 Test MSE 665.7274760660318 Test RE 0.4343475852954583 Lambda1 1.2836417e-05\n",
      "61 Train Loss 643.98096 Test MSE 665.1875122697816 Test RE 0.43417140243136226 Lambda1 5.8526017e-05\n",
      "62 Train Loss 642.0782 Test MSE 661.2494730221891 Test RE 0.43288430512926446 Lambda1 0.00028034823\n",
      "63 Train Loss 640.6799 Test MSE 659.6276592643707 Test RE 0.4323531223045462 Lambda1 0.00030220678\n",
      "64 Train Loss 639.2383 Test MSE 658.7776812984467 Test RE 0.4320744733704125 Lambda1 0.00017086722\n",
      "65 Train Loss 638.3815 Test MSE 658.3783932983832 Test RE 0.4319435124204747 Lambda1 6.9639296e-05\n",
      "66 Train Loss 636.8272 Test MSE 656.9580456691921 Test RE 0.4314773358302225 Lambda1 -0.0001718877\n",
      "67 Train Loss 635.4996 Test MSE 656.573877417055 Test RE 0.431351160243435 Lambda1 -8.4553714e-05\n",
      "68 Train Loss 634.7377 Test MSE 655.4034323881249 Test RE 0.4309665134862644 Lambda1 7.915658e-05\n",
      "69 Train Loss 634.0042 Test MSE 654.2038358805934 Test RE 0.43057193002991956 Lambda1 5.9211825e-06\n",
      "70 Train Loss 633.4732 Test MSE 653.59541967535 Test RE 0.4303716653251755 Lambda1 2.0926469e-05\n",
      "71 Train Loss 632.3403 Test MSE 652.0801208778763 Test RE 0.42987248795322097 Lambda1 5.949854e-07\n",
      "72 Train Loss 629.69244 Test MSE 648.2398969430174 Test RE 0.42860481836319925 Lambda1 -4.8158927e-07\n",
      "73 Train Loss 626.0341 Test MSE 639.8245729252009 Test RE 0.42581369821331044 Lambda1 1.2871406e-05\n",
      "74 Train Loss 615.52484 Test MSE 612.6360083844673 Test RE 0.41666827083495794 Lambda1 -0.00011340599\n",
      "Training time: 125.18\n",
      "Training time: 125.18\n",
      "inv_HT_atanh_tune12\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.25\n",
      "Training time: 4.25\n",
      "inv_HT_atanh_tune12\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 1356598.0 Test MSE 3509.146348990025 Test RE 0.9972175454604002 Lambda1 2.9171712e-05\n",
      "1 Train Loss 1300812.6 Test MSE 3509.289741935477 Test RE 0.9972379197175745 Lambda1 2.0196156e-05\n",
      "2 Train Loss 1251382.1 Test MSE 3509.314296252205 Test RE 0.9972414085222476 Lambda1 2.5997842e-05\n",
      "3 Train Loss 1227388.9 Test MSE 3509.552580706005 Test RE 0.9972752645793941 Lambda1 3.1317442e-05\n",
      "4 Train Loss 1196332.0 Test MSE 3509.938970286056 Test RE 0.9973301613442007 Lambda1 7.2926916e-05\n",
      "5 Train Loss 1174690.9 Test MSE 3510.000237762264 Test RE 0.9973388657170975 Lambda1 6.492603e-05\n",
      "6 Train Loss 1146499.6 Test MSE 3510.273057739449 Test RE 0.997377624785636 Lambda1 3.301466e-05\n",
      "7 Train Loss 1120261.0 Test MSE 3510.3373501377305 Test RE 0.9973867584776339 Lambda1 -6.3954058e-06\n",
      "8 Train Loss 1105336.2 Test MSE 3510.2560334079662 Test RE 0.9973752062119797 Lambda1 -1.9397541e-05\n",
      "9 Train Loss 1083288.4 Test MSE 3510.315266344238 Test RE 0.9973836211555406 Lambda1 1.2394117e-05\n",
      "10 Train Loss 1054230.4 Test MSE 3510.3948563733848 Test RE 0.9973949280233715 Lambda1 4.1200972e-05\n",
      "11 Train Loss 1039613.3 Test MSE 3510.2979730503544 Test RE 0.997381164386275 Lambda1 5.3210333e-05\n",
      "12 Train Loss 1026384.4 Test MSE 3510.1218259763614 Test RE 0.9973561397332185 Lambda1 4.619745e-05\n",
      "13 Train Loss 1016305.2 Test MSE 3509.901199847363 Test RE 0.9973247951969008 Lambda1 7.919859e-06\n",
      "14 Train Loss 1005415.25 Test MSE 3509.917199010228 Test RE 0.997327068244308 Lambda1 -8.195899e-06\n",
      "15 Train Loss 994948.44 Test MSE 3509.8408705755373 Test RE 0.9973162239959196 Lambda1 2.7312071e-06\n",
      "16 Train Loss 979110.94 Test MSE 3509.886715060673 Test RE 0.9973227372968344 Lambda1 2.9774098e-06\n",
      "17 Train Loss 963458.7 Test MSE 3510.107569099152 Test RE 0.9973541142766729 Lambda1 -1.1925849e-06\n",
      "18 Train Loss 953842.3 Test MSE 3510.192462391191 Test RE 0.997366174898818 Lambda1 -5.932957e-06\n",
      "19 Train Loss 944923.5 Test MSE 3509.9978971292408 Test RE 0.9973385331808418 Lambda1 2.18479e-05\n",
      "20 Train Loss 930134.25 Test MSE 3509.5178927739876 Test RE 0.9972703361018653 Lambda1 5.582476e-06\n",
      "21 Train Loss 921401.9 Test MSE 3509.471669938455 Test RE 0.9972637687020232 Lambda1 -1.6214717e-05\n",
      "22 Train Loss 912850.3 Test MSE 3509.339565641363 Test RE 0.9972449989153764 Lambda1 -2.3317545e-05\n",
      "23 Train Loss 904666.44 Test MSE 3509.4296562105856 Test RE 0.9972577993000831 Lambda1 2.3995467e-06\n",
      "24 Train Loss 893616.06 Test MSE 3509.501380721616 Test RE 0.9972679900532214 Lambda1 5.0278537e-05\n",
      "25 Train Loss 884042.7 Test MSE 3509.648356381359 Test RE 0.9972888723054705 Lambda1 6.8739005e-06\n",
      "26 Train Loss 874133.25 Test MSE 3509.7223074878348 Test RE 0.9972993790884302 Lambda1 -2.9156856e-05\n",
      "27 Train Loss 863803.94 Test MSE 3509.4528346376464 Test RE 0.9972610925458892 Lambda1 -8.033473e-06\n",
      "28 Train Loss 850105.5 Test MSE 3509.5539038278007 Test RE 0.9972754525686724 Lambda1 -6.214494e-06\n",
      "29 Train Loss 841017.56 Test MSE 3509.4588473422214 Test RE 0.997261946843418 Lambda1 2.3987125e-06\n",
      "30 Train Loss 833267.06 Test MSE 3509.4783774854077 Test RE 0.9972647217215921 Lambda1 3.252741e-05\n",
      "31 Train Loss 824483.06 Test MSE 3509.438140656134 Test RE 0.9972590047914607 Lambda1 -2.4882162e-05\n",
      "32 Train Loss 819326.8 Test MSE 3509.427651349902 Test RE 0.9972575144442185 Lambda1 -3.7914346e-05\n",
      "33 Train Loss 812209.5 Test MSE 3509.330149716513 Test RE 0.9972436610581971 Lambda1 -3.750821e-06\n",
      "34 Train Loss 808365.06 Test MSE 3509.1170060547324 Test RE 0.9972133761627751 Lambda1 8.575347e-06\n",
      "35 Train Loss 804052.6 Test MSE 3508.8679897774236 Test RE 0.9971779930781205 Lambda1 2.6045254e-05\n",
      "36 Train Loss 800137.0 Test MSE 3508.8455003717936 Test RE 0.9971747974639836 Lambda1 2.6405989e-05\n",
      "37 Train Loss 796062.75 Test MSE 3508.7439634427824 Test RE 0.9971603695276297 Lambda1 -5.06614e-07\n",
      "38 Train Loss 789850.06 Test MSE 3508.6644493615404 Test RE 0.9971490707921467 Lambda1 -2.1020733e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Train Loss 785083.06 Test MSE 3508.6952547352644 Test RE 0.9971534481674704 Lambda1 -2.7951917e-06\n",
      "40 Train Loss 779101.44 Test MSE 3508.7859318691617 Test RE 0.9971663330756759 Lambda1 -1.6061344e-05\n",
      "41 Train Loss 772752.75 Test MSE 3508.9600620991455 Test RE 0.9971910759143772 Lambda1 3.1163927e-06\n",
      "42 Train Loss 765335.2 Test MSE 3509.034443429963 Test RE 0.9972016448585843 Lambda1 -1.3835688e-05\n",
      "43 Train Loss 762137.06 Test MSE 3508.9865818895205 Test RE 0.9971948441602198 Lambda1 -2.998738e-05\n",
      "44 Train Loss 758311.6 Test MSE 3509.0491089368 Test RE 0.9972037286871268 Lambda1 1.2121304e-06\n",
      "45 Train Loss 755040.7 Test MSE 3508.9812673579245 Test RE 0.9971940890097809 Lambda1 8.538566e-06\n",
      "46 Train Loss 751938.7 Test MSE 3508.9637706551403 Test RE 0.9971916028707832 Lambda1 -1.3595966e-06\n",
      "47 Train Loss 749258.06 Test MSE 3508.984833524726 Test RE 0.9971945957322854 Lambda1 8.64947e-06\n",
      "48 Train Loss 744892.6 Test MSE 3508.7995955615265 Test RE 0.9971682746248347 Lambda1 2.0618549e-05\n",
      "49 Train Loss 739164.06 Test MSE 3508.6006405196354 Test RE 0.9971400036360226 Lambda1 -4.725463e-06\n",
      "50 Train Loss 733967.94 Test MSE 3508.602289116843 Test RE 0.9971402379006485 Lambda1 9.596165e-07\n",
      "51 Train Loss 728028.0 Test MSE 3508.3513411079334 Test RE 0.9971045777130307 Lambda1 2.163971e-05\n",
      "52 Train Loss 722377.0 Test MSE 3507.9988232547976 Test RE 0.9970544821038786 Lambda1 2.3845876e-05\n",
      "53 Train Loss 718456.2 Test MSE 3507.8847869710685 Test RE 0.9970382760961819 Lambda1 -1.9622394e-05\n",
      "54 Train Loss 714328.2 Test MSE 3507.851679436439 Test RE 0.9970335710446846 Lambda1 -3.8302354e-05\n",
      "55 Train Loss 709285.2 Test MSE 3507.6781425636054 Test RE 0.9970089086239513 Lambda1 1.226748e-05\n",
      "56 Train Loss 704010.8 Test MSE 3507.488360355265 Test RE 0.9969819367775979 Lambda1 1.20623e-05\n",
      "57 Train Loss 700976.9 Test MSE 3507.4670828310855 Test RE 0.996978912770494 Lambda1 1.6370728e-05\n",
      "58 Train Loss 696760.2 Test MSE 3507.5209023283146 Test RE 0.9969865616944518 Lambda1 1.2497753e-06\n",
      "59 Train Loss 691830.2 Test MSE 3507.443002710259 Test RE 0.9969754904413147 Lambda1 3.5737546e-06\n",
      "60 Train Loss 687319.6 Test MSE 3507.434924766853 Test RE 0.9969743423804309 Lambda1 4.8786787e-06\n",
      "61 Train Loss 683582.6 Test MSE 3507.500684402308 Test RE 0.9969836882932459 Lambda1 -1.2845971e-05\n",
      "62 Train Loss 677793.6 Test MSE 3507.5755263762235 Test RE 0.9969943249025979 Lambda1 -2.0525096e-05\n",
      "63 Train Loss 673522.25 Test MSE 3507.5252442764818 Test RE 0.9969871787773938 Lambda1 -8.672648e-06\n",
      "64 Train Loss 667801.94 Test MSE 3507.46746251589 Test RE 0.9969789667321881 Lambda1 -1.150938e-05\n",
      "65 Train Loss 661708.2 Test MSE 3507.4352373734573 Test RE 0.996974386809018 Lambda1 3.148462e-05\n",
      "66 Train Loss 657576.6 Test MSE 3507.4772290301507 Test RE 0.9969803547710826 Lambda1 4.1629315e-05\n",
      "67 Train Loss 652645.0 Test MSE 3507.423880680874 Test RE 0.996972772760572 Lambda1 2.4725514e-06\n",
      "68 Train Loss 648693.7 Test MSE 3507.2989323932625 Test RE 0.9969550145490715 Lambda1 -2.5787043e-05\n",
      "69 Train Loss 644942.3 Test MSE 3507.3415175933874 Test RE 0.996961066984401 Lambda1 -2.9127112e-05\n",
      "70 Train Loss 638054.56 Test MSE 3507.2103739347344 Test RE 0.9969424280318775 Lambda1 1.5280779e-05\n",
      "71 Train Loss 633824.3 Test MSE 3507.0908358452825 Test RE 0.9969254382314511 Lambda1 3.0257606e-05\n",
      "72 Train Loss 628843.56 Test MSE 3507.1328402396284 Test RE 0.9969314082969291 Lambda1 -2.3577615e-05\n",
      "73 Train Loss 621246.94 Test MSE 3506.9649886418797 Test RE 0.9969075514116068 Lambda1 -2.270645e-05\n",
      "74 Train Loss 617705.75 Test MSE 3506.988323278788 Test RE 0.9969108680168656 Lambda1 -1.5121443e-05\n",
      "Training time: 125.31\n",
      "Training time: 125.31\n",
      "inv_HT_atanh_tune12\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.01\n",
      "Training time: 4.01\n",
      "inv_HT_atanh_tune12\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.84\n",
      "Training time: 3.84\n",
      "inv_HT_atanh_tune12\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 433358.44 Test MSE 3517.1584328310364 Test RE 0.9983553200662119 Lambda1 3.524116e-05\n",
      "1 Train Loss 418789.8 Test MSE 3517.2179685906262 Test RE 0.9983637697269149 Lambda1 4.88294e-05\n",
      "2 Train Loss 407264.03 Test MSE 3517.5070259613194 Test RE 0.9984047934109562 Lambda1 3.593954e-05\n",
      "3 Train Loss 393902.0 Test MSE 3517.832475823692 Test RE 0.9984509799835312 Lambda1 -1.9535431e-05\n",
      "4 Train Loss 386739.4 Test MSE 3518.046368230259 Test RE 0.9984813335944921 Lambda1 -8.968362e-06\n",
      "5 Train Loss 380603.62 Test MSE 3518.0743902391546 Test RE 0.9984853101476935 Lambda1 4.5565675e-06\n",
      "6 Train Loss 373485.66 Test MSE 3518.1222941716255 Test RE 0.9984921080724589 Lambda1 2.6202302e-05\n",
      "7 Train Loss 369552.75 Test MSE 3518.1690171712935 Test RE 0.9984987383694808 Lambda1 2.5524228e-05\n",
      "8 Train Loss 362672.6 Test MSE 3518.05525610794 Test RE 0.9984825948590444 Lambda1 -1.9884772e-05\n",
      "9 Train Loss 358284.2 Test MSE 3518.037777277352 Test RE 0.998480114464547 Lambda1 1.0998929e-07\n",
      "10 Train Loss 351331.06 Test MSE 3517.9559278040992 Test RE 0.9984684992472085 Lambda1 3.97756e-05\n",
      "11 Train Loss 345499.56 Test MSE 3517.9545053255515 Test RE 0.9984682973828032 Lambda1 -4.2643933e-08\n",
      "12 Train Loss 340162.84 Test MSE 3517.5018763765656 Test RE 0.9984040625848417 Lambda1 -1.0510821e-05\n",
      "13 Train Loss 330726.34 Test MSE 3516.7448264505574 Test RE 0.99829661667274 Lambda1 -8.744414e-06\n",
      "14 Train Loss 323778.5 Test MSE 3516.80199620054 Test RE 0.9983047310141059 Lambda1 1.6344138e-06\n",
      "15 Train Loss 318926.7 Test MSE 3516.7061703100653 Test RE 0.9982911300079096 Lambda1 4.9996357e-05\n",
      "16 Train Loss 316090.56 Test MSE 3516.596861030869 Test RE 0.9982756150165318 Lambda1 6.35238e-05\n",
      "17 Train Loss 311505.28 Test MSE 3516.439562658344 Test RE 0.9982532881915094 Lambda1 5.765184e-05\n",
      "18 Train Loss 306309.72 Test MSE 3516.2933628784044 Test RE 0.9982325362454804 Lambda1 4.5924e-05\n",
      "19 Train Loss 302850.97 Test MSE 3516.0580348760127 Test RE 0.9981991323205314 Lambda1 5.7996378e-05\n",
      "20 Train Loss 298446.6 Test MSE 3515.879006396001 Test RE 0.9981737191523607 Lambda1 4.8561105e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Train Loss 294404.6 Test MSE 3515.5460432864047 Test RE 0.9981264531779145 Lambda1 2.07386e-05\n",
      "22 Train Loss 288936.4 Test MSE 3515.2972172080026 Test RE 0.9980911294639246 Lambda1 -2.2348166e-05\n",
      "23 Train Loss 284179.7 Test MSE 3514.863388905707 Test RE 0.9980295395735673 Lambda1 -3.8880808e-05\n",
      "24 Train Loss 277658.94 Test MSE 3514.7115431859343 Test RE 0.99800798138854 Lambda1 1.193398e-07\n",
      "25 Train Loss 273829.84 Test MSE 3514.686787572112 Test RE 0.9980044666842207 Lambda1 1.7501616e-05\n",
      "26 Train Loss 270550.03 Test MSE 3514.539285338466 Test RE 0.9979835246428045 Lambda1 3.464802e-05\n",
      "27 Train Loss 266416.12 Test MSE 3514.084657877139 Test RE 0.9979189748749774 Lambda1 2.5998983e-05\n",
      "28 Train Loss 263160.16 Test MSE 3513.797069307348 Test RE 0.9978781397798293 Lambda1 6.9325524e-06\n",
      "29 Train Loss 259120.64 Test MSE 3513.2730432342037 Test RE 0.9978037283026927 Lambda1 -3.2328535e-05\n",
      "30 Train Loss 256016.8 Test MSE 3513.250610758442 Test RE 0.997800542776916 Lambda1 -1.932447e-05\n",
      "31 Train Loss 252092.27 Test MSE 3513.0523148955235 Test RE 0.997772383312425 Lambda1 -1.9314748e-05\n",
      "32 Train Loss 248957.25 Test MSE 3512.859960510707 Test RE 0.9977450668219441 Lambda1 -2.4876628e-05\n",
      "33 Train Loss 246025.27 Test MSE 3512.3093785886203 Test RE 0.997666873849625 Lambda1 2.2486815e-06\n",
      "34 Train Loss 243234.39 Test MSE 3512.063271596003 Test RE 0.9976319200530268 Lambda1 2.0189449e-05\n",
      "35 Train Loss 240311.72 Test MSE 3511.9551096078762 Test RE 0.997616557761069 Lambda1 -2.7573262e-05\n",
      "36 Train Loss 237673.36 Test MSE 3511.8013739137455 Test RE 0.9975947222096928 Lambda1 -1.699729e-05\n",
      "37 Train Loss 234062.84 Test MSE 3511.6509379036693 Test RE 0.9975733548598728 Lambda1 4.945111e-06\n",
      "38 Train Loss 232140.86 Test MSE 3511.554591879194 Test RE 0.9975596700022648 Lambda1 -1.1219164e-05\n",
      "39 Train Loss 230628.31 Test MSE 3511.41714744074 Test RE 0.9975401472571757 Lambda1 -6.88775e-06\n",
      "40 Train Loss 228819.53 Test MSE 3511.2743141597 Test RE 0.9975198586706625 Lambda1 1.1784178e-05\n",
      "41 Train Loss 226948.0 Test MSE 3511.2969847348213 Test RE 0.9975230789135663 Lambda1 1.4997589e-05\n",
      "42 Train Loss 224816.48 Test MSE 3511.5033408724144 Test RE 0.9975523903028299 Lambda1 2.8635945e-06\n",
      "43 Train Loss 223260.6 Test MSE 3511.476136044487 Test RE 0.997548526104073 Lambda1 -2.6477594e-06\n",
      "44 Train Loss 222101.17 Test MSE 3511.676357024964 Test RE 0.9975769653258145 Lambda1 -3.2938934e-07\n",
      "45 Train Loss 220774.62 Test MSE 3511.9142185187607 Test RE 0.9976107499211283 Lambda1 -8.54961e-06\n",
      "46 Train Loss 219683.23 Test MSE 3511.892483241974 Test RE 0.9976076628042492 Lambda1 -2.3389326e-05\n",
      "47 Train Loss 218358.6 Test MSE 3512.011899960049 Test RE 0.9976246237477983 Lambda1 -2.6754731e-05\n",
      "48 Train Loss 216393.52 Test MSE 3512.21659262253 Test RE 0.9976536958965868 Lambda1 -1.8241908e-05\n",
      "49 Train Loss 214343.84 Test MSE 3512.226993659866 Test RE 0.9976551731155145 Lambda1 -1.39308395e-05\n",
      "50 Train Loss 212087.42 Test MSE 3512.3542695720444 Test RE 0.9976732494417454 Lambda1 2.2129352e-06\n",
      "51 Train Loss 210062.05 Test MSE 3512.455685337731 Test RE 0.997687652753367 Lambda1 5.414129e-06\n",
      "52 Train Loss 208278.69 Test MSE 3512.4244257181713 Test RE 0.9976832132089787 Lambda1 2.1480482e-06\n",
      "53 Train Loss 205979.95 Test MSE 3512.5750232148184 Test RE 0.9977046011401735 Lambda1 -6.051861e-06\n",
      "54 Train Loss 203711.47 Test MSE 3512.618015025726 Test RE 0.9977107067743628 Lambda1 -8.499696e-06\n",
      "55 Train Loss 202073.45 Test MSE 3512.491751570683 Test RE 0.9976927749166271 Lambda1 1.072212e-06\n",
      "56 Train Loss 199791.44 Test MSE 3512.6756960703506 Test RE 0.9977188984933741 Lambda1 1.8178958e-05\n",
      "57 Train Loss 197846.61 Test MSE 3512.86998103169 Test RE 0.9977464898673205 Lambda1 9.8785085e-06\n",
      "58 Train Loss 195933.53 Test MSE 3512.7742881826825 Test RE 0.9977329001451072 Lambda1 2.3692992e-05\n",
      "59 Train Loss 194907.0 Test MSE 3512.7885851706183 Test RE 0.9977349305290079 Lambda1 3.4756693e-05\n",
      "60 Train Loss 194177.73 Test MSE 3512.772864749241 Test RE 0.9977326979962639 Lambda1 3.2113847e-05\n",
      "61 Train Loss 192750.84 Test MSE 3512.6313506145225 Test RE 0.9977126006676125 Lambda1 2.5631825e-05\n",
      "62 Train Loss 191468.98 Test MSE 3512.373603079579 Test RE 0.9976759952491528 Lambda1 8.035159e-06\n",
      "63 Train Loss 190168.73 Test MSE 3512.258148454545 Test RE 0.9976595978968864 Lambda1 2.3551476e-07\n",
      "64 Train Loss 188826.44 Test MSE 3511.98983588572 Test RE 0.9976214899744724 Lambda1 8.28191e-06\n",
      "65 Train Loss 187145.67 Test MSE 3511.7775993033774 Test RE 0.997591345386313 Lambda1 -2.7855524e-06\n",
      "66 Train Loss 186221.75 Test MSE 3511.6748580783246 Test RE 0.9975767524196911 Lambda1 -1.6888014e-05\n",
      "67 Train Loss 185004.5 Test MSE 3511.4250452623487 Test RE 0.9975412690819788 Lambda1 -2.0799422e-05\n",
      "68 Train Loss 183344.45 Test MSE 3511.2207462394526 Test RE 0.9975122495716209 Lambda1 1.0069598e-06\n",
      "69 Train Loss 181164.31 Test MSE 3511.2144740096137 Test RE 0.9975113586238079 Lambda1 6.631426e-06\n",
      "70 Train Loss 179472.47 Test MSE 3511.262971417585 Test RE 0.9975182474864263 Lambda1 -7.652626e-06\n",
      "71 Train Loss 177868.92 Test MSE 3511.2899479115044 Test RE 0.9975220793687819 Lambda1 -2.6886748e-05\n",
      "72 Train Loss 176136.36 Test MSE 3511.224873153005 Test RE 0.9975128357844952 Lambda1 -7.154681e-06\n",
      "73 Train Loss 174292.8 Test MSE 3511.3412554829292 Test RE 0.9975293673240819 Lambda1 6.918843e-06\n",
      "74 Train Loss 172564.03 Test MSE 3511.558307914415 Test RE 0.9975601978263083 Lambda1 -1.3355377e-05\n",
      "Training time: 126.67\n",
      "Training time: 126.67\n",
      "inv_HT_atanh_tune12\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.92\n",
      "Training time: 3.92\n",
      "inv_HT_atanh_tune12\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.17\n",
      "Training time: 4.17\n",
      "inv_HT_atanh_tune12\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.97\n",
      "Training time: 3.97\n",
      "inv_HT_atanh_tune12\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.03\n",
      "Training time: 4.03\n",
      "inv_HT_atanh_tune12\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.08\n",
      "Training time: 4.08\n",
      "inv_HT_atanh_tune13\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.87\n",
      "Training time: 3.87\n",
      "inv_HT_atanh_tune13\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.87\n",
      "Training time: 2.87\n",
      "inv_HT_atanh_tune13\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.68\n",
      "Training time: 4.68\n",
      "inv_HT_atanh_tune13\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.44\n",
      "Training time: 4.44\n",
      "inv_HT_atanh_tune13\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.25\n",
      "Training time: 4.25\n",
      "inv_HT_atanh_tune13\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.08\n",
      "Training time: 4.08\n",
      "inv_HT_atanh_tune13\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.11\n",
      "Training time: 4.11\n",
      "inv_HT_atanh_tune13\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.04\n",
      "Training time: 4.04\n",
      "inv_HT_atanh_tune13\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.03\n",
      "Training time: 4.03\n",
      "inv_HT_atanh_tune13\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.73\n",
      "Training time: 3.73\n",
      "inv_HT_atanh_tune14\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.89\n",
      "Training time: 3.89\n",
      "inv_HT_atanh_tune14\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.06\n",
      "Training time: 4.06\n",
      "inv_HT_atanh_tune14\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.78\n",
      "Training time: 3.78\n",
      "inv_HT_atanh_tune14\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.24\n",
      "Training time: 4.24\n",
      "inv_HT_atanh_tune14\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.96\n",
      "Training time: 3.96\n",
      "inv_HT_atanh_tune14\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.93\n",
      "Training time: 3.93\n",
      "inv_HT_atanh_tune14\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.14\n",
      "Training time: 4.14\n",
      "inv_HT_atanh_tune14\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.87\n",
      "Training time: 3.87\n",
      "inv_HT_atanh_tune14\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.85\n",
      "Training time: 3.85\n",
      "inv_HT_atanh_tune14\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.15\n",
      "Training time: 4.15\n",
      "inv_HT_atanh_tune15\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 854.8518 Test MSE 858.0890035979916 Test RE 0.49312309054496145 Lambda1 -0.06584595\n",
      "1 Train Loss 854.7214 Test MSE 858.0788793712696 Test RE 0.4931201814617103 Lambda1 -0.06620763\n",
      "2 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "3 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "4 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "5 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "6 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "7 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "8 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "9 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "10 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "11 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "12 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "13 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "14 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "15 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "16 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "17 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "18 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "19 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "20 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "21 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "22 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "23 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "24 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "25 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "26 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "27 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "28 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "29 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "30 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "31 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "32 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "33 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "34 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "35 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "36 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "37 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "38 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "39 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "40 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "41 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "42 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "43 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "44 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "45 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "46 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "47 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "48 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "49 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "50 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "51 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "52 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "53 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "54 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "55 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "56 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "57 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "58 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "59 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "60 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "61 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "62 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "63 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "64 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "66 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "67 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "68 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "69 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "70 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "71 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "72 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "73 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "74 Train Loss 854.72125 Test MSE 858.0792977703661 Test RE 0.49312030168434856 Lambda1 -0.06620965\n",
      "Training time: 101.51\n",
      "Training time: 101.51\n",
      "inv_HT_atanh_tune15\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 854.77295 Test MSE 858.0540727497701 Test RE 0.49311305348408463 Lambda1 -0.07764565\n",
      "1 Train Loss 854.7203 Test MSE 858.0788326499211 Test RE 0.4931201680368139 Lambda1 -0.077924095\n",
      "2 Train Loss 854.6186 Test MSE 857.9210405022154 Test RE 0.49307482600942487 Lambda1 -0.078740284\n",
      "3 Train Loss 853.6881 Test MSE 856.8640622744963 Test RE 0.49277099270006386 Lambda1 -0.41862863\n",
      "4 Train Loss 839.35785 Test MSE 839.044252687563 Test RE 0.48762010547898205 Lambda1 -0.5065002\n",
      "5 Train Loss 824.48535 Test MSE 826.4555398230858 Test RE 0.4839482438169312 Lambda1 -0.56034386\n",
      "6 Train Loss 812.699 Test MSE 815.7106232212435 Test RE 0.4807919967171741 Lambda1 -0.7014938\n",
      "7 Train Loss 800.4635 Test MSE 802.4336053474703 Test RE 0.4768631077704553 Lambda1 -0.7703491\n",
      "8 Train Loss 777.1777 Test MSE 779.4096464826022 Test RE 0.46997208076164304 Lambda1 -0.6420452\n",
      "9 Train Loss 759.74554 Test MSE 761.787244466341 Test RE 0.4646286853515925 Lambda1 -0.6533396\n",
      "10 Train Loss 741.9272 Test MSE 744.6151252104626 Test RE 0.45936203328457575 Lambda1 -0.6323955\n",
      "11 Train Loss 730.1962 Test MSE 734.5521055170251 Test RE 0.45624747566667495 Lambda1 -0.6476268\n",
      "12 Train Loss 725.9852 Test MSE 731.2388624478813 Test RE 0.45521734639589645 Lambda1 -0.6945665\n",
      "13 Train Loss 721.1834 Test MSE 728.1356291939992 Test RE 0.45425039317748395 Lambda1 -0.7336137\n",
      "14 Train Loss 710.5335 Test MSE 716.3187452243063 Test RE 0.450549309849764 Lambda1 -0.690891\n",
      "15 Train Loss 703.2556 Test MSE 711.3062855636608 Test RE 0.4489701770451413 Lambda1 -0.6673958\n",
      "16 Train Loss 699.3228 Test MSE 705.37796328332 Test RE 0.44709531034363786 Lambda1 -0.6297232\n",
      "17 Train Loss 696.31525 Test MSE 701.6341897193279 Test RE 0.44590725901020073 Lambda1 -0.6021277\n",
      "18 Train Loss 689.12897 Test MSE 696.5339483058027 Test RE 0.4442836332691924 Lambda1 -0.5949877\n",
      "19 Train Loss 686.0031 Test MSE 693.1440836502154 Test RE 0.44320120342343633 Lambda1 -0.5943936\n",
      "20 Train Loss 682.71857 Test MSE 690.8514606776391 Test RE 0.44246763671136286 Lambda1 -0.5778879\n",
      "21 Train Loss 678.2913 Test MSE 688.7192994837068 Test RE 0.44178431952059133 Lambda1 -0.59496343\n",
      "22 Train Loss 675.30963 Test MSE 684.0553379743477 Test RE 0.44028591134610473 Lambda1 -0.59060174\n",
      "23 Train Loss 671.2194 Test MSE 679.4198671339499 Test RE 0.438791586452218 Lambda1 -0.5904563\n",
      "24 Train Loss 669.9194 Test MSE 679.026068540334 Test RE 0.4386644040104332 Lambda1 -0.5855763\n",
      "25 Train Loss 667.94336 Test MSE 679.506687187215 Test RE 0.43881962117257406 Lambda1 -0.5825071\n",
      "26 Train Loss 666.32385 Test MSE 677.6097786120766 Test RE 0.43820668942217555 Lambda1 -0.60002154\n",
      "27 Train Loss 664.4853 Test MSE 677.8347425260355 Test RE 0.4382794248772556 Lambda1 -0.61281145\n",
      "28 Train Loss 663.24084 Test MSE 678.2696683410832 Test RE 0.438420011112172 Lambda1 -0.61247575\n",
      "29 Train Loss 662.5053 Test MSE 678.3570578536527 Test RE 0.4384482536231158 Lambda1 -0.614235\n",
      "30 Train Loss 661.82007 Test MSE 677.7687577380908 Test RE 0.43825809189080617 Lambda1 -0.6097808\n",
      "31 Train Loss 661.4155 Test MSE 676.5030016564094 Test RE 0.4378486697433581 Lambda1 -0.6155851\n",
      "32 Train Loss 660.8009 Test MSE 676.028703135471 Test RE 0.4376951542464457 Lambda1 -0.6347608\n",
      "33 Train Loss 659.9113 Test MSE 675.1075911694645 Test RE 0.43739686538110745 Lambda1 -0.6679433\n",
      "34 Train Loss 659.2916 Test MSE 674.2851189749679 Test RE 0.43713034757002867 Lambda1 -0.6770944\n",
      "35 Train Loss 657.3231 Test MSE 669.5075022340559 Test RE 0.4355789609525976 Lambda1 -0.7338247\n",
      "36 Train Loss 654.1314 Test MSE 667.2258840066452 Test RE 0.43483612158588664 Lambda1 -0.7971383\n",
      "37 Train Loss 652.13513 Test MSE 666.0577402754798 Test RE 0.4344553108140104 Lambda1 -0.85171753\n",
      "38 Train Loss 649.4766 Test MSE 664.5006947712708 Test RE 0.4339471998388171 Lambda1 -0.9418528\n",
      "39 Train Loss 648.5096 Test MSE 661.6248293838757 Test RE 0.4330071504639789 Lambda1 -0.97657335\n",
      "40 Train Loss 647.44183 Test MSE 659.4245969363335 Test RE 0.4322865685540203 Lambda1 -1.0434785\n",
      "41 Train Loss 644.6235 Test MSE 654.8531751318912 Test RE 0.4307855621554783 Lambda1 -1.1597872\n",
      "42 Train Loss 639.98944 Test MSE 650.5881777949643 Test RE 0.42938043756694994 Lambda1 -1.2751557\n",
      "43 Train Loss 631.1238 Test MSE 643.3908031317502 Test RE 0.4269987414305452 Lambda1 -1.4091724\n",
      "44 Train Loss 626.2258 Test MSE 636.3651373742636 Test RE 0.42466098254457074 Lambda1 -1.4883456\n",
      "45 Train Loss 619.2891 Test MSE 627.3632680345402 Test RE 0.4216467083283982 Lambda1 -1.5405711\n",
      "46 Train Loss 618.2315 Test MSE 626.6014454504477 Test RE 0.4213906225987564 Lambda1 -1.5562055\n",
      "47 Train Loss 615.828 Test MSE 626.4419229308836 Test RE 0.42133697959072314 Lambda1 -1.5483143\n",
      "48 Train Loss 611.8548 Test MSE 622.0220764766359 Test RE 0.41984798190212763 Lambda1 -1.5894529\n",
      "49 Train Loss 608.5245 Test MSE 617.1488535201291 Test RE 0.41820010151270975 Lambda1 -1.6300114\n",
      "50 Train Loss 606.39355 Test MSE 615.9620645951695 Test RE 0.41779780465866717 Lambda1 -1.6518298\n",
      "51 Train Loss 602.52905 Test MSE 613.3590073372199 Test RE 0.4169140627015033 Lambda1 -1.6690149\n",
      "52 Train Loss 597.7856 Test MSE 608.1606877000435 Test RE 0.41514359544483354 Lambda1 -1.678488\n",
      "53 Train Loss 589.1251 Test MSE 603.3393888659974 Test RE 0.41349475982712486 Lambda1 -1.6335055\n",
      "54 Train Loss 586.70905 Test MSE 603.0156574696591 Test RE 0.4133838113298002 Lambda1 -1.6230776\n",
      "55 Train Loss 585.361 Test MSE 601.7035550100316 Test RE 0.4129338252609714 Lambda1 -1.6369339\n",
      "56 Train Loss 581.7381 Test MSE 600.240872478039 Test RE 0.41243161898705316 Lambda1 -1.6657407\n",
      "57 Train Loss 579.8972 Test MSE 597.6925482259024 Test RE 0.41155519800721246 Lambda1 -1.6944727\n",
      "58 Train Loss 578.4947 Test MSE 595.5380185507656 Test RE 0.41081275239505793 Lambda1 -1.7173369\n",
      "59 Train Loss 573.31537 Test MSE 586.7403930124609 Test RE 0.40776708291145036 Lambda1 -1.7807628\n",
      "60 Train Loss 565.6258 Test MSE 575.1511588536155 Test RE 0.4037199122810225 Lambda1 -1.8452313\n",
      "61 Train Loss 559.9549 Test MSE 567.7185553195974 Test RE 0.40110281977213713 Lambda1 -1.8765465\n",
      "62 Train Loss 554.27765 Test MSE 563.2638264512098 Test RE 0.3995260496877993 Lambda1 -1.8883955\n",
      "63 Train Loss 549.7574 Test MSE 558.076202666038 Test RE 0.39768198927547144 Lambda1 -1.9269216\n",
      "64 Train Loss 547.0363 Test MSE 557.105279931336 Test RE 0.3973359016192968 Lambda1 -1.9689578\n",
      "65 Train Loss 541.21326 Test MSE 551.2292825571739 Test RE 0.3952349219326479 Lambda1 -2.041008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 Train Loss 538.91974 Test MSE 548.2909463956775 Test RE 0.39418011147105836 Lambda1 -2.068701\n",
      "67 Train Loss 533.5072 Test MSE 542.3275880180013 Test RE 0.39203064701158996 Lambda1 -2.0845501\n",
      "68 Train Loss 531.2186 Test MSE 538.1483200704357 Test RE 0.39051719847638133 Lambda1 -2.0811992\n",
      "69 Train Loss 527.5056 Test MSE 533.1923469091084 Test RE 0.38871484285648855 Lambda1 -2.067282\n",
      "70 Train Loss 523.2393 Test MSE 530.6987891154232 Test RE 0.38780483463880366 Lambda1 -2.0486755\n",
      "71 Train Loss 518.39197 Test MSE 526.363735116542 Test RE 0.3862176799395396 Lambda1 -2.0291169\n",
      "72 Train Loss 515.72815 Test MSE 522.8160774931165 Test RE 0.384913938316273 Lambda1 -2.0133793\n",
      "73 Train Loss 511.78854 Test MSE 515.2371642992774 Test RE 0.38211383416805716 Lambda1 -2.010985\n",
      "74 Train Loss 506.9209 Test MSE 506.54299151072036 Test RE 0.37887620101544345 Lambda1 -1.991028\n",
      "Training time: 126.44\n",
      "Training time: 126.44\n",
      "inv_HT_atanh_tune15\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 854.7334 Test MSE 858.0632842555998 Test RE 0.49311570034616503 Lambda1 -0.20284237\n",
      "1 Train Loss 854.7189 Test MSE 858.0892891338998 Test RE 0.4931231725902605 Lambda1 -0.20315313\n",
      "2 Train Loss 854.6023 Test MSE 857.6866679930849 Test RE 0.4930074706979448 Lambda1 -0.2033415\n",
      "3 Train Loss 853.9321 Test MSE 856.5288001758847 Test RE 0.49267458090756777 Lambda1 -0.39497823\n",
      "4 Train Loss 850.19403 Test MSE 852.3784042297337 Test RE 0.4914794794312533 Lambda1 -0.5478946\n",
      "5 Train Loss 844.64795 Test MSE 847.524528918115 Test RE 0.4900781147439981 Lambda1 -0.64436436\n",
      "6 Train Loss 839.2781 Test MSE 842.4090655324138 Test RE 0.48859687679170044 Lambda1 -0.8368339\n",
      "7 Train Loss 835.40515 Test MSE 838.1407611301922 Test RE 0.4873574975706027 Lambda1 -0.8959316\n",
      "8 Train Loss 831.356 Test MSE 834.6844106601015 Test RE 0.4863515695631344 Lambda1 -0.91724586\n",
      "9 Train Loss 827.68896 Test MSE 829.7675837876415 Test RE 0.4849169923570111 Lambda1 -0.8629242\n",
      "10 Train Loss 824.3162 Test MSE 825.3238523418753 Test RE 0.4836167887302888 Lambda1 -0.7929487\n",
      "11 Train Loss 819.3058 Test MSE 820.9404440413618 Test RE 0.4823308013592953 Lambda1 -0.6157323\n",
      "12 Train Loss 816.4353 Test MSE 818.1385147139041 Test RE 0.48150698280166004 Lambda1 -0.5852797\n",
      "13 Train Loss 809.79803 Test MSE 812.5892815221254 Test RE 0.4798712323844723 Lambda1 -0.59439194\n",
      "14 Train Loss 806.91296 Test MSE 809.6585102931822 Test RE 0.47900507328226916 Lambda1 -0.5183393\n",
      "15 Train Loss 803.33936 Test MSE 805.7183165235176 Test RE 0.4778381154814467 Lambda1 -0.58190495\n",
      "16 Train Loss 801.4509 Test MSE 801.9158909076182 Test RE 0.47670925158329486 Lambda1 -0.5658782\n",
      "17 Train Loss 795.36475 Test MSE 798.8902199729806 Test RE 0.4758090771098819 Lambda1 -0.6088339\n",
      "18 Train Loss 793.83594 Test MSE 796.8625652608864 Test RE 0.47520487052212507 Lambda1 -0.63546264\n",
      "19 Train Loss 792.0364 Test MSE 795.4566189450136 Test RE 0.47478547104041285 Lambda1 -0.62518\n",
      "20 Train Loss 788.6564 Test MSE 791.933733657338 Test RE 0.47373294930336185 Lambda1 -0.5803841\n",
      "21 Train Loss 786.91815 Test MSE 789.724356976386 Test RE 0.47307166572043324 Lambda1 -0.5958398\n",
      "22 Train Loss 785.96735 Test MSE 787.6283641117815 Test RE 0.4724434632226757 Lambda1 -0.6050739\n",
      "23 Train Loss 783.73676 Test MSE 786.4217079620671 Test RE 0.4720814299689084 Lambda1 -0.5656931\n",
      "24 Train Loss 781.5161 Test MSE 783.5467352181998 Test RE 0.471217730554702 Lambda1 -0.5473882\n",
      "25 Train Loss 779.4738 Test MSE 780.2362467581984 Test RE 0.47022122837469704 Lambda1 -0.5376535\n",
      "26 Train Loss 777.6477 Test MSE 778.0469137224298 Test RE 0.46956104752612704 Lambda1 -0.51325774\n",
      "27 Train Loss 774.4123 Test MSE 776.1463787228523 Test RE 0.46898719855059734 Lambda1 -0.4985852\n",
      "28 Train Loss 771.64343 Test MSE 772.9255542128111 Test RE 0.4680130937640804 Lambda1 -0.5366477\n",
      "29 Train Loss 767.0266 Test MSE 771.5180880404914 Test RE 0.46758678314485475 Lambda1 -0.5495042\n",
      "30 Train Loss 763.6896 Test MSE 767.8528894465968 Test RE 0.4664747945199041 Lambda1 -0.5351541\n",
      "31 Train Loss 761.39386 Test MSE 764.8358075169208 Test RE 0.4655574456937829 Lambda1 -0.5284093\n",
      "32 Train Loss 757.22656 Test MSE 759.703497978946 Test RE 0.4639927916868243 Lambda1 -0.52510446\n",
      "33 Train Loss 754.4432 Test MSE 754.5017837648173 Test RE 0.46240157647054964 Lambda1 -0.4999458\n",
      "34 Train Loss 750.1177 Test MSE 749.5515677212057 Test RE 0.46088219336390746 Lambda1 -0.52414715\n",
      "35 Train Loss 745.19684 Test MSE 745.6583662716698 Test RE 0.45968371463511315 Lambda1 -0.49801323\n",
      "36 Train Loss 742.45386 Test MSE 741.1860622704063 Test RE 0.45830309763810423 Lambda1 -0.5394985\n",
      "37 Train Loss 739.1676 Test MSE 740.129562088759 Test RE 0.4579763443439103 Lambda1 -0.5282101\n",
      "38 Train Loss 737.7245 Test MSE 739.9935414713502 Test RE 0.4579342590856865 Lambda1 -0.4809849\n",
      "39 Train Loss 734.5462 Test MSE 735.740419477379 Test RE 0.4566163713156622 Lambda1 -0.4486935\n",
      "40 Train Loss 732.3268 Test MSE 735.0789144428185 Test RE 0.45641105300956686 Lambda1 -0.40568644\n",
      "41 Train Loss 728.747 Test MSE 731.2916866221248 Test RE 0.4552337883872896 Lambda1 -0.41096917\n",
      "42 Train Loss 725.26154 Test MSE 725.6998672992564 Test RE 0.45348997669289176 Lambda1 -0.3876941\n",
      "43 Train Loss 714.3587 Test MSE 706.0446342118142 Test RE 0.44730654110721974 Lambda1 -0.40027967\n",
      "44 Train Loss 694.6657 Test MSE 693.094722232885 Test RE 0.443185422123718 Lambda1 -0.4736379\n",
      "45 Train Loss 687.8361 Test MSE 689.9577069135887 Test RE 0.4421813341400402 Lambda1 -0.47455204\n",
      "46 Train Loss 679.5639 Test MSE 681.9149931065944 Test RE 0.4395965650785734 Lambda1 -0.51506305\n",
      "47 Train Loss 675.1957 Test MSE 679.9419800116632 Test RE 0.4389601528660825 Lambda1 -0.58810216\n",
      "48 Train Loss 672.22833 Test MSE 677.8398409864357 Test RE 0.4382810731743626 Lambda1 -0.59215474\n",
      "49 Train Loss 665.89355 Test MSE 670.5067891903132 Test RE 0.43590390583864597 Lambda1 -0.6262787\n",
      "50 Train Loss 664.5957 Test MSE 669.1181694719039 Test RE 0.4354522933492125 Lambda1 -0.6393912\n",
      "51 Train Loss 661.9839 Test MSE 665.1219773672707 Test RE 0.43415001441606244 Lambda1 -0.63825977\n",
      "52 Train Loss 659.1371 Test MSE 661.3367055740189 Test RE 0.43291285740705143 Lambda1 -0.6201096\n",
      "53 Train Loss 658.49603 Test MSE 659.8079997589733 Test RE 0.43241222037775506 Lambda1 -0.654218\n",
      "54 Train Loss 657.14795 Test MSE 658.5415372235242 Test RE 0.43199702617170155 Lambda1 -0.6765805\n",
      "55 Train Loss 653.5348 Test MSE 654.7256401804588 Test RE 0.43074361161370495 Lambda1 -0.76309735\n",
      "56 Train Loss 649.16907 Test MSE 653.6062959670443 Test RE 0.4303752461552695 Lambda1 -0.7785991\n",
      "57 Train Loss 647.7903 Test MSE 653.0595618919158 Test RE 0.43019520654977045 Lambda1 -0.8053871\n",
      "58 Train Loss 645.37286 Test MSE 653.555388158653 Test RE 0.4303584853870991 Lambda1 -0.7920759\n",
      "59 Train Loss 643.6911 Test MSE 653.0363302810601 Test RE 0.43018755470827175 Lambda1 -0.80054015\n",
      "60 Train Loss 642.59467 Test MSE 653.288024944882 Test RE 0.4302704486270086 Lambda1 -0.8235257\n",
      "61 Train Loss 639.4574 Test MSE 652.6160455350246 Test RE 0.43004910118093426 Lambda1 -0.81621385\n",
      "62 Train Loss 638.02673 Test MSE 651.059571557608 Test RE 0.4295359665297591 Lambda1 -0.8713864\n",
      "63 Train Loss 636.5358 Test MSE 648.4855893107458 Test RE 0.4286860344045171 Lambda1 -0.949347\n",
      "64 Train Loss 635.8228 Test MSE 647.7277380214854 Test RE 0.4284354696715632 Lambda1 -0.9699611\n",
      "65 Train Loss 635.02716 Test MSE 647.1849665898388 Test RE 0.42825592566296233 Lambda1 -1.005789\n",
      "66 Train Loss 633.9377 Test MSE 646.004516291097 Test RE 0.42786518299053583 Lambda1 -1.0420595\n",
      "67 Train Loss 633.42865 Test MSE 645.8502025964979 Test RE 0.42781407700062 Lambda1 -1.0328798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 Train Loss 632.55023 Test MSE 645.1674553838833 Test RE 0.42758788979639395 Lambda1 -1.0520009\n",
      "69 Train Loss 631.3639 Test MSE 643.2499883663166 Test RE 0.42695201165322827 Lambda1 -1.1072484\n",
      "70 Train Loss 630.22253 Test MSE 642.7094422385436 Test RE 0.42677258237541094 Lambda1 -1.149921\n",
      "71 Train Loss 629.29803 Test MSE 642.8088437133152 Test RE 0.4268055834340378 Lambda1 -1.1354326\n",
      "72 Train Loss 628.3969 Test MSE 642.8981831030242 Test RE 0.42683524172665493 Lambda1 -1.1583437\n",
      "73 Train Loss 626.66925 Test MSE 641.6685169764685 Test RE 0.426426844200688 Lambda1 -1.2198793\n",
      "74 Train Loss 626.34143 Test MSE 641.1180463299656 Test RE 0.42624389473428814 Lambda1 -1.2602581\n",
      "Training time: 125.86\n",
      "Training time: 125.86\n",
      "inv_HT_atanh_tune15\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 854.75916 Test MSE 858.0519793629529 Test RE 0.49311245196197323 Lambda1 -0.059687372\n",
      "1 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "2 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "3 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "4 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "5 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "6 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "7 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "8 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "9 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "10 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "11 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "12 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "13 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "14 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "15 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "16 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "17 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "18 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "19 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "20 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "21 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "22 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "23 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "24 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "25 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "26 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "27 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "28 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "29 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "30 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "31 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "32 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "33 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "34 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "35 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "36 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "37 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "38 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "39 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "40 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "41 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "42 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "43 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "44 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "45 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "46 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "47 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "48 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "49 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "50 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "51 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "52 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "53 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "54 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "55 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "56 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "57 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "58 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "59 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "60 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "61 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "62 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "63 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "64 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "65 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "66 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "67 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "69 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "70 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "71 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "72 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "73 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "74 Train Loss 854.7215 Test MSE 858.0791735002473 Test RE 0.49312026597662256 Lambda1 -0.059838835\n",
      "Training time: 102.26\n",
      "Training time: 102.26\n",
      "inv_HT_atanh_tune15\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 854.81396 Test MSE 858.0698544478589 Test RE 0.49311758823649604 Lambda1 -0.058054335\n",
      "1 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "2 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "3 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "4 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "5 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "6 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "7 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "8 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "9 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "10 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "11 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "12 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "13 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "14 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "15 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "16 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "17 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "18 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "19 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "20 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "21 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "22 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "23 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "24 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "25 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "26 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "27 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "28 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "29 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "30 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "31 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "32 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "33 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "34 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "35 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "36 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "37 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "38 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "39 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "40 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "41 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "42 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "43 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "44 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "45 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "46 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "47 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "48 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "49 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "50 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "51 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "52 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "53 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "54 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "55 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "56 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "57 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "58 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "59 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "60 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "61 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "62 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "63 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "64 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "65 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "66 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "67 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "68 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "70 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "71 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "72 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "73 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "74 Train Loss 854.7209 Test MSE 858.078121453226 Test RE 0.4931199636817517 Lambda1 -0.058320522\n",
      "Training time: 128.70\n",
      "Training time: 128.70\n",
      "inv_HT_atanh_tune15\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 855.7455 Test MSE 858.7664592219189 Test RE 0.49331771085781434 Lambda1 0.0044352184\n",
      "1 Train Loss 854.71826 Test MSE 858.0846049935493 Test RE 0.49312182665730686 Lambda1 0.004501994\n",
      "2 Train Loss 854.5108 Test MSE 857.6676873952106 Test RE 0.4930020155424254 Lambda1 -0.038158223\n",
      "3 Train Loss 853.7231 Test MSE 856.9251682135559 Test RE 0.4927885629884917 Lambda1 -1.283813\n",
      "4 Train Loss 849.46106 Test MSE 852.5353162130846 Test RE 0.49152471489867067 Lambda1 -1.0832974\n",
      "5 Train Loss 842.25543 Test MSE 843.6576514210574 Test RE 0.48895883234744936 Lambda1 -0.99393165\n",
      "6 Train Loss 835.0824 Test MSE 834.450864155383 Test RE 0.4862835236920371 Lambda1 -1.3037164\n",
      "7 Train Loss 826.36 Test MSE 821.8676651930463 Test RE 0.48260311167976816 Lambda1 -1.6211106\n",
      "8 Train Loss 816.5237 Test MSE 814.7454373032734 Test RE 0.4805074650431251 Lambda1 -1.8496921\n",
      "9 Train Loss 810.5029 Test MSE 811.6192410894014 Test RE 0.4795847201733133 Lambda1 -2.082067\n",
      "10 Train Loss 799.77405 Test MSE 800.7395555128343 Test RE 0.47635947938634327 Lambda1 -2.1610816\n",
      "11 Train Loss 791.8261 Test MSE 792.7105029897133 Test RE 0.47396522318033996 Lambda1 -2.1687422\n",
      "12 Train Loss 786.9117 Test MSE 789.2391207019125 Test RE 0.4729263068982627 Lambda1 -2.0874014\n",
      "13 Train Loss 780.1227 Test MSE 781.6641235781138 Test RE 0.470651297531619 Lambda1 -1.9904325\n",
      "14 Train Loss 774.1017 Test MSE 771.6103871190555 Test RE 0.4676147517299101 Lambda1 -1.7972298\n",
      "15 Train Loss 759.06213 Test MSE 756.9036588435418 Test RE 0.4631369944360829 Lambda1 -1.911978\n",
      "16 Train Loss 739.15015 Test MSE 733.6445821774213 Test RE 0.45596554658510235 Lambda1 -1.8548259\n",
      "17 Train Loss 728.261 Test MSE 722.9557449949367 Test RE 0.4526317633650495 Lambda1 -1.6335592\n",
      "18 Train Loss 721.08826 Test MSE 710.4560812422436 Test RE 0.4487017761295408 Lambda1 -1.6304399\n",
      "19 Train Loss 705.86163 Test MSE 697.7956067699347 Test RE 0.44468582515670874 Lambda1 -1.5462569\n",
      "20 Train Loss 697.48596 Test MSE 691.4099708155902 Test RE 0.44264645426423926 Lambda1 -1.5967484\n",
      "21 Train Loss 685.72516 Test MSE 680.592844041413 Test RE 0.43917019654907463 Lambda1 -1.6404243\n",
      "22 Train Loss 677.64545 Test MSE 674.5121077177523 Test RE 0.43720391831634425 Lambda1 -1.6856912\n",
      "23 Train Loss 673.80475 Test MSE 672.7015380097671 Test RE 0.4366167382761012 Lambda1 -1.6662748\n",
      "24 Train Loss 669.2559 Test MSE 665.3085565606842 Test RE 0.43421090375841054 Lambda1 -1.695331\n",
      "25 Train Loss 667.3507 Test MSE 661.7966382823827 Test RE 0.4330633678618545 Lambda1 -1.6846081\n",
      "26 Train Loss 664.21484 Test MSE 664.2176612057706 Test RE 0.43385477352124957 Lambda1 -1.7024873\n",
      "27 Train Loss 663.2618 Test MSE 662.747669968346 Test RE 0.43337442197046955 Lambda1 -1.714746\n",
      "28 Train Loss 661.12946 Test MSE 662.4909118212821 Test RE 0.43329046604135707 Lambda1 -1.7138739\n",
      "29 Train Loss 658.27704 Test MSE 659.1984629137002 Test RE 0.4322124409864161 Lambda1 -1.6879855\n",
      "30 Train Loss 655.92487 Test MSE 658.8157752523164 Test RE 0.43208696558712584 Lambda1 -1.6722045\n",
      "31 Train Loss 651.62415 Test MSE 654.253785398395 Test RE 0.43058836714676235 Lambda1 -1.6408128\n",
      "32 Train Loss 647.43243 Test MSE 649.6494110375841 Test RE 0.42907053830279573 Lambda1 -1.618808\n",
      "33 Train Loss 645.75916 Test MSE 645.8633468897447 Test RE 0.4278184303979322 Lambda1 -1.626118\n",
      "34 Train Loss 644.29376 Test MSE 645.3762479928964 Test RE 0.42765707336139236 Lambda1 -1.6290654\n",
      "35 Train Loss 643.6958 Test MSE 645.2960478152822 Test RE 0.427630500305006 Lambda1 -1.6395687\n",
      "36 Train Loss 639.6822 Test MSE 644.0539645437345 Test RE 0.4272187446982743 Lambda1 -1.6345695\n",
      "37 Train Loss 636.404 Test MSE 640.8693220235115 Test RE 0.42616120520296796 Lambda1 -1.6475648\n",
      "38 Train Loss 634.2834 Test MSE 639.2482530131066 Test RE 0.4256218798537691 Lambda1 -1.6627426\n",
      "39 Train Loss 632.56 Test MSE 638.1611157481622 Test RE 0.4252598090164377 Lambda1 -1.671796\n",
      "40 Train Loss 631.17566 Test MSE 638.1061985274285 Test RE 0.42524151066772714 Lambda1 -1.6784617\n",
      "41 Train Loss 630.5128 Test MSE 636.4419745864232 Test RE 0.42468661938354374 Lambda1 -1.6918515\n",
      "42 Train Loss 629.7998 Test MSE 635.1409522176222 Test RE 0.42425232256381074 Lambda1 -1.7064006\n",
      "43 Train Loss 628.8701 Test MSE 634.835391177352 Test RE 0.42415025815126034 Lambda1 -1.717441\n",
      "44 Train Loss 628.3968 Test MSE 633.9242732392106 Test RE 0.42384577790917616 Lambda1 -1.7089151\n",
      "45 Train Loss 626.8283 Test MSE 631.0480163710477 Test RE 0.4228831429795509 Lambda1 -1.656547\n",
      "46 Train Loss 625.3053 Test MSE 628.8645878430789 Test RE 0.422150920418922 Lambda1 -1.6285697\n",
      "47 Train Loss 624.9914 Test MSE 628.6706864279406 Test RE 0.4220858332974792 Lambda1 -1.6333134\n",
      "48 Train Loss 622.1685 Test MSE 628.1509034173149 Test RE 0.4219113075743593 Lambda1 -1.7081102\n",
      "49 Train Loss 619.18164 Test MSE 625.0841082394625 Test RE 0.42088010732358183 Lambda1 -1.7481743\n",
      "50 Train Loss 616.45905 Test MSE 623.5450513252912 Test RE 0.42036165097229344 Lambda1 -1.8305458\n",
      "51 Train Loss 615.2712 Test MSE 623.762693049968 Test RE 0.42043500593891286 Lambda1 -1.8522243\n",
      "52 Train Loss 613.6697 Test MSE 623.2639773400493 Test RE 0.42026689756025765 Lambda1 -1.8489103\n",
      "53 Train Loss 613.0179 Test MSE 622.9803128996189 Test RE 0.42017124921042165 Lambda1 -1.8298205\n",
      "54 Train Loss 611.10815 Test MSE 619.1689834581169 Test RE 0.4188839951371486 Lambda1 -1.8630279\n",
      "55 Train Loss 608.4586 Test MSE 616.5813775310503 Test RE 0.41800778721583703 Lambda1 -1.8491964\n",
      "56 Train Loss 605.7941 Test MSE 612.3588732557407 Test RE 0.41657401708943625 Lambda1 -1.8777971\n",
      "57 Train Loss 604.3936 Test MSE 610.4130090995475 Test RE 0.4159116265718392 Lambda1 -1.9090831\n",
      "58 Train Loss 602.4635 Test MSE 606.583623033641 Test RE 0.41460497686673997 Lambda1 -1.894942\n",
      "59 Train Loss 601.3651 Test MSE 604.3849093892023 Test RE 0.41385287512524505 Lambda1 -1.8999746\n",
      "60 Train Loss 600.62476 Test MSE 605.7236017992358 Test RE 0.41431095676212193 Lambda1 -1.8896695\n",
      "61 Train Loss 598.7838 Test MSE 604.7656391376978 Test RE 0.4139832070477493 Lambda1 -1.8952374\n",
      "62 Train Loss 597.1697 Test MSE 603.4037169869825 Test RE 0.4135168026710526 Lambda1 -1.8376431\n",
      "63 Train Loss 595.91077 Test MSE 603.1325212795034 Test RE 0.41342386606648934 Lambda1 -1.7856113\n",
      "64 Train Loss 595.6702 Test MSE 602.5185341357106 Test RE 0.4132133803407473 Lambda1 -1.781804\n",
      "65 Train Loss 595.181 Test MSE 602.0829704366256 Test RE 0.41306399631580454 Lambda1 -1.7757801\n",
      "66 Train Loss 594.5878 Test MSE 602.1373077884133 Test RE 0.4130826351898456 Lambda1 -1.7368512\n",
      "67 Train Loss 592.8019 Test MSE 601.3323422472023 Test RE 0.41280642867661405 Lambda1 -1.7491331\n",
      "68 Train Loss 591.95026 Test MSE 600.4066041716396 Test RE 0.41248855302493803 Lambda1 -1.7831522\n",
      "69 Train Loss 588.39 Test MSE 599.5218475298789 Test RE 0.4121845202815586 Lambda1 -1.8259165\n",
      "70 Train Loss 584.81134 Test MSE 596.660896750659 Test RE 0.411199860719079 Lambda1 -1.8558495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 Train Loss 582.2369 Test MSE 595.6804219377829 Test RE 0.41086186565803223 Lambda1 -1.8778727\n",
      "72 Train Loss 581.42804 Test MSE 595.7713055507838 Test RE 0.41089320728456075 Lambda1 -1.9171978\n",
      "73 Train Loss 581.1337 Test MSE 596.0277127138983 Test RE 0.4109816175750067 Lambda1 -1.9263382\n",
      "74 Train Loss 580.48175 Test MSE 596.0722109855247 Test RE 0.4109969588334238 Lambda1 -1.9586574\n",
      "Training time: 130.32\n",
      "Training time: 130.32\n",
      "inv_HT_atanh_tune15\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 855.0328 Test MSE 858.2037263329959 Test RE 0.4931560536400826 Lambda1 -0.0016572706\n",
      "1 Train Loss 854.7182 Test MSE 858.0786506591661 Test RE 0.49312011574364867 Lambda1 -0.0015569547\n",
      "2 Train Loss 854.4785 Test MSE 857.6654237935942 Test RE 0.493001364963587 Lambda1 -0.0027683985\n",
      "3 Train Loss 849.5769 Test MSE 844.3336939490482 Test RE 0.4891547001679548 Lambda1 0.008839099\n",
      "4 Train Loss 780.71014 Test MSE 788.1810598916367 Test RE 0.47260919602786644 Lambda1 -0.001013455\n",
      "5 Train Loss 765.07306 Test MSE 770.8241998522676 Test RE 0.46737646666926097 Lambda1 -0.0007656718\n",
      "6 Train Loss 716.07086 Test MSE 717.400153696479 Test RE 0.45088927309504273 Lambda1 -0.00067063817\n",
      "7 Train Loss 678.9737 Test MSE 693.0840018562315 Test RE 0.4431819946462953 Lambda1 -0.0010721446\n",
      "8 Train Loss 645.667 Test MSE 661.8229822577578 Test RE 0.4330719871993566 Lambda1 -0.00087987946\n",
      "9 Train Loss 633.13776 Test MSE 655.9222341433363 Test RE 0.43113705116101236 Lambda1 -2.9521407e-06\n",
      "10 Train Loss 632.14996 Test MSE 653.833533726808 Test RE 0.43045005342733644 Lambda1 0.00019290259\n",
      "11 Train Loss 630.9259 Test MSE 652.9250042892699 Test RE 0.43015088515775285 Lambda1 0.00036520508\n",
      "12 Train Loss 625.9173 Test MSE 648.4687672653707 Test RE 0.4286804742017858 Lambda1 -0.00017429037\n",
      "13 Train Loss 624.7953 Test MSE 647.9410833706592 Test RE 0.42850602183734365 Lambda1 -0.00012769227\n",
      "14 Train Loss 624.24664 Test MSE 647.9144701090219 Test RE 0.42849722160822745 Lambda1 -8.888695e-05\n",
      "15 Train Loss 624.08527 Test MSE 647.634066057802 Test RE 0.42840448918357893 Lambda1 -8.3187115e-05\n",
      "16 Train Loss 623.99963 Test MSE 647.5667226183261 Test RE 0.4283822150450424 Lambda1 -9.87644e-05\n",
      "17 Train Loss 623.872 Test MSE 647.4059946490971 Test RE 0.428329048881632 Lambda1 -5.347712e-05\n",
      "18 Train Loss 623.2083 Test MSE 646.320903804501 Test RE 0.4279699459040254 Lambda1 7.975965e-06\n",
      "19 Train Loss 622.2604 Test MSE 644.4310958160521 Test RE 0.4273438071764051 Lambda1 1.1638618e-05\n",
      "20 Train Loss 619.93854 Test MSE 640.2473830568802 Test RE 0.42595436850008106 Lambda1 -6.26709e-05\n",
      "21 Train Loss 616.25214 Test MSE 638.4724526287763 Test RE 0.4253635311916239 Lambda1 6.719264e-05\n",
      "22 Train Loss 607.90546 Test MSE 624.9414882806204 Test RE 0.42083209032308605 Lambda1 0.0013117996\n",
      "23 Train Loss 516.0057 Test MSE 517.8738282606502 Test RE 0.3830902972278259 Lambda1 5.0962786e-05\n",
      "24 Train Loss 385.072 Test MSE 392.67017888837916 Test RE 0.33358240140412926 Lambda1 -5.553216e-05\n",
      "25 Train Loss 365.43793 Test MSE 373.3138119895386 Test RE 0.3252566613720941 Lambda1 -7.818457e-05\n",
      "26 Train Loss 348.43057 Test MSE 357.61781682710733 Test RE 0.31834552139614736 Lambda1 -7.216645e-05\n",
      "27 Train Loss 323.52847 Test MSE 333.3883193547682 Test RE 0.30737204071183827 Lambda1 3.848791e-05\n",
      "28 Train Loss 313.6298 Test MSE 322.4469147452565 Test RE 0.3022861738171509 Lambda1 3.3438173e-06\n",
      "29 Train Loss 303.28528 Test MSE 306.2709991105681 Test RE 0.2946063532576222 Lambda1 1.9020295e-05\n",
      "30 Train Loss 289.14035 Test MSE 297.8806298119467 Test RE 0.2905429236084424 Lambda1 7.572839e-05\n",
      "31 Train Loss 284.6789 Test MSE 292.0070531931103 Test RE 0.2876642161731588 Lambda1 0.00013500279\n",
      "32 Train Loss 283.26843 Test MSE 290.2730626659225 Test RE 0.28680884345405083 Lambda1 0.0002403094\n",
      "33 Train Loss 279.64752 Test MSE 287.43385528423465 Test RE 0.28540273501290503 Lambda1 -6.9370726e-05\n",
      "34 Train Loss 278.44678 Test MSE 287.30117212069433 Test RE 0.2853368546230656 Lambda1 0.00020241756\n",
      "35 Train Loss 277.44205 Test MSE 286.3921571453621 Test RE 0.2848850970443847 Lambda1 9.32795e-05\n",
      "36 Train Loss 276.27896 Test MSE 284.48593741158385 Test RE 0.28393541976053466 Lambda1 9.274509e-05\n",
      "37 Train Loss 274.4498 Test MSE 282.3355172906777 Test RE 0.2828602547340996 Lambda1 -3.305212e-06\n",
      "38 Train Loss 273.68796 Test MSE 281.3591592827364 Test RE 0.28237074485434543 Lambda1 3.8228027e-05\n",
      "39 Train Loss 272.61563 Test MSE 280.70861407479777 Test RE 0.2820441138660528 Lambda1 0.0006398356\n",
      "40 Train Loss 272.07175 Test MSE 280.3565371393345 Test RE 0.28186718238120545 Lambda1 0.0004938565\n",
      "41 Train Loss 271.73825 Test MSE 280.24626235336694 Test RE 0.2818117424391943 Lambda1 0.00025575\n",
      "42 Train Loss 271.54468 Test MSE 279.8862924963042 Test RE 0.2816306939423049 Lambda1 0.0002396378\n",
      "43 Train Loss 271.38635 Test MSE 279.76162694911716 Test RE 0.2815679656916407 Lambda1 0.00019246082\n",
      "44 Train Loss 271.13104 Test MSE 279.5602544248404 Test RE 0.2814666110857953 Lambda1 0.00019262476\n",
      "45 Train Loss 270.99368 Test MSE 279.532816187303 Test RE 0.28145279807568635 Lambda1 0.00019985043\n",
      "46 Train Loss 270.9233 Test MSE 279.48704116884636 Test RE 0.28142975241922 Lambda1 0.00025436011\n",
      "47 Train Loss 270.89124 Test MSE 279.4004200710181 Test RE 0.281386137439525 Lambda1 0.00028870685\n",
      "48 Train Loss 270.81754 Test MSE 279.26903563023643 Test RE 0.28131997056070984 Lambda1 0.00039017445\n",
      "49 Train Loss 270.74057 Test MSE 279.15310960348944 Test RE 0.2812615758098469 Lambda1 0.00037917178\n",
      "50 Train Loss 270.61572 Test MSE 279.1604372939519 Test RE 0.2812652673041749 Lambda1 0.00043198175\n",
      "51 Train Loss 270.53397 Test MSE 279.04440163338876 Test RE 0.28120680595206704 Lambda1 0.0003718518\n",
      "52 Train Loss 270.4972 Test MSE 279.08000159601517 Test RE 0.2812247432990437 Lambda1 0.00028793036\n",
      "53 Train Loss 270.4098 Test MSE 279.2533367290386 Test RE 0.2813120633530938 Lambda1 0.00023675137\n",
      "54 Train Loss 270.388 Test MSE 279.2099573215138 Test RE 0.2812902128987195 Lambda1 0.0002838817\n",
      "55 Train Loss 270.3714 Test MSE 279.1958974804939 Test RE 0.2812831305125808 Lambda1 0.0002795315\n",
      "56 Train Loss 270.3477 Test MSE 279.1999144864872 Test RE 0.2812851540236121 Lambda1 0.0002811851\n",
      "57 Train Loss 270.33072 Test MSE 279.16803430961676 Test RE 0.281269094426179 Lambda1 0.0002676199\n",
      "58 Train Loss 270.31512 Test MSE 279.1044239837806 Test RE 0.28123704806727895 Lambda1 0.0002917787\n",
      "59 Train Loss 270.30435 Test MSE 279.0886172985449 Test RE 0.2812290842228263 Lambda1 0.00030723674\n",
      "60 Train Loss 270.29074 Test MSE 279.04582115261246 Test RE 0.2812075212109235 Lambda1 0.00035041606\n",
      "61 Train Loss 270.28177 Test MSE 279.03305901548515 Test RE 0.2812010906362486 Lambda1 0.00034865725\n",
      "62 Train Loss 270.26822 Test MSE 279.06509205327535 Test RE 0.28121723113710456 Lambda1 0.00036384325\n",
      "63 Train Loss 270.25232 Test MSE 278.99734537395693 Test RE 0.2811830944956695 Lambda1 0.0004918443\n",
      "64 Train Loss 270.2191 Test MSE 278.921358097865 Test RE 0.28114480059644115 Lambda1 0.000610048\n",
      "65 Train Loss 270.18845 Test MSE 278.76469706238584 Test RE 0.2810658345713941 Lambda1 0.00074648525\n",
      "66 Train Loss 270.09875 Test MSE 278.45887400940626 Test RE 0.28091161848806157 Lambda1 0.001292738\n",
      "67 Train Loss 270.07745 Test MSE 278.50268370399766 Test RE 0.28093371541035617 Lambda1 0.0011774513\n",
      "68 Train Loss 270.04367 Test MSE 278.3357181112896 Test RE 0.2808494912729046 Lambda1 0.0014559869\n",
      "69 Train Loss 270.0253 Test MSE 278.39447245081413 Test RE 0.28087913219674476 Lambda1 0.0013369696\n",
      "70 Train Loss 270.01334 Test MSE 278.34797773746067 Test RE 0.2808556763789405 Lambda1 0.0014766557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 Train Loss 269.99872 Test MSE 278.26506771758085 Test RE 0.2808138447763547 Lambda1 0.001562497\n",
      "72 Train Loss 269.9639 Test MSE 278.047480503846 Test RE 0.28070403320047466 Lambda1 0.002038362\n",
      "73 Train Loss 269.94574 Test MSE 277.9183681623263 Test RE 0.2806388526722629 Lambda1 0.0023045053\n",
      "74 Train Loss 269.90222 Test MSE 277.87545184370646 Test RE 0.28061718362697113 Lambda1 0.0024577696\n",
      "Training time: 120.68\n",
      "Training time: 120.68\n",
      "inv_HT_atanh_tune15\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 854.875 Test MSE 858.1015664399578 Test RE 0.4931267003131533 Lambda1 -0.29279476\n",
      "1 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "2 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "3 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "4 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "5 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "6 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "7 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "8 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "9 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "10 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "11 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "12 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "13 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "14 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "15 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "16 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "17 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "18 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "19 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "20 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "21 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "22 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "23 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "24 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "25 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "26 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "27 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "28 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "29 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "30 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "31 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "32 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "33 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "34 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "35 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "36 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "37 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "38 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "39 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "40 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "41 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "42 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "43 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "44 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "45 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "46 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "47 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "48 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "49 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "50 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "51 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "52 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "53 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "54 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "55 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "56 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "57 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "58 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "59 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "60 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "61 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "62 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "63 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "64 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "65 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "66 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "67 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "68 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "69 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "70 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "71 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "72 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "74 Train Loss 854.7201 Test MSE 858.07682496328 Test RE 0.49311959114847015 Lambda1 -0.29470682\n",
      "Training time: 121.17\n",
      "Training time: 121.17\n",
      "inv_HT_atanh_tune15\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 854.77106 Test MSE 858.0542720538685 Test RE 0.49311311075288033 Lambda1 -0.0032098216\n",
      "1 Train Loss 854.7214 Test MSE 858.0796874971982 Test RE 0.4931204136682826 Lambda1 -0.0031734207\n",
      "2 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "3 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "4 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "5 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "6 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "7 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "8 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "9 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "10 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "11 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "12 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "13 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "14 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "15 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "16 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "17 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "18 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "19 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "20 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "21 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "22 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "23 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "24 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "25 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "26 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "27 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "28 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "29 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "30 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "31 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "32 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "33 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "34 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "35 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "36 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "37 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "38 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "39 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "40 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "41 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "42 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "43 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "44 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "45 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "46 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "47 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "48 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "49 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "50 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "51 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "52 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "53 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "54 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "55 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "56 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "57 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "58 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "59 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "60 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "61 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "62 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "63 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "64 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "65 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "66 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "67 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "68 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "69 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "70 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "71 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "72 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "73 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 Train Loss 854.6885 Test MSE 858.0203796578191 Test RE 0.4931033718876189 Lambda1 -0.002698225\n",
      "Training time: 146.30\n",
      "Training time: 146.30\n",
      "inv_HT_atanh_tune15\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 854.7698 Test MSE 858.0539878491055 Test RE 0.49311302908840365 Lambda1 -0.016520068\n",
      "1 Train Loss 854.72076 Test MSE 858.0837151740262 Test RE 0.49312157097765724 Lambda1 -0.016585687\n",
      "2 Train Loss 854.54846 Test MSE 857.6791558104986 Test RE 0.49300531165176614 Lambda1 -0.017188223\n",
      "3 Train Loss 853.16016 Test MSE 856.6030253435603 Test RE 0.4926959275693246 Lambda1 -0.39546782\n",
      "4 Train Loss 850.2163 Test MSE 852.3175527919514 Test RE 0.491461935716745 Lambda1 -0.43039\n",
      "5 Train Loss 837.3856 Test MSE 840.2950496556487 Test RE 0.48798342756306523 Lambda1 -0.8683203\n",
      "6 Train Loss 808.5198 Test MSE 810.669201783631 Test RE 0.47930394951006783 Lambda1 -0.748925\n",
      "7 Train Loss 773.68115 Test MSE 779.1384441625088 Test RE 0.46989030822477984 Lambda1 -0.84413636\n",
      "8 Train Loss 752.7888 Test MSE 755.7229972863721 Test RE 0.46277563969778934 Lambda1 -0.98407656\n",
      "9 Train Loss 733.0881 Test MSE 734.0247005746776 Test RE 0.4560836544893895 Lambda1 -1.1581941\n",
      "10 Train Loss 719.06555 Test MSE 725.0957712262376 Test RE 0.45330118752419385 Lambda1 -1.2953572\n",
      "11 Train Loss 707.6455 Test MSE 712.5232916762045 Test RE 0.4493540945963674 Lambda1 -1.3564018\n",
      "12 Train Loss 696.4215 Test MSE 701.5926758604782 Test RE 0.4458940672322724 Lambda1 -1.3221637\n",
      "13 Train Loss 683.277 Test MSE 684.1519845445517 Test RE 0.44031701308451204 Lambda1 -1.3198434\n",
      "14 Train Loss 669.24976 Test MSE 673.5508091734663 Test RE 0.436892260944503 Lambda1 -1.4379349\n",
      "15 Train Loss 657.48065 Test MSE 669.0946882207892 Test RE 0.4354446526552197 Lambda1 -1.4835331\n",
      "16 Train Loss 652.0751 Test MSE 663.534030381201 Test RE 0.43363144849582436 Lambda1 -1.5037677\n",
      "17 Train Loss 646.1372 Test MSE 659.3561912530087 Test RE 0.4322641462595096 Lambda1 -1.5579022\n",
      "18 Train Loss 643.8218 Test MSE 657.8740042075897 Test RE 0.43177802298366164 Lambda1 -1.5812068\n",
      "19 Train Loss 641.27045 Test MSE 656.3640366905663 Test RE 0.43128222491319373 Lambda1 -1.644047\n",
      "20 Train Loss 639.3953 Test MSE 654.0370690869328 Test RE 0.43051704677112695 Lambda1 -1.6769177\n",
      "21 Train Loss 638.5335 Test MSE 653.2394794486828 Test RE 0.4302544617424465 Lambda1 -1.6986905\n",
      "22 Train Loss 635.4464 Test MSE 649.5304652454654 Test RE 0.429031256753024 Lambda1 -1.712717\n",
      "23 Train Loss 633.12555 Test MSE 646.357172086495 Test RE 0.42798195349719403 Lambda1 -1.7452965\n",
      "24 Train Loss 631.5872 Test MSE 644.1321584193832 Test RE 0.427244677987529 Lambda1 -1.7600645\n",
      "25 Train Loss 629.9516 Test MSE 642.0690391821321 Test RE 0.4265599088377721 Lambda1 -1.7498611\n",
      "26 Train Loss 627.05286 Test MSE 640.5794984214195 Test RE 0.4260648317781958 Lambda1 -1.7420056\n",
      "27 Train Loss 623.1834 Test MSE 639.8738053964112 Test RE 0.4258300804048245 Lambda1 -1.6863939\n",
      "28 Train Loss 621.0496 Test MSE 638.32376935608 Test RE 0.4253140003747035 Lambda1 -1.6362329\n",
      "29 Train Loss 620.1548 Test MSE 636.7847845589049 Test RE 0.42480097954115 Lambda1 -1.6136159\n",
      "30 Train Loss 618.919 Test MSE 635.4600350181934 Test RE 0.42435887735373107 Lambda1 -1.604428\n",
      "31 Train Loss 617.69806 Test MSE 632.7712546434509 Test RE 0.42346014471400556 Lambda1 -1.6033635\n",
      "32 Train Loss 616.8832 Test MSE 632.0024969453849 Test RE 0.42320283436812917 Lambda1 -1.6090956\n",
      "33 Train Loss 615.61536 Test MSE 631.5829562252071 Test RE 0.4230623441815082 Lambda1 -1.6129569\n",
      "34 Train Loss 612.9137 Test MSE 629.7873214405466 Test RE 0.42246051811233765 Lambda1 -1.6160941\n",
      "35 Train Loss 611.8924 Test MSE 628.8460316412641 Test RE 0.4221446920705504 Lambda1 -1.6162132\n",
      "36 Train Loss 611.1918 Test MSE 627.9363946348109 Test RE 0.42183926166284336 Lambda1 -1.6177737\n",
      "37 Train Loss 610.4254 Test MSE 627.964033273644 Test RE 0.42184854519443 Lambda1 -1.6100729\n",
      "38 Train Loss 609.5091 Test MSE 627.6539946103574 Test RE 0.42174439471808023 Lambda1 -1.6143738\n",
      "39 Train Loss 608.9903 Test MSE 627.2548045385546 Test RE 0.421610257953281 Lambda1 -1.6194202\n",
      "40 Train Loss 608.66547 Test MSE 627.0666824904423 Test RE 0.42154702995375537 Lambda1 -1.6139487\n",
      "41 Train Loss 608.43036 Test MSE 626.8297501088197 Test RE 0.42146738325836064 Lambda1 -1.5971271\n",
      "42 Train Loss 608.3222 Test MSE 626.6897498240031 Test RE 0.42142031397940016 Lambda1 -1.5828677\n",
      "43 Train Loss 608.009 Test MSE 626.5459526250274 Test RE 0.421371962672115 Lambda1 -1.5628048\n",
      "44 Train Loss 607.8462 Test MSE 626.3493922798817 Test RE 0.4213058609639582 Lambda1 -1.558915\n",
      "45 Train Loss 607.5194 Test MSE 626.2083725644197 Test RE 0.4212584307454139 Lambda1 -1.5657868\n",
      "46 Train Loss 607.29865 Test MSE 626.2635093429201 Test RE 0.42127697594742247 Lambda1 -1.5794337\n",
      "47 Train Loss 607.14667 Test MSE 626.216323616853 Test RE 0.4212611051245707 Lambda1 -1.590254\n",
      "48 Train Loss 606.68427 Test MSE 625.9612806664729 Test RE 0.42117531159547333 Lambda1 -1.5735826\n",
      "49 Train Loss 606.5715 Test MSE 625.9982622145999 Test RE 0.4211877528482248 Lambda1 -1.5647082\n",
      "50 Train Loss 606.41504 Test MSE 625.8137923517185 Test RE 0.4211256902384159 Lambda1 -1.5550667\n",
      "51 Train Loss 606.3181 Test MSE 625.5270585257925 Test RE 0.4210292040181136 Lambda1 -1.5465788\n",
      "52 Train Loss 606.05206 Test MSE 625.1075636786018 Test RE 0.42088800372904644 Lambda1 -1.5463794\n",
      "53 Train Loss 605.8517 Test MSE 624.9388298575213 Test RE 0.42083119523853774 Lambda1 -1.5415819\n",
      "54 Train Loss 605.60376 Test MSE 624.7803940720693 Test RE 0.4207778468593134 Lambda1 -1.5381043\n",
      "55 Train Loss 605.1388 Test MSE 624.3553369928194 Test RE 0.42063468853144076 Lambda1 -1.5541577\n",
      "56 Train Loss 604.8258 Test MSE 623.8318270463027 Test RE 0.42045830450032434 Lambda1 -1.5567014\n",
      "57 Train Loss 604.5868 Test MSE 623.6374450834337 Test RE 0.42039279335246493 Lambda1 -1.5494395\n",
      "58 Train Loss 604.3264 Test MSE 623.2959843016125 Test RE 0.42027768856875014 Lambda1 -1.5273917\n",
      "59 Train Loss 604.1764 Test MSE 622.9260709308613 Test RE 0.4201529569695475 Lambda1 -1.5178392\n",
      "60 Train Loss 604.08624 Test MSE 622.6477547951378 Test RE 0.4200590867519474 Lambda1 -1.5112375\n",
      "61 Train Loss 603.8368 Test MSE 622.0592663096643 Test RE 0.4198605327773744 Lambda1 -1.5146664\n",
      "62 Train Loss 602.88556 Test MSE 619.9333516418973 Test RE 0.4191424729126059 Lambda1 -1.5601692\n",
      "63 Train Loss 599.96094 Test MSE 610.3639750199811 Test RE 0.41589492128127503 Lambda1 -1.6488335\n",
      "64 Train Loss 598.2752 Test MSE 607.0852581678679 Test RE 0.41477637734143136 Lambda1 -1.637265\n",
      "65 Train Loss 594.9618 Test MSE 601.0493049634764 Test RE 0.4127092666299995 Lambda1 -1.5910757\n",
      "66 Train Loss 590.1345 Test MSE 595.7496795335877 Test RE 0.4108857496711077 Lambda1 -1.5456529\n",
      "67 Train Loss 582.4518 Test MSE 578.6205870419396 Test RE 0.4049357416131424 Lambda1 -1.5753365\n",
      "68 Train Loss 571.3805 Test MSE 563.8680013922577 Test RE 0.3997402645209834 Lambda1 -1.6295183\n",
      "69 Train Loss 564.1372 Test MSE 561.9121265661446 Test RE 0.39904637775517904 Lambda1 -1.6691203\n",
      "70 Train Loss 553.1664 Test MSE 539.4027179225552 Test RE 0.3909720719625774 Lambda1 -1.7395744\n",
      "71 Train Loss 538.962 Test MSE 534.8239528482867 Test RE 0.38930913588623733 Lambda1 -1.7692696\n",
      "72 Train Loss 530.4347 Test MSE 516.643934185278 Test RE 0.38263512790609544 Lambda1 -1.8015417\n",
      "73 Train Loss 520.6226 Test MSE 505.18168528838737 Test RE 0.37836675410916554 Lambda1 -1.7989535\n",
      "74 Train Loss 509.47836 Test MSE 495.1415374194369 Test RE 0.3745879917964381 Lambda1 -1.7706753\n",
      "Training time: 145.20\n",
      "Training time: 145.20\n",
      "inv_HT_atanh_tune16\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 29288.129 Test MSE 3590.8829693000994 Test RE 1.0087645137470764 Lambda1 5.8563997e-05\n",
      "1 Train Loss 27543.795 Test MSE 3590.437378300011 Test RE 1.0087019232452845 Lambda1 0.00012507493\n",
      "2 Train Loss 26655.135 Test MSE 3589.8195445839465 Test RE 1.0086151320267787 Lambda1 0.0002094655\n",
      "3 Train Loss 24862.463 Test MSE 3588.8342883625032 Test RE 1.0084767110676935 Lambda1 0.0004791584\n",
      "4 Train Loss 23073.738 Test MSE 3587.921080474743 Test RE 1.0083483953798764 Lambda1 0.000638855\n",
      "5 Train Loss 21868.734 Test MSE 3588.4762843413732 Test RE 1.0084264095361841 Lambda1 0.00046944717\n",
      "6 Train Loss 21151.998 Test MSE 3589.2660257786506 Test RE 1.008537369209126 Lambda1 0.00039661926\n",
      "7 Train Loss 20522.355 Test MSE 3590.2428268693648 Test RE 1.0086745941151423 Lambda1 0.0003176548\n",
      "8 Train Loss 20021.746 Test MSE 3590.7425518663517 Test RE 1.0087447902542388 Lambda1 0.00019551847\n",
      "9 Train Loss 18667.023 Test MSE 3591.151125918705 Test RE 1.0088021788329777 Lambda1 -5.7724064e-05\n",
      "10 Train Loss 17221.732 Test MSE 3593.0492115507127 Test RE 1.00906874238144 Lambda1 -0.00019133929\n",
      "11 Train Loss 16213.684 Test MSE 3596.754253550113 Test RE 1.0095888686800396 Lambda1 -0.00041534906\n",
      "12 Train Loss 15568.094 Test MSE 3599.278433557217 Test RE 1.0099430681755937 Lambda1 -0.00051654456\n",
      "13 Train Loss 15060.087 Test MSE 3599.3430732336224 Test RE 1.0099521369516302 Lambda1 -0.0006034791\n",
      "14 Train Loss 14378.227 Test MSE 3599.719569560788 Test RE 1.0100049567745069 Lambda1 -0.0005803479\n",
      "15 Train Loss 13604.977 Test MSE 3600.8457317731477 Test RE 1.010162933034622 Lambda1 -0.000457086\n",
      "16 Train Loss 13077.555 Test MSE 3602.9970362743943 Test RE 1.0104646459844302 Lambda1 -0.00047283137\n",
      "17 Train Loss 12846.133 Test MSE 3603.3746033824773 Test RE 1.0105175891614329 Lambda1 -0.0006637188\n",
      "18 Train Loss 12608.783 Test MSE 3604.9426588839674 Test RE 1.010737435207539 Lambda1 -0.00055924815\n",
      "19 Train Loss 12386.619 Test MSE 3605.9146139712193 Test RE 1.0108736821990258 Lambda1 -0.0005483124\n",
      "20 Train Loss 11917.0625 Test MSE 3608.4189194253936 Test RE 1.0112246468448527 Lambda1 -0.00066081894\n",
      "21 Train Loss 11615.18 Test MSE 3609.479956535257 Test RE 1.0113733086325534 Lambda1 -0.00066859904\n",
      "22 Train Loss 11323.356 Test MSE 3609.873970285967 Test RE 1.011428508290272 Lambda1 -0.00036422358\n",
      "23 Train Loss 10920.148 Test MSE 3609.605588266108 Test RE 1.011390909433116 Lambda1 -0.0002156973\n",
      "24 Train Loss 10541.579 Test MSE 3607.56018610304 Test RE 1.0111043138139477 Lambda1 -9.8792974e-05\n",
      "25 Train Loss 10286.146 Test MSE 3606.857715582076 Test RE 1.0110058670081974 Lambda1 6.900645e-05\n",
      "26 Train Loss 9965.59 Test MSE 3606.886896346369 Test RE 1.0110099566988036 Lambda1 0.000112471855\n",
      "27 Train Loss 9494.305 Test MSE 3607.459251881805 Test RE 1.011090169110175 Lambda1 -0.00010947316\n",
      "28 Train Loss 9116.795 Test MSE 3607.311383051368 Test RE 1.0110694467343357 Lambda1 -0.00022093664\n",
      "29 Train Loss 8820.05 Test MSE 3607.8472515968465 Test RE 1.0111445414706315 Lambda1 -0.00042269717\n",
      "30 Train Loss 8573.644 Test MSE 3610.363195616927 Test RE 1.011497042494931 Lambda1 -0.00050231616\n",
      "31 Train Loss 8328.3125 Test MSE 3610.881419576246 Test RE 1.0115696339719038 Lambda1 -0.0005489429\n",
      "32 Train Loss 7953.875 Test MSE 3613.00039745951 Test RE 1.0118664007523277 Lambda1 -0.00076079817\n",
      "33 Train Loss 7660.166 Test MSE 3614.5092857678287 Test RE 1.0120776703121515 Lambda1 -0.0007771799\n",
      "34 Train Loss 7345.8154 Test MSE 3615.2895780146546 Test RE 1.0121869069582552 Lambda1 -0.00071291643\n",
      "35 Train Loss 6998.3174 Test MSE 3615.9989007117697 Test RE 1.0122861980272495 Lambda1 -0.0006592758\n",
      "36 Train Loss 6597.924 Test MSE 3616.4114731863997 Test RE 1.0123439454880554 Lambda1 -0.0003307354\n",
      "37 Train Loss 6394.4907 Test MSE 3616.058201885689 Test RE 1.012294498570947 Lambda1 -0.00034388856\n",
      "38 Train Loss 6215.258 Test MSE 3613.9185575102524 Test RE 1.0119949637452295 Lambda1 -0.00032589497\n",
      "39 Train Loss 5966.331 Test MSE 3615.438400960061 Test RE 1.012207740017868 Lambda1 -2.5569003e-05\n",
      "40 Train Loss 5764.533 Test MSE 3614.4338141482995 Test RE 1.0120671040729257 Lambda1 0.00039216247\n",
      "41 Train Loss 5693.7393 Test MSE 3615.090652396899 Test RE 1.012159059579612 Lambda1 0.00041169094\n",
      "42 Train Loss 5575.758 Test MSE 3614.7344584480634 Test RE 1.012109194465844 Lambda1 0.0004207714\n",
      "43 Train Loss 5428.0415 Test MSE 3616.0731178639144 Test RE 1.0122965863895443 Lambda1 0.00035387493\n",
      "44 Train Loss 5326.194 Test MSE 3615.868934495691 Test RE 1.0122680060708165 Lambda1 0.00024750776\n",
      "45 Train Loss 5274.9873 Test MSE 3616.2484970329597 Test RE 1.0123211342313327 Lambda1 0.00011201868\n",
      "46 Train Loss 5201.209 Test MSE 3615.549541612985 Test RE 1.012223297821943 Lambda1 0.000315298\n",
      "47 Train Loss 5137.425 Test MSE 3616.5871548938617 Test RE 1.0123685345253244 Lambda1 0.00030322387\n",
      "48 Train Loss 5053.4873 Test MSE 3617.9123630494496 Test RE 1.0125539961365888 Lambda1 0.00012281707\n",
      "49 Train Loss 4995.2847 Test MSE 3617.8927850654377 Test RE 1.0125512564637085 Lambda1 -0.00020690118\n",
      "50 Train Loss 4948.6187 Test MSE 3618.4506839932574 Test RE 1.0126293239352817 Lambda1 -0.00013667419\n",
      "51 Train Loss 4888.2534 Test MSE 3619.2269382955274 Test RE 1.012737936126641 Lambda1 -0.00010326392\n",
      "52 Train Loss 4828.744 Test MSE 3619.0805058747364 Test RE 1.0127174484409849 Lambda1 -0.0002244557\n",
      "53 Train Loss 4777.578 Test MSE 3618.8991785066514 Test RE 1.012692077951236 Lambda1 -0.00010030541\n",
      "54 Train Loss 4724.745 Test MSE 3618.782744605101 Test RE 1.0126757867211793 Lambda1 -0.0002494096\n",
      "55 Train Loss 4676.843 Test MSE 3617.8036328022504 Test RE 1.012538780721945 Lambda1 -0.00040468253\n",
      "56 Train Loss 4649.957 Test MSE 3617.74160824941 Test RE 1.0125301010725418 Lambda1 -0.00033851425\n",
      "57 Train Loss 4614.0684 Test MSE 3618.174704549078 Test RE 1.0125907065502056 Lambda1 -0.00032722473\n",
      "58 Train Loss 4590.4883 Test MSE 3618.189061637186 Test RE 1.0125927155520862 Lambda1 -0.00032851557\n",
      "59 Train Loss 4563.624 Test MSE 3618.4859246221736 Test RE 1.0126342549970073 Lambda1 -0.00030635242\n",
      "60 Train Loss 4538.137 Test MSE 3618.411067667637 Test RE 1.0126237805734484 Lambda1 -0.00022453503\n",
      "61 Train Loss 4515.034 Test MSE 3618.7545319010237 Test RE 1.012671839208991 Lambda1 -0.00027168205\n",
      "62 Train Loss 4478.86 Test MSE 3618.6201495151336 Test RE 1.0126530362589592 Lambda1 -0.00031134445\n",
      "63 Train Loss 4447.001 Test MSE 3619.1903373927585 Test RE 1.0127328152517863 Lambda1 -0.00019794877\n",
      "64 Train Loss 4414.3027 Test MSE 3619.4424009556906 Test RE 1.0127680812330355 Lambda1 -9.007455e-05\n",
      "65 Train Loss 4384.742 Test MSE 3619.7281809746064 Test RE 1.012808062967302 Lambda1 -9.24009e-05\n",
      "66 Train Loss 4332.4375 Test MSE 3619.3984724681027 Test RE 1.0127619353270993 Lambda1 3.9494767e-05\n",
      "67 Train Loss 4302.162 Test MSE 3619.1808374401726 Test RE 1.0127314860981436 Lambda1 0.00020531268\n",
      "68 Train Loss 4277.2607 Test MSE 3618.5427600096255 Test RE 1.0126422076649784 Lambda1 0.0001358899\n",
      "69 Train Loss 4260.5635 Test MSE 3617.8919342339464 Test RE 1.0125511374012326 Lambda1 3.8552636e-05\n",
      "70 Train Loss 4247.721 Test MSE 3618.1588505643613 Test RE 1.0125884880814358 Lambda1 -5.411554e-05\n",
      "71 Train Loss 4226.001 Test MSE 3618.5065864721955 Test RE 1.0126371461050108 Lambda1 -0.0002355518\n",
      "72 Train Loss 4210.1304 Test MSE 3618.7768060815365 Test RE 1.012674955806212 Lambda1 -0.00018201392\n",
      "73 Train Loss 4189.477 Test MSE 3618.401358729958 Test RE 1.01262242203408 Lambda1 -0.000345959\n",
      "74 Train Loss 4172.2476 Test MSE 3619.0298687104246 Test RE 1.012710363586264 Lambda1 -0.00036082382\n",
      "Training time: 118.73\n",
      "Training time: 118.73\n",
      "inv_HT_atanh_tune16\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 13771.822 Test MSE 3567.7070805577623 Test RE 1.0055039147328062 Lambda1 -0.00041221327\n",
      "1 Train Loss 4887.791 Test MSE 3552.959434691232 Test RE 1.0034235629342907 Lambda1 0.0014996649\n",
      "2 Train Loss 3936.0427 Test MSE 3568.451944685073 Test RE 1.0056088735608508 Lambda1 0.0004146195\n",
      "3 Train Loss 3638.068 Test MSE 3543.0077031200744 Test RE 1.0020172980407154 Lambda1 -0.0012664811\n",
      "4 Train Loss 3472.3176 Test MSE 3422.809925884731 Test RE 0.984873749364673 Lambda1 0.003543911\n",
      "5 Train Loss 3072.5554 Test MSE 3048.247998754835 Test RE 0.9294249340878418 Lambda1 -0.002366888\n",
      "6 Train Loss 854.7574 Test MSE 858.1773746078242 Test RE 0.493148482237073 Lambda1 -0.45125505\n",
      "7 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "8 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "9 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "10 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "11 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "12 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "13 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "14 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "15 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "16 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "17 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "18 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "19 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "20 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "21 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "22 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "23 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "24 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "25 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "26 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "27 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "28 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "29 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "30 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "31 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "32 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "33 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "34 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "35 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "36 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "37 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "38 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "39 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "40 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "41 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "42 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "43 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "44 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "45 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "46 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "47 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "48 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "49 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "50 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "51 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "52 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "53 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "54 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "55 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "56 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "57 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "58 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "59 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "60 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "61 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "62 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "63 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "64 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "65 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "66 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "67 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "68 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "69 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "70 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "71 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "72 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "73 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "74 Train Loss 854.7214 Test MSE 858.0791096089367 Test RE 0.49312024761811807 Lambda1 -0.4496512\n",
      "Training time: 112.70\n",
      "Training time: 112.70\n",
      "inv_HT_atanh_tune16\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 5163.49 Test MSE 3529.310415565685 Test RE 1.0000785201612927 Lambda1 0.0005347602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 4221.5586 Test MSE 3529.096721116527 Test RE 1.0000482430761983 Lambda1 -0.00043672387\n",
      "2 Train Loss 3667.1384 Test MSE 3524.9529129350044 Test RE 0.9994609504074745 Lambda1 3.2928423e-05\n",
      "3 Train Loss 3517.8562 Test MSE 3450.69172907774 Test RE 0.988876945812404 Lambda1 0.00040655077\n",
      "4 Train Loss 3303.8264 Test MSE 3250.5206203648336 Test RE 0.9597666021822372 Lambda1 0.0006189781\n",
      "5 Train Loss 3092.757 Test MSE 3033.51826387491 Test RE 0.9271766325437694 Lambda1 0.0046215546\n",
      "6 Train Loss 2250.9258 Test MSE 2195.2415315792136 Test RE 0.7887335704638443 Lambda1 0.027146319\n",
      "7 Train Loss 856.9557 Test MSE 858.5064656263776 Test RE 0.4932430286475371 Lambda1 0.11430838\n",
      "8 Train Loss 854.7049 Test MSE 858.013037164007 Test RE 0.4931012620215315 Lambda1 0.11675873\n",
      "9 Train Loss 854.41473 Test MSE 857.5217599795727 Test RE 0.492960072974514 Lambda1 0.1157643\n",
      "10 Train Loss 854.0285 Test MSE 856.7395326720791 Test RE 0.4927351837495991 Lambda1 0.11516707\n",
      "11 Train Loss 852.9218 Test MSE 854.4894395249173 Test RE 0.4920877121829273 Lambda1 0.113283835\n",
      "12 Train Loss 849.89404 Test MSE 849.5633046291815 Test RE 0.4906672181614457 Lambda1 0.1100044\n",
      "13 Train Loss 844.9985 Test MSE 844.2898103014039 Test RE 0.4891419882676805 Lambda1 0.10992839\n",
      "14 Train Loss 842.00024 Test MSE 839.1903511632714 Test RE 0.48766255702607797 Lambda1 0.1113122\n",
      "15 Train Loss 838.869 Test MSE 833.6854572222746 Test RE 0.48606044874081755 Lambda1 0.112491764\n",
      "16 Train Loss 833.21576 Test MSE 825.8339733155818 Test RE 0.48376622398369096 Lambda1 0.12061777\n",
      "17 Train Loss 828.1393 Test MSE 817.4511302220839 Test RE 0.481304663770823 Lambda1 0.11423129\n",
      "18 Train Loss 815.6667 Test MSE 794.0283483629808 Test RE 0.47435903245371075 Lambda1 0.10557025\n",
      "19 Train Loss 796.9101 Test MSE 756.1649336021604 Test RE 0.4629109323056563 Lambda1 0.071229674\n",
      "20 Train Loss 710.89594 Test MSE 676.872472839271 Test RE 0.437968218646767 Lambda1 0.006183194\n",
      "21 Train Loss 642.3015 Test MSE 623.8207572538952 Test RE 0.42045457400230146 Lambda1 0.0017408552\n",
      "22 Train Loss 607.747 Test MSE 593.8564882654155 Test RE 0.4102323676207433 Lambda1 0.00028264738\n",
      "23 Train Loss 576.0659 Test MSE 567.2016283510088 Test RE 0.4009201693355845 Lambda1 0.00011736417\n",
      "24 Train Loss 536.5572 Test MSE 509.01741430246045 Test RE 0.37980046391832034 Lambda1 -0.00018349578\n",
      "25 Train Loss 509.35162 Test MSE 491.236083932315 Test RE 0.37310777649812404 Lambda1 -0.00014655007\n",
      "26 Train Loss 490.41895 Test MSE 479.6358707876526 Test RE 0.3686761115924651 Lambda1 -0.00013963867\n",
      "27 Train Loss 479.98303 Test MSE 475.1848078794336 Test RE 0.366961450959889 Lambda1 -2.0259058e-06\n",
      "28 Train Loss 467.22763 Test MSE 468.95253026755347 Test RE 0.36454707030811623 Lambda1 -9.852012e-05\n",
      "29 Train Loss 463.02316 Test MSE 467.12050280596185 Test RE 0.3638342968930538 Lambda1 -5.2011852e-05\n",
      "30 Train Loss 453.04764 Test MSE 457.494598651574 Test RE 0.36006603482479443 Lambda1 -0.0001667171\n",
      "31 Train Loss 444.4534 Test MSE 450.59172891444007 Test RE 0.35733929676978626 Lambda1 -5.3177857e-05\n",
      "32 Train Loss 441.5322 Test MSE 447.2820604905417 Test RE 0.35602452082368574 Lambda1 -2.7034479e-05\n",
      "33 Train Loss 439.37744 Test MSE 444.8420906854958 Test RE 0.35505211761658845 Lambda1 3.3621043e-05\n",
      "34 Train Loss 435.68094 Test MSE 441.7781534163777 Test RE 0.353827259262001 Lambda1 -1.6446182e-05\n",
      "35 Train Loss 433.77408 Test MSE 439.9007600016921 Test RE 0.35307464146804063 Lambda1 -6.86212e-05\n",
      "36 Train Loss 432.27573 Test MSE 437.53486200951363 Test RE 0.3521238988714386 Lambda1 -0.00012446218\n",
      "37 Train Loss 430.07602 Test MSE 436.8012538153928 Test RE 0.35182857457661826 Lambda1 -6.5134926e-05\n",
      "38 Train Loss 427.53656 Test MSE 434.4312296631308 Test RE 0.350872789343069 Lambda1 -0.00013577298\n",
      "39 Train Loss 425.2297 Test MSE 432.87779803212436 Test RE 0.3502449051389488 Lambda1 -5.000206e-05\n",
      "40 Train Loss 423.59628 Test MSE 430.85939010437477 Test RE 0.34942739602456846 Lambda1 0.00010817271\n",
      "41 Train Loss 420.94476 Test MSE 427.32852068985085 Test RE 0.3479926813132635 Lambda1 0.0002879234\n",
      "42 Train Loss 418.24823 Test MSE 425.97237671155676 Test RE 0.3474400582464933 Lambda1 0.00024728075\n",
      "43 Train Loss 415.80396 Test MSE 423.59222550689253 Test RE 0.3464680253970848 Lambda1 -3.8284565e-05\n",
      "44 Train Loss 412.6177 Test MSE 419.11234354121126 Test RE 0.3446310446517012 Lambda1 1.0559927e-05\n",
      "45 Train Loss 409.8155 Test MSE 417.6238296446364 Test RE 0.344018506750878 Lambda1 -4.7315338e-05\n",
      "46 Train Loss 405.76767 Test MSE 413.02937256331484 Test RE 0.3421209264606334 Lambda1 -7.771634e-05\n",
      "47 Train Loss 401.43207 Test MSE 409.27688455310215 Test RE 0.340563247983727 Lambda1 1.5187879e-05\n",
      "48 Train Loss 397.82443 Test MSE 405.68413733115216 Test RE 0.3390651732779253 Lambda1 -3.5216945e-06\n",
      "49 Train Loss 385.67883 Test MSE 384.9298524845101 Test RE 0.33027824427342256 Lambda1 1.3265874e-05\n",
      "50 Train Loss 372.08444 Test MSE 372.5671933681162 Test RE 0.3249312458453103 Lambda1 -3.275562e-05\n",
      "51 Train Loss 361.28467 Test MSE 358.3593732006815 Test RE 0.3186754111369696 Lambda1 -5.987303e-05\n",
      "52 Train Loss 352.25534 Test MSE 353.1023227865441 Test RE 0.3163293273565699 Lambda1 -0.00026718157\n",
      "53 Train Loss 345.03107 Test MSE 347.9658382454022 Test RE 0.31402011990469897 Lambda1 -0.00020283883\n",
      "54 Train Loss 338.1429 Test MSE 343.0212740374529 Test RE 0.31178103789896366 Lambda1 -0.000104399514\n",
      "55 Train Loss 335.06958 Test MSE 339.5016956365491 Test RE 0.31017739515196313 Lambda1 -0.00012325104\n",
      "56 Train Loss 331.56772 Test MSE 337.08050532977427 Test RE 0.3090693855849058 Lambda1 -3.0810028e-05\n",
      "57 Train Loss 330.03214 Test MSE 335.1337988795178 Test RE 0.30817562495290957 Lambda1 -9.584099e-05\n",
      "58 Train Loss 327.48242 Test MSE 333.38960435982534 Test RE 0.30737263307549234 Lambda1 -0.00013114876\n",
      "59 Train Loss 323.05707 Test MSE 329.650929313361 Test RE 0.3056443153713147 Lambda1 -0.00016231812\n",
      "60 Train Loss 321.14047 Test MSE 327.2869226573101 Test RE 0.30454641880270994 Lambda1 -0.00025803372\n",
      "61 Train Loss 318.83282 Test MSE 325.2337894194228 Test RE 0.30358967712467944 Lambda1 -0.0002388858\n",
      "62 Train Loss 313.754 Test MSE 319.91982116738643 Test RE 0.30109929927293627 Lambda1 -0.00027568554\n",
      "63 Train Loss 311.52695 Test MSE 317.0204422937708 Test RE 0.2997317879965313 Lambda1 -0.00012376995\n",
      "64 Train Loss 307.8407 Test MSE 313.6015228567474 Test RE 0.29811117212100535 Lambda1 -0.00021443123\n",
      "65 Train Loss 305.10983 Test MSE 310.19523521700546 Test RE 0.29648773469608297 Lambda1 -0.00015109686\n",
      "66 Train Loss 301.8214 Test MSE 306.6468826324005 Test RE 0.2947870816288116 Lambda1 -0.00041673137\n",
      "67 Train Loss 300.0283 Test MSE 304.89220945297984 Test RE 0.29394246671543056 Lambda1 -0.00037415087\n",
      "68 Train Loss 296.63467 Test MSE 302.39457790571055 Test RE 0.29273602444256297 Lambda1 -0.00023600046\n",
      "69 Train Loss 294.7895 Test MSE 301.10499365236626 Test RE 0.292111160220242 Lambda1 -0.00025806425\n",
      "70 Train Loss 293.1223 Test MSE 299.2772261726506 Test RE 0.2912232241249542 Lambda1 -0.00020691068\n",
      "71 Train Loss 290.91953 Test MSE 296.56523268861935 Test RE 0.2899007164057859 Lambda1 -7.613217e-05\n",
      "72 Train Loss 287.96622 Test MSE 294.684595712233 Test RE 0.2889800672081788 Lambda1 2.9799974e-05\n",
      "73 Train Loss 286.8475 Test MSE 293.2679460714685 Test RE 0.28828461737436023 Lambda1 9.663936e-06\n",
      "74 Train Loss 285.1492 Test MSE 291.4652578045325 Test RE 0.28739722346636315 Lambda1 2.3408438e-05\n",
      "Training time: 119.83\n",
      "Training time: 119.83\n",
      "inv_HT_atanh_tune16\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 21698.969 Test MSE 3518.4320869834446 Test RE 0.9985360688621439 Lambda1 0.0014323563\n",
      "1 Train Loss 11922.806 Test MSE 3486.090632634753 Test RE 0.9939361984000057 Lambda1 -0.00026665407\n",
      "2 Train Loss 8183.6064 Test MSE 3470.483724370593 Test RE 0.9917088220303233 Lambda1 -0.0003994838\n",
      "3 Train Loss 5875.6313 Test MSE 3461.4298612816624 Test RE 0.9904143829439804 Lambda1 -0.0002922906\n",
      "4 Train Loss 4415.867 Test MSE 3446.5164362638734 Test RE 0.9882785005077733 Lambda1 -0.0007627518\n",
      "5 Train Loss 3750.7102 Test MSE 3437.247408096878 Test RE 0.9869486724504072 Lambda1 0.000925263\n",
      "6 Train Loss 3492.1123 Test MSE 3421.700316056985 Test RE 0.9847140977629594 Lambda1 6.966367e-05\n",
      "7 Train Loss 3409.9143 Test MSE 3379.1287187621697 Test RE 0.9785691907313153 Lambda1 0.00026699807\n",
      "8 Train Loss 1234.846 Test MSE 1230.904299546657 Test RE 0.5906107057487286 Lambda1 0.08933298\n",
      "9 Train Loss 854.7258 Test MSE 858.0815757279705 Test RE 0.49312095623131075 Lambda1 0.1342365\n",
      "10 Train Loss 854.5642 Test MSE 857.7709282339882 Test RE 0.4930316869470656 Lambda1 0.13014518\n",
      "11 Train Loss 854.4814 Test MSE 857.6286735019795 Test RE 0.49299080248927285 Lambda1 0.12098836\n",
      "12 Train Loss 854.32355 Test MSE 857.2755130139673 Test RE 0.4928892883929032 Lambda1 0.09603336\n",
      "13 Train Loss 854.0339 Test MSE 856.782349661164 Test RE 0.49274749622765773 Lambda1 0.018051755\n",
      "14 Train Loss 853.9141 Test MSE 856.4638433582616 Test RE 0.4926558990023807 Lambda1 0.02610102\n",
      "15 Train Loss 852.0233 Test MSE 852.847295854028 Test RE 0.49161464177604364 Lambda1 0.016796788\n",
      "16 Train Loss 846.835 Test MSE 844.8854748055909 Test RE 0.4893145079069744 Lambda1 0.003189356\n",
      "17 Train Loss 830.4183 Test MSE 819.4565511277543 Test RE 0.48189468508275707 Lambda1 0.013901972\n",
      "18 Train Loss 811.3421 Test MSE 792.8974494331217 Test RE 0.4740211079510204 Lambda1 0.00086325017\n",
      "19 Train Loss 759.7572 Test MSE 732.1632017733077 Test RE 0.4555049695159148 Lambda1 0.00043996103\n",
      "20 Train Loss 719.4948 Test MSE 700.6432812439577 Test RE 0.4455922733632839 Lambda1 0.00083900755\n",
      "21 Train Loss 695.1038 Test MSE 680.0414954031438 Test RE 0.43899227449888434 Lambda1 0.0003638458\n",
      "22 Train Loss 651.4454 Test MSE 634.7981985847903 Test RE 0.42413783329356075 Lambda1 -0.00014238496\n",
      "23 Train Loss 632.57446 Test MSE 617.8808586382579 Test RE 0.4184480432309463 Lambda1 -6.9779075e-05\n",
      "24 Train Loss 607.7525 Test MSE 601.7987241012057 Test RE 0.4129664800315282 Lambda1 -2.8937991e-05\n",
      "25 Train Loss 587.7594 Test MSE 583.1428249744166 Test RE 0.40651505950855604 Lambda1 -0.00013899143\n",
      "26 Train Loss 561.24115 Test MSE 539.7940861577576 Test RE 0.39111388280226805 Lambda1 -0.00020609552\n",
      "27 Train Loss 536.94257 Test MSE 526.691567207039 Test RE 0.38633793408362316 Lambda1 -3.9002014e-05\n",
      "28 Train Loss 521.4553 Test MSE 515.0285691326458 Test RE 0.38203647642537436 Lambda1 -4.768432e-05\n",
      "29 Train Loss 508.61166 Test MSE 501.9915303094575 Test RE 0.3771701942891818 Lambda1 -8.556519e-05\n",
      "30 Train Loss 501.43146 Test MSE 496.8501486511585 Test RE 0.3752337405470855 Lambda1 -8.627198e-05\n",
      "31 Train Loss 488.7066 Test MSE 492.38168866580884 Test RE 0.37354258283236136 Lambda1 -9.6210526e-05\n",
      "32 Train Loss 484.1737 Test MSE 489.3070056527161 Test RE 0.3723744609514126 Lambda1 -5.1505915e-05\n",
      "33 Train Loss 479.67633 Test MSE 485.3142227168235 Test RE 0.3708520466625679 Lambda1 -4.5109944e-05\n",
      "34 Train Loss 475.99106 Test MSE 481.267558374553 Test RE 0.3693026842906814 Lambda1 -5.098957e-05\n",
      "35 Train Loss 471.05072 Test MSE 476.9786768705001 Test RE 0.3676534561106006 Lambda1 -3.5252e-05\n",
      "36 Train Loss 466.9304 Test MSE 473.90366232123375 Test RE 0.3664664347756197 Lambda1 -2.7981641e-05\n",
      "37 Train Loss 465.8251 Test MSE 474.0219920958732 Test RE 0.36651218371947186 Lambda1 -1.4422353e-05\n",
      "38 Train Loss 464.5744 Test MSE 473.3184838943912 Test RE 0.36624010766509657 Lambda1 -3.2874097e-05\n",
      "39 Train Loss 462.88293 Test MSE 472.3414399098711 Test RE 0.36586190824922366 Lambda1 -2.6220367e-05\n",
      "40 Train Loss 459.93683 Test MSE 469.4989503733239 Test RE 0.36475939228555787 Lambda1 1.2350849e-06\n",
      "41 Train Loss 456.03137 Test MSE 460.6582288867146 Test RE 0.36130883998213636 Lambda1 1.0227924e-05\n",
      "42 Train Loss 437.9054 Test MSE 438.60344060129245 Test RE 0.35255362760072834 Lambda1 0.0001229958\n",
      "43 Train Loss 426.56726 Test MSE 428.97258497010824 Test RE 0.3486614561349993 Lambda1 0.00022258815\n",
      "44 Train Loss 420.39117 Test MSE 426.65107231411366 Test RE 0.3477167336845468 Lambda1 0.00016801187\n",
      "45 Train Loss 416.35043 Test MSE 421.0204765849825 Test RE 0.3454146711799075 Lambda1 0.00020528567\n",
      "46 Train Loss 411.74948 Test MSE 417.45524701554984 Test RE 0.34394906459484326 Lambda1 0.00013316341\n",
      "47 Train Loss 408.35684 Test MSE 415.3046389417178 Test RE 0.3430619577949362 Lambda1 1.2945177e-05\n",
      "48 Train Loss 405.08548 Test MSE 410.909611326157 Test RE 0.3412418757030886 Lambda1 -4.5000597e-05\n",
      "49 Train Loss 400.72495 Test MSE 405.6741290142031 Test RE 0.33906099084590735 Lambda1 1.663052e-05\n",
      "50 Train Loss 397.60077 Test MSE 402.0389433461425 Test RE 0.3375384347093897 Lambda1 1.785546e-05\n",
      "51 Train Loss 390.68567 Test MSE 394.92456905173725 Test RE 0.3345386092276366 Lambda1 2.9980058e-05\n",
      "52 Train Loss 388.0156 Test MSE 391.48362346476 Test RE 0.3330780169418419 Lambda1 1.2421017e-05\n",
      "53 Train Loss 384.81213 Test MSE 390.28274287159627 Test RE 0.332566764216216 Lambda1 3.115564e-05\n",
      "54 Train Loss 382.87485 Test MSE 388.7972930187754 Test RE 0.33193327193523203 Lambda1 -1.26935665e-05\n",
      "55 Train Loss 381.5432 Test MSE 386.6237696961698 Test RE 0.3310041555752933 Lambda1 1.5048161e-05\n",
      "56 Train Loss 380.3663 Test MSE 385.501518593508 Test RE 0.3305234043054771 Lambda1 -1.2056844e-05\n",
      "57 Train Loss 379.453 Test MSE 383.67967063439244 Test RE 0.32974146637520624 Lambda1 1.1975866e-05\n",
      "58 Train Loss 378.63177 Test MSE 384.1580439684183 Test RE 0.329946963824989 Lambda1 -2.6365738e-05\n",
      "59 Train Loss 377.66138 Test MSE 384.9621791385706 Test RE 0.33029211247086937 Lambda1 -1.0653488e-05\n",
      "60 Train Loss 376.20676 Test MSE 383.5455114825135 Test RE 0.3296838118959225 Lambda1 -4.845215e-07\n",
      "61 Train Loss 375.24872 Test MSE 382.50723710705904 Test RE 0.32923727528619307 Lambda1 -1.7443945e-05\n",
      "62 Train Loss 374.35376 Test MSE 382.62713116982934 Test RE 0.32928886973726307 Lambda1 1.7249637e-05\n",
      "63 Train Loss 373.3727 Test MSE 381.22437524995485 Test RE 0.32868470969824193 Lambda1 4.231036e-05\n",
      "64 Train Loss 371.37735 Test MSE 378.8486097837144 Test RE 0.32765893821834324 Lambda1 -4.296534e-05\n",
      "65 Train Loss 368.76398 Test MSE 376.6328121842065 Test RE 0.32669933248783783 Lambda1 -0.00012091051\n",
      "66 Train Loss 368.0642 Test MSE 376.06758387248664 Test RE 0.3264540948985957 Lambda1 -0.00019543948\n",
      "67 Train Loss 367.6628 Test MSE 375.82199160018774 Test RE 0.32634748148619397 Lambda1 -0.00016006682\n",
      "68 Train Loss 366.31863 Test MSE 375.21221866679133 Test RE 0.3260826238543484 Lambda1 -9.2935385e-05\n",
      "69 Train Loss 363.7914 Test MSE 373.5563992502965 Test RE 0.3253623235602087 Lambda1 3.2817683e-05\n",
      "70 Train Loss 361.89322 Test MSE 372.6551232931712 Test RE 0.3249695872438889 Lambda1 3.302599e-06\n",
      "71 Train Loss 360.37802 Test MSE 371.6446970504458 Test RE 0.3245287229432068 Lambda1 -9.336902e-06\n",
      "72 Train Loss 359.61655 Test MSE 370.98316985409826 Test RE 0.3242397638949119 Lambda1 8.500072e-06\n",
      "73 Train Loss 359.3 Test MSE 370.56734273735583 Test RE 0.3240579962304048 Lambda1 8.20336e-06\n",
      "74 Train Loss 358.79416 Test MSE 369.5723051730054 Test RE 0.32362262782144735 Lambda1 -2.9220038e-05\n",
      "Training time: 119.58\n",
      "Training time: 119.58\n",
      "inv_HT_atanh_tune16\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 9338.456 Test MSE 3586.61017387319 Test RE 1.0081641701278992 Lambda1 -5.6486126e-05\n",
      "1 Train Loss 6505.5312 Test MSE 3577.640439981944 Test RE 1.0069027248017974 Lambda1 -0.0017229102\n",
      "2 Train Loss 4774.4644 Test MSE 3579.4953355398707 Test RE 1.0071637148872348 Lambda1 -0.00057484687\n",
      "3 Train Loss 4240.087 Test MSE 3568.7633857118108 Test RE 1.0056527554782435 Lambda1 3.243806e-05\n",
      "4 Train Loss 3896.3323 Test MSE 3570.9339245724896 Test RE 1.0059585303857554 Lambda1 -0.00013737773\n",
      "5 Train Loss 3763.527 Test MSE 3562.7659083411045 Test RE 1.0048073764031225 Lambda1 -6.702912e-05\n",
      "6 Train Loss 3686.7195 Test MSE 3561.926391335426 Test RE 1.004688984878054 Lambda1 0.000118635406\n",
      "7 Train Loss 3588.8547 Test MSE 3529.059151305192 Test RE 1.0000429199401693 Lambda1 -0.00093558687\n",
      "8 Train Loss 3421.8206 Test MSE 3390.9546002580696 Test RE 0.9802800364778476 Lambda1 0.0019257551\n",
      "9 Train Loss 855.00665 Test MSE 858.2255129529441 Test RE 0.4931623133048906 Lambda1 0.30160058\n",
      "10 Train Loss 854.6845 Test MSE 858.0204154387814 Test RE 0.4931033821692586 Lambda1 0.29981047\n",
      "11 Train Loss 854.58044 Test MSE 857.7834687382929 Test RE 0.49303529096486987 Lambda1 0.29039606\n",
      "12 Train Loss 854.5072 Test MSE 857.6593993745378 Test RE 0.4929996334888468 Lambda1 0.25355953\n",
      "13 Train Loss 854.45795 Test MSE 857.5863664060946 Test RE 0.49297864264257985 Lambda1 0.23230396\n",
      "14 Train Loss 854.43304 Test MSE 857.4933310217311 Test RE 0.49295190148666346 Lambda1 0.22978447\n",
      "15 Train Loss 854.2878 Test MSE 857.3209647977665 Test RE 0.4929023544376574 Lambda1 0.25104877\n",
      "16 Train Loss 854.04944 Test MSE 856.7115444632068 Test RE 0.49272713527783635 Lambda1 0.24547364\n",
      "17 Train Loss 853.603 Test MSE 855.3991825517494 Test RE 0.4923495962110685 Lambda1 0.22560069\n",
      "18 Train Loss 852.8837 Test MSE 854.7409666948797 Test RE 0.4921601322250951 Lambda1 0.20181474\n",
      "19 Train Loss 852.0142 Test MSE 853.0755976005187 Test RE 0.4916804384149157 Lambda1 0.18773852\n",
      "20 Train Loss 851.0935 Test MSE 851.9804748014876 Test RE 0.4913647434299921 Lambda1 0.17692491\n",
      "21 Train Loss 850.02155 Test MSE 850.3644441242201 Test RE 0.490898513614721 Lambda1 0.145563\n",
      "22 Train Loss 847.89703 Test MSE 846.4703070231612 Test RE 0.48977321982672795 Lambda1 0.1230662\n",
      "23 Train Loss 846.1808 Test MSE 844.5172982176509 Test RE 0.48920788175343705 Lambda1 0.09908689\n",
      "24 Train Loss 843.45435 Test MSE 840.7631756740407 Test RE 0.48811933573748134 Lambda1 0.09646408\n",
      "25 Train Loss 841.5559 Test MSE 838.2648323136168 Test RE 0.48739356835172265 Lambda1 0.109179504\n",
      "26 Train Loss 838.83057 Test MSE 835.3444877784566 Test RE 0.48654383750006563 Lambda1 0.10551585\n",
      "27 Train Loss 835.8387 Test MSE 831.0579359541765 Test RE 0.48529388742659113 Lambda1 0.09841477\n",
      "28 Train Loss 834.6846 Test MSE 831.204225406206 Test RE 0.4853365981991277 Lambda1 0.096406244\n",
      "29 Train Loss 831.6868 Test MSE 826.0437903105051 Test RE 0.48382767454903713 Lambda1 0.09276355\n",
      "30 Train Loss 829.7749 Test MSE 824.2905256664161 Test RE 0.48331394358346097 Lambda1 0.09890921\n",
      "31 Train Loss 828.0485 Test MSE 822.2364970317083 Test RE 0.48271138910369016 Lambda1 0.108718164\n",
      "32 Train Loss 826.29736 Test MSE 819.3929128228281 Test RE 0.4818759729519644 Lambda1 0.10612185\n",
      "33 Train Loss 824.5275 Test MSE 818.3983694160729 Test RE 0.48158344413291915 Lambda1 0.105299965\n",
      "34 Train Loss 823.6303 Test MSE 817.3031060232508 Test RE 0.48126108443001603 Lambda1 0.10619886\n",
      "35 Train Loss 822.4384 Test MSE 815.4277968298749 Test RE 0.4807086384461522 Lambda1 0.110115595\n",
      "36 Train Loss 821.2699 Test MSE 815.2171424188492 Test RE 0.4806465422456411 Lambda1 0.10947788\n",
      "37 Train Loss 819.2275 Test MSE 812.2782482914512 Test RE 0.47977938390182384 Lambda1 0.11086666\n",
      "38 Train Loss 817.462 Test MSE 810.073091944349 Test RE 0.4791276936813902 Lambda1 0.110778555\n",
      "39 Train Loss 816.54376 Test MSE 809.4651918067427 Test RE 0.47894788493514573 Lambda1 0.10789034\n",
      "40 Train Loss 815.9161 Test MSE 808.4879614422177 Test RE 0.4786586916703731 Lambda1 0.10706233\n",
      "41 Train Loss 813.93884 Test MSE 807.1605983237093 Test RE 0.47826560301794585 Lambda1 0.10531702\n",
      "42 Train Loss 812.6288 Test MSE 805.0265524704021 Test RE 0.47763294314926874 Lambda1 0.10370358\n",
      "43 Train Loss 809.8916 Test MSE 802.2770745073229 Test RE 0.4768165946241054 Lambda1 0.09938625\n",
      "44 Train Loss 807.39044 Test MSE 800.4345971246158 Test RE 0.4762687609673301 Lambda1 0.10079479\n",
      "45 Train Loss 805.16296 Test MSE 795.740006335721 Test RE 0.4748700364530707 Lambda1 0.113873035\n",
      "46 Train Loss 802.4878 Test MSE 793.4793576759065 Test RE 0.4741950183347059 Lambda1 0.11382496\n",
      "47 Train Loss 801.23395 Test MSE 793.2066120729522 Test RE 0.4741135129237466 Lambda1 0.11782209\n",
      "48 Train Loss 799.5242 Test MSE 789.2025975486162 Test RE 0.47291536410577645 Lambda1 0.1312361\n",
      "49 Train Loss 796.954 Test MSE 787.5301332409966 Test RE 0.47241400137113465 Lambda1 0.13702767\n",
      "50 Train Loss 794.55896 Test MSE 784.5371793232329 Test RE 0.4715154584422629 Lambda1 0.12744792\n",
      "51 Train Loss 793.3171 Test MSE 783.3434847663644 Test RE 0.47115661012363935 Lambda1 0.13060382\n",
      "52 Train Loss 790.68475 Test MSE 780.7812284854263 Test RE 0.47038542046731535 Lambda1 0.13738818\n",
      "53 Train Loss 789.8042 Test MSE 780.1421025712147 Test RE 0.47019285880657785 Lambda1 0.14098418\n",
      "54 Train Loss 788.7328 Test MSE 779.7579318328605 Test RE 0.4700770743998842 Lambda1 0.13497251\n",
      "55 Train Loss 787.8101 Test MSE 779.5019169063555 Test RE 0.4699998987645947 Lambda1 0.13136417\n",
      "56 Train Loss 786.5655 Test MSE 779.1045833853582 Test RE 0.4698800975722292 Lambda1 0.12860271\n",
      "57 Train Loss 785.511 Test MSE 777.2281723922214 Test RE 0.4693139221777838 Lambda1 0.13364626\n",
      "58 Train Loss 783.49066 Test MSE 774.334010963559 Test RE 0.468439316041603 Lambda1 0.1260667\n",
      "59 Train Loss 782.18774 Test MSE 773.0627811830068 Test RE 0.46805463797759006 Lambda1 0.1173631\n",
      "60 Train Loss 779.2978 Test MSE 770.8711427059126 Test RE 0.4673906979628897 Lambda1 0.11720432\n",
      "61 Train Loss 778.2205 Test MSE 768.0887352054951 Test RE 0.4665464278085701 Lambda1 0.116663806\n",
      "62 Train Loss 776.6028 Test MSE 765.5283810134471 Test RE 0.465768183597003 Lambda1 0.1220492\n",
      "63 Train Loss 775.5572 Test MSE 765.5832446609293 Test RE 0.4657848735618946 Lambda1 0.12361405\n",
      "64 Train Loss 774.2219 Test MSE 764.3987767580879 Test RE 0.46542441583626615 Lambda1 0.12690876\n",
      "65 Train Loss 772.59827 Test MSE 762.4481791857414 Test RE 0.4648302000477345 Lambda1 0.12742446\n",
      "66 Train Loss 770.05145 Test MSE 759.2913659672923 Test RE 0.46386691874771574 Lambda1 0.13432738\n",
      "67 Train Loss 769.78156 Test MSE 758.8681007205919 Test RE 0.4637376099445492 Lambda1 0.13561098\n",
      "68 Train Loss 769.0593 Test MSE 758.1733326303737 Test RE 0.4635252780050884 Lambda1 0.13745621\n",
      "69 Train Loss 768.1689 Test MSE 757.3695934393761 Test RE 0.46327952139231177 Lambda1 0.138646\n",
      "70 Train Loss 767.13086 Test MSE 755.2913724732839 Test RE 0.4626434656217171 Lambda1 0.14274865\n",
      "71 Train Loss 765.65607 Test MSE 752.8974902395104 Test RE 0.4619097137425596 Lambda1 0.14549795\n",
      "72 Train Loss 764.8419 Test MSE 751.9746006756903 Test RE 0.46162652620690675 Lambda1 0.14561316\n",
      "73 Train Loss 764.72064 Test MSE 752.0857414656044 Test RE 0.4616606388226389 Lambda1 0.14465122\n",
      "74 Train Loss 764.218 Test MSE 751.9807484599235 Test RE 0.4616284132217584 Lambda1 0.14149599\n",
      "Training time: 120.90\n",
      "Training time: 120.90\n",
      "inv_HT_atanh_tune16\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 41253.414 Test MSE 3574.712064807629 Test RE 1.0064905547403329 Lambda1 -0.00014988217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 29220.418 Test MSE 3575.9870829258525 Test RE 1.0066700348298994 Lambda1 0.00043289526\n",
      "2 Train Loss 16938.832 Test MSE 3584.4711412411957 Test RE 1.0078634937817157 Lambda1 -5.5318866e-05\n",
      "3 Train Loss 12417.704 Test MSE 3574.582504153075 Test RE 1.0064723151235064 Lambda1 0.00012956081\n",
      "4 Train Loss 8981.587 Test MSE 3569.286834019493 Test RE 1.0057265048242776 Lambda1 -2.1445892e-05\n",
      "5 Train Loss 7210.994 Test MSE 3566.7528568185066 Test RE 1.005369439025029 Lambda1 9.445218e-05\n",
      "6 Train Loss 5711.338 Test MSE 3568.3685928171662 Test RE 1.0055971289908296 Lambda1 3.4505996e-05\n",
      "7 Train Loss 4891.8193 Test MSE 3569.0550072226733 Test RE 1.0056938430978704 Lambda1 1.4615777e-05\n",
      "8 Train Loss 4449.539 Test MSE 3568.133010235127 Test RE 1.0055639338384592 Lambda1 -5.500076e-05\n",
      "9 Train Loss 4100.6836 Test MSE 3575.619760342902 Test RE 1.006618331324689 Lambda1 0.0001319306\n",
      "10 Train Loss 3890.447 Test MSE 3572.5524352772845 Test RE 1.006186477785181 Lambda1 -0.00031572624\n",
      "11 Train Loss 3775.7944 Test MSE 3567.9725087622032 Test RE 1.0055413174860086 Lambda1 0.00023755067\n",
      "12 Train Loss 3694.3796 Test MSE 3561.6855376193425 Test RE 1.0046550162976107 Lambda1 5.4178745e-05\n",
      "13 Train Loss 3654.7139 Test MSE 3553.4825548590397 Test RE 1.003497429775774 Lambda1 -0.00014908882\n",
      "14 Train Loss 3616.8315 Test MSE 3542.2113630860604 Test RE 1.0019046830821137 Lambda1 7.111968e-05\n",
      "15 Train Loss 3588.7952 Test MSE 3527.284279280055 Test RE 0.9997914121938365 Lambda1 4.866234e-05\n",
      "16 Train Loss 3563.345 Test MSE 3506.2053786056927 Test RE 0.9967995802750461 Lambda1 2.4440891e-05\n",
      "17 Train Loss 3542.9348 Test MSE 3482.289183809324 Test RE 0.9933941258316422 Lambda1 0.00033502252\n",
      "18 Train Loss 3501.9592 Test MSE 3430.944585165478 Test RE 0.9860433816408102 Lambda1 1.9609517e-05\n",
      "19 Train Loss 3362.6648 Test MSE 3273.1010009864317 Test RE 0.9630944365283728 Lambda1 0.00032826027\n",
      "20 Train Loss 3132.5508 Test MSE 3029.0989463507 Test RE 0.9265010174647459 Lambda1 0.0019200257\n",
      "21 Train Loss 2624.6614 Test MSE 2587.5964504774674 Test RE 0.8563226748404453 Lambda1 -0.0073151165\n",
      "22 Train Loss 854.83044 Test MSE 858.140037255214 Test RE 0.4931377542339846 Lambda1 -0.2754124\n",
      "23 Train Loss 854.3348 Test MSE 857.1849273734284 Test RE 0.49286324666491704 Lambda1 -0.26053303\n",
      "24 Train Loss 852.30066 Test MSE 854.1757079148663 Test RE 0.4919973672140927 Lambda1 -0.251462\n",
      "25 Train Loss 846.13776 Test MSE 844.2496002113188 Test RE 0.4891303402071685 Lambda1 -0.22719502\n",
      "26 Train Loss 834.55817 Test MSE 828.0712018096647 Test RE 0.48442105510454847 Lambda1 -0.203422\n",
      "27 Train Loss 821.8097 Test MSE 804.9473403599743 Test RE 0.4776094437727196 Lambda1 -0.1884754\n",
      "28 Train Loss 807.2857 Test MSE 793.6752448621837 Test RE 0.474253547264797 Lambda1 -0.17508426\n",
      "29 Train Loss 795.74286 Test MSE 782.4497221646909 Test RE 0.47088774828367325 Lambda1 -0.16903831\n",
      "30 Train Loss 783.6039 Test MSE 770.1349949222911 Test RE 0.46716747595525276 Lambda1 -0.16498823\n",
      "31 Train Loss 772.2964 Test MSE 756.9399252639861 Test RE 0.4631480897185383 Lambda1 -0.1710585\n",
      "32 Train Loss 766.5279 Test MSE 751.3265099341537 Test RE 0.46142755647672473 Lambda1 -0.17565766\n",
      "33 Train Loss 761.5428 Test MSE 745.11447665588 Test RE 0.45951603542834996 Lambda1 -0.17991975\n",
      "34 Train Loss 754.0033 Test MSE 737.4056347094474 Test RE 0.4571328135050112 Lambda1 -0.18845122\n",
      "35 Train Loss 747.13293 Test MSE 730.845353631447 Test RE 0.4550948446017411 Lambda1 -0.1899987\n",
      "36 Train Loss 739.78033 Test MSE 715.2094734448062 Test RE 0.45020032046825365 Lambda1 -0.19424342\n",
      "37 Train Loss 732.1969 Test MSE 710.9352565912465 Test RE 0.4488530666840229 Lambda1 -0.19214778\n",
      "38 Train Loss 719.5929 Test MSE 693.6224438922727 Test RE 0.44335411049854867 Lambda1 -0.20226651\n",
      "39 Train Loss 710.5522 Test MSE 684.1566084458105 Test RE 0.4403185010425951 Lambda1 -0.20327534\n",
      "40 Train Loss 702.0084 Test MSE 663.076603374146 Test RE 0.4334819543298917 Lambda1 -0.19827475\n",
      "41 Train Loss 694.7711 Test MSE 664.375228988026 Test RE 0.43390623066001904 Lambda1 -0.19210866\n",
      "42 Train Loss 683.8683 Test MSE 655.0615482983935 Test RE 0.43085409432078964 Lambda1 -0.18577944\n",
      "43 Train Loss 660.93146 Test MSE 633.2918900020522 Test RE 0.42363431744369323 Lambda1 -0.17378093\n",
      "44 Train Loss 650.86163 Test MSE 626.1009532280185 Test RE 0.42122229801082656 Lambda1 -0.16734408\n",
      "45 Train Loss 638.1003 Test MSE 614.7127998620124 Test RE 0.41737391087922054 Lambda1 -0.16268782\n",
      "46 Train Loss 631.2475 Test MSE 605.6109926756121 Test RE 0.4142724430223484 Lambda1 -0.16064905\n",
      "47 Train Loss 623.36255 Test MSE 599.1717680666007 Test RE 0.41206415902424587 Lambda1 -0.1602966\n",
      "48 Train Loss 615.7191 Test MSE 585.9172623992359 Test RE 0.40748095690591574 Lambda1 -0.14668344\n",
      "49 Train Loss 601.32556 Test MSE 577.6959042693676 Test RE 0.40461205211129253 Lambda1 -0.13238253\n",
      "50 Train Loss 582.30975 Test MSE 556.2143697783882 Test RE 0.39701806921099975 Lambda1 -0.108593464\n",
      "51 Train Loss 565.71826 Test MSE 538.2673579999539 Test RE 0.39056038711658825 Lambda1 -0.073274516\n",
      "52 Train Loss 546.71515 Test MSE 510.8723348151774 Test RE 0.3804918538373454 Lambda1 -0.028343044\n",
      "53 Train Loss 504.25363 Test MSE 481.57735357644987 Test RE 0.3694215264941914 Lambda1 -0.005903325\n",
      "54 Train Loss 478.1682 Test MSE 469.76017797866723 Test RE 0.3648608536116063 Lambda1 -0.000698057\n",
      "55 Train Loss 470.2991 Test MSE 468.2104464836552 Test RE 0.36425852129938424 Lambda1 -0.0004009715\n",
      "56 Train Loss 464.4102 Test MSE 465.0301199017387 Test RE 0.36301929757802587 Lambda1 -7.833529e-06\n",
      "57 Train Loss 461.04413 Test MSE 465.4914371563445 Test RE 0.36319931339490913 Lambda1 -2.9889656e-05\n",
      "58 Train Loss 459.0052 Test MSE 463.61857144484213 Test RE 0.3624679260141599 Lambda1 -7.621759e-05\n",
      "59 Train Loss 456.85626 Test MSE 461.8443431446866 Test RE 0.3617736944357883 Lambda1 3.7782425e-05\n",
      "60 Train Loss 454.2782 Test MSE 459.1735905006504 Test RE 0.3607261456616549 Lambda1 0.00011321428\n",
      "61 Train Loss 450.9683 Test MSE 457.6047851318707 Test RE 0.3601093927361061 Lambda1 1.5373138e-05\n",
      "62 Train Loss 446.58298 Test MSE 452.3321003212588 Test RE 0.35802872768919003 Lambda1 3.9267787e-05\n",
      "63 Train Loss 443.11404 Test MSE 450.30185514008906 Test RE 0.3572243368752293 Lambda1 3.003453e-05\n",
      "64 Train Loss 437.97797 Test MSE 445.62408691285384 Test RE 0.3553640569633385 Lambda1 1.5120419e-05\n",
      "65 Train Loss 435.7352 Test MSE 440.8968647969538 Test RE 0.3534741639340219 Lambda1 4.587474e-06\n",
      "66 Train Loss 421.38544 Test MSE 422.9488301441321 Test RE 0.3462047998009805 Lambda1 7.9523496e-05\n",
      "67 Train Loss 403.2282 Test MSE 396.5161898126189 Test RE 0.3352120583348262 Lambda1 1.1492375e-07\n",
      "68 Train Loss 389.77277 Test MSE 388.24136306417694 Test RE 0.33169587617216406 Lambda1 -5.7421867e-06\n",
      "69 Train Loss 383.62396 Test MSE 381.6365488312158 Test RE 0.3288623459724353 Lambda1 -7.941619e-05\n",
      "70 Train Loss 380.54034 Test MSE 378.84897714437636 Test RE 0.32765909707992036 Lambda1 -4.3412078e-05\n",
      "71 Train Loss 374.83298 Test MSE 373.05968621412916 Test RE 0.3251459366011006 Lambda1 -9.581883e-05\n",
      "72 Train Loss 370.82892 Test MSE 372.70431574691696 Test RE 0.3249910353910685 Lambda1 -6.9970156e-07\n",
      "73 Train Loss 368.04755 Test MSE 368.6363134571473 Test RE 0.32321255903839946 Lambda1 1.3954759e-05\n",
      "74 Train Loss 364.405 Test MSE 364.89242881829705 Test RE 0.3215670908998791 Lambda1 -3.8768198e-05\n",
      "Training time: 120.39\n",
      "Training time: 120.39\n",
      "inv_HT_atanh_tune16\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 28899.414 Test MSE 3521.5798250352127 Test RE 0.9989826352903695 Lambda1 0.0003090647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 15951.035 Test MSE 3508.2850928158127 Test RE 0.997095163492396 Lambda1 -0.0006755199\n",
      "2 Train Loss 8965.072 Test MSE 3507.820653279592 Test RE 0.9970291617664159 Lambda1 0.00030502313\n",
      "3 Train Loss 5545.6187 Test MSE 3507.1937697476073 Test RE 0.9969400681166659 Lambda1 -0.00013082987\n",
      "4 Train Loss 4565.395 Test MSE 3509.7450221767826 Test RE 0.9973026063107783 Lambda1 7.90345e-05\n",
      "5 Train Loss 3959.6748 Test MSE 3501.4006881862924 Test RE 0.9961163694035895 Lambda1 -0.00048736093\n",
      "6 Train Loss 3723.6045 Test MSE 3483.8831666688197 Test RE 0.99362145790614 Lambda1 7.3930045e-05\n",
      "7 Train Loss 3583.8157 Test MSE 3461.5190492611887 Test RE 0.9904271424815487 Lambda1 0.00028456637\n",
      "8 Train Loss 3497.429 Test MSE 3428.266259659231 Test RE 0.9856584350640721 Lambda1 -1.0277294e-05\n",
      "9 Train Loss 3401.7014 Test MSE 3351.835428591672 Test RE 0.9746092168706046 Lambda1 -8.1002654e-05\n",
      "10 Train Loss 3189.8767 Test MSE 3124.681100876587 Test RE 0.9410051948277975 Lambda1 8.032622e-05\n",
      "11 Train Loss 2978.2405 Test MSE 2958.773011389886 Test RE 0.9156826700602552 Lambda1 -0.0018741781\n",
      "12 Train Loss 2226.2761 Test MSE 2195.5265823179393 Test RE 0.788784777081813 Lambda1 0.04044297\n",
      "13 Train Loss 854.7939 Test MSE 858.0649081657542 Test RE 0.49311616696397104 Lambda1 0.08843706\n",
      "14 Train Loss 854.72205 Test MSE 858.0783878618032 Test RE 0.4931200402315287 Lambda1 0.087622896\n",
      "15 Train Loss 854.72046 Test MSE 858.0768025527805 Test RE 0.4931195847090374 Lambda1 0.08666452\n",
      "16 Train Loss 854.71704 Test MSE 858.0731416025028 Test RE 0.49311853277044587 Lambda1 0.080760606\n",
      "17 Train Loss 854.69934 Test MSE 858.0259014364723 Test RE 0.4931049585651364 Lambda1 0.07674865\n",
      "18 Train Loss 854.64716 Test MSE 857.9023628624997 Test RE 0.4930694586599584 Lambda1 0.05889837\n",
      "19 Train Loss 854.5076 Test MSE 857.5936954142596 Test RE 0.49298074915739826 Lambda1 0.023029294\n",
      "20 Train Loss 854.2762 Test MSE 857.2622354778783 Test RE 0.4928854714282547 Lambda1 -0.0007281359\n",
      "21 Train Loss 852.8112 Test MSE 854.3652386716631 Test RE 0.4920519481746536 Lambda1 -0.0325926\n",
      "22 Train Loss 845.1911 Test MSE 842.256139439966 Test RE 0.48855252624661544 Lambda1 0.0018981218\n",
      "23 Train Loss 828.59625 Test MSE 830.9697288288319 Test RE 0.4852681325948187 Lambda1 0.00015343592\n",
      "24 Train Loss 819.6091 Test MSE 823.2713351529366 Test RE 0.4830150554305069 Lambda1 0.0012130586\n",
      "25 Train Loss 791.38324 Test MSE 793.985536987738 Test RE 0.4743462443483981 Lambda1 0.0006002393\n",
      "26 Train Loss 769.66376 Test MSE 774.3480250616041 Test RE 0.4684435549906275 Lambda1 0.00063674216\n",
      "27 Train Loss 758.8988 Test MSE 766.7661835853537 Test RE 0.4661445877950132 Lambda1 3.801455e-05\n",
      "28 Train Loss 753.57837 Test MSE 762.6579250723764 Test RE 0.4648941319455604 Lambda1 3.4778364e-05\n",
      "29 Train Loss 749.5914 Test MSE 759.510630790535 Test RE 0.46393389063805063 Lambda1 0.000120851866\n",
      "30 Train Loss 742.7923 Test MSE 753.791978917585 Test RE 0.4621840209149682 Lambda1 2.2324377e-05\n",
      "31 Train Loss 736.6426 Test MSE 747.9607045423078 Test RE 0.46039284079894216 Lambda1 -0.00019403119\n",
      "32 Train Loss 715.55334 Test MSE 726.9052861168959 Test RE 0.45386645363886363 Lambda1 9.3195136e-05\n",
      "33 Train Loss 702.9552 Test MSE 714.9598222064998 Test RE 0.45012174007931444 Lambda1 8.22969e-05\n",
      "34 Train Loss 699.0451 Test MSE 712.3727878941667 Test RE 0.4493066343496898 Lambda1 3.378892e-05\n",
      "35 Train Loss 697.07947 Test MSE 711.8607371177237 Test RE 0.4491451253948868 Lambda1 8.739352e-05\n",
      "36 Train Loss 695.05334 Test MSE 711.3593292183795 Test RE 0.44898691707370947 Lambda1 5.565332e-05\n",
      "37 Train Loss 691.5584 Test MSE 708.8190438965132 Test RE 0.4481845272762773 Lambda1 -0.0001825541\n",
      "38 Train Loss 689.17834 Test MSE 706.5969238300554 Test RE 0.44748145530856764 Lambda1 -0.0002878354\n",
      "39 Train Loss 687.8959 Test MSE 705.0441518804245 Test RE 0.44698950665974696 Lambda1 -0.00041431258\n",
      "40 Train Loss 686.7278 Test MSE 703.1139254731614 Test RE 0.4463772171450243 Lambda1 -0.00022215866\n",
      "41 Train Loss 684.743 Test MSE 702.7684063943349 Test RE 0.44626752596140834 Lambda1 -6.728419e-05\n",
      "42 Train Loss 683.6874 Test MSE 702.0336554989372 Test RE 0.44603417653180044 Lambda1 -8.301521e-06\n",
      "43 Train Loss 682.0755 Test MSE 701.3860051331499 Test RE 0.44582838806981995 Lambda1 -7.997365e-06\n",
      "44 Train Loss 681.4247 Test MSE 701.0794248135569 Test RE 0.44573094019651605 Lambda1 -6.971132e-06\n",
      "45 Train Loss 680.08 Test MSE 699.47150286863 Test RE 0.4452195060108548 Lambda1 -6.4039964e-06\n",
      "46 Train Loss 679.6535 Test MSE 699.3859565163937 Test RE 0.4451922796913269 Lambda1 -7.192769e-06\n",
      "47 Train Loss 678.6573 Test MSE 697.7933961835727 Test RE 0.4446851207834012 Lambda1 -1.1636119e-05\n",
      "48 Train Loss 677.0257 Test MSE 695.1037026983844 Test RE 0.44382725835143183 Lambda1 -5.526103e-06\n",
      "49 Train Loss 675.43335 Test MSE 692.7807117719061 Test RE 0.4430850169264466 Lambda1 -1.6383581e-05\n",
      "50 Train Loss 674.1431 Test MSE 691.4329278095739 Test RE 0.4426538028331538 Lambda1 -3.7590103e-06\n",
      "51 Train Loss 673.2032 Test MSE 690.5940646541776 Test RE 0.4423852021890159 Lambda1 -7.2289854e-06\n",
      "52 Train Loss 671.0151 Test MSE 688.1634787293578 Test RE 0.4416060157788629 Lambda1 -2.6632185e-06\n",
      "53 Train Loss 664.6354 Test MSE 679.2615028825902 Test RE 0.4387404450636798 Lambda1 -3.7539077e-05\n",
      "54 Train Loss 662.1091 Test MSE 674.7401681449669 Test RE 0.43727782394716064 Lambda1 3.9393373e-05\n",
      "55 Train Loss 661.1554 Test MSE 675.1137267937416 Test RE 0.43739885298779674 Lambda1 7.366546e-05\n",
      "56 Train Loss 657.7966 Test MSE 656.280844271057 Test RE 0.4312548921018623 Lambda1 -7.289746e-07\n",
      "57 Train Loss 648.29596 Test MSE 643.6357820345568 Test RE 0.42708002618554075 Lambda1 3.397605e-05\n",
      "58 Train Loss 641.9472 Test MSE 617.702297757363 Test RE 0.4183875753841524 Lambda1 0.00023836314\n",
      "59 Train Loss 624.9045 Test MSE 602.3295731266903 Test RE 0.4131485795622183 Lambda1 9.256336e-05\n",
      "60 Train Loss 618.14526 Test MSE 586.8917286282461 Test RE 0.40781966639189937 Lambda1 -8.584295e-06\n",
      "61 Train Loss 593.6327 Test MSE 561.0692444909985 Test RE 0.39874697572173695 Lambda1 8.157348e-05\n",
      "62 Train Loss 567.5418 Test MSE 526.8147591170873 Test RE 0.3863831132058063 Lambda1 0.000110223256\n",
      "63 Train Loss 547.41 Test MSE 497.1716389480477 Test RE 0.375355119698349 Lambda1 0.00010262497\n",
      "64 Train Loss 514.46344 Test MSE 472.6323405935686 Test RE 0.3659745525025476 Lambda1 7.6487195e-05\n",
      "65 Train Loss 479.83084 Test MSE 456.0586933132311 Test RE 0.35950053409000793 Lambda1 8.976465e-05\n",
      "66 Train Loss 467.95456 Test MSE 453.91251846203886 Test RE 0.35865364653731674 Lambda1 0.00011166745\n",
      "67 Train Loss 462.5171 Test MSE 454.30260413751006 Test RE 0.35880772424940893 Lambda1 0.00010838918\n",
      "68 Train Loss 458.31787 Test MSE 450.32832759699824 Test RE 0.3572348370172747 Lambda1 7.595907e-05\n",
      "69 Train Loss 454.13522 Test MSE 446.8463267861201 Test RE 0.35585106239881337 Lambda1 9.101741e-05\n",
      "70 Train Loss 451.47385 Test MSE 446.5546376012509 Test RE 0.35573489846867723 Lambda1 7.998849e-05\n",
      "71 Train Loss 449.47455 Test MSE 445.39739226050034 Test RE 0.3552736563524991 Lambda1 5.623781e-05\n",
      "72 Train Loss 444.4646 Test MSE 441.6526791974373 Test RE 0.35377700852929506 Lambda1 6.6752844e-05\n",
      "73 Train Loss 440.045 Test MSE 439.72322590906407 Test RE 0.35300338777013684 Lambda1 5.482164e-05\n",
      "74 Train Loss 437.9915 Test MSE 438.1108852847281 Test RE 0.35235561166176127 Lambda1 6.905473e-05\n",
      "Training time: 122.71\n",
      "Training time: 122.71\n",
      "inv_HT_atanh_tune16\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 12950.967 Test MSE 3604.1620553275034 Test RE 1.010627998244598 Lambda1 0.0021484308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 6928.676 Test MSE 3613.7213388293653 Test RE 1.0119673500854804 Lambda1 -0.00030016096\n",
      "2 Train Loss 4874.53 Test MSE 3616.5808860233487 Test RE 1.012367657122163 Lambda1 -0.00026850967\n",
      "3 Train Loss 4115.6436 Test MSE 3623.3963593326866 Test RE 1.0133211152859203 Lambda1 0.00041954085\n",
      "4 Train Loss 3786.4268 Test MSE 3615.9994050984615 Test RE 1.0122862686278888 Lambda1 -0.00013401627\n",
      "5 Train Loss 3650.8948 Test MSE 3588.8885261360588 Test RE 1.0084843315539627 Lambda1 0.00023335376\n",
      "6 Train Loss 3517.7046 Test MSE 3483.0197164304113 Test RE 0.993498320044066 Lambda1 0.0010247035\n",
      "7 Train Loss 949.7621 Test MSE 949.825012341911 Test RE 0.5188131455687995 Lambda1 0.048482392\n",
      "8 Train Loss 854.70087 Test MSE 858.0225948996678 Test RE 0.4931040084357395 Lambda1 0.06730366\n",
      "9 Train Loss 854.6136 Test MSE 857.9084750067789 Test RE 0.4930712150989388 Lambda1 0.068688065\n",
      "10 Train Loss 854.37976 Test MSE 857.434307930091 Test RE 0.49293493573025615 Lambda1 0.067814864\n",
      "11 Train Loss 854.2827 Test MSE 857.4040496254452 Test RE 0.4929262379752744 Lambda1 0.06506069\n",
      "12 Train Loss 853.53656 Test MSE 855.7496098094433 Test RE 0.492450435126865 Lambda1 0.04976532\n",
      "13 Train Loss 850.3923 Test MSE 850.5959310577413 Test RE 0.49096532547395494 Lambda1 -0.047782876\n",
      "14 Train Loss 842.9977 Test MSE 833.5515289974716 Test RE 0.4860214053419586 Lambda1 -0.017762665\n",
      "15 Train Loss 822.9525 Test MSE 816.5162919493233 Test RE 0.48102937469475276 Lambda1 0.0030479264\n",
      "16 Train Loss 790.0507 Test MSE 787.4614838716228 Test RE 0.4723934106478761 Lambda1 -5.155505e-05\n",
      "17 Train Loss 759.8788 Test MSE 761.5720353985279 Test RE 0.46456305064185477 Lambda1 1.586177e-05\n",
      "18 Train Loss 747.139 Test MSE 750.2811606697513 Test RE 0.4611064438537835 Lambda1 -0.00016977554\n",
      "19 Train Loss 727.995 Test MSE 733.886532127131 Test RE 0.4560407272308254 Lambda1 -5.3935917e-05\n",
      "20 Train Loss 701.81934 Test MSE 706.1997101390599 Test RE 0.44735566170439056 Lambda1 -0.00016858448\n",
      "21 Train Loss 675.3807 Test MSE 685.7220840987575 Test RE 0.44082197789221195 Lambda1 -0.00016111028\n",
      "22 Train Loss 667.4649 Test MSE 678.1403502199722 Test RE 0.4383782147903101 Lambda1 -0.00024331361\n",
      "23 Train Loss 663.8312 Test MSE 678.5088802517444 Test RE 0.4384973152053742 Lambda1 -0.00010745148\n",
      "24 Train Loss 659.8241 Test MSE 674.6582046207383 Test RE 0.43725126415295396 Lambda1 -0.00013489086\n",
      "25 Train Loss 657.7853 Test MSE 675.5805135510685 Test RE 0.4375500398946964 Lambda1 -3.0690648e-05\n",
      "26 Train Loss 655.5257 Test MSE 674.4222661591755 Test RE 0.4371748006852312 Lambda1 -2.0093186e-05\n",
      "27 Train Loss 654.13025 Test MSE 673.6005884552964 Test RE 0.4369084050726644 Lambda1 -8.646876e-06\n",
      "28 Train Loss 652.95575 Test MSE 673.4350638169212 Test RE 0.43685472077494963 Lambda1 1.8189541e-05\n",
      "29 Train Loss 650.5416 Test MSE 670.6817785542052 Test RE 0.43596078339176825 Lambda1 0.00011550833\n",
      "30 Train Loss 648.91394 Test MSE 670.9112365002543 Test RE 0.4360353538620356 Lambda1 0.00011729374\n",
      "31 Train Loss 647.7387 Test MSE 668.7038068010069 Test RE 0.43531744189353144 Lambda1 0.00019078563\n",
      "32 Train Loss 647.1938 Test MSE 668.7950556679092 Test RE 0.435347141790494 Lambda1 0.00012208348\n",
      "33 Train Loss 645.38385 Test MSE 666.4546945901667 Test RE 0.43458475396289825 Lambda1 0.00012839944\n",
      "34 Train Loss 644.1641 Test MSE 665.0855390417917 Test RE 0.434138121923599 Lambda1 3.348416e-06\n",
      "35 Train Loss 643.34454 Test MSE 664.0256366012608 Test RE 0.43379205551606725 Lambda1 -6.515476e-05\n",
      "36 Train Loss 642.12836 Test MSE 663.3055624873765 Test RE 0.4335567881252327 Lambda1 -0.00024386865\n",
      "37 Train Loss 640.90515 Test MSE 662.0013156771709 Test RE 0.43313033059967737 Lambda1 -0.00053153356\n",
      "38 Train Loss 638.8642 Test MSE 658.4998968945591 Test RE 0.431983368124614 Lambda1 -0.00067985913\n",
      "39 Train Loss 637.3276 Test MSE 656.240896594511 Test RE 0.43124176670497055 Lambda1 -0.00060320995\n",
      "40 Train Loss 635.0867 Test MSE 654.6693536267233 Test RE 0.43072509577053253 Lambda1 -0.00069320184\n",
      "41 Train Loss 633.36896 Test MSE 653.8942348849166 Test RE 0.43047003420913255 Lambda1 -0.0004119556\n",
      "42 Train Loss 632.3361 Test MSE 652.8613804972501 Test RE 0.43012992678043804 Lambda1 -6.430364e-05\n",
      "43 Train Loss 631.1639 Test MSE 651.553516141066 Test RE 0.42969887538402524 Lambda1 -4.0416915e-05\n",
      "44 Train Loss 629.44556 Test MSE 650.647276423344 Test RE 0.4293999393190023 Lambda1 1.6593822e-05\n",
      "45 Train Loss 628.561 Test MSE 650.4442090428781 Test RE 0.42933292610874296 Lambda1 0.000101420424\n",
      "46 Train Loss 628.114 Test MSE 650.0970707986887 Test RE 0.4292183445915756 Lambda1 7.8703946e-05\n",
      "47 Train Loss 627.5903 Test MSE 650.220266334302 Test RE 0.42925901180985804 Lambda1 3.3958317e-05\n",
      "48 Train Loss 626.7147 Test MSE 648.5988824531754 Test RE 0.42872347938996963 Lambda1 0.00010165766\n",
      "49 Train Loss 626.1305 Test MSE 647.3304918655235 Test RE 0.4283040715268494 Lambda1 0.00013128565\n",
      "50 Train Loss 625.1459 Test MSE 645.4121560272881 Test RE 0.42766897038374896 Lambda1 -1.0685373e-05\n",
      "51 Train Loss 621.07 Test MSE 635.3472479150295 Test RE 0.4243212161871886 Lambda1 -7.8725054e-05\n",
      "52 Train Loss 608.6049 Test MSE 608.7170361578061 Test RE 0.41533343976231524 Lambda1 -0.00017182875\n",
      "53 Train Loss 601.25745 Test MSE 594.2506989757136 Test RE 0.4103685041852396 Lambda1 5.4389704e-05\n",
      "54 Train Loss 591.1327 Test MSE 579.2213426331678 Test RE 0.40514590031271064 Lambda1 1.2753517e-05\n",
      "55 Train Loss 567.209 Test MSE 541.0975029787975 Test RE 0.39158580076078564 Lambda1 0.00052227563\n",
      "56 Train Loss 524.59607 Test MSE 489.60936777615154 Test RE 0.3724894956260111 Lambda1 -0.00037822512\n",
      "57 Train Loss 476.9736 Test MSE 426.68734012609355 Test RE 0.34773151233852057 Lambda1 -0.0017641529\n",
      "58 Train Loss 446.61996 Test MSE 412.880481836024 Test RE 0.34205925624003647 Lambda1 -0.0028863973\n",
      "59 Train Loss 432.58118 Test MSE 405.2993896646573 Test RE 0.33890435175769296 Lambda1 -0.0020787115\n",
      "60 Train Loss 417.9246 Test MSE 401.2005799796011 Test RE 0.33718632014418504 Lambda1 -0.0017468283\n",
      "61 Train Loss 408.49762 Test MSE 389.25078059907844 Test RE 0.33212679662185035 Lambda1 -0.0022264973\n",
      "62 Train Loss 403.7412 Test MSE 384.9755840551297 Test RE 0.33029786303534703 Lambda1 -0.0022462856\n",
      "63 Train Loss 393.90463 Test MSE 372.7055627306056 Test RE 0.32499157906357773 Lambda1 -0.0015975368\n",
      "64 Train Loss 390.2655 Test MSE 371.1589337667019 Test RE 0.32431656374141804 Lambda1 -0.001665189\n",
      "65 Train Loss 382.45425 Test MSE 363.41809289889454 Test RE 0.32091679270329465 Lambda1 -0.00168332\n",
      "66 Train Loss 373.90207 Test MSE 346.51471698082776 Test RE 0.3133646570714969 Lambda1 -0.0016937067\n",
      "67 Train Loss 365.72275 Test MSE 339.5234897176554 Test RE 0.31018735080595355 Lambda1 -0.0016122517\n",
      "68 Train Loss 358.6979 Test MSE 334.31583021410404 Test RE 0.3077993095728739 Lambda1 -0.0018152561\n",
      "69 Train Loss 351.56018 Test MSE 335.2439881419478 Test RE 0.3082262836064616 Lambda1 -0.0017971568\n",
      "70 Train Loss 348.5606 Test MSE 336.4984066490347 Test RE 0.30880240690131955 Lambda1 -0.0016799805\n",
      "71 Train Loss 346.17593 Test MSE 338.291991753581 Test RE 0.309624293875778 Lambda1 -0.001339934\n",
      "72 Train Loss 344.53293 Test MSE 335.9599725002651 Test RE 0.3085552492275894 Lambda1 -0.0014022675\n",
      "73 Train Loss 336.46606 Test MSE 334.5876403531741 Test RE 0.3079244098033744 Lambda1 -0.0011842104\n",
      "74 Train Loss 334.98138 Test MSE 334.48198227911985 Test RE 0.30787578686415634 Lambda1 -0.0011927456\n",
      "Training time: 120.73\n",
      "Training time: 120.73\n",
      "inv_HT_atanh_tune16\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 12437.063 Test MSE 3352.548605449958 Test RE 0.9747128961509997 Lambda1 -0.0006484918\n",
      "1 Train Loss 4779.1904 Test MSE 3338.4115420746148 Test RE 0.9726556351257006 Lambda1 0.0019088424\n",
      "2 Train Loss 3523.3018 Test MSE 3327.9438097978646 Test RE 0.9711295362121879 Lambda1 -0.0023290592\n",
      "3 Train Loss 3343.057 Test MSE 3306.215553974946 Test RE 0.967954076031854 Lambda1 0.0009158279\n",
      "4 Train Loss 2506.5671 Test MSE 2480.1583759727628 Test RE 0.8383567741499993 Lambda1 0.24742582\n",
      "5 Train Loss 855.71967 Test MSE 858.0902499120057 Test RE 0.4931234486581599 Lambda1 1.7134987\n",
      "6 Train Loss 854.7468 Test MSE 858.0816094116938 Test RE 0.4931209659099649 Lambda1 1.6860378\n",
      "7 Train Loss 854.72095 Test MSE 858.073484760253 Test RE 0.49311863137359574 Lambda1 1.6595687\n",
      "8 Train Loss 854.7136 Test MSE 858.0641203512063 Test RE 0.49311594059171426 Lambda1 1.6188142\n",
      "9 Train Loss 854.6837 Test MSE 858.0140252678281 Test RE 0.49310154595375616 Lambda1 1.6183956\n",
      "10 Train Loss 854.61005 Test MSE 857.853538572238 Test RE 0.4930554278623188 Lambda1 1.6137369\n",
      "11 Train Loss 854.4742 Test MSE 857.5138740424816 Test RE 0.49295780629088987 Lambda1 1.5772665\n",
      "12 Train Loss 853.76306 Test MSE 856.5478680786789 Test RE 0.49268006479715387 Lambda1 1.5531062\n",
      "13 Train Loss 852.1666 Test MSE 854.0819874928826 Test RE 0.4919703754222876 Lambda1 1.508708\n",
      "14 Train Loss 849.8322 Test MSE 851.0625175684498 Test RE 0.49109996425114416 Lambda1 1.4830583\n",
      "15 Train Loss 847.6579 Test MSE 846.9743254772422 Test RE 0.48991901230372187 Lambda1 1.5257264\n",
      "16 Train Loss 844.96295 Test MSE 843.3265314176098 Test RE 0.48886286928756145 Lambda1 1.5372845\n",
      "17 Train Loss 841.29834 Test MSE 837.6656900949602 Test RE 0.48721935714148434 Lambda1 1.5535764\n",
      "18 Train Loss 839.061 Test MSE 835.5190088173839 Test RE 0.4865946594679642 Lambda1 1.5633603\n",
      "19 Train Loss 836.48914 Test MSE 832.1766796406258 Test RE 0.48562042114753307 Lambda1 1.5412099\n",
      "20 Train Loss 834.5636 Test MSE 831.0143102166697 Test RE 0.4852811496975589 Lambda1 1.5576988\n",
      "21 Train Loss 830.1541 Test MSE 823.7765694638288 Test RE 0.48316324372463215 Lambda1 1.5503883\n",
      "22 Train Loss 825.463 Test MSE 821.6234590219693 Test RE 0.48253140706170716 Lambda1 1.568302\n",
      "23 Train Loss 820.65796 Test MSE 817.9989564275687 Test RE 0.48146591325531213 Lambda1 1.5306246\n",
      "24 Train Loss 817.48944 Test MSE 815.3035693975754 Test RE 0.48067201995297115 Lambda1 1.5799696\n",
      "25 Train Loss 815.7413 Test MSE 813.3773618690678 Test RE 0.48010387478307837 Lambda1 1.5590019\n",
      "26 Train Loss 813.19904 Test MSE 809.405444549942 Test RE 0.4789302088506004 Lambda1 1.5724853\n",
      "27 Train Loss 808.4166 Test MSE 803.7130653287452 Test RE 0.47724312939746205 Lambda1 1.5817165\n",
      "28 Train Loss 806.7561 Test MSE 800.8947769390094 Test RE 0.4764056477155273 Lambda1 1.5782868\n",
      "29 Train Loss 805.0685 Test MSE 798.9555862370821 Test RE 0.47582854237860456 Lambda1 1.5749358\n",
      "30 Train Loss 802.96204 Test MSE 797.7055034084259 Test RE 0.47545614499170874 Lambda1 1.5817853\n",
      "31 Train Loss 800.578 Test MSE 795.2084297907683 Test RE 0.47471139673225077 Lambda1 1.5789217\n",
      "32 Train Loss 798.7909 Test MSE 793.181794977704 Test RE 0.4741060960590466 Lambda1 1.586285\n",
      "33 Train Loss 797.5927 Test MSE 791.8169510995265 Test RE 0.47369801848597265 Lambda1 1.5945406\n",
      "34 Train Loss 796.65326 Test MSE 791.2429074449826 Test RE 0.4735262788918318 Lambda1 1.5892069\n",
      "35 Train Loss 795.9096 Test MSE 790.0630023097751 Test RE 0.4731730848673924 Lambda1 1.5796111\n",
      "36 Train Loss 794.7618 Test MSE 789.3057383058285 Test RE 0.4729462657116172 Lambda1 1.5748216\n",
      "37 Train Loss 794.08704 Test MSE 789.1539669091239 Test RE 0.47290079336630403 Lambda1 1.5725937\n",
      "38 Train Loss 792.59076 Test MSE 787.8767168089323 Test RE 0.47251794210504056 Lambda1 1.5472054\n",
      "39 Train Loss 790.5713 Test MSE 785.7961130291901 Test RE 0.47189362328411955 Lambda1 1.5238518\n",
      "40 Train Loss 789.36176 Test MSE 783.9159414281966 Test RE 0.4713287360729516 Lambda1 1.5367217\n",
      "41 Train Loss 788.74713 Test MSE 783.2335801736529 Test RE 0.4711235568763975 Lambda1 1.5467677\n",
      "42 Train Loss 788.2683 Test MSE 782.7558648630479 Test RE 0.470979859472159 Lambda1 1.5390706\n",
      "43 Train Loss 787.7706 Test MSE 781.8498783028039 Test RE 0.47070721701558615 Lambda1 1.5409249\n",
      "44 Train Loss 786.38214 Test MSE 782.0863688494517 Test RE 0.4707784003678469 Lambda1 1.5458779\n",
      "45 Train Loss 785.5303 Test MSE 782.2660900898004 Test RE 0.47083248903455394 Lambda1 1.5392742\n",
      "46 Train Loss 785.1602 Test MSE 782.5251200275804 Test RE 0.47091043541207006 Lambda1 1.5398278\n",
      "47 Train Loss 784.8518 Test MSE 782.6339090200539 Test RE 0.4709431679660122 Lambda1 1.5426782\n",
      "48 Train Loss 784.1397 Test MSE 782.0229673698401 Test RE 0.47075931765953644 Lambda1 1.5436544\n",
      "49 Train Loss 783.8948 Test MSE 781.5292318553102 Test RE 0.4706106856487973 Lambda1 1.54156\n",
      "50 Train Loss 783.7131 Test MSE 781.0407701951033 Test RE 0.4704635950492866 Lambda1 1.5466386\n",
      "51 Train Loss 783.5073 Test MSE 780.9225393875075 Test RE 0.4704279852590704 Lambda1 1.5463707\n",
      "52 Train Loss 783.1743 Test MSE 780.1770432726299 Test RE 0.47020338809623197 Lambda1 1.5451959\n",
      "53 Train Loss 782.6019 Test MSE 779.5023743709252 Test RE 0.47000003667847606 Lambda1 1.5511456\n",
      "54 Train Loss 781.78864 Test MSE 778.6548049645089 Test RE 0.46974444669582277 Lambda1 1.5512574\n",
      "55 Train Loss 781.373 Test MSE 777.7039449263261 Test RE 0.46945754313862126 Lambda1 1.5460829\n",
      "56 Train Loss 780.9569 Test MSE 777.2614067386071 Test RE 0.4693239560232329 Lambda1 1.549866\n",
      "57 Train Loss 780.4729 Test MSE 776.1609926536944 Test RE 0.46899161377072296 Lambda1 1.5549011\n",
      "58 Train Loss 780.0686 Test MSE 775.8442159195753 Test RE 0.4688958985809198 Lambda1 1.5584459\n",
      "59 Train Loss 779.7331 Test MSE 776.1124112070224 Test RE 0.4689769359861981 Lambda1 1.558239\n",
      "60 Train Loss 779.35046 Test MSE 776.3463907232627 Test RE 0.4690476233801552 Lambda1 1.5637509\n",
      "61 Train Loss 778.7376 Test MSE 775.9255033356116 Test RE 0.46892046171930035 Lambda1 1.5584\n",
      "62 Train Loss 777.6764 Test MSE 774.6289595943093 Test RE 0.4685285232689608 Lambda1 1.5698701\n",
      "63 Train Loss 776.84467 Test MSE 772.884712229325 Test RE 0.4680007285139884 Lambda1 1.5806551\n",
      "64 Train Loss 775.8027 Test MSE 772.2709602374345 Test RE 0.4678148706354234 Lambda1 1.5739099\n",
      "65 Train Loss 774.7076 Test MSE 771.2105484398269 Test RE 0.46749358002489716 Lambda1 1.5675199\n",
      "66 Train Loss 773.5391 Test MSE 769.4437390373397 Test RE 0.46695776936753125 Lambda1 1.5590564\n",
      "67 Train Loss 772.4428 Test MSE 767.8873052828073 Test RE 0.4664852483063386 Lambda1 1.5687933\n",
      "68 Train Loss 771.8609 Test MSE 766.2578792544103 Test RE 0.4659900539882883 Lambda1 1.5824045\n",
      "69 Train Loss 771.43085 Test MSE 765.9739214892259 Test RE 0.4659037033229193 Lambda1 1.591215\n",
      "70 Train Loss 769.66766 Test MSE 764.5018199892497 Test RE 0.4654557850776805 Lambda1 1.5821885\n",
      "71 Train Loss 768.3578 Test MSE 763.789996645115 Test RE 0.46523904298248725 Lambda1 1.5753906\n",
      "72 Train Loss 767.72577 Test MSE 762.5465316660515 Test RE 0.4648601796123208 Lambda1 1.5832919\n",
      "73 Train Loss 766.83746 Test MSE 761.0381622457336 Test RE 0.46440018934295796 Lambda1 1.5916913\n",
      "74 Train Loss 765.3343 Test MSE 759.8243524763383 Test RE 0.4640296964699512 Lambda1 1.5787258\n",
      "Training time: 121.36\n",
      "Training time: 121.36\n",
      "inv_HT_atanh_tune16\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 14632.037 Test MSE 3677.7880794644266 Test RE 1.0208983950003072 Lambda1 0.0028486669\n",
      "1 Train Loss 8273.748 Test MSE 3647.244085811908 Test RE 1.0166502809342568 Lambda1 -0.0018182416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Train Loss 5168.0913 Test MSE 3618.411746508464 Test RE 1.01262387556132 Lambda1 0.00010835964\n",
      "3 Train Loss 4211.2183 Test MSE 3588.3171407183027 Test RE 1.0084040481773437 Lambda1 -0.0011024191\n",
      "4 Train Loss 3847.822 Test MSE 3571.745095939848 Test RE 1.006072780388458 Lambda1 0.00014998257\n",
      "5 Train Loss 3663.017 Test MSE 3533.495525643288 Test RE 1.0006712986571356 Lambda1 -0.0016071352\n",
      "6 Train Loss 3551.8354 Test MSE 3474.2348042887143 Test RE 0.9922446226129366 Lambda1 0.0014626344\n",
      "7 Train Loss 3394.3647 Test MSE 3340.6183308311 Test RE 0.9729770590864195 Lambda1 -0.0035585142\n",
      "8 Train Loss 1719.913 Test MSE 1711.7820661604787 Test RE 0.6964876286439707 Lambda1 0.2301211\n",
      "9 Train Loss 854.6739 Test MSE 857.9875653457152 Test RE 0.49309394262286177 Lambda1 0.56087357\n",
      "10 Train Loss 854.54614 Test MSE 857.7359217877519 Test RE 0.4930216262987216 Lambda1 0.55665946\n",
      "11 Train Loss 853.9893 Test MSE 856.8304721714869 Test RE 0.4927613339972698 Lambda1 0.4604298\n",
      "12 Train Loss 852.48956 Test MSE 854.4849414165514 Test RE 0.49208641698449324 Lambda1 0.4583801\n",
      "13 Train Loss 847.1069 Test MSE 847.4498447989663 Test RE 0.49005652135205224 Lambda1 0.4158582\n",
      "14 Train Loss 840.45734 Test MSE 837.1586012672368 Test RE 0.48707186339937786 Lambda1 0.37524882\n",
      "15 Train Loss 830.0955 Test MSE 819.5836469416968 Test RE 0.48193205400886174 Lambda1 0.37769607\n",
      "16 Train Loss 822.2589 Test MSE 806.4185699747345 Test RE 0.47804571602366874 Lambda1 0.36669916\n",
      "17 Train Loss 813.2885 Test MSE 790.068979849779 Test RE 0.47317487485482734 Lambda1 0.35019538\n",
      "18 Train Loss 796.403 Test MSE 773.2462200272834 Test RE 0.4681101666559732 Lambda1 0.35611764\n",
      "19 Train Loss 782.6413 Test MSE 758.6601357855142 Test RE 0.4636740628208177 Lambda1 0.37721232\n",
      "20 Train Loss 764.56573 Test MSE 734.689774279485 Test RE 0.4562902283061168 Lambda1 0.3659356\n",
      "21 Train Loss 739.45074 Test MSE 704.5497390275877 Test RE 0.4468327532749935 Lambda1 0.33984572\n",
      "22 Train Loss 723.9507 Test MSE 697.3185342016769 Test RE 0.44453378659446263 Lambda1 0.354326\n",
      "23 Train Loss 711.3023 Test MSE 694.2160017323538 Test RE 0.443543767015949 Lambda1 0.3405241\n",
      "24 Train Loss 698.93604 Test MSE 678.3011381964106 Test RE 0.43843018173820336 Lambda1 0.3249164\n",
      "25 Train Loss 686.6334 Test MSE 665.2193398391834 Test RE 0.4341817893184396 Lambda1 0.31405628\n",
      "26 Train Loss 679.6321 Test MSE 657.5642737442588 Test RE 0.43167636936098874 Lambda1 0.31013057\n",
      "27 Train Loss 673.306 Test MSE 650.0828305556444 Test RE 0.4292136435959148 Lambda1 0.30893594\n",
      "28 Train Loss 663.8493 Test MSE 646.9458185097657 Test RE 0.42817679369088796 Lambda1 0.30043295\n",
      "29 Train Loss 656.80853 Test MSE 643.2176433051309 Test RE 0.42694127713103874 Lambda1 0.2936409\n",
      "30 Train Loss 649.2584 Test MSE 630.3143923643945 Test RE 0.4226372603869724 Lambda1 0.28292\n",
      "31 Train Loss 643.85956 Test MSE 625.0496779989812 Test RE 0.4208685159211865 Lambda1 0.27853316\n",
      "32 Train Loss 638.1393 Test MSE 620.095429844781 Test RE 0.4191972605917394 Lambda1 0.28289983\n",
      "33 Train Loss 630.49414 Test MSE 604.2447285332483 Test RE 0.41380487788564246 Lambda1 0.2741256\n",
      "34 Train Loss 624.7124 Test MSE 600.3549712372239 Test RE 0.41247081633436367 Lambda1 0.28012577\n",
      "35 Train Loss 621.6613 Test MSE 600.1958033102097 Test RE 0.4124161349541928 Lambda1 0.28305182\n",
      "36 Train Loss 616.3541 Test MSE 596.681637158475 Test RE 0.4112070074742451 Lambda1 0.28781354\n",
      "37 Train Loss 613.0041 Test MSE 595.1026639075951 Test RE 0.4106625672418557 Lambda1 0.28331676\n",
      "38 Train Loss 611.4748 Test MSE 591.9105268965669 Test RE 0.40955968710143015 Lambda1 0.28441176\n",
      "39 Train Loss 608.60925 Test MSE 588.5171855213345 Test RE 0.4083840251162737 Lambda1 0.2813189\n",
      "40 Train Loss 604.9809 Test MSE 587.4316823104836 Test RE 0.4080072249269737 Lambda1 0.2748743\n",
      "41 Train Loss 600.2224 Test MSE 582.090448767732 Test RE 0.40614808254848317 Lambda1 0.2658192\n",
      "42 Train Loss 597.874 Test MSE 579.9028846472379 Test RE 0.4053841881267703 Lambda1 0.25941837\n",
      "43 Train Loss 596.5245 Test MSE 581.2372460327763 Test RE 0.40585031634254914 Lambda1 0.25647247\n",
      "44 Train Loss 593.39233 Test MSE 578.5736062383036 Test RE 0.4049193020038639 Lambda1 0.25566256\n",
      "45 Train Loss 591.8134 Test MSE 576.4722536255199 Test RE 0.4041833090222849 Lambda1 0.25619924\n",
      "46 Train Loss 589.6685 Test MSE 576.746125291747 Test RE 0.40427930775552967 Lambda1 0.25678015\n",
      "47 Train Loss 587.25354 Test MSE 573.915111033442 Test RE 0.4032858650695343 Lambda1 0.2560011\n",
      "48 Train Loss 584.7454 Test MSE 569.9298080144188 Test RE 0.4018832044429343 Lambda1 0.25483483\n",
      "49 Train Loss 582.2285 Test MSE 567.0659849298363 Test RE 0.4008722274465253 Lambda1 0.25449154\n",
      "50 Train Loss 581.13696 Test MSE 568.0892604007812 Test RE 0.4012337531255606 Lambda1 0.25751862\n",
      "51 Train Loss 579.94116 Test MSE 566.8143279418786 Test RE 0.40078326646573575 Lambda1 0.26038334\n",
      "52 Train Loss 579.395 Test MSE 567.4024518670179 Test RE 0.4009911379899078 Lambda1 0.2600781\n",
      "53 Train Loss 579.066 Test MSE 566.2586254232882 Test RE 0.40058675512691255 Lambda1 0.25898984\n",
      "54 Train Loss 578.72906 Test MSE 565.8916332265535 Test RE 0.40045692395611887 Lambda1 0.25759417\n",
      "55 Train Loss 577.65295 Test MSE 566.9850777312042 Test RE 0.40084362882947516 Lambda1 0.25839177\n",
      "56 Train Loss 576.44965 Test MSE 564.1991076940944 Test RE 0.39985761210582843 Lambda1 0.25917143\n",
      "57 Train Loss 575.3369 Test MSE 564.9859832380941 Test RE 0.40013635106830914 Lambda1 0.25834128\n",
      "58 Train Loss 574.4028 Test MSE 563.3242883997601 Test RE 0.39954749210440643 Lambda1 0.25527734\n",
      "59 Train Loss 573.3727 Test MSE 563.152018346827 Test RE 0.3994863946761022 Lambda1 0.25635627\n",
      "60 Train Loss 572.8163 Test MSE 563.0707017137997 Test RE 0.3994575516104203 Lambda1 0.25658613\n",
      "61 Train Loss 572.09973 Test MSE 562.103094719045 Test RE 0.39911418077323146 Lambda1 0.25348985\n",
      "62 Train Loss 570.7761 Test MSE 560.4977070076968 Test RE 0.3985438306442611 Lambda1 0.2586864\n",
      "63 Train Loss 569.58136 Test MSE 559.6167825847851 Test RE 0.39823051531089243 Lambda1 0.2589544\n",
      "64 Train Loss 568.89905 Test MSE 558.3715863097576 Test RE 0.39778721972146835 Lambda1 0.26083994\n",
      "65 Train Loss 567.4041 Test MSE 555.1541288654454 Test RE 0.3966394961211042 Lambda1 0.26355177\n",
      "66 Train Loss 565.5747 Test MSE 554.6119700399881 Test RE 0.3964457714239724 Lambda1 0.26127374\n",
      "67 Train Loss 563.72894 Test MSE 553.0521408131548 Test RE 0.3958878830748315 Lambda1 0.26009944\n",
      "68 Train Loss 563.2012 Test MSE 552.9629906619504 Test RE 0.39585597388902144 Lambda1 0.26119277\n",
      "69 Train Loss 561.7722 Test MSE 549.7666639633791 Test RE 0.3947102201994744 Lambda1 0.26264292\n",
      "70 Train Loss 560.55585 Test MSE 547.1071882793175 Test RE 0.3937543648524091 Lambda1 0.26477495\n",
      "71 Train Loss 559.88184 Test MSE 546.3059502241915 Test RE 0.3934659327297952 Lambda1 0.26433146\n",
      "72 Train Loss 559.3873 Test MSE 546.0189972720872 Test RE 0.39336258309476735 Lambda1 0.26287007\n",
      "73 Train Loss 558.73346 Test MSE 545.1482087896144 Test RE 0.3930487915544994 Lambda1 0.2606094\n",
      "74 Train Loss 557.8129 Test MSE 544.211628760217 Test RE 0.39271101194738944 Lambda1 0.26335728\n",
      "Training time: 120.41\n",
      "Training time: 120.41\n",
      "inv_HT_atanh_tune17\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 2256971.0 Test MSE 3511.1362370780907 Test RE 0.9975002452806646 Lambda1 0.00011508542\n",
      "1 Train Loss 2074501.4 Test MSE 3511.332133574116 Test RE 0.9975280716115357 Lambda1 -4.5689245e-05\n",
      "2 Train Loss 1980541.1 Test MSE 3511.5521006091735 Test RE 0.9975593161431867 Lambda1 -1.11331865e-05\n",
      "3 Train Loss 1883365.5 Test MSE 3511.9130230217643 Test RE 0.9976105801218845 Lambda1 -3.967075e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 1800192.6 Test MSE 3512.357459844467 Test RE 0.9976737025350961 Lambda1 5.8367123e-06\n",
      "5 Train Loss 1729902.5 Test MSE 3512.8203805007593 Test RE 0.9977394459216874 Lambda1 -9.666451e-07\n",
      "6 Train Loss 1672807.0 Test MSE 3513.015638496135 Test RE 0.9977671749080719 Lambda1 -2.0778396e-05\n",
      "7 Train Loss 1619425.9 Test MSE 3513.1704900151817 Test RE 0.997789165140396 Lambda1 2.0003456e-05\n",
      "8 Train Loss 1569036.2 Test MSE 3512.9547025326283 Test RE 0.9977585213502638 Lambda1 6.262951e-06\n",
      "9 Train Loss 1529150.6 Test MSE 3513.00271153564 Test RE 0.9977653391478524 Lambda1 -1.7601214e-05\n",
      "10 Train Loss 1497430.4 Test MSE 3513.234092008653 Test RE 0.9977981970238364 Lambda1 6.9044895e-06\n",
      "11 Train Loss 1467521.2 Test MSE 3513.34089570684 Test RE 0.9978133636373863 Lambda1 -1.3707084e-05\n",
      "12 Train Loss 1440175.8 Test MSE 3513.0190808414436 Test RE 0.9977676637556413 Lambda1 3.958246e-07\n",
      "13 Train Loss 1410945.6 Test MSE 3512.8774551992215 Test RE 0.9977475512957954 Lambda1 -1.3772251e-06\n",
      "14 Train Loss 1380495.2 Test MSE 3512.5606878429376 Test RE 0.9977025652432913 Lambda1 1.6975959e-05\n",
      "15 Train Loss 1360915.4 Test MSE 3512.171838133584 Test RE 0.997647339565426 Lambda1 1.7179227e-05\n",
      "16 Train Loss 1336216.2 Test MSE 3511.834884125194 Test RE 0.9975994818083678 Lambda1 1.0070427e-05\n",
      "17 Train Loss 1303328.1 Test MSE 3510.7733419222595 Test RE 0.9974486953924868 Lambda1 -2.1248079e-05\n",
      "18 Train Loss 1274226.4 Test MSE 3510.6983849303447 Test RE 0.9974380472894065 Lambda1 3.120084e-05\n",
      "19 Train Loss 1248157.1 Test MSE 3510.6859068113035 Test RE 0.9974362746845875 Lambda1 9.393503e-06\n",
      "20 Train Loss 1224840.4 Test MSE 3510.3368032137328 Test RE 0.9973866807792935 Lambda1 1.0551843e-05\n",
      "21 Train Loss 1202465.0 Test MSE 3510.26897939424 Test RE 0.9973770453931823 Lambda1 3.626045e-05\n",
      "22 Train Loss 1181483.2 Test MSE 3510.055101642155 Test RE 0.9973466602558748 Lambda1 -9.95517e-08\n",
      "23 Train Loss 1165568.5 Test MSE 3509.9260967002847 Test RE 0.9973283323626633 Lambda1 1.8974137e-05\n",
      "24 Train Loss 1148013.9 Test MSE 3509.884137125793 Test RE 0.9973223710409191 Lambda1 -5.5642486e-06\n",
      "25 Train Loss 1134083.9 Test MSE 3509.959899502868 Test RE 0.9973331348016085 Lambda1 -9.415437e-06\n",
      "26 Train Loss 1121473.5 Test MSE 3509.8520790256075 Test RE 0.9973178164271136 Lambda1 1.7618784e-05\n",
      "27 Train Loss 1106610.6 Test MSE 3510.152952568596 Test RE 0.9973605618345857 Lambda1 2.0681326e-05\n",
      "28 Train Loss 1085204.1 Test MSE 3510.402239596853 Test RE 0.9973959769061995 Lambda1 -4.801143e-06\n",
      "29 Train Loss 1066000.9 Test MSE 3510.7856595453113 Test RE 0.9974504451759492 Lambda1 9.274245e-06\n",
      "30 Train Loss 1047875.5 Test MSE 3510.980094371102 Test RE 0.9974780652640479 Lambda1 -3.66525e-06\n",
      "31 Train Loss 1032739.94 Test MSE 3511.075128124027 Test RE 0.997491564833969 Lambda1 -9.700085e-06\n",
      "32 Train Loss 1019819.94 Test MSE 3511.1626658545597 Test RE 0.9975039994302649 Lambda1 6.372786e-06\n",
      "33 Train Loss 1007699.5 Test MSE 3511.3597364316142 Test RE 0.9975319924270626 Lambda1 1.3512588e-05\n",
      "34 Train Loss 991485.0 Test MSE 3511.745654409867 Test RE 0.9975868080799396 Lambda1 5.269436e-06\n",
      "35 Train Loss 974576.56 Test MSE 3511.93343473851 Test RE 0.9976134792419454 Lambda1 3.092399e-05\n",
      "36 Train Loss 960701.7 Test MSE 3512.0733233699525 Test RE 0.9976333476986293 Lambda1 1.3728578e-05\n",
      "37 Train Loss 948749.3 Test MSE 3512.077661969931 Test RE 0.997633963905964 Lambda1 3.8235095e-05\n",
      "38 Train Loss 934493.3 Test MSE 3512.3308806888676 Test RE 0.9976699276666429 Lambda1 3.1335258e-05\n",
      "39 Train Loss 921195.75 Test MSE 3512.7770630819873 Test RE 0.9977332942222035 Lambda1 2.9789931e-05\n",
      "40 Train Loss 906973.44 Test MSE 3513.0774171285793 Test RE 0.9977759480572561 Lambda1 3.0416724e-05\n",
      "41 Train Loss 894599.7 Test MSE 3513.1217715786106 Test RE 0.9977822467604283 Lambda1 2.9867158e-05\n",
      "42 Train Loss 880443.8 Test MSE 3513.427933143364 Test RE 0.9978257231809105 Lambda1 1.3303463e-05\n",
      "43 Train Loss 870440.3 Test MSE 3513.4952461939215 Test RE 0.9978352817050817 Lambda1 1.5137912e-05\n",
      "44 Train Loss 861437.75 Test MSE 3513.407770065224 Test RE 0.9978228599847282 Lambda1 5.9653667e-07\n",
      "45 Train Loss 849236.56 Test MSE 3513.5746055735863 Test RE 0.9978465507030836 Lambda1 2.3181185e-05\n",
      "46 Train Loss 840953.1 Test MSE 3513.409525866747 Test RE 0.9978231093122742 Lambda1 1.4092683e-05\n",
      "47 Train Loss 829071.06 Test MSE 3513.347547290039 Test RE 0.9978143081849923 Lambda1 -1.0833105e-05\n",
      "48 Train Loss 821796.06 Test MSE 3513.3762280464566 Test RE 0.9978183809404387 Lambda1 2.309545e-05\n",
      "49 Train Loss 814214.5 Test MSE 3513.485195495834 Test RE 0.9978338545011687 Lambda1 -3.6092185e-06\n",
      "50 Train Loss 803830.7 Test MSE 3513.6065229795704 Test RE 0.99785108292521 Lambda1 -1.1714174e-05\n",
      "51 Train Loss 796429.75 Test MSE 3513.5627942917213 Test RE 0.9978448735141668 Lambda1 2.2276095e-05\n",
      "52 Train Loss 787037.7 Test MSE 3513.5766020140372 Test RE 0.9978468341951334 Lambda1 -6.4716232e-06\n",
      "53 Train Loss 773159.5 Test MSE 3513.703056983015 Test RE 0.9978647904786352 Lambda1 1.815434e-05\n",
      "54 Train Loss 764668.75 Test MSE 3513.8855854616595 Test RE 0.9978907084878963 Lambda1 3.5463116e-05\n",
      "55 Train Loss 754662.2 Test MSE 3513.939668060954 Test RE 0.9978983877811397 Lambda1 1.5390715e-05\n",
      "56 Train Loss 743608.94 Test MSE 3514.0338614939146 Test RE 0.9979117623484168 Lambda1 1.0789759e-05\n",
      "57 Train Loss 731673.0 Test MSE 3514.165128857714 Test RE 0.9979304007605969 Lambda1 -2.0110805e-05\n",
      "58 Train Loss 724873.2 Test MSE 3514.396900967405 Test RE 0.9979633088073603 Lambda1 1.4964478e-06\n",
      "59 Train Loss 717768.75 Test MSE 3514.4547855611004 Test RE 0.9979715273530195 Lambda1 -4.204091e-06\n",
      "60 Train Loss 710046.5 Test MSE 3514.5311226174144 Test RE 0.9979823657048622 Lambda1 -1.276121e-05\n",
      "61 Train Loss 701160.9 Test MSE 3514.4208820250424 Test RE 0.9979667136838549 Lambda1 -4.8023576e-06\n",
      "62 Train Loss 693048.3 Test MSE 3514.497463768242 Test RE 0.9979775868286512 Lambda1 -1.8225055e-05\n",
      "63 Train Loss 685104.25 Test MSE 3514.2493118264033 Test RE 0.9979423535626841 Lambda1 1.047353e-05\n",
      "64 Train Loss 677733.8 Test MSE 3513.9811697934315 Test RE 0.9979042806526116 Lambda1 3.2982775e-06\n",
      "65 Train Loss 671748.4 Test MSE 3513.7633219244003 Test RE 0.9978733478331241 Lambda1 -1.6179889e-05\n",
      "66 Train Loss 663050.9 Test MSE 3513.3781631270044 Test RE 0.9978186557272178 Lambda1 8.13807e-06\n",
      "67 Train Loss 657177.1 Test MSE 3513.210146939972 Test RE 0.997794796682948 Lambda1 1.8110339e-05\n",
      "68 Train Loss 652523.44 Test MSE 3513.0974359685943 Test RE 0.9977787909050285 Lambda1 3.7269998e-05\n",
      "69 Train Loss 646504.0 Test MSE 3513.1273801740804 Test RE 0.9977830432250999 Lambda1 1.9119043e-05\n",
      "70 Train Loss 638700.75 Test MSE 3513.2464782432726 Test RE 0.9977999559376114 Lambda1 6.1018472e-05\n",
      "71 Train Loss 630519.1 Test MSE 3513.0155220033644 Test RE 0.9977671583649255 Lambda1 -3.302186e-06\n",
      "72 Train Loss 624078.4 Test MSE 3512.8753352844224 Test RE 0.9977472502405853 Lambda1 1.990615e-05\n",
      "73 Train Loss 617183.3 Test MSE 3512.706314672639 Test RE 0.9977232468440649 Lambda1 2.5335055e-05\n",
      "74 Train Loss 609541.7 Test MSE 3512.3162794127525 Test RE 0.997667853934167 Lambda1 -5.51615e-06\n",
      "Training time: 116.88\n",
      "Training time: 116.88\n",
      "inv_HT_atanh_tune17\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.97\n",
      "Training time: 3.97\n",
      "inv_HT_atanh_tune17\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.89\n",
      "Training time: 3.89\n",
      "inv_HT_atanh_tune17\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.79\n",
      "Training time: 3.79\n",
      "inv_HT_atanh_tune17\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 419747.44 Test MSE 3517.620241987518 Test RE 0.9984208608292836 Lambda1 3.863887e-06\n",
      "1 Train Loss 400416.4 Test MSE 3517.543020319058 Test RE 0.9984099016946216 Lambda1 -7.822706e-06\n",
      "2 Train Loss 386189.22 Test MSE 3517.552943375663 Test RE 0.9984113099604088 Lambda1 3.8731396e-06\n",
      "3 Train Loss 369161.72 Test MSE 3517.562846011529 Test RE 0.9984127153261345 Lambda1 4.9962135e-05\n",
      "4 Train Loss 360728.4 Test MSE 3517.846991998711 Test RE 0.9984530400125861 Lambda1 7.2241644e-05\n",
      "5 Train Loss 354041.88 Test MSE 3517.7621129279746 Test RE 0.9984409945373836 Lambda1 6.056409e-05\n",
      "6 Train Loss 348551.53 Test MSE 3517.7338470241853 Test RE 0.9984369831953886 Lambda1 6.38714e-05\n",
      "7 Train Loss 339180.12 Test MSE 3517.418811297852 Test RE 0.998392273962607 Lambda1 5.2765372e-05\n",
      "8 Train Loss 326669.7 Test MSE 3516.8230905298874 Test RE 0.9983077250037063 Lambda1 1.825652e-05\n",
      "9 Train Loss 320210.4 Test MSE 3516.7896967244046 Test RE 0.9983029853036613 Lambda1 2.8217846e-06\n",
      "10 Train Loss 310451.94 Test MSE 3517.4421872895673 Test RE 0.9983955915048168 Lambda1 2.9923858e-05\n",
      "11 Train Loss 302802.03 Test MSE 3517.8958626628437 Test RE 0.9984599753474395 Lambda1 4.046835e-05\n",
      "12 Train Loss 297522.34 Test MSE 3517.905352164584 Test RE 0.9984613220162514 Lambda1 4.6527948e-05\n",
      "13 Train Loss 293776.06 Test MSE 3517.62152974704 Test RE 0.9984210435843514 Lambda1 2.266327e-06\n",
      "14 Train Loss 289503.9 Test MSE 3516.9737231827094 Test RE 0.9983291045452513 Lambda1 -3.6698362e-05\n",
      "15 Train Loss 286708.38 Test MSE 3516.729809539193 Test RE 0.9982944852488592 Lambda1 -3.784556e-05\n",
      "16 Train Loss 283682.8 Test MSE 3516.6417087529517 Test RE 0.9982819805805093 Lambda1 -2.7814289e-05\n",
      "17 Train Loss 281116.12 Test MSE 3516.3934645821155 Test RE 0.9982467449670108 Lambda1 -1.2408445e-05\n",
      "18 Train Loss 278629.47 Test MSE 3516.025619193901 Test RE 0.998194530948766 Lambda1 -2.4824541e-05\n",
      "19 Train Loss 276671.5 Test MSE 3515.9374367925034 Test RE 0.998182013442887 Lambda1 -3.5998448e-05\n",
      "20 Train Loss 274729.12 Test MSE 3515.8642101307005 Test RE 0.9981716187873202 Lambda1 -6.742505e-06\n",
      "21 Train Loss 272793.38 Test MSE 3515.8551803549767 Test RE 0.9981703369870144 Lambda1 2.6542288e-05\n",
      "22 Train Loss 271287.3 Test MSE 3515.846598222027 Test RE 0.9981691187292469 Lambda1 4.7154997e-05\n",
      "23 Train Loss 268558.66 Test MSE 3515.714238101916 Test RE 0.9981503296518186 Lambda1 5.5849676e-05\n",
      "24 Train Loss 263645.22 Test MSE 3515.863129390364 Test RE 0.9981714653734867 Lambda1 5.7141965e-06\n",
      "25 Train Loss 261640.1 Test MSE 3515.8021109633833 Test RE 0.9981628036146669 Lambda1 2.7402491e-05\n",
      "26 Train Loss 259122.83 Test MSE 3515.7559905393355 Test RE 0.9981562566246356 Lambda1 2.9658943e-06\n",
      "27 Train Loss 253395.97 Test MSE 3516.2327112813064 Test RE 0.9982239270862847 Lambda1 3.0247807e-05\n",
      "28 Train Loss 248818.36 Test MSE 3516.0801078572226 Test RE 0.9982022655447124 Lambda1 2.5748645e-06\n",
      "29 Train Loss 245407.81 Test MSE 3515.990161471517 Test RE 0.9981894977383132 Lambda1 -3.3848e-05\n",
      "30 Train Loss 241703.97 Test MSE 3515.6227312441697 Test RE 0.9981373396605424 Lambda1 2.5657173e-05\n",
      "31 Train Loss 238431.89 Test MSE 3515.247717463785 Test RE 0.9980841022587578 Lambda1 -5.5805867e-06\n",
      "32 Train Loss 235695.45 Test MSE 3515.390426100989 Test RE 0.9981043616811242 Lambda1 -1.7712982e-05\n",
      "33 Train Loss 232644.16 Test MSE 3514.73516413797 Test RE 0.9980113349865183 Lambda1 4.9814585e-06\n",
      "34 Train Loss 229929.22 Test MSE 3514.361287239276 Test RE 0.9979582522805924 Lambda1 -2.372133e-05\n",
      "35 Train Loss 228016.55 Test MSE 3514.0927959310616 Test RE 0.9979201303840951 Lambda1 -2.8679528e-06\n",
      "36 Train Loss 225743.2 Test MSE 3513.964819417453 Test RE 0.9979019590509337 Lambda1 -1.2188642e-06\n",
      "37 Train Loss 223558.08 Test MSE 3513.5285294909118 Test RE 0.9978400079345274 Lambda1 9.5116075e-06\n",
      "38 Train Loss 222622.06 Test MSE 3513.366544418104 Test RE 0.9978170058373479 Lambda1 2.4288112e-05\n",
      "39 Train Loss 221217.03 Test MSE 3513.3023741706847 Test RE 0.9978078934296744 Lambda1 1.7616994e-05\n",
      "40 Train Loss 219995.94 Test MSE 3513.32138160953 Test RE 0.9978105925635438 Lambda1 -1.6065118e-05\n",
      "41 Train Loss 219134.34 Test MSE 3513.3477379545093 Test RE 0.9978143352599872 Lambda1 -3.7059384e-05\n",
      "42 Train Loss 217840.77 Test MSE 3513.14997439455 Test RE 0.9977862517756836 Lambda1 -1.3659773e-05\n",
      "43 Train Loss 216941.42 Test MSE 3512.9618721728943 Test RE 0.9977595395196717 Lambda1 -9.1479023e-07\n",
      "44 Train Loss 216163.16 Test MSE 3512.760527582173 Test RE 0.9977309459322581 Lambda1 6.187125e-06\n",
      "45 Train Loss 215451.17 Test MSE 3512.646663586547 Test RE 0.9977147753803801 Lambda1 2.3765901e-06\n",
      "46 Train Loss 214557.48 Test MSE 3512.5679529195695 Test RE 0.9977035970235917 Lambda1 -1.2549051e-06\n",
      "47 Train Loss 213965.89 Test MSE 3512.572148246323 Test RE 0.9977041928400142 Lambda1 1.8191224e-05\n",
      "48 Train Loss 213494.52 Test MSE 3512.58379931808 Test RE 0.9977058475125751 Lambda1 1.9623736e-05\n",
      "49 Train Loss 212577.86 Test MSE 3512.714913341365 Test RE 0.9977244679946055 Lambda1 1.3898258e-05\n",
      "50 Train Loss 211809.05 Test MSE 3512.79112880511 Test RE 0.9977352917623217 Lambda1 -1.0899941e-05\n",
      "51 Train Loss 211089.14 Test MSE 3512.789459580025 Test RE 0.9977350547079519 Lambda1 -2.672038e-05\n",
      "52 Train Loss 210170.66 Test MSE 3512.9186135359905 Test RE 0.9977533962918086 Lambda1 -2.185036e-05\n",
      "53 Train Loss 209425.52 Test MSE 3512.920298993284 Test RE 0.9977536356469892 Lambda1 -3.6858617e-06\n",
      "54 Train Loss 208553.66 Test MSE 3512.7965558241895 Test RE 0.997736062477997 Lambda1 1.2623415e-05\n",
      "55 Train Loss 207950.3 Test MSE 3512.805300229542 Test RE 0.99773730430954 Lambda1 1.6783604e-05\n",
      "56 Train Loss 207432.12 Test MSE 3512.7851461766036 Test RE 0.9977344421413337 Lambda1 9.79061e-06\n",
      "57 Train Loss 206722.12 Test MSE 3512.58204135599 Test RE 0.9977055978488836 Lambda1 1.6783202e-05\n",
      "58 Train Loss 205968.53 Test MSE 3512.4563409678026 Test RE 0.9976877458668542 Lambda1 9.894845e-06\n",
      "59 Train Loss 205457.73 Test MSE 3512.3117730102017 Test RE 0.9976672139157226 Lambda1 1.5073359e-05\n",
      "60 Train Loss 204808.34 Test MSE 3512.246867641346 Test RE 0.9976579957338162 Lambda1 3.7831232e-06\n",
      "61 Train Loss 203689.23 Test MSE 3512.0864206596025 Test RE 0.9976352078933823 Lambda1 5.316734e-06\n",
      "62 Train Loss 201874.34 Test MSE 3511.877991324676 Test RE 0.997605604474911 Lambda1 6.3106295e-06\n",
      "63 Train Loss 200558.89 Test MSE 3511.795922302963 Test RE 0.9975939478919454 Lambda1 1.5912767e-05\n",
      "64 Train Loss 199246.9 Test MSE 3511.762715373744 Test RE 0.9975892313436807 Lambda1 4.6853434e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Train Loss 198599.61 Test MSE 3511.837977800274 Test RE 0.997599921215128 Lambda1 3.9688435e-05\n",
      "66 Train Loss 198016.14 Test MSE 3511.8946079772963 Test RE 0.9976079645862553 Lambda1 3.921633e-05\n",
      "67 Train Loss 197558.69 Test MSE 3512.0014566187574 Test RE 0.9976231404751797 Lambda1 2.2935614e-05\n",
      "68 Train Loss 196645.94 Test MSE 3512.0117382936737 Test RE 0.9976246007862644 Lambda1 4.4631222e-05\n",
      "69 Train Loss 195836.3 Test MSE 3511.973634371511 Test RE 0.9976191888577064 Lambda1 5.4725784e-05\n",
      "70 Train Loss 194727.17 Test MSE 3512.0000401626835 Test RE 0.9976229392950954 Lambda1 4.9742157e-05\n",
      "71 Train Loss 194003.53 Test MSE 3511.871957644302 Test RE 0.9976047474924138 Lambda1 4.2617456e-05\n",
      "72 Train Loss 193459.12 Test MSE 3511.8005643452534 Test RE 0.9975946072229356 Lambda1 3.7535607e-05\n",
      "73 Train Loss 192540.44 Test MSE 3511.827829631397 Test RE 0.9975984798303246 Lambda1 4.3148546e-05\n",
      "74 Train Loss 191765.2 Test MSE 3511.8457150234653 Test RE 0.9976010201623234 Lambda1 3.1150616e-05\n",
      "Training time: 120.31\n",
      "Training time: 120.31\n",
      "inv_HT_atanh_tune17\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.00\n",
      "Training time: 4.00\n",
      "inv_HT_atanh_tune17\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.89\n",
      "Training time: 3.89\n",
      "inv_HT_atanh_tune17\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.10\n",
      "Training time: 4.10\n",
      "inv_HT_atanh_tune17\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.03\n",
      "Training time: 4.03\n",
      "inv_HT_atanh_tune17\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.03\n",
      "Training time: 4.03\n",
      "inv_HT_atanh_tune18\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.74\n",
      "Training time: 3.74\n",
      "inv_HT_atanh_tune18\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.93\n",
      "Training time: 3.93\n",
      "inv_HT_atanh_tune18\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.11\n",
      "Training time: 4.11\n",
      "inv_HT_atanh_tune18\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.90\n",
      "Training time: 3.90\n",
      "inv_HT_atanh_tune18\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.86\n",
      "Training time: 3.86\n",
      "inv_HT_atanh_tune18\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.16\n",
      "Training time: 4.16\n",
      "inv_HT_atanh_tune18\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.87\n",
      "Training time: 3.87\n",
      "inv_HT_atanh_tune18\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.88\n",
      "Training time: 3.88\n",
      "inv_HT_atanh_tune18\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.94\n",
      "Training time: 3.94\n",
      "inv_HT_atanh_tune18\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.02\n",
      "Training time: 4.02\n",
      "inv_HT_atanh_tune19\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.89\n",
      "Training time: 3.89\n",
      "inv_HT_atanh_tune19\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.92\n",
      "Training time: 3.92\n",
      "inv_HT_atanh_tune19\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.89\n",
      "Training time: 3.89\n",
      "inv_HT_atanh_tune19\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.05\n",
      "Training time: 4.05\n",
      "inv_HT_atanh_tune19\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.92\n",
      "Training time: 3.92\n",
      "inv_HT_atanh_tune19\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.23\n",
      "Training time: 3.23\n",
      "inv_HT_atanh_tune19\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.27\n",
      "Training time: 4.27\n",
      "inv_HT_atanh_tune19\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.17\n",
      "Training time: 4.17\n",
      "inv_HT_atanh_tune19\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.14\n",
      "Training time: 4.14\n",
      "inv_HT_atanh_tune19\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.00\n",
      "Training time: 4.00\n",
      "inv_HT_atanh_tune20\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 854.72076 Test MSE 858.0765867056988 Test RE 0.493119522687532 Lambda1 -0.042431\n",
      "1 Train Loss 854.636 Test MSE 858.0125847312612 Test RE 0.4931011320146564 Lambda1 -0.04498755\n",
      "2 Train Loss 854.3665 Test MSE 857.316128083273 Test RE 0.4929009640416318 Lambda1 -0.4192454\n",
      "3 Train Loss 852.36975 Test MSE 853.8338580800533 Test RE 0.49189890617833576 Lambda1 -0.72825795\n",
      "4 Train Loss 850.02435 Test MSE 850.8721843779534 Test RE 0.4910450459297512 Lambda1 -0.70126384\n",
      "5 Train Loss 846.306 Test MSE 847.5786954316934 Test RE 0.49009377529293735 Lambda1 -0.93933636\n",
      "6 Train Loss 841.6139 Test MSE 844.139483403123 Test RE 0.4890984401457023 Lambda1 -0.82689834\n",
      "7 Train Loss 834.0265 Test MSE 833.6472243629156 Test RE 0.48604930323403167 Lambda1 -0.8231671\n",
      "8 Train Loss 823.68036 Test MSE 822.4856510265811 Test RE 0.48278451913487364 Lambda1 -0.9210337\n",
      "9 Train Loss 812.7743 Test MSE 811.9687943594832 Test RE 0.4796879843349496 Lambda1 -0.8723623\n",
      "10 Train Loss 800.21985 Test MSE 799.031813354766 Test RE 0.47585124087012426 Lambda1 -0.82932687\n",
      "11 Train Loss 786.46796 Test MSE 784.2224647386244 Test RE 0.4714208754955519 Lambda1 -0.88233644\n",
      "12 Train Loss 768.4888 Test MSE 767.6736401191882 Test RE 0.4664203438726997 Lambda1 -0.75128394\n",
      "13 Train Loss 754.7501 Test MSE 753.4359971252774 Test RE 0.4620748737429098 Lambda1 -0.6964733\n",
      "14 Train Loss 746.20526 Test MSE 742.942045083937 Test RE 0.45884567143331606 Lambda1 -0.658892\n",
      "15 Train Loss 736.23517 Test MSE 736.8141336680719 Test RE 0.4569494349358134 Lambda1 -0.6732515\n",
      "16 Train Loss 727.45856 Test MSE 727.5564254502824 Test RE 0.4540696879209671 Lambda1 -0.68028045\n",
      "17 Train Loss 716.4111 Test MSE 719.0142627979319 Test RE 0.45139622557287107 Lambda1 -0.6719473\n",
      "18 Train Loss 712.67957 Test MSE 715.421505892119 Test RE 0.45026704917185234 Lambda1 -0.6320934\n",
      "19 Train Loss 707.7596 Test MSE 711.8277967069728 Test RE 0.4491347334774057 Lambda1 -0.5747063\n",
      "20 Train Loss 704.3445 Test MSE 709.9737050780313 Test RE 0.44854942343719756 Lambda1 -0.5426635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Train Loss 699.6251 Test MSE 704.665450573608 Test RE 0.4468694444999831 Lambda1 -0.52576834\n",
      "22 Train Loss 697.3739 Test MSE 701.6272677334109 Test RE 0.44590505945135284 Lambda1 -0.5251955\n",
      "23 Train Loss 694.3079 Test MSE 698.7487371878565 Test RE 0.44498942333596636 Lambda1 -0.52932507\n",
      "24 Train Loss 693.4345 Test MSE 697.5140247184053 Test RE 0.4445960938798477 Lambda1 -0.5396099\n",
      "25 Train Loss 690.1413 Test MSE 698.3951328287467 Test RE 0.4448768148234755 Lambda1 -0.55050653\n",
      "26 Train Loss 688.5174 Test MSE 697.3646756224404 Test RE 0.4445484937047621 Lambda1 -0.5470732\n",
      "27 Train Loss 687.05054 Test MSE 695.4239865871751 Test RE 0.443929498023534 Lambda1 -0.54548764\n",
      "28 Train Loss 686.4689 Test MSE 695.3111914221931 Test RE 0.443893494712951 Lambda1 -0.5438446\n",
      "29 Train Loss 684.6202 Test MSE 693.45320805252 Test RE 0.44330002056596696 Lambda1 -0.5358257\n",
      "30 Train Loss 683.3636 Test MSE 694.1555946693948 Test RE 0.4435244691611974 Lambda1 -0.53998405\n",
      "31 Train Loss 682.32263 Test MSE 694.3885770857488 Test RE 0.44359889392578 Lambda1 -0.52656204\n",
      "32 Train Loss 679.68585 Test MSE 691.983098147401 Test RE 0.44282987672027163 Lambda1 -0.5024402\n",
      "33 Train Loss 678.2214 Test MSE 690.3476291466574 Test RE 0.44230626352235597 Lambda1 -0.46379337\n",
      "34 Train Loss 678.13293 Test MSE 690.0648566703675 Test RE 0.44221566797135414 Lambda1 -0.45672458\n",
      "35 Train Loss 677.07904 Test MSE 688.0953227723785 Test RE 0.4415841468285249 Lambda1 -0.44415143\n",
      "36 Train Loss 676.8771 Test MSE 687.8976896284386 Test RE 0.4415207268867272 Lambda1 -0.4453028\n",
      "37 Train Loss 676.7405 Test MSE 687.8370907371085 Test RE 0.4415012790413398 Lambda1 -0.45999122\n",
      "38 Train Loss 676.11255 Test MSE 688.797121353911 Test RE 0.44180927853571306 Lambda1 -0.48851633\n",
      "39 Train Loss 675.5701 Test MSE 687.8422426825125 Test RE 0.44150293247516653 Lambda1 -0.49452034\n",
      "40 Train Loss 674.42676 Test MSE 687.251043928994 Test RE 0.4413131563339804 Lambda1 -0.48412558\n",
      "41 Train Loss 672.70215 Test MSE 687.2930288695056 Test RE 0.4413266362870529 Lambda1 -0.4944037\n",
      "42 Train Loss 672.396 Test MSE 687.1159467549784 Test RE 0.44126977837837966 Lambda1 -0.48954925\n",
      "43 Train Loss 671.9985 Test MSE 686.2585271952007 Test RE 0.4409943725702861 Lambda1 -0.4958368\n",
      "44 Train Loss 670.70807 Test MSE 685.126054014353 Test RE 0.4406303548675617 Lambda1 -0.48509353\n",
      "45 Train Loss 669.6953 Test MSE 684.68228549974 Test RE 0.44048762963434424 Lambda1 -0.4688256\n",
      "46 Train Loss 669.3005 Test MSE 684.1853919647953 Test RE 0.44032776338288215 Lambda1 -0.46366498\n",
      "47 Train Loss 668.76624 Test MSE 683.4850742094148 Test RE 0.4401023505744456 Lambda1 -0.4766022\n",
      "48 Train Loss 667.8225 Test MSE 683.2817396625146 Test RE 0.440036881208685 Lambda1 -0.48348692\n",
      "49 Train Loss 666.9686 Test MSE 683.4931836401139 Test RE 0.440104961435356 Lambda1 -0.49364206\n",
      "50 Train Loss 665.5954 Test MSE 682.8884560523974 Test RE 0.43991022466809915 Lambda1 -0.5043161\n",
      "51 Train Loss 665.00037 Test MSE 683.1650024196684 Test RE 0.4399992899168893 Lambda1 -0.52046174\n",
      "52 Train Loss 664.4631 Test MSE 682.5802368006058 Test RE 0.43981093751370814 Lambda1 -0.54699576\n",
      "53 Train Loss 664.1994 Test MSE 682.2059147085135 Test RE 0.4396903263985775 Lambda1 -0.56120706\n",
      "54 Train Loss 663.9572 Test MSE 681.8302120457383 Test RE 0.4395692371696747 Lambda1 -0.5698116\n",
      "55 Train Loss 663.68365 Test MSE 681.6534168177456 Test RE 0.4395122444019135 Lambda1 -0.5625728\n",
      "56 Train Loss 663.566 Test MSE 681.3505400278437 Test RE 0.43941459004804095 Lambda1 -0.5717992\n",
      "57 Train Loss 662.5718 Test MSE 680.1735467735732 Test RE 0.4390348944845544 Lambda1 -0.62349623\n",
      "58 Train Loss 660.8857 Test MSE 677.6818579638762 Test RE 0.4382299954717668 Lambda1 -0.6561795\n",
      "59 Train Loss 660.05414 Test MSE 676.3547569451323 Test RE 0.43780069338125316 Lambda1 -0.6750718\n",
      "60 Train Loss 659.5864 Test MSE 675.350979189962 Test RE 0.4374757028623064 Lambda1 -0.6949701\n",
      "61 Train Loss 658.72925 Test MSE 675.0906296761278 Test RE 0.4373913707379472 Lambda1 -0.69164824\n",
      "62 Train Loss 658.2432 Test MSE 674.5489668186412 Test RE 0.4372158637825584 Lambda1 -0.69947755\n",
      "63 Train Loss 657.9461 Test MSE 674.6045634035916 Test RE 0.43723388116103357 Lambda1 -0.6842842\n",
      "64 Train Loss 657.7696 Test MSE 674.3803110947791 Test RE 0.43716120239296036 Lambda1 -0.68059105\n",
      "65 Train Loss 657.5397 Test MSE 674.1793089862412 Test RE 0.43709604859878876 Lambda1 -0.6734812\n",
      "66 Train Loss 657.0882 Test MSE 672.6191154832406 Test RE 0.4365899892982471 Lambda1 -0.70356935\n",
      "67 Train Loss 656.7794 Test MSE 671.6031574887118 Test RE 0.43626014087501996 Lambda1 -0.7307555\n",
      "68 Train Loss 656.6026 Test MSE 671.7643090865344 Test RE 0.43631247818248864 Lambda1 -0.73241717\n",
      "69 Train Loss 656.5016 Test MSE 671.7834750432424 Test RE 0.436318702304781 Lambda1 -0.7282473\n",
      "70 Train Loss 656.27966 Test MSE 671.6277021923063 Test RE 0.43626811267942306 Lambda1 -0.7215739\n",
      "71 Train Loss 655.60474 Test MSE 671.721744344743 Test RE 0.4362986550142155 Lambda1 -0.7278137\n",
      "72 Train Loss 655.06805 Test MSE 670.5607849873061 Test RE 0.4359214571187743 Lambda1 -0.7631103\n",
      "73 Train Loss 654.49786 Test MSE 669.2401589322415 Test RE 0.43549198601618716 Lambda1 -0.78283215\n",
      "74 Train Loss 654.0151 Test MSE 668.654639089072 Test RE 0.43530143783028674 Lambda1 -0.7866735\n",
      "Training time: 131.59\n",
      "Training time: 131.59\n",
      "inv_HT_atanh_tune20\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "1 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "2 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "3 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "4 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "5 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "6 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "7 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "8 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "9 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "10 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "11 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "12 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "13 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "14 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "15 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "16 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "17 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "18 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "19 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "20 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "21 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "22 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "24 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "25 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "26 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "27 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "28 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "29 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "30 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "31 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "32 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "33 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "34 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "35 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "36 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "37 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "38 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "39 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "40 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "41 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "42 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "43 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "44 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "45 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "46 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "47 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "48 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "49 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "50 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "51 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "52 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "53 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "54 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "55 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "56 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "57 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "58 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "59 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "60 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "61 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "62 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "63 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "64 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "65 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "66 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "67 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "68 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "69 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "70 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "71 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "72 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "73 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "74 Train Loss 854.7217 Test MSE 858.0789094874883 Test RE 0.4931201901152942 Lambda1 -0.0732759\n",
      "Training time: 122.19\n",
      "Training time: 122.19\n",
      "inv_HT_atanh_tune20\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 854.72046 Test MSE 858.0801691103727 Test RE 0.4931205520547623 Lambda1 -0.32505196\n",
      "1 Train Loss 854.4779 Test MSE 857.5868661111678 Test RE 0.49297878626886793 Lambda1 -0.3266264\n",
      "2 Train Loss 852.60254 Test MSE 854.6742832388348 Test RE 0.4921409336725812 Lambda1 -0.7914655\n",
      "3 Train Loss 848.08905 Test MSE 846.9970730556897 Test RE 0.48992559124950746 Lambda1 -0.9217253\n",
      "4 Train Loss 839.53937 Test MSE 841.6490677620642 Test RE 0.48837642787833885 Lambda1 -1.1206646\n",
      "5 Train Loss 831.55164 Test MSE 832.7004091833309 Test RE 0.4857732094698516 Lambda1 -1.1362059\n",
      "6 Train Loss 823.2916 Test MSE 824.2740943594107 Test RE 0.48330912639879353 Lambda1 -1.0449058\n",
      "7 Train Loss 811.46747 Test MSE 809.7321043808594 Test RE 0.47902684242314203 Lambda1 -1.1060244\n",
      "8 Train Loss 806.25793 Test MSE 805.033816605036 Test RE 0.4776350980981751 Lambda1 -1.1261256\n",
      "9 Train Loss 795.5488 Test MSE 794.9070977103551 Test RE 0.4746214458959453 Lambda1 -1.1943456\n",
      "10 Train Loss 786.2737 Test MSE 782.7478780504339 Test RE 0.4709774566556188 Lambda1 -1.1988354\n",
      "11 Train Loss 781.1986 Test MSE 775.1255540897672 Test RE 0.46867867993766343 Lambda1 -1.2246749\n",
      "12 Train Loss 773.1475 Test MSE 766.6298412575865 Test RE 0.4661031422637337 Lambda1 -1.2317812\n",
      "13 Train Loss 766.2028 Test MSE 762.9698575672353 Test RE 0.4649891947231093 Lambda1 -1.248558\n",
      "14 Train Loss 761.4069 Test MSE 759.0481522290667 Test RE 0.46379262062693477 Lambda1 -1.2426623\n",
      "15 Train Loss 758.4519 Test MSE 756.4515652401711 Test RE 0.4629986594280015 Lambda1 -1.242811\n",
      "16 Train Loss 756.6331 Test MSE 753.8260264411049 Test RE 0.4621944588368831 Lambda1 -1.2518244\n",
      "17 Train Loss 755.2417 Test MSE 749.6976034937427 Test RE 0.4609270882130995 Lambda1 -1.295457\n",
      "18 Train Loss 750.88995 Test MSE 748.1218253779878 Test RE 0.4604424255461401 Lambda1 -1.332457\n",
      "19 Train Loss 747.663 Test MSE 745.1113790928788 Test RE 0.45951508028562116 Lambda1 -1.3248771\n",
      "20 Train Loss 746.4697 Test MSE 741.6021324352138 Test RE 0.45843171547227823 Lambda1 -1.3066623\n",
      "21 Train Loss 744.9855 Test MSE 742.5511129686112 Test RE 0.4587249344840415 Lambda1 -1.2978349\n",
      "22 Train Loss 742.4821 Test MSE 739.4632278583233 Test RE 0.4577701412369379 Lambda1 -1.292609\n",
      "23 Train Loss 738.7826 Test MSE 736.1631741756756 Test RE 0.4567475378178261 Lambda1 -1.297783\n",
      "24 Train Loss 734.87317 Test MSE 732.5336595575097 Test RE 0.45562019247533764 Lambda1 -1.3025702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 Train Loss 725.3255 Test MSE 722.8969905536018 Test RE 0.4526133703535011 Lambda1 -1.3365006\n",
      "26 Train Loss 720.414 Test MSE 715.9370919011337 Test RE 0.45042926793214394 Lambda1 -1.3497963\n",
      "27 Train Loss 709.917 Test MSE 706.1981563541624 Test RE 0.4473551695667959 Lambda1 -1.3240745\n",
      "28 Train Loss 706.7141 Test MSE 703.5941066788713 Test RE 0.44652961447328393 Lambda1 -1.3099027\n",
      "29 Train Loss 705.9422 Test MSE 702.2403534560833 Test RE 0.44609983404497433 Lambda1 -1.3194213\n",
      "30 Train Loss 705.0323 Test MSE 700.5522901515069 Test RE 0.4455633383507865 Lambda1 -1.3213506\n",
      "31 Train Loss 703.57965 Test MSE 697.1957698474433 Test RE 0.4444946543308709 Lambda1 -1.3243779\n",
      "32 Train Loss 702.873 Test MSE 694.1750337216957 Test RE 0.44353067932146956 Lambda1 -1.3262285\n",
      "33 Train Loss 700.3606 Test MSE 692.4805678315513 Test RE 0.4429890242927049 Lambda1 -1.3399825\n",
      "34 Train Loss 696.3517 Test MSE 690.9350242121502 Test RE 0.44249439574983107 Lambda1 -1.3491666\n",
      "35 Train Loss 693.5251 Test MSE 687.9896079562026 Test RE 0.44155022436278096 Lambda1 -1.355615\n",
      "36 Train Loss 691.21924 Test MSE 684.8143835920852 Test RE 0.44053011997657837 Lambda1 -1.3565348\n",
      "37 Train Loss 689.1113 Test MSE 682.4392644867352 Test RE 0.4397655184085484 Lambda1 -1.3690029\n",
      "38 Train Loss 687.6626 Test MSE 680.5011553722035 Test RE 0.4391406133059504 Lambda1 -1.3778023\n",
      "39 Train Loss 686.76996 Test MSE 681.2337126009984 Test RE 0.4393769164324733 Lambda1 -1.3844266\n",
      "40 Train Loss 686.50916 Test MSE 680.061091595887 Test RE 0.43899859949177783 Lambda1 -1.3844796\n",
      "41 Train Loss 684.7962 Test MSE 679.1292009699351 Test RE 0.4386977155504287 Lambda1 -1.3688277\n",
      "42 Train Loss 682.36365 Test MSE 675.5641944841485 Test RE 0.43754475521629066 Lambda1 -1.3455305\n",
      "43 Train Loss 681.24133 Test MSE 674.6763707069971 Test RE 0.4372571509046881 Lambda1 -1.3034766\n",
      "44 Train Loss 680.90924 Test MSE 673.250833053497 Test RE 0.43679496179584687 Lambda1 -1.2829254\n",
      "45 Train Loss 680.37195 Test MSE 671.8256558009186 Test RE 0.43633240014437297 Lambda1 -1.286335\n",
      "46 Train Loss 677.71075 Test MSE 671.0289843420474 Test RE 0.4360736152344583 Lambda1 -1.3105265\n",
      "47 Train Loss 676.3574 Test MSE 670.9231989415579 Test RE 0.4360392411304198 Lambda1 -1.3349946\n",
      "48 Train Loss 675.3558 Test MSE 669.446527739528 Test RE 0.43555912561720483 Lambda1 -1.3238312\n",
      "49 Train Loss 675.1367 Test MSE 668.50277249814 Test RE 0.4352520016224198 Lambda1 -1.311308\n",
      "50 Train Loss 674.6482 Test MSE 667.3980483021584 Test RE 0.4348922183499004 Lambda1 -1.3233943\n",
      "51 Train Loss 673.7007 Test MSE 665.1103832486114 Test RE 0.4341462304419266 Lambda1 -1.3129294\n",
      "52 Train Loss 672.4916 Test MSE 661.9263432927497 Test RE 0.43310580365967827 Lambda1 -1.28603\n",
      "53 Train Loss 670.3262 Test MSE 658.1009935734141 Test RE 0.43185250576175965 Lambda1 -1.2798773\n",
      "54 Train Loss 667.0608 Test MSE 651.2332780344458 Test RE 0.42959326405408454 Lambda1 -1.3003869\n",
      "55 Train Loss 665.84644 Test MSE 649.0636298469072 Test RE 0.4288770507616683 Lambda1 -1.3047619\n",
      "56 Train Loss 662.4261 Test MSE 641.1735002118571 Test RE 0.42626232844367407 Lambda1 -1.3394812\n",
      "57 Train Loss 660.2583 Test MSE 640.013752217844 Test RE 0.4258766445145357 Lambda1 -1.3520435\n",
      "58 Train Loss 658.79517 Test MSE 639.2882186053338 Test RE 0.4256351845163574 Lambda1 -1.3463886\n",
      "59 Train Loss 656.09033 Test MSE 639.2243464055135 Test RE 0.4256139210753514 Lambda1 -1.3471645\n",
      "60 Train Loss 647.3511 Test MSE 638.1269473222318 Test RE 0.425248424232541 Lambda1 -1.3951827\n",
      "61 Train Loss 641.3761 Test MSE 636.4061420261296 Test RE 0.4246746639942846 Lambda1 -1.425207\n",
      "62 Train Loss 639.569 Test MSE 634.7697562003972 Test RE 0.4241283313554271 Lambda1 -1.4431546\n",
      "63 Train Loss 638.40436 Test MSE 634.0118089948697 Test RE 0.42387504037892715 Lambda1 -1.4786665\n",
      "64 Train Loss 637.20953 Test MSE 631.2968850497119 Test RE 0.42296652173373056 Lambda1 -1.508893\n",
      "65 Train Loss 634.8781 Test MSE 627.7651315729047 Test RE 0.42178173162403443 Lambda1 -1.5408887\n",
      "66 Train Loss 631.27747 Test MSE 625.4313614831902 Test RE 0.42099699694549836 Lambda1 -1.5793421\n",
      "67 Train Loss 628.2916 Test MSE 622.0378385804667 Test RE 0.419853301364098 Lambda1 -1.6228006\n",
      "68 Train Loss 626.17145 Test MSE 621.1372793196123 Test RE 0.4195492686311868 Lambda1 -1.6202403\n",
      "69 Train Loss 624.04474 Test MSE 620.4395590343662 Test RE 0.4193135636620063 Lambda1 -1.6161232\n",
      "70 Train Loss 621.0872 Test MSE 615.4136326910765 Test RE 0.4176117666827516 Lambda1 -1.6282988\n",
      "71 Train Loss 617.0179 Test MSE 614.3505571369828 Test RE 0.4172509160895124 Lambda1 -1.6374367\n",
      "72 Train Loss 615.2062 Test MSE 613.6441142180568 Test RE 0.41701094825858104 Lambda1 -1.6317405\n",
      "73 Train Loss 614.91187 Test MSE 614.4020934137257 Test RE 0.4172684167717226 Lambda1 -1.6290216\n",
      "74 Train Loss 614.29285 Test MSE 613.7983232638072 Test RE 0.41706334248785026 Lambda1 -1.6467124\n",
      "Training time: 133.00\n",
      "Training time: 133.00\n",
      "inv_HT_atanh_tune20\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 854.7214 Test MSE 858.0792057218969 Test RE 0.4931202752351787 Lambda1 -0.04329074\n",
      "1 Train Loss 854.7056 Test MSE 858.0444043270916 Test RE 0.49311027531484614 Lambda1 -0.043259878\n",
      "2 Train Loss 854.4153 Test MSE 857.5232471905811 Test RE 0.4929605004478234 Lambda1 -0.04945102\n",
      "3 Train Loss 852.1506 Test MSE 854.9504960189523 Test RE 0.49222045205581505 Lambda1 -0.6314043\n",
      "4 Train Loss 847.37384 Test MSE 850.097713700367 Test RE 0.49082151848705124 Lambda1 -0.58599997\n",
      "5 Train Loss 837.55316 Test MSE 839.4916328417343 Test RE 0.4877500881856751 Lambda1 -0.77343076\n",
      "6 Train Loss 828.1681 Test MSE 826.7300397752758 Test RE 0.4840286067248543 Lambda1 -0.7367684\n",
      "7 Train Loss 813.43915 Test MSE 813.2352236721811 Test RE 0.4800619237257749 Lambda1 -0.881061\n",
      "8 Train Loss 809.2164 Test MSE 807.6584576145261 Test RE 0.478413078172925 Lambda1 -0.9305973\n",
      "9 Train Loss 800.0844 Test MSE 798.3157179441752 Test RE 0.47563796321039403 Lambda1 -0.98000133\n",
      "10 Train Loss 795.9755 Test MSE 794.0503154946011 Test RE 0.4743655940804165 Lambda1 -1.048526\n",
      "11 Train Loss 789.6731 Test MSE 790.6369704373384 Test RE 0.47334492999928707 Lambda1 -1.1089985\n",
      "12 Train Loss 783.5196 Test MSE 781.6214457589326 Test RE 0.4706384488895057 Lambda1 -1.1519496\n",
      "13 Train Loss 779.46954 Test MSE 779.9112580455101 Test RE 0.47012328848235 Lambda1 -1.2190646\n",
      "14 Train Loss 774.21906 Test MSE 775.5488602125006 Test RE 0.4688066382243557 Lambda1 -1.2199636\n",
      "15 Train Loss 770.677 Test MSE 770.6785466562101 Test RE 0.46733230738348097 Lambda1 -1.2678614\n",
      "16 Train Loss 766.2673 Test MSE 768.3385828961318 Test RE 0.46662230190000503 Lambda1 -1.3308195\n",
      "17 Train Loss 763.87366 Test MSE 767.1722740839593 Test RE 0.4662680099249741 Lambda1 -1.34629\n",
      "18 Train Loss 760.9008 Test MSE 764.3273376920461 Test RE 0.46540266654552676 Lambda1 -1.3983626\n",
      "19 Train Loss 756.9101 Test MSE 759.2750415596605 Test RE 0.46386193226050626 Lambda1 -1.4584566\n",
      "20 Train Loss 753.991 Test MSE 755.1578899684312 Test RE 0.4626025823681703 Lambda1 -1.533477\n",
      "21 Train Loss 751.2711 Test MSE 751.0436937066314 Test RE 0.4613407024378389 Lambda1 -1.6122798\n",
      "22 Train Loss 748.05725 Test MSE 749.4503356408668 Test RE 0.4608510696623194 Lambda1 -1.6590532\n",
      "23 Train Loss 747.0024 Test MSE 748.269987200181 Test RE 0.46048801745988127 Lambda1 -1.6908585\n",
      "24 Train Loss 744.34937 Test MSE 747.3524114607138 Test RE 0.4602055911606349 Lambda1 -1.7167228\n",
      "25 Train Loss 742.7533 Test MSE 746.8709894235812 Test RE 0.46005734195399256 Lambda1 -1.7113028\n",
      "26 Train Loss 741.72687 Test MSE 744.5275620105058 Test RE 0.4593350230944821 Lambda1 -1.7473062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 Train Loss 739.7399 Test MSE 743.5339932979708 Test RE 0.4590284305033785 Lambda1 -1.7532113\n",
      "28 Train Loss 738.5111 Test MSE 742.7372007816485 Test RE 0.4587824105088526 Lambda1 -1.7584941\n",
      "29 Train Loss 737.5091 Test MSE 743.6103093775498 Test RE 0.4590519871609721 Lambda1 -1.7439526\n",
      "30 Train Loss 736.4081 Test MSE 741.8345445703234 Test RE 0.45850354424868645 Lambda1 -1.7493558\n",
      "31 Train Loss 736.1108 Test MSE 741.0672826398021 Test RE 0.45826637321993363 Lambda1 -1.7527635\n",
      "32 Train Loss 734.7381 Test MSE 739.6845490188457 Test RE 0.45783864135661817 Lambda1 -1.7755692\n",
      "33 Train Loss 733.68445 Test MSE 738.1409626881903 Test RE 0.4573606791293324 Lambda1 -1.8079587\n",
      "34 Train Loss 731.92554 Test MSE 735.2974453318899 Test RE 0.4564788909650864 Lambda1 -1.822277\n",
      "35 Train Loss 730.82184 Test MSE 735.3003634590456 Test RE 0.45647979676323286 Lambda1 -1.7891372\n",
      "36 Train Loss 729.9112 Test MSE 733.8239528272619 Test RE 0.4560212832728154 Lambda1 -1.7896059\n",
      "37 Train Loss 729.37506 Test MSE 733.4779378603492 Test RE 0.45591375830483594 Lambda1 -1.7891088\n",
      "38 Train Loss 728.81177 Test MSE 732.6418303014509 Test RE 0.455653831172507 Lambda1 -1.7957981\n",
      "39 Train Loss 728.4104 Test MSE 732.5341029971387 Test RE 0.45562033038027616 Lambda1 -1.7859987\n",
      "40 Train Loss 728.23517 Test MSE 731.848481223733 Test RE 0.4554070595261804 Lambda1 -1.7850337\n",
      "41 Train Loss 727.91833 Test MSE 731.2782335216405 Test RE 0.4552296010471506 Lambda1 -1.7920943\n",
      "42 Train Loss 727.6748 Test MSE 731.3350010384402 Test RE 0.4552472699393318 Lambda1 -1.7936932\n",
      "43 Train Loss 727.39453 Test MSE 730.3200027026381 Test RE 0.45493124810445845 Lambda1 -1.7905198\n",
      "44 Train Loss 727.0436 Test MSE 729.2356447901764 Test RE 0.4545933882997126 Lambda1 -1.7963722\n",
      "45 Train Loss 725.67755 Test MSE 729.0587005246606 Test RE 0.45453823289570294 Lambda1 -1.7934083\n",
      "46 Train Loss 723.91296 Test MSE 729.1119614366484 Test RE 0.454554835590843 Lambda1 -1.7886649\n",
      "47 Train Loss 722.4201 Test MSE 728.8146731706997 Test RE 0.45446215599999734 Lambda1 -1.7723807\n",
      "48 Train Loss 721.56616 Test MSE 728.6346027845082 Test RE 0.45440600988055696 Lambda1 -1.7938317\n",
      "49 Train Loss 720.848 Test MSE 727.5685721729719 Test RE 0.45407347830434325 Lambda1 -1.807474\n",
      "50 Train Loss 720.43835 Test MSE 726.7986470538037 Test RE 0.4538331606710313 Lambda1 -1.828581\n",
      "51 Train Loss 720.22906 Test MSE 726.2702232146183 Test RE 0.45366814944822353 Lambda1 -1.8455034\n",
      "52 Train Loss 720.0277 Test MSE 725.6220633431543 Test RE 0.4534656661882456 Lambda1 -1.8590748\n",
      "53 Train Loss 719.8712 Test MSE 725.3253148671813 Test RE 0.45337293264772804 Lambda1 -1.8642944\n",
      "54 Train Loss 719.7274 Test MSE 724.7308326733255 Test RE 0.4531871005952989 Lambda1 -1.8824134\n",
      "55 Train Loss 719.4471 Test MSE 724.6839212284007 Test RE 0.4531724330768255 Lambda1 -1.8727673\n",
      "56 Train Loss 718.9643 Test MSE 723.4035776714593 Test RE 0.45277193232712526 Lambda1 -1.8886127\n",
      "57 Train Loss 718.74963 Test MSE 722.8778610448234 Test RE 0.45260738172107157 Lambda1 -1.8979892\n",
      "58 Train Loss 718.35626 Test MSE 721.9641471114198 Test RE 0.45232124456123973 Lambda1 -1.909505\n",
      "59 Train Loss 718.01166 Test MSE 721.1880919738591 Test RE 0.4520780741702581 Lambda1 -1.908936\n",
      "60 Train Loss 717.40894 Test MSE 719.6896156627921 Test RE 0.45160816864445846 Lambda1 -1.9247093\n",
      "61 Train Loss 716.1984 Test MSE 719.3548125673303 Test RE 0.4515031112708909 Lambda1 -1.9146469\n",
      "62 Train Loss 715.5896 Test MSE 719.9801698898847 Test RE 0.45169932142647523 Lambda1 -1.905596\n",
      "63 Train Loss 715.2191 Test MSE 720.1401241012433 Test RE 0.4517494944724001 Lambda1 -1.9001554\n",
      "64 Train Loss 714.94806 Test MSE 720.2527464781277 Test RE 0.4517848175373948 Lambda1 -1.8895645\n",
      "65 Train Loss 714.48236 Test MSE 720.9308935879277 Test RE 0.45199745434246624 Lambda1 -1.8732849\n",
      "66 Train Loss 714.0528 Test MSE 721.181211459447 Test RE 0.45207591763359434 Lambda1 -1.8804743\n",
      "67 Train Loss 713.5496 Test MSE 720.9756675451124 Test RE 0.45201148994604395 Lambda1 -1.890187\n",
      "68 Train Loss 713.1612 Test MSE 721.469216308617 Test RE 0.4521661772369357 Lambda1 -1.8733562\n",
      "69 Train Loss 713.0093 Test MSE 721.5178485620222 Test RE 0.4521814166186439 Lambda1 -1.8733014\n",
      "70 Train Loss 712.57324 Test MSE 720.962937570844 Test RE 0.45200749943686674 Lambda1 -1.8918173\n",
      "71 Train Loss 712.40063 Test MSE 720.4958063111576 Test RE 0.45186104180735076 Lambda1 -1.8958484\n",
      "72 Train Loss 712.1509 Test MSE 720.2642686773056 Test RE 0.45178843122291007 Lambda1 -1.8827791\n",
      "73 Train Loss 712.04645 Test MSE 720.3581727814311 Test RE 0.45181788111168386 Lambda1 -1.8811984\n",
      "74 Train Loss 711.68146 Test MSE 719.6933973085504 Test RE 0.45160935514196715 Lambda1 -1.8893241\n",
      "Training time: 129.31\n",
      "Training time: 129.31\n",
      "inv_HT_atanh_tune20\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 854.7201 Test MSE 858.0770412226482 Test RE 0.4931196532884333 Lambda1 -0.0037989558\n",
      "1 Train Loss 854.5014 Test MSE 857.6608210225727 Test RE 0.49300004208442105 Lambda1 -0.007383072\n",
      "2 Train Loss 854.34283 Test MSE 857.2618829949288 Test RE 0.49288537009768424 Lambda1 -0.33333623\n",
      "3 Train Loss 851.5867 Test MSE 854.2457200820257 Test RE 0.49201752998376114 Lambda1 -1.5153568\n",
      "4 Train Loss 843.40137 Test MSE 845.5731690270441 Test RE 0.4895136060831092 Lambda1 -1.5907359\n",
      "5 Train Loss 834.55634 Test MSE 832.029137045018 Test RE 0.48557736966863835 Lambda1 -1.0686958\n",
      "6 Train Loss 819.36816 Test MSE 820.0059436262761 Test RE 0.48205619759080365 Lambda1 -1.3399389\n",
      "7 Train Loss 805.94257 Test MSE 806.7317943970293 Test RE 0.4781385468820914 Lambda1 -1.5361619\n",
      "8 Train Loss 778.5834 Test MSE 777.9947542054173 Test RE 0.4695453078017544 Lambda1 -1.5108304\n",
      "9 Train Loss 756.8532 Test MSE 755.633588264254 Test RE 0.4627482635682904 Lambda1 -1.5015908\n",
      "10 Train Loss 726.3437 Test MSE 725.0498233549549 Test RE 0.4532868249010657 Lambda1 -1.6622401\n",
      "11 Train Loss 710.7061 Test MSE 708.2433760228674 Test RE 0.44800249362315664 Lambda1 -1.7498983\n",
      "12 Train Loss 699.05536 Test MSE 696.2004053074065 Test RE 0.44417724546130577 Lambda1 -1.8034661\n",
      "13 Train Loss 686.61755 Test MSE 685.413347394117 Test RE 0.44072272977811683 Lambda1 -1.7847676\n",
      "14 Train Loss 672.3855 Test MSE 672.4423871203081 Test RE 0.4365326292699561 Lambda1 -1.7979602\n",
      "15 Train Loss 656.4422 Test MSE 661.8891953200566 Test RE 0.4330936503223246 Lambda1 -1.7110279\n",
      "16 Train Loss 645.0084 Test MSE 653.6152367434133 Test RE 0.4303781897282548 Lambda1 -1.7949058\n",
      "17 Train Loss 639.4823 Test MSE 648.7767798116673 Test RE 0.4287822703835617 Lambda1 -1.7939476\n",
      "18 Train Loss 637.60236 Test MSE 648.5256177347103 Test RE 0.42869926473825537 Lambda1 -1.7756805\n",
      "19 Train Loss 635.3026 Test MSE 645.5274787722019 Test RE 0.42770717678889897 Lambda1 -1.7606878\n",
      "20 Train Loss 630.81274 Test MSE 643.0302653494656 Test RE 0.4268790857315928 Lambda1 -1.7551166\n",
      "21 Train Loss 626.91986 Test MSE 638.4763257241948 Test RE 0.42536482135773723 Lambda1 -1.6906111\n",
      "22 Train Loss 624.8718 Test MSE 637.2836429069839 Test RE 0.4249673418919425 Lambda1 -1.6272346\n",
      "23 Train Loss 622.13885 Test MSE 634.8853063092718 Test RE 0.42416693263116295 Lambda1 -1.5540658\n",
      "24 Train Loss 620.5164 Test MSE 632.8543986387884 Test RE 0.42348796441311237 Lambda1 -1.5082346\n",
      "25 Train Loss 619.5548 Test MSE 631.5196142688087 Test RE 0.42304112902028784 Lambda1 -1.5085255\n",
      "26 Train Loss 618.24023 Test MSE 631.5785461286165 Test RE 0.4230608671395453 Lambda1 -1.4912835\n",
      "27 Train Loss 617.33704 Test MSE 631.6851252601135 Test RE 0.42309656147876173 Lambda1 -1.4271369\n",
      "28 Train Loss 616.2872 Test MSE 632.2948127015223 Test RE 0.4233006935980229 Lambda1 -1.3860325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 Train Loss 615.2306 Test MSE 631.7188642208309 Test RE 0.42310786034198733 Lambda1 -1.3631401\n",
      "30 Train Loss 613.2373 Test MSE 629.3307264090952 Test RE 0.4223073486680424 Lambda1 -1.3395854\n",
      "31 Train Loss 612.59454 Test MSE 628.3983935420977 Test RE 0.4219944154696216 Lambda1 -1.3308805\n",
      "32 Train Loss 611.1694 Test MSE 627.6152521061754 Test RE 0.4217313782422306 Lambda1 -1.2979794\n",
      "33 Train Loss 610.7603 Test MSE 626.848237715726 Test RE 0.4214735985551391 Lambda1 -1.2692639\n",
      "34 Train Loss 609.63055 Test MSE 626.9879429328264 Test RE 0.42152056269786187 Lambda1 -1.184967\n",
      "35 Train Loss 608.9994 Test MSE 626.8941975785275 Test RE 0.42148904927554365 Lambda1 -1.0959603\n",
      "36 Train Loss 608.38934 Test MSE 625.8490916147972 Test RE 0.4211375669476656 Lambda1 -1.0348198\n",
      "37 Train Loss 607.8069 Test MSE 625.1679309685476 Test RE 0.4209083260353548 Lambda1 -1.03329\n",
      "38 Train Loss 607.6487 Test MSE 624.9274010405254 Test RE 0.4208273471621529 Lambda1 -1.0451342\n",
      "39 Train Loss 607.24146 Test MSE 624.9037774605237 Test RE 0.42081939300424615 Lambda1 -1.0740577\n",
      "40 Train Loss 607.1522 Test MSE 624.805381259027 Test RE 0.4207862609754616 Lambda1 -1.0902913\n",
      "41 Train Loss 606.8807 Test MSE 624.3291403886567 Test RE 0.42062586397644175 Lambda1 -1.0598403\n",
      "42 Train Loss 606.5547 Test MSE 623.9692461673056 Test RE 0.42050461171530185 Lambda1 -1.0028021\n",
      "43 Train Loss 606.1958 Test MSE 623.4711455876919 Test RE 0.42033673853138726 Lambda1 -0.9329332\n",
      "44 Train Loss 605.9904 Test MSE 623.3913617895755 Test RE 0.4203098430329571 Lambda1 -0.898823\n",
      "45 Train Loss 605.81445 Test MSE 623.238122409228 Test RE 0.4202581804799763 Lambda1 -0.90369886\n",
      "46 Train Loss 605.61383 Test MSE 623.3528756435104 Test RE 0.4202968685544227 Lambda1 -0.8676245\n",
      "47 Train Loss 605.50323 Test MSE 623.4009469946843 Test RE 0.4203130743422238 Lambda1 -0.86413854\n",
      "48 Train Loss 605.2239 Test MSE 623.5583918866713 Test RE 0.42036614770464686 Lambda1 -0.90296984\n",
      "49 Train Loss 605.0138 Test MSE 623.515454313963 Test RE 0.4203516744708695 Lambda1 -0.9281193\n",
      "50 Train Loss 604.8519 Test MSE 623.4339919941452 Test RE 0.4203242140921135 Lambda1 -0.93817616\n",
      "51 Train Loss 604.57385 Test MSE 623.050095859902 Test RE 0.4201947812321762 Lambda1 -0.9449173\n",
      "52 Train Loss 604.02014 Test MSE 622.3241417282434 Test RE 0.4199499124429533 Lambda1 -0.9236757\n",
      "53 Train Loss 603.79553 Test MSE 621.8129502346966 Test RE 0.41977739870662595 Lambda1 -0.91246516\n",
      "54 Train Loss 603.6015 Test MSE 621.5323731538173 Test RE 0.419682681150172 Lambda1 -0.9156518\n",
      "55 Train Loss 603.3253 Test MSE 620.6378653669448 Test RE 0.4193805692957619 Lambda1 -0.9087165\n",
      "56 Train Loss 603.17566 Test MSE 619.9559560059803 Test RE 0.4191501143491177 Lambda1 -0.9021212\n",
      "57 Train Loss 602.906 Test MSE 618.7991668234739 Test RE 0.4187588811443613 Lambda1 -0.9152911\n",
      "58 Train Loss 601.1829 Test MSE 616.7431982421011 Test RE 0.4180626363259844 Lambda1 -0.846539\n",
      "59 Train Loss 600.1869 Test MSE 615.9905756321914 Test RE 0.4178074738505892 Lambda1 -0.8378516\n",
      "60 Train Loss 599.17596 Test MSE 614.7991678362562 Test RE 0.41740323064699897 Lambda1 -0.8101166\n",
      "61 Train Loss 597.75616 Test MSE 613.62752388232 Test RE 0.4170053111162712 Lambda1 -0.77367616\n",
      "62 Train Loss 596.18665 Test MSE 611.8641229403427 Test RE 0.41640569930929483 Lambda1 -0.73898727\n",
      "63 Train Loss 593.85187 Test MSE 608.543737150239 Test RE 0.41527431377088814 Lambda1 -0.7577219\n",
      "64 Train Loss 589.9377 Test MSE 604.6968819449718 Test RE 0.41395967302861886 Lambda1 -0.7541344\n",
      "65 Train Loss 587.55853 Test MSE 601.074315937496 Test RE 0.41271785340755623 Lambda1 -0.75452423\n",
      "66 Train Loss 586.03937 Test MSE 600.8582026009292 Test RE 0.41264365139231546 Lambda1 -0.7676039\n",
      "67 Train Loss 584.04114 Test MSE 599.3527696497396 Test RE 0.4121263937933407 Lambda1 -0.736591\n",
      "68 Train Loss 581.1913 Test MSE 596.089562643537 Test RE 0.41100294084922967 Lambda1 -0.69165546\n",
      "69 Train Loss 579.88055 Test MSE 595.1041209356798 Test RE 0.41066306996732127 Lambda1 -0.6712443\n",
      "70 Train Loss 577.5279 Test MSE 591.762647962249 Test RE 0.4095085230904928 Lambda1 -0.6563324\n",
      "71 Train Loss 574.97284 Test MSE 587.9642978379535 Test RE 0.4081921500459463 Lambda1 -0.6442062\n",
      "72 Train Loss 571.57996 Test MSE 581.8705251920885 Test RE 0.4060713505069822 Lambda1 -0.6778252\n",
      "73 Train Loss 566.09674 Test MSE 572.3659028899027 Test RE 0.4027411887507534 Lambda1 -0.6456708\n",
      "74 Train Loss 561.85913 Test MSE 562.0917763706864 Test RE 0.39911016252807735 Lambda1 -0.65097713\n",
      "Training time: 128.52\n",
      "Training time: 128.52\n",
      "inv_HT_atanh_tune20\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 854.7221 Test MSE 858.0779748329621 Test RE 0.493119921551915 Lambda1 0.0067450395\n",
      "1 Train Loss 854.5307 Test MSE 857.7136188195192 Test RE 0.49301521644874924 Lambda1 0.00062968617\n",
      "2 Train Loss 854.4368 Test MSE 857.6195975856563 Test RE 0.4929881939272883 Lambda1 -0.62473804\n",
      "3 Train Loss 852.05676 Test MSE 854.9060821853523 Test RE 0.4922076667066745 Lambda1 -1.3921641\n",
      "4 Train Loss 847.02124 Test MSE 848.5064577734049 Test RE 0.4903619310362892 Lambda1 -1.1743667\n",
      "5 Train Loss 843.4952 Test MSE 844.9447803986216 Test RE 0.48933168099061375 Lambda1 -1.4350778\n",
      "6 Train Loss 837.4061 Test MSE 836.1040395996757 Test RE 0.48676498655610195 Lambda1 -1.3897024\n",
      "7 Train Loss 830.686 Test MSE 830.3527553167345 Test RE 0.4850879496302254 Lambda1 -1.4802374\n",
      "8 Train Loss 824.81116 Test MSE 821.2271849941978 Test RE 0.48241502910069267 Lambda1 -1.4826838\n",
      "9 Train Loss 819.676 Test MSE 813.7647322090544 Test RE 0.48021818572423713 Lambda1 -1.4832351\n",
      "10 Train Loss 812.29803 Test MSE 808.0038652729617 Test RE 0.47851536762191654 Lambda1 -1.6593386\n",
      "11 Train Loss 803.9224 Test MSE 804.1722181055704 Test RE 0.4773794319102396 Lambda1 -1.9278942\n",
      "12 Train Loss 795.4248 Test MSE 794.056325353869 Test RE 0.47436738922173943 Lambda1 -1.7027375\n",
      "13 Train Loss 775.40625 Test MSE 772.9184491001735 Test RE 0.4680109426556259 Lambda1 -1.7140541\n",
      "14 Train Loss 764.3768 Test MSE 760.2011058001577 Test RE 0.4641447250709086 Lambda1 -1.666626\n",
      "15 Train Loss 752.0723 Test MSE 747.6416118740852 Test RE 0.46029462464212506 Lambda1 -1.7391509\n",
      "16 Train Loss 739.54926 Test MSE 728.7846684723787 Test RE 0.45445280098785235 Lambda1 -1.7923859\n",
      "17 Train Loss 731.64703 Test MSE 726.1774199112388 Test RE 0.45363916350979494 Lambda1 -1.8087287\n",
      "18 Train Loss 727.01263 Test MSE 718.1316862955929 Test RE 0.4511191003110873 Lambda1 -1.8341509\n",
      "19 Train Loss 721.8577 Test MSE 711.3301489869614 Test RE 0.4489777081718465 Lambda1 -1.7491244\n",
      "20 Train Loss 714.9774 Test MSE 709.7563128638463 Test RE 0.4484807458106904 Lambda1 -1.6976193\n",
      "21 Train Loss 704.88165 Test MSE 695.3663379338554 Test RE 0.4439110974013092 Lambda1 -1.6979327\n",
      "22 Train Loss 696.8116 Test MSE 686.8942588459713 Test RE 0.4411985880162605 Lambda1 -1.8042156\n",
      "23 Train Loss 686.3444 Test MSE 673.6204995929251 Test RE 0.4369148623703923 Lambda1 -1.7952763\n",
      "24 Train Loss 682.01117 Test MSE 673.1515361732085 Test RE 0.43676274945071897 Lambda1 -1.7681347\n",
      "25 Train Loss 676.5539 Test MSE 671.3810574378193 Test RE 0.4361879990078853 Lambda1 -1.8267071\n",
      "26 Train Loss 670.2215 Test MSE 663.4284934761699 Test RE 0.4335969619896509 Lambda1 -1.8563572\n",
      "27 Train Loss 668.98285 Test MSE 661.8275970937511 Test RE 0.43307349708399645 Lambda1 -1.8913656\n",
      "28 Train Loss 654.30225 Test MSE 647.8037479547141 Test RE 0.4284606070820929 Lambda1 -1.9322313\n",
      "29 Train Loss 643.3524 Test MSE 633.8817450858122 Test RE 0.42383156037456754 Lambda1 -1.9226291\n",
      "30 Train Loss 638.3525 Test MSE 630.672735268019 Test RE 0.42275738103293 Lambda1 -1.9232395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 Train Loss 630.5899 Test MSE 631.9698061284881 Test RE 0.4231918889799824 Lambda1 -1.9333358\n",
      "32 Train Loss 621.44574 Test MSE 627.0266911785648 Test RE 0.4215335876134036 Lambda1 -1.9627382\n",
      "33 Train Loss 619.96246 Test MSE 626.0795841356711 Test RE 0.42121510970122333 Lambda1 -1.9472023\n",
      "34 Train Loss 617.5936 Test MSE 624.0762210182369 Test RE 0.42054065635258026 Lambda1 -1.979388\n",
      "35 Train Loss 611.97125 Test MSE 619.916511356606 Test RE 0.4191367799403381 Lambda1 -2.0668607\n",
      "36 Train Loss 608.7766 Test MSE 617.8358530454976 Test RE 0.41843280336317284 Lambda1 -2.150555\n",
      "37 Train Loss 606.8504 Test MSE 616.4688636984939 Test RE 0.41796964642398576 Lambda1 -2.1755645\n",
      "38 Train Loss 604.6041 Test MSE 614.5012264814615 Test RE 0.4173020783023791 Lambda1 -2.132046\n",
      "39 Train Loss 603.1586 Test MSE 612.7396878603591 Test RE 0.4167035267779391 Lambda1 -2.1265013\n",
      "40 Train Loss 601.7824 Test MSE 611.8913061311072 Test RE 0.41641494900168463 Lambda1 -2.044808\n",
      "41 Train Loss 600.5399 Test MSE 611.6348261952888 Test RE 0.4163276677538215 Lambda1 -1.9939389\n",
      "42 Train Loss 599.08636 Test MSE 610.8454473315204 Test RE 0.4160589237648387 Lambda1 -1.9811168\n",
      "43 Train Loss 596.6583 Test MSE 609.7698411081216 Test RE 0.41569245402600297 Lambda1 -1.9556867\n",
      "44 Train Loss 595.41016 Test MSE 609.1437429756405 Test RE 0.4154789873256879 Lambda1 -1.9452254\n",
      "45 Train Loss 594.49005 Test MSE 609.3401509509945 Test RE 0.4155459639718409 Lambda1 -1.9376842\n",
      "46 Train Loss 593.61975 Test MSE 608.593530579219 Test RE 0.41529130310774076 Lambda1 -1.954076\n",
      "47 Train Loss 592.5645 Test MSE 607.5649722793786 Test RE 0.41494022153566107 Lambda1 -2.0046878\n",
      "48 Train Loss 591.59326 Test MSE 606.8684010981941 Test RE 0.4147022895403366 Lambda1 -2.0313473\n",
      "49 Train Loss 591.15857 Test MSE 607.2112001776895 Test RE 0.4148193985323918 Lambda1 -2.0118377\n",
      "50 Train Loss 590.6021 Test MSE 606.9697589388303 Test RE 0.414736919431732 Lambda1 -2.022999\n",
      "51 Train Loss 590.2168 Test MSE 605.881474260527 Test RE 0.4143649451067488 Lambda1 -2.089181\n",
      "52 Train Loss 589.31775 Test MSE 605.2840314689536 Test RE 0.41416059787218135 Lambda1 -2.1262696\n",
      "53 Train Loss 588.95905 Test MSE 605.3477205977198 Test RE 0.4141823866791212 Lambda1 -2.1296077\n",
      "54 Train Loss 588.4295 Test MSE 604.5409028179154 Test RE 0.4139062799694403 Lambda1 -2.1974807\n",
      "55 Train Loss 588.09955 Test MSE 604.6465554625975 Test RE 0.4139424465732718 Lambda1 -2.1891875\n",
      "56 Train Loss 587.3605 Test MSE 603.7025622491736 Test RE 0.4136191903732576 Lambda1 -2.2641153\n",
      "57 Train Loss 587.0447 Test MSE 603.034816503079 Test RE 0.41339037829970415 Lambda1 -2.3246582\n",
      "58 Train Loss 586.6476 Test MSE 602.8478294677075 Test RE 0.41332628197207155 Lambda1 -2.3791807\n",
      "59 Train Loss 586.34576 Test MSE 602.695320892313 Test RE 0.4132739969786101 Lambda1 -2.391771\n",
      "60 Train Loss 585.8183 Test MSE 601.6162980273579 Test RE 0.4129038830529548 Lambda1 -2.508159\n",
      "61 Train Loss 584.667 Test MSE 600.7521757684214 Test RE 0.41260724244477176 Lambda1 -2.6293855\n",
      "62 Train Loss 584.12585 Test MSE 599.94273917407 Test RE 0.41232918105018024 Lambda1 -2.758809\n",
      "63 Train Loss 583.675 Test MSE 599.2998337943114 Test RE 0.41210819353963507 Lambda1 -2.8564055\n",
      "64 Train Loss 583.5403 Test MSE 599.3755875009498 Test RE 0.4121342387134848 Lambda1 -2.874111\n",
      "65 Train Loss 583.19037 Test MSE 599.4984136671462 Test RE 0.4121764645535665 Lambda1 -2.8093565\n",
      "66 Train Loss 582.9849 Test MSE 599.3111907262357 Test RE 0.41211209831506007 Lambda1 -2.8101215\n",
      "67 Train Loss 582.67377 Test MSE 598.8338192282393 Test RE 0.41194793505230354 Lambda1 -2.8922832\n",
      "68 Train Loss 582.463 Test MSE 598.507987127087 Test RE 0.4118358470903801 Lambda1 -2.947285\n",
      "69 Train Loss 581.99756 Test MSE 597.808939324855 Test RE 0.4115952679647001 Lambda1 -3.007306\n",
      "70 Train Loss 581.69476 Test MSE 597.3594426372032 Test RE 0.4114404981977891 Lambda1 -3.0664425\n",
      "71 Train Loss 581.5831 Test MSE 597.1034088431547 Test RE 0.4113523151421696 Lambda1 -3.083896\n",
      "72 Train Loss 581.5133 Test MSE 597.0031835354292 Test RE 0.4113177904334211 Lambda1 -3.0910103\n",
      "73 Train Loss 581.35364 Test MSE 596.819459716311 Test RE 0.41125449538553416 Lambda1 -3.058586\n",
      "74 Train Loss 581.15405 Test MSE 596.3612743210034 Test RE 0.4110966025927449 Lambda1 -3.0996437\n",
      "Training time: 129.83\n",
      "Training time: 129.83\n",
      "inv_HT_atanh_tune20\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 854.7187 Test MSE 858.0755889209595 Test RE 0.493119235983941 Lambda1 -0.017697215\n",
      "1 Train Loss 854.4999 Test MSE 857.6405498758294 Test RE 0.4929942159262218 Lambda1 -0.027538668\n",
      "2 Train Loss 853.91595 Test MSE 856.7904834712866 Test RE 0.49274983515601123 Lambda1 -0.4665631\n",
      "3 Train Loss 848.35443 Test MSE 851.4903295552675 Test RE 0.4912233817737496 Lambda1 -0.5325644\n",
      "4 Train Loss 829.8432 Test MSE 830.2286741082174 Test RE 0.48505170446369966 Lambda1 -0.5872613\n",
      "5 Train Loss 811.7487 Test MSE 810.9364984306534 Test RE 0.47938296187375895 Lambda1 -0.7128979\n",
      "6 Train Loss 795.13574 Test MSE 791.0219678225292 Test RE 0.4734601626456989 Lambda1 -0.74723727\n",
      "7 Train Loss 772.8798 Test MSE 764.7129132793101 Test RE 0.46552004117814555 Lambda1 -0.6163134\n",
      "8 Train Loss 758.8572 Test MSE 745.5739276302684 Test RE 0.4596576865175365 Lambda1 -0.6969573\n",
      "9 Train Loss 735.73975 Test MSE 734.338501220148 Test RE 0.45618113353348433 Lambda1 -0.80768937\n",
      "10 Train Loss 726.62714 Test MSE 724.637672569858 Test RE 0.45315797232303684 Lambda1 -0.92396337\n",
      "11 Train Loss 717.7837 Test MSE 715.1338312395189 Test RE 0.4501765127260575 Lambda1 -1.0182527\n",
      "12 Train Loss 711.96094 Test MSE 713.9593609419512 Test RE 0.44980669649161076 Lambda1 -1.0661776\n",
      "13 Train Loss 706.8646 Test MSE 706.7542232963739 Test RE 0.4475312607031079 Lambda1 -1.1321663\n",
      "14 Train Loss 702.13556 Test MSE 702.7839056691422 Test RE 0.4462724470597552 Lambda1 -1.1543667\n",
      "15 Train Loss 691.99457 Test MSE 698.3443222509708 Test RE 0.44486063139228454 Lambda1 -1.1204921\n",
      "16 Train Loss 685.7306 Test MSE 692.1531019264846 Test RE 0.44288426976072875 Lambda1 -1.1688079\n",
      "17 Train Loss 679.2077 Test MSE 683.1594200673665 Test RE 0.439997492228148 Lambda1 -1.1624182\n",
      "18 Train Loss 672.0307 Test MSE 677.6471900066896 Test RE 0.43821878613153786 Lambda1 -1.0763184\n",
      "19 Train Loss 670.17163 Test MSE 674.3506209564107 Test RE 0.4371515790993715 Lambda1 -1.0456687\n",
      "20 Train Loss 667.5782 Test MSE 673.4692304194053 Test RE 0.43686580250582463 Lambda1 -1.0439717\n",
      "21 Train Loss 664.6379 Test MSE 671.4412538566501 Test RE 0.4362075530053204 Lambda1 -1.0632282\n",
      "22 Train Loss 663.9529 Test MSE 671.5801495227953 Test RE 0.4362526680509957 Lambda1 -1.0576771\n",
      "23 Train Loss 662.0254 Test MSE 669.8438090235511 Test RE 0.43568834715819527 Lambda1 -1.0376822\n",
      "24 Train Loss 660.8802 Test MSE 667.9384563375402 Test RE 0.4350682539960978 Lambda1 -1.0337937\n",
      "25 Train Loss 659.4438 Test MSE 666.528513626943 Test RE 0.43460882142007273 Lambda1 -1.064896\n",
      "26 Train Loss 657.13794 Test MSE 663.3676984848036 Test RE 0.4335770946431686 Lambda1 -1.0943154\n",
      "27 Train Loss 655.0019 Test MSE 664.3977195287397 Test RE 0.43391357493079646 Lambda1 -1.0924176\n",
      "28 Train Loss 652.61597 Test MSE 662.7569496892195 Test RE 0.4333774559906362 Lambda1 -1.1343108\n",
      "29 Train Loss 651.94415 Test MSE 663.1613821739228 Test RE 0.4335096652346461 Lambda1 -1.150234\n",
      "30 Train Loss 651.5827 Test MSE 663.2799319172336 Test RE 0.4335484115823192 Lambda1 -1.1425083\n",
      "31 Train Loss 650.72784 Test MSE 662.6645504682442 Test RE 0.43334724496446225 Lambda1 -1.1479342\n",
      "32 Train Loss 649.60236 Test MSE 662.1569607581933 Test RE 0.43318124488168647 Lambda1 -1.1422276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 Train Loss 648.2829 Test MSE 661.0808680310342 Test RE 0.4328291133215435 Lambda1 -1.1678352\n",
      "34 Train Loss 647.43787 Test MSE 660.8214186639135 Test RE 0.43274417041574587 Lambda1 -1.2173388\n",
      "35 Train Loss 647.1914 Test MSE 661.1416559536592 Test RE 0.432849012685688 Lambda1 -1.232529\n",
      "36 Train Loss 646.31085 Test MSE 660.916340940587 Test RE 0.43277524960424685 Lambda1 -1.2295578\n",
      "37 Train Loss 645.5584 Test MSE 659.5505907758998 Test RE 0.43232786428626807 Lambda1 -1.2241386\n",
      "38 Train Loss 643.9471 Test MSE 656.7267168928075 Test RE 0.43140136300903814 Lambda1 -1.2452798\n",
      "39 Train Loss 641.296 Test MSE 653.9967191523851 Test RE 0.4305037664821238 Lambda1 -1.2988579\n",
      "40 Train Loss 638.71356 Test MSE 652.7724597668004 Test RE 0.4301006336020927 Lambda1 -1.3187685\n",
      "41 Train Loss 637.7051 Test MSE 651.9770964912763 Test RE 0.42983852809354606 Lambda1 -1.3219601\n",
      "42 Train Loss 636.6364 Test MSE 649.6858460669719 Test RE 0.42908257016045576 Lambda1 -1.3403772\n",
      "43 Train Loss 635.51764 Test MSE 648.8497669109995 Test RE 0.42880638861266546 Lambda1 -1.3490041\n",
      "44 Train Loss 635.0849 Test MSE 649.405309682201 Test RE 0.42898992055840174 Lambda1 -1.3412613\n",
      "45 Train Loss 634.5959 Test MSE 648.5299676018698 Test RE 0.42870070244686326 Lambda1 -1.3351731\n",
      "46 Train Loss 634.1733 Test MSE 647.5717634944089 Test RE 0.42838388237705965 Lambda1 -1.3254973\n",
      "47 Train Loss 633.55304 Test MSE 647.3946151497597 Test RE 0.42832528448063256 Lambda1 -1.2892613\n",
      "48 Train Loss 631.8825 Test MSE 645.7327427889535 Test RE 0.427775172279431 Lambda1 -1.2897438\n",
      "49 Train Loss 631.1464 Test MSE 644.9908513729393 Test RE 0.4275293632044929 Lambda1 -1.2608157\n",
      "50 Train Loss 629.767 Test MSE 643.7930401190265 Test RE 0.4271321967506621 Lambda1 -1.2922401\n",
      "51 Train Loss 628.3737 Test MSE 641.7301932808795 Test RE 0.4264473374921587 Lambda1 -1.2996725\n",
      "52 Train Loss 625.26184 Test MSE 639.2473897444563 Test RE 0.42562159246452086 Lambda1 -1.3031266\n",
      "53 Train Loss 622.64716 Test MSE 637.906536246284 Test RE 0.42517497681104016 Lambda1 -1.3427912\n",
      "54 Train Loss 620.2122 Test MSE 634.5738625098076 Test RE 0.42406288205326004 Lambda1 -1.3391296\n",
      "55 Train Loss 618.37274 Test MSE 632.2435525286785 Test RE 0.42328353474579544 Lambda1 -1.3590957\n",
      "56 Train Loss 616.4493 Test MSE 630.0422192540435 Test RE 0.42254600202489107 Lambda1 -1.3736719\n",
      "57 Train Loss 615.0617 Test MSE 628.4632251216642 Test RE 0.42201618339519076 Lambda1 -1.4017097\n",
      "58 Train Loss 613.8664 Test MSE 626.8168556231939 Test RE 0.4214630482431007 Lambda1 -1.4322085\n",
      "59 Train Loss 612.392 Test MSE 625.1138978801666 Test RE 0.42089013614821175 Lambda1 -1.4729493\n",
      "60 Train Loss 610.5361 Test MSE 623.0334529001944 Test RE 0.4201891690578408 Lambda1 -1.532979\n",
      "61 Train Loss 606.7187 Test MSE 620.1992519276591 Test RE 0.4192323520543424 Lambda1 -1.4540486\n",
      "62 Train Loss 604.7327 Test MSE 617.7189902912692 Test RE 0.418393228513201 Lambda1 -1.3685663\n",
      "63 Train Loss 602.81714 Test MSE 613.790474760266 Test RE 0.4170606760309212 Lambda1 -1.3252028\n",
      "64 Train Loss 599.0201 Test MSE 609.9712978683954 Test RE 0.4157611169326285 Lambda1 -1.3923347\n",
      "65 Train Loss 590.17346 Test MSE 599.0926810939962 Test RE 0.412036963164855 Lambda1 -1.3893781\n",
      "66 Train Loss 575.5977 Test MSE 582.1546230727047 Test RE 0.40617047043736276 Lambda1 -1.3860562\n",
      "67 Train Loss 560.3709 Test MSE 560.0105455051363 Test RE 0.39837059405897673 Lambda1 -1.362564\n",
      "68 Train Loss 553.00006 Test MSE 551.936510862294 Test RE 0.39548838427006805 Lambda1 -1.327153\n",
      "69 Train Loss 542.5487 Test MSE 545.1552402016125 Test RE 0.393051326350572 Lambda1 -1.2970924\n",
      "70 Train Loss 532.8603 Test MSE 530.8596076039539 Test RE 0.3878635887424641 Lambda1 -1.3498307\n",
      "71 Train Loss 520.9472 Test MSE 519.1346216468385 Test RE 0.38355634134189814 Lambda1 -1.4713715\n",
      "72 Train Loss 515.2594 Test MSE 515.8174521579184 Test RE 0.38232895221108276 Lambda1 -1.5611025\n",
      "73 Train Loss 506.15732 Test MSE 508.4930090590302 Test RE 0.37960477250073493 Lambda1 -1.5487286\n",
      "74 Train Loss 496.3817 Test MSE 498.3624842513325 Test RE 0.37580438359883034 Lambda1 -1.5122724\n",
      "Training time: 129.87\n",
      "Training time: 129.87\n",
      "inv_HT_atanh_tune20\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 854.72076 Test MSE 858.0749248649229 Test RE 0.49311904517390465 Lambda1 -0.22789903\n",
      "1 Train Loss 854.375 Test MSE 857.5032288546263 Test RE 0.4929547464892367 Lambda1 -0.2637355\n",
      "2 Train Loss 854.1415 Test MSE 857.2824562192632 Test RE 0.49289128438167007 Lambda1 -0.32158875\n",
      "3 Train Loss 851.5612 Test MSE 853.6159895259005 Test RE 0.4918361444713588 Lambda1 -0.52964604\n",
      "4 Train Loss 848.27783 Test MSE 847.3662373976928 Test RE 0.49003234684598107 Lambda1 -0.6687742\n",
      "5 Train Loss 843.85254 Test MSE 844.7078402012734 Test RE 0.4892630667586206 Lambda1 -0.7395422\n",
      "6 Train Loss 834.10046 Test MSE 835.783466262835 Test RE 0.48667166154581887 Lambda1 -0.830367\n",
      "7 Train Loss 828.58527 Test MSE 831.1291096462069 Test RE 0.48531466781788574 Lambda1 -0.8714282\n",
      "8 Train Loss 823.3966 Test MSE 827.0131772197325 Test RE 0.4841114843779453 Lambda1 -0.8309623\n",
      "9 Train Loss 819.9852 Test MSE 824.0576208838414 Test RE 0.48324565814404075 Lambda1 -0.84341663\n",
      "10 Train Loss 816.09015 Test MSE 819.83247927323 Test RE 0.48200520772311245 Lambda1 -0.8066792\n",
      "11 Train Loss 812.39545 Test MSE 815.770874927162 Test RE 0.4808097530159496 Lambda1 -0.9173786\n",
      "12 Train Loss 805.5877 Test MSE 805.8979964895243 Test RE 0.47789139287902593 Lambda1 -0.98800606\n",
      "13 Train Loss 798.36865 Test MSE 796.8776783277605 Test RE 0.47520937680040237 Lambda1 -0.95863444\n",
      "14 Train Loss 786.4136 Test MSE 786.4537180628944 Test RE 0.47209103754983517 Lambda1 -1.068286\n",
      "15 Train Loss 781.7086 Test MSE 781.4117536287245 Test RE 0.4705753135959617 Lambda1 -1.0911982\n",
      "16 Train Loss 778.7218 Test MSE 779.2680447941862 Test RE 0.46992938699762804 Lambda1 -1.0640496\n",
      "17 Train Loss 774.17804 Test MSE 776.353496899743 Test RE 0.4690497700556611 Lambda1 -1.0570971\n",
      "18 Train Loss 764.1985 Test MSE 768.2659143419271 Test RE 0.4666002350855859 Lambda1 -1.0661783\n",
      "19 Train Loss 759.3599 Test MSE 762.4233539063994 Test RE 0.46482263256065065 Lambda1 -1.1211817\n",
      "20 Train Loss 752.8274 Test MSE 754.1113946190691 Test RE 0.46228193466401674 Lambda1 -1.2079169\n",
      "21 Train Loss 749.10767 Test MSE 749.1178890845772 Test RE 0.4607488445128539 Lambda1 -1.1688702\n",
      "22 Train Loss 746.61755 Test MSE 743.2684700769261 Test RE 0.45894646142608314 Lambda1 -1.1202289\n",
      "23 Train Loss 741.2268 Test MSE 737.7866708189076 Test RE 0.45725090428506937 Lambda1 -1.1586897\n",
      "24 Train Loss 735.86176 Test MSE 735.9346991191758 Test RE 0.4566766544041045 Lambda1 -1.1779121\n",
      "25 Train Loss 732.4116 Test MSE 734.5390196092222 Test RE 0.4562434116670297 Lambda1 -1.1640042\n",
      "26 Train Loss 730.4607 Test MSE 733.0688325410684 Test RE 0.45578659513521647 Lambda1 -1.1698393\n",
      "27 Train Loss 727.4949 Test MSE 729.1829562798074 Test RE 0.4545769654319603 Lambda1 -1.2265364\n",
      "28 Train Loss 727.15027 Test MSE 727.9179148924208 Test RE 0.45418247712116966 Lambda1 -1.2189649\n",
      "29 Train Loss 726.8165 Test MSE 726.0371023794285 Test RE 0.4535953335853077 Lambda1 -1.2251339\n",
      "30 Train Loss 725.98914 Test MSE 724.9615991915302 Test RE 0.4532592460601176 Lambda1 -1.2470839\n",
      "31 Train Loss 722.04767 Test MSE 722.896785587281 Test RE 0.4526133061877173 Lambda1 -1.2682685\n",
      "32 Train Loss 720.5336 Test MSE 722.4122401966173 Test RE 0.4524615912692233 Lambda1 -1.2541051\n",
      "33 Train Loss 719.83545 Test MSE 720.5756804017986 Test RE 0.45188608774742517 Lambda1 -1.2670404\n",
      "34 Train Loss 718.72253 Test MSE 717.050443165394 Test RE 0.4507793623658948 Lambda1 -1.295442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 Train Loss 717.3306 Test MSE 717.1444445830191 Test RE 0.4508089087603267 Lambda1 -1.2740349\n",
      "36 Train Loss 714.7078 Test MSE 711.0097332233531 Test RE 0.44887657669351905 Lambda1 -1.268156\n",
      "37 Train Loss 712.74255 Test MSE 707.890418215439 Test RE 0.44789084719699873 Lambda1 -1.2007972\n",
      "38 Train Loss 706.8312 Test MSE 706.6369129742066 Test RE 0.4474941175114771 Lambda1 -1.2027211\n",
      "39 Train Loss 704.31085 Test MSE 701.6833435563267 Test RE 0.44592287802448655 Lambda1 -1.211433\n",
      "40 Train Loss 701.96875 Test MSE 697.3232783750701 Test RE 0.4445352987740995 Lambda1 -1.2203312\n",
      "41 Train Loss 700.2305 Test MSE 698.2626434520796 Test RE 0.4448346150394542 Lambda1 -1.2338561\n",
      "42 Train Loss 699.04297 Test MSE 697.0552352276438 Test RE 0.4444498534025302 Lambda1 -1.2234027\n",
      "43 Train Loss 697.81665 Test MSE 698.5336704960683 Test RE 0.4449209367947981 Lambda1 -1.2525923\n",
      "44 Train Loss 694.9453 Test MSE 694.4633744122895 Test RE 0.44362278481263623 Lambda1 -1.2884396\n",
      "45 Train Loss 692.1111 Test MSE 693.3135865013495 Test RE 0.4432553907686105 Lambda1 -1.3386478\n",
      "46 Train Loss 690.6499 Test MSE 691.6666109216599 Test RE 0.4427285982150283 Lambda1 -1.3801647\n",
      "47 Train Loss 689.4185 Test MSE 690.5434165389337 Test RE 0.44236897964332816 Lambda1 -1.396322\n",
      "48 Train Loss 687.7293 Test MSE 689.0907935793284 Test RE 0.4419034523376748 Lambda1 -1.4428964\n",
      "49 Train Loss 687.1596 Test MSE 687.5716850206024 Test RE 0.4414160929773682 Lambda1 -1.4687814\n",
      "50 Train Loss 685.50757 Test MSE 684.7813567676824 Test RE 0.44051949703502785 Lambda1 -1.4607464\n",
      "51 Train Loss 684.5797 Test MSE 683.1756373761184 Test RE 0.4400027146788857 Lambda1 -1.4527127\n",
      "52 Train Loss 681.2744 Test MSE 680.3034541182317 Test RE 0.43907681844252094 Lambda1 -1.4767109\n",
      "53 Train Loss 680.1783 Test MSE 678.4856596454575 Test RE 0.4384898117953761 Lambda1 -1.4957649\n",
      "54 Train Loss 679.2656 Test MSE 677.0778024988666 Test RE 0.4380346425670572 Lambda1 -1.4798323\n",
      "55 Train Loss 678.7546 Test MSE 676.329119699996 Test RE 0.43779239587913227 Lambda1 -1.4674377\n",
      "56 Train Loss 676.675 Test MSE 673.4336943134134 Test RE 0.4368542765789101 Lambda1 -1.4750417\n",
      "57 Train Loss 675.69104 Test MSE 671.0997972561969 Test RE 0.43609662380008557 Lambda1 -1.495805\n",
      "58 Train Loss 673.9099 Test MSE 670.6430064975541 Test RE 0.4359481817817121 Lambda1 -1.5226135\n",
      "59 Train Loss 673.12213 Test MSE 670.0762954404601 Test RE 0.435763948987994 Lambda1 -1.5459547\n",
      "60 Train Loss 672.1717 Test MSE 669.0958162724322 Test RE 0.43544501972124 Lambda1 -1.5555221\n",
      "61 Train Loss 670.87024 Test MSE 670.8992232054089 Test RE 0.43603145003360577 Lambda1 -1.5507594\n",
      "62 Train Loss 670.45123 Test MSE 670.4443526156593 Test RE 0.4358836100110621 Lambda1 -1.5648946\n",
      "63 Train Loss 669.166 Test MSE 668.7179037958948 Test RE 0.43532203034909955 Lambda1 -1.5386592\n",
      "64 Train Loss 667.45154 Test MSE 667.4520082178157 Test RE 0.43490979876774294 Lambda1 -1.5384004\n",
      "65 Train Loss 664.52966 Test MSE 666.5190035185708 Test RE 0.43460572088374916 Lambda1 -1.5338172\n",
      "66 Train Loss 662.3336 Test MSE 664.8789427469984 Test RE 0.434070688271486 Lambda1 -1.5452602\n",
      "67 Train Loss 661.7574 Test MSE 663.2946467831639 Test RE 0.4335532206916117 Lambda1 -1.5601889\n",
      "68 Train Loss 661.1793 Test MSE 661.5550177496444 Test RE 0.4329843053915813 Lambda1 -1.5658333\n",
      "69 Train Loss 659.8913 Test MSE 660.0738476917822 Test RE 0.4324993247458884 Lambda1 -1.5326418\n",
      "70 Train Loss 657.7208 Test MSE 659.8668793244053 Test RE 0.4324315136234086 Lambda1 -1.4847984\n",
      "71 Train Loss 657.0658 Test MSE 659.4305647797702 Test RE 0.4322885246629796 Lambda1 -1.4659649\n",
      "72 Train Loss 656.52484 Test MSE 660.0987391160879 Test RE 0.4325074794567224 Lambda1 -1.4661449\n",
      "73 Train Loss 655.73663 Test MSE 659.2907680920874 Test RE 0.4322427005002081 Lambda1 -1.482099\n",
      "74 Train Loss 654.9454 Test MSE 658.341017543283 Test RE 0.4319312516565793 Lambda1 -1.481648\n",
      "Training time: 131.16\n",
      "Training time: 131.16\n",
      "inv_HT_atanh_tune20\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 854.72156 Test MSE 858.0763011816144 Test RE 0.49311944064502305 Lambda1 -0.038419828\n",
      "1 Train Loss 854.63416 Test MSE 857.7917442278512 Test RE 0.49303766924488474 Lambda1 -0.038436707\n",
      "2 Train Loss 853.4412 Test MSE 856.1189032000846 Test RE 0.4925566806179898 Lambda1 -0.92936873\n",
      "3 Train Loss 851.35455 Test MSE 854.210668476985 Test RE 0.49200743559250887 Lambda1 -0.6243334\n",
      "4 Train Loss 847.2852 Test MSE 849.0362066268757 Test RE 0.4905149812197136 Lambda1 -1.0156077\n",
      "5 Train Loss 839.93225 Test MSE 843.0236671763632 Test RE 0.4887750786323509 Lambda1 -0.8199636\n",
      "6 Train Loss 833.1087 Test MSE 834.0990669646097 Test RE 0.48618100644420126 Lambda1 -0.73675436\n",
      "7 Train Loss 827.30505 Test MSE 826.7520653032052 Test RE 0.4840350543643965 Lambda1 -0.63085294\n",
      "8 Train Loss 820.89044 Test MSE 821.8003762800887 Test RE 0.4825833551515207 Lambda1 -0.5523806\n",
      "9 Train Loss 816.31006 Test MSE 815.9763155497991 Test RE 0.48087029185098806 Lambda1 -0.528461\n",
      "10 Train Loss 808.5999 Test MSE 807.0314607354451 Test RE 0.47822734264127675 Lambda1 -0.59935814\n",
      "11 Train Loss 803.1285 Test MSE 800.8462183233594 Test RE 0.4763912051507435 Lambda1 -0.64093816\n",
      "12 Train Loss 789.979 Test MSE 785.646181775187 Test RE 0.47184860207777496 Lambda1 -0.63444674\n",
      "13 Train Loss 784.05994 Test MSE 782.2210051926478 Test RE 0.470818920927438 Lambda1 -0.5779896\n",
      "14 Train Loss 780.51685 Test MSE 780.7563841446306 Test RE 0.4703779366112546 Lambda1 -0.6388045\n",
      "15 Train Loss 774.4792 Test MSE 771.7939502990869 Test RE 0.4676703703088146 Lambda1 -0.7652596\n",
      "16 Train Loss 767.2199 Test MSE 765.9965738300472 Test RE 0.4659105924161084 Lambda1 -0.87542844\n",
      "17 Train Loss 759.9675 Test MSE 761.3937614967758 Test RE 0.4645086734384519 Lambda1 -0.88267016\n",
      "18 Train Loss 753.60486 Test MSE 755.0846395079972 Test RE 0.4625801455543728 Lambda1 -0.8339146\n",
      "19 Train Loss 751.61566 Test MSE 751.2570610656226 Test RE 0.46140622995499864 Lambda1 -0.87544775\n",
      "20 Train Loss 748.6899 Test MSE 747.954273023991 Test RE 0.4603908613959227 Lambda1 -0.83504117\n",
      "21 Train Loss 746.1654 Test MSE 744.2685888964359 Test RE 0.4592551296335604 Lambda1 -0.8141645\n",
      "22 Train Loss 740.95105 Test MSE 736.7429248352475 Test RE 0.45692735363970005 Lambda1 -0.75998163\n",
      "23 Train Loss 738.7532 Test MSE 732.6946752119843 Test RE 0.4556702638622351 Lambda1 -0.7522128\n",
      "24 Train Loss 737.69604 Test MSE 730.2615682774423 Test RE 0.4549130477418347 Lambda1 -0.76396525\n",
      "25 Train Loss 735.20575 Test MSE 732.5874175945389 Test RE 0.45563691034045367 Lambda1 -0.7700687\n",
      "26 Train Loss 733.20935 Test MSE 730.8017913460905 Test RE 0.4550812813582904 Lambda1 -0.7690727\n",
      "27 Train Loss 731.9786 Test MSE 728.2434664828094 Test RE 0.45428402929919687 Lambda1 -0.7719474\n",
      "28 Train Loss 730.12714 Test MSE 723.8661138573211 Test RE 0.45291665786073954 Lambda1 -0.8003233\n",
      "29 Train Loss 724.73047 Test MSE 723.3395732677682 Test RE 0.45275190198758836 Lambda1 -0.80273664\n",
      "30 Train Loss 721.258 Test MSE 720.9378320260694 Test RE 0.4519996294114358 Lambda1 -0.7360231\n",
      "31 Train Loss 720.15125 Test MSE 719.6653926735613 Test RE 0.45160056856824743 Lambda1 -0.7389285\n",
      "32 Train Loss 718.51074 Test MSE 716.247095991633 Test RE 0.45052677636182903 Lambda1 -0.7494563\n",
      "33 Train Loss 717.5213 Test MSE 715.5910443994022 Test RE 0.45032039749919267 Lambda1 -0.7126373\n",
      "34 Train Loss 713.21063 Test MSE 711.5831466665968 Test RE 0.4490575446726685 Lambda1 -0.6968606\n",
      "35 Train Loss 710.5013 Test MSE 708.6385780868864 Test RE 0.44812746960190947 Lambda1 -0.6913537\n",
      "36 Train Loss 709.40045 Test MSE 707.2730442030838 Test RE 0.44769549458430524 Lambda1 -0.67925155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 Train Loss 708.3745 Test MSE 704.599918177791 Test RE 0.44684866506048393 Lambda1 -0.68031466\n",
      "38 Train Loss 706.62524 Test MSE 699.961073688591 Test RE 0.44537528673354954 Lambda1 -0.66836756\n",
      "39 Train Loss 704.4768 Test MSE 699.0313428892003 Test RE 0.44507940119820577 Lambda1 -0.65130454\n",
      "40 Train Loss 700.4121 Test MSE 697.1286649831644 Test RE 0.44447326258374326 Lambda1 -0.5981733\n",
      "41 Train Loss 698.90015 Test MSE 696.1290120692221 Test RE 0.4441544703633208 Lambda1 -0.58463955\n",
      "42 Train Loss 697.368 Test MSE 696.6729003333446 Test RE 0.44432794628025146 Lambda1 -0.5506476\n",
      "43 Train Loss 693.73474 Test MSE 696.3725774282045 Test RE 0.444232165144095 Lambda1 -0.5550115\n",
      "44 Train Loss 691.7718 Test MSE 697.2510521765182 Test RE 0.4445122765065407 Lambda1 -0.5478383\n",
      "45 Train Loss 688.82434 Test MSE 697.9271810751604 Test RE 0.44472774751197913 Lambda1 -0.5208917\n",
      "46 Train Loss 688.1099 Test MSE 697.787617616481 Test RE 0.44468327951624453 Lambda1 -0.52508336\n",
      "47 Train Loss 687.3869 Test MSE 697.6010756709901 Test RE 0.44462383619373425 Lambda1 -0.51998675\n",
      "48 Train Loss 686.7685 Test MSE 697.2208758663131 Test RE 0.44450265738481937 Lambda1 -0.5240455\n",
      "49 Train Loss 684.3826 Test MSE 694.6993128056001 Test RE 0.443698137064786 Lambda1 -0.52721745\n",
      "50 Train Loss 682.3878 Test MSE 693.373456012797 Test RE 0.4432745285089759 Lambda1 -0.54440755\n",
      "51 Train Loss 680.8386 Test MSE 690.8933275921084 Test RE 0.4424810436990481 Lambda1 -0.54621017\n",
      "52 Train Loss 680.2862 Test MSE 690.2277165087404 Test RE 0.44226784779385014 Lambda1 -0.53530645\n",
      "53 Train Loss 679.8449 Test MSE 689.2901094082202 Test RE 0.44196735682282107 Lambda1 -0.5196722\n",
      "54 Train Loss 679.0866 Test MSE 689.4637238684085 Test RE 0.44202301342657685 Lambda1 -0.51327723\n",
      "55 Train Loss 678.77924 Test MSE 688.96890716608 Test RE 0.4418643686571934 Lambda1 -0.5142334\n",
      "56 Train Loss 678.213 Test MSE 688.5311966426751 Test RE 0.44172398538984653 Lambda1 -0.5063182\n",
      "57 Train Loss 676.995 Test MSE 687.3556181011103 Test RE 0.44134673082101095 Lambda1 -0.47806287\n",
      "58 Train Loss 675.7748 Test MSE 686.148678473805 Test RE 0.4409590763919892 Lambda1 -0.46481922\n",
      "59 Train Loss 675.4805 Test MSE 685.785773297838 Test RE 0.44084244897479896 Lambda1 -0.45585287\n",
      "60 Train Loss 674.5106 Test MSE 684.9174361803167 Test RE 0.44056326476908075 Lambda1 -0.46939275\n",
      "61 Train Loss 674.12274 Test MSE 685.1169544148272 Test RE 0.44062742871035454 Lambda1 -0.4799521\n",
      "62 Train Loss 672.84564 Test MSE 683.7563556001176 Test RE 0.4401896822058681 Lambda1 -0.47670475\n",
      "63 Train Loss 672.3909 Test MSE 682.7468901365572 Test RE 0.4398646245988889 Lambda1 -0.4715972\n",
      "64 Train Loss 672.2692 Test MSE 682.5441770088237 Test RE 0.439799320051438 Lambda1 -0.4764396\n",
      "65 Train Loss 671.80273 Test MSE 682.3072000193532 Test RE 0.43972296501807445 Lambda1 -0.47352082\n",
      "66 Train Loss 671.20734 Test MSE 681.3858079421267 Test RE 0.4394259623407525 Lambda1 -0.47607914\n",
      "67 Train Loss 670.4081 Test MSE 680.2527686426254 Test RE 0.4390604616007539 Lambda1 -0.4779744\n",
      "68 Train Loss 669.74567 Test MSE 679.5514768964658 Test RE 0.4388340833403082 Lambda1 -0.47509038\n",
      "69 Train Loss 668.0859 Test MSE 679.3344486362929 Test RE 0.4387640025545157 Lambda1 -0.45994356\n",
      "70 Train Loss 667.0601 Test MSE 678.9782834801216 Test RE 0.4386489686871626 Lambda1 -0.44666648\n",
      "71 Train Loss 665.9501 Test MSE 677.6617998526506 Test RE 0.43822351003104587 Lambda1 -0.4430831\n",
      "72 Train Loss 663.6107 Test MSE 675.0329267068178 Test RE 0.437372677455095 Lambda1 -0.42559052\n",
      "73 Train Loss 662.0479 Test MSE 673.9756483790309 Test RE 0.4370300231591329 Lambda1 -0.41282082\n",
      "74 Train Loss 661.6331 Test MSE 672.8368560656552 Test RE 0.4366606501404043 Lambda1 -0.40461865\n",
      "Training time: 130.18\n",
      "Training time: 130.18\n",
      "inv_HT_atanh_tune20\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 854.7204 Test MSE 858.0773037390195 Test RE 0.49311972871987253 Lambda1 0.039376713\n",
      "1 Train Loss 854.50415 Test MSE 857.6245175689558 Test RE 0.49298960801006403 Lambda1 0.026112843\n",
      "2 Train Loss 852.85187 Test MSE 855.6322634660328 Test RE 0.4924166698534754 Lambda1 -0.8327664\n",
      "3 Train Loss 849.81696 Test MSE 851.3197664047759 Test RE 0.4911741805081252 Lambda1 -0.4734958\n",
      "4 Train Loss 840.3606 Test MSE 842.896582274277 Test RE 0.48873823608772093 Lambda1 -0.7865497\n",
      "5 Train Loss 834.79736 Test MSE 837.6084155733622 Test RE 0.4872027002979857 Lambda1 -0.91636884\n",
      "6 Train Loss 828.1366 Test MSE 829.2264275930524 Test RE 0.484758840460732 Lambda1 -0.899395\n",
      "7 Train Loss 813.5142 Test MSE 818.1933335995744 Test RE 0.48152311407626885 Lambda1 -1.0965234\n",
      "8 Train Loss 796.8419 Test MSE 798.2223233338001 Test RE 0.47561014005687247 Lambda1 -1.155039\n",
      "9 Train Loss 771.70795 Test MSE 770.933936690285 Test RE 0.46740973404096703 Lambda1 -1.1615206\n",
      "10 Train Loss 760.7326 Test MSE 759.7027749550118 Test RE 0.46399257089149143 Lambda1 -1.1235707\n",
      "11 Train Loss 750.5464 Test MSE 743.6066939056531 Test RE 0.4590508711923289 Lambda1 -1.0883133\n",
      "12 Train Loss 736.79266 Test MSE 734.4892279039068 Test RE 0.45622794787144466 Lambda1 -1.0611526\n",
      "13 Train Loss 718.1018 Test MSE 712.0013949377377 Test RE 0.44918949689233634 Lambda1 -1.0467824\n",
      "14 Train Loss 691.7205 Test MSE 692.3474763983161 Test RE 0.44294645206755884 Lambda1 -1.1476924\n",
      "15 Train Loss 676.1659 Test MSE 675.5469687789432 Test RE 0.43753917686779703 Lambda1 -1.2111423\n",
      "16 Train Loss 667.37726 Test MSE 669.1850910384293 Test RE 0.43547406859187443 Lambda1 -1.3523986\n",
      "17 Train Loss 657.19775 Test MSE 657.539284348424 Test RE 0.431668166790847 Lambda1 -1.4221681\n",
      "18 Train Loss 652.44904 Test MSE 654.1634768401655 Test RE 0.43055864843709 Lambda1 -1.3969518\n",
      "19 Train Loss 642.7613 Test MSE 647.1304997695552 Test RE 0.4282379043626119 Lambda1 -1.3742756\n",
      "20 Train Loss 638.9757 Test MSE 642.8116332947839 Test RE 0.42680650953179305 Lambda1 -1.3587643\n",
      "21 Train Loss 634.5912 Test MSE 635.3575753938436 Test RE 0.42432466481357145 Lambda1 -1.314125\n",
      "22 Train Loss 629.25464 Test MSE 631.6789963864892 Test RE 0.4230945089437705 Lambda1 -1.3393573\n",
      "23 Train Loss 622.11334 Test MSE 629.5752320816064 Test RE 0.4223893774890405 Lambda1 -1.3320268\n",
      "24 Train Loss 618.52344 Test MSE 630.3722740740504 Test RE 0.422656665310856 Lambda1 -1.3668896\n",
      "25 Train Loss 615.84045 Test MSE 628.8931184957869 Test RE 0.42216049649056714 Lambda1 -1.3355931\n",
      "26 Train Loss 612.23517 Test MSE 627.5787598084892 Test RE 0.421719117409927 Lambda1 -1.338791\n",
      "27 Train Loss 610.3779 Test MSE 625.4707141285885 Test RE 0.4210102414723605 Lambda1 -1.3013836\n",
      "28 Train Loss 608.0679 Test MSE 622.6441753719602 Test RE 0.42005787935066763 Lambda1 -1.268672\n",
      "29 Train Loss 606.49506 Test MSE 622.6983081373024 Test RE 0.42007613889705325 Lambda1 -1.2562698\n",
      "30 Train Loss 605.4605 Test MSE 622.3051558701479 Test RE 0.4199435064804314 Lambda1 -1.2542444\n",
      "31 Train Loss 604.56415 Test MSE 620.7652326812841 Test RE 0.41942359973233595 Lambda1 -1.2180549\n",
      "32 Train Loss 604.06586 Test MSE 620.3610740823444 Test RE 0.4192870414607083 Lambda1 -1.2132349\n",
      "33 Train Loss 603.4925 Test MSE 619.8573880539765 Test RE 0.4191167922959446 Lambda1 -1.1887863\n",
      "34 Train Loss 602.5112 Test MSE 618.6255355581075 Test RE 0.41870012643079635 Lambda1 -1.134506\n",
      "35 Train Loss 600.9767 Test MSE 616.9757687960844 Test RE 0.41814145348586207 Lambda1 -1.0788498\n",
      "36 Train Loss 598.9463 Test MSE 615.6213191340529 Test RE 0.41768222741152716 Lambda1 -1.0626721\n",
      "37 Train Loss 596.90656 Test MSE 613.859748425875 Test RE 0.417084210533215 Lambda1 -1.001283\n",
      "38 Train Loss 593.36896 Test MSE 607.5721618328669 Test RE 0.4149426766032076 Lambda1 -0.91332144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Train Loss 591.7258 Test MSE 604.6130625153592 Test RE 0.41393098173948106 Lambda1 -0.8783255\n",
      "40 Train Loss 588.74945 Test MSE 599.2004456231949 Test RE 0.41207402001276183 Lambda1 -0.8344565\n",
      "41 Train Loss 585.96466 Test MSE 594.3579763757572 Test RE 0.4104055435018223 Lambda1 -0.87844515\n",
      "42 Train Loss 580.3688 Test MSE 582.6652201775895 Test RE 0.40634855371939627 Lambda1 -0.9461628\n",
      "43 Train Loss 573.8519 Test MSE 565.9437005085334 Test RE 0.40047534640907956 Lambda1 -0.98847014\n",
      "44 Train Loss 558.8504 Test MSE 558.3326466330174 Test RE 0.3977733490516568 Lambda1 -0.9770896\n",
      "45 Train Loss 552.66266 Test MSE 555.8573893787253 Test RE 0.3968906449440665 Lambda1 -0.92943597\n",
      "46 Train Loss 541.99884 Test MSE 545.571346880011 Test RE 0.3932013020573016 Lambda1 -0.9300622\n",
      "47 Train Loss 529.2421 Test MSE 530.2151280812107 Test RE 0.3876280782130465 Lambda1 -1.0024847\n",
      "48 Train Loss 518.8038 Test MSE 521.8359933495553 Test RE 0.3845529844078113 Lambda1 -1.0175722\n",
      "49 Train Loss 507.64206 Test MSE 504.72278304631766 Test RE 0.37819486268229435 Lambda1 -1.0347685\n",
      "50 Train Loss 493.33496 Test MSE 487.02166679416393 Test RE 0.37150384413288523 Lambda1 -1.0290122\n",
      "51 Train Loss 484.48184 Test MSE 478.3503208659669 Test RE 0.3681817057523349 Lambda1 -1.0645171\n",
      "52 Train Loss 468.72772 Test MSE 457.29935365897353 Test RE 0.35998919392578366 Lambda1 -1.0546994\n",
      "53 Train Loss 449.6672 Test MSE 438.90222582518356 Test RE 0.3526736903628625 Lambda1 -1.0769277\n",
      "54 Train Loss 435.91058 Test MSE 426.08564184383647 Test RE 0.3474862469557813 Lambda1 -1.1731921\n",
      "55 Train Loss 410.65204 Test MSE 403.79138506449516 Test RE 0.3382732804963925 Lambda1 -1.2110903\n",
      "56 Train Loss 400.80844 Test MSE 395.6961281565349 Test RE 0.3348652416828395 Lambda1 -1.1982807\n",
      "57 Train Loss 391.34482 Test MSE 376.9181933500807 Test RE 0.3268230819048394 Lambda1 -1.1541504\n",
      "58 Train Loss 380.70956 Test MSE 369.01161032450057 Test RE 0.3233770433812217 Lambda1 -1.0548528\n",
      "59 Train Loss 367.1818 Test MSE 349.7484585437305 Test RE 0.3148234509733508 Lambda1 -1.0031492\n",
      "60 Train Loss 358.53387 Test MSE 340.23760204855813 Test RE 0.3105133845968073 Lambda1 -0.9052873\n",
      "61 Train Loss 354.71832 Test MSE 339.4676810802863 Test RE 0.31016185648032824 Lambda1 -0.87107885\n",
      "62 Train Loss 350.83154 Test MSE 333.96921264256304 Test RE 0.307639705528557 Lambda1 -0.8498061\n",
      "63 Train Loss 333.69568 Test MSE 321.1190583708916 Test RE 0.30166311510980376 Lambda1 -0.763315\n",
      "64 Train Loss 330.663 Test MSE 316.84777968769566 Test RE 0.2996501536390987 Lambda1 -0.69600403\n",
      "65 Train Loss 325.3286 Test MSE 315.7582458449632 Test RE 0.29913451150587955 Lambda1 -0.7267674\n",
      "66 Train Loss 317.31915 Test MSE 306.7976634991182 Test RE 0.294859547373011 Lambda1 -0.6764255\n",
      "67 Train Loss 308.47052 Test MSE 303.47186743833544 Test RE 0.29325700118299924 Lambda1 -0.64969903\n",
      "68 Train Loss 304.9565 Test MSE 302.2178678542233 Test RE 0.2926504789976276 Lambda1 -0.6472277\n",
      "69 Train Loss 301.85767 Test MSE 298.37062485865846 Test RE 0.29078178790854703 Lambda1 -0.6600581\n",
      "70 Train Loss 298.435 Test MSE 295.76164232976606 Test RE 0.2895076840799292 Lambda1 -0.6738512\n",
      "71 Train Loss 297.32205 Test MSE 294.6840727431454 Test RE 0.2889798107853605 Lambda1 -0.6940174\n",
      "72 Train Loss 295.5967 Test MSE 294.66653494097574 Test RE 0.28897121149780336 Lambda1 -0.67241466\n",
      "73 Train Loss 294.70782 Test MSE 294.30796305609175 Test RE 0.28879533727886947 Lambda1 -0.6742729\n",
      "74 Train Loss 293.67905 Test MSE 292.9007339280627 Test RE 0.28810407467817756 Lambda1 -0.6789471\n",
      "Training time: 126.58\n",
      "Training time: 126.58\n",
      "inv_HT_atanh_tune21\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 27748.959 Test MSE 3577.5944704767235 Test RE 1.0068962558777221 Lambda1 -0.00015931904\n",
      "1 Train Loss 20820.344 Test MSE 3575.967965833802 Test RE 1.0066673440162872 Lambda1 -8.454405e-05\n",
      "2 Train Loss 17951.389 Test MSE 3577.254226733808 Test RE 1.0068483747805952 Lambda1 -0.00036776118\n",
      "3 Train Loss 15091.99 Test MSE 3577.9289825256355 Test RE 1.0069433281576348 Lambda1 0.0006533322\n",
      "4 Train Loss 12121.887 Test MSE 3581.2610305894173 Test RE 1.0074120913435891 Lambda1 0.00037229576\n",
      "5 Train Loss 10099.156 Test MSE 3585.4452159096927 Test RE 1.0080004271775735 Lambda1 -7.0189024e-05\n",
      "6 Train Loss 9033.064 Test MSE 3586.6679085405394 Test RE 1.008172284445589 Lambda1 -0.00034241547\n",
      "7 Train Loss 8011.266 Test MSE 3585.6862778082923 Test RE 1.0080343122877895 Lambda1 0.00041464952\n",
      "8 Train Loss 7006.872 Test MSE 3584.3768619257494 Test RE 1.0078502392036577 Lambda1 -0.00014808823\n",
      "9 Train Loss 6116.4307 Test MSE 3585.8768116258702 Test RE 1.0080610940611388 Lambda1 0.00029636442\n",
      "10 Train Loss 5535.8945 Test MSE 3587.2604257532084 Test RE 1.0082555559915205 Lambda1 0.0005871223\n",
      "11 Train Loss 5230.35 Test MSE 3584.638008749465 Test RE 1.0078869529903394 Lambda1 0.000104646366\n",
      "12 Train Loss 4961.417 Test MSE 3584.971246819348 Test RE 1.0079337999099791 Lambda1 0.00035631825\n",
      "13 Train Loss 4727.9956 Test MSE 3585.3995085855117 Test RE 1.0079940021527356 Lambda1 0.00022440046\n",
      "14 Train Loss 4569.169 Test MSE 3583.4649448102286 Test RE 1.0077220252375498 Lambda1 0.00044935438\n",
      "15 Train Loss 4435.448 Test MSE 3582.340389022098 Test RE 1.0075638921770254 Lambda1 -1.3661796e-05\n",
      "16 Train Loss 4342.6157 Test MSE 3582.2608302716703 Test RE 1.0075527038251937 Lambda1 -0.00028784628\n",
      "17 Train Loss 4232.9873 Test MSE 3578.712060609397 Test RE 1.0070535137029795 Lambda1 -9.2585964e-05\n",
      "18 Train Loss 4130.208 Test MSE 3577.9343001101806 Test RE 1.0069440764263005 Lambda1 0.00013061214\n",
      "19 Train Loss 4082.872 Test MSE 3575.240384657368 Test RE 1.0065649284637508 Lambda1 0.0002979969\n",
      "20 Train Loss 4018.5784 Test MSE 3575.104963830974 Test RE 1.0065458652497348 Lambda1 0.0004946882\n",
      "21 Train Loss 3963.5518 Test MSE 3574.1881623632744 Test RE 1.0064167974455063 Lambda1 0.00072383566\n",
      "22 Train Loss 3926.078 Test MSE 3574.7038119594263 Test RE 1.0064893929098961 Lambda1 0.00065691385\n",
      "23 Train Loss 3898.4646 Test MSE 3573.51430955545 Test RE 1.0063219215293961 Lambda1 0.00044004116\n",
      "24 Train Loss 3876.831 Test MSE 3573.3458970145016 Test RE 1.006298208286078 Lambda1 -0.00018720975\n",
      "25 Train Loss 3853.725 Test MSE 3572.5374751861714 Test RE 1.0061843710761242 Lambda1 -0.00031549853\n",
      "26 Train Loss 3821.6245 Test MSE 3570.7458318111667 Test RE 1.0059320364749968 Lambda1 0.00047249455\n",
      "27 Train Loss 3804.5747 Test MSE 3570.6572385735617 Test RE 1.0059195573833861 Lambda1 0.00071028684\n",
      "28 Train Loss 3788.5657 Test MSE 3569.4375956928275 Test RE 1.0057477448389867 Lambda1 0.00041323097\n",
      "29 Train Loss 3780.3623 Test MSE 3569.2644638964875 Test RE 1.0057233531777552 Lambda1 0.00016278845\n",
      "30 Train Loss 3767.252 Test MSE 3569.2183101647233 Test RE 1.0057168507125558 Lambda1 0.0002603864\n",
      "31 Train Loss 3757.0325 Test MSE 3568.198743377252 Test RE 1.0055731961855143 Lambda1 8.406167e-05\n",
      "32 Train Loss 3748.4443 Test MSE 3568.020814986666 Test RE 1.0055481243966649 Lambda1 -8.455231e-05\n",
      "33 Train Loss 3736.931 Test MSE 3566.9989215961546 Test RE 1.0054041178703053 Lambda1 -0.0002089075\n",
      "34 Train Loss 3733.441 Test MSE 3566.8027113594853 Test RE 1.0053764653117896 Lambda1 0.000120565666\n",
      "35 Train Loss 3726.5366 Test MSE 3566.086515632519 Test RE 1.0052755230203656 Lambda1 -6.717129e-05\n",
      "36 Train Loss 3719.0254 Test MSE 3565.7713424568196 Test RE 1.0052310985681678 Lambda1 -0.00034262842\n",
      "37 Train Loss 3713.0544 Test MSE 3564.846167163042 Test RE 1.0051006814408656 Lambda1 -0.0002324162\n",
      "38 Train Loss 3708.5183 Test MSE 3564.649189536454 Test RE 1.0050729123489595 Lambda1 0.00029825338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Train Loss 3701.1035 Test MSE 3563.359093348161 Test RE 1.0048910209476538 Lambda1 0.00078817405\n",
      "40 Train Loss 3692.4294 Test MSE 3562.1616753083804 Test RE 1.0047221668260293 Lambda1 0.00016756964\n",
      "41 Train Loss 3688.18 Test MSE 3561.5923956664205 Test RE 1.0046418798007155 Lambda1 -0.00026932437\n",
      "42 Train Loss 3682.1858 Test MSE 3559.8891869678864 Test RE 1.0044016334140144 Lambda1 -0.0004078268\n",
      "43 Train Loss 3677.21 Test MSE 3558.8590925000162 Test RE 1.0042563053759805 Lambda1 0.00013697609\n",
      "44 Train Loss 3670.1907 Test MSE 3556.7029143230757 Test RE 1.0039520388189784 Lambda1 0.0006027985\n",
      "45 Train Loss 3664.5298 Test MSE 3554.7326110656077 Test RE 1.003673921128784 Lambda1 0.00021687348\n",
      "46 Train Loss 3661.529 Test MSE 3554.4053020218134 Test RE 1.0036277124332524 Lambda1 -3.756969e-05\n",
      "47 Train Loss 3654.8389 Test MSE 3551.4812917015993 Test RE 1.0032148133229615 Lambda1 -0.00022337193\n",
      "48 Train Loss 3653.1458 Test MSE 3550.82871187472 Test RE 1.0031226394140729 Lambda1 2.9662262e-05\n",
      "49 Train Loss 3649.6401 Test MSE 3549.5831809605525 Test RE 1.0029466903689233 Lambda1 0.00041145692\n",
      "50 Train Loss 3642.3855 Test MSE 3545.8835800155207 Test RE 1.0024238867510349 Lambda1 0.00030642236\n",
      "51 Train Loss 3640.1826 Test MSE 3545.46419662746 Test RE 1.002364605002541 Lambda1 -5.7156874e-05\n",
      "52 Train Loss 3634.8325 Test MSE 3543.448780925255 Test RE 1.0020796679073727 Lambda1 -0.00022466101\n",
      "53 Train Loss 3630.3743 Test MSE 3542.498285505275 Test RE 1.0019452598666347 Lambda1 0.00025790703\n",
      "54 Train Loss 3627.66 Test MSE 3540.8914250807425 Test RE 1.0017179952827449 Lambda1 0.00045410855\n",
      "55 Train Loss 3623.8633 Test MSE 3539.3238502688737 Test RE 1.001496237321287 Lambda1 0.00040645586\n",
      "56 Train Loss 3618.5227 Test MSE 3535.8472154841834 Test RE 1.001004237681751 Lambda1 1.668445e-05\n",
      "57 Train Loss 3616.158 Test MSE 3534.7872309504987 Test RE 1.0008541847413541 Lambda1 -0.0003233884\n",
      "58 Train Loss 3611.677 Test MSE 3532.6575092370963 Test RE 1.000552630232995 Lambda1 -0.00026703367\n",
      "59 Train Loss 3609.2966 Test MSE 3530.490829587935 Test RE 1.0002457494324424 Lambda1 0.00019198532\n",
      "60 Train Loss 3607.5085 Test MSE 3529.5458921812183 Test RE 1.0001118823694188 Lambda1 0.00037509797\n",
      "61 Train Loss 3599.0327 Test MSE 3521.8526194657547 Test RE 0.9990213269621931 Lambda1 0.0005990529\n",
      "62 Train Loss 3597.3052 Test MSE 3521.8048656169735 Test RE 0.9990145539253497 Lambda1 0.0005139196\n",
      "63 Train Loss 3594.3904 Test MSE 3519.4205081732007 Test RE 0.9986763166881223 Lambda1 7.5750206e-05\n",
      "64 Train Loss 3591.6133 Test MSE 3518.1989427930994 Test RE 0.9985029849863902 Lambda1 -0.00019746019\n",
      "65 Train Loss 3589.1604 Test MSE 3516.8761769359417 Test RE 0.9983152596973628 Lambda1 -5.036369e-05\n",
      "66 Train Loss 3586.7349 Test MSE 3515.325724119142 Test RE 0.9980951764101279 Lambda1 0.00035526257\n",
      "67 Train Loss 3584.475 Test MSE 3515.0829586534446 Test RE 0.998060712004439 Lambda1 0.0006077923\n",
      "68 Train Loss 3581.5305 Test MSE 3513.0500208650783 Test RE 0.9977720575386545 Lambda1 0.00065979204\n",
      "69 Train Loss 3578.0193 Test MSE 3510.3941458112477 Test RE 0.997394827078729 Lambda1 0.00037810762\n",
      "70 Train Loss 3576.2944 Test MSE 3509.570484635477 Test RE 0.997277808368498 Lambda1 6.915587e-05\n",
      "71 Train Loss 3575.4988 Test MSE 3508.61560685041 Test RE 0.9971421303401904 Lambda1 -1.1060523e-05\n",
      "72 Train Loss 3574.4128 Test MSE 3508.4765425310998 Test RE 0.9971223692319362 Lambda1 -5.8122918e-05\n",
      "73 Train Loss 3572.5151 Test MSE 3504.8658462156072 Test RE 0.9966091503464313 Lambda1 -0.0002461386\n",
      "74 Train Loss 3568.8384 Test MSE 3500.5995038728356 Test RE 0.9960023980904998 Lambda1 -0.0003049919\n",
      "Training time: 120.79\n",
      "Training time: 120.79\n",
      "inv_HT_atanh_tune21\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 5893.823 Test MSE 3504.617517980893 Test RE 0.996573843634881 Lambda1 -0.0007422365\n",
      "1 Train Loss 3805.7815 Test MSE 3506.2636824550123 Test RE 0.9968078680113097 Lambda1 -0.0006123326\n",
      "2 Train Loss 3480.702 Test MSE 3430.918615023302 Test RE 0.9860396497625131 Lambda1 -0.0004511083\n",
      "3 Train Loss 855.2917 Test MSE 858.3920550547732 Test RE 0.4932101610458532 Lambda1 0.08206178\n",
      "4 Train Loss 854.62146 Test MSE 857.8214212845849 Test RE 0.493046197993516 Lambda1 0.08436614\n",
      "5 Train Loss 853.05505 Test MSE 855.12897363347 Test RE 0.49227182681267306 Lambda1 0.091506995\n",
      "6 Train Loss 849.4566 Test MSE 846.0045847273648 Test RE 0.48963846631580193 Lambda1 0.090569116\n",
      "7 Train Loss 846.79913 Test MSE 843.1218023308705 Test RE 0.4888035266011898 Lambda1 0.0964965\n",
      "8 Train Loss 840.7478 Test MSE 831.0836576034827 Test RE 0.4853013974099864 Lambda1 0.108238846\n",
      "9 Train Loss 826.914 Test MSE 819.3555789109288 Test RE 0.481864994995134 Lambda1 0.08458757\n",
      "10 Train Loss 788.2465 Test MSE 779.3531328537107 Test RE 0.46995504202639615 Lambda1 0.02274045\n",
      "11 Train Loss 770.1448 Test MSE 764.1269508394475 Test RE 0.465341654277373 Lambda1 0.0047609056\n",
      "12 Train Loss 763.90314 Test MSE 760.743503877897 Test RE 0.4643102775247857 Lambda1 0.001482848\n",
      "13 Train Loss 760.8497 Test MSE 757.6511565383242 Test RE 0.4633656288209974 Lambda1 0.00065363036\n",
      "14 Train Loss 758.925 Test MSE 756.0089206890603 Test RE 0.46286317565537766 Lambda1 0.0013901544\n",
      "15 Train Loss 756.9096 Test MSE 753.8375356689631 Test RE 0.46219798715830657 Lambda1 0.000787407\n",
      "16 Train Loss 753.75134 Test MSE 750.4079779284158 Test RE 0.4611454117683665 Lambda1 0.001106603\n",
      "17 Train Loss 749.29395 Test MSE 744.8626170376489 Test RE 0.4594383672857751 Lambda1 0.0007869697\n",
      "18 Train Loss 746.51666 Test MSE 741.5543311862851 Test RE 0.4584169407293798 Lambda1 0.00040799764\n",
      "19 Train Loss 742.14233 Test MSE 734.994617084759 Test RE 0.45638488211260464 Lambda1 0.00069502334\n",
      "20 Train Loss 734.96826 Test MSE 728.6691236504178 Test RE 0.45441677405719877 Lambda1 0.00071581895\n",
      "21 Train Loss 727.3783 Test MSE 719.3762776199795 Test RE 0.4515098474917302 Lambda1 0.00048485523\n",
      "22 Train Loss 722.8166 Test MSE 715.7973146248033 Test RE 0.4503852955983455 Lambda1 0.00016739321\n",
      "23 Train Loss 721.7727 Test MSE 715.0510327451967 Test RE 0.4501504511591416 Lambda1 0.00010393959\n",
      "24 Train Loss 720.4564 Test MSE 714.3487958080773 Test RE 0.44992935510476034 Lambda1 0.00027116732\n",
      "25 Train Loss 718.7628 Test MSE 713.647665035946 Test RE 0.4497084988776928 Lambda1 6.2329105e-05\n",
      "26 Train Loss 717.8845 Test MSE 712.9132778559928 Test RE 0.44947705051689346 Lambda1 2.9615816e-05\n",
      "27 Train Loss 716.78394 Test MSE 711.5473558342479 Test RE 0.44904625130134995 Lambda1 5.6018504e-05\n",
      "28 Train Loss 716.0108 Test MSE 710.4186348316753 Test RE 0.448689950984551 Lambda1 8.514492e-05\n",
      "29 Train Loss 714.3841 Test MSE 708.8444712520043 Test RE 0.44819256603115926 Lambda1 0.000105755564\n",
      "30 Train Loss 713.41864 Test MSE 707.0716543698725 Test RE 0.4476317513519007 Lambda1 0.00024361945\n",
      "31 Train Loss 712.16113 Test MSE 706.2648423023835 Test RE 0.4473762908336646 Lambda1 0.00015372752\n",
      "32 Train Loss 710.72614 Test MSE 703.2430513580099 Test RE 0.44641820353782025 Lambda1 8.877868e-05\n",
      "33 Train Loss 707.79175 Test MSE 700.2000659186116 Test RE 0.445451313924771 Lambda1 0.000107711756\n",
      "34 Train Loss 706.7387 Test MSE 697.7138649771022 Test RE 0.4446597785029149 Lambda1 8.995029e-05\n",
      "35 Train Loss 703.80634 Test MSE 691.9557283202439 Test RE 0.442821119066315 Lambda1 6.352173e-06\n",
      "36 Train Loss 701.09576 Test MSE 689.7599465877418 Test RE 0.4421179591020995 Lambda1 2.740883e-05\n",
      "37 Train Loss 698.62006 Test MSE 686.8820980720365 Test RE 0.4411946825097236 Lambda1 -5.368626e-05\n",
      "38 Train Loss 696.7948 Test MSE 686.6351682214971 Test RE 0.44111537200631823 Lambda1 4.8104575e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Train Loss 693.3972 Test MSE 683.8821789096102 Test RE 0.4402301817013339 Lambda1 1.3099379e-05\n",
      "40 Train Loss 689.6883 Test MSE 679.8053299405973 Test RE 0.4389160410507068 Lambda1 9.985798e-05\n",
      "41 Train Loss 688.14246 Test MSE 678.6500036910819 Test RE 0.4385429144846664 Lambda1 0.00013735914\n",
      "42 Train Loss 687.102 Test MSE 677.2027931025818 Test RE 0.4380750719591375 Lambda1 0.00039458307\n",
      "43 Train Loss 686.3778 Test MSE 676.6927242333793 Test RE 0.43791006189861276 Lambda1 9.740992e-05\n",
      "44 Train Loss 684.9072 Test MSE 675.6106136065276 Test RE 0.4375597871664928 Lambda1 6.84516e-05\n",
      "45 Train Loss 684.06287 Test MSE 674.8362524709522 Test RE 0.4373089574495801 Lambda1 1.9905992e-05\n",
      "46 Train Loss 683.00964 Test MSE 673.4706200238488 Test RE 0.43686625320967254 Lambda1 -5.5097207e-06\n",
      "47 Train Loss 682.25385 Test MSE 672.7928872876142 Test RE 0.43664638238091313 Lambda1 3.625938e-05\n",
      "48 Train Loss 681.75024 Test MSE 672.0540408710978 Test RE 0.4364065587726762 Lambda1 3.1203155e-05\n",
      "49 Train Loss 680.8279 Test MSE 671.0648516669121 Test RE 0.4360852694143731 Lambda1 6.9976435e-05\n",
      "50 Train Loss 680.4072 Test MSE 670.5797827674328 Test RE 0.4359276321602653 Lambda1 -2.9115317e-05\n",
      "51 Train Loss 679.44684 Test MSE 669.2679248377272 Test RE 0.43550101992109136 Lambda1 9.696392e-05\n",
      "52 Train Loss 678.52405 Test MSE 669.1474004044011 Test RE 0.4354618047764617 Lambda1 2.3641162e-05\n",
      "53 Train Loss 677.6752 Test MSE 668.5202684911369 Test RE 0.4352576972728048 Lambda1 1.981601e-05\n",
      "54 Train Loss 676.7489 Test MSE 667.8563068493664 Test RE 0.4350414987366525 Lambda1 1.4284195e-05\n",
      "55 Train Loss 676.20886 Test MSE 666.5443190285984 Test RE 0.4346139743326064 Lambda1 4.213396e-05\n",
      "56 Train Loss 675.735 Test MSE 665.6776364477981 Test RE 0.43433132629775356 Lambda1 0.000120126206\n",
      "57 Train Loss 673.4646 Test MSE 662.7805335962119 Test RE 0.4333851666926485 Lambda1 -0.00023332486\n",
      "58 Train Loss 672.2779 Test MSE 661.3047677597731 Test RE 0.4329024039895717 Lambda1 -0.00017886145\n",
      "59 Train Loss 670.64813 Test MSE 658.3209061891521 Test RE 0.4319246541725448 Lambda1 0.0001432902\n",
      "60 Train Loss 668.9041 Test MSE 656.64122047789 Test RE 0.4313732809549083 Lambda1 0.00021046073\n",
      "61 Train Loss 667.13983 Test MSE 654.2448400346971 Test RE 0.43058542350103934 Lambda1 5.3506283e-05\n",
      "62 Train Loss 666.0233 Test MSE 652.7553537292681 Test RE 0.4300949981270725 Lambda1 5.3573705e-05\n",
      "63 Train Loss 664.88196 Test MSE 650.8140427162793 Test RE 0.42945496517611903 Lambda1 2.3901855e-05\n",
      "64 Train Loss 663.02325 Test MSE 646.6519655815995 Test RE 0.4280795403416025 Lambda1 1.2510175e-05\n",
      "65 Train Loss 661.74725 Test MSE 645.1011434406897 Test RE 0.4275659149492144 Lambda1 1.5776637e-05\n",
      "66 Train Loss 661.0602 Test MSE 644.1309363504826 Test RE 0.42724427269594806 Lambda1 -1.24197895e-05\n",
      "67 Train Loss 659.6947 Test MSE 639.9282132985031 Test RE 0.4258481839970081 Lambda1 -6.39138e-05\n",
      "68 Train Loss 658.1337 Test MSE 633.904749824094 Test RE 0.4238392511195185 Lambda1 -2.6918702e-05\n",
      "69 Train Loss 652.6712 Test MSE 622.0826385392784 Test RE 0.419868420277116 Lambda1 -4.3813154e-05\n",
      "70 Train Loss 640.6408 Test MSE 598.257204827081 Test RE 0.41174955587583645 Lambda1 -4.1006135e-07\n",
      "71 Train Loss 623.19226 Test MSE 580.9178036179517 Test RE 0.4057387752946258 Lambda1 3.8545317e-05\n",
      "72 Train Loss 593.002 Test MSE 540.7358818591478 Test RE 0.3914549284485117 Lambda1 0.000150523\n",
      "73 Train Loss 556.87897 Test MSE 515.8519060610953 Test RE 0.3823417207832647 Lambda1 9.243111e-05\n",
      "74 Train Loss 531.4156 Test MSE 494.9882821077825 Test RE 0.37453001641167677 Lambda1 5.270758e-05\n",
      "Training time: 127.42\n",
      "Training time: 127.42\n",
      "inv_HT_atanh_tune21\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 4307.016 Test MSE 3514.7221861556554 Test RE 0.9980094924315595 Lambda1 -0.0003167946\n",
      "1 Train Loss 3716.3486 Test MSE 3517.565392650067 Test RE 0.9984130767405496 Lambda1 -0.0010028252\n",
      "2 Train Loss 3498.287 Test MSE 3442.3107669200426 Test RE 0.9876753347584347 Lambda1 5.1706564e-05\n",
      "3 Train Loss 2634.512 Test MSE 2623.833197098555 Test RE 0.8622978075721591 Lambda1 -0.05392173\n",
      "4 Train Loss 854.7216 Test MSE 858.0787101390282 Test RE 0.49312013283457373 Lambda1 -0.36301363\n",
      "5 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "6 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "7 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "8 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "9 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "10 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "11 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "12 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "13 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "14 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "15 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "16 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "17 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "18 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "19 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "20 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "21 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "22 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "23 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "24 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "25 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "26 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "27 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "28 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "29 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "30 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "31 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "32 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "33 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "34 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "35 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "36 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "37 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "38 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "40 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "41 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "42 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "43 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "44 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "45 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "46 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "47 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "48 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "49 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "50 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "51 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "52 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "53 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "54 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "55 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "56 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "57 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "58 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "59 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "60 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "61 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "62 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "63 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "64 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "65 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "66 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "67 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "68 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "69 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "70 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "71 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "72 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "73 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "74 Train Loss 854.72144 Test MSE 858.0767586418079 Test RE 0.4931195720916581 Lambda1 -0.36296123\n",
      "Training time: 126.05\n",
      "Training time: 126.05\n",
      "inv_HT_atanh_tune21\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 18336.84 Test MSE 3517.7430359836026 Test RE 0.9984382872438651 Lambda1 0.00028046162\n",
      "1 Train Loss 9209.28 Test MSE 3477.506238878162 Test RE 0.9927116750397945 Lambda1 0.00043781998\n",
      "2 Train Loss 6037.997 Test MSE 3463.0832702184534 Test RE 0.9906508985679168 Lambda1 -0.00025056693\n",
      "3 Train Loss 4325.882 Test MSE 3465.1254719925605 Test RE 0.9909429520203827 Lambda1 -0.00075630494\n",
      "4 Train Loss 3649.5476 Test MSE 3449.9981650624113 Test RE 0.9887775622706085 Lambda1 -0.0006012906\n",
      "5 Train Loss 3493.1516 Test MSE 3431.444096029158 Test RE 0.9861151580048811 Lambda1 0.0010264155\n",
      "6 Train Loss 3190.0103 Test MSE 3171.0273581513857 Test RE 0.9479581507891786 Lambda1 0.0027692006\n",
      "7 Train Loss 854.7738 Test MSE 858.1155976500344 Test RE 0.49313073196646373 Lambda1 0.10914259\n",
      "8 Train Loss 854.6607 Test MSE 857.9557919886814 Test RE 0.4930848123070072 Lambda1 0.10847994\n",
      "9 Train Loss 854.54285 Test MSE 857.7690549812264 Test RE 0.49303114859035363 Lambda1 0.06482548\n",
      "10 Train Loss 854.3814 Test MSE 857.3941600021172 Test RE 0.49292339516803474 Lambda1 -0.012491547\n",
      "11 Train Loss 854.067 Test MSE 856.788977541584 Test RE 0.49274940211727347 Lambda1 -0.064296186\n",
      "12 Train Loss 853.55725 Test MSE 856.0356446443436 Test RE 0.4925327291815708 Lambda1 -0.058028385\n",
      "13 Train Loss 852.54095 Test MSE 854.1998944528408 Test RE 0.492004332776716 Lambda1 -0.04946979\n",
      "14 Train Loss 851.2499 Test MSE 853.1030941898763 Test RE 0.491688362346985 Lambda1 -0.0320715\n",
      "15 Train Loss 847.8753 Test MSE 848.2406019968853 Test RE 0.4902851044170235 Lambda1 -0.04234515\n",
      "16 Train Loss 842.27686 Test MSE 833.5167780081398 Test RE 0.4860112740542562 Lambda1 -0.023083428\n",
      "17 Train Loss 821.4572 Test MSE 819.4110966055977 Test RE 0.48188131976377124 Lambda1 -0.0068599433\n",
      "18 Train Loss 809.50684 Test MSE 813.3293260048329 Test RE 0.480089697756438 Lambda1 -0.0015537015\n",
      "19 Train Loss 800.34 Test MSE 802.8571380043264 Test RE 0.47698893777969276 Lambda1 -2.5237678e-06\n",
      "20 Train Loss 789.63556 Test MSE 795.43308576868 Test RE 0.47477844784607126 Lambda1 5.369433e-05\n",
      "21 Train Loss 784.2236 Test MSE 791.3717395780875 Test RE 0.4735648276844696 Lambda1 -4.529531e-05\n",
      "22 Train Loss 780.3729 Test MSE 787.8455899996067 Test RE 0.47250860808053924 Lambda1 -8.172627e-05\n",
      "23 Train Loss 766.3627 Test MSE 772.5125450172512 Test RE 0.46788803673975615 Lambda1 2.3947781e-05\n",
      "24 Train Loss 760.1576 Test MSE 765.3674402503423 Test RE 0.4657192206656276 Lambda1 0.00010813595\n",
      "25 Train Loss 754.2461 Test MSE 760.1708837593884 Test RE 0.4641354988673978 Lambda1 0.00014420183\n",
      "26 Train Loss 748.7459 Test MSE 759.0576283520082 Test RE 0.4637955156620933 Lambda1 3.3251286e-05\n",
      "27 Train Loss 746.1894 Test MSE 754.558420258594 Test RE 0.4624189311760826 Lambda1 4.931095e-05\n",
      "28 Train Loss 744.3397 Test MSE 753.3181812465463 Test RE 0.46203874467164835 Lambda1 3.2367883e-05\n",
      "29 Train Loss 742.70404 Test MSE 750.3120652504616 Test RE 0.4611159403967566 Lambda1 2.0297393e-05\n",
      "30 Train Loss 739.8216 Test MSE 747.5515333409079 Test RE 0.46026689483620853 Lambda1 3.3512857e-05\n",
      "31 Train Loss 738.00824 Test MSE 746.4931873890212 Test RE 0.4599409680499344 Lambda1 6.768177e-05\n",
      "32 Train Loss 735.6178 Test MSE 745.3056012838805 Test RE 0.459574965433709 Lambda1 9.145727e-05\n",
      "33 Train Loss 733.8363 Test MSE 744.3077702138931 Test RE 0.4592672180009056 Lambda1 8.631281e-05\n",
      "34 Train Loss 731.6092 Test MSE 743.2759143279465 Test RE 0.458948759723567 Lambda1 8.976221e-05\n",
      "35 Train Loss 730.4141 Test MSE 742.4364153217883 Test RE 0.4586895048001218 Lambda1 0.00011335061\n",
      "36 Train Loss 729.8057 Test MSE 742.1269327146827 Test RE 0.45859389309689075 Lambda1 0.00018974229\n",
      "37 Train Loss 729.19135 Test MSE 742.0902655975578 Test RE 0.4585825638195406 Lambda1 0.00016698672\n",
      "38 Train Loss 728.218 Test MSE 740.931064800077 Test RE 0.45822425361372926 Lambda1 0.00011270113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Train Loss 726.9979 Test MSE 739.8995868573061 Test RE 0.4579051869384763 Lambda1 7.342929e-05\n",
      "40 Train Loss 725.8176 Test MSE 739.023885736646 Test RE 0.4576341320486269 Lambda1 0.000116298244\n",
      "41 Train Loss 724.90015 Test MSE 738.6694925598633 Test RE 0.45752439144011303 Lambda1 5.4558484e-05\n",
      "42 Train Loss 724.6299 Test MSE 738.496916155038 Test RE 0.45747094228071816 Lambda1 4.827181e-05\n",
      "43 Train Loss 724.423 Test MSE 738.1638042629412 Test RE 0.4573677555265595 Lambda1 6.301918e-05\n",
      "44 Train Loss 724.26434 Test MSE 737.8772552747383 Test RE 0.45727897374924614 Lambda1 4.861023e-05\n",
      "45 Train Loss 724.0483 Test MSE 737.7097736114994 Test RE 0.4572270747451417 Lambda1 4.526308e-05\n",
      "46 Train Loss 723.85455 Test MSE 737.5363182446656 Test RE 0.4571733184067753 Lambda1 5.3681262e-05\n",
      "47 Train Loss 722.7529 Test MSE 736.5734944069148 Test RE 0.45687481031277466 Lambda1 2.4371175e-05\n",
      "48 Train Loss 720.5539 Test MSE 734.8677486745182 Test RE 0.45634549180830425 Lambda1 1.0953423e-05\n",
      "49 Train Loss 718.8923 Test MSE 733.4694754782174 Test RE 0.4559111282819198 Lambda1 6.195731e-06\n",
      "50 Train Loss 717.1457 Test MSE 731.6506031628526 Test RE 0.4553454886133886 Lambda1 5.203666e-06\n",
      "51 Train Loss 716.998 Test MSE 731.555206846489 Test RE 0.4553158025186071 Lambda1 5.7023294e-06\n",
      "52 Train Loss 716.81384 Test MSE 731.1582297785105 Test RE 0.4551922476192948 Lambda1 6.250626e-06\n",
      "53 Train Loss 716.592 Test MSE 730.668517279338 Test RE 0.45503978358201064 Lambda1 1.0789372e-05\n",
      "54 Train Loss 716.3342 Test MSE 730.584692551935 Test RE 0.45501368099310274 Lambda1 1.0272872e-05\n",
      "55 Train Loss 715.9164 Test MSE 730.7182509561517 Test RE 0.45505526968543175 Lambda1 1.09132325e-05\n",
      "56 Train Loss 712.07324 Test MSE 724.7935067235219 Test RE 0.45320669577172124 Lambda1 2.3246339e-05\n",
      "57 Train Loss 707.1551 Test MSE 721.5826466478212 Test RE 0.45220172092051525 Lambda1 2.2275062e-05\n",
      "58 Train Loss 703.4823 Test MSE 717.7474698420345 Test RE 0.4509984047799831 Lambda1 2.8012053e-05\n",
      "59 Train Loss 702.3564 Test MSE 716.7735323892229 Test RE 0.45069231289961525 Lambda1 2.7262853e-05\n",
      "60 Train Loss 702.2168 Test MSE 716.7320015658094 Test RE 0.4506792558508029 Lambda1 3.205758e-05\n",
      "61 Train Loss 702.1801 Test MSE 716.6666951221399 Test RE 0.4506587231204057 Lambda1 3.8020113e-05\n",
      "62 Train Loss 701.8826 Test MSE 716.5928094130377 Test RE 0.4506354918904647 Lambda1 2.7540938e-05\n",
      "63 Train Loss 701.05927 Test MSE 715.2084676148738 Test RE 0.4502000039000775 Lambda1 2.6320427e-05\n",
      "64 Train Loss 698.80585 Test MSE 714.4207152110778 Test RE 0.44995200358991855 Lambda1 1.5613297e-05\n",
      "65 Train Loss 698.37897 Test MSE 713.8456030200815 Test RE 0.449770860338855 Lambda1 2.333852e-05\n",
      "66 Train Loss 697.78156 Test MSE 712.7407807414221 Test RE 0.449422669298817 Lambda1 1.2897053e-05\n",
      "67 Train Loss 697.0232 Test MSE 712.5559339089253 Test RE 0.449364387419853 Lambda1 9.6152435e-06\n",
      "68 Train Loss 696.7929 Test MSE 711.8141655595283 Test RE 0.449130433103697 Lambda1 6.8698814e-06\n",
      "69 Train Loss 696.14 Test MSE 710.9765383757424 Test RE 0.44886609824098844 Lambda1 5.017847e-06\n",
      "70 Train Loss 695.7151 Test MSE 710.4813664314885 Test RE 0.4487097607247032 Lambda1 8.00104e-06\n",
      "71 Train Loss 694.9213 Test MSE 710.718967818095 Test RE 0.4487847839092002 Lambda1 1.538485e-05\n",
      "72 Train Loss 694.1652 Test MSE 710.5375565826489 Test RE 0.44872750402662775 Lambda1 2.733317e-05\n",
      "73 Train Loss 692.8653 Test MSE 709.0869502087863 Test RE 0.44826921752373877 Lambda1 2.1277016e-05\n",
      "74 Train Loss 692.024 Test MSE 707.4574039477236 Test RE 0.44775383955436526 Lambda1 5.822233e-05\n",
      "Training time: 132.86\n",
      "Training time: 132.86\n",
      "inv_HT_atanh_tune21\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 9263.336 Test MSE 3585.852805686868 Test RE 1.0080577197826974 Lambda1 -8.052458e-05\n",
      "1 Train Loss 8885.018 Test MSE 3585.788974112062 Test RE 1.0080487475518793 Lambda1 -0.00022532431\n",
      "2 Train Loss 5674.4565 Test MSE 3584.18638114984 Test RE 1.0078234592843685 Lambda1 0.0025252944\n",
      "3 Train Loss 4576.659 Test MSE 3574.7301881349163 Test RE 1.0064931061254914 Lambda1 -0.00051338895\n",
      "4 Train Loss 3939.4863 Test MSE 3554.479265372808 Test RE 1.0036381545854958 Lambda1 0.00018791028\n",
      "5 Train Loss 3701.596 Test MSE 3542.528447795958 Test RE 1.0019495253450361 Lambda1 0.00023017029\n",
      "6 Train Loss 3579.3674 Test MSE 3528.5467280772714 Test RE 0.9999703136445004 Lambda1 0.000654264\n",
      "7 Train Loss 3415.4197 Test MSE 3312.335230682063 Test RE 0.9688494846005267 Lambda1 -0.0022788725\n",
      "8 Train Loss 3234.5813 Test MSE 3199.5196017515054 Test RE 0.9522074131535672 Lambda1 0.0041991277\n",
      "9 Train Loss 2382.3213 Test MSE 2372.0963775138243 Test RE 0.8198895210862961 Lambda1 0.029998085\n",
      "10 Train Loss 854.7214 Test MSE 858.0785130898472 Test RE 0.49312007621452086 Lambda1 0.2264685\n",
      "11 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "12 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "13 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "14 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "15 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "16 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "17 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "18 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "19 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "20 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "21 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "22 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "23 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "24 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "25 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "26 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "27 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "28 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "29 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "30 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "31 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "32 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "33 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "34 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "35 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "36 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "37 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "38 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "39 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "41 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "42 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "43 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "44 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "45 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "46 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "47 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "48 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "49 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "50 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "51 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "52 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "53 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "54 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "55 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "56 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "57 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "58 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "59 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "60 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "61 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "62 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "63 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "64 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "65 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "66 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "67 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "68 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "69 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "70 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "71 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "72 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "73 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "74 Train Loss 854.7212 Test MSE 858.0783745416858 Test RE 0.49312003640412955 Lambda1 0.22646493\n",
      "Training time: 106.73\n",
      "Training time: 106.73\n",
      "inv_HT_atanh_tune21\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 29562.844 Test MSE 3579.331146371599 Test RE 1.007140615643696 Lambda1 -0.00013017654\n",
      "1 Train Loss 18140.307 Test MSE 3579.2320980265504 Test RE 1.0071266805958814 Lambda1 -2.6232738e-07\n",
      "2 Train Loss 12752.381 Test MSE 3565.154361278233 Test RE 1.005144127834602 Lambda1 0.0005285565\n",
      "3 Train Loss 8483.926 Test MSE 3565.0546234450153 Test RE 1.0051300679106299 Lambda1 0.00020598967\n",
      "4 Train Loss 6359.6436 Test MSE 3563.714922738265 Test RE 1.0049411928253185 Lambda1 0.00046135427\n",
      "5 Train Loss 5474.994 Test MSE 3574.221135336905 Test RE 1.0064214396854334 Lambda1 -0.0003638488\n",
      "6 Train Loss 4835.8135 Test MSE 3569.6730737073503 Test RE 1.0057809191934706 Lambda1 4.9570473e-05\n",
      "7 Train Loss 4386.2983 Test MSE 3569.803976149647 Test RE 1.0057993603740816 Lambda1 -0.00026051304\n",
      "8 Train Loss 4070.0676 Test MSE 3569.603827010735 Test RE 1.0057711637701445 Lambda1 0.0001016714\n",
      "9 Train Loss 3883.563 Test MSE 3564.4798539271183 Test RE 1.0050490395011313 Lambda1 0.0002865881\n",
      "10 Train Loss 3784.3516 Test MSE 3556.593037241667 Test RE 1.0039365311740427 Lambda1 0.00017476924\n",
      "11 Train Loss 3695.0356 Test MSE 3548.019570648907 Test RE 1.0027257643549017 Lambda1 0.00021289836\n",
      "12 Train Loss 3654.3997 Test MSE 3544.1788047443138 Test RE 1.0021828871715799 Lambda1 0.00033993152\n",
      "13 Train Loss 3626.1045 Test MSE 3534.787831546548 Test RE 1.0008542697689677 Lambda1 -0.00012570403\n",
      "14 Train Loss 3596.3254 Test MSE 3523.0622504466032 Test RE 0.9991928763612392 Lambda1 0.00018775779\n",
      "15 Train Loss 3570.8782 Test MSE 3504.6015933936665 Test RE 0.9965715794726653 Lambda1 0.00035852843\n",
      "16 Train Loss 3539.5933 Test MSE 3472.7278293326804 Test RE 0.992029402562482 Lambda1 0.00022364863\n",
      "17 Train Loss 3493.8108 Test MSE 3421.1625943888844 Test RE 0.9846367206020677 Lambda1 -9.2737086e-05\n",
      "18 Train Loss 3444.0896 Test MSE 3363.2727254392444 Test RE 0.9762706054373991 Lambda1 7.099373e-05\n",
      "19 Train Loss 3388.6775 Test MSE 3306.8340213217743 Test RE 0.9680446055188194 Lambda1 0.00015246219\n",
      "20 Train Loss 3250.5293 Test MSE 3174.8937675940333 Test RE 0.9485358938005652 Lambda1 0.00019271001\n",
      "21 Train Loss 3073.6848 Test MSE 3030.6423443236904 Test RE 0.9267370245603229 Lambda1 4.940215e-06\n",
      "22 Train Loss 2898.28 Test MSE 2871.438022109906 Test RE 0.90206720471414 Lambda1 6.686884e-05\n",
      "23 Train Loss 2616.2017 Test MSE 2578.3552890398437 Test RE 0.8547922016324699 Lambda1 0.0006160395\n",
      "24 Train Loss 858.2092 Test MSE 858.4139575985993 Test RE 0.4932164533269828 Lambda1 -0.010298581\n",
      "25 Train Loss 854.4334 Test MSE 857.2942659589727 Test RE 0.49289467935242015 Lambda1 -0.009689281\n",
      "26 Train Loss 850.8378 Test MSE 850.8031038660941 Test RE 0.4910251120711317 Lambda1 -0.009323981\n",
      "27 Train Loss 836.7648 Test MSE 831.090070230724 Test RE 0.48530326969500226 Lambda1 -0.00915393\n",
      "28 Train Loss 804.9661 Test MSE 798.4160171190246 Test RE 0.47566784148776503 Lambda1 -0.0053311633\n",
      "29 Train Loss 772.623 Test MSE 775.8528557040529 Test RE 0.46889850938096506 Lambda1 0.0006478396\n",
      "30 Train Loss 747.1846 Test MSE 754.02752599761 Test RE 0.46225622756894985 Lambda1 0.0006079463\n",
      "31 Train Loss 732.26215 Test MSE 739.9455821238806 Test RE 0.45791941937231334 Lambda1 0.00014885911\n",
      "32 Train Loss 711.96106 Test MSE 723.5213705208824 Test RE 0.45280879358050163 Lambda1 0.00014114099\n",
      "33 Train Loss 699.69934 Test MSE 714.2583452710891 Test RE 0.44990086927239464 Lambda1 3.835228e-05\n",
      "34 Train Loss 693.6151 Test MSE 709.9853469864302 Test RE 0.44855310100289886 Lambda1 0.00011600713\n",
      "35 Train Loss 688.3184 Test MSE 705.452391277703 Test RE 0.4471188973645222 Lambda1 0.00021345938\n",
      "36 Train Loss 684.61145 Test MSE 702.1724539392062 Test RE 0.4460782668598361 Lambda1 0.00027679972\n",
      "37 Train Loss 683.1006 Test MSE 700.7911468530605 Test RE 0.4456392903680568 Lambda1 0.00024177702\n",
      "38 Train Loss 681.8255 Test MSE 700.334873716158 Test RE 0.4454941926842932 Lambda1 0.00012088012\n",
      "39 Train Loss 680.4975 Test MSE 699.4289489019008 Test RE 0.4452059628256565 Lambda1 9.306574e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 Train Loss 677.01715 Test MSE 696.4567447103669 Test RE 0.44425901046044347 Lambda1 0.00010638921\n",
      "41 Train Loss 674.43384 Test MSE 692.9577198031076 Test RE 0.44314161824057424 Lambda1 0.000103382525\n",
      "42 Train Loss 667.82935 Test MSE 686.7460891639918 Test RE 0.4411510000655897 Lambda1 0.00011194151\n",
      "43 Train Loss 661.0069 Test MSE 680.0400181619491 Test RE 0.4389917976913449 Lambda1 0.0005545974\n",
      "44 Train Loss 657.8197 Test MSE 677.6104553275734 Test RE 0.438206908236302 Lambda1 0.0006612643\n",
      "45 Train Loss 653.29047 Test MSE 673.533955463472 Test RE 0.4368867949117804 Lambda1 0.0008048756\n",
      "46 Train Loss 649.5216 Test MSE 670.7433316213757 Test RE 0.43598078848849847 Lambda1 0.00080018403\n",
      "47 Train Loss 647.988 Test MSE 668.6682501604828 Test RE 0.43530586828545575 Lambda1 0.0009446913\n",
      "48 Train Loss 645.3301 Test MSE 667.7241329802048 Test RE 0.4349984475871734 Lambda1 0.0008458117\n",
      "49 Train Loss 643.937 Test MSE 665.2520222207144 Test RE 0.43419245491379754 Lambda1 0.00046629872\n",
      "50 Train Loss 642.3826 Test MSE 663.8078588684066 Test RE 0.4337209151924131 Lambda1 0.00025440112\n",
      "51 Train Loss 641.2017 Test MSE 662.8484060304011 Test RE 0.43340735665726693 Lambda1 0.00028790318\n",
      "52 Train Loss 640.38855 Test MSE 662.4716022792506 Test RE 0.4332841514630869 Lambda1 0.0007446802\n",
      "53 Train Loss 639.1951 Test MSE 661.0621882073151 Test RE 0.43282299816298736 Lambda1 0.0005900621\n",
      "54 Train Loss 637.80176 Test MSE 659.9943328738988 Test RE 0.43247327376620426 Lambda1 0.0011557377\n",
      "55 Train Loss 637.0693 Test MSE 659.0511195397974 Test RE 0.432164134443012 Lambda1 0.0019631467\n",
      "56 Train Loss 634.70276 Test MSE 656.6823281979902 Test RE 0.43138678337755293 Lambda1 3.5442394e-05\n",
      "57 Train Loss 631.56415 Test MSE 646.9032511950938 Test RE 0.42816270701190223 Lambda1 0.00020712933\n",
      "58 Train Loss 620.68665 Test MSE 633.0926525477478 Test RE 0.42356767324389477 Lambda1 0.00030498757\n",
      "59 Train Loss 603.6142 Test MSE 608.6540897799966 Test RE 0.4153119647501439 Lambda1 -0.00050868595\n",
      "60 Train Loss 561.47327 Test MSE 548.0548475146661 Test RE 0.39409523362955123 Lambda1 -0.00026038248\n",
      "61 Train Loss 523.11664 Test MSE 518.5811706361351 Test RE 0.3833518315248897 Lambda1 0.00013940153\n",
      "62 Train Loss 497.17596 Test MSE 497.21543787760186 Test RE 0.3753716530130986 Lambda1 9.388276e-05\n",
      "63 Train Loss 471.89456 Test MSE 468.48350493205373 Test RE 0.3643647228695161 Lambda1 4.9651346e-05\n",
      "64 Train Loss 450.70123 Test MSE 456.4462413472422 Test RE 0.35965324923535197 Lambda1 0.00015775021\n",
      "65 Train Loss 435.7568 Test MSE 442.7767534354326 Test RE 0.354226930976935 Lambda1 6.4646454e-05\n",
      "66 Train Loss 423.45285 Test MSE 431.13436674996035 Test RE 0.34953888140697803 Lambda1 -9.28959e-06\n",
      "67 Train Loss 418.94977 Test MSE 428.0935927216383 Test RE 0.34830405815246956 Lambda1 -2.7904845e-05\n",
      "68 Train Loss 415.19138 Test MSE 423.13373503020426 Test RE 0.346280468486543 Lambda1 -2.9244704e-05\n",
      "69 Train Loss 411.13068 Test MSE 421.8424993702946 Test RE 0.34575170974066294 Lambda1 -3.0007997e-05\n",
      "70 Train Loss 405.8238 Test MSE 415.7242286316496 Test RE 0.34323521485462566 Lambda1 -7.853622e-06\n",
      "71 Train Loss 402.1549 Test MSE 415.83145289171125 Test RE 0.34327947588950164 Lambda1 1.0234266e-05\n",
      "72 Train Loss 398.62613 Test MSE 411.64835633514866 Test RE 0.34154848514145386 Lambda1 -1.5855467e-05\n",
      "73 Train Loss 396.78214 Test MSE 410.3110222917938 Test RE 0.3409932345381124 Lambda1 -6.147134e-06\n",
      "74 Train Loss 393.12094 Test MSE 406.13234426526196 Test RE 0.33925242414111434 Lambda1 2.4938512e-05\n",
      "Training time: 123.78\n",
      "Training time: 123.78\n",
      "inv_HT_atanh_tune21\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 19494.15 Test MSE 3559.1798848140597 Test RE 1.0043015657299732 Lambda1 -0.00047317718\n",
      "1 Train Loss 7191.6406 Test MSE 3571.0470883595453 Test RE 1.005974469797888 Lambda1 -0.0017620593\n",
      "2 Train Loss 4751.0312 Test MSE 3573.3367890409863 Test RE 1.0062969258265104 Lambda1 0.00016686253\n",
      "3 Train Loss 3973.1692 Test MSE 3572.132291862951 Test RE 1.0061273106967215 Lambda1 0.00032687865\n",
      "4 Train Loss 3717.9692 Test MSE 3569.572600107116 Test RE 1.0057667645161368 Lambda1 -0.0007344115\n",
      "5 Train Loss 3581.118 Test MSE 3516.4431883497014 Test RE 0.9982538028253203 Lambda1 -0.0014984943\n",
      "6 Train Loss 3228.6777 Test MSE 3152.082450423678 Test RE 0.9451221797435955 Lambda1 0.019923555\n",
      "7 Train Loss 1073.2987 Test MSE 1071.631937768197 Test RE 0.5510766311657695 Lambda1 -0.520226\n",
      "8 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "9 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "10 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "11 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "12 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "13 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "14 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "15 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "16 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "17 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "18 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "19 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "20 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "21 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "22 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "23 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "24 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "25 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "26 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "27 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "28 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "29 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "30 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "31 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "32 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "33 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "34 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "35 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "36 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "37 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "38 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "39 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "41 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "42 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "43 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "44 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "45 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "46 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "47 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "48 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "49 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "50 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "51 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "52 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "53 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "54 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "55 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "56 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "57 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "58 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "59 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "60 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "61 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "62 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "63 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "64 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "65 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "66 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "67 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "68 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "69 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "70 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "71 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "72 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "73 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "74 Train Loss 854.7214 Test MSE 858.0796726585904 Test RE 0.49312040940456414 Lambda1 -0.81769574\n",
      "Training time: 106.72\n",
      "Training time: 106.72\n",
      "inv_HT_atanh_tune21\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 8171.0137 Test MSE 3612.113145858277 Test RE 1.0117421501143942 Lambda1 0.0016949405\n",
      "1 Train Loss 3752.6042 Test MSE 3633.9547405746334 Test RE 1.014796422819415 Lambda1 0.0032657767\n",
      "2 Train Loss 3572.6873 Test MSE 3521.996582321265 Test RE 0.9990417452631218 Lambda1 -0.010827763\n",
      "3 Train Loss 1175.0444 Test MSE 1184.321466626857 Test RE 0.5793272689700041 Lambda1 -3.3026862\n",
      "4 Train Loss 854.7189 Test MSE 858.0715882477463 Test RE 0.4931180864283289 Lambda1 -2.65022\n",
      "5 Train Loss 854.6965 Test MSE 858.0138391739314 Test RE 0.493101492479576 Lambda1 -2.6632104\n",
      "6 Train Loss 854.66077 Test MSE 857.9636863088527 Test RE 0.4930870808157474 Lambda1 -2.6630628\n",
      "7 Train Loss 854.55554 Test MSE 857.7988413509535 Test RE 0.49303970886686665 Lambda1 -2.6471634\n",
      "8 Train Loss 854.3212 Test MSE 857.2381059837167 Test RE 0.4928785347172325 Lambda1 -2.6364787\n",
      "9 Train Loss 853.7885 Test MSE 855.9023602648687 Test RE 0.49249438412252505 Lambda1 -2.633735\n",
      "10 Train Loss 852.71783 Test MSE 853.5744744024951 Test RE 0.49182418424172875 Lambda1 -2.5889578\n",
      "11 Train Loss 850.9543 Test MSE 851.7752013139203 Test RE 0.49130554592855874 Lambda1 -2.5400033\n",
      "12 Train Loss 847.3004 Test MSE 846.5202583887368 Test RE 0.48978767070588713 Lambda1 -2.4989583\n",
      "13 Train Loss 843.8218 Test MSE 841.5777692650072 Test RE 0.4883557415590004 Lambda1 -2.528918\n",
      "14 Train Loss 841.5628 Test MSE 837.7179006911578 Test RE 0.48723454077443545 Lambda1 -2.57927\n",
      "15 Train Loss 837.5139 Test MSE 833.7447649331216 Test RE 0.4860777374074433 Lambda1 -2.6024594\n",
      "16 Train Loss 835.02313 Test MSE 832.5276422615367 Test RE 0.4857228132560455 Lambda1 -2.615573\n",
      "17 Train Loss 832.37885 Test MSE 828.4674493850838 Test RE 0.484536943515257 Lambda1 -2.6220357\n",
      "18 Train Loss 830.5598 Test MSE 827.8066807456163 Test RE 0.4843436766088333 Lambda1 -2.6259644\n",
      "19 Train Loss 828.5697 Test MSE 826.4258529309385 Test RE 0.48393955185006376 Lambda1 -2.6145158\n",
      "20 Train Loss 826.9389 Test MSE 825.9832730280082 Test RE 0.48380995123176423 Lambda1 -2.6186583\n",
      "21 Train Loss 826.3392 Test MSE 825.7967212671101 Test RE 0.483755312900689 Lambda1 -2.6286094\n",
      "22 Train Loss 825.4304 Test MSE 824.7666863505528 Test RE 0.48345351929459035 Lambda1 -2.6133878\n",
      "23 Train Loss 824.6646 Test MSE 823.8832958852734 Test RE 0.4831945414185679 Lambda1 -2.5900176\n",
      "24 Train Loss 823.88245 Test MSE 823.0190287914053 Test RE 0.4829410354175584 Lambda1 -2.6020377\n",
      "25 Train Loss 822.87396 Test MSE 821.6490530875319 Test RE 0.482538922575144 Lambda1 -2.6167037\n",
      "26 Train Loss 822.1506 Test MSE 820.7589583920013 Test RE 0.4822774838758084 Lambda1 -2.6227503\n",
      "27 Train Loss 821.16705 Test MSE 818.9625277389425 Test RE 0.48174940395919164 Lambda1 -2.625784\n",
      "28 Train Loss 818.7565 Test MSE 814.1095174356908 Test RE 0.4803199071403194 Lambda1 -2.6216667\n",
      "29 Train Loss 816.4044 Test MSE 810.6013563609305 Test RE 0.47928389246374814 Lambda1 -2.6167989\n",
      "30 Train Loss 814.59 Test MSE 809.7974252539946 Test RE 0.4790461635173723 Lambda1 -2.6078844\n",
      "31 Train Loss 813.6069 Test MSE 808.2689690530448 Test RE 0.4785938609526109 Lambda1 -2.6265252\n",
      "32 Train Loss 811.5075 Test MSE 805.9579686836473 Test RE 0.47790917407610656 Lambda1 -2.6635551\n",
      "33 Train Loss 807.51404 Test MSE 805.3176912953013 Test RE 0.47771930360723774 Lambda1 -2.6691098\n",
      "34 Train Loss 805.6429 Test MSE 803.4063673567985 Test RE 0.4771520624031639 Lambda1 -2.7023575\n",
      "35 Train Loss 804.06604 Test MSE 801.746892178338 Test RE 0.4766590171985766 Lambda1 -2.713055\n",
      "36 Train Loss 802.81696 Test MSE 800.5080489935536 Test RE 0.4762906128639263 Lambda1 -2.7301133\n",
      "37 Train Loss 800.28467 Test MSE 798.029980834744 Test RE 0.4755528342463582 Lambda1 -2.7438881\n",
      "38 Train Loss 799.1251 Test MSE 797.156120279271 Test RE 0.4752923924710237 Lambda1 -2.7488663\n",
      "39 Train Loss 797.3134 Test MSE 794.6578115346286 Test RE 0.474547018429451 Lambda1 -2.7796183\n",
      "40 Train Loss 797.11554 Test MSE 794.3244232493666 Test RE 0.47444746299093193 Lambda1 -2.7845871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 Train Loss 796.9539 Test MSE 794.172675909494 Test RE 0.4744021417234713 Lambda1 -2.7977922\n",
      "42 Train Loss 796.39703 Test MSE 793.7914511454155 Test RE 0.47428826500688226 Lambda1 -2.7946002\n",
      "43 Train Loss 795.97754 Test MSE 793.9769236190328 Test RE 0.47434367142358597 Lambda1 -2.789737\n",
      "44 Train Loss 795.71185 Test MSE 793.9186486060363 Test RE 0.47432626355565816 Lambda1 -2.78265\n",
      "45 Train Loss 795.426 Test MSE 793.7236470744605 Test RE 0.4742680081989455 Lambda1 -2.7908695\n",
      "46 Train Loss 795.05316 Test MSE 793.5639091002289 Test RE 0.4742202822535448 Lambda1 -2.7809575\n",
      "47 Train Loss 794.6629 Test MSE 793.4103535678435 Test RE 0.47417439894782565 Lambda1 -2.7840822\n",
      "48 Train Loss 794.0374 Test MSE 792.779403686186 Test RE 0.47398582075302237 Lambda1 -2.8113506\n",
      "49 Train Loss 792.86774 Test MSE 791.393634844415 Test RE 0.4735713788132174 Lambda1 -2.8156054\n",
      "50 Train Loss 791.6998 Test MSE 790.0234598244643 Test RE 0.47316124361307316 Lambda1 -2.789176\n",
      "51 Train Loss 790.99054 Test MSE 789.0594722541124 Test RE 0.47287247954099837 Lambda1 -2.7690825\n",
      "52 Train Loss 790.1704 Test MSE 787.7199601571486 Test RE 0.4724709334718541 Lambda1 -2.7824707\n",
      "53 Train Loss 789.92206 Test MSE 787.133724607615 Test RE 0.4722950900098546 Lambda1 -2.794394\n",
      "54 Train Loss 789.76556 Test MSE 786.8858482707614 Test RE 0.4722207189142526 Lambda1 -2.7937014\n",
      "55 Train Loss 789.5732 Test MSE 786.5220733542916 Test RE 0.47211155320036574 Lambda1 -2.79972\n",
      "56 Train Loss 789.5164 Test MSE 786.2238476796878 Test RE 0.4720220394154817 Lambda1 -2.8073068\n",
      "57 Train Loss 789.4451 Test MSE 786.2566197407073 Test RE 0.47203187692799675 Lambda1 -2.8132415\n",
      "58 Train Loss 789.35657 Test MSE 786.3815242274113 Test RE 0.47206936885903633 Lambda1 -2.8105304\n",
      "59 Train Loss 789.2439 Test MSE 786.2569216713966 Test RE 0.472031967560554 Lambda1 -2.8058662\n",
      "60 Train Loss 789.2127 Test MSE 786.2054717419927 Test RE 0.47201652323946175 Lambda1 -2.7964005\n",
      "61 Train Loss 789.0825 Test MSE 786.1719293154331 Test RE 0.4720064541487423 Lambda1 -2.8031285\n",
      "62 Train Loss 788.8503 Test MSE 786.0707660555342 Test RE 0.4719760846799271 Lambda1 -2.8079798\n",
      "63 Train Loss 788.58636 Test MSE 786.1555152179236 Test RE 0.4720015267275801 Lambda1 -2.8105564\n",
      "64 Train Loss 788.457 Test MSE 786.1386023098862 Test RE 0.4719964495124515 Lambda1 -2.8097215\n",
      "65 Train Loss 788.33325 Test MSE 786.1429601224244 Test RE 0.4719977577227271 Lambda1 -2.8068516\n",
      "66 Train Loss 788.0927 Test MSE 785.9860639677861 Test RE 0.47195065539388054 Lambda1 -2.8038514\n",
      "67 Train Loss 787.41254 Test MSE 784.9787390168447 Test RE 0.4716481308888768 Lambda1 -2.7992904\n",
      "68 Train Loss 786.9976 Test MSE 783.86073912397 Test RE 0.47131214061356236 Lambda1 -2.7879186\n",
      "69 Train Loss 786.93146 Test MSE 783.6634785972236 Test RE 0.47125283343925894 Lambda1 -2.7895036\n",
      "70 Train Loss 786.6377 Test MSE 783.1798047357596 Test RE 0.47110738334203234 Lambda1 -2.7962031\n",
      "71 Train Loss 786.23706 Test MSE 782.8567682628899 Test RE 0.47101021500175155 Lambda1 -2.7993882\n",
      "72 Train Loss 786.1109 Test MSE 782.8209673956393 Test RE 0.47099944498119684 Lambda1 -2.7977192\n",
      "73 Train Loss 785.91754 Test MSE 782.616073980483 Test RE 0.4709378018952402 Lambda1 -2.7932975\n",
      "74 Train Loss 785.7654 Test MSE 782.1697688230503 Test RE 0.470803501086293 Lambda1 -2.7886326\n",
      "Training time: 134.97\n",
      "Training time: 134.97\n",
      "inv_HT_atanh_tune21\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 6376.167 Test MSE 3375.2720921459586 Test RE 0.9780106068872039 Lambda1 -0.0014354335\n",
      "1 Train Loss 3932.1172 Test MSE 3388.404207444711 Test RE 0.9799113247545725 Lambda1 -0.0017292574\n",
      "2 Train Loss 3491.008 Test MSE 3362.2637330256985 Test RE 0.9761241523238016 Lambda1 -0.0017470976\n",
      "3 Train Loss 3353.4758 Test MSE 3299.498346863957 Test RE 0.9669702844819387 Lambda1 0.003212664\n",
      "4 Train Loss 2764.0012 Test MSE 2740.5042874516225 Test RE 0.8812607212141995 Lambda1 0.038775615\n",
      "5 Train Loss 854.7125 Test MSE 858.0603078240885 Test RE 0.49311484509084236 Lambda1 0.20960273\n",
      "6 Train Loss 854.42395 Test MSE 857.6322334152576 Test RE 0.4929918256609184 Lambda1 0.20696124\n",
      "7 Train Loss 854.1426 Test MSE 856.9008867145702 Test RE 0.4927815812063006 Lambda1 0.20673351\n",
      "8 Train Loss 853.4828 Test MSE 855.4138296418022 Test RE 0.4923538114699498 Lambda1 0.19582066\n",
      "9 Train Loss 851.9741 Test MSE 852.883339726863 Test RE 0.4916250302175035 Lambda1 0.1928702\n",
      "10 Train Loss 849.13995 Test MSE 848.8040152043192 Test RE 0.49044790448126613 Lambda1 0.18684348\n",
      "11 Train Loss 843.31415 Test MSE 843.8622460455717 Test RE 0.4890181172269395 Lambda1 0.1773853\n",
      "12 Train Loss 836.17126 Test MSE 835.0404795475586 Test RE 0.48645529511217095 Lambda1 0.15959819\n",
      "13 Train Loss 828.8409 Test MSE 821.5111763738151 Test RE 0.48249843468562437 Lambda1 0.15938531\n",
      "14 Train Loss 818.15216 Test MSE 812.324214320748 Test RE 0.47979295883213513 Lambda1 0.14406519\n",
      "15 Train Loss 810.1305 Test MSE 805.8624372374196 Test RE 0.47788084957947075 Lambda1 0.12988596\n",
      "16 Train Loss 804.8772 Test MSE 803.3392799036195 Test RE 0.4771321399913981 Lambda1 0.105878845\n",
      "17 Train Loss 795.0517 Test MSE 792.667177309903 Test RE 0.47395227069263185 Lambda1 0.05818055\n",
      "18 Train Loss 780.8152 Test MSE 780.9551470620089 Test RE 0.470437806593276 Lambda1 0.011472255\n",
      "19 Train Loss 768.2055 Test MSE 773.1045043439929 Test RE 0.46806726855300756 Lambda1 0.0021599242\n",
      "20 Train Loss 757.0301 Test MSE 763.0245480898369 Test RE 0.46500585989415505 Lambda1 0.00021470473\n",
      "21 Train Loss 739.9258 Test MSE 750.0509811082707 Test RE 0.46103570675791344 Lambda1 -0.00012253596\n",
      "22 Train Loss 736.21606 Test MSE 746.7616734407995 Test RE 0.460023672510705 Lambda1 -0.0001397427\n",
      "23 Train Loss 733.25635 Test MSE 745.0750587800438 Test RE 0.45950388066142667 Lambda1 2.1271813e-05\n",
      "24 Train Loss 726.5392 Test MSE 739.729372066875 Test RE 0.4578525130892078 Lambda1 0.00010398924\n",
      "25 Train Loss 724.0534 Test MSE 737.5941509122478 Test RE 0.4571912422931073 Lambda1 0.00011710749\n",
      "26 Train Loss 722.16486 Test MSE 736.144141849217 Test RE 0.4567416335395762 Lambda1 0.00017314851\n",
      "27 Train Loss 720.474 Test MSE 734.2939728209411 Test RE 0.4561673024981155 Lambda1 0.0001903594\n",
      "28 Train Loss 720.0725 Test MSE 734.254826211752 Test RE 0.456155142762515 Lambda1 0.00011883582\n",
      "29 Train Loss 719.32733 Test MSE 732.9139894496402 Test RE 0.4557384556362224 Lambda1 0.000108610686\n",
      "30 Train Loss 718.7017 Test MSE 732.344020928104 Test RE 0.45556121308206976 Lambda1 0.0001361361\n",
      "31 Train Loss 717.0231 Test MSE 731.4097875452902 Test RE 0.4552705461962256 Lambda1 2.816838e-05\n",
      "32 Train Loss 716.42194 Test MSE 731.2817191734355 Test RE 0.45523068597619154 Lambda1 1.7237515e-05\n",
      "33 Train Loss 714.67523 Test MSE 729.2180897813855 Test RE 0.4545879165163365 Lambda1 -9.709515e-05\n",
      "34 Train Loss 711.65656 Test MSE 725.9832508361183 Test RE 0.4535785112620171 Lambda1 -0.00022891219\n",
      "35 Train Loss 709.1631 Test MSE 723.3618557579674 Test RE 0.452758875449638 Lambda1 -6.7788904e-05\n",
      "36 Train Loss 706.64484 Test MSE 721.1529240834944 Test RE 0.45206705150768933 Lambda1 -0.00021669519\n",
      "37 Train Loss 701.23346 Test MSE 714.473634989237 Test RE 0.4499686680838035 Lambda1 -0.0006211525\n",
      "38 Train Loss 697.5809 Test MSE 708.2533189136681 Test RE 0.44800563832199164 Lambda1 -0.00075072754\n",
      "39 Train Loss 692.4835 Test MSE 702.0839088974681 Test RE 0.44605014039047314 Lambda1 -0.000680262\n",
      "40 Train Loss 684.3061 Test MSE 698.8124582615364 Test RE 0.4450097128591539 Lambda1 8.041014e-05\n",
      "41 Train Loss 682.8916 Test MSE 698.7001010052817 Test RE 0.4449739363931608 Lambda1 0.00019328437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 Train Loss 681.5124 Test MSE 698.7646518766115 Test RE 0.44499449084269027 Lambda1 0.00013484266\n",
      "43 Train Loss 680.58527 Test MSE 698.7797398675044 Test RE 0.44499929506151026 Lambda1 9.9240184e-05\n",
      "44 Train Loss 679.91656 Test MSE 698.1102905291996 Test RE 0.4447860834793032 Lambda1 0.00015040566\n",
      "45 Train Loss 679.7122 Test MSE 697.8482560236907 Test RE 0.44470260079632234 Lambda1 0.00014338657\n",
      "46 Train Loss 679.50024 Test MSE 697.0915937316906 Test RE 0.4444614445363898 Lambda1 0.00019370028\n",
      "47 Train Loss 678.95087 Test MSE 696.8525666902138 Test RE 0.4443852368947271 Lambda1 0.00014037572\n",
      "48 Train Loss 678.5079 Test MSE 697.2616926132756 Test RE 0.44451566824521677 Lambda1 7.281126e-05\n",
      "49 Train Loss 678.1027 Test MSE 696.6151104431619 Test RE 0.4443095171180657 Lambda1 0.00019860457\n",
      "50 Train Loss 677.3952 Test MSE 696.6128710181312 Test RE 0.4443088029513481 Lambda1 5.9600177e-05\n",
      "51 Train Loss 676.29913 Test MSE 695.9941218938526 Test RE 0.4441114359717363 Lambda1 6.431436e-06\n",
      "52 Train Loss 675.553 Test MSE 695.0778052797556 Test RE 0.4438189904575022 Lambda1 -1.9649317e-06\n",
      "53 Train Loss 675.03796 Test MSE 693.8281954731917 Test RE 0.44341986244051923 Lambda1 2.0826237e-06\n",
      "54 Train Loss 673.4603 Test MSE 692.048192758312 Test RE 0.44285070465720017 Lambda1 2.4267303e-05\n",
      "55 Train Loss 670.9596 Test MSE 690.283946713131 Test RE 0.4422858623594862 Lambda1 4.2024294e-07\n",
      "56 Train Loss 668.7193 Test MSE 686.9226444575231 Test RE 0.44120770409358545 Lambda1 8.709659e-05\n",
      "57 Train Loss 665.88904 Test MSE 684.6021370312161 Test RE 0.44046184728032034 Lambda1 9.164602e-05\n",
      "58 Train Loss 664.4836 Test MSE 683.9781476098146 Test RE 0.4402610692120531 Lambda1 6.0557344e-05\n",
      "59 Train Loss 663.7035 Test MSE 683.5722955052421 Test RE 0.44013043097751264 Lambda1 4.0574352e-05\n",
      "60 Train Loss 663.2477 Test MSE 682.759836011671 Test RE 0.4398687948159824 Lambda1 2.178727e-05\n",
      "61 Train Loss 662.5103 Test MSE 681.7799455943726 Test RE 0.4395530337274218 Lambda1 0.000112324895\n",
      "62 Train Loss 661.34094 Test MSE 680.9092159667584 Test RE 0.43927225828759736 Lambda1 2.7987151e-05\n",
      "63 Train Loss 658.73566 Test MSE 678.2022775280029 Test RE 0.43839823053114185 Lambda1 4.296682e-05\n",
      "64 Train Loss 655.9469 Test MSE 675.4472247740941 Test RE 0.4375068745092351 Lambda1 1.4724928e-06\n",
      "65 Train Loss 654.9371 Test MSE 674.4007931060146 Test RE 0.4371678409858481 Lambda1 -8.145339e-06\n",
      "66 Train Loss 652.8861 Test MSE 673.6756974941372 Test RE 0.4369327628717881 Lambda1 -3.0283545e-06\n",
      "67 Train Loss 651.534 Test MSE 671.9351136344435 Test RE 0.43636794363220116 Lambda1 -8.805552e-06\n",
      "68 Train Loss 649.91425 Test MSE 670.3917786753885 Test RE 0.43586651942922866 Lambda1 -2.5846103e-07\n",
      "69 Train Loss 648.3968 Test MSE 668.0734019826499 Test RE 0.43511220086019575 Lambda1 2.0440077e-06\n",
      "70 Train Loss 646.29724 Test MSE 666.9766182313541 Test RE 0.4347548898091626 Lambda1 -2.2410281e-06\n",
      "71 Train Loss 645.2586 Test MSE 666.0564062032687 Test RE 0.4344548757203149 Lambda1 1.3203718e-06\n",
      "72 Train Loss 643.6651 Test MSE 665.6039561529079 Test RE 0.4343072887275983 Lambda1 1.1194642e-06\n",
      "73 Train Loss 642.63 Test MSE 665.0015439891579 Test RE 0.4341107069493903 Lambda1 5.367837e-07\n",
      "74 Train Loss 642.4123 Test MSE 664.8480614168053 Test RE 0.4340606076124037 Lambda1 1.4317432e-06\n",
      "Training time: 129.49\n",
      "Training time: 129.49\n",
      "inv_HT_atanh_tune21\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 9723.512 Test MSE 3641.6904235090487 Test RE 1.0158759588449808 Lambda1 -0.0004465818\n",
      "1 Train Loss 4845.4766 Test MSE 3601.6775779156274 Test RE 1.0102796072379925 Lambda1 -0.0014991045\n",
      "2 Train Loss 3757.7458 Test MSE 3557.007034936635 Test RE 1.0039949600287212 Lambda1 0.0011773517\n",
      "3 Train Loss 3561.8672 Test MSE 3500.3071398213974 Test RE 0.9959608050181832 Lambda1 -0.00078988774\n",
      "4 Train Loss 3274.1843 Test MSE 3259.9583820638068 Test RE 0.9611589150051654 Lambda1 -0.0046825046\n",
      "5 Train Loss 854.71136 Test MSE 858.060857923243 Test RE 0.49311500315782986 Lambda1 -0.29015848\n",
      "6 Train Loss 854.6718 Test MSE 857.9817241806027 Test RE 0.493092264132314 Lambda1 -0.26065657\n",
      "7 Train Loss 854.55927 Test MSE 857.7124113094524 Test RE 0.4930148694092285 Lambda1 -0.14418623\n",
      "8 Train Loss 854.4401 Test MSE 857.5848746031413 Test RE 0.4929782138651741 Lambda1 0.22289202\n",
      "9 Train Loss 854.03186 Test MSE 856.8927380815977 Test RE 0.49277923816648744 Lambda1 0.26276544\n",
      "10 Train Loss 853.2066 Test MSE 854.7518477755189 Test RE 0.4921632648800463 Lambda1 0.2844549\n",
      "11 Train Loss 851.55475 Test MSE 852.0478457651135 Test RE 0.4913841705546954 Lambda1 0.17149423\n",
      "12 Train Loss 849.55774 Test MSE 849.0069625016755 Test RE 0.4905065335204664 Lambda1 0.16199902\n",
      "13 Train Loss 845.5923 Test MSE 843.3170367838081 Test RE 0.48886011733632173 Lambda1 0.1248669\n",
      "14 Train Loss 841.8617 Test MSE 841.2570543021768 Test RE 0.4882626795011361 Lambda1 0.10143499\n",
      "15 Train Loss 840.10846 Test MSE 837.5708396806483 Test RE 0.4871917719922413 Lambda1 0.10639423\n",
      "16 Train Loss 837.4264 Test MSE 835.1686751820122 Test RE 0.48649263405169846 Lambda1 0.10416334\n",
      "17 Train Loss 834.1022 Test MSE 829.759897371274 Test RE 0.48491474637716586 Lambda1 0.10564641\n",
      "18 Train Loss 831.8825 Test MSE 827.4529809907847 Test RE 0.484240191974766 Lambda1 0.10851727\n",
      "19 Train Loss 829.4352 Test MSE 822.9041931761898 Test RE 0.4829073419283142 Lambda1 0.11366698\n",
      "20 Train Loss 827.5752 Test MSE 820.592953579202 Test RE 0.4822287092439716 Lambda1 0.11418116\n",
      "21 Train Loss 825.5062 Test MSE 818.5370894677598 Test RE 0.48162425705001 Lambda1 0.11093367\n",
      "22 Train Loss 823.917 Test MSE 817.0844282284258 Test RE 0.48119669696321943 Lambda1 0.10902413\n",
      "23 Train Loss 822.832 Test MSE 814.480503512668 Test RE 0.4804293344953174 Lambda1 0.10344327\n",
      "24 Train Loss 819.67065 Test MSE 810.8392359488905 Test RE 0.4793542127825398 Lambda1 0.093802616\n",
      "25 Train Loss 818.0382 Test MSE 808.2565187861661 Test RE 0.4785901748997809 Lambda1 0.09050448\n",
      "26 Train Loss 813.4468 Test MSE 803.4574041244312 Test RE 0.47716721781715377 Lambda1 0.09722187\n",
      "27 Train Loss 810.29706 Test MSE 799.0027900588462 Test RE 0.4758425986004664 Lambda1 0.1005546\n",
      "28 Train Loss 809.41174 Test MSE 797.9416517291991 Test RE 0.4755265154864835 Lambda1 0.1020242\n",
      "29 Train Loss 808.41455 Test MSE 796.3002409287387 Test RE 0.4750371713277527 Lambda1 0.10065517\n",
      "30 Train Loss 806.1009 Test MSE 794.9666103322411 Test RE 0.47463921239851803 Lambda1 0.093332954\n",
      "31 Train Loss 805.12933 Test MSE 793.8562859284655 Test RE 0.4743076339159602 Lambda1 0.09496561\n",
      "32 Train Loss 803.88025 Test MSE 792.0536899051265 Test RE 0.47376882672142 Lambda1 0.091670156\n",
      "33 Train Loss 801.14984 Test MSE 789.0034164483764 Test RE 0.4728556825063152 Lambda1 0.085354336\n",
      "34 Train Loss 799.1052 Test MSE 788.7709992935484 Test RE 0.4727860327021561 Lambda1 0.08073022\n",
      "35 Train Loss 798.2727 Test MSE 787.3420274975991 Test RE 0.4723575787085606 Lambda1 0.08190139\n",
      "36 Train Loss 796.8934 Test MSE 786.6005722129855 Test RE 0.4721351121662899 Lambda1 0.075777695\n",
      "37 Train Loss 795.83136 Test MSE 785.8037548892197 Test RE 0.4718959178591314 Lambda1 0.07819664\n",
      "38 Train Loss 795.2748 Test MSE 784.5746350675294 Test RE 0.4715267139647029 Lambda1 0.08006175\n",
      "39 Train Loss 793.74884 Test MSE 782.8017003026124 Test RE 0.47099364873509525 Lambda1 0.074615516\n",
      "40 Train Loss 791.07574 Test MSE 778.1545804155093 Test RE 0.4695935355004501 Lambda1 0.059053242\n",
      "41 Train Loss 780.01337 Test MSE 755.8153103144506 Test RE 0.46280390330513316 Lambda1 0.015369699\n",
      "42 Train Loss 752.1967 Test MSE 739.702330498187 Test RE 0.4578441443777561 Lambda1 0.0049861115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 Train Loss 738.5187 Test MSE 729.0569905177266 Test RE 0.45453769983575515 Lambda1 0.0010020435\n",
      "44 Train Loss 733.28986 Test MSE 726.1546272443927 Test RE 0.4536320442251869 Lambda1 0.00027103216\n",
      "45 Train Loss 726.91785 Test MSE 720.7211861561874 Test RE 0.45193171009417815 Lambda1 -0.00012959646\n",
      "46 Train Loss 721.71204 Test MSE 715.5563012406675 Test RE 0.4503094654570136 Lambda1 -0.00016850312\n",
      "47 Train Loss 717.27704 Test MSE 710.4998935086767 Test RE 0.448715611142863 Lambda1 4.4587716e-05\n",
      "48 Train Loss 715.7067 Test MSE 708.7390934741961 Test RE 0.448159250336401 Lambda1 5.3368734e-05\n",
      "49 Train Loss 714.8108 Test MSE 708.8452339900296 Test RE 0.4481928071654523 Lambda1 5.8654412e-05\n",
      "50 Train Loss 714.02136 Test MSE 707.5678746826744 Test RE 0.4477887969699257 Lambda1 7.848822e-05\n",
      "51 Train Loss 713.40173 Test MSE 706.6109807058948 Test RE 0.4474859063327385 Lambda1 9.452695e-05\n",
      "52 Train Loss 712.02454 Test MSE 705.3943418481681 Test RE 0.4471005009913526 Lambda1 4.5763227e-05\n",
      "53 Train Loss 710.9356 Test MSE 704.9394106689983 Test RE 0.4469563030928122 Lambda1 2.9906565e-05\n",
      "54 Train Loss 710.3968 Test MSE 704.2726445476428 Test RE 0.4467448765369802 Lambda1 1.1924339e-05\n",
      "55 Train Loss 709.4813 Test MSE 703.381999920452 Test RE 0.44646230358533723 Lambda1 3.9323604e-05\n",
      "56 Train Loss 706.9514 Test MSE 699.2109421140675 Test RE 0.4451365737288306 Lambda1 9.164014e-05\n",
      "57 Train Loss 703.7045 Test MSE 694.6993542572953 Test RE 0.44369815030219634 Lambda1 0.00040499214\n",
      "58 Train Loss 702.2456 Test MSE 693.2230200870163 Test RE 0.44322643896149827 Lambda1 0.00032463556\n",
      "59 Train Loss 700.39734 Test MSE 690.2578987012093 Test RE 0.4422775174050975 Lambda1 0.00034317374\n",
      "60 Train Loss 699.00415 Test MSE 689.3371363031707 Test RE 0.4419824332018998 Lambda1 0.0001947076\n",
      "61 Train Loss 697.6498 Test MSE 688.3940902518139 Test RE 0.4416800032166657 Lambda1 0.00014105829\n",
      "62 Train Loss 696.9931 Test MSE 688.0913566346866 Test RE 0.4415828741952177 Lambda1 9.230536e-05\n",
      "63 Train Loss 696.5456 Test MSE 688.0120130508642 Test RE 0.4415574140746006 Lambda1 5.191563e-05\n",
      "64 Train Loss 696.3807 Test MSE 687.8488637594957 Test RE 0.4415050573939983 Lambda1 3.6054804e-05\n",
      "65 Train Loss 696.17285 Test MSE 688.1135591112649 Test RE 0.44158999836139184 Lambda1 2.888416e-05\n",
      "66 Train Loss 695.75446 Test MSE 688.1573950500926 Test RE 0.44160406377517303 Lambda1 2.1181379e-05\n",
      "67 Train Loss 695.4905 Test MSE 687.6067685629299 Test RE 0.4414273545250544 Lambda1 3.1111907e-05\n",
      "68 Train Loss 695.22876 Test MSE 687.2668327093367 Test RE 0.441318225628913 Lambda1 1.8774252e-05\n",
      "69 Train Loss 694.6847 Test MSE 686.8245342612804 Test RE 0.4411761950725078 Lambda1 1.2513783e-05\n",
      "70 Train Loss 694.2789 Test MSE 686.3451434112609 Test RE 0.4410222017754253 Lambda1 2.5629986e-05\n",
      "71 Train Loss 693.799 Test MSE 686.0270307903985 Test RE 0.4409199857185292 Lambda1 0.00010363509\n",
      "72 Train Loss 693.24274 Test MSE 685.2758335659671 Test RE 0.44067851666929414 Lambda1 6.1236016e-05\n",
      "73 Train Loss 692.73663 Test MSE 684.8903015414512 Test RE 0.44055453770005054 Lambda1 0.000234442\n",
      "74 Train Loss 692.1751 Test MSE 684.1584853549568 Test RE 0.440319105025108 Lambda1 0.0004927175\n",
      "Training time: 127.91\n",
      "Training time: 127.91\n",
      "inv_HT_atanh_tune22\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1501240.8 Test MSE 3553.4544091554103 Test RE 1.003493455618487 Lambda1 -5.8929305e-05\n",
      "1 Train Loss 1373378.9 Test MSE 3553.147480504934 Test RE 1.0034501164465446 Lambda1 1.8679937e-05\n",
      "2 Train Loss 1307421.8 Test MSE 3553.206713167333 Test RE 1.0034584804075959 Lambda1 5.889568e-06\n",
      "3 Train Loss 1249352.9 Test MSE 3552.9257525374032 Test RE 1.0034188066814367 Lambda1 8.166758e-06\n",
      "4 Train Loss 1194346.9 Test MSE 3552.365323169256 Test RE 1.003339665209807 Lambda1 3.0160263e-06\n",
      "5 Train Loss 1154775.5 Test MSE 3551.9515049268507 Test RE 1.0032812235362336 Lambda1 3.382792e-06\n",
      "6 Train Loss 1111794.2 Test MSE 3551.445395216374 Test RE 1.0032097433290963 Lambda1 2.6750566e-05\n",
      "7 Train Loss 1080171.0 Test MSE 3550.847171887465 Test RE 1.0031252469226497 Lambda1 -3.1397405e-05\n",
      "8 Train Loss 1038997.2 Test MSE 3550.5891071673746 Test RE 1.0030887942217597 Lambda1 9.662354e-06\n",
      "9 Train Loss 1003600.6 Test MSE 3550.5083040232525 Test RE 1.003077380173681 Lambda1 3.5274174e-05\n",
      "10 Train Loss 969811.94 Test MSE 3550.547388882646 Test RE 1.003082901218432 Lambda1 2.1043801e-05\n",
      "11 Train Loss 940088.1 Test MSE 3550.6265923128003 Test RE 1.0030940892346365 Lambda1 1.568058e-05\n",
      "12 Train Loss 920874.25 Test MSE 3550.652005898284 Test RE 1.003097679047745 Lambda1 2.647231e-06\n",
      "13 Train Loss 901168.56 Test MSE 3550.9409362186866 Test RE 1.0031384911911918 Lambda1 -8.486339e-06\n",
      "14 Train Loss 878752.6 Test MSE 3551.004522904594 Test RE 1.0031474727495668 Lambda1 3.3779925e-05\n",
      "15 Train Loss 854944.0 Test MSE 3551.0901536770666 Test RE 1.003159567886955 Lambda1 9.704766e-06\n",
      "16 Train Loss 837615.0 Test MSE 3551.304991464998 Test RE 1.0031899125576953 Lambda1 1.0935639e-05\n",
      "17 Train Loss 820652.9 Test MSE 3551.2928636201077 Test RE 1.003188199589917 Lambda1 1.4665655e-05\n",
      "18 Train Loss 804905.94 Test MSE 3551.3380289341735 Test RE 1.0031945788393597 Lambda1 -1.6521153e-07\n",
      "19 Train Loss 788135.25 Test MSE 3551.2730734655784 Test RE 1.0031854043716804 Lambda1 -4.384378e-06\n",
      "20 Train Loss 768758.56 Test MSE 3551.1494442490452 Test RE 1.0031679424499378 Lambda1 -3.456729e-06\n",
      "21 Train Loss 751601.94 Test MSE 3550.748629095538 Test RE 1.0031113275051793 Lambda1 6.9824528e-06\n",
      "22 Train Loss 735889.3 Test MSE 3550.4444448743275 Test RE 1.003068359499689 Lambda1 8.897072e-06\n",
      "23 Train Loss 723598.6 Test MSE 3550.4295543385388 Test RE 1.0030662560671189 Lambda1 -2.1517576e-06\n",
      "24 Train Loss 709933.9 Test MSE 3550.143202022236 Test RE 1.0030258051677061 Lambda1 1.2753275e-05\n",
      "25 Train Loss 696086.5 Test MSE 3549.959292033141 Test RE 1.002999824686996 Lambda1 2.196058e-05\n",
      "26 Train Loss 683282.0 Test MSE 3549.963026632603 Test RE 1.003000352270739 Lambda1 1.8510847e-05\n",
      "27 Train Loss 671142.8 Test MSE 3550.187851341355 Test RE 1.0030321125581267 Lambda1 1.8462097e-05\n",
      "28 Train Loss 661745.2 Test MSE 3550.3207059572414 Test RE 1.003050880043814 Lambda1 -1.802554e-05\n",
      "29 Train Loss 650880.75 Test MSE 3550.377293693912 Test RE 1.0030588737094006 Lambda1 -4.653454e-05\n",
      "30 Train Loss 641252.06 Test MSE 3550.5724255741543 Test RE 1.0030864378326678 Lambda1 -1.8861758e-05\n",
      "31 Train Loss 631081.7 Test MSE 3550.8305417646757 Test RE 1.003122897889482 Lambda1 -2.6270176e-05\n",
      "32 Train Loss 620955.7 Test MSE 3551.104828429184 Test RE 1.003161640645199 Lambda1 3.1185795e-05\n",
      "33 Train Loss 611285.7 Test MSE 3551.131848425202 Test RE 1.0031654571154647 Lambda1 1.9873765e-05\n",
      "34 Train Loss 602410.4 Test MSE 3551.3070562926373 Test RE 1.0031902041989318 Lambda1 1.349166e-05\n",
      "35 Train Loss 594616.25 Test MSE 3551.234432591492 Test RE 1.00317994660125 Lambda1 3.437438e-05\n",
      "36 Train Loss 586437.94 Test MSE 3551.1484222429776 Test RE 1.0031677980961426 Lambda1 -4.8040974e-06\n",
      "37 Train Loss 577592.5 Test MSE 3551.0740330261992 Test RE 1.0031572908955506 Lambda1 -3.3790464e-05\n",
      "38 Train Loss 567491.7 Test MSE 3551.1490905322266 Test RE 1.0031678924890186 Lambda1 2.4757444e-05\n",
      "39 Train Loss 559370.44 Test MSE 3551.0373531642667 Test RE 1.003152109960768 Lambda1 -1.6093325e-05\n",
      "40 Train Loss 553906.44 Test MSE 3550.973692781559 Test RE 1.003143118034311 Lambda1 3.992238e-05\n",
      "41 Train Loss 546297.9 Test MSE 3551.0792522865117 Test RE 1.0031580281002757 Lambda1 2.1241654e-05\n",
      "42 Train Loss 540082.06 Test MSE 3551.232249414835 Test RE 1.0031796382409572 Lambda1 2.5719535e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 Train Loss 532171.56 Test MSE 3551.35283788698 Test RE 1.0031966704799702 Lambda1 1.489982e-05\n",
      "44 Train Loss 524922.1 Test MSE 3551.4627757707767 Test RE 1.0032121981493523 Lambda1 -1.2421453e-05\n",
      "45 Train Loss 518231.66 Test MSE 3551.369730392466 Test RE 1.0031990563996296 Lambda1 -1.1120228e-05\n",
      "46 Train Loss 511817.9 Test MSE 3551.317174582243 Test RE 1.0031916333293371 Lambda1 3.5981438e-06\n",
      "47 Train Loss 504729.34 Test MSE 3551.3952604237566 Test RE 1.003202662284009 Lambda1 -1.6787984e-05\n",
      "48 Train Loss 499165.62 Test MSE 3551.4160926024124 Test RE 1.003205604629891 Lambda1 1.6854041e-05\n",
      "49 Train Loss 493756.88 Test MSE 3551.547841272164 Test RE 1.0032242126688633 Lambda1 8.982263e-06\n",
      "50 Train Loss 487470.75 Test MSE 3551.7143696139633 Test RE 1.003247732457097 Lambda1 1.4682477e-06\n",
      "51 Train Loss 482356.8 Test MSE 3551.676788089979 Test RE 1.0032424246431044 Lambda1 3.235058e-05\n",
      "52 Train Loss 477329.66 Test MSE 3551.812035561328 Test RE 1.0032615261432514 Lambda1 2.868991e-05\n",
      "53 Train Loss 469999.28 Test MSE 3552.253117768568 Test RE 1.0033238192863272 Lambda1 9.527976e-06\n",
      "54 Train Loss 464316.84 Test MSE 3552.3084663816862 Test RE 1.0033316357852855 Lambda1 3.5933757e-05\n",
      "55 Train Loss 458478.16 Test MSE 3552.5132609784523 Test RE 1.0033605569696802 Lambda1 3.855389e-05\n",
      "56 Train Loss 451423.03 Test MSE 3552.7020469250106 Test RE 1.0033872166669304 Lambda1 1.01725755e-05\n",
      "57 Train Loss 446343.5 Test MSE 3552.8944695745195 Test RE 1.0034143891978515 Lambda1 3.4702247e-05\n",
      "58 Train Loss 441139.22 Test MSE 3553.0033198559754 Test RE 1.0034297599192545 Lambda1 2.894962e-05\n",
      "59 Train Loss 437395.7 Test MSE 3553.17585441024 Test RE 1.0034541229987495 Lambda1 1.1695298e-05\n",
      "60 Train Loss 433076.22 Test MSE 3553.2122770622072 Test RE 1.0034592660550068 Lambda1 2.331026e-05\n",
      "61 Train Loss 429605.75 Test MSE 3553.3928104411416 Test RE 1.003484757860276 Lambda1 2.5338164e-05\n",
      "62 Train Loss 425386.9 Test MSE 3553.7318694548817 Test RE 1.0035326321672466 Lambda1 2.6089703e-05\n",
      "63 Train Loss 420432.0 Test MSE 3553.9453568520994 Test RE 1.0035627748960787 Lambda1 2.7420245e-05\n",
      "64 Train Loss 415754.88 Test MSE 3553.973675545856 Test RE 1.0035667732032305 Lambda1 1.98125e-05\n",
      "65 Train Loss 412413.75 Test MSE 3554.1113271708796 Test RE 1.003586207964453 Lambda1 2.3223545e-05\n",
      "66 Train Loss 407749.72 Test MSE 3554.2021068256945 Test RE 1.0035990247587796 Lambda1 3.178562e-05\n",
      "67 Train Loss 403169.78 Test MSE 3554.448654813057 Test RE 1.0036338329972199 Lambda1 2.7765174e-05\n",
      "68 Train Loss 399132.53 Test MSE 3554.632987642843 Test RE 1.0036598567612383 Lambda1 -5.8386104e-06\n",
      "69 Train Loss 395381.9 Test MSE 3554.6817126492197 Test RE 1.0036667355537194 Lambda1 1.146936e-05\n",
      "70 Train Loss 391438.03 Test MSE 3554.8190119634896 Test RE 1.003686118642707 Lambda1 1.45192835e-05\n",
      "71 Train Loss 386978.84 Test MSE 3554.9363243790367 Test RE 1.0037026798053463 Lambda1 1.730078e-05\n",
      "72 Train Loss 383252.66 Test MSE 3554.9834329844466 Test RE 1.0037093301181135 Lambda1 2.0482814e-05\n",
      "73 Train Loss 379410.88 Test MSE 3554.9943429807163 Test RE 1.0037108702739053 Lambda1 1.2766038e-05\n",
      "74 Train Loss 375404.0 Test MSE 3554.917772243742 Test RE 1.0037000607918811 Lambda1 1.9192383e-05\n",
      "Training time: 118.04\n",
      "Training time: 118.04\n",
      "inv_HT_atanh_tune22\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.04\n",
      "Training time: 4.04\n",
      "inv_HT_atanh_tune22\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 714696.6 Test MSE 3554.7297203289277 Test RE 1.003673513030915 Lambda1 -0.00019628489\n",
      "1 Train Loss 616574.06 Test MSE 3554.8746664717683 Test RE 1.0036939755038548 Lambda1 -1.5023428e-05\n",
      "2 Train Loss 594521.25 Test MSE 3554.985575485029 Test RE 1.0037096325734614 Lambda1 3.8462364e-05\n",
      "3 Train Loss 569596.3 Test MSE 3555.3215188618465 Test RE 1.0037570563436131 Lambda1 1.7952852e-06\n",
      "4 Train Loss 543295.0 Test MSE 3555.8841548878595 Test RE 1.0038364763815566 Lambda1 7.6005135e-06\n",
      "5 Train Loss 524344.75 Test MSE 3555.849122351109 Test RE 1.0038315314756858 Lambda1 1.1143995e-06\n",
      "6 Train Loss 514285.6 Test MSE 3555.9292809534095 Test RE 1.0038428459713613 Lambda1 -2.0015572e-05\n",
      "7 Train Loss 504789.28 Test MSE 3555.961075944128 Test RE 1.0038473338421965 Lambda1 1.2189826e-05\n",
      "8 Train Loss 495048.22 Test MSE 3556.281888366952 Test RE 1.003892615472837 Lambda1 2.4501933e-05\n",
      "9 Train Loss 481435.97 Test MSE 3556.7432664443636 Test RE 1.0039577339080132 Lambda1 3.7121945e-06\n",
      "10 Train Loss 469507.78 Test MSE 3556.599779535956 Test RE 1.0039374827634235 Lambda1 -9.042091e-06\n",
      "11 Train Loss 458988.53 Test MSE 3556.8060244250914 Test RE 1.0039665911796045 Lambda1 2.7271244e-06\n",
      "12 Train Loss 445298.03 Test MSE 3557.199121521797 Test RE 1.0040220686846684 Lambda1 1.2630257e-05\n",
      "13 Train Loss 434677.62 Test MSE 3557.2977160543696 Test RE 1.0040359827779268 Lambda1 4.1268922e-05\n",
      "14 Train Loss 429886.2 Test MSE 3557.5488648221904 Test RE 1.0040714251234202 Lambda1 7.567436e-06\n",
      "15 Train Loss 422724.56 Test MSE 3557.5940656284674 Test RE 1.0040778037700024 Lambda1 1.0202137e-05\n",
      "16 Train Loss 415834.12 Test MSE 3557.19173425153 Test RE 1.0040210261528937 Lambda1 1.5434534e-05\n",
      "17 Train Loss 406916.84 Test MSE 3556.534700519431 Test RE 1.0039282976474475 Lambda1 3.6275258e-06\n",
      "18 Train Loss 399758.94 Test MSE 3556.254409879496 Test RE 1.0038887370536347 Lambda1 3.3094148e-07\n",
      "19 Train Loss 391502.5 Test MSE 3555.9686387295774 Test RE 1.003848401328287 Lambda1 -1.6321022e-05\n",
      "20 Train Loss 381563.75 Test MSE 3555.7117421746925 Test RE 1.0038121397804578 Lambda1 4.0881692e-05\n",
      "21 Train Loss 376275.3 Test MSE 3555.973661931849 Test RE 1.0038491103519456 Lambda1 2.3007935e-05\n",
      "22 Train Loss 366405.03 Test MSE 3554.9607305017166 Test RE 1.0037061252184927 Lambda1 -3.3716642e-06\n",
      "23 Train Loss 357669.38 Test MSE 3554.4671764105833 Test RE 1.0036364478721986 Lambda1 -4.326434e-05\n",
      "24 Train Loss 348376.62 Test MSE 3554.672263612611 Test RE 1.003665401581961 Lambda1 -4.123069e-05\n",
      "25 Train Loss 340512.8 Test MSE 3554.247430619786 Test RE 1.003605423771718 Lambda1 -5.9865833e-06\n",
      "26 Train Loss 333563.12 Test MSE 3554.7671903965293 Test RE 1.0036788028304549 Lambda1 -3.5847672e-05\n",
      "27 Train Loss 325158.4 Test MSE 3554.0487743673557 Test RE 1.003577376304253 Lambda1 -7.083179e-05\n",
      "28 Train Loss 319288.53 Test MSE 3553.665746881034 Test RE 1.0035232960001295 Lambda1 -4.6912748e-05\n",
      "29 Train Loss 311484.3 Test MSE 3553.5482779690487 Test RE 1.0035067097801076 Lambda1 6.981956e-05\n",
      "30 Train Loss 307882.72 Test MSE 3553.404452406854 Test RE 1.003486401715346 Lambda1 7.698925e-05\n",
      "31 Train Loss 306232.03 Test MSE 3553.2854639347515 Test RE 1.0034696003189796 Lambda1 6.667948e-05\n",
      "32 Train Loss 305245.34 Test MSE 3553.073871685574 Test RE 1.0034397223999545 Lambda1 5.9930917e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 Train Loss 300983.94 Test MSE 3552.1421939079305 Test RE 1.0033081540991002 Lambda1 4.8801703e-06\n",
      "34 Train Loss 297602.03 Test MSE 3552.0437318832255 Test RE 1.0032942486258045 Lambda1 -2.6454605e-05\n",
      "35 Train Loss 294081.6 Test MSE 3552.022680570325 Test RE 1.0032912755919616 Lambda1 -3.0455381e-05\n",
      "36 Train Loss 289372.66 Test MSE 3551.7905013547456 Test RE 1.0032584848120252 Lambda1 -6.804983e-05\n",
      "37 Train Loss 285970.66 Test MSE 3551.453087881294 Test RE 1.0032108298377371 Lambda1 -0.00011762672\n",
      "38 Train Loss 282725.84 Test MSE 3551.2460449886626 Test RE 1.003181586779429 Lambda1 -9.880372e-05\n",
      "39 Train Loss 279147.16 Test MSE 3551.357362943916 Test RE 1.0031973096055107 Lambda1 -6.578653e-05\n",
      "40 Train Loss 274023.56 Test MSE 3551.460940200182 Test RE 1.003211938894622 Lambda1 -0.00011511061\n",
      "41 Train Loss 269997.34 Test MSE 3551.092920719331 Test RE 1.0031599587224815 Lambda1 -0.000109849636\n",
      "42 Train Loss 267264.47 Test MSE 3551.452448430667 Test RE 1.0032107395220544 Lambda1 -0.0001114284\n",
      "43 Train Loss 264606.78 Test MSE 3551.396048805867 Test RE 1.0032027736355948 Lambda1 -7.871953e-05\n",
      "44 Train Loss 262703.3 Test MSE 3551.1817496912486 Test RE 1.0031725054387375 Lambda1 -4.378391e-05\n",
      "45 Train Loss 258513.98 Test MSE 3551.196121927653 Test RE 1.0031745354415411 Lambda1 -4.4640878e-05\n",
      "46 Train Loss 256025.98 Test MSE 3551.2375147463063 Test RE 1.0031803819365241 Lambda1 -5.05722e-05\n",
      "47 Train Loss 252540.5 Test MSE 3551.022789003281 Test RE 1.0031500528036126 Lambda1 -5.6609995e-05\n",
      "48 Train Loss 250685.19 Test MSE 3551.148734685743 Test RE 1.003167842227291 Lambda1 -6.8309855e-05\n",
      "49 Train Loss 249393.11 Test MSE 3551.172199885328 Test RE 1.0031711565766057 Lambda1 -0.00011877312\n",
      "50 Train Loss 248134.64 Test MSE 3551.3553368211255 Test RE 1.0031970234330685 Lambda1 -0.000115997114\n",
      "51 Train Loss 245557.3 Test MSE 3551.5088653799107 Test RE 1.0032187077921388 Lambda1 -9.6507865e-05\n",
      "52 Train Loss 243685.52 Test MSE 3551.480260745374 Test RE 1.0032146677118001 Lambda1 -7.085772e-05\n",
      "53 Train Loss 241982.36 Test MSE 3551.6684215666114 Test RE 1.0032412429961481 Lambda1 -6.864136e-05\n",
      "54 Train Loss 241011.25 Test MSE 3551.457052297741 Test RE 1.0032113897697295 Lambda1 -7.4027965e-05\n",
      "55 Train Loss 239147.0 Test MSE 3551.2113546934634 Test RE 1.0031766869851129 Lambda1 -9.4604555e-05\n",
      "56 Train Loss 237904.33 Test MSE 3551.0402184128984 Test RE 1.0031525146706244 Lambda1 -0.00010086455\n",
      "57 Train Loss 237140.53 Test MSE 3551.026113312547 Test RE 1.003150522355687 Lambda1 -9.564224e-05\n",
      "58 Train Loss 236159.55 Test MSE 3550.954799749026 Test RE 1.0031404494084057 Lambda1 -7.488255e-05\n",
      "59 Train Loss 235609.11 Test MSE 3550.9889275921405 Test RE 1.0031452699340888 Lambda1 -6.717763e-05\n",
      "60 Train Loss 234457.23 Test MSE 3550.851034012273 Test RE 1.0031257924535697 Lambda1 -7.684526e-05\n",
      "61 Train Loss 232453.81 Test MSE 3551.16706653051 Test RE 1.003170431515265 Lambda1 -8.801107e-05\n",
      "62 Train Loss 231135.3 Test MSE 3551.022176532346 Test RE 1.0031499662932866 Lambda1 -8.387184e-05\n",
      "63 Train Loss 230175.23 Test MSE 3551.106921567591 Test RE 1.003161936293448 Lambda1 -7.012855e-05\n",
      "64 Train Loss 228767.75 Test MSE 3551.2181807301777 Test RE 1.0031776511233919 Lambda1 -7.265615e-05\n",
      "65 Train Loss 225611.94 Test MSE 3551.11402633402 Test RE 1.003162939815426 Lambda1 -6.41132e-05\n",
      "66 Train Loss 223557.05 Test MSE 3550.9594340045937 Test RE 1.003141103993997 Lambda1 -8.586029e-05\n",
      "67 Train Loss 222609.44 Test MSE 3550.8527935648867 Test RE 1.0031260409929161 Lambda1 -0.000102534206\n",
      "68 Train Loss 221640.56 Test MSE 3550.8357731577203 Test RE 1.0031236368332523 Lambda1 -0.00010775061\n",
      "69 Train Loss 220855.73 Test MSE 3550.88176974518 Test RE 1.0031301339113519 Lambda1 -0.000108987486\n",
      "70 Train Loss 219806.53 Test MSE 3550.8773298670144 Test RE 1.0031295067745927 Lambda1 -0.00012563891\n",
      "71 Train Loss 218738.92 Test MSE 3551.0970694850503 Test RE 1.0031605447215706 Lambda1 -0.0001298718\n",
      "72 Train Loss 217365.75 Test MSE 3551.1434912204595 Test RE 1.0031671016109314 Lambda1 -0.00013223493\n",
      "73 Train Loss 215845.7 Test MSE 3551.0617860985494 Test RE 1.0031555610519538 Lambda1 -0.000110819354\n",
      "74 Train Loss 213708.1 Test MSE 3551.200797177011 Test RE 1.0031751957950386 Lambda1 -9.5979216e-05\n",
      "Training time: 121.23\n",
      "Training time: 121.23\n",
      "inv_HT_atanh_tune22\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.96\n",
      "Training time: 3.96\n",
      "inv_HT_atanh_tune22\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 454901.5 Test MSE 3516.912781223434 Test RE 0.9983204550073269 Lambda1 3.9206767e-05\n",
      "1 Train Loss 429694.97 Test MSE 3516.929223106273 Test RE 0.9983227886234548 Lambda1 1.8441318e-05\n",
      "2 Train Loss 416113.38 Test MSE 3517.1121180765904 Test RE 0.9983487467578719 Lambda1 2.0206146e-05\n",
      "3 Train Loss 407254.9 Test MSE 3516.9178249759807 Test RE 0.9983211708737478 Lambda1 2.8784034e-05\n",
      "4 Train Loss 399932.2 Test MSE 3516.9608506933964 Test RE 0.9983272775494444 Lambda1 7.079137e-05\n",
      "5 Train Loss 391727.28 Test MSE 3517.1509858339346 Test RE 0.9983542631401091 Lambda1 0.00010854748\n",
      "6 Train Loss 382907.78 Test MSE 3517.1359134937406 Test RE 0.9983521239724603 Lambda1 8.178735e-05\n",
      "7 Train Loss 367703.38 Test MSE 3517.210984410276 Test RE 0.9983627784951878 Lambda1 7.04963e-05\n",
      "8 Train Loss 352826.03 Test MSE 3516.9000952757283 Test RE 0.9983186544718438 Lambda1 6.649422e-05\n",
      "9 Train Loss 340570.25 Test MSE 3516.294755968897 Test RE 0.9982327339861085 Lambda1 -4.9471187e-06\n",
      "10 Train Loss 332613.16 Test MSE 3515.97504278651 Test RE 0.9981873516389042 Lambda1 -1.7788709e-05\n",
      "11 Train Loss 325488.38 Test MSE 3515.5265104821406 Test RE 0.9981236803176701 Lambda1 1.9058166e-06\n",
      "12 Train Loss 318887.12 Test MSE 3514.9784274347267 Test RE 0.9980458717745718 Lambda1 -1.3539015e-05\n",
      "13 Train Loss 312922.22 Test MSE 3514.611320536203 Test RE 0.9979937520960114 Lambda1 -2.28443e-05\n",
      "14 Train Loss 305995.2 Test MSE 3513.8791554288887 Test RE 0.997889795471132 Lambda1 1.6866175e-06\n",
      "15 Train Loss 299550.3 Test MSE 3512.9281510408096 Test RE 0.997754750731295 Lambda1 -8.268222e-07\n",
      "16 Train Loss 295076.5 Test MSE 3512.616314804122 Test RE 0.9977104653120834 Lambda1 -2.22896e-05\n",
      "17 Train Loss 291154.56 Test MSE 3512.424271088393 Test RE 0.9976831912481452 Lambda1 2.6891558e-07\n",
      "18 Train Loss 288090.66 Test MSE 3512.364904953205 Test RE 0.9976747599139862 Lambda1 -2.7004257e-06\n",
      "19 Train Loss 283399.88 Test MSE 3512.519959058819 Test RE 0.9976967809545915 Lambda1 -2.1466405e-05\n",
      "20 Train Loss 279472.34 Test MSE 3512.423288220893 Test RE 0.9976830516593059 Lambda1 -2.4111205e-05\n",
      "21 Train Loss 276358.47 Test MSE 3512.2567748773067 Test RE 0.9976594028140379 Lambda1 -3.6751324e-05\n",
      "22 Train Loss 273668.62 Test MSE 3512.1734974537862 Test RE 0.9976475752338765 Lambda1 -9.97968e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 Train Loss 269064.1 Test MSE 3511.8496036157876 Test RE 0.9976015724734028 Lambda1 2.8740415e-05\n",
      "24 Train Loss 263554.62 Test MSE 3511.505400718268 Test RE 0.997552682884618 Lambda1 2.3789347e-05\n",
      "25 Train Loss 258381.17 Test MSE 3510.712311440744 Test RE 0.9974400256447138 Lambda1 -2.514204e-05\n",
      "26 Train Loss 255248.81 Test MSE 3510.4027966174067 Test RE 0.997396056038162 Lambda1 4.0854127e-05\n",
      "27 Train Loss 252090.95 Test MSE 3510.157639541746 Test RE 0.9973612277030932 Lambda1 2.9928328e-06\n",
      "28 Train Loss 248637.88 Test MSE 3510.022878083361 Test RE 0.9973420822461889 Lambda1 -5.8029414e-06\n",
      "29 Train Loss 245011.53 Test MSE 3510.1429684845916 Test RE 0.9973591434150972 Lambda1 5.1836483e-05\n",
      "30 Train Loss 242372.77 Test MSE 3510.191580226513 Test RE 0.9973660495721766 Lambda1 -3.2546027e-06\n",
      "31 Train Loss 240519.7 Test MSE 3510.3231761304473 Test RE 0.9973847448561444 Lambda1 3.804861e-05\n",
      "32 Train Loss 237943.52 Test MSE 3510.4274109055027 Test RE 0.997399552809367 Lambda1 6.971809e-06\n",
      "33 Train Loss 234895.38 Test MSE 3510.608356697724 Test RE 0.9974252580733592 Lambda1 -6.9834205e-06\n",
      "34 Train Loss 232075.89 Test MSE 3510.7001964362153 Test RE 0.99743830462633 Lambda1 -2.0472748e-06\n",
      "35 Train Loss 228834.12 Test MSE 3510.51125646044 Test RE 0.9974114640398043 Lambda1 -2.4400726e-05\n",
      "36 Train Loss 225767.8 Test MSE 3510.3832393682023 Test RE 0.9973932776746064 Lambda1 -3.6166854e-05\n",
      "37 Train Loss 223969.02 Test MSE 3510.2989989832477 Test RE 0.9973813101354501 Lambda1 -5.55872e-05\n",
      "38 Train Loss 220842.94 Test MSE 3510.114646353978 Test RE 0.997355119733845 Lambda1 -8.6893915e-06\n",
      "39 Train Loss 218108.23 Test MSE 3509.9036377187595 Test RE 0.9973251415526857 Lambda1 -4.0465235e-05\n",
      "40 Train Loss 215858.33 Test MSE 3509.522801517281 Test RE 0.9972710335398772 Lambda1 -3.9381477e-05\n",
      "41 Train Loss 213408.3 Test MSE 3509.3244871054276 Test RE 0.9972428564879448 Lambda1 -2.4673536e-05\n",
      "42 Train Loss 212110.05 Test MSE 3509.146360790049 Test RE 0.9972175471370461 Lambda1 -3.158219e-05\n",
      "43 Train Loss 210761.52 Test MSE 3509.064910791146 Test RE 0.9972059739749309 Lambda1 -3.500978e-05\n",
      "44 Train Loss 209836.77 Test MSE 3508.8480866927102 Test RE 0.9971751649657105 Lambda1 -1.8064615e-05\n",
      "45 Train Loss 208311.6 Test MSE 3508.656467793752 Test RE 0.997147936625921 Lambda1 -2.380825e-05\n",
      "46 Train Loss 206795.92 Test MSE 3508.4484815368264 Test RE 0.9971183817033465 Lambda1 -6.018752e-06\n",
      "47 Train Loss 205271.27 Test MSE 3508.2066818644903 Test RE 0.9970840207809121 Lambda1 -5.475468e-05\n",
      "48 Train Loss 203803.12 Test MSE 3507.965223805657 Test RE 0.9970497072217781 Lambda1 -5.228465e-05\n",
      "49 Train Loss 202290.75 Test MSE 3507.6698507959068 Test RE 0.9970077302132174 Lambda1 -2.2580154e-05\n",
      "50 Train Loss 200659.92 Test MSE 3507.542981671568 Test RE 0.9969896996335386 Lambda1 1.6910699e-05\n",
      "51 Train Loss 199168.06 Test MSE 3507.4521001188664 Test RE 0.9969767833899841 Lambda1 5.0244624e-05\n",
      "52 Train Loss 197857.8 Test MSE 3507.3810229637998 Test RE 0.9969666816650835 Lambda1 3.842877e-05\n",
      "53 Train Loss 197021.05 Test MSE 3507.429196471039 Test RE 0.996973528257517 Lambda1 2.238031e-05\n",
      "54 Train Loss 195527.4 Test MSE 3507.2591596684074 Test RE 0.9969493618045144 Lambda1 2.3137442e-05\n",
      "55 Train Loss 194822.11 Test MSE 3507.266944829145 Test RE 0.9969504682820185 Lambda1 2.9661816e-05\n",
      "56 Train Loss 193199.92 Test MSE 3507.1601588036337 Test RE 0.9969352910528285 Lambda1 4.5171015e-05\n",
      "57 Train Loss 192172.88 Test MSE 3507.0035461783923 Test RE 0.9969130316764205 Lambda1 3.6378602e-05\n",
      "58 Train Loss 190680.33 Test MSE 3506.84528852401 Test RE 0.9968905379868851 Lambda1 4.2668133e-05\n",
      "59 Train Loss 189387.83 Test MSE 3507.0079111668856 Test RE 0.9969136520796366 Lambda1 -6.850255e-06\n",
      "60 Train Loss 187907.11 Test MSE 3506.9914504718745 Test RE 0.9969113124911254 Lambda1 -2.4972824e-05\n",
      "61 Train Loss 186680.4 Test MSE 3506.9709049018265 Test RE 0.9969083923041946 Lambda1 2.6297741e-05\n",
      "62 Train Loss 185123.94 Test MSE 3506.9743651968306 Test RE 0.9969088841241115 Lambda1 5.2993026e-05\n",
      "63 Train Loss 183198.55 Test MSE 3507.1909225930444 Test RE 0.9969396634565265 Lambda1 7.2257176e-06\n",
      "64 Train Loss 182480.05 Test MSE 3507.199728679451 Test RE 0.9969409150466881 Lambda1 3.8841405e-05\n",
      "65 Train Loss 180695.52 Test MSE 3507.2966652457185 Test RE 0.996954692328964 Lambda1 4.9270915e-05\n",
      "66 Train Loss 179231.88 Test MSE 3507.4975912434147 Test RE 0.996983248688251 Lambda1 2.6970132e-05\n",
      "67 Train Loss 177948.28 Test MSE 3507.503853135013 Test RE 0.9969841386387185 Lambda1 4.5109555e-05\n",
      "68 Train Loss 176098.81 Test MSE 3507.567548091731 Test RE 0.9969931910269586 Lambda1 4.211225e-05\n",
      "69 Train Loss 174169.64 Test MSE 3508.0547260726853 Test RE 0.997062426509776 Lambda1 3.774503e-05\n",
      "70 Train Loss 172292.48 Test MSE 3508.20507828798 Test RE 0.9970837929008521 Lambda1 3.6328718e-05\n",
      "71 Train Loss 170458.78 Test MSE 3508.2822335132278 Test RE 0.9970947571688956 Lambda1 3.904041e-05\n",
      "72 Train Loss 167901.39 Test MSE 3508.4211499068742 Test RE 0.99711449780369 Lambda1 3.7496477e-05\n",
      "73 Train Loss 166141.84 Test MSE 3508.268934545852 Test RE 0.9970928673062321 Lambda1 5.273549e-05\n",
      "74 Train Loss 164440.53 Test MSE 3508.3295354169795 Test RE 0.9971014790227999 Lambda1 5.089713e-05\n",
      "Training time: 117.86\n",
      "Training time: 117.86\n",
      "inv_HT_atanh_tune22\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.76\n",
      "Training time: 3.76\n",
      "inv_HT_atanh_tune22\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.05\n",
      "Training time: 4.05\n",
      "inv_HT_atanh_tune22\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.91\n",
      "Training time: 3.91\n",
      "inv_HT_atanh_tune22\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.76\n",
      "Training time: 3.76\n",
      "inv_HT_atanh_tune22\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.95\n",
      "Training time: 3.95\n",
      "inv_HT_atanh_tune23\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.92\n",
      "Training time: 3.92\n",
      "inv_HT_atanh_tune23\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.81\n",
      "Training time: 3.81\n",
      "inv_HT_atanh_tune23\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.95\n",
      "Training time: 3.95\n",
      "inv_HT_atanh_tune23\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.92\n",
      "Training time: 3.92\n",
      "inv_HT_atanh_tune23\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.76\n",
      "Training time: 3.76\n",
      "inv_HT_atanh_tune23\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.85\n",
      "Training time: 3.85\n",
      "inv_HT_atanh_tune23\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.77\n",
      "Training time: 3.77\n",
      "inv_HT_atanh_tune23\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.95\n",
      "Training time: 3.95\n",
      "inv_HT_atanh_tune23\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.82\n",
      "Training time: 3.82\n",
      "inv_HT_atanh_tune23\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.01\n",
      "Training time: 4.01\n",
      "inv_HT_atanh_tune24\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.22\n",
      "Training time: 4.22\n",
      "inv_HT_atanh_tune24\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.01\n",
      "Training time: 4.01\n",
      "inv_HT_atanh_tune24\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.00\n",
      "Training time: 4.00\n",
      "inv_HT_atanh_tune24\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.73\n",
      "Training time: 3.73\n",
      "inv_HT_atanh_tune24\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.99\n",
      "Training time: 3.99\n",
      "inv_HT_atanh_tune24\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.92\n",
      "Training time: 3.92\n",
      "inv_HT_atanh_tune24\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 4.06\n",
      "Training time: 4.06\n",
      "inv_HT_atanh_tune24\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.90\n",
      "Training time: 3.90\n",
      "inv_HT_atanh_tune24\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.88\n",
      "Training time: 3.88\n",
      "inv_HT_atanh_tune24\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.76\n",
      "Training time: 3.76\n"
     ]
    }
   ],
   "source": [
    "nan_tune = []\n",
    "for tune_reps in range(25):\n",
    "    label = \"inv_HT_atanh_tune\" + str(tune_reps)\n",
    "    max_reps = 10 #10\n",
    "    max_iter = 75#75\n",
    "\n",
    "    train_loss_full = []\n",
    "    test_mse_full = []\n",
    "    test_re_full = []\n",
    "    alpha_full = []\n",
    "\n",
    "    lambda1_full = []\n",
    "    elapsed_time= np.zeros((max_reps,1))\n",
    "    \n",
    "    time_threshold = np.empty((max_reps,1))\n",
    "    time_threshold[:] = np.nan\n",
    "    epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "    \n",
    "    n_val = lrn_tune[tune_reps,1]\n",
    "\n",
    "    for reps in range(max_reps):\n",
    "        print(label)\n",
    "        'Generate Training data'\n",
    "        print(reps)\n",
    "        torch.manual_seed(reps*36)\n",
    "        \n",
    "        train_loss = []\n",
    "        test_mse_loss = []\n",
    "        test_re_loss = []   \n",
    "        alpha_val = []\n",
    "\n",
    "        lambda1_val = []\n",
    "\n",
    "        N_f = 50000 #Total number of collocation points \n",
    "        N_train = 5000\n",
    "\n",
    "        layers = np.array([2,50,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "        \n",
    "        PINN = Sequentialmodel(layers,n_val)\n",
    "\n",
    "        PINN.to(device)\n",
    "\n",
    "        'Neural Network Summary'\n",
    "        print(PINN)\n",
    "\n",
    "        params = list(PINN.parameters())\n",
    "\n",
    "        optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lrn_tune[tune_reps,0], \n",
    "                                  max_iter = 10, \n",
    "                                  max_eval = 15, \n",
    "                                  tolerance_grad = -1, \n",
    "                                  tolerance_change = -1, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "        \n",
    "        \n",
    "        nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "        torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "        train_loss_full.append(train_loss)\n",
    "        test_mse_full.append(test_mse_loss)\n",
    "        test_re_full.append(test_re_loss)\n",
    "        #elapsed_time[reps] = time.time() - start_time\n",
    "        alpha_full.append(alpha_val)\n",
    "  \n",
    "        lambda1_full.append(lambda1_val)\n",
    "        \n",
    "        if(nan_flag == 1):\n",
    "            nan_tune.append(tune_reps)\n",
    "            break\n",
    "\n",
    "        print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "    mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"alpha\": alpha_full,\"lambda1\": lambda1_full, \"label\": label}\n",
    "    savemat(label+'.mat', mdic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   0.45157087880994384\n",
      "1   0.5496346316641214\n",
      "2   [[nan]]\n",
      "3   nan\n",
      "4   [[nan]]\n",
      "5   0.4452546488691146\n",
      "6   0.5348069177700265\n",
      "7   [[nan]]\n",
      "8   nan\n",
      "9   [[nan]]\n",
      "10   0.37100782790329834\n",
      "11   0.45469307172131684\n",
      "12   [[nan]]\n",
      "13   nan\n",
      "14   nan\n",
      "15   0.43369057243853765\n",
      "16   0.4417018073557088\n",
      "17   [[nan]]\n",
      "18   nan\n",
      "19   nan\n",
      "20   0.41398014507702124\n",
      "21   0.4982081909821813\n",
      "22   [[nan]]\n",
      "23   nan\n",
      "24   nan\n"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(25):\n",
    "    label = \"inv_HT_atanh_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 1.  ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrn_tune[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
