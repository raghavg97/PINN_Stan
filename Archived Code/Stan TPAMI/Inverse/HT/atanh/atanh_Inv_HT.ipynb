{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1]).reshape(-1,1)\n",
    "n_value = np.array([1.0,3.0,5.0,8.0,10.0]).reshape(-1,1)\n",
    "\n",
    "LR_tune,N_value = np.meshgrid(lr_tune,n_value)\n",
    "\n",
    "LR_tune = LR_tune.flatten('F').reshape(-1,1)\n",
    "N_value = N_value.flatten('F').reshape(-1,1)\n",
    "\n",
    "lrn_tune = np.hstack((LR_tune,N_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_2D_4(xt): #True function for 2D_4 Heat Transfer in a rod x \\in [0,1] t \\in [0,0.1]\n",
    "    term1 = 4*u0/np.pi\n",
    "    \n",
    "    resol_n = 10000\n",
    "    \n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "\n",
    "    u = np.zeros((np.shape(xt)[0],1))\n",
    "    \n",
    "    for i in range(resol_n):\n",
    "        j = 2*i-1\n",
    "        term2 = np.sin(j*np.pi*x)/j\n",
    "        term3 = np.exp(-1*np.square(j*np.pi)*t)\n",
    "        \n",
    "        u = u + term2*term3\n",
    "        \n",
    "    u = term1*u\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = 50.0\n",
    "loss_thresh = 0.1\n",
    "label = \"inv_HT_atanh\"\n",
    "    \n",
    "x_ll = np.array(0.0)\n",
    "x_ul = np.array(1.0)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,100).reshape(-1,1)\n",
    "t = np.linspace(0,0.1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = true_2D_4(xt)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_f,N_train,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #X_Train\n",
    "    np.random.seed(seed)\n",
    "    x_train = np.random.uniform(x_ll,x_ul,(N_train,1))\n",
    "    t_train = np.random.uniform(0,0.1,(N_train,1))\n",
    "    \n",
    "    xt_train = np.hstack((x_train,t_train))\n",
    "    u_train = true_2D_4(xt_train)\n",
    "    \n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "        \n",
    "    \n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requiresGrad = True\n",
    "        \n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    \n",
    "    def loss_PDE(self, xt_coll,f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_tt[:,[0]]\n",
    "                \n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        f = du_dt - self.lambda1*d2u_dx2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_train = self.loss_function(self.forward(xt_train),u_train)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "        \n",
    "        #print(self.iter,\"train_loss\",loss_train.cpu().detach().numpy(),\"F Loss\",(loss_f).cpu().detach().numpy())\n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                    \n",
    "    \n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_coll,f_hat, xt_train, u_train,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xt_coll, xt_train, u_train = trainingdata(N_f,N_train,123)\n",
    "    \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_train = torch.from_numpy(xt_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    nan_flag = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_coll,f_hat, xt_train, u_train,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_coll,f_hat, xt_train, u_train).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1],\"Lambda1\",lambda1_val[-1])\n",
    "\n",
    "        if(np.isnan(loss_np)):\n",
    "            nan_flag =1\n",
    "            print(\"NAN BREAK!\")\n",
    "            break\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_HT_atanh\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 21311.184 Test MSE 3572.210103913615 Test RE 1.0061382689139065 Lambda1 -0.00090391986\n",
      "1 Train Loss 15290.309 Test MSE 3568.5830097285166 Test RE 1.0056273408069494 Lambda1 -0.00032179378\n",
      "2 Train Loss 11761.734 Test MSE 3578.443643513143 Test RE 1.0070157465616012 Lambda1 0.0003764709\n",
      "3 Train Loss 9901.561 Test MSE 3578.9466787133724 Test RE 1.0070865240583304 Lambda1 -0.00016840693\n",
      "4 Train Loss 8083.01 Test MSE 3582.7278759009564 Test RE 1.0076183827039626 Lambda1 0.00063263584\n",
      "5 Train Loss 6791.3813 Test MSE 3579.0234545924777 Test RE 1.0070973260547689 Lambda1 -0.00029089962\n",
      "6 Train Loss 5966.876 Test MSE 3578.267929973111 Test RE 1.0069910223390708 Lambda1 2.378473e-06\n",
      "7 Train Loss 5375.625 Test MSE 3578.101358682671 Test RE 1.0069675839396215 Lambda1 0.00013308812\n",
      "8 Train Loss 5002.0645 Test MSE 3572.8915228287183 Test RE 1.006234227569087 Lambda1 -0.00011468428\n",
      "9 Train Loss 4634.651 Test MSE 3570.3576913201673 Test RE 1.0058773625239774 Lambda1 -2.2261287e-05\n",
      "10 Train Loss 4410.476 Test MSE 3568.417271963793 Test RE 1.0056039880696872 Lambda1 4.6449408e-05\n",
      "11 Train Loss 4192.093 Test MSE 3563.016199741901 Test RE 1.0048426706429623 Lambda1 -0.0003098462\n",
      "12 Train Loss 4037.4846 Test MSE 3560.5176362896696 Test RE 1.0044902861271365 Lambda1 0.00016624125\n",
      "13 Train Loss 3940.754 Test MSE 3558.1631137891463 Test RE 1.0041581032323155 Lambda1 0.00035439464\n",
      "14 Train Loss 3851.0083 Test MSE 3555.491081784378 Test RE 1.0037809920059444 Lambda1 -0.00025900017\n",
      "15 Train Loss 3796.247 Test MSE 3552.11322964121 Test RE 1.003304063589398 Lambda1 0.00016592714\n",
      "16 Train Loss 3761.7034 Test MSE 3550.2932174231933 Test RE 1.003046996950719 Lambda1 4.246361e-05\n",
      "17 Train Loss 3735.2744 Test MSE 3548.564056655972 Test RE 1.0028027015295622 Lambda1 3.7535665e-05\n",
      "18 Train Loss 3713.7114 Test MSE 3545.5292682690324 Test RE 1.002373803404698 Lambda1 -7.762747e-05\n",
      "19 Train Loss 3695.271 Test MSE 3541.7502326781005 Test RE 1.0018394662295442 Lambda1 0.00019011887\n",
      "20 Train Loss 3680.9731 Test MSE 3538.736275796824 Test RE 1.0014131030740985 Lambda1 0.00016221091\n",
      "21 Train Loss 3662.8938 Test MSE 3532.968453145574 Test RE 1.0005966634995773 Lambda1 7.86844e-05\n",
      "22 Train Loss 3643.6277 Test MSE 3527.800486072389 Test RE 0.9998645676579341 Lambda1 -6.985496e-05\n",
      "23 Train Loss 3625.0037 Test MSE 3521.4614140767367 Test RE 0.9989658400604443 Lambda1 0.00021329644\n",
      "24 Train Loss 3613.5151 Test MSE 3520.0432801948564 Test RE 0.9987646721653716 Lambda1 -1.2864266e-05\n",
      "25 Train Loss 3602.3008 Test MSE 3516.1499747919706 Test RE 0.9982121829788952 Lambda1 0.00023281662\n",
      "26 Train Loss 3593.298 Test MSE 3513.4570497882 Test RE 0.9978298577864226 Lambda1 0.00023571806\n",
      "27 Train Loss 3581.877 Test MSE 3506.3863628667377 Test RE 0.9968253064787306 Lambda1 -0.00015394921\n",
      "28 Train Loss 3575.9417 Test MSE 3501.1405886116268 Test RE 0.9960793707452199 Lambda1 -4.7443573e-05\n",
      "29 Train Loss 3567.938 Test MSE 3498.6433340983444 Test RE 0.9957240711853957 Lambda1 0.00018739914\n",
      "30 Train Loss 3559.4907 Test MSE 3492.5626183057593 Test RE 0.9948584001808746 Lambda1 0.00011979593\n",
      "31 Train Loss 3553.888 Test MSE 3488.498602118963 Test RE 0.9942794130684236 Lambda1 1.3554968e-05\n",
      "32 Train Loss 3545.905 Test MSE 3481.4133222735636 Test RE 0.9932691892775916 Lambda1 0.0001277236\n",
      "33 Train Loss 3536.93 Test MSE 3472.6509210464114 Test RE 0.992018417580726 Lambda1 -0.00011126249\n",
      "34 Train Loss 3531.0742 Test MSE 3467.9113616639534 Test RE 0.9913412208712807 Lambda1 7.2858393e-06\n",
      "35 Train Loss 3524.3013 Test MSE 3460.798500146794 Test RE 0.99032405355838 Lambda1 0.00012007566\n",
      "36 Train Loss 3516.754 Test MSE 3454.067476623369 Test RE 0.9893605275344762 Lambda1 2.7932218e-05\n",
      "37 Train Loss 3504.1082 Test MSE 3443.5224275511346 Test RE 0.987849145624133 Lambda1 -0.000118940705\n",
      "38 Train Loss 3490.7751 Test MSE 3429.2778538707807 Test RE 0.9858038457103144 Lambda1 7.096178e-05\n",
      "39 Train Loss 3480.3992 Test MSE 3417.5872372631043 Test RE 0.9841220785654522 Lambda1 1.47035435e-05\n",
      "40 Train Loss 3467.9795 Test MSE 3401.916339743271 Test RE 0.981863204863986 Lambda1 -9.835408e-05\n",
      "41 Train Loss 3454.1868 Test MSE 3387.5408587739685 Test RE 0.9797864785166054 Lambda1 0.00019444898\n",
      "42 Train Loss 3440.0464 Test MSE 3368.3502773398145 Test RE 0.9770072681401313 Lambda1 0.00022111373\n",
      "43 Train Loss 3423.0352 Test MSE 3351.7606233895362 Test RE 0.974598341299921 Lambda1 2.5926436e-05\n",
      "44 Train Loss 3403.878 Test MSE 3322.1494193070193 Test RE 0.9702837353829878 Lambda1 -0.00012217433\n",
      "45 Train Loss 3381.5056 Test MSE 3309.153866143417 Test RE 0.9683841023804363 Lambda1 0.00013989664\n",
      "46 Train Loss 3360.374 Test MSE 3285.1564995615963 Test RE 0.9648664429849966 Lambda1 6.65626e-05\n",
      "47 Train Loss 3347.0066 Test MSE 3274.5242089927233 Test RE 0.9633037998729301 Lambda1 6.867043e-05\n",
      "48 Train Loss 3328.728 Test MSE 3253.594619703829 Test RE 0.9602203179171999 Lambda1 3.6449368e-05\n",
      "49 Train Loss 3305.006 Test MSE 3207.2636075698088 Test RE 0.9533590615530113 Lambda1 9.514813e-05\n",
      "50 Train Loss 3271.2092 Test MSE 3176.2411026359155 Test RE 0.9487371383435659 Lambda1 0.0001129815\n",
      "51 Train Loss 3229.0073 Test MSE 3128.753863102777 Test RE 0.9416182561720521 Lambda1 -0.00012558463\n",
      "52 Train Loss 3205.298 Test MSE 3114.7833893111324 Test RE 0.9395136529745312 Lambda1 8.484658e-05\n",
      "53 Train Loss 3186.9944 Test MSE 3089.7913698503944 Test RE 0.9357368842321816 Lambda1 1.1291968e-06\n",
      "54 Train Loss 3158.9722 Test MSE 3073.770273970228 Test RE 0.9333077535074946 Lambda1 -8.062292e-06\n",
      "55 Train Loss 3140.3015 Test MSE 3064.917122296291 Test RE 0.9319627158879548 Lambda1 0.00013193236\n",
      "56 Train Loss 3123.5535 Test MSE 3055.607060298199 Test RE 0.930546163754952 Lambda1 8.65631e-05\n",
      "57 Train Loss 3112.1172 Test MSE 3044.399341925551 Test RE 0.9288380121233755 Lambda1 0.0002975016\n",
      "58 Train Loss 3100.7397 Test MSE 3032.0892818869506 Test RE 0.92695822694019 Lambda1 -2.0408575e-05\n",
      "59 Train Loss 3089.8948 Test MSE 3026.544073896415 Test RE 0.9261102096192891 Lambda1 -6.158718e-06\n",
      "60 Train Loss 3076.5718 Test MSE 3017.6024932253636 Test RE 0.9247411539843097 Lambda1 3.1960863e-05\n",
      "61 Train Loss 3057.3396 Test MSE 3001.772031386512 Test RE 0.9223123500903111 Lambda1 7.187235e-06\n",
      "62 Train Loss 3048.98 Test MSE 2995.399903934111 Test RE 0.9213328929471717 Lambda1 9.30593e-06\n",
      "63 Train Loss 3035.807 Test MSE 2980.7190031022415 Test RE 0.9190723248696868 Lambda1 9.68155e-06\n",
      "64 Train Loss 3018.9663 Test MSE 2968.1002585533183 Test RE 0.9171248351285547 Lambda1 9.832058e-06\n",
      "65 Train Loss 3003.9824 Test MSE 2950.1372585580857 Test RE 0.9143453948470035 Lambda1 6.3885504e-06\n",
      "66 Train Loss 2989.8193 Test MSE 2928.434298288016 Test RE 0.9109759528580432 Lambda1 1.3434828e-05\n",
      "67 Train Loss 2980.458 Test MSE 2922.3428361142232 Test RE 0.9100279950553253 Lambda1 1.6576587e-05\n",
      "68 Train Loss 2952.691 Test MSE 2879.2252746583895 Test RE 0.9032895659299306 Lambda1 8.1671154e-05\n",
      "69 Train Loss 2941.8157 Test MSE 2876.7504131303804 Test RE 0.9029012675201559 Lambda1 1.8682904e-05\n",
      "70 Train Loss 2925.8965 Test MSE 2860.0824050486826 Test RE 0.9002817443967659 Lambda1 2.8941955e-05\n",
      "71 Train Loss 2899.6875 Test MSE 2832.0559663931417 Test RE 0.89585987735916 Lambda1 7.919983e-06\n",
      "72 Train Loss 2861.9697 Test MSE 2785.2082103447924 Test RE 0.8884193397220266 Lambda1 1.8324361e-05\n",
      "73 Train Loss 2832.7173 Test MSE 2763.846822319007 Test RE 0.8850058788396342 Lambda1 2.553215e-05\n",
      "74 Train Loss 2814.045 Test MSE 2739.9988993318116 Test RE 0.8811794589376837 Lambda1 7.307353e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 Train Loss 2789.238 Test MSE 2712.8961262806242 Test RE 0.8768105230279057 Lambda1 1.796106e-05\n",
      "76 Train Loss 2737.8342 Test MSE 2668.9465506038737 Test RE 0.8696792513743353 Lambda1 4.8907394e-05\n",
      "77 Train Loss 2695.6768 Test MSE 2604.499438095583 Test RE 0.8591150056567345 Lambda1 3.7260146e-05\n",
      "78 Train Loss 2658.1104 Test MSE 2571.0585745498556 Test RE 0.853581818853269 Lambda1 4.2630607e-05\n",
      "79 Train Loss 2629.4937 Test MSE 2552.61730253731 Test RE 0.8505150932034518 Lambda1 4.238512e-05\n",
      "80 Train Loss 2563.8464 Test MSE 2494.4467048862016 Test RE 0.8407682157856204 Lambda1 1.876501e-05\n",
      "81 Train Loss 2537.835 Test MSE 2470.9083971737323 Test RE 0.8367919493951236 Lambda1 1.9411042e-05\n",
      "82 Train Loss 2504.1702 Test MSE 2443.738505853805 Test RE 0.8321785869846279 Lambda1 6.3229427e-06\n",
      "83 Train Loss 2472.0928 Test MSE 2395.5880075550276 Test RE 0.8239393333813668 Lambda1 -6.321793e-07\n",
      "84 Train Loss 2433.8313 Test MSE 2355.723161621043 Test RE 0.817055008650791 Lambda1 -1.6302996e-05\n",
      "85 Train Loss 2402.0305 Test MSE 2309.2217098842407 Test RE 0.8089505722068873 Lambda1 -2.3178163e-06\n",
      "86 Train Loss 2353.7478 Test MSE 2273.642186588509 Test RE 0.8026943943020088 Lambda1 -3.853685e-06\n",
      "87 Train Loss 2307.444 Test MSE 2222.5749804789834 Test RE 0.7936287299716397 Lambda1 -3.912196e-06\n",
      "88 Train Loss 2249.2456 Test MSE 2150.4491650660357 Test RE 0.7806453209099983 Lambda1 5.9163165e-05\n",
      "89 Train Loss 2194.5344 Test MSE 2085.741720971437 Test RE 0.7688107279518032 Lambda1 3.3703178e-05\n",
      "90 Train Loss 2116.953 Test MSE 1996.4137017364976 Test RE 0.7521672874194774 Lambda1 4.073113e-06\n",
      "91 Train Loss 2016.2999 Test MSE 1904.251651731531 Test RE 0.7346007055282943 Lambda1 1.708428e-05\n",
      "92 Train Loss 1950.6914 Test MSE 1871.1703536097534 Test RE 0.7281918846639763 Lambda1 2.2955535e-05\n",
      "93 Train Loss 1913.5706 Test MSE 1841.853924946074 Test RE 0.7224649170665132 Lambda1 -8.402499e-06\n",
      "94 Train Loss 1856.3857 Test MSE 1779.8127112499144 Test RE 0.7101928942392289 Lambda1 -1.665751e-05\n",
      "95 Train Loss 1816.2439 Test MSE 1743.5701518073008 Test RE 0.7029248283123762 Lambda1 -1.1230533e-05\n",
      "96 Train Loss 1785.0907 Test MSE 1693.2998612051315 Test RE 0.6927174166444264 Lambda1 6.9737425e-06\n",
      "97 Train Loss 1749.9738 Test MSE 1665.8467901702431 Test RE 0.6870790378321017 Lambda1 1.613714e-05\n",
      "98 Train Loss 1718.1686 Test MSE 1639.9102729189049 Test RE 0.6817092922904617 Lambda1 2.8152948e-05\n",
      "99 Train Loss 1691.8538 Test MSE 1584.5315381959485 Test RE 0.6700999946877013 Lambda1 2.6321164e-05\n",
      "100 Train Loss 1660.0814 Test MSE 1561.7135491997108 Test RE 0.6652576230257514 Lambda1 4.795382e-05\n",
      "101 Train Loss 1625.9009 Test MSE 1516.453513629463 Test RE 0.6555468297747509 Lambda1 6.1116836e-05\n",
      "102 Train Loss 1594.1509 Test MSE 1493.7274122605445 Test RE 0.6506161601169347 Lambda1 2.1129206e-05\n",
      "103 Train Loss 1554.5345 Test MSE 1456.4177095711682 Test RE 0.6424393680449711 Lambda1 6.57325e-05\n",
      "104 Train Loss 1513.6942 Test MSE 1431.4650956472867 Test RE 0.6369121768165604 Lambda1 4.3745742e-05\n",
      "105 Train Loss 1482.6605 Test MSE 1390.5464712306523 Test RE 0.627743066079472 Lambda1 8.4640444e-05\n",
      "106 Train Loss 1442.689 Test MSE 1342.4706252906649 Test RE 0.6167960253592525 Lambda1 6.263095e-05\n",
      "107 Train Loss 1381.7946 Test MSE 1275.8094775382049 Test RE 0.6012873714397444 Lambda1 4.1646905e-05\n",
      "108 Train Loss 1350.5504 Test MSE 1261.6192135028975 Test RE 0.5979340944879591 Lambda1 -6.527774e-06\n",
      "109 Train Loss 1287.8413 Test MSE 1201.5983402242814 Test RE 0.5835375807185644 Lambda1 -3.1499945e-05\n",
      "110 Train Loss 1180.487 Test MSE 1113.2599881091428 Test RE 0.5616780739313674 Lambda1 1.3078428e-05\n",
      "111 Train Loss 1121.5582 Test MSE 1066.6488488023813 Test RE 0.549793884951105 Lambda1 -1.6647378e-05\n",
      "112 Train Loss 1073.3048 Test MSE 1015.4851629395114 Test RE 0.5364459388549035 Lambda1 -9.655197e-06\n",
      "113 Train Loss 1024.9337 Test MSE 972.7690126377781 Test RE 0.5250419866407163 Lambda1 -7.881961e-06\n",
      "114 Train Loss 975.7791 Test MSE 907.2625264600048 Test RE 0.5070556860710433 Lambda1 3.1959473e-06\n",
      "115 Train Loss 952.29816 Test MSE 881.1574940746128 Test RE 0.49970758341209986 Lambda1 6.980491e-06\n",
      "116 Train Loss 929.2582 Test MSE 869.0370236984655 Test RE 0.49625890188113286 Lambda1 -5.6628305e-06\n",
      "117 Train Loss 907.8206 Test MSE 842.7892053937361 Test RE 0.4887071048316823 Lambda1 -1.0564213e-07\n",
      "118 Train Loss 884.259 Test MSE 816.6752595401368 Test RE 0.4810761982303202 Lambda1 -1.8616903e-06\n",
      "119 Train Loss 863.83826 Test MSE 795.6586532174517 Test RE 0.47484576147294005 Lambda1 -7.951887e-06\n",
      "120 Train Loss 845.77844 Test MSE 777.8942994021019 Test RE 0.4695149929416774 Lambda1 -8.9125515e-06\n",
      "121 Train Loss 811.39465 Test MSE 769.5526386146685 Test RE 0.4669908125271579 Lambda1 5.6530926e-06\n",
      "122 Train Loss 801.51556 Test MSE 774.4488243009673 Test RE 0.4684740433601638 Lambda1 5.292318e-06\n",
      "123 Train Loss 784.0542 Test MSE 762.3480325742482 Test RE 0.4647996716135176 Lambda1 1.9145928e-06\n",
      "124 Train Loss 775.12317 Test MSE 756.8322754842727 Test RE 0.4631151547649496 Lambda1 3.1248364e-06\n",
      "125 Train Loss 769.7113 Test MSE 751.9704564362324 Test RE 0.46162525416028527 Lambda1 2.434513e-06\n",
      "126 Train Loss 764.93555 Test MSE 747.6207839717798 Test RE 0.4602882131221594 Lambda1 2.9657747e-07\n",
      "127 Train Loss 760.9478 Test MSE 745.2475738459211 Test RE 0.45955707446600275 Lambda1 3.3246545e-06\n",
      "128 Train Loss 755.8627 Test MSE 740.4514635198838 Test RE 0.45807592637802913 Lambda1 -2.9995212e-07\n",
      "129 Train Loss 748.0158 Test MSE 734.94030057579 Test RE 0.45636801825335466 Lambda1 8.769098e-07\n",
      "130 Train Loss 744.9552 Test MSE 729.3525201475129 Test RE 0.4546298159184252 Lambda1 2.251135e-06\n",
      "131 Train Loss 742.6063 Test MSE 730.4747498395898 Test RE 0.4549794431279208 Lambda1 1.0728052e-06\n",
      "132 Train Loss 740.7893 Test MSE 728.440838318329 Test RE 0.4543455861839823 Lambda1 8.748072e-07\n",
      "133 Train Loss 735.3026 Test MSE 722.6805039369608 Test RE 0.45254559301076214 Lambda1 5.176747e-07\n",
      "134 Train Loss 730.8652 Test MSE 719.1333961708464 Test RE 0.4514336199127967 Lambda1 2.418134e-07\n",
      "135 Train Loss 727.2785 Test MSE 719.6238976343805 Test RE 0.4515875490083207 Lambda1 -4.4244229e-07\n",
      "136 Train Loss 723.9444 Test MSE 718.1247309617465 Test RE 0.4511169156897955 Lambda1 -1.7727887e-06\n",
      "137 Train Loss 721.8922 Test MSE 716.0448057998303 Test RE 0.45046315056536573 Lambda1 -4.665905e-06\n",
      "138 Train Loss 719.3289 Test MSE 715.0460040294021 Test RE 0.4501488682772493 Lambda1 -5.719297e-06\n",
      "139 Train Loss 717.72687 Test MSE 714.9999600520011 Test RE 0.4501343748195732 Lambda1 -4.4208746e-06\n",
      "140 Train Loss 715.1573 Test MSE 714.5985390728393 Test RE 0.45000799806412356 Lambda1 -2.3465063e-06\n",
      "141 Train Loss 710.14166 Test MSE 713.1726728166917 Test RE 0.4495588146541575 Lambda1 -2.7522067e-06\n",
      "142 Train Loss 708.4677 Test MSE 712.7540473498382 Test RE 0.44942685194626997 Lambda1 -2.6958558e-06\n",
      "143 Train Loss 705.7527 Test MSE 708.9118671684881 Test RE 0.44821387227816717 Lambda1 -9.922633e-07\n",
      "144 Train Loss 704.2528 Test MSE 706.9932257916679 Test RE 0.44760692494627585 Lambda1 2.8486397e-07\n",
      "145 Train Loss 700.12775 Test MSE 705.8849569029987 Test RE 0.44725595737638774 Lambda1 -9.152575e-07\n",
      "146 Train Loss 697.1329 Test MSE 702.0215279624019 Test RE 0.4460303239250979 Lambda1 -3.289424e-06\n",
      "147 Train Loss 694.2135 Test MSE 700.1718699074034 Test RE 0.4454423450048176 Lambda1 -1.0895033e-06\n",
      "148 Train Loss 690.38074 Test MSE 699.7514615059127 Test RE 0.4453085951133408 Lambda1 -1.5018337e-06\n",
      "149 Train Loss 689.8686 Test MSE 699.739151908901 Test RE 0.4453046782987571 Lambda1 -1.2910841e-06\n",
      "150 Train Loss 689.3214 Test MSE 699.8412020132009 Test RE 0.4453371487786453 Lambda1 -9.2376996e-07\n",
      "151 Train Loss 688.612 Test MSE 699.9492737459831 Test RE 0.44537153264980905 Lambda1 -6.452117e-07\n",
      "152 Train Loss 688.1096 Test MSE 699.5221015604197 Test RE 0.4452356089665036 Lambda1 -2.319589e-06\n",
      "153 Train Loss 687.26434 Test MSE 699.0281916757822 Test RE 0.44507839799443244 Lambda1 -1.6591512e-06\n",
      "154 Train Loss 686.79517 Test MSE 698.5946733008354 Test RE 0.44494036379857227 Lambda1 -1.7417419e-06\n",
      "155 Train Loss 686.16156 Test MSE 698.008762124447 Test RE 0.4447537389743395 Lambda1 -1.8973084e-06\n",
      "156 Train Loss 684.46466 Test MSE 696.5229005804258 Test RE 0.44428010986373273 Lambda1 -1.5305436e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157 Train Loss 683.56604 Test MSE 693.9449211343497 Test RE 0.4434571600743195 Lambda1 -1.7467232e-06\n",
      "158 Train Loss 683.4444 Test MSE 694.0963389165795 Test RE 0.4435055382922342 Lambda1 -1.986197e-06\n",
      "159 Train Loss 683.2514 Test MSE 694.1179501941036 Test RE 0.44351244269881895 Lambda1 -1.4037613e-06\n",
      "160 Train Loss 682.96265 Test MSE 694.0508087852877 Test RE 0.4434909918990249 Lambda1 -1.7444244e-06\n",
      "161 Train Loss 681.1352 Test MSE 692.9062383738973 Test RE 0.44312515692776167 Lambda1 -1.0887761e-06\n",
      "162 Train Loss 679.06805 Test MSE 692.2597789731434 Test RE 0.4429183978788467 Lambda1 6.6713716e-07\n",
      "163 Train Loss 677.7757 Test MSE 691.7860047895665 Test RE 0.4427668079522294 Lambda1 -9.667626e-07\n",
      "164 Train Loss 676.8005 Test MSE 689.0913205497229 Test RE 0.44190362130669464 Lambda1 -1.399506e-06\n",
      "165 Train Loss 675.7735 Test MSE 688.4456929666986 Test RE 0.4416965572955642 Lambda1 -1.5368855e-06\n",
      "166 Train Loss 674.1864 Test MSE 688.3649064784679 Test RE 0.44167064082824303 Lambda1 6.140188e-07\n",
      "167 Train Loss 673.556 Test MSE 688.5233045620378 Test RE 0.441721453818787 Lambda1 1.3799307e-07\n",
      "168 Train Loss 673.07556 Test MSE 688.355763168722 Test RE 0.44166770754035534 Lambda1 7.018211e-08\n",
      "169 Train Loss 671.7786 Test MSE 687.2053700716656 Test RE 0.44129849152610395 Lambda1 3.9675064e-07\n",
      "170 Train Loss 671.6313 Test MSE 687.0504320331715 Test RE 0.44124874091297306 Lambda1 5.5315604e-07\n",
      "171 Train Loss 671.56177 Test MSE 686.7552051323868 Test RE 0.4411539280074391 Lambda1 6.9156914e-07\n",
      "172 Train Loss 670.85736 Test MSE 684.7047164827295 Test RE 0.4404948450171673 Lambda1 -3.4109092e-08\n",
      "173 Train Loss 670.34155 Test MSE 684.8073484046598 Test RE 0.44052785715909504 Lambda1 2.140771e-07\n",
      "174 Train Loss 670.2103 Test MSE 684.8513150899796 Test RE 0.44054199853684584 Lambda1 2.3557516e-07\n",
      "175 Train Loss 668.2526 Test MSE 684.6715354047556 Test RE 0.4404841716053099 Lambda1 4.9689476e-07\n",
      "176 Train Loss 667.50256 Test MSE 684.3677935273513 Test RE 0.4403864544341003 Lambda1 3.052837e-07\n",
      "177 Train Loss 667.38586 Test MSE 684.3102856028233 Test RE 0.44036795104799625 Lambda1 4.1713167e-07\n",
      "178 Train Loss 666.5328 Test MSE 683.2894674281185 Test RE 0.4400393695616315 Lambda1 7.08958e-07\n",
      "179 Train Loss 664.6468 Test MSE 680.0555222979991 Test RE 0.43899680191872986 Lambda1 4.2081064e-07\n",
      "180 Train Loss 663.92804 Test MSE 680.1759392612322 Test RE 0.4390356666291404 Lambda1 2.8775932e-07\n",
      "181 Train Loss 663.2447 Test MSE 679.4162806635627 Test RE 0.43879042832071213 Lambda1 3.3224478e-07\n",
      "182 Train Loss 661.6602 Test MSE 677.0420893046951 Test RE 0.4380230901114337 Lambda1 -4.6435173e-07\n",
      "183 Train Loss 659.06256 Test MSE 677.13952421057 Test RE 0.4380546075043981 Lambda1 -3.0280455e-07\n",
      "184 Train Loss 658.60077 Test MSE 677.3238996612384 Test RE 0.4381142414613921 Lambda1 -3.908199e-07\n",
      "185 Train Loss 658.3224 Test MSE 677.896786908116 Test RE 0.4382994829770824 Lambda1 -7.2713686e-07\n",
      "186 Train Loss 658.1484 Test MSE 677.7672009754639 Test RE 0.4382575885744448 Lambda1 -8.011862e-07\n",
      "187 Train Loss 657.9948 Test MSE 677.8091538135093 Test RE 0.43827115212923085 Lambda1 -8.116268e-07\n",
      "188 Train Loss 657.8525 Test MSE 677.3383468654589 Test RE 0.43811891388781166 Lambda1 -8.06981e-07\n",
      "189 Train Loss 657.6711 Test MSE 677.3365486132456 Test RE 0.4381183323107954 Lambda1 -1.5599348e-06\n",
      "190 Train Loss 656.8602 Test MSE 676.9065889635547 Test RE 0.4379792558735031 Lambda1 -8.724628e-07\n",
      "191 Train Loss 656.5496 Test MSE 676.88421529519 Test RE 0.4379720175899974 Lambda1 -1.3093519e-06\n",
      "192 Train Loss 656.3529 Test MSE 677.1943932565713 Test RE 0.4380723550665068 Lambda1 -8.567948e-07\n",
      "193 Train Loss 656.0107 Test MSE 677.2124525834704 Test RE 0.4380781962542076 Lambda1 -1.0371368e-06\n",
      "194 Train Loss 655.5216 Test MSE 677.7203842027892 Test RE 0.43824245199052864 Lambda1 -9.953558e-07\n",
      "195 Train Loss 655.4543 Test MSE 677.8350974513793 Test RE 0.4382795396223738 Lambda1 -8.9953875e-07\n",
      "196 Train Loss 655.219 Test MSE 677.5299884356286 Test RE 0.43818088871834515 Lambda1 -1.572874e-06\n",
      "197 Train Loss 654.994 Test MSE 676.5799779269813 Test RE 0.4378735794620792 Lambda1 -2.0503958e-06\n",
      "198 Train Loss 654.85333 Test MSE 676.0197943571383 Test RE 0.43769227024023794 Lambda1 -1.4930763e-06\n",
      "199 Train Loss 654.74963 Test MSE 676.0400156889825 Test RE 0.43769881639061003 Lambda1 -2.0121688e-06\n",
      "Training time: 937.42\n",
      "Training time: 937.42\n",
      "inv_HT_atanh\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 6227.9307 Test MSE 3552.7074270212593 Test RE 1.0033879764151006 Lambda1 -0.001636349\n",
      "1 Train Loss 3670.711 Test MSE 3538.2899404792183 Test RE 1.001349947741905 Lambda1 5.18687e-05\n",
      "2 Train Loss 3143.635 Test MSE 3128.679782810727 Test RE 0.9416071086399389 Lambda1 -0.0070490846\n",
      "3 Train Loss 854.60126 Test MSE 857.7707235370929 Test RE 0.49303162811896933 Lambda1 -0.08958478\n",
      "4 Train Loss 854.12396 Test MSE 856.741415132954 Test RE 0.49273572507759933 Lambda1 -0.084127896\n",
      "5 Train Loss 841.7349 Test MSE 836.1336117152069 Test RE 0.48677359466134873 Lambda1 0.012950495\n",
      "6 Train Loss 792.8145 Test MSE 795.2175102980859 Test RE 0.47471410709587664 Lambda1 -0.0018049302\n",
      "7 Train Loss 763.71313 Test MSE 765.3657678410689 Test RE 0.46571871184233066 Lambda1 -0.00019357109\n",
      "8 Train Loss 722.08704 Test MSE 727.9631090603791 Test RE 0.4541965762937135 Lambda1 -2.5045574e-05\n",
      "9 Train Loss 710.3956 Test MSE 720.5482290002925 Test RE 0.45187748002937767 Lambda1 5.2219624e-05\n",
      "10 Train Loss 702.9261 Test MSE 716.2532870608046 Test RE 0.4505287234806547 Lambda1 0.00011103684\n",
      "11 Train Loss 698.3745 Test MSE 711.1246323018022 Test RE 0.4489128444259489 Lambda1 0.00047186285\n",
      "12 Train Loss 683.9391 Test MSE 698.4680288326218 Test RE 0.44490003154873026 Lambda1 0.0002125861\n",
      "13 Train Loss 677.7014 Test MSE 693.3553639973003 Test RE 0.44326874534704186 Lambda1 0.000101147125\n",
      "14 Train Loss 673.3809 Test MSE 689.9377472374813 Test RE 0.44217493819725073 Lambda1 0.00020025439\n",
      "15 Train Loss 670.92566 Test MSE 688.3900036862159 Test RE 0.4416786922256137 Lambda1 0.00013759756\n",
      "16 Train Loss 669.05566 Test MSE 687.6099332444901 Test RE 0.44142837034940746 Lambda1 0.00011834378\n",
      "17 Train Loss 668.20764 Test MSE 686.0023980085502 Test RE 0.44091206971607555 Lambda1 0.00020735757\n",
      "18 Train Loss 665.5569 Test MSE 683.8717383083517 Test RE 0.4402268212647899 Lambda1 0.00039361336\n",
      "19 Train Loss 662.3115 Test MSE 681.3791502220164 Test RE 0.43942381555241405 Lambda1 0.00016934244\n",
      "20 Train Loss 661.83453 Test MSE 680.9534427725989 Test RE 0.4392865239877561 Lambda1 0.00033373397\n",
      "21 Train Loss 660.1748 Test MSE 679.7561750174053 Test RE 0.4389001723355357 Lambda1 0.00032157585\n",
      "22 Train Loss 658.8843 Test MSE 679.4809168417513 Test RE 0.4388112999590612 Lambda1 0.00061970076\n",
      "23 Train Loss 657.4232 Test MSE 678.0686535219452 Test RE 0.43835504030934463 Lambda1 0.0007743191\n",
      "24 Train Loss 656.13605 Test MSE 677.9008101565673 Test RE 0.4383007836065689 Lambda1 0.00039032518\n",
      "25 Train Loss 654.34766 Test MSE 674.7936385907544 Test RE 0.437295149858734 Lambda1 0.00043219543\n",
      "26 Train Loss 652.2327 Test MSE 674.5664899382206 Test RE 0.4372215426409398 Lambda1 0.00014260088\n",
      "27 Train Loss 650.8309 Test MSE 672.8281207755996 Test RE 0.43665781559754235 Lambda1 6.0448685e-05\n",
      "28 Train Loss 648.2338 Test MSE 669.6102527527584 Test RE 0.43561238421295073 Lambda1 2.4258832e-05\n",
      "29 Train Loss 647.0095 Test MSE 667.2597869632175 Test RE 0.43484716885131397 Lambda1 8.416382e-06\n",
      "30 Train Loss 643.27435 Test MSE 664.652870798202 Test RE 0.4339968857014556 Lambda1 -1.1434205e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 Train Loss 639.3216 Test MSE 662.5284994257556 Test RE 0.4333027576210353 Lambda1 1.0380403e-05\n",
      "32 Train Loss 638.38165 Test MSE 661.0892143815609 Test RE 0.43283184561366567 Lambda1 1.5800041e-06\n",
      "33 Train Loss 637.8712 Test MSE 660.8574087070858 Test RE 0.43275595443854326 Lambda1 -4.3037944e-06\n",
      "34 Train Loss 635.0114 Test MSE 656.950651723238 Test RE 0.4314749077230557 Lambda1 -8.2740735e-07\n",
      "35 Train Loss 633.7749 Test MSE 656.986317822409 Test RE 0.4314866200319977 Lambda1 -1.2946134e-07\n",
      "36 Train Loss 633.47876 Test MSE 656.841804396664 Test RE 0.43143916163194507 Lambda1 -9.830577e-07\n",
      "37 Train Loss 633.34204 Test MSE 656.6708873519689 Test RE 0.4313830255093655 Lambda1 -5.897661e-08\n",
      "38 Train Loss 632.26746 Test MSE 655.3129504473169 Test RE 0.43093676384613033 Lambda1 1.6109124e-06\n",
      "39 Train Loss 630.9653 Test MSE 653.7611829656391 Test RE 0.43042623677599917 Lambda1 7.229258e-06\n",
      "40 Train Loss 630.09033 Test MSE 653.6239603610186 Test RE 0.4303810617866269 Lambda1 -5.2426503e-06\n",
      "41 Train Loss 629.4184 Test MSE 652.7089371888462 Test RE 0.4300797061216843 Lambda1 4.9205414e-06\n",
      "42 Train Loss 628.59265 Test MSE 651.6604825200432 Test RE 0.42973414604506976 Lambda1 -4.9238506e-06\n",
      "43 Train Loss 628.04913 Test MSE 651.0755429392786 Test RE 0.42954123504979164 Lambda1 7.5309517e-06\n",
      "44 Train Loss 627.9528 Test MSE 651.0369139847196 Test RE 0.42952849230806767 Lambda1 -7.4414243e-06\n",
      "45 Train Loss 627.8521 Test MSE 651.040888003165 Test RE 0.4295298032564253 Lambda1 -4.7338613e-06\n",
      "46 Train Loss 627.7377 Test MSE 651.0478733752215 Test RE 0.4295321075797728 Lambda1 -3.3919346e-06\n",
      "47 Train Loss 627.5571 Test MSE 651.0270840434779 Test RE 0.4295252495919572 Lambda1 -7.400641e-06\n",
      "48 Train Loss 627.4007 Test MSE 650.8773978852089 Test RE 0.4294758678671722 Lambda1 -5.2749488e-06\n",
      "49 Train Loss 627.23224 Test MSE 650.8012576331402 Test RE 0.4294507468863776 Lambda1 -1.8281713e-06\n",
      "50 Train Loss 626.86285 Test MSE 650.6567963503657 Test RE 0.42940308068397903 Lambda1 2.1515743e-06\n",
      "51 Train Loss 626.3999 Test MSE 649.7271553854246 Test RE 0.4290962112355179 Lambda1 -7.42502e-07\n",
      "52 Train Loss 626.164 Test MSE 649.8068764407316 Test RE 0.4291225353265187 Lambda1 -3.3601253e-07\n",
      "53 Train Loss 626.06647 Test MSE 649.9322568293389 Test RE 0.4291639329764116 Lambda1 -2.233173e-06\n",
      "54 Train Loss 625.95953 Test MSE 649.8234837406401 Test RE 0.42912801889507307 Lambda1 -7.6988215e-07\n",
      "55 Train Loss 625.8778 Test MSE 649.6509372294116 Test RE 0.42907104230045456 Lambda1 -2.9403411e-06\n",
      "56 Train Loss 625.8003 Test MSE 649.7113865220145 Test RE 0.4290910041262383 Lambda1 9.901166e-07\n",
      "57 Train Loss 625.6863 Test MSE 649.7501635084251 Test RE 0.4291038087408122 Lambda1 -1.10224825e-07\n",
      "58 Train Loss 625.5732 Test MSE 649.4082516233044 Test RE 0.42899089226407777 Lambda1 7.1516894e-07\n",
      "59 Train Loss 625.453 Test MSE 649.2187660060144 Test RE 0.4289283017947862 Lambda1 -5.9605844e-07\n",
      "60 Train Loss 625.3792 Test MSE 649.0207161237983 Test RE 0.4288628726330682 Lambda1 -1.2056951e-06\n",
      "61 Train Loss 625.1265 Test MSE 648.8339129930037 Test RE 0.4288011498785276 Lambda1 -1.1049369e-06\n",
      "62 Train Loss 624.9656 Test MSE 648.7947474591173 Test RE 0.42878820783043575 Lambda1 -5.599805e-07\n",
      "63 Train Loss 624.86115 Test MSE 648.5109000415925 Test RE 0.4286944002426651 Lambda1 -1.0796373e-06\n",
      "64 Train Loss 624.77826 Test MSE 648.7666295608608 Test RE 0.4287789161755012 Lambda1 -8.0606446e-07\n",
      "65 Train Loss 624.42194 Test MSE 648.3661502164068 Test RE 0.4286465545530166 Lambda1 -3.883594e-07\n",
      "66 Train Loss 624.2768 Test MSE 648.0875430005203 Test RE 0.4285544485298971 Lambda1 -1.0566752e-06\n",
      "67 Train Loss 624.0571 Test MSE 648.169628351962 Test RE 0.4285815875553856 Lambda1 2.8708016e-08\n",
      "68 Train Loss 623.97644 Test MSE 647.9692362717097 Test RE 0.4285153309835792 Lambda1 -6.707943e-07\n",
      "69 Train Loss 623.87537 Test MSE 647.824626304066 Test RE 0.42846751154829127 Lambda1 9.385259e-07\n",
      "70 Train Loss 623.81415 Test MSE 647.8533046704399 Test RE 0.4284769952972791 Lambda1 5.7066137e-07\n",
      "71 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "72 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "73 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "74 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "75 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "76 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "77 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "78 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "79 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "80 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "81 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "82 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "83 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "84 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "85 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "86 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "87 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "88 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "89 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "90 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "91 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "92 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "93 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "94 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "95 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "96 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "97 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "98 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "99 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "100 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "101 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "102 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "103 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "104 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "105 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "106 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "107 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "108 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "109 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "110 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "111 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "112 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "113 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "115 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "116 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "117 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "118 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "119 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "120 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "121 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "122 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "123 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "124 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "125 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "126 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "127 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "128 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "129 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "130 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "131 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "132 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "133 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "134 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "135 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "136 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "137 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "138 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "139 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "140 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "141 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "142 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "143 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "144 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "145 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "146 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "147 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "148 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "149 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "150 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "151 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "152 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "153 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "154 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "155 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "156 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "157 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "158 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "159 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "160 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "161 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "162 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "163 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "164 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "165 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "166 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "167 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "168 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "169 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "170 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "171 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "172 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "173 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "174 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "175 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "176 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "177 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "178 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "179 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "180 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "181 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "182 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "183 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "184 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "185 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "186 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "187 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "188 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "189 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "190 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "191 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "192 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "193 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "194 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "195 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "197 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "198 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "199 Train Loss 623.7998 Test MSE 647.8734380561974 Test RE 0.4284836531514184 Lambda1 4.9628557e-08\n",
      "Training time: 454.46\n",
      "Training time: 454.46\n",
      "inv_HT_atanh\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 4856.808 Test MSE 3517.805497163952 Test RE 0.9984471513587905 Lambda1 0.0007134958\n",
      "1 Train Loss 3685.713 Test MSE 3512.7913082628475 Test RE 0.9977353172479413 Lambda1 2.3601842e-05\n",
      "2 Train Loss 3483.5999 Test MSE 3422.778780258435 Test RE 0.9848692684586059 Lambda1 8.8835506e-05\n",
      "3 Train Loss 3095.4524 Test MSE 3063.6034719213258 Test RE 0.9317629707832745 Lambda1 -4.9437866e-05\n",
      "4 Train Loss 2109.4695 Test MSE 2071.0858782802684 Test RE 0.7661048722297698 Lambda1 0.0059970235\n",
      "5 Train Loss 854.7251 Test MSE 858.0562866191409 Test RE 0.49311368962533497 Lambda1 0.008864308\n",
      "6 Train Loss 854.48126 Test MSE 857.6167431363154 Test RE 0.4929873735105207 Lambda1 -0.0018894627\n",
      "7 Train Loss 853.07513 Test MSE 854.7138048367221 Test RE 0.4921523122594994 Lambda1 -0.0012428982\n",
      "8 Train Loss 832.6292 Test MSE 820.7836027170498 Test RE 0.48228472431694985 Lambda1 0.0002497465\n",
      "9 Train Loss 786.35333 Test MSE 782.3043782416545 Test RE 0.47084401138320514 Lambda1 7.492619e-05\n",
      "10 Train Loss 757.95483 Test MSE 755.179401748716 Test RE 0.4626091712778371 Lambda1 -9.845566e-05\n",
      "11 Train Loss 745.9458 Test MSE 746.2945400118203 Test RE 0.4598797671256527 Lambda1 -6.3306696e-05\n",
      "12 Train Loss 734.38885 Test MSE 738.5293567912817 Test RE 0.4574809900447169 Lambda1 -0.00017747837\n",
      "13 Train Loss 720.95795 Test MSE 730.8859738021199 Test RE 0.4551074914450941 Lambda1 3.0055371e-05\n",
      "14 Train Loss 715.4788 Test MSE 727.3707351871734 Test RE 0.45401173935680383 Lambda1 2.7965598e-05\n",
      "15 Train Loss 707.25287 Test MSE 719.3775835718582 Test RE 0.4515102573258355 Lambda1 4.397763e-05\n",
      "16 Train Loss 702.56616 Test MSE 717.2277725812819 Test RE 0.4508350986814121 Lambda1 2.7272283e-05\n",
      "17 Train Loss 701.16534 Test MSE 716.7373345968423 Test RE 0.4506809325458446 Lambda1 1.5318903e-05\n",
      "18 Train Loss 700.31287 Test MSE 716.493176029421 Test RE 0.45060416315049323 Lambda1 3.0035728e-06\n",
      "19 Train Loss 700.1189 Test MSE 716.379430104775 Test RE 0.450568394197439 Lambda1 1.1290086e-06\n",
      "20 Train Loss 699.7377 Test MSE 716.0828940783223 Test RE 0.4504751310571135 Lambda1 6.2103154e-06\n",
      "21 Train Loss 699.0819 Test MSE 715.261464984349 Test RE 0.4502166836340586 Lambda1 8.508901e-06\n",
      "22 Train Loss 697.61786 Test MSE 714.0157064657617 Test RE 0.44982444546675293 Lambda1 -1.3146147e-06\n",
      "23 Train Loss 697.0424 Test MSE 713.9158900199457 Test RE 0.4497930025683018 Lambda1 -1.2414112e-05\n",
      "24 Train Loss 696.80176 Test MSE 713.8030049333076 Test RE 0.4497574403052411 Lambda1 -1.3773344e-05\n",
      "25 Train Loss 696.6263 Test MSE 713.6365818856508 Test RE 0.4497050068139772 Lambda1 -1.5934811e-05\n",
      "26 Train Loss 696.3881 Test MSE 713.8952950009441 Test RE 0.44978651471561965 Lambda1 -1.4480224e-05\n",
      "27 Train Loss 696.01196 Test MSE 713.3544245817513 Test RE 0.4496160959452245 Lambda1 -1.7221382e-05\n",
      "28 Train Loss 695.7651 Test MSE 713.2095040923059 Test RE 0.4495704230706496 Lambda1 -1.5266562e-05\n",
      "29 Train Loss 695.23193 Test MSE 712.5225405414019 Test RE 0.44935385774404824 Lambda1 -1.3711091e-05\n",
      "30 Train Loss 694.87915 Test MSE 712.0423351397433 Test RE 0.44920241094315444 Lambda1 -6.921503e-06\n",
      "31 Train Loss 694.4692 Test MSE 711.57127216098 Test RE 0.4490537978451843 Lambda1 1.0344024e-06\n",
      "32 Train Loss 693.9127 Test MSE 710.6617724082219 Test RE 0.4487667254731213 Lambda1 9.86464e-06\n",
      "33 Train Loss 692.6225 Test MSE 710.0073111137867 Test RE 0.44856003917513265 Lambda1 1.5698484e-05\n",
      "34 Train Loss 690.9192 Test MSE 708.3370624055514 Test RE 0.4480321235120969 Lambda1 1.8204515e-05\n",
      "35 Train Loss 690.71375 Test MSE 708.5514138270014 Test RE 0.4480999083727838 Lambda1 9.552291e-06\n",
      "36 Train Loss 690.5088 Test MSE 708.5670224773696 Test RE 0.4481048439329597 Lambda1 5.6946687e-06\n",
      "37 Train Loss 689.9299 Test MSE 708.19032156002 Test RE 0.4479857133908245 Lambda1 9.645249e-07\n",
      "38 Train Loss 689.67957 Test MSE 708.544285888046 Test RE 0.44809765445248706 Lambda1 1.01198225e-07\n",
      "39 Train Loss 688.73083 Test MSE 707.6650250810736 Test RE 0.4478195370363049 Lambda1 -2.5938418e-06\n",
      "40 Train Loss 686.0482 Test MSE 703.56479070902 Test RE 0.44652031181927454 Lambda1 -6.5641193e-06\n",
      "41 Train Loss 681.32355 Test MSE 698.1024811523002 Test RE 0.44478359568337295 Lambda1 -6.8232357e-06\n",
      "42 Train Loss 679.0956 Test MSE 695.6796616240603 Test RE 0.44401109663284677 Lambda1 -2.4540199e-05\n",
      "43 Train Loss 676.9822 Test MSE 694.7644488231946 Test RE 0.44371893746844127 Lambda1 -7.1604954e-06\n",
      "44 Train Loss 675.4858 Test MSE 694.2315425087958 Test RE 0.44354873159176683 Lambda1 5.435978e-06\n",
      "45 Train Loss 674.9634 Test MSE 694.3321845463847 Test RE 0.4435808808299245 Lambda1 4.0563036e-06\n",
      "46 Train Loss 674.51984 Test MSE 695.0338310969934 Test RE 0.44380495110396445 Lambda1 4.25058e-06\n",
      "47 Train Loss 674.0964 Test MSE 694.561016841035 Test RE 0.44365397067878387 Lambda1 7.961745e-06\n",
      "48 Train Loss 673.76306 Test MSE 694.2923522283921 Test RE 0.4435681570152239 Lambda1 3.8215762e-06\n",
      "49 Train Loss 673.6058 Test MSE 693.9926189590145 Test RE 0.44347240017318745 Lambda1 4.9637174e-06\n",
      "50 Train Loss 673.438 Test MSE 693.8431856362699 Test RE 0.44342465245928187 Lambda1 5.4273955e-06\n",
      "51 Train Loss 673.25745 Test MSE 693.7593855942873 Test RE 0.4433978739830254 Lambda1 1.4852914e-06\n",
      "52 Train Loss 673.113 Test MSE 693.3128502417928 Test RE 0.44325515541255345 Lambda1 1.580205e-06\n",
      "53 Train Loss 672.84454 Test MSE 693.1074009718627 Test RE 0.4431894756869676 Lambda1 -5.3567856e-06\n",
      "54 Train Loss 672.33356 Test MSE 693.1658941559796 Test RE 0.4432081762639972 Lambda1 -2.128189e-05\n",
      "55 Train Loss 671.8944 Test MSE 693.3478575953721 Test RE 0.44326634588311375 Lambda1 -1.6658145e-05\n",
      "56 Train Loss 671.58044 Test MSE 693.252066384915 Test RE 0.4432357245391229 Lambda1 -2.2890787e-05\n",
      "57 Train Loss 671.5337 Test MSE 693.026470669318 Test RE 0.4431636005445483 Lambda1 -3.0971154e-05\n",
      "58 Train Loss 671.4588 Test MSE 692.6391133503503 Test RE 0.4430397332283301 Lambda1 -4.1092884e-05\n",
      "59 Train Loss 671.05927 Test MSE 692.2153369973136 Test RE 0.4429041803226169 Lambda1 -4.804627e-05\n",
      "60 Train Loss 670.89056 Test MSE 692.4246942858018 Test RE 0.44297115240722684 Lambda1 -1.9984709e-05\n",
      "61 Train Loss 670.7467 Test MSE 692.1915846552012 Test RE 0.44289658145756877 Lambda1 -1.3331892e-05\n",
      "62 Train Loss 670.4278 Test MSE 691.5903640998924 Test RE 0.44270419514399645 Lambda1 4.6459286e-06\n",
      "63 Train Loss 670.0996 Test MSE 691.4770916622039 Test RE 0.4426679394069056 Lambda1 1.4551234e-05\n",
      "64 Train Loss 669.83954 Test MSE 691.2672765923711 Test RE 0.4426007748888485 Lambda1 9.390071e-06\n",
      "65 Train Loss 669.7768 Test MSE 690.9082167887959 Test RE 0.4424858115492592 Lambda1 1.0602757e-05\n",
      "66 Train Loss 669.39734 Test MSE 689.9826535388237 Test RE 0.4421893279867712 Lambda1 1.4485681e-05\n",
      "67 Train Loss 668.96747 Test MSE 689.5136958876765 Test RE 0.44203903195050165 Lambda1 -8.267006e-06\n",
      "68 Train Loss 668.3467 Test MSE 688.8519504984047 Test RE 0.44182686248167763 Lambda1 -3.685735e-05\n",
      "69 Train Loss 668.017 Test MSE 688.3579574714267 Test RE 0.4416684115018276 Lambda1 -2.8865348e-05\n",
      "70 Train Loss 667.4984 Test MSE 688.276811783216 Test RE 0.44164237814004104 Lambda1 -1.0027966e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 Train Loss 667.0989 Test MSE 688.2342078014093 Test RE 0.4416287092101127 Lambda1 7.498575e-06\n",
      "72 Train Loss 666.68207 Test MSE 687.1042457350776 Test RE 0.44126602113153895 Lambda1 5.9183025e-05\n",
      "73 Train Loss 665.37555 Test MSE 685.7197436557401 Test RE 0.44082122560522546 Lambda1 1.7565386e-05\n",
      "74 Train Loss 664.21533 Test MSE 685.3536503686813 Test RE 0.440703536681712 Lambda1 0.00015095039\n",
      "75 Train Loss 663.37646 Test MSE 684.5141068914554 Test RE 0.44043352779196204 Lambda1 0.00023733487\n",
      "76 Train Loss 662.5153 Test MSE 683.9484163536708 Test RE 0.44025150044372924 Lambda1 0.0002879684\n",
      "77 Train Loss 661.9599 Test MSE 682.6330820863185 Test RE 0.43982796224103693 Lambda1 0.0003315458\n",
      "78 Train Loss 661.3873 Test MSE 681.3913774781932 Test RE 0.43942775823502866 Lambda1 0.0006007976\n",
      "79 Train Loss 660.71924 Test MSE 681.6447536553487 Test RE 0.43950945150309406 Lambda1 0.00053143763\n",
      "80 Train Loss 660.2887 Test MSE 681.4082299361335 Test RE 0.43943319225743505 Lambda1 0.00053737813\n",
      "81 Train Loss 659.8422 Test MSE 680.4301620275943 Test RE 0.4391177060163742 Lambda1 0.0006996605\n",
      "82 Train Loss 659.221 Test MSE 679.9268476659712 Test RE 0.43895526823332603 Lambda1 0.00076941715\n",
      "83 Train Loss 657.44763 Test MSE 677.2729768717409 Test RE 0.4380977719269122 Lambda1 0.0008872795\n",
      "84 Train Loss 656.13324 Test MSE 675.8110413719286 Test RE 0.43762468595976767 Lambda1 0.00094723934\n",
      "85 Train Loss 655.5825 Test MSE 676.2780086991596 Test RE 0.43777585331899294 Lambda1 0.0007110395\n",
      "86 Train Loss 654.5387 Test MSE 673.7408190697594 Test RE 0.43695388064586727 Lambda1 0.0010562504\n",
      "87 Train Loss 652.5492 Test MSE 672.7960261754624 Test RE 0.4366474009576929 Lambda1 0.0013669484\n",
      "88 Train Loss 651.481 Test MSE 672.3938643921205 Test RE 0.4365168791204655 Lambda1 0.0018490192\n",
      "89 Train Loss 651.04254 Test MSE 672.3668355327623 Test RE 0.4365081054890197 Lambda1 0.0018828217\n",
      "90 Train Loss 650.55035 Test MSE 671.7539280913309 Test RE 0.4363091069292228 Lambda1 0.0021891897\n",
      "91 Train Loss 650.1582 Test MSE 671.8177070293262 Test RE 0.4363298188817969 Lambda1 0.0021991336\n",
      "92 Train Loss 649.2029 Test MSE 670.6498087558143 Test RE 0.4359503926633381 Lambda1 0.0025250185\n",
      "93 Train Loss 648.1585 Test MSE 669.8691822588034 Test RE 0.4356965988715761 Lambda1 0.0022233783\n",
      "94 Train Loss 647.4847 Test MSE 669.7492759684375 Test RE 0.4356576023761806 Lambda1 0.0021921694\n",
      "95 Train Loss 647.1376 Test MSE 668.7305531039631 Test RE 0.4353261475533795 Lambda1 0.002413145\n",
      "96 Train Loss 646.7378 Test MSE 667.550087865128 Test RE 0.4349417518012475 Lambda1 0.0022012966\n",
      "97 Train Loss 646.43353 Test MSE 667.4152956896833 Test RE 0.43489783771467183 Lambda1 0.0021229174\n",
      "98 Train Loss 646.1049 Test MSE 667.5009699022277 Test RE 0.43492575012121665 Lambda1 0.0020496212\n",
      "99 Train Loss 645.52344 Test MSE 667.1164523288305 Test RE 0.4348004614003653 Lambda1 0.0016616877\n",
      "100 Train Loss 644.3444 Test MSE 665.3873283288277 Test RE 0.43423660803347464 Lambda1 0.0016827568\n",
      "101 Train Loss 642.1153 Test MSE 661.5473730497769 Test RE 0.43298180367627764 Lambda1 0.001984366\n",
      "102 Train Loss 641.1727 Test MSE 661.5324065985435 Test RE 0.43297690588817866 Lambda1 0.0020249276\n",
      "103 Train Loss 640.08704 Test MSE 660.7527698717738 Test RE 0.4327216922277395 Lambda1 0.0019615216\n",
      "104 Train Loss 638.9234 Test MSE 660.0122662950114 Test RE 0.43247914932624176 Lambda1 0.0023830396\n",
      "105 Train Loss 638.61017 Test MSE 659.7268487637904 Test RE 0.4323856279743143 Lambda1 0.0025767407\n",
      "106 Train Loss 638.24084 Test MSE 659.7069111991691 Test RE 0.4323790943751833 Lambda1 0.002193423\n",
      "107 Train Loss 636.40515 Test MSE 656.7641374188862 Test RE 0.4314136535368816 Lambda1 0.0016857343\n",
      "108 Train Loss 634.7831 Test MSE 655.9830235828176 Test RE 0.4311570291180819 Lambda1 0.00047123976\n",
      "109 Train Loss 633.6399 Test MSE 654.8103830720861 Test RE 0.43077148686089395 Lambda1 0.00034530033\n",
      "110 Train Loss 632.2114 Test MSE 654.1895823512517 Test RE 0.43056723944058756 Lambda1 0.00019975918\n",
      "111 Train Loss 631.2287 Test MSE 653.6537786058014 Test RE 0.4303908786402533 Lambda1 -7.916016e-06\n",
      "112 Train Loss 630.43646 Test MSE 652.4687136640994 Test RE 0.4300005553970293 Lambda1 -2.0952852e-05\n",
      "113 Train Loss 629.99054 Test MSE 651.9611843759274 Test RE 0.4298332827544003 Lambda1 -1.8083681e-05\n",
      "114 Train Loss 629.2169 Test MSE 651.4495283135903 Test RE 0.42966458408298636 Lambda1 -1.8466442e-05\n",
      "115 Train Loss 628.6963 Test MSE 651.2967768255262 Test RE 0.42961420738493244 Lambda1 -6.757022e-06\n",
      "116 Train Loss 628.5326 Test MSE 651.1002358475218 Test RE 0.4295493804346499 Lambda1 -4.885743e-06\n",
      "117 Train Loss 628.2262 Test MSE 651.200049975807 Test RE 0.4295823042853362 Lambda1 3.1707798e-06\n",
      "118 Train Loss 627.3456 Test MSE 651.1334817271605 Test RE 0.4295603469217755 Lambda1 -1.0759857e-05\n",
      "119 Train Loss 626.9644 Test MSE 650.8899428792521 Test RE 0.42948000670058434 Lambda1 4.262397e-06\n",
      "120 Train Loss 626.7902 Test MSE 650.7802298254451 Test RE 0.4294438089152536 Lambda1 4.3175314e-06\n",
      "121 Train Loss 626.69073 Test MSE 650.5779543855174 Test RE 0.4293770638895479 Lambda1 -6.835121e-07\n",
      "122 Train Loss 626.51514 Test MSE 650.6162277236525 Test RE 0.42938969377636815 Lambda1 -2.7887656e-06\n",
      "123 Train Loss 626.2234 Test MSE 650.0612421125002 Test RE 0.4292065167108198 Lambda1 -1.2087563e-05\n",
      "124 Train Loss 626.0939 Test MSE 649.7081638972637 Test RE 0.42908993996064587 Lambda1 4.1227004e-06\n",
      "125 Train Loss 625.8425 Test MSE 649.4673914198556 Test RE 0.42901042532086453 Lambda1 6.5006116e-06\n",
      "126 Train Loss 625.6793 Test MSE 649.4741641969833 Test RE 0.4290126622187132 Lambda1 3.974792e-06\n",
      "127 Train Loss 625.5851 Test MSE 649.4978180568138 Test RE 0.4290204744717195 Lambda1 6.364585e-06\n",
      "128 Train Loss 625.4728 Test MSE 649.417357574088 Test RE 0.4289938998915973 Lambda1 2.963612e-06\n",
      "129 Train Loss 625.3988 Test MSE 649.3878446228381 Test RE 0.42898415190769 Lambda1 1.7851903e-06\n",
      "130 Train Loss 625.3777 Test MSE 649.3808417034212 Test RE 0.4289818388450334 Lambda1 1.9204338e-06\n",
      "131 Train Loss 625.27826 Test MSE 649.217592426632 Test RE 0.4289279141121856 Lambda1 1.0929791e-06\n",
      "132 Train Loss 625.12036 Test MSE 649.1455798487186 Test RE 0.42890412466024525 Lambda1 2.3768262e-06\n",
      "133 Train Loss 624.97205 Test MSE 649.1069290225865 Test RE 0.4288913557636895 Lambda1 2.8434315e-06\n",
      "134 Train Loss 624.91425 Test MSE 648.9984824980387 Test RE 0.42885552675177435 Lambda1 7.760765e-06\n",
      "135 Train Loss 624.8314 Test MSE 648.8366380704239 Test RE 0.4288020503517311 Lambda1 4.0707796e-06\n",
      "136 Train Loss 624.7362 Test MSE 648.853991182199 Test RE 0.42880778446084317 Lambda1 2.3319749e-06\n",
      "137 Train Loss 624.65735 Test MSE 648.7495222029613 Test RE 0.42877326289248585 Lambda1 1.3609023e-06\n",
      "138 Train Loss 624.55884 Test MSE 648.4396123810785 Test RE 0.42867083744614287 Lambda1 9.906105e-07\n",
      "139 Train Loss 624.50494 Test MSE 648.2983774700267 Test RE 0.42862415107512736 Lambda1 -9.363785e-08\n",
      "140 Train Loss 624.4065 Test MSE 648.162825080649 Test RE 0.42857933832590667 Lambda1 1.5797449e-06\n",
      "141 Train Loss 624.2906 Test MSE 648.006491782153 Test RE 0.4285276497226703 Lambda1 -1.3494612e-06\n",
      "142 Train Loss 624.24817 Test MSE 648.0383845652511 Test RE 0.42853819496519563 Lambda1 -2.655999e-07\n",
      "143 Train Loss 624.1744 Test MSE 648.1076771844375 Test RE 0.42856110544466364 Lambda1 -2.6576547e-06\n",
      "144 Train Loss 624.1319 Test MSE 648.1409942371014 Test RE 0.4285721207511675 Lambda1 -1.449322e-06\n",
      "145 Train Loss 624.0296 Test MSE 648.0832889583243 Test RE 0.42855304201335576 Lambda1 -3.2561907e-06\n",
      "146 Train Loss 623.9732 Test MSE 648.177930780844 Test RE 0.42858433240534527 Lambda1 -4.3083664e-06\n",
      "147 Train Loss 623.9478 Test MSE 648.1240432587354 Test RE 0.4285665164426056 Lambda1 -3.417588e-06\n",
      "148 Train Loss 623.9231 Test MSE 648.037991959353 Test RE 0.4285380651529403 Lambda1 -4.7145095e-06\n",
      "149 Train Loss 623.90485 Test MSE 647.9120218402488 Test RE 0.42849641202810057 Lambda1 -5.0247254e-06\n",
      "150 Train Loss 623.8667 Test MSE 648.0960559094001 Test RE 0.42855726314424164 Lambda1 -3.5297562e-06\n",
      "151 Train Loss 623.8496 Test MSE 648.0903593550263 Test RE 0.42855537970034935 Lambda1 -3.3099236e-06\n",
      "152 Train Loss 623.8364 Test MSE 648.0589743169518 Test RE 0.4285450027601919 Lambda1 -3.7177454e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 Train Loss 623.8157 Test MSE 648.0277171691866 Test RE 0.42853466785509226 Lambda1 -2.597667e-06\n",
      "154 Train Loss 623.80646 Test MSE 647.9757132412535 Test RE 0.4285174726545542 Lambda1 -2.3813311e-06\n",
      "155 Train Loss 623.79193 Test MSE 647.953352387117 Test RE 0.42851007878224345 Lambda1 -1.3687416e-06\n",
      "156 Train Loss 623.7822 Test MSE 647.982433099171 Test RE 0.42851969462754397 Lambda1 -1.3716301e-06\n",
      "157 Train Loss 623.74524 Test MSE 647.9225590327167 Test RE 0.42849989639839725 Lambda1 -1.9911506e-06\n",
      "158 Train Loss 623.72205 Test MSE 647.9748192659945 Test RE 0.42851717705385267 Lambda1 -1.5578614e-06\n",
      "159 Train Loss 623.70026 Test MSE 647.9516998311384 Test RE 0.428509532340635 Lambda1 -9.3563375e-07\n",
      "160 Train Loss 623.67645 Test MSE 647.9189240304291 Test RE 0.42849869440267524 Lambda1 -5.4167106e-07\n",
      "161 Train Loss 623.6701 Test MSE 647.9396903444477 Test RE 0.4285055612087031 Lambda1 -5.2630537e-07\n",
      "162 Train Loss 623.6468 Test MSE 647.8798961810471 Test RE 0.4284857887491059 Lambda1 -2.3814567e-07\n",
      "163 Train Loss 623.625 Test MSE 647.9876928339908 Test RE 0.42852143379150115 Lambda1 7.5449225e-07\n",
      "164 Train Loss 623.6171 Test MSE 648.0041615650002 Test RE 0.4285268792339559 Lambda1 -1.5070611e-10\n",
      "165 Train Loss 623.6087 Test MSE 647.9114321526434 Test RE 0.4284962170331999 Lambda1 -7.5479034e-07\n",
      "166 Train Loss 623.59906 Test MSE 647.9313855697211 Test RE 0.4285028150784605 Lambda1 -2.2277072e-07\n",
      "167 Train Loss 623.5883 Test MSE 647.9096631238002 Test RE 0.4284956320591996 Lambda1 -5.297437e-07\n",
      "168 Train Loss 623.58246 Test MSE 647.886952119311 Test RE 0.4284881220218764 Lambda1 -5.279187e-07\n",
      "169 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "170 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "171 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "172 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "173 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "174 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "175 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "176 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "177 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "178 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "179 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "180 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "181 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "182 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "183 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "184 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "185 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "186 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "187 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "188 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "189 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "190 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "191 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "192 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "193 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "194 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "195 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "196 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "197 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "198 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "199 Train Loss 623.5812 Test MSE 647.9009301588467 Test RE 0.4284927442723985 Lambda1 -1.2732848e-07\n",
      "Training time: 790.00\n",
      "Training time: 790.00\n",
      "inv_HT_atanh\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 11390.714 Test MSE 3501.4136701636085 Test RE 0.9961182160285945 Lambda1 -3.9325605e-06\n",
      "1 Train Loss 5991.7026 Test MSE 3494.921359296811 Test RE 0.99519428781928 Lambda1 -5.2394327e-05\n",
      "2 Train Loss 4190.8965 Test MSE 3504.530047750357 Test RE 0.9965614070298419 Lambda1 -3.6814294e-05\n",
      "3 Train Loss 3680.4739 Test MSE 3482.7991250466966 Test RE 0.9934668587465311 Lambda1 -0.00038623964\n",
      "4 Train Loss 3534.8447 Test MSE 3467.5223198575864 Test RE 0.9912856133374939 Lambda1 0.00014102171\n",
      "5 Train Loss 3464.4602 Test MSE 3415.3940033436065 Test RE 0.9838062481016783 Lambda1 -8.618226e-07\n",
      "6 Train Loss 3041.3909 Test MSE 2948.474631686683 Test RE 0.9140877069261405 Lambda1 -0.0029110943\n",
      "7 Train Loss 855.3938 Test MSE 858.4432736685173 Test RE 0.4932248752803158 Lambda1 0.09019238\n",
      "8 Train Loss 854.18787 Test MSE 857.1731189260482 Test RE 0.49285985184918263 Lambda1 0.08667058\n",
      "9 Train Loss 831.8015 Test MSE 828.9810316807851 Test RE 0.4846871069533739 Lambda1 0.06338071\n",
      "10 Train Loss 767.13983 Test MSE 770.7979779097165 Test RE 0.4673685169817124 Lambda1 0.0012401877\n",
      "11 Train Loss 741.4936 Test MSE 747.7208515371877 Test RE 0.46031901642558964 Lambda1 0.000236503\n",
      "12 Train Loss 725.37006 Test MSE 729.2610360713935 Test RE 0.45460130248230185 Lambda1 0.0001425973\n",
      "13 Train Loss 709.61053 Test MSE 714.8515192051552 Test RE 0.45008764628816234 Lambda1 0.00022735028\n",
      "14 Train Loss 617.07074 Test MSE 583.9952509661784 Test RE 0.40681206861594665 Lambda1 -0.0012574706\n",
      "15 Train Loss 543.0741 Test MSE 533.3391276156464 Test RE 0.38876834316784004 Lambda1 0.00024427514\n",
      "16 Train Loss 470.42953 Test MSE 455.27492733987737 Test RE 0.35919148894760966 Lambda1 0.0003414185\n",
      "17 Train Loss 433.67245 Test MSE 432.3721275752671 Test RE 0.3500402743459276 Lambda1 3.2756816e-05\n",
      "18 Train Loss 420.7797 Test MSE 418.3790718157291 Test RE 0.34432943240508135 Lambda1 1.8829669e-05\n",
      "19 Train Loss 409.618 Test MSE 409.31659716466453 Test RE 0.3405797702067352 Lambda1 -2.3616109e-05\n",
      "20 Train Loss 395.0551 Test MSE 397.3487554153105 Test RE 0.3355637964111169 Lambda1 0.00019536178\n",
      "21 Train Loss 381.72357 Test MSE 384.0051892452193 Test RE 0.32988131510244423 Lambda1 0.00040523906\n",
      "22 Train Loss 373.04218 Test MSE 376.9297592872808 Test RE 0.32682809623727294 Lambda1 9.707301e-06\n",
      "23 Train Loss 368.6094 Test MSE 371.47852678118056 Test RE 0.3244561629752689 Lambda1 0.00021653241\n",
      "24 Train Loss 366.05795 Test MSE 369.09134112789496 Test RE 0.32341197686052797 Lambda1 0.00022184444\n",
      "25 Train Loss 360.4706 Test MSE 363.441778768473 Test RE 0.32092725045099296 Lambda1 0.0003494917\n",
      "26 Train Loss 354.71722 Test MSE 360.2500322061843 Test RE 0.31951495097205984 Lambda1 0.00027047016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 Train Loss 350.50204 Test MSE 353.6043865766258 Test RE 0.3165541362527515 Lambda1 0.00046545122\n",
      "28 Train Loss 346.58368 Test MSE 351.0242650517616 Test RE 0.31539713211173315 Lambda1 8.823388e-05\n",
      "29 Train Loss 343.4765 Test MSE 347.03640252713217 Test RE 0.3136004570544699 Lambda1 0.00056708395\n",
      "30 Train Loss 338.81046 Test MSE 341.24673118765276 Test RE 0.31097352789443733 Lambda1 0.0006878319\n",
      "31 Train Loss 332.84418 Test MSE 334.3956879131624 Test RE 0.3078360692395265 Lambda1 0.0008478949\n",
      "32 Train Loss 330.03052 Test MSE 332.48097606178163 Test RE 0.30695348679316664 Lambda1 0.0007045859\n",
      "33 Train Loss 327.15186 Test MSE 331.0242786297945 Test RE 0.30628032167193053 Lambda1 0.00034475425\n",
      "34 Train Loss 324.54288 Test MSE 328.6194194976776 Test RE 0.30516574531189633 Lambda1 0.0006299466\n",
      "35 Train Loss 321.3202 Test MSE 324.9169384744698 Test RE 0.30344175873649865 Lambda1 0.0006816386\n",
      "36 Train Loss 318.98148 Test MSE 323.6063394198355 Test RE 0.30282915243431335 Lambda1 0.0005267363\n",
      "37 Train Loss 316.44138 Test MSE 320.3097614841676 Test RE 0.30128274367538144 Lambda1 7.237208e-05\n",
      "38 Train Loss 314.47592 Test MSE 319.3908310044973 Test RE 0.30085026114926366 Lambda1 4.7354897e-05\n",
      "39 Train Loss 313.08218 Test MSE 318.69216194925554 Test RE 0.30052102526685986 Lambda1 3.354942e-05\n",
      "40 Train Loss 311.49707 Test MSE 317.4401884835193 Test RE 0.29993015006107 Lambda1 6.8718015e-05\n",
      "41 Train Loss 309.9905 Test MSE 316.19352930198613 Test RE 0.29934062403462286 Lambda1 9.664628e-06\n",
      "42 Train Loss 308.50568 Test MSE 314.9816036389932 Test RE 0.29876640786904535 Lambda1 -4.3086628e-05\n",
      "43 Train Loss 306.66748 Test MSE 313.068313526322 Test RE 0.2978576285768296 Lambda1 -0.00010640641\n",
      "44 Train Loss 305.03284 Test MSE 312.6718420841595 Test RE 0.2976689645532695 Lambda1 -0.00022397468\n",
      "45 Train Loss 304.25644 Test MSE 312.05024123096746 Test RE 0.29737293000385284 Lambda1 -0.00017953386\n",
      "46 Train Loss 303.46976 Test MSE 311.02273726651003 Test RE 0.29688293870491606 Lambda1 -0.00015231177\n",
      "47 Train Loss 302.51358 Test MSE 310.6359911675277 Test RE 0.29669829941652126 Lambda1 -9.040248e-05\n",
      "48 Train Loss 301.9965 Test MSE 310.1519294179061 Test RE 0.29646703791389967 Lambda1 -6.115904e-05\n",
      "49 Train Loss 301.26492 Test MSE 309.0426178964044 Test RE 0.2959363804109522 Lambda1 -1.5291584e-05\n",
      "50 Train Loss 300.7658 Test MSE 309.0126400059757 Test RE 0.29592202678620433 Lambda1 1.3906459e-05\n",
      "51 Train Loss 300.2954 Test MSE 308.2133227784738 Test RE 0.29553905097542227 Lambda1 8.37743e-06\n",
      "52 Train Loss 299.8713 Test MSE 308.06108874829596 Test RE 0.2954660550106137 Lambda1 2.1244095e-05\n",
      "53 Train Loss 298.8913 Test MSE 307.2257649482483 Test RE 0.29506519723670954 Lambda1 3.766768e-05\n",
      "54 Train Loss 298.19113 Test MSE 306.36042129251956 Test RE 0.2946493583419065 Lambda1 2.6998527e-05\n",
      "55 Train Loss 297.55188 Test MSE 305.7573202787881 Test RE 0.29435919215655704 Lambda1 3.0452997e-05\n",
      "56 Train Loss 297.0105 Test MSE 305.27802648099254 Test RE 0.29412838841310046 Lambda1 5.4341952e-05\n",
      "57 Train Loss 295.92242 Test MSE 303.7982864623833 Test RE 0.2934146746735142 Lambda1 2.4859739e-05\n",
      "58 Train Loss 294.43707 Test MSE 301.5590272265904 Test RE 0.29233131318573236 Lambda1 1.001504e-05\n",
      "59 Train Loss 292.57123 Test MSE 299.5755245189172 Test RE 0.2913683233214858 Lambda1 -8.656567e-06\n",
      "60 Train Loss 291.64832 Test MSE 297.24429820357045 Test RE 0.29023242928813636 Lambda1 3.8309054e-06\n",
      "61 Train Loss 289.3779 Test MSE 294.1066911559207 Test RE 0.2886965694321096 Lambda1 2.9643352e-06\n",
      "62 Train Loss 287.4211 Test MSE 291.30354791631066 Test RE 0.2873174859681489 Lambda1 3.9974722e-05\n",
      "63 Train Loss 285.1744 Test MSE 289.00738821921635 Test RE 0.28618287563359673 Lambda1 6.21959e-05\n",
      "64 Train Loss 283.70407 Test MSE 288.51540721862324 Test RE 0.285939185481988 Lambda1 5.2809122e-05\n",
      "65 Train Loss 282.4506 Test MSE 288.003839558485 Test RE 0.2856855731356185 Lambda1 5.2653406e-05\n",
      "66 Train Loss 280.98865 Test MSE 287.069808733379 Test RE 0.28522194072132934 Lambda1 6.672693e-05\n",
      "67 Train Loss 279.65186 Test MSE 286.55876411804945 Test RE 0.28496795012092974 Lambda1 6.4051455e-05\n",
      "68 Train Loss 278.62766 Test MSE 286.2515841076626 Test RE 0.28481517180744603 Lambda1 5.2677296e-05\n",
      "69 Train Loss 277.58185 Test MSE 285.6313681689486 Test RE 0.2845064526472025 Lambda1 4.580651e-05\n",
      "70 Train Loss 277.22006 Test MSE 285.4669228538459 Test RE 0.2844245420198204 Lambda1 2.7880293e-05\n",
      "71 Train Loss 276.855 Test MSE 285.2445882928929 Test RE 0.28431375909114043 Lambda1 1.3416973e-05\n",
      "72 Train Loss 276.6555 Test MSE 285.17631797775533 Test RE 0.284279733289647 Lambda1 1.3905883e-05\n",
      "73 Train Loss 276.4345 Test MSE 284.7099383484034 Test RE 0.284047181497683 Lambda1 1.6012926e-05\n",
      "74 Train Loss 276.02597 Test MSE 284.12918459469455 Test RE 0.28375733268488806 Lambda1 1.4698853e-05\n",
      "75 Train Loss 275.711 Test MSE 283.8751954214064 Test RE 0.2836304759451148 Lambda1 1.4452559e-05\n",
      "76 Train Loss 275.51392 Test MSE 283.45916758218976 Test RE 0.28342256514462066 Lambda1 1.3536123e-05\n",
      "77 Train Loss 275.27405 Test MSE 283.64436705033256 Test RE 0.28351513780326537 Lambda1 1.0995217e-05\n",
      "78 Train Loss 275.03436 Test MSE 283.4265345382979 Test RE 0.2834062502600166 Lambda1 1.1550315e-05\n",
      "79 Train Loss 274.75992 Test MSE 283.38101131166604 Test RE 0.2833834893617187 Lambda1 1.1943145e-05\n",
      "80 Train Loss 274.53528 Test MSE 282.90117676055627 Test RE 0.2831434683413148 Lambda1 9.036196e-06\n",
      "81 Train Loss 274.09793 Test MSE 282.7687016956137 Test RE 0.28307716631672997 Lambda1 1.12204e-05\n",
      "82 Train Loss 273.88214 Test MSE 282.4542741382163 Test RE 0.2829197372619464 Lambda1 1.0259834e-05\n",
      "83 Train Loss 273.52325 Test MSE 282.54909583787946 Test RE 0.28296722225632687 Lambda1 6.933255e-06\n",
      "84 Train Loss 273.41745 Test MSE 282.6097587191173 Test RE 0.2829975969522505 Lambda1 4.981197e-06\n",
      "85 Train Loss 273.2981 Test MSE 282.4939494220488 Test RE 0.28293960689705644 Lambda1 3.2599557e-06\n",
      "86 Train Loss 273.07156 Test MSE 282.2272630543101 Test RE 0.2828060218183798 Lambda1 4.2850075e-07\n",
      "87 Train Loss 272.97412 Test MSE 282.28452515574145 Test RE 0.2828347101272213 Lambda1 2.9473907e-07\n",
      "88 Train Loss 272.8034 Test MSE 281.984423023719 Test RE 0.28268432662554405 Lambda1 1.2998412e-06\n",
      "89 Train Loss 272.42905 Test MSE 281.9017892956394 Test RE 0.2826429041755035 Lambda1 3.3263774e-07\n",
      "90 Train Loss 272.26245 Test MSE 281.95326649398055 Test RE 0.2826687092630089 Lambda1 2.9438802e-06\n",
      "91 Train Loss 272.11505 Test MSE 282.00209924070066 Test RE 0.2826931865319488 Lambda1 9.2555047e-07\n",
      "92 Train Loss 272.07275 Test MSE 281.9344575895168 Test RE 0.2826592807901903 Lambda1 1.3742398e-06\n",
      "93 Train Loss 271.96878 Test MSE 281.64558824797035 Test RE 0.282514437682416 Lambda1 1.7764515e-07\n",
      "94 Train Loss 271.8708 Test MSE 281.7126183072347 Test RE 0.28254805410266665 Lambda1 6.0178155e-07\n",
      "95 Train Loss 271.6042 Test MSE 281.5218089028166 Test RE 0.282452350263787 Lambda1 1.7295205e-06\n",
      "96 Train Loss 271.50064 Test MSE 281.6167187847281 Test RE 0.28249995804998185 Lambda1 2.6432504e-06\n",
      "97 Train Loss 271.39362 Test MSE 281.6503942504926 Test RE 0.28251684808642114 Lambda1 -7.993047e-07\n",
      "98 Train Loss 271.34662 Test MSE 281.6126921816218 Test RE 0.28249793842685783 Lambda1 -7.0404496e-07\n",
      "99 Train Loss 271.2929 Test MSE 281.41525822575204 Test RE 0.2823988937705606 Lambda1 -3.8437534e-07\n",
      "100 Train Loss 271.25507 Test MSE 281.2770215531924 Test RE 0.28232952532395245 Lambda1 -2.0020153e-07\n",
      "101 Train Loss 271.22458 Test MSE 281.141009595585 Test RE 0.2822612566213422 Lambda1 -3.1699818e-07\n",
      "102 Train Loss 271.14328 Test MSE 281.0352932494202 Test RE 0.28220818283747157 Lambda1 -1.1841354e-06\n",
      "103 Train Loss 271.0863 Test MSE 281.0754592491626 Test RE 0.28222834893242504 Lambda1 -1.5491742e-06\n",
      "104 Train Loss 271.02908 Test MSE 280.9671550869407 Test RE 0.2821739694963629 Lambda1 -1.3038775e-06\n",
      "105 Train Loss 271.00726 Test MSE 280.97280457541956 Test RE 0.2821768063592266 Lambda1 -1.2015726e-06\n",
      "106 Train Loss 270.98077 Test MSE 280.95992834494865 Test RE 0.2821703405818257 Lambda1 -1.2426411e-06\n",
      "107 Train Loss 270.94226 Test MSE 281.0298189871646 Test RE 0.2822054342696513 Lambda1 -1.7462827e-06\n",
      "108 Train Loss 270.88556 Test MSE 280.9465845848117 Test RE 0.2821636398788616 Lambda1 2.0347181e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 Train Loss 270.77615 Test MSE 280.8866380388243 Test RE 0.28213353515624295 Lambda1 -2.3850962e-06\n",
      "110 Train Loss 270.73233 Test MSE 280.92925694140706 Test RE 0.2821549383914773 Lambda1 -1.6068373e-06\n",
      "111 Train Loss 270.7124 Test MSE 280.9983950223165 Test RE 0.28218965611914876 Lambda1 -1.5918083e-06\n",
      "112 Train Loss 270.6908 Test MSE 281.01661913400835 Test RE 0.2821988066561689 Lambda1 -1.096729e-06\n",
      "113 Train Loss 270.6627 Test MSE 281.02856065863955 Test RE 0.2822048024727362 Lambda1 -8.945336e-07\n",
      "114 Train Loss 270.6304 Test MSE 281.08070721921695 Test RE 0.28223098366797644 Lambda1 -1.0399452e-06\n",
      "115 Train Loss 270.6083 Test MSE 281.013100892167 Test RE 0.28219704012940666 Lambda1 -4.766304e-07\n",
      "116 Train Loss 270.59073 Test MSE 281.0053916129625 Test RE 0.28219316922323323 Lambda1 -1.0699408e-06\n",
      "117 Train Loss 270.5516 Test MSE 280.9726301951435 Test RE 0.28217671879545486 Lambda1 -1.2512701e-06\n",
      "118 Train Loss 270.52402 Test MSE 280.8603622865039 Test RE 0.2821203386504909 Lambda1 -1.0561453e-06\n",
      "119 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "120 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "121 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "122 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "123 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "124 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "125 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "126 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "127 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "128 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "129 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "130 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "131 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "132 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "133 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "134 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "135 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "136 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "137 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "138 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "139 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "140 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "141 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "142 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "143 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "144 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "145 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "146 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "147 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "148 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "149 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "150 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "151 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "152 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "153 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "154 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "155 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "156 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "157 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "158 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "159 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "160 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "161 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "162 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "163 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "164 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "165 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "166 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "167 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "168 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "169 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "170 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "171 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "172 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "173 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "174 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "175 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "176 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "177 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "178 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "179 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "180 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "181 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "182 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "183 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "184 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "185 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "186 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "187 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "188 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "189 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "190 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "192 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "193 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "194 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "195 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "196 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "197 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "198 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "199 Train Loss 270.5184 Test MSE 280.86759590185846 Test RE 0.282123971660249 Lambda1 -7.911377e-07\n",
      "Training time: 614.29\n",
      "Training time: 614.29\n",
      "inv_HT_atanh\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 8146.838 Test MSE 3587.3823617328353 Test RE 1.0082726918514267 Lambda1 -0.0010298395\n",
      "1 Train Loss 4417.6484 Test MSE 3576.4432410457134 Test RE 1.0067342389302802 Lambda1 0.00061526574\n",
      "2 Train Loss 3729.7036 Test MSE 3545.474721281609 Test RE 1.002366092753141 Lambda1 0.00029854022\n",
      "3 Train Loss 3503.8767 Test MSE 3462.0638132592944 Test RE 0.9905050747153158 Lambda1 -0.0004260869\n",
      "4 Train Loss 2544.2683 Test MSE 2469.5048370417253 Test RE 0.8365542524705923 Lambda1 0.110673845\n",
      "5 Train Loss 854.7284 Test MSE 858.0562013515565 Test RE 0.4931136651242534 Lambda1 0.5937187\n",
      "6 Train Loss 854.72064 Test MSE 858.0756285678196 Test RE 0.4931192473760779 Lambda1 0.5941716\n",
      "7 Train Loss 854.72064 Test MSE 858.0761532432704 Test RE 0.4931193981364045 Lambda1 0.5941794\n",
      "8 Train Loss 854.72064 Test MSE 858.0765144762255 Test RE 0.49311950193311177 Lambda1 0.59418535\n",
      "9 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "10 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "11 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "12 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "13 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "14 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "15 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "16 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "17 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "18 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "19 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "20 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "21 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "22 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "23 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "24 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "25 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "26 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "27 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "28 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "29 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "30 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "31 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "32 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "33 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "34 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "35 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "36 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "37 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "38 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "39 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "40 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "41 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "42 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "43 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "44 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "45 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "46 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "47 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "48 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "49 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "50 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "51 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "52 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "53 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "54 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "55 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "56 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "57 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "58 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "59 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "60 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "61 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "62 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "63 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "64 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "65 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "66 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "68 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "69 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "70 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "71 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "72 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "73 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "74 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "75 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "76 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "77 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "78 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "79 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "80 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "81 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "82 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "83 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "84 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "85 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "86 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "87 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "88 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "89 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "90 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "91 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "92 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "93 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "94 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "95 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "96 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "97 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "98 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "99 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "100 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "101 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "102 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "103 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "104 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "105 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "106 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "107 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "108 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "109 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "110 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "111 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "112 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "113 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "114 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "115 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "116 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "117 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "118 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "119 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "120 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "121 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "122 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "123 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "124 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "125 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "126 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "127 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "128 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "129 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "130 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "131 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "132 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "133 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "134 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "135 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "136 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "137 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "138 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "139 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "140 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "141 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "142 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "143 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "144 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "145 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "146 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "147 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "148 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "149 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "150 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "152 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "153 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "154 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "155 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "156 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "157 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "158 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "159 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "160 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "161 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "162 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "163 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "164 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "165 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "166 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "167 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "168 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "169 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "170 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "171 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "172 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "173 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "174 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "175 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "176 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "177 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "178 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "179 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "180 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "181 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "182 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "183 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "184 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "185 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "186 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "187 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "188 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "189 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "190 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "191 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "192 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "193 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "194 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "195 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "196 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "197 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "198 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "199 Train Loss 854.72064 Test MSE 858.0769426401218 Test RE 0.49311962496173306 Lambda1 0.5941898\n",
      "Training time: 212.78\n",
      "Training time: 212.78\n",
      "inv_HT_atanh\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 30371.723 Test MSE 3576.210461739662 Test RE 1.006701475833669 Lambda1 -0.0004633428\n",
      "1 Train Loss 16225.311 Test MSE 3565.8296300906964 Test RE 1.0052393145048006 Lambda1 -0.0003919829\n",
      "2 Train Loss 10320.791 Test MSE 3557.053048436948 Test RE 1.0040014538552098 Lambda1 0.00021334736\n",
      "3 Train Loss 6815.412 Test MSE 3559.834373229631 Test RE 1.0043939006985736 Lambda1 0.00019354594\n",
      "4 Train Loss 4842.374 Test MSE 3553.331873975765 Test RE 1.0034761535376435 Lambda1 4.6586974e-05\n",
      "5 Train Loss 4295.905 Test MSE 3552.218710024008 Test RE 1.0033189600896237 Lambda1 0.00030518897\n",
      "6 Train Loss 3979.8809 Test MSE 3550.9404753771314 Test RE 1.0031384260974712 Lambda1 0.0001749257\n",
      "7 Train Loss 3812.3271 Test MSE 3548.1538763728836 Test RE 1.002744742623288 Lambda1 0.00014945643\n",
      "8 Train Loss 3742.825 Test MSE 3547.7635700117776 Test RE 1.002689588812463 Lambda1 0.00022764888\n",
      "9 Train Loss 3677.72 Test MSE 3545.6871387374745 Test RE 1.002396119319817 Lambda1 0.00010315607\n",
      "10 Train Loss 3635.0742 Test MSE 3537.721035054948 Test RE 1.001269443271231 Lambda1 0.00014883197\n",
      "11 Train Loss 3597.711 Test MSE 3522.855588392342 Test RE 0.999163569714623 Lambda1 0.00029067634\n",
      "12 Train Loss 3564.17 Test MSE 3500.172117620054 Test RE 0.9959415955442942 Lambda1 0.00038348514\n",
      "13 Train Loss 3544.1218 Test MSE 3483.6747525914343 Test RE 0.9935917370764399 Lambda1 0.00012859279\n",
      "14 Train Loss 3509.647 Test MSE 3437.2644716079167 Test RE 0.9869511221996787 Lambda1 0.00027786652\n",
      "15 Train Loss 3461.708 Test MSE 3372.3923545211987 Test RE 0.9775933049848132 Lambda1 0.00018393648\n",
      "16 Train Loss 3386.6094 Test MSE 3263.2909253868165 Test RE 0.9616500693638574 Lambda1 7.974231e-05\n",
      "17 Train Loss 3285.06 Test MSE 3188.733202697849 Test RE 0.9506009907072813 Lambda1 0.00021816944\n",
      "18 Train Loss 3110.0522 Test MSE 3026.712076577266 Test RE 0.9261359133320162 Lambda1 0.00016407132\n",
      "19 Train Loss 2941.527 Test MSE 2897.6607433947197 Test RE 0.9061768001117755 Lambda1 0.00042907704\n",
      "20 Train Loss 861.07227 Test MSE 860.9076378202024 Test RE 0.4939323271608334 Lambda1 -0.057465114\n",
      "21 Train Loss 851.2832 Test MSE 849.8129871218272 Test RE 0.49073931521024616 Lambda1 -0.051112812\n",
      "22 Train Loss 803.5466 Test MSE 773.5727461996617 Test RE 0.4682089929315059 Lambda1 -0.03269432\n",
      "23 Train Loss 607.29285 Test MSE 575.4870860656852 Test RE 0.4038377949501211 Lambda1 -0.0022297604\n",
      "24 Train Loss 539.3265 Test MSE 518.8956583094591 Test RE 0.38346805359382724 Lambda1 -0.0013798982\n",
      "25 Train Loss 445.5082 Test MSE 430.03681706995974 Test RE 0.3490936828952484 Lambda1 -0.00079924683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 Train Loss 410.83112 Test MSE 411.05913013292013 Test RE 0.34130395436482464 Lambda1 -0.000362437\n",
      "27 Train Loss 394.97052 Test MSE 393.8922998767391 Test RE 0.3341011081471487 Lambda1 -0.00031402745\n",
      "28 Train Loss 367.4974 Test MSE 373.63357330758515 Test RE 0.32539593057901567 Lambda1 -0.00023599259\n",
      "29 Train Loss 353.36295 Test MSE 358.59166240892876 Test RE 0.3187786774006696 Lambda1 -3.5207213e-05\n",
      "30 Train Loss 342.587 Test MSE 350.6679232203352 Test RE 0.31523700396633375 Lambda1 -2.393539e-05\n",
      "31 Train Loss 337.22125 Test MSE 346.69839139707625 Test RE 0.31344769747707235 Lambda1 3.080623e-05\n",
      "32 Train Loss 333.3234 Test MSE 341.17027819306014 Test RE 0.3109386906534415 Lambda1 1.0638276e-05\n",
      "33 Train Loss 329.5833 Test MSE 338.2735624016456 Test RE 0.3096158599593161 Lambda1 1.6141672e-05\n",
      "34 Train Loss 323.22247 Test MSE 332.6724221242401 Test RE 0.307041847607299 Lambda1 7.9253325e-05\n",
      "35 Train Loss 310.29517 Test MSE 314.6119205205888 Test RE 0.2985910304460264 Lambda1 0.00038234284\n",
      "36 Train Loss 303.53955 Test MSE 309.85330104114354 Test RE 0.29632427757102475 Lambda1 0.00033642925\n",
      "37 Train Loss 297.52304 Test MSE 305.6697412599979 Test RE 0.2943170320285493 Lambda1 0.0004640389\n",
      "38 Train Loss 296.09323 Test MSE 304.0156423207 Test RE 0.2935196192998869 Lambda1 0.0005787208\n",
      "39 Train Loss 294.30307 Test MSE 302.256855702936 Test RE 0.29266935518903664 Lambda1 0.0004248229\n",
      "40 Train Loss 291.9354 Test MSE 299.2613752463931 Test RE 0.29121551184588557 Lambda1 0.000587762\n",
      "41 Train Loss 290.16644 Test MSE 298.27679420713696 Test RE 0.2907360622443231 Lambda1 0.0002470272\n",
      "42 Train Loss 288.48508 Test MSE 296.7119107994104 Test RE 0.28997239849840156 Lambda1 0.00019862785\n",
      "43 Train Loss 286.2513 Test MSE 295.498934032178 Test RE 0.2893790788848761 Lambda1 0.00015172381\n",
      "44 Train Loss 284.10922 Test MSE 293.0967026370868 Test RE 0.28820043828620806 Lambda1 8.38227e-05\n",
      "45 Train Loss 282.27203 Test MSE 291.5385874538063 Test RE 0.28743337427963334 Lambda1 0.00010031697\n",
      "46 Train Loss 281.21155 Test MSE 290.86868703308596 Test RE 0.2871029506483099 Lambda1 8.432534e-05\n",
      "47 Train Loss 280.0175 Test MSE 289.1102433722885 Test RE 0.2862337960705786 Lambda1 0.00010734497\n",
      "48 Train Loss 278.5959 Test MSE 287.68160713915654 Test RE 0.28552570909103264 Lambda1 4.6548685e-05\n",
      "49 Train Loss 277.26758 Test MSE 286.45398638814953 Test RE 0.2849158473264896 Lambda1 1.795564e-05\n",
      "50 Train Loss 276.32 Test MSE 285.125744176836 Test RE 0.284254524772663 Lambda1 1.0414337e-05\n",
      "51 Train Loss 275.6782 Test MSE 284.3757712454204 Test RE 0.2838804379470597 Lambda1 1.3189576e-06\n",
      "52 Train Loss 274.81506 Test MSE 283.7225172403001 Test RE 0.2835541924056028 Lambda1 1.5255326e-05\n",
      "53 Train Loss 274.25375 Test MSE 283.1664967239709 Test RE 0.2832762108241015 Lambda1 2.2185834e-05\n",
      "54 Train Loss 273.26233 Test MSE 282.39904233447953 Test RE 0.28289207449879855 Lambda1 2.221826e-05\n",
      "55 Train Loss 272.6107 Test MSE 281.56897916647085 Test RE 0.282476012362708 Lambda1 2.2357424e-05\n",
      "56 Train Loss 271.62305 Test MSE 280.8386800145096 Test RE 0.2821094486695587 Lambda1 1.644142e-05\n",
      "57 Train Loss 271.08206 Test MSE 280.8606457074162 Test RE 0.2821204809966428 Lambda1 1.3254061e-05\n",
      "58 Train Loss 270.72305 Test MSE 280.9101693773743 Test RE 0.2821453528075538 Lambda1 1.0075979e-05\n",
      "59 Train Loss 270.3357 Test MSE 280.8041847434488 Test RE 0.2820921224580384 Lambda1 9.499184e-06\n",
      "60 Train Loss 270.18396 Test MSE 280.78309246684967 Test RE 0.2820815277498183 Lambda1 1.1206351e-05\n",
      "61 Train Loss 270.0958 Test MSE 280.69356520422417 Test RE 0.28203655353117046 Lambda1 1.2795067e-05\n",
      "62 Train Loss 270.04022 Test MSE 280.623238896652 Test RE 0.2820012199247255 Lambda1 1.13403985e-05\n",
      "63 Train Loss 269.996 Test MSE 280.5826611066629 Test RE 0.28198083066542123 Lambda1 9.998955e-06\n",
      "64 Train Loss 269.93018 Test MSE 280.59425825613846 Test RE 0.281986658074783 Lambda1 1.4804675e-05\n",
      "65 Train Loss 269.8646 Test MSE 280.5955511280072 Test RE 0.28198730771779407 Lambda1 1.3161873e-05\n",
      "66 Train Loss 269.82245 Test MSE 280.4479816163159 Test RE 0.2819131472370446 Lambda1 1.5088734e-05\n",
      "67 Train Loss 269.7688 Test MSE 280.44998454502337 Test RE 0.2819141539316269 Lambda1 1.0112485e-05\n",
      "68 Train Loss 269.73965 Test MSE 280.4141523094312 Test RE 0.2818961437023153 Lambda1 1.01068135e-05\n",
      "69 Train Loss 269.70743 Test MSE 280.44812692802157 Test RE 0.2819132202724694 Lambda1 6.781413e-06\n",
      "70 Train Loss 269.69186 Test MSE 280.4745377513182 Test RE 0.2819264943582006 Lambda1 4.457573e-06\n",
      "71 Train Loss 269.65677 Test MSE 280.37899502039227 Test RE 0.2818784716005637 Lambda1 6.1193364e-06\n",
      "72 Train Loss 269.62704 Test MSE 280.33198610366156 Test RE 0.28185484044898734 Lambda1 2.8861239e-06\n",
      "73 Train Loss 269.58722 Test MSE 280.21184938513625 Test RE 0.2817944393069538 Lambda1 2.7965357e-06\n",
      "74 Train Loss 269.5658 Test MSE 280.2827967232449 Test RE 0.281830111067585 Lambda1 2.7310218e-06\n",
      "75 Train Loss 269.53064 Test MSE 280.1945869519356 Test RE 0.28178575920899146 Lambda1 5.04904e-06\n",
      "76 Train Loss 269.50058 Test MSE 280.10468809056084 Test RE 0.2817405508924582 Lambda1 7.5868724e-07\n",
      "77 Train Loss 269.4953 Test MSE 280.0814989746026 Test RE 0.28172888837874993 Lambda1 9.265904e-07\n",
      "78 Train Loss 269.48022 Test MSE 280.07008400401685 Test RE 0.28172314726455017 Lambda1 -1.3169857e-07\n",
      "79 Train Loss 269.46652 Test MSE 280.0573535211598 Test RE 0.2817167443806688 Lambda1 1.0527987e-06\n",
      "80 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "81 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "82 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "83 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "84 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "85 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "86 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "87 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "88 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "89 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "90 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "91 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "92 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "93 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "94 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "95 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "96 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "97 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "98 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "99 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "100 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "101 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "102 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "103 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "104 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "105 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "106 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "107 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "109 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "110 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "111 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "112 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "113 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "114 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "115 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "116 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "117 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "118 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "119 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "120 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "121 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "122 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "123 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "124 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "125 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "126 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "127 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "128 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "129 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "130 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "131 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "132 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "133 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "134 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "135 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "136 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "137 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "138 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "139 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "140 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "141 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "142 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "143 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "144 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "145 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "146 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "147 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "148 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "149 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "150 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "151 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "152 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "153 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "154 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "155 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "156 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "157 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "158 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "159 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "160 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "161 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "162 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "163 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "164 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "165 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "166 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "167 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "168 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "169 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "170 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "171 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "172 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "173 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "174 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "175 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "176 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "177 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "178 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "179 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "180 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "181 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "182 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "183 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "184 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "185 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "186 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "187 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "188 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "190 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "191 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "192 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "193 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "194 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "195 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "196 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "197 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "198 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "199 Train Loss 269.46588 Test MSE 280.0537411735816 Test RE 0.28171492749911203 Lambda1 1.2364782e-06\n",
      "Training time: 447.25\n",
      "Training time: 447.25\n",
      "inv_HT_atanh\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 25089.242 Test MSE 3511.826068018725 Test RE 0.9975982296212554 Lambda1 0.000808949\n",
      "1 Train Loss 14682.768 Test MSE 3512.8636035083337 Test RE 0.9977455841756118 Lambda1 -2.1337486e-05\n",
      "2 Train Loss 9233.391 Test MSE 3514.2680787939807 Test RE 0.997945018189626 Lambda1 0.0001900797\n",
      "3 Train Loss 6243.0967 Test MSE 3501.4438288463175 Test RE 0.9961225059456622 Lambda1 0.00012253504\n",
      "4 Train Loss 4929.1562 Test MSE 3499.9454681192906 Test RE 0.9959093495129328 Lambda1 -3.1916643e-06\n",
      "5 Train Loss 4275.9453 Test MSE 3497.8857799333746 Test RE 0.9956162642896588 Lambda1 0.00029342182\n",
      "6 Train Loss 3992.5903 Test MSE 3493.3507956218377 Test RE 0.9949706502212392 Lambda1 -3.906371e-05\n",
      "7 Train Loss 3813.8228 Test MSE 3487.198585323541 Test RE 0.9940941327331324 Lambda1 0.00016824689\n",
      "8 Train Loss 3676.4924 Test MSE 3468.030365857313 Test RE 0.9913582300647054 Lambda1 0.00011091709\n",
      "9 Train Loss 3582.9158 Test MSE 3451.2463109827304 Test RE 0.9889564068698556 Lambda1 0.00017583233\n",
      "10 Train Loss 3505.5222 Test MSE 3415.7496355788307 Test RE 0.9838574668054973 Lambda1 -0.00032478265\n",
      "11 Train Loss 3454.6675 Test MSE 3381.682436166685 Test RE 0.9789388891681501 Lambda1 -0.00010596966\n",
      "12 Train Loss 3388.6807 Test MSE 3301.4723996733733 Test RE 0.9672595049604026 Lambda1 0.0002707681\n",
      "13 Train Loss 3302.6086 Test MSE 3222.532248396661 Test RE 0.9556256687622773 Lambda1 0.0002003659\n",
      "14 Train Loss 3205.8215 Test MSE 3134.5777229054606 Test RE 0.9424942124495391 Lambda1 0.00012620856\n",
      "15 Train Loss 3085.0237 Test MSE 3034.9228802886482 Test RE 0.9273912639854247 Lambda1 -1.8024311e-05\n",
      "16 Train Loss 2986.8286 Test MSE 2944.4383039849445 Test RE 0.9134618203969194 Lambda1 0.00022417745\n",
      "17 Train Loss 2899.9402 Test MSE 2860.411496201428 Test RE 0.9003335377007669 Lambda1 7.339617e-05\n",
      "18 Train Loss 2776.1663 Test MSE 2715.6856570442164 Test RE 0.8772611966607183 Lambda1 -4.5478107e-05\n",
      "19 Train Loss 2597.223 Test MSE 2526.81723751064 Test RE 0.8462059721955687 Lambda1 8.2875085e-05\n",
      "20 Train Loss 1952.97 Test MSE 1789.4920451706043 Test RE 0.7121214323660809 Lambda1 0.0009553455\n",
      "21 Train Loss 857.31964 Test MSE 859.0556543557599 Test RE 0.49340076782345493 Lambda1 -0.0024309056\n",
      "22 Train Loss 829.00446 Test MSE 830.4133032326499 Test RE 0.48510563520349254 Lambda1 0.00014253161\n",
      "23 Train Loss 763.7225 Test MSE 729.63269045453 Test RE 0.45471712729511454 Lambda1 -8.824036e-05\n",
      "24 Train Loss 675.3503 Test MSE 677.9120086507313 Test RE 0.43830440381797986 Lambda1 -0.00013164364\n",
      "25 Train Loss 656.9051 Test MSE 661.6544612760551 Test RE 0.43301684680317826 Lambda1 -3.7782873e-05\n",
      "26 Train Loss 623.60034 Test MSE 626.0687067745774 Test RE 0.42121145063865123 Lambda1 7.5912953e-07\n",
      "27 Train Loss 602.556 Test MSE 609.1958571649989 Test RE 0.4154967597215982 Lambda1 -6.595215e-07\n",
      "28 Train Loss 591.6195 Test MSE 597.4657720400762 Test RE 0.41147711457375696 Lambda1 -7.456596e-07\n",
      "29 Train Loss 555.59875 Test MSE 557.3110635162832 Test RE 0.39740927882464167 Lambda1 -2.5580435e-05\n",
      "30 Train Loss 515.8612 Test MSE 520.9891995932027 Test RE 0.38424084679936593 Lambda1 -0.00012324087\n",
      "31 Train Loss 511.1272 Test MSE 518.4830228701206 Test RE 0.3833155528202111 Lambda1 -0.00010212544\n",
      "32 Train Loss 507.32224 Test MSE 515.0804319004849 Test RE 0.38205571125184556 Lambda1 -6.301977e-05\n",
      "33 Train Loss 496.98325 Test MSE 504.36807322726406 Test RE 0.3780619451545592 Lambda1 -6.12066e-06\n",
      "34 Train Loss 490.04935 Test MSE 497.2410326918698 Test RE 0.37538131426189664 Lambda1 7.6476545e-07\n",
      "35 Train Loss 478.49164 Test MSE 483.57331671225717 Test RE 0.3701862938866061 Lambda1 -1.44697815e-05\n",
      "36 Train Loss 456.4199 Test MSE 458.89803032895895 Test RE 0.3606178895702106 Lambda1 -2.4767129e-05\n",
      "37 Train Loss 441.53033 Test MSE 445.4588828309517 Test RE 0.3552981796503735 Lambda1 8.231783e-06\n",
      "38 Train Loss 430.4732 Test MSE 438.0354393320432 Test RE 0.3523252712270913 Lambda1 -4.9842783e-06\n",
      "39 Train Loss 420.86877 Test MSE 427.62104698897673 Test RE 0.34811176954517364 Lambda1 -1.2806868e-05\n",
      "40 Train Loss 415.28122 Test MSE 420.5763054060798 Test RE 0.345232419084715 Lambda1 -2.5220932e-06\n",
      "41 Train Loss 404.8463 Test MSE 408.01149995684636 Test RE 0.34003637105351 Lambda1 9.740929e-07\n",
      "42 Train Loss 393.39413 Test MSE 387.48802906808487 Test RE 0.33137391265206223 Lambda1 1.0013555e-05\n",
      "43 Train Loss 387.57727 Test MSE 386.0298787888769 Test RE 0.3307498309278477 Lambda1 -1.1270791e-06\n",
      "44 Train Loss 380.73984 Test MSE 383.7169051796874 Test RE 0.32975746601845374 Lambda1 -3.7928828e-07\n",
      "45 Train Loss 376.76053 Test MSE 378.89018097210845 Test RE 0.32767691478926697 Lambda1 1.5796705e-06\n",
      "46 Train Loss 371.15326 Test MSE 373.9767340921996 Test RE 0.32554532493696897 Lambda1 8.078248e-07\n",
      "47 Train Loss 367.30243 Test MSE 371.28796234805947 Test RE 0.3243729310629909 Lambda1 2.1912642e-06\n",
      "48 Train Loss 366.79047 Test MSE 371.59074341822816 Test RE 0.32450516531002416 Lambda1 1.7492387e-06\n",
      "49 Train Loss 366.06558 Test MSE 371.4576952651178 Test RE 0.32444706553350383 Lambda1 1.1043006e-06\n",
      "50 Train Loss 364.66574 Test MSE 370.2437504505981 Test RE 0.3239164759743293 Lambda1 2.4280719e-06\n",
      "51 Train Loss 360.4429 Test MSE 364.9151218413345 Test RE 0.32157709003300994 Lambda1 1.0330842e-06\n",
      "52 Train Loss 360.1039 Test MSE 365.0899404413684 Test RE 0.32165410921080206 Lambda1 1.2346603e-06\n",
      "53 Train Loss 359.3215 Test MSE 364.5923599788837 Test RE 0.32143484355535784 Lambda1 4.1917474e-07\n",
      "54 Train Loss 355.78397 Test MSE 362.3916792665555 Test RE 0.3204632843537339 Lambda1 4.3772428e-07\n",
      "55 Train Loss 353.81454 Test MSE 362.98698461761495 Test RE 0.3207263908717372 Lambda1 1.2938196e-07\n",
      "56 Train Loss 352.53693 Test MSE 361.9974957781559 Test RE 0.32028894852863043 Lambda1 -9.4056695e-06\n",
      "57 Train Loss 350.6545 Test MSE 359.88750009242057 Test RE 0.31935414101103066 Lambda1 -1.0656499e-05\n",
      "58 Train Loss 347.6148 Test MSE 354.57097425363696 Test RE 0.31698649586801747 Lambda1 -8.829509e-07\n",
      "59 Train Loss 346.94745 Test MSE 354.9707361294967 Test RE 0.31716513911307803 Lambda1 -8.446799e-07\n",
      "60 Train Loss 344.85156 Test MSE 350.2176623182348 Test RE 0.3150345553242233 Lambda1 -8.324494e-06\n",
      "61 Train Loss 341.51065 Test MSE 350.5713779419728 Test RE 0.3151936057291779 Lambda1 1.0965679e-05\n",
      "62 Train Loss 338.39774 Test MSE 347.4391442678843 Test RE 0.313782373664617 Lambda1 2.9409625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 Train Loss 336.33008 Test MSE 343.74318526107237 Test RE 0.3121089474908746 Lambda1 -9.38311e-06\n",
      "64 Train Loss 332.72195 Test MSE 336.8543649298153 Test RE 0.30896569403384105 Lambda1 -1.6458716e-07\n",
      "65 Train Loss 328.08102 Test MSE 331.1420177723047 Test RE 0.30633478591937013 Lambda1 -2.4376533e-07\n",
      "66 Train Loss 325.49854 Test MSE 328.55311350779317 Test RE 0.30513495690378567 Lambda1 -9.628145e-06\n",
      "67 Train Loss 324.29144 Test MSE 327.73720153767795 Test RE 0.30475584312968224 Lambda1 -1.5802827e-05\n",
      "68 Train Loss 321.4289 Test MSE 325.1409209042053 Test RE 0.3035463299442743 Lambda1 -7.951024e-06\n",
      "69 Train Loss 318.23697 Test MSE 322.41949017566577 Test RE 0.30227331860888396 Lambda1 -3.146157e-07\n",
      "70 Train Loss 317.2065 Test MSE 320.77182200169244 Test RE 0.30149997197899897 Lambda1 8.888847e-06\n",
      "71 Train Loss 313.8755 Test MSE 317.7702405404111 Test RE 0.30008603272578627 Lambda1 -6.4244987e-06\n",
      "72 Train Loss 309.77502 Test MSE 312.5921094622529 Test RE 0.29763100872056436 Lambda1 -4.2321608e-07\n",
      "73 Train Loss 304.45068 Test MSE 309.19541821790034 Test RE 0.29600953146422004 Lambda1 -2.7574915e-06\n",
      "74 Train Loss 303.4962 Test MSE 309.19645092402396 Test RE 0.29601002579657837 Lambda1 7.074016e-08\n",
      "75 Train Loss 302.79117 Test MSE 309.1536499538173 Test RE 0.2959895372774888 Lambda1 2.4093964e-08\n",
      "76 Train Loss 302.36337 Test MSE 308.65900292811205 Test RE 0.2957526502870673 Lambda1 -1.4744403e-07\n",
      "77 Train Loss 300.57166 Test MSE 307.6710701931072 Test RE 0.2952789594230443 Lambda1 -6.083672e-07\n",
      "78 Train Loss 299.00507 Test MSE 305.2384964743937 Test RE 0.29410934466848393 Lambda1 9.870713e-07\n",
      "79 Train Loss 298.42575 Test MSE 304.1806218834946 Test RE 0.29359925035149503 Lambda1 3.6327283e-06\n",
      "80 Train Loss 297.76004 Test MSE 303.56525617989354 Test RE 0.29330212034985276 Lambda1 1.0673292e-06\n",
      "81 Train Loss 297.38516 Test MSE 303.1963159664604 Test RE 0.2931238327437922 Lambda1 5.497205e-07\n",
      "82 Train Loss 296.15396 Test MSE 302.15831881853427 Test RE 0.2926216456389589 Lambda1 -1.263757e-06\n",
      "83 Train Loss 294.0645 Test MSE 299.9649377561656 Test RE 0.2915576342374573 Lambda1 -2.5727081e-06\n",
      "84 Train Loss 293.47202 Test MSE 299.4994050845637 Test RE 0.2913313039404265 Lambda1 -2.9903754e-06\n",
      "85 Train Loss 293.30017 Test MSE 299.3958386882494 Test RE 0.2912809286448669 Lambda1 -3.5380178e-06\n",
      "86 Train Loss 293.14795 Test MSE 299.84619967907474 Test RE 0.29149992346018777 Lambda1 -3.221386e-06\n",
      "87 Train Loss 292.87933 Test MSE 299.6182356747506 Test RE 0.2913890930995701 Lambda1 -4.0731766e-06\n",
      "88 Train Loss 292.4566 Test MSE 299.3734039267024 Test RE 0.2912700150987239 Lambda1 -2.2396807e-06\n",
      "89 Train Loss 292.078 Test MSE 299.21940109917324 Test RE 0.2911950883090991 Lambda1 -1.3072987e-06\n",
      "90 Train Loss 291.5054 Test MSE 298.9044305106314 Test RE 0.2910417860211778 Lambda1 -9.89512e-07\n",
      "91 Train Loss 290.69934 Test MSE 298.5998820269619 Test RE 0.29089347954830813 Lambda1 2.4851786e-07\n",
      "92 Train Loss 290.24023 Test MSE 298.28059851688334 Test RE 0.29073791630493057 Lambda1 6.0093896e-07\n",
      "93 Train Loss 289.88818 Test MSE 298.0822713372393 Test RE 0.2906412442129662 Lambda1 2.439364e-07\n",
      "94 Train Loss 289.45157 Test MSE 298.00349449276246 Test RE 0.2906028365060768 Lambda1 1.2159002e-07\n",
      "95 Train Loss 288.48087 Test MSE 296.5984486819513 Test RE 0.2899169507272602 Lambda1 -2.3273155e-06\n",
      "96 Train Loss 287.76398 Test MSE 295.6041024241032 Test RE 0.2894305694683069 Lambda1 1.2498604e-06\n",
      "97 Train Loss 287.43927 Test MSE 294.9262014709317 Test RE 0.28909850730384257 Lambda1 1.018851e-06\n",
      "98 Train Loss 287.25357 Test MSE 295.08317745481634 Test RE 0.28917543415090935 Lambda1 3.1242968e-07\n",
      "99 Train Loss 287.18207 Test MSE 294.8273409530018 Test RE 0.2890500497176836 Lambda1 5.3478243e-07\n",
      "100 Train Loss 287.1234 Test MSE 295.1768347286905 Test RE 0.2892213216094742 Lambda1 2.683657e-07\n",
      "101 Train Loss 286.9596 Test MSE 294.93919284559024 Test RE 0.28910487456732 Lambda1 2.1112214e-07\n",
      "102 Train Loss 286.53735 Test MSE 294.26885666401245 Test RE 0.2887761496925669 Lambda1 -2.0707292e-07\n",
      "103 Train Loss 286.2032 Test MSE 293.91832426009785 Test RE 0.2886041036902692 Lambda1 -4.1219383e-07\n",
      "104 Train Loss 286.0017 Test MSE 293.8672895521827 Test RE 0.28857904661771344 Lambda1 -1.7105114e-07\n",
      "105 Train Loss 284.73724 Test MSE 292.4360746825205 Test RE 0.28787545907274575 Lambda1 -3.2305742e-08\n",
      "106 Train Loss 284.56192 Test MSE 291.29249059870125 Test RE 0.28731203290918655 Lambda1 7.218522e-07\n",
      "107 Train Loss 284.07928 Test MSE 291.07582515954306 Test RE 0.2872051606592825 Lambda1 9.513062e-07\n",
      "108 Train Loss 283.42548 Test MSE 290.9858914721659 Test RE 0.2871607883474378 Lambda1 1.1752362e-06\n",
      "109 Train Loss 283.18262 Test MSE 290.8693875992545 Test RE 0.2871032963962401 Lambda1 1.2004881e-07\n",
      "110 Train Loss 283.0002 Test MSE 290.73190503783917 Test RE 0.2870354371378478 Lambda1 -2.836473e-07\n",
      "111 Train Loss 282.57156 Test MSE 290.69835943408594 Test RE 0.2870188771135839 Lambda1 -2.5004272e-08\n",
      "112 Train Loss 282.3798 Test MSE 290.3713474342878 Test RE 0.28685739524651876 Lambda1 -7.1877784e-09\n",
      "113 Train Loss 281.88287 Test MSE 289.8047726577301 Test RE 0.2865773994341481 Lambda1 -1.1624826e-07\n",
      "114 Train Loss 280.59668 Test MSE 286.76513161058443 Test RE 0.28507054258138204 Lambda1 6.2472527e-06\n",
      "115 Train Loss 280.39264 Test MSE 287.0011327930983 Test RE 0.2851878217439617 Lambda1 1.5627757e-06\n",
      "116 Train Loss 280.13116 Test MSE 287.06750652648356 Test RE 0.28522079702524006 Lambda1 1.9040264e-06\n",
      "117 Train Loss 280.005 Test MSE 287.0780013054043 Test RE 0.28522601061051683 Lambda1 1.7356429e-06\n",
      "118 Train Loss 279.9777 Test MSE 287.1704471111074 Test RE 0.28527193162467807 Lambda1 1.6583518e-06\n",
      "119 Train Loss 279.44354 Test MSE 286.4194503033548 Test RE 0.28489867148860243 Lambda1 2.801055e-06\n",
      "120 Train Loss 279.17422 Test MSE 286.2187611032981 Test RE 0.284798842190166 Lambda1 4.7381504e-06\n",
      "121 Train Loss 279.0898 Test MSE 286.51032353047987 Test RE 0.28494386326670246 Lambda1 5.789706e-06\n",
      "122 Train Loss 278.9811 Test MSE 286.3858990772956 Test RE 0.28488198445903035 Lambda1 1.4297555e-05\n",
      "123 Train Loss 278.82916 Test MSE 286.5299952310353 Test RE 0.28495364517276517 Lambda1 1.2248003e-05\n",
      "124 Train Loss 278.38565 Test MSE 286.31029468095466 Test RE 0.28484437829157855 Lambda1 1.0348732e-05\n",
      "125 Train Loss 278.2868 Test MSE 286.09807974052836 Test RE 0.28473879452599954 Lambda1 4.930514e-06\n",
      "126 Train Loss 278.20212 Test MSE 285.8747418611525 Test RE 0.2846276344403471 Lambda1 2.0615882e-06\n",
      "127 Train Loss 278.15308 Test MSE 285.7325083791086 Test RE 0.28455681913143027 Lambda1 6.7532233e-06\n",
      "128 Train Loss 277.9822 Test MSE 285.7005417559824 Test RE 0.2845409011403761 Lambda1 4.927439e-06\n",
      "129 Train Loss 277.7102 Test MSE 285.27903582396885 Test RE 0.2843309261326574 Lambda1 -2.5374427e-06\n",
      "130 Train Loss 277.5608 Test MSE 285.13468369851716 Test RE 0.2842589808409446 Lambda1 1.1691418e-05\n",
      "131 Train Loss 277.486 Test MSE 284.94585821886903 Test RE 0.2841648424726506 Lambda1 -1.2368707e-07\n",
      "132 Train Loss 276.9726 Test MSE 283.84968090475405 Test RE 0.2836177293979826 Lambda1 -2.748545e-06\n",
      "133 Train Loss 276.79276 Test MSE 283.4631482088448 Test RE 0.283424555193957 Lambda1 -8.8547824e-07\n",
      "134 Train Loss 276.43997 Test MSE 282.6284753519177 Test RE 0.2830069679563414 Lambda1 3.1285103e-06\n",
      "135 Train Loss 276.0376 Test MSE 282.63819040119085 Test RE 0.28301183194431745 Lambda1 8.24241e-06\n",
      "136 Train Loss 275.77713 Test MSE 282.4559265350233 Test RE 0.2829205648206515 Lambda1 1.1091128e-05\n",
      "137 Train Loss 275.34866 Test MSE 282.0956832725688 Test RE 0.28274008932762695 Lambda1 1.3299025e-05\n",
      "138 Train Loss 274.59726 Test MSE 282.0408879960762 Test RE 0.28271262776988726 Lambda1 9.426828e-06\n",
      "139 Train Loss 274.01993 Test MSE 281.42195967895475 Test RE 0.28240225618899617 Lambda1 1.4526573e-05\n",
      "140 Train Loss 273.49036 Test MSE 281.1528245433679 Test RE 0.28226718757210656 Lambda1 2.2195934e-05\n",
      "141 Train Loss 273.19485 Test MSE 280.9696532955878 Test RE 0.28217522396306993 Lambda1 2.1971106e-05\n",
      "142 Train Loss 272.40182 Test MSE 281.484399002779 Test RE 0.2824335828629861 Lambda1 1.5028382e-05\n",
      "143 Train Loss 271.8477 Test MSE 281.152498325507 Test RE 0.28226702381663843 Lambda1 1.5729553e-05\n",
      "144 Train Loss 271.47647 Test MSE 280.853197117907 Test RE 0.28211673997129805 Lambda1 1.7049646e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 Train Loss 271.33426 Test MSE 280.85575018139025 Test RE 0.28211802224317056 Lambda1 1.6391476e-05\n",
      "146 Train Loss 271.1333 Test MSE 280.922784550434 Test RE 0.2821516880578971 Lambda1 2.1335152e-05\n",
      "147 Train Loss 270.92096 Test MSE 281.1366239143893 Test RE 0.28225905503446774 Lambda1 2.409331e-05\n",
      "148 Train Loss 270.7721 Test MSE 281.13624859015306 Test RE 0.2822588666230503 Lambda1 2.817762e-05\n",
      "149 Train Loss 270.63605 Test MSE 280.92147658764475 Test RE 0.2821510312148805 Lambda1 2.784816e-05\n",
      "150 Train Loss 270.5424 Test MSE 280.89086027441624 Test RE 0.2821356556376329 Lambda1 2.7122662e-05\n",
      "151 Train Loss 270.43765 Test MSE 280.89573060719 Test RE 0.2821381015850957 Lambda1 2.4231344e-05\n",
      "152 Train Loss 270.34988 Test MSE 280.81691449849274 Test RE 0.28209851645627154 Lambda1 1.7210543e-05\n",
      "153 Train Loss 270.26184 Test MSE 280.809190849779 Test RE 0.28209463698051124 Lambda1 1.5842648e-05\n",
      "154 Train Loss 270.18954 Test MSE 280.7764391028398 Test RE 0.2820781856643206 Lambda1 1.876823e-05\n",
      "155 Train Loss 270.14716 Test MSE 280.76194636277364 Test RE 0.2820709056044428 Lambda1 2.2823288e-05\n",
      "156 Train Loss 270.11432 Test MSE 280.6809313460927 Test RE 0.2820302063071112 Lambda1 2.1521417e-05\n",
      "157 Train Loss 270.03476 Test MSE 280.7134626886149 Test RE 0.28204654969632986 Lambda1 1.8876828e-05\n",
      "158 Train Loss 269.9688 Test MSE 280.8391648938361 Test RE 0.2821096922061335 Lambda1 1.6854925e-05\n",
      "159 Train Loss 269.90558 Test MSE 280.782930717653 Test RE 0.28208144650121697 Lambda1 2.0752705e-05\n",
      "160 Train Loss 269.86627 Test MSE 280.7320001425951 Test RE 0.2820558622860624 Lambda1 2.5313422e-05\n",
      "161 Train Loss 269.84543 Test MSE 280.710350885717 Test RE 0.2820449864016902 Lambda1 2.508013e-05\n",
      "162 Train Loss 269.75812 Test MSE 280.41862884067007 Test RE 0.2818983937882093 Lambda1 2.7070086e-05\n",
      "163 Train Loss 269.7134 Test MSE 280.36313513033764 Test RE 0.2818704991331716 Lambda1 2.7626471e-05\n",
      "164 Train Loss 269.7011 Test MSE 280.3605179160217 Test RE 0.281869183487256 Lambda1 2.7134993e-05\n",
      "165 Train Loss 269.67206 Test MSE 280.2731254157245 Test RE 0.2818252486764258 Lambda1 2.8971055e-05\n",
      "166 Train Loss 269.63934 Test MSE 280.2131442939707 Test RE 0.28179509041733375 Lambda1 2.591129e-05\n",
      "167 Train Loss 269.59268 Test MSE 280.180417981851 Test RE 0.28177863440242007 Lambda1 2.1378122e-05\n",
      "168 Train Loss 269.56274 Test MSE 280.27864207840236 Test RE 0.281828022269467 Lambda1 2.225628e-05\n",
      "169 Train Loss 269.54407 Test MSE 280.32737280947225 Test RE 0.28185252126189536 Lambda1 2.0554746e-05\n",
      "170 Train Loss 269.5289 Test MSE 280.35376057831553 Test RE 0.28186578661676776 Lambda1 1.9358065e-05\n",
      "171 Train Loss 269.5232 Test MSE 280.37910998856603 Test RE 0.2818785293920716 Lambda1 1.9809038e-05\n",
      "172 Train Loss 269.51883 Test MSE 280.3420573325015 Test RE 0.281859903373027 Lambda1 2.1193668e-05\n",
      "173 Train Loss 269.51236 Test MSE 280.3328287120186 Test RE 0.28185526404150857 Lambda1 2.1005959e-05\n",
      "174 Train Loss 269.49472 Test MSE 280.3942513648284 Test RE 0.28188614046419047 Lambda1 1.9551602e-05\n",
      "175 Train Loss 269.47687 Test MSE 280.40095785923126 Test RE 0.2818895115328021 Lambda1 1.8120347e-05\n",
      "176 Train Loss 269.4661 Test MSE 280.4179625550554 Test RE 0.28189805888721337 Lambda1 1.6828066e-05\n",
      "177 Train Loss 269.44876 Test MSE 280.362620152502 Test RE 0.28187024025975344 Lambda1 1.2731036e-05\n",
      "178 Train Loss 269.42914 Test MSE 280.2900580487886 Test RE 0.2818337617499566 Lambda1 1.2920715e-05\n",
      "179 Train Loss 269.41602 Test MSE 280.1569758473639 Test RE 0.2817668462288145 Lambda1 1.3477894e-05\n",
      "180 Train Loss 269.3986 Test MSE 280.1241090265477 Test RE 0.28175031790207256 Lambda1 1.351488e-05\n",
      "181 Train Loss 269.3855 Test MSE 280.09536474116624 Test RE 0.2817358619537064 Lambda1 1.2078596e-05\n",
      "182 Train Loss 269.37253 Test MSE 280.0806012763733 Test RE 0.28172843688922294 Lambda1 1.2660019e-05\n",
      "183 Train Loss 269.3623 Test MSE 280.03798187610596 Test RE 0.28170700099938717 Lambda1 1.1678893e-05\n",
      "184 Train Loss 269.34726 Test MSE 280.0390330438432 Test RE 0.2817075297152259 Lambda1 1.1651097e-05\n",
      "185 Train Loss 269.3298 Test MSE 280.06851008874435 Test RE 0.28172235566092596 Lambda1 1.1013034e-05\n",
      "186 Train Loss 269.3213 Test MSE 280.09113130989743 Test RE 0.28173373283258485 Lambda1 9.912364e-06\n",
      "187 Train Loss 269.3209 Test MSE 280.0862920764155 Test RE 0.2817312990154277 Lambda1 9.511636e-06\n",
      "188 Train Loss 269.3177 Test MSE 280.05298943963095 Test RE 0.2817145494023642 Lambda1 9.380534e-06\n",
      "189 Train Loss 269.31088 Test MSE 280.1114654106891 Test RE 0.2817439593223021 Lambda1 9.751482e-06\n",
      "190 Train Loss 269.30634 Test MSE 280.1210729819397 Test RE 0.2817487910630328 Lambda1 8.889654e-06\n",
      "191 Train Loss 269.28894 Test MSE 280.14102070902817 Test RE 0.28175882270377905 Lambda1 9.736743e-06\n",
      "192 Train Loss 269.2821 Test MSE 280.1471498908201 Test RE 0.2817619049756927 Lambda1 1.003373e-05\n",
      "193 Train Loss 269.2772 Test MSE 280.15957491781495 Test RE 0.2817681532285482 Lambda1 8.910867e-06\n",
      "194 Train Loss 269.27145 Test MSE 280.11998429013806 Test RE 0.28174824355353323 Lambda1 6.926984e-06\n",
      "195 Train Loss 269.26172 Test MSE 280.17669125331355 Test RE 0.2817767604028423 Lambda1 7.768269e-06\n",
      "196 Train Loss 269.24768 Test MSE 280.1552393265519 Test RE 0.2817659729777571 Lambda1 7.2630296e-06\n",
      "197 Train Loss 269.23734 Test MSE 280.1431207921972 Test RE 0.281759878807328 Lambda1 6.983018e-06\n",
      "198 Train Loss 269.23068 Test MSE 280.1859075498876 Test RE 0.28178139482982467 Lambda1 6.014949e-06\n",
      "199 Train Loss 269.22818 Test MSE 280.17687366344325 Test RE 0.28177685212875786 Lambda1 5.6538383e-06\n",
      "Training time: 619.24\n",
      "Training time: 619.24\n",
      "inv_HT_atanh\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 10448.691 Test MSE 3602.78976312184 Test RE 1.0104355805698753 Lambda1 0.0009813812\n",
      "1 Train Loss 5819.8047 Test MSE 3605.2873893496067 Test RE 1.0107857610326845 Lambda1 0.0009466496\n",
      "2 Train Loss 4313.618 Test MSE 3595.0597212239595 Test RE 1.0093510177733258 Lambda1 -0.0010600984\n",
      "3 Train Loss 3803.3276 Test MSE 3588.4018084167005 Test RE 1.0084159449443844 Lambda1 0.0005685287\n",
      "4 Train Loss 3630.762 Test MSE 3572.295814117969 Test RE 1.006150339284469 Lambda1 -0.0002490607\n",
      "5 Train Loss 3122.2263 Test MSE 3087.0419854155257 Test RE 0.9353204689007024 Lambda1 0.0022842335\n",
      "6 Train Loss 856.6142 Test MSE 859.0933082981749 Test RE 0.493411581023058 Lambda1 0.08703668\n",
      "7 Train Loss 854.6504 Test MSE 857.9669719176255 Test RE 0.4930880249639168 Lambda1 0.084031135\n",
      "8 Train Loss 854.01575 Test MSE 856.600663655275 Test RE 0.492695248377816 Lambda1 -0.026192244\n",
      "9 Train Loss 847.5726 Test MSE 845.5233169364235 Test RE 0.4894991758497253 Lambda1 -0.0105462475\n",
      "10 Train Loss 823.3521 Test MSE 820.4905431742245 Test RE 0.4821986171147914 Lambda1 -0.0036381355\n",
      "11 Train Loss 798.77026 Test MSE 802.0232692150101 Test RE 0.47674116672532774 Lambda1 0.0005791002\n",
      "12 Train Loss 775.47595 Test MSE 780.1178014118052 Test RE 0.47018553557625903 Lambda1 0.00021493286\n",
      "13 Train Loss 754.16797 Test MSE 762.3831931672148 Test RE 0.46481039010621006 Lambda1 -0.00015485188\n",
      "14 Train Loss 743.86414 Test MSE 753.0244609045636 Test RE 0.4619486609510685 Lambda1 -4.084225e-05\n",
      "15 Train Loss 727.6122 Test MSE 735.5022479419538 Test RE 0.45654245813779604 Lambda1 0.00064860913\n",
      "16 Train Loss 695.0355 Test MSE 703.870900597269 Test RE 0.44661743821082994 Lambda1 9.399493e-05\n",
      "17 Train Loss 681.62396 Test MSE 697.1534366255546 Test RE 0.4444811594293742 Lambda1 5.543763e-05\n",
      "18 Train Loss 675.7695 Test MSE 692.8273650790451 Test RE 0.44309993581277696 Lambda1 1.9478734e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 Train Loss 673.06805 Test MSE 691.1822152821468 Test RE 0.4425735427593025 Lambda1 1.7577826e-05\n",
      "20 Train Loss 669.96844 Test MSE 688.8955190898956 Test RE 0.44184083462017476 Lambda1 9.314085e-07\n",
      "21 Train Loss 667.2188 Test MSE 688.2408185882591 Test RE 0.44163083022227645 Lambda1 -1.6476038e-06\n",
      "22 Train Loss 666.7832 Test MSE 687.9161347026901 Test RE 0.44152664624662824 Lambda1 -6.1422543e-06\n",
      "23 Train Loss 665.03143 Test MSE 686.6792092716539 Test RE 0.4411295184363407 Lambda1 -6.486249e-07\n",
      "24 Train Loss 663.93555 Test MSE 686.1403801152057 Test RE 0.44095640987981627 Lambda1 -7.986057e-06\n",
      "25 Train Loss 663.585 Test MSE 686.2121735182092 Test RE 0.44097947872555476 Lambda1 -5.8821583e-06\n",
      "26 Train Loss 663.171 Test MSE 685.9562767717125 Test RE 0.4408972477913567 Lambda1 -7.1440004e-06\n",
      "27 Train Loss 662.7183 Test MSE 685.4137885309805 Test RE 0.44072287160422235 Lambda1 -1.111274e-06\n",
      "28 Train Loss 662.10046 Test MSE 684.4140231573986 Test RE 0.4404013284238286 Lambda1 8.8089e-06\n",
      "29 Train Loss 661.1527 Test MSE 682.9503149942601 Test RE 0.4399301486868361 Lambda1 8.353352e-06\n",
      "30 Train Loss 660.70496 Test MSE 682.7666957896413 Test RE 0.43987100452028144 Lambda1 6.7563324e-06\n",
      "31 Train Loss 660.0674 Test MSE 682.6013879253293 Test RE 0.4398177516752012 Lambda1 8.2736815e-06\n",
      "32 Train Loss 659.5047 Test MSE 681.6776684193571 Test RE 0.4395200627306926 Lambda1 2.116038e-05\n",
      "33 Train Loss 658.8348 Test MSE 681.6100967128187 Test RE 0.4394982783463019 Lambda1 4.8721686e-06\n",
      "34 Train Loss 657.9623 Test MSE 680.6487129498541 Test RE 0.43918822161075877 Lambda1 1.1609963e-05\n",
      "35 Train Loss 657.30237 Test MSE 678.0872903518897 Test RE 0.4383610643986198 Lambda1 -3.99015e-05\n",
      "36 Train Loss 655.62976 Test MSE 677.3607903345682 Test RE 0.4381261723184793 Lambda1 2.146192e-06\n",
      "37 Train Loss 654.2915 Test MSE 675.911992926734 Test RE 0.43765737057132686 Lambda1 6.738808e-05\n",
      "38 Train Loss 652.8418 Test MSE 674.2283804341303 Test RE 0.43711195575093587 Lambda1 -9.311374e-05\n",
      "39 Train Loss 650.69006 Test MSE 671.1617978363053 Test RE 0.4361167680612715 Lambda1 -0.0001602101\n",
      "40 Train Loss 647.0264 Test MSE 667.5109024874109 Test RE 0.4349289860123882 Lambda1 7.761648e-05\n",
      "41 Train Loss 646.18866 Test MSE 667.9757817464065 Test RE 0.43508040996155456 Lambda1 7.811958e-05\n",
      "42 Train Loss 644.67755 Test MSE 665.9115747280354 Test RE 0.4344076378580728 Lambda1 6.837404e-05\n",
      "43 Train Loss 641.10706 Test MSE 661.5357406480663 Test RE 0.4329779969644534 Lambda1 4.9731578e-05\n",
      "44 Train Loss 639.0928 Test MSE 659.6811738732416 Test RE 0.4323706600310625 Lambda1 3.193899e-05\n",
      "45 Train Loss 637.377 Test MSE 658.7876302891596 Test RE 0.4320777359951304 Lambda1 4.0683863e-05\n",
      "46 Train Loss 635.7986 Test MSE 658.2558082692112 Test RE 0.4319032982567419 Lambda1 1.9731946e-05\n",
      "47 Train Loss 635.2387 Test MSE 658.0173400004122 Test RE 0.431825057730554 Lambda1 2.8482511e-05\n",
      "48 Train Loss 634.661 Test MSE 657.6756951242443 Test RE 0.43171294064487437 Lambda1 2.8117045e-05\n",
      "49 Train Loss 633.2658 Test MSE 655.9952041010391 Test RE 0.43116103203544565 Lambda1 3.8835555e-05\n",
      "50 Train Loss 631.8365 Test MSE 653.9236310998643 Test RE 0.430479710122119 Lambda1 7.489623e-05\n",
      "51 Train Loss 631.3046 Test MSE 653.6233906282619 Test RE 0.43038087421528837 Lambda1 9.708123e-05\n",
      "52 Train Loss 631.05347 Test MSE 654.0546328999931 Test RE 0.4305228273839712 Lambda1 6.556262e-05\n",
      "53 Train Loss 630.8157 Test MSE 653.912954045696 Test RE 0.43047619574083174 Lambda1 4.5689736e-05\n",
      "54 Train Loss 630.55585 Test MSE 653.7714779083692 Test RE 0.43042962577807503 Lambda1 2.61592e-05\n",
      "55 Train Loss 630.30756 Test MSE 653.876041375178 Test RE 0.43046404561432716 Lambda1 1.946758e-05\n",
      "56 Train Loss 629.91223 Test MSE 653.8103494138066 Test RE 0.4304424216856742 Lambda1 1.51327295e-05\n",
      "57 Train Loss 629.85443 Test MSE 654.0229831341335 Test RE 0.430512410737683 Lambda1 1.4410697e-05\n",
      "58 Train Loss 629.78784 Test MSE 653.7177244605081 Test RE 0.43041193033491615 Lambda1 1.7901304e-05\n",
      "59 Train Loss 629.4621 Test MSE 653.4707447784854 Test RE 0.43033061615286267 Lambda1 1.475517e-05\n",
      "60 Train Loss 629.1524 Test MSE 652.9369084324293 Test RE 0.4301548064001192 Lambda1 2.1490545e-05\n",
      "61 Train Loss 629.07947 Test MSE 652.4968797468296 Test RE 0.4300098365320925 Lambda1 2.8128745e-05\n",
      "62 Train Loss 629.008 Test MSE 652.2756161830221 Test RE 0.42993692156555896 Lambda1 2.9991463e-05\n",
      "63 Train Loss 628.6929 Test MSE 652.1654835416774 Test RE 0.4299006239587891 Lambda1 2.33426e-05\n",
      "64 Train Loss 628.6228 Test MSE 652.0386653742671 Test RE 0.429858823325864 Lambda1 2.1411426e-05\n",
      "65 Train Loss 628.51965 Test MSE 651.9082203981817 Test RE 0.4298158230165004 Lambda1 2.096471e-05\n",
      "66 Train Loss 628.19336 Test MSE 651.0518910590852 Test RE 0.42953343292129204 Lambda1 3.5075474e-05\n",
      "67 Train Loss 627.72845 Test MSE 650.23107952963 Test RE 0.42926258109436344 Lambda1 9.757991e-05\n",
      "68 Train Loss 627.2955 Test MSE 649.6547713049932 Test RE 0.4290723084329917 Lambda1 0.00017031464\n",
      "69 Train Loss 626.7719 Test MSE 649.0945755563779 Test RE 0.4288872745254199 Lambda1 0.00018464768\n",
      "70 Train Loss 626.39215 Test MSE 649.0707389088666 Test RE 0.42887939945680253 Lambda1 0.00013595003\n",
      "71 Train Loss 626.022 Test MSE 649.0583120883075 Test RE 0.4288752938697464 Lambda1 4.690383e-05\n",
      "72 Train Loss 625.7689 Test MSE 648.9997376149639 Test RE 0.4288559414396237 Lambda1 3.0092799e-05\n",
      "73 Train Loss 625.6773 Test MSE 648.9193151042647 Test RE 0.42882936921002507 Lambda1 3.7772992e-05\n",
      "74 Train Loss 625.5703 Test MSE 648.9632811602222 Test RE 0.4288438961444708 Lambda1 3.714327e-05\n",
      "75 Train Loss 625.2929 Test MSE 648.3953825634485 Test RE 0.4286562174600049 Lambda1 3.4809607e-05\n",
      "76 Train Loss 625.19244 Test MSE 648.2959836111424 Test RE 0.4286233597213967 Lambda1 2.1524658e-05\n",
      "77 Train Loss 624.8454 Test MSE 647.8675049737162 Test RE 0.4284816911673896 Lambda1 1.7196203e-06\n",
      "78 Train Loss 624.7575 Test MSE 647.7174609217373 Test RE 0.4284320707963382 Lambda1 4.2425695e-06\n",
      "79 Train Loss 624.70416 Test MSE 647.6769946731791 Test RE 0.4284186874073569 Lambda1 4.38003e-06\n",
      "80 Train Loss 624.60504 Test MSE 647.6566991590464 Test RE 0.4284119749212614 Lambda1 5.562484e-06\n",
      "81 Train Loss 624.4734 Test MSE 647.6919821491946 Test RE 0.42842364425908214 Lambda1 9.28553e-06\n",
      "82 Train Loss 624.3913 Test MSE 647.4376702778651 Test RE 0.4283395271797199 Lambda1 6.3704338e-06\n",
      "83 Train Loss 624.35095 Test MSE 647.5136427016837 Test RE 0.42836465781313954 Lambda1 1.1614116e-05\n",
      "84 Train Loss 624.29785 Test MSE 647.4951910165064 Test RE 0.4283585543848906 Lambda1 1.4584035e-05\n",
      "85 Train Loss 624.2344 Test MSE 647.3502191478175 Test RE 0.42831059772255736 Lambda1 1.5499698e-05\n",
      "86 Train Loss 624.18933 Test MSE 647.2276911700247 Test RE 0.42827006130703316 Lambda1 2.4295754e-05\n",
      "87 Train Loss 624.1345 Test MSE 647.2150289920623 Test RE 0.4282658720098744 Lambda1 1.9615922e-05\n",
      "88 Train Loss 624.03406 Test MSE 647.0293380033968 Test RE 0.4282044312100373 Lambda1 2.163319e-05\n",
      "89 Train Loss 623.98444 Test MSE 647.0214529442696 Test RE 0.4282018220334922 Lambda1 1.6033056e-05\n",
      "90 Train Loss 623.9221 Test MSE 646.9270296312678 Test RE 0.428170575998514 Lambda1 1.1703781e-05\n",
      "91 Train Loss 623.7727 Test MSE 646.8109681277627 Test RE 0.4281321664503224 Lambda1 9.337326e-06\n",
      "92 Train Loss 623.5295 Test MSE 646.4086333248049 Test RE 0.42799899055030294 Lambda1 -3.3049055e-06\n",
      "93 Train Loss 623.34064 Test MSE 645.7991545759825 Test RE 0.4277971694474066 Lambda1 -7.83581e-06\n",
      "94 Train Loss 622.65643 Test MSE 643.2771433488688 Test RE 0.4269610235088673 Lambda1 6.575615e-06\n",
      "95 Train Loss 621.3664 Test MSE 639.6805292562949 Test RE 0.4257657638087293 Lambda1 -6.6332464e-06\n",
      "96 Train Loss 619.0438 Test MSE 632.7641074635243 Test RE 0.4234577532065137 Lambda1 4.8341262e-06\n",
      "97 Train Loss 601.4691 Test MSE 606.154283501666 Test RE 0.41445822231443763 Lambda1 -2.3671504e-05\n",
      "98 Train Loss 529.78705 Test MSE 522.0727128332516 Test RE 0.38464019654327863 Lambda1 0.00048343785\n",
      "99 Train Loss 487.02423 Test MSE 495.19418241251765 Test RE 0.37460790494913904 Lambda1 -0.00044065987\n",
      "100 Train Loss 469.35025 Test MSE 485.64512337575445 Test RE 0.3709784537101461 Lambda1 -0.0003618542\n",
      "101 Train Loss 464.1523 Test MSE 482.65729528699666 Test RE 0.3698355101443256 Lambda1 -0.00010620547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 Train Loss 451.52124 Test MSE 468.84174261943883 Test RE 0.3645040065697359 Lambda1 -8.588272e-05\n",
      "103 Train Loss 432.94986 Test MSE 450.0807980213333 Test RE 0.35713664383606836 Lambda1 -1.9410518e-05\n",
      "104 Train Loss 420.7191 Test MSE 438.31691494651324 Test RE 0.3524384527609876 Lambda1 -2.49974e-05\n",
      "105 Train Loss 411.4672 Test MSE 426.3425693206088 Test RE 0.347590997387705 Lambda1 -7.606688e-06\n",
      "106 Train Loss 400.86237 Test MSE 416.898444598217 Test RE 0.3437196081749811 Lambda1 1.4699021e-05\n",
      "107 Train Loss 393.59656 Test MSE 409.5644645548235 Test RE 0.3406828760214225 Lambda1 6.137664e-05\n",
      "108 Train Loss 388.359 Test MSE 406.5704654330199 Test RE 0.3394353615588699 Lambda1 2.244901e-05\n",
      "109 Train Loss 379.3897 Test MSE 395.9666617920958 Test RE 0.33497969419554857 Lambda1 7.4571755e-05\n",
      "110 Train Loss 373.9186 Test MSE 389.18890056603465 Test RE 0.33210039611729314 Lambda1 5.0919803e-05\n",
      "111 Train Loss 370.0709 Test MSE 385.85714852455675 Test RE 0.33067582513050525 Lambda1 5.1724808e-05\n",
      "112 Train Loss 366.05893 Test MSE 380.19108321112134 Test RE 0.32823896466652386 Lambda1 6.800853e-05\n",
      "113 Train Loss 362.89777 Test MSE 375.0757559500958 Test RE 0.3260233211915171 Lambda1 5.4322012e-05\n",
      "114 Train Loss 360.70633 Test MSE 373.4507120474952 Test RE 0.32531629427725695 Lambda1 4.1602296e-05\n",
      "115 Train Loss 357.15793 Test MSE 371.1459743137494 Test RE 0.32431090174388055 Lambda1 -5.2205023e-06\n",
      "116 Train Loss 354.0056 Test MSE 367.8356410340885 Test RE 0.3228613618737075 Lambda1 -1.7328908e-05\n",
      "117 Train Loss 351.1018 Test MSE 364.94724890272033 Test RE 0.32159124551590357 Lambda1 -1.33791455e-05\n",
      "118 Train Loss 349.61084 Test MSE 363.77256294056616 Test RE 0.32107326216091975 Lambda1 -5.5715336e-06\n",
      "119 Train Loss 346.99356 Test MSE 360.91171190996715 Test RE 0.3198082466692587 Lambda1 1.4195377e-05\n",
      "120 Train Loss 343.34732 Test MSE 357.30604621801336 Test RE 0.318206724594735 Lambda1 1.1352265e-06\n",
      "121 Train Loss 340.68588 Test MSE 354.31490460811744 Test RE 0.3168720120465971 Lambda1 1.6343125e-05\n",
      "122 Train Loss 338.28052 Test MSE 351.6230860892871 Test RE 0.31566603938869436 Lambda1 1.5889595e-05\n",
      "123 Train Loss 334.9025 Test MSE 349.58064156139653 Test RE 0.31474791231218563 Lambda1 1.8399098e-05\n",
      "124 Train Loss 333.63394 Test MSE 348.4927862815385 Test RE 0.3142578008323249 Lambda1 1.2285325e-05\n",
      "125 Train Loss 332.20538 Test MSE 347.3053033926989 Test RE 0.31372193005383725 Lambda1 1.4785276e-06\n",
      "126 Train Loss 330.6358 Test MSE 344.7735702009962 Test RE 0.31257637744540245 Lambda1 -1.4163308e-05\n",
      "127 Train Loss 329.83194 Test MSE 344.4191552530453 Test RE 0.3124156774459194 Lambda1 -9.976207e-06\n",
      "128 Train Loss 328.22488 Test MSE 342.1223128451775 Test RE 0.3113722251682012 Lambda1 -8.974429e-06\n",
      "129 Train Loss 327.33765 Test MSE 340.77301768586085 Test RE 0.31075760858283236 Lambda1 -2.3006607e-05\n",
      "130 Train Loss 324.6868 Test MSE 337.2452865756873 Test RE 0.30914492038301267 Lambda1 7.1476725e-06\n",
      "131 Train Loss 321.59534 Test MSE 335.1546531913182 Test RE 0.30818521319888326 Lambda1 -1.0482278e-05\n",
      "132 Train Loss 320.46164 Test MSE 334.0272196737238 Test RE 0.3076664213012865 Lambda1 -5.7772136e-06\n",
      "133 Train Loss 317.94955 Test MSE 330.30433469584796 Test RE 0.3059470761998585 Lambda1 -3.5061796e-06\n",
      "134 Train Loss 314.46738 Test MSE 325.5844808682067 Test RE 0.3037533095986798 Lambda1 -1.39952945e-05\n",
      "135 Train Loss 313.0958 Test MSE 323.8384416850112 Test RE 0.30293773302288524 Lambda1 -1.26992745e-05\n",
      "136 Train Loss 311.95166 Test MSE 323.6089296027472 Test RE 0.3028303643718292 Lambda1 -9.794909e-06\n",
      "137 Train Loss 311.49506 Test MSE 323.28400715459964 Test RE 0.3026782964192605 Lambda1 -8.074661e-06\n",
      "138 Train Loss 311.04868 Test MSE 322.9154737484811 Test RE 0.3025057254457199 Lambda1 -1.00686575e-05\n",
      "139 Train Loss 309.90692 Test MSE 321.55244359024147 Test RE 0.3018666101234245 Lambda1 -1.9467296e-05\n",
      "140 Train Loss 309.12082 Test MSE 321.3186641168449 Test RE 0.3017568565669602 Lambda1 -2.1451544e-05\n",
      "141 Train Loss 308.71582 Test MSE 321.21696016352337 Test RE 0.3017090966675834 Lambda1 -1.6318809e-05\n",
      "142 Train Loss 307.74545 Test MSE 320.62150305069457 Test RE 0.3014293197782608 Lambda1 -1.7147542e-05\n",
      "143 Train Loss 307.07578 Test MSE 319.85197575619145 Test RE 0.30106737050893756 Lambda1 -1.0890899e-05\n",
      "144 Train Loss 306.44678 Test MSE 319.09856749633724 Test RE 0.3007125808086099 Lambda1 -1.183756e-05\n",
      "145 Train Loss 306.3032 Test MSE 318.93145011866375 Test RE 0.30063382633500807 Lambda1 -1.0056604e-05\n",
      "146 Train Loss 305.57196 Test MSE 317.82542179646754 Test RE 0.3001120867788124 Lambda1 -6.88125e-06\n",
      "147 Train Loss 304.93588 Test MSE 317.39330646504084 Test RE 0.29990800124287154 Lambda1 -9.902437e-07\n",
      "148 Train Loss 304.3184 Test MSE 316.3855745663627 Test RE 0.29943151492532283 Lambda1 -1.249786e-06\n",
      "149 Train Loss 304.0508 Test MSE 315.86959796669765 Test RE 0.2991872517381124 Lambda1 -1.7758489e-06\n",
      "150 Train Loss 303.77252 Test MSE 315.62662223709464 Test RE 0.29907215799938364 Lambda1 -7.034031e-07\n",
      "151 Train Loss 303.1413 Test MSE 315.6009382163361 Test RE 0.2990599892986991 Lambda1 -3.5068592e-07\n",
      "152 Train Loss 302.6947 Test MSE 315.1848158838336 Test RE 0.29886276778744325 Lambda1 -4.2877724e-07\n",
      "153 Train Loss 302.4212 Test MSE 314.70175730643797 Test RE 0.298633658430005 Lambda1 1.3569343e-06\n",
      "154 Train Loss 302.2596 Test MSE 314.46223807094077 Test RE 0.29851999188834677 Lambda1 -5.936056e-07\n",
      "155 Train Loss 302.12408 Test MSE 314.1526670005971 Test RE 0.29837301731182037 Lambda1 -2.6574904e-07\n",
      "156 Train Loss 301.62338 Test MSE 313.82492380865654 Test RE 0.29821733624697616 Lambda1 -2.9984458e-06\n",
      "157 Train Loss 301.13474 Test MSE 313.858317874555 Test RE 0.2982332024572603 Lambda1 -1.4354991e-07\n",
      "158 Train Loss 300.8341 Test MSE 313.70033435382163 Test RE 0.29815813377308975 Lambda1 -6.41833e-08\n",
      "159 Train Loss 300.66962 Test MSE 313.63374229843737 Test RE 0.29812648570868916 Lambda1 1.897363e-06\n",
      "160 Train Loss 300.5838 Test MSE 313.5050537605569 Test RE 0.2980653165921999 Lambda1 3.1513239e-06\n",
      "161 Train Loss 300.51068 Test MSE 313.40545011607855 Test RE 0.2980179636868523 Lambda1 2.9013204e-06\n",
      "162 Train Loss 300.42215 Test MSE 313.5841772708572 Test RE 0.29810292760676055 Lambda1 1.7186574e-06\n",
      "163 Train Loss 300.3073 Test MSE 313.55947138201094 Test RE 0.298091184280095 Lambda1 1.7624876e-06\n",
      "164 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "165 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "166 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "167 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "168 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "169 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "170 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "171 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "172 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "173 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "174 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "175 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "176 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "177 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "178 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "179 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "180 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "181 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "182 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "184 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "185 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "186 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "187 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "188 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "189 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "190 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "191 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "192 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "193 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "194 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "195 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "196 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "197 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "198 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "199 Train Loss 300.30103 Test MSE 313.50103877825677 Test RE 0.29806340796140174 Lambda1 2.229092e-06\n",
      "Training time: 527.37\n",
      "Training time: 527.37\n",
      "inv_HT_atanh\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 19044.213 Test MSE 3384.13308716904 Test RE 0.9792935355810919 Lambda1 -0.00045813428\n",
      "1 Train Loss 9941.3 Test MSE 3346.042685207149 Test RE 0.9737666781221342 Lambda1 0.0010712852\n",
      "2 Train Loss 5307.4346 Test MSE 3348.3405845359616 Test RE 0.9741009885249973 Lambda1 -0.0009022883\n",
      "3 Train Loss 4152.268 Test MSE 3348.6122817928513 Test RE 0.9741405088771177 Lambda1 -0.00074781995\n",
      "4 Train Loss 3686.2212 Test MSE 3352.291594520885 Test RE 0.9746755340277573 Lambda1 0.0013103079\n",
      "5 Train Loss 3418.215 Test MSE 3331.0727060773374 Test RE 0.971585951629014 Lambda1 -0.00088310544\n",
      "6 Train Loss 3285.5732 Test MSE 3240.3500597700063 Test RE 0.9582639179844621 Lambda1 0.001335817\n",
      "7 Train Loss 854.78546 Test MSE 858.264411785374 Test RE 0.4931734894005267 Lambda1 0.024340952\n",
      "8 Train Loss 853.4016 Test MSE 855.7163121085031 Test RE 0.49244085427129686 Lambda1 0.022541523\n",
      "9 Train Loss 836.03156 Test MSE 833.7719606783675 Test RE 0.4860856649765719 Lambda1 0.018126918\n",
      "10 Train Loss 802.1427 Test MSE 805.8078207376475 Test RE 0.477864655362971 Lambda1 0.004331065\n",
      "11 Train Loss 737.37036 Test MSE 723.6682295345659 Test RE 0.4528547463895 Lambda1 0.0012922897\n",
      "12 Train Loss 686.5773 Test MSE 678.219692217648 Test RE 0.43840385902898804 Lambda1 -0.000646358\n",
      "13 Train Loss 652.73865 Test MSE 649.7922980151396 Test RE 0.42911772161473266 Lambda1 0.00057046674\n",
      "14 Train Loss 635.29865 Test MSE 632.490811433941 Test RE 0.4233662958845088 Lambda1 0.00037578546\n",
      "15 Train Loss 620.76074 Test MSE 620.2980252546024 Test RE 0.41926573433066117 Lambda1 0.00024879118\n",
      "16 Train Loss 611.474 Test MSE 609.6534704236261 Test RE 0.41565278600672706 Lambda1 0.00013549598\n",
      "17 Train Loss 599.3112 Test MSE 603.781588230832 Test RE 0.4136462613142947 Lambda1 -1.5146691e-05\n",
      "18 Train Loss 595.1385 Test MSE 598.9733169973786 Test RE 0.41199591369832694 Lambda1 -9.232569e-06\n",
      "19 Train Loss 589.3637 Test MSE 594.1124368074526 Test RE 0.4103207619276888 Lambda1 3.7961318e-06\n",
      "20 Train Loss 580.7403 Test MSE 582.7585865550196 Test RE 0.4063811090971613 Lambda1 -8.747654e-05\n",
      "21 Train Loss 574.5157 Test MSE 575.1322957824324 Test RE 0.40371329188239585 Lambda1 -6.1012193e-05\n",
      "22 Train Loss 556.2011 Test MSE 555.7195981454015 Test RE 0.3968414493751348 Lambda1 -2.5569334e-05\n",
      "23 Train Loss 551.8275 Test MSE 553.6882064735541 Test RE 0.3961154731236777 Lambda1 -2.991747e-05\n",
      "24 Train Loss 546.73486 Test MSE 551.2524184835157 Test RE 0.3952432161492151 Lambda1 -3.4147666e-05\n",
      "25 Train Loss 542.791 Test MSE 547.9402761516671 Test RE 0.3940540384934369 Lambda1 -1.7384957e-05\n",
      "26 Train Loss 535.9486 Test MSE 541.3060587881816 Test RE 0.39166125816484826 Lambda1 -3.5265832e-06\n",
      "27 Train Loss 524.5383 Test MSE 527.6574517658546 Test RE 0.38669201884979604 Lambda1 5.7617476e-06\n",
      "28 Train Loss 521.7855 Test MSE 523.3079738605792 Test RE 0.38509497067361725 Lambda1 1.3434625e-06\n",
      "29 Train Loss 520.69275 Test MSE 522.9860121369575 Test RE 0.38497648889716496 Lambda1 5.1521956e-06\n",
      "30 Train Loss 515.94763 Test MSE 516.0417156321643 Test RE 0.382412056322017 Lambda1 8.56206e-06\n",
      "31 Train Loss 512.0955 Test MSE 513.4434077782387 Test RE 0.3814481050360048 Lambda1 5.477985e-06\n",
      "32 Train Loss 508.72855 Test MSE 511.5114836608317 Test RE 0.380729794802744 Lambda1 3.9823403e-06\n",
      "33 Train Loss 506.7388 Test MSE 507.4543787220642 Test RE 0.3792168905026357 Lambda1 4.4513686e-06\n",
      "34 Train Loss 504.0537 Test MSE 500.971647859043 Test RE 0.37678685630640046 Lambda1 1.2082115e-06\n",
      "35 Train Loss 501.4173 Test MSE 501.48989843591863 Test RE 0.3769816972034065 Lambda1 -2.0825255e-06\n",
      "36 Train Loss 498.71088 Test MSE 500.6637541467135 Test RE 0.37667105321185074 Lambda1 5.2479495e-06\n",
      "37 Train Loss 496.26654 Test MSE 500.150170011727 Test RE 0.3764778078330425 Lambda1 7.6006484e-07\n",
      "38 Train Loss 493.0664 Test MSE 497.3866637367677 Test RE 0.37543628073420876 Lambda1 2.6036366e-06\n",
      "39 Train Loss 492.46844 Test MSE 496.6376215265981 Test RE 0.37515347904578744 Lambda1 1.6055411e-06\n",
      "40 Train Loss 487.12146 Test MSE 487.64752301042034 Test RE 0.37174247144337585 Lambda1 1.3115779e-06\n",
      "41 Train Loss 480.3661 Test MSE 483.63008403787455 Test RE 0.37020802158379745 Lambda1 -1.8337477e-06\n",
      "42 Train Loss 476.42307 Test MSE 482.58561480887 Test RE 0.36980804658913735 Lambda1 -6.8476135e-07\n",
      "43 Train Loss 475.27625 Test MSE 480.98719582355653 Test RE 0.3691950999289624 Lambda1 6.942014e-07\n",
      "44 Train Loss 474.58356 Test MSE 480.58286445882413 Test RE 0.36903988941115584 Lambda1 -5.1448615e-07\n",
      "45 Train Loss 471.95694 Test MSE 478.41636922913136 Test RE 0.3682071232743762 Lambda1 8.12167e-07\n",
      "46 Train Loss 470.34006 Test MSE 476.7372813799659 Test RE 0.3675604109468949 Lambda1 1.2857033e-06\n",
      "47 Train Loss 469.65457 Test MSE 475.8694855326942 Test RE 0.36722572694425487 Lambda1 9.3892965e-07\n",
      "48 Train Loss 467.50897 Test MSE 474.34631785251014 Test RE 0.366637546063035 Lambda1 -2.8397866e-07\n",
      "49 Train Loss 466.50177 Test MSE 472.8489571749199 Test RE 0.3660584095180828 Lambda1 -9.583135e-07\n",
      "50 Train Loss 463.81552 Test MSE 471.600669718666 Test RE 0.3655749061826904 Lambda1 5.4204577e-07\n",
      "51 Train Loss 460.66217 Test MSE 467.66858669311216 Test RE 0.3640476821492724 Lambda1 1.237382e-06\n",
      "52 Train Loss 458.7572 Test MSE 466.27544837191385 Test RE 0.3635050467701051 Lambda1 -3.268494e-07\n",
      "53 Train Loss 457.43744 Test MSE 466.55889993507645 Test RE 0.36361551840203915 Lambda1 1.2795476e-07\n",
      "54 Train Loss 456.8426 Test MSE 466.4094037666436 Test RE 0.36355725836030933 Lambda1 -2.7943898e-07\n",
      "55 Train Loss 456.69653 Test MSE 466.11085992745564 Test RE 0.36344088511307476 Lambda1 -1.9978384e-07\n",
      "56 Train Loss 456.05362 Test MSE 465.31652602154065 Test RE 0.363131069848591 Lambda1 -3.9264404e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 Train Loss 454.03018 Test MSE 463.14071497862284 Test RE 0.3622810781313766 Lambda1 3.597703e-07\n",
      "58 Train Loss 453.28424 Test MSE 462.37043551070633 Test RE 0.3619796861425002 Lambda1 1.0097416e-06\n",
      "59 Train Loss 451.04398 Test MSE 461.3473706939118 Test RE 0.36157899685541545 Lambda1 1.1620562e-06\n",
      "60 Train Loss 449.89795 Test MSE 459.1582055010071 Test RE 0.36072010239367813 Lambda1 1.0561316e-06\n",
      "61 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "62 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "63 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "64 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "65 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "66 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "67 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "68 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "69 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "70 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "71 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "72 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "73 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "74 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "75 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "76 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "77 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "78 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "79 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "80 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "81 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "82 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "83 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "84 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "85 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "86 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "87 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "88 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "89 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "90 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "91 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "92 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "93 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "94 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "95 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "96 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "97 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "98 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "99 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "100 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "101 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "102 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "103 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "104 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "105 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "106 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "107 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "108 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "109 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "110 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "111 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "112 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "113 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "114 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "115 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "116 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "117 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "118 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "119 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "120 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "121 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "122 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "123 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "124 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "125 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "126 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "127 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "128 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "129 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "130 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "131 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "132 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "133 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "134 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "135 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "136 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "138 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "139 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "140 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "141 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "142 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "143 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "144 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "145 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "146 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "147 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "148 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "149 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "150 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "151 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "152 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "153 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "154 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "155 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "156 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "157 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "158 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "159 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "160 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "161 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "162 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "163 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "164 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "165 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "166 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "167 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "168 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "169 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "170 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "171 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "172 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "173 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "174 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "175 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "176 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "177 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "178 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "179 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "180 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "181 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "182 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "183 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "184 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "185 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "186 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "187 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "188 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "189 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "190 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "191 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "192 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "193 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "194 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "195 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "196 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "197 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "198 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "199 Train Loss 446.93036 Test MSE 454.70007939630665 Test RE 0.35896465270522776 Lambda1 -2.8106235e-06\n",
      "Training time: 273.76\n",
      "Training time: 273.76\n",
      "inv_HT_atanh\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 19021.38 Test MSE 3669.717516114691 Test RE 1.019777646603222 Lambda1 0.0008492633\n",
      "1 Train Loss 8552.929 Test MSE 3679.0731954827493 Test RE 1.0210767438152337 Lambda1 0.0005004265\n",
      "2 Train Loss 5240.8086 Test MSE 3661.520650353954 Test RE 1.0186380965672142 Lambda1 -0.00034086424\n",
      "3 Train Loss 4168.03 Test MSE 3659.021092837073 Test RE 1.0182903477167016 Lambda1 0.00010760907\n",
      "4 Train Loss 3897.5103 Test MSE 3638.3287361645466 Test RE 1.01540696705256 Lambda1 -0.0006420538\n",
      "5 Train Loss 3752.0637 Test MSE 3596.8381844586975 Test RE 1.009600648080403 Lambda1 0.00018990935\n",
      "6 Train Loss 3604.9082 Test MSE 3513.3526715654784 Test RE 0.9978150358490533 Lambda1 1.9430374e-05\n",
      "7 Train Loss 3400.2676 Test MSE 3334.605040759591 Test RE 0.9721009595119664 Lambda1 0.0003548948\n",
      "8 Train Loss 2999.096 Test MSE 2979.411961260071 Test RE 0.9188707966980182 Lambda1 3.7486065e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 855.862 Test MSE 858.8127624789797 Test RE 0.49333101011318115 Lambda1 -0.12787277\n",
      "10 Train Loss 854.6865 Test MSE 858.026230129249 Test RE 0.4931050530145129 Lambda1 -0.12931909\n",
      "11 Train Loss 854.5149 Test MSE 857.7211326661173 Test RE 0.4930173759297886 Lambda1 -0.10515917\n",
      "12 Train Loss 854.4164 Test MSE 857.5117400847188 Test RE 0.49295719291784507 Lambda1 -0.08866099\n",
      "13 Train Loss 853.8497 Test MSE 856.5657043897595 Test RE 0.4926851944282462 Lambda1 -0.065564156\n",
      "14 Train Loss 853.14453 Test MSE 854.7881929581521 Test RE 0.492173728486323 Lambda1 -0.054955136\n",
      "15 Train Loss 851.04565 Test MSE 851.7262002365724 Test RE 0.4912914137682947 Lambda1 -0.058792222\n",
      "16 Train Loss 850.2268 Test MSE 850.489878930948 Test RE 0.4909347177916804 Lambda1 -0.060927033\n",
      "17 Train Loss 848.6353 Test MSE 847.9781259384114 Test RE 0.4902092426520059 Lambda1 -0.062106717\n",
      "18 Train Loss 847.26105 Test MSE 845.8439232118347 Test RE 0.48959197141812183 Lambda1 -0.06185923\n",
      "19 Train Loss 843.8424 Test MSE 841.4485801878773 Test RE 0.4883182568182731 Lambda1 -0.063174985\n",
      "20 Train Loss 841.2998 Test MSE 838.5150100115109 Test RE 0.48746629352148385 Lambda1 -0.058418266\n",
      "21 Train Loss 838.1687 Test MSE 835.0169716033574 Test RE 0.4864484477528524 Lambda1 -0.063242376\n",
      "22 Train Loss 833.156 Test MSE 829.4796557610346 Test RE 0.48483285234624784 Lambda1 -0.068966314\n",
      "23 Train Loss 825.7446 Test MSE 819.3217202633836 Test RE 0.48185503871607493 Lambda1 -0.06285414\n",
      "24 Train Loss 805.2595 Test MSE 794.6936948616426 Test RE 0.47455773255908756 Lambda1 -0.053441774\n",
      "25 Train Loss 777.1603 Test MSE 763.564200920885 Test RE 0.4651702696599465 Lambda1 -0.048458412\n",
      "26 Train Loss 723.97906 Test MSE 711.3541554537153 Test RE 0.44898528431458595 Lambda1 0.0056895902\n",
      "27 Train Loss 670.2547 Test MSE 677.8382569787625 Test RE 0.4382805610762632 Lambda1 0.0003768858\n",
      "28 Train Loss 650.5603 Test MSE 666.4393815846083 Test RE 0.43457976124768904 Lambda1 -0.00033359043\n",
      "29 Train Loss 646.0963 Test MSE 666.3056093801822 Test RE 0.43453614316939326 Lambda1 -5.294488e-05\n",
      "30 Train Loss 643.98096 Test MSE 665.1875122697816 Test RE 0.4341714024313624 Lambda1 5.8526017e-05\n",
      "31 Train Loss 640.6799 Test MSE 659.6276592643707 Test RE 0.43235312230454626 Lambda1 0.00030220678\n",
      "32 Train Loss 638.3815 Test MSE 658.3783932983832 Test RE 0.43194351242047474 Lambda1 6.9639296e-05\n",
      "33 Train Loss 635.4996 Test MSE 656.573877417055 Test RE 0.4313511602434351 Lambda1 -8.4553714e-05\n",
      "34 Train Loss 634.0042 Test MSE 654.2038358805934 Test RE 0.4305719300299196 Lambda1 5.9211825e-06\n",
      "35 Train Loss 632.3403 Test MSE 652.0801208778763 Test RE 0.4298724879532211 Lambda1 5.949854e-07\n",
      "36 Train Loss 626.0341 Test MSE 639.8245729252009 Test RE 0.42581369821331055 Lambda1 1.2871406e-05\n",
      "37 Train Loss 602.9852 Test MSE 600.5539254618479 Test RE 0.41253915591487367 Lambda1 -4.07052e-05\n",
      "38 Train Loss 582.6396 Test MSE 563.4109746005829 Test RE 0.39957823276525467 Lambda1 -0.00011400473\n",
      "39 Train Loss 553.2622 Test MSE 518.6781295982973 Test RE 0.38338766743699676 Lambda1 5.6187262e-05\n",
      "40 Train Loss 466.51303 Test MSE 446.985721436437 Test RE 0.3559065623140424 Lambda1 -0.001011315\n",
      "41 Train Loss 383.5089 Test MSE 368.6538134186423 Test RE 0.32322023074626566 Lambda1 5.0651204e-05\n",
      "42 Train Loss 336.88525 Test MSE 329.8337757239889 Test RE 0.3057290689904498 Lambda1 -0.0005135965\n",
      "43 Train Loss 319.87906 Test MSE 322.8007206296173 Test RE 0.3024519705735706 Lambda1 -0.0017488628\n",
      "44 Train Loss 312.31375 Test MSE 313.78733646097015 Test RE 0.29819947671207386 Lambda1 -0.0022413034\n",
      "45 Train Loss 300.17603 Test MSE 302.658344595795 Test RE 0.29286366757432225 Lambda1 -0.0004895808\n",
      "46 Train Loss 290.21945 Test MSE 293.8435666127697 Test RE 0.28856739836414746 Lambda1 -0.0005794335\n",
      "47 Train Loss 287.0941 Test MSE 292.4975245994807 Test RE 0.2879057032788871 Lambda1 -0.0004721569\n",
      "48 Train Loss 284.14688 Test MSE 291.158016448383 Test RE 0.2872457069604814 Lambda1 2.0449097e-05\n",
      "49 Train Loss 282.71487 Test MSE 290.26839771043154 Test RE 0.2868065388036428 Lambda1 0.0004534235\n",
      "50 Train Loss 280.85083 Test MSE 288.49636901412885 Test RE 0.28592975122240055 Lambda1 0.00055456115\n",
      "51 Train Loss 279.88678 Test MSE 287.8786409435274 Test RE 0.2856234709669458 Lambda1 0.00036111835\n",
      "52 Train Loss 279.0626 Test MSE 287.10212840740263 Test RE 0.2852379960856053 Lambda1 0.0005870204\n",
      "53 Train Loss 278.01477 Test MSE 285.7003646446084 Test RE 0.2845408129441174 Lambda1 0.00027211063\n",
      "54 Train Loss 277.30832 Test MSE 285.2422312682183 Test RE 0.2843125844221768 Lambda1 0.00033293077\n",
      "55 Train Loss 276.8653 Test MSE 285.0928537287738 Test RE 0.28423812932519976 Lambda1 0.00043004088\n",
      "56 Train Loss 276.38647 Test MSE 284.71439241498206 Test RE 0.2840494033381237 Lambda1 0.0002759889\n",
      "57 Train Loss 276.17444 Test MSE 284.6007477872565 Test RE 0.283992708082395 Lambda1 0.00030602983\n",
      "58 Train Loss 275.916 Test MSE 284.59362867356657 Test RE 0.283989156108208 Lambda1 0.00029219108\n",
      "59 Train Loss 275.5989 Test MSE 284.6543855694529 Test RE 0.28401946841505626 Lambda1 0.00026714223\n",
      "60 Train Loss 275.1873 Test MSE 284.3261512781409 Test RE 0.2838556700976446 Lambda1 0.00021529497\n",
      "61 Train Loss 274.9113 Test MSE 284.05148797054187 Test RE 0.2837185325625389 Lambda1 0.00016427255\n",
      "62 Train Loss 274.68204 Test MSE 283.9668570530161 Test RE 0.2836762635558405 Lambda1 8.069968e-05\n",
      "63 Train Loss 274.46136 Test MSE 283.743997907839 Test RE 0.28356492616426526 Lambda1 1.2189908e-05\n",
      "64 Train Loss 274.31662 Test MSE 283.70953408615674 Test RE 0.2835477046055854 Lambda1 1.3638364e-05\n",
      "65 Train Loss 274.043 Test MSE 283.0928146643425 Test RE 0.2832393531222498 Lambda1 -1.648001e-05\n",
      "66 Train Loss 273.91885 Test MSE 282.87388260264873 Test RE 0.2831298092448148 Lambda1 -2.6080877e-05\n",
      "67 Train Loss 273.63507 Test MSE 282.37316948219024 Test RE 0.28287911519049525 Lambda1 2.4628325e-05\n",
      "68 Train Loss 273.37222 Test MSE 282.2294401998409 Test RE 0.28280711262135966 Lambda1 2.6974069e-05\n",
      "69 Train Loss 273.0886 Test MSE 281.9011535273397 Test RE 0.282642585455462 Lambda1 3.887541e-05\n",
      "70 Train Loss 272.97696 Test MSE 281.83413852603485 Test RE 0.2826089878294228 Lambda1 4.3516688e-05\n",
      "71 Train Loss 272.8412 Test MSE 281.64674588962083 Test RE 0.2825150182882509 Lambda1 2.2642178e-05\n",
      "72 Train Loss 272.66617 Test MSE 281.6842595620577 Test RE 0.2825338323223721 Lambda1 3.6335212e-05\n",
      "73 Train Loss 272.45166 Test MSE 281.46076134089446 Test RE 0.28242172392920173 Lambda1 7.766401e-05\n",
      "74 Train Loss 272.24088 Test MSE 281.45464773762757 Test RE 0.28241865667438826 Lambda1 4.6788322e-05\n",
      "75 Train Loss 271.86127 Test MSE 281.25868103098384 Test RE 0.2823203205988173 Lambda1 3.3209057e-05\n",
      "76 Train Loss 271.44254 Test MSE 280.959685946974 Test RE 0.28217021886067106 Lambda1 4.802998e-05\n",
      "77 Train Loss 271.2015 Test MSE 280.8104145345659 Test RE 0.2820952516230353 Lambda1 2.4034158e-05\n",
      "78 Train Loss 270.90195 Test MSE 280.3890961229352 Test RE 0.2818835491166274 Lambda1 4.3947224e-05\n",
      "79 Train Loss 270.7728 Test MSE 280.1102160720898 Test RE 0.28174333101171956 Lambda1 3.6399946e-05\n",
      "80 Train Loss 270.53 Test MSE 279.88472757488216 Test RE 0.2816299066037786 Lambda1 6.879241e-06\n",
      "81 Train Loss 270.43457 Test MSE 279.776239047043 Test RE 0.281575318817605 Lambda1 2.1010103e-05\n",
      "82 Train Loss 270.37784 Test MSE 279.7908486525184 Test RE 0.2815826704973469 Lambda1 2.0504573e-05\n",
      "83 Train Loss 270.3236 Test MSE 279.6315827234114 Test RE 0.2815025161433166 Lambda1 1.2620203e-05\n",
      "84 Train Loss 270.23965 Test MSE 279.7131498769677 Test RE 0.2815435695975661 Lambda1 1.1488251e-05\n",
      "85 Train Loss 270.2111 Test MSE 279.7375054453781 Test RE 0.2815558268053708 Lambda1 3.3119952e-06\n",
      "86 Train Loss 270.20987 Test MSE 279.7466361394558 Test RE 0.28156042179012036 Lambda1 3.9797596e-06\n",
      "87 Train Loss 270.18936 Test MSE 279.68221995125145 Test RE 0.2815280030029301 Lambda1 5.353213e-06\n",
      "88 Train Loss 270.1558 Test MSE 279.69296344379626 Test RE 0.2815334101485048 Lambda1 1.2139677e-05\n",
      "89 Train Loss 270.11914 Test MSE 279.7919597840097 Test RE 0.28158322962046234 Lambda1 2.504039e-06\n",
      "90 Train Loss 270.10153 Test MSE 279.7125806361931 Test RE 0.28154328311450083 Lambda1 4.2498355e-06\n",
      "91 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "93 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "94 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "95 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "96 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "97 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "98 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "99 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "100 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "101 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "102 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "103 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "104 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "105 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "106 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "107 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "108 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "109 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "110 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "111 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "112 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "113 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "114 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "115 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "116 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "117 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "118 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "119 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "120 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "121 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "122 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "123 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "124 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "125 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "126 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "127 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "128 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "129 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "130 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "131 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "132 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "133 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "134 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "135 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "136 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "137 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "138 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "139 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "140 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "141 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "142 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "143 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "144 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "145 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "146 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "147 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "148 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "149 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "150 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "151 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "152 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "153 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "154 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "155 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "156 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "157 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "158 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "159 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "160 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "161 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "162 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "163 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "164 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "165 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "166 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "167 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "168 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "169 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "170 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "171 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "172 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "173 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "174 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "176 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "177 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "178 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "179 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "180 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "181 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "182 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "183 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "184 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "185 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "186 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "187 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "188 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "189 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "190 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "191 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "192 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "193 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "194 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "195 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "196 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "197 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "198 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "199 Train Loss 270.08582 Test MSE 279.7043397237661 Test RE 0.2815391356595941 Lambda1 7.978391e-06\n",
      "Training time: 339.41\n",
      "Training time: 339.41\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_reps = 10 #10\n",
    "max_iter = 200#75\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "\n",
    "lambda1_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 3.0\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    'Generate Training data'\n",
    "    print(reps)\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    alpha_val = []\n",
    "\n",
    "    lambda1_val = []\n",
    "\n",
    "    N_f = 50000 #Total number of collocation points \n",
    "    N_train = 5000\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,n_val)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    alpha_full.append(alpha_val)\n",
    "\n",
    "    lambda1_full.append(lambda1_val)\n",
    "\n",
    "    if(nan_flag == 1):\n",
    "        nan_tune.append(tune_reps)\n",
    "        break\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"alpha\": alpha_full,\"lambda1\": lambda1_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'inv_HT_atanh_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'inv_HT_atanh_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3848/653522486.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"inv_HT_atanh_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'inv_HT_atanh_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(25):\n",
    "    label = \"inv_HT_atanh_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_tune[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
