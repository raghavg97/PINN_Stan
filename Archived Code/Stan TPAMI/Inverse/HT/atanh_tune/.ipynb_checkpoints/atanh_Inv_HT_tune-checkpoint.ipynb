{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1]).reshape(-1,1)\n",
    "n_value = np.array([1.0,3.0,5.0,8.0,10.0]).reshape(-1,1)\n",
    "\n",
    "LR_tune,N_value = np.meshgrid(lr_tune,n_value)\n",
    "\n",
    "LR_tune = LR_tune.flatten('F').reshape(-1,1)\n",
    "N_value = N_value.flatten('F').reshape(-1,1)\n",
    "\n",
    "lrn_tune = np.hstack((LR_tune,N_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_2D_4(xt): #True function for 2D_4 Heat Transfer in a rod x \\in [0,1] t \\in [0,0.1]\n",
    "    term1 = 4*u0/np.pi\n",
    "    \n",
    "    resol_n = 10000\n",
    "    \n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "\n",
    "    u = np.zeros((np.shape(xt)[0],1))\n",
    "    \n",
    "    for i in range(resol_n):\n",
    "        j = 2*i-1\n",
    "        term2 = np.sin(j*np.pi*x)/j\n",
    "        term3 = np.exp(-1*np.square(j*np.pi)*t)\n",
    "        \n",
    "        u = u + term2*term3\n",
    "        \n",
    "    u = term1*u\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = 50.0\n",
    "loss_thresh = 0.1\n",
    "\n",
    "x_ll = np.array(0.0)\n",
    "x_ul = np.array(1.0)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,100).reshape(-1,1)\n",
    "t = np.linspace(0,0.1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = true_2D_4(xt)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_f,N_train,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #X_Train\n",
    "    np.random.seed(seed)\n",
    "    x_train = np.random.uniform(x_ll,x_ul,(N_train,1))\n",
    "    t_train = np.random.uniform(0,0.1,(N_train,1))\n",
    "    \n",
    "    xt_train = np.hstack((x_train,t_train))\n",
    "    u_train = true_2D_4(xt_train)\n",
    "    \n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "        \n",
    "    \n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requiresGrad = True\n",
    "        \n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    \n",
    "    def loss_PDE(self, xt_coll,f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_tt[:,[0]]\n",
    "                \n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        f = du_dt - self.lambda1*d2u_dx2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_train = self.loss_function(self.forward(xt_train),u_train)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "        \n",
    "        #print(self.iter,\"train_loss\",loss_train.cpu().detach().numpy(),\"F Loss\",(loss_f).cpu().detach().numpy())\n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                    \n",
    "    \n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_coll,f_hat, xt_train, u_train,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xt_coll, xt_train, u_train = trainingdata(N_f,N_train,123)\n",
    "    \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_train = torch.from_numpy(xt_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    nan_flag = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_coll,f_hat, xt_train, u_train,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_coll,f_hat, xt_train, u_train).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1],\"Lambda1\",lambda1_val[-1])\n",
    "\n",
    "        if(np.isnan(loss_np)):\n",
    "            nan_flag =1\n",
    "            print(\"NAN BREAK!\")\n",
    "            break\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_HT_atanh_tune0\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1019.8298 Test MSE 1027.0576375692217 Test RE 0.5394939501351783 Lambda1 -0.07625545\n",
      "1 Train Loss 838.1357 Test MSE 858.0963389696396 Test RE 0.493125198271195 Lambda1 -0.09567029\n",
      "2 Train Loss 838.0638 Test MSE 858.2607571280938 Test RE 0.4931724393849503 Lambda1 -0.09602721\n",
      "3 Train Loss 838.06366 Test MSE 858.2645049967282 Test RE 0.49317351618094946 Lambda1 -0.096033126\n",
      "4 Train Loss 838.06354 Test MSE 858.267382109024 Test RE 0.49317434279931116 Lambda1 -0.096037656\n",
      "5 Train Loss 838.0635 Test MSE 858.2698004913032 Test RE 0.493175037619641 Lambda1 -0.0960415\n",
      "6 Train Loss 838.0634 Test MSE 858.2719158704572 Test RE 0.49317564538397707 Lambda1 -0.0960448\n",
      "7 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "8 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "9 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "10 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "11 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "12 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "13 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "14 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "15 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "16 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "17 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "18 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "19 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "20 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "21 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "22 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "23 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "24 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "25 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "26 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "27 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "28 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "29 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "30 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "31 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "32 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "33 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "34 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "35 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "36 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "37 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "38 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "39 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "40 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "41 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "42 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "43 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "44 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "45 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "46 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "47 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "48 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "49 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "50 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "51 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "52 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "53 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "54 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "55 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "56 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "57 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "58 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "59 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "60 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "61 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "62 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "63 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "64 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "65 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "66 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "67 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "68 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "69 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "70 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "71 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "72 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "73 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "74 Train Loss 838.0633 Test MSE 858.2737297386086 Test RE 0.49317616652133883 Lambda1 -0.09604768\n",
      "Training time: 51.92\n",
      "Training time: 51.92\n",
      "inv_HT_atanh_tune0\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 845.22107 Test MSE 862.862424861552 Test RE 0.4944927734874957 Lambda1 -0.05587272\n",
      "1 Train Loss 838.07074 Test MSE 858.2029185294875 Test RE 0.49315582154292914 Lambda1 -0.05805434\n",
      "2 Train Loss 838.06305 Test MSE 858.2586413675491 Test RE 0.4931718315070859 Lambda1 -0.05811001\n",
      "3 Train Loss 838.06274 Test MSE 858.2653777015986 Test RE 0.4931737669164847 Lambda1 -0.058116116\n",
      "4 Train Loss 838.0627 Test MSE 858.2681441190239 Test RE 0.49317456173090846 Lambda1 -0.058118556\n",
      "5 Train Loss 838.06256 Test MSE 858.2705123014642 Test RE 0.49317524212813585 Lambda1 -0.058120593\n",
      "6 Train Loss 838.06244 Test MSE 858.2727898920677 Test RE 0.49317589649673893 Lambda1 -0.058122486\n",
      "7 Train Loss 838.0624 Test MSE 858.2747872853777 Test RE 0.4931764703618751 Lambda1 -0.058124106\n",
      "8 Train Loss 838.0624 Test MSE 858.2768170835732 Test RE 0.4931770535364789 Lambda1 -0.058125667\n",
      "9 Train Loss 838.0623 Test MSE 858.2787134749854 Test RE 0.49317759838179787 Lambda1 -0.05812711\n",
      "10 Train Loss 838.06226 Test MSE 858.2813632088976 Test RE 0.49317835966620244 Lambda1 -0.058128983\n",
      "11 Train Loss 838.0622 Test MSE 858.2834249915197 Test RE 0.493178952027871 Lambda1 -0.058130372\n",
      "12 Train Loss 838.062 Test MSE 858.2857371831268 Test RE 0.49317961633259394 Lambda1 -0.058131825\n",
      "13 Train Loss 838.0619 Test MSE 858.2884377296774 Test RE 0.49318039221258725 Lambda1 -0.058133423\n",
      "14 Train Loss 838.06177 Test MSE 858.2919030642133 Test RE 0.49318138781805343 Lambda1 -0.05813536\n",
      "15 Train Loss 838.04565 Test MSE 858.2361236727577 Test RE 0.49316536191558585 Lambda1 -0.05820979\n",
      "16 Train Loss 837.9666 Test MSE 857.7867658481698 Test RE 0.4930362385175009 Lambda1 -0.05960454\n",
      "17 Train Loss 837.8731 Test MSE 857.7946308871547 Test RE 0.49303849883469336 Lambda1 -0.06416227\n",
      "18 Train Loss 837.86 Test MSE 857.8768051704947 Test RE 0.49306211411134615 Lambda1 -0.07551576\n",
      "19 Train Loss 837.85486 Test MSE 857.9032649625663 Test RE 0.4930697178956791 Lambda1 -0.08868774\n",
      "20 Train Loss 837.75745 Test MSE 857.8327326313173 Test RE 0.4930494486689933 Lambda1 -0.41201022\n",
      "21 Train Loss 835.2716 Test MSE 854.9724878984973 Test RE 0.49222678270497594 Lambda1 -0.9670416\n",
      "22 Train Loss 832.3696 Test MSE 849.7683727349645 Test RE 0.49072643336368055 Lambda1 -0.7006502\n",
      "23 Train Loss 828.10596 Test MSE 843.5876607712589 Test RE 0.48893854967938116 Lambda1 -0.8227146\n",
      "24 Train Loss 815.59015 Test MSE 829.5650512235406 Test RE 0.484857808630484 Lambda1 -0.5877986\n",
      "25 Train Loss 795.977 Test MSE 802.2054080125105 Test RE 0.47679529740756 Lambda1 -0.6740187\n",
      "26 Train Loss 782.7744 Test MSE 787.9734556025869 Test RE 0.4725469500777788 Lambda1 -1.194483\n",
      "27 Train Loss 742.85 Test MSE 742.0889312096826 Test RE 0.4585821515197732 Lambda1 -1.4068455\n",
      "28 Train Loss 723.2234 Test MSE 716.5738119175168 Test RE 0.45062951848243826 Lambda1 -1.5534886\n",
      "29 Train Loss 708.7824 Test MSE 708.045259351612 Test RE 0.4479398294503607 Lambda1 -1.6755584\n",
      "30 Train Loss 702.70856 Test MSE 703.1129573371024 Test RE 0.4463769098306526 Lambda1 -1.729415\n",
      "31 Train Loss 694.5943 Test MSE 691.8985265919264 Test RE 0.44280281539758237 Lambda1 -1.8009907\n",
      "32 Train Loss 682.5703 Test MSE 684.4718834282338 Test RE 0.4404199437653788 Lambda1 -1.8301958\n",
      "33 Train Loss 675.39453 Test MSE 680.0131827900137 Test RE 0.43898313597727356 Lambda1 -1.8234702\n",
      "34 Train Loss 667.1407 Test MSE 672.3452031198713 Test RE 0.43650108342976435 Lambda1 -1.8851533\n",
      "35 Train Loss 657.3259 Test MSE 661.284058039261 Test RE 0.4328956254523343 Lambda1 -1.9702725\n",
      "36 Train Loss 655.7253 Test MSE 659.6813379084138 Test RE 0.4323707137873269 Lambda1 -1.9702659\n",
      "37 Train Loss 653.42206 Test MSE 658.8594910519442 Test RE 0.4321013009404853 Lambda1 -1.9020613\n",
      "38 Train Loss 651.68353 Test MSE 658.4814430532853 Test RE 0.43197731511807835 Lambda1 -1.8608136\n",
      "39 Train Loss 649.72394 Test MSE 655.9709954309748 Test RE 0.43115307623683274 Lambda1 -1.8891512\n",
      "40 Train Loss 646.0034 Test MSE 651.0202521508647 Test RE 0.42952299586231174 Lambda1 -1.9424925\n",
      "41 Train Loss 643.8117 Test MSE 649.1916845926132 Test RE 0.4289193555763402 Lambda1 -1.9464413\n",
      "42 Train Loss 638.40735 Test MSE 645.4408989362489 Test RE 0.4276784932237752 Lambda1 -2.0573788\n",
      "43 Train Loss 637.7919 Test MSE 644.0743209735004 Test RE 0.42722549613536875 Lambda1 -2.0862029\n",
      "44 Train Loss 637.04535 Test MSE 643.2675765677654 Test RE 0.4269578486270293 Lambda1 -2.0937526\n",
      "45 Train Loss 636.373 Test MSE 643.0928817073219 Test RE 0.42689986932791263 Lambda1 -2.08313\n",
      "46 Train Loss 634.8748 Test MSE 641.1929183888731 Test RE 0.4262687831510797 Lambda1 -2.0850546\n",
      "47 Train Loss 633.9018 Test MSE 639.7485443214383 Test RE 0.42578839832323573 Lambda1 -2.11952\n",
      "48 Train Loss 632.79224 Test MSE 638.138424747654 Test RE 0.42525224849913024 Lambda1 -2.1784744\n",
      "49 Train Loss 632.4413 Test MSE 637.8617990897778 Test RE 0.4251600675320907 Lambda1 -2.1921186\n",
      "50 Train Loss 632.227 Test MSE 637.9344400015863 Test RE 0.42518427586043217 Lambda1 -2.1818445\n",
      "51 Train Loss 631.36176 Test MSE 637.8755022652881 Test RE 0.4251646343612563 Lambda1 -2.149876\n",
      "52 Train Loss 630.01215 Test MSE 636.5045179317964 Test RE 0.42470748591219604 Lambda1 -2.158544\n",
      "53 Train Loss 629.6425 Test MSE 635.7168278473448 Test RE 0.4244446115493022 Lambda1 -2.1915872\n",
      "54 Train Loss 629.0127 Test MSE 635.4812614348136 Test RE 0.42436596477277505 Lambda1 -2.2038715\n",
      "55 Train Loss 628.2526 Test MSE 635.3147916731753 Test RE 0.42431037798182336 Lambda1 -2.2165613\n",
      "56 Train Loss 627.3491 Test MSE 634.1650263456222 Test RE 0.4239262547936938 Lambda1 -2.234006\n",
      "57 Train Loss 627.19543 Test MSE 633.6677742248431 Test RE 0.42376002080198943 Lambda1 -2.2398746\n",
      "58 Train Loss 626.9339 Test MSE 633.3649538966464 Test RE 0.4236587544220442 Lambda1 -2.240866\n",
      "59 Train Loss 626.6188 Test MSE 632.9211527790751 Test RE 0.4235102987996714 Lambda1 -2.2454388\n",
      "60 Train Loss 625.58936 Test MSE 631.8746196336899 Test RE 0.4231600174656519 Lambda1 -2.2531736\n",
      "61 Train Loss 625.34515 Test MSE 631.84289296899 Test RE 0.4231493938196786 Lambda1 -2.2577057\n",
      "62 Train Loss 625.2361 Test MSE 632.0542594109656 Test RE 0.42322016465854223 Lambda1 -2.257857\n",
      "63 Train Loss 624.93567 Test MSE 632.6414429988189 Test RE 0.4234167065229104 Lambda1 -2.2390072\n",
      "64 Train Loss 624.6111 Test MSE 632.4529230993903 Test RE 0.4233536151595123 Lambda1 -2.2221036\n",
      "65 Train Loss 624.13666 Test MSE 631.9790185848902 Test RE 0.4231949734806827 Lambda1 -2.2066395\n",
      "66 Train Loss 623.8981 Test MSE 631.8343693486368 Test RE 0.4231465396473915 Lambda1 -2.1922836\n",
      "67 Train Loss 623.72845 Test MSE 631.5627597433012 Test RE 0.42305557987667447 Lambda1 -2.1799955\n",
      "68 Train Loss 623.5301 Test MSE 631.4313180026006 Test RE 0.42301155412108987 Lambda1 -2.1682553\n",
      "69 Train Loss 623.3911 Test MSE 631.2753323864126 Test RE 0.42295930157071915 Lambda1 -2.1572287\n",
      "70 Train Loss 623.05646 Test MSE 630.551994345609 Test RE 0.4227169111056666 Lambda1 -2.1456227\n",
      "71 Train Loss 622.5914 Test MSE 629.7222434475086 Test RE 0.42243869043179405 Lambda1 -2.1528335\n",
      "72 Train Loss 622.316 Test MSE 629.332350594541 Test RE 0.4223078936160152 Lambda1 -2.164624\n",
      "73 Train Loss 622.07196 Test MSE 629.589012940841 Test RE 0.4223940003334629 Lambda1 -2.1646042\n",
      "74 Train Loss 621.8507 Test MSE 629.6168381054204 Test RE 0.4224033342421539 Lambda1 -2.1642108\n",
      "Training time: 76.13\n",
      "Training time: 76.13\n",
      "inv_HT_atanh_tune0\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 974.2701 Test MSE 983.2450094531922 Test RE 0.5278615710846886 Lambda1 -0.1634144\n",
      "1 Train Loss 838.0891 Test MSE 858.1479346788361 Test RE 0.4931400233908938 Lambda1 -0.19751702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Train Loss 838.0627 Test MSE 858.2432990924809 Test RE 0.4931674235050312 Lambda1 -0.19786496\n",
      "3 Train Loss 838.0609 Test MSE 858.2715229394138 Test RE 0.4931755324919907 Lambda1 -0.19795375\n",
      "4 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "5 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "6 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "7 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "8 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "9 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "10 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "11 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "12 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "13 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "14 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "15 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "16 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "17 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "18 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "19 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "20 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "21 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "22 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "23 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "24 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "25 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "26 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "27 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "28 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "29 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "30 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "31 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "32 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "33 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "34 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "35 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "36 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "37 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "38 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "39 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "40 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "41 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "42 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "43 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "44 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "45 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "46 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "47 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "48 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "49 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "50 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "51 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "52 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "53 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "54 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "55 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "56 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "57 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "58 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "59 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "60 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "61 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "62 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "63 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "64 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "65 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "66 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "67 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "68 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "69 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "70 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "71 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "72 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "73 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "74 Train Loss 838.06067 Test MSE 858.2745885977633 Test RE 0.493176413277557 Lambda1 -0.19796425\n",
      "Training time: 123.27\n",
      "Training time: 123.27\n",
      "inv_HT_atanh_tune0\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 907.34576 Test MSE 919.5444935958784 Test RE 0.5104762540175221 Lambda1 -0.04410196\n",
      "1 Train Loss 838.07385 Test MSE 858.1900310112735 Test RE 0.4931521187015873 Lambda1 -0.050192278\n",
      "2 Train Loss 838.06366 Test MSE 858.2466216852304 Test RE 0.4931683781253127 Lambda1 -0.05024225\n",
      "3 Train Loss 838.06256 Test MSE 858.2663093884044 Test RE 0.4931740345978912 Lambda1 -0.050257437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 838.06244 Test MSE 858.2690620763886 Test RE 0.49317482546730784 Lambda1 -0.050259367\n",
      "5 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "6 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "7 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "8 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "9 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "10 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "11 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "12 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "13 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "14 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "15 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "16 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "17 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "18 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "19 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "20 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "21 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "22 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "23 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "24 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "25 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "26 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "27 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "28 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "29 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "30 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "31 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "32 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "33 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "34 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "35 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "36 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "37 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "38 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "39 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "40 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "41 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "42 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "43 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "44 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "45 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "46 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "47 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "48 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "49 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "50 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "51 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "52 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "53 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "54 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "55 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "56 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "57 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "58 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "59 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "60 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "61 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "62 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "63 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "64 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "65 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "66 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "67 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "68 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "69 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "70 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "71 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "72 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "73 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "74 Train Loss 838.0623 Test MSE 858.2712850625802 Test RE 0.49317546414820956 Lambda1 -0.050260857\n",
      "Training time: 123.58\n",
      "Training time: 123.58\n",
      "inv_HT_atanh_tune0\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 872.8374 Test MSE 887.3748468113056 Test RE 0.5014674261003634 Lambda1 -0.05284492\n",
      "1 Train Loss 838.0888 Test MSE 858.1534183663063 Test RE 0.4931415990058537 Lambda1 -0.057859756\n",
      "2 Train Loss 838.0638 Test MSE 858.2538554196324 Test RE 0.4931704564565615 Lambda1 -0.057971846\n",
      "3 Train Loss 838.0632 Test MSE 858.2652192652893 Test RE 0.4931737213963943 Lambda1 -0.05798232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 838.063 Test MSE 858.2686104900381 Test RE 0.49317469572300104 Lambda1 -0.057985228\n",
      "5 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "6 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "7 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "8 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "9 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "10 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "11 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "12 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "13 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "14 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "15 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "16 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "17 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "18 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "19 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "20 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "21 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "22 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "23 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "24 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "25 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "26 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "27 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "28 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "29 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "30 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "31 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "32 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "33 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "34 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "35 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "36 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "37 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "38 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "39 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "40 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "41 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "42 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "43 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "44 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "45 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "46 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "47 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "48 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "49 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "50 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "51 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "52 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "53 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "54 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "55 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "56 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "57 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "58 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "59 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "60 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "61 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "62 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "63 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "64 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "65 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "66 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "67 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "68 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "69 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "70 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "71 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "72 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "73 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "74 Train Loss 838.0629 Test MSE 858.2693704897675 Test RE 0.49317491407687525 Lambda1 -0.05798587\n",
      "Training time: 134.71\n",
      "Training time: 134.71\n",
      "inv_HT_atanh_tune0\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 1550.6694 Test MSE 1545.1647136383363 Test RE 0.6617235051125324 Lambda1 -0.0048829233\n",
      "1 Train Loss 838.0898 Test MSE 858.1452957757887 Test RE 0.49313926515932405 Lambda1 -0.008759647\n",
      "2 Train Loss 838.05994 Test MSE 858.2584923757593 Test RE 0.493171788700318 Lambda1 -0.008780353\n",
      "3 Train Loss 838.0581 Test MSE 858.2827688860625 Test RE 0.49317876352518397 Lambda1 -0.008786739\n",
      "4 Train Loss 838.056 Test MSE 858.3067155973464 Test RE 0.4931856434996597 Lambda1 -0.008797281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 837.8986 Test MSE 857.9090484926604 Test RE 0.49307137990051236 Lambda1 -0.0098012425\n",
      "6 Train Loss 837.89325 Test MSE 857.9033303928543 Test RE 0.4930697366983193 Lambda1 -0.020658134\n",
      "7 Train Loss 837.825 Test MSE 857.8839660078634 Test RE 0.4930641719419244 Lambda1 -0.5018827\n",
      "8 Train Loss 835.66833 Test MSE 853.6390350518793 Test RE 0.4918427836078576 Lambda1 -3.0830004\n",
      "9 Train Loss 832.1924 Test MSE 849.1370099102475 Test RE 0.4905440989727171 Lambda1 -1.8386717\n",
      "10 Train Loss 827.1864 Test MSE 841.7129096342871 Test RE 0.48839495001272176 Lambda1 -1.71443\n",
      "11 Train Loss 816.0927 Test MSE 831.682583039859 Test RE 0.4854762336019072 Lambda1 -2.275309\n",
      "12 Train Loss 808.95294 Test MSE 823.232886512756 Test RE 0.48300377634904834 Lambda1 -2.049225\n",
      "13 Train Loss 799.91895 Test MSE 811.9914989544459 Test RE 0.4796946909013436 Lambda1 -2.4355404\n",
      "14 Train Loss 794.3463 Test MSE 805.4201921220293 Test RE 0.4777497046937122 Lambda1 -2.4559953\n",
      "15 Train Loss 791.14606 Test MSE 801.8347044147907 Test RE 0.476685119793237 Lambda1 -2.5187469\n",
      "16 Train Loss 788.404 Test MSE 798.3197794895882 Test RE 0.4756391731469437 Lambda1 -2.585282\n",
      "17 Train Loss 785.3205 Test MSE 794.5728422184076 Test RE 0.4745216471230816 Lambda1 -2.5285914\n",
      "18 Train Loss 774.4617 Test MSE 778.0885334331946 Test RE 0.46957360636627904 Lambda1 -2.5883937\n",
      "19 Train Loss 772.5968 Test MSE 774.2743819757227 Test RE 0.46842127918693 Lambda1 -2.6474912\n",
      "20 Train Loss 768.73004 Test MSE 768.8035966524526 Test RE 0.46676348507961113 Lambda1 -2.639548\n",
      "21 Train Loss 764.0759 Test MSE 768.092515396586 Test RE 0.4665475758741511 Lambda1 -2.6261168\n",
      "22 Train Loss 757.0336 Test MSE 759.3871468042764 Test RE 0.4638961750797805 Lambda1 -2.8608932\n",
      "23 Train Loss 755.6051 Test MSE 759.8650218845073 Test RE 0.4640421148404248 Lambda1 -2.9320297\n",
      "24 Train Loss 754.22046 Test MSE 759.676522698498 Test RE 0.46398455396854144 Lambda1 -3.0320547\n",
      "25 Train Loss 752.482 Test MSE 757.7256472147347 Test RE 0.4633884068306822 Lambda1 -3.1294928\n",
      "26 Train Loss 747.23206 Test MSE 750.7939993179621 Test RE 0.4612640066598986 Lambda1 -3.134011\n",
      "27 Train Loss 745.2091 Test MSE 747.2717664230058 Test RE 0.4601807606402944 Lambda1 -3.1366088\n",
      "28 Train Loss 743.7843 Test MSE 742.875956010665 Test RE 0.45882526246614264 Lambda1 -3.1224606\n",
      "29 Train Loss 737.15155 Test MSE 735.7344141701858 Test RE 0.4566145078002823 Lambda1 -3.1430519\n",
      "30 Train Loss 728.70483 Test MSE 721.6451162760734 Test RE 0.45222129474352835 Lambda1 -3.2418697\n",
      "31 Train Loss 718.57764 Test MSE 715.895713492622 Test RE 0.45041625120499396 Lambda1 -3.1517797\n",
      "32 Train Loss 715.4441 Test MSE 709.733991166261 Test RE 0.4484736934392564 Lambda1 -3.0677001\n",
      "33 Train Loss 712.9576 Test MSE 704.0049656083686 Test RE 0.4466599693915651 Lambda1 -2.9708464\n",
      "34 Train Loss 709.80286 Test MSE 701.8063595375489 Test RE 0.44596196491235623 Lambda1 -2.9759037\n",
      "35 Train Loss 699.199 Test MSE 695.0797283363953 Test RE 0.44381960440924956 Lambda1 -3.1394863\n",
      "36 Train Loss 697.1659 Test MSE 689.6391524155539 Test RE 0.4420792444655822 Lambda1 -3.1342015\n",
      "37 Train Loss 695.5649 Test MSE 684.6203044156155 Test RE 0.4404676915409302 Lambda1 -3.1029387\n",
      "38 Train Loss 692.1656 Test MSE 680.0852100769155 Test RE 0.4390063840023533 Lambda1 -3.0576344\n",
      "39 Train Loss 690.0138 Test MSE 677.8039309129408 Test RE 0.43826946356317475 Lambda1 -3.0416162\n",
      "40 Train Loss 687.8042 Test MSE 677.6974505522026 Test RE 0.4382350369971717 Lambda1 -3.0527582\n",
      "41 Train Loss 685.11316 Test MSE 676.6486259102751 Test RE 0.43789579293034064 Lambda1 -3.0093122\n",
      "42 Train Loss 682.4954 Test MSE 672.0448390745419 Test RE 0.43640357111255434 Lambda1 -2.944408\n",
      "43 Train Loss 679.609 Test MSE 669.6939280121118 Test RE 0.4356396006724061 Lambda1 -2.9072344\n",
      "44 Train Loss 678.60547 Test MSE 669.0093969533627 Test RE 0.43541689813062107 Lambda1 -2.8682606\n",
      "45 Train Loss 677.9324 Test MSE 667.2224189019768 Test RE 0.43483499246704754 Lambda1 -2.8453445\n",
      "46 Train Loss 675.0656 Test MSE 657.5508453688313 Test RE 0.43167196162308624 Lambda1 -2.8550076\n",
      "47 Train Loss 673.18835 Test MSE 654.7607236951256 Test RE 0.4307551521708859 Lambda1 -2.883853\n",
      "48 Train Loss 672.7472 Test MSE 655.0628848134978 Test RE 0.4308545338540768 Lambda1 -2.8862917\n",
      "49 Train Loss 671.8522 Test MSE 654.9319120301317 Test RE 0.43081145933423376 Lambda1 -2.8999114\n",
      "50 Train Loss 669.28326 Test MSE 651.4951518733093 Test RE 0.429679629365664 Lambda1 -2.9391975\n",
      "51 Train Loss 666.07355 Test MSE 651.1134794488439 Test RE 0.4295537490030375 Lambda1 -2.9304433\n",
      "52 Train Loss 663.543 Test MSE 652.4370604575066 Test RE 0.42999012496472633 Lambda1 -2.9633405\n",
      "53 Train Loss 661.0679 Test MSE 653.0266702439332 Test RE 0.43018437292272216 Lambda1 -3.0051553\n",
      "54 Train Loss 658.6745 Test MSE 651.4572296176079 Test RE 0.4296671237792055 Lambda1 -3.0008001\n",
      "55 Train Loss 657.62134 Test MSE 650.9628260282637 Test RE 0.42950405145612575 Lambda1 -2.9740229\n",
      "56 Train Loss 656.2852 Test MSE 650.3189536110057 Test RE 0.4292915859987916 Lambda1 -2.9381866\n",
      "57 Train Loss 654.6512 Test MSE 646.1107685573438 Test RE 0.42790036832980993 Lambda1 -2.9233816\n",
      "58 Train Loss 652.5041 Test MSE 642.1362878255107 Test RE 0.4265822466413141 Lambda1 -2.9133875\n",
      "59 Train Loss 651.3161 Test MSE 640.4582726006157 Test RE 0.42602451476608255 Lambda1 -2.8989222\n",
      "60 Train Loss 650.5619 Test MSE 639.1819505511693 Test RE 0.4255998066528427 Lambda1 -2.885141\n",
      "61 Train Loss 649.31726 Test MSE 637.2604528648507 Test RE 0.4249596097774216 Lambda1 -2.8770664\n",
      "62 Train Loss 648.0304 Test MSE 637.2437802677762 Test RE 0.424954050647981 Lambda1 -2.8695874\n",
      "63 Train Loss 645.58246 Test MSE 636.2902034394605 Test RE 0.4246359792458859 Lambda1 -2.857823\n",
      "64 Train Loss 645.00446 Test MSE 635.6817107078015 Test RE 0.4244328881798337 Lambda1 -2.8458858\n",
      "65 Train Loss 644.2683 Test MSE 635.6394259411074 Test RE 0.42441877156784413 Lambda1 -2.8275323\n",
      "66 Train Loss 643.6379 Test MSE 636.616069324999 Test RE 0.4247447006018315 Lambda1 -2.8274972\n",
      "67 Train Loss 643.315 Test MSE 637.2951167557612 Test RE 0.42497116749544217 Lambda1 -2.831402\n",
      "68 Train Loss 642.8949 Test MSE 637.7371738122107 Test RE 0.4251185316697472 Lambda1 -2.8387957\n",
      "69 Train Loss 640.43713 Test MSE 634.7556363853121 Test RE 0.42412361417502004 Lambda1 -2.842774\n",
      "70 Train Loss 636.927 Test MSE 629.5622758528584 Test RE 0.4223850312240731 Lambda1 -2.8914\n",
      "71 Train Loss 633.18005 Test MSE 627.2558458277116 Test RE 0.42161060790517046 Lambda1 -2.8752096\n",
      "72 Train Loss 632.20795 Test MSE 625.6961159915453 Test RE 0.421086094699897 Lambda1 -2.8615127\n",
      "73 Train Loss 631.17065 Test MSE 624.5022988249563 Test RE 0.42068419047655936 Lambda1 -2.8558245\n",
      "74 Train Loss 630.4665 Test MSE 623.0244166160502 Test RE 0.4201861219000393 Lambda1 -2.8595605\n",
      "Training time: 158.41\n",
      "Training time: 158.41\n",
      "inv_HT_atanh_tune0\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 1279.9995 Test MSE 1279.9623240176027 Test RE 0.6022651919794834 Lambda1 -0.0248105\n",
      "1 Train Loss 838.0829 Test MSE 858.1619832257055 Test RE 0.4931440599165896 Lambda1 -0.03893261\n",
      "2 Train Loss 838.0623 Test MSE 858.2520517838275 Test RE 0.49316993825302824 Lambda1 -0.03901296\n",
      "3 Train Loss 838.0619 Test MSE 858.2576062860318 Test RE 0.49317153411817727 Lambda1 -0.039017815\n",
      "4 Train Loss 838.0614 Test MSE 858.2673015889762 Test RE 0.4931743196652486 Lambda1 -0.03902706\n",
      "5 Train Loss 838.0613 Test MSE 858.2705019795798 Test RE 0.49317523916258027 Lambda1 -0.039030515\n",
      "6 Train Loss 838.06104 Test MSE 858.2743073758776 Test RE 0.4931763324805637 Lambda1 -0.039035305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 838.061 Test MSE 858.2773744090209 Test RE 0.4931772136596886 Lambda1 -0.039039753\n",
      "8 Train Loss 838.0608 Test MSE 858.2805897308514 Test RE 0.49317813744145383 Lambda1 -0.039045036\n",
      "9 Train Loss 838.0603 Test MSE 858.2908398126908 Test RE 0.4931810823415473 Lambda1 -0.03906455\n",
      "10 Train Loss 838.0591 Test MSE 858.311716907529 Test RE 0.4931870803815706 Lambda1 -0.039119568\n",
      "11 Train Loss 837.907 Test MSE 857.9236112803665 Test RE 0.49307556476333125 Lambda1 -0.040838864\n",
      "12 Train Loss 837.857 Test MSE 857.9222204661885 Test RE 0.493075165090939 Lambda1 -0.0662547\n",
      "13 Train Loss 837.811 Test MSE 857.7089358528634 Test RE 0.4930138705583961 Lambda1 -0.10987928\n",
      "14 Train Loss 837.2018 Test MSE 855.1828083768058 Test RE 0.4922873220836671 Lambda1 -0.16423571\n",
      "15 Train Loss 816.2849 Test MSE 827.4625287658307 Test RE 0.48424298573053837 Lambda1 -0.15767857\n",
      "16 Train Loss 778.0071 Test MSE 785.4190051106125 Test RE 0.4717803775104277 Lambda1 -0.0044004656\n",
      "17 Train Loss 721.16974 Test MSE 735.6175836309259 Test RE 0.45657825243686345 Lambda1 -0.0023703529\n",
      "18 Train Loss 710.3542 Test MSE 719.8145531219681 Test RE 0.451647366325919 Lambda1 -0.00035725097\n",
      "19 Train Loss 692.65717 Test MSE 702.589881267739 Test RE 0.4462108394156339 Lambda1 -0.00033574802\n",
      "20 Train Loss 677.1118 Test MSE 682.5162977457758 Test RE 0.4397903379177141 Lambda1 -0.0005045148\n",
      "21 Train Loss 669.4596 Test MSE 675.0967195190923 Test RE 0.43739334353881104 Lambda1 -0.00062251516\n",
      "22 Train Loss 662.44104 Test MSE 668.0812872628753 Test RE 0.4351147686704837 Lambda1 -0.00030525928\n",
      "23 Train Loss 657.38666 Test MSE 664.677331614866 Test RE 0.43400487169011936 Lambda1 -9.9561774e-05\n",
      "24 Train Loss 651.7215 Test MSE 658.5513435406951 Test RE 0.4320002425854322 Lambda1 -3.297897e-05\n",
      "25 Train Loss 648.8006 Test MSE 656.3374570136317 Test RE 0.4312734923672131 Lambda1 -0.00010362938\n",
      "26 Train Loss 647.2591 Test MSE 653.898429458806 Test RE 0.4304714148876687 Lambda1 -6.6113695e-05\n",
      "27 Train Loss 645.0462 Test MSE 650.3465464513224 Test RE 0.42930069325952097 Lambda1 -1.2093556e-05\n",
      "28 Train Loss 643.8506 Test MSE 649.778076438838 Test RE 0.42911302568048487 Lambda1 -6.4807195e-05\n",
      "29 Train Loss 640.4219 Test MSE 649.6486245669954 Test RE 0.4290702785846654 Lambda1 1.6757405e-05\n",
      "30 Train Loss 639.56116 Test MSE 649.361233863174 Test RE 0.4289753623136047 Lambda1 1.5366571e-05\n",
      "31 Train Loss 638.7869 Test MSE 648.9901237845306 Test RE 0.42885276504113606 Lambda1 -3.1040145e-05\n",
      "32 Train Loss 638.2442 Test MSE 648.6730628894351 Test RE 0.428747995301361 Lambda1 1.833212e-05\n",
      "33 Train Loss 638.0399 Test MSE 648.4867718369976 Test RE 0.4286864252630548 Lambda1 9.7625234e-05\n",
      "34 Train Loss 637.71625 Test MSE 648.813934906955 Test RE 0.4287945482721358 Lambda1 0.00010594439\n",
      "35 Train Loss 637.3368 Test MSE 649.1767182632426 Test RE 0.42891441143926995 Lambda1 3.8311515e-05\n",
      "36 Train Loss 637.27966 Test MSE 649.10105479671 Test RE 0.4288894150892893 Lambda1 3.2795822e-05\n",
      "37 Train Loss 637.21924 Test MSE 649.1314193917489 Test RE 0.4288994465790012 Lambda1 6.045104e-05\n",
      "38 Train Loss 637.1269 Test MSE 649.1683128512577 Test RE 0.4289116346807211 Lambda1 6.642307e-05\n",
      "39 Train Loss 636.71674 Test MSE 648.8279898991763 Test RE 0.42879919264787913 Lambda1 5.0150316e-05\n",
      "40 Train Loss 636.5746 Test MSE 648.6302283335405 Test RE 0.4287338390690361 Lambda1 2.197669e-05\n",
      "41 Train Loss 636.3909 Test MSE 648.1701753250198 Test RE 0.4285817683896316 Lambda1 2.7518142e-08\n",
      "42 Train Loss 636.1247 Test MSE 647.8983284573557 Test RE 0.428491883946939 Lambda1 1.6480173e-05\n",
      "43 Train Loss 636.00757 Test MSE 648.0294288875378 Test RE 0.4285352338263859 Lambda1 2.3872699e-05\n",
      "44 Train Loss 635.7381 Test MSE 647.6423261511173 Test RE 0.42840722116600527 Lambda1 -4.2970814e-06\n",
      "45 Train Loss 635.6861 Test MSE 647.5216858947435 Test RE 0.4283673183045302 Lambda1 -1.803132e-05\n",
      "46 Train Loss 635.6805 Test MSE 647.5193590387097 Test RE 0.42836654863920737 Lambda1 -2.1396672e-05\n",
      "47 Train Loss 635.6583 Test MSE 647.5438644910234 Test RE 0.42837465435571437 Lambda1 -3.046611e-05\n",
      "48 Train Loss 635.5583 Test MSE 647.5467078510246 Test RE 0.4283755948495461 Lambda1 -1.6684042e-05\n",
      "49 Train Loss 635.4918 Test MSE 647.4553192151178 Test RE 0.42834536533701817 Lambda1 -1.1412936e-06\n",
      "50 Train Loss 635.3773 Test MSE 647.5225023680719 Test RE 0.4283675883729985 Lambda1 -4.451954e-07\n",
      "51 Train Loss 635.2235 Test MSE 647.7743235535642 Test RE 0.4284508762492728 Lambda1 7.9164586e-08\n",
      "52 Train Loss 635.10864 Test MSE 648.1772170324 Test RE 0.4285840964350428 Lambda1 -5.5604473e-06\n",
      "53 Train Loss 635.0077 Test MSE 648.4886571942346 Test RE 0.4286870484264731 Lambda1 -2.6968398e-06\n",
      "54 Train Loss 634.80865 Test MSE 648.5702766834244 Test RE 0.4287140250872448 Lambda1 4.0255286e-06\n",
      "55 Train Loss 634.002 Test MSE 647.5584803585139 Test RE 0.4283794888018407 Lambda1 1.4486841e-05\n",
      "56 Train Loss 633.5768 Test MSE 646.3804885509785 Test RE 0.42798967286386636 Lambda1 2.2662849e-05\n",
      "57 Train Loss 633.29803 Test MSE 645.5270744609769 Test RE 0.4277070428466228 Lambda1 4.2833286e-05\n",
      "58 Train Loss 632.01135 Test MSE 643.1254626967924 Test RE 0.42691068319662595 Lambda1 0.0001384178\n",
      "59 Train Loss 630.6336 Test MSE 642.0914212755263 Test RE 0.42656734356825554 Lambda1 0.00012579505\n",
      "60 Train Loss 628.67035 Test MSE 640.2794373247342 Test RE 0.425965031163368 Lambda1 3.0359315e-05\n",
      "61 Train Loss 626.3614 Test MSE 637.2890856710148 Test RE 0.42496915661922297 Lambda1 1.4449919e-05\n",
      "62 Train Loss 622.85486 Test MSE 633.6514396195705 Test RE 0.42375455895150516 Lambda1 4.236267e-06\n",
      "63 Train Loss 618.4499 Test MSE 626.1056032000556 Test RE 0.42122386219010177 Lambda1 1.50171145e-05\n",
      "64 Train Loss 603.0396 Test MSE 610.171194048555 Test RE 0.4158292367372305 Lambda1 0.00014043908\n",
      "65 Train Loss 565.21735 Test MSE 575.5661525136179 Test RE 0.40386553573190065 Lambda1 0.00020790861\n",
      "66 Train Loss 479.8989 Test MSE 484.6971332212114 Test RE 0.3706161977168679 Lambda1 7.402825e-05\n",
      "67 Train Loss 427.46985 Test MSE 431.98066102021824 Test RE 0.3498817765127986 Lambda1 9.593311e-05\n",
      "68 Train Loss 382.5468 Test MSE 385.5605547218064 Test RE 0.33054871169619354 Lambda1 -5.5327246e-06\n",
      "69 Train Loss 338.3385 Test MSE 348.69868845427817 Test RE 0.31435062457451507 Lambda1 1.8558781e-05\n",
      "70 Train Loss 306.7548 Test MSE 330.28979508550873 Test RE 0.30594034240959433 Lambda1 6.798886e-05\n",
      "71 Train Loss 300.63083 Test MSE 329.1737178750998 Test RE 0.3054230058356734 Lambda1 0.0001260347\n",
      "72 Train Loss 295.4231 Test MSE 329.5316742330335 Test RE 0.30558902516695474 Lambda1 6.015775e-05\n",
      "73 Train Loss 292.27472 Test MSE 329.0646318933024 Test RE 0.30537239406536515 Lambda1 4.883228e-05\n",
      "74 Train Loss 287.9029 Test MSE 323.6648465567589 Test RE 0.30285652653638384 Lambda1 6.471626e-06\n",
      "Training time: 157.88\n",
      "Training time: 157.88\n",
      "inv_HT_atanh_tune0\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 945.80426 Test MSE 956.0224853181757 Test RE 0.5205029845159524 Lambda1 -0.1925712\n",
      "1 Train Loss 838.0961 Test MSE 858.1448255331127 Test RE 0.49313913004516546 Lambda1 -0.22825627\n",
      "2 Train Loss 838.0651 Test MSE 858.2578447578242 Test RE 0.49317160263344045 Lambda1 -0.22877467\n",
      "3 Train Loss 838.0645 Test MSE 858.2689293150482 Test RE 0.49317478732393505 Lambda1 -0.22881824\n",
      "4 Train Loss 838.0643 Test MSE 858.2721113710603 Test RE 0.4931757015527326 Lambda1 -0.22883135\n",
      "5 Train Loss 838.0642 Test MSE 858.2748790209137 Test RE 0.49317649671812336 Lambda1 -0.22884321\n",
      "6 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "8 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "9 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "10 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "11 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "12 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "13 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "14 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "15 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "16 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "17 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "18 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "19 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "20 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "21 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "22 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "23 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "24 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "25 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "26 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "27 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "28 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "29 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "30 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "31 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "32 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "33 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "34 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "35 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "36 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "37 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "38 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "39 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "40 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "41 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "42 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "43 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "44 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "45 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "46 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "47 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "48 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "49 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "50 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "51 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "52 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "53 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "54 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "55 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "56 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "57 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "58 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "59 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "60 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "61 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "62 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "63 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "64 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "65 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "66 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "67 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "68 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "69 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "70 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "71 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "72 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "73 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "74 Train Loss 838.064 Test MSE 858.2782937518232 Test RE 0.49317747779271465 Lambda1 -0.22885858\n",
      "Training time: 133.27\n",
      "Training time: 133.27\n",
      "inv_HT_atanh_tune0\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 854.7591 Test MSE 871.0416791959013 Test RE 0.4968309460307194 Lambda1 -0.0030323272\n",
      "1 Train Loss 838.0837 Test MSE 858.1660522463307 Test RE 0.4931452290495807 Lambda1 -0.0023442146\n",
      "2 Train Loss 838.06366 Test MSE 858.2647968608137 Test RE 0.49317360003599897 Lambda1 -0.0023211087\n",
      "3 Train Loss 838.06354 Test MSE 858.2676075286595 Test RE 0.49317440756419423 Lambda1 -0.0023203762\n",
      "4 Train Loss 838.0634 Test MSE 858.2700379466851 Test RE 0.49317510584239493 Lambda1 -0.0023196314\n",
      "5 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "6 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "7 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "9 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "10 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "11 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "12 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "13 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "14 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "15 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "16 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "17 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "18 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "19 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "20 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "21 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "22 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "23 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "24 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "25 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "26 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "27 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "28 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "29 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "30 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "31 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "32 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "33 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "34 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "35 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "36 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "37 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "38 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "39 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "40 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "41 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "42 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "43 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "44 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "45 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "46 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "47 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "48 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "49 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "50 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "51 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "52 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "53 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "54 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "55 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "56 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "57 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "58 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "59 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "60 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "61 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "62 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "63 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "64 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "65 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "66 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "67 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "68 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "69 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "70 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "71 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "72 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "73 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "74 Train Loss 838.0633 Test MSE 858.2714652678761 Test RE 0.49317551592253056 Lambda1 -0.0023191194\n",
      "Training time: 130.44\n",
      "Training time: 130.44\n",
      "inv_HT_atanh_tune0\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 915.96954 Test MSE 927.6838667719126 Test RE 0.5127305238208019 Lambda1 -0.014498206\n",
      "1 Train Loss 838.0883 Test MSE 858.1527677762823 Test RE 0.49314141207362294 Lambda1 -0.016822675\n",
      "2 Train Loss 838.0647 Test MSE 858.2363364823801 Test RE 0.4931654230586239 Lambda1 -0.016851133\n",
      "3 Train Loss 838.06274 Test MSE 858.2591500485162 Test RE 0.4931719776559711 Lambda1 -0.016857734\n",
      "4 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "5 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "6 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "8 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "9 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "10 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "11 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "12 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "13 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "14 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "15 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "16 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "17 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "18 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "19 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "20 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "21 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "22 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "23 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "24 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "25 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "26 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "27 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "28 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "29 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "30 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "31 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "32 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "33 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "34 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "35 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "36 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "37 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "38 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "39 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "40 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "41 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "42 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "43 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "44 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "45 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "46 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "47 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "48 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "49 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "50 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "51 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "52 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "53 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "54 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "55 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "56 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "57 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "58 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "59 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "60 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "61 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "62 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "63 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "64 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "65 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "66 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "67 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "68 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "69 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "70 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "71 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "72 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "73 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "74 Train Loss 838.0623 Test MSE 858.2668165937962 Test RE 0.49317418032217847 Lambda1 -0.016859902\n",
      "Training time: 131.64\n",
      "Training time: 131.64\n",
      "inv_HT_atanh_tune1\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 30161.605 Test MSE 3591.3597015569294 Test RE 1.0088314742449227 Lambda1 4.9474394e-05\n",
      "1 Train Loss 28407.8 Test MSE 3590.4823745840517 Test RE 1.0087082438813615 Lambda1 0.00012643215\n",
      "2 Train Loss 27276.574 Test MSE 3590.3487736339844 Test RE 1.008689476816198 Lambda1 0.00015319862\n",
      "3 Train Loss 26498.258 Test MSE 3590.0177999256202 Test RE 1.0086429830892572 Lambda1 0.00025235998\n",
      "4 Train Loss 26132.756 Test MSE 3589.5533101153665 Test RE 1.0085777299945469 Lambda1 0.00037690433\n",
      "5 Train Loss 25751.375 Test MSE 3588.79917779012 Test RE 1.0084717779503753 Lambda1 0.00056469155\n",
      "6 Train Loss 25155.518 Test MSE 3588.669890242856 Test RE 1.0084536125403751 Lambda1 0.0006641889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 24161.152 Test MSE 3588.7561294250036 Test RE 1.00846572952173 Lambda1 0.000855744\n",
      "8 Train Loss 23209.639 Test MSE 3589.0469674971782 Test RE 1.0085065924664278 Lambda1 0.00096875976\n",
      "9 Train Loss 22645.9 Test MSE 3589.945376108808 Test RE 1.0086328090250698 Lambda1 0.0006977209\n",
      "10 Train Loss 21710.766 Test MSE 3590.169353442598 Test RE 1.0086642729249518 Lambda1 0.00035663272\n",
      "11 Train Loss 21173.678 Test MSE 3590.2159866433076 Test RE 1.0086708237427706 Lambda1 0.00012188341\n",
      "12 Train Loss 20199.887 Test MSE 3590.1005546632887 Test RE 1.008654608310788 Lambda1 -0.00039774948\n",
      "13 Train Loss 19841.848 Test MSE 3589.880341780177 Test RE 1.0086236729452998 Lambda1 -0.00039517757\n",
      "14 Train Loss 19448.479 Test MSE 3589.9115654596962 Test RE 1.0086280592856294 Lambda1 -0.0002972054\n",
      "15 Train Loss 18892.234 Test MSE 3589.7504995209483 Test RE 1.0086054323428078 Lambda1 -0.00041282302\n",
      "16 Train Loss 18632.053 Test MSE 3589.8512492157383 Test RE 1.0086195859694322 Lambda1 -0.00042697132\n",
      "17 Train Loss 17766.277 Test MSE 3590.731606313675 Test RE 1.0087432527898437 Lambda1 -0.00046828078\n",
      "18 Train Loss 16726.926 Test MSE 3593.1067770054383 Test RE 1.0090768256645968 Lambda1 -0.00040798297\n",
      "19 Train Loss 15967.614 Test MSE 3595.0995104393764 Test RE 1.009356603379278 Lambda1 -0.00026472492\n",
      "20 Train Loss 15314.128 Test MSE 3598.064697214708 Test RE 1.0097727690467304 Lambda1 -0.00053867575\n",
      "21 Train Loss 14824.965 Test MSE 3597.9680781917846 Test RE 1.0097592112147418 Lambda1 -0.0006246476\n",
      "22 Train Loss 14591.22 Test MSE 3598.8157081636855 Test RE 1.009878146645738 Lambda1 -0.0005134427\n",
      "23 Train Loss 14132.424 Test MSE 3601.0936001573054 Test RE 1.010197700303733 Lambda1 -0.0002919273\n",
      "24 Train Loss 13590.547 Test MSE 3602.796615108927 Test RE 1.0104365414208702 Lambda1 -2.4157534e-05\n",
      "25 Train Loss 13244.629 Test MSE 3602.102796179351 Test RE 1.0103392428733966 Lambda1 0.0002429355\n",
      "26 Train Loss 12757.029 Test MSE 3605.9417318887636 Test RE 1.0108774832787182 Lambda1 0.0004864439\n",
      "27 Train Loss 12270.048 Test MSE 3605.3340314482143 Test RE 1.0107922993487142 Lambda1 0.00037040783\n",
      "28 Train Loss 11812.899 Test MSE 3604.538124846104 Test RE 1.0106807229091856 Lambda1 -0.0002993636\n",
      "29 Train Loss 11589.92 Test MSE 3605.4634292271808 Test RE 1.0108104381818763 Lambda1 -0.00024274155\n",
      "30 Train Loss 11187.48 Test MSE 3607.4512580531896 Test RE 1.011089048863841 Lambda1 4.395833e-05\n",
      "31 Train Loss 10913.912 Test MSE 3607.9175825625152 Test RE 1.0111543969911816 Lambda1 0.0002178325\n",
      "32 Train Loss 10599.482 Test MSE 3607.2176442012415 Test RE 1.0110563099278627 Lambda1 0.00020593265\n",
      "33 Train Loss 10228.308 Test MSE 3608.1360674521916 Test RE 1.0111850127975741 Lambda1 4.3759806e-05\n",
      "34 Train Loss 9842.472 Test MSE 3608.282596557019 Test RE 1.0112055450790207 Lambda1 -0.00028291042\n",
      "35 Train Loss 9364.979 Test MSE 3608.435878627342 Test RE 1.0112270231685605 Lambda1 -0.00020565296\n",
      "36 Train Loss 8976.479 Test MSE 3607.760789164613 Test RE 1.0111324253063239 Lambda1 0.00049792335\n",
      "37 Train Loss 8521.197 Test MSE 3609.1101537466384 Test RE 1.0113214980872038 Lambda1 0.0010053518\n",
      "38 Train Loss 8195.34 Test MSE 3609.136409128597 Test RE 1.01132517663715 Lambda1 0.0006559468\n",
      "39 Train Loss 7888.536 Test MSE 3609.6186833702227 Test RE 1.011392744018215 Lambda1 -6.0445247e-05\n",
      "40 Train Loss 7655.8755 Test MSE 3609.6301110279082 Test RE 1.0113943449963139 Lambda1 -0.00011850308\n",
      "41 Train Loss 7404.45 Test MSE 3610.8141170349845 Test RE 1.011560206699673 Lambda1 4.294823e-05\n",
      "42 Train Loss 7127.2837 Test MSE 3612.502566150448 Test RE 1.0117966863774226 Lambda1 -9.487069e-05\n",
      "43 Train Loss 6926.839 Test MSE 3610.7879413899736 Test RE 1.0115565401735094 Lambda1 -0.000544606\n",
      "44 Train Loss 6696.7534 Test MSE 3611.609548468241 Test RE 1.0116716195903324 Lambda1 -0.00016476336\n",
      "45 Train Loss 6484.538 Test MSE 3611.5066961723805 Test RE 1.011657214172449 Lambda1 -0.00029150772\n",
      "46 Train Loss 6268.0107 Test MSE 3610.0930734099293 Test RE 1.0114592024066205 Lambda1 -4.2038166e-05\n",
      "47 Train Loss 6077.2983 Test MSE 3608.86666839346 Test RE 1.0112873835666 Lambda1 0.00038625317\n",
      "48 Train Loss 5925.0786 Test MSE 3609.389065147449 Test RE 1.0113605747064165 Lambda1 0.00021534313\n",
      "49 Train Loss 5839.544 Test MSE 3609.4149311800106 Test RE 1.0113641985660529 Lambda1 0.0002790479\n",
      "50 Train Loss 5692.16 Test MSE 3610.248988180504 Test RE 1.0114810439109243 Lambda1 0.00041354037\n",
      "51 Train Loss 5569.936 Test MSE 3609.3338162196683 Test RE 1.0113528342273255 Lambda1 0.00015132279\n",
      "52 Train Loss 5449.4766 Test MSE 3609.8689232560755 Test RE 1.0114278012418074 Lambda1 -6.533694e-05\n",
      "53 Train Loss 5328.6333 Test MSE 3609.132590210455 Test RE 1.011324641582687 Lambda1 -0.0001269951\n",
      "54 Train Loss 5216.0303 Test MSE 3608.8412857328685 Test RE 1.0112838271579112 Lambda1 -0.00010146181\n",
      "55 Train Loss 5147.989 Test MSE 3608.9306641167364 Test RE 1.011296350063092 Lambda1 2.301286e-05\n",
      "56 Train Loss 5049.3794 Test MSE 3608.866651541467 Test RE 1.0112873812054421 Lambda1 6.255657e-05\n",
      "57 Train Loss 4945.2383 Test MSE 3607.012800109106 Test RE 1.011027601949484 Lambda1 -3.2655214e-05\n",
      "58 Train Loss 4864.4854 Test MSE 3606.5684243117603 Test RE 1.0109653218773083 Lambda1 -4.8881666e-06\n",
      "59 Train Loss 4815.9653 Test MSE 3606.463958524073 Test RE 1.0109506802511097 Lambda1 0.00012457126\n",
      "60 Train Loss 4791.1396 Test MSE 3606.8697888061656 Test RE 1.0110075590752456 Lambda1 0.0001904625\n",
      "61 Train Loss 4725.355 Test MSE 3608.696915469192 Test RE 1.0112635989514858 Lambda1 0.00036157758\n",
      "62 Train Loss 4691.6143 Test MSE 3609.1646958727283 Test RE 1.0113291397791886 Lambda1 0.00032462995\n",
      "63 Train Loss 4658.554 Test MSE 3609.4163368394525 Test RE 1.011364395500116 Lambda1 0.0003471627\n",
      "64 Train Loss 4627.699 Test MSE 3610.2577148724213 Test RE 1.011482266385904 Lambda1 0.00046743\n",
      "65 Train Loss 4585.8354 Test MSE 3611.329708024449 Test RE 1.011632424860747 Lambda1 0.00053533656\n",
      "66 Train Loss 4547.488 Test MSE 3612.3430598265313 Test RE 1.0117743487114201 Lambda1 0.0003478466\n",
      "67 Train Loss 4513.332 Test MSE 3612.4270319040866 Test RE 1.0117861084334248 Lambda1 0.00018578372\n",
      "68 Train Loss 4471.963 Test MSE 3612.6903867276847 Test RE 1.0118229886103187 Lambda1 6.880487e-05\n",
      "69 Train Loss 4434.5234 Test MSE 3613.6512847918934 Test RE 1.0119575412574622 Lambda1 0.00021872733\n",
      "70 Train Loss 4387.7964 Test MSE 3612.9094976193846 Test RE 1.0118536718480282 Lambda1 0.0004896944\n",
      "71 Train Loss 4357.127 Test MSE 3611.0277789652832 Test RE 1.0115901346746647 Lambda1 0.00047418883\n",
      "72 Train Loss 4334.0273 Test MSE 3609.2484406443014 Test RE 1.0113408728315214 Lambda1 0.00043207797\n",
      "73 Train Loss 4316.261 Test MSE 3609.34994000354 Test RE 1.0113550932060091 Lambda1 0.00033511486\n",
      "74 Train Loss 4280.061 Test MSE 3608.617118910528 Test RE 1.011252418211861 Lambda1 0.0001089006\n",
      "Training time: 154.22\n",
      "Training time: 154.22\n",
      "inv_HT_atanh_tune1\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 20452.498 Test MSE 3567.2201445828973 Test RE 1.0054352946463951 Lambda1 8.659675e-07\n",
      "1 Train Loss 14424.182 Test MSE 3583.105888771263 Test RE 1.0076715381045074 Lambda1 -7.379871e-05\n",
      "2 Train Loss 12725.378 Test MSE 3583.910375514514 Test RE 1.0077846540605424 Lambda1 -0.00121531\n",
      "3 Train Loss 10483.498 Test MSE 3585.3502287538254 Test RE 1.0079870748988364 Lambda1 0.00079578644\n",
      "4 Train Loss 8155.5137 Test MSE 3593.673023577992 Test RE 1.009156333987013 Lambda1 -0.0007814722\n",
      "5 Train Loss 6093.721 Test MSE 3583.0826862244844 Test RE 1.007668275490371 Lambda1 0.00024615315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 4815.6074 Test MSE 3572.4763316676126 Test RE 1.006175760681352 Lambda1 -0.0004569529\n",
      "7 Train Loss 4233.1943 Test MSE 3564.948074795865 Test RE 1.0051150476567394 Lambda1 -0.00041158847\n",
      "8 Train Loss 3981.9307 Test MSE 3542.0677632928923 Test RE 1.001884374474201 Lambda1 0.00047148278\n",
      "9 Train Loss 3746.6365 Test MSE 3532.702299777673 Test RE 1.0005589732128926 Lambda1 -8.8124e-05\n",
      "10 Train Loss 3624.2273 Test MSE 3493.1618939718774 Test RE 0.9949437485229262 Lambda1 -0.00015477816\n",
      "11 Train Loss 3518.1658 Test MSE 3435.944552412867 Test RE 0.986761608005288 Lambda1 6.8685775e-05\n",
      "12 Train Loss 3420.1768 Test MSE 3369.754116630462 Test RE 0.9772108423232645 Lambda1 -0.0008217199\n",
      "13 Train Loss 3022.1223 Test MSE 2993.3126168905987 Test RE 0.9210118304171057 Lambda1 0.018727377\n",
      "14 Train Loss 1146.1866 Test MSE 1149.3245192432992 Test RE 0.5707034613358648 Lambda1 0.20290443\n",
      "15 Train Loss 838.1199 Test MSE 858.1156911563689 Test RE 0.49313075883396634 Lambda1 0.2996487\n",
      "16 Train Loss 838.0596 Test MSE 858.2617807612844 Test RE 0.49317273348410534 Lambda1 0.30111063\n",
      "17 Train Loss 838.0581 Test MSE 858.2752219477469 Test RE 0.49317659524334045 Lambda1 0.3012718\n",
      "18 Train Loss 838.05536 Test MSE 858.2865929459676 Test RE 0.4931798621975 Lambda1 0.30131325\n",
      "19 Train Loss 838.03687 Test MSE 858.2927085394163 Test RE 0.49318161923424414 Lambda1 0.3007556\n",
      "20 Train Loss 837.99927 Test MSE 858.0697327023339 Test RE 0.49311755325399337 Lambda1 0.29988587\n",
      "21 Train Loss 837.88904 Test MSE 857.896096629647 Test RE 0.4930676579342488 Lambda1 0.29886222\n",
      "22 Train Loss 837.78766 Test MSE 857.7756369580844 Test RE 0.4930330401912257 Lambda1 0.29128027\n",
      "23 Train Loss 837.4149 Test MSE 856.6632415329868 Test RE 0.492713244663154 Lambda1 0.25124466\n",
      "24 Train Loss 836.0978 Test MSE 853.51098595181 Test RE 0.4918058930808209 Lambda1 0.19065145\n",
      "25 Train Loss 831.9865 Test MSE 846.6360304894328 Test RE 0.4898211618175109 Lambda1 0.1378979\n",
      "26 Train Loss 808.4601 Test MSE 816.2357197439782 Test RE 0.48094672167858504 Lambda1 0.00038872426\n",
      "27 Train Loss 782.9222 Test MSE 796.1775580607347 Test RE 0.4750005763573724 Lambda1 0.002605818\n",
      "28 Train Loss 759.18005 Test MSE 772.6088216230074 Test RE 0.4679171917799631 Lambda1 0.00081469095\n",
      "29 Train Loss 742.597 Test MSE 757.9481064033445 Test RE 0.4634564244861176 Lambda1 -0.00048482165\n",
      "30 Train Loss 733.0138 Test MSE 749.4330129872909 Test RE 0.4608457436192311 Lambda1 -2.510911e-05\n",
      "31 Train Loss 729.2132 Test MSE 743.3001893092664 Test RE 0.4589562541690291 Lambda1 2.3541334e-05\n",
      "32 Train Loss 725.6579 Test MSE 739.1267096405784 Test RE 0.4576659673467006 Lambda1 4.4349432e-05\n",
      "33 Train Loss 715.0106 Test MSE 727.5075568204849 Test RE 0.45405443815110125 Lambda1 2.6002534e-05\n",
      "34 Train Loss 706.42584 Test MSE 720.584613414942 Test RE 0.45188888876608313 Lambda1 9.269451e-05\n",
      "35 Train Loss 702.1961 Test MSE 716.8404497822864 Test RE 0.45071335054220013 Lambda1 7.067074e-05\n",
      "36 Train Loss 700.93604 Test MSE 715.3582197287075 Test RE 0.4502471334270591 Lambda1 3.9244205e-05\n",
      "37 Train Loss 699.50836 Test MSE 714.1472227252773 Test RE 0.4498658706791479 Lambda1 4.046923e-05\n",
      "38 Train Loss 698.6415 Test MSE 712.7081937635481 Test RE 0.44941239523117676 Lambda1 5.172877e-05\n",
      "39 Train Loss 697.7163 Test MSE 711.5853340323627 Test RE 0.44905823486070207 Lambda1 5.824252e-05\n",
      "40 Train Loss 696.6334 Test MSE 711.0993217301651 Test RE 0.44890485543126685 Lambda1 -5.1730376e-06\n",
      "41 Train Loss 695.5194 Test MSE 710.4024205970512 Test RE 0.4486848306193933 Lambda1 6.465667e-07\n",
      "42 Train Loss 692.8102 Test MSE 707.9928029328776 Test RE 0.4479232360517225 Lambda1 5.905801e-06\n",
      "43 Train Loss 681.70496 Test MSE 691.618568640454 Test RE 0.4427132222659572 Lambda1 -5.7652083e-05\n",
      "44 Train Loss 678.70734 Test MSE 689.0718996995909 Test RE 0.441897394117195 Lambda1 -4.780294e-05\n",
      "45 Train Loss 675.57825 Test MSE 686.7054182226776 Test RE 0.4411379368007463 Lambda1 -4.5640987e-05\n",
      "46 Train Loss 675.0669 Test MSE 686.486279920492 Test RE 0.44106754422089667 Lambda1 -2.0268662e-05\n",
      "47 Train Loss 674.2943 Test MSE 686.0050113213247 Test RE 0.44091290953824686 Lambda1 -2.602508e-06\n",
      "48 Train Loss 673.70844 Test MSE 685.6473389987051 Test RE 0.44079795199196714 Lambda1 1.2290805e-05\n",
      "49 Train Loss 673.1131 Test MSE 685.0663089012274 Test RE 0.4406111422831145 Lambda1 2.5775984e-05\n",
      "50 Train Loss 672.88617 Test MSE 684.8533105083313 Test RE 0.44054264032939905 Lambda1 3.5192024e-05\n",
      "51 Train Loss 672.783 Test MSE 684.6789828706982 Test RE 0.4404865672664233 Lambda1 3.659363e-05\n",
      "52 Train Loss 672.7136 Test MSE 684.4423438123557 Test RE 0.4404104401052231 Lambda1 4.002503e-05\n",
      "53 Train Loss 672.53864 Test MSE 684.2238587605718 Test RE 0.44034014143149236 Lambda1 2.3556426e-05\n",
      "54 Train Loss 671.66205 Test MSE 683.134736099398 Test RE 0.43998954314496647 Lambda1 -1.6240128e-05\n",
      "55 Train Loss 671.1313 Test MSE 682.5529071278143 Test RE 0.4398021326811902 Lambda1 2.8172228e-06\n",
      "56 Train Loss 670.98895 Test MSE 682.3525885132754 Test RE 0.43973759041790733 Lambda1 -7.748623e-06\n",
      "57 Train Loss 670.83167 Test MSE 682.1692465874812 Test RE 0.4396785097062796 Lambda1 -2.7850252e-05\n",
      "58 Train Loss 670.4056 Test MSE 681.3726558856488 Test RE 0.43942172143721764 Lambda1 -3.80743e-05\n",
      "59 Train Loss 669.8166 Test MSE 680.4207392518227 Test RE 0.4391146654971283 Lambda1 4.5105127e-05\n",
      "60 Train Loss 668.27057 Test MSE 679.2864754203159 Test RE 0.43874850997061937 Lambda1 4.4497818e-05\n",
      "61 Train Loss 667.15594 Test MSE 677.528639116302 Test RE 0.4381804523935727 Lambda1 -2.7396554e-06\n",
      "62 Train Loss 666.39703 Test MSE 677.1215279956147 Test RE 0.43804878641658984 Lambda1 4.7720705e-06\n",
      "63 Train Loss 666.02216 Test MSE 676.8560961403708 Test RE 0.4379629203687923 Lambda1 4.0188406e-06\n",
      "64 Train Loss 665.51636 Test MSE 676.546551379737 Test RE 0.4378627627196734 Lambda1 -6.0766697e-06\n",
      "65 Train Loss 664.7657 Test MSE 675.6111835631939 Test RE 0.4375599717329132 Lambda1 7.772732e-07\n",
      "66 Train Loss 663.57745 Test MSE 673.5074642513674 Test RE 0.43687820308470127 Lambda1 7.130591e-06\n",
      "67 Train Loss 662.2652 Test MSE 670.9088991238785 Test RE 0.4360345943132554 Lambda1 2.8791092e-05\n",
      "68 Train Loss 660.162 Test MSE 666.478561032779 Test RE 0.4345925353614829 Lambda1 0.00013051879\n",
      "69 Train Loss 657.62555 Test MSE 663.2462366315151 Test RE 0.4335373990954078 Lambda1 0.00031276242\n",
      "70 Train Loss 655.201 Test MSE 661.2375572184096 Test RE 0.43288040479520934 Lambda1 0.0006040391\n",
      "71 Train Loss 654.26984 Test MSE 660.9782787702279 Test RE 0.43279552791334586 Lambda1 0.0006739504\n",
      "72 Train Loss 653.4102 Test MSE 660.1735321057527 Test RE 0.43253198155713357 Lambda1 0.0008093221\n",
      "73 Train Loss 652.1004 Test MSE 659.7597414888573 Test RE 0.43239640680220376 Lambda1 0.0009886198\n",
      "74 Train Loss 651.5198 Test MSE 659.6771375187393 Test RE 0.4323693372678862 Lambda1 0.0011024161\n",
      "Training time: 162.11\n",
      "Training time: 162.11\n",
      "inv_HT_atanh_tune1\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 7534.6953 Test MSE 3518.4095893539406 Test RE 0.9985328764272923 Lambda1 -8.855883e-05\n",
      "1 Train Loss 5527.998 Test MSE 3516.2703118212135 Test RE 0.9982292642839832 Lambda1 -0.0013014339\n",
      "2 Train Loss 4727.321 Test MSE 3516.0381493721397 Test RE 0.9981963095968195 Lambda1 -0.0003238009\n",
      "3 Train Loss 4084.9675 Test MSE 3517.9148467397536 Test RE 0.9984626694032233 Lambda1 0.0023799872\n",
      "4 Train Loss 3772.597 Test MSE 3517.4305413608467 Test RE 0.9983939387052966 Lambda1 -0.00011115495\n",
      "5 Train Loss 3627.7622 Test MSE 3508.8733649999367 Test RE 0.9971787568645556 Lambda1 -0.0011704848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 3561.5474 Test MSE 3492.0162796213485 Test RE 0.994780584698868 Lambda1 0.0010823717\n",
      "7 Train Loss 3451.4033 Test MSE 3391.1303958714634 Test RE 0.98030544623958 Lambda1 0.00082602917\n",
      "8 Train Loss 3272.8481 Test MSE 3216.98244593321 Test RE 0.9548024312166263 Lambda1 -0.00023338551\n",
      "9 Train Loss 3091.849 Test MSE 3040.41043761434 Test RE 0.9282293107083353 Lambda1 -0.000646555\n",
      "10 Train Loss 2986.6423 Test MSE 2952.3533450409295 Test RE 0.9146887497339077 Lambda1 0.0007943803\n",
      "11 Train Loss 2525.6968 Test MSE 2495.769805127088 Test RE 0.8409911656593728 Lambda1 0.008099155\n",
      "12 Train Loss 1883.6926 Test MSE 1872.7099599818896 Test RE 0.7284914026477654 Lambda1 0.024048463\n",
      "13 Train Loss 838.213 Test MSE 858.0688933022034 Test RE 0.4931173120597035 Lambda1 0.0762684\n",
      "14 Train Loss 838.0671 Test MSE 858.2551285422403 Test RE 0.4931708222377466 Lambda1 0.07686966\n",
      "15 Train Loss 838.058 Test MSE 858.3179896746906 Test RE 0.49318888254855103 Lambda1 0.07700445\n",
      "16 Train Loss 837.98145 Test MSE 858.0655245977085 Test RE 0.4931163440907275 Lambda1 0.07774781\n",
      "17 Train Loss 837.8414 Test MSE 857.7981161492504 Test RE 0.49303950045360306 Lambda1 0.079453684\n",
      "18 Train Loss 837.7469 Test MSE 857.5732458203539 Test RE 0.4929748714811305 Lambda1 0.08162027\n",
      "19 Train Loss 837.58185 Test MSE 857.3318664178306 Test RE 0.49290548827978625 Lambda1 0.08392155\n",
      "20 Train Loss 837.21 Test MSE 856.327363357828 Test RE 0.49261664436444486 Lambda1 0.09080958\n",
      "21 Train Loss 836.155 Test MSE 854.9465218926616 Test RE 0.49221930804307906 Lambda1 0.0979091\n",
      "22 Train Loss 834.67444 Test MSE 851.7158519067395 Test RE 0.4912884292052773 Lambda1 0.102144465\n",
      "23 Train Loss 831.6606 Test MSE 846.1208468296937 Test RE 0.4896721094206154 Lambda1 0.094908595\n",
      "24 Train Loss 819.4368 Test MSE 823.2823886600536 Test RE 0.4830182979777718 Lambda1 0.030090813\n",
      "25 Train Loss 796.22375 Test MSE 804.739003179944 Test RE 0.47754763212434775 Lambda1 -0.004450478\n",
      "26 Train Loss 767.1015 Test MSE 767.6503829050725 Test RE 0.4664132785526216 Lambda1 -0.002142332\n",
      "27 Train Loss 737.5338 Test MSE 748.2789578784017 Test RE 0.46049077774528546 Lambda1 -0.00034606145\n",
      "28 Train Loss 730.7862 Test MSE 745.143211517904 Test RE 0.4595248958135592 Lambda1 -0.0003458192\n",
      "29 Train Loss 728.1585 Test MSE 744.2424908471271 Test RE 0.45924707758872785 Lambda1 -0.00030625743\n",
      "30 Train Loss 726.9462 Test MSE 742.6594608826781 Test RE 0.45875840024552733 Lambda1 -0.00046949644\n",
      "31 Train Loss 724.92804 Test MSE 741.2212781040316 Test RE 0.4583139851431141 Lambda1 -0.00035344606\n",
      "32 Train Loss 721.9723 Test MSE 740.1912045336487 Test RE 0.4579954154601245 Lambda1 -0.0002340051\n",
      "33 Train Loss 720.46765 Test MSE 736.1983360312117 Test RE 0.4567584456571049 Lambda1 -0.00012213766\n",
      "34 Train Loss 718.74774 Test MSE 735.8934761625362 Test RE 0.4566638639856215 Lambda1 -1.6218539e-05\n",
      "35 Train Loss 717.4775 Test MSE 736.0200427933775 Test RE 0.45670313320296363 Lambda1 2.6256346e-05\n",
      "36 Train Loss 716.81964 Test MSE 735.605262845157 Test RE 0.4565744288289757 Lambda1 3.5235807e-05\n",
      "37 Train Loss 716.58234 Test MSE 735.5218555715534 Test RE 0.45654854354119284 Lambda1 3.55672e-05\n",
      "38 Train Loss 716.23083 Test MSE 735.4773883820036 Test RE 0.45653474263417754 Lambda1 4.025233e-05\n",
      "39 Train Loss 715.64215 Test MSE 735.4545221211874 Test RE 0.45652764566301096 Lambda1 4.0448846e-05\n",
      "40 Train Loss 715.45276 Test MSE 735.4879263561637 Test RE 0.4565380132541614 Lambda1 4.0092706e-05\n",
      "41 Train Loss 714.91394 Test MSE 734.7683413676164 Test RE 0.45631462529479255 Lambda1 3.9820094e-05\n",
      "42 Train Loss 714.11194 Test MSE 733.2596205378336 Test RE 0.4558459026315711 Lambda1 6.524392e-05\n",
      "43 Train Loss 713.5747 Test MSE 732.4255065279212 Test RE 0.45558655680380106 Lambda1 9.822708e-05\n",
      "44 Train Loss 713.32153 Test MSE 731.7694732306795 Test RE 0.45538247672669147 Lambda1 0.00010891654\n",
      "45 Train Loss 712.92194 Test MSE 729.9466401697439 Test RE 0.45481494566394004 Lambda1 0.00013833045\n",
      "46 Train Loss 712.4072 Test MSE 729.0573009726729 Test RE 0.4545377966138287 Lambda1 0.00019638242\n",
      "47 Train Loss 712.1819 Test MSE 728.4178269669962 Test RE 0.45433840976832124 Lambda1 0.00026384206\n",
      "48 Train Loss 711.90985 Test MSE 727.795371247249 Test RE 0.45414424511875295 Lambda1 0.00033033296\n",
      "49 Train Loss 711.0898 Test MSE 727.0689553037787 Test RE 0.4539175467963126 Lambda1 0.00035262117\n",
      "50 Train Loss 710.7353 Test MSE 727.0604198793078 Test RE 0.4539148824064501 Lambda1 0.00026739296\n",
      "51 Train Loss 710.229 Test MSE 726.2275360995479 Test RE 0.45365481689757753 Lambda1 0.00015699942\n",
      "52 Train Loss 709.9952 Test MSE 725.7040602487867 Test RE 0.4534912867784548 Lambda1 0.00016643926\n",
      "53 Train Loss 709.59 Test MSE 725.3137158730658 Test RE 0.4533693075908786 Lambda1 0.000179698\n",
      "54 Train Loss 709.2487 Test MSE 725.1474021570896 Test RE 0.45331732604449765 Lambda1 0.00014021163\n",
      "55 Train Loss 708.9064 Test MSE 724.6425991946566 Test RE 0.4531595127725567 Lambda1 0.00013198651\n",
      "56 Train Loss 708.45654 Test MSE 723.8997612450969 Test RE 0.4529271841756451 Lambda1 0.00016364927\n",
      "57 Train Loss 707.98627 Test MSE 723.5044612865353 Test RE 0.4528035023098821 Lambda1 0.00019157976\n",
      "58 Train Loss 704.85364 Test MSE 717.6401788347252 Test RE 0.45096469523476695 Lambda1 3.7955248e-05\n",
      "59 Train Loss 703.3564 Test MSE 715.5829450355698 Test RE 0.4503178490330756 Lambda1 2.8975213e-05\n",
      "60 Train Loss 701.99286 Test MSE 714.5216721279883 Test RE 0.4499837944950349 Lambda1 9.052324e-06\n",
      "61 Train Loss 698.8328 Test MSE 710.2216122497723 Test RE 0.44862772838907305 Lambda1 2.849202e-05\n",
      "62 Train Loss 697.04626 Test MSE 708.2557639846178 Test RE 0.44800641163614136 Lambda1 4.6502468e-05\n",
      "63 Train Loss 694.1871 Test MSE 707.3938699869421 Test RE 0.4477337335992625 Lambda1 8.0464975e-05\n",
      "64 Train Loss 692.41046 Test MSE 707.1752006119723 Test RE 0.44766452659369993 Lambda1 8.966988e-05\n",
      "65 Train Loss 689.6481 Test MSE 704.5790527404179 Test RE 0.44684204870869154 Lambda1 3.4132656e-05\n",
      "66 Train Loss 688.79456 Test MSE 704.0246244124871 Test RE 0.44666620566827936 Lambda1 3.6136058e-05\n",
      "67 Train Loss 688.1852 Test MSE 703.4415083749296 Test RE 0.4464811892831191 Lambda1 5.2166197e-05\n",
      "68 Train Loss 687.8607 Test MSE 703.5245380068753 Test RE 0.44650753836440765 Lambda1 4.639791e-05\n",
      "69 Train Loss 687.5046 Test MSE 703.9424514463298 Test RE 0.44664013771781225 Lambda1 3.1339383e-05\n",
      "70 Train Loss 687.37415 Test MSE 704.024504356545 Test RE 0.4466661675837206 Lambda1 2.62803e-05\n",
      "71 Train Loss 687.159 Test MSE 703.972237154185 Test RE 0.44664958689359885 Lambda1 2.299616e-05\n",
      "72 Train Loss 686.9712 Test MSE 703.9840481064067 Test RE 0.446653333728009 Lambda1 1.5390484e-05\n",
      "73 Train Loss 686.8689 Test MSE 703.931215096475 Test RE 0.44663657306216237 Lambda1 1.3452643e-05\n",
      "74 Train Loss 686.8277 Test MSE 703.7932739798855 Test RE 0.44659280986144256 Lambda1 1.4002703e-05\n",
      "Training time: 157.81\n",
      "Training time: 157.81\n",
      "inv_HT_atanh_tune1\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 24585.818 Test MSE 3521.5681851250656 Test RE 0.9989809843157645 Lambda1 0.0003392895\n",
      "1 Train Loss 22004.195 Test MSE 3519.132878628933 Test RE 0.9986355067476274 Lambda1 0.00014230036\n",
      "2 Train Loss 18550.49 Test MSE 3513.9626688302205 Test RE 0.9979016536870998 Lambda1 -0.00056251656\n",
      "3 Train Loss 16539.977 Test MSE 3511.2802802428523 Test RE 0.9975207061242312 Lambda1 -0.00060222566\n",
      "4 Train Loss 14235.193 Test MSE 3506.309065246979 Test RE 0.9968143190061323 Lambda1 -0.00045978534\n",
      "5 Train Loss 11980.867 Test MSE 3497.391811575713 Test RE 0.9955459617803581 Lambda1 0.00044782652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 8444.533 Test MSE 3463.9529248731346 Test RE 0.9907752776281162 Lambda1 -0.0002536921\n",
      "7 Train Loss 7078.7266 Test MSE 3455.7684430945806 Test RE 0.9896041044136358 Lambda1 -0.0005993437\n",
      "8 Train Loss 5376.746 Test MSE 3452.8227339053533 Test RE 0.9891822434839534 Lambda1 0.000692676\n",
      "9 Train Loss 4634.5176 Test MSE 3453.4830943300685 Test RE 0.9892768306706469 Lambda1 -0.00082111044\n",
      "10 Train Loss 4246.846 Test MSE 3451.4915645205765 Test RE 0.988991545009372 Lambda1 -0.000623784\n",
      "11 Train Loss 3987.115 Test MSE 3446.1663292065336 Test RE 0.9882283031459601 Lambda1 0.0008385917\n",
      "12 Train Loss 3856.621 Test MSE 3447.3457463638356 Test RE 0.9883973944779868 Lambda1 -0.00036409037\n",
      "13 Train Loss 3712.7666 Test MSE 3445.963963745163 Test RE 0.9881992873943485 Lambda1 0.00034335343\n",
      "14 Train Loss 3632.881 Test MSE 3441.945912339666 Test RE 0.9876229909342704 Lambda1 5.7225556e-05\n",
      "15 Train Loss 3570.1748 Test MSE 3433.6828667033897 Test RE 0.9864367901900108 Lambda1 0.00014993791\n",
      "16 Train Loss 3508.2534 Test MSE 3419.461913841165 Test RE 0.9843919557552692 Lambda1 0.00016616164\n",
      "17 Train Loss 3465.9067 Test MSE 3397.4136713314483 Test RE 0.9812132082385872 Lambda1 -0.000122067824\n",
      "18 Train Loss 3350.4297 Test MSE 3314.130432144497 Test RE 0.9691119949282828 Lambda1 0.00017810462\n",
      "19 Train Loss 3229.4043 Test MSE 3193.9760618291557 Test RE 0.9513821504740917 Lambda1 -0.0013849109\n",
      "20 Train Loss 1217.9786 Test MSE 1219.1303443949323 Test RE 0.5877792375321045 Lambda1 -0.2271037\n",
      "21 Train Loss 838.18317 Test MSE 858.0674414218953 Test RE 0.49311689487431 Lambda1 -0.37062\n",
      "22 Train Loss 838.0555 Test MSE 858.2397628905939 Test RE 0.49316640751028257 Lambda1 -0.37278575\n",
      "23 Train Loss 838.0088 Test MSE 858.2706441329043 Test RE 0.49317528000430605 Lambda1 -0.3729142\n",
      "24 Train Loss 837.90546 Test MSE 857.8826485696962 Test RE 0.49306379334654665 Lambda1 -0.36801827\n",
      "25 Train Loss 837.4545 Test MSE 857.1181687719796 Test RE 0.4928440538976065 Lambda1 -0.3536607\n",
      "26 Train Loss 836.5742 Test MSE 855.3703700874664 Test RE 0.4923413042200879 Lambda1 -0.33960736\n",
      "27 Train Loss 835.16815 Test MSE 852.2301958545165 Test RE 0.49143674926507386 Lambda1 -0.32933453\n",
      "28 Train Loss 832.0611 Test MSE 847.7073654136602 Test RE 0.49013097417751883 Lambda1 -0.31878337\n",
      "29 Train Loss 829.3991 Test MSE 845.0367785089951 Test RE 0.4893583196335892 Lambda1 -0.32681426\n",
      "30 Train Loss 827.4001 Test MSE 843.8182194539327 Test RE 0.4890053603570378 Lambda1 -0.32641053\n",
      "31 Train Loss 823.44415 Test MSE 838.0763705219558 Test RE 0.48733877646199203 Lambda1 -0.32162797\n",
      "32 Train Loss 820.0855 Test MSE 833.9702875773404 Test RE 0.4861434734271866 Lambda1 -0.31896818\n",
      "33 Train Loss 818.57086 Test MSE 831.9387091417574 Test RE 0.4855509818087746 Lambda1 -0.31824827\n",
      "34 Train Loss 817.61224 Test MSE 830.989883373277 Test RE 0.48527401746553306 Lambda1 -0.31961012\n",
      "35 Train Loss 817.1105 Test MSE 830.1607165586523 Test RE 0.4850318523429284 Lambda1 -0.32190576\n",
      "36 Train Loss 816.21594 Test MSE 829.2077486457025 Test RE 0.48475338065171664 Lambda1 -0.32186878\n",
      "37 Train Loss 814.61896 Test MSE 827.3859035794624 Test RE 0.4842205641302914 Lambda1 -0.31975192\n",
      "38 Train Loss 813.5116 Test MSE 827.1213717209455 Test RE 0.4841431504317859 Lambda1 -0.32066077\n",
      "39 Train Loss 812.7604 Test MSE 826.0502032729952 Test RE 0.4838295526351283 Lambda1 -0.32394266\n",
      "40 Train Loss 812.3646 Test MSE 825.1430349975046 Test RE 0.4835638088646973 Lambda1 -0.32750925\n",
      "41 Train Loss 811.7101 Test MSE 824.6803964492735 Test RE 0.48342822835574156 Lambda1 -0.33062235\n",
      "42 Train Loss 810.3719 Test MSE 822.7703007175196 Test RE 0.4828680540730951 Lambda1 -0.33914217\n",
      "43 Train Loss 808.61615 Test MSE 820.3341008045893 Test RE 0.4821526446833518 Lambda1 -0.35011867\n",
      "44 Train Loss 807.8733 Test MSE 818.4853809351362 Test RE 0.48160904425314943 Lambda1 -0.35943666\n",
      "45 Train Loss 806.0706 Test MSE 815.0941841526239 Test RE 0.48061029319549814 Lambda1 -0.37017226\n",
      "46 Train Loss 804.4543 Test MSE 812.3038123871446 Test RE 0.47978693367285796 Lambda1 -0.367475\n",
      "47 Train Loss 802.9568 Test MSE 810.9147536343185 Test RE 0.47937653464131075 Lambda1 -0.36967528\n",
      "48 Train Loss 801.3836 Test MSE 808.0440330194622 Test RE 0.4785272615285776 Lambda1 -0.37319976\n",
      "49 Train Loss 799.265 Test MSE 804.9308657011125 Test RE 0.47760455619031417 Lambda1 -0.3801717\n",
      "50 Train Loss 795.8857 Test MSE 799.5422368948939 Test RE 0.4760032040925323 Lambda1 -0.37936428\n",
      "51 Train Loss 794.0759 Test MSE 799.1829841575769 Test RE 0.4758962524771962 Lambda1 -0.3739812\n",
      "52 Train Loss 793.1572 Test MSE 797.5383990414808 Test RE 0.47540634280338306 Lambda1 -0.3715804\n",
      "53 Train Loss 792.39 Test MSE 796.9780139368298 Test RE 0.47523929288587974 Lambda1 -0.36744684\n",
      "54 Train Loss 791.44476 Test MSE 797.3500096261467 Test RE 0.4753501907656808 Lambda1 -0.36691904\n",
      "55 Train Loss 790.8626 Test MSE 797.1643361745771 Test RE 0.47529484176694925 Lambda1 -0.3714236\n",
      "56 Train Loss 789.60376 Test MSE 795.66228748514 Test RE 0.4748468459295913 Lambda1 -0.3724202\n",
      "57 Train Loss 788.67487 Test MSE 795.7187751170395 Test RE 0.4748637013833495 Lambda1 -0.3696725\n",
      "58 Train Loss 787.86707 Test MSE 795.8913944274351 Test RE 0.4749152058860287 Lambda1 -0.3706127\n",
      "59 Train Loss 787.2103 Test MSE 795.5805160235061 Test RE 0.4748224449249609 Lambda1 -0.37508234\n",
      "60 Train Loss 786.9389 Test MSE 795.476403431466 Test RE 0.47479137540281874 Lambda1 -0.37690082\n",
      "61 Train Loss 786.5097 Test MSE 796.1009454599159 Test RE 0.47497772221845874 Lambda1 -0.37887383\n",
      "62 Train Loss 786.1867 Test MSE 795.6586034424691 Test RE 0.47484574662018897 Lambda1 -0.3800079\n",
      "63 Train Loss 785.7293 Test MSE 794.758178574837 Test RE 0.4745769856525595 Lambda1 -0.38209987\n",
      "64 Train Loss 784.545 Test MSE 793.8735372936186 Test RE 0.47431278749972583 Lambda1 -0.39022532\n",
      "65 Train Loss 782.72144 Test MSE 790.5510754353619 Test RE 0.4733192171439875 Lambda1 -0.40345654\n",
      "66 Train Loss 781.3635 Test MSE 787.4487157874935 Test RE 0.4723895808838925 Lambda1 -0.41494864\n",
      "67 Train Loss 780.4015 Test MSE 786.6980694529256 Test RE 0.47216437126273914 Lambda1 -0.42184263\n",
      "68 Train Loss 779.46765 Test MSE 786.1242996701095 Test RE 0.4719921558510801 Lambda1 -0.4295938\n",
      "69 Train Loss 778.21234 Test MSE 784.3692409470227 Test RE 0.4714649893372845 Lambda1 -0.43519452\n",
      "70 Train Loss 777.803 Test MSE 783.1014826996595 Test RE 0.47108382616409705 Lambda1 -0.43879503\n",
      "71 Train Loss 777.1467 Test MSE 781.6485495488241 Test RE 0.47064660883388887 Lambda1 -0.44462258\n",
      "72 Train Loss 776.5505 Test MSE 781.370343500805 Test RE 0.4705628445986528 Lambda1 -0.44739637\n",
      "73 Train Loss 775.8416 Test MSE 780.6658949193119 Test RE 0.47035067755240056 Lambda1 -0.45071754\n",
      "74 Train Loss 775.08295 Test MSE 780.7943482802511 Test RE 0.47038937249257873 Lambda1 -0.44857386\n",
      "Training time: 159.59\n",
      "Training time: 159.59\n",
      "inv_HT_atanh_tune1\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 9455.271 Test MSE 3586.801802180147 Test RE 1.0081911022731904 Lambda1 -3.6030553e-05\n",
      "1 Train Loss 9386.828 Test MSE 3586.5160645651176 Test RE 1.0081509433970508 Lambda1 -7.957746e-05\n",
      "2 Train Loss 9006.143 Test MSE 3586.028805187849 Test RE 1.0080824580372112 Lambda1 -0.00015338635\n",
      "3 Train Loss 7173.579 Test MSE 3583.870372743102 Test RE 1.007779029716262 Lambda1 -0.0010318939\n",
      "4 Train Loss 6417.379 Test MSE 3583.535318241834 Test RE 1.0077319202009363 Lambda1 -0.001341758\n",
      "5 Train Loss 6151.6543 Test MSE 3584.079362952719 Test RE 1.0078084131562839 Lambda1 -0.0010686371\n",
      "6 Train Loss 5686.438 Test MSE 3585.855647943981 Test RE 1.0080581192913798 Lambda1 -1.8248515e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 5257.6494 Test MSE 3586.56324187797 Test RE 1.0081575740234299 Lambda1 0.0005156671\n",
      "8 Train Loss 4937.4326 Test MSE 3586.519586299304 Test RE 1.0081514383674772 Lambda1 -0.000106411666\n",
      "9 Train Loss 4778.109 Test MSE 3584.746545011828 Test RE 1.007902211358752 Lambda1 -0.0005574791\n",
      "10 Train Loss 4626.6177 Test MSE 3581.842863334111 Test RE 1.0074939230687716 Lambda1 -0.0012659833\n",
      "11 Train Loss 4332.364 Test MSE 3576.0143470165963 Test RE 1.0066738723564663 Lambda1 -0.001895489\n",
      "12 Train Loss 4276.355 Test MSE 3575.3940353087755 Test RE 1.0065865574563033 Lambda1 -0.0015853042\n",
      "13 Train Loss 4260.816 Test MSE 3574.8995854092414 Test RE 1.0065169534034712 Lambda1 -0.001238693\n",
      "14 Train Loss 4203.4062 Test MSE 3573.776639688751 Test RE 1.006358857679123 Lambda1 -0.000527736\n",
      "15 Train Loss 4092.9048 Test MSE 3567.9863021955252 Test RE 1.0055432611464037 Lambda1 -8.451752e-05\n",
      "16 Train Loss 4007.5996 Test MSE 3560.6074664063403 Test RE 1.0045029574462654 Lambda1 0.00012040039\n",
      "17 Train Loss 3882.3152 Test MSE 3557.439528326174 Test RE 1.0040559956101438 Lambda1 0.00083709677\n",
      "18 Train Loss 3834.674 Test MSE 3557.9201920451164 Test RE 1.0041238248710562 Lambda1 -9.346324e-06\n",
      "19 Train Loss 3779.8281 Test MSE 3552.2242301650062 Test RE 1.0033197396671811 Lambda1 -0.0010496036\n",
      "20 Train Loss 3715.831 Test MSE 3546.56054021869 Test RE 1.00251957075717 Lambda1 -0.0014343676\n",
      "21 Train Loss 3671.3564 Test MSE 3542.3357926483923 Test RE 1.0019222802062893 Lambda1 -0.000111991205\n",
      "22 Train Loss 3615.6008 Test MSE 3530.773353030435 Test RE 1.0002857703860153 Lambda1 0.0010514621\n",
      "23 Train Loss 3581.0278 Test MSE 3516.76226954972 Test RE 0.9982990924516415 Lambda1 0.00040366675\n",
      "24 Train Loss 3531.5396 Test MSE 3474.062998500432 Test RE 0.9922200883648575 Lambda1 9.090986e-05\n",
      "25 Train Loss 3398.5605 Test MSE 3337.3995177784523 Test RE 0.9725081958819403 Lambda1 -0.0009748173\n",
      "26 Train Loss 3153.1667 Test MSE 3123.125833710276 Test RE 0.9407709794643746 Lambda1 -0.013338996\n",
      "27 Train Loss 2804.923 Test MSE 2731.799621772405 Test RE 0.879860033556634 Lambda1 -0.061978\n",
      "28 Train Loss 1694.4579 Test MSE 1685.961243733044 Test RE 0.6912146975510404 Lambda1 -0.23978074\n",
      "29 Train Loss 851.29553 Test MSE 862.452157190514 Test RE 0.4943752005617717 Lambda1 -0.4869102\n",
      "30 Train Loss 838.5508 Test MSE 858.3324324870557 Test RE 0.4931930319457515 Lambda1 -0.5081156\n",
      "31 Train Loss 838.0942 Test MSE 858.2880095514128 Test RE 0.49318026919499697 Lambda1 -0.5079183\n",
      "32 Train Loss 838.0656 Test MSE 858.2808966457561 Test RE 0.4931782256199058 Lambda1 -0.5075587\n",
      "33 Train Loss 838.0645 Test MSE 858.2813498889271 Test RE 0.4931783558392981 Lambda1 -0.5074868\n",
      "34 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "35 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "36 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "37 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "38 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "39 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "40 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "41 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "42 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "43 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "44 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "45 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "46 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "47 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "48 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "49 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "50 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "51 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "52 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "53 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "54 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "55 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "56 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "57 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "58 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "59 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "60 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "61 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "62 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "63 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "64 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "65 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "66 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "67 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "68 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "69 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "70 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "71 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "72 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "73 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "74 Train Loss 838.06433 Test MSE 858.2813941848405 Test RE 0.493178368565769 Lambda1 -0.50748104\n",
      "Training time: 154.57\n",
      "Training time: 154.57\n",
      "inv_HT_atanh_tune1\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 46592.195 Test MSE 3576.649958031945 Test RE 1.006763332927684 Lambda1 0.00037842029\n",
      "1 Train Loss 40224.383 Test MSE 3577.224862207667 Test RE 1.006844242325335 Lambda1 -0.0005501931\n",
      "2 Train Loss 36520.145 Test MSE 3575.5213530030333 Test RE 1.0066044792765876 Lambda1 -4.4466375e-05\n",
      "3 Train Loss 29314.367 Test MSE 3564.8243384528996 Test RE 1.0050976041573034 Lambda1 0.0006052573\n",
      "4 Train Loss 25389.734 Test MSE 3555.64869734582 Test RE 1.0038032406556572 Lambda1 4.5702505e-05\n",
      "5 Train Loss 21441.164 Test MSE 3558.8828903429626 Test RE 1.0042596630663614 Lambda1 0.00013511757\n",
      "6 Train Loss 19146.902 Test MSE 3564.1539637906394 Test RE 1.005003094080085 Lambda1 -0.000110860245\n",
      "7 Train Loss 16460.535 Test MSE 3563.162102744778 Test RE 1.0048632442320213 Lambda1 -0.00041931437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 14676.185 Test MSE 3564.284001877065 Test RE 1.0050214276718026 Lambda1 5.052025e-07\n",
      "9 Train Loss 13240.186 Test MSE 3565.1434322644377 Test RE 1.0051425871940936 Lambda1 -0.00021880076\n",
      "10 Train Loss 12467.437 Test MSE 3566.548632430438 Test RE 1.0053406559966205 Lambda1 0.0003160617\n",
      "11 Train Loss 11974.68 Test MSE 3567.4345711438827 Test RE 1.0054655126851006 Lambda1 0.0005202466\n",
      "12 Train Loss 11599.641 Test MSE 3568.9438826189844 Test RE 1.0056781865446038 Lambda1 0.00018828455\n",
      "13 Train Loss 10624.96 Test MSE 3572.3243680678315 Test RE 1.0061543604392709 Lambda1 -0.00015939573\n",
      "14 Train Loss 9839.82 Test MSE 3573.7832102530256 Test RE 1.0063597827988469 Lambda1 -0.0002092064\n",
      "15 Train Loss 9139.204 Test MSE 3573.0952036517583 Test RE 1.0062629084978942 Lambda1 3.4058696e-05\n",
      "16 Train Loss 8738.177 Test MSE 3572.3413420164993 Test RE 1.0061567508146145 Lambda1 0.00015009582\n",
      "17 Train Loss 8311.129 Test MSE 3572.7950174750476 Test RE 1.0062206380653025 Lambda1 -0.000117593125\n",
      "18 Train Loss 7792.847 Test MSE 3574.6449315015234 Test RE 1.0064811037195907 Lambda1 7.382998e-06\n",
      "19 Train Loss 7380.263 Test MSE 3576.302693730782 Test RE 1.0067144573799058 Lambda1 2.8531475e-05\n",
      "20 Train Loss 7017.831 Test MSE 3576.67526302765 Test RE 1.0067668943744368 Lambda1 -0.00012703754\n",
      "21 Train Loss 6659.8213 Test MSE 3576.4059863956963 Test RE 1.0067289955044973 Lambda1 -0.00018641274\n",
      "22 Train Loss 6423.3867 Test MSE 3577.3057295057715 Test RE 1.0068556226992946 Lambda1 8.0809245e-05\n",
      "23 Train Loss 6149.591 Test MSE 3577.3320933752684 Test RE 1.0068593328324702 Lambda1 9.0441936e-05\n",
      "24 Train Loss 5963.956 Test MSE 3577.47332561425 Test RE 1.0068792079224418 Lambda1 -0.00014772943\n",
      "25 Train Loss 5892.2607 Test MSE 3577.9761282303502 Test RE 1.0069499622882538 Lambda1 -0.0002099579\n",
      "26 Train Loss 5728.6226 Test MSE 3578.3609113608377 Test RE 1.0070041055983836 Lambda1 -0.00021104184\n",
      "27 Train Loss 5605.842 Test MSE 3578.2553916202464 Test RE 1.0069892580748963 Lambda1 -0.00025617678\n",
      "28 Train Loss 5561.868 Test MSE 3578.849503302914 Test RE 1.0070728517799423 Lambda1 -0.00011061206\n",
      "29 Train Loss 5443.992 Test MSE 3578.491810397555 Test RE 1.0070225239001147 Lambda1 0.0002839134\n",
      "30 Train Loss 5365.008 Test MSE 3578.527129477575 Test RE 1.00702749345478 Lambda1 0.0002391351\n",
      "31 Train Loss 5277.206 Test MSE 3578.694453820046 Test RE 1.007051036414979 Lambda1 0.00018751893\n",
      "32 Train Loss 5162.2173 Test MSE 3578.6634285373234 Test RE 1.0070466711204227 Lambda1 0.00032450713\n",
      "33 Train Loss 5111.7256 Test MSE 3578.840901252699 Test RE 1.0070716414892558 Lambda1 0.00037137663\n",
      "34 Train Loss 4984.16 Test MSE 3579.3433273796836 Test RE 1.007142329368557 Lambda1 0.00032684134\n",
      "35 Train Loss 4881.504 Test MSE 3578.6189391488983 Test RE 1.0070404113769937 Lambda1 0.0002830194\n",
      "36 Train Loss 4771.846 Test MSE 3578.21828644827 Test RE 1.006984037010107 Lambda1 0.00014209097\n",
      "37 Train Loss 4698.944 Test MSE 3578.150432986232 Test RE 1.0069744892866894 Lambda1 -7.6107185e-06\n",
      "38 Train Loss 4644.742 Test MSE 3577.251388379122 Test RE 1.0068479753410826 Lambda1 -9.466586e-05\n",
      "39 Train Loss 4583.8257 Test MSE 3576.9177994882853 Test RE 1.0068010285244462 Lambda1 -0.00019909318\n",
      "40 Train Loss 4520.6543 Test MSE 3578.104116870387 Test RE 1.0069679720511933 Lambda1 -4.230306e-05\n",
      "41 Train Loss 4451.206 Test MSE 3578.921102069737 Test RE 1.0070829255220195 Lambda1 0.00014290531\n",
      "42 Train Loss 4376.0015 Test MSE 3579.4518796163084 Test RE 1.0071576012653216 Lambda1 0.00013963718\n",
      "43 Train Loss 4355.1626 Test MSE 3579.5416882459117 Test RE 1.0071702360037753 Lambda1 -1.6751523e-05\n",
      "44 Train Loss 4307.9355 Test MSE 3580.0194202218013 Test RE 1.0072374431235476 Lambda1 -0.00023140649\n",
      "45 Train Loss 4279.3276 Test MSE 3579.6916557775908 Test RE 1.0071913338516911 Lambda1 -0.00033214924\n",
      "46 Train Loss 4239.751 Test MSE 3580.2807631255127 Test RE 1.0072742068283402 Lambda1 -6.230853e-05\n",
      "47 Train Loss 4191.458 Test MSE 3581.363291815326 Test RE 1.0074264743303598 Lambda1 0.00024930816\n",
      "48 Train Loss 4158.115 Test MSE 3581.78340297774 Test RE 1.0074855605862991 Lambda1 0.0002648734\n",
      "49 Train Loss 4132.7764 Test MSE 3582.259314928645 Test RE 1.0075524907212077 Lambda1 0.0002378981\n",
      "50 Train Loss 4112.5786 Test MSE 3582.531008566492 Test RE 1.007590698509527 Lambda1 0.00014508398\n",
      "51 Train Loss 4087.233 Test MSE 3582.8908432290464 Test RE 1.007641299181847 Lambda1 -6.783649e-05\n",
      "52 Train Loss 4070.5925 Test MSE 3582.942832346803 Test RE 1.0076486097858282 Lambda1 -0.00022889183\n",
      "53 Train Loss 4058.973 Test MSE 3582.785442864437 Test RE 1.0076264778342159 Lambda1 -0.00018488281\n",
      "54 Train Loss 4036.4292 Test MSE 3582.183455269016 Test RE 1.0075418224547685 Lambda1 8.642669e-05\n",
      "55 Train Loss 4014.1409 Test MSE 3581.6866236687556 Test RE 1.0074719494317375 Lambda1 0.00025276848\n",
      "56 Train Loss 3996.987 Test MSE 3581.7180481883056 Test RE 1.0074763690328716 Lambda1 0.00024890233\n",
      "57 Train Loss 3986.4507 Test MSE 3581.9238633761197 Test RE 1.0075053147729645 Lambda1 0.00015753148\n",
      "58 Train Loss 3976.9658 Test MSE 3581.5785551441604 Test RE 1.0074567503314438 Lambda1 3.9903574e-05\n",
      "59 Train Loss 3961.0283 Test MSE 3581.764869706799 Test RE 1.0074829540597567 Lambda1 -0.0001091014\n",
      "60 Train Loss 3945.6777 Test MSE 3582.1599848143087 Test RE 1.0075385217439397 Lambda1 -0.0002309941\n",
      "61 Train Loss 3931.6455 Test MSE 3581.6492438338755 Test RE 1.0074666922390612 Lambda1 -0.00021794118\n",
      "62 Train Loss 3917.6953 Test MSE 3581.6580694949375 Test RE 1.007467933504394 Lambda1 -5.2761105e-05\n",
      "63 Train Loss 3906.6157 Test MSE 3581.8817203845315 Test RE 1.0074993878724339 Lambda1 7.838797e-05\n",
      "64 Train Loss 3893.2764 Test MSE 3581.727068421703 Test RE 1.007477637651192 Lambda1 0.00027313313\n",
      "65 Train Loss 3884.825 Test MSE 3581.474112024197 Test RE 1.0074420609016683 Lambda1 0.00032890122\n",
      "66 Train Loss 3873.6802 Test MSE 3581.7792045532688 Test RE 1.0074849701188147 Lambda1 0.00035722536\n",
      "67 Train Loss 3865.9133 Test MSE 3581.747163998735 Test RE 1.0074804639156858 Lambda1 0.00034598177\n",
      "68 Train Loss 3858.5205 Test MSE 3581.801066570317 Test RE 1.0074880447946568 Lambda1 0.00034879224\n",
      "69 Train Loss 3847.3865 Test MSE 3581.843001866863 Test RE 1.0074939425518852 Lambda1 0.00028526533\n",
      "70 Train Loss 3836.9114 Test MSE 3581.153879809308 Test RE 1.0073970204233194 Lambda1 0.00028165485\n",
      "71 Train Loss 3829.7131 Test MSE 3580.643852396939 Test RE 1.0073252812007198 Lambda1 0.00030970213\n",
      "72 Train Loss 3819.64 Test MSE 3579.993942284701 Test RE 1.0072338590120014 Lambda1 0.0003251461\n",
      "73 Train Loss 3812.4297 Test MSE 3579.4231498143936 Test RE 1.0071535593760301 Lambda1 0.0002958322\n",
      "74 Train Loss 3805.397 Test MSE 3579.25582218794 Test RE 1.0071300183504144 Lambda1 0.00025567887\n",
      "Training time: 154.68\n",
      "Training time: 154.68\n",
      "inv_HT_atanh_tune1\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 44197.7 Test MSE 3525.3984324275852 Test RE 0.9995241094450037 Lambda1 -0.00015007544\n",
      "1 Train Loss 36453.082 Test MSE 3522.547497748145 Test RE 0.9991198779323754 Lambda1 -9.2209986e-05\n",
      "2 Train Loss 28397.168 Test MSE 3511.9871075565616 Test RE 0.9976211024676094 Lambda1 -2.9803361e-06\n",
      "3 Train Loss 24104.348 Test MSE 3507.9736185369393 Test RE 0.9970509002152695 Lambda1 -0.0002120292\n",
      "4 Train Loss 19303.514 Test MSE 3498.5876312239407 Test RE 0.9957161445537918 Lambda1 0.00055224786\n",
      "5 Train Loss 17855.852 Test MSE 3497.289444440551 Test RE 0.9955313920753815 Lambda1 -8.9793195e-07\n",
      "6 Train Loss 16144.197 Test MSE 3495.0476347147724 Test RE 0.9952122663982823 Lambda1 -0.0005962362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 14596.522 Test MSE 3491.6031337306113 Test RE 0.9947217359371834 Lambda1 9.1712944e-05\n",
      "8 Train Loss 11673.393 Test MSE 3481.969987803629 Test RE 0.9933485961976984 Lambda1 -0.00019674892\n",
      "9 Train Loss 10907.605 Test MSE 3482.535634314954 Test RE 0.9934292777303153 Lambda1 6.4824824e-05\n",
      "10 Train Loss 9342.118 Test MSE 3478.861621430439 Test RE 0.9929051143770657 Lambda1 0.00021000746\n",
      "11 Train Loss 8321.485 Test MSE 3472.366979904159 Test RE 0.9919778605816139 Lambda1 -0.00015749881\n",
      "12 Train Loss 7309.913 Test MSE 3465.365256389806 Test RE 0.9909772377266086 Lambda1 0.000224001\n",
      "13 Train Loss 6706.6475 Test MSE 3463.61770564958 Test RE 0.9907273360188976 Lambda1 0.00033507374\n",
      "14 Train Loss 6297.2324 Test MSE 3460.76143590509 Test RE 0.990318750489108 Lambda1 2.4793842e-05\n",
      "15 Train Loss 5863.5654 Test MSE 3461.658926978278 Test RE 0.9904471535353853 Lambda1 -0.0002690017\n",
      "16 Train Loss 5574.0947 Test MSE 3463.5695647975 Test RE 0.9907204509311198 Lambda1 -1.8505129e-05\n",
      "17 Train Loss 5198.0244 Test MSE 3463.3710779282637 Test RE 0.9906920629033528 Lambda1 6.2889034e-05\n",
      "18 Train Loss 4925.3364 Test MSE 3462.7699576577074 Test RE 0.9906060843999928 Lambda1 7.901842e-05\n",
      "19 Train Loss 4655.751 Test MSE 3465.6238916685284 Test RE 0.9910142175069601 Lambda1 0.00015218387\n",
      "20 Train Loss 4453.8325 Test MSE 3465.2109497199976 Test RE 0.9909551742371367 Lambda1 -3.8531045e-05\n",
      "21 Train Loss 4349.723 Test MSE 3463.1534071221918 Test RE 0.9906609302116137 Lambda1 -0.0001036779\n",
      "22 Train Loss 4229.561 Test MSE 3459.403454227387 Test RE 0.9901244339065751 Lambda1 -5.5049743e-05\n",
      "23 Train Loss 4140.078 Test MSE 3456.153578402979 Test RE 0.9896592471220977 Lambda1 4.8904723e-05\n",
      "24 Train Loss 4057.2761 Test MSE 3449.4763413132073 Test RE 0.9887027814878493 Lambda1 0.0002098269\n",
      "25 Train Loss 3972.9392 Test MSE 3446.5943875101166 Test RE 0.988289676590433 Lambda1 0.00023748082\n",
      "26 Train Loss 3911.2844 Test MSE 3444.7954972811795 Test RE 0.9880317325913116 Lambda1 4.317484e-06\n",
      "27 Train Loss 3841.6714 Test MSE 3440.2103328807466 Test RE 0.9873739582133665 Lambda1 9.535972e-05\n",
      "28 Train Loss 3806.4338 Test MSE 3438.033673375599 Test RE 0.9870615475228945 Lambda1 0.00037932012\n",
      "29 Train Loss 3773.733 Test MSE 3434.774348549171 Test RE 0.9865935595163665 Lambda1 0.0003207161\n",
      "30 Train Loss 3736.8037 Test MSE 3430.0047955505834 Test RE 0.985908326006249 Lambda1 9.7674034e-05\n",
      "31 Train Loss 3720.5034 Test MSE 3427.062469697661 Test RE 0.9854853694652749 Lambda1 0.00039122085\n",
      "32 Train Loss 3695.2876 Test MSE 3424.046361452525 Test RE 0.9850516182805741 Lambda1 0.00028123189\n",
      "33 Train Loss 3671.0337 Test MSE 3421.9865283438376 Test RE 0.9847552807062884 Lambda1 0.00018118549\n",
      "34 Train Loss 3655.13 Test MSE 3419.924892474179 Test RE 0.9844585944586507 Lambda1 0.0003337754\n",
      "35 Train Loss 3638.8462 Test MSE 3419.130287919393 Test RE 0.9843442204955628 Lambda1 0.00036429905\n",
      "36 Train Loss 3624.3936 Test MSE 3416.3865087029567 Test RE 0.9839491835940225 Lambda1 0.00029931014\n",
      "37 Train Loss 3605.517 Test MSE 3412.8595892595754 Test RE 0.9834411606277321 Lambda1 0.0003093921\n",
      "38 Train Loss 3586.2551 Test MSE 3409.1495743173086 Test RE 0.9829064809244134 Lambda1 0.00021014648\n",
      "39 Train Loss 3572.8591 Test MSE 3405.3032625433625 Test RE 0.9823518511511564 Lambda1 0.00018040986\n",
      "40 Train Loss 3561.1816 Test MSE 3402.7113739489346 Test RE 0.9819779297943982 Lambda1 0.00012928691\n",
      "41 Train Loss 3545.3809 Test MSE 3397.748701465509 Test RE 0.9812615873764351 Lambda1 0.00013014449\n",
      "42 Train Loss 3533.9053 Test MSE 3391.314552500599 Test RE 0.980332063808733 Lambda1 0.00012731458\n",
      "43 Train Loss 3522.805 Test MSE 3388.4719579102284 Test RE 0.9799211212712359 Lambda1 0.00020441393\n",
      "44 Train Loss 3507.8943 Test MSE 3383.7340355433025 Test RE 0.979235795505812 Lambda1 0.00021897473\n",
      "45 Train Loss 3492.7737 Test MSE 3377.800300664078 Test RE 0.9783768220888399 Lambda1 0.00020357387\n",
      "46 Train Loss 3476.2495 Test MSE 3370.6112957397067 Test RE 0.9773351231206626 Lambda1 0.00030670426\n",
      "47 Train Loss 3465.4575 Test MSE 3363.0540234000287 Test RE 0.9762388631642068 Lambda1 0.0003409385\n",
      "48 Train Loss 3449.4956 Test MSE 3349.4554800956985 Test RE 0.9742631480263226 Lambda1 0.00021312099\n",
      "49 Train Loss 3436.669 Test MSE 3338.7831517373256 Test RE 0.9727097683818927 Lambda1 0.00015365289\n",
      "50 Train Loss 3419.8088 Test MSE 3319.2436302983265 Test RE 0.9698593028472625 Lambda1 0.00024373553\n",
      "51 Train Loss 3393.7285 Test MSE 3278.4837599708844 Test RE 0.963886036849883 Lambda1 0.00024645551\n",
      "52 Train Loss 3372.8042 Test MSE 3260.2587776396194 Test RE 0.961203197968448 Lambda1 1.6943315e-05\n",
      "53 Train Loss 3337.8743 Test MSE 3228.344368259705 Test RE 0.9564870579404497 Lambda1 -5.213322e-05\n",
      "54 Train Loss 3315.707 Test MSE 3222.757360428352 Test RE 0.9556590460976295 Lambda1 -2.2004802e-05\n",
      "55 Train Loss 3294.9463 Test MSE 3207.251734162609 Test RE 0.9533572968663178 Lambda1 -8.775515e-05\n",
      "56 Train Loss 3266.675 Test MSE 3187.211585425194 Test RE 0.9503741571353831 Lambda1 3.9006343e-05\n",
      "57 Train Loss 3243.1162 Test MSE 3161.3013872374745 Test RE 0.9465032762600301 Lambda1 0.00019276574\n",
      "58 Train Loss 3196.4866 Test MSE 3109.7916887624424 Test RE 0.9387605265649057 Lambda1 0.00013026607\n",
      "59 Train Loss 3138.3428 Test MSE 3062.6691379909885 Test RE 0.9316208759995234 Lambda1 7.1009155e-05\n",
      "60 Train Loss 3093.2544 Test MSE 3018.681611041856 Test RE 0.9249064864723541 Lambda1 0.00011216212\n",
      "61 Train Loss 3061.534 Test MSE 2998.860590647806 Test RE 0.9218649628219684 Lambda1 6.873027e-05\n",
      "62 Train Loss 3026.8794 Test MSE 2970.2284116287246 Test RE 0.9174535693579807 Lambda1 4.8584232e-05\n",
      "63 Train Loss 2984.9216 Test MSE 2941.8001168132537 Test RE 0.9130525023781129 Lambda1 0.00018779121\n",
      "64 Train Loss 2861.3125 Test MSE 2806.2806104812425 Test RE 0.8917738198057478 Lambda1 1.1023461e-05\n",
      "65 Train Loss 2757.5515 Test MSE 2716.025894483427 Test RE 0.8773161492201444 Lambda1 -0.00036055248\n",
      "66 Train Loss 1331.4276 Test MSE 1320.747087278366 Test RE 0.6117852491383551 Lambda1 0.012725955\n",
      "67 Train Loss 895.4191 Test MSE 908.3131569864664 Test RE 0.5073491921006014 Lambda1 0.01613766\n",
      "68 Train Loss 838.2873 Test MSE 858.4388387745487 Test RE 0.4932236012282514 Lambda1 0.020034278\n",
      "69 Train Loss 838.02545 Test MSE 858.1442986749172 Test RE 0.49313897866370615 Lambda1 0.019974332\n",
      "70 Train Loss 837.68463 Test MSE 857.4218806575738 Test RE 0.49293136352716177 Lambda1 0.019573938\n",
      "71 Train Loss 837.51917 Test MSE 857.0924807291204 Test RE 0.4928366685133623 Lambda1 0.019000053\n",
      "72 Train Loss 836.8156 Test MSE 855.7181564726867 Test RE 0.49244138496106615 Lambda1 0.017831612\n",
      "73 Train Loss 835.10864 Test MSE 853.5695214106627 Test RE 0.49182275729846 Lambda1 0.014104216\n",
      "74 Train Loss 830.9736 Test MSE 839.8147564305833 Test RE 0.4878439476061587 Lambda1 0.00973054\n",
      "Training time: 156.46\n",
      "Training time: 156.46\n",
      "inv_HT_atanh_tune1\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 17050.72 Test MSE 3607.8246408872246 Test RE 1.0111413729978778 Lambda1 -0.00041836046\n",
      "1 Train Loss 11443.0 Test MSE 3606.585876956944 Test RE 1.010967767969498 Lambda1 0.00030584726\n",
      "2 Train Loss 10387.623 Test MSE 3606.642569237531 Test RE 1.0109757136895074 Lambda1 0.00015059912\n",
      "3 Train Loss 7417.2656 Test MSE 3615.1205957001216 Test RE 1.0121632513587109 Lambda1 -0.00082548475\n",
      "4 Train Loss 5625.458 Test MSE 3613.004250260236 Test RE 1.0118669402649398 Lambda1 1.2693767e-05\n",
      "5 Train Loss 4730.7964 Test MSE 3609.867216725136 Test RE 1.011427562170379 Lambda1 -1.2118719e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 4057.276 Test MSE 3603.2307028735163 Test RE 1.0104974114858927 Lambda1 0.0005089177\n",
      "7 Train Loss 3811.8274 Test MSE 3599.9873267151997 Test RE 1.0100425195650498 Lambda1 -0.0009609954\n",
      "8 Train Loss 3717.2961 Test MSE 3590.9655916399734 Test RE 1.0087761189714457 Lambda1 -0.00020156118\n",
      "9 Train Loss 3652.3398 Test MSE 3575.545859051498 Test RE 1.0066079288233045 Lambda1 0.0003152471\n",
      "10 Train Loss 3611.505 Test MSE 3551.145570706764 Test RE 1.0031673953292717 Lambda1 -0.00017094401\n",
      "11 Train Loss 3316.8079 Test MSE 3260.6211343866808 Test RE 0.9612566122540294 Lambda1 -0.00039244175\n",
      "12 Train Loss 2310.0547 Test MSE 2288.3473897802455 Test RE 0.805285998187392 Lambda1 0.0069568446\n",
      "13 Train Loss 838.41376 Test MSE 858.0856870700367 Test RE 0.4931221375796581 Lambda1 0.028752133\n",
      "14 Train Loss 838.0496 Test MSE 858.2271978888789 Test RE 0.4931627974122046 Lambda1 0.029077014\n",
      "15 Train Loss 837.84686 Test MSE 857.8194689993562 Test RE 0.4930456369400537 Lambda1 0.030366294\n",
      "16 Train Loss 837.6741 Test MSE 857.4114627325948 Test RE 0.4929283688884198 Lambda1 0.0376146\n",
      "17 Train Loss 837.2318 Test MSE 856.7436072598366 Test RE 0.49273635545364913 Lambda1 0.046602372\n",
      "18 Train Loss 834.62946 Test MSE 852.9811999544996 Test RE 0.4916532340498162 Lambda1 0.06958029\n",
      "19 Train Loss 825.33765 Test MSE 839.0822891264986 Test RE 0.48763115800816 Lambda1 0.0793667\n",
      "20 Train Loss 817.3401 Test MSE 824.4892390382491 Test RE 0.4833721968041974 Lambda1 0.058906857\n",
      "21 Train Loss 793.7017 Test MSE 809.4021805829852 Test RE 0.47892924319490665 Lambda1 0.008809045\n",
      "22 Train Loss 768.4862 Test MSE 788.9556661435009 Test RE 0.47284137373137186 Lambda1 -0.00059949706\n",
      "23 Train Loss 747.84357 Test MSE 773.366125542335 Test RE 0.4681464596331651 Lambda1 -0.0003841324\n",
      "24 Train Loss 741.5183 Test MSE 768.9012545088582 Test RE 0.4667931296300099 Lambda1 -0.00020599256\n",
      "25 Train Loss 737.41223 Test MSE 764.5362861471341 Test RE 0.46546627706824867 Lambda1 -0.00026108228\n",
      "26 Train Loss 731.1504 Test MSE 760.6421397566091 Test RE 0.464279343332836 Lambda1 -0.00012370879\n",
      "27 Train Loss 724.79205 Test MSE 755.5156484234079 Test RE 0.4627121491160255 Lambda1 -6.052864e-05\n",
      "28 Train Loss 720.15717 Test MSE 752.1829821693977 Test RE 0.4616904829959629 Lambda1 -2.8856011e-05\n",
      "29 Train Loss 712.6037 Test MSE 742.6329648245893 Test RE 0.4587502165501188 Lambda1 -2.782555e-05\n",
      "30 Train Loss 709.1282 Test MSE 739.7250835260193 Test RE 0.45785118589974616 Lambda1 -2.4403165e-05\n",
      "31 Train Loss 706.6277 Test MSE 737.3697384816578 Test RE 0.4571216869672319 Lambda1 -4.0982453e-05\n",
      "32 Train Loss 701.6294 Test MSE 734.0180788779772 Test RE 0.4560815973008245 Lambda1 -4.5627865e-05\n",
      "33 Train Loss 699.1158 Test MSE 731.8771186050759 Test RE 0.45541596952489344 Lambda1 -2.0034999e-05\n",
      "34 Train Loss 698.48944 Test MSE 731.6294943145356 Test RE 0.45533891998067105 Lambda1 -1.4430544e-05\n",
      "35 Train Loss 697.2918 Test MSE 732.0085829822636 Test RE 0.4554568700305345 Lambda1 -1.6929565e-05\n",
      "36 Train Loss 695.4699 Test MSE 731.3223370705316 Test RE 0.45524332833817305 Lambda1 -2.1648708e-05\n",
      "37 Train Loss 693.73486 Test MSE 730.2222936835925 Test RE 0.45490081461456094 Lambda1 -1.4621044e-05\n",
      "38 Train Loss 690.92004 Test MSE 728.4856451121512 Test RE 0.4543595594916887 Lambda1 -6.198862e-06\n",
      "39 Train Loss 689.7597 Test MSE 727.3486198834177 Test RE 0.4540048373167851 Lambda1 -8.070865e-06\n",
      "40 Train Loss 688.73175 Test MSE 726.0579857883541 Test RE 0.45360185704581096 Lambda1 -4.392987e-06\n",
      "41 Train Loss 685.5282 Test MSE 722.9475498641109 Test RE 0.45262919793344863 Lambda1 1.825247e-05\n",
      "42 Train Loss 682.6658 Test MSE 720.5294326317604 Test RE 0.4518715861067388 Lambda1 9.155394e-06\n",
      "43 Train Loss 680.1587 Test MSE 716.9275268252946 Test RE 0.4507407245513215 Lambda1 3.804328e-05\n",
      "44 Train Loss 678.80383 Test MSE 715.220041624133 Test RE 0.4502036466127756 Lambda1 6.7940935e-05\n",
      "45 Train Loss 673.85333 Test MSE 713.0980099783352 Test RE 0.4495352816326791 Lambda1 6.962016e-05\n",
      "46 Train Loss 672.06146 Test MSE 711.991451051913 Test RE 0.4491863601676769 Lambda1 5.659824e-05\n",
      "47 Train Loss 669.25256 Test MSE 708.7288931103388 Test RE 0.44815602531049964 Lambda1 4.16633e-05\n",
      "48 Train Loss 666.61346 Test MSE 708.1695948769241 Test RE 0.4479791577195809 Lambda1 2.17115e-05\n",
      "49 Train Loss 664.73035 Test MSE 708.1475284180952 Test RE 0.4479721781833802 Lambda1 3.2484247e-06\n",
      "50 Train Loss 664.03674 Test MSE 708.0378481964208 Test RE 0.4479374851367482 Lambda1 2.7638705e-06\n",
      "51 Train Loss 663.64764 Test MSE 707.961028008406 Test RE 0.44791318447589323 Lambda1 1.3846967e-06\n",
      "52 Train Loss 663.30945 Test MSE 707.3700226597625 Test RE 0.44772618664184916 Lambda1 4.09102e-07\n",
      "53 Train Loss 662.62366 Test MSE 706.5787314454666 Test RE 0.4474756947349958 Lambda1 1.0385083e-06\n",
      "54 Train Loss 662.13434 Test MSE 706.1657081626295 Test RE 0.44734489197484717 Lambda1 1.6584537e-06\n",
      "55 Train Loss 661.93933 Test MSE 705.9357402270863 Test RE 0.44727204550485283 Lambda1 2.7398332e-06\n",
      "56 Train Loss 661.2599 Test MSE 705.3958268840681 Test RE 0.447100971621684 Lambda1 1.7376161e-06\n",
      "57 Train Loss 660.91425 Test MSE 705.2354709151298 Test RE 0.44705014952873284 Lambda1 -1.0904587e-06\n",
      "58 Train Loss 660.6673 Test MSE 705.0998916124821 Test RE 0.4470071754701682 Lambda1 -1.4108848e-06\n",
      "59 Train Loss 660.45416 Test MSE 704.7293279775873 Test RE 0.44688969823396835 Lambda1 -9.0800825e-08\n",
      "60 Train Loss 660.24854 Test MSE 704.4632877958472 Test RE 0.44680533829999036 Lambda1 -7.4281485e-07\n",
      "61 Train Loss 659.9443 Test MSE 704.4326587807797 Test RE 0.44679562497891845 Lambda1 -7.9403185e-07\n",
      "62 Train Loss 659.3345 Test MSE 704.5972815730363 Test RE 0.4468478290084304 Lambda1 -1.5336623e-07\n",
      "63 Train Loss 658.94324 Test MSE 704.5092954346715 Test RE 0.4468199282178518 Lambda1 -8.9913465e-07\n",
      "64 Train Loss 658.75574 Test MSE 704.2571005758684 Test RE 0.4467399464661323 Lambda1 -9.2058605e-07\n",
      "65 Train Loss 658.5474 Test MSE 704.1677174503545 Test RE 0.4467115958256139 Lambda1 -1.1627344e-06\n",
      "66 Train Loss 658.12177 Test MSE 704.2353842927516 Test RE 0.44673305863643287 Lambda1 -1.7587294e-06\n",
      "67 Train Loss 657.5811 Test MSE 703.4906369328519 Test RE 0.446496780198451 Lambda1 -1.1413302e-06\n",
      "68 Train Loss 656.7362 Test MSE 702.6349591721735 Test RE 0.4462251535464611 Lambda1 -2.9318405e-06\n",
      "69 Train Loss 655.1883 Test MSE 701.4911508251372 Test RE 0.44586180417544113 Lambda1 -3.638843e-06\n",
      "70 Train Loss 654.14026 Test MSE 700.751597992291 Test RE 0.44562671545540794 Lambda1 -1.994596e-06\n",
      "71 Train Loss 652.9954 Test MSE 699.9975746423147 Test RE 0.4453868991013034 Lambda1 -2.742934e-06\n",
      "72 Train Loss 652.3014 Test MSE 699.6100875375113 Test RE 0.4452636089812551 Lambda1 -2.5497839e-06\n",
      "73 Train Loss 651.51404 Test MSE 699.2523082307234 Test RE 0.44514974092773574 Lambda1 -4.2619813e-06\n",
      "74 Train Loss 650.8614 Test MSE 699.0113470560252 Test RE 0.44507303539129894 Lambda1 -2.9358687e-06\n",
      "Training time: 156.28\n",
      "Training time: 156.28\n",
      "inv_HT_atanh_tune1\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 25273.613 Test MSE 3393.044016568379 Test RE 0.9805820012520516 Lambda1 -0.00066620903\n",
      "1 Train Loss 15992.35 Test MSE 3377.253711533888 Test RE 0.9782976593609994 Lambda1 0.00035850971\n",
      "2 Train Loss 10781.804 Test MSE 3368.49844694197 Test RE 0.9770287565802217 Lambda1 6.6185053e-06\n",
      "3 Train Loss 5800.8525 Test MSE 3364.780022182864 Test RE 0.9764893454549345 Lambda1 -0.0004283046\n",
      "4 Train Loss 3967.1165 Test MSE 3382.115963151142 Test RE 0.9790016364591382 Lambda1 7.6317665e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 3486.918 Test MSE 3367.128242723152 Test RE 0.9768300233595212 Lambda1 0.0001905501\n",
      "6 Train Loss 3329.835 Test MSE 3291.5348961161058 Test RE 0.9658026718107553 Lambda1 -0.0022683796\n",
      "7 Train Loss 2693.2202 Test MSE 2671.940977074465 Test RE 0.8701669832355745 Lambda1 -0.04466494\n",
      "8 Train Loss 879.08386 Test MSE 864.251451451752 Test RE 0.49489062799310546 Lambda1 -0.40132487\n",
      "9 Train Loss 838.0678 Test MSE 858.3624733975499 Test RE 0.49320166254010744 Lambda1 -0.42132926\n",
      "10 Train Loss 838.0532 Test MSE 858.2295182298998 Test RE 0.4931634640801327 Lambda1 -0.42045432\n",
      "11 Train Loss 837.9893 Test MSE 858.101460476785 Test RE 0.49312666986613335 Lambda1 -0.4184089\n",
      "12 Train Loss 837.9228 Test MSE 858.1001007747664 Test RE 0.49312627917480767 Lambda1 -0.40051052\n",
      "13 Train Loss 837.85944 Test MSE 857.7002722727791 Test RE 0.4930113806251812 Lambda1 -0.3915649\n",
      "14 Train Loss 837.7609 Test MSE 857.6154332786741 Test RE 0.4929869970349388 Lambda1 -0.37671462\n",
      "15 Train Loss 837.7224 Test MSE 857.5917960824816 Test RE 0.4929802032493965 Lambda1 -0.3672533\n",
      "16 Train Loss 837.63947 Test MSE 857.2931643967341 Test RE 0.492894362684992 Lambda1 -0.38020942\n",
      "17 Train Loss 837.46497 Test MSE 857.1143791462499 Test RE 0.4928429643765581 Lambda1 -0.3935067\n",
      "18 Train Loss 837.0388 Test MSE 856.578639065158 Test RE 0.4926889143407838 Lambda1 -0.35872307\n",
      "19 Train Loss 836.6374 Test MSE 855.8909715666172 Test RE 0.49249110752880965 Lambda1 -0.36365753\n",
      "20 Train Loss 835.4974 Test MSE 853.8599336812019 Test RE 0.49190641727761714 Lambda1 -0.36266443\n",
      "21 Train Loss 834.44995 Test MSE 851.9510515727892 Test RE 0.4913562586920942 Lambda1 -0.39000103\n",
      "22 Train Loss 833.2567 Test MSE 850.7723320159339 Test RE 0.4910162322917035 Lambda1 -0.39154413\n",
      "23 Train Loss 830.4523 Test MSE 845.8753565144357 Test RE 0.4896010684567082 Lambda1 -0.41879272\n",
      "24 Train Loss 829.07806 Test MSE 843.6625944974871 Test RE 0.4889602647752445 Lambda1 -0.41750494\n",
      "25 Train Loss 828.22797 Test MSE 842.7390414027863 Test RE 0.4886925603504306 Lambda1 -0.42249086\n",
      "26 Train Loss 826.98004 Test MSE 839.6920285920339 Test RE 0.4878083003278052 Lambda1 -0.42783767\n",
      "27 Train Loss 825.64386 Test MSE 838.9292697421048 Test RE 0.4875866925112873 Lambda1 -0.43037117\n",
      "28 Train Loss 823.87036 Test MSE 836.3744748536404 Test RE 0.48684370150289 Lambda1 -0.43372092\n",
      "29 Train Loss 821.3391 Test MSE 833.2520141481522 Test RE 0.48593407798305055 Lambda1 -0.42768046\n",
      "30 Train Loss 818.5689 Test MSE 832.2442700846987 Test RE 0.4856401420999816 Lambda1 -0.42182055\n",
      "31 Train Loss 817.2146 Test MSE 830.7929911801301 Test RE 0.4852165243914343 Lambda1 -0.42813528\n",
      "32 Train Loss 815.3239 Test MSE 828.8626823541679 Test RE 0.4846525075849838 Lambda1 -0.4395811\n",
      "33 Train Loss 813.11414 Test MSE 826.8539195204211 Test RE 0.4840648695262105 Lambda1 -0.4573609\n",
      "34 Train Loss 811.8162 Test MSE 824.2308565037898 Test RE 0.48329645007885197 Lambda1 -0.47192723\n",
      "35 Train Loss 810.0239 Test MSE 822.2700748041193 Test RE 0.48272124527508975 Lambda1 -0.4776066\n",
      "36 Train Loss 806.6404 Test MSE 818.7186318193147 Test RE 0.48167766351980634 Lambda1 -0.48723003\n",
      "37 Train Loss 802.6414 Test MSE 811.1359583542951 Test RE 0.4794419133516419 Lambda1 -0.5179966\n",
      "38 Train Loss 796.6321 Test MSE 808.3928905698006 Test RE 0.4786305478763749 Lambda1 -0.5294022\n",
      "39 Train Loss 793.7291 Test MSE 804.0339839846013 Test RE 0.47733840029987123 Lambda1 -0.55923295\n",
      "40 Train Loss 790.1195 Test MSE 800.6713894935353 Test RE 0.4763392029928914 Lambda1 -0.5750986\n",
      "41 Train Loss 786.7946 Test MSE 797.4803280483471 Test RE 0.47538903465805643 Lambda1 -0.5916249\n",
      "42 Train Loss 784.64276 Test MSE 792.6233764876654 Test RE 0.4739391757982427 Lambda1 -0.619355\n",
      "43 Train Loss 783.7164 Test MSE 791.1742394071882 Test RE 0.4735057309503476 Lambda1 -0.6217047\n",
      "44 Train Loss 780.6264 Test MSE 787.1616572215439 Test RE 0.47230346998327033 Lambda1 -0.63713527\n",
      "45 Train Loss 779.2722 Test MSE 786.6078199545177 Test RE 0.47213728728889975 Lambda1 -0.6466822\n",
      "46 Train Loss 777.3405 Test MSE 785.580817683824 Test RE 0.47182897326942963 Lambda1 -0.65123117\n",
      "47 Train Loss 775.48114 Test MSE 781.6019748208603 Test RE 0.470632586825963 Lambda1 -0.6766215\n",
      "48 Train Loss 773.9707 Test MSE 777.8999332460918 Test RE 0.46951669315302624 Lambda1 -0.7002096\n",
      "49 Train Loss 772.5626 Test MSE 776.3360895762208 Test RE 0.4690445115317435 Lambda1 -0.70626044\n",
      "50 Train Loss 770.339 Test MSE 773.1118459081933 Test RE 0.4680694909807162 Lambda1 -0.7117479\n",
      "51 Train Loss 769.0316 Test MSE 770.6897975238227 Test RE 0.4673357185818042 Lambda1 -0.726803\n",
      "52 Train Loss 767.8723 Test MSE 767.3261084844834 Test RE 0.46631475592116195 Lambda1 -0.73833954\n",
      "53 Train Loss 766.9998 Test MSE 764.9673160001771 Test RE 0.4655974687427681 Lambda1 -0.74826574\n",
      "54 Train Loss 764.8325 Test MSE 763.3830914021995 Test RE 0.4651150996036823 Lambda1 -0.7573282\n",
      "55 Train Loss 762.77075 Test MSE 763.2900546354789 Test RE 0.46508675595548754 Lambda1 -0.7592627\n",
      "56 Train Loss 761.373 Test MSE 761.4605812225789 Test RE 0.464529055573697 Lambda1 -0.7691251\n",
      "57 Train Loss 758.45746 Test MSE 758.7388256127904 Test RE 0.4636981088225328 Lambda1 -0.7812075\n",
      "58 Train Loss 755.9332 Test MSE 757.4693274527617 Test RE 0.4633100238084201 Lambda1 -0.7872505\n",
      "59 Train Loss 754.5785 Test MSE 755.9643139329801 Test RE 0.462849520307686 Lambda1 -0.7975195\n",
      "60 Train Loss 752.9036 Test MSE 753.8154247381898 Test RE 0.46219120870661545 Lambda1 -0.80447036\n",
      "61 Train Loss 750.8219 Test MSE 751.1993511690096 Test RE 0.4613885075144406 Lambda1 -0.8091079\n",
      "62 Train Loss 749.7616 Test MSE 749.0646476743219 Test RE 0.46073247101911263 Lambda1 -0.8192089\n",
      "63 Train Loss 749.25964 Test MSE 748.0700090842197 Test RE 0.4604264797253461 Lambda1 -0.8264416\n",
      "64 Train Loss 748.5972 Test MSE 747.9677798158622 Test RE 0.4603950183181406 Lambda1 -0.829909\n",
      "65 Train Loss 746.9613 Test MSE 748.1883872024056 Test RE 0.4604629083105605 Lambda1 -0.8313661\n",
      "66 Train Loss 745.39404 Test MSE 746.2185654385456 Test RE 0.4598563580984792 Lambda1 -0.83955455\n",
      "67 Train Loss 743.80817 Test MSE 746.3364583657956 Test RE 0.45989268235626357 Lambda1 -0.8410868\n",
      "68 Train Loss 743.3967 Test MSE 747.2174039977783 Test RE 0.4601640217520429 Lambda1 -0.840633\n",
      "69 Train Loss 742.6607 Test MSE 746.9406569581038 Test RE 0.46007879834593796 Lambda1 -0.84238684\n",
      "70 Train Loss 739.75543 Test MSE 745.6627486988095 Test RE 0.45968506547319 Lambda1 -0.8532603\n",
      "71 Train Loss 737.4539 Test MSE 745.3840451954486 Test RE 0.4595991500831609 Lambda1 -0.8685634\n",
      "72 Train Loss 735.9607 Test MSE 744.2927856477611 Test RE 0.4592625949438094 Lambda1 -0.88797385\n",
      "73 Train Loss 735.14465 Test MSE 742.7912131438738 Test RE 0.4587990916924706 Lambda1 -0.8916171\n",
      "74 Train Loss 733.5384 Test MSE 740.5946914070137 Test RE 0.4581202277791795 Lambda1 -0.8897317\n",
      "Training time: 163.54\n",
      "Training time: 163.54\n",
      "inv_HT_atanh_tune1\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 25250.023 Test MSE 3672.9961251399004 Test RE 1.0202330911620399 Lambda1 -0.00028593285\n",
      "1 Train Loss 18071.072 Test MSE 3677.01892675553 Test RE 1.0207916368337315 Lambda1 0.0017043173\n",
      "2 Train Loss 12438.47 Test MSE 3689.317476042071 Test RE 1.0224973359952012 Lambda1 -0.0018820381\n",
      "3 Train Loss 8935.227 Test MSE 3688.023622864706 Test RE 1.0223180240428604 Lambda1 0.0009072141\n",
      "4 Train Loss 6851.537 Test MSE 3680.3020550851675 Test RE 1.0212472562154529 Lambda1 -8.399112e-05\n",
      "5 Train Loss 5082.228 Test MSE 3683.7210038032085 Test RE 1.0217215082485338 Lambda1 -8.442506e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 4275.1055 Test MSE 3685.151183473243 Test RE 1.0219198272208319 Lambda1 0.0002786557\n",
      "7 Train Loss 3974.7944 Test MSE 3683.076580919753 Test RE 1.02163213536742 Lambda1 7.439579e-05\n",
      "8 Train Loss 3856.8733 Test MSE 3672.251259610346 Test RE 1.0201296367929669 Lambda1 -0.00018669806\n",
      "9 Train Loss 3791.3306 Test MSE 3663.169604222116 Test RE 1.0188674409192098 Lambda1 0.00019115904\n",
      "10 Train Loss 3732.4775 Test MSE 3637.640097622101 Test RE 1.0153108677835156 Lambda1 -6.8926485e-05\n",
      "11 Train Loss 3695.2844 Test MSE 3611.403817604424 Test RE 1.0116428048697088 Lambda1 -0.0007125659\n",
      "12 Train Loss 3625.4182 Test MSE 3534.2309462226144 Test RE 1.000775427270525 Lambda1 -0.0020595102\n",
      "13 Train Loss 3535.9778 Test MSE 3455.3169042165846 Test RE 0.9895394502952063 Lambda1 0.001182391\n",
      "14 Train Loss 3427.329 Test MSE 3358.9200512886373 Test RE 0.9756386668567487 Lambda1 0.0062156515\n",
      "15 Train Loss 3239.9302 Test MSE 3149.7690313395265 Test RE 0.9447752876414817 Lambda1 0.00016385189\n",
      "16 Train Loss 2854.274 Test MSE 2819.555857465323 Test RE 0.8938806208887381 Lambda1 0.0028342134\n",
      "17 Train Loss 886.6546 Test MSE 899.8241816529695 Test RE 0.5049728173255775 Lambda1 0.06547227\n",
      "18 Train Loss 838.07697 Test MSE 858.1617192510298 Test RE 0.4931439840698639 Lambda1 0.07686199\n",
      "19 Train Loss 837.9434 Test MSE 858.2987892123264 Test RE 0.4931833662319586 Lambda1 0.07618487\n",
      "20 Train Loss 837.56793 Test MSE 857.2705131583721 Test RE 0.4928878510610175 Lambda1 0.070876256\n",
      "21 Train Loss 835.5374 Test MSE 854.1631002451647 Test RE 0.4919937362496836 Lambda1 0.04906519\n",
      "22 Train Loss 824.8642 Test MSE 834.1339449003335 Test RE 0.4861911711915938 Lambda1 0.028773906\n",
      "23 Train Loss 800.3306 Test MSE 810.0835651114994 Test RE 0.47913079091314226 Lambda1 -0.0022465943\n",
      "24 Train Loss 777.21564 Test MSE 791.9370439238135 Test RE 0.473733939399247 Lambda1 0.001469815\n",
      "25 Train Loss 763.6297 Test MSE 780.1271313978052 Test RE 0.4701883472101815 Lambda1 0.0003421968\n",
      "26 Train Loss 745.1623 Test MSE 767.598499655218 Test RE 0.46639751652989303 Lambda1 0.0003448091\n",
      "27 Train Loss 737.34076 Test MSE 761.232017424666 Test RE 0.4644593326645499 Lambda1 0.0007121219\n",
      "28 Train Loss 727.84174 Test MSE 754.5200569725423 Test RE 0.46240717586678837 Lambda1 0.0005191812\n",
      "29 Train Loss 725.1013 Test MSE 752.6103818637916 Test RE 0.461821633496009 Lambda1 0.00026059683\n",
      "30 Train Loss 720.86725 Test MSE 749.9410878010268 Test RE 0.4610019313241336 Lambda1 -0.00012867176\n",
      "31 Train Loss 708.2753 Test MSE 730.1087580988518 Test RE 0.4548654490566098 Lambda1 -0.00015186062\n",
      "32 Train Loss 699.1397 Test MSE 724.0269026919348 Test RE 0.45296695714719837 Lambda1 -0.00017999129\n",
      "33 Train Loss 689.29443 Test MSE 718.5452585151573 Test RE 0.4512489814160579 Lambda1 -0.00028665113\n",
      "34 Train Loss 685.2851 Test MSE 716.7555629600856 Test RE 0.45068666346226005 Lambda1 -5.7539975e-05\n",
      "35 Train Loss 678.3741 Test MSE 711.3429179928164 Test RE 0.4489817379274142 Lambda1 3.1159914e-05\n",
      "36 Train Loss 673.41235 Test MSE 705.442234536804 Test RE 0.44711567865891333 Lambda1 9.947357e-05\n",
      "37 Train Loss 671.86334 Test MSE 705.7198251804747 Test RE 0.4472036397389087 Lambda1 6.136934e-05\n",
      "38 Train Loss 668.41473 Test MSE 703.7707346551332 Test RE 0.44658565862731026 Lambda1 5.8930418e-05\n",
      "39 Train Loss 661.5682 Test MSE 700.0223783233238 Test RE 0.4453947899405799 Lambda1 3.512436e-05\n",
      "40 Train Loss 657.6475 Test MSE 698.1263529886066 Test RE 0.4447912003765995 Lambda1 3.705843e-05\n",
      "41 Train Loss 656.19794 Test MSE 698.4556059354725 Test RE 0.4448960750527409 Lambda1 1.5872096e-05\n",
      "42 Train Loss 655.401 Test MSE 698.2063711788657 Test RE 0.44481669029468207 Lambda1 1.4928157e-06\n",
      "43 Train Loss 654.7697 Test MSE 697.4259206106028 Test RE 0.444568014172628 Lambda1 -8.020149e-06\n",
      "44 Train Loss 652.0226 Test MSE 695.3979767517022 Test RE 0.44392119615201825 Lambda1 -2.570799e-05\n",
      "45 Train Loss 650.94995 Test MSE 694.3657771545746 Test RE 0.4435916111826533 Lambda1 -2.9167124e-05\n",
      "46 Train Loss 650.2877 Test MSE 693.5791998979131 Test RE 0.44334028979417384 Lambda1 -2.4631789e-05\n",
      "47 Train Loss 649.6221 Test MSE 694.6354841273858 Test RE 0.44367775319823727 Lambda1 -1.2185207e-05\n",
      "48 Train Loss 648.60114 Test MSE 694.4186960445875 Test RE 0.4436085143259359 Lambda1 -1.6678936e-05\n",
      "49 Train Loss 647.81586 Test MSE 693.3800929501491 Test RE 0.4432766500051264 Lambda1 -1.8588855e-05\n",
      "50 Train Loss 645.9587 Test MSE 687.8592321695314 Test RE 0.4415083849332724 Lambda1 -2.034453e-05\n",
      "51 Train Loss 644.9716 Test MSE 687.2531119807901 Test RE 0.4413138203255357 Lambda1 -2.2748629e-05\n",
      "52 Train Loss 644.72046 Test MSE 687.5739046975464 Test RE 0.4414168054851365 Lambda1 -1.5944332e-05\n",
      "53 Train Loss 644.544 Test MSE 687.7285055881639 Test RE 0.4414664289474982 Lambda1 -1.5246418e-05\n",
      "54 Train Loss 644.35266 Test MSE 687.8222568897842 Test RE 0.44149651832258474 Lambda1 -5.1223265e-06\n",
      "55 Train Loss 644.11945 Test MSE 687.4883233349224 Test RE 0.4413893333639377 Lambda1 2.9605337e-06\n",
      "56 Train Loss 643.8896 Test MSE 687.3449328971228 Test RE 0.44134330035645125 Lambda1 3.2596233e-06\n",
      "57 Train Loss 643.75037 Test MSE 687.3590280046108 Test RE 0.44134782555847746 Lambda1 8.4351e-06\n",
      "58 Train Loss 643.65015 Test MSE 687.3652895356556 Test RE 0.4413498357938168 Lambda1 1.188836e-05\n",
      "59 Train Loss 643.572 Test MSE 687.3418076886601 Test RE 0.44134229700913674 Lambda1 1.2354802e-05\n",
      "60 Train Loss 643.49243 Test MSE 687.1826216824503 Test RE 0.44129118736830614 Lambda1 1.6618464e-05\n",
      "61 Train Loss 643.3653 Test MSE 686.8913508685923 Test RE 0.44119765410477785 Lambda1 2.5268639e-05\n",
      "62 Train Loss 643.1536 Test MSE 686.9901282659114 Test RE 0.44122937585326194 Lambda1 2.3927003e-05\n",
      "63 Train Loss 642.9528 Test MSE 687.0010177265194 Test RE 0.44123287279642726 Lambda1 1.7063281e-05\n",
      "64 Train Loss 642.5143 Test MSE 686.5159718494012 Test RE 0.441077082652286 Lambda1 -7.3370677e-07\n",
      "65 Train Loss 642.16876 Test MSE 685.4417912466414 Test RE 0.4407318744222389 Lambda1 -3.4190463e-05\n",
      "66 Train Loss 641.9042 Test MSE 684.6705842362662 Test RE 0.4404838656375663 Lambda1 -5.6998706e-05\n",
      "67 Train Loss 641.7464 Test MSE 684.9305848421947 Test RE 0.4405674935923643 Lambda1 -3.4608478e-05\n",
      "68 Train Loss 641.6863 Test MSE 685.1852501703561 Test RE 0.44064939009449533 Lambda1 -2.0941414e-05\n",
      "69 Train Loss 641.4799 Test MSE 684.8711464554873 Test RE 0.4405483769185152 Lambda1 -2.4537769e-05\n",
      "70 Train Loss 641.14386 Test MSE 683.9498860401457 Test RE 0.4402519734555183 Lambda1 -2.081486e-05\n",
      "71 Train Loss 640.5336 Test MSE 683.7770761011909 Test RE 0.440196351892818 Lambda1 4.246174e-06\n",
      "72 Train Loss 640.1743 Test MSE 683.5535774878443 Test RE 0.4401244049686206 Lambda1 -6.1445803e-06\n",
      "73 Train Loss 639.73157 Test MSE 682.7062898519368 Test RE 0.43985154589086795 Lambda1 -4.957521e-05\n",
      "74 Train Loss 638.8798 Test MSE 682.5530753056913 Test RE 0.43980218686379174 Lambda1 -5.6678884e-05\n",
      "Training time: 160.67\n",
      "Training time: 160.67\n",
      "inv_HT_atanh_tune2\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.84\n",
      "Training time: 2.84\n",
      "inv_HT_atanh_tune2\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.89\n",
      "Training time: 2.89\n",
      "inv_HT_atanh_tune2\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 705516.1 Test MSE 3558.3803975985497 Test RE 1.004188762867648 Lambda1 5.3569056e-07\n",
      "1 Train Loss 666043.0 Test MSE 3558.379223211608 Test RE 1.0041885971593572 Lambda1 7.5995267e-06\n",
      "2 Train Loss 652453.8 Test MSE 3558.348464818377 Test RE 1.004184257080271 Lambda1 2.5129413e-05\n",
      "3 Train Loss 640595.0 Test MSE 3558.225821466499 Test RE 1.0041669516384883 Lambda1 3.2226355e-05\n",
      "4 Train Loss 632086.6 Test MSE 3558.202400129488 Test RE 1.004163646765151 Lambda1 2.6315149e-05\n",
      "5 Train Loss 621283.8 Test MSE 3558.3409314699197 Test RE 1.004183194104848 Lambda1 2.4227433e-05\n",
      "6 Train Loss 615016.8 Test MSE 3558.5220864415483 Test RE 1.00420875523942 Lambda1 2.2727061e-05\n",
      "7 Train Loss 605563.56 Test MSE 3558.8356691047684 Test RE 1.0042530005062928 Lambda1 9.546427e-07\n",
      "8 Train Loss 597194.2 Test MSE 3559.0551574258516 Test RE 1.0042839682759568 Lambda1 -6.48199e-06\n",
      "9 Train Loss 585115.0 Test MSE 3559.480824824126 Test RE 1.0043440232826437 Lambda1 -7.6713095e-06\n",
      "10 Train Loss 581704.5 Test MSE 3559.479479134537 Test RE 1.0043438334325656 Lambda1 1.3499532e-07\n",
      "11 Train Loss 579736.5 Test MSE 3559.4874091244346 Test RE 1.0043449521961467 Lambda1 7.631421e-06\n",
      "12 Train Loss 574194.5 Test MSE 3559.533844090502 Test RE 1.0043515032196955 Lambda1 1.5045268e-05\n",
      "13 Train Loss 567224.56 Test MSE 3559.6041037376344 Test RE 1.0043614153369973 Lambda1 1.10485935e-05\n",
      "14 Train Loss 561287.0 Test MSE 3559.6258340864792 Test RE 1.004364480999635 Lambda1 1.3228744e-05\n",
      "15 Train Loss 555935.75 Test MSE 3559.6002336170814 Test RE 1.0043608693491994 Lambda1 1.17599175e-05\n",
      "16 Train Loss 549765.7 Test MSE 3559.635695663721 Test RE 1.004365872242916 Lambda1 -8.144248e-06\n",
      "17 Train Loss 543469.1 Test MSE 3559.805930682158 Test RE 1.0043898882115767 Lambda1 -9.527981e-06\n",
      "18 Train Loss 536455.5 Test MSE 3559.891713445218 Test RE 1.004401989829253 Lambda1 -7.343806e-06\n",
      "19 Train Loss 531022.94 Test MSE 3559.9642831781707 Test RE 1.0044122273334504 Lambda1 2.0817188e-06\n",
      "20 Train Loss 527652.8 Test MSE 3560.0732012627886 Test RE 1.0044275923498533 Lambda1 6.1890264e-06\n",
      "21 Train Loss 521695.06 Test MSE 3560.2096841634684 Test RE 1.004446845588441 Lambda1 -1.0490601e-05\n",
      "22 Train Loss 516831.78 Test MSE 3560.2678736524276 Test RE 1.0044550540951014 Lambda1 -3.0029621e-05\n",
      "23 Train Loss 514269.25 Test MSE 3560.382501215409 Test RE 1.004471223848659 Lambda1 -4.3836335e-05\n",
      "24 Train Loss 512258.53 Test MSE 3560.484821055271 Test RE 1.0044856572130985 Lambda1 -5.143356e-05\n",
      "25 Train Loss 510023.62 Test MSE 3560.59551182383 Test RE 1.0045012711576646 Lambda1 -6.22232e-05\n",
      "26 Train Loss 508426.6 Test MSE 3560.64363247823 Test RE 1.004508058940079 Lambda1 -7.217734e-05\n",
      "27 Train Loss 506896.78 Test MSE 3560.6851312040008 Test RE 1.0045139126125209 Lambda1 -7.224645e-05\n",
      "28 Train Loss 503742.75 Test MSE 3560.718666434652 Test RE 1.0045186429558666 Lambda1 -7.285372e-05\n",
      "29 Train Loss 501312.34 Test MSE 3560.791599478919 Test RE 1.004528930517784 Lambda1 -7.078888e-05\n",
      "30 Train Loss 498261.0 Test MSE 3560.9450982296735 Test RE 1.0045505819216798 Lambda1 -6.301126e-05\n",
      "31 Train Loss 496705.38 Test MSE 3560.9768363732 Test RE 1.004555058612366 Lambda1 -6.21275e-05\n",
      "32 Train Loss 492871.6 Test MSE 3561.2408150956153 Test RE 1.004592292250606 Lambda1 -6.015559e-05\n",
      "33 Train Loss 490109.53 Test MSE 3561.341927940408 Test RE 1.0046065536371285 Lambda1 -6.156436e-05\n",
      "34 Train Loss 488322.25 Test MSE 3561.406641381277 Test RE 1.0046156809903553 Lambda1 -6.3681946e-05\n",
      "35 Train Loss 485292.25 Test MSE 3561.7875537564382 Test RE 1.0046694041890885 Lambda1 -6.362323e-05\n",
      "36 Train Loss 482763.4 Test MSE 3562.1000318761967 Test RE 1.0047134733982257 Lambda1 -4.8113183e-05\n",
      "37 Train Loss 479589.3 Test MSE 3562.4096714977063 Test RE 1.004757140383498 Lambda1 -2.428424e-05\n",
      "38 Train Loss 477424.7 Test MSE 3562.453562289922 Test RE 1.004763329937074 Lambda1 -1.5484227e-05\n",
      "39 Train Loss 475020.12 Test MSE 3562.5055937457064 Test RE 1.0047706674513703 Lambda1 -1.2027667e-05\n",
      "40 Train Loss 473558.8 Test MSE 3562.596402725681 Test RE 1.004783473272383 Lambda1 -1.6378832e-05\n",
      "41 Train Loss 471412.1 Test MSE 3562.693077046903 Test RE 1.004797106040498 Lambda1 -3.883689e-05\n",
      "42 Train Loss 469477.9 Test MSE 3562.6608482824495 Test RE 1.0047925612422524 Lambda1 -6.095725e-05\n",
      "43 Train Loss 467521.3 Test MSE 3562.7224283745527 Test RE 1.0048012450538368 Lambda1 -6.386077e-05\n",
      "44 Train Loss 466950.9 Test MSE 3562.785054611896 Test RE 1.004810076312234 Lambda1 -5.6864374e-05\n",
      "45 Train Loss 465756.72 Test MSE 3562.8581979804517 Test RE 1.0048203905489064 Lambda1 -3.7856164e-05\n",
      "46 Train Loss 464931.97 Test MSE 3562.869992741867 Test RE 1.0048220537651087 Lambda1 -4.0575396e-05\n",
      "47 Train Loss 464067.38 Test MSE 3562.85641638986 Test RE 1.004820139321005 Lambda1 -5.3325635e-05\n",
      "48 Train Loss 461635.9 Test MSE 3562.944469809519 Test RE 1.0048325559458149 Lambda1 -6.225493e-05\n",
      "49 Train Loss 459671.53 Test MSE 3563.0722765167056 Test RE 1.0048505780044912 Lambda1 -5.5163517e-05\n",
      "50 Train Loss 457038.0 Test MSE 3563.2783459514735 Test RE 1.004879635231051 Lambda1 -4.5610304e-05\n",
      "51 Train Loss 455192.06 Test MSE 3563.418278516566 Test RE 1.0048993662190382 Lambda1 -5.0375038e-05\n",
      "52 Train Loss 453716.53 Test MSE 3563.3770775980724 Test RE 1.0048935567836415 Lambda1 -6.649846e-05\n",
      "53 Train Loss 452765.72 Test MSE 3563.418204316069 Test RE 1.0048993557566075 Lambda1 -7.156614e-05\n",
      "54 Train Loss 451798.06 Test MSE 3563.480598024716 Test RE 1.0049081533660336 Lambda1 -7.198642e-05\n",
      "55 Train Loss 449797.84 Test MSE 3563.4513304765633 Test RE 1.0049040266060654 Lambda1 -7.192079e-05\n",
      "56 Train Loss 447368.4 Test MSE 3563.3879238503546 Test RE 1.0048950861363455 Lambda1 -7.300924e-05\n",
      "57 Train Loss 445598.03 Test MSE 3563.40801811152 Test RE 1.004897919478697 Lambda1 -5.6846853e-05\n",
      "58 Train Loss 444295.1 Test MSE 3563.494292008471 Test RE 1.0049100842286167 Lambda1 -3.4535675e-05\n",
      "59 Train Loss 442065.5 Test MSE 3563.605068191103 Test RE 1.004925703621939 Lambda1 -6.1305573e-06\n",
      "60 Train Loss 439512.94 Test MSE 3563.6512352168843 Test RE 1.00493221308072 Lambda1 1.1041231e-05\n",
      "61 Train Loss 436102.94 Test MSE 3563.8091155726856 Test RE 1.0049544735781226 Lambda1 1.8953253e-05\n",
      "62 Train Loss 432780.47 Test MSE 3563.893405047333 Test RE 1.0049663578543309 Lambda1 2.5113017e-05\n",
      "63 Train Loss 430363.47 Test MSE 3563.8143277800264 Test RE 1.0049552084700597 Lambda1 4.6614336e-05\n",
      "64 Train Loss 428022.38 Test MSE 3563.85272937508 Test RE 1.004960622862541 Lambda1 5.501788e-05\n",
      "65 Train Loss 424364.62 Test MSE 3564.1402698861616 Test RE 1.005001163407374 Lambda1 3.8611168e-05\n",
      "66 Train Loss 421475.66 Test MSE 3564.2076862891086 Test RE 1.0050106682581759 Lambda1 4.0788666e-05\n",
      "67 Train Loss 419876.1 Test MSE 3564.1821503210126 Test RE 1.0050070680237637 Lambda1 5.1284707e-05\n",
      "68 Train Loss 415676.2 Test MSE 3564.260122674673 Test RE 1.005018061055918 Lambda1 3.080991e-05\n",
      "69 Train Loss 412469.38 Test MSE 3564.3034021659128 Test RE 1.0050241628185326 Lambda1 2.5182333e-05\n",
      "70 Train Loss 410212.28 Test MSE 3564.316870836086 Test RE 1.005026061692541 Lambda1 2.3999557e-05\n",
      "71 Train Loss 407887.84 Test MSE 3564.3218165148346 Test RE 1.0050267589557884 Lambda1 3.696488e-05\n",
      "72 Train Loss 406274.28 Test MSE 3564.4346823940555 Test RE 1.005042671145633 Lambda1 3.8535065e-05\n",
      "73 Train Loss 404803.2 Test MSE 3564.5246362572952 Test RE 1.005055352946348 Lambda1 3.0218642e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 Train Loss 402667.88 Test MSE 3564.5893425173394 Test RE 1.0050644752115208 Lambda1 9.448002e-06\n",
      "Training time: 157.30\n",
      "Training time: 157.30\n",
      "inv_HT_atanh_tune2\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.89\n",
      "Training time: 2.89\n",
      "inv_HT_atanh_tune2\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 428780.97 Test MSE 3518.3429117977753 Test RE 0.9985234147592146 Lambda1 -8.956781e-06\n",
      "1 Train Loss 419601.75 Test MSE 3518.3657019747066 Test RE 0.9985266487373988 Lambda1 -9.149936e-06\n",
      "2 Train Loss 410664.03 Test MSE 3518.3034621606007 Test RE 0.9985178167407964 Lambda1 8.29715e-06\n",
      "3 Train Loss 398319.7 Test MSE 3518.0946697891272 Test RE 0.9984881879725088 Lambda1 3.2034306e-05\n",
      "4 Train Loss 392368.53 Test MSE 3518.045483581061 Test RE 0.9984812080552479 Lambda1 1.0658038e-05\n",
      "5 Train Loss 386105.53 Test MSE 3517.8913362997578 Test RE 0.9984593330041176 Lambda1 -9.551e-06\n",
      "6 Train Loss 382238.97 Test MSE 3517.835079685767 Test RE 0.9984513495048457 Lambda1 -9.687351e-06\n",
      "7 Train Loss 375937.78 Test MSE 3517.819621450291 Test RE 0.9984491557816214 Lambda1 -1.7322871e-05\n",
      "8 Train Loss 373314.2 Test MSE 3517.817338154469 Test RE 0.9984488317520518 Lambda1 1.8845247e-07\n",
      "9 Train Loss 369303.97 Test MSE 3517.7234899116247 Test RE 0.9984355133667601 Lambda1 3.8577135e-05\n",
      "10 Train Loss 366073.56 Test MSE 3517.6319825349055 Test RE 0.9984225270122201 Lambda1 4.28227e-05\n",
      "11 Train Loss 364264.78 Test MSE 3517.6366583614486 Test RE 0.998423190590591 Lambda1 2.8274384e-05\n",
      "12 Train Loss 362181.03 Test MSE 3517.664640761565 Test RE 0.9984271617542382 Lambda1 3.152207e-05\n",
      "13 Train Loss 357811.25 Test MSE 3517.611532080454 Test RE 0.998419624743849 Lambda1 3.2113774e-05\n",
      "14 Train Loss 352385.12 Test MSE 3517.435664683919 Test RE 0.9983946658117424 Lambda1 3.1121937e-05\n",
      "15 Train Loss 348270.25 Test MSE 3517.3768260916063 Test RE 0.9983863153559028 Lambda1 4.2315838e-05\n",
      "16 Train Loss 343159.4 Test MSE 3517.2981409849135 Test RE 0.9983751481456523 Lambda1 2.477759e-05\n",
      "17 Train Loss 339337.3 Test MSE 3517.119921315712 Test RE 0.9983498542502429 Lambda1 1.1522457e-05\n",
      "18 Train Loss 336179.34 Test MSE 3516.891328881774 Test RE 0.9983174102425747 Lambda1 2.484609e-05\n",
      "19 Train Loss 334314.84 Test MSE 3516.7001099190547 Test RE 0.9982902698227103 Lambda1 3.987209e-05\n",
      "20 Train Loss 328441.84 Test MSE 3516.2448730879196 Test RE 0.9982256533935087 Lambda1 -9.867991e-06\n",
      "21 Train Loss 324376.34 Test MSE 3515.917976525436 Test RE 0.9981792510337445 Lambda1 1.07643355e-05\n",
      "22 Train Loss 320484.53 Test MSE 3515.8600807686635 Test RE 0.9981710326137717 Lambda1 3.804791e-05\n",
      "23 Train Loss 317169.5 Test MSE 3515.9020756666177 Test RE 0.9981769938813594 Lambda1 2.6543195e-05\n",
      "24 Train Loss 315336.25 Test MSE 3516.006842560499 Test RE 0.9981918656158272 Lambda1 -1.4409987e-07\n",
      "25 Train Loss 314127.5 Test MSE 3516.1013348461597 Test RE 0.9982052786723745 Lambda1 -3.4046438e-06\n",
      "26 Train Loss 313273.66 Test MSE 3516.1473096616396 Test RE 0.9982118046722123 Lambda1 1.00815305e-05\n",
      "27 Train Loss 311987.72 Test MSE 3516.1715177322394 Test RE 0.9982152409247523 Lambda1 2.9437377e-05\n",
      "28 Train Loss 310971.8 Test MSE 3516.142114192596 Test RE 0.9982110671916704 Lambda1 2.8087035e-05\n",
      "29 Train Loss 309804.62 Test MSE 3516.010538800945 Test RE 0.9981923902957187 Lambda1 2.629052e-05\n",
      "30 Train Loss 307632.16 Test MSE 3515.790370532848 Test RE 0.9981611370147744 Lambda1 3.9973033e-06\n",
      "31 Train Loss 305466.06 Test MSE 3515.412764695808 Test RE 0.9981075329097508 Lambda1 -6.0762613e-07\n",
      "32 Train Loss 304204.97 Test MSE 3515.131505553224 Test RE 0.9980676041015301 Lambda1 2.5943782e-05\n",
      "33 Train Loss 303336.53 Test MSE 3514.9225270256857 Test RE 0.9980379355389098 Lambda1 3.0613544e-05\n",
      "34 Train Loss 302247.4 Test MSE 3514.8485791841263 Test RE 0.9980274369946699 Lambda1 1.3171129e-05\n",
      "35 Train Loss 301259.03 Test MSE 3514.8605456084265 Test RE 0.9980291359027934 Lambda1 -2.334094e-05\n",
      "36 Train Loss 299955.38 Test MSE 3514.7168651856214 Test RE 0.9980087369834288 Lambda1 -3.242515e-05\n",
      "37 Train Loss 298922.0 Test MSE 3514.622579264266 Test RE 0.9979953505844702 Lambda1 -1.5647647e-05\n",
      "38 Train Loss 297921.94 Test MSE 3514.6124493916823 Test RE 0.997993912368446 Lambda1 6.201797e-06\n",
      "39 Train Loss 296988.03 Test MSE 3514.566456245885 Test RE 0.9979873823412603 Lambda1 1.5131914e-05\n",
      "40 Train Loss 295381.34 Test MSE 3514.407634539757 Test RE 0.9979648327819574 Lambda1 3.6603735e-05\n",
      "41 Train Loss 294544.16 Test MSE 3514.4081769628897 Test RE 0.9979649097962433 Lambda1 2.9600233e-05\n",
      "42 Train Loss 293574.66 Test MSE 3514.4245309665935 Test RE 0.9979672317665469 Lambda1 2.8129003e-05\n",
      "43 Train Loss 292611.25 Test MSE 3514.4027882741048 Test RE 0.9979641446995178 Lambda1 2.0966216e-05\n",
      "44 Train Loss 291922.8 Test MSE 3514.402181603445 Test RE 0.997964058563177 Lambda1 8.56881e-06\n",
      "45 Train Loss 291350.78 Test MSE 3514.411795154509 Test RE 0.9979654235139394 Lambda1 1.4104735e-06\n",
      "46 Train Loss 291031.53 Test MSE 3514.402915597268 Test RE 0.9979641627771197 Lambda1 6.0327043e-06\n",
      "47 Train Loss 289974.72 Test MSE 3514.3332973300985 Test RE 0.9979542781848499 Lambda1 -2.3645343e-06\n",
      "48 Train Loss 288994.38 Test MSE 3514.1783956629706 Test RE 0.9979322844705691 Lambda1 -2.4610695e-06\n",
      "49 Train Loss 288036.38 Test MSE 3514.1184857155636 Test RE 0.9979237780262797 Lambda1 9.946108e-06\n",
      "50 Train Loss 287264.16 Test MSE 3514.213658806142 Test RE 0.9979372913508098 Lambda1 5.2843143e-06\n",
      "51 Train Loss 286702.78 Test MSE 3514.132938630445 Test RE 0.9979258301614953 Lambda1 -9.56074e-06\n",
      "52 Train Loss 285854.7 Test MSE 3513.9275170886717 Test RE 0.9978966624461486 Lambda1 -2.040766e-05\n",
      "53 Train Loss 285005.5 Test MSE 3513.8688168752014 Test RE 0.9978883274717563 Lambda1 -1.6325659e-05\n",
      "54 Train Loss 284534.7 Test MSE 3513.958157988248 Test RE 0.9979010131882481 Lambda1 -1.0440043e-05\n",
      "55 Train Loss 284132.6 Test MSE 3513.9869562070653 Test RE 0.9979051022683628 Lambda1 -1.1473027e-05\n",
      "56 Train Loss 283106.03 Test MSE 3513.922752428104 Test RE 0.997895985903951 Lambda1 -1.1636133e-05\n",
      "57 Train Loss 282472.66 Test MSE 3513.8478051579023 Test RE 0.9978853439541766 Lambda1 -6.160959e-06\n",
      "58 Train Loss 282249.88 Test MSE 3513.814846631176 Test RE 0.9978806640548578 Lambda1 -5.26266e-06\n",
      "59 Train Loss 281853.9 Test MSE 3513.7725509458883 Test RE 0.9978746583067787 Lambda1 -5.640621e-06\n",
      "60 Train Loss 281307.03 Test MSE 3513.68766758425 Test RE 0.9978626052404779 Lambda1 -7.3158265e-07\n",
      "61 Train Loss 280966.97 Test MSE 3513.6435522379325 Test RE 0.997856340996648 Lambda1 -4.4676983e-07\n",
      "62 Train Loss 280647.6 Test MSE 3513.6181972693107 Test RE 0.9978527406510975 Lambda1 9.538853e-07\n",
      "63 Train Loss 280227.75 Test MSE 3513.606410313846 Test RE 0.9978510669268881 Lambda1 4.9583505e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 Train Loss 279781.16 Test MSE 3513.6222611603766 Test RE 0.9978533177148872 Lambda1 8.991841e-06\n",
      "65 Train Loss 279436.03 Test MSE 3513.5573824661446 Test RE 0.9978441050399994 Lambda1 3.4163052e-06\n",
      "66 Train Loss 279133.6 Test MSE 3513.4598748930102 Test RE 0.9978302589544639 Lambda1 6.4022e-07\n",
      "67 Train Loss 278713.5 Test MSE 3513.396459499639 Test RE 0.9978212538588923 Lambda1 4.236948e-06\n",
      "68 Train Loss 278517.7 Test MSE 3513.4416458568594 Test RE 0.9978276704081158 Lambda1 5.5323635e-06\n",
      "69 Train Loss 277775.4 Test MSE 3513.4320901063643 Test RE 0.9978263134766936 Lambda1 3.0989331e-06\n",
      "70 Train Loss 277251.84 Test MSE 3513.4987686444642 Test RE 0.9978357818942529 Lambda1 -9.982217e-06\n",
      "71 Train Loss 277020.66 Test MSE 3513.514190905563 Test RE 0.997837971857604 Lambda1 -1.0780878e-05\n",
      "72 Train Loss 276742.6 Test MSE 3513.4910350466835 Test RE 0.997834683720418 Lambda1 -9.807655e-06\n",
      "73 Train Loss 276537.0 Test MSE 3513.548182319486 Test RE 0.9978427986264248 Lambda1 -1.3044439e-05\n",
      "74 Train Loss 276201.1 Test MSE 3513.5713350278133 Test RE 0.9978460862894958 Lambda1 -1.7210014e-05\n",
      "Training time: 160.66\n",
      "Training time: 160.66\n",
      "inv_HT_atanh_tune2\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.79\n",
      "Training time: 2.79\n",
      "inv_HT_atanh_tune2\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.95\n",
      "Training time: 2.95\n",
      "inv_HT_atanh_tune2\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.86\n",
      "Training time: 2.86\n",
      "inv_HT_atanh_tune2\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.99\n",
      "Training time: 2.99\n",
      "inv_HT_atanh_tune2\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.84\n",
      "Training time: 2.84\n",
      "inv_HT_atanh_tune3\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartlab/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio5.py:493: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  narr = np.asanyarray(source)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.71\n",
      "Training time: 2.71\n",
      "inv_HT_atanh_tune3\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.77\n",
      "Training time: 2.77\n",
      "inv_HT_atanh_tune3\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.89\n",
      "Training time: 2.89\n",
      "inv_HT_atanh_tune3\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.02\n",
      "Training time: 3.02\n",
      "inv_HT_atanh_tune3\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.80\n",
      "Training time: 2.80\n",
      "inv_HT_atanh_tune3\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.82\n",
      "Training time: 2.82\n",
      "inv_HT_atanh_tune3\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.80\n",
      "Training time: 2.80\n",
      "inv_HT_atanh_tune3\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.85\n",
      "Training time: 2.85\n",
      "inv_HT_atanh_tune3\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.96\n",
      "Training time: 2.96\n",
      "inv_HT_atanh_tune3\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.87\n",
      "Training time: 2.87\n",
      "inv_HT_atanh_tune4\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 1118437000.0 Test MSE 3552.4578826708494 Test RE 1.0033527365025883 Lambda1 8.2819423e-07\n",
      "1 Train Loss 1048875400.0 Test MSE 3552.455354678734 Test RE 1.003352379500712 Lambda1 7.247904e-07\n",
      "2 Train Loss 1004769000.0 Test MSE 3552.461174964162 Test RE 1.0033532014385353 Lambda1 7.0557985e-07\n",
      "3 Train Loss 1000228540.0 Test MSE 3552.462500065724 Test RE 1.0033533885686268 Lambda1 7.447323e-07\n",
      "4 Train Loss 974555140.0 Test MSE 3552.4690792476654 Test RE 1.0033543176764552 Lambda1 8.723312e-07\n",
      "5 Train Loss 954390900.0 Test MSE 3552.468784183014 Test RE 1.0033542760076342 Lambda1 8.169977e-07\n",
      "6 Train Loss 936516800.0 Test MSE 3552.4662508405117 Test RE 1.0033539182507276 Lambda1 9.549516e-07\n",
      "7 Train Loss 925130940.0 Test MSE 3552.461098457401 Test RE 1.003353190634293 Lambda1 1.0443057e-06\n",
      "8 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "9 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "10 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "11 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "12 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "13 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "14 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "15 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "16 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "17 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "18 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "19 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "20 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "21 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "22 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "23 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "24 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "25 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "26 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "27 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "28 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "29 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "30 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "31 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "32 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "33 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "34 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "35 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "36 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "37 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "38 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "39 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "40 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "41 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "42 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "43 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "44 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "45 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "46 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "47 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "48 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "49 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "50 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "51 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "52 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "53 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "54 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "55 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "56 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "57 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "58 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "59 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "60 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "61 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "62 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "63 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "64 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "65 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "66 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "67 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "68 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "69 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "70 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "71 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "72 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "73 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "74 Train Loss 919935500.0 Test MSE 3552.456712725388 Test RE 1.0033525712834477 Lambda1 1.0287315e-06\n",
      "Training time: 135.26\n",
      "Training time: 135.26\n",
      "inv_HT_atanh_tune4\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.72\n",
      "Training time: 2.72\n",
      "inv_HT_atanh_tune4\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.04\n",
      "Training time: 3.04\n",
      "inv_HT_atanh_tune4\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.86\n",
      "Training time: 2.86\n",
      "inv_HT_atanh_tune4\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.88\n",
      "Training time: 2.88\n",
      "inv_HT_atanh_tune4\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.97\n",
      "Training time: 2.97\n",
      "inv_HT_atanh_tune4\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.90\n",
      "Training time: 2.90\n",
      "inv_HT_atanh_tune4\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.92\n",
      "Training time: 2.92\n",
      "inv_HT_atanh_tune4\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.98\n",
      "Training time: 2.98\n",
      "inv_HT_atanh_tune4\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.11\n",
      "Training time: 3.11\n",
      "inv_HT_atanh_tune5\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 921.1525 Test MSE 932.5886911208561 Test RE 0.5140841842060212 Lambda1 -0.046965398\n",
      "1 Train Loss 846.8001 Test MSE 864.1719069705388 Test RE 0.4948678529523288 Lambda1 -0.05164742\n",
      "2 Train Loss 839.0681 Test MSE 858.3222000592903 Test RE 0.4931900921890073 Lambda1 -0.052996952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Train Loss 838.18317 Test MSE 858.0692037052445 Test RE 0.49311740125131776 Lambda1 -0.053441953\n",
      "4 Train Loss 838.0664 Test MSE 858.2288201613495 Test RE 0.4931632635149183 Lambda1 -0.053635404\n",
      "5 Train Loss 838.0628 Test MSE 858.2737211859669 Test RE 0.49317616406410475 Lambda1 -0.05366911\n",
      "6 Train Loss 838.06274 Test MSE 858.2769062527184 Test RE 0.493177079155355 Lambda1 -0.05367135\n",
      "7 Train Loss 838.0627 Test MSE 858.2797615711937 Test RE 0.4931778995062491 Lambda1 -0.053673312\n",
      "8 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "9 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "10 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "11 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "12 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "13 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "14 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "15 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "16 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "17 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "18 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "19 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "20 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "21 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "22 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "23 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "24 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "25 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "26 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "27 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "28 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "29 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "30 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "31 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "32 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "33 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "34 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "35 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "36 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "37 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "38 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "39 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "40 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "41 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "42 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "43 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "44 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "45 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "46 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "47 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "48 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "49 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "50 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "51 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "52 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "53 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "54 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "55 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "56 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "57 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "58 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "59 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "60 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "61 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "62 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "63 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "64 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "65 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "66 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "67 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "68 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "69 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "70 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "71 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "72 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "73 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "74 Train Loss 838.0626 Test MSE 858.2807235200116 Test RE 0.4931781758798662 Lambda1 -0.05367394\n",
      "Training time: 137.55\n",
      "Training time: 137.55\n",
      "inv_HT_atanh_tune5\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 995.5014 Test MSE 1003.6297914151074 Test RE 0.5333053528563096 Lambda1 -0.06597094\n",
      "1 Train Loss 853.4048 Test MSE 869.8500525990007 Test RE 0.49649098549500115 Lambda1 -0.07555175\n",
      "2 Train Loss 839.8969 Test MSE 858.8118680495363 Test RE 0.493330753217898 Lambda1 -0.078218944\n",
      "3 Train Loss 838.2854 Test MSE 858.051305321943 Test RE 0.49311225828018684 Lambda1 -0.079126894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 838.07025 Test MSE 858.2111039487099 Test RE 0.4931581733628944 Lambda1 -0.07952532\n",
      "5 Train Loss 838.0636 Test MSE 858.2739943793011 Test RE 0.49317624255446657 Lambda1 -0.07959903\n",
      "6 Train Loss 838.06354 Test MSE 858.2777193737065 Test RE 0.4931773127702503 Lambda1 -0.079603\n",
      "7 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "8 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "9 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "10 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "11 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "12 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "13 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "14 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "15 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "16 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "17 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "18 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "19 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "20 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "21 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "22 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "23 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "24 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "25 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "26 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "27 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "28 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "29 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "30 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "31 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "32 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "33 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "34 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "35 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "36 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "37 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "38 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "39 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "40 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "41 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "42 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "43 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "44 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "45 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "46 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "47 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "48 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "49 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "50 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "51 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "52 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "53 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "54 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "55 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "56 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "57 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "58 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "59 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "60 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "61 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "62 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "63 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "64 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "65 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "66 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "67 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "68 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "69 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "70 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "71 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "72 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "73 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "74 Train Loss 838.0635 Test MSE 858.2799470096778 Test RE 0.4931779527838407 Lambda1 -0.079605386\n",
      "Training time: 118.99\n",
      "Training time: 118.99\n",
      "inv_HT_atanh_tune5\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 1143.8633 Test MSE 1147.226803277347 Test RE 0.5701824072280072 Lambda1 -0.19199435\n",
      "1 Train Loss 857.8336 Test MSE 873.7667755665436 Test RE 0.4976075190399792 Lambda1 -0.24346597\n",
      "2 Train Loss 838.0557 Test MSE 858.2681930949477 Test RE 0.49317457580208157 Lambda1 -0.25901178\n",
      "3 Train Loss 838.02954 Test MSE 858.0455293952517 Test RE 0.4931105985979336 Lambda1 -0.25895107\n",
      "4 Train Loss 837.86 Test MSE 857.8489474440435 Test RE 0.4930541084740998 Lambda1 -0.26499647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 837.7009 Test MSE 857.7995704988664 Test RE 0.4930399184141295 Lambda1 -0.3332915\n",
      "6 Train Loss 837.1709 Test MSE 855.9518912630106 Test RE 0.4925086342208142 Lambda1 -0.44830906\n",
      "7 Train Loss 833.12946 Test MSE 850.8623379049405 Test RE 0.49104220468292836 Lambda1 -0.5284259\n",
      "8 Train Loss 827.0876 Test MSE 841.7017821782878 Test RE 0.48839172170814443 Lambda1 -0.60340065\n",
      "9 Train Loss 819.2699 Test MSE 830.7800655562345 Test RE 0.48521274983461754 Lambda1 -0.7636483\n",
      "10 Train Loss 811.60004 Test MSE 825.1385401510872 Test RE 0.48356249179121935 Lambda1 -0.9785369\n",
      "11 Train Loss 801.9469 Test MSE 812.7634483946855 Test RE 0.4799226563914935 Lambda1 -1.0290216\n",
      "12 Train Loss 794.1912 Test MSE 807.2421616704501 Test RE 0.47828976670847 Lambda1 -1.0313808\n",
      "13 Train Loss 779.50134 Test MSE 784.9680539642862 Test RE 0.4716449208642183 Lambda1 -1.1302761\n",
      "14 Train Loss 766.6065 Test MSE 772.2004713547243 Test RE 0.4677935202922522 Lambda1 -1.2024287\n",
      "15 Train Loss 761.56213 Test MSE 767.2520197045129 Test RE 0.46629224298633254 Lambda1 -1.2295096\n",
      "16 Train Loss 757.7594 Test MSE 763.7936190722268 Test RE 0.46524014622587345 Lambda1 -1.2342027\n",
      "17 Train Loss 754.8253 Test MSE 758.6332711847114 Test RE 0.4636658532627515 Lambda1 -1.2514617\n",
      "18 Train Loss 750.96796 Test MSE 753.7920222953542 Test RE 0.46218403421340604 Lambda1 -1.2572938\n",
      "19 Train Loss 747.3864 Test MSE 750.3767940947126 Test RE 0.4611358300263971 Lambda1 -1.2635568\n",
      "20 Train Loss 743.2066 Test MSE 740.6381889429176 Test RE 0.45813368102742724 Lambda1 -1.3065684\n",
      "21 Train Loss 736.1824 Test MSE 736.6728323398148 Test RE 0.45690561746880665 Lambda1 -1.2897661\n",
      "22 Train Loss 727.89874 Test MSE 724.673891939572 Test RE 0.45316929721799604 Lambda1 -1.3085126\n",
      "23 Train Loss 719.6007 Test MSE 716.3327667002998 Test RE 0.4505537194338071 Lambda1 -1.3071856\n",
      "24 Train Loss 713.4797 Test MSE 711.3427244173997 Test RE 0.4489816768374456 Lambda1 -1.2955233\n",
      "25 Train Loss 710.1592 Test MSE 706.2231286418062 Test RE 0.4473630790912788 Lambda1 -1.3080602\n",
      "26 Train Loss 707.4886 Test MSE 698.4164005235478 Test RE 0.44488358851922966 Lambda1 -1.3317038\n",
      "27 Train Loss 703.977 Test MSE 697.6794339155423 Test RE 0.4446488067436721 Lambda1 -1.3359658\n",
      "28 Train Loss 701.90955 Test MSE 691.8820535350327 Test RE 0.4427975441337283 Lambda1 -1.338938\n",
      "29 Train Loss 699.4108 Test MSE 690.7519779995386 Test RE 0.4424357778728594 Lambda1 -1.3289046\n",
      "30 Train Loss 697.9454 Test MSE 690.2711197531257 Test RE 0.44228175302922484 Lambda1 -1.3060453\n",
      "31 Train Loss 693.37177 Test MSE 683.2320240801367 Test RE 0.4400208723747746 Lambda1 -1.3173168\n",
      "32 Train Loss 692.55273 Test MSE 681.9469965583537 Test RE 0.4396068804716534 Lambda1 -1.3216319\n",
      "33 Train Loss 691.08826 Test MSE 680.1645905187069 Test RE 0.4390320039565612 Lambda1 -1.328213\n",
      "34 Train Loss 687.3435 Test MSE 672.9304987878812 Test RE 0.43669103541803067 Lambda1 -1.3443304\n",
      "35 Train Loss 685.33563 Test MSE 668.9014862130194 Test RE 0.43538178049631693 Lambda1 -1.3596188\n",
      "36 Train Loss 683.7752 Test MSE 664.337982602478 Test RE 0.4338940676047366 Lambda1 -1.3778498\n",
      "37 Train Loss 681.0827 Test MSE 659.644978161495 Test RE 0.43235879810512284 Lambda1 -1.4046113\n",
      "38 Train Loss 679.40344 Test MSE 657.8692144624222 Test RE 0.43177645117017605 Lambda1 -1.4133512\n",
      "39 Train Loss 677.5325 Test MSE 659.039264895884 Test RE 0.43216024766184125 Lambda1 -1.4102519\n",
      "40 Train Loss 675.3905 Test MSE 657.4163900681334 Test RE 0.4316278254538954 Lambda1 -1.4253786\n",
      "41 Train Loss 673.51465 Test MSE 657.6292370076036 Test RE 0.4316976923055247 Lambda1 -1.428376\n",
      "42 Train Loss 671.1383 Test MSE 655.8283083650864 Test RE 0.4311061813941863 Lambda1 -1.4394145\n",
      "43 Train Loss 669.7182 Test MSE 654.5363749817079 Test RE 0.4306813483825005 Lambda1 -1.4266886\n",
      "44 Train Loss 667.384 Test MSE 653.3231101305611 Test RE 0.43028200242512293 Lambda1 -1.410603\n",
      "45 Train Loss 665.6087 Test MSE 651.9989928070088 Test RE 0.4298457459859797 Lambda1 -1.3950355\n",
      "46 Train Loss 664.21655 Test MSE 652.4089282802826 Test RE 0.4299808545771804 Lambda1 -1.3767141\n",
      "47 Train Loss 663.24084 Test MSE 654.175926414349 Test RE 0.430562745460633 Lambda1 -1.3569897\n",
      "48 Train Loss 662.4107 Test MSE 652.5241574988868 Test RE 0.4300188247583042 Lambda1 -1.3534662\n",
      "49 Train Loss 661.1314 Test MSE 650.8485023397202 Test RE 0.4294663345224488 Lambda1 -1.3670194\n",
      "50 Train Loss 659.4886 Test MSE 650.7403398544628 Test RE 0.42943064720350727 Lambda1 -1.3898083\n",
      "51 Train Loss 657.8368 Test MSE 648.1465815453536 Test RE 0.4285739680030831 Lambda1 -1.4083865\n",
      "52 Train Loss 656.8612 Test MSE 647.5872112519899 Test RE 0.42838899187323815 Lambda1 -1.410324\n",
      "53 Train Loss 655.7849 Test MSE 647.5347994954657 Test RE 0.42837165592723553 Lambda1 -1.4166738\n",
      "54 Train Loss 655.2735 Test MSE 646.3046055321329 Test RE 0.42796454981156085 Lambda1 -1.4216853\n",
      "55 Train Loss 652.321 Test MSE 644.27459818585 Test RE 0.427291914620074 Lambda1 -1.4342282\n",
      "56 Train Loss 651.06134 Test MSE 642.6793690268493 Test RE 0.4267625976361883 Lambda1 -1.4352963\n",
      "57 Train Loss 650.22217 Test MSE 642.3290128652106 Test RE 0.4266462571280509 Lambda1 -1.4319041\n",
      "58 Train Loss 649.4201 Test MSE 641.9553801056124 Test RE 0.42652215234420277 Lambda1 -1.4257286\n",
      "59 Train Loss 646.39716 Test MSE 638.2241571670334 Test RE 0.42528081336708845 Lambda1 -1.4295399\n",
      "60 Train Loss 644.3501 Test MSE 633.0959189141568 Test RE 0.4235687659160572 Lambda1 -1.4348518\n",
      "61 Train Loss 642.0296 Test MSE 627.7374386524824 Test RE 0.42177242838611917 Lambda1 -1.4392737\n",
      "62 Train Loss 641.20496 Test MSE 624.2653264859521 Test RE 0.4206043669309372 Lambda1 -1.4438026\n",
      "63 Train Loss 639.2881 Test MSE 621.4508398317274 Test RE 0.4196551530221433 Lambda1 -1.4482557\n",
      "64 Train Loss 636.44354 Test MSE 619.7120329544457 Test RE 0.4190676484637535 Lambda1 -1.451954\n",
      "65 Train Loss 634.5033 Test MSE 617.0626563843846 Test RE 0.41817089550390507 Lambda1 -1.4385122\n",
      "66 Train Loss 630.47565 Test MSE 615.1232476834075 Test RE 0.41751322929406987 Lambda1 -1.4099292\n",
      "67 Train Loss 629.43585 Test MSE 612.8317061067788 Test RE 0.4167348148510701 Lambda1 -1.4019665\n",
      "68 Train Loss 628.86053 Test MSE 611.979488179269 Test RE 0.4164449535137918 Lambda1 -1.4021659\n",
      "69 Train Loss 628.071 Test MSE 611.3596801829287 Test RE 0.41623401400947324 Lambda1 -1.40447\n",
      "70 Train Loss 627.5232 Test MSE 612.3471950823765 Test RE 0.4165700448706956 Lambda1 -1.4025542\n",
      "71 Train Loss 626.7937 Test MSE 612.3281250972019 Test RE 0.4165635583165859 Lambda1 -1.3923057\n",
      "72 Train Loss 626.03143 Test MSE 612.427547151647 Test RE 0.4165973750906969 Lambda1 -1.3883636\n",
      "73 Train Loss 625.0069 Test MSE 610.9745469631325 Test RE 0.41610288759915576 Lambda1 -1.3931859\n",
      "74 Train Loss 623.58673 Test MSE 608.6535189991217 Test RE 0.41531177001541186 Lambda1 -1.3908558\n",
      "Training time: 165.85\n",
      "Training time: 165.85\n",
      "inv_HT_atanh_tune5\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 969.41595 Test MSE 978.5914163146268 Test RE 0.5266109334550335 Lambda1 -0.028752029\n",
      "1 Train Loss 850.4963 Test MSE 867.318214173176 Test RE 0.49576790053018743 Lambda1 -0.032914087\n",
      "2 Train Loss 839.45605 Test MSE 858.5384508572632 Test RE 0.4932522168982045 Lambda1 -0.034012895\n",
      "3 Train Loss 838.2272 Test MSE 858.0562976732994 Test RE 0.4931136928016749 Lambda1 -0.03436287\n",
      "4 Train Loss 838.08246 Test MSE 858.166790802316 Test RE 0.4931454412551795 Lambda1 -0.034481395\n",
      "5 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "6 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "8 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "9 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "10 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "11 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "12 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "13 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "14 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "15 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "16 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "17 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "18 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "19 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "20 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "21 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "22 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "23 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "24 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "25 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "26 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "27 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "28 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "29 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "30 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "31 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "32 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "33 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "34 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "35 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "36 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "37 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "38 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "39 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "40 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "41 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "42 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "43 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "44 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "45 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "46 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "47 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "48 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "49 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "50 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "51 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "52 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "53 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "54 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "55 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "56 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "57 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "58 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "59 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "60 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "61 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "62 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "63 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "64 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "65 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "66 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "67 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "68 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "69 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "70 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "71 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "72 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "73 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "74 Train Loss 838.0627 Test MSE 858.2735229523761 Test RE 0.49317610711019516 Lambda1 -0.03454013\n",
      "Training time: 138.62\n",
      "Training time: 138.62\n",
      "inv_HT_atanh_tune5\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 871.4988 Test MSE 886.1463937576723 Test RE 0.5011201982385793 Lambda1 -0.043857682\n",
      "1 Train Loss 840.9582 Test MSE 859.5382533202824 Test RE 0.4935393393268852 Lambda1 -0.04585053\n",
      "2 Train Loss 838.4046 Test MSE 858.0611526770713 Test RE 0.4931150878531759 Lambda1 -0.046357334\n",
      "3 Train Loss 838.1042 Test MSE 858.1283386463654 Test RE 0.49313439286806166 Lambda1 -0.046528205\n",
      "4 Train Loss 838.063 Test MSE 858.2718972622993 Test RE 0.49317564003771674 Lambda1 -0.04661169\n",
      "5 Train Loss 838.0629 Test MSE 858.2768503622159 Test RE 0.49317706309764986 Lambda1 -0.04661349\n",
      "6 Train Loss 838.0629 Test MSE 858.2790055993356 Test RE 0.4931776823109288 Lambda1 -0.04661403\n",
      "7 Train Loss 838.06287 Test MSE 858.2809594531656 Test RE 0.493178243664841 Lambda1 -0.046614274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "9 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "10 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "11 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "12 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "13 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "14 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "15 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "16 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "17 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "18 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "19 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "20 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "21 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "22 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "23 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "24 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "25 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "26 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "27 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "28 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "29 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "30 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "31 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "32 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "33 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "34 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "35 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "36 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "37 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "38 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "39 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "40 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "41 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "42 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "43 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "44 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "45 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "46 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "47 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "48 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "49 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "50 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "51 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "52 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "53 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "54 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "55 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "56 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "57 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "58 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "59 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "60 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "61 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "62 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "63 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "64 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "65 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "66 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "67 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "68 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "69 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "70 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "71 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "72 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "73 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "74 Train Loss 838.0628 Test MSE 858.2822185148311 Test RE 0.4931786054004629 Lambda1 -0.046614267\n",
      "Training time: 119.51\n",
      "Training time: 119.51\n",
      "inv_HT_atanh_tune5\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 1237.6732 Test MSE 1238.6290331309747 Test RE 0.5924610424702784 Lambda1 0.0056251786\n",
      "1 Train Loss 872.3742 Test MSE 886.9489765299932 Test RE 0.5013470791558005 Lambda1 0.008086526\n",
      "2 Train Loss 842.1495 Test MSE 860.4204315716054 Test RE 0.49379254388904886 Lambda1 0.008777556\n",
      "3 Train Loss 838.5508 Test MSE 858.0938323766794 Test RE 0.4931244780344353 Lambda1 0.009010669\n",
      "4 Train Loss 838.05414 Test MSE 858.2761331404355 Test RE 0.49317685703511593 Lambda1 0.009128785\n",
      "5 Train Loss 838.05396 Test MSE 858.2760446173743 Test RE 0.49317683160185233 Lambda1 0.00912816\n",
      "6 Train Loss 838.0538 Test MSE 858.2758952641838 Test RE 0.4931767886916856 Lambda1 0.009127442\n",
      "7 Train Loss 838.0536 Test MSE 858.2757263603348 Test RE 0.49317674016447977 Lambda1 0.009126654\n",
      "8 Train Loss 838.0534 Test MSE 858.275550744403 Test RE 0.49317668970884254 Lambda1 0.009125943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 838.0532 Test MSE 858.2753818689243 Test RE 0.4931766411897783 Lambda1 0.009125283\n",
      "10 Train Loss 838.0531 Test MSE 858.275214455891 Test RE 0.4931765930908793 Lambda1 0.00912461\n",
      "11 Train Loss 838.0528 Test MSE 858.2749898635906 Test RE 0.49317652856398225 Lambda1 0.009123691\n",
      "12 Train Loss 838.0526 Test MSE 858.2747768953793 Test RE 0.4931764673767574 Lambda1 0.009122858\n",
      "13 Train Loss 838.0525 Test MSE 858.2746029478999 Test RE 0.49317641740045015 Lambda1 0.009122204\n",
      "14 Train Loss 838.05225 Test MSE 858.2744005105909 Test RE 0.4931763592388127 Lambda1 0.00912141\n",
      "15 Train Loss 838.0521 Test MSE 858.2742096950968 Test RE 0.4931763044161967 Lambda1 0.009120608\n",
      "16 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "17 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "18 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "19 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "20 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "21 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "22 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "23 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "24 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "25 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "26 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "27 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "28 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "29 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "30 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "31 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "32 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "33 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "34 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "35 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "36 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "37 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "38 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "39 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "40 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "41 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "42 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "43 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "44 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "45 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "46 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "47 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "48 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "49 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "50 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "51 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "52 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "53 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "54 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "55 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "56 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "57 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "58 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "59 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "60 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "61 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "62 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "63 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "64 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "65 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "66 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "67 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "68 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "69 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "70 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "71 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "72 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "73 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "74 Train Loss 838.05176 Test MSE 858.2739951510958 Test RE 0.49317624277620853 Lambda1 0.009119747\n",
      "Training time: 120.07\n",
      "Training time: 120.07\n",
      "inv_HT_atanh_tune5\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 1162.5453 Test MSE 1165.4070742796769 Test RE 0.5746825307928911 Lambda1 -0.026082901\n",
      "1 Train Loss 868.5726 Test MSE 883.4697608868172 Test RE 0.5003628012623456 Lambda1 -0.03368328\n",
      "2 Train Loss 841.6528 Test MSE 860.0468027518924 Test RE 0.49368532005305055 Lambda1 -0.035661314\n",
      "3 Train Loss 838.4943 Test MSE 858.0809354900979 Test RE 0.493120772265835 Lambda1 -0.036317\n",
      "4 Train Loss 838.0654 Test MSE 858.2305879904103 Test RE 0.49316377143767703 Lambda1 -0.036633153\n",
      "5 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "6 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "7 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "8 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "10 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "11 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "12 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "13 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "14 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "15 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "16 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "17 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "18 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "19 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "20 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "21 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "22 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "23 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "24 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "25 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "26 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "27 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "28 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "29 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "30 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "31 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "32 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "33 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "34 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "35 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "36 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "37 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "38 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "39 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "40 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "41 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "42 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "43 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "44 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "45 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "46 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "47 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "48 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "49 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "50 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "51 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "52 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "53 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "54 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "55 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "56 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "57 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "58 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "59 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "60 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "61 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "62 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "63 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "64 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "65 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "66 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "67 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "68 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "69 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "70 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "71 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "72 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "73 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "74 Train Loss 838.0622 Test MSE 858.2737348510658 Test RE 0.49317616799018377 Lambda1 -0.036658432\n",
      "Training time: 137.02\n",
      "Training time: 137.02\n",
      "inv_HT_atanh_tune5\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 922.5673 Test MSE 933.9281032796985 Test RE 0.5144532233946989 Lambda1 -0.18097702\n",
      "1 Train Loss 847.5503 Test MSE 864.8009268113445 Test RE 0.4950479242273108 Lambda1 -0.20026135\n",
      "2 Train Loss 839.16595 Test MSE 858.3712136946937 Test RE 0.493204173552787 Lambda1 -0.20616433\n",
      "3 Train Loss 838.1894 Test MSE 858.0588918787608 Test RE 0.4931144382290272 Lambda1 -0.20811464\n",
      "4 Train Loss 838.05585 Test MSE 858.2698242812836 Test RE 0.49317504445468546 Lambda1 -0.20912552\n",
      "5 Train Loss 838.0554 Test MSE 858.2769213225025 Test RE 0.49317708348500217 Lambda1 -0.20915115\n",
      "6 Train Loss 837.9035 Test MSE 857.9848300979238 Test RE 0.49309315663525144 Lambda1 -0.21132241\n",
      "7 Train Loss 837.85187 Test MSE 857.8358510183737 Test RE 0.4930503448329907 Lambda1 -0.21323393\n",
      "8 Train Loss 837.5714 Test MSE 858.1325520108439 Test RE 0.49313560349838925 Lambda1 -0.5190538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "10 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "11 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "12 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "13 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "14 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "15 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "16 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "17 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "18 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "19 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "20 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "21 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "22 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "23 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "24 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "25 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "26 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "27 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "28 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "29 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "30 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "31 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "32 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "33 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "34 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "35 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "36 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "37 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "38 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "39 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "40 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "41 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "42 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "43 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "44 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "45 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "46 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "47 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "48 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "49 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "50 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "51 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "52 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "53 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "54 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "55 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "56 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "57 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "58 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "59 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "60 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "61 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "62 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "63 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "64 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "65 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "66 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "67 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "68 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "69 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "70 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "71 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "72 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "73 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "74 Train Loss 835.95795 Test MSE 852.2652787603598 Test RE 0.49144686440289337 Lambda1 -0.57473326\n",
      "Training time: 132.50\n",
      "Training time: 132.50\n",
      "inv_HT_atanh_tune5\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 1051.2896 Test MSE 1057.4385586912376 Test RE 0.5474150609511396 Lambda1 -0.00060620805\n",
      "1 Train Loss 857.1528 Test MSE 873.1628718600053 Test RE 0.4974355286557337 Lambda1 0.000801827\n",
      "2 Train Loss 840.3498 Test MSE 859.1132041524073 Test RE 0.49341729448163174 Lambda1 0.0012096638\n",
      "3 Train Loss 838.3403 Test MSE 858.0540801183846 Test RE 0.4931130556014105 Lambda1 0.0013575645\n",
      "4 Train Loss 838.0655 Test MSE 858.2456593194563 Test RE 0.4931681016264084 Lambda1 0.001432891\n",
      "5 Train Loss 838.06366 Test MSE 858.2783587421334 Test RE 0.4931774964648381 Lambda1 0.0014389489\n",
      "6 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "7 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "8 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "10 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "11 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "12 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "13 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "14 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "15 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "16 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "17 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "18 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "19 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "20 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "21 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "22 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "23 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "24 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "25 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "26 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "27 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "28 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "29 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "30 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "31 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "32 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "33 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "34 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "35 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "36 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "37 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "38 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "39 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "40 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "41 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "42 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "43 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "44 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "45 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "46 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "47 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "48 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "49 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "50 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "51 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "52 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "53 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "54 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "55 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "56 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "57 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "58 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "59 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "60 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "61 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "62 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "63 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "64 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "65 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "66 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "67 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "68 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "69 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "70 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "71 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "72 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "73 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "74 Train Loss 838.06366 Test MSE 858.2792211322763 Test RE 0.4931777442348656 Lambda1 0.0014391415\n",
      "Training time: 116.87\n",
      "Training time: 116.87\n",
      "inv_HT_atanh_tune5\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 1045.7657 Test MSE 1052.0979907255826 Test RE 0.5460309578554579 Lambda1 -0.0012485171\n",
      "1 Train Loss 853.6525 Test MSE 870.0680340824688 Test RE 0.4965531910796391 Lambda1 -0.001498754\n",
      "2 Train Loss 839.89124 Test MSE 858.809028995343 Test RE 0.4933299377925684 Lambda1 -0.001505598\n",
      "3 Train Loss 838.28503 Test MSE 858.0526612204345 Test RE 0.4931126478897002 Lambda1 -0.0015061192\n",
      "4 Train Loss 838.06464 Test MSE 858.2655356124801 Test RE 0.4931738122856111 Lambda1 -0.001507373\n",
      "5 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "6 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "7 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "8 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "10 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "11 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "12 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "13 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "14 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "15 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "16 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "17 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "18 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "19 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "20 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "21 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "22 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "23 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "24 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "25 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "26 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "27 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "28 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "29 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "30 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "31 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "32 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "33 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "34 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "35 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "36 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "37 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "38 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "39 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "40 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "41 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "42 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "43 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "44 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "45 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "46 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "47 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "48 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "49 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "50 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "51 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "52 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "53 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "54 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "55 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "56 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "57 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "58 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "59 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "60 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "61 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "62 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "63 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "64 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "65 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "66 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "67 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "68 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "69 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "70 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "71 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "72 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "73 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "74 Train Loss 838.06415 Test MSE 858.2782249631979 Test RE 0.4931774580293105 Lambda1 -0.0015076686\n",
      "Training time: 138.72\n",
      "Training time: 138.72\n",
      "inv_HT_atanh_tune6\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 29907.738 Test MSE 3591.115976759885 Test RE 1.00879724188748 Lambda1 3.8937513e-05\n",
      "1 Train Loss 28063.266 Test MSE 3590.6445992669737 Test RE 1.008731031282835 Lambda1 9.305169e-05\n",
      "2 Train Loss 27108.662 Test MSE 3590.501254030256 Test RE 1.0087108958687556 Lambda1 6.85806e-05\n",
      "3 Train Loss 26223.7 Test MSE 3589.8801174102264 Test RE 1.0086236414254683 Lambda1 2.8159468e-05\n",
      "4 Train Loss 25410.137 Test MSE 3589.53595400237 Test RE 1.0085752916674497 Lambda1 0.00013020773\n",
      "5 Train Loss 24453.314 Test MSE 3589.2617730701836 Test RE 1.0085367717308882 Lambda1 0.00025801116\n",
      "6 Train Loss 23334.652 Test MSE 3588.8179425728185 Test RE 1.0084744144491171 Lambda1 0.00040439298\n",
      "7 Train Loss 22034.242 Test MSE 3589.3829557256067 Test RE 1.0085537969773377 Lambda1 0.00023406581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 21051.113 Test MSE 3590.5470955562905 Test RE 1.0087173351784686 Lambda1 -0.00023753247\n",
      "9 Train Loss 20417.023 Test MSE 3591.3544876614837 Test RE 1.0088307419395879 Lambda1 -0.00047794412\n",
      "10 Train Loss 19417.496 Test MSE 3591.7781931063323 Test RE 1.0088902507506825 Lambda1 -0.00052940304\n",
      "11 Train Loss 18516.484 Test MSE 3592.4295405590065 Test RE 1.0089817247045587 Lambda1 -0.0003722046\n",
      "12 Train Loss 16738.73 Test MSE 3593.546018394925 Test RE 1.0091385013661085 Lambda1 0.0001700919\n",
      "13 Train Loss 15951.12 Test MSE 3595.154010617361 Test RE 1.0093642540584724 Lambda1 -7.183735e-07\n",
      "14 Train Loss 14984.273 Test MSE 3600.4678860781414 Test RE 1.0101099321902 Lambda1 4.5342244e-05\n",
      "15 Train Loss 14537.941 Test MSE 3602.8871493967863 Test RE 1.010449236916591 Lambda1 5.245911e-05\n",
      "16 Train Loss 13881.066 Test MSE 3601.9403357442266 Test RE 1.0103164586813436 Lambda1 0.0006955329\n",
      "17 Train Loss 12733.207 Test MSE 3601.26615774074 Test RE 1.0102219033938389 Lambda1 0.00018218007\n",
      "18 Train Loss 12045.608 Test MSE 3603.6203376008534 Test RE 1.0105520449901833 Lambda1 0.00026905217\n",
      "19 Train Loss 11483.005 Test MSE 3605.511087059747 Test RE 1.0108171187205055 Lambda1 0.00014299434\n",
      "20 Train Loss 10905.134 Test MSE 3606.0921795540726 Test RE 1.0108985710530494 Lambda1 0.00032744347\n",
      "21 Train Loss 9933.836 Test MSE 3606.6984364062614 Test RE 1.0109835437046777 Lambda1 0.00013738194\n",
      "22 Train Loss 9124.529 Test MSE 3604.9858472966775 Test RE 1.0107434896748195 Lambda1 3.4692413e-05\n",
      "23 Train Loss 8626.783 Test MSE 3605.491503261326 Test RE 1.0108143735250124 Lambda1 0.00022499145\n",
      "24 Train Loss 8417.215 Test MSE 3607.2136715576135 Test RE 1.0110557531874733 Lambda1 0.00036363685\n",
      "25 Train Loss 8030.787 Test MSE 3607.284375120957 Test RE 1.011065661790167 Lambda1 0.00039691728\n",
      "26 Train Loss 7607.8965 Test MSE 3611.7808021729 Test RE 1.0116956048047738 Lambda1 -0.00037624722\n",
      "27 Train Loss 7401.1094 Test MSE 3611.7563664057093 Test RE 1.011692182448679 Lambda1 -0.00040839703\n",
      "28 Train Loss 7035.782 Test MSE 3612.813865788938 Test RE 1.0118402801398787 Lambda1 3.5388974e-05\n",
      "29 Train Loss 6724.9287 Test MSE 3612.5081938011817 Test RE 1.0117974744787521 Lambda1 0.00012070371\n",
      "30 Train Loss 6568.4873 Test MSE 3611.6955131120885 Test RE 1.0116836595785696 Lambda1 -7.325365e-05\n",
      "31 Train Loss 6462.9224 Test MSE 3611.9996250176328 Test RE 1.0117262515645464 Lambda1 -0.00036579458\n",
      "32 Train Loss 6327.9243 Test MSE 3612.6549338149107 Test RE 1.0118180238671046 Lambda1 -0.0006455903\n",
      "33 Train Loss 6176.7524 Test MSE 3612.340304197106 Test RE 1.011773962801824 Lambda1 -0.0002590875\n",
      "34 Train Loss 6023.121 Test MSE 3614.2365112960047 Test RE 1.01203948059692 Lambda1 -0.00015586181\n",
      "35 Train Loss 5847.3604 Test MSE 3616.5876914563037 Test RE 1.0123686096235993 Lambda1 -0.000349484\n",
      "36 Train Loss 5710.202 Test MSE 3616.6255493367194 Test RE 1.0123739082687213 Lambda1 -0.0002238736\n",
      "37 Train Loss 5537.9844 Test MSE 3616.5053396698618 Test RE 1.0123570834495847 Lambda1 7.841049e-05\n",
      "38 Train Loss 5451.3374 Test MSE 3616.4496801232904 Test RE 1.012349293117794 Lambda1 6.517618e-05\n",
      "39 Train Loss 5343.581 Test MSE 3617.480666511384 Test RE 1.012493584350586 Lambda1 -3.8371127e-05\n",
      "40 Train Loss 5248.896 Test MSE 3618.2267544168294 Test RE 1.0125979899219495 Lambda1 6.635468e-05\n",
      "41 Train Loss 5170.8438 Test MSE 3620.308365440652 Test RE 1.0128892281724484 Lambda1 0.00015875822\n",
      "42 Train Loss 5032.138 Test MSE 3621.545599934868 Test RE 1.0130622900107693 Lambda1 -1.9874135e-05\n",
      "43 Train Loss 4903.143 Test MSE 3625.2217698710633 Test RE 1.0135763308284593 Lambda1 -4.0943443e-05\n",
      "44 Train Loss 4816.9854 Test MSE 3626.856658086893 Test RE 1.0138048543947553 Lambda1 0.00030727888\n",
      "45 Train Loss 4727.72 Test MSE 3628.475563364961 Test RE 1.0140310931291898 Lambda1 0.00040815937\n",
      "46 Train Loss 4664.707 Test MSE 3627.4791777214227 Test RE 1.01389185622691 Lambda1 0.0002921846\n",
      "47 Train Loss 4617.5234 Test MSE 3626.916950340378 Test RE 1.0138132810202052 Lambda1 -4.5231198e-05\n",
      "48 Train Loss 4559.725 Test MSE 3625.5993149093024 Test RE 1.0136291083930027 Lambda1 -0.0002035245\n",
      "49 Train Loss 4512.2656 Test MSE 3625.8960787733704 Test RE 1.0136705915120117 Lambda1 -0.00026461593\n",
      "50 Train Loss 4469.7305 Test MSE 3626.4376954683976 Test RE 1.0137462969931925 Lambda1 -0.0002723786\n",
      "51 Train Loss 4408.3896 Test MSE 3626.922789976786 Test RE 1.013814097181513 Lambda1 -0.00013763472\n",
      "52 Train Loss 4353.612 Test MSE 3625.9130169368036 Test RE 1.013672959160963 Lambda1 0.00016024079\n",
      "53 Train Loss 4298.8066 Test MSE 3625.1966222019128 Test RE 1.0135728153019554 Lambda1 0.00012661371\n",
      "54 Train Loss 4236.4653 Test MSE 3624.545957767824 Test RE 1.0134818512183645 Lambda1 4.16089e-05\n",
      "55 Train Loss 4197.3613 Test MSE 3623.2667141546926 Test RE 1.0133029868011407 Lambda1 -0.00017424399\n",
      "56 Train Loss 4151.367 Test MSE 3622.905596429619 Test RE 1.0132524894438877 Lambda1 0.00016485192\n",
      "57 Train Loss 4129.9355 Test MSE 3622.944058542511 Test RE 1.0132578679622015 Lambda1 0.0003798191\n",
      "58 Train Loss 4109.507 Test MSE 3623.0340026259737 Test RE 1.0132704455763097 Lambda1 0.00036591114\n",
      "59 Train Loss 4083.8638 Test MSE 3623.316838913283 Test RE 1.013309995861739 Lambda1 4.4565408e-05\n",
      "60 Train Loss 4062.3726 Test MSE 3623.008191407086 Test RE 1.0132668361992208 Lambda1 8.800788e-05\n",
      "61 Train Loss 4039.23 Test MSE 3621.690040466349 Test RE 1.0130824921306332 Lambda1 6.654539e-05\n",
      "62 Train Loss 4016.7893 Test MSE 3619.8740924552435 Test RE 1.0128284759413504 Lambda1 0.00015626218\n",
      "63 Train Loss 4001.893 Test MSE 3617.859536171153 Test RE 1.012546603715682 Lambda1 0.0001877743\n",
      "64 Train Loss 3979.667 Test MSE 3617.6217801283615 Test RE 1.0125133322430686 Lambda1 1.367625e-05\n",
      "65 Train Loss 3957.2415 Test MSE 3615.5579201566043 Test RE 1.0122244706660766 Lambda1 3.374596e-05\n",
      "66 Train Loss 3926.844 Test MSE 3614.4747444540603 Test RE 1.0120728344446626 Lambda1 -5.8273123e-05\n",
      "67 Train Loss 3903.4707 Test MSE 3613.477718974054 Test RE 1.0119332385047177 Lambda1 5.614728e-06\n",
      "68 Train Loss 3890.6638 Test MSE 3612.4095565166967 Test RE 1.011783661134776 Lambda1 -8.552046e-05\n",
      "69 Train Loss 3874.6528 Test MSE 3610.7062146897315 Test RE 1.0115450923057199 Lambda1 3.282103e-05\n",
      "70 Train Loss 3862.0369 Test MSE 3609.831257606712 Test RE 1.011422524570396 Lambda1 -6.6583474e-05\n",
      "71 Train Loss 3850.7285 Test MSE 3607.739043758 Test RE 1.0111293780534507 Lambda1 9.56755e-06\n",
      "72 Train Loss 3839.3477 Test MSE 3606.7222791756326 Test RE 1.0109868853491284 Lambda1 -1.5833935e-05\n",
      "73 Train Loss 3826.5928 Test MSE 3604.6536765200035 Test RE 1.0106969226148514 Lambda1 7.671009e-05\n",
      "74 Train Loss 3818.0713 Test MSE 3602.20403409104 Test RE 1.0103534406798114 Lambda1 0.00015936722\n",
      "Training time: 161.46\n",
      "Training time: 161.46\n",
      "inv_HT_atanh_tune6\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 16744.164 Test MSE 3572.3221458870294 Test RE 1.0061540474977486 Lambda1 -0.0001451463\n",
      "1 Train Loss 11904.588 Test MSE 3576.682416177959 Test RE 1.0067679011126487 Lambda1 -9.73667e-05\n",
      "2 Train Loss 8540.19 Test MSE 3593.179003280291 Test RE 1.0090869675137586 Lambda1 0.00041821323\n",
      "3 Train Loss 6296.1567 Test MSE 3588.64372829822 Test RE 1.008449936644225 Lambda1 3.711787e-05\n",
      "4 Train Loss 4845.2725 Test MSE 3580.6472765588605 Test RE 1.0073257628520917 Lambda1 -0.00024766702\n",
      "5 Train Loss 4096.9976 Test MSE 3565.4505485409654 Test RE 1.0051858798423177 Lambda1 -0.00019826557\n",
      "6 Train Loss 3797.1843 Test MSE 3561.314418937481 Test RE 1.0046026736701734 Lambda1 -0.0005718913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 3673.6492 Test MSE 3551.9496399410746 Test RE 1.0032809601450454 Lambda1 0.00020321392\n",
      "8 Train Loss 3613.8062 Test MSE 3522.36236573782 Test RE 0.9990936225728576 Lambda1 0.0010076882\n",
      "9 Train Loss 3534.737 Test MSE 3459.1259442022288 Test RE 0.99008471969416 Lambda1 -0.00080280326\n",
      "10 Train Loss 3112.8167 Test MSE 3056.9527788172277 Test RE 0.9307510519056454 Lambda1 -0.021402152\n",
      "11 Train Loss 2147.7227 Test MSE 2133.065961054626 Test RE 0.7774837367899137 Lambda1 -0.22045137\n",
      "12 Train Loss 838.0933 Test MSE 858.3108591436205 Test RE 0.49318683394535473 Lambda1 -0.79890364\n",
      "13 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "14 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "15 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "16 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "17 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "18 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "19 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "20 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "21 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "22 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "23 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "24 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "25 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "26 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "27 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "28 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "29 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "30 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "31 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "32 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "33 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "34 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "35 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "36 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "37 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "38 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "39 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "40 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "41 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "42 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "43 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "44 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "45 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "46 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "47 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "48 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "49 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "50 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "51 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "52 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "53 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "54 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "55 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "56 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "57 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "58 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "59 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "60 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "61 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "62 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "63 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "64 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "65 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "66 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "67 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "68 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "69 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "70 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "71 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "72 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "73 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "74 Train Loss 838.062 Test MSE 858.2766228551237 Test RE 0.49317699773337553 Lambda1 -0.80168945\n",
      "Training time: 159.17\n",
      "Training time: 159.17\n",
      "inv_HT_atanh_tune6\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 6711.5894 Test MSE 3519.7214563020816 Test RE 0.9987190145333197 Lambda1 -0.00034782215\n",
      "1 Train Loss 4905.0063 Test MSE 3518.741894561038 Test RE 0.998580029811712 Lambda1 0.00031933747\n",
      "2 Train Loss 4392.4775 Test MSE 3519.913888742553 Test RE 0.9987463154593934 Lambda1 0.00048405727\n",
      "3 Train Loss 3884.7737 Test MSE 3513.439342439112 Test RE 0.9978273433193845 Lambda1 -0.0003487783\n",
      "4 Train Loss 3750.5698 Test MSE 3514.9593533168345 Test RE 0.9980431638105204 Lambda1 0.0005124074\n",
      "5 Train Loss 3686.6511 Test MSE 3512.4260708712754 Test RE 0.9976834468569157 Lambda1 -0.00013970117\n",
      "6 Train Loss 3637.3662 Test MSE 3504.5396985534153 Test RE 0.9965627791983099 Lambda1 0.00040919034\n",
      "7 Train Loss 3594.6638 Test MSE 3495.5542814497308 Test RE 0.9952843974285634 Lambda1 -1.4100608e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 3563.3562 Test MSE 3486.9043230455236 Test RE 0.9940521892406372 Lambda1 0.00013913201\n",
      "9 Train Loss 3491.8965 Test MSE 3403.3262845377667 Test RE 0.9820666533567841 Lambda1 -0.00017806969\n",
      "10 Train Loss 3443.7646 Test MSE 3360.0989055285527 Test RE 0.9758098580231986 Lambda1 0.00014962193\n",
      "11 Train Loss 3344.5315 Test MSE 3236.967531740036 Test RE 0.9577636323920361 Lambda1 -0.00021803568\n",
      "12 Train Loss 3221.7266 Test MSE 3148.111793244298 Test RE 0.9445267101131445 Lambda1 0.0014276453\n",
      "13 Train Loss 3139.1372 Test MSE 3072.632236212503 Test RE 0.9331349628141993 Lambda1 0.0019344949\n",
      "14 Train Loss 3081.4539 Test MSE 3039.640363186242 Test RE 0.9281117524152301 Lambda1 -5.967632e-05\n",
      "15 Train Loss 2946.2673 Test MSE 2859.7317025126113 Test RE 0.9002265465518211 Lambda1 -0.007241905\n",
      "16 Train Loss 2764.3752 Test MSE 2699.9387674303966 Test RE 0.874714101646541 Lambda1 -0.016667921\n",
      "17 Train Loss 891.4387 Test MSE 903.7461979412062 Test RE 0.5060721199768496 Lambda1 -0.22862545\n",
      "18 Train Loss 837.44806 Test MSE 856.8949339178588 Test RE 0.49277986955346187 Lambda1 -0.2716555\n",
      "19 Train Loss 834.48334 Test MSE 852.954800973055 Test RE 0.4916456258838752 Lambda1 -0.27188367\n",
      "20 Train Loss 829.11096 Test MSE 844.3484890067324 Test RE 0.4891589858196791 Lambda1 -0.28117657\n",
      "21 Train Loss 825.5631 Test MSE 839.0011764417567 Test RE 0.487607588193357 Lambda1 -0.28579482\n",
      "22 Train Loss 822.97473 Test MSE 835.5887290979933 Test RE 0.48661496110571273 Lambda1 -0.2877416\n",
      "23 Train Loss 819.4875 Test MSE 832.5892481825614 Test RE 0.4857407843396481 Lambda1 -0.28470922\n",
      "24 Train Loss 817.493 Test MSE 830.8863229292689 Test RE 0.4852437783738451 Lambda1 -0.2814024\n",
      "25 Train Loss 815.0776 Test MSE 827.2924610991847 Test RE 0.4841932201497507 Lambda1 -0.27175814\n",
      "26 Train Loss 811.7306 Test MSE 822.2525037488948 Test RE 0.4827160876223722 Lambda1 -0.24992068\n",
      "27 Train Loss 807.5168 Test MSE 821.2034534636455 Test RE 0.482408058721533 Lambda1 -0.23826675\n",
      "28 Train Loss 802.73346 Test MSE 815.3498741876131 Test RE 0.4806856695319626 Lambda1 -0.21056023\n",
      "29 Train Loss 800.2493 Test MSE 810.7304318606323 Test RE 0.4793220501505817 Lambda1 -0.17631431\n",
      "30 Train Loss 798.5498 Test MSE 810.751768659442 Test RE 0.47932835750655245 Lambda1 -0.17770872\n",
      "31 Train Loss 796.6607 Test MSE 806.3225721407326 Test RE 0.4780172613719321 Lambda1 -0.1721622\n",
      "32 Train Loss 795.2034 Test MSE 803.9805952152892 Test RE 0.4773225521309448 Lambda1 -0.17901917\n",
      "33 Train Loss 793.2733 Test MSE 802.7783414672356 Test RE 0.47696553012907117 Lambda1 -0.18312767\n",
      "34 Train Loss 790.3336 Test MSE 796.2104029296424 Test RE 0.47501037390228296 Lambda1 -0.16612993\n",
      "35 Train Loss 786.3565 Test MSE 792.2868091998 Test RE 0.4738385420265425 Lambda1 -0.12703042\n",
      "36 Train Loss 776.20557 Test MSE 784.8293671529109 Test RE 0.47160325431532274 Lambda1 -0.07449194\n",
      "37 Train Loss 752.6868 Test MSE 763.1525457635088 Test RE 0.46504486071828094 Lambda1 -0.012326423\n",
      "38 Train Loss 739.97675 Test MSE 754.539131252611 Test RE 0.4624130206605447 Lambda1 -0.006330122\n",
      "39 Train Loss 737.2083 Test MSE 752.9268960180094 Test RE 0.461918734015759 Lambda1 -0.0018389719\n",
      "40 Train Loss 735.7617 Test MSE 752.4046423439205 Test RE 0.46175850557628506 Lambda1 -0.00061105983\n",
      "41 Train Loss 734.7102 Test MSE 752.0766581949479 Test RE 0.4616578509748152 Lambda1 -0.00012405548\n",
      "42 Train Loss 734.25604 Test MSE 752.0364740018379 Test RE 0.46164551739429327 Lambda1 0.00012102882\n",
      "43 Train Loss 734.05096 Test MSE 751.4932237959708 Test RE 0.46147874733848426 Lambda1 0.000115577204\n",
      "44 Train Loss 733.51465 Test MSE 751.3735299157707 Test RE 0.46144199492358756 Lambda1 0.00017151491\n",
      "45 Train Loss 733.1077 Test MSE 750.8582674789014 Test RE 0.46128374839669556 Lambda1 0.00020110849\n",
      "46 Train Loss 732.34735 Test MSE 750.7480085703012 Test RE 0.46124987881561996 Lambda1 0.00027903012\n",
      "47 Train Loss 731.9113 Test MSE 749.6685322373067 Test RE 0.4609181513701505 Lambda1 0.00048703476\n",
      "48 Train Loss 731.00037 Test MSE 748.3483618442482 Test RE 0.46051213284609893 Lambda1 0.0009479629\n",
      "49 Train Loss 729.35693 Test MSE 745.7658435786743 Test RE 0.45971684226419074 Lambda1 0.0014366314\n",
      "50 Train Loss 728.63983 Test MSE 744.9467939500724 Test RE 0.45946432711363094 Lambda1 0.0023430763\n",
      "51 Train Loss 727.4745 Test MSE 743.7323470101893 Test RE 0.45908965428228304 Lambda1 0.003155966\n",
      "52 Train Loss 726.56665 Test MSE 742.7311284414922 Test RE 0.4587805350887541 Lambda1 0.004443202\n",
      "53 Train Loss 725.7832 Test MSE 742.022779088648 Test RE 0.45856171134285406 Lambda1 0.005170089\n",
      "54 Train Loss 724.6985 Test MSE 740.6094724949493 Test RE 0.4581247994306264 Lambda1 0.0041887695\n",
      "55 Train Loss 723.4429 Test MSE 739.0053727424747 Test RE 0.4576284000071859 Lambda1 0.0022266589\n",
      "56 Train Loss 722.7501 Test MSE 738.3608363523498 Test RE 0.45742879219029847 Lambda1 0.0025076244\n",
      "57 Train Loss 721.70886 Test MSE 738.7028018418252 Test RE 0.4575347070395197 Lambda1 0.0017649563\n",
      "58 Train Loss 720.5268 Test MSE 737.4968163365606 Test RE 0.45716107530783834 Lambda1 0.0011190907\n",
      "59 Train Loss 719.65765 Test MSE 736.1622364663327 Test RE 0.4567472469199756 Lambda1 0.00090712844\n",
      "60 Train Loss 718.3075 Test MSE 734.8950702701713 Test RE 0.4563539749477144 Lambda1 0.0005822958\n",
      "61 Train Loss 716.8495 Test MSE 734.0975126873517 Test RE 0.4561062747029335 Lambda1 0.00056262687\n",
      "62 Train Loss 716.00085 Test MSE 733.2017257723467 Test RE 0.45582790654134486 Lambda1 0.00050327735\n",
      "63 Train Loss 714.97363 Test MSE 731.5523094828624 Test RE 0.45531490086638843 Lambda1 0.0010281105\n",
      "64 Train Loss 713.5516 Test MSE 730.079296754966 Test RE 0.4548562716001971 Lambda1 0.0015188161\n",
      "65 Train Loss 711.82904 Test MSE 727.2569972289933 Test RE 0.45397624137445525 Lambda1 0.0014266378\n",
      "66 Train Loss 710.5173 Test MSE 724.6605523278881 Test RE 0.45316512628653405 Lambda1 0.00097477663\n",
      "67 Train Loss 707.67163 Test MSE 721.6646952392979 Test RE 0.4522274293128932 Lambda1 0.002611052\n",
      "68 Train Loss 705.59625 Test MSE 719.5927569267146 Test RE 0.45157777799596605 Lambda1 0.0027795967\n",
      "69 Train Loss 701.4609 Test MSE 713.8218785413263 Test RE 0.44976338626614554 Lambda1 0.002114928\n",
      "70 Train Loss 693.98846 Test MSE 705.8125925682709 Test RE 0.4472330313960102 Lambda1 0.0014721546\n",
      "71 Train Loss 685.39856 Test MSE 694.5581008936426 Test RE 0.44365303939055634 Lambda1 0.00054168195\n",
      "72 Train Loss 681.67194 Test MSE 690.8556683731325 Test RE 0.44246898415459207 Lambda1 4.1063668e-05\n",
      "73 Train Loss 678.9947 Test MSE 690.1963014351016 Test RE 0.4422577829741034 Lambda1 6.2418694e-05\n",
      "74 Train Loss 677.0523 Test MSE 688.739302858488 Test RE 0.44179073513371286 Lambda1 4.037307e-05\n",
      "Training time: 163.65\n",
      "Training time: 163.65\n",
      "inv_HT_atanh_tune6\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 21318.719 Test MSE 3516.9993309219426 Test RE 0.9983327390484996 Lambda1 -0.00028771878\n",
      "1 Train Loss 15231.473 Test MSE 3506.304156812872 Test RE 0.9968136212925219 Lambda1 -8.162452e-05\n",
      "2 Train Loss 11612.152 Test MSE 3485.4462239844174 Test RE 0.9938443289172896 Lambda1 -0.0003178407\n",
      "3 Train Loss 8841.582 Test MSE 3473.6549770638853 Test RE 0.9921618195663631 Lambda1 -0.00027441717\n",
      "4 Train Loss 7076.74 Test MSE 3465.337984042975 Test RE 0.9909733382346211 Lambda1 -0.00020815824\n",
      "5 Train Loss 5613.7974 Test MSE 3467.484389449214 Test RE 0.9912801916029695 Lambda1 7.777502e-05\n",
      "6 Train Loss 4525.1377 Test MSE 3465.150488192254 Test RE 0.9909465290311189 Lambda1 0.00016735237\n",
      "7 Train Loss 4052.8003 Test MSE 3463.1881651718413 Test RE 0.9906659015991547 Lambda1 -0.0004925439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 3807.146 Test MSE 3465.532731568554 Test RE 0.9910011835552015 Lambda1 0.00031309723\n",
      "9 Train Loss 3667.3103 Test MSE 3460.430284636291 Test RE 0.9902713688417093 Lambda1 0.0008381022\n",
      "10 Train Loss 3581.8193 Test MSE 3452.675002425642 Test RE 0.9891610818242139 Lambda1 0.00012017148\n",
      "11 Train Loss 3534.7793 Test MSE 3446.5682058434686 Test RE 0.9882859228676135 Lambda1 0.0005354167\n",
      "12 Train Loss 3511.212 Test MSE 3436.5568562820645 Test RE 0.986849527211478 Lambda1 6.1472914e-05\n",
      "13 Train Loss 3484.8042 Test MSE 3416.206940316893 Test RE 0.9839233246216588 Lambda1 0.00030107144\n",
      "14 Train Loss 3438.2588 Test MSE 3365.960942685113 Test RE 0.9766606872839708 Lambda1 -0.0006135342\n",
      "15 Train Loss 3283.6501 Test MSE 3234.166007605044 Test RE 0.9573490810072912 Lambda1 -0.0037754318\n",
      "16 Train Loss 2917.3352 Test MSE 2865.0779709626977 Test RE 0.9010676403986019 Lambda1 0.0017197609\n",
      "17 Train Loss 849.1285 Test MSE 863.4002945036499 Test RE 0.49464687175826216 Lambda1 0.108003475\n",
      "18 Train Loss 838.0889 Test MSE 858.277806510365 Test RE 0.4931773378051668 Lambda1 0.11299841\n",
      "19 Train Loss 838.0553 Test MSE 858.2651053400623 Test RE 0.4931736886647114 Lambda1 0.11402505\n",
      "20 Train Loss 838.02325 Test MSE 858.2171263181953 Test RE 0.4931599036921285 Lambda1 0.11635001\n",
      "21 Train Loss 837.93726 Test MSE 858.0025777629525 Test RE 0.4930982564963504 Lambda1 0.10761292\n",
      "22 Train Loss 837.84686 Test MSE 857.8359706769502 Test RE 0.49305037922051376 Lambda1 0.08947943\n",
      "23 Train Loss 837.7133 Test MSE 857.4583184816346 Test RE 0.4929418374583046 Lambda1 0.06745352\n",
      "24 Train Loss 837.59125 Test MSE 857.4049411292393 Test RE 0.49292649424039076 Lambda1 0.025393033\n",
      "25 Train Loss 837.3191 Test MSE 856.7676735824122 Test RE 0.4927432760006477 Lambda1 -0.0005542087\n",
      "26 Train Loss 836.70135 Test MSE 855.7421534067702 Test RE 0.4924482896881191 Lambda1 0.0023479573\n",
      "27 Train Loss 830.6467 Test MSE 846.1473118727276 Test RE 0.48967976736461 Lambda1 0.0008137447\n",
      "28 Train Loss 817.8533 Test MSE 827.897120549931 Test RE 0.48437013372255133 Lambda1 0.00016464437\n",
      "29 Train Loss 802.8373 Test MSE 814.2507760879371 Test RE 0.4803615762268149 Lambda1 -0.0010883509\n",
      "30 Train Loss 778.6944 Test MSE 784.7984034131741 Test RE 0.47159395118201236 Lambda1 -0.000178956\n",
      "31 Train Loss 762.49023 Test MSE 769.691207934665 Test RE 0.4670328549316193 Lambda1 -0.00021210087\n",
      "32 Train Loss 753.0916 Test MSE 760.2744529756393 Test RE 0.4641671157799008 Lambda1 -0.00018542148\n",
      "33 Train Loss 744.0569 Test MSE 750.9544404381583 Test RE 0.4613132889935059 Lambda1 -0.00013572698\n",
      "34 Train Loss 733.1678 Test MSE 737.3415639964165 Test RE 0.4571129537018295 Lambda1 -3.4971912e-05\n",
      "35 Train Loss 726.71826 Test MSE 731.0185569811767 Test RE 0.45514876796703385 Lambda1 -2.195657e-05\n",
      "36 Train Loss 717.4435 Test MSE 727.6582617187509 Test RE 0.4541014649371924 Lambda1 1.2101208e-05\n",
      "37 Train Loss 712.283 Test MSE 723.6403283921628 Test RE 0.452846016362368 Lambda1 1.0290491e-05\n",
      "38 Train Loss 709.9472 Test MSE 722.5555863255752 Test RE 0.45250647935174415 Lambda1 2.0664282e-05\n",
      "39 Train Loss 708.51294 Test MSE 721.0441216455212 Test RE 0.45203294788622633 Lambda1 1.9925305e-05\n",
      "40 Train Loss 706.28345 Test MSE 719.6912315088708 Test RE 0.4516086756191757 Lambda1 2.2588085e-05\n",
      "41 Train Loss 704.8192 Test MSE 719.0762852098493 Test RE 0.4514156939480927 Lambda1 2.5245246e-05\n",
      "42 Train Loss 703.15497 Test MSE 719.8921088927034 Test RE 0.4516716968399269 Lambda1 2.8393539e-05\n",
      "43 Train Loss 701.1653 Test MSE 720.3646432718676 Test RE 0.4518199102944046 Lambda1 2.0356896e-05\n",
      "44 Train Loss 700.3781 Test MSE 719.7444164067 Test RE 0.4516253621623016 Lambda1 2.2538878e-05\n",
      "45 Train Loss 699.4151 Test MSE 719.5306319172464 Test RE 0.4515582844150509 Lambda1 1.9143317e-05\n",
      "46 Train Loss 698.2902 Test MSE 718.8610621586571 Test RE 0.45134813342924335 Lambda1 2.1824384e-05\n",
      "47 Train Loss 697.65027 Test MSE 718.5911786514597 Test RE 0.45126340019026645 Lambda1 1.8963858e-05\n",
      "48 Train Loss 697.08093 Test MSE 718.5811939740423 Test RE 0.45126026507306305 Lambda1 1.5312626e-05\n",
      "49 Train Loss 696.87994 Test MSE 718.1737217456958 Test RE 0.4511323031242375 Lambda1 1.5286001e-05\n",
      "50 Train Loss 696.30566 Test MSE 718.1483741129526 Test RE 0.4511243417936477 Lambda1 1.1032276e-05\n",
      "51 Train Loss 695.7267 Test MSE 718.1419273511842 Test RE 0.4511223169351426 Lambda1 7.32161e-06\n",
      "52 Train Loss 695.3505 Test MSE 718.0298750681958 Test RE 0.45108712106677734 Lambda1 6.3215703e-06\n",
      "53 Train Loss 695.0167 Test MSE 717.9712062350936 Test RE 0.45106869196197447 Lambda1 4.012398e-06\n",
      "54 Train Loss 694.5149 Test MSE 717.5725926414924 Test RE 0.4509434591713597 Lambda1 2.6804366e-06\n",
      "55 Train Loss 694.1674 Test MSE 717.3519202053587 Test RE 0.45087411535553046 Lambda1 9.2145643e-07\n",
      "56 Train Loss 693.92676 Test MSE 717.2104592832228 Test RE 0.4508296572513377 Lambda1 7.34366e-07\n",
      "57 Train Loss 693.7915 Test MSE 716.928878300675 Test RE 0.45074114939531373 Lambda1 5.350819e-07\n",
      "58 Train Loss 693.708 Test MSE 716.9438549639206 Test RE 0.4507458573679976 Lambda1 2.6273563e-07\n",
      "59 Train Loss 693.5776 Test MSE 717.0327632454138 Test RE 0.45077380502173703 Lambda1 6.986148e-07\n",
      "60 Train Loss 693.4467 Test MSE 716.8147139636702 Test RE 0.4507052597730807 Lambda1 1.1070603e-06\n",
      "61 Train Loss 693.175 Test MSE 716.4969264829124 Test RE 0.45060534248325024 Lambda1 1.0459702e-06\n",
      "62 Train Loss 692.9167 Test MSE 716.1059573637856 Test RE 0.4504823853519545 Lambda1 5.5287074e-07\n",
      "63 Train Loss 692.6403 Test MSE 716.1203389469922 Test RE 0.4504869088566915 Lambda1 1.0817371e-06\n",
      "64 Train Loss 692.27563 Test MSE 716.1488362568297 Test RE 0.4504958721109158 Lambda1 3.0748508e-06\n",
      "65 Train Loss 691.9005 Test MSE 715.8475244632638 Test RE 0.4504010915338193 Lambda1 3.7078437e-06\n",
      "66 Train Loss 691.6497 Test MSE 715.7277710208411 Test RE 0.45036341637584354 Lambda1 3.935251e-06\n",
      "67 Train Loss 691.56177 Test MSE 715.6763289800376 Test RE 0.45034723143061683 Lambda1 4.5020684e-06\n",
      "68 Train Loss 691.5091 Test MSE 715.6479565051967 Test RE 0.4503383044956206 Lambda1 5.046658e-06\n",
      "69 Train Loss 691.4026 Test MSE 715.5454053038026 Test RE 0.4503060369621662 Lambda1 5.396304e-06\n",
      "70 Train Loss 691.2769 Test MSE 715.4355243104565 Test RE 0.45027146055792805 Lambda1 5.330528e-06\n",
      "71 Train Loss 690.9451 Test MSE 715.1595620895663 Test RE 0.45018461143405136 Lambda1 6.206046e-06\n",
      "72 Train Loss 690.7835 Test MSE 715.0959819732484 Test RE 0.4501645995180602 Lambda1 6.2497297e-06\n",
      "73 Train Loss 690.71716 Test MSE 714.9235782922908 Test RE 0.45011033078050794 Lambda1 6.601207e-06\n",
      "74 Train Loss 690.63513 Test MSE 714.7643199613178 Test RE 0.45006019408423986 Lambda1 7.907735e-06\n",
      "Training time: 163.54\n",
      "Training time: 163.54\n",
      "inv_HT_atanh_tune6\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 9451.717 Test MSE 3586.9398918702 Test RE 1.008210509458833 Lambda1 -1.6439304e-05\n",
      "1 Train Loss 8935.273 Test MSE 3585.427648981997 Test RE 1.0079979578200402 Lambda1 -0.00027680592\n",
      "2 Train Loss 7822.675 Test MSE 3582.8304420760655 Test RE 1.007632805627977 Lambda1 -0.00052935473\n",
      "3 Train Loss 7045.9033 Test MSE 3582.4215810818196 Test RE 1.0075753100931506 Lambda1 -0.0009010381\n",
      "4 Train Loss 6419.333 Test MSE 3582.808691059022 Test RE 1.0076297470026885 Lambda1 -0.001776371\n",
      "5 Train Loss 5717.069 Test MSE 3581.623941716257 Test RE 1.0074631336709228 Lambda1 -0.0021159416\n",
      "6 Train Loss 5328.649 Test MSE 3579.6306471616776 Test RE 1.0071827510438742 Lambda1 -0.0018323984\n",
      "7 Train Loss 4830.776 Test MSE 3577.6260947582887 Test RE 1.0069007061166464 Lambda1 -9.053146e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 4706.7695 Test MSE 3575.929178751716 Test RE 1.0066618845441726 Lambda1 0.000735134\n",
      "9 Train Loss 4442.9316 Test MSE 3572.7732321878425 Test RE 1.006217570321575 Lambda1 -0.0010441669\n",
      "10 Train Loss 4297.034 Test MSE 3568.1674667168 Test RE 1.0055687890542695 Lambda1 -0.0023676013\n",
      "11 Train Loss 4106.5327 Test MSE 3560.580572053238 Test RE 1.0044991637806484 Lambda1 -0.0006200474\n",
      "12 Train Loss 3933.9998 Test MSE 3559.7587652464413 Test RE 1.0043832343876682 Lambda1 5.8616017e-05\n",
      "13 Train Loss 3853.8386 Test MSE 3558.9410135015182 Test RE 1.0042678637443692 Lambda1 0.0004097873\n",
      "14 Train Loss 3801.0898 Test MSE 3557.5071465469637 Test RE 1.0040655378886338 Lambda1 -9.625885e-05\n",
      "15 Train Loss 3714.775 Test MSE 3544.770582016664 Test RE 1.0022665517288272 Lambda1 0.00026227164\n",
      "16 Train Loss 3663.3179 Test MSE 3541.1842690457734 Test RE 1.0017594171973725 Lambda1 0.00013515576\n",
      "17 Train Loss 3627.825 Test MSE 3532.977148038748 Test RE 1.0005978947695222 Lambda1 -6.133398e-05\n",
      "18 Train Loss 3591.6138 Test MSE 3507.2020467735065 Test RE 0.9969412445121603 Lambda1 0.0010260934\n",
      "19 Train Loss 3543.05 Test MSE 3462.878612198819 Test RE 0.9906216258605168 Lambda1 -0.0012076745\n",
      "20 Train Loss 3409.9497 Test MSE 3353.5448980133383 Test RE 0.9748577153897847 Lambda1 0.00038689352\n",
      "21 Train Loss 3370.9553 Test MSE 3333.8797836738263 Test RE 0.9719952406278533 Lambda1 -0.0005539508\n",
      "22 Train Loss 2897.9187 Test MSE 2872.3732523872245 Test RE 0.9022140948564775 Lambda1 -0.018610116\n",
      "23 Train Loss 891.50635 Test MSE 904.6078423913175 Test RE 0.5063133106722963 Lambda1 -0.16591741\n",
      "24 Train Loss 838.0561 Test MSE 858.2408748794788 Test RE 0.49316672699873404 Lambda1 -0.18598612\n",
      "25 Train Loss 838.055 Test MSE 858.2631082732541 Test RE 0.4931731148901451 Lambda1 -0.18586014\n",
      "26 Train Loss 838.0548 Test MSE 858.2682051625611 Test RE 0.49317457926920294 Lambda1 -0.18575169\n",
      "27 Train Loss 838.0538 Test MSE 858.2800082522937 Test RE 0.4931779703792111 Lambda1 -0.1850572\n",
      "28 Train Loss 838.0436 Test MSE 858.2647152191535 Test RE 0.4931735765796527 Lambda1 -0.17621848\n",
      "29 Train Loss 838.0098 Test MSE 858.0777192866992 Test RE 0.49311984812329535 Lambda1 -0.16834867\n",
      "30 Train Loss 837.9332 Test MSE 857.8606241508571 Test RE 0.4930574640932295 Lambda1 -0.16532052\n",
      "31 Train Loss 837.8233 Test MSE 857.8897195006793 Test RE 0.49306582533359916 Lambda1 -0.15371186\n",
      "32 Train Loss 837.69855 Test MSE 857.3453103970706 Test RE 0.4929093529356059 Lambda1 -0.13024911\n",
      "33 Train Loss 837.3348 Test MSE 856.7345141028089 Test RE 0.49273374058678343 Lambda1 -0.07914938\n",
      "34 Train Loss 837.0762 Test MSE 856.0873392629575 Test RE 0.49254760058755104 Lambda1 -0.06033275\n",
      "35 Train Loss 835.7655 Test MSE 853.8206240283039 Test RE 0.4918950940544999 Lambda1 -0.029621542\n",
      "36 Train Loss 831.6682 Test MSE 842.0846153571893 Test RE 0.4885027772529181 Lambda1 -0.004413309\n",
      "37 Train Loss 815.52704 Test MSE 821.4554581641581 Test RE 0.48248207191079184 Lambda1 -0.00044375658\n",
      "38 Train Loss 802.51666 Test MSE 806.3979453331159 Test RE 0.4780396028307322 Lambda1 -0.0024458503\n",
      "39 Train Loss 784.9232 Test MSE 787.7121832165442 Test RE 0.4724686011788808 Lambda1 0.0005074252\n",
      "40 Train Loss 768.81274 Test MSE 772.1761988108417 Test RE 0.4677861681677724 Lambda1 1.9405546e-05\n",
      "41 Train Loss 759.0502 Test MSE 764.1610766837769 Test RE 0.4653520452206179 Lambda1 6.788587e-05\n",
      "42 Train Loss 749.79224 Test MSE 755.8034750009169 Test RE 0.4628002797673002 Lambda1 3.1091735e-05\n",
      "43 Train Loss 727.87103 Test MSE 731.0907438448947 Test RE 0.45517124000327563 Lambda1 0.00028231117\n",
      "44 Train Loss 718.77466 Test MSE 724.8084509209266 Test RE 0.45321136798204936 Lambda1 0.00025751648\n",
      "45 Train Loss 713.79785 Test MSE 719.3805820301403 Test RE 0.4515111983012639 Lambda1 0.00011964608\n",
      "46 Train Loss 710.0585 Test MSE 716.8624725185045 Test RE 0.450720273885461 Lambda1 6.409894e-05\n",
      "47 Train Loss 708.5573 Test MSE 714.8911916066289 Test RE 0.4501001354621992 Lambda1 2.4484942e-05\n",
      "48 Train Loss 704.88055 Test MSE 712.270200313583 Test RE 0.44927428124685215 Lambda1 2.9434075e-05\n",
      "49 Train Loss 703.1118 Test MSE 710.9534955300317 Test RE 0.4488588242765921 Lambda1 4.3694192e-05\n",
      "50 Train Loss 701.9814 Test MSE 710.7019595106361 Test RE 0.4487794139137444 Lambda1 3.1986405e-05\n",
      "51 Train Loss 699.6332 Test MSE 709.3933261014836 Test RE 0.4483660491251489 Lambda1 4.1505e-05\n",
      "52 Train Loss 698.1631 Test MSE 707.8264472212468 Test RE 0.44787060912809246 Lambda1 4.6017733e-05\n",
      "53 Train Loss 697.5693 Test MSE 707.7806678264425 Test RE 0.4478561256505078 Lambda1 3.0563842e-05\n",
      "54 Train Loss 696.56854 Test MSE 707.0975863101629 Test RE 0.4476399597515358 Lambda1 4.121901e-05\n",
      "55 Train Loss 694.7031 Test MSE 705.3667567946676 Test RE 0.447091758780729 Lambda1 2.0871925e-05\n",
      "56 Train Loss 694.1798 Test MSE 705.1444895810095 Test RE 0.44702131197547434 Lambda1 2.285426e-05\n",
      "57 Train Loss 692.8215 Test MSE 703.0565149585682 Test RE 0.44635899302234006 Lambda1 5.8433194e-05\n",
      "58 Train Loss 691.8626 Test MSE 702.746079881991 Test RE 0.44626043708513563 Lambda1 7.181943e-05\n",
      "59 Train Loss 690.96765 Test MSE 701.693878024008 Test RE 0.4459262253623983 Lambda1 5.1218587e-05\n",
      "60 Train Loss 690.27814 Test MSE 701.0828507612171 Test RE 0.4457320292664218 Lambda1 4.5459114e-05\n",
      "61 Train Loss 689.16113 Test MSE 699.422811204286 Test RE 0.4452040094138231 Lambda1 5.015472e-05\n",
      "62 Train Loss 688.55286 Test MSE 698.6238747680954 Test RE 0.4449496630218478 Lambda1 4.0851322e-05\n",
      "63 Train Loss 687.5599 Test MSE 698.6384423072333 Test RE 0.44495430199002184 Lambda1 2.2683002e-05\n",
      "64 Train Loss 685.94965 Test MSE 696.7719528999373 Test RE 0.44435953230805886 Lambda1 1.8243816e-05\n",
      "65 Train Loss 685.2235 Test MSE 696.0923820618722 Test RE 0.4441427846015874 Lambda1 1.2615348e-05\n",
      "66 Train Loss 684.8011 Test MSE 695.333229531972 Test RE 0.44390052933009305 Lambda1 1.6562557e-05\n",
      "67 Train Loss 684.5608 Test MSE 694.9196723248901 Test RE 0.4437685022963199 Lambda1 1.8565514e-05\n",
      "68 Train Loss 683.40027 Test MSE 694.2430150935484 Test RE 0.4435523965285209 Lambda1 2.4423054e-05\n",
      "69 Train Loss 681.9874 Test MSE 692.6672934994464 Test RE 0.4430487457129789 Lambda1 2.7663025e-05\n",
      "70 Train Loss 680.834 Test MSE 690.9665668188833 Test RE 0.4425044960247666 Lambda1 3.1324624e-05\n",
      "71 Train Loss 679.86 Test MSE 689.4788918528252 Test RE 0.44202787558325973 Lambda1 3.2752083e-05\n",
      "72 Train Loss 679.27826 Test MSE 689.427567380284 Test RE 0.44201142310730573 Lambda1 2.7962318e-05\n",
      "73 Train Loss 678.807 Test MSE 689.111800034835 Test RE 0.44191018784666153 Lambda1 1.8507804e-05\n",
      "74 Train Loss 678.11523 Test MSE 688.6157081643898 Test RE 0.4417510935448035 Lambda1 1.581143e-05\n",
      "Training time: 161.76\n",
      "Training time: 161.76\n",
      "inv_HT_atanh_tune6\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 50000.297 Test MSE 3575.9017010394773 Test RE 1.0066580169034367 Lambda1 0.00092176785\n",
      "1 Train Loss 44134.7 Test MSE 3577.068433263582 Test RE 1.0068222278714467 Lambda1 -0.00037539913\n",
      "2 Train Loss 40624.863 Test MSE 3579.451926611203 Test RE 1.0071576078768456 Lambda1 -0.00015432174\n",
      "3 Train Loss 31812.809 Test MSE 3571.310068980855 Test RE 1.0060115103232807 Lambda1 -0.00022997512\n",
      "4 Train Loss 26406.836 Test MSE 3572.834948618609 Test RE 1.006226261033915 Lambda1 -0.00038400717\n",
      "5 Train Loss 23606.23 Test MSE 3569.746225836171 Test RE 1.0057912247084175 Lambda1 0.00032530777\n",
      "6 Train Loss 18956.24 Test MSE 3561.8455211740256 Test RE 1.0046775795579246 Lambda1 -5.330259e-05\n",
      "7 Train Loss 14253.208 Test MSE 3558.6716304442243 Test RE 1.0042298555467155 Lambda1 -4.5385368e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 11005.93 Test MSE 3555.88732054231 Test RE 1.0038369232182032 Lambda1 -0.0002599398\n",
      "9 Train Loss 9297.105 Test MSE 3557.6526979949713 Test RE 1.0040860777908918 Lambda1 0.0001757507\n",
      "10 Train Loss 8165.751 Test MSE 3556.9558544755287 Test RE 1.0039877369463182 Lambda1 8.01606e-05\n",
      "11 Train Loss 7426.58 Test MSE 3555.5602035005168 Test RE 1.0037907491289388 Lambda1 -1.9829495e-06\n",
      "12 Train Loss 6945.6143 Test MSE 3556.681464490357 Test RE 1.0039490114877285 Lambda1 0.00011133931\n",
      "13 Train Loss 6216.984 Test MSE 3559.812964407985 Test RE 1.0043908804847155 Lambda1 -0.0002537687\n",
      "14 Train Loss 5740.6436 Test MSE 3561.577833198584 Test RE 1.004639825932896 Lambda1 0.0003046428\n",
      "15 Train Loss 5398.377 Test MSE 3560.5357232654073 Test RE 1.0044928374652027 Lambda1 -0.00017109654\n",
      "16 Train Loss 5069.0234 Test MSE 3558.349015753141 Test RE 1.0041843348185813 Lambda1 5.123488e-05\n",
      "17 Train Loss 4889.7407 Test MSE 3557.813679581084 Test RE 1.0041087946711833 Lambda1 -0.00023521871\n",
      "18 Train Loss 4633.4907 Test MSE 3556.86263669859 Test RE 1.0039745810172138 Lambda1 0.00021527144\n",
      "19 Train Loss 4522.481 Test MSE 3555.929955086405 Test RE 1.0038429411256091 Lambda1 -3.2605065e-05\n",
      "20 Train Loss 4432.7866 Test MSE 3555.546857975101 Test RE 1.0037888653009683 Lambda1 -0.00020175382\n",
      "21 Train Loss 4293.4927 Test MSE 3555.1581905759767 Test RE 1.0037340002287933 Lambda1 7.081201e-05\n",
      "22 Train Loss 4180.3135 Test MSE 3556.018846244299 Test RE 1.0038554880830652 Lambda1 0.00023670444\n",
      "23 Train Loss 4056.5598 Test MSE 3557.570237071691 Test RE 1.0040744411371982 Lambda1 -4.5667406e-05\n",
      "24 Train Loss 3974.8381 Test MSE 3558.6154013546957 Test RE 1.0042219218063524 Lambda1 1.7266291e-05\n",
      "25 Train Loss 3924.4775 Test MSE 3558.6746304337717 Test RE 1.0042302788336535 Lambda1 -1.18317985e-05\n",
      "26 Train Loss 3891.5078 Test MSE 3557.5821018046854 Test RE 1.0040761154632727 Lambda1 -9.549876e-05\n",
      "27 Train Loss 3856.9946 Test MSE 3555.3104852826477 Test RE 1.0037554988133712 Lambda1 -8.733573e-05\n",
      "28 Train Loss 3837.0984 Test MSE 3554.002167278325 Test RE 1.0035707959255862 Lambda1 2.4433051e-05\n",
      "29 Train Loss 3800.0364 Test MSE 3551.0246078258738 Test RE 1.003150309708715 Lambda1 -5.73956e-06\n",
      "30 Train Loss 3772.3235 Test MSE 3547.883488407275 Test RE 1.0027065346867086 Lambda1 -0.00012777754\n",
      "31 Train Loss 3749.067 Test MSE 3546.47638283736 Test RE 1.0025076761452831 Lambda1 -2.7222666e-07\n",
      "32 Train Loss 3721.7178 Test MSE 3542.7168028448655 Test RE 1.001976161651014 Lambda1 -2.3858274e-05\n",
      "33 Train Loss 3703.4043 Test MSE 3540.9151488665716 Test RE 1.0017213510059972 Lambda1 -7.5727214e-05\n",
      "34 Train Loss 3691.4055 Test MSE 3538.985340209559 Test RE 1.001448343334818 Lambda1 4.895298e-05\n",
      "35 Train Loss 3678.8835 Test MSE 3535.7255904634253 Test RE 1.0009870214106937 Lambda1 3.779581e-05\n",
      "36 Train Loss 3669.6177 Test MSE 3533.5242415766493 Test RE 1.0006753647655742 Lambda1 0.00015313011\n",
      "37 Train Loss 3659.401 Test MSE 3530.1430874489365 Test RE 1.000196487703887 Lambda1 -2.4033232e-05\n",
      "38 Train Loss 3648.8884 Test MSE 3526.298781762907 Test RE 0.9996517352204446 Lambda1 0.00013112926\n",
      "39 Train Loss 3642.0862 Test MSE 3525.5892794057518 Test RE 0.999551163631641 Lambda1 0.00029589722\n",
      "40 Train Loss 3633.9849 Test MSE 3522.3774299673205 Test RE 0.9990957590026127 Lambda1 0.00025738613\n",
      "41 Train Loss 3626.443 Test MSE 3520.1098905017675 Test RE 0.9987741220075076 Lambda1 0.00026141683\n",
      "42 Train Loss 3619.2776 Test MSE 3516.2061943058698 Test RE 0.9982201631247624 Lambda1 0.00017311638\n",
      "43 Train Loss 3613.2756 Test MSE 3512.8324694990306 Test RE 0.9977411627273812 Lambda1 0.0003052318\n",
      "44 Train Loss 3605.2659 Test MSE 3506.923548576689 Test RE 0.9969016614132613 Lambda1 0.00010427999\n",
      "45 Train Loss 3596.1392 Test MSE 3502.7096079564503 Test RE 0.9963025398387519 Lambda1 0.00025509455\n",
      "46 Train Loss 3584.938 Test MSE 3493.845922816051 Test RE 0.9950411583952747 Lambda1 2.832404e-05\n",
      "47 Train Loss 3579.012 Test MSE 3490.7993486228556 Test RE 0.994607234301819 Lambda1 5.793299e-06\n",
      "48 Train Loss 3567.2502 Test MSE 3480.8097812865785 Test RE 0.993183088519776 Lambda1 0.000357606\n",
      "49 Train Loss 3560.772 Test MSE 3475.476453822177 Test RE 0.9924219148923217 Lambda1 0.0003573251\n",
      "50 Train Loss 3549.0815 Test MSE 3465.1263641284017 Test RE 0.990943079585124 Lambda1 0.0004063593\n",
      "51 Train Loss 3538.5015 Test MSE 3451.311095814754 Test RE 0.9889656888897781 Lambda1 0.0003212807\n",
      "52 Train Loss 3524.8142 Test MSE 3435.4424440981134 Test RE 0.9866895056615577 Lambda1 0.0002824617\n",
      "53 Train Loss 3514.7793 Test MSE 3423.7723703523016 Test RE 0.9850122057336005 Lambda1 0.000120369106\n",
      "54 Train Loss 3504.9282 Test MSE 3413.9784865469255 Test RE 0.9836023567514747 Lambda1 -0.00029778786\n",
      "55 Train Loss 3492.3972 Test MSE 3401.8461737080293 Test RE 0.9818530791294591 Lambda1 -5.9103582e-05\n",
      "56 Train Loss 3482.503 Test MSE 3388.21412214171 Test RE 0.979883838459833 Lambda1 7.438038e-05\n",
      "57 Train Loss 3471.31 Test MSE 3380.848363707178 Test RE 0.9788181569090985 Lambda1 -4.015659e-05\n",
      "58 Train Loss 3451.4133 Test MSE 3351.8683514836425 Test RE 0.9746140033339996 Lambda1 -1.9832434e-05\n",
      "59 Train Loss 3435.2966 Test MSE 3336.804141151709 Test RE 0.9724214465324125 Lambda1 -5.414178e-05\n",
      "60 Train Loss 3415.7268 Test MSE 3306.4242119633036 Test RE 0.9679846197396804 Lambda1 0.00019027198\n",
      "61 Train Loss 3402.397 Test MSE 3289.1308019299113 Test RE 0.9654499025428703 Lambda1 -0.00028914388\n",
      "62 Train Loss 3381.91 Test MSE 3257.667950176283 Test RE 0.9608212027414992 Lambda1 -1.757139e-05\n",
      "63 Train Loss 3353.7803 Test MSE 3208.2264004050294 Test RE 0.9535021458939071 Lambda1 0.000420951\n",
      "64 Train Loss 3325.2893 Test MSE 3194.737196059537 Test RE 0.9514955023545464 Lambda1 7.191832e-05\n",
      "65 Train Loss 3305.061 Test MSE 3183.380421492482 Test RE 0.9498027902956078 Lambda1 0.0007929395\n",
      "66 Train Loss 3272.6401 Test MSE 3146.8748345940066 Test RE 0.9443411297795794 Lambda1 0.00079381943\n",
      "67 Train Loss 3243.0042 Test MSE 3114.582462156652 Test RE 0.9394833496073742 Lambda1 0.00094409066\n",
      "68 Train Loss 3204.3535 Test MSE 3063.597260277432 Test RE 0.9317620261795171 Lambda1 0.00078505685\n",
      "69 Train Loss 3164.004 Test MSE 3012.7325576131407 Test RE 0.9239946593229991 Lambda1 -0.00073732855\n",
      "70 Train Loss 2994.2576 Test MSE 2888.1346049949602 Test RE 0.9046860334472373 Lambda1 0.00029839762\n",
      "71 Train Loss 2894.2068 Test MSE 2824.00268848224 Test RE 0.8945852300983836 Lambda1 0.00042089893\n",
      "72 Train Loss 2797.7668 Test MSE 2726.794478990701 Test RE 0.8790536339842671 Lambda1 -0.00016919863\n",
      "73 Train Loss 2587.7312 Test MSE 2532.366728698911 Test RE 0.8471346972640179 Lambda1 0.0013546424\n",
      "74 Train Loss 928.05835 Test MSE 935.6760599668369 Test RE 0.514934428309093 Lambda1 -0.0076852865\n",
      "Training time: 159.02\n",
      "Training time: 159.02\n",
      "inv_HT_atanh_tune6\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 44608.797 Test MSE 3528.586724467711 Test RE 0.9999759810045835 Lambda1 -0.00026405687\n",
      "1 Train Loss 31508.252 Test MSE 3518.954348001084 Test RE 0.9986101753248869 Lambda1 -5.8194088e-05\n",
      "2 Train Loss 25941.629 Test MSE 3512.5527318528057 Test RE 0.997701435338791 Lambda1 -0.00051906897\n",
      "3 Train Loss 17541.56 Test MSE 3503.0782783003488 Test RE 0.9963549703249427 Lambda1 0.0006204638\n",
      "4 Train Loss 13382.036 Test MSE 3497.7450015944787 Test RE 0.9955962289564653 Lambda1 -0.0007786184\n",
      "5 Train Loss 11095.861 Test MSE 3488.0721638797745 Test RE 0.9942186402591366 Lambda1 0.00053650205\n",
      "6 Train Loss 10045.102 Test MSE 3488.2318609003046 Test RE 0.9942413995269888 Lambda1 -0.00034721842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 8455.141 Test MSE 3485.3815770657284 Test RE 0.993835112124473 Lambda1 -6.744763e-05\n",
      "8 Train Loss 7264.9136 Test MSE 3485.1878274399614 Test RE 0.9938074884837234 Lambda1 2.896256e-05\n",
      "9 Train Loss 5906.2446 Test MSE 3481.9916536356754 Test RE 0.9933516866451243 Lambda1 -2.781664e-05\n",
      "10 Train Loss 5172.031 Test MSE 3477.9389223170415 Test RE 0.9927734314410487 Lambda1 -1.3258599e-05\n",
      "11 Train Loss 4629.8066 Test MSE 3478.283595475829 Test RE 0.9928226234891031 Lambda1 0.00057562144\n",
      "12 Train Loss 4138.725 Test MSE 3472.402975565017 Test RE 0.991983002147341 Lambda1 -0.0005109655\n",
      "13 Train Loss 3911.5137 Test MSE 3467.75853770578 Test RE 0.9913193774124095 Lambda1 -0.00022591448\n",
      "14 Train Loss 3811.1782 Test MSE 3465.249877427198 Test RE 0.990960740349142 Lambda1 -0.00025159176\n",
      "15 Train Loss 3715.3455 Test MSE 3455.191683568179 Test RE 0.9895215196830981 Lambda1 3.8568585e-05\n",
      "16 Train Loss 3630.5657 Test MSE 3451.7876069501353 Test RE 0.9890339581482156 Lambda1 0.00031657703\n",
      "17 Train Loss 3560.0564 Test MSE 3430.166166595392 Test RE 0.9859315176918185 Lambda1 -2.781651e-05\n",
      "18 Train Loss 3508.2742 Test MSE 3402.59140746729 Test RE 0.9819606192642512 Lambda1 2.8936509e-05\n",
      "19 Train Loss 3463.0205 Test MSE 3368.57573395769 Test RE 0.9770399650169964 Lambda1 0.00037178813\n",
      "20 Train Loss 3404.2773 Test MSE 3313.263410328903 Test RE 0.968985220164827 Lambda1 -0.00023094942\n",
      "21 Train Loss 3243.999 Test MSE 3174.80277517136 Test RE 0.9485223011887011 Lambda1 -0.00019299529\n",
      "22 Train Loss 3110.9556 Test MSE 3062.3384975332847 Test RE 0.9315705865537728 Lambda1 8.019621e-05\n",
      "23 Train Loss 3049.49 Test MSE 3004.0309374566937 Test RE 0.9226593160051829 Lambda1 0.00026995764\n",
      "24 Train Loss 2225.6785 Test MSE 2196.766448481062 Test RE 0.7890074684235443 Lambda1 0.009647477\n",
      "25 Train Loss 874.4679 Test MSE 888.6913345599424 Test RE 0.5018392706773606 Lambda1 0.039988216\n",
      "26 Train Loss 841.97766 Test MSE 860.214249052817 Test RE 0.4937333765990138 Lambda1 0.043590717\n",
      "27 Train Loss 838.47974 Test MSE 858.0167506596063 Test RE 0.4931023290958684 Lambda1 0.044573292\n",
      "28 Train Loss 837.99 Test MSE 858.1561512023839 Test RE 0.4931423842233051 Lambda1 0.045045048\n",
      "29 Train Loss 837.8696 Test MSE 857.9036433805226 Test RE 0.49306982664128124 Lambda1 0.044794008\n",
      "30 Train Loss 837.8181 Test MSE 857.673673086149 Test RE 0.4930037358780471 Lambda1 0.044697072\n",
      "31 Train Loss 837.70874 Test MSE 857.6104175340422 Test RE 0.492985555421132 Lambda1 0.044782802\n",
      "32 Train Loss 837.40076 Test MSE 856.7625979979482 Test RE 0.492741816466227 Lambda1 0.04389531\n",
      "33 Train Loss 835.3189 Test MSE 853.9541523057366 Test RE 0.4919335560774089 Lambda1 0.043837633\n",
      "34 Train Loss 829.09656 Test MSE 845.2488787466456 Test RE 0.489419729093188 Lambda1 0.042815726\n",
      "35 Train Loss 820.8299 Test MSE 835.3123428114421 Test RE 0.4865344760412304 Lambda1 0.04102821\n",
      "36 Train Loss 806.2483 Test MSE 804.2993729414644 Test RE 0.4774171717765574 Lambda1 0.027990242\n",
      "37 Train Loss 783.96783 Test MSE 781.8852848035863 Test RE 0.4707178750122369 Lambda1 0.01526194\n",
      "38 Train Loss 763.10754 Test MSE 763.6205760258913 Test RE 0.465187441456859 Lambda1 0.0068706092\n",
      "39 Train Loss 754.8247 Test MSE 759.5246863490503 Test RE 0.46393818341521154 Lambda1 0.004035525\n",
      "40 Train Loss 751.53796 Test MSE 757.9656744460038 Test RE 0.46346179554977507 Lambda1 0.0034167934\n",
      "41 Train Loss 746.0566 Test MSE 755.4798018004001 Test RE 0.4627011719345766 Lambda1 0.002725343\n",
      "42 Train Loss 740.61017 Test MSE 753.2449150012055 Test RE 0.4620162756353251 Lambda1 0.0017146277\n",
      "43 Train Loss 737.41296 Test MSE 751.0782987015559 Test RE 0.4613513306535523 Lambda1 0.0015929303\n",
      "44 Train Loss 734.63837 Test MSE 747.7917757553181 Test RE 0.4603408474285023 Lambda1 0.0011090488\n",
      "45 Train Loss 731.30524 Test MSE 744.9550296044339 Test RE 0.45946686687818983 Lambda1 0.0008139967\n",
      "46 Train Loss 721.87024 Test MSE 735.2199051771877 Test RE 0.4564548215376414 Lambda1 0.000710306\n",
      "47 Train Loss 716.0083 Test MSE 727.6265259934632 Test RE 0.45409156235157183 Lambda1 0.00021085305\n",
      "48 Train Loss 701.4885 Test MSE 711.3367762727063 Test RE 0.4489797996736891 Lambda1 7.661028e-05\n",
      "49 Train Loss 690.3484 Test MSE 698.1528388764011 Test RE 0.44479963765879155 Lambda1 8.03374e-05\n",
      "50 Train Loss 685.47235 Test MSE 694.4341741431074 Test RE 0.44361345815742237 Lambda1 1.7937884e-05\n",
      "51 Train Loss 682.948 Test MSE 693.71150405913 Test RE 0.44338257261365616 Lambda1 4.0942952e-05\n",
      "52 Train Loss 681.29895 Test MSE 692.2617866887493 Test RE 0.44291904016196193 Lambda1 4.0060968e-05\n",
      "53 Train Loss 679.4872 Test MSE 688.7353661027479 Test RE 0.44178947251904094 Lambda1 3.8994967e-05\n",
      "54 Train Loss 676.716 Test MSE 685.5139410043143 Test RE 0.4407550695801677 Lambda1 8.680928e-06\n",
      "55 Train Loss 675.3708 Test MSE 684.2998471563002 Test RE 0.4403645923560885 Lambda1 9.949797e-05\n",
      "56 Train Loss 673.6359 Test MSE 679.9399457606294 Test RE 0.43895949622489927 Lambda1 2.7197077e-05\n",
      "57 Train Loss 669.53986 Test MSE 672.0967196381399 Test RE 0.43642041554388417 Lambda1 1.1347283e-05\n",
      "58 Train Loss 662.221 Test MSE 660.3988815982839 Test RE 0.4326057974110983 Lambda1 -0.00014300001\n",
      "59 Train Loss 643.6074 Test MSE 615.212391515916 Test RE 0.4175434812648603 Lambda1 -0.0001727828\n",
      "60 Train Loss 629.4719 Test MSE 594.2753134283913 Test RE 0.4103770030322314 Lambda1 -5.7499456e-05\n",
      "61 Train Loss 613.72125 Test MSE 584.5696049280015 Test RE 0.4070120674190216 Lambda1 -0.00023676516\n",
      "62 Train Loss 592.3985 Test MSE 563.8598124488313 Test RE 0.39973736183605696 Lambda1 -0.00018847587\n",
      "63 Train Loss 568.25385 Test MSE 543.040464029303 Test RE 0.39228821962231897 Lambda1 -2.380795e-05\n",
      "64 Train Loss 538.6692 Test MSE 519.2517681095687 Test RE 0.38359961502490453 Lambda1 0.00019326316\n",
      "65 Train Loss 524.5897 Test MSE 518.6038094634591 Test RE 0.3833601991071456 Lambda1 0.00012340468\n",
      "66 Train Loss 518.68805 Test MSE 521.8630108992552 Test RE 0.38456293920685314 Lambda1 2.9909961e-05\n",
      "67 Train Loss 509.9911 Test MSE 519.8081166997385 Test RE 0.38380506253918856 Lambda1 3.612614e-05\n",
      "68 Train Loss 503.61255 Test MSE 514.12869754851 Test RE 0.3817025783775629 Lambda1 1.8312605e-05\n",
      "69 Train Loss 500.577 Test MSE 512.9390111786311 Test RE 0.3812606954791026 Lambda1 2.303166e-05\n",
      "70 Train Loss 496.23032 Test MSE 514.2546861812199 Test RE 0.38174934413911643 Lambda1 1.764404e-05\n",
      "71 Train Loss 494.7003 Test MSE 514.6391947590383 Test RE 0.3818920345934923 Lambda1 1.0705673e-05\n",
      "72 Train Loss 492.19083 Test MSE 512.6470289819443 Test RE 0.3811521668047127 Lambda1 2.6211865e-05\n",
      "73 Train Loss 488.6728 Test MSE 509.61765551491726 Test RE 0.3800243312332211 Lambda1 3.323082e-05\n",
      "74 Train Loss 483.32077 Test MSE 506.0399265654695 Test RE 0.3786880169079859 Lambda1 1.978915e-05\n",
      "Training time: 162.49\n",
      "Training time: 162.49\n",
      "inv_HT_atanh_tune6\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 16458.46 Test MSE 3607.7351670342728 Test RE 1.0111288347948812 Lambda1 -0.0002532708\n",
      "1 Train Loss 12112.119 Test MSE 3606.5250560103514 Test RE 1.0109592435258894 Lambda1 -0.00034816933\n",
      "2 Train Loss 9827.51 Test MSE 3605.1850294022074 Test RE 1.010771412008523 Lambda1 0.00025707757\n",
      "3 Train Loss 8895.267 Test MSE 3604.1105288282342 Test RE 1.0106207740537692 Lambda1 0.00024449918\n",
      "4 Train Loss 7510.6895 Test MSE 3604.55766370691 Test RE 1.0106834621676595 Lambda1 -0.00083526515\n",
      "5 Train Loss 6442.6035 Test MSE 3611.9628191076463 Test RE 1.0117210968574488 Lambda1 -0.00018301315\n",
      "6 Train Loss 5127.6865 Test MSE 3614.431839265801 Test RE 1.0120668275823343 Lambda1 0.001078353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 4706.4297 Test MSE 3611.4215521647757 Test RE 1.0116452888093703 Lambda1 -0.00029524235\n",
      "8 Train Loss 4175.0347 Test MSE 3610.0177740384775 Test RE 1.0114486538365708 Lambda1 0.0007893463\n",
      "9 Train Loss 3931.759 Test MSE 3609.3984895523326 Test RE 1.0113618950774188 Lambda1 -8.7834196e-05\n",
      "10 Train Loss 3768.3079 Test MSE 3602.7773408867365 Test RE 1.010433838603235 Lambda1 0.00030606083\n",
      "11 Train Loss 3695.0032 Test MSE 3600.8468775854462 Test RE 1.0101630937547845 Lambda1 0.0001452589\n",
      "12 Train Loss 3613.1797 Test MSE 3556.4581906672156 Test RE 1.0039174990998085 Lambda1 -0.001329174\n",
      "13 Train Loss 3340.317 Test MSE 3292.6997978693494 Test RE 0.9659735595175992 Lambda1 -0.00018559353\n",
      "14 Train Loss 2505.079 Test MSE 2467.117219917941 Test RE 0.8361497474365147 Lambda1 -0.019509025\n",
      "15 Train Loss 853.1693 Test MSE 869.4970620997237 Test RE 0.4963902357255479 Lambda1 -0.06687489\n",
      "16 Train Loss 838.5913 Test MSE 857.373979130416 Test RE 0.49291759405415 Lambda1 -0.06859638\n",
      "17 Train Loss 836.48596 Test MSE 855.1959382500263 Test RE 0.49229110118519614 Lambda1 -0.057716243\n",
      "18 Train Loss 825.981 Test MSE 837.6125636368721 Test RE 0.4872039066762906 Lambda1 -0.017620359\n",
      "19 Train Loss 782.0682 Test MSE 776.8584667487862 Test RE 0.4692022891658399 Lambda1 -0.005736118\n",
      "20 Train Loss 733.107 Test MSE 709.796074904103 Test RE 0.44849330805170157 Lambda1 -0.0054555438\n",
      "21 Train Loss 668.23987 Test MSE 646.0452347790533 Test RE 0.4278786672219434 Lambda1 -0.001774704\n",
      "22 Train Loss 602.7079 Test MSE 586.2305021583057 Test RE 0.4075898649293471 Lambda1 -0.00044187903\n",
      "23 Train Loss 534.05664 Test MSE 524.5345488342244 Test RE 0.38554601615585427 Lambda1 1.0501022e-05\n",
      "24 Train Loss 500.89093 Test MSE 498.39942954776825 Test RE 0.37581831316564324 Lambda1 -1.3781032e-06\n",
      "25 Train Loss 476.30832 Test MSE 479.76432657178344 Test RE 0.36872547759220303 Lambda1 3.888291e-05\n",
      "26 Train Loss 452.2644 Test MSE 466.13263915676345 Test RE 0.3634493759792998 Lambda1 0.00010594867\n",
      "27 Train Loss 444.3938 Test MSE 460.80293231253455 Test RE 0.36136558326652257 Lambda1 7.886334e-05\n",
      "28 Train Loss 434.44327 Test MSE 455.0384709451151 Test RE 0.35909820009047005 Lambda1 0.00017350177\n",
      "29 Train Loss 429.61047 Test MSE 453.8887297591855 Test RE 0.3586442482320641 Lambda1 0.00014909018\n",
      "30 Train Loss 424.26038 Test MSE 453.48837963183263 Test RE 0.3584860432175714 Lambda1 0.0001328132\n",
      "31 Train Loss 419.93716 Test MSE 451.12729647503755 Test RE 0.357551598160617 Lambda1 0.00011182601\n",
      "32 Train Loss 418.07587 Test MSE 452.96239070904824 Test RE 0.35827808375903764 Lambda1 6.8721354e-05\n",
      "33 Train Loss 414.0868 Test MSE 449.8244524571308 Test RE 0.35703492495102196 Lambda1 5.9543494e-05\n",
      "34 Train Loss 410.8744 Test MSE 450.20304276098545 Test RE 0.3571851408083778 Lambda1 3.0152292e-05\n",
      "35 Train Loss 410.076 Test MSE 448.8276516650217 Test RE 0.3566391149018454 Lambda1 3.8294445e-05\n",
      "36 Train Loss 409.515 Test MSE 448.05873470833853 Test RE 0.35633349267550757 Lambda1 4.2572716e-05\n",
      "37 Train Loss 406.91678 Test MSE 446.0388211477192 Test RE 0.3555293839386809 Lambda1 6.327636e-05\n",
      "38 Train Loss 405.39752 Test MSE 447.938858724952 Test RE 0.35628582182119173 Lambda1 2.6940328e-05\n",
      "39 Train Loss 405.07428 Test MSE 449.7681520671133 Test RE 0.3570125808627616 Lambda1 1.0800641e-05\n",
      "40 Train Loss 404.85522 Test MSE 450.8825668189506 Test RE 0.35745460186835115 Lambda1 6.9587554e-06\n",
      "41 Train Loss 404.2768 Test MSE 451.09296784449054 Test RE 0.3575379939181752 Lambda1 1.0301475e-05\n",
      "42 Train Loss 404.01654 Test MSE 450.43266806880104 Test RE 0.35727622003752674 Lambda1 1.2940174e-05\n",
      "43 Train Loss 401.75137 Test MSE 447.3714777251929 Test RE 0.35606010590528375 Lambda1 7.2832167e-06\n",
      "44 Train Loss 400.84845 Test MSE 446.8863406652715 Test RE 0.3558669947919001 Lambda1 5.3154163e-06\n",
      "45 Train Loss 400.74606 Test MSE 446.2575622665066 Test RE 0.355616550525117 Lambda1 7.1811382e-06\n",
      "46 Train Loss 400.58615 Test MSE 446.10514450559936 Test RE 0.3555558155237435 Lambda1 5.579537e-06\n",
      "47 Train Loss 400.2252 Test MSE 444.96106204216414 Test RE 0.35509959312428113 Lambda1 5.191637e-06\n",
      "48 Train Loss 399.68484 Test MSE 443.7550195487923 Test RE 0.35461802762730493 Lambda1 3.8672724e-06\n",
      "49 Train Loss 394.37637 Test MSE 440.69852004850907 Test RE 0.35339464688989336 Lambda1 1.6229565e-06\n",
      "50 Train Loss 393.4483 Test MSE 439.4664464285275 Test RE 0.35290030330914246 Lambda1 1.4551231e-06\n",
      "51 Train Loss 392.33148 Test MSE 439.7965296517616 Test RE 0.35303281013134524 Lambda1 1.510973e-06\n",
      "52 Train Loss 391.1945 Test MSE 437.51656991271864 Test RE 0.35211653814167343 Lambda1 1.2803357e-06\n",
      "53 Train Loss 390.66373 Test MSE 436.7923893053175 Test RE 0.35182500452760834 Lambda1 1.4417625e-06\n",
      "54 Train Loss 390.5039 Test MSE 436.22356392140966 Test RE 0.3515958428069538 Lambda1 2.2689326e-06\n",
      "55 Train Loss 390.3842 Test MSE 435.74191937665995 Test RE 0.35140168664060106 Lambda1 1.4243429e-06\n",
      "56 Train Loss 389.92865 Test MSE 434.25689707630386 Test RE 0.35080238153782906 Lambda1 3.055684e-06\n",
      "57 Train Loss 389.2201 Test MSE 432.0675797766898 Test RE 0.34991697456917226 Lambda1 2.352778e-06\n",
      "58 Train Loss 388.2817 Test MSE 430.1024331192135 Test RE 0.34912031465522764 Lambda1 7.021991e-06\n",
      "59 Train Loss 387.54132 Test MSE 428.8602447980969 Test RE 0.34861579907664964 Lambda1 9.412416e-06\n",
      "60 Train Loss 387.12692 Test MSE 428.40261230157 Test RE 0.34842974720608133 Lambda1 1.4486816e-05\n",
      "61 Train Loss 385.5094 Test MSE 426.7313830905976 Test RE 0.34774945841990795 Lambda1 1.618903e-05\n",
      "62 Train Loss 384.52298 Test MSE 426.56106146060245 Test RE 0.3476800527373527 Lambda1 -4.7743247e-06\n",
      "63 Train Loss 384.13055 Test MSE 425.508856758448 Test RE 0.34725097411525035 Lambda1 -1.0661499e-06\n",
      "64 Train Loss 383.90656 Test MSE 424.5704828183226 Test RE 0.34686786679822845 Lambda1 -1.13433525e-05\n",
      "65 Train Loss 383.38052 Test MSE 425.3313723848149 Test RE 0.34717854548209626 Lambda1 -2.5743202e-06\n",
      "66 Train Loss 382.76785 Test MSE 424.9068556102464 Test RE 0.3470052454159895 Lambda1 -1.0469319e-05\n",
      "67 Train Loss 381.91284 Test MSE 424.4826140320735 Test RE 0.34683197118511877 Lambda1 -3.0866472e-06\n",
      "68 Train Loss 380.7938 Test MSE 422.02451785731773 Test RE 0.3458262949424573 Lambda1 -3.7374912e-06\n",
      "69 Train Loss 377.64554 Test MSE 418.83396206497014 Test RE 0.34451657076802966 Lambda1 -3.444671e-06\n",
      "70 Train Loss 373.4844 Test MSE 415.2009064178694 Test RE 0.34301911104845095 Lambda1 1.5512376e-07\n",
      "71 Train Loss 371.81775 Test MSE 413.5588630933427 Test RE 0.34234015030139386 Lambda1 4.3476203e-07\n",
      "72 Train Loss 370.2282 Test MSE 410.90139661287066 Test RE 0.34123846471183517 Lambda1 -1.3512809e-05\n",
      "73 Train Loss 367.8723 Test MSE 407.18702742349086 Test RE 0.339692640033241 Lambda1 -4.530692e-05\n",
      "74 Train Loss 366.23804 Test MSE 403.73836971120994 Test RE 0.3382510731551514 Lambda1 -5.531371e-05\n",
      "Training time: 167.47\n",
      "Training time: 167.47\n",
      "inv_HT_atanh_tune6\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 26084.783 Test MSE 3392.172569352154 Test RE 0.9804560700342682 Lambda1 0.0003171794\n",
      "1 Train Loss 15279.675 Test MSE 3373.3787182364417 Test RE 0.9777362587051863 Lambda1 -0.000845721\n",
      "2 Train Loss 10113.852 Test MSE 3368.586359725666 Test RE 0.9770415059934515 Lambda1 -0.0003245445\n",
      "3 Train Loss 6433.033 Test MSE 3352.7470523392353 Test RE 0.9747417437387055 Lambda1 0.00046113587\n",
      "4 Train Loss 4869.955 Test MSE 3349.472613705515 Test RE 0.9742656398676667 Lambda1 -0.0008486967\n",
      "5 Train Loss 3935.2822 Test MSE 3348.0858590840703 Test RE 0.9740639353896451 Lambda1 0.00019791444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 3540.5837 Test MSE 3348.303043881369 Test RE 0.9740955278362883 Lambda1 -0.00014975457\n",
      "7 Train Loss 3415.4648 Test MSE 3327.0099265320873 Test RE 0.9709932680976511 Lambda1 -0.00042261824\n",
      "8 Train Loss 3325.725 Test MSE 3270.858219899168 Test RE 0.9627644161391449 Lambda1 -2.0097215e-05\n",
      "9 Train Loss 3056.3247 Test MSE 3023.0046652268725 Test RE 0.9255685288573305 Lambda1 0.0018595102\n",
      "10 Train Loss 1594.6528 Test MSE 1588.366173665878 Test RE 0.6709103403308507 Lambda1 -0.014337542\n",
      "11 Train Loss 838.3323 Test MSE 858.0578111653822 Test RE 0.49311412769350127 Lambda1 -0.034655344\n",
      "12 Train Loss 838.0619 Test MSE 858.2798336872409 Test RE 0.49317792022562373 Lambda1 -0.035298895\n",
      "13 Train Loss 837.96576 Test MSE 858.2571858690908 Test RE 0.4931714133282818 Lambda1 -0.03500216\n",
      "14 Train Loss 837.88324 Test MSE 857.898579270557 Test RE 0.49306837137090986 Lambda1 -0.03442855\n",
      "15 Train Loss 837.8611 Test MSE 857.7721628882695 Test RE 0.49303204177575255 Lambda1 -0.035480753\n",
      "16 Train Loss 837.81146 Test MSE 857.9199560792734 Test RE 0.4930745143829482 Lambda1 -0.037149873\n",
      "17 Train Loss 837.6343 Test MSE 857.3429690091275 Test RE 0.49290867987377773 Lambda1 -0.022834338\n",
      "18 Train Loss 837.37915 Test MSE 857.0579231064221 Test RE 0.49282673292548695 Lambda1 0.14203979\n",
      "19 Train Loss 837.09406 Test MSE 856.4699301343232 Test RE 0.4926576496196591 Lambda1 0.117736384\n",
      "20 Train Loss 835.79785 Test MSE 853.8332087663051 Test RE 0.49189871914149025 Lambda1 0.02850445\n",
      "21 Train Loss 828.02234 Test MSE 839.9727099331427 Test RE 0.4878898226260648 Lambda1 -0.008910155\n",
      "22 Train Loss 810.28564 Test MSE 823.0875894489781 Test RE 0.48296115042356935 Lambda1 0.0062791076\n",
      "23 Train Loss 784.02423 Test MSE 791.9963702762436 Test RE 0.47375168347406116 Lambda1 -0.002767051\n",
      "24 Train Loss 743.21906 Test MSE 747.1671725164995 Test RE 0.46014855429317275 Lambda1 -0.00048052135\n",
      "25 Train Loss 632.5027 Test MSE 639.6986468248746 Test RE 0.42577179321360653 Lambda1 8.8673456e-05\n",
      "26 Train Loss 565.0797 Test MSE 576.6744142150137 Test RE 0.40425417346817166 Lambda1 3.1334897e-05\n",
      "27 Train Loss 516.58344 Test MSE 525.4099500866623 Test RE 0.3858676029475198 Lambda1 0.00041991723\n",
      "28 Train Loss 485.21768 Test MSE 510.71226833876705 Test RE 0.38043224132992287 Lambda1 -1.2407387e-05\n",
      "29 Train Loss 473.22336 Test MSE 507.60136331241296 Test RE 0.379271806772955 Lambda1 2.6738126e-05\n",
      "30 Train Loss 468.89545 Test MSE 506.5699265964616 Test RE 0.3788862741261888 Lambda1 -2.0751029e-06\n",
      "31 Train Loss 467.70688 Test MSE 506.13781338199016 Test RE 0.37872464126315003 Lambda1 4.3355828e-07\n",
      "32 Train Loss 466.89655 Test MSE 505.1998981024733 Test RE 0.37837357448828945 Lambda1 -6.208212e-08\n",
      "33 Train Loss 465.80722 Test MSE 505.2409449905819 Test RE 0.3783889453765028 Lambda1 -1.6893133e-06\n",
      "34 Train Loss 465.09134 Test MSE 504.8854505088023 Test RE 0.378255802117437 Lambda1 4.537703e-08\n",
      "35 Train Loss 464.75595 Test MSE 503.92508249275556 Test RE 0.37789588118446865 Lambda1 8.09002e-07\n",
      "36 Train Loss 464.29034 Test MSE 503.1507720822252 Test RE 0.3776054399918044 Lambda1 2.1738451e-06\n",
      "37 Train Loss 461.63806 Test MSE 498.5400201432535 Test RE 0.37587131562887727 Lambda1 1.4342941e-05\n",
      "38 Train Loss 450.234 Test MSE 488.52238087508726 Test RE 0.37207578195466384 Lambda1 -1.1055303e-05\n",
      "39 Train Loss 445.3077 Test MSE 481.3269307930943 Test RE 0.36932546342564837 Lambda1 1.02978365e-05\n",
      "40 Train Loss 439.89984 Test MSE 479.8919247200369 Test RE 0.3687745074571774 Lambda1 4.1473754e-06\n",
      "41 Train Loss 438.8016 Test MSE 479.1493578033963 Test RE 0.3684890830217735 Lambda1 2.4732726e-06\n",
      "42 Train Loss 434.14706 Test MSE 474.229871300807 Test RE 0.3665925406557196 Lambda1 3.1473763e-07\n",
      "43 Train Loss 424.18542 Test MSE 464.91006402623907 Test RE 0.3629724345781905 Lambda1 1.2715784e-05\n",
      "44 Train Loss 422.87286 Test MSE 463.86870647028667 Test RE 0.3625656935579828 Lambda1 1.9352217e-05\n",
      "45 Train Loss 420.10217 Test MSE 461.0855934151398 Test RE 0.3614763988970395 Lambda1 1.9172301e-05\n",
      "46 Train Loss 415.73828 Test MSE 458.63149001704966 Test RE 0.36051314607642565 Lambda1 6.0060634e-06\n",
      "47 Train Loss 409.59436 Test MSE 451.0982280711702 Test RE 0.35754007854988734 Lambda1 1.1943822e-05\n",
      "48 Train Loss 402.13492 Test MSE 441.9234264147178 Test RE 0.35388543022571 Lambda1 2.3430277e-05\n",
      "49 Train Loss 388.28696 Test MSE 425.8296959909763 Test RE 0.34738186533020243 Lambda1 8.76273e-05\n",
      "50 Train Loss 382.8788 Test MSE 416.07752098002334 Test RE 0.3433810286114126 Lambda1 9.413177e-05\n",
      "51 Train Loss 377.46036 Test MSE 412.6486086484203 Test RE 0.3419631927150874 Lambda1 5.8934373e-05\n",
      "52 Train Loss 375.64133 Test MSE 412.1873890877974 Test RE 0.3417720322386102 Lambda1 4.9758783e-05\n",
      "53 Train Loss 374.10812 Test MSE 410.1969147716257 Test RE 0.34094581612152086 Lambda1 0.00015701365\n",
      "54 Train Loss 373.25543 Test MSE 409.22769043268687 Test RE 0.340542779917325 Lambda1 0.00026317927\n",
      "55 Train Loss 372.4529 Test MSE 409.73664757185725 Test RE 0.34075448091710503 Lambda1 0.00022854906\n",
      "56 Train Loss 371.1284 Test MSE 408.4788554285667 Test RE 0.34023106210592985 Lambda1 0.0001926214\n",
      "57 Train Loss 370.23923 Test MSE 404.0633488940591 Test RE 0.3383871791760718 Lambda1 0.0002139576\n",
      "58 Train Loss 368.21393 Test MSE 402.41217120824865 Test RE 0.337695073172294 Lambda1 9.273805e-05\n",
      "59 Train Loss 363.82806 Test MSE 392.25871387886554 Test RE 0.33340758107271595 Lambda1 0.00031973273\n",
      "60 Train Loss 360.73108 Test MSE 388.1531493188879 Test RE 0.331658191112582 Lambda1 0.0005673433\n",
      "61 Train Loss 358.51343 Test MSE 384.6327499962042 Test RE 0.3301507594482545 Lambda1 0.0007940249\n",
      "62 Train Loss 352.2396 Test MSE 373.7351836908889 Test RE 0.32544017360208827 Lambda1 0.0012966449\n",
      "63 Train Loss 346.92288 Test MSE 368.9005616440044 Test RE 0.3233283819095162 Lambda1 0.0009899738\n",
      "64 Train Loss 339.69003 Test MSE 364.99363543204134 Test RE 0.3216116827541526 Lambda1 0.00019836896\n",
      "65 Train Loss 334.06094 Test MSE 354.6274106773833 Test RE 0.3170117219492466 Lambda1 7.741394e-05\n",
      "66 Train Loss 326.0244 Test MSE 347.86720104729477 Test RE 0.3139756094144113 Lambda1 0.00012337108\n",
      "67 Train Loss 319.36874 Test MSE 343.98793173388 Test RE 0.3122200391067717 Lambda1 0.0002506835\n",
      "68 Train Loss 316.04434 Test MSE 341.80562345556325 Test RE 0.31122807919136286 Lambda1 0.00019110808\n",
      "69 Train Loss 313.6618 Test MSE 338.6293419134114 Test RE 0.309778636544681 Lambda1 0.000119292614\n",
      "70 Train Loss 312.55225 Test MSE 336.718885405562 Test RE 0.3089035562983456 Lambda1 0.00019775413\n",
      "71 Train Loss 311.34555 Test MSE 334.4504385927499 Test RE 0.30786126924175616 Lambda1 0.00013060542\n",
      "72 Train Loss 310.6516 Test MSE 332.0794432213231 Test RE 0.3067680789815543 Lambda1 0.00014307401\n",
      "73 Train Loss 309.06924 Test MSE 330.9569634886926 Test RE 0.3062491784104525 Lambda1 6.779301e-05\n",
      "74 Train Loss 304.51688 Test MSE 323.3110423053961 Test RE 0.30269095213704167 Lambda1 -2.5878995e-05\n",
      "Training time: 164.56\n",
      "Training time: 164.56\n",
      "inv_HT_atanh_tune6\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 22710.635 Test MSE 3669.535550429631 Test RE 1.0197523630714984 Lambda1 -0.00030230588\n",
      "1 Train Loss 16286.967 Test MSE 3680.419369085604 Test RE 1.0212635328205444 Lambda1 0.00057078246\n",
      "2 Train Loss 11534.216 Test MSE 3686.740354865624 Test RE 1.0221401479931718 Lambda1 -0.00055727136\n",
      "3 Train Loss 9403.35 Test MSE 3688.695705548662 Test RE 1.0224111702658998 Lambda1 -5.1981144e-05\n",
      "4 Train Loss 7851.894 Test MSE 3681.8425892566606 Test RE 1.0214609753163084 Lambda1 0.0002876234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 6900.747 Test MSE 3679.973321428881 Test RE 1.0212016450350914 Lambda1 -0.00061274273\n",
      "6 Train Loss 5847.4326 Test MSE 3678.434623228554 Test RE 1.020988126446282 Lambda1 -0.00031968922\n",
      "7 Train Loss 5129.7446 Test MSE 3673.0278416813135 Test RE 1.020237496039438 Lambda1 0.000344694\n",
      "8 Train Loss 4553.282 Test MSE 3675.726851135329 Test RE 1.0206122719868018 Lambda1 -0.00025506716\n",
      "9 Train Loss 4318.27 Test MSE 3674.1016297241285 Test RE 1.0203866153513357 Lambda1 -0.00012658439\n",
      "10 Train Loss 4167.7554 Test MSE 3675.4447960657494 Test RE 1.0205731131483422 Lambda1 0.00028778284\n",
      "11 Train Loss 4034.76 Test MSE 3674.2484699812053 Test RE 1.020407005687664 Lambda1 -0.00019730692\n",
      "12 Train Loss 3937.8289 Test MSE 3673.5742102406657 Test RE 1.0203133741687354 Lambda1 -0.00026487382\n",
      "13 Train Loss 3885.1072 Test MSE 3671.5102372034835 Test RE 1.0200267057835748 Lambda1 0.000121702935\n",
      "14 Train Loss 3841.6206 Test MSE 3663.15805202528 Test RE 1.0188658343642618 Lambda1 9.905198e-05\n",
      "15 Train Loss 3804.8066 Test MSE 3657.388679696401 Test RE 1.018063175483663 Lambda1 -0.0004057288\n",
      "16 Train Loss 3771.3232 Test MSE 3647.577819921334 Test RE 1.016696793192134 Lambda1 -0.00022526336\n",
      "17 Train Loss 3735.7754 Test MSE 3631.028145401208 Test RE 1.0143877087990443 Lambda1 0.00031476797\n",
      "18 Train Loss 3707.4956 Test MSE 3613.6365526387794 Test RE 1.0119554784783693 Lambda1 0.000155369\n",
      "19 Train Loss 3689.7153 Test MSE 3601.5764524381175 Test RE 1.0102654241632127 Lambda1 -0.0005074112\n",
      "20 Train Loss 3671.8618 Test MSE 3584.9582264870437 Test RE 1.0079319695403575 Lambda1 -0.00015910031\n",
      "21 Train Loss 3641.9944 Test MSE 3548.772382377158 Test RE 1.0028321368908297 Lambda1 9.067103e-05\n",
      "22 Train Loss 3611.255 Test MSE 3516.2988995641463 Test RE 0.9982333221437409 Lambda1 0.00028847862\n",
      "23 Train Loss 3575.9766 Test MSE 3456.5363810958675 Test RE 0.9897140528080511 Lambda1 0.00019477156\n",
      "24 Train Loss 3542.5837 Test MSE 3430.661261627593 Test RE 0.9860026676245679 Lambda1 -0.00040703907\n",
      "25 Train Loss 3529.5254 Test MSE 3410.7631559837355 Test RE 0.9831390627194038 Lambda1 -0.00022053409\n",
      "26 Train Loss 3505.4695 Test MSE 3391.2198921517015 Test RE 0.9803183819132125 Lambda1 3.918618e-05\n",
      "27 Train Loss 3483.1453 Test MSE 3383.418379891125 Test RE 0.9791901198528979 Lambda1 0.00035622317\n",
      "28 Train Loss 3454.8833 Test MSE 3354.317428268704 Test RE 0.9749699941479114 Lambda1 0.00042505955\n",
      "29 Train Loss 3423.9885 Test MSE 3337.610216614058 Test RE 0.9725388939013759 Lambda1 -0.00021920333\n",
      "30 Train Loss 3408.4946 Test MSE 3326.813250191368 Test RE 0.9709645675182338 Lambda1 -0.0005631398\n",
      "31 Train Loss 3385.2573 Test MSE 3321.1907486712066 Test RE 0.9701437281883464 Lambda1 -0.00057200145\n",
      "32 Train Loss 3371.8325 Test MSE 3308.0068412457094 Test RE 0.9682162563099511 Lambda1 -0.0009952413\n",
      "33 Train Loss 3337.8508 Test MSE 3279.1476630017655 Test RE 0.9639836268510282 Lambda1 -0.00012441295\n",
      "34 Train Loss 3303.9185 Test MSE 3229.6410828438416 Test RE 0.9566791325854705 Lambda1 0.0006857036\n",
      "35 Train Loss 3269.512 Test MSE 3192.861161332129 Test RE 0.9512160895846776 Lambda1 0.00024823495\n",
      "36 Train Loss 3220.389 Test MSE 3147.859844275814 Test RE 0.9444889132832541 Lambda1 0.00012008751\n",
      "37 Train Loss 3176.6719 Test MSE 3114.231921406978 Test RE 0.9394304795247067 Lambda1 -0.00030888524\n",
      "38 Train Loss 3118.2722 Test MSE 3065.4221274354863 Test RE 0.9320394922830161 Lambda1 -0.00055265956\n",
      "39 Train Loss 2990.3086 Test MSE 2939.7258909135876 Test RE 0.9127305547562662 Lambda1 0.0006290091\n",
      "40 Train Loss 2943.603 Test MSE 2896.9864603520664 Test RE 0.9060713607152863 Lambda1 -8.251958e-05\n",
      "41 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "42 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "43 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "44 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "45 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "46 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "47 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "48 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "49 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "50 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "51 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "52 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "53 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "54 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "55 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "56 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "57 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "58 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "59 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "60 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "61 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "62 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "63 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "64 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "65 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "66 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "67 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "68 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "69 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "70 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "71 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "72 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "73 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "74 Train Loss 2585.606 Test MSE 2195.91363858938 Test RE 0.7888543026896279 Lambda1 -0.0024974828\n",
      "Training time: 161.68\n",
      "Training time: 161.68\n",
      "inv_HT_atanh_tune7\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.20\n",
      "Training time: 3.20\n",
      "inv_HT_atanh_tune7\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.95\n",
      "Training time: 2.95\n",
      "inv_HT_atanh_tune7\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 661363.0 Test MSE 3554.8833457040123 Test RE 1.003695200763339 Lambda1 -6.559197e-05\n",
      "1 Train Loss 623935.75 Test MSE 3554.8531897220523 Test RE 1.0036909435943104 Lambda1 3.8813698e-05\n",
      "2 Train Loss 600594.0 Test MSE 3554.8127822542356 Test RE 1.0036852391780398 Lambda1 -2.2508944e-05\n",
      "3 Train Loss 581267.75 Test MSE 3554.77329035938 Test RE 1.0036796639840069 Lambda1 -4.320065e-05\n",
      "4 Train Loss 561979.5 Test MSE 3554.733905391679 Test RE 1.003674103854371 Lambda1 -3.0380927e-05\n",
      "5 Train Loss 548127.3 Test MSE 3554.760519645004 Test RE 1.0036778610957806 Lambda1 -2.0950443e-05\n",
      "6 Train Loss 533298.5 Test MSE 3554.83173505517 Test RE 1.003687914793093 Lambda1 -2.0654947e-05\n",
      "7 Train Loss 522295.47 Test MSE 3554.544219069402 Test RE 1.0036473246643598 Lambda1 -2.7540356e-05\n",
      "8 Train Loss 509715.47 Test MSE 3554.252207984072 Test RE 1.0036060982586794 Lambda1 -3.479856e-05\n",
      "9 Train Loss 503497.6 Test MSE 3554.0358549635275 Test RE 1.0035755522390808 Lambda1 -3.2372332e-05\n",
      "10 Train Loss 495819.6 Test MSE 3553.755314887155 Test RE 1.0035359425207595 Lambda1 -5.628841e-05\n",
      "11 Train Loss 489480.22 Test MSE 3553.4047840669004 Test RE 1.0034864485459762 Lambda1 -6.4751555e-05\n",
      "12 Train Loss 481381.5 Test MSE 3553.0413451859936 Test RE 1.0034351294112378 Lambda1 -5.768952e-05\n",
      "13 Train Loss 475737.66 Test MSE 3552.5410021316366 Test RE 1.0033644745236099 Lambda1 -6.4556625e-05\n",
      "14 Train Loss 471970.84 Test MSE 3552.3904424001917 Test RE 1.0033432125752824 Lambda1 -4.9608734e-05\n",
      "15 Train Loss 466174.25 Test MSE 3552.2135682844973 Test RE 1.0033182339509723 Lambda1 -5.4269716e-05\n",
      "16 Train Loss 460421.8 Test MSE 3552.0405105651976 Test RE 1.0032937936862223 Lambda1 -4.772606e-05\n",
      "17 Train Loss 455581.28 Test MSE 3551.966422667191 Test RE 1.0032833303601931 Lambda1 -6.613765e-05\n",
      "18 Train Loss 452628.03 Test MSE 3551.835958320156 Test RE 1.003264904805027 Lambda1 -6.899807e-05\n",
      "19 Train Loss 449831.25 Test MSE 3551.985582208927 Test RE 1.0032860362448541 Lambda1 -6.0605737e-05\n",
      "20 Train Loss 444920.3 Test MSE 3551.7864844122455 Test RE 1.0032579174879133 Lambda1 -7.4847696e-05\n",
      "21 Train Loss 441514.7 Test MSE 3551.8366299474 Test RE 1.003264999660198 Lambda1 -7.134859e-05\n",
      "22 Train Loss 438002.72 Test MSE 3551.827709706841 Test RE 1.0032637398372255 Lambda1 -7.389989e-05\n",
      "23 Train Loss 434777.78 Test MSE 3551.8498674095945 Test RE 1.0032668692098865 Lambda1 -6.331261e-05\n",
      "24 Train Loss 428366.22 Test MSE 3551.83955883998 Test RE 1.0032654133130643 Lambda1 -6.118053e-05\n",
      "25 Train Loss 421967.97 Test MSE 3551.664552680084 Test RE 1.0032406965730134 Lambda1 -5.5127653e-05\n",
      "26 Train Loss 417691.66 Test MSE 3551.720921556791 Test RE 1.0032486578156803 Lambda1 -6.282651e-05\n",
      "27 Train Loss 414137.1 Test MSE 3551.6274870757074 Test RE 1.0032354615882306 Lambda1 -7.345605e-05\n",
      "28 Train Loss 409408.72 Test MSE 3551.3818599403926 Test RE 1.0032007695894085 Lambda1 -6.731762e-05\n",
      "29 Train Loss 403676.62 Test MSE 3550.87986493589 Test RE 1.0031298648553852 Lambda1 -7.198773e-05\n",
      "30 Train Loss 401074.56 Test MSE 3550.679329684917 Test RE 1.0031015386732702 Lambda1 -6.4348606e-05\n",
      "31 Train Loss 396703.66 Test MSE 3550.7666181153013 Test RE 1.0031138685139251 Lambda1 -6.229505e-05\n",
      "32 Train Loss 393812.66 Test MSE 3550.6453687174917 Test RE 1.003096741509548 Lambda1 -6.942499e-05\n",
      "33 Train Loss 386147.3 Test MSE 3550.0621105233945 Test RE 1.0030143496677781 Lambda1 -0.00011998223\n",
      "34 Train Loss 382738.84 Test MSE 3550.114588741128 Test RE 1.0030217630889056 Lambda1 -0.00011774619\n",
      "35 Train Loss 378954.44 Test MSE 3549.79314156225 Test RE 1.0029763524680988 Lambda1 -0.000121893034\n",
      "36 Train Loss 376498.66 Test MSE 3549.3417110968953 Test RE 1.002912575726847 Lambda1 -9.812378e-05\n",
      "37 Train Loss 372617.06 Test MSE 3548.855476091373 Test RE 1.0028438773675685 Lambda1 -8.136655e-05\n",
      "38 Train Loss 369038.78 Test MSE 3548.5501772276752 Test RE 1.0028007404064745 Lambda1 -7.926139e-05\n",
      "39 Train Loss 365204.4 Test MSE 3548.3500522157706 Test RE 1.0027724628945829 Lambda1 -7.831681e-05\n",
      "40 Train Loss 361383.3 Test MSE 3548.38835262183 Test RE 1.0027778747745142 Lambda1 -9.450551e-05\n",
      "41 Train Loss 357273.75 Test MSE 3548.232084393849 Test RE 1.002755793757546 Lambda1 -9.7564705e-05\n",
      "42 Train Loss 355558.25 Test MSE 3548.129513590836 Test RE 1.0027413000311114 Lambda1 -9.777344e-05\n",
      "43 Train Loss 352919.72 Test MSE 3548.1006506896465 Test RE 1.0027372215325943 Lambda1 -9.6511336e-05\n",
      "44 Train Loss 350757.94 Test MSE 3548.1674343132295 Test RE 1.0027466584280822 Lambda1 -7.847033e-05\n",
      "45 Train Loss 348773.1 Test MSE 3548.1236506168752 Test RE 1.0027404715595833 Lambda1 -7.986846e-05\n",
      "46 Train Loss 346733.88 Test MSE 3548.010849620882 Test RE 1.0027245320047569 Lambda1 -7.905083e-05\n",
      "47 Train Loss 344455.78 Test MSE 3547.966514139358 Test RE 1.0027182670276007 Lambda1 -7.727741e-05\n",
      "48 Train Loss 340899.38 Test MSE 3547.6182081836155 Test RE 1.0026690470990938 Lambda1 -7.29281e-05\n",
      "49 Train Loss 337975.38 Test MSE 3547.2746043779507 Test RE 1.0026204892748414 Lambda1 -6.6943634e-05\n",
      "50 Train Loss 334183.7 Test MSE 3547.0363558635736 Test RE 1.0025868187982456 Lambda1 -6.743431e-05\n",
      "51 Train Loss 330071.56 Test MSE 3546.920326217074 Test RE 1.0025704204964165 Lambda1 -7.229586e-05\n",
      "52 Train Loss 328110.34 Test MSE 3547.0077402463116 Test RE 1.0025827746194351 Lambda1 -5.9109854e-05\n",
      "53 Train Loss 325592.75 Test MSE 3547.174551634285 Test RE 1.0026063494563375 Lambda1 -7.7149685e-05\n",
      "54 Train Loss 322464.78 Test MSE 3547.3435515541205 Test RE 1.0026302330250885 Lambda1 -0.00010497089\n",
      "55 Train Loss 319889.78 Test MSE 3547.210770799189 Test RE 1.0026114681037965 Lambda1 -0.00013298824\n",
      "56 Train Loss 318839.1 Test MSE 3547.286917942807 Test RE 1.0026222294589382 Lambda1 -0.00013931589\n",
      "57 Train Loss 317761.4 Test MSE 3547.1989372701764 Test RE 1.0026097957416849 Lambda1 -0.00013132118\n",
      "58 Train Loss 316534.0 Test MSE 3547.0504158542626 Test RE 1.0025888058581396 Lambda1 -0.00010108452\n",
      "59 Train Loss 314724.4 Test MSE 3546.9120000892617 Test RE 1.0025692437665223 Lambda1 -7.856716e-05\n",
      "60 Train Loss 311257.2 Test MSE 3546.955841750895 Test RE 1.0025754398838387 Lambda1 -6.090622e-05\n",
      "61 Train Loss 309549.03 Test MSE 3547.0992143110893 Test RE 1.0025957023795515 Lambda1 -5.098172e-05\n",
      "62 Train Loss 306789.16 Test MSE 3547.293384528172 Test RE 1.0026231433319106 Lambda1 -6.9899645e-05\n",
      "63 Train Loss 304782.4 Test MSE 3547.393980111046 Test RE 1.0026373596277638 Lambda1 -9.280647e-05\n",
      "64 Train Loss 303601.16 Test MSE 3547.4690089165115 Test RE 1.0026479626628462 Lambda1 -9.682616e-05\n",
      "65 Train Loss 302441.88 Test MSE 3547.6651139680007 Test RE 1.0026756756058712 Lambda1 -0.000101977515\n",
      "66 Train Loss 300767.44 Test MSE 3547.900524223879 Test RE 1.0027089420239537 Lambda1 -7.9270976e-05\n",
      "67 Train Loss 298361.44 Test MSE 3547.997619968502 Test RE 1.0027226625474863 Lambda1 -7.2874594e-05\n",
      "68 Train Loss 297093.44 Test MSE 3548.049313510678 Test RE 1.0027299672448111 Lambda1 -8.063575e-05\n",
      "69 Train Loss 295776.0 Test MSE 3548.277657612513 Test RE 1.0027622333958264 Lambda1 -9.633681e-05\n",
      "70 Train Loss 294791.47 Test MSE 3548.516910985846 Test RE 1.0027960399670761 Lambda1 -0.00010204665\n",
      "71 Train Loss 294095.1 Test MSE 3548.6109235919735 Test RE 1.00280932366377 Lambda1 -0.000107329775\n",
      "72 Train Loss 293310.5 Test MSE 3548.70956409 Test RE 1.0028232610784162 Lambda1 -0.00011110068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 Train Loss 291961.88 Test MSE 3548.8231014038265 Test RE 1.002839303099688 Lambda1 -9.357875e-05\n",
      "74 Train Loss 290333.53 Test MSE 3548.7902717307306 Test RE 1.0028346645249144 Lambda1 -8.036774e-05\n",
      "Training time: 165.98\n",
      "Training time: 165.98\n",
      "inv_HT_atanh_tune7\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.00\n",
      "Training time: 3.00\n",
      "inv_HT_atanh_tune7\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 404525.8 Test MSE 3517.265997708255 Test RE 0.99837058624606 Lambda1 1.3165718e-05\n",
      "1 Train Loss 394873.25 Test MSE 3517.3494243926107 Test RE 0.9983824264443747 Lambda1 1.65994e-05\n",
      "2 Train Loss 386435.78 Test MSE 3517.5012561775593 Test RE 0.9984039745665189 Lambda1 3.371665e-05\n",
      "3 Train Loss 373682.6 Test MSE 3517.7385745448155 Test RE 0.998437654100303 Lambda1 4.1059608e-05\n",
      "4 Train Loss 367940.28 Test MSE 3517.9219062249126 Test RE 0.9984636712223646 Lambda1 -1.5457346e-05\n",
      "5 Train Loss 360983.06 Test MSE 3517.904760273668 Test RE 0.9984612380202151 Lambda1 -3.410159e-05\n",
      "6 Train Loss 353098.16 Test MSE 3517.572577163075 Test RE 0.9984140963531617 Lambda1 5.548773e-05\n",
      "7 Train Loss 343493.0 Test MSE 3517.477226380099 Test RE 0.998400564264074 Lambda1 3.342659e-05\n",
      "8 Train Loss 338537.3 Test MSE 3517.459496860286 Test RE 0.9983980480878842 Lambda1 -2.6966387e-05\n",
      "9 Train Loss 333005.94 Test MSE 3517.316330823493 Test RE 0.9983777297094807 Lambda1 4.6883706e-06\n",
      "10 Train Loss 329158.84 Test MSE 3517.273875601933 Test RE 0.9983717043094833 Lambda1 4.983552e-05\n",
      "11 Train Loss 324350.6 Test MSE 3517.2728351446854 Test RE 0.9983715566435366 Lambda1 3.4326134e-05\n",
      "12 Train Loss 317381.53 Test MSE 3516.8942094850986 Test RE 0.9983178190916966 Lambda1 -3.391937e-05\n",
      "13 Train Loss 311243.22 Test MSE 3516.1793795495896 Test RE 0.9982163568801992 Lambda1 -2.9815541e-05\n",
      "14 Train Loss 307677.62 Test MSE 3516.075131736017 Test RE 0.99820155919317 Lambda1 -1.4987535e-05\n",
      "15 Train Loss 302853.0 Test MSE 3515.7503822341578 Test RE 0.9981554604989731 Lambda1 -6.242622e-06\n",
      "16 Train Loss 299248.38 Test MSE 3515.581660522464 Test RE 0.9981315093505573 Lambda1 -1.1837183e-05\n",
      "17 Train Loss 296302.94 Test MSE 3515.406718200624 Test RE 0.9981066745390001 Lambda1 2.637395e-06\n",
      "18 Train Loss 293084.66 Test MSE 3515.0778565081114 Test RE 0.9980599876612712 Lambda1 2.7763155e-05\n",
      "19 Train Loss 289809.84 Test MSE 3514.581573199621 Test RE 0.9979895286248849 Lambda1 1.833998e-05\n",
      "20 Train Loss 285243.3 Test MSE 3514.3943623006635 Test RE 0.9979629483619139 Lambda1 -1.1001762e-05\n",
      "21 Train Loss 282268.56 Test MSE 3514.4001470519565 Test RE 0.9979637696933423 Lambda1 1.9886818e-05\n",
      "22 Train Loss 279364.44 Test MSE 3514.4230600684173 Test RE 0.9979670229260498 Lambda1 4.3576605e-05\n",
      "23 Train Loss 276377.8 Test MSE 3514.286297831922 Test RE 0.9979476050119992 Lambda1 3.7827976e-05\n",
      "24 Train Loss 272413.75 Test MSE 3514.3972227003896 Test RE 0.9979633544877028 Lambda1 -4.850697e-06\n",
      "25 Train Loss 268355.1 Test MSE 3514.724175034273 Test RE 0.998009774803729 Lambda1 1.18884645e-05\n",
      "26 Train Loss 264585.28 Test MSE 3514.8843140423223 Test RE 0.9980325103680293 Lambda1 4.149748e-05\n",
      "27 Train Loss 261188.11 Test MSE 3514.7865478065023 Test RE 0.9980186301728995 Lambda1 -8.153062e-06\n",
      "28 Train Loss 258456.4 Test MSE 3514.3935366968035 Test RE 0.9979628311408476 Lambda1 -1.87008e-05\n",
      "29 Train Loss 255743.94 Test MSE 3514.455523673081 Test RE 0.9979716321508829 Lambda1 -3.8005976e-06\n",
      "30 Train Loss 253086.0 Test MSE 3514.6439556810124 Test RE 0.9979983855521811 Lambda1 2.3242561e-05\n",
      "31 Train Loss 250935.05 Test MSE 3514.663844511963 Test RE 0.9980012093080648 Lambda1 1.35302735e-05\n",
      "32 Train Loss 248927.16 Test MSE 3514.6624896515564 Test RE 0.998001016949348 Lambda1 -1.2855005e-05\n",
      "33 Train Loss 247496.88 Test MSE 3514.7891889641874 Test RE 0.9980190051493086 Lambda1 1.2765709e-05\n",
      "34 Train Loss 245793.31 Test MSE 3514.931607080977 Test RE 0.9980392246475198 Lambda1 1.6046402e-05\n",
      "35 Train Loss 243369.03 Test MSE 3514.5660520380884 Test RE 0.9979873249523478 Lambda1 2.2273307e-05\n",
      "36 Train Loss 241183.86 Test MSE 3514.4730728213026 Test RE 0.9979741237929183 Lambda1 -1.6402367e-06\n",
      "37 Train Loss 239338.6 Test MSE 3514.285397324199 Test RE 0.9979474771539496 Lambda1 -1.8430494e-05\n",
      "38 Train Loss 237644.16 Test MSE 3514.131227285627 Test RE 0.9979255871719107 Lambda1 -1.4599024e-05\n",
      "39 Train Loss 236324.92 Test MSE 3514.142732333701 Test RE 0.9979272207438686 Lambda1 -2.4528936e-06\n",
      "40 Train Loss 234452.67 Test MSE 3514.1206251641615 Test RE 0.9979240818017947 Lambda1 9.558382e-07\n",
      "41 Train Loss 232855.06 Test MSE 3513.9858060402557 Test RE 0.9979049389556593 Lambda1 1.7185901e-05\n",
      "42 Train Loss 230190.03 Test MSE 3513.8967075454584 Test RE 0.9978922877389892 Lambda1 8.288689e-06\n",
      "43 Train Loss 227866.98 Test MSE 3513.72295858136 Test RE 0.9978676164254812 Lambda1 1.6228954e-05\n",
      "44 Train Loss 225751.45 Test MSE 3513.598025024706 Test RE 0.9978498762308157 Lambda1 1.1996476e-05\n",
      "45 Train Loss 224676.9 Test MSE 3513.4959409471035 Test RE 0.9978353803602893 Lambda1 -1.4641926e-05\n",
      "46 Train Loss 222984.4 Test MSE 3513.3195458513514 Test RE 0.9978103318787108 Lambda1 -5.603315e-05\n",
      "47 Train Loss 221512.05 Test MSE 3513.1185699282764 Test RE 0.9977817921006132 Lambda1 -3.639102e-05\n",
      "48 Train Loss 219967.69 Test MSE 3512.978094174643 Test RE 0.9977618432222025 Lambda1 6.8257373e-06\n",
      "49 Train Loss 218546.42 Test MSE 3512.7364425539554 Test RE 0.9977275254857193 Lambda1 9.3097815e-06\n",
      "50 Train Loss 216531.45 Test MSE 3512.56876725723 Test RE 0.9977037126751007 Lambda1 -3.2845273e-06\n",
      "51 Train Loss 214842.97 Test MSE 3512.4854053423337 Test RE 0.9976918736206983 Lambda1 4.659027e-06\n",
      "52 Train Loss 213220.11 Test MSE 3512.599736703855 Test RE 0.9977081109183549 Lambda1 2.5792443e-07\n",
      "53 Train Loss 211481.14 Test MSE 3512.3614706189705 Test RE 0.9976742721586541 Lambda1 -9.053956e-06\n",
      "54 Train Loss 210129.97 Test MSE 3512.354661038918 Test RE 0.9976733050392148 Lambda1 8.651071e-06\n",
      "55 Train Loss 208807.25 Test MSE 3512.4380618202053 Test RE 0.9976851498338337 Lambda1 1.2696581e-05\n",
      "56 Train Loss 207111.4 Test MSE 3512.449159982583 Test RE 0.9976867260129514 Lambda1 1.8528293e-05\n",
      "57 Train Loss 205415.97 Test MSE 3512.284484571679 Test RE 0.9976633382869243 Lambda1 -1.9793151e-06\n",
      "58 Train Loss 204106.78 Test MSE 3512.2094470129005 Test RE 0.9976526810321427 Lambda1 5.028556e-06\n",
      "59 Train Loss 202986.97 Test MSE 3512.1641371948176 Test RE 0.9976462458226312 Lambda1 1.3270611e-05\n",
      "60 Train Loss 201650.95 Test MSE 3511.926298668556 Test RE 0.9976124656915292 Lambda1 2.8548066e-05\n",
      "61 Train Loss 200777.42 Test MSE 3511.9632967572593 Test RE 0.9976177205935614 Lambda1 2.7038355e-05\n",
      "62 Train Loss 200183.28 Test MSE 3511.9225230176517 Test RE 0.9976119294277953 Lambda1 6.405334e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 Train Loss 199487.38 Test MSE 3511.878062396644 Test RE 0.9976056145694807 Lambda1 -1.4210359e-06\n",
      "64 Train Loss 198924.47 Test MSE 3511.926952152827 Test RE 0.9976125585072617 Lambda1 -7.799742e-06\n",
      "65 Train Loss 198000.78 Test MSE 3511.8320410006886 Test RE 0.9975990779881142 Lambda1 1.2234201e-06\n",
      "66 Train Loss 196613.03 Test MSE 3511.7566043751253 Test RE 0.997588363365181 Lambda1 1.4524485e-05\n",
      "67 Train Loss 195627.72 Test MSE 3511.6660655510345 Test RE 0.9975755035531724 Lambda1 1.5948568e-05\n",
      "68 Train Loss 194432.77 Test MSE 3511.525088006895 Test RE 0.997555479275029 Lambda1 -3.5582043e-06\n",
      "69 Train Loss 194013.9 Test MSE 3511.5279580607785 Test RE 0.9975558869379849 Lambda1 -2.1178903e-05\n",
      "70 Train Loss 193443.77 Test MSE 3511.574079490603 Test RE 0.997562438011095 Lambda1 -2.1715585e-05\n",
      "71 Train Loss 193013.73 Test MSE 3511.668084884186 Test RE 0.9975757903738665 Lambda1 -2.6461381e-05\n",
      "72 Train Loss 192563.5 Test MSE 3511.679362486183 Test RE 0.9975773922128529 Lambda1 -2.8046248e-05\n",
      "73 Train Loss 191842.27 Test MSE 3511.546209424215 Test RE 0.9975584793609412 Lambda1 -1.5858242e-05\n",
      "74 Train Loss 191335.16 Test MSE 3511.3836364177114 Test RE 0.9975353872601148 Lambda1 -1.1328577e-05\n",
      "Training time: 162.89\n",
      "Training time: 162.89\n",
      "inv_HT_atanh_tune7\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.01\n",
      "Training time: 3.01\n",
      "inv_HT_atanh_tune7\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.05\n",
      "Training time: 3.05\n",
      "inv_HT_atanh_tune7\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.10\n",
      "Training time: 3.10\n",
      "inv_HT_atanh_tune7\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.06\n",
      "Training time: 3.06\n",
      "inv_HT_atanh_tune7\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.03\n",
      "Training time: 3.03\n",
      "inv_HT_atanh_tune8\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.95\n",
      "Training time: 2.95\n",
      "inv_HT_atanh_tune8\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.07\n",
      "Training time: 3.07\n",
      "inv_HT_atanh_tune8\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.90\n",
      "Training time: 2.90\n",
      "inv_HT_atanh_tune8\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.14\n",
      "Training time: 3.14\n",
      "inv_HT_atanh_tune8\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.03\n",
      "Training time: 3.03\n",
      "inv_HT_atanh_tune8\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.97\n",
      "Training time: 2.97\n",
      "inv_HT_atanh_tune8\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.95\n",
      "Training time: 2.95\n",
      "inv_HT_atanh_tune8\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.14\n",
      "Training time: 3.14\n",
      "inv_HT_atanh_tune8\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.92\n",
      "Training time: 2.92\n",
      "inv_HT_atanh_tune8\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.27\n",
      "Training time: 3.27\n",
      "inv_HT_atanh_tune9\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1225243400.0 Test MSE 3552.502451018183 Test RE 1.0033590304031048 Lambda1 -1.6318443e-06\n",
      "1 Train Loss 1089988100.0 Test MSE 3552.5066004315577 Test RE 1.003359616377389 Lambda1 -6.8839745e-07\n",
      "2 Train Loss 1031913500.0 Test MSE 3552.532438627735 Test RE 1.0033632652031874 Lambda1 1.5323992e-07\n",
      "3 Train Loss 990358340.0 Test MSE 3552.541883201035 Test RE 1.0033645989463367 Lambda1 5.0846353e-07\n",
      "4 Train Loss 957101300.0 Test MSE 3552.546300487268 Test RE 1.0033652227457739 Lambda1 -4.4991804e-08\n",
      "5 Train Loss 930841860.0 Test MSE 3552.546293988687 Test RE 1.0033652218280587 Lambda1 -1.9028728e-07\n",
      "6 Train Loss 900578940.0 Test MSE 3552.5328469321603 Test RE 1.0033633228631327 Lambda1 -2.1121761e-07\n",
      "7 Train Loss 876851840.0 Test MSE 3552.5189390316964 Test RE 1.003361358815018 Lambda1 3.2639704e-07\n",
      "8 Train Loss 868344640.0 Test MSE 3552.5079712987103 Test RE 1.0033598099692331 Lambda1 2.0881454e-07\n",
      "9 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "10 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "12 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "13 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "14 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "15 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "16 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "17 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "18 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "19 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "20 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "21 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "22 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "23 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "24 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "25 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "26 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "27 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "28 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "29 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "30 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "31 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "32 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "33 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "34 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "35 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "36 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "37 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "38 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "39 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "40 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "41 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "42 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "43 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "44 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "45 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "46 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "47 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "48 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "49 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "50 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "51 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "52 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "53 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "54 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "55 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "56 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "57 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "58 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "59 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "60 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "61 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "62 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "63 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "64 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "65 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "66 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "67 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "68 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "69 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "70 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "71 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "72 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "73 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "74 Train Loss 864079200.0 Test MSE 3552.501590544136 Test RE 1.0033589088881345 Lambda1 4.276601e-08\n",
      "Training time: 153.85\n",
      "Training time: 153.85\n",
      "inv_HT_atanh_tune9\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.46\n",
      "Training time: 3.46\n",
      "inv_HT_atanh_tune9\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.47\n",
      "Training time: 3.47\n",
      "inv_HT_atanh_tune9\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.43\n",
      "Training time: 3.43\n",
      "inv_HT_atanh_tune9\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.38\n",
      "Training time: 3.38\n",
      "inv_HT_atanh_tune9\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.13\n",
      "Training time: 3.13\n",
      "inv_HT_atanh_tune9\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.27\n",
      "Training time: 3.27\n",
      "inv_HT_atanh_tune9\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.25\n",
      "Training time: 3.25\n",
      "inv_HT_atanh_tune9\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.04\n",
      "Training time: 3.04\n",
      "inv_HT_atanh_tune9\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.13\n",
      "Training time: 3.13\n",
      "inv_HT_atanh_tune10\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 858.8065 Test MSE 874.6376885332664 Test RE 0.4978554484172573 Lambda1 -0.06511372\n",
      "1 Train Loss 838.12756 Test MSE 858.1029260716267 Test RE 0.4931270909839313 Lambda1 -0.0693723\n",
      "2 Train Loss 838.0632 Test MSE 858.2699635670228 Test RE 0.4931750844725481 Lambda1 -0.06960946\n",
      "3 Train Loss 838.0627 Test MSE 858.2914678326418 Test RE 0.4931812627742677 Lambda1 -0.0696364\n",
      "4 Train Loss 837.9965 Test MSE 858.0256493137963 Test RE 0.4931048861180515 Lambda1 -0.071071446\n",
      "5 Train Loss 837.90234 Test MSE 857.9417114436394 Test RE 0.4930807661012116 Lambda1 -0.086346634\n",
      "6 Train Loss 837.80865 Test MSE 857.7963599398648 Test RE 0.4930389957421652 Lambda1 -0.4968302\n",
      "7 Train Loss 837.1747 Test MSE 853.6391571031071 Test RE 0.4918428187690974 Lambda1 -0.60628647\n",
      "8 Train Loss 834.52386 Test MSE 853.4189689364154 Test RE 0.4917793815707363 Lambda1 -0.73716235\n",
      "9 Train Loss 831.13135 Test MSE 845.4647364669321 Test RE 0.489482218547866 Lambda1 -0.9682647\n",
      "10 Train Loss 824.33704 Test MSE 838.83101468656 Test RE 0.487558138697454 Lambda1 -0.7299018\n",
      "11 Train Loss 801.61755 Test MSE 813.4220757524783 Test RE 0.480117071003348 Lambda1 -0.6895546\n",
      "12 Train Loss 791.3811 Test MSE 801.218404864548 Test RE 0.4765018916940607 Lambda1 -0.7437825\n",
      "13 Train Loss 785.0911 Test MSE 792.2202294260803 Test RE 0.47381863211175274 Lambda1 -0.9001488\n",
      "14 Train Loss 770.7012 Test MSE 774.1695621100157 Test RE 0.46838957110088414 Lambda1 -0.9497834\n",
      "15 Train Loss 764.52954 Test MSE 768.4510282198056 Test RE 0.4666564454256387 Lambda1 -0.9791717\n",
      "16 Train Loss 758.5227 Test MSE 760.8437414035187 Test RE 0.4643408658769062 Lambda1 -1.0730255\n",
      "17 Train Loss 746.5246 Test MSE 742.4196718280607 Test RE 0.45868433256719454 Lambda1 -1.2308153\n",
      "18 Train Loss 736.8429 Test MSE 730.9053324335705 Test RE 0.45511351851373105 Lambda1 -1.2858863\n",
      "19 Train Loss 719.14575 Test MSE 710.6232797524035 Test RE 0.4487545716907625 Lambda1 -1.4539708\n",
      "20 Train Loss 706.47314 Test MSE 695.961157519637 Test RE 0.4441009186207565 Lambda1 -1.5852435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Train Loss 683.8195 Test MSE 661.7788393332039 Test RE 0.4330575442264224 Lambda1 -1.6899052\n",
      "22 Train Loss 672.2667 Test MSE 660.9934651663646 Test RE 0.4328004997609785 Lambda1 -1.7405148\n",
      "23 Train Loss 662.2786 Test MSE 656.3074909796866 Test RE 0.43126364704794323 Lambda1 -1.7874111\n",
      "24 Train Loss 650.5788 Test MSE 641.703865398942 Test RE 0.42643858960213465 Lambda1 -1.982803\n",
      "25 Train Loss 646.8058 Test MSE 639.1529434151968 Test RE 0.4255901493310753 Lambda1 -2.0497532\n",
      "26 Train Loss 640.5256 Test MSE 635.7973973056596 Test RE 0.424471507320725 Lambda1 -2.046079\n",
      "27 Train Loss 633.4786 Test MSE 626.3577246989973 Test RE 0.42130866330190064 Lambda1 -2.0792072\n",
      "28 Train Loss 631.8203 Test MSE 622.8147385328872 Test RE 0.42011540939478514 Lambda1 -2.1059241\n",
      "29 Train Loss 628.7434 Test MSE 620.7653096318455 Test RE 0.41942362572837993 Lambda1 -2.0664032\n",
      "30 Train Loss 624.61304 Test MSE 617.8100021088852 Test RE 0.418424049425559 Lambda1 -2.038292\n",
      "31 Train Loss 620.6532 Test MSE 612.4539598065221 Test RE 0.4166063584586931 Lambda1 -2.0766728\n",
      "32 Train Loss 616.79675 Test MSE 607.6643181503084 Test RE 0.4149741445847048 Lambda1 -2.1064014\n",
      "33 Train Loss 614.4836 Test MSE 602.9481392356636 Test RE 0.41336066787914355 Lambda1 -2.1844642\n",
      "34 Train Loss 610.152 Test MSE 599.8683546817281 Test RE 0.41230361873768223 Lambda1 -2.1881444\n",
      "35 Train Loss 606.841 Test MSE 596.1632892468812 Test RE 0.4110283572596054 Lambda1 -2.1851413\n",
      "36 Train Loss 602.9162 Test MSE 589.976185968188 Test RE 0.40888992678138547 Lambda1 -2.2470267\n",
      "37 Train Loss 601.5061 Test MSE 587.8688626817768 Test RE 0.4081590209404565 Lambda1 -2.2625077\n",
      "38 Train Loss 600.3699 Test MSE 587.3447298124884 Test RE 0.40797702689593324 Lambda1 -2.2632718\n",
      "39 Train Loss 598.14996 Test MSE 586.6432003663808 Test RE 0.40773330851682543 Lambda1 -2.2817612\n",
      "40 Train Loss 597.1489 Test MSE 586.8194995708433 Test RE 0.4077945703349803 Lambda1 -2.2670662\n",
      "41 Train Loss 595.4615 Test MSE 585.4578890285851 Test RE 0.4073211880984167 Lambda1 -2.2451572\n",
      "42 Train Loss 594.2735 Test MSE 584.7374937784932 Test RE 0.40707051031611713 Lambda1 -2.2690926\n",
      "43 Train Loss 592.39465 Test MSE 581.7694933456418 Test RE 0.40603609531052465 Lambda1 -2.307199\n",
      "44 Train Loss 590.33215 Test MSE 578.7831526017761 Test RE 0.4049926217034469 Lambda1 -2.319549\n",
      "45 Train Loss 585.6983 Test MSE 574.5393195297918 Test RE 0.4035051187946702 Lambda1 -2.3707948\n",
      "46 Train Loss 582.19995 Test MSE 573.9519521066029 Test RE 0.4032988088353865 Lambda1 -2.3882906\n",
      "47 Train Loss 580.6218 Test MSE 572.1127853306101 Test RE 0.4026521267197722 Lambda1 -2.3956642\n",
      "48 Train Loss 578.4805 Test MSE 570.7529511632935 Test RE 0.40217331740295925 Lambda1 -2.416313\n",
      "49 Train Loss 577.7101 Test MSE 569.1335669233963 Test RE 0.40160237357544565 Lambda1 -2.4171555\n",
      "50 Train Loss 576.5818 Test MSE 566.8960278153116 Test RE 0.4008121496120309 Lambda1 -2.4224246\n",
      "51 Train Loss 573.5985 Test MSE 564.6608679336215 Test RE 0.4000212073517801 Lambda1 -2.3924553\n",
      "52 Train Loss 571.73804 Test MSE 562.583678947268 Test RE 0.399284760691835 Lambda1 -2.3434393\n",
      "53 Train Loss 570.53046 Test MSE 561.316584206922 Test RE 0.3988348571174705 Lambda1 -2.3220987\n",
      "54 Train Loss 569.46545 Test MSE 559.5269439728687 Test RE 0.39819854887082173 Lambda1 -2.320046\n",
      "55 Train Loss 568.6369 Test MSE 557.532051027627 Test RE 0.3974880622806359 Lambda1 -2.3149214\n",
      "56 Train Loss 565.8883 Test MSE 554.1105764262016 Test RE 0.396266528697089 Lambda1 -2.2919016\n",
      "57 Train Loss 561.3509 Test MSE 548.7449595720531 Test RE 0.39434327842953115 Lambda1 -2.2765117\n",
      "58 Train Loss 559.66046 Test MSE 544.4747810604321 Test RE 0.39280594773236516 Lambda1 -2.2858713\n",
      "59 Train Loss 554.7556 Test MSE 533.7301606696084 Test RE 0.3889108354491591 Lambda1 -2.3266783\n",
      "60 Train Loss 547.51154 Test MSE 523.1936525567759 Test RE 0.38505290465705827 Lambda1 -2.3530436\n",
      "61 Train Loss 536.6031 Test MSE 517.4104081874935 Test RE 0.38291885443830587 Lambda1 -2.3688636\n",
      "62 Train Loss 529.6824 Test MSE 512.8978952401812 Test RE 0.3812454147097568 Lambda1 -2.4003096\n",
      "63 Train Loss 521.9789 Test MSE 498.5964954878848 Test RE 0.37589260465292157 Lambda1 -2.428214\n",
      "64 Train Loss 514.8455 Test MSE 493.037983351445 Test RE 0.37379144705769024 Lambda1 -2.4264848\n",
      "65 Train Loss 509.98163 Test MSE 491.83886663419986 Test RE 0.3733366216198189 Lambda1 -2.4040465\n",
      "66 Train Loss 499.90472 Test MSE 480.24750260180247 Test RE 0.3689111046475909 Lambda1 -2.4116812\n",
      "67 Train Loss 487.09967 Test MSE 467.91190559365043 Test RE 0.36414237331060717 Lambda1 -2.355874\n",
      "68 Train Loss 473.48975 Test MSE 455.10561199662425 Test RE 0.3591246916323795 Lambda1 -2.291342\n",
      "69 Train Loss 469.2559 Test MSE 447.2789703239191 Test RE 0.35602329097669605 Lambda1 -2.2647827\n",
      "70 Train Loss 464.3822 Test MSE 443.49035129645017 Test RE 0.3545122596604258 Lambda1 -2.2053823\n",
      "71 Train Loss 458.0162 Test MSE 439.06005921491527 Test RE 0.3527370970595562 Lambda1 -2.2072313\n",
      "72 Train Loss 454.3138 Test MSE 438.4380168888607 Test RE 0.3524871367520369 Lambda1 -2.216169\n",
      "73 Train Loss 447.4513 Test MSE 439.7922173661899 Test RE 0.3530310793514021 Lambda1 -2.1887057\n",
      "74 Train Loss 440.45084 Test MSE 433.7560189782858 Test RE 0.3506000129324578 Lambda1 -2.1637862\n",
      "Training time: 147.30\n",
      "Training time: 147.30\n",
      "inv_HT_atanh_tune10\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 857.4562 Test MSE 873.432177005029 Test RE 0.4975122334858377 Lambda1 -0.0551846\n",
      "1 Train Loss 838.1221 Test MSE 858.1061118508533 Test RE 0.49312800637096516 Lambda1 -0.05879907\n",
      "2 Train Loss 838.0623 Test MSE 858.2695177640296 Test RE 0.4931749563899128 Lambda1 -0.05899864\n",
      "3 Train Loss 838.06165 Test MSE 858.2943013798049 Test RE 0.49318207686322657 Lambda1 -0.05902274\n",
      "4 Train Loss 837.93176 Test MSE 857.8983070432021 Test RE 0.49306829314096956 Lambda1 -0.061774\n",
      "5 Train Loss 837.7514 Test MSE 857.6230544428572 Test RE 0.4929891874843826 Lambda1 -0.28705612\n",
      "6 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 11.67\n",
      "Training time: 11.67\n",
      "inv_HT_atanh_tune10\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 871.2002 Test MSE 885.8720017559741 Test RE 0.5010426072100214 Lambda1 -0.1578761\n",
      "1 Train Loss 838.15393 Test MSE 858.0772107348437 Test RE 0.4931197019960593 Lambda1 -0.17111777\n",
      "2 Train Loss 838.0581 Test MSE 858.266352753381 Test RE 0.493174047057008 Lambda1 -0.17183734\n",
      "3 Train Loss 837.8718 Test MSE 857.8699953233804 Test RE 0.4930601571378868 Lambda1 -0.17355031\n",
      "4 Train Loss 837.37585 Test MSE 857.3102555340132 Test RE 0.4928992758719746 Lambda1 -0.35017344\n",
      "5 Train Loss 834.5474 Test MSE 851.5545190089866 Test RE 0.49124189682037533 Lambda1 -0.35676658\n",
      "6 Train Loss 830.4054 Test MSE 845.0211482474562 Test RE 0.4893537938933574 Lambda1 -0.4828302\n",
      "7 Train Loss 818.1344 Test MSE 831.4701814522641 Test RE 0.4854142372890484 Lambda1 -0.630828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 810.516 Test MSE 822.9091465641061 Test RE 0.4829087953318891 Lambda1 -0.74203455\n",
      "9 Train Loss 801.854 Test MSE 811.7438680997674 Test RE 0.47962153972649746 Lambda1 -0.9302691\n",
      "10 Train Loss 793.5488 Test MSE 802.6734369514567 Test RE 0.4769343649430325 Lambda1 -0.9115045\n",
      "11 Train Loss 785.36487 Test MSE 793.0543634115903 Test RE 0.47406800989172987 Lambda1 -0.887844\n",
      "12 Train Loss 762.29694 Test MSE 763.7085035406651 Test RE 0.4652142228189256 Lambda1 -0.9367823\n",
      "13 Train Loss 749.7525 Test MSE 755.1866091150845 Test RE 0.4626113788233684 Lambda1 -0.97542274\n",
      "14 Train Loss 737.952 Test MSE 737.1682289972501 Test RE 0.4570592212564826 Lambda1 -0.8738351\n",
      "15 Train Loss 712.5003 Test MSE 712.9605811134685 Test RE 0.449491962131414 Lambda1 -0.69942254\n",
      "16 Train Loss 690.1781 Test MSE 688.3993417339946 Test RE 0.44168168791317586 Lambda1 -0.5064603\n",
      "17 Train Loss 684.15717 Test MSE 683.1112466953196 Test RE 0.43998197861891697 Lambda1 -0.46662974\n",
      "18 Train Loss 674.7757 Test MSE 669.9052796343552 Test RE 0.4357083379486231 Lambda1 -0.5071532\n",
      "19 Train Loss 666.0374 Test MSE 657.0604406935145 Test RE 0.43151096005568007 Lambda1 -0.58008003\n",
      "20 Train Loss 650.00885 Test MSE 644.7538788983585 Test RE 0.4274508179054931 Lambda1 -0.633281\n",
      "21 Train Loss 639.6439 Test MSE 637.821130988602 Test RE 0.4251465138686857 Lambda1 -0.73743886\n",
      "22 Train Loss 632.81036 Test MSE 632.7837350330075 Test RE 0.4234643207266406 Lambda1 -0.7136223\n",
      "23 Train Loss 621.0983 Test MSE 623.6045126034098 Test RE 0.42038169334543674 Lambda1 -0.78173\n",
      "24 Train Loss 619.241 Test MSE 622.8251603334135 Test RE 0.42011892435710685 Lambda1 -0.8037796\n",
      "25 Train Loss 617.6578 Test MSE 621.8144411930311 Test RE 0.41977790196909087 Lambda1 -0.8028262\n",
      "26 Train Loss 615.7049 Test MSE 620.8396613702766 Test RE 0.4194487430645917 Lambda1 -0.80014586\n",
      "27 Train Loss 613.8689 Test MSE 619.4971038164736 Test RE 0.4189949714343196 Lambda1 -0.7904746\n",
      "28 Train Loss 612.5437 Test MSE 618.8047625973128 Test RE 0.4187607745492171 Lambda1 -0.78198934\n",
      "29 Train Loss 611.3362 Test MSE 618.9912174723632 Test RE 0.4188238591534741 Lambda1 -0.76860505\n",
      "30 Train Loss 610.5082 Test MSE 619.1467840846217 Test RE 0.41887648584173875 Lambda1 -0.77051616\n",
      "31 Train Loss 609.1495 Test MSE 617.0718501423373 Test RE 0.4181740107043116 Lambda1 -0.7584653\n",
      "32 Train Loss 607.2455 Test MSE 615.228667051756 Test RE 0.4175490043159584 Lambda1 -0.7375273\n",
      "33 Train Loss 604.2824 Test MSE 614.6003272596383 Test RE 0.41733572615507303 Lambda1 -0.7384229\n",
      "34 Train Loss 600.5556 Test MSE 611.1714990937082 Test RE 0.41616994910543076 Lambda1 -0.7489148\n",
      "35 Train Loss 594.32605 Test MSE 603.7233709593664 Test RE 0.41362631872439926 Lambda1 -0.7574209\n",
      "36 Train Loss 586.235 Test MSE 590.9713408739675 Test RE 0.4092346333816422 Lambda1 -0.7503423\n",
      "37 Train Loss 557.6356 Test MSE 566.6336615534103 Test RE 0.400719388549726 Lambda1 -0.7688907\n",
      "38 Train Loss 521.1279 Test MSE 524.9300241268586 Test RE 0.3856913308878608 Lambda1 -0.7972966\n",
      "39 Train Loss 498.66 Test MSE 505.2064866276903 Test RE 0.3783760417450315 Lambda1 -0.74944293\n",
      "40 Train Loss 451.75525 Test MSE 448.6599450006469 Test RE 0.356572478694925 Lambda1 -0.6784514\n",
      "41 Train Loss 428.1851 Test MSE 420.2988232093489 Test RE 0.3451185138851843 Lambda1 -0.5709052\n",
      "42 Train Loss 405.1633 Test MSE 386.45253419159553 Test RE 0.3309308466568057 Lambda1 -0.48248327\n",
      "43 Train Loss 394.8158 Test MSE 378.97678377848416 Test RE 0.32771436115294017 Lambda1 -0.48799384\n",
      "44 Train Loss 379.26422 Test MSE 363.3569903165573 Test RE 0.32088981321407045 Lambda1 -0.52554375\n",
      "45 Train Loss 356.09998 Test MSE 351.81555751327545 Test RE 0.31575242219820737 Lambda1 -0.53908265\n",
      "46 Train Loss 345.51428 Test MSE 340.1125765374735 Test RE 0.3104563279083745 Lambda1 -0.50008816\n",
      "47 Train Loss 335.44318 Test MSE 331.26092605294673 Test RE 0.30638978116512283 Lambda1 -0.48970854\n",
      "48 Train Loss 320.2415 Test MSE 327.4158906747624 Test RE 0.3046064164326637 Lambda1 -0.49772683\n",
      "49 Train Loss 312.52798 Test MSE 320.4927344621443 Test RE 0.3013687834048904 Lambda1 -0.47122464\n",
      "50 Train Loss 303.7804 Test MSE 310.98348822087564 Test RE 0.2968642057663722 Lambda1 -0.47755426\n",
      "51 Train Loss 296.80707 Test MSE 309.56882773513996 Test RE 0.2961882201151134 Lambda1 -0.50155246\n",
      "52 Train Loss 292.40506 Test MSE 308.0771425359744 Test RE 0.29547375362574463 Lambda1 -0.5302297\n",
      "53 Train Loss 290.0406 Test MSE 305.5606571442976 Test RE 0.29426451100062123 Lambda1 -0.5315702\n",
      "54 Train Loss 287.027 Test MSE 304.1621566403204 Test RE 0.29359033876501966 Lambda1 -0.51952374\n",
      "55 Train Loss 284.89297 Test MSE 303.77067684712785 Test RE 0.2934013414015596 Lambda1 -0.52543086\n",
      "56 Train Loss 283.62476 Test MSE 302.325218125756 Test RE 0.2927024503110513 Lambda1 -0.53689724\n",
      "57 Train Loss 280.16837 Test MSE 301.04388308003996 Test RE 0.2920815160987714 Lambda1 -0.546127\n",
      "58 Train Loss 277.3323 Test MSE 299.1516745956222 Test RE 0.2911621313196727 Lambda1 -0.52954495\n",
      "59 Train Loss 275.8573 Test MSE 296.40777723606163 Test RE 0.2898237476585333 Lambda1 -0.5374309\n",
      "60 Train Loss 274.24927 Test MSE 294.72687663495464 Test RE 0.2890007976871916 Lambda1 -0.5511161\n",
      "61 Train Loss 271.74008 Test MSE 292.33494396778946 Test RE 0.2878256779908853 Lambda1 -0.55625814\n",
      "62 Train Loss 268.1333 Test MSE 289.12808409189637 Test RE 0.28624262754271834 Lambda1 -0.58941054\n",
      "63 Train Loss 266.81583 Test MSE 289.4208870100731 Test RE 0.286387531259296 Lambda1 -0.6205155\n",
      "64 Train Loss 265.41492 Test MSE 289.2802066783183 Test RE 0.28631791985288135 Lambda1 -0.6452699\n",
      "65 Train Loss 264.1377 Test MSE 288.0343206413488 Test RE 0.2857006905994149 Lambda1 -0.64719456\n",
      "66 Train Loss 262.68692 Test MSE 287.1032662744647 Test RE 0.2852385613245342 Lambda1 -0.6595514\n",
      "67 Train Loss 260.1921 Test MSE 286.66975389974374 Test RE 0.28502313159248205 Lambda1 -0.67522395\n",
      "68 Train Loss 258.04907 Test MSE 282.97454136755823 Test RE 0.28318017968155856 Lambda1 -0.6837148\n",
      "69 Train Loss 256.32962 Test MSE 281.2460805684977 Test RE 0.2823139965158781 Lambda1 -0.68953264\n",
      "70 Train Loss 255.12656 Test MSE 282.05067314082396 Test RE 0.2827175319524165 Lambda1 -0.68366593\n",
      "71 Train Loss 252.17061 Test MSE 279.18141798171513 Test RE 0.2812758365451754 Lambda1 -0.6722235\n",
      "72 Train Loss 249.86287 Test MSE 275.89146151937376 Test RE 0.2796136061233318 Lambda1 -0.67871916\n",
      "73 Train Loss 245.22598 Test MSE 268.4718999358275 Test RE 0.27582815137691885 Lambda1 -0.6692912\n",
      "74 Train Loss 242.37856 Test MSE 263.8837328335059 Test RE 0.27346105192325065 Lambda1 -0.67240924\n",
      "Training time: 150.02\n",
      "Training time: 150.02\n",
      "inv_HT_atanh_tune10\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 874.83606 Test MSE 889.2122872498062 Test RE 0.5019863387344212 Lambda1 -0.042585682\n",
      "1 Train Loss 838.1756 Test MSE 858.072576083097 Test RE 0.49311837027372374 Lambda1 -0.046286304\n",
      "2 Train Loss 838.06354 Test MSE 858.2658297570556 Test RE 0.49317389679581414 Lambda1 -0.0464838\n",
      "3 Train Loss 838.0628 Test MSE 858.292508564146 Test RE 0.4931815617805735 Lambda1 -0.04649523\n",
      "4 Train Loss 837.9711 Test MSE 858.0778472388666 Test RE 0.4931198848890506 Lambda1 -0.046129875\n",
      "5 Train Loss 837.86865 Test MSE 857.9737377875192 Test RE 0.49308996918960096 Lambda1 -0.10067092\n",
      "6 Train Loss 837.5204 Test MSE 857.3638802520924 Test RE 0.49291469104473645 Lambda1 -0.82364815\n",
      "7 Train Loss 832.8694 Test MSE 850.0627858363783 Test RE 0.4908114352207047 Lambda1 -1.1000592\n",
      "8 Train Loss 820.57947 Test MSE 832.210407723282 Test RE 0.485630262134518 Lambda1 -0.89464724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 806.8069 Test MSE 814.2082897897699 Test RE 0.48034904381543375 Lambda1 -0.74206257\n",
      "10 Train Loss 791.1485 Test MSE 798.1557028780527 Test RE 0.4755902921871041 Lambda1 -0.60052544\n",
      "11 Train Loss 775.7582 Test MSE 780.412697406347 Test RE 0.47027439570089186 Lambda1 -0.64480615\n",
      "12 Train Loss 760.67413 Test MSE 764.5393827568305 Test RE 0.4654672197088753 Lambda1 -0.68023574\n",
      "13 Train Loss 750.8633 Test MSE 745.960875925514 Test RE 0.45977695080511477 Lambda1 -0.7726645\n",
      "14 Train Loss 738.30457 Test MSE 731.778229502091 Test RE 0.4553852012463522 Lambda1 -0.9536648\n",
      "15 Train Loss 726.2486 Test MSE 722.7890515287685 Test RE 0.4525795782155657 Lambda1 -1.0648667\n",
      "16 Train Loss 715.1826 Test MSE 707.3103066217826 Test RE 0.4477072877647378 Lambda1 -1.168005\n",
      "17 Train Loss 688.2802 Test MSE 676.8732154329645 Test RE 0.43796845889316566 Lambda1 -1.2217348\n",
      "18 Train Loss 667.3813 Test MSE 648.4027945834603 Test RE 0.428658667507821 Lambda1 -1.2744825\n",
      "19 Train Loss 641.4982 Test MSE 620.9238641097788 Test RE 0.4194771864276533 Lambda1 -1.323132\n",
      "20 Train Loss 631.3686 Test MSE 606.826395390302 Test RE 0.41468793703437834 Lambda1 -1.3766745\n",
      "21 Train Loss 622.20245 Test MSE 592.9704661417578 Test RE 0.40992622410263113 Lambda1 -1.4304556\n",
      "22 Train Loss 601.4941 Test MSE 576.701173102556 Test RE 0.404263552474952 Lambda1 -1.5525565\n",
      "23 Train Loss 587.86346 Test MSE 565.5476298426948 Test RE 0.40033518732764634 Lambda1 -1.5858244\n",
      "24 Train Loss 572.38824 Test MSE 552.9116597229568 Test RE 0.3958376000278645 Lambda1 -1.6638105\n",
      "25 Train Loss 557.24725 Test MSE 535.3395926717515 Test RE 0.38949676298360747 Lambda1 -1.728426\n",
      "26 Train Loss 545.87756 Test MSE 525.6825268110144 Test RE 0.38596768183773517 Lambda1 -1.7511628\n",
      "27 Train Loss 529.82684 Test MSE 512.232174325573 Test RE 0.3809979137367915 Lambda1 -1.7372121\n",
      "28 Train Loss 521.4223 Test MSE 507.0006448798757 Test RE 0.3790473166224265 Lambda1 -1.727507\n",
      "29 Train Loss 509.49472 Test MSE 486.02517405701906 Test RE 0.3711235833885459 Lambda1 -1.7699223\n",
      "30 Train Loss 501.21198 Test MSE 482.5269738670241 Test RE 0.36978557746623564 Lambda1 -1.7909553\n",
      "31 Train Loss 492.44727 Test MSE 476.0298946630115 Test RE 0.36728761512786756 Lambda1 -1.7874402\n",
      "32 Train Loss 470.69458 Test MSE 454.3223490511515 Test RE 0.3588155214208188 Lambda1 -1.7949787\n",
      "33 Train Loss 447.2783 Test MSE 435.4822626286001 Test RE 0.35129697165661666 Lambda1 -1.8070782\n",
      "34 Train Loss 437.2144 Test MSE 426.74618303857136 Test RE 0.34775548871141326 Lambda1 -1.8184781\n",
      "35 Train Loss 424.7863 Test MSE 408.56159973513877 Test RE 0.3402655201413987 Lambda1 -1.8336214\n",
      "36 Train Loss 405.35016 Test MSE 387.8365027977977 Test RE 0.3315228839029915 Lambda1 -1.8297673\n",
      "37 Train Loss 394.18927 Test MSE 380.90147565177836 Test RE 0.3285454811352676 Lambda1 -1.8462082\n",
      "38 Train Loss 382.50504 Test MSE 361.8080549943811 Test RE 0.3202051306425216 Lambda1 -1.8812885\n",
      "39 Train Loss 358.3895 Test MSE 323.6748871347416 Test RE 0.30286122403600724 Lambda1 -1.9495045\n",
      "40 Train Loss 341.27628 Test MSE 312.6052737837595 Test RE 0.2976372757838077 Lambda1 -1.9793955\n",
      "41 Train Loss 304.02917 Test MSE 280.2961857526931 Test RE 0.28183684245930385 Lambda1 -1.9890343\n",
      "42 Train Loss 274.99442 Test MSE 247.0790563762946 Test RE 0.26461053913957766 Lambda1 -2.0240998\n",
      "43 Train Loss 260.59277 Test MSE 227.81939992195908 Test RE 0.2540882150887568 Lambda1 -2.058109\n",
      "44 Train Loss 245.82153 Test MSE 196.39135708352822 Test RE 0.2359121758678047 Lambda1 -2.0889847\n",
      "45 Train Loss 239.65324 Test MSE 194.05225889917884 Test RE 0.2345030642034094 Lambda1 -2.090384\n",
      "46 Train Loss 226.5864 Test MSE 187.70200773667673 Test RE 0.2306341584667314 Lambda1 -2.1109886\n",
      "47 Train Loss 211.9263 Test MSE 188.35577111712243 Test RE 0.231035457061676 Lambda1 -2.128161\n",
      "48 Train Loss 201.5581 Test MSE 174.92471875509634 Test RE 0.22264593113828865 Lambda1 -2.169304\n",
      "49 Train Loss 181.05542 Test MSE 154.24044514080862 Test RE 0.2090683566769315 Lambda1 -2.2467394\n",
      "50 Train Loss 167.80377 Test MSE 150.84421264459178 Test RE 0.2067537983496877 Lambda1 -2.2706754\n",
      "51 Train Loss 158.6351 Test MSE 144.05180199648282 Test RE 0.20204519115609704 Lambda1 -2.31816\n",
      "52 Train Loss 147.65303 Test MSE 131.3115944649638 Test RE 0.19290376676644094 Lambda1 -2.3681653\n",
      "53 Train Loss 140.45996 Test MSE 126.04596489208394 Test RE 0.18899644891379513 Lambda1 -2.359062\n",
      "54 Train Loss 132.05954 Test MSE 126.79866598847951 Test RE 0.1895599183119022 Lambda1 -2.3461566\n",
      "55 Train Loss 124.653564 Test MSE 120.76931364722668 Test RE 0.18499818589432712 Lambda1 -2.3589735\n",
      "56 Train Loss 120.559845 Test MSE 116.26331758102859 Test RE 0.18151416674662388 Lambda1 -2.3637464\n",
      "57 Train Loss 117.73842 Test MSE 114.46800366261196 Test RE 0.18010726236913366 Lambda1 -2.3668776\n",
      "58 Train Loss 112.497696 Test MSE 115.1093122708863 Test RE 0.180611084428623 Lambda1 -2.3824544\n",
      "59 Train Loss 109.845055 Test MSE 113.87209744166853 Test RE 0.17963784260890658 Lambda1 -2.3877583\n",
      "60 Train Loss 108.79352 Test MSE 113.26064787838608 Test RE 0.1791549002153018 Lambda1 -2.385083\n",
      "61 Train Loss 107.10672 Test MSE 113.19792852455552 Test RE 0.1791052888286459 Lambda1 -2.3760145\n",
      "62 Train Loss 106.051834 Test MSE 112.77205536168246 Test RE 0.17876805645340546 Lambda1 -2.3743966\n",
      "63 Train Loss 105.54465 Test MSE 112.64609074952048 Test RE 0.17866818800404302 Lambda1 -2.3774452\n",
      "64 Train Loss 105.28963 Test MSE 113.60565554688387 Test RE 0.17942755809257382 Lambda1 -2.3787615\n",
      "65 Train Loss 103.77492 Test MSE 112.99594821651128 Test RE 0.17894542771860664 Lambda1 -2.3738031\n",
      "66 Train Loss 101.72444 Test MSE 107.96907533105845 Test RE 0.1749197555040198 Lambda1 -2.3554864\n",
      "67 Train Loss 101.25906 Test MSE 107.62389182295225 Test RE 0.1746399172675002 Lambda1 -2.3476493\n",
      "68 Train Loss 100.83322 Test MSE 107.82929229004134 Test RE 0.17480648818987427 Lambda1 -2.342442\n",
      "69 Train Loss 98.84964 Test MSE 104.70678088266878 Test RE 0.17225687910001278 Lambda1 -2.3299525\n",
      "70 Train Loss 97.600746 Test MSE 103.20937548522411 Test RE 0.17102072599565676 Lambda1 -2.3424833\n",
      "71 Train Loss 97.307724 Test MSE 103.17138033849761 Test RE 0.17098924360517143 Lambda1 -2.3494065\n",
      "72 Train Loss 96.53051 Test MSE 102.37848034381888 Test RE 0.17033092701150498 Lambda1 -2.3504665\n",
      "73 Train Loss 95.53669 Test MSE 100.46782748701223 Test RE 0.16873402883930383 Lambda1 -2.347996\n",
      "74 Train Loss 95.0876 Test MSE 99.19689830582531 Test RE 0.16766337999930986 Lambda1 -2.348757\n",
      "Training time: 148.38\n",
      "Training time: 148.38\n",
      "inv_HT_atanh_tune10\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 871.20166 Test MSE 885.8741867330313 Test RE 0.5010432251130165 Lambda1 -0.05935534\n",
      "1 Train Loss 838.1654 Test MSE 858.0759144508432 Test RE 0.49311932952175663 Lambda1 -0.06477443\n",
      "2 Train Loss 838.06256 Test MSE 858.2652813168773 Test RE 0.4931737392243404 Lambda1 -0.06506997\n",
      "3 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "4 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "5 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "6 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "7 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "8 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "9 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "11 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "12 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "13 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "14 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "15 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "16 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "17 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "18 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "19 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "20 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "21 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "22 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "23 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "24 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "25 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "26 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "27 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "28 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "29 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "30 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "31 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "32 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "33 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "34 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "35 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "36 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "37 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "38 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "39 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "40 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "41 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "42 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "43 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "44 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "45 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "46 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "47 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "48 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "49 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "50 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "51 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "52 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "53 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "54 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "55 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "56 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "57 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "58 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "59 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "60 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "61 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "62 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "63 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "64 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "65 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "66 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "67 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "68 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "69 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "70 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "71 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "72 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "73 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "74 Train Loss 838.0621 Test MSE 858.282642853907 Test RE 0.49317872731544793 Lambda1 -0.06508351\n",
      "Training time: 117.04\n",
      "Training time: 117.04\n",
      "inv_HT_atanh_tune10\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 1050.0778 Test MSE 1056.2670959629475 Test RE 0.5471117553640856 Lambda1 0.0038361365\n",
      "1 Train Loss 838.56635 Test MSE 858.0986930839817 Test RE 0.4931258746942565 Lambda1 0.005046227\n",
      "2 Train Loss 838.04913 Test MSE 858.2449577466341 Test RE 0.49316790005626143 Lambda1 0.0050709355\n",
      "3 Train Loss 837.90643 Test MSE 857.8584963787052 Test RE 0.49305685262176063 Lambda1 -0.009128954\n",
      "4 Train Loss 837.88306 Test MSE 857.9310612578465 Test RE 0.4930777056263583 Lambda1 -0.09315102\n",
      "5 Train Loss 836.6104 Test MSE 855.8658136820603 Test RE 0.4924838693840642 Lambda1 -2.29141\n",
      "6 Train Loss 829.06476 Test MSE 845.6535891930077 Test RE 0.48953688368397963 Lambda1 -2.555741\n",
      "7 Train Loss 814.12354 Test MSE 828.5662510684921 Test RE 0.4845658351986281 Lambda1 -2.5910032\n",
      "8 Train Loss 795.95386 Test MSE 807.5120076287512 Test RE 0.4783697016910602 Lambda1 -2.2821033\n",
      "9 Train Loss 764.5019 Test MSE 769.9166001344366 Test RE 0.4671012316106253 Lambda1 -2.4466605\n",
      "10 Train Loss 737.76636 Test MSE 735.5874739621513 Test RE 0.45656890820629187 Lambda1 -2.6618047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Train Loss 721.5449 Test MSE 718.006169969518 Test RE 0.45107967489124545 Lambda1 -3.110776\n",
      "12 Train Loss 705.09814 Test MSE 707.1387800495351 Test RE 0.4476529987546613 Lambda1 -3.4228585\n",
      "13 Train Loss 689.92163 Test MSE 689.3043274468728 Test RE 0.44197191504618194 Lambda1 -3.6421869\n",
      "14 Train Loss 678.3258 Test MSE 675.6689535114301 Test RE 0.4375786787031303 Lambda1 -3.9047902\n",
      "15 Train Loss 666.3658 Test MSE 663.1968220948285 Test RE 0.43352124864662245 Lambda1 -3.8413694\n",
      "16 Train Loss 656.60706 Test MSE 654.7359824200264 Test RE 0.43074701367691864 Lambda1 -3.7553458\n",
      "17 Train Loss 643.0522 Test MSE 636.5626498078212 Test RE 0.42472687970951883 Lambda1 -3.9074688\n",
      "18 Train Loss 635.9354 Test MSE 623.308698073188 Test RE 0.4202819748848968 Lambda1 -3.9826088\n",
      "19 Train Loss 629.7765 Test MSE 618.9284648048098 Test RE 0.41880262865794604 Lambda1 -3.9034398\n",
      "20 Train Loss 626.4575 Test MSE 616.5761055570878 Test RE 0.4180060001598528 Lambda1 -3.7815645\n",
      "21 Train Loss 621.43695 Test MSE 614.0666332378499 Test RE 0.41715448808409594 Lambda1 -3.5958471\n",
      "22 Train Loss 618.62366 Test MSE 609.3790929672776 Test RE 0.4155592422197403 Lambda1 -3.4209454\n",
      "23 Train Loss 614.43823 Test MSE 605.6869840584762 Test RE 0.4142984334260201 Lambda1 -3.3105943\n",
      "24 Train Loss 612.57404 Test MSE 604.2624046530951 Test RE 0.41381093040937755 Lambda1 -3.3357763\n",
      "25 Train Loss 608.4104 Test MSE 595.7849053326792 Test RE 0.4108978970254562 Lambda1 -3.391483\n",
      "26 Train Loss 604.6923 Test MSE 594.6544575516308 Test RE 0.41050789120536657 Lambda1 -3.3628988\n",
      "27 Train Loss 602.10425 Test MSE 593.5169393076524 Test RE 0.41011507169797357 Lambda1 -3.3473134\n",
      "28 Train Loss 600.83466 Test MSE 593.0895877960504 Test RE 0.40996739701137763 Lambda1 -3.3682487\n",
      "29 Train Loss 598.2615 Test MSE 589.0977555181041 Test RE 0.40858541011686494 Lambda1 -3.4842854\n",
      "30 Train Loss 597.27466 Test MSE 587.7803176156616 Test RE 0.4081282812374677 Lambda1 -3.490626\n",
      "31 Train Loss 595.22424 Test MSE 583.5827726457655 Test RE 0.4066683766949554 Lambda1 -3.5321903\n",
      "32 Train Loss 593.4219 Test MSE 582.5943761264376 Test RE 0.40632384977979397 Lambda1 -3.5673451\n",
      "33 Train Loss 592.0775 Test MSE 582.2903289200274 Test RE 0.4062178088022958 Lambda1 -3.5691426\n",
      "34 Train Loss 591.10425 Test MSE 581.6676344049062 Test RE 0.4060005483993285 Lambda1 -3.5703154\n",
      "35 Train Loss 590.44867 Test MSE 580.5788409835059 Test RE 0.4056203847484699 Lambda1 -3.56864\n",
      "36 Train Loss 589.8345 Test MSE 579.4519228204065 Test RE 0.40522653384203333 Lambda1 -3.549044\n",
      "37 Train Loss 589.06036 Test MSE 577.7029044691121 Test RE 0.40461450353649964 Lambda1 -3.526709\n",
      "38 Train Loss 588.464 Test MSE 577.9044370237958 Test RE 0.404685072581219 Lambda1 -3.50704\n",
      "39 Train Loss 587.9064 Test MSE 577.2722374015615 Test RE 0.40446365902081804 Lambda1 -3.4695435\n",
      "40 Train Loss 583.8061 Test MSE 571.6178028050281 Test RE 0.4024779050458308 Lambda1 -3.4409208\n",
      "41 Train Loss 581.138 Test MSE 570.6362939278072 Test RE 0.40213221483408296 Lambda1 -3.4773366\n",
      "42 Train Loss 578.78467 Test MSE 569.836856232321 Test RE 0.4018504308603226 Lambda1 -3.520537\n",
      "43 Train Loss 574.60406 Test MSE 563.3225381477376 Test RE 0.3995468714056689 Lambda1 -3.5493603\n",
      "44 Train Loss 571.1642 Test MSE 557.3880172326484 Test RE 0.39743671508717154 Lambda1 -3.6329641\n",
      "45 Train Loss 569.34784 Test MSE 555.1059095132779 Test RE 0.39662227017080176 Lambda1 -3.7192588\n",
      "46 Train Loss 567.01245 Test MSE 553.9339003002879 Test RE 0.39620334959560394 Lambda1 -3.751199\n",
      "47 Train Loss 563.7977 Test MSE 548.7107565715871 Test RE 0.3943309886277956 Lambda1 -3.8639371\n",
      "48 Train Loss 561.77124 Test MSE 545.6577946352753 Test RE 0.393232452908347 Lambda1 -3.9704163\n",
      "49 Train Loss 557.5825 Test MSE 544.1579109371175 Test RE 0.39269162968858323 Lambda1 -4.0292463\n",
      "50 Train Loss 555.9206 Test MSE 543.0102604486507 Test RE 0.3922773100544124 Lambda1 -4.046163\n",
      "51 Train Loss 554.5398 Test MSE 541.1959086908174 Test RE 0.39162140666129003 Lambda1 -4.0656433\n",
      "52 Train Loss 553.10254 Test MSE 539.618531427267 Test RE 0.39105027755204164 Lambda1 -4.042304\n",
      "53 Train Loss 550.0443 Test MSE 536.4927248591512 Test RE 0.3899160292360568 Lambda1 -3.9619913\n",
      "54 Train Loss 547.0287 Test MSE 530.1868754137167 Test RE 0.387617750637957 Lambda1 -3.882965\n",
      "55 Train Loss 546.6008 Test MSE 528.584616255395 Test RE 0.38703160445623735 Lambda1 -3.8784719\n",
      "56 Train Loss 545.97186 Test MSE 527.6790689906982 Test RE 0.38669993982454465 Lambda1 -3.8520596\n",
      "57 Train Loss 544.1234 Test MSE 524.5343530962567 Test RE 0.3855459442196959 Lambda1 -3.738907\n",
      "58 Train Loss 542.0362 Test MSE 523.9145108964715 Test RE 0.3853180770776459 Lambda1 -3.631511\n",
      "59 Train Loss 540.9053 Test MSE 524.4689974135522 Test RE 0.38552192443606415 Lambda1 -3.5470936\n",
      "60 Train Loss 539.44495 Test MSE 521.5585068081867 Test RE 0.3844507276946929 Lambda1 -3.4887342\n",
      "61 Train Loss 538.5076 Test MSE 520.1701359680133 Test RE 0.3839386893910015 Lambda1 -3.5203855\n",
      "62 Train Loss 532.831 Test MSE 515.368278849318 Test RE 0.3821624501258875 Lambda1 -3.537664\n",
      "63 Train Loss 519.2813 Test MSE 503.6152345555066 Test RE 0.3777796850794542 Lambda1 -3.4609578\n",
      "64 Train Loss 511.71695 Test MSE 494.50026085106936 Test RE 0.3743453416530908 Lambda1 -3.47753\n",
      "65 Train Loss 507.3981 Test MSE 489.3212898197662 Test RE 0.37237989620997747 Lambda1 -3.524638\n",
      "66 Train Loss 504.95166 Test MSE 485.90673962976115 Test RE 0.37107836300761193 Lambda1 -3.5438318\n",
      "67 Train Loss 502.60474 Test MSE 483.4056143963568 Test RE 0.3701220983654834 Lambda1 -3.5520635\n",
      "68 Train Loss 500.16705 Test MSE 481.1590079907843 Test RE 0.3692610336452835 Lambda1 -3.5247283\n",
      "69 Train Loss 496.1533 Test MSE 470.48773582448337 Test RE 0.3651432899317615 Lambda1 -3.513212\n",
      "70 Train Loss 483.45496 Test MSE 451.852723804843 Test RE 0.3578389599706422 Lambda1 -3.529877\n",
      "71 Train Loss 471.5417 Test MSE 442.71874082288446 Test RE 0.3542037248096185 Lambda1 -3.4969923\n",
      "72 Train Loss 469.4132 Test MSE 440.10372962516425 Test RE 0.3531560861586456 Lambda1 -3.478302\n",
      "73 Train Loss 460.8001 Test MSE 430.68223585036856 Test RE 0.3493555525003919 Lambda1 -3.477066\n",
      "74 Train Loss 441.4481 Test MSE 409.3340801850158 Test RE 0.34058704367123016 Lambda1 -3.5128362\n",
      "Training time: 148.45\n",
      "Training time: 148.45\n",
      "inv_HT_atanh_tune10\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 964.9153 Test MSE 974.2819105728023 Test RE 0.5254501135032467 Lambda1 -0.0059342613\n",
      "1 Train Loss 838.42267 Test MSE 858.062966019094 Test RE 0.49311560890335726 Lambda1 -0.0061438037\n",
      "2 Train Loss 838.0618 Test MSE 858.2498020821236 Test RE 0.49316929188930836 Lambda1 -0.006151353\n",
      "3 Train Loss 838.0596 Test MSE 858.2973093634542 Test RE 0.49318294106698557 Lambda1 -0.006159287\n",
      "4 Train Loss 837.87274 Test MSE 857.8407921383949 Test RE 0.49305176481177754 Lambda1 -0.0071913353\n",
      "5 Train Loss 837.7332 Test MSE 857.6462685654514 Test RE 0.4929958595496531 Lambda1 -0.020921659\n",
      "6 Train Loss 825.5192 Test MSE 839.236520591386 Test RE 0.48767597161769716 Lambda1 0.029323181\n",
      "7 Train Loss 753.37476 Test MSE 765.864697700123 Test RE 0.4658704844491322 Lambda1 0.0035693336\n",
      "8 Train Loss 728.2688 Test MSE 736.0334290456375 Test RE 0.4567072862933665 Lambda1 -0.00015459902\n",
      "9 Train Loss 687.59076 Test MSE 694.8122402862991 Test RE 0.443734198477117 Lambda1 0.00018826961\n",
      "10 Train Loss 663.6254 Test MSE 671.5110670454587 Test RE 0.4362302297834707 Lambda1 0.00013197902\n",
      "11 Train Loss 658.45953 Test MSE 667.4487701909705 Test RE 0.43490874382199846 Lambda1 3.3397915e-05\n",
      "12 Train Loss 654.19794 Test MSE 661.2792785598333 Test RE 0.432894061056936 Lambda1 -0.00013674473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Train Loss 651.93097 Test MSE 658.1428980940975 Test RE 0.4318662546273377 Lambda1 -0.00042790413\n",
      "14 Train Loss 647.7933 Test MSE 653.6020969832147 Test RE 0.4303738637163205 Lambda1 -0.0009554358\n",
      "15 Train Loss 645.07 Test MSE 651.5074277644052 Test RE 0.4296836774967006 Lambda1 -0.0011438994\n",
      "16 Train Loss 642.337 Test MSE 649.7313020087846 Test RE 0.429097580500693 Lambda1 -0.0011170914\n",
      "17 Train Loss 636.88525 Test MSE 643.9210572786949 Test RE 0.4271746618980194 Lambda1 -0.001795724\n",
      "18 Train Loss 631.19775 Test MSE 639.1657525531732 Test RE 0.42559441389379665 Lambda1 -0.001726037\n",
      "19 Train Loss 611.2694 Test MSE 608.3853017468181 Test RE 0.4152202515627194 Lambda1 0.0002393386\n",
      "20 Train Loss 524.0883 Test MSE 528.8096800893878 Test RE 0.3871139919717441 Lambda1 1.7106358e-06\n",
      "21 Train Loss 424.98978 Test MSE 435.887052886937 Test RE 0.35146020283109747 Lambda1 0.00014872573\n",
      "22 Train Loss 340.55682 Test MSE 348.45710823671715 Test RE 0.3142417138553081 Lambda1 -0.000107582055\n",
      "23 Train Loss 293.97934 Test MSE 316.79765151178015 Test RE 0.29962644902329294 Lambda1 -9.5430274e-05\n",
      "24 Train Loss 281.8578 Test MSE 300.34628895017386 Test RE 0.2917429067847046 Lambda1 0.00014590666\n",
      "25 Train Loss 271.6719 Test MSE 294.8740444808818 Test RE 0.28907294298537567 Lambda1 0.00021011641\n",
      "26 Train Loss 266.50308 Test MSE 292.45476392608504 Test RE 0.2878846578157861 Lambda1 0.00036040597\n",
      "27 Train Loss 262.98773 Test MSE 290.60987078350314 Test RE 0.2869751894810601 Lambda1 0.00028980867\n",
      "28 Train Loss 260.84558 Test MSE 287.387087324502 Test RE 0.285379515328856 Lambda1 -4.8174945e-05\n",
      "29 Train Loss 259.53085 Test MSE 287.61026522134995 Test RE 0.2854903032565919 Lambda1 8.099155e-05\n",
      "30 Train Loss 258.34903 Test MSE 287.80288311068745 Test RE 0.2855858863117288 Lambda1 -6.203734e-05\n",
      "31 Train Loss 257.9188 Test MSE 286.4844309335001 Test RE 0.2849309874596105 Lambda1 -6.5419634e-05\n",
      "32 Train Loss 257.75006 Test MSE 286.3164039415791 Test RE 0.2848474172658904 Lambda1 -5.560909e-05\n",
      "33 Train Loss 256.58777 Test MSE 286.83432962503235 Test RE 0.28510493505709833 Lambda1 -3.1382628e-05\n",
      "34 Train Loss 255.49734 Test MSE 284.54855178405705 Test RE 0.28396666464412623 Lambda1 2.586002e-05\n",
      "35 Train Loss 255.06004 Test MSE 283.3765241663281 Test RE 0.2833812457605492 Lambda1 7.2714836e-05\n",
      "36 Train Loss 254.72182 Test MSE 283.37639294416334 Test RE 0.28338118014836605 Lambda1 4.992969e-05\n",
      "37 Train Loss 254.44002 Test MSE 283.5083483218563 Test RE 0.283447151272766 Lambda1 3.705128e-05\n",
      "38 Train Loss 254.1512 Test MSE 283.76379259618056 Test RE 0.2835748170896896 Lambda1 2.5638552e-05\n",
      "39 Train Loss 253.8333 Test MSE 284.5465003604824 Test RE 0.2839656410280217 Lambda1 1.4445963e-05\n",
      "40 Train Loss 253.57402 Test MSE 284.9689414772337 Test RE 0.2841763522339773 Lambda1 5.33432e-06\n",
      "41 Train Loss 253.41136 Test MSE 284.6098942316555 Test RE 0.28399727149739845 Lambda1 3.0773426e-06\n",
      "42 Train Loss 253.36145 Test MSE 284.49909800229364 Test RE 0.28394198724620023 Lambda1 2.531755e-06\n",
      "43 Train Loss 253.22069 Test MSE 284.4674708192664 Test RE 0.28392620418246356 Lambda1 3.7611019e-06\n",
      "44 Train Loss 253.05171 Test MSE 284.8676709865362 Test RE 0.2841258533344523 Lambda1 7.560155e-07\n",
      "45 Train Loss 252.95445 Test MSE 284.86139182562516 Test RE 0.28412272191245336 Lambda1 -1.912476e-06\n",
      "46 Train Loss 252.82162 Test MSE 284.8342153788832 Test RE 0.2841091686012437 Lambda1 -2.263262e-06\n",
      "47 Train Loss 252.59856 Test MSE 284.7894192990182 Test RE 0.28408682669715635 Lambda1 -1.2717952e-06\n",
      "48 Train Loss 252.3003 Test MSE 284.55860672638465 Test RE 0.2839716817900486 Lambda1 1.8137673e-06\n",
      "49 Train Loss 252.1849 Test MSE 284.1980792511512 Test RE 0.2837917328452561 Lambda1 1.0238351e-06\n",
      "50 Train Loss 252.12643 Test MSE 284.30172191648904 Test RE 0.2838434753667898 Lambda1 8.547082e-07\n",
      "51 Train Loss 252.07166 Test MSE 284.71917439094005 Test RE 0.2840517887316099 Lambda1 8.437447e-07\n",
      "52 Train Loss 251.73843 Test MSE 286.0682149699319 Test RE 0.28472393269834506 Lambda1 -3.494779e-07\n",
      "53 Train Loss 251.44835 Test MSE 285.47090476547 Test RE 0.2844265256987514 Lambda1 3.297395e-07\n",
      "54 Train Loss 251.25816 Test MSE 285.8351767843782 Test RE 0.2846079375202836 Lambda1 5.525502e-07\n",
      "55 Train Loss 251.02748 Test MSE 287.2034530339065 Test RE 0.2852883250121505 Lambda1 1.2587009e-08\n",
      "56 Train Loss 250.96523 Test MSE 287.62863811989723 Test RE 0.2854994218473484 Lambda1 6.7872236e-08\n",
      "57 Train Loss 250.94069 Test MSE 287.8523472087762 Test RE 0.2856104267910151 Lambda1 1.9894909e-08\n",
      "58 Train Loss 250.9058 Test MSE 288.03097993216204 Test RE 0.2856990337730909 Lambda1 1.4595548e-07\n",
      "59 Train Loss 250.87268 Test MSE 288.24166063364777 Test RE 0.2858035021711383 Lambda1 -1.9584387e-08\n",
      "60 Train Loss 250.85632 Test MSE 288.5874398989541 Test RE 0.2859748779961364 Lambda1 2.1547683e-08\n",
      "61 Train Loss 250.81032 Test MSE 288.8064780031265 Test RE 0.2860833849945865 Lambda1 1.4256546e-08\n",
      "62 Train Loss 250.72627 Test MSE 288.39156094008604 Test RE 0.2858778086816926 Lambda1 2.5120615e-08\n",
      "63 Train Loss 250.62169 Test MSE 289.0683903402897 Test RE 0.2862130770090887 Lambda1 9.368032e-08\n",
      "64 Train Loss 250.29437 Test MSE 289.76233997717884 Test RE 0.28655641859886943 Lambda1 8.555479e-08\n",
      "65 Train Loss 250.19441 Test MSE 289.3620090310243 Test RE 0.28635839933157353 Lambda1 2.448347e-08\n",
      "66 Train Loss 250.18657 Test MSE 289.3560319709261 Test RE 0.2863554418075246 Lambda1 8.19672e-08\n",
      "67 Train Loss 250.18478 Test MSE 289.41589637831413 Test RE 0.28638506208562114 Lambda1 5.625075e-08\n",
      "68 Train Loss 250.1836 Test MSE 289.4070094497539 Test RE 0.28638066512063537 Lambda1 5.7039284e-08\n",
      "69 Train Loss 250.18275 Test MSE 289.3844585659628 Test RE 0.28636950736926914 Lambda1 6.736002e-08\n",
      "70 Train Loss 250.18246 Test MSE 289.4005280199633 Test RE 0.28637745827611144 Lambda1 5.8636346e-08\n",
      "71 Train Loss 250.18225 Test MSE 289.3998516812665 Test RE 0.286377123639075 Lambda1 5.463382e-08\n",
      "72 Train Loss 250.18225 Test MSE 289.3998516812665 Test RE 0.286377123639075 Lambda1 5.463382e-08\n",
      "73 Train Loss 250.18225 Test MSE 289.3998516812665 Test RE 0.286377123639075 Lambda1 5.463382e-08\n",
      "74 Train Loss 250.18225 Test MSE 289.3998516812665 Test RE 0.286377123639075 Lambda1 5.463382e-08\n",
      "Training time: 150.00\n",
      "Training time: 150.00\n",
      "inv_HT_atanh_tune10\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 903.81335 Test MSE 916.2199524890324 Test RE 0.5095526249740313 Lambda1 -0.10737295\n",
      "1 Train Loss 838.2498 Test MSE 858.0572183371836 Test RE 0.49311395734832947 Lambda1 -0.12019923\n",
      "2 Train Loss 838.06647 Test MSE 858.2645552087309 Test RE 0.49317353060728863 Lambda1 -0.120861515\n",
      "3 Train Loss 838.06494 Test MSE 858.3014444472201 Test RE 0.493184129087905 Lambda1 -0.120966636\n",
      "4 Train Loss 837.9867 Test MSE 857.9278948424384 Test RE 0.4930767957104246 Lambda1 -0.12210777\n",
      "5 Train Loss 837.8469 Test MSE 857.8708527807419 Test RE 0.4930604035493234 Lambda1 -0.12158518\n",
      "6 Train Loss 836.5461 Test MSE 855.2547300850365 Test RE 0.4923080225703971 Lambda1 -0.36938283\n",
      "7 Train Loss 829.2821 Test MSE 844.2894691597576 Test RE 0.4891418894469214 Lambda1 -0.42232785\n",
      "8 Train Loss 814.91815 Test MSE 825.6389511321323 Test RE 0.4837090994774761 Lambda1 -0.6526224\n",
      "9 Train Loss 799.36536 Test MSE 808.3117004489347 Test RE 0.4786065118857986 Lambda1 -0.76680875\n",
      "10 Train Loss 792.3395 Test MSE 796.9453534844102 Test RE 0.47522955504552244 Lambda1 -0.7685853\n",
      "11 Train Loss 776.68207 Test MSE 780.7531033353058 Test RE 0.4703769483247845 Lambda1 -0.7545192\n",
      "12 Train Loss 764.41626 Test MSE 766.2052264657308 Test RE 0.46597404364894607 Lambda1 -0.7446006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Train Loss 750.25256 Test MSE 752.9936364232447 Test RE 0.46193920609645805 Lambda1 -0.66757953\n",
      "14 Train Loss 735.86957 Test MSE 734.7975009823546 Test RE 0.4563236797313497 Lambda1 -0.6017183\n",
      "15 Train Loss 722.04596 Test MSE 722.4801924319408 Test RE 0.45248287070763693 Lambda1 -0.5233269\n",
      "16 Train Loss 713.0397 Test MSE 712.2765974678982 Test RE 0.4492762987891082 Lambda1 -0.45680988\n",
      "17 Train Loss 706.61163 Test MSE 704.7042346510997 Test RE 0.44688174195310837 Lambda1 -0.41602808\n",
      "18 Train Loss 701.63696 Test MSE 698.5909272220742 Test RE 0.4449391708436696 Lambda1 -0.37468347\n",
      "19 Train Loss 695.0369 Test MSE 690.6162163814109 Test RE 0.44239229718030665 Lambda1 -0.35312074\n",
      "20 Train Loss 688.96796 Test MSE 683.1182281758115 Test RE 0.4399842269479902 Lambda1 -0.3277507\n",
      "21 Train Loss 678.622 Test MSE 675.1393515076431 Test RE 0.4374071538959776 Lambda1 -0.3350659\n",
      "22 Train Loss 670.60913 Test MSE 671.4935318697309 Test RE 0.43622453410394263 Lambda1 -0.36409894\n",
      "23 Train Loss 662.6478 Test MSE 663.3621694455277 Test RE 0.4335752877495062 Lambda1 -0.39695328\n",
      "24 Train Loss 656.7264 Test MSE 656.6379692980764 Test RE 0.4313722130400445 Lambda1 -0.4126433\n",
      "25 Train Loss 653.38696 Test MSE 652.3824239099258 Test RE 0.42997212041746097 Lambda1 -0.42076412\n",
      "26 Train Loss 645.4673 Test MSE 644.2759849775741 Test RE 0.4272923744896496 Lambda1 -0.46490207\n",
      "27 Train Loss 631.34656 Test MSE 633.0413605304259 Test RE 0.42355051455439935 Lambda1 -0.5175411\n",
      "28 Train Loss 628.0711 Test MSE 631.2834475195333 Test RE 0.4229620201626155 Lambda1 -0.519564\n",
      "29 Train Loss 626.662 Test MSE 631.1042342784575 Test RE 0.42290197916578937 Lambda1 -0.5335322\n",
      "30 Train Loss 624.52313 Test MSE 628.4059625182337 Test RE 0.42199695689563005 Lambda1 -0.5660441\n",
      "31 Train Loss 621.61945 Test MSE 625.0163015554464 Test RE 0.4208572789889246 Lambda1 -0.59974074\n",
      "32 Train Loss 618.9191 Test MSE 624.3442247264151 Test RE 0.4206309452900213 Lambda1 -0.5970811\n",
      "33 Train Loss 617.8026 Test MSE 623.9002295208358 Test RE 0.4204813552640687 Lambda1 -0.597016\n",
      "34 Train Loss 615.79004 Test MSE 622.8571198312144 Test RE 0.42012970315865006 Lambda1 -0.62589735\n",
      "35 Train Loss 614.9733 Test MSE 622.2633362844342 Test RE 0.41992939591236655 Lambda1 -0.6550116\n",
      "36 Train Loss 614.6444 Test MSE 621.8722012235554 Test RE 0.41979739799539106 Lambda1 -0.6816802\n",
      "37 Train Loss 613.8663 Test MSE 621.548946109858 Test RE 0.4196882764631123 Lambda1 -0.7052117\n",
      "38 Train Loss 613.3973 Test MSE 621.5780779852431 Test RE 0.41969811170085936 Lambda1 -0.72155637\n",
      "39 Train Loss 612.79724 Test MSE 621.5335435342757 Test RE 0.4196830762930469 Lambda1 -0.7605651\n",
      "40 Train Loss 612.1104 Test MSE 621.0646233517185 Test RE 0.4195247300543944 Lambda1 -0.7905665\n",
      "41 Train Loss 611.79895 Test MSE 620.6340711045964 Test RE 0.41937928735469415 Lambda1 -0.80258256\n",
      "42 Train Loss 611.30585 Test MSE 620.0112482820228 Test RE 0.4191688053923104 Lambda1 -0.81176347\n",
      "43 Train Loss 611.1774 Test MSE 619.962635123072 Test RE 0.4191523722072167 Lambda1 -0.8140369\n",
      "44 Train Loss 610.973 Test MSE 619.6967807181007 Test RE 0.4190624914248326 Lambda1 -0.8183825\n",
      "45 Train Loss 610.6225 Test MSE 619.0934955317222 Test RE 0.4188584595815247 Lambda1 -0.81336945\n",
      "46 Train Loss 609.76373 Test MSE 618.0479601582443 Test RE 0.4185046225667154 Lambda1 -0.8098354\n",
      "47 Train Loss 608.0391 Test MSE 616.1351628998865 Test RE 0.41785650552330184 Lambda1 -0.8145982\n",
      "48 Train Loss 606.8455 Test MSE 612.476979554989 Test RE 0.4166141876867643 Lambda1 -0.79455864\n",
      "49 Train Loss 605.4065 Test MSE 609.4593101881001 Test RE 0.4155865929369863 Lambda1 -0.7777898\n",
      "50 Train Loss 604.37933 Test MSE 607.9782035613528 Test RE 0.4150813069685017 Lambda1 -0.7728844\n",
      "51 Train Loss 602.524 Test MSE 603.4972555074485 Test RE 0.4135488527313368 Lambda1 -0.8069309\n",
      "52 Train Loss 598.6038 Test MSE 597.8914354047904 Test RE 0.4116236665236285 Lambda1 -0.86634725\n",
      "53 Train Loss 593.7547 Test MSE 593.7991453351007 Test RE 0.4102125610728394 Lambda1 -0.91193724\n",
      "54 Train Loss 588.29065 Test MSE 588.6747254560983 Test RE 0.4084386815386023 Lambda1 -0.9457819\n",
      "55 Train Loss 573.7579 Test MSE 570.2410897862914 Test RE 0.40199293886757254 Lambda1 -0.9972791\n",
      "56 Train Loss 559.61005 Test MSE 550.4853520660739 Test RE 0.3949681304272319 Lambda1 -1.055659\n",
      "57 Train Loss 546.593 Test MSE 543.1251358182756 Test RE 0.3923188015521065 Lambda1 -1.0999074\n",
      "58 Train Loss 537.26935 Test MSE 532.2370763727519 Test RE 0.38836647485569414 Lambda1 -1.1408664\n",
      "59 Train Loss 526.95874 Test MSE 516.4950448097375 Test RE 0.38257998895392564 Lambda1 -1.1532207\n",
      "60 Train Loss 503.49142 Test MSE 495.63137342095774 Test RE 0.37477323309871435 Lambda1 -1.16323\n",
      "61 Train Loss 485.93335 Test MSE 474.69120023415695 Test RE 0.36677080719503546 Lambda1 -1.198933\n",
      "62 Train Loss 453.7298 Test MSE 448.4623153068358 Test RE 0.3564939369479775 Lambda1 -1.2062621\n",
      "63 Train Loss 428.873 Test MSE 433.1396819031776 Test RE 0.3503508352922037 Lambda1 -1.2118082\n",
      "64 Train Loss 411.49612 Test MSE 415.1357408520919 Test RE 0.34299219165684863 Lambda1 -1.2734334\n",
      "65 Train Loss 400.12512 Test MSE 396.9193370283098 Test RE 0.33538242398314144 Lambda1 -1.3046129\n",
      "66 Train Loss 369.11472 Test MSE 363.5888228475321 Test RE 0.32099216549728105 Lambda1 -1.3098965\n",
      "67 Train Loss 359.6604 Test MSE 347.4877271071386 Test RE 0.3138043111837439 Lambda1 -1.3271459\n",
      "68 Train Loss 341.02856 Test MSE 325.33666006410726 Test RE 0.3036376856614337 Lambda1 -1.3671111\n",
      "69 Train Loss 325.61133 Test MSE 317.47700299717553 Test RE 0.29994754146721064 Lambda1 -1.3761866\n",
      "70 Train Loss 315.64554 Test MSE 320.9707956706796 Test RE 0.30159346718557556 Lambda1 -1.3585759\n",
      "71 Train Loss 305.2708 Test MSE 311.13037739491807 Test RE 0.29693430754291505 Lambda1 -1.3483864\n",
      "72 Train Loss 283.23102 Test MSE 299.15562020639817 Test RE 0.29116405143035984 Lambda1 -1.2965777\n",
      "73 Train Loss 273.65564 Test MSE 298.5639689595636 Test RE 0.2908759859190404 Lambda1 -1.2458221\n",
      "74 Train Loss 268.92377 Test MSE 296.7974281489199 Test RE 0.29001418294238457 Lambda1 -1.2109715\n",
      "Training time: 151.32\n",
      "Training time: 151.32\n",
      "inv_HT_atanh_tune10\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 865.44617 Test MSE 880.6233889046283 Test RE 0.4995561139744498 Lambda1 0.026341142\n",
      "1 Train Loss 838.1444 Test MSE 858.0887315801693 Test RE 0.4931230123839087 Lambda1 0.029447686\n",
      "2 Train Loss 838.06256 Test MSE 858.2678692081379 Test RE 0.49317448274681264 Lambda1 0.029621143\n",
      "3 Train Loss 838.06177 Test MSE 858.2959362471705 Test RE 0.4931825465662744 Lambda1 0.029662326\n",
      "4 Train Loss 837.8986 Test MSE 857.7036317509748 Test RE 0.49301234614857864 Lambda1 0.026629372\n",
      "5 Train Loss 837.8641 Test MSE 857.9146321293271 Test RE 0.49307298445694026 Lambda1 0.011525281\n",
      "6 Train Loss 837.6582 Test MSE 857.4937418335253 Test RE 0.4929520195694658 Lambda1 -0.3269894\n",
      "7 Train Loss 834.4988 Test MSE 851.9222062853947 Test RE 0.4913479404721446 Lambda1 -0.41832796\n",
      "8 Train Loss 826.206 Test MSE 837.0354826552901 Test RE 0.48703604592307814 Lambda1 -0.43846303\n",
      "9 Train Loss 798.2124 Test MSE 801.1162704686237 Test RE 0.4764715199604496 Lambda1 -0.57434255\n",
      "10 Train Loss 774.41547 Test MSE 782.1735056320808 Test RE 0.47080462571469417 Lambda1 -0.5465939\n",
      "11 Train Loss 757.22455 Test MSE 763.3927306340357 Test RE 0.4651180360966262 Lambda1 -0.59846836\n",
      "12 Train Loss 745.168 Test MSE 746.5076312244404 Test RE 0.4599454177084325 Lambda1 -0.6002534\n",
      "13 Train Loss 734.9257 Test MSE 734.3008496538977 Test RE 0.45616943854928244 Lambda1 -0.5449935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 Train Loss 729.511 Test MSE 731.0999992199937 Test RE 0.45517412115535066 Lambda1 -0.49176106\n",
      "15 Train Loss 721.5556 Test MSE 724.1918695819844 Test RE 0.45301855764297666 Lambda1 -0.4763727\n",
      "16 Train Loss 710.85754 Test MSE 712.8776789123876 Test RE 0.4494658281786202 Lambda1 -0.48568577\n",
      "17 Train Loss 703.2571 Test MSE 705.1666837805498 Test RE 0.4470283468472469 Lambda1 -0.5404714\n",
      "18 Train Loss 692.9959 Test MSE 691.6750846207729 Test RE 0.4427313101700227 Lambda1 -0.58946913\n",
      "19 Train Loss 686.39 Test MSE 687.0659192258861 Test RE 0.441253714103773 Lambda1 -0.5721359\n",
      "20 Train Loss 675.39 Test MSE 672.0019308568565 Test RE 0.43638963930530095 Lambda1 -0.6192694\n",
      "21 Train Loss 667.1463 Test MSE 659.5756723133691 Test RE 0.4323360845385063 Lambda1 -0.67424846\n",
      "22 Train Loss 653.2435 Test MSE 644.1572882506481 Test RE 0.427253012055166 Lambda1 -0.7425684\n",
      "23 Train Loss 645.2676 Test MSE 641.5690765359074 Test RE 0.4263938008807205 Lambda1 -0.7546484\n",
      "24 Train Loss 639.9655 Test MSE 636.7852116494347 Test RE 0.4248011219977928 Lambda1 -0.7606109\n",
      "25 Train Loss 634.67804 Test MSE 630.516033019825 Test RE 0.4227048568441325 Lambda1 -0.78557205\n",
      "26 Train Loss 629.35205 Test MSE 628.4024542673344 Test RE 0.42199577893635054 Lambda1 -0.8265049\n",
      "27 Train Loss 622.8146 Test MSE 623.1820925503057 Test RE 0.4202392891967201 Lambda1 -0.8939981\n",
      "28 Train Loss 617.1963 Test MSE 616.2465685082312 Test RE 0.4178942808824301 Lambda1 -0.9365942\n",
      "29 Train Loss 610.8292 Test MSE 596.0487270064434 Test RE 0.41098886255005984 Lambda1 -0.9903043\n",
      "30 Train Loss 598.9106 Test MSE 588.2551122346545 Test RE 0.40829308600352887 Lambda1 -0.9845726\n",
      "31 Train Loss 586.5531 Test MSE 567.7280527866498 Test RE 0.4011061748191474 Lambda1 -0.9495723\n",
      "32 Train Loss 534.0299 Test MSE 481.44271558626906 Test RE 0.36936988198849935 Lambda1 -0.88521874\n",
      "33 Train Loss 458.17178 Test MSE 435.3835204713259 Test RE 0.35125714249441825 Lambda1 -0.7966448\n",
      "34 Train Loss 400.82013 Test MSE 385.0079513685768 Test RE 0.33031174785065 Lambda1 -0.7199461\n",
      "35 Train Loss 374.49628 Test MSE 357.65043895164274 Test RE 0.3183600409054285 Lambda1 -0.7150736\n",
      "36 Train Loss 347.91525 Test MSE 330.41558078264103 Test RE 0.3059985931588101 Lambda1 -0.71624506\n",
      "37 Train Loss 322.67813 Test MSE 310.75457105049287 Test RE 0.2967549237190647 Lambda1 -0.6544266\n",
      "38 Train Loss 303.8308 Test MSE 296.64963441288666 Test RE 0.2899419659809053 Lambda1 -0.5310356\n",
      "39 Train Loss 288.30048 Test MSE 289.00472814090386 Test RE 0.28618155859008226 Lambda1 -0.492366\n",
      "40 Train Loss 275.02164 Test MSE 282.6266384785759 Test RE 0.283006048288217 Lambda1 -0.49747458\n",
      "41 Train Loss 268.54422 Test MSE 282.85102579295756 Test RE 0.2831183702691835 Lambda1 -0.5158426\n",
      "42 Train Loss 256.4521 Test MSE 277.08002922426385 Test RE 0.2802152603592533 Lambda1 -0.55845046\n",
      "43 Train Loss 249.25604 Test MSE 269.679245009287 Test RE 0.27644766908707474 Lambda1 -0.54716396\n",
      "44 Train Loss 244.15895 Test MSE 263.25734380491167 Test RE 0.27313629758439806 Lambda1 -0.5231762\n",
      "45 Train Loss 238.86292 Test MSE 257.2511469707985 Test RE 0.27000252805794633 Lambda1 -0.5144544\n",
      "46 Train Loss 236.12065 Test MSE 256.2315651494334 Test RE 0.26946693670440713 Lambda1 -0.5008499\n",
      "47 Train Loss 230.46474 Test MSE 248.6323385907676 Test RE 0.2654409836607444 Lambda1 -0.48811346\n",
      "48 Train Loss 225.76224 Test MSE 240.9607316563874 Test RE 0.26131377700748026 Lambda1 -0.50321716\n",
      "49 Train Loss 220.57396 Test MSE 236.16716363363807 Test RE 0.2587014884810876 Lambda1 -0.52614504\n",
      "50 Train Loss 212.13797 Test MSE 227.46160733522353 Test RE 0.253888612680163 Lambda1 -0.5401688\n",
      "51 Train Loss 204.83415 Test MSE 222.9739066265603 Test RE 0.2513715899654962 Lambda1 -0.55268055\n",
      "52 Train Loss 200.4286 Test MSE 218.15192066405834 Test RE 0.24863867978423523 Lambda1 -0.56434053\n",
      "53 Train Loss 196.96451 Test MSE 213.212871751745 Test RE 0.24580792487352895 Lambda1 -0.5699277\n",
      "54 Train Loss 192.86719 Test MSE 210.49934479706008 Test RE 0.24423873662754714 Lambda1 -0.5799651\n",
      "55 Train Loss 190.64957 Test MSE 209.27158734170916 Test RE 0.2435254221537499 Lambda1 -0.5810568\n",
      "56 Train Loss 188.80858 Test MSE 206.76322484227876 Test RE 0.2420615551912101 Lambda1 -0.5780719\n",
      "57 Train Loss 187.19269 Test MSE 206.37561395567187 Test RE 0.24183455711601357 Lambda1 -0.579382\n",
      "58 Train Loss 183.52048 Test MSE 198.8591879733018 Test RE 0.2373897709819078 Lambda1 -0.59096825\n",
      "59 Train Loss 178.62477 Test MSE 195.83590369465404 Test RE 0.23557832461139483 Lambda1 -0.5967854\n",
      "60 Train Loss 176.487 Test MSE 195.27239584030704 Test RE 0.2352391481200594 Lambda1 -0.6057722\n",
      "61 Train Loss 173.68217 Test MSE 188.75699174099066 Test RE 0.23128139292720168 Lambda1 -0.6327156\n",
      "62 Train Loss 169.23161 Test MSE 179.7598280308213 Test RE 0.22570204423572085 Lambda1 -0.6620977\n",
      "63 Train Loss 164.85872 Test MSE 175.827115308485 Test RE 0.22321948206356498 Lambda1 -0.70501673\n",
      "64 Train Loss 161.00218 Test MSE 173.11481785697575 Test RE 0.22149110622204005 Lambda1 -0.7507029\n",
      "65 Train Loss 153.78687 Test MSE 167.53368071514853 Test RE 0.21789147341543127 Lambda1 -0.82200783\n",
      "66 Train Loss 150.24707 Test MSE 163.49280995802354 Test RE 0.21524769015802747 Lambda1 -0.8520722\n",
      "67 Train Loss 147.43616 Test MSE 159.09947125468312 Test RE 0.21233595451467308 Lambda1 -0.88949835\n",
      "68 Train Loss 145.09084 Test MSE 155.50368401620997 Test RE 0.20992275230283586 Lambda1 -0.9118253\n",
      "69 Train Loss 142.8698 Test MSE 150.58675104256213 Test RE 0.20657727882527457 Lambda1 -0.91161686\n",
      "70 Train Loss 141.6204 Test MSE 147.44838428067146 Test RE 0.204413314208923 Lambda1 -0.9294398\n",
      "71 Train Loss 139.33897 Test MSE 143.19296686048995 Test RE 0.201441995232932 Lambda1 -0.9692458\n",
      "72 Train Loss 135.56776 Test MSE 141.54917553110778 Test RE 0.2002824257075217 Lambda1 -0.9821969\n",
      "73 Train Loss 132.87134 Test MSE 139.67324205553572 Test RE 0.19895084022057605 Lambda1 -1.0101506\n",
      "74 Train Loss 130.39471 Test MSE 138.17991126438534 Test RE 0.1978844304980179 Lambda1 -1.0689664\n",
      "Training time: 148.53\n",
      "Training time: 148.53\n",
      "inv_HT_atanh_tune10\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 862.6639 Test MSE 878.1033107511735 Test RE 0.4988408125080875 Lambda1 0.00029659973\n",
      "1 Train Loss 838.12396 Test MSE 858.0965257926741 Test RE 0.49312525195231244 Lambda1 0.0003652689\n",
      "2 Train Loss 838.0574 Test MSE 858.2673497483123 Test RE 0.4931743335018165 Lambda1 0.0003649193\n",
      "3 Train Loss 837.90924 Test MSE 857.9711352226092 Test RE 0.4930892213231057 Lambda1 -0.0017225918\n",
      "4 Train Loss 837.67395 Test MSE 857.2334378318228 Test RE 0.4928771927126125 Lambda1 -0.28505474\n",
      "5 Train Loss 835.46906 Test MSE 853.1504029020481 Test RE 0.4917019954124926 Lambda1 -0.6639571\n",
      "6 Train Loss 825.1506 Test MSE 841.1675475968619 Test RE 0.48823670411937664 Lambda1 -0.67049617\n",
      "7 Train Loss 812.8563 Test MSE 824.0588744476531 Test RE 0.4832460257027064 Lambda1 -1.0269625\n",
      "8 Train Loss 801.04626 Test MSE 812.8937046417639 Test RE 0.4799611118730976 Lambda1 -1.2820612\n",
      "9 Train Loss 791.7214 Test MSE 800.1637620761874 Test RE 0.47618817900189636 Lambda1 -1.3224475\n",
      "10 Train Loss 777.81775 Test MSE 785.0636863434562 Test RE 0.4716736501558084 Lambda1 -1.2399526\n",
      "11 Train Loss 767.77344 Test MSE 773.4671942782528 Test RE 0.468177048912665 Lambda1 -1.1882937\n",
      "12 Train Loss 744.5145 Test MSE 749.009959952374 Test RE 0.4607156521308848 Lambda1 -0.82210255\n",
      "13 Train Loss 736.4663 Test MSE 738.2583342809278 Test RE 0.45739704008224785 Lambda1 -0.867098\n",
      "14 Train Loss 710.7152 Test MSE 699.9959812785132 Test RE 0.4453863921968568 Lambda1 -0.8542638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Train Loss 697.46576 Test MSE 684.3784526291108 Test RE 0.44038988395386874 Lambda1 -0.9126871\n",
      "16 Train Loss 679.78613 Test MSE 673.3872725842017 Test RE 0.4368392195060332 Lambda1 -0.9787959\n",
      "17 Train Loss 664.1713 Test MSE 668.4129829861732 Test RE 0.43522277034710505 Lambda1 -1.0841706\n",
      "18 Train Loss 645.9584 Test MSE 649.9515139944151 Test RE 0.429170290884792 Lambda1 -1.2198769\n",
      "19 Train Loss 637.7567 Test MSE 640.3011023567533 Test RE 0.4259722377574239 Lambda1 -1.294973\n",
      "20 Train Loss 630.2493 Test MSE 632.3321759849156 Test RE 0.42331320016127316 Lambda1 -1.3551496\n",
      "21 Train Loss 624.45746 Test MSE 625.0383992248202 Test RE 0.4208647187011225 Lambda1 -1.4492692\n",
      "22 Train Loss 618.3575 Test MSE 624.1330405257593 Test RE 0.420559800143294 Lambda1 -1.4796087\n",
      "23 Train Loss 617.8331 Test MSE 623.6755339271284 Test RE 0.42040563096431904 Lambda1 -1.4836943\n",
      "24 Train Loss 616.2488 Test MSE 622.1870534324826 Test RE 0.4199036556896429 Lambda1 -1.5168719\n",
      "25 Train Loss 614.9787 Test MSE 621.8820801216882 Test RE 0.4198007323776428 Lambda1 -1.520852\n",
      "26 Train Loss 614.1166 Test MSE 621.8037140535826 Test RE 0.41977428108542003 Lambda1 -1.5149412\n",
      "27 Train Loss 613.573 Test MSE 621.4274936232077 Test RE 0.4196472702998927 Lambda1 -1.5133348\n",
      "28 Train Loss 612.52344 Test MSE 620.0115226619112 Test RE 0.41916889814181885 Lambda1 -1.5150899\n",
      "29 Train Loss 611.97266 Test MSE 619.3977997339983 Test RE 0.41896138808220007 Lambda1 -1.5119671\n",
      "30 Train Loss 611.30426 Test MSE 619.2890855421817 Test RE 0.4189246192662137 Lambda1 -1.5120102\n",
      "31 Train Loss 610.24225 Test MSE 618.7759418079912 Test RE 0.41875102255850816 Lambda1 -1.5249616\n",
      "32 Train Loss 609.06287 Test MSE 617.5176672305408 Test RE 0.4183250429216121 Lambda1 -1.5612912\n",
      "33 Train Loss 607.37915 Test MSE 615.0915100839719 Test RE 0.41750245825053833 Lambda1 -1.6117815\n",
      "34 Train Loss 606.191 Test MSE 613.3829046678057 Test RE 0.4169221844018621 Lambda1 -1.6300784\n",
      "35 Train Loss 605.3766 Test MSE 612.2290942416805 Test RE 0.41652987187304824 Lambda1 -1.6384304\n",
      "36 Train Loss 603.5133 Test MSE 609.1138612035473 Test RE 0.4154687964628463 Lambda1 -1.6809042\n",
      "37 Train Loss 602.13007 Test MSE 608.0686210363841 Test RE 0.4151121709108444 Lambda1 -1.6705692\n",
      "38 Train Loss 600.4373 Test MSE 606.277842154162 Test RE 0.414500461799883 Lambda1 -1.6539471\n",
      "39 Train Loss 597.71204 Test MSE 604.5936941506671 Test RE 0.413924351688862 Lambda1 -1.6466869\n",
      "40 Train Loss 596.5464 Test MSE 603.8370537402619 Test RE 0.413665260381317 Lambda1 -1.6284616\n",
      "41 Train Loss 592.96875 Test MSE 598.320944686898 Test RE 0.41177148971946426 Lambda1 -1.5976505\n",
      "42 Train Loss 589.3157 Test MSE 594.0353370900717 Test RE 0.41029413679812715 Lambda1 -1.5627764\n",
      "43 Train Loss 586.18243 Test MSE 589.4161805387571 Test RE 0.40869582153834094 Lambda1 -1.506268\n",
      "44 Train Loss 582.1835 Test MSE 584.7638757083292 Test RE 0.4070796932269324 Lambda1 -1.5258645\n",
      "45 Train Loss 570.0485 Test MSE 564.738664679036 Test RE 0.40004876307412607 Lambda1 -1.5533\n",
      "46 Train Loss 554.0669 Test MSE 543.3670111865215 Test RE 0.39240614946316754 Lambda1 -1.5546029\n",
      "47 Train Loss 539.4617 Test MSE 529.6066635504478 Test RE 0.3874055971581214 Lambda1 -1.5747427\n",
      "48 Train Loss 530.06525 Test MSE 520.3152413807161 Test RE 0.38399223696454937 Lambda1 -1.5576591\n",
      "49 Train Loss 524.1849 Test MSE 511.4451670883756 Test RE 0.38070511352554387 Lambda1 -1.5923307\n",
      "50 Train Loss 515.4409 Test MSE 496.82875381524076 Test RE 0.3752256615007599 Lambda1 -1.5989798\n",
      "51 Train Loss 500.04242 Test MSE 480.4624685091993 Test RE 0.3689936604522341 Lambda1 -1.6002145\n",
      "52 Train Loss 474.13177 Test MSE 453.6005126500437 Test RE 0.35853036148370676 Lambda1 -1.584699\n",
      "53 Train Loss 455.39954 Test MSE 430.25216694526335 Test RE 0.3491810799143016 Lambda1 -1.5285442\n",
      "54 Train Loss 434.04895 Test MSE 405.5060856323461 Test RE 0.3389907585422797 Lambda1 -1.4696697\n",
      "55 Train Loss 418.04733 Test MSE 388.91084556664384 Test RE 0.331981740787443 Lambda1 -1.4721315\n",
      "56 Train Loss 400.67307 Test MSE 380.7173257528714 Test RE 0.328466052549042 Lambda1 -1.4626455\n",
      "57 Train Loss 367.41577 Test MSE 349.75447525041864 Test RE 0.31482615890839727 Lambda1 -1.4975855\n",
      "58 Train Loss 349.36267 Test MSE 341.87489675709054 Test RE 0.3112596156887621 Lambda1 -1.5277629\n",
      "59 Train Loss 342.15723 Test MSE 335.4333665277955 Test RE 0.30831332939549555 Lambda1 -1.5729176\n",
      "60 Train Loss 336.18048 Test MSE 337.23962893674985 Test RE 0.30914232725904023 Lambda1 -1.6210574\n",
      "61 Train Loss 326.13696 Test MSE 333.02588293127417 Test RE 0.30720491860459376 Lambda1 -1.652302\n",
      "62 Train Loss 321.55096 Test MSE 328.78746059955813 Test RE 0.3052437593170254 Lambda1 -1.6778897\n",
      "63 Train Loss 317.0596 Test MSE 329.6797910129432 Test RE 0.30565769501103285 Lambda1 -1.6862158\n",
      "64 Train Loss 308.61636 Test MSE 325.3515571923863 Test RE 0.3036446373492226 Lambda1 -1.6642292\n",
      "65 Train Loss 296.79694 Test MSE 311.0500891447199 Test RE 0.29689599261784333 Lambda1 -1.648268\n",
      "66 Train Loss 288.6601 Test MSE 306.50113569794325 Test RE 0.2947170182721215 Lambda1 -1.6365525\n",
      "67 Train Loss 283.392 Test MSE 300.94256199393607 Test RE 0.29203235963254126 Lambda1 -1.6538883\n",
      "68 Train Loss 278.4356 Test MSE 299.5652963442653 Test RE 0.2913633492977542 Lambda1 -1.6906916\n",
      "69 Train Loss 276.13156 Test MSE 300.8800715275971 Test RE 0.29200203792318136 Lambda1 -1.6822938\n",
      "70 Train Loss 273.44946 Test MSE 298.9494770069788 Test RE 0.2910637159718537 Lambda1 -1.6693641\n",
      "71 Train Loss 271.3799 Test MSE 298.5762643487087 Test RE 0.2908819752496219 Lambda1 -1.672244\n",
      "72 Train Loss 268.75058 Test MSE 298.7902761275253 Test RE 0.29098620493092753 Lambda1 -1.670653\n",
      "73 Train Loss 267.44382 Test MSE 298.0505991905131 Test RE 0.2906258030452911 Lambda1 -1.6550143\n",
      "74 Train Loss 266.53265 Test MSE 297.0068323561454 Test RE 0.2901164740610021 Lambda1 -1.6479146\n",
      "Training time: 150.25\n",
      "Training time: 150.25\n",
      "inv_HT_atanh_tune11\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 26867.148 Test MSE 3574.2432990707725 Test RE 1.006424560088666 Lambda1 -5.182768e-05\n",
      "1 Train Loss 19338.97 Test MSE 3574.2747125154965 Test RE 1.0064289827300725 Lambda1 7.537466e-05\n",
      "2 Train Loss 15836.115 Test MSE 3571.0027881613837 Test RE 1.0059682300304664 Lambda1 0.0006689313\n",
      "3 Train Loss 13312.615 Test MSE 3571.3328492775454 Test RE 1.0060147188383202 Lambda1 -0.0009109791\n",
      "4 Train Loss 11076.559 Test MSE 3580.306115600432 Test RE 1.0072777731476525 Lambda1 -0.00026472582\n",
      "5 Train Loss 9452.421 Test MSE 3582.6097006228274 Test RE 1.007601764561491 Lambda1 -0.0002908273\n",
      "6 Train Loss 7878.5776 Test MSE 3582.595017400493 Test RE 1.0075996997460175 Lambda1 0.0004004904\n",
      "7 Train Loss 7005.9883 Test MSE 3582.098419899107 Test RE 1.0075298636590924 Lambda1 3.069381e-06\n",
      "8 Train Loss 6258.856 Test MSE 3580.092474981266 Test RE 1.0072477200389922 Lambda1 -0.00016857314\n",
      "9 Train Loss 5522.883 Test MSE 3583.023848438041 Test RE 1.0076600019980475 Lambda1 9.339425e-05\n",
      "10 Train Loss 5274.4062 Test MSE 3585.0375725259937 Test RE 1.007943123780064 Lambda1 0.0002519736\n",
      "11 Train Loss 5020.5854 Test MSE 3587.966578399116 Test RE 1.008354788721949 Lambda1 -0.00017863406\n",
      "12 Train Loss 4765.911 Test MSE 3590.609976035147 Test RE 1.0087261678646413 Lambda1 -0.00044663597\n",
      "13 Train Loss 4558.473 Test MSE 3591.633427531439 Test RE 1.0088699190318258 Lambda1 6.874534e-05\n",
      "14 Train Loss 4387.2886 Test MSE 3591.4021673214347 Test RE 1.0088374386534937 Lambda1 0.00020086946\n",
      "15 Train Loss 4274.026 Test MSE 3589.910867747347 Test RE 1.0086279612703613 Lambda1 -0.00027610868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 Train Loss 4149.7573 Test MSE 3586.2157709813932 Test RE 1.0081087370341684 Lambda1 0.00011331618\n",
      "17 Train Loss 4045.5369 Test MSE 3584.464252656343 Test RE 1.0078625253325 Lambda1 0.00012881802\n",
      "18 Train Loss 3982.5183 Test MSE 3583.7451189816275 Test RE 1.0077614189764763 Lambda1 -0.00019866506\n",
      "19 Train Loss 3933.8992 Test MSE 3582.4944509439156 Test RE 1.0075855575610815 Lambda1 0.00018965792\n",
      "20 Train Loss 3892.823 Test MSE 3580.8020795353964 Test RE 1.0073475375940597 Lambda1 0.00037988022\n",
      "21 Train Loss 3854.3809 Test MSE 3578.1679853382325 Test RE 1.006976959103058 Lambda1 0.00016107602\n",
      "22 Train Loss 3812.9927 Test MSE 3573.9003561551585 Test RE 1.006376276518753 Lambda1 0.00019900933\n",
      "23 Train Loss 3789.6118 Test MSE 3571.3921965072727 Test RE 1.0060230776137569 Lambda1 0.00021077946\n",
      "24 Train Loss 3768.5857 Test MSE 3568.1535646749994 Test RE 1.0055668301393528 Lambda1 0.00018454387\n",
      "25 Train Loss 3743.6838 Test MSE 3562.9866472998388 Test RE 1.0048385034409746 Lambda1 0.0002609329\n",
      "26 Train Loss 3725.9177 Test MSE 3560.3702273142517 Test RE 1.004469492462897 Lambda1 0.00016850955\n",
      "27 Train Loss 3711.393 Test MSE 3556.8333091768845 Test RE 1.003970441955628 Lambda1 0.00029238756\n",
      "28 Train Loss 3695.553 Test MSE 3552.2278184449233 Test RE 1.003320246418857 Lambda1 0.00023532382\n",
      "29 Train Loss 3682.3765 Test MSE 3548.9743710282296 Test RE 1.0028606760314829 Lambda1 5.856362e-05\n",
      "30 Train Loss 3671.5479 Test MSE 3544.318792933558 Test RE 1.0022026791163563 Lambda1 4.6554036e-05\n",
      "31 Train Loss 3658.7637 Test MSE 3538.695064094205 Test RE 1.0014072718880562 Lambda1 7.066235e-05\n",
      "32 Train Loss 3645.9863 Test MSE 3533.8822309850475 Test RE 1.0007260538346872 Lambda1 -5.567566e-05\n",
      "33 Train Loss 3634.805 Test MSE 3525.027676267998 Test RE 0.9994715495044675 Lambda1 -8.896309e-05\n",
      "34 Train Loss 3625.1755 Test MSE 3516.9750129493473 Test RE 0.9983292876019865 Lambda1 2.3498014e-06\n",
      "35 Train Loss 3612.446 Test MSE 3507.624421825614 Test RE 0.9970012739070175 Lambda1 0.00025378764\n",
      "36 Train Loss 3604.3538 Test MSE 3500.8680734231098 Test RE 0.9960406045156266 Lambda1 -6.441718e-05\n",
      "37 Train Loss 3590.9197 Test MSE 3491.817150187888 Test RE 0.9947522210111801 Lambda1 -4.8269005e-05\n",
      "38 Train Loss 3582.4224 Test MSE 3487.143865528542 Test RE 0.9940863332288573 Lambda1 0.0002369907\n",
      "39 Train Loss 3573.8762 Test MSE 3480.0273510995685 Test RE 0.9930714564351282 Lambda1 6.2830026e-05\n",
      "40 Train Loss 3561.2932 Test MSE 3471.8917711602517 Test RE 0.9919099799853062 Lambda1 0.00016119584\n",
      "41 Train Loss 3548.6426 Test MSE 3459.900156696013 Test RE 0.9901955125773969 Lambda1 0.00024647682\n",
      "42 Train Loss 3539.3872 Test MSE 3448.145477733741 Test RE 0.988512034352046 Lambda1 0.000105223684\n",
      "43 Train Loss 3529.004 Test MSE 3437.3207171117047 Test RE 0.9869591971290962 Lambda1 -5.1351148e-05\n",
      "44 Train Loss 3514.604 Test MSE 3414.007531702228 Test RE 0.9836065408467403 Lambda1 -1.8555736e-05\n",
      "45 Train Loss 3488.984 Test MSE 3374.289037628992 Test RE 0.97786817277614 Lambda1 -0.00015798137\n",
      "46 Train Loss 3462.8523 Test MSE 3345.8668888523994 Test RE 0.973741097623467 Lambda1 -6.9841e-05\n",
      "47 Train Loss 3448.5054 Test MSE 3334.9203372424513 Test RE 0.9721469158943566 Lambda1 0.0002455429\n",
      "48 Train Loss 3437.4556 Test MSE 3316.9726423235547 Test RE 0.9695274628369831 Lambda1 0.0002348015\n",
      "49 Train Loss 3421.373 Test MSE 3294.483500836501 Test RE 0.9662351649976632 Lambda1 -1.8756971e-05\n",
      "50 Train Loss 3400.2478 Test MSE 3274.164024695972 Test RE 0.9632508186730169 Lambda1 -4.6182213e-06\n",
      "51 Train Loss 3377.656 Test MSE 3265.753950521908 Test RE 0.9620129119030747 Lambda1 0.0002059342\n",
      "52 Train Loss 3365.201 Test MSE 3256.295835500556 Test RE 0.9606188347064668 Lambda1 5.252186e-05\n",
      "53 Train Loss 3348.8882 Test MSE 3239.8461960849518 Test RE 0.9581894116577827 Lambda1 -4.089165e-05\n",
      "54 Train Loss 3320.6465 Test MSE 3213.1807592191085 Test RE 0.9542380929253587 Lambda1 0.00013963832\n",
      "55 Train Loss 3304.5703 Test MSE 3204.1011390678914 Test RE 0.9528889237730411 Lambda1 0.00026096287\n",
      "56 Train Loss 3285.009 Test MSE 3178.8003289177077 Test RE 0.9491192793785642 Lambda1 0.00045183755\n",
      "57 Train Loss 3266.808 Test MSE 3154.8339239832135 Test RE 0.9455345914559774 Lambda1 0.00076009653\n",
      "58 Train Loss 3247.1948 Test MSE 3128.3907475500446 Test RE 0.9415636136259171 Lambda1 0.00093961094\n",
      "59 Train Loss 3223.7507 Test MSE 3090.7497328013606 Test RE 0.9358819920933211 Lambda1 0.0007761677\n",
      "60 Train Loss 3201.331 Test MSE 3084.8826074087933 Test RE 0.9349932845315718 Lambda1 0.00089281844\n",
      "61 Train Loss 3169.5657 Test MSE 3072.0898105033657 Test RE 0.9330525939044985 Lambda1 0.0005269197\n",
      "62 Train Loss 3132.156 Test MSE 3039.1888600511425 Test RE 0.9280428197656237 Lambda1 3.319008e-05\n",
      "63 Train Loss 3102.9485 Test MSE 3025.023459808299 Test RE 0.9258775295181484 Lambda1 2.058922e-05\n",
      "64 Train Loss 3074.2024 Test MSE 3002.3783501806374 Test RE 0.9224054929190083 Lambda1 0.00015670969\n",
      "65 Train Loss 3050.3225 Test MSE 2980.6936034377354 Test RE 0.9190684090060284 Lambda1 0.00016528938\n",
      "66 Train Loss 3029.0718 Test MSE 2959.0174385017303 Test RE 0.9157204919964583 Lambda1 -0.0001027141\n",
      "67 Train Loss 3014.7563 Test MSE 2943.339121447873 Test RE 0.9132913031537088 Lambda1 5.495861e-05\n",
      "68 Train Loss 2984.743 Test MSE 2918.2604625056315 Test RE 0.9093921400528594 Lambda1 3.1434058e-05\n",
      "69 Train Loss 2966.3796 Test MSE 2900.1713680877156 Test RE 0.9065692852001457 Lambda1 0.00017682856\n",
      "70 Train Loss 2942.9243 Test MSE 2872.716670524367 Test RE 0.9022680271619524 Lambda1 8.727292e-05\n",
      "71 Train Loss 2894.5337 Test MSE 2814.4121415507557 Test RE 0.8930648954526114 Lambda1 0.00014842983\n",
      "72 Train Loss 2854.346 Test MSE 2784.6418566037682 Test RE 0.8883290080197269 Lambda1 -4.6631743e-05\n",
      "73 Train Loss 2813.348 Test MSE 2745.949880672841 Test RE 0.8821358531758104 Lambda1 4.4143526e-05\n",
      "74 Train Loss 2770.1748 Test MSE 2683.0895327894955 Test RE 0.8719804601939716 Lambda1 3.471944e-05\n",
      "Training time: 150.82\n",
      "Training time: 150.82\n",
      "inv_HT_atanh_tune11\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 10670.212 Test MSE 3557.2386560472614 Test RE 1.0040276479936132 Lambda1 0.000980394\n",
      "1 Train Loss 6286.911 Test MSE 3543.8687111130334 Test RE 1.0021390438236943 Lambda1 -0.001423941\n",
      "2 Train Loss 4491.5215 Test MSE 3531.585331762927 Test RE 1.0004007825943495 Lambda1 -0.00011957687\n",
      "3 Train Loss 3715.5574 Test MSE 3527.918936401316 Test RE 0.9998813533709103 Lambda1 0.00035839758\n",
      "4 Train Loss 3493.612 Test MSE 3437.2182464968987 Test RE 0.9869444858062685 Lambda1 -0.0010999553\n",
      "5 Train Loss 3012.2197 Test MSE 2985.39464512683 Test RE 0.9197928841211379 Lambda1 -0.048545346\n",
      "6 Train Loss 859.04364 Test MSE 874.7617952967953 Test RE 0.4978907687776468 Lambda1 -0.80232394\n",
      "7 Train Loss 838.1039 Test MSE 858.1292860849077 Test RE 0.4931346650968067 Lambda1 -0.87997496\n",
      "8 Train Loss 838.0633 Test MSE 858.2722168064477 Test RE 0.4931757318450891 Lambda1 -0.88340807\n",
      "9 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "10 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "11 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "12 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "13 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "14 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "15 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "17 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "18 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "19 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "20 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "21 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "22 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "23 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "24 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "25 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "26 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "27 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "28 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "29 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "30 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "31 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "32 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "33 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "34 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "35 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "36 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "37 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "38 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "39 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "40 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "41 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "42 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "43 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "44 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "45 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "46 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "47 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "48 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "49 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "50 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "51 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "52 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "53 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "54 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "55 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "56 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "57 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "58 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "59 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "60 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "61 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "62 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "63 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "64 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "65 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "66 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "67 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "68 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "69 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "70 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "71 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "72 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "73 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "74 Train Loss 838.0631 Test MSE 858.2817951501339 Test RE 0.4931784837653922 Lambda1 -0.88357663\n",
      "Training time: 136.89\n",
      "Training time: 136.89\n",
      "inv_HT_atanh_tune11\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 6677.63 Test MSE 3518.521232773654 Test RE 0.9985487186335305 Lambda1 -0.00041776546\n",
      "1 Train Loss 4608.1865 Test MSE 3518.404199517949 Test RE 0.9985321116029647 Lambda1 0.00016133093\n",
      "2 Train Loss 3894.8403 Test MSE 3518.7831057308344 Test RE 0.998585877431671 Lambda1 0.00010847021\n",
      "3 Train Loss 3687.7886 Test MSE 3512.663727875087 Test RE 0.9977171988054723 Lambda1 6.50084e-05\n",
      "4 Train Loss 3608.8523 Test MSE 3502.2884755613154 Test RE 0.9962426450809737 Lambda1 2.0001964e-05\n",
      "5 Train Loss 3533.268 Test MSE 3459.276438695725 Test RE 0.9901062570247179 Lambda1 -1.6347027e-05\n",
      "6 Train Loss 3251.7983 Test MSE 3150.8031395293 Test RE 0.9449303656290415 Lambda1 0.00021940691\n",
      "7 Train Loss 3135.8643 Test MSE 3089.98641167175 Test RE 0.9357664177709017 Lambda1 -0.00024549657\n",
      "8 Train Loss 2922.3435 Test MSE 2893.2053433435944 Test RE 0.9054798701280032 Lambda1 0.0044544865\n",
      "9 Train Loss 840.3907 Test MSE 858.1320566956364 Test RE 0.49313546117911794 Lambda1 0.24288067\n",
      "10 Train Loss 838.2294 Test MSE 858.2434541485243 Test RE 0.4931674680545116 Lambda1 0.24036561\n",
      "11 Train Loss 837.99225 Test MSE 858.1653894152219 Test RE 0.4931450386015541 Lambda1 0.23938583\n",
      "12 Train Loss 837.9138 Test MSE 857.8255360867852 Test RE 0.4930473805153657 Lambda1 0.23945035\n",
      "13 Train Loss 837.88983 Test MSE 857.9200381627275 Test RE 0.49307453797096346 Lambda1 0.2388062\n",
      "14 Train Loss 837.5624 Test MSE 856.8797452174349 Test RE 0.4927755022041612 Lambda1 0.223395\n",
      "15 Train Loss 835.31885 Test MSE 853.1264916653782 Test RE 0.49169510490108687 Lambda1 0.19665092\n",
      "16 Train Loss 827.3977 Test MSE 843.455925128968 Test RE 0.4889003715790577 Lambda1 0.16722922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 Train Loss 824.4425 Test MSE 840.9535077296053 Test RE 0.488174582864061 Lambda1 0.17589639\n",
      "18 Train Loss 820.7913 Test MSE 835.3580102896165 Test RE 0.486547775556725 Lambda1 0.18489374\n",
      "19 Train Loss 817.7191 Test MSE 831.5618327674049 Test RE 0.4854409896779592 Lambda1 0.16726342\n",
      "20 Train Loss 815.25635 Test MSE 829.0489306154607 Test RE 0.48470695605914516 Lambda1 0.16382693\n",
      "21 Train Loss 813.0806 Test MSE 825.9529838136876 Test RE 0.48380108037558067 Lambda1 0.15457939\n",
      "22 Train Loss 801.84735 Test MSE 815.3194621985048 Test RE 0.48067670482605673 Lambda1 0.15007842\n",
      "23 Train Loss 790.7871 Test MSE 805.4926663593499 Test RE 0.4777711989193011 Lambda1 0.10928905\n",
      "24 Train Loss 785.4976 Test MSE 800.393635478738 Test RE 0.4762565744614043 Lambda1 0.09334672\n",
      "25 Train Loss 775.8352 Test MSE 789.9056048342724 Test RE 0.47312594941205416 Lambda1 0.08218378\n",
      "26 Train Loss 772.6403 Test MSE 785.9001668436619 Test RE 0.471924865934286 Lambda1 0.09031871\n",
      "27 Train Loss 769.12396 Test MSE 783.4909772868078 Test RE 0.4712009641053317 Lambda1 0.093110085\n",
      "28 Train Loss 767.6819 Test MSE 782.4829053314234 Test RE 0.4708977331942415 Lambda1 0.09668021\n",
      "29 Train Loss 766.04767 Test MSE 781.1687258177556 Test RE 0.4705021308077288 Lambda1 0.09906708\n",
      "30 Train Loss 762.1909 Test MSE 777.2534937470358 Test RE 0.46932156701618316 Lambda1 0.10409831\n",
      "31 Train Loss 757.62476 Test MSE 771.5706418533626 Test RE 0.46760270827365424 Lambda1 0.10392307\n",
      "32 Train Loss 752.4807 Test MSE 765.4755524790035 Test RE 0.46575211216473306 Lambda1 0.10181474\n",
      "33 Train Loss 746.5681 Test MSE 758.9851014230954 Test RE 0.46377335761592886 Lambda1 0.09427574\n",
      "34 Train Loss 739.7591 Test MSE 751.4757596051809 Test RE 0.4614733850814087 Lambda1 0.07912279\n",
      "35 Train Loss 727.5306 Test MSE 737.0587964703622 Test RE 0.4570252948066237 Lambda1 0.020914827\n",
      "36 Train Loss 691.07294 Test MSE 718.8911452667777 Test RE 0.45135757740483345 Lambda1 0.009496716\n",
      "37 Train Loss 678.1471 Test MSE 709.3190248880151 Test RE 0.44834256778364817 Lambda1 0.004600363\n",
      "38 Train Loss 671.0027 Test MSE 704.9967207610347 Test RE 0.44697447102745064 Lambda1 0.0031729243\n",
      "39 Train Loss 669.03345 Test MSE 703.0986652115706 Test RE 0.4463723730722389 Lambda1 0.0030673523\n",
      "40 Train Loss 664.84015 Test MSE 700.9128164548821 Test RE 0.44567797407720827 Lambda1 0.0014547311\n",
      "41 Train Loss 662.5628 Test MSE 700.495327840376 Test RE 0.44554522347641706 Lambda1 0.0009972417\n",
      "42 Train Loss 661.29297 Test MSE 700.5108653108994 Test RE 0.4455501646995163 Lambda1 0.0005337919\n",
      "43 Train Loss 660.61163 Test MSE 701.5220670610196 Test RE 0.44587162911537037 Lambda1 0.00028886995\n",
      "44 Train Loss 659.8173 Test MSE 702.7522429249493 Test RE 0.44626239392010986 Lambda1 0.00018572422\n",
      "45 Train Loss 659.6289 Test MSE 702.9825332577544 Test RE 0.4463355075232754 Lambda1 0.00014675203\n",
      "46 Train Loss 659.3751 Test MSE 702.8262293432614 Test RE 0.4462858847645057 Lambda1 0.00012199899\n",
      "47 Train Loss 658.974 Test MSE 703.444562014428 Test RE 0.44648215836946037 Lambda1 8.886585e-05\n",
      "48 Train Loss 658.33453 Test MSE 703.8712865415553 Test RE 0.4466175606547502 Lambda1 5.2255928e-05\n",
      "49 Train Loss 658.0736 Test MSE 703.4073020187318 Test RE 0.44647033359712157 Lambda1 4.6059937e-05\n",
      "50 Train Loss 657.83746 Test MSE 703.2171816654444 Test RE 0.4464099924308144 Lambda1 4.117205e-05\n",
      "51 Train Loss 657.5731 Test MSE 703.2665697281861 Test RE 0.44642566819798746 Lambda1 3.0453772e-05\n",
      "52 Train Loss 656.7945 Test MSE 704.123185596105 Test RE 0.4466974704903404 Lambda1 9.761821e-06\n",
      "53 Train Loss 656.39233 Test MSE 703.3958406289364 Test RE 0.4464666961658951 Lambda1 9.202796e-06\n",
      "54 Train Loss 656.312 Test MSE 703.4013209129477 Test RE 0.446468435413833 Lambda1 7.5492426e-06\n",
      "55 Train Loss 656.25256 Test MSE 703.5551048340491 Test RE 0.44651723821833916 Lambda1 6.4798573e-06\n",
      "56 Train Loss 656.0946 Test MSE 703.5904237866437 Test RE 0.44652844581469037 Lambda1 6.195614e-06\n",
      "57 Train Loss 656.05457 Test MSE 703.4490198445407 Test RE 0.4464835730782836 Lambda1 6.5570202e-06\n",
      "58 Train Loss 656.03827 Test MSE 703.4352193976365 Test RE 0.44647919344103154 Lambda1 5.925218e-06\n",
      "59 Train Loss 656.0115 Test MSE 703.4611332833779 Test RE 0.44648741730012376 Lambda1 5.392532e-06\n",
      "60 Train Loss 655.9983 Test MSE 703.408797919699 Test RE 0.446470808339882 Lambda1 5.08906e-06\n",
      "61 Train Loss 655.97034 Test MSE 703.319249341536 Test RE 0.44644238809639564 Lambda1 4.2292068e-06\n",
      "62 Train Loss 655.92834 Test MSE 703.3295776126054 Test RE 0.44644566609644853 Lambda1 4.786322e-06\n",
      "63 Train Loss 655.852 Test MSE 703.2473733008819 Test RE 0.44641957531888843 Lambda1 7.717512e-06\n",
      "64 Train Loss 655.84424 Test MSE 703.2436258264821 Test RE 0.4464183858738778 Lambda1 8.293269e-06\n",
      "65 Train Loss 655.8368 Test MSE 703.2140740344196 Test RE 0.44640900605056727 Lambda1 9.0199455e-06\n",
      "66 Train Loss 655.8295 Test MSE 703.2028058603069 Test RE 0.4464054294479777 Lambda1 8.733561e-06\n",
      "67 Train Loss 655.82715 Test MSE 703.2344690028505 Test RE 0.44641547949294386 Lambda1 8.573292e-06\n",
      "68 Train Loss 655.7696 Test MSE 703.2262841057585 Test RE 0.44641288158602027 Lambda1 7.5630765e-06\n",
      "69 Train Loss 655.5932 Test MSE 703.0527315184804 Test RE 0.446357791998849 Lambda1 8.6819955e-06\n",
      "70 Train Loss 655.31964 Test MSE 702.7265920207637 Test RE 0.44625424941507186 Lambda1 1.3456342e-05\n",
      "71 Train Loss 655.1609 Test MSE 702.4123278614213 Test RE 0.4461544542736118 Lambda1 1.8487914e-05\n",
      "72 Train Loss 655.1156 Test MSE 702.1744209276421 Test RE 0.4460788916566173 Lambda1 2.3362587e-05\n",
      "73 Train Loss 655.0326 Test MSE 701.6069459887564 Test RE 0.44589860186711255 Lambda1 3.703117e-05\n",
      "74 Train Loss 654.8553 Test MSE 700.8771586530876 Test RE 0.4456666373612754 Lambda1 5.5711917e-05\n",
      "Training time: 153.60\n",
      "Training time: 153.60\n",
      "inv_HT_atanh_tune11\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 20075.732 Test MSE 3515.970925249513 Test RE 0.9981867671531637 Lambda1 -0.0013005078\n",
      "1 Train Loss 11282.266 Test MSE 3499.122735365188 Test RE 0.9957922883462255 Lambda1 0.0009552906\n",
      "2 Train Loss 8120.538 Test MSE 3499.1444233922266 Test RE 0.995795374367815 Lambda1 2.7174618e-05\n",
      "3 Train Loss 6314.8477 Test MSE 3492.934405623046 Test RE 0.9949113506835865 Lambda1 5.6129713e-05\n",
      "4 Train Loss 5118.1494 Test MSE 3493.3095653964942 Test RE 0.994964778640118 Lambda1 0.00045212873\n",
      "5 Train Loss 4538.3994 Test MSE 3498.5039116563275 Test RE 0.9957042309714295 Lambda1 -8.5741274e-05\n",
      "6 Train Loss 4197.306 Test MSE 3499.6270549566507 Test RE 0.9958640462558802 Lambda1 0.00029702025\n",
      "7 Train Loss 3882.0388 Test MSE 3495.5143908948644 Test RE 0.995278718420753 Lambda1 0.00034094654\n",
      "8 Train Loss 3668.3438 Test MSE 3484.3117047447718 Test RE 0.9936825666623921 Lambda1 0.00013190959\n",
      "9 Train Loss 3535.8638 Test MSE 3460.824115845433 Test RE 0.990327718578977 Lambda1 -9.2129136e-05\n",
      "10 Train Loss 3467.9106 Test MSE 3414.6386164604114 Test RE 0.9836974472678174 Lambda1 0.00032321026\n",
      "11 Train Loss 1491.2617 Test MSE 1457.6088285466133 Test RE 0.6427020211709363 Lambda1 0.15660343\n",
      "12 Train Loss 1257.494 Test MSE 1250.3161498338688 Test RE 0.5952495709518203 Lambda1 0.18294783\n",
      "13 Train Loss 838.7265 Test MSE 858.2562598716619 Test RE 0.49317114728006717 Lambda1 0.22540407\n",
      "14 Train Loss 838.08746 Test MSE 858.286593927235 Test RE 0.493179862479423 Lambda1 0.22501688\n",
      "15 Train Loss 838.06494 Test MSE 858.2846753142965 Test RE 0.493179311252245 Lambda1 0.22453706\n",
      "16 Train Loss 838.0634 Test MSE 858.2849775163479 Test RE 0.49317939807646005 Lambda1 0.22417592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "18 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "19 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "20 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "21 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "22 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "23 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "24 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "25 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "26 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "27 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "28 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "29 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "30 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "31 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "32 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "33 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "34 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "35 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "36 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "37 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "38 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "39 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "40 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "41 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "42 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "43 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "44 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "45 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "46 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "47 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "48 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "49 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "50 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "51 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "52 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "53 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "54 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "55 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "56 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "57 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "58 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "59 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "60 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "61 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "62 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "63 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "64 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "65 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "66 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "67 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "68 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "69 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "70 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "71 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "72 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "73 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "74 Train Loss 838.06274 Test MSE 858.283374406139 Test RE 0.493178937494417 Lambda1 0.22375466\n",
      "Training time: 137.59\n",
      "Training time: 137.59\n",
      "inv_HT_atanh_tune11\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 9431.121 Test MSE 3587.037838648791 Test RE 1.0082242747156245 Lambda1 -7.472648e-05\n",
      "1 Train Loss 8259.652 Test MSE 3587.304798006303 Test RE 1.0082617917295293 Lambda1 -0.00090846955\n",
      "2 Train Loss 4572.5024 Test MSE 3584.5380245797664 Test RE 1.0078728966975412 Lambda1 -0.001928461\n",
      "3 Train Loss 3899.065 Test MSE 3567.5321881353757 Test RE 1.0054792690476935 Lambda1 0.0013192073\n",
      "4 Train Loss 3655.2224 Test MSE 3542.4130985803044 Test RE 1.001933212839096 Lambda1 -0.0007149206\n",
      "5 Train Loss 3528.6436 Test MSE 3475.649419105421 Test RE 0.9924466096937745 Lambda1 -0.00049432385\n",
      "6 Train Loss 3084.0215 Test MSE 3058.3255230059685 Test RE 0.9309600083050993 Lambda1 0.01993357\n",
      "7 Train Loss 844.2774 Test MSE 861.7817307142282 Test RE 0.49418301212700216 Lambda1 0.4758999\n",
      "8 Train Loss 838.03217 Test MSE 858.1134856400348 Test RE 0.49313012511484583 Lambda1 0.49403676\n",
      "9 Train Loss 837.9712 Test MSE 858.2230157803751 Test RE 0.4931615958289605 Lambda1 0.4929501\n",
      "10 Train Loss 837.7361 Test MSE 857.584806591389 Test RE 0.49297819431707235 Lambda1 0.47190592\n",
      "11 Train Loss 837.0396 Test MSE 856.5320369158633 Test RE 0.49267551179162317 Lambda1 0.4459482\n",
      "12 Train Loss 836.01404 Test MSE 853.3188696366977 Test RE 0.4917505398077423 Lambda1 0.4360731\n",
      "13 Train Loss 834.2173 Test MSE 851.4368410848319 Test RE 0.4912079528257844 Lambda1 0.42079982\n",
      "14 Train Loss 832.02325 Test MSE 847.5698954222744 Test RE 0.49009123108019564 Lambda1 0.40765077\n",
      "15 Train Loss 827.7052 Test MSE 840.8648892354362 Test RE 0.48814886061200363 Lambda1 0.3589377\n",
      "16 Train Loss 823.0021 Test MSE 837.4376189391735 Test RE 0.487153025042681 Lambda1 0.3332342\n",
      "17 Train Loss 820.1415 Test MSE 833.8744184271079 Test RE 0.4861155302846886 Lambda1 0.31425375\n",
      "18 Train Loss 815.1084 Test MSE 828.7399890483464 Test RE 0.4846166356484731 Lambda1 0.29823908\n",
      "19 Train Loss 811.97095 Test MSE 826.5728661054009 Test RE 0.48398259402122223 Lambda1 0.29227453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Train Loss 807.7412 Test MSE 821.9784846109636 Test RE 0.48263564732720654 Lambda1 0.2827696\n",
      "21 Train Loss 802.82715 Test MSE 813.0091903680172 Test RE 0.47999520408512336 Lambda1 0.28697014\n",
      "22 Train Loss 798.7581 Test MSE 809.7273861417567 Test RE 0.47902544679707065 Lambda1 0.27593014\n",
      "23 Train Loss 796.57196 Test MSE 807.625673450757 Test RE 0.4784033682938772 Lambda1 0.26995632\n",
      "24 Train Loss 793.73975 Test MSE 803.4177721274064 Test RE 0.47715544910180474 Lambda1 0.25682303\n",
      "25 Train Loss 791.4166 Test MSE 800.3111149507816 Test RE 0.47623202281877586 Lambda1 0.24582556\n",
      "26 Train Loss 785.053 Test MSE 791.5177150571616 Test RE 0.47360850226973383 Lambda1 0.24114245\n",
      "27 Train Loss 782.0727 Test MSE 789.1261769743224 Test RE 0.47289246672887614 Lambda1 0.24619266\n",
      "28 Train Loss 779.2228 Test MSE 787.0199437798777 Test RE 0.47226095345420316 Lambda1 0.25371188\n",
      "29 Train Loss 777.00214 Test MSE 783.6364716334224 Test RE 0.47124471310558563 Lambda1 0.25875103\n",
      "30 Train Loss 775.0188 Test MSE 781.8419784154522 Test RE 0.4707048389737867 Lambda1 0.25589222\n",
      "31 Train Loss 771.1444 Test MSE 777.5570067215424 Test RE 0.46941319174522017 Lambda1 0.26048228\n",
      "32 Train Loss 769.49506 Test MSE 777.390118284654 Test RE 0.4693628135497036 Lambda1 0.26219067\n",
      "33 Train Loss 765.62164 Test MSE 772.7209021277883 Test RE 0.4679511303603451 Lambda1 0.26668832\n",
      "34 Train Loss 761.4765 Test MSE 766.1949827295539 Test RE 0.46597092873210805 Lambda1 0.27396026\n",
      "35 Train Loss 759.46686 Test MSE 765.6703742828928 Test RE 0.46581137786799764 Lambda1 0.2731086\n",
      "36 Train Loss 757.754 Test MSE 763.564224169084 Test RE 0.4651702767414537 Lambda1 0.2768473\n",
      "37 Train Loss 756.52277 Test MSE 762.8440943464091 Test RE 0.46495087017581155 Lambda1 0.27705163\n",
      "38 Train Loss 754.0301 Test MSE 760.1254465803072 Test RE 0.46412162743179375 Lambda1 0.28345886\n",
      "39 Train Loss 753.1088 Test MSE 758.9287539044424 Test RE 0.46375614188737846 Lambda1 0.28435344\n",
      "40 Train Loss 750.6727 Test MSE 755.484299955523 Test RE 0.46270254940260697 Lambda1 0.28475648\n",
      "41 Train Loss 748.60095 Test MSE 753.9523498443356 Test RE 0.46223318364149635 Lambda1 0.28739488\n",
      "42 Train Loss 746.5325 Test MSE 751.8203120417805 Test RE 0.4615791659770526 Lambda1 0.2911019\n",
      "43 Train Loss 745.11584 Test MSE 747.3733480308844 Test RE 0.4602120372890109 Lambda1 0.29419327\n",
      "44 Train Loss 741.6779 Test MSE 741.5908106254068 Test RE 0.4584282160914952 Lambda1 0.29705852\n",
      "45 Train Loss 736.48816 Test MSE 737.5399666250158 Test RE 0.4571744491579313 Lambda1 0.29847467\n",
      "46 Train Loss 731.9864 Test MSE 733.8774763011861 Test RE 0.45603791355459017 Lambda1 0.30206686\n",
      "47 Train Loss 730.4451 Test MSE 732.2156341883523 Test RE 0.45552127926624547 Lambda1 0.30480424\n",
      "48 Train Loss 729.7882 Test MSE 731.4638292306754 Test RE 0.45528736517621954 Lambda1 0.30590615\n",
      "49 Train Loss 728.1221 Test MSE 731.4295650844671 Test RE 0.45527670148041044 Lambda1 0.3043427\n",
      "50 Train Loss 726.05066 Test MSE 729.7571514096917 Test RE 0.4547559085308377 Lambda1 0.3042724\n",
      "51 Train Loss 724.38086 Test MSE 726.9253991473404 Test RE 0.4538727327008193 Lambda1 0.30830196\n",
      "52 Train Loss 723.3247 Test MSE 725.7196222243618 Test RE 0.45349614907899943 Lambda1 0.3121578\n",
      "53 Train Loss 720.83124 Test MSE 724.7110872190681 Test RE 0.4531809269610942 Lambda1 0.31831884\n",
      "54 Train Loss 719.4435 Test MSE 722.7412679244214 Test RE 0.452564617944039 Lambda1 0.32083088\n",
      "55 Train Loss 718.76917 Test MSE 722.2148225808301 Test RE 0.4523997636956053 Lambda1 0.3205401\n",
      "56 Train Loss 718.25616 Test MSE 722.025971420068 Test RE 0.4523406110697402 Lambda1 0.31988388\n",
      "57 Train Loss 717.3146 Test MSE 720.2278702135511 Test RE 0.4517770155430597 Lambda1 0.32156023\n",
      "58 Train Loss 716.67365 Test MSE 719.2029338851918 Test RE 0.45145544542072397 Lambda1 0.32451463\n",
      "59 Train Loss 715.58905 Test MSE 719.1014782635082 Test RE 0.45142360162105155 Lambda1 0.32750696\n",
      "60 Train Loss 714.2665 Test MSE 718.2122034971055 Test RE 0.45114438942596935 Lambda1 0.32991734\n",
      "61 Train Loss 713.71027 Test MSE 716.8290902703554 Test RE 0.45070977938214096 Lambda1 0.33136925\n",
      "62 Train Loss 712.38153 Test MSE 714.3961996232215 Test RE 0.4499442833962893 Lambda1 0.33664295\n",
      "63 Train Loss 711.2962 Test MSE 712.8637307693048 Test RE 0.44946143103971836 Lambda1 0.34357798\n",
      "64 Train Loss 710.1089 Test MSE 711.8230900243459 Test RE 0.4491332486111563 Lambda1 0.3440003\n",
      "65 Train Loss 709.7391 Test MSE 711.5798065891962 Test RE 0.4490564907629999 Lambda1 0.34571958\n",
      "66 Train Loss 709.1326 Test MSE 710.613662692292 Test RE 0.4487515351210246 Lambda1 0.34796992\n",
      "67 Train Loss 706.8508 Test MSE 707.2300439754003 Test RE 0.4476818850589824 Lambda1 0.35181066\n",
      "68 Train Loss 705.5235 Test MSE 706.2232199820354 Test RE 0.44736310802140233 Lambda1 0.35644644\n",
      "69 Train Loss 702.98676 Test MSE 702.7906196774336 Test RE 0.44627457877458926 Lambda1 0.3636208\n",
      "70 Train Loss 702.3555 Test MSE 702.0210146639257 Test RE 0.4460301608626288 Lambda1 0.36557844\n",
      "71 Train Loss 701.8561 Test MSE 701.1621175457915 Test RE 0.4457572265352455 Lambda1 0.3673614\n",
      "72 Train Loss 701.5168 Test MSE 700.3575428515979 Test RE 0.4455014027254098 Lambda1 0.3684171\n",
      "73 Train Loss 701.3199 Test MSE 700.398334449791 Test RE 0.4455143764199141 Lambda1 0.3685247\n",
      "74 Train Loss 700.55975 Test MSE 700.3929646895515 Test RE 0.44551266859891253 Lambda1 0.3679976\n",
      "Training time: 160.18\n",
      "Training time: 160.18\n",
      "inv_HT_atanh_tune11\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 40274.824 Test MSE 3575.733548495979 Test RE 1.0066343481745217 Lambda1 0.00036764308\n",
      "1 Train Loss 29871.078 Test MSE 3577.134917109657 Test RE 1.0068315842905713 Lambda1 -0.0002682354\n",
      "2 Train Loss 19984.826 Test MSE 3578.060611201101 Test RE 1.0069618502409676 Lambda1 -2.9002607e-05\n",
      "3 Train Loss 14666.73 Test MSE 3562.399222739761 Test RE 1.004755666876403 Lambda1 -0.00018980677\n",
      "4 Train Loss 11473.76 Test MSE 3561.569449885542 Test RE 1.0046386435615557 Lambda1 0.00040643313\n",
      "5 Train Loss 9073.391 Test MSE 3559.8516251889882 Test RE 1.0043963344833995 Lambda1 7.0971764e-05\n",
      "6 Train Loss 7297.9023 Test MSE 3556.9035621570224 Test RE 1.0039803568941577 Lambda1 -0.00017481166\n",
      "7 Train Loss 5984.868 Test MSE 3558.92875153172 Test RE 1.004266133691416 Lambda1 -0.00019200568\n",
      "8 Train Loss 5340.4434 Test MSE 3558.286898613095 Test RE 1.0041755698926413 Lambda1 0.00023056375\n",
      "9 Train Loss 4914.9043 Test MSE 3555.4785140790486 Test RE 1.0037792179563716 Lambda1 -2.3115572e-05\n",
      "10 Train Loss 4594.1973 Test MSE 3556.105439360752 Test RE 1.0038677105219436 Lambda1 -0.00019568595\n",
      "11 Train Loss 4331.5205 Test MSE 3558.239415492273 Test RE 1.0041688698217446 Lambda1 -6.86322e-06\n",
      "12 Train Loss 4194.9595 Test MSE 3557.137804326453 Test RE 1.0040134152356321 Lambda1 -0.00011259265\n",
      "13 Train Loss 4079.7039 Test MSE 3557.058614080836 Test RE 1.0040022393245707 Lambda1 1.8534465e-05\n",
      "14 Train Loss 3984.7202 Test MSE 3556.8255602305217 Test RE 1.0039693483258925 Lambda1 1.6169604e-05\n",
      "15 Train Loss 3919.5562 Test MSE 3555.504813711232 Test RE 1.003782930392729 Lambda1 -3.1601812e-05\n",
      "16 Train Loss 3860.821 Test MSE 3554.6908636366948 Test RE 1.003668027446575 Lambda1 -3.1738346e-05\n",
      "17 Train Loss 3813.1436 Test MSE 3552.8653115829406 Test RE 1.003410271765067 Lambda1 0.00010117547\n",
      "18 Train Loss 3773.196 Test MSE 3550.6364545636825 Test RE 1.0030954823350429 Lambda1 0.0001751359\n",
      "19 Train Loss 3747.2625 Test MSE 3549.910751857964 Test RE 1.0029929674317977 Lambda1 0.00012624054\n",
      "20 Train Loss 3725.6013 Test MSE 3548.431554342382 Test RE 1.002783979179297 Lambda1 0.00014009452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Train Loss 3711.288 Test MSE 3547.1725331070265 Test RE 1.0026060641886545 Lambda1 3.0404364e-05\n",
      "22 Train Loss 3696.5146 Test MSE 3545.5061615345912 Test RE 1.0023705370908038 Lambda1 4.682632e-05\n",
      "23 Train Loss 3678.332 Test MSE 3542.30219465247 Test RE 1.0019175287283837 Lambda1 0.00013789716\n",
      "24 Train Loss 3659.27 Test MSE 3536.741270304533 Test RE 1.0011307838874135 Lambda1 7.523754e-05\n",
      "25 Train Loss 3641.247 Test MSE 3531.2973266219756 Test RE 1.000359989803617 Lambda1 0.00011120671\n",
      "26 Train Loss 3630.959 Test MSE 3528.993259938591 Test RE 1.0000335839529542 Lambda1 7.315356e-05\n",
      "27 Train Loss 3619.9773 Test MSE 3524.8553309874155 Test RE 0.9994471161771344 Lambda1 6.7830595e-05\n",
      "28 Train Loss 3610.2458 Test MSE 3521.013379674636 Test RE 0.99890228898786 Lambda1 0.0002104566\n",
      "29 Train Loss 3600.0288 Test MSE 3514.775609598674 Test RE 0.9980170772274305 Lambda1 0.00025151455\n",
      "30 Train Loss 3589.2585 Test MSE 3506.961652323447 Test RE 0.9969070772121436 Lambda1 0.00016585266\n",
      "31 Train Loss 3579.7798 Test MSE 3500.062030980815 Test RE 0.9959259333537575 Lambda1 0.000347878\n",
      "32 Train Loss 3567.1995 Test MSE 3489.329040537204 Test RE 0.9943977503223931 Lambda1 0.00028600253\n",
      "33 Train Loss 3555.1375 Test MSE 3474.9316482532136 Test RE 0.9923441272589459 Lambda1 0.00027252058\n",
      "34 Train Loss 3540.5576 Test MSE 3454.713953641529 Test RE 0.9894531095299628 Lambda1 0.00022724151\n",
      "35 Train Loss 3523.9739 Test MSE 3440.20088796179 Test RE 0.9873726028204522 Lambda1 0.00012240653\n",
      "36 Train Loss 3510.661 Test MSE 3430.060194672349 Test RE 0.9859162878369344 Lambda1 0.00016903674\n",
      "37 Train Loss 3495.5854 Test MSE 3415.8553089307597 Test RE 0.9838726855310087 Lambda1 0.00020474166\n",
      "38 Train Loss 3474.7834 Test MSE 3395.9272254892276 Test RE 0.980998533194024 Lambda1 0.0001879342\n",
      "39 Train Loss 3452.4038 Test MSE 3365.3372146447787 Test RE 0.9765701932333756 Lambda1 2.9296374e-05\n",
      "40 Train Loss 3437.4207 Test MSE 3348.3503984216172 Test RE 0.9741024160543171 Lambda1 -8.207661e-05\n",
      "41 Train Loss 3420.6467 Test MSE 3325.190026472375 Test RE 0.9707276613715626 Lambda1 -4.5121742e-05\n",
      "42 Train Loss 3400.0295 Test MSE 3302.299235895751 Test RE 0.9673806198798023 Lambda1 -9.739355e-06\n",
      "43 Train Loss 3380.0107 Test MSE 3287.3544641372173 Test RE 0.9651891654573039 Lambda1 0.0001420551\n",
      "44 Train Loss 3354.7998 Test MSE 3263.2676816162043 Test RE 0.9616466445369057 Lambda1 0.00017661574\n",
      "45 Train Loss 3334.6448 Test MSE 3240.8958728813855 Test RE 0.9583446208371692 Lambda1 0.00021707198\n",
      "46 Train Loss 3314.2551 Test MSE 3219.10302795299 Test RE 0.9551170744177792 Lambda1 -7.9618796e-05\n",
      "47 Train Loss 3296.4468 Test MSE 3205.4374891103134 Test RE 0.9530876160663017 Lambda1 0.00024337594\n",
      "48 Train Loss 3267.7302 Test MSE 3158.961279088211 Test RE 0.9461528936483748 Lambda1 -0.00030564624\n",
      "49 Train Loss 3215.0972 Test MSE 3104.604290375991 Test RE 0.937977233373077 Lambda1 0.000105853745\n",
      "50 Train Loss 3168.3762 Test MSE 3079.952776762524 Test RE 0.9342458976587273 Lambda1 -5.8184043e-05\n",
      "51 Train Loss 3135.5825 Test MSE 3057.9832797629665 Test RE 0.9309079171061522 Lambda1 5.196708e-05\n",
      "52 Train Loss 3096.239 Test MSE 3013.3076348667005 Test RE 0.9240828422177711 Lambda1 1.1715923e-05\n",
      "53 Train Loss 3053.5332 Test MSE 2974.919243376856 Test RE 0.918177743043639 Lambda1 3.380113e-05\n",
      "54 Train Loss 3020.375 Test MSE 2960.409987611567 Test RE 0.9159359411765242 Lambda1 -7.591163e-05\n",
      "55 Train Loss 2983.796 Test MSE 2925.8301763274535 Test RE 0.9105708182588055 Lambda1 -0.00013517216\n",
      "56 Train Loss 2945.3345 Test MSE 2880.580904013392 Test RE 0.9035021894019579 Lambda1 3.7912487e-06\n",
      "57 Train Loss 2833.7327 Test MSE 2765.5945489838014 Test RE 0.8852856526525354 Lambda1 -1.8234801e-05\n",
      "58 Train Loss 2640.59 Test MSE 2429.0499848483955 Test RE 0.8296738394697328 Lambda1 0.00076727057\n",
      "59 Train Loss 2508.1487 Test MSE 2413.9017402274967 Test RE 0.8270827527784846 Lambda1 -0.00030382953\n",
      "60 Train Loss 2389.4788 Test MSE 2342.241293985409 Test RE 0.8147136399119692 Lambda1 0.00044712817\n",
      "61 Train Loss 1834.3254 Test MSE 1781.9103209927355 Test RE 0.7106112722115798 Lambda1 0.0047929585\n",
      "62 Train Loss 842.3738 Test MSE 859.0881727806334 Test RE 0.49341010625449 Lambda1 0.017799241\n",
      "63 Train Loss 837.92804 Test MSE 857.7485001982245 Test RE 0.4930252412841121 Lambda1 0.017557565\n",
      "64 Train Loss 835.1758 Test MSE 854.0229704868417 Test RE 0.4919533775692517 Lambda1 0.017707776\n",
      "65 Train Loss 815.73755 Test MSE 824.1376983445491 Test RE 0.48326913717279685 Lambda1 0.017606914\n",
      "66 Train Loss 791.42096 Test MSE 807.5073945781222 Test RE 0.4783683353046882 Lambda1 0.017937647\n",
      "67 Train Loss 774.31793 Test MSE 792.5906604228234 Test RE 0.47392939461762834 Lambda1 0.016217798\n",
      "68 Train Loss 756.2963 Test MSE 773.8222829670881 Test RE 0.4682845035661359 Lambda1 0.012343969\n",
      "69 Train Loss 728.3423 Test MSE 741.9394084794158 Test RE 0.45853594957095156 Lambda1 0.0064436626\n",
      "70 Train Loss 695.4744 Test MSE 697.3425566700114 Test RE 0.44454144357353276 Lambda1 6.9606576e-05\n",
      "71 Train Loss 632.5394 Test MSE 640.2015622077818 Test RE 0.425939126001184 Lambda1 -0.0006798331\n",
      "72 Train Loss 573.5485 Test MSE 594.8043094731005 Test RE 0.4105596115957029 Lambda1 -0.0003556155\n",
      "73 Train Loss 548.8531 Test MSE 568.1009479713995 Test RE 0.4012378804907757 Lambda1 -4.695519e-05\n",
      "74 Train Loss 534.9553 Test MSE 561.1421257782213 Test RE 0.39877287292557767 Lambda1 -1.218365e-05\n",
      "Training time: 151.59\n",
      "Training time: 151.59\n",
      "inv_HT_atanh_tune11\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 34499.83 Test MSE 3521.943356170758 Test RE 0.9990341962286527 Lambda1 -0.000810657\n",
      "1 Train Loss 25061.496 Test MSE 3511.6904571433292 Test RE 0.997578968064356 Lambda1 0.00079803664\n",
      "2 Train Loss 16430.293 Test MSE 3511.6321972903265 Test RE 0.9975706929834794 Lambda1 0.00024404642\n",
      "3 Train Loss 11767.459 Test MSE 3512.4969465373792 Test RE 0.9976935127089395 Lambda1 -0.00018513612\n",
      "4 Train Loss 9157.028 Test MSE 3514.8066820652866 Test RE 0.9980214887158465 Lambda1 -0.00030539779\n",
      "5 Train Loss 7647.44 Test MSE 3514.829073945908 Test RE 0.9980246677729699 Lambda1 0.00013484857\n",
      "6 Train Loss 6434.911 Test MSE 3510.989503079353 Test RE 0.9974794017817104 Lambda1 -0.00032596927\n",
      "7 Train Loss 5514.037 Test MSE 3505.9743245796517 Test RE 0.9967667358854878 Lambda1 0.00012825595\n",
      "8 Train Loss 4957.0 Test MSE 3500.5900479977013 Test RE 0.9960010528808213 Lambda1 3.818822e-05\n",
      "9 Train Loss 4470.038 Test MSE 3495.1265587349394 Test RE 0.9952235031134535 Lambda1 -0.00023566106\n",
      "10 Train Loss 4195.9746 Test MSE 3490.1517238430292 Test RE 0.9945149685905932 Lambda1 7.483364e-05\n",
      "11 Train Loss 4003.7605 Test MSE 3489.029794741487 Test RE 0.9943551094990745 Lambda1 0.00038517502\n",
      "12 Train Loss 3878.407 Test MSE 3483.5442263744003 Test RE 0.9935731229699643 Lambda1 5.204697e-05\n",
      "13 Train Loss 3763.7058 Test MSE 3478.3626767060364 Test RE 0.9928339096861639 Lambda1 -5.420826e-05\n",
      "14 Train Loss 3695.4707 Test MSE 3471.549533818598 Test RE 0.9918610906443978 Lambda1 0.00012308087\n",
      "15 Train Loss 3644.672 Test MSE 3462.415491317468 Test RE 0.9905553814217222 Lambda1 -7.422572e-05\n",
      "16 Train Loss 3592.719 Test MSE 3447.6551740123855 Test RE 0.9884417518824303 Lambda1 2.0387895e-05\n",
      "17 Train Loss 3546.641 Test MSE 3426.69600251789 Test RE 0.9854326774436888 Lambda1 2.960687e-05\n",
      "18 Train Loss 3505.7754 Test MSE 3404.056969843178 Test RE 0.9821720712775112 Lambda1 9.228695e-05\n",
      "19 Train Loss 3463.3623 Test MSE 3367.7584166305446 Test RE 0.9769214282525285 Lambda1 7.137634e-05\n",
      "20 Train Loss 3415.4165 Test MSE 3322.5582600774983 Test RE 0.9703434375962111 Lambda1 4.817189e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Train Loss 3352.3635 Test MSE 3257.670446933689 Test RE 0.9608215709400124 Lambda1 -7.4683834e-05\n",
      "22 Train Loss 3285.4792 Test MSE 3197.8207036957842 Test RE 0.9519545754895645 Lambda1 6.5439686e-05\n",
      "23 Train Loss 3221.8965 Test MSE 3144.99411054342 Test RE 0.9440588957204002 Lambda1 -1.683578e-05\n",
      "24 Train Loss 3114.8364 Test MSE 3047.2513459644633 Test RE 0.9292729796399951 Lambda1 -4.465272e-05\n",
      "25 Train Loss 3033.2197 Test MSE 2976.9255351792267 Test RE 0.9184873013624516 Lambda1 8.503343e-05\n",
      "26 Train Loss 2964.0786 Test MSE 2909.6053952437496 Test RE 0.9080425869862655 Lambda1 1.3044977e-05\n",
      "27 Train Loss 2896.9607 Test MSE 2831.1585981688527 Test RE 0.8957179345639927 Lambda1 1.7792594e-05\n",
      "28 Train Loss 2822.3882 Test MSE 2762.1736902045036 Test RE 0.8847379631381266 Lambda1 0.00011158461\n",
      "29 Train Loss 2592.9473 Test MSE 2513.411251916869 Test RE 0.8439582212305049 Lambda1 -0.0006932905\n",
      "30 Train Loss 2066.5054 Test MSE 2029.3755226694732 Test RE 0.7583512021335721 Lambda1 0.006738906\n",
      "31 Train Loss 838.10095 Test MSE 858.0597186546324 Test RE 0.49311467579723195 Lambda1 0.04054061\n",
      "32 Train Loss 817.9711 Test MSE 817.2374392393351 Test RE 0.48124175041414163 Lambda1 0.03671697\n",
      "33 Train Loss 769.9797 Test MSE 767.1069434997016 Test RE 0.4662481563594358 Lambda1 0.031050742\n",
      "34 Train Loss 713.75555 Test MSE 702.4024658803847 Test RE 0.4461513222227941 Lambda1 0.032068074\n",
      "35 Train Loss 649.4388 Test MSE 640.044241597422 Test RE 0.4258867884840858 Lambda1 0.01925533\n",
      "36 Train Loss 622.78503 Test MSE 618.502665174561 Test RE 0.41865854360164995 Lambda1 0.014991807\n",
      "37 Train Loss 576.11755 Test MSE 573.0039464538154 Test RE 0.402965603630129 Lambda1 0.0046339165\n",
      "38 Train Loss 526.3542 Test MSE 528.9376678405554 Test RE 0.3871608357168683 Lambda1 0.00057616696\n",
      "39 Train Loss 491.30182 Test MSE 503.70674446457747 Test RE 0.37781400593789977 Lambda1 0.00065491133\n",
      "40 Train Loss 471.15253 Test MSE 491.35087909720085 Test RE 0.3731513690477793 Lambda1 0.0006582672\n",
      "41 Train Loss 455.86395 Test MSE 485.7275638493139 Test RE 0.37100994001579846 Lambda1 0.00022151254\n",
      "42 Train Loss 451.53897 Test MSE 482.4960436733243 Test RE 0.3697737255662885 Lambda1 0.0001389209\n",
      "43 Train Loss 445.805 Test MSE 477.980224118713 Test RE 0.36803924821401107 Lambda1 6.994551e-05\n",
      "44 Train Loss 440.59003 Test MSE 477.5088713200639 Test RE 0.36785773535034816 Lambda1 3.848095e-05\n",
      "45 Train Loss 438.98502 Test MSE 476.36101986203334 Test RE 0.3674153350925665 Lambda1 3.6030557e-05\n",
      "46 Train Loss 436.8804 Test MSE 475.2797735046938 Test RE 0.3669981177283808 Lambda1 2.8378669e-05\n",
      "47 Train Loss 433.33813 Test MSE 472.5836590856796 Test RE 0.36595570418184603 Lambda1 4.089743e-05\n",
      "48 Train Loss 431.5602 Test MSE 471.34061234532624 Test RE 0.365474096784096 Lambda1 4.986739e-05\n",
      "49 Train Loss 430.5295 Test MSE 470.7908209652842 Test RE 0.3652608824635223 Lambda1 5.0056493e-05\n",
      "50 Train Loss 428.04633 Test MSE 469.7815036492903 Test RE 0.36486913529920656 Lambda1 3.4081393e-05\n",
      "51 Train Loss 427.11456 Test MSE 469.367456646931 Test RE 0.3647083091782585 Lambda1 2.7743865e-05\n",
      "52 Train Loss 426.23425 Test MSE 468.7866082109567 Test RE 0.3644825736406386 Lambda1 2.7622584e-05\n",
      "53 Train Loss 424.7212 Test MSE 466.86307971509274 Test RE 0.36373403127012643 Lambda1 2.8171282e-05\n",
      "54 Train Loss 423.69357 Test MSE 465.960013333547 Test RE 0.363382070504072 Lambda1 3.1976993e-05\n",
      "55 Train Loss 423.40918 Test MSE 465.61677615204314 Test RE 0.3632482079282522 Lambda1 3.6097375e-05\n",
      "56 Train Loss 423.08942 Test MSE 464.7813543959788 Test RE 0.36292218692238903 Lambda1 4.8807633e-05\n",
      "57 Train Loss 422.7562 Test MSE 464.4455387634147 Test RE 0.36279105325602506 Lambda1 4.7932408e-05\n",
      "58 Train Loss 422.14093 Test MSE 465.62142902579495 Test RE 0.36325002287985647 Lambda1 3.300639e-05\n",
      "59 Train Loss 421.5947 Test MSE 465.97222397508983 Test RE 0.3633868317486105 Lambda1 3.070479e-05\n",
      "60 Train Loss 420.3888 Test MSE 465.43484033242527 Test RE 0.3631772329109212 Lambda1 3.230286e-05\n",
      "61 Train Loss 419.34525 Test MSE 464.5424898517247 Test RE 0.3628289168472801 Lambda1 2.8989489e-05\n",
      "62 Train Loss 419.10107 Test MSE 464.57843859678724 Test RE 0.36284295538217076 Lambda1 2.8675637e-05\n",
      "63 Train Loss 418.77603 Test MSE 464.3631377108373 Test RE 0.3627588689758167 Lambda1 2.708245e-05\n",
      "64 Train Loss 416.7934 Test MSE 463.24447077809765 Test RE 0.3623216561422235 Lambda1 2.1757853e-05\n",
      "65 Train Loss 415.71478 Test MSE 461.7203909635544 Test RE 0.36172514382740034 Lambda1 2.0616413e-05\n",
      "66 Train Loss 414.4299 Test MSE 459.52791169398466 Test RE 0.36086529594582617 Lambda1 2.3755743e-05\n",
      "67 Train Loss 412.70343 Test MSE 457.16017221930923 Test RE 0.3599344074639259 Lambda1 2.102089e-05\n",
      "68 Train Loss 411.7061 Test MSE 456.7716122708122 Test RE 0.3597814131378895 Lambda1 1.9918383e-05\n",
      "69 Train Loss 411.319 Test MSE 456.4579349967408 Test RE 0.35965785616572055 Lambda1 2.0170233e-05\n",
      "70 Train Loss 410.95908 Test MSE 455.91983125182753 Test RE 0.3594457990454059 Lambda1 1.7167586e-05\n",
      "71 Train Loss 410.5236 Test MSE 454.06543366659133 Test RE 0.3587140535268892 Lambda1 1.5681819e-05\n",
      "72 Train Loss 410.15732 Test MSE 453.3937296323179 Test RE 0.3584486304899357 Lambda1 1.3779123e-05\n",
      "73 Train Loss 409.63446 Test MSE 452.1957698777412 Test RE 0.3579747696641669 Lambda1 1.203953e-05\n",
      "74 Train Loss 408.41876 Test MSE 447.8856061525772 Test RE 0.35626464292581894 Lambda1 1.4994402e-05\n",
      "Training time: 153.23\n",
      "Training time: 153.23\n",
      "inv_HT_atanh_tune11\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 14845.731 Test MSE 3606.965348900257 Test RE 1.0110209517596551 Lambda1 -0.00037535088\n",
      "1 Train Loss 10434.112 Test MSE 3602.694108350248 Test RE 1.0104221668498734 Lambda1 0.0010007409\n",
      "2 Train Loss 6853.53 Test MSE 3605.7254477990336 Test RE 1.0108471666497891 Lambda1 -0.0012664468\n",
      "3 Train Loss 5200.673 Test MSE 3601.3559243119003 Test RE 1.010234493908856 Lambda1 0.00040326157\n",
      "4 Train Loss 4312.8945 Test MSE 3594.6582834668575 Test RE 1.0092946622532428 Lambda1 -0.00068914023\n",
      "5 Train Loss 3977.6865 Test MSE 3591.656107427572 Test RE 1.0088731043553443 Lambda1 0.0006094535\n",
      "6 Train Loss 3788.114 Test MSE 3586.7542809579427 Test RE 1.0081844235333643 Lambda1 -0.00025379792\n",
      "7 Train Loss 3668.3008 Test MSE 3570.3150771542164 Test RE 1.0058713596586537 Lambda1 0.0001511254\n",
      "8 Train Loss 3598.771 Test MSE 3536.3286152415767 Test RE 1.001072377899218 Lambda1 0.0001492813\n",
      "9 Train Loss 3050.047 Test MSE 3000.7283645627417 Test RE 0.9221519997248717 Lambda1 -0.011624151\n",
      "10 Train Loss 951.94196 Test MSE 918.3020365657153 Test RE 0.5101312684283079 Lambda1 0.031953327\n",
      "11 Train Loss 839.7916 Test MSE 858.1676240053471 Test RE 0.4931456806551314 Lambda1 0.04415856\n",
      "12 Train Loss 838.2978 Test MSE 858.3064611947786 Test RE 0.49318557040941047 Lambda1 0.045072056\n",
      "13 Train Loss 838.127 Test MSE 858.3143667634489 Test RE 0.4931878416863917 Lambda1 0.04453474\n",
      "14 Train Loss 838.0735 Test MSE 858.2645888432381 Test RE 0.49317354027077076 Lambda1 0.04373207\n",
      "15 Train Loss 838.0478 Test MSE 858.2521387274401 Test RE 0.4931699632328549 Lambda1 0.041873295\n",
      "16 Train Loss 838.0347 Test MSE 858.188104017367 Test RE 0.49315156503536733 Lambda1 0.03756985\n",
      "17 Train Loss 837.9996 Test MSE 858.1292401892048 Test RE 0.49313465190953837 Lambda1 0.030488808\n",
      "18 Train Loss 837.97284 Test MSE 858.1470989696845 Test RE 0.49313978326811664 Lambda1 0.029542783\n",
      "19 Train Loss 837.9429 Test MSE 858.016426184432 Test RE 0.49310223585789886 Lambda1 0.028462265\n",
      "20 Train Loss 837.9341 Test MSE 857.9771403674586 Test RE 0.4930909469446883 Lambda1 0.024886062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Train Loss 837.88684 Test MSE 857.8483976471226 Test RE 0.49305395047444567 Lambda1 0.0057401834\n",
      "22 Train Loss 837.80084 Test MSE 857.6872396781184 Test RE 0.4930076350032535 Lambda1 -0.018624872\n",
      "23 Train Loss 837.7108 Test MSE 857.5523402942698 Test RE 0.4929688626871916 Lambda1 -0.05032817\n",
      "24 Train Loss 837.4716 Test MSE 857.0675968955184 Test RE 0.4928295142359991 Lambda1 -0.067193836\n",
      "25 Train Loss 836.8211 Test MSE 855.9506053924858 Test RE 0.4925082642802952 Lambda1 -0.08286217\n",
      "26 Train Loss 835.50433 Test MSE 853.9399645440733 Test RE 0.49192946952017474 Lambda1 -0.058495883\n",
      "27 Train Loss 831.8337 Test MSE 847.9120302401722 Test RE 0.49019013758751234 Lambda1 -0.04850092\n",
      "28 Train Loss 824.7843 Test MSE 837.0536353125475 Test RE 0.48704132703162367 Lambda1 -0.046266288\n",
      "29 Train Loss 821.027 Test MSE 831.4339748492708 Test RE 0.48540366842422183 Lambda1 -0.037584648\n",
      "30 Train Loss 813.1395 Test MSE 819.4951654471131 Test RE 0.4819060388366781 Lambda1 -0.020785576\n",
      "31 Train Loss 802.31415 Test MSE 802.5935796635006 Test RE 0.4769106394585988 Lambda1 -0.023470607\n",
      "32 Train Loss 787.81384 Test MSE 789.4753336710097 Test RE 0.47299707313808803 Lambda1 -0.019549085\n",
      "33 Train Loss 740.7122 Test MSE 737.2314286376323 Test RE 0.4570788133646246 Lambda1 -0.0010748005\n",
      "34 Train Loss 727.8909 Test MSE 725.9541151184598 Test RE 0.4535694094900278 Lambda1 -1.1127879e-05\n",
      "35 Train Loss 717.60815 Test MSE 715.7962704551366 Test RE 0.4503849670983456 Lambda1 -0.00014043851\n",
      "36 Train Loss 707.85956 Test MSE 707.2037948800884 Test RE 0.44767357704547933 Lambda1 -0.0001758558\n",
      "37 Train Loss 702.7944 Test MSE 704.2717860594795 Test RE 0.4467446042522988 Lambda1 -2.7042965e-05\n",
      "38 Train Loss 697.1773 Test MSE 702.2664735673077 Test RE 0.4461081303988335 Lambda1 0.00026516378\n",
      "39 Train Loss 690.48486 Test MSE 697.2313113549253 Test RE 0.4445059838665867 Lambda1 0.00025574362\n",
      "40 Train Loss 688.2567 Test MSE 693.7934692650454 Test RE 0.4434087656854319 Lambda1 0.00025216612\n",
      "41 Train Loss 683.32153 Test MSE 688.5018993542288 Test RE 0.44171458752033516 Lambda1 9.581601e-05\n",
      "42 Train Loss 677.4334 Test MSE 684.6543488277076 Test RE 0.4404786430691647 Lambda1 6.339372e-05\n",
      "43 Train Loss 675.0314 Test MSE 682.6892968896332 Test RE 0.43984607177457463 Lambda1 2.542671e-05\n",
      "44 Train Loss 672.522 Test MSE 679.9012168345344 Test RE 0.4389469946267184 Lambda1 2.099796e-06\n",
      "45 Train Loss 670.52783 Test MSE 678.7944508581759 Test RE 0.438589582804808 Lambda1 -3.4056084e-05\n",
      "46 Train Loss 669.1138 Test MSE 678.0129410628058 Test RE 0.4383370315584526 Lambda1 -2.8754948e-05\n",
      "47 Train Loss 668.0134 Test MSE 675.9845137375745 Test RE 0.4376808487877288 Lambda1 -2.5789073e-05\n",
      "48 Train Loss 666.4563 Test MSE 674.0582062899346 Test RE 0.43705678909716417 Lambda1 -3.0128771e-05\n",
      "49 Train Loss 665.1002 Test MSE 671.6422100483363 Test RE 0.4362728245761845 Lambda1 -5.7979382e-06\n",
      "50 Train Loss 663.91614 Test MSE 669.5839951774823 Test RE 0.43560384324672724 Lambda1 3.2809225e-05\n",
      "51 Train Loss 662.9033 Test MSE 669.2965487183532 Test RE 0.43551033278030693 Lambda1 3.258109e-05\n",
      "52 Train Loss 660.97437 Test MSE 669.0050554514573 Test RE 0.4354154853206006 Lambda1 2.873601e-05\n",
      "53 Train Loss 659.8707 Test MSE 666.3047678510052 Test RE 0.43453586876456063 Lambda1 3.5945824e-05\n",
      "54 Train Loss 658.905 Test MSE 664.9649385664104 Test RE 0.4340987588383096 Lambda1 -1.924858e-05\n",
      "55 Train Loss 657.9247 Test MSE 664.0336048396005 Test RE 0.4337946582379565 Lambda1 -5.1136445e-05\n",
      "56 Train Loss 657.5057 Test MSE 664.255671778044 Test RE 0.43386718724740087 Lambda1 -1.4428795e-05\n",
      "57 Train Loss 657.01117 Test MSE 663.7962750117371 Test RE 0.43371713082712743 Lambda1 -2.523365e-05\n",
      "58 Train Loss 656.146 Test MSE 661.090365732816 Test RE 0.4328322225229095 Lambda1 -7.766354e-05\n",
      "59 Train Loss 655.1373 Test MSE 659.2833875948032 Test RE 0.4322402811010903 Lambda1 -0.00018241737\n",
      "60 Train Loss 654.1739 Test MSE 658.8151542150391 Test RE 0.43208676193218465 Lambda1 -0.00034601416\n",
      "61 Train Loss 653.21735 Test MSE 658.0621961925647 Test RE 0.4318397759579412 Lambda1 -0.00020480398\n",
      "62 Train Loss 652.295 Test MSE 657.2371773798028 Test RE 0.4315689902495308 Lambda1 -0.0002545479\n",
      "63 Train Loss 651.40515 Test MSE 655.8016137913766 Test RE 0.43109740751793113 Lambda1 -0.00042290826\n",
      "64 Train Loss 650.027 Test MSE 654.4248167183176 Test RE 0.43064464445546374 Lambda1 -0.00021837608\n",
      "65 Train Loss 649.2148 Test MSE 653.8772195925574 Test RE 0.4304644334401222 Lambda1 -0.0004309284\n",
      "66 Train Loss 647.39136 Test MSE 652.6500550893437 Test RE 0.4300603065350048 Lambda1 -0.0008058959\n",
      "67 Train Loss 646.4551 Test MSE 652.294055713469 Test RE 0.429942998581724 Lambda1 -0.00017194603\n",
      "68 Train Loss 645.59344 Test MSE 651.7747304597758 Test RE 0.4297718145016188 Lambda1 -1.8367034e-05\n",
      "69 Train Loss 644.4722 Test MSE 650.8104093859224 Test RE 0.42945376640516664 Lambda1 0.00014081794\n",
      "70 Train Loss 643.6782 Test MSE 650.4576838509198 Test RE 0.4293373731840827 Lambda1 0.00020549331\n",
      "71 Train Loss 643.20447 Test MSE 650.1326515738876 Test RE 0.42923009030860637 Lambda1 0.00027626482\n",
      "72 Train Loss 642.81696 Test MSE 649.9503761246268 Test RE 0.4291699152105215 Lambda1 0.00033439352\n",
      "73 Train Loss 642.36426 Test MSE 650.1851887078701 Test RE 0.42924743297179324 Lambda1 0.0002901307\n",
      "74 Train Loss 641.55695 Test MSE 649.7756414330354 Test RE 0.42911222164159507 Lambda1 0.00019291874\n",
      "Training time: 154.10\n",
      "Training time: 154.10\n",
      "inv_HT_atanh_tune11\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 29859.832 Test MSE 3391.9908576156085 Test RE 0.9804298091708488 Lambda1 0.00032535495\n",
      "1 Train Loss 20411.521 Test MSE 3385.1169986587965 Test RE 0.9794358863857857 Lambda1 0.00014193496\n",
      "2 Train Loss 13384.272 Test MSE 3365.128180431 Test RE 0.9765398634715197 Lambda1 -0.00065861945\n",
      "3 Train Loss 8324.266 Test MSE 3354.9559360008193 Test RE 0.9750627844503953 Lambda1 0.00073471206\n",
      "4 Train Loss 5213.3784 Test MSE 3349.1354812445697 Test RE 0.9742166075458556 Lambda1 -0.000281135\n",
      "5 Train Loss 4118.713 Test MSE 3355.2082116379397 Test RE 0.9750994436444126 Lambda1 -0.0003182374\n",
      "6 Train Loss 3653.4766 Test MSE 3347.1265503948985 Test RE 0.9739243787922216 Lambda1 -8.511683e-06\n",
      "7 Train Loss 3462.1025 Test MSE 3337.59914327442 Test RE 0.972537280582007 Lambda1 -0.0001804644\n",
      "8 Train Loss 3375.88 Test MSE 3306.7025767364416 Test RE 0.9680253657436406 Lambda1 -3.8305767e-05\n",
      "9 Train Loss 3279.7837 Test MSE 3213.8958435811755 Test RE 0.954344268523454 Lambda1 0.000719995\n",
      "10 Train Loss 1153.4528 Test MSE 1064.6745864881448 Test RE 0.5492848420389387 Lambda1 0.06239241\n",
      "11 Train Loss 838.152 Test MSE 857.9847147817508 Test RE 0.49309312349852485 Lambda1 0.0843957\n",
      "12 Train Loss 838.02106 Test MSE 858.1967219888278 Test RE 0.49315404115652517 Lambda1 0.084392875\n",
      "13 Train Loss 837.8553 Test MSE 857.8704578712642 Test RE 0.49306029006239904 Lambda1 0.056932602\n",
      "14 Train Loss 836.8505 Test MSE 856.0983757822471 Test RE 0.49255077549350135 Lambda1 5.1437877e-05\n",
      "15 Train Loss 834.5794 Test MSE 852.1452602724804 Test RE 0.49141225969255 Lambda1 -0.026228087\n",
      "16 Train Loss 820.10236 Test MSE 831.3519176880033 Test RE 0.4853797147302938 Lambda1 -0.00817238\n",
      "17 Train Loss 803.62 Test MSE 811.0942780626283 Test RE 0.47942959511121797 Lambda1 0.0002786123\n",
      "18 Train Loss 777.311 Test MSE 791.2186219621723 Test RE 0.47351901190565115 Lambda1 -0.00060032634\n",
      "19 Train Loss 759.51715 Test MSE 772.4293234859698 Test RE 0.46786283364895515 Lambda1 4.1456053e-05\n",
      "20 Train Loss 752.99554 Test MSE 767.5297272646008 Test RE 0.4663766228005044 Lambda1 -6.1091625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Train Loss 749.5996 Test MSE 765.1258346321821 Test RE 0.4656457074381337 Lambda1 -7.953059e-05\n",
      "22 Train Loss 745.44257 Test MSE 762.2987943958301 Test RE 0.46478466123916795 Lambda1 1.9575245e-06\n",
      "23 Train Loss 741.9831 Test MSE 761.1388815344933 Test RE 0.4644309187541884 Lambda1 -2.1314694e-05\n",
      "24 Train Loss 739.721 Test MSE 758.7376420224117 Test RE 0.4636977471507588 Lambda1 3.827192e-05\n",
      "25 Train Loss 739.06067 Test MSE 758.577962013778 Test RE 0.46364895086580593 Lambda1 4.594062e-05\n",
      "26 Train Loss 738.1772 Test MSE 758.1891970302074 Test RE 0.46353012749765166 Lambda1 2.3812294e-05\n",
      "27 Train Loss 737.4674 Test MSE 757.8106561899086 Test RE 0.4634143997932606 Lambda1 2.3012613e-05\n",
      "28 Train Loss 736.64996 Test MSE 757.4769395675119 Test RE 0.4633123517972097 Lambda1 1.1813717e-05\n",
      "29 Train Loss 735.86664 Test MSE 756.1557939889587 Test RE 0.46290813474161674 Lambda1 3.803371e-06\n",
      "30 Train Loss 735.1356 Test MSE 756.0950082189652 Test RE 0.4628895282631992 Lambda1 -2.786324e-06\n",
      "31 Train Loss 734.4306 Test MSE 755.4677612782435 Test RE 0.46269748475068556 Lambda1 -1.9553233e-06\n",
      "32 Train Loss 733.91156 Test MSE 754.3798206807029 Test RE 0.4623642020051581 Lambda1 9.248863e-06\n",
      "33 Train Loss 733.65 Test MSE 753.9691008130432 Test RE 0.4622383184557787 Lambda1 2.4455134e-05\n",
      "34 Train Loss 733.44617 Test MSE 753.5386280605723 Test RE 0.46210634394196876 Lambda1 4.1596297e-05\n",
      "35 Train Loss 732.4852 Test MSE 752.1035334077351 Test RE 0.46166609949709364 Lambda1 8.5809676e-05\n",
      "36 Train Loss 731.7542 Test MSE 751.8858565762645 Test RE 0.46159928603214523 Lambda1 9.230906e-05\n",
      "37 Train Loss 729.7042 Test MSE 748.1597189403068 Test RE 0.46045408646968283 Lambda1 0.00012500379\n",
      "38 Train Loss 726.477 Test MSE 744.9633640280358 Test RE 0.45946943708755617 Lambda1 0.00011367377\n",
      "39 Train Loss 725.05035 Test MSE 744.5967232379844 Test RE 0.45935635705034755 Lambda1 4.0373223e-05\n",
      "40 Train Loss 724.21454 Test MSE 744.4202303171306 Test RE 0.459301912847579 Lambda1 2.3965009e-05\n",
      "41 Train Loss 723.545 Test MSE 745.1529275265646 Test RE 0.4595278917029696 Lambda1 1.771039e-05\n",
      "42 Train Loss 723.22687 Test MSE 745.0559142634357 Test RE 0.4594979772050695 Lambda1 9.898231e-06\n",
      "43 Train Loss 723.13116 Test MSE 745.0047327566383 Test RE 0.45948219436092647 Lambda1 7.5252915e-06\n",
      "44 Train Loss 722.81274 Test MSE 745.0600572123561 Test RE 0.4594992547427455 Lambda1 5.785915e-06\n",
      "45 Train Loss 722.67175 Test MSE 745.0650020620018 Test RE 0.45950077955335317 Lambda1 5.537398e-06\n",
      "46 Train Loss 722.5784 Test MSE 745.0520305741056 Test RE 0.45949677961057855 Lambda1 5.1760285e-06\n",
      "47 Train Loss 722.2904 Test MSE 744.694530333023 Test RE 0.45938652561956556 Lambda1 2.6273174e-06\n",
      "48 Train Loss 721.1645 Test MSE 743.7876773095911 Test RE 0.45910673105439276 Lambda1 2.011893e-06\n",
      "49 Train Loss 720.1466 Test MSE 742.8420504163464 Test RE 0.45881479172698575 Lambda1 1.4809987e-06\n",
      "50 Train Loss 719.8324 Test MSE 742.3588386841695 Test RE 0.45866554010878036 Lambda1 1.6631976e-06\n",
      "51 Train Loss 719.582 Test MSE 742.0234116505837 Test RE 0.4585619068008846 Lambda1 1.8261708e-06\n",
      "52 Train Loss 719.2966 Test MSE 741.905084445635 Test RE 0.45852534291946734 Lambda1 1.9134302e-06\n",
      "53 Train Loss 718.96246 Test MSE 741.3144763028812 Test RE 0.45834279752114204 Lambda1 2.548464e-06\n",
      "54 Train Loss 718.42633 Test MSE 740.2747145594467 Test RE 0.45802125076457234 Lambda1 2.0422585e-06\n",
      "55 Train Loss 717.9042 Test MSE 740.1830698553758 Test RE 0.4579928987755378 Lambda1 3.6820838e-06\n",
      "56 Train Loss 717.6479 Test MSE 740.0112559481315 Test RE 0.4579397402262627 Lambda1 3.3780686e-06\n",
      "57 Train Loss 717.29694 Test MSE 739.5495306521356 Test RE 0.45779685364668327 Lambda1 4.445911e-06\n",
      "58 Train Loss 716.45605 Test MSE 738.1847140463761 Test RE 0.45737423335230665 Lambda1 1.2798784e-05\n",
      "59 Train Loss 715.9373 Test MSE 736.8781256068033 Test RE 0.4569692774198552 Lambda1 2.7534847e-05\n",
      "60 Train Loss 715.28516 Test MSE 735.2022870374092 Test RE 0.4564493524711965 Lambda1 6.2424464e-05\n",
      "61 Train Loss 714.7975 Test MSE 733.5256204698389 Test RE 0.45592857729479996 Lambda1 0.000111409936\n",
      "62 Train Loss 714.45404 Test MSE 733.8664930897133 Test RE 0.4560345010097538 Lambda1 0.000109368506\n",
      "63 Train Loss 713.33734 Test MSE 733.9284434767317 Test RE 0.4560537490050632 Lambda1 8.553527e-05\n",
      "64 Train Loss 712.9553 Test MSE 732.923418826252 Test RE 0.4557413873006296 Lambda1 0.00010876297\n",
      "65 Train Loss 712.0157 Test MSE 730.1850551670175 Test RE 0.4548892153744093 Lambda1 0.00022473931\n",
      "66 Train Loss 711.1355 Test MSE 729.8687063525284 Test RE 0.4547906655254759 Lambda1 0.0002805618\n",
      "67 Train Loss 710.17957 Test MSE 727.877263515986 Test RE 0.45416979478415714 Lambda1 0.00023877916\n",
      "68 Train Loss 707.51776 Test MSE 720.0082639761433 Test RE 0.45170813413857375 Lambda1 0.00029395428\n",
      "69 Train Loss 701.75885 Test MSE 716.4812521413055 Test RE 0.4506004136549222 Lambda1 4.4851276e-05\n",
      "70 Train Loss 698.3471 Test MSE 712.4117981245365 Test RE 0.4493189364165716 Lambda1 3.1170246e-05\n",
      "71 Train Loss 694.5603 Test MSE 709.5995217182327 Test RE 0.44843120648816176 Lambda1 1.1642389e-05\n",
      "72 Train Loss 690.073 Test MSE 705.1297866316527 Test RE 0.44701665153632336 Lambda1 1.3629005e-05\n",
      "73 Train Loss 686.79297 Test MSE 703.3753344158282 Test RE 0.4464601881604334 Lambda1 7.874883e-06\n",
      "74 Train Loss 686.4183 Test MSE 702.8627493161878 Test RE 0.4462974794774177 Lambda1 5.7455413e-06\n",
      "Training time: 153.99\n",
      "Training time: 153.99\n",
      "inv_HT_atanh_tune11\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 25831.592 Test MSE 3669.704572594742 Test RE 1.019775848164528 Lambda1 -0.0016679182\n",
      "1 Train Loss 18962.273 Test MSE 3672.140179907065 Test RE 1.0201142080346561 Lambda1 0.0008278057\n",
      "2 Train Loss 12415.424 Test MSE 3675.3813733753873 Test RE 1.0205643077145279 Lambda1 -0.000111452966\n",
      "3 Train Loss 9443.52 Test MSE 3682.992262340989 Test RE 1.0216204409238194 Lambda1 0.0004704176\n",
      "4 Train Loss 7607.5273 Test MSE 3687.7902355516894 Test RE 1.0222856761167167 Lambda1 -0.0003315031\n",
      "5 Train Loss 6448.9077 Test MSE 3694.250329977508 Test RE 1.0231806798908905 Lambda1 -0.00057978486\n",
      "6 Train Loss 5577.333 Test MSE 3694.837026686551 Test RE 1.0232619241017753 Lambda1 0.00043038532\n",
      "7 Train Loss 5059.998 Test MSE 3691.91328976269 Test RE 1.0228569886840377 Lambda1 0.0003163069\n",
      "8 Train Loss 4657.451 Test MSE 3700.2574919750296 Test RE 1.0240122309407158 Lambda1 -0.00057528657\n",
      "9 Train Loss 4397.6353 Test MSE 3695.4846746410503 Test RE 1.0233516011902308 Lambda1 -0.00013177985\n",
      "10 Train Loss 4188.71 Test MSE 3681.5027960441857 Test RE 1.0214138394724734 Lambda1 0.00063972344\n",
      "11 Train Loss 4096.446 Test MSE 3678.606779835523 Test RE 1.0210120181030706 Lambda1 0.00011840302\n",
      "12 Train Loss 3972.0317 Test MSE 3675.597495134391 Test RE 1.0205943131600292 Lambda1 -3.4310804e-05\n",
      "13 Train Loss 3884.2012 Test MSE 3675.3812131841323 Test RE 1.0205642854739134 Lambda1 0.00021273566\n",
      "14 Train Loss 3832.5703 Test MSE 3663.370874685661 Test RE 1.0188954310380858 Lambda1 0.0008576668\n",
      "15 Train Loss 3797.582 Test MSE 3649.8232661303723 Test RE 1.0170096839428016 Lambda1 5.3246782e-05\n",
      "16 Train Loss 3760.05 Test MSE 3635.0214710722157 Test RE 1.0149453562734942 Lambda1 -0.00020190638\n",
      "17 Train Loss 3734.0 Test MSE 3621.020034777689 Test RE 1.0129887786403893 Lambda1 0.0004694702\n",
      "18 Train Loss 3703.8794 Test MSE 3605.090967710976 Test RE 1.0107582260710692 Lambda1 -0.00020722082\n",
      "19 Train Loss 3674.1157 Test MSE 3584.631973514843 Test RE 1.0078861045313348 Lambda1 0.00010667752\n",
      "20 Train Loss 3628.6338 Test MSE 3535.3846673109942 Test RE 1.0009387614610996 Lambda1 0.00092278473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Train Loss 3586.527 Test MSE 3490.641820395167 Test RE 0.9945847923813798 Lambda1 0.00014096271\n",
      "22 Train Loss 3510.1416 Test MSE 3402.1194510307982 Test RE 0.9818925154794133 Lambda1 0.00010595572\n",
      "23 Train Loss 3442.9214 Test MSE 3344.935406404846 Test RE 0.9736055444308066 Lambda1 0.0013277722\n",
      "24 Train Loss 3378.9458 Test MSE 3291.4714255448816 Test RE 0.9657933599935152 Lambda1 0.00092644256\n",
      "25 Train Loss 3310.01 Test MSE 3238.3818534035763 Test RE 0.9579728464031079 Lambda1 0.0009987684\n",
      "26 Train Loss 3231.6145 Test MSE 3158.694009262629 Test RE 0.9461128672843979 Lambda1 0.0012126925\n",
      "27 Train Loss 3154.2659 Test MSE 3102.02756447148 Test RE 0.9375879064722151 Lambda1 -0.00015851614\n",
      "28 Train Loss 3010.519 Test MSE 2963.5402765664016 Test RE 0.9164200610542841 Lambda1 -8.232909e-05\n",
      "29 Train Loss 959.044 Test MSE 968.6038249732817 Test RE 0.5239167223899088 Lambda1 -0.015726235\n",
      "30 Train Loss 838.1025 Test MSE 858.0837075448576 Test RE 0.4931215687855009 Lambda1 -0.016305093\n",
      "31 Train Loss 837.83136 Test MSE 857.7493224637553 Test RE 0.4930254775990485 Lambda1 -0.015279658\n",
      "32 Train Loss 837.55664 Test MSE 857.2946168663462 Test RE 0.4928947802281444 Lambda1 -0.017346086\n",
      "33 Train Loss 837.35364 Test MSE 856.83854239395 Test RE 0.49276365457546584 Lambda1 -0.01861504\n",
      "34 Train Loss 836.7608 Test MSE 855.5679185989499 Test RE 0.4923981542585014 Lambda1 -0.0022281536\n",
      "35 Train Loss 831.6365 Test MSE 848.0695542346133 Test RE 0.4902356689161071 Lambda1 -0.009866111\n",
      "36 Train Loss 818.0158 Test MSE 837.4975837236329 Test RE 0.4871704660451935 Lambda1 0.00025618816\n",
      "37 Train Loss 782.24243 Test MSE 781.7130085577046 Test RE 0.4706660144788774 Lambda1 0.0007763278\n",
      "38 Train Loss 733.05865 Test MSE 755.4994769487847 Test RE 0.46270719701620167 Lambda1 3.1492862e-05\n",
      "39 Train Loss 724.44165 Test MSE 748.4282210523285 Test RE 0.4605367037244735 Lambda1 3.956206e-05\n",
      "40 Train Loss 718.6121 Test MSE 747.2662353674777 Test RE 0.46017905758515143 Lambda1 4.901404e-06\n",
      "41 Train Loss 716.39685 Test MSE 745.8489973197011 Test RE 0.459742471025579 Lambda1 1.070501e-05\n",
      "42 Train Loss 712.0868 Test MSE 740.6563300007833 Test RE 0.4581392917148385 Lambda1 7.954176e-06\n",
      "43 Train Loss 708.2731 Test MSE 734.3009872517976 Test RE 0.45616948128922596 Lambda1 3.2130738e-05\n",
      "44 Train Loss 700.0835 Test MSE 725.4939730176384 Test RE 0.45342564044254746 Lambda1 -4.758354e-06\n",
      "45 Train Loss 688.93286 Test MSE 718.1545185311677 Test RE 0.45112627168083663 Lambda1 6.2626195e-06\n",
      "46 Train Loss 685.172 Test MSE 717.4385266014891 Test RE 0.4509013317068219 Lambda1 -1.9177828e-06\n",
      "47 Train Loss 682.6177 Test MSE 716.5066680537567 Test RE 0.4506084057126821 Lambda1 -1.1328281e-06\n",
      "48 Train Loss 678.758 Test MSE 713.9002579862195 Test RE 0.4497880781661603 Lambda1 1.8047806e-06\n",
      "49 Train Loss 674.6639 Test MSE 709.413371766673 Test RE 0.4483723839269297 Lambda1 1.091126e-05\n",
      "50 Train Loss 671.96893 Test MSE 708.7240095864603 Test RE 0.44815448128961777 Lambda1 8.959504e-06\n",
      "51 Train Loss 669.51807 Test MSE 707.5715896570971 Test RE 0.4477899724908925 Lambda1 5.181423e-06\n",
      "52 Train Loss 667.9876 Test MSE 706.2556495432244 Test RE 0.44737337929423115 Lambda1 6.178238e-06\n",
      "53 Train Loss 667.045 Test MSE 705.7469789652177 Test RE 0.44721224312096663 Lambda1 6.0499974e-06\n",
      "54 Train Loss 666.125 Test MSE 705.9004768726006 Test RE 0.4472608741554187 Lambda1 5.2223886e-06\n",
      "55 Train Loss 664.1826 Test MSE 705.5440505122804 Test RE 0.4471479434385571 Lambda1 5.6101726e-06\n",
      "56 Train Loss 663.5449 Test MSE 705.1177838584879 Test RE 0.4470128469443149 Lambda1 7.5769303e-06\n",
      "57 Train Loss 662.684 Test MSE 704.6005707265147 Test RE 0.44684887197965273 Lambda1 5.326368e-06\n",
      "58 Train Loss 661.6512 Test MSE 704.024858082992 Test RE 0.4466662797940312 Lambda1 3.5882028e-06\n",
      "59 Train Loss 660.91327 Test MSE 703.0934046539099 Test RE 0.4463707031984965 Lambda1 2.3924897e-06\n",
      "60 Train Loss 660.7833 Test MSE 702.9148591513788 Test RE 0.446314023288485 Lambda1 2.3764273e-06\n",
      "61 Train Loss 660.55634 Test MSE 702.3335740007235 Test RE 0.44612944234737656 Lambda1 4.059922e-06\n",
      "62 Train Loss 659.90295 Test MSE 700.2326692823253 Test RE 0.4454616845622893 Lambda1 6.82976e-06\n",
      "63 Train Loss 659.6961 Test MSE 699.9369129238491 Test RE 0.4453676000915436 Lambda1 8.611797e-06\n",
      "64 Train Loss 659.3542 Test MSE 699.6823989716638 Test RE 0.4452866195258238 Lambda1 9.999624e-06\n",
      "65 Train Loss 658.8225 Test MSE 698.4835290604459 Test RE 0.44490496807646296 Lambda1 1.0215877e-05\n",
      "66 Train Loss 657.79724 Test MSE 696.5700326405204 Test RE 0.4442951413024652 Lambda1 9.414984e-06\n",
      "67 Train Loss 657.1381 Test MSE 696.292868614546 Test RE 0.44420674036909763 Lambda1 9.277899e-06\n",
      "68 Train Loss 656.93604 Test MSE 696.8133762181806 Test RE 0.4443727407707987 Lambda1 6.34666e-06\n",
      "69 Train Loss 656.71906 Test MSE 696.6736903341863 Test RE 0.44432819820576197 Lambda1 6.0899715e-06\n",
      "70 Train Loss 656.56744 Test MSE 696.2685093101969 Test RE 0.44419897017475174 Lambda1 6.4779706e-06\n",
      "71 Train Loss 656.25665 Test MSE 695.8708777690931 Test RE 0.44407211340654157 Lambda1 4.7806157e-06\n",
      "72 Train Loss 655.20184 Test MSE 695.5002697246915 Test RE 0.443953845333218 Lambda1 4.86666e-06\n",
      "73 Train Loss 654.3597 Test MSE 694.0104004301578 Test RE 0.443478081459242 Lambda1 1.399267e-05\n",
      "74 Train Loss 652.18396 Test MSE 693.5089963999419 Test RE 0.4433178519624405 Lambda1 2.5224372e-05\n",
      "Training time: 155.77\n",
      "Training time: 155.77\n",
      "inv_HT_atanh_tune12\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.91\n",
      "Training time: 2.91\n",
      "inv_HT_atanh_tune12\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 1352558.2 Test MSE 3509.142526367867 Test RE 0.997217002310223 Lambda1 2.3956625e-05\n",
      "1 Train Loss 1327844.0 Test MSE 3509.229756064421 Test RE 0.9972293965625633 Lambda1 1.7037639e-05\n",
      "2 Train Loss 1296250.6 Test MSE 3509.2439075530433 Test RE 0.9972314072981397 Lambda1 1.5207303e-05\n",
      "3 Train Loss 1280555.9 Test MSE 3509.2027477873394 Test RE 0.9972255590395809 Lambda1 1.0398158e-05\n",
      "4 Train Loss 1243636.9 Test MSE 3509.3161279228816 Test RE 0.9972416687750282 Lambda1 1.1344957e-05\n",
      "5 Train Loss 1230235.9 Test MSE 3509.3877468207775 Test RE 0.9972518446871231 Lambda1 2.3034598e-05\n",
      "6 Train Loss 1215892.1 Test MSE 3509.3101197878937 Test RE 0.9972408151093981 Lambda1 3.1482257e-05\n",
      "7 Train Loss 1211314.4 Test MSE 3509.334567555832 Test RE 0.9972442887650076 Lambda1 3.6060865e-05\n",
      "8 Train Loss 1208759.6 Test MSE 3509.3686436007793 Test RE 0.9972491304320046 Lambda1 4.499893e-05\n",
      "9 Train Loss 1206551.0 Test MSE 3509.364690688236 Test RE 0.9972485687868619 Lambda1 5.503439e-05\n",
      "10 Train Loss 1203571.9 Test MSE 3509.3367452295997 Test RE 0.9972445981787083 Lambda1 6.4809545e-05\n",
      "11 Train Loss 1195580.5 Test MSE 3509.3846716185512 Test RE 0.9972514077516821 Lambda1 5.9973827e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 Train Loss 1193580.8 Test MSE 3509.4343997523197 Test RE 0.997258473274611 Lambda1 5.422204e-05\n",
      "13 Train Loss 1189906.6 Test MSE 3509.487098850515 Test RE 0.9972659608664751 Lambda1 5.3359505e-05\n",
      "14 Train Loss 1186860.6 Test MSE 3509.526766042212 Test RE 0.997271596822264 Lambda1 4.2655873e-05\n",
      "15 Train Loss 1183284.6 Test MSE 3509.563555823367 Test RE 0.9972768239241059 Lambda1 3.2541448e-05\n",
      "16 Train Loss 1179249.4 Test MSE 3509.630317133148 Test RE 0.997286309318699 Lambda1 3.8133647e-05\n",
      "17 Train Loss 1177357.1 Test MSE 3509.6297895094917 Test RE 0.9972862343546967 Lambda1 3.343579e-05\n",
      "18 Train Loss 1175115.8 Test MSE 3509.650391463008 Test RE 0.9972891614461171 Lambda1 2.4241004e-05\n",
      "19 Train Loss 1170860.4 Test MSE 3509.6559878686094 Test RE 0.9972899565726685 Lambda1 2.3060527e-05\n",
      "20 Train Loss 1165786.0 Test MSE 3509.647101437171 Test RE 0.9972886940052736 Lambda1 1.6968585e-05\n",
      "21 Train Loss 1162061.1 Test MSE 3509.643788395489 Test RE 0.99728822329416 Lambda1 1.0284528e-05\n",
      "22 Train Loss 1159782.6 Test MSE 3509.605400771876 Test RE 0.9972827692321607 Lambda1 8.565511e-06\n",
      "23 Train Loss 1154299.4 Test MSE 3509.5863845857853 Test RE 0.9972800674269366 Lambda1 1.5396863e-05\n",
      "24 Train Loss 1151088.4 Test MSE 3509.6086110288084 Test RE 0.9972832253422965 Lambda1 1.6218242e-05\n",
      "25 Train Loss 1148987.4 Test MSE 3509.630300491363 Test RE 0.997286306954258 Lambda1 2.341835e-05\n",
      "26 Train Loss 1145775.2 Test MSE 3509.6341338982857 Test RE 0.9972868515989733 Lambda1 3.405869e-05\n",
      "27 Train Loss 1144226.0 Test MSE 3509.6505922384276 Test RE 0.9972891899719138 Lambda1 4.1300373e-05\n",
      "28 Train Loss 1143522.1 Test MSE 3509.65452569682 Test RE 0.9972897488301722 Lambda1 4.7809106e-05\n",
      "29 Train Loss 1141525.9 Test MSE 3509.624630945072 Test RE 0.9972855014330664 Lambda1 4.983633e-05\n",
      "30 Train Loss 1137108.1 Test MSE 3509.6157611905173 Test RE 0.9972842412294635 Lambda1 5.289941e-05\n",
      "31 Train Loss 1134491.2 Test MSE 3509.5600319639316 Test RE 0.9972763232544114 Lambda1 5.737799e-05\n",
      "32 Train Loss 1133688.8 Test MSE 3509.542595643589 Test RE 0.9972738458995746 Lambda1 5.8328675e-05\n",
      "33 Train Loss 1133150.9 Test MSE 3509.5625090022727 Test RE 0.9972766751918664 Lambda1 5.757708e-05\n",
      "34 Train Loss 1132171.5 Test MSE 3509.5848875811657 Test RE 0.9972798547333508 Lambda1 5.11739e-05\n",
      "35 Train Loss 1131227.8 Test MSE 3509.589674800141 Test RE 0.9972805348986082 Lambda1 4.703641e-05\n",
      "36 Train Loss 1129455.6 Test MSE 3509.620629335176 Test RE 0.9972849328895483 Lambda1 4.7725793e-05\n",
      "37 Train Loss 1127771.1 Test MSE 3509.5646016238234 Test RE 0.9972769725113112 Lambda1 6.075927e-05\n",
      "38 Train Loss 1126643.6 Test MSE 3509.508585597833 Test RE 0.9972690137298005 Lambda1 6.6850276e-05\n",
      "39 Train Loss 1122723.0 Test MSE 3509.4189331876687 Test RE 0.9972562757439143 Lambda1 7.720105e-05\n",
      "40 Train Loss 1120058.5 Test MSE 3509.300000011655 Test RE 0.9972393772397264 Lambda1 7.475938e-05\n",
      "41 Train Loss 1116346.8 Test MSE 3509.1809021862523 Test RE 0.9972224550545413 Lambda1 6.2316154e-05\n",
      "42 Train Loss 1114961.6 Test MSE 3509.18053613181 Test RE 0.9972224030427286 Lambda1 6.186687e-05\n",
      "43 Train Loss 1114414.4 Test MSE 3509.149329526826 Test RE 0.9972179689598312 Lambda1 5.9366335e-05\n",
      "44 Train Loss 1113715.8 Test MSE 3509.1131995004544 Test RE 0.997212835293396 Lambda1 6.0594466e-05\n",
      "45 Train Loss 1110881.0 Test MSE 3509.026645714285 Test RE 0.9972005368758406 Lambda1 6.0598177e-05\n",
      "46 Train Loss 1108610.8 Test MSE 3509.000475647668 Test RE 0.9971968183442119 Lambda1 4.8898433e-05\n",
      "47 Train Loss 1106517.9 Test MSE 3509.014054421083 Test RE 0.9971987477677959 Lambda1 5.1594456e-05\n",
      "48 Train Loss 1104266.1 Test MSE 3508.9451341930003 Test RE 0.9971889547744444 Lambda1 5.3547566e-05\n",
      "49 Train Loss 1102702.8 Test MSE 3508.8100066344978 Test RE 0.9971697539888862 Lambda1 4.4541626e-05\n",
      "50 Train Loss 1099236.8 Test MSE 3508.7169919538906 Test RE 0.9971565369664916 Lambda1 4.091051e-05\n",
      "51 Train Loss 1097626.9 Test MSE 3508.617763174287 Test RE 0.9971424367517918 Lambda1 3.222467e-05\n",
      "52 Train Loss 1093701.0 Test MSE 3508.659874660612 Test RE 0.9971484207356461 Lambda1 2.1819613e-05\n",
      "53 Train Loss 1085349.5 Test MSE 3508.4119674898916 Test RE 0.9971131929536311 Lambda1 3.0990675e-05\n",
      "54 Train Loss 1074109.9 Test MSE 3508.305960429501 Test RE 0.99709812889646 Lambda1 5.057322e-06\n",
      "55 Train Loss 1054263.8 Test MSE 3508.255381948458 Test RE 0.9970909413975436 Lambda1 5.264744e-07\n",
      "56 Train Loss 1048455.25 Test MSE 3508.1886579321117 Test RE 0.9970814594440702 Lambda1 -1.7760376e-06\n",
      "57 Train Loss 1043059.3 Test MSE 3508.119835508502 Test RE 0.9970716791976496 Lambda1 -1.0205388e-05\n",
      "58 Train Loss 1036208.1 Test MSE 3508.073846639642 Test RE 0.9970651437383258 Lambda1 -1.0833109e-05\n",
      "59 Train Loss 1031201.9 Test MSE 3507.974298696243 Test RE 0.9970509968741227 Lambda1 -1.4302603e-05\n",
      "60 Train Loss 1027154.75 Test MSE 3507.9817304071635 Test RE 0.9970520530094056 Lambda1 -1.4997894e-05\n",
      "61 Train Loss 1022994.2 Test MSE 3507.9350312423817 Test RE 0.9970454164792496 Lambda1 -1.8411132e-05\n",
      "62 Train Loss 1017965.94 Test MSE 3507.736476841232 Test RE 0.997017198944543 Lambda1 -1.7361537e-05\n",
      "63 Train Loss 1013405.3 Test MSE 3507.572577889709 Test RE 0.9969939058631623 Lambda1 -1.3198086e-05\n",
      "64 Train Loss 1009393.3 Test MSE 3507.316254131938 Test RE 0.9969574764112116 Lambda1 -4.896746e-06\n",
      "65 Train Loss 1001671.0 Test MSE 3507.2656294535177 Test RE 0.996950281332395 Lambda1 1.4391053e-05\n",
      "66 Train Loss 996146.6 Test MSE 3507.0932421449625 Test RE 0.9969257802386962 Lambda1 -1.0041742e-05\n",
      "67 Train Loss 990326.06 Test MSE 3507.0286188075306 Test RE 0.9969165952864544 Lambda1 9.820428e-06\n",
      "68 Train Loss 984495.94 Test MSE 3507.067646571652 Test RE 0.9969221423352119 Lambda1 -1.0417281e-05\n",
      "69 Train Loss 979593.1 Test MSE 3506.9195907298626 Test RE 0.996901098871031 Lambda1 -5.6484772e-05\n",
      "70 Train Loss 975346.75 Test MSE 3506.8570380639494 Test RE 0.996892208005722 Lambda1 -2.7716753e-05\n",
      "71 Train Loss 968291.5 Test MSE 3506.6035628506174 Test RE 0.9968561797290387 Lambda1 2.4177058e-05\n",
      "72 Train Loss 961380.8 Test MSE 3506.407101040125 Test RE 0.9968282542864358 Lambda1 3.271089e-05\n",
      "73 Train Loss 951482.3 Test MSE 3506.170733150502 Test RE 0.9967946554836016 Lambda1 4.4378445e-05\n",
      "74 Train Loss 941165.1 Test MSE 3505.823107260061 Test RE 0.9967452397184571 Lambda1 1.986499e-05\n",
      "Training time: 151.32\n",
      "Training time: 151.32\n",
      "inv_HT_atanh_tune12\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.36\n",
      "Training time: 3.36\n",
      "inv_HT_atanh_tune12\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.17\n",
      "Training time: 3.17\n",
      "inv_HT_atanh_tune12\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 455503.03 Test MSE 3515.857469555602 Test RE 0.9981706619451927 Lambda1 4.6125337e-05\n",
      "1 Train Loss 433470.16 Test MSE 3515.8846096793413 Test RE 0.9981745145506002 Lambda1 -4.5769934e-06\n",
      "2 Train Loss 412575.1 Test MSE 3515.8843220344042 Test RE 0.9981744737187921 Lambda1 -3.0736384e-05\n",
      "3 Train Loss 399530.38 Test MSE 3515.7304711235547 Test RE 0.9981526340167497 Lambda1 3.7077814e-05\n",
      "4 Train Loss 392930.53 Test MSE 3515.6565337704124 Test RE 0.9981421381678196 Lambda1 5.6231973e-05\n",
      "5 Train Loss 385022.62 Test MSE 3515.5539525262593 Test RE 0.9981275759646717 Lambda1 3.284369e-05\n",
      "6 Train Loss 379528.72 Test MSE 3515.4038642423034 Test RE 0.9981062693859618 Lambda1 1.0229466e-05\n",
      "7 Train Loss 375735.12 Test MSE 3515.2487235137387 Test RE 0.9980842450826031 Lambda1 -1.4343375e-05\n",
      "8 Train Loss 368615.34 Test MSE 3515.1823493620395 Test RE 0.9980748222347451 Lambda1 -3.6398225e-05\n",
      "9 Train Loss 365348.0 Test MSE 3515.198577050702 Test RE 0.9980771260169068 Lambda1 -2.4186626e-05\n",
      "10 Train Loss 358289.9 Test MSE 3515.0581404277013 Test RE 0.9980571885968738 Lambda1 -1.0697003e-05\n",
      "11 Train Loss 352797.12 Test MSE 3514.992603394623 Test RE 0.9980478843394114 Lambda1 -2.9771658e-05\n",
      "12 Train Loss 346503.5 Test MSE 3514.7213420082353 Test RE 0.9980093725832265 Lambda1 -3.4374407e-05\n",
      "13 Train Loss 339377.28 Test MSE 3514.475113767015 Test RE 0.9979744135676116 Lambda1 -2.5805803e-05\n",
      "14 Train Loss 334387.25 Test MSE 3514.514128325549 Test RE 0.997979952861796 Lambda1 -1.6291435e-05\n",
      "15 Train Loss 328267.84 Test MSE 3514.446076847222 Test RE 0.9979702908804507 Lambda1 -1.3949453e-05\n",
      "16 Train Loss 322183.75 Test MSE 3514.3717620283596 Test RE 0.9979597395201741 Lambda1 2.6773148e-05\n",
      "17 Train Loss 316425.28 Test MSE 3514.0789900384434 Test RE 0.997918170106935 Lambda1 9.574782e-06\n",
      "18 Train Loss 313162.38 Test MSE 3513.975498420987 Test RE 0.9979034753709599 Lambda1 8.742912e-06\n",
      "19 Train Loss 310640.8 Test MSE 3513.945487643857 Test RE 0.99789921411149 Lambda1 -8.402583e-06\n",
      "20 Train Loss 307681.06 Test MSE 3513.807885955893 Test RE 0.9978796756808233 Lambda1 -4.8503527e-05\n",
      "21 Train Loss 303573.62 Test MSE 3513.590183807665 Test RE 0.9978487627907234 Lambda1 -2.3497982e-05\n",
      "22 Train Loss 300276.84 Test MSE 3513.4996386727207 Test RE 0.9978359054385216 Lambda1 1.6956572e-05\n",
      "23 Train Loss 297105.88 Test MSE 3513.4053460246378 Test RE 0.9978225157657123 Lambda1 5.1727084e-06\n",
      "24 Train Loss 293439.9 Test MSE 3513.446756709633 Test RE 0.9978283961563956 Lambda1 1.5632908e-05\n",
      "25 Train Loss 291270.12 Test MSE 3513.418701068154 Test RE 0.9978244122093539 Lambda1 2.6896503e-05\n",
      "26 Train Loss 288118.62 Test MSE 3513.2998332286784 Test RE 0.9978075326049891 Lambda1 2.3005634e-05\n",
      "27 Train Loss 285933.66 Test MSE 3513.388627737221 Test RE 0.9978201417294413 Lambda1 2.816476e-05\n",
      "28 Train Loss 284560.5 Test MSE 3513.5060791525325 Test RE 0.9978368199880764 Lambda1 4.2251337e-05\n",
      "29 Train Loss 283994.66 Test MSE 3513.5142154005384 Test RE 0.9978379753358906 Lambda1 4.999067e-05\n",
      "30 Train Loss 283155.75 Test MSE 3513.5930816356845 Test RE 0.9978491742777432 Lambda1 4.6613033e-05\n",
      "31 Train Loss 282243.66 Test MSE 3513.6866085488214 Test RE 0.9978624548611569 Lambda1 4.212695e-05\n",
      "32 Train Loss 280175.84 Test MSE 3513.9913021742486 Test RE 0.9979057193539612 Lambda1 4.183708e-05\n",
      "33 Train Loss 279424.5 Test MSE 3514.1091959018863 Test RE 0.9979224589853701 Lambda1 4.0487168e-05\n",
      "34 Train Loss 278817.62 Test MSE 3514.139405039455 Test RE 0.997926748310257 Lambda1 4.2777858e-05\n",
      "35 Train Loss 278585.0 Test MSE 3514.169572335451 Test RE 0.9979310316757584 Lambda1 4.762473e-05\n",
      "36 Train Loss 278441.94 Test MSE 3514.2122764830433 Test RE 0.9979370950804578 Lambda1 4.883008e-05\n",
      "37 Train Loss 278273.0 Test MSE 3514.165162095148 Test RE 0.9979304054798755 Lambda1 4.8724694e-05\n",
      "38 Train Loss 277952.75 Test MSE 3514.082256263345 Test RE 0.997918633873462 Lambda1 4.7298094e-05\n",
      "39 Train Loss 277623.66 Test MSE 3514.073659808647 Test RE 0.9979174132748893 Lambda1 4.4120952e-05\n",
      "40 Train Loss 277226.4 Test MSE 3514.08969315262 Test RE 0.9979196898257593 Lambda1 4.258716e-05\n",
      "41 Train Loss 276060.34 Test MSE 3514.1205916068466 Test RE 0.9979240770370671 Lambda1 3.205233e-05\n",
      "42 Train Loss 274721.8 Test MSE 3514.096955809922 Test RE 0.9979207210380516 Lambda1 2.4633211e-05\n",
      "43 Train Loss 271757.28 Test MSE 3514.1489050286928 Test RE 0.99792809718773 Lambda1 2.0210417e-05\n",
      "44 Train Loss 270272.44 Test MSE 3514.1637739494167 Test RE 0.9979302083814316 Lambda1 2.3337416e-05\n",
      "45 Train Loss 269262.44 Test MSE 3514.1082382258282 Test RE 0.9979223230068818 Lambda1 2.2802058e-05\n",
      "46 Train Loss 268493.1 Test MSE 3514.085693594444 Test RE 0.9979191219348773 Lambda1 3.1342614e-05\n",
      "47 Train Loss 267692.94 Test MSE 3513.990202552751 Test RE 0.9979055632183061 Lambda1 3.63709e-05\n",
      "48 Train Loss 266552.75 Test MSE 3513.9175261775417 Test RE 0.9978952438192833 Lambda1 4.499472e-05\n",
      "49 Train Loss 265266.88 Test MSE 3513.8510808164433 Test RE 0.9978858090754724 Lambda1 5.2383355e-05\n",
      "50 Train Loss 264188.0 Test MSE 3513.816663210578 Test RE 0.9978809219980485 Lambda1 5.2383984e-05\n",
      "51 Train Loss 263591.94 Test MSE 3513.817305495859 Test RE 0.9978810131986232 Lambda1 5.8705315e-05\n",
      "52 Train Loss 262926.1 Test MSE 3513.800116616314 Test RE 0.997878572480165 Lambda1 5.5563534e-05\n",
      "53 Train Loss 261739.25 Test MSE 3513.938837505544 Test RE 0.997898269849412 Lambda1 4.2379874e-05\n",
      "54 Train Loss 260064.08 Test MSE 3513.96339370814 Test RE 0.9979017566131841 Lambda1 2.8964885e-05\n",
      "55 Train Loss 258371.67 Test MSE 3513.9090958059237 Test RE 0.9978940467745708 Lambda1 2.1772867e-05\n",
      "56 Train Loss 256844.23 Test MSE 3513.850993957533 Test RE 0.9978857967420991 Lambda1 1.46616485e-05\n",
      "57 Train Loss 254635.47 Test MSE 3513.66691723987 Test RE 0.9978596587601369 Lambda1 1.7103896e-05\n",
      "58 Train Loss 252375.75 Test MSE 3513.606582029502 Test RE 0.9978510913101897 Lambda1 1.5247261e-05\n",
      "59 Train Loss 250435.27 Test MSE 3513.4655734345847 Test RE 0.997831068153247 Lambda1 -1.12134e-05\n",
      "60 Train Loss 248666.0 Test MSE 3513.348328457751 Test RE 0.9978144191134254 Lambda1 -7.425965e-06\n",
      "61 Train Loss 247577.42 Test MSE 3513.26234252332 Test RE 0.9978022087485318 Lambda1 -4.8188244e-06\n",
      "62 Train Loss 245515.45 Test MSE 3513.360458480137 Test RE 0.9978161416156744 Lambda1 -8.9507125e-07\n",
      "63 Train Loss 243763.58 Test MSE 3513.591039663569 Test RE 0.9978488843209327 Lambda1 -1.9474763e-05\n",
      "64 Train Loss 242684.28 Test MSE 3513.5703189297496 Test RE 0.9978459420047422 Lambda1 -4.695797e-05\n",
      "65 Train Loss 240721.55 Test MSE 3513.5603743095935 Test RE 0.9978445298790425 Lambda1 -2.566536e-05\n",
      "66 Train Loss 239096.7 Test MSE 3513.6799907082445 Test RE 0.9978615151504642 Lambda1 -1.2176306e-05\n",
      "67 Train Loss 235972.5 Test MSE 3513.9125546972705 Test RE 0.9978945379093926 Lambda1 -5.358807e-06\n",
      "68 Train Loss 233989.83 Test MSE 3514.108397910215 Test RE 0.9979223456801468 Lambda1 9.375762e-06\n",
      "69 Train Loss 232159.86 Test MSE 3514.162378653466 Test RE 0.9979300102677117 Lambda1 -2.4737812e-05\n",
      "70 Train Loss 230548.3 Test MSE 3514.2221964313685 Test RE 0.9979385035721268 Lambda1 -4.169124e-05\n",
      "71 Train Loss 229249.44 Test MSE 3514.243159713037 Test RE 0.9979414800536311 Lambda1 -3.477791e-05\n",
      "72 Train Loss 227919.8 Test MSE 3514.3628873964935 Test RE 0.9979584794754923 Lambda1 -3.169876e-05\n",
      "73 Train Loss 226651.0 Test MSE 3514.347396641294 Test RE 0.9979562800515801 Lambda1 -3.4459692e-05\n",
      "74 Train Loss 225105.2 Test MSE 3514.4013921976716 Test RE 0.9979639464817199 Lambda1 -3.6178342e-05\n",
      "Training time: 158.58\n",
      "Training time: 158.58\n",
      "inv_HT_atanh_tune12\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.19\n",
      "Training time: 3.19\n",
      "inv_HT_atanh_tune12\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.90\n",
      "Training time: 2.90\n",
      "inv_HT_atanh_tune12\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.94\n",
      "Training time: 2.94\n",
      "inv_HT_atanh_tune12\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.88\n",
      "Training time: 2.88\n",
      "inv_HT_atanh_tune12\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.93\n",
      "Training time: 2.93\n",
      "inv_HT_atanh_tune13\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.02\n",
      "Training time: 3.02\n",
      "inv_HT_atanh_tune13\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.19\n",
      "Training time: 3.19\n",
      "inv_HT_atanh_tune13\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.85\n",
      "Training time: 2.85\n",
      "inv_HT_atanh_tune13\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.93\n",
      "Training time: 2.93\n",
      "inv_HT_atanh_tune13\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.95\n",
      "Training time: 2.95\n",
      "inv_HT_atanh_tune13\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.97\n",
      "Training time: 2.97\n",
      "inv_HT_atanh_tune13\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.82\n",
      "Training time: 2.82\n",
      "inv_HT_atanh_tune13\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.94\n",
      "Training time: 2.94\n",
      "inv_HT_atanh_tune13\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.83\n",
      "Training time: 2.83\n",
      "inv_HT_atanh_tune13\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.02\n",
      "Training time: 3.02\n",
      "inv_HT_atanh_tune14\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.78\n",
      "Training time: 2.78\n",
      "inv_HT_atanh_tune14\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.84\n",
      "Training time: 2.84\n",
      "inv_HT_atanh_tune14\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.85\n",
      "Training time: 2.85\n",
      "inv_HT_atanh_tune14\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.89\n",
      "Training time: 2.89\n",
      "inv_HT_atanh_tune14\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.93\n",
      "Training time: 2.93\n",
      "inv_HT_atanh_tune14\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.04\n",
      "Training time: 3.04\n",
      "inv_HT_atanh_tune14\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.98\n",
      "Training time: 2.98\n",
      "inv_HT_atanh_tune14\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.90\n",
      "Training time: 2.90\n",
      "inv_HT_atanh_tune14\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.07\n",
      "Training time: 3.07\n",
      "inv_HT_atanh_tune14\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.94\n",
      "Training time: 2.94\n",
      "inv_HT_atanh_tune15\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.20404 Test MSE 858.0625745218441 Test RE 0.49311549640961577 Lambda1 -0.059699617\n",
      "1 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "2 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "3 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "4 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "5 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "6 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "7 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "8 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "9 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "10 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "11 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "12 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "13 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "14 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "15 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "16 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "17 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "18 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "19 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "20 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "21 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "22 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "23 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "24 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "25 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "26 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "27 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "28 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "29 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "30 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "31 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "32 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "33 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "34 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "35 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "36 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "37 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "39 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "40 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "41 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "42 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "43 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "44 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "45 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "46 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "47 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "48 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "49 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "50 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "51 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "52 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "53 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "54 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "55 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "56 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "57 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "58 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "59 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "60 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "61 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "62 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "63 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "64 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "65 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "66 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "67 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "68 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "69 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "70 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "71 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "72 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "73 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "74 Train Loss 838.06323 Test MSE 858.2840921308193 Test RE 0.4931791437005674 Lambda1 -0.060030114\n",
      "Training time: 117.50\n",
      "Training time: 117.50\n",
      "inv_HT_atanh_tune15\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 838.11694 Test MSE 858.112195542783 Test RE 0.49312975442607965 Lambda1 -0.07790233\n",
      "1 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "2 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "3 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "4 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "5 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "6 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "7 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "8 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "9 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "10 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "11 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "12 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "13 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "14 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "15 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "16 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "17 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "18 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "19 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "20 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "21 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "22 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "23 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "24 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "25 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "26 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "27 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "28 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "29 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "30 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "31 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "32 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "33 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "34 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "35 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "36 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "37 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "38 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "40 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "41 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "42 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "43 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "44 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "45 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "46 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "47 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "48 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "49 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "50 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "51 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "52 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "53 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "54 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "55 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "56 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "57 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "58 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "59 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "60 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "61 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "62 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "63 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "64 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "65 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "66 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "67 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "68 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "69 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "70 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "71 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "72 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "73 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "74 Train Loss 838.0626 Test MSE 858.2811631989571 Test RE 0.4931783022021909 Lambda1 -0.07817864\n",
      "Training time: 127.63\n",
      "Training time: 127.63\n",
      "inv_HT_atanh_tune15\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 838.0764 Test MSE 858.2102325362399 Test RE 0.49315792299076033 Lambda1 -0.20165095\n",
      "1 Train Loss 838.0639 Test MSE 858.3163025744868 Test RE 0.49318839784504775 Lambda1 -0.20199047\n",
      "2 Train Loss 837.9659 Test MSE 858.0868672863738 Test RE 0.49312247670115367 Lambda1 -0.20392565\n",
      "3 Train Loss 837.34393 Test MSE 856.5356774113683 Test RE 0.49267655879337136 Lambda1 -0.37077874\n",
      "4 Train Loss 833.9258 Test MSE 853.4922069723707 Test RE 0.49180048268557847 Lambda1 -0.4813252\n",
      "5 Train Loss 829.1319 Test MSE 844.3741956857858 Test RE 0.4891664321273998 Lambda1 -0.80978847\n",
      "6 Train Loss 823.6751 Test MSE 839.2243892217513 Test RE 0.4876724468671002 Lambda1 -0.98433423\n",
      "7 Train Loss 818.13586 Test MSE 831.0444836240994 Test RE 0.4852899596860003 Lambda1 -0.8674411\n",
      "8 Train Loss 815.063 Test MSE 829.8962086302013 Test RE 0.48495457514288326 Lambda1 -0.8848194\n",
      "9 Train Loss 811.8769 Test MSE 826.0736363094393 Test RE 0.4838364151206593 Lambda1 -1.029932\n",
      "10 Train Loss 808.7266 Test MSE 822.5848145296942 Test RE 0.48281362186826815 Lambda1 -1.1015188\n",
      "11 Train Loss 805.71155 Test MSE 819.6978440687825 Test RE 0.4819656279700221 Lambda1 -1.0919765\n",
      "12 Train Loss 802.32605 Test MSE 815.0995779717354 Test RE 0.48061188339252475 Lambda1 -1.0323739\n",
      "13 Train Loss 798.6806 Test MSE 810.8215618693155 Test RE 0.4793489884481135 Lambda1 -1.0879861\n",
      "14 Train Loss 797.095 Test MSE 809.1516307650631 Test RE 0.478855111369508 Lambda1 -1.1251818\n",
      "15 Train Loss 794.79266 Test MSE 805.6674790698542 Test RE 0.47782304045103585 Lambda1 -1.1392931\n",
      "16 Train Loss 792.3524 Test MSE 800.6135019520514 Test RE 0.4763219833169127 Lambda1 -1.1825488\n",
      "17 Train Loss 789.7973 Test MSE 797.1529604242581 Test RE 0.47529145046198756 Lambda1 -1.149726\n",
      "18 Train Loss 787.1793 Test MSE 793.5569446831555 Test RE 0.47421820134043957 Lambda1 -1.0671502\n",
      "19 Train Loss 781.891 Test MSE 787.2431919806813 Test RE 0.4723279301130048 Lambda1 -0.97766507\n",
      "20 Train Loss 779.46625 Test MSE 782.8553692812236 Test RE 0.4710097941489363 Lambda1 -0.9382785\n",
      "21 Train Loss 777.41644 Test MSE 780.4104924216069 Test RE 0.47027373134176365 Lambda1 -0.8951165\n",
      "22 Train Loss 775.53894 Test MSE 779.5636129889382 Test RE 0.47001849819212316 Lambda1 -0.90175265\n",
      "23 Train Loss 773.40326 Test MSE 779.0707376683552 Test RE 0.46986989123929973 Lambda1 -0.921834\n",
      "24 Train Loss 769.7591 Test MSE 774.2372186232703 Test RE 0.46841003749122634 Lambda1 -0.9124603\n",
      "25 Train Loss 767.8667 Test MSE 771.7553801998151 Test RE 0.4676586843413557 Lambda1 -0.8322697\n",
      "26 Train Loss 764.4562 Test MSE 766.2903532986521 Test RE 0.4659999282236552 Lambda1 -0.8098683\n",
      "27 Train Loss 760.56396 Test MSE 759.7175332821378 Test RE 0.4639970777335844 Lambda1 -0.7876415\n",
      "28 Train Loss 757.67883 Test MSE 755.6448271918331 Test RE 0.4627517049021353 Lambda1 -0.80278534\n",
      "29 Train Loss 750.9392 Test MSE 749.4544992211528 Test RE 0.460852349792351 Lambda1 -0.79949206\n",
      "30 Train Loss 748.24615 Test MSE 745.656019173398 Test RE 0.45968299116458705 Lambda1 -0.783053\n",
      "31 Train Loss 745.5119 Test MSE 744.4377321079686 Test RE 0.45930731205520076 Lambda1 -0.76085216\n",
      "32 Train Loss 744.7007 Test MSE 744.7054600513425 Test RE 0.45938989676484565 Lambda1 -0.75190735\n",
      "33 Train Loss 743.9929 Test MSE 745.1995225912267 Test RE 0.4595422588189831 Lambda1 -0.7594422\n",
      "34 Train Loss 743.38385 Test MSE 743.7999048037434 Test RE 0.4591105047803962 Lambda1 -0.7528305\n",
      "35 Train Loss 742.79987 Test MSE 742.6538517694095 Test RE 0.45875666780096547 Lambda1 -0.73153293\n",
      "36 Train Loss 742.4671 Test MSE 742.5081122824029 Test RE 0.4587116520498764 Lambda1 -0.7346793\n",
      "37 Train Loss 742.0682 Test MSE 741.0011484501155 Test RE 0.45824592450214635 Lambda1 -0.7329814\n",
      "38 Train Loss 740.31195 Test MSE 740.4533397655358 Test RE 0.4580765067418092 Lambda1 -0.7306057\n",
      "39 Train Loss 739.553 Test MSE 740.6403127336107 Test RE 0.4581343378794266 Lambda1 -0.72977597\n",
      "40 Train Loss 738.1408 Test MSE 739.6153080348934 Test RE 0.4578172119919157 Lambda1 -0.7148729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 Train Loss 736.59717 Test MSE 738.0625373893281 Test RE 0.45733638187274683 Lambda1 -0.69623893\n",
      "42 Train Loss 734.3116 Test MSE 737.6156053274092 Test RE 0.4571978914099573 Lambda1 -0.6748593\n",
      "43 Train Loss 732.75836 Test MSE 735.851296505897 Test RE 0.45665077635702706 Lambda1 -0.67325467\n",
      "44 Train Loss 732.1816 Test MSE 735.26055129492 Test RE 0.4564674387558834 Lambda1 -0.66349834\n",
      "45 Train Loss 731.00653 Test MSE 734.4510180942526 Test RE 0.45621608070640773 Lambda1 -0.6820408\n",
      "46 Train Loss 729.2417 Test MSE 731.6341334966635 Test RE 0.4553403636053796 Lambda1 -0.6913564\n",
      "47 Train Loss 727.3285 Test MSE 727.783933623939 Test RE 0.45414067656678275 Lambda1 -0.6957367\n",
      "48 Train Loss 726.5756 Test MSE 726.256959515894 Test RE 0.453664006813061 Lambda1 -0.7196737\n",
      "49 Train Loss 725.37195 Test MSE 723.7567865470864 Test RE 0.4528824540012172 Lambda1 -0.7314707\n",
      "50 Train Loss 724.85205 Test MSE 723.5645680895991 Test RE 0.45282231076729307 Lambda1 -0.7235927\n",
      "51 Train Loss 724.2667 Test MSE 722.08807971025 Test RE 0.45236006570081416 Lambda1 -0.727344\n",
      "52 Train Loss 723.61206 Test MSE 722.0732271781553 Test RE 0.4523554134103719 Lambda1 -0.7536192\n",
      "53 Train Loss 723.2367 Test MSE 721.7292735557983 Test RE 0.4522476626934014 Lambda1 -0.764373\n",
      "54 Train Loss 723.1086 Test MSE 721.6285157148549 Test RE 0.4522160933153665 Lambda1 -0.75548613\n",
      "55 Train Loss 721.7872 Test MSE 718.5945877170188 Test RE 0.45126447060744995 Lambda1 -0.75443906\n",
      "56 Train Loss 721.3756 Test MSE 718.7915780985203 Test RE 0.45132631957648883 Lambda1 -0.7383273\n",
      "57 Train Loss 720.46405 Test MSE 717.4586783970166 Test RE 0.45090766424075845 Lambda1 -0.7405062\n",
      "58 Train Loss 719.01385 Test MSE 713.466558492747 Test RE 0.4496514326785526 Lambda1 -0.728388\n",
      "59 Train Loss 717.5048 Test MSE 710.4639036634438 Test RE 0.448704246320674 Lambda1 -0.7221746\n",
      "60 Train Loss 715.84045 Test MSE 710.9741108992697 Test RE 0.4488653319619596 Lambda1 -0.699581\n",
      "61 Train Loss 714.9158 Test MSE 708.2653930097107 Test RE 0.44800945704045486 Lambda1 -0.71353513\n",
      "62 Train Loss 714.2779 Test MSE 707.9217528786104 Test RE 0.4479007599978844 Lambda1 -0.72330266\n",
      "63 Train Loss 713.7229 Test MSE 707.0281733296847 Test RE 0.4476179876878624 Lambda1 -0.7306679\n",
      "64 Train Loss 713.1503 Test MSE 706.7933563328274 Test RE 0.4475436504520058 Lambda1 -0.72867095\n",
      "65 Train Loss 712.59436 Test MSE 706.6589573389139 Test RE 0.44750109750814565 Lambda1 -0.71318924\n",
      "66 Train Loss 712.01746 Test MSE 704.5221057827348 Test RE 0.44682399054379945 Lambda1 -0.72287285\n",
      "67 Train Loss 711.7351 Test MSE 704.310295759117 Test RE 0.44675681812036216 Lambda1 -0.7197112\n",
      "68 Train Loss 711.3822 Test MSE 703.4829743488684 Test RE 0.44649434851833075 Lambda1 -0.7304824\n",
      "69 Train Loss 711.07043 Test MSE 702.2881264228203 Test RE 0.4461150077315822 Lambda1 -0.74638724\n",
      "70 Train Loss 710.1237 Test MSE 700.7385114093476 Test RE 0.4456225543816284 Lambda1 -0.7702107\n",
      "71 Train Loss 708.7202 Test MSE 698.5887477052752 Test RE 0.4449384767656935 Lambda1 -0.79222834\n",
      "72 Train Loss 707.99976 Test MSE 696.5339783570436 Test RE 0.44428364285327254 Lambda1 -0.80179554\n",
      "73 Train Loss 707.99976 Test MSE 696.5339783570436 Test RE 0.44428364285327254 Lambda1 -0.80179554\n",
      "74 Train Loss 707.99976 Test MSE 696.5339783570436 Test RE 0.44428364285327254 Lambda1 -0.80179554\n",
      "Training time: 154.63\n",
      "Training time: 154.63\n",
      "inv_HT_atanh_tune15\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 838.0906 Test MSE 858.1515102460278 Test RE 0.49314105075073505 Lambda1 -0.05711388\n",
      "1 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "2 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "3 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "4 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "5 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "6 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "7 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "8 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "9 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "10 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "11 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "12 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "13 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "14 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "15 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "16 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "17 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "18 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "19 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "20 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "21 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "22 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "23 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "24 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "25 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "26 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "27 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "28 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "29 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "30 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "31 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "32 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "33 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "34 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "35 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "36 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "37 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "38 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "39 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "40 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "41 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "43 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "44 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "45 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "46 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "47 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "48 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "49 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "50 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "51 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "52 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "53 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "54 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "55 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "56 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "57 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "58 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "59 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "60 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "61 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "62 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "63 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "64 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "65 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "66 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "67 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "68 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "69 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "70 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "71 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "72 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "73 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "74 Train Loss 838.0633 Test MSE 858.2824822865786 Test RE 0.49317868118356173 Lambda1 -0.057233866\n",
      "Training time: 135.56\n",
      "Training time: 135.56\n",
      "inv_HT_atanh_tune15\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 838.1572 Test MSE 858.0810754056932 Test RE 0.49312081246908435 Lambda1 -0.05773345\n",
      "1 Train Loss 838.0628 Test MSE 858.283416649652 Test RE 0.4931789496312072 Lambda1 -0.057998993\n",
      "2 Train Loss 838.0154 Test MSE 858.2898624803275 Test RE 0.4931808015497995 Lambda1 -0.058091234\n",
      "3 Train Loss 837.8837 Test MSE 857.9018751425061 Test RE 0.4930693185042264 Lambda1 -0.073407434\n",
      "4 Train Loss 835.68146 Test MSE 854.7222387861337 Test RE 0.4921547404275256 Lambda1 -1.028846\n",
      "5 Train Loss 831.18365 Test MSE 847.1902748327273 Test RE 0.48998146458174846 Lambda1 -0.9338023\n",
      "6 Train Loss 822.50574 Test MSE 836.9817719177652 Test RE 0.48702041965308956 Lambda1 -1.537969\n",
      "7 Train Loss 807.2927 Test MSE 818.2710191068892 Test RE 0.4815459732697943 Lambda1 -1.3504558\n",
      "8 Train Loss 791.8626 Test MSE 795.0261616224077 Test RE 0.47465698977921017 Lambda1 -1.3127692\n",
      "9 Train Loss 784.2601 Test MSE 787.1894504436654 Test RE 0.47231180799062916 Lambda1 -1.4282259\n",
      "10 Train Loss 771.6745 Test MSE 768.4466568281382 Test RE 0.4666551181185474 Lambda1 -1.5271002\n",
      "11 Train Loss 758.48895 Test MSE 756.8282381465649 Test RE 0.4631139195145305 Lambda1 -1.5376422\n",
      "12 Train Loss 740.01166 Test MSE 738.8233761895906 Test RE 0.45757204593804496 Lambda1 -1.4962007\n",
      "13 Train Loss 731.15155 Test MSE 729.2761639356019 Test RE 0.4546060176053979 Lambda1 -1.4514908\n",
      "14 Train Loss 727.97644 Test MSE 724.1886411008244 Test RE 0.4530175478529129 Lambda1 -1.4509087\n",
      "15 Train Loss 722.31696 Test MSE 712.7145782165801 Test RE 0.4494144081488959 Lambda1 -1.4443641\n",
      "16 Train Loss 713.68427 Test MSE 709.141262392956 Test RE 0.4482863846808192 Lambda1 -1.4659191\n",
      "17 Train Loss 707.6493 Test MSE 702.3239430933821 Test RE 0.4461263835116536 Lambda1 -1.5182353\n",
      "18 Train Loss 702.2768 Test MSE 696.9789119865161 Test RE 0.4444255204800979 Lambda1 -1.4640472\n",
      "19 Train Loss 698.02686 Test MSE 695.2024187554111 Test RE 0.4438587725854592 Lambda1 -1.5166497\n",
      "20 Train Loss 693.44214 Test MSE 688.8419877811817 Test RE 0.4418236674468048 Lambda1 -1.545922\n",
      "21 Train Loss 690.7329 Test MSE 685.6422029011502 Test RE 0.44079630100800304 Lambda1 -1.573209\n",
      "22 Train Loss 685.6244 Test MSE 676.6914541417243 Test RE 0.43790965093943135 Lambda1 -1.6107416\n",
      "23 Train Loss 680.6565 Test MSE 670.6023723528114 Test RE 0.43593497456887087 Lambda1 -1.6672559\n",
      "24 Train Loss 673.48975 Test MSE 665.9852981567361 Test RE 0.43443168394420245 Lambda1 -1.8510689\n",
      "25 Train Loss 664.0353 Test MSE 650.3618267269627 Test RE 0.4293057365663779 Lambda1 -1.9598098\n",
      "26 Train Loss 659.3415 Test MSE 645.8299795662484 Test RE 0.4278073790351223 Lambda1 -1.9959024\n",
      "27 Train Loss 653.00775 Test MSE 640.6406589990168 Test RE 0.42608517097833104 Lambda1 -2.0895445\n",
      "28 Train Loss 650.77673 Test MSE 639.0867520101857 Test RE 0.4255681114607885 Lambda1 -2.0634274\n",
      "29 Train Loss 648.76526 Test MSE 635.7572789216628 Test RE 0.42445811517753723 Lambda1 -2.0926015\n",
      "30 Train Loss 646.63196 Test MSE 637.1856533292861 Test RE 0.4249346688650256 Lambda1 -2.0788043\n",
      "31 Train Loss 644.85834 Test MSE 635.98687520928 Test RE 0.42453475229497173 Lambda1 -2.1387553\n",
      "32 Train Loss 639.37115 Test MSE 631.7890710883335 Test RE 0.4231313709998851 Lambda1 -2.1844368\n",
      "33 Train Loss 636.1418 Test MSE 627.7252970135033 Test RE 0.4217683494249133 Lambda1 -2.2643015\n",
      "34 Train Loss 634.7252 Test MSE 624.8552568253763 Test RE 0.4208030554325034 Lambda1 -2.3775053\n",
      "35 Train Loss 632.9363 Test MSE 624.0150797710411 Test RE 0.4205200554955404 Lambda1 -2.4725485\n",
      "36 Train Loss 632.5328 Test MSE 623.6421649886494 Test RE 0.4203943841889351 Lambda1 -2.5249348\n",
      "37 Train Loss 631.5345 Test MSE 623.2261682625684 Test RE 0.4202541500365278 Lambda1 -2.5757642\n",
      "38 Train Loss 630.0484 Test MSE 622.3233769585602 Test RE 0.41994965440615334 Lambda1 -2.628308\n",
      "39 Train Loss 629.7361 Test MSE 622.0637213722532 Test RE 0.41986203625277 Lambda1 -2.6545951\n",
      "40 Train Loss 629.4121 Test MSE 621.9595664937971 Test RE 0.41982688510304844 Lambda1 -2.6974435\n",
      "41 Train Loss 628.6456 Test MSE 620.8067452178485 Test RE 0.41943762358992764 Lambda1 -2.83694\n",
      "42 Train Loss 628.11127 Test MSE 619.6819066741468 Test RE 0.41905746219802903 Lambda1 -2.9288034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 Train Loss 627.9154 Test MSE 619.0158644300873 Test RE 0.41883219742298705 Lambda1 -2.9520278\n",
      "44 Train Loss 627.0499 Test MSE 617.8908244300494 Test RE 0.4184514177886795 Lambda1 -3.0592554\n",
      "45 Train Loss 626.5028 Test MSE 616.7392027439662 Test RE 0.418061282139028 Lambda1 -3.0879042\n",
      "46 Train Loss 625.9762 Test MSE 616.8731362676865 Test RE 0.4181066735952911 Lambda1 -3.098611\n",
      "47 Train Loss 624.7091 Test MSE 615.4877126794987 Test RE 0.4176369007895202 Lambda1 -3.0942705\n",
      "48 Train Loss 621.7083 Test MSE 613.4772968281045 Test RE 0.4169542627915976 Lambda1 -3.1740394\n",
      "49 Train Loss 620.8977 Test MSE 612.2159366930866 Test RE 0.4165253959819744 Lambda1 -3.2079918\n",
      "50 Train Loss 619.5416 Test MSE 609.8041138740634 Test RE 0.4157041360812925 Lambda1 -3.2584817\n",
      "51 Train Loss 618.35846 Test MSE 607.8286037828475 Test RE 0.4150302361475847 Lambda1 -3.3085194\n",
      "52 Train Loss 615.4427 Test MSE 603.6803102308243 Test RE 0.4136115674582639 Lambda1 -3.404835\n",
      "53 Train Loss 613.353 Test MSE 600.1264731224868 Test RE 0.41239231463284426 Lambda1 -3.4651623\n",
      "54 Train Loss 611.9287 Test MSE 599.0826506373626 Test RE 0.4120335138352937 Lambda1 -3.4425826\n",
      "55 Train Loss 609.64 Test MSE 596.5022466847606 Test RE 0.41114518877518436 Lambda1 -3.4417677\n",
      "56 Train Loss 607.59735 Test MSE 595.4717991530313 Test RE 0.4107899120990114 Lambda1 -3.4401872\n",
      "57 Train Loss 605.1404 Test MSE 594.1665236493175 Test RE 0.4103394389050685 Lambda1 -3.4486964\n",
      "58 Train Loss 603.3817 Test MSE 591.3660992733289 Test RE 0.4093712913113383 Lambda1 -3.4873185\n",
      "59 Train Loss 600.97656 Test MSE 587.0467354832525 Test RE 0.4078735184632179 Lambda1 -3.5165982\n",
      "60 Train Loss 599.0879 Test MSE 585.2292374555814 Test RE 0.40724164033984156 Lambda1 -3.5398047\n",
      "61 Train Loss 597.0756 Test MSE 581.9807701459175 Test RE 0.40610981713750766 Lambda1 -3.58596\n",
      "62 Train Loss 594.6921 Test MSE 577.7366733600752 Test RE 0.4046263289776418 Lambda1 -3.6229458\n",
      "63 Train Loss 593.55475 Test MSE 578.7391549405711 Test RE 0.404977228143072 Lambda1 -3.6322997\n",
      "64 Train Loss 590.1219 Test MSE 574.7732874417098 Test RE 0.40358726951747165 Lambda1 -3.6542807\n",
      "65 Train Loss 586.04004 Test MSE 573.5890017210157 Test RE 0.4031712714987164 Lambda1 -3.687916\n",
      "66 Train Loss 582.33405 Test MSE 568.1498256387856 Test RE 0.401255140759328 Lambda1 -3.744967\n",
      "67 Train Loss 580.3018 Test MSE 566.3340317397236 Test RE 0.40061342647874737 Lambda1 -3.768317\n",
      "68 Train Loss 577.53406 Test MSE 560.4516342190637 Test RE 0.3985274501998856 Lambda1 -3.7741585\n",
      "69 Train Loss 575.8297 Test MSE 556.9347970578779 Test RE 0.3972751014871104 Lambda1 -3.759346\n",
      "70 Train Loss 572.0842 Test MSE 555.4426680718199 Test RE 0.39674255865150687 Lambda1 -3.717918\n",
      "71 Train Loss 569.5937 Test MSE 556.6980838571651 Test RE 0.39719066588000984 Lambda1 -3.6600144\n",
      "72 Train Loss 568.3893 Test MSE 556.5140364382922 Test RE 0.397125003753673 Lambda1 -3.5967011\n",
      "73 Train Loss 567.593 Test MSE 554.5245274586276 Test RE 0.3964145174932861 Lambda1 -3.5946188\n",
      "74 Train Loss 566.70984 Test MSE 552.0999860275743 Test RE 0.3955469487436523 Lambda1 -3.594317\n",
      "Training time: 149.94\n",
      "Training time: 149.94\n",
      "inv_HT_atanh_tune15\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 838.9575 Test MSE 858.267407593299 Test RE 0.4931743501211499 Lambda1 0.006449462\n",
      "1 Train Loss 838.0611 Test MSE 858.309468512564 Test RE 0.49318643441574456 Lambda1 0.0065336297\n",
      "2 Train Loss 837.89044 Test MSE 857.8779516624352 Test RE 0.4930624435826339 Lambda1 -0.006284874\n",
      "3 Train Loss 837.54553 Test MSE 857.1489789618338 Test RE 0.4928529117674821 Lambda1 -1.6738529\n",
      "4 Train Loss 835.45734 Test MSE 855.061583420653 Test RE 0.4922524291792085 Lambda1 -2.1436574\n",
      "5 Train Loss 828.9437 Test MSE 843.7846215578907 Test RE 0.4889956250168734 Lambda1 -2.3549168\n",
      "6 Train Loss 816.2997 Test MSE 831.5371874893726 Test RE 0.4854337960356267 Lambda1 -2.3992739\n",
      "7 Train Loss 806.4291 Test MSE 820.6876695057093 Test RE 0.48225653876704627 Lambda1 -3.2334604\n",
      "8 Train Loss 789.30646 Test MSE 799.479798195977 Test RE 0.47598461745655096 Lambda1 -3.2656677\n",
      "9 Train Loss 773.7992 Test MSE 782.8403072678473 Test RE 0.47100526305020324 Lambda1 -3.5892954\n",
      "10 Train Loss 761.56964 Test MSE 769.3831674837946 Test RE 0.466939389262033 Lambda1 -3.7353911\n",
      "11 Train Loss 742.07904 Test MSE 745.3777281600949 Test RE 0.4595972025567485 Lambda1 -3.4733255\n",
      "12 Train Loss 729.48615 Test MSE 732.5375844074603 Test RE 0.4556214130598606 Lambda1 -3.552687\n",
      "13 Train Loss 720.78265 Test MSE 725.6144937900743 Test RE 0.45346330094774684 Lambda1 -3.6291723\n",
      "14 Train Loss 708.7463 Test MSE 707.5360716856555 Test RE 0.44777873349311276 Lambda1 -3.7126558\n",
      "15 Train Loss 699.4234 Test MSE 692.0082210315177 Test RE 0.44283791525671234 Lambda1 -3.7918007\n",
      "16 Train Loss 689.7962 Test MSE 683.3625898458075 Test RE 0.4400629144011978 Lambda1 -3.6659727\n",
      "17 Train Loss 683.1789 Test MSE 676.3926480428732 Test RE 0.43781295655850183 Lambda1 -3.582794\n",
      "18 Train Loss 681.21564 Test MSE 674.7899334185756 Test RE 0.4372939493020344 Lambda1 -3.5383492\n",
      "19 Train Loss 675.91394 Test MSE 671.696071858309 Test RE 0.43629031750063973 Lambda1 -3.484512\n",
      "20 Train Loss 672.0391 Test MSE 662.9913167712443 Test RE 0.433454075656453 Lambda1 -3.5363476\n",
      "21 Train Loss 663.0387 Test MSE 654.0895944848861 Test RE 0.4305343337310586 Lambda1 -3.5868313\n",
      "22 Train Loss 655.2981 Test MSE 646.467776103931 Test RE 0.4280185698608855 Lambda1 -3.6982963\n",
      "23 Train Loss 653.9499 Test MSE 643.679279170745 Test RE 0.42709445705213905 Lambda1 -3.728275\n",
      "24 Train Loss 652.12994 Test MSE 642.5447705517797 Test RE 0.42671790614188493 Lambda1 -3.7542067\n",
      "25 Train Loss 650.95306 Test MSE 643.0015064144702 Test RE 0.42686953973834274 Lambda1 -3.811208\n",
      "26 Train Loss 649.8875 Test MSE 642.0992864329804 Test RE 0.4265699561316201 Lambda1 -3.8140652\n",
      "27 Train Loss 647.72015 Test MSE 638.1888099132585 Test RE 0.4252690363786966 Lambda1 -3.8780098\n",
      "28 Train Loss 646.505 Test MSE 638.245169786819 Test RE 0.4252878141914702 Lambda1 -3.8576837\n",
      "29 Train Loss 644.1085 Test MSE 634.6391704847427 Test RE 0.4240847029768645 Lambda1 -3.794209\n",
      "30 Train Loss 642.3719 Test MSE 632.9272199084271 Test RE 0.42351232866199756 Lambda1 -3.7656422\n",
      "31 Train Loss 641.4567 Test MSE 632.0669103327165 Test RE 0.42322440013225004 Lambda1 -3.772177\n",
      "32 Train Loss 640.6261 Test MSE 630.5317533577823 Test RE 0.42271012635469124 Lambda1 -3.7945013\n",
      "33 Train Loss 639.89166 Test MSE 629.2594487064493 Test RE 0.42228343282481606 Lambda1 -3.8215618\n",
      "34 Train Loss 639.1827 Test MSE 629.8893467161981 Test RE 0.4224947359378433 Lambda1 -3.8211503\n",
      "35 Train Loss 638.34753 Test MSE 629.2983205885665 Test RE 0.42229647569477846 Lambda1 -3.840329\n",
      "36 Train Loss 637.9879 Test MSE 628.8342326833699 Test RE 0.4221407317284632 Lambda1 -3.8881958\n",
      "37 Train Loss 637.6087 Test MSE 627.9049552006406 Test RE 0.4218287012352111 Lambda1 -3.8969872\n",
      "38 Train Loss 637.3376 Test MSE 627.3433518665273 Test RE 0.421640015512799 Lambda1 -3.8884208\n",
      "39 Train Loss 636.52234 Test MSE 627.450946570281 Test RE 0.42167617138136615 Lambda1 -3.8527985\n",
      "40 Train Loss 636.01074 Test MSE 627.340386238238 Test RE 0.4216390189062104 Lambda1 -3.8031702\n",
      "41 Train Loss 634.7519 Test MSE 627.540395674303 Test RE 0.4217062272859232 Lambda1 -3.860276\n",
      "42 Train Loss 633.6637 Test MSE 626.4391067965054 Test RE 0.4213360325413296 Lambda1 -3.8848302\n",
      "43 Train Loss 633.1255 Test MSE 626.3627708481803 Test RE 0.42131036040085984 Lambda1 -3.9158247\n",
      "44 Train Loss 631.7839 Test MSE 624.4376722793709 Test RE 0.42066242268685644 Lambda1 -3.8177102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 Train Loss 630.1215 Test MSE 623.051624401181 Test RE 0.42019529666799654 Lambda1 -3.8392193\n",
      "46 Train Loss 629.3533 Test MSE 622.9061152489098 Test RE 0.42014622703294474 Lambda1 -3.8315978\n",
      "47 Train Loss 628.4928 Test MSE 621.8830386968735 Test RE 0.41980105592001726 Lambda1 -3.8199215\n",
      "48 Train Loss 628.19415 Test MSE 621.2126711348757 Test RE 0.4195747296864634 Lambda1 -3.8605707\n",
      "49 Train Loss 627.78577 Test MSE 621.2479416460592 Test RE 0.41958664058737916 Lambda1 -3.856101\n",
      "50 Train Loss 627.17365 Test MSE 621.3892138270741 Test RE 0.4196343450113334 Lambda1 -3.8497198\n",
      "51 Train Loss 626.5956 Test MSE 621.7376406159755 Test RE 0.41975197769128303 Lambda1 -3.8670197\n",
      "52 Train Loss 626.38794 Test MSE 621.5986864165101 Test RE 0.4197050691920445 Lambda1 -3.843568\n",
      "53 Train Loss 626.27594 Test MSE 621.3262786934945 Test RE 0.4196130939083953 Lambda1 -3.8531168\n",
      "54 Train Loss 625.88446 Test MSE 620.5594579602624 Test RE 0.41935407753358106 Lambda1 -3.8827133\n",
      "55 Train Loss 625.638 Test MSE 620.6374320248293 Test RE 0.4193804228856692 Lambda1 -3.8689399\n",
      "56 Train Loss 624.97015 Test MSE 619.2414028397338 Test RE 0.4189084912228196 Lambda1 -3.9645927\n",
      "57 Train Loss 624.067 Test MSE 618.4906596802907 Test RE 0.41865448037975933 Lambda1 -4.1178727\n",
      "58 Train Loss 623.307 Test MSE 618.8570907106008 Test RE 0.4187784800512896 Lambda1 -4.1223307\n",
      "59 Train Loss 623.00964 Test MSE 618.4120553462856 Test RE 0.4186278760149756 Lambda1 -4.2076206\n",
      "60 Train Loss 622.7128 Test MSE 618.3192889272865 Test RE 0.4185964761860955 Lambda1 -4.2013206\n",
      "61 Train Loss 622.512 Test MSE 618.2083275934231 Test RE 0.4185589145992137 Lambda1 -4.2350335\n",
      "62 Train Loss 622.3166 Test MSE 617.6374275046146 Test RE 0.41836560556108937 Lambda1 -4.2947145\n",
      "63 Train Loss 621.95245 Test MSE 616.744978116496 Test RE 0.4180632395741818 Lambda1 -4.459011\n",
      "64 Train Loss 621.4256 Test MSE 616.5557595630713 Test RE 0.41799910334884816 Lambda1 -4.5599446\n",
      "65 Train Loss 621.2281 Test MSE 616.6784243585678 Test RE 0.4180406820898928 Lambda1 -4.53327\n",
      "66 Train Loss 620.92993 Test MSE 616.582900576861 Test RE 0.4180083034855779 Lambda1 -4.5312285\n",
      "67 Train Loss 620.66156 Test MSE 616.3275062917426 Test RE 0.41792172308498826 Lambda1 -4.521073\n",
      "68 Train Loss 620.29565 Test MSE 615.6569787809931 Test RE 0.4176943242839208 Lambda1 -4.5782285\n",
      "69 Train Loss 619.64874 Test MSE 615.135102047507 Test RE 0.4175172523333631 Lambda1 -4.6301446\n",
      "70 Train Loss 619.54865 Test MSE 614.8190756733233 Test RE 0.41740998856785727 Lambda1 -4.646543\n",
      "71 Train Loss 619.19666 Test MSE 614.4078189952028 Test RE 0.41727036101863996 Lambda1 -4.7400527\n",
      "72 Train Loss 618.5965 Test MSE 613.2767925819959 Test RE 0.41688612014592985 Lambda1 -4.861265\n",
      "73 Train Loss 617.8394 Test MSE 612.214090570815 Test RE 0.41652476797039895 Lambda1 -5.0048604\n",
      "74 Train Loss 617.098 Test MSE 610.9556864044587 Test RE 0.4160964650783391 Lambda1 -5.1978784\n",
      "Training time: 143.18\n",
      "Training time: 143.18\n",
      "inv_HT_atanh_tune15\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 838.3712 Test MSE 858.0539694009874 Test RE 0.4931130237874509 Lambda1 0.0011771899\n",
      "1 Train Loss 838.06024 Test MSE 858.2872250119028 Test RE 0.49318004379309754 Lambda1 0.001303164\n",
      "2 Train Loss 837.8953 Test MSE 857.9250558116928 Test RE 0.49307597987185037 Lambda1 -0.0006889971\n",
      "3 Train Loss 837.7439 Test MSE 857.5498311425306 Test RE 0.492968141486557 Lambda1 -0.06988286\n",
      "4 Train Loss 829.77484 Test MSE 845.5296054640015 Test RE 0.48950099615632203 Lambda1 -0.28882635\n",
      "5 Train Loss 806.1721 Test MSE 809.691950353136 Test RE 0.4790149649790632 Lambda1 -0.080998704\n",
      "6 Train Loss 747.79987 Test MSE 755.0408526154339 Test RE 0.46256673299124634 Lambda1 -0.0027895602\n",
      "7 Train Loss 708.29926 Test MSE 714.6303040492816 Test RE 0.45001799971807765 Lambda1 -0.00016236925\n",
      "8 Train Loss 678.2716 Test MSE 685.2119385351115 Test RE 0.44065797178618715 Lambda1 -5.067124e-05\n",
      "9 Train Loss 665.1372 Test MSE 675.3230050222111 Test RE 0.4374666422806176 Lambda1 -1.1610679e-05\n",
      "10 Train Loss 658.0492 Test MSE 667.5251812720226 Test RE 0.4349336377896693 Lambda1 -1.2365504e-06\n",
      "11 Train Loss 657.6477 Test MSE 667.9074319105126 Test RE 0.4350581498465989 Lambda1 -1.7217006e-06\n",
      "12 Train Loss 656.27545 Test MSE 665.6711492602615 Test RE 0.43432920996134916 Lambda1 -2.4605888e-06\n",
      "13 Train Loss 652.98596 Test MSE 659.8027396370318 Test RE 0.43241049673576964 Lambda1 3.7055952e-07\n",
      "14 Train Loss 652.0475 Test MSE 659.7683746500172 Test RE 0.43239923581359857 Lambda1 2.1401338e-05\n",
      "15 Train Loss 651.18604 Test MSE 660.3545388247218 Test RE 0.4325912734139555 Lambda1 8.328523e-05\n",
      "16 Train Loss 650.86505 Test MSE 660.2470608411511 Test RE 0.4325560681363234 Lambda1 8.043483e-05\n",
      "17 Train Loss 649.8804 Test MSE 660.9305480207458 Test RE 0.43277990105499753 Lambda1 5.7853023e-05\n",
      "18 Train Loss 639.5971 Test MSE 645.8904840886592 Test RE 0.42782741811333014 Lambda1 -3.5150733e-05\n",
      "19 Train Loss 607.77606 Test MSE 617.166168886559 Test RE 0.4182059681988148 Lambda1 9.930937e-05\n",
      "20 Train Loss 578.3973 Test MSE 584.7490396374418 Test RE 0.40707452917547177 Lambda1 2.6054522e-07\n",
      "21 Train Loss 557.8812 Test MSE 557.0945799579928 Test RE 0.39733208590966107 Lambda1 -8.5613436e-05\n",
      "22 Train Loss 554.71967 Test MSE 560.8080521855389 Test RE 0.3986541513806956 Lambda1 -7.653631e-05\n",
      "23 Train Loss 512.68976 Test MSE 502.9550893648955 Test RE 0.3775320047032353 Lambda1 -1.5943095e-05\n",
      "24 Train Loss 470.6798 Test MSE 458.1985898696837 Test RE 0.36034296256047343 Lambda1 -0.00010280662\n",
      "25 Train Loss 421.01154 Test MSE 409.6611949117339 Test RE 0.34072310464657546 Lambda1 0.0009245484\n",
      "26 Train Loss 389.30606 Test MSE 378.17571377260094 Test RE 0.3273678214291695 Lambda1 0.0019380918\n",
      "27 Train Loss 291.765 Test MSE 300.89767331493607 Test RE 0.2920105790049817 Lambda1 0.0016613054\n",
      "28 Train Loss 273.92816 Test MSE 291.05250903029463 Test RE 0.2871936573907586 Lambda1 0.001871744\n",
      "29 Train Loss 266.6247 Test MSE 286.1957384221345 Test RE 0.28478738772650136 Lambda1 0.0018389304\n",
      "30 Train Loss 261.23068 Test MSE 283.561701932977 Test RE 0.28347382106527885 Lambda1 0.0012317229\n",
      "31 Train Loss 258.59155 Test MSE 282.7295615326888 Test RE 0.28305757420880584 Lambda1 0.0009053385\n",
      "32 Train Loss 257.34787 Test MSE 283.3763368386116 Test RE 0.28338115209511483 Lambda1 0.0008246598\n",
      "33 Train Loss 256.60718 Test MSE 283.4535906642603 Test RE 0.28341977703200955 Lambda1 0.0006362235\n",
      "34 Train Loss 255.71121 Test MSE 282.6756646032524 Test RE 0.2830305931935338 Lambda1 0.00048993155\n",
      "35 Train Loss 255.37262 Test MSE 282.0168395956116 Test RE 0.28270057467455556 Lambda1 0.00036530624\n",
      "36 Train Loss 255.00365 Test MSE 282.02919271707094 Test RE 0.28270676614223356 Lambda1 0.00029516587\n",
      "37 Train Loss 254.61821 Test MSE 283.09137631252224 Test RE 0.28323863357315576 Lambda1 0.0001923501\n",
      "38 Train Loss 254.36584 Test MSE 282.95702039468296 Test RE 0.28317141269314444 Lambda1 0.00010621266\n",
      "39 Train Loss 254.09079 Test MSE 282.83057987355767 Test RE 0.2831081374620469 Lambda1 7.206319e-05\n",
      "40 Train Loss 253.90982 Test MSE 283.1610981738884 Test RE 0.2832735104903438 Lambda1 4.0997496e-05\n",
      "41 Train Loss 253.72414 Test MSE 283.0784469851297 Test RE 0.28323216547284497 Lambda1 2.418642e-05\n",
      "42 Train Loss 253.43251 Test MSE 283.6729679877131 Test RE 0.28352943139630776 Lambda1 1.4277474e-05\n",
      "43 Train Loss 253.0457 Test MSE 283.7015676995559 Test RE 0.28354372365637454 Lambda1 5.8154283e-06\n",
      "44 Train Loss 252.69673 Test MSE 283.42970982068994 Test RE 0.2834078377831419 Lambda1 3.1707543e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 Train Loss 252.53484 Test MSE 283.7667171612938 Test RE 0.28357627839465865 Lambda1 1.04769815e-05\n",
      "46 Train Loss 252.3815 Test MSE 284.31020078989684 Test RE 0.2838477079386776 Lambda1 1.35327755e-05\n",
      "47 Train Loss 252.18137 Test MSE 285.1139553586265 Test RE 0.28424864831354835 Lambda1 9.184384e-06\n",
      "48 Train Loss 252.00708 Test MSE 284.9346498794558 Test RE 0.284159253608603 Lambda1 1.2790977e-05\n",
      "49 Train Loss 251.82913 Test MSE 285.0809887923013 Test RE 0.28423221458129666 Lambda1 5.5065793e-06\n",
      "50 Train Loss 251.76953 Test MSE 284.83344326512554 Test RE 0.28410878352681346 Lambda1 4.973216e-06\n",
      "51 Train Loss 251.65808 Test MSE 285.11185898982967 Test RE 0.2842476033084179 Lambda1 4.355146e-06\n",
      "52 Train Loss 251.38246 Test MSE 285.2054315158108 Test RE 0.284294243922349 Lambda1 1.6064466e-06\n",
      "53 Train Loss 251.26897 Test MSE 285.58131307453505 Test RE 0.28448152257491083 Lambda1 3.327531e-07\n",
      "54 Train Loss 251.07973 Test MSE 285.5557068548611 Test RE 0.28446876848470537 Lambda1 -3.3850853e-07\n",
      "55 Train Loss 250.87419 Test MSE 285.7354942397265 Test RE 0.2845583059149694 Lambda1 -6.024624e-07\n",
      "56 Train Loss 250.67891 Test MSE 285.6834490245472 Test RE 0.2845323893365102 Lambda1 -3.6151542e-07\n",
      "57 Train Loss 250.58528 Test MSE 285.9458304357588 Test RE 0.284663021468536 Lambda1 -8.8440686e-08\n",
      "58 Train Loss 250.53664 Test MSE 286.0736441635212 Test RE 0.28472663452597724 Lambda1 -2.3727608e-07\n",
      "59 Train Loss 250.37921 Test MSE 286.1701785245156 Test RE 0.2847746703830423 Lambda1 -2.749901e-07\n",
      "60 Train Loss 250.29361 Test MSE 286.1662144293573 Test RE 0.28477269799408256 Lambda1 -1.7957731e-07\n",
      "61 Train Loss 250.23167 Test MSE 286.2005724734738 Test RE 0.28478979284798817 Lambda1 -1.5828056e-07\n",
      "62 Train Loss 250.20691 Test MSE 286.0585419178219 Test RE 0.28471911885923906 Lambda1 -1.866392e-07\n",
      "63 Train Loss 250.19919 Test MSE 286.14242813394713 Test RE 0.28476086251536126 Lambda1 -1.6399106e-07\n",
      "64 Train Loss 250.15012 Test MSE 286.3341146542453 Test RE 0.28485622705233443 Lambda1 -4.280932e-08\n",
      "65 Train Loss 250.11424 Test MSE 286.4783800649119 Test RE 0.2849279784146514 Lambda1 -8.2594136e-07\n",
      "66 Train Loss 250.0392 Test MSE 286.84734796432025 Test RE 0.2851114049079514 Lambda1 -1.0543331e-07\n",
      "67 Train Loss 249.98962 Test MSE 286.77370158497735 Test RE 0.28507480221536197 Lambda1 -3.2730568e-07\n",
      "68 Train Loss 249.90155 Test MSE 287.0118153196952 Test RE 0.2851931292110662 Lambda1 7.458695e-08\n",
      "69 Train Loss 249.7964 Test MSE 287.25392034211336 Test RE 0.28531338930235717 Lambda1 1.7259083e-09\n",
      "70 Train Loss 249.74644 Test MSE 287.1280097714695 Test RE 0.28525085245503895 Lambda1 2.1800926e-07\n",
      "71 Train Loss 249.71208 Test MSE 287.19484277458 Test RE 0.2852840485578704 Lambda1 1.8555305e-08\n",
      "72 Train Loss 249.68028 Test MSE 287.2347297595997 Test RE 0.28530385867248126 Lambda1 1.14874865e-07\n",
      "73 Train Loss 249.53552 Test MSE 287.16797435226914 Test RE 0.28527070341645566 Lambda1 -7.2008206e-08\n",
      "74 Train Loss 249.27551 Test MSE 288.41610864463644 Test RE 0.28588997529058047 Lambda1 3.7846878e-08\n",
      "Training time: 149.16\n",
      "Training time: 149.16\n",
      "inv_HT_atanh_tune15\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 838.23193 Test MSE 858.0549421024376 Test RE 0.4931133032871295 Lambda1 -0.28742433\n",
      "1 Train Loss 838.06195 Test MSE 858.2874303543024 Test RE 0.49318010278894686 Lambda1 -0.2894081\n",
      "2 Train Loss 837.943 Test MSE 858.0378586900234 Test RE 0.4931083944523663 Lambda1 -0.29217955\n",
      "3 Train Loss 837.7983 Test MSE 857.6543324233417 Test RE 0.4929981771946735 Lambda1 -0.3218542\n",
      "4 Train Loss 836.8154 Test MSE 856.4594863451777 Test RE 0.492654645878303 Lambda1 -0.82180095\n",
      "5 Train Loss 832.39703 Test MSE 850.4242046240475 Test RE 0.4909157625868213 Lambda1 -0.74680084\n",
      "6 Train Loss 821.8085 Test MSE 836.033277389554 Test RE 0.4867443878665569 Lambda1 -0.80291224\n",
      "7 Train Loss 814.4613 Test MSE 827.7507283188879 Test RE 0.484327307652979 Lambda1 -0.80820626\n",
      "8 Train Loss 804.7622 Test MSE 817.8003307260767 Test RE 0.48140745516682354 Lambda1 -0.8204892\n",
      "9 Train Loss 797.2881 Test MSE 807.0898302528449 Test RE 0.47824463651137183 Lambda1 -0.8150995\n",
      "10 Train Loss 786.42694 Test MSE 795.2326363782496 Test RE 0.4747186219169163 Lambda1 -0.8606316\n",
      "11 Train Loss 778.59436 Test MSE 783.4979016578915 Test RE 0.4712030463010533 Lambda1 -0.82568014\n",
      "12 Train Loss 767.3815 Test MSE 772.7097758911885 Test RE 0.4679477613860354 Lambda1 -0.93484783\n",
      "13 Train Loss 754.85785 Test MSE 755.7309628143398 Test RE 0.46277807858253994 Lambda1 -0.9956414\n",
      "14 Train Loss 743.92566 Test MSE 736.281554845136 Test RE 0.45678426059047517 Lambda1 -0.9972751\n",
      "15 Train Loss 735.3898 Test MSE 721.6192279993729 Test RE 0.45221318318374487 Lambda1 -0.9678251\n",
      "16 Train Loss 729.0554 Test MSE 717.5679546989425 Test RE 0.450942001860101 Lambda1 -0.9710918\n",
      "17 Train Loss 717.6049 Test MSE 705.8948646169033 Test RE 0.44725909618000953 Lambda1 -0.9681706\n",
      "18 Train Loss 709.33777 Test MSE 693.4574383390075 Test RE 0.4433013726998997 Lambda1 -1.028119\n",
      "19 Train Loss 703.7555 Test MSE 683.0131171686457 Test RE 0.439950375586346 Lambda1 -1.0564263\n",
      "20 Train Loss 698.8825 Test MSE 679.0820910778381 Test RE 0.4386824994763493 Lambda1 -1.0586199\n",
      "21 Train Loss 691.45715 Test MSE 669.9258597486688 Test RE 0.4357150305802593 Lambda1 -1.0931207\n",
      "22 Train Loss 686.1808 Test MSE 667.8262479373326 Test RE 0.43503170844108147 Lambda1 -1.1088915\n",
      "23 Train Loss 683.3024 Test MSE 666.1013612407803 Test RE 0.43446953709546826 Lambda1 -1.1139102\n",
      "24 Train Loss 678.381 Test MSE 665.0624351657625 Test RE 0.43413058126914966 Lambda1 -1.113906\n",
      "25 Train Loss 675.2053 Test MSE 661.4335164706614 Test RE 0.43294454258939596 Lambda1 -1.1442553\n",
      "26 Train Loss 674.10425 Test MSE 661.3042031134855 Test RE 0.43290221917555 Lambda1 -1.1487311\n",
      "27 Train Loss 671.09674 Test MSE 656.0500974813871 Test RE 0.43117907133761285 Lambda1 -1.1585616\n",
      "28 Train Loss 668.79926 Test MSE 653.8457509452197 Test RE 0.4304540749997099 Lambda1 -1.1612383\n",
      "29 Train Loss 667.613 Test MSE 652.377233533047 Test RE 0.42997040997761055 Lambda1 -1.1804023\n",
      "30 Train Loss 665.79095 Test MSE 648.6127982736623 Test RE 0.4287280785398748 Lambda1 -1.1940197\n",
      "31 Train Loss 662.3387 Test MSE 649.0560082780536 Test RE 0.4288745327299859 Lambda1 -1.1875978\n",
      "32 Train Loss 660.77234 Test MSE 646.5750811257127 Test RE 0.42805409107163744 Lambda1 -1.2087486\n",
      "33 Train Loss 660.06335 Test MSE 646.2975661519757 Test RE 0.4279622191664306 Lambda1 -1.2167829\n",
      "34 Train Loss 658.974 Test MSE 646.1440592975915 Test RE 0.42791139193227684 Lambda1 -1.2215871\n",
      "35 Train Loss 656.43274 Test MSE 643.1799966828765 Test RE 0.4269287828125604 Lambda1 -1.250367\n",
      "36 Train Loss 654.96735 Test MSE 641.2022349069472 Test RE 0.42627187997754407 Lambda1 -1.253516\n",
      "37 Train Loss 653.65326 Test MSE 640.2684474507855 Test RE 0.4259613754766366 Lambda1 -1.2758515\n",
      "38 Train Loss 652.90796 Test MSE 640.1474609402774 Test RE 0.42592112828388945 Lambda1 -1.2764341\n",
      "39 Train Loss 651.9313 Test MSE 639.708853548989 Test RE 0.42577518990509267 Lambda1 -1.28267\n",
      "40 Train Loss 650.9098 Test MSE 638.9389247918194 Test RE 0.4255188895137469 Lambda1 -1.3193074\n",
      "41 Train Loss 647.8463 Test MSE 637.0593722379069 Test RE 0.4248925587889012 Lambda1 -1.3629925\n",
      "42 Train Loss 646.88947 Test MSE 636.2433770852722 Test RE 0.4246203538908958 Lambda1 -1.409707\n",
      "43 Train Loss 646.34155 Test MSE 635.8643279974755 Test RE 0.42449384889062813 Lambda1 -1.4436634\n",
      "44 Train Loss 644.73474 Test MSE 634.8127112885975 Test RE 0.4241426815678479 Lambda1 -1.4716063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 Train Loss 643.4809 Test MSE 632.8642660999086 Test RE 0.42349126591085356 Lambda1 -1.5248272\n",
      "46 Train Loss 642.4639 Test MSE 633.1579550911216 Test RE 0.42358951786095544 Lambda1 -1.5619498\n",
      "47 Train Loss 640.47424 Test MSE 631.1993114649782 Test RE 0.42293383350451486 Lambda1 -1.5751731\n",
      "48 Train Loss 639.9416 Test MSE 631.1470851918043 Test RE 0.4229163360877373 Lambda1 -1.5654938\n",
      "49 Train Loss 639.2401 Test MSE 630.4123932627268 Test RE 0.42267011480206257 Lambda1 -1.5796025\n",
      "50 Train Loss 638.3649 Test MSE 628.2563455919868 Test RE 0.4219467173610709 Lambda1 -1.6119076\n",
      "51 Train Loss 636.22363 Test MSE 625.5808215079177 Test RE 0.42104729699978843 Lambda1 -1.6655868\n",
      "52 Train Loss 635.31165 Test MSE 624.932108803109 Test RE 0.4208289322674827 Lambda1 -1.6865146\n",
      "53 Train Loss 634.9702 Test MSE 624.8561447416795 Test RE 0.4208033544119523 Lambda1 -1.6883386\n",
      "54 Train Loss 634.4269 Test MSE 624.2792404620621 Test RE 0.42060905423796496 Lambda1 -1.6969311\n",
      "55 Train Loss 633.5902 Test MSE 622.7579776277944 Test RE 0.42009626511901504 Lambda1 -1.7260388\n",
      "56 Train Loss 632.7609 Test MSE 621.9199272176842 Test RE 0.4198135065010637 Lambda1 -1.7462748\n",
      "57 Train Loss 631.9084 Test MSE 621.0255209364742 Test RE 0.41951152314500195 Lambda1 -1.7619259\n",
      "58 Train Loss 630.96356 Test MSE 618.9285227812734 Test RE 0.418802648273052 Lambda1 -1.796601\n",
      "59 Train Loss 630.38983 Test MSE 617.9757276156929 Test RE 0.4184801661020004 Lambda1 -1.8078597\n",
      "60 Train Loss 630.0439 Test MSE 617.81219592275 Test RE 0.4184247923268256 Lambda1 -1.8155353\n",
      "61 Train Loss 629.1325 Test MSE 616.4565421855757 Test RE 0.4179654693726902 Lambda1 -1.833085\n",
      "62 Train Loss 627.8417 Test MSE 613.836304402509 Test RE 0.41707624598971493 Lambda1 -1.8652376\n",
      "63 Train Loss 626.4857 Test MSE 612.629218145046 Test RE 0.41666596172716874 Lambda1 -1.8885564\n",
      "64 Train Loss 624.0935 Test MSE 610.0143404089387 Test RE 0.41577578573680735 Lambda1 -1.9051204\n",
      "65 Train Loss 622.2577 Test MSE 610.7424916398268 Test RE 0.41602385970766464 Lambda1 -1.8839244\n",
      "66 Train Loss 621.75757 Test MSE 610.1556428714773 Test RE 0.415823937670869 Lambda1 -1.8828449\n",
      "67 Train Loss 619.93994 Test MSE 608.5795038785825 Test RE 0.4152865173188214 Lambda1 -1.884056\n",
      "68 Train Loss 618.8051 Test MSE 607.6303805064201 Test RE 0.41496255640932433 Lambda1 -1.8721598\n",
      "69 Train Loss 618.3145 Test MSE 606.2679697257563 Test RE 0.41449708699174426 Lambda1 -1.8850694\n",
      "70 Train Loss 616.8988 Test MSE 605.9127938033314 Test RE 0.4143756547533364 Lambda1 -1.8729063\n",
      "71 Train Loss 615.8013 Test MSE 604.9902774632651 Test RE 0.4140600862999877 Lambda1 -1.8783132\n",
      "72 Train Loss 615.6077 Test MSE 605.0656949693525 Test RE 0.4140858936617827 Lambda1 -1.8736507\n",
      "73 Train Loss 615.04626 Test MSE 604.8670202992053 Test RE 0.41401790506700686 Lambda1 -1.8619827\n",
      "74 Train Loss 614.20197 Test MSE 604.1926398241071 Test RE 0.4137870415476048 Lambda1 -1.8733612\n",
      "Training time: 144.16\n",
      "Training time: 144.16\n",
      "inv_HT_atanh_tune15\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 838.1042 Test MSE 858.1294187583563 Test RE 0.49313470321802877 Lambda1 -0.011723194\n",
      "1 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "2 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "3 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "4 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "5 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "6 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "7 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "8 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "9 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "10 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "11 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "12 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "13 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "14 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "15 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "16 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "17 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "18 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "19 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "20 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "21 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "22 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "23 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "24 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "25 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "26 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "27 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "28 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "29 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "30 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "31 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "32 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "33 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "34 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "35 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "36 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "37 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "38 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "39 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "40 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "41 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "42 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "43 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "44 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "45 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "47 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "48 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "49 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "50 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "51 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "52 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "53 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "54 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "55 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "56 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "57 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "58 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "59 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "60 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "61 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "62 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "63 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "64 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "65 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "66 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "67 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "68 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "69 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "70 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "71 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "72 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "73 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "74 Train Loss 838.0633 Test MSE 858.2804892953513 Test RE 0.49317810858574246 Lambda1 -0.011720366\n",
      "Training time: 120.82\n",
      "Training time: 120.82\n",
      "inv_HT_atanh_tune15\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 838.1085 Test MSE 858.1225036583925 Test RE 0.49313271629008765 Lambda1 -0.017603887\n",
      "1 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "2 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "3 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "4 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "5 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "6 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "7 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "8 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "9 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "10 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "11 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "12 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "13 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "14 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "15 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "16 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "17 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "18 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "19 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "20 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "21 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "22 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "23 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "24 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "25 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "26 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "27 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "28 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "29 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "30 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "31 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "32 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "33 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "34 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "35 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "36 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "37 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "38 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "39 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "40 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "41 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "42 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "43 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "44 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "45 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "47 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "48 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "49 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "50 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "51 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "52 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "53 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "54 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "55 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "56 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "57 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "58 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "59 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "60 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "61 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "62 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "63 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "64 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "65 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "66 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "67 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "68 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "69 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "70 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "71 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "72 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "73 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "74 Train Loss 838.06256 Test MSE 858.2835867159232 Test RE 0.4931789984921657 Lambda1 -0.01766284\n",
      "Training time: 106.19\n",
      "Training time: 106.19\n",
      "inv_HT_atanh_tune16\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 29029.633 Test MSE 3590.846849646961 Test RE 1.0087594402990874 Lambda1 6.0633203e-05\n",
      "1 Train Loss 27268.162 Test MSE 3590.227610891901 Test RE 1.008672456657191 Lambda1 0.00012434216\n",
      "2 Train Loss 25995.814 Test MSE 3589.374926494719 Test RE 1.0085526689400224 Lambda1 0.0002582087\n",
      "3 Train Loss 23461.326 Test MSE 3587.9745901640035 Test RE 1.0083559145263294 Lambda1 0.0005124107\n",
      "4 Train Loss 21706.49 Test MSE 3588.2804167695163 Test RE 1.0083988880044943 Lambda1 0.0003296345\n",
      "5 Train Loss 20280.223 Test MSE 3590.582298702895 Test RE 1.008722280098595 Lambda1 -0.00021382044\n",
      "6 Train Loss 19001.059 Test MSE 3593.5990415467813 Test RE 1.0091459463113455 Lambda1 -0.00039947365\n",
      "7 Train Loss 17821.668 Test MSE 3595.5820852063393 Test RE 1.0094243447146238 Lambda1 6.764761e-05\n",
      "8 Train Loss 17115.326 Test MSE 3597.1244048051954 Test RE 1.0096408170404767 Lambda1 0.00026159483\n",
      "9 Train Loss 16156.724 Test MSE 3599.0287431714682 Test RE 1.0099080365084774 Lambda1 0.00028084798\n",
      "10 Train Loss 15216.841 Test MSE 3601.282926423265 Test RE 1.010224255354221 Lambda1 0.000119988326\n",
      "11 Train Loss 14438.182 Test MSE 3601.5711933527314 Test RE 1.0102646865592582 Lambda1 0.00012289755\n",
      "12 Train Loss 13837.072 Test MSE 3602.582353888329 Test RE 1.010406495236094 Lambda1 0.00043467165\n",
      "13 Train Loss 13437.926 Test MSE 3605.7027259669676 Test RE 1.0108439816685582 Lambda1 0.0005018372\n",
      "14 Train Loss 13135.031 Test MSE 3606.937007259037 Test RE 1.0110169797157629 Lambda1 0.0003809545\n",
      "15 Train Loss 12842.686 Test MSE 3608.8577072546473 Test RE 1.0112861280072956 Lambda1 9.446098e-05\n",
      "16 Train Loss 12424.533 Test MSE 3608.1637063876406 Test RE 1.0111888857146778 Lambda1 -0.00026710783\n",
      "17 Train Loss 12051.606 Test MSE 3607.6285121555993 Test RE 1.0111138887670763 Lambda1 -0.0005177136\n",
      "18 Train Loss 11772.795 Test MSE 3607.5910269810906 Test RE 1.0111086357485217 Lambda1 -0.00065106625\n",
      "19 Train Loss 11428.716 Test MSE 3607.8749354800725 Test RE 1.0111484208413144 Lambda1 -0.00037781862\n",
      "20 Train Loss 11086.697 Test MSE 3607.20462195774 Test RE 1.0110544849432566 Lambda1 -0.00029470355\n",
      "21 Train Loss 10740.3125 Test MSE 3606.920687009564 Test RE 1.0110146924470962 Lambda1 -1.4481873e-05\n",
      "22 Train Loss 10400.229 Test MSE 3607.312997733427 Test RE 1.0110696730185873 Lambda1 0.00013463537\n",
      "23 Train Loss 10273.294 Test MSE 3607.6772188380583 Test RE 1.0111207142809453 Lambda1 0.00025400307\n",
      "24 Train Loss 10110.179 Test MSE 3607.522471516232 Test RE 1.0110990285963612 Lambda1 0.00044520802\n",
      "25 Train Loss 9880.509 Test MSE 3607.563714105618 Test RE 1.0111048082170306 Lambda1 0.00081520434\n",
      "26 Train Loss 9580.432 Test MSE 3606.8201028158455 Test RE 1.0110005955462398 Lambda1 0.001117209\n",
      "27 Train Loss 9233.607 Test MSE 3604.078567667807 Test RE 1.010616292964185 Lambda1 0.0011869221\n",
      "28 Train Loss 8782.927 Test MSE 3603.666416096592 Test RE 1.0105585057941644 Lambda1 0.0013282899\n",
      "29 Train Loss 8601.577 Test MSE 3603.1062134977287 Test RE 1.0104799553071284 Lambda1 0.001178026\n",
      "30 Train Loss 8370.535 Test MSE 3603.551655482433 Test RE 1.010542414787854 Lambda1 0.0007556546\n",
      "31 Train Loss 8089.596 Test MSE 3604.8217440641715 Test RE 1.0107204842916406 Lambda1 0.0003930204\n",
      "32 Train Loss 7851.6943 Test MSE 3605.8062664899076 Test RE 1.0108584951449961 Lambda1 6.487903e-05\n",
      "33 Train Loss 7670.998 Test MSE 3606.055029438118 Test RE 1.0108933638794682 Lambda1 -0.00024572495\n",
      "34 Train Loss 7535.9077 Test MSE 3604.737097829254 Test RE 1.0107086176596884 Lambda1 -0.00023728904\n",
      "35 Train Loss 7375.1885 Test MSE 3607.0064193608628 Test RE 1.011026707703219 Lambda1 -0.00039861727\n",
      "36 Train Loss 7198.6514 Test MSE 3606.8037162250525 Test RE 1.0109982989426674 Lambda1 -0.00022968631\n",
      "37 Train Loss 6936.5625 Test MSE 3605.1200761961827 Test RE 1.0107623066312752 Lambda1 7.717006e-05\n",
      "38 Train Loss 6639.7446 Test MSE 3603.6936084619365 Test RE 1.0105623184978223 Lambda1 0.00012728776\n",
      "39 Train Loss 6531.551 Test MSE 3603.063958440424 Test RE 1.0104740301399153 Lambda1 2.7851946e-05\n",
      "40 Train Loss 6449.518 Test MSE 3603.5605935017943 Test RE 1.0105436680295072 Lambda1 0.0001669005\n",
      "41 Train Loss 6377.4746 Test MSE 3604.454414744872 Test RE 1.0106689870537302 Lambda1 0.00022428221\n",
      "42 Train Loss 6224.8545 Test MSE 3603.469071018552 Test RE 1.0105308351589652 Lambda1 0.00015173861\n",
      "43 Train Loss 6074.144 Test MSE 3604.103418170365 Test RE 1.010619777111244 Lambda1 -0.00011041707\n",
      "44 Train Loss 5990.093 Test MSE 3603.4619994013033 Test RE 1.0105298436018502 Lambda1 -0.0002113996\n",
      "45 Train Loss 5775.657 Test MSE 3605.318859438783 Test RE 1.0107901725324053 Lambda1 -0.0003283818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 Train Loss 5677.495 Test MSE 3604.8974476724857 Test RE 1.0107310971306909 Lambda1 -0.00033883558\n",
      "47 Train Loss 5610.203 Test MSE 3604.233236289162 Test RE 1.0106379779731052 Lambda1 -0.00041201225\n",
      "48 Train Loss 5546.6426 Test MSE 3602.5528094429164 Test RE 1.0104023521023897 Lambda1 -0.00041775688\n",
      "49 Train Loss 5506.9624 Test MSE 3600.297524294786 Test RE 1.010086034439889 Lambda1 -0.00039612717\n",
      "50 Train Loss 5456.238 Test MSE 3600.731560445168 Test RE 1.010146918386072 Lambda1 -0.00049038616\n",
      "51 Train Loss 5405.439 Test MSE 3600.4530073565743 Test RE 1.010107845078131 Lambda1 -0.00047918994\n",
      "52 Train Loss 5291.3203 Test MSE 3600.442437938984 Test RE 1.0101063624508835 Lambda1 -0.0003715352\n",
      "53 Train Loss 5159.974 Test MSE 3599.814866602387 Test RE 1.010018325850279 Lambda1 -0.00029573872\n",
      "54 Train Loss 5038.414 Test MSE 3600.918986023213 Test RE 1.010173208169586 Lambda1 -0.00012655262\n",
      "55 Train Loss 4909.7275 Test MSE 3600.516663784592 Test RE 1.010116774450767 Lambda1 0.000104483064\n",
      "56 Train Loss 4824.0117 Test MSE 3601.059904541518 Test RE 1.0101929740570723 Lambda1 0.00017996684\n",
      "57 Train Loss 4774.8706 Test MSE 3599.3371033201097 Test RE 1.0099512993919306 Lambda1 0.00014406769\n",
      "58 Train Loss 4725.689 Test MSE 3600.020776227373 Test RE 1.0100472119914403 Lambda1 0.0003237813\n",
      "59 Train Loss 4665.342 Test MSE 3598.3892381088376 Test RE 1.0098183081344658 Lambda1 0.0002627954\n",
      "60 Train Loss 4620.118 Test MSE 3597.640251937941 Test RE 1.009713208427483 Lambda1 0.00012621474\n",
      "61 Train Loss 4518.661 Test MSE 3598.3743938215935 Test RE 1.0098162252513347 Lambda1 4.0456816e-05\n",
      "62 Train Loss 4456.535 Test MSE 3598.1344376898264 Test RE 1.009782555098033 Lambda1 0.0002943009\n",
      "63 Train Loss 4392.643 Test MSE 3599.5797522509865 Test RE 1.0099853416983195 Lambda1 0.00029999806\n",
      "64 Train Loss 4350.221 Test MSE 3602.448526259689 Test RE 1.0103877279259066 Lambda1 0.00038321016\n",
      "65 Train Loss 4301.0938 Test MSE 3599.710171614866 Test RE 1.0100036383415045 Lambda1 -6.180753e-05\n",
      "66 Train Loss 4269.8604 Test MSE 3600.7748476552993 Test RE 1.010152990250882 Lambda1 -0.0002228717\n",
      "67 Train Loss 4229.476 Test MSE 3601.112984696807 Test RE 1.010200419226517 Lambda1 -0.00010712956\n",
      "68 Train Loss 4210.6294 Test MSE 3601.025328673374 Test RE 1.0101881243206283 Lambda1 7.292783e-06\n",
      "69 Train Loss 4184.9155 Test MSE 3600.464991922243 Test RE 1.0101095262129356 Lambda1 0.00035578312\n",
      "70 Train Loss 4162.656 Test MSE 3601.195622979285 Test RE 1.0102120101914538 Lambda1 0.00057993544\n",
      "71 Train Loss 4135.462 Test MSE 3600.6784082321747 Test RE 1.0101394627147167 Lambda1 0.00044446337\n",
      "72 Train Loss 4088.5334 Test MSE 3602.63278670467 Test RE 1.0104135675889039 Lambda1 0.0003370523\n",
      "73 Train Loss 4056.2793 Test MSE 3604.1136524328845 Test RE 1.0106212119952587 Lambda1 7.681738e-06\n",
      "74 Train Loss 4037.1133 Test MSE 3602.083475442192 Test RE 1.0103365332720564 Lambda1 -0.00012575282\n",
      "Training time: 145.19\n",
      "Training time: 145.19\n",
      "inv_HT_atanh_tune16\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 13834.322 Test MSE 3567.624742840845 Test RE 1.0054923118499883 Lambda1 -0.00012465462\n",
      "1 Train Loss 4965.0664 Test MSE 3539.82341424753 Test RE 1.0015669137856356 Lambda1 0.0005716516\n",
      "2 Train Loss 3902.2708 Test MSE 3557.099068138376 Test RE 1.0040079485148437 Lambda1 7.701074e-05\n",
      "3 Train Loss 3605.7314 Test MSE 3510.808146083653 Test RE 0.9974536394996619 Lambda1 -0.0020671384\n",
      "4 Train Loss 3071.8704 Test MSE 3046.480537657332 Test RE 0.9291554414796118 Lambda1 0.03679135\n",
      "5 Train Loss 838.0881 Test MSE 858.3099560180958 Test RE 0.49318657447657915 Lambda1 0.62795466\n",
      "6 Train Loss 838.06006 Test MSE 858.2883632634506 Test RE 0.4931803708180989 Lambda1 0.624348\n",
      "7 Train Loss 838.0549 Test MSE 858.2607747251786 Test RE 0.4931724444407548 Lambda1 0.60843325\n",
      "8 Train Loss 837.9958 Test MSE 857.9921390063367 Test RE 0.4930952568851156 Lambda1 0.5692093\n",
      "9 Train Loss 837.909 Test MSE 857.9469082309429 Test RE 0.493082259461396 Lambda1 0.5533567\n",
      "10 Train Loss 837.7928 Test MSE 857.6837581656791 Test RE 0.49300663439727704 Lambda1 0.5124341\n",
      "11 Train Loss 837.69055 Test MSE 857.4584589224477 Test RE 0.49294187782711796 Lambda1 0.47883675\n",
      "12 Train Loss 837.6237 Test MSE 857.320403203232 Test RE 0.4929021929979371 Lambda1 0.47149497\n",
      "13 Train Loss 837.2 Test MSE 856.1849785432573 Test RE 0.49257568803835716 Lambda1 0.53979295\n",
      "14 Train Loss 836.73615 Test MSE 855.8555179529814 Test RE 0.4924809071815419 Lambda1 0.5767242\n",
      "15 Train Loss 834.9705 Test MSE 852.2756953951488 Test RE 0.49144986769716703 Lambda1 0.6075377\n",
      "16 Train Loss 833.4769 Test MSE 849.4388301196063 Test RE 0.49063127157670544 Lambda1 0.64375633\n",
      "17 Train Loss 830.79254 Test MSE 845.4698489906058 Test RE 0.4894836984946569 Lambda1 0.6748703\n",
      "18 Train Loss 828.53613 Test MSE 841.9433485146042 Test RE 0.48846180029042385 Lambda1 0.6903403\n",
      "19 Train Loss 826.4654 Test MSE 836.9924745719969 Test RE 0.4870235334571577 Lambda1 0.7095316\n",
      "20 Train Loss 823.4269 Test MSE 834.5958245284197 Test RE 0.4863257603190444 Lambda1 0.76149356\n",
      "21 Train Loss 820.93933 Test MSE 832.3409739802689 Test RE 0.4856683561298708 Lambda1 0.77026653\n",
      "22 Train Loss 817.7962 Test MSE 826.1103032224229 Test RE 0.48384715301985565 Lambda1 0.8153911\n",
      "23 Train Loss 815.24414 Test MSE 818.5985788375094 Test RE 0.48164234677144785 Lambda1 0.8621487\n",
      "24 Train Loss 813.32733 Test MSE 816.746291177926 Test RE 0.4810971189615787 Lambda1 0.90478444\n",
      "25 Train Loss 812.01776 Test MSE 818.381881390066 Test RE 0.4815785929500225 Lambda1 0.92509556\n",
      "26 Train Loss 810.57007 Test MSE 817.9193013081099 Test RE 0.48144247058572104 Lambda1 0.9222723\n",
      "27 Train Loss 809.4434 Test MSE 815.439436167656 Test RE 0.4807120692282969 Lambda1 0.9418527\n",
      "28 Train Loss 807.13794 Test MSE 810.5213245570958 Test RE 0.4792602316957319 Lambda1 1.0078411\n",
      "29 Train Loss 805.3161 Test MSE 809.9860117127258 Test RE 0.4791019406522637 Lambda1 1.0326842\n",
      "30 Train Loss 804.3379 Test MSE 809.18729223002 Test RE 0.47886566346257364 Lambda1 1.0119776\n",
      "31 Train Loss 803.0029 Test MSE 805.8467462138998 Test RE 0.47787619712527707 Lambda1 1.0420504\n",
      "32 Train Loss 801.87646 Test MSE 804.1312780578841 Test RE 0.4773672801689731 Lambda1 1.064024\n",
      "33 Train Loss 801.20026 Test MSE 803.0769964216892 Test RE 0.4770542438282071 Lambda1 1.0861967\n",
      "34 Train Loss 800.09076 Test MSE 804.0822162369154 Test RE 0.47735271733208307 Lambda1 1.0902791\n",
      "35 Train Loss 799.3899 Test MSE 804.9401180913927 Test RE 0.4776073011285712 Lambda1 1.0744734\n",
      "36 Train Loss 798.0854 Test MSE 804.549606281505 Test RE 0.4774914329328626 Lambda1 1.0866318\n",
      "37 Train Loss 796.2206 Test MSE 801.7657841870383 Test RE 0.47666463305648693 Lambda1 1.1368141\n",
      "38 Train Loss 793.2159 Test MSE 798.0558677792719 Test RE 0.47556054730871367 Lambda1 1.19306\n",
      "39 Train Loss 791.4546 Test MSE 794.8439447768744 Test RE 0.4746025919115984 Lambda1 1.2131412\n",
      "40 Train Loss 788.7821 Test MSE 787.3156036287162 Test RE 0.47234965228077314 Lambda1 1.3036499\n",
      "41 Train Loss 785.4551 Test MSE 779.8018919405602 Test RE 0.4700903248887422 Lambda1 1.4102731\n",
      "42 Train Loss 783.3247 Test MSE 782.6946597026272 Test RE 0.47096144570953685 Lambda1 1.4431993\n",
      "43 Train Loss 779.87476 Test MSE 775.951134609581 Test RE 0.46892820661846774 Lambda1 1.5377717\n",
      "44 Train Loss 777.09296 Test MSE 769.173678601855 Test RE 0.46687581543160417 Lambda1 1.6002983\n",
      "45 Train Loss 773.414 Test MSE 765.6498141674738 Test RE 0.46580512374069466 Lambda1 1.5729929\n",
      "46 Train Loss 771.9391 Test MSE 763.0984157892689 Test RE 0.465028367742623 Lambda1 1.5540438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 Train Loss 771.3879 Test MSE 762.7395122656369 Test RE 0.46491899787286434 Lambda1 1.5264164\n",
      "48 Train Loss 769.43207 Test MSE 760.7039237879823 Test RE 0.46429819876139083 Lambda1 1.5599364\n",
      "49 Train Loss 766.8101 Test MSE 760.7257839678185 Test RE 0.4643048699306126 Lambda1 1.5604565\n",
      "50 Train Loss 766.4276 Test MSE 760.5700270327097 Test RE 0.46425733479563114 Lambda1 1.5734437\n",
      "51 Train Loss 764.9029 Test MSE 760.070316655554 Test RE 0.46410479635840446 Lambda1 1.5685775\n",
      "52 Train Loss 763.9125 Test MSE 759.4299265737891 Test RE 0.4639092415979879 Lambda1 1.5664884\n",
      "53 Train Loss 763.54285 Test MSE 759.4507042994821 Test RE 0.4639155877483714 Lambda1 1.5760611\n",
      "54 Train Loss 763.1599 Test MSE 759.3721324292006 Test RE 0.4638915890490408 Lambda1 1.5924779\n",
      "55 Train Loss 762.72565 Test MSE 759.5769225265658 Test RE 0.46395413677464586 Lambda1 1.6110907\n",
      "56 Train Loss 761.8635 Test MSE 758.4972208832469 Test RE 0.4636242753950393 Lambda1 1.6163586\n",
      "57 Train Loss 761.53815 Test MSE 757.8351473215773 Test RE 0.463421888109301 Lambda1 1.6128352\n",
      "58 Train Loss 761.0227 Test MSE 757.5998786103236 Test RE 0.4633499482329139 Lambda1 1.6379051\n",
      "59 Train Loss 760.25635 Test MSE 758.207936462115 Test RE 0.46353585577604395 Lambda1 1.6582941\n",
      "60 Train Loss 759.79047 Test MSE 757.4162798867934 Test RE 0.4632938001156036 Lambda1 1.6585046\n",
      "61 Train Loss 759.2767 Test MSE 757.5537307661833 Test RE 0.46333583595054184 Lambda1 1.6862565\n",
      "62 Train Loss 757.7422 Test MSE 755.5539463539142 Test RE 0.4627238766647536 Lambda1 1.6776748\n",
      "63 Train Loss 753.6968 Test MSE 752.6293270211223 Test RE 0.46182744608420634 Lambda1 1.7345006\n",
      "64 Train Loss 752.42145 Test MSE 751.3518809656777 Test RE 0.4614353472268831 Lambda1 1.7795554\n",
      "65 Train Loss 751.3493 Test MSE 752.0327454610975 Test RE 0.4616443729908552 Lambda1 1.7764881\n",
      "66 Train Loss 750.04425 Test MSE 752.589488224545 Test RE 0.4618152230067249 Lambda1 1.8218852\n",
      "67 Train Loss 749.7053 Test MSE 752.6856824385344 Test RE 0.46184473613016724 Lambda1 1.8403132\n",
      "68 Train Loss 749.3963 Test MSE 753.0351064692816 Test RE 0.4619519262414019 Lambda1 1.8607469\n",
      "69 Train Loss 748.87897 Test MSE 752.2945995261157 Test RE 0.46172473713421663 Lambda1 1.8809916\n",
      "70 Train Loss 748.20197 Test MSE 751.248893526282 Test RE 0.4614037217830472 Lambda1 1.8317873\n",
      "71 Train Loss 747.63275 Test MSE 750.8163399767815 Test RE 0.4612708693047455 Lambda1 1.8400478\n",
      "72 Train Loss 745.9634 Test MSE 749.8123977539242 Test RE 0.4609623756133291 Lambda1 1.8989904\n",
      "73 Train Loss 745.4196 Test MSE 748.6991045622789 Test RE 0.46062003871187845 Lambda1 1.8979756\n",
      "74 Train Loss 744.96893 Test MSE 747.2732591185313 Test RE 0.4601812202518106 Lambda1 1.8889325\n",
      "Training time: 149.89\n",
      "Training time: 149.89\n",
      "inv_HT_atanh_tune16\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 5243.4365 Test MSE 3529.6259758369633 Test RE 1.00012322832777 Lambda1 0.0002983971\n",
      "1 Train Loss 4094.336 Test MSE 3529.017175159913 Test RE 1.0000369724524794 Lambda1 -0.0007381973\n",
      "2 Train Loss 3737.0408 Test MSE 3520.2836400379388 Test RE 0.998798771010591 Lambda1 0.0004984556\n",
      "3 Train Loss 3606.8528 Test MSE 3501.915800798732 Test RE 0.9961896391148634 Lambda1 -2.3874774e-05\n",
      "4 Train Loss 3460.0273 Test MSE 3394.195532928558 Test RE 0.9807483799411667 Lambda1 0.00015728342\n",
      "5 Train Loss 3090.8496 Test MSE 3046.1145175775273 Test RE 0.9290996230096948 Lambda1 4.9462426e-05\n",
      "6 Train Loss 1204.0172 Test MSE 1205.228044955011 Test RE 0.5844182710161366 Lambda1 -0.06470167\n",
      "7 Train Loss 838.0611 Test MSE 858.1826746429916 Test RE 0.4931500050576114 Lambda1 -0.104148515\n",
      "8 Train Loss 837.9285 Test MSE 857.9784735845197 Test RE 0.4930913300533849 Lambda1 -0.101316646\n",
      "9 Train Loss 837.86707 Test MSE 857.9026649041687 Test RE 0.4930695454574284 Lambda1 -0.09446916\n",
      "10 Train Loss 837.6503 Test MSE 857.3568226407042 Test RE 0.4929126622635059 Lambda1 -0.06506817\n",
      "11 Train Loss 837.2614 Test MSE 856.1290033820668 Test RE 0.49255958611243655 Lambda1 -0.012305819\n",
      "12 Train Loss 835.7087 Test MSE 852.7102501745637 Test RE 0.4915751409358801 Lambda1 -0.0065959734\n",
      "13 Train Loss 829.8682 Test MSE 840.3917360386976 Test RE 0.48801150103291147 Lambda1 0.0020486298\n",
      "14 Train Loss 821.4427 Test MSE 830.1662221284814 Test RE 0.4850334606895122 Lambda1 -2.7671944e-05\n",
      "15 Train Loss 802.97473 Test MSE 811.4533264198925 Test RE 0.47953569829011167 Lambda1 0.0009275387\n",
      "16 Train Loss 783.15656 Test MSE 786.1949362240047 Test RE 0.472013360608693 Lambda1 5.150165e-05\n",
      "17 Train Loss 755.6919 Test MSE 756.2578706451601 Test RE 0.4629393786468514 Lambda1 0.0004012851\n",
      "18 Train Loss 742.48425 Test MSE 746.8127226863126 Test RE 0.46003939604132027 Lambda1 0.00017416668\n",
      "19 Train Loss 728.4575 Test MSE 733.4496276170055 Test RE 0.45590495970754935 Lambda1 6.695546e-05\n",
      "20 Train Loss 722.3196 Test MSE 728.0950766079875 Test RE 0.45423774355295465 Lambda1 -6.3598345e-06\n",
      "21 Train Loss 719.1821 Test MSE 725.5865669992232 Test RE 0.45345457462218214 Lambda1 -2.3986035e-05\n",
      "22 Train Loss 716.7571 Test MSE 724.4104644049967 Test RE 0.45308692353048363 Lambda1 5.336412e-05\n",
      "23 Train Loss 715.24097 Test MSE 723.4475101424581 Test RE 0.452785680591861 Lambda1 -1.034691e-05\n",
      "24 Train Loss 713.137 Test MSE 721.8573529369497 Test RE 0.45228778925699564 Lambda1 4.0728217e-05\n",
      "25 Train Loss 710.9955 Test MSE 720.6935066915568 Test RE 0.45192303173968235 Lambda1 5.529455e-05\n",
      "26 Train Loss 709.8105 Test MSE 719.7612763267318 Test RE 0.4516306517642255 Lambda1 6.19447e-05\n",
      "27 Train Loss 709.222 Test MSE 719.3785123519885 Test RE 0.4515105487955934 Lambda1 6.0601793e-05\n",
      "28 Train Loss 708.5567 Test MSE 719.0765393633513 Test RE 0.4514157737232652 Lambda1 5.2987456e-05\n",
      "29 Train Loss 707.3628 Test MSE 718.1160824132173 Test RE 0.451114199226961 Lambda1 6.416842e-05\n",
      "30 Train Loss 706.7606 Test MSE 717.7571245535852 Test RE 0.4510014380507701 Lambda1 5.7885543e-05\n",
      "31 Train Loss 706.20795 Test MSE 717.161321932342 Test RE 0.45081421342016514 Lambda1 5.4413547e-05\n",
      "32 Train Loss 705.5305 Test MSE 717.1927748325172 Test RE 0.45082409910317234 Lambda1 5.7671343e-05\n",
      "33 Train Loss 704.24567 Test MSE 716.5990436748096 Test RE 0.4506374521205096 Lambda1 3.7989463e-05\n",
      "34 Train Loss 703.9162 Test MSE 716.612718472206 Test RE 0.45064175183770333 Lambda1 3.339179e-05\n",
      "35 Train Loss 703.48425 Test MSE 716.4369725523572 Test RE 0.45058648955759356 Lambda1 2.1856167e-05\n",
      "36 Train Loss 703.0603 Test MSE 716.3236236628712 Test RE 0.4505508440643863 Lambda1 1.9368536e-05\n",
      "37 Train Loss 702.7951 Test MSE 716.1746356862603 Test RE 0.450503986647447 Lambda1 2.4905748e-05\n",
      "38 Train Loss 702.3585 Test MSE 715.8520597885396 Test RE 0.4504025183126719 Lambda1 2.6899334e-05\n",
      "39 Train Loss 702.17224 Test MSE 715.825912546854 Test RE 0.4503942925271322 Lambda1 2.7248981e-05\n",
      "40 Train Loss 702.05096 Test MSE 715.8014557131762 Test RE 0.45038659839913514 Lambda1 2.40079e-05\n",
      "41 Train Loss 701.7987 Test MSE 715.4436374117015 Test RE 0.45027401360950764 Lambda1 2.7965154e-05\n",
      "42 Train Loss 701.7328 Test MSE 715.2939753395699 Test RE 0.45022691523376956 Lambda1 2.9149338e-05\n",
      "43 Train Loss 701.6668 Test MSE 715.2537192307403 Test RE 0.450214245870308 Lambda1 2.6751099e-05\n",
      "44 Train Loss 701.4002 Test MSE 715.2283572080554 Test RE 0.45020626377610007 Lambda1 2.3307466e-05\n",
      "45 Train Loss 701.2795 Test MSE 715.3003015855462 Test RE 0.4502289061913418 Lambda1 1.912507e-05\n",
      "46 Train Loss 701.17883 Test MSE 715.1253641182292 Test RE 0.45017384769312857 Lambda1 1.7512124e-05\n",
      "47 Train Loss 701.062 Test MSE 714.9493690103341 Test RE 0.45011844952514646 Lambda1 1.97275e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 Train Loss 700.96234 Test MSE 714.7723533527967 Test RE 0.45006272323928326 Lambda1 2.021218e-05\n",
      "49 Train Loss 700.906 Test MSE 714.6599691809245 Test RE 0.45002734000534844 Lambda1 2.3999213e-05\n",
      "50 Train Loss 700.72986 Test MSE 714.2738251578234 Test RE 0.44990574452293264 Lambda1 2.3889615e-05\n",
      "51 Train Loss 700.5811 Test MSE 713.8789183931472 Test RE 0.4497813556820475 Lambda1 2.0142557e-05\n",
      "52 Train Loss 700.4737 Test MSE 713.7781639456184 Test RE 0.44974961425508125 Lambda1 1.8905388e-05\n",
      "53 Train Loss 700.3405 Test MSE 713.5055661315387 Test RE 0.44966372449568476 Lambda1 1.0397666e-05\n",
      "54 Train Loss 700.2063 Test MSE 713.1855545200589 Test RE 0.4495628747207915 Lambda1 9.156374e-06\n",
      "55 Train Loss 699.99603 Test MSE 713.0452209449764 Test RE 0.4495186422844818 Lambda1 2.1553053e-06\n",
      "56 Train Loss 699.6303 Test MSE 712.490038613612 Test RE 0.44934360892217906 Lambda1 -1.5638825e-05\n",
      "57 Train Loss 699.06976 Test MSE 711.9530818802785 Test RE 0.4491742566931709 Lambda1 -1.0283207e-05\n",
      "58 Train Loss 698.9593 Test MSE 711.9388577842176 Test RE 0.44916976964911554 Lambda1 -9.949264e-06\n",
      "59 Train Loss 698.7231 Test MSE 712.0251586366862 Test RE 0.4491969928853116 Lambda1 -1.2388763e-05\n",
      "60 Train Loss 698.3848 Test MSE 711.4166894135942 Test RE 0.44900501865543474 Lambda1 -1.1946381e-05\n",
      "61 Train Loss 697.832 Test MSE 710.68040063549 Test RE 0.4487726070852296 Lambda1 -1.0047165e-05\n",
      "62 Train Loss 696.2437 Test MSE 704.3871783741533 Test RE 0.446781201474806 Lambda1 -8.042911e-06\n",
      "63 Train Loss 685.3569 Test MSE 690.0211461915294 Test RE 0.4422016622119577 Lambda1 -2.2639042e-05\n",
      "64 Train Loss 681.8028 Test MSE 688.4611566718442 Test RE 0.4417015179097717 Lambda1 -2.4588326e-05\n",
      "65 Train Loss 678.6057 Test MSE 685.6445852630358 Test RE 0.4407970668122304 Lambda1 -9.41847e-06\n",
      "66 Train Loss 677.87177 Test MSE 685.2898313833354 Test RE 0.44068301741599497 Lambda1 -1.2010742e-06\n",
      "67 Train Loss 677.0311 Test MSE 685.2608681367042 Test RE 0.44067370475352796 Lambda1 3.1578463e-06\n",
      "68 Train Loss 675.91907 Test MSE 684.4547351345975 Test RE 0.440414426739767 Lambda1 4.7049866e-06\n",
      "69 Train Loss 674.7372 Test MSE 684.7078332625554 Test RE 0.4404958475835664 Lambda1 8.5992144e-07\n",
      "70 Train Loss 674.16583 Test MSE 684.429901081299 Test RE 0.44040643689471476 Lambda1 2.6589332e-06\n",
      "71 Train Loss 673.4157 Test MSE 683.7403729228672 Test RE 0.4401845374999542 Lambda1 6.4217625e-06\n",
      "72 Train Loss 672.7073 Test MSE 683.8881813290161 Test RE 0.44023211364258713 Lambda1 4.337343e-06\n",
      "73 Train Loss 672.1952 Test MSE 684.0695853712233 Test RE 0.44029049642505014 Lambda1 1.6973497e-06\n",
      "74 Train Loss 671.9015 Test MSE 683.7661852838182 Test RE 0.44019284627868316 Lambda1 3.2231508e-06\n",
      "Training time: 145.03\n",
      "Training time: 145.03\n",
      "inv_HT_atanh_tune16\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 21206.748 Test MSE 3521.1037817236265 Test RE 0.998915112318008 Lambda1 5.517224e-05\n",
      "1 Train Loss 14501.702 Test MSE 3489.5699618632166 Test RE 0.9944320789120206 Lambda1 -5.2538795e-05\n",
      "2 Train Loss 9108.096 Test MSE 3474.5960653045363 Test RE 0.9922962095094153 Lambda1 0.00025654587\n",
      "3 Train Loss 6969.411 Test MSE 3463.7574573242987 Test RE 0.9907473229833336 Lambda1 -1.6030022e-05\n",
      "4 Train Loss 5329.342 Test MSE 3459.681097568515 Test RE 0.9901641656036855 Lambda1 0.00031626382\n",
      "5 Train Loss 4195.1094 Test MSE 3472.1907101207494 Test RE 0.9919526820876969 Lambda1 -0.00037293273\n",
      "6 Train Loss 3723.3816 Test MSE 3472.6263735113776 Test RE 0.9920149113760355 Lambda1 -0.0001316168\n",
      "7 Train Loss 3576.0295 Test MSE 3460.107468373712 Test RE 0.9902251775978829 Lambda1 0.00056804833\n",
      "8 Train Loss 3517.153 Test MSE 3438.2143473281244 Test RE 0.9870874829826977 Lambda1 -0.0003088521\n",
      "9 Train Loss 3395.9043 Test MSE 3328.620614170555 Test RE 0.9712282805631318 Lambda1 6.590663e-05\n",
      "10 Train Loss 3170.1758 Test MSE 3129.8999494315212 Test RE 0.9417907013447998 Lambda1 -0.0017680128\n",
      "11 Train Loss 838.0728 Test MSE 858.2338053385139 Test RE 0.4931646958267981 Lambda1 0.03440667\n",
      "12 Train Loss 837.9553 Test MSE 857.8794819414383 Test RE 0.4930628833437716 Lambda1 0.034124345\n",
      "13 Train Loss 837.8564 Test MSE 857.7965832342561 Test RE 0.49303905991405983 Lambda1 0.03327974\n",
      "14 Train Loss 837.7663 Test MSE 857.6986981647365 Test RE 0.4930109282214916 Lambda1 -0.0011726264\n",
      "15 Train Loss 837.5488 Test MSE 857.2613862503696 Test RE 0.4928852272952535 Lambda1 -0.016911957\n",
      "16 Train Loss 836.4729 Test MSE 855.1351968968943 Test RE 0.49227361808146136 Lambda1 -0.021220705\n",
      "17 Train Loss 829.71313 Test MSE 845.0657468227353 Test RE 0.4893667072944626 Lambda1 -0.014807893\n",
      "18 Train Loss 818.00903 Test MSE 825.1333420824857 Test RE 0.4835609686561082 Lambda1 -0.0048548123\n",
      "19 Train Loss 799.972 Test MSE 806.6569765678664 Test RE 0.47811637463238954 Lambda1 0.00015511352\n",
      "20 Train Loss 786.1051 Test MSE 796.0944794386353 Test RE 0.4749757933033423 Lambda1 -0.0010894361\n",
      "21 Train Loss 778.9492 Test MSE 789.1710345256671 Test RE 0.4729059072262744 Lambda1 -0.00021420191\n",
      "22 Train Loss 771.97595 Test MSE 779.1406771126346 Test RE 0.4698909815588189 Lambda1 0.00014431919\n",
      "23 Train Loss 763.08545 Test MSE 771.0187643803802 Test RE 0.46743544843461593 Lambda1 0.00040709804\n",
      "24 Train Loss 753.9653 Test MSE 762.7630962060906 Test RE 0.4649261854755976 Lambda1 -0.00032867686\n",
      "25 Train Loss 745.387 Test MSE 757.3544820011988 Test RE 0.463274899570398 Lambda1 -0.00019409454\n",
      "26 Train Loss 743.48737 Test MSE 756.6126383941684 Test RE 0.4630479505447569 Lambda1 -0.000152478\n",
      "27 Train Loss 739.3427 Test MSE 753.0941391220931 Test RE 0.4619700327765523 Lambda1 -7.569407e-05\n",
      "28 Train Loss 729.76227 Test MSE 740.0751957492087 Test RE 0.4579595236709054 Lambda1 3.7736187e-05\n",
      "29 Train Loss 724.9096 Test MSE 736.8835288030276 Test RE 0.4569709527919842 Lambda1 6.814466e-05\n",
      "30 Train Loss 718.85406 Test MSE 732.474601169138 Test RE 0.4556018255810002 Lambda1 2.2805614e-05\n",
      "31 Train Loss 714.49774 Test MSE 729.0603680016713 Test RE 0.45453875269720867 Lambda1 3.9536617e-06\n",
      "32 Train Loss 711.7711 Test MSE 726.3000768561922 Test RE 0.45367747346201204 Lambda1 -1.0819075e-05\n",
      "33 Train Loss 708.9408 Test MSE 723.1857072092504 Test RE 0.45270374559034815 Lambda1 -1.21093435e-05\n",
      "34 Train Loss 707.72504 Test MSE 722.4974379890359 Test RE 0.45248827104435047 Lambda1 -7.2305825e-06\n",
      "35 Train Loss 706.7477 Test MSE 721.8828631361121 Test RE 0.4522957810365353 Lambda1 -3.032638e-07\n",
      "36 Train Loss 706.1882 Test MSE 721.8993683158437 Test RE 0.4523009516679966 Lambda1 2.42911e-06\n",
      "37 Train Loss 705.8279 Test MSE 721.4170945178688 Test RE 0.4521498438050913 Lambda1 1.8784062e-06\n",
      "38 Train Loss 704.80115 Test MSE 720.4090113591733 Test RE 0.45183382413425854 Lambda1 2.5275074e-06\n",
      "39 Train Loss 703.34784 Test MSE 718.053193611848 Test RE 0.4510944456988642 Lambda1 -4.429439e-06\n",
      "40 Train Loss 702.4436 Test MSE 716.7947011720972 Test RE 0.45069896809602195 Lambda1 3.3400956e-06\n",
      "41 Train Loss 699.8426 Test MSE 712.8409220729756 Test RE 0.4494542405271045 Lambda1 -2.3216075e-05\n",
      "42 Train Loss 694.0499 Test MSE 706.6192016254856 Test RE 0.44748850941636925 Lambda1 -1.2330897e-05\n",
      "43 Train Loss 690.3506 Test MSE 703.2828967879417 Test RE 0.44643085028439045 Lambda1 5.425385e-06\n",
      "44 Train Loss 689.02936 Test MSE 702.5428914115442 Test RE 0.44619591767086736 Lambda1 -6.767182e-06\n",
      "45 Train Loss 688.2664 Test MSE 702.9705876621829 Test RE 0.4463317152767684 Lambda1 -1.6578877e-06\n",
      "46 Train Loss 687.18005 Test MSE 702.6467098177277 Test RE 0.4462288847952597 Lambda1 -1.5007853e-06\n",
      "47 Train Loss 686.34375 Test MSE 702.6665348610715 Test RE 0.4462351798824343 Lambda1 -3.6419783e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 Train Loss 685.8164 Test MSE 702.5518467567751 Test RE 0.44619876150132837 Lambda1 -3.2049911e-06\n",
      "49 Train Loss 685.40436 Test MSE 702.2590766330451 Test RE 0.4461057809764075 Lambda1 -3.4650936e-06\n",
      "50 Train Loss 684.859 Test MSE 701.5752691102763 Test RE 0.4458885358070919 Lambda1 -2.7555454e-06\n",
      "51 Train Loss 684.42 Test MSE 701.1589783023019 Test RE 0.4457562286618561 Lambda1 -6.030751e-07\n",
      "52 Train Loss 684.0917 Test MSE 701.2207050814811 Test RE 0.44577584938383014 Lambda1 -6.8442796e-07\n",
      "53 Train Loss 684.03656 Test MSE 701.1413873974163 Test RE 0.4457506369880383 Lambda1 -4.852656e-07\n",
      "54 Train Loss 683.956 Test MSE 701.0828480676014 Test RE 0.44573202841015297 Lambda1 -6.300933e-07\n",
      "55 Train Loss 683.91833 Test MSE 701.0488753598534 Test RE 0.44572122875400255 Lambda1 -9.509908e-07\n",
      "56 Train Loss 683.8027 Test MSE 701.1609294757386 Test RE 0.44575684888290035 Lambda1 -9.267105e-07\n",
      "57 Train Loss 683.727 Test MSE 701.1160597508871 Test RE 0.4457425858896896 Lambda1 -1.5483424e-06\n",
      "58 Train Loss 683.6667 Test MSE 701.1229565210562 Test RE 0.4457447782347135 Lambda1 -1.0957096e-06\n",
      "59 Train Loss 683.6424 Test MSE 701.0707328308545 Test RE 0.44572817710185564 Lambda1 -6.7935e-07\n",
      "60 Train Loss 683.40533 Test MSE 700.8872467894728 Test RE 0.44566984472050125 Lambda1 3.647713e-07\n",
      "61 Train Loss 682.6906 Test MSE 700.7174287163687 Test RE 0.44561585074395793 Lambda1 7.7409567e-07\n",
      "62 Train Loss 682.2171 Test MSE 700.5731928260941 Test RE 0.44556998553196697 Lambda1 5.9841744e-07\n",
      "63 Train Loss 682.08 Test MSE 700.4086515201373 Test RE 0.44551765768573753 Lambda1 1.7678807e-06\n",
      "64 Train Loss 682.05664 Test MSE 700.3532732621928 Test RE 0.4455000447683295 Lambda1 2.033983e-06\n",
      "65 Train Loss 681.9986 Test MSE 700.2811770256224 Test RE 0.44547711369590565 Lambda1 1.7879441e-06\n",
      "66 Train Loss 681.9003 Test MSE 700.1712991288881 Test RE 0.4454421634429875 Lambda1 2.3322032e-06\n",
      "67 Train Loss 681.8215 Test MSE 700.131905118082 Test RE 0.44542963222366105 Lambda1 2.1580615e-06\n",
      "68 Train Loss 681.7882 Test MSE 700.0949147991278 Test RE 0.44541786529682803 Lambda1 1.8595435e-06\n",
      "69 Train Loss 681.74365 Test MSE 700.0641316684843 Test RE 0.44540807269101584 Lambda1 2.1226845e-06\n",
      "70 Train Loss 681.7078 Test MSE 700.029392121294 Test RE 0.4453970212272828 Lambda1 1.6370149e-06\n",
      "71 Train Loss 681.68945 Test MSE 700.0600378270304 Test RE 0.44540677035840437 Lambda1 1.342936e-06\n",
      "72 Train Loss 681.6214 Test MSE 700.1894079981238 Test RE 0.4454479237488897 Lambda1 4.2841506e-07\n",
      "73 Train Loss 681.44025 Test MSE 700.166438272575 Test RE 0.445440617225574 Lambda1 -2.0586577e-07\n",
      "74 Train Loss 681.30164 Test MSE 700.1298514770291 Test RE 0.4454289789515824 Lambda1 -4.6977146e-07\n",
      "Training time: 150.21\n",
      "Training time: 150.21\n",
      "inv_HT_atanh_tune16\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 9297.435 Test MSE 3586.437815530104 Test RE 1.0081399456394313 Lambda1 -7.828253e-05\n",
      "1 Train Loss 8395.378 Test MSE 3584.6193448805525 Test RE 1.0078843291417934 Lambda1 -0.000324166\n",
      "2 Train Loss 5119.573 Test MSE 3580.349969077322 Test RE 1.0072839419633706 Lambda1 -0.00039878642\n",
      "3 Train Loss 4124.67 Test MSE 3560.0203550988927 Test RE 1.0044201373989086 Lambda1 -0.00021433161\n",
      "4 Train Loss 3778.4941 Test MSE 3555.081269529662 Test RE 1.0037231415436076 Lambda1 8.55149e-05\n",
      "5 Train Loss 3638.3396 Test MSE 3539.8874762242413 Test RE 1.001575976676422 Lambda1 -4.4718152e-05\n",
      "6 Train Loss 3584.4807 Test MSE 3518.822711203847 Test RE 0.9985914971805366 Lambda1 -6.241847e-05\n",
      "7 Train Loss 883.2955 Test MSE 859.0489171941509 Test RE 0.493398833066835 Lambda1 -0.39368555\n",
      "8 Train Loss 838.06555 Test MSE 858.2862716752246 Test RE 0.4931797698948375 Lambda1 -0.54949796\n",
      "9 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "10 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "11 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "12 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "13 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "14 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "15 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "16 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "17 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "18 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "19 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "20 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "21 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "22 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "23 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "24 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "25 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "26 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "27 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "28 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "29 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "30 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "31 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "32 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "33 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "34 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "35 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "36 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "37 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "38 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "39 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "40 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "41 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "42 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "43 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "44 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "45 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "46 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "47 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "48 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "50 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "51 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "52 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "53 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "54 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "55 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "56 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "57 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "58 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "59 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "60 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "61 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "62 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "63 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "64 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "65 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "66 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "67 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "68 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "69 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "70 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "71 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "72 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "73 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "74 Train Loss 838.0631 Test MSE 858.283779505556 Test RE 0.4931790538816629 Lambda1 -0.5499497\n",
      "Training time: 131.10\n",
      "Training time: 131.10\n",
      "inv_HT_atanh_tune16\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 41271.977 Test MSE 3574.7082860551313 Test RE 1.0064900227702562 Lambda1 -0.00014780759\n",
      "1 Train Loss 29233.969 Test MSE 3575.969062663072 Test RE 1.0066674983999582 Lambda1 0.00044213375\n",
      "2 Train Loss 16769.848 Test MSE 3584.7059692755283 Test RE 1.0078965071214288 Lambda1 -0.00014562088\n",
      "3 Train Loss 11775.838 Test MSE 3570.872696672397 Test RE 1.0059499061723538 Lambda1 0.00034061808\n",
      "4 Train Loss 8671.719 Test MSE 3566.2832249862686 Test RE 1.0053032487037856 Lambda1 -0.00027781393\n",
      "5 Train Loss 6466.4946 Test MSE 3569.133374432499 Test RE 1.0057048842540917 Lambda1 -0.0002596323\n",
      "6 Train Loss 5359.054 Test MSE 3573.94224480509 Test RE 1.006382174223756 Lambda1 -0.00020574495\n",
      "7 Train Loss 4838.6323 Test MSE 3575.3357499591484 Test RE 1.0065783528377414 Lambda1 0.00034158002\n",
      "8 Train Loss 4464.0703 Test MSE 3575.079644362495 Test RE 1.0065423009835541 Lambda1 0.0005399417\n",
      "9 Train Loss 4190.301 Test MSE 3574.4612351167025 Test RE 1.0064552425055369 Lambda1 0.00029036115\n",
      "10 Train Loss 4051.49 Test MSE 3575.4218073957404 Test RE 1.0065904668093495 Lambda1 8.2233004e-05\n",
      "11 Train Loss 3981.7168 Test MSE 3575.2101090047354 Test RE 1.0065606665860358 Lambda1 6.7468245e-05\n",
      "12 Train Loss 3901.3484 Test MSE 3573.5543403447627 Test RE 1.0063275579624027 Lambda1 0.00010150659\n",
      "13 Train Loss 3834.7603 Test MSE 3569.0566105937705 Test RE 1.0056940689980398 Lambda1 0.00026676228\n",
      "14 Train Loss 3799.7104 Test MSE 3566.8497239369594 Test RE 1.0053830910193275 Lambda1 0.00036804087\n",
      "15 Train Loss 3750.057 Test MSE 3563.3779053214653 Test RE 1.0048936734950977 Lambda1 0.00011309302\n",
      "16 Train Loss 3713.8145 Test MSE 3560.329334779522 Test RE 1.0044637240429117 Lambda1 9.814669e-05\n",
      "17 Train Loss 3681.95 Test MSE 3554.8671704280378 Test RE 1.0036929172755622 Lambda1 1.8850612e-06\n",
      "18 Train Loss 3652.6045 Test MSE 3548.6871153708666 Test RE 1.0028200892024253 Lambda1 0.0001289731\n",
      "19 Train Loss 3637.1409 Test MSE 3545.165154112497 Test RE 1.0023223318421546 Lambda1 0.00023988957\n",
      "20 Train Loss 3621.6694 Test MSE 3538.1314575356378 Test RE 1.0013275218430917 Lambda1 2.7702881e-05\n",
      "21 Train Loss 3607.9893 Test MSE 3530.353582060382 Test RE 1.0002263070097164 Lambda1 0.0002155129\n",
      "22 Train Loss 3593.1775 Test MSE 3516.2858517616733 Test RE 0.9982314700879765 Lambda1 0.00018767234\n",
      "23 Train Loss 3576.8704 Test MSE 3497.1748252034845 Test RE 0.9955150783007579 Lambda1 7.65708e-05\n",
      "24 Train Loss 3560.7058 Test MSE 3478.1247600191305 Test RE 0.9927999546596616 Lambda1 8.87426e-06\n",
      "25 Train Loss 3534.82 Test MSE 3446.121701828248 Test RE 0.9882219044151846 Lambda1 5.2169315e-05\n",
      "26 Train Loss 3513.5444 Test MSE 3424.749779180894 Test RE 0.9851527949463682 Lambda1 -9.0826725e-06\n",
      "27 Train Loss 3493.104 Test MSE 3405.4683255331606 Test RE 0.982375659305113 Lambda1 -0.00016355579\n",
      "28 Train Loss 3472.7927 Test MSE 3378.890513595636 Test RE 0.9785346989488802 Lambda1 9.722401e-05\n",
      "29 Train Loss 3448.2854 Test MSE 3343.6738625758826 Test RE 0.973421929241363 Lambda1 -1.909738e-05\n",
      "30 Train Loss 3419.534 Test MSE 3318.564234447875 Test RE 0.9697600404579617 Lambda1 1.8188086e-05\n",
      "31 Train Loss 3381.6255 Test MSE 3277.9228494890094 Test RE 0.9638035784715806 Lambda1 5.8804486e-05\n",
      "32 Train Loss 3346.6904 Test MSE 3238.6569038021917 Test RE 0.958013528019817 Lambda1 0.00036722046\n",
      "33 Train Loss 3324.3723 Test MSE 3216.1061260571933 Test RE 0.9546723762115118 Lambda1 9.33203e-05\n",
      "34 Train Loss 3290.821 Test MSE 3171.207969346813 Test RE 0.9479851466800441 Lambda1 -0.00014369539\n",
      "35 Train Loss 3233.0327 Test MSE 3131.1994250504254 Test RE 0.9419861879533846 Lambda1 -4.2067884e-05\n",
      "36 Train Loss 3190.6255 Test MSE 3103.340716666765 Test RE 0.9377863356160611 Lambda1 0.00016082861\n",
      "37 Train Loss 3091.5134 Test MSE 3011.6606007007886 Test RE 0.9238302619587492 Lambda1 0.0005875285\n",
      "38 Train Loss 2721.6284 Test MSE 2672.5053130758265 Test RE 0.8702588716106285 Lambda1 0.0032964884\n",
      "39 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 77.77\n",
      "Training time: 77.77\n",
      "inv_HT_atanh_tune16\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 28099.068 Test MSE 3521.8504541325756 Test RE 0.999021019849064 Lambda1 0.00038642803\n",
      "1 Train Loss 15458.666 Test MSE 3511.287192818789 Test RE 0.9975216880216707 Lambda1 -0.0008493314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Train Loss 9260.535 Test MSE 3518.8694785097778 Test RE 0.9985981331043543 Lambda1 0.0005959645\n",
      "3 Train Loss 5852.5337 Test MSE 3493.267340017598 Test RE 0.9949587653036336 Lambda1 3.9995164e-05\n",
      "4 Train Loss 4637.7363 Test MSE 3490.827802014487 Test RE 0.9946112877991222 Lambda1 -0.000108085456\n",
      "5 Train Loss 4104.9062 Test MSE 3481.747248054203 Test RE 0.9933168237015628 Lambda1 0.0003645965\n",
      "6 Train Loss 3836.9238 Test MSE 3476.925070012033 Test RE 0.9926287194409322 Lambda1 -0.00028566748\n",
      "7 Train Loss 3684.5557 Test MSE 3460.5672447055554 Test RE 0.9902909655799962 Lambda1 0.0004555011\n",
      "8 Train Loss 3577.6372 Test MSE 3436.4384328127885 Test RE 0.9868325236878392 Lambda1 -6.074355e-06\n",
      "9 Train Loss 3518.7786 Test MSE 3410.8641079748504 Test RE 0.9831536121189065 Lambda1 4.8412916e-05\n",
      "10 Train Loss 3446.6824 Test MSE 3351.8111617504637 Test RE 0.9746056888438835 Lambda1 0.00037748538\n",
      "11 Train Loss 3366.392 Test MSE 3279.968982823063 Test RE 0.9641043425630822 Lambda1 -0.0008279857\n",
      "12 Train Loss 3182.0195 Test MSE 3081.530452662479 Test RE 0.9344851462186898 Lambda1 0.00022969366\n",
      "13 Train Loss 3060.3955 Test MSE 2985.457174605159 Test RE 0.9198025166613705 Lambda1 0.00020820467\n",
      "14 Train Loss 2811.205 Test MSE 2735.8629665987874 Test RE 0.8805141529554884 Lambda1 -7.354516e-06\n",
      "15 Train Loss 972.2901 Test MSE 970.5190399580363 Test RE 0.5244344354169381 Lambda1 -0.005557804\n",
      "16 Train Loss 838.01953 Test MSE 858.1651822893086 Test RE 0.49314497908906163 Lambda1 -0.0077455183\n",
      "17 Train Loss 837.6588 Test MSE 857.4810593262188 Test RE 0.49294837412553894 Lambda1 -0.00879783\n",
      "18 Train Loss 835.58936 Test MSE 850.9399247750737 Test RE 0.491064592299347 Lambda1 -0.01583878\n",
      "19 Train Loss 827.8732 Test MSE 842.4951983484635 Test RE 0.48862185465041164 Lambda1 -0.017498627\n",
      "20 Train Loss 791.455 Test MSE 800.9682867076388 Test RE 0.47642751055344185 Lambda1 -0.021023197\n",
      "21 Train Loss 783.61224 Test MSE 795.6152917238354 Test RE 0.47483282231749585 Lambda1 -0.021734757\n",
      "22 Train Loss 759.19525 Test MSE 767.6478886321166 Test RE 0.46641252080990575 Lambda1 -0.011113852\n",
      "23 Train Loss 737.5656 Test MSE 741.3076997086806 Test RE 0.4583407025868456 Lambda1 -0.0024538164\n",
      "24 Train Loss 724.4538 Test MSE 730.0894091539041 Test RE 0.45485942171839033 Lambda1 -0.002609681\n",
      "25 Train Loss 710.9161 Test MSE 719.6321125710144 Test RE 0.4515901265717707 Lambda1 -0.0009587081\n",
      "26 Train Loss 704.0589 Test MSE 710.7840970943045 Test RE 0.4488053464380458 Lambda1 -0.0006939347\n",
      "27 Train Loss 698.47296 Test MSE 705.7285753897598 Test RE 0.44720641216584894 Lambda1 -4.199682e-05\n",
      "28 Train Loss 696.38086 Test MSE 704.8153406682236 Test RE 0.4469169689966973 Lambda1 2.8958635e-05\n",
      "29 Train Loss 695.2247 Test MSE 705.3618367549378 Test RE 0.447090199511717 Lambda1 -0.0001748828\n",
      "30 Train Loss 692.1099 Test MSE 702.007473142054 Test RE 0.4460258590282253 Lambda1 5.991579e-06\n",
      "31 Train Loss 690.24945 Test MSE 700.7264179883556 Test RE 0.445618709063926 Lambda1 2.6433632e-05\n",
      "32 Train Loss 688.7082 Test MSE 699.1166806193226 Test RE 0.44510656801058635 Lambda1 1.9984374e-05\n",
      "33 Train Loss 683.50745 Test MSE 689.4823045373379 Test RE 0.4420289695252471 Lambda1 0.00011583226\n",
      "34 Train Loss 672.68225 Test MSE 678.6587657644401 Test RE 0.4385457454967963 Lambda1 -0.00012430293\n",
      "35 Train Loss 670.84534 Test MSE 678.5698619766539 Test RE 0.43851701997463066 Lambda1 -5.9995287e-05\n",
      "36 Train Loss 669.20905 Test MSE 677.9004583369432 Test RE 0.43830066987102323 Lambda1 -0.000111588946\n",
      "37 Train Loss 667.7018 Test MSE 677.2189262543358 Test RE 0.43808029010767097 Lambda1 -7.2377654e-05\n",
      "38 Train Loss 666.9814 Test MSE 676.6301530249837 Test RE 0.4378898154897214 Lambda1 -3.356947e-05\n",
      "39 Train Loss 665.8317 Test MSE 675.3901969701545 Test RE 0.4374884048331447 Lambda1 -0.00014519217\n",
      "40 Train Loss 665.61163 Test MSE 674.809230231661 Test RE 0.4373002018547612 Lambda1 -0.00024056228\n",
      "41 Train Loss 665.16907 Test MSE 673.5529543895857 Test RE 0.4368929566808616 Lambda1 -0.00028053718\n",
      "42 Train Loss 664.0774 Test MSE 671.4868448871456 Test RE 0.43622236205537734 Lambda1 -0.00013306864\n",
      "43 Train Loss 662.8355 Test MSE 670.9872451073593 Test RE 0.4360600527338505 Lambda1 -0.00024700176\n",
      "44 Train Loss 661.3722 Test MSE 668.7007907432626 Test RE 0.4353164601853306 Lambda1 -0.0002201666\n",
      "45 Train Loss 658.72125 Test MSE 665.86281096107 Test RE 0.43439173203705583 Lambda1 -0.0001734914\n",
      "46 Train Loss 654.5157 Test MSE 661.9395873902978 Test RE 0.4331101365185214 Lambda1 -7.193906e-05\n",
      "47 Train Loss 648.7991 Test MSE 654.700517033855 Test RE 0.4307353472827753 Lambda1 0.00020136204\n",
      "48 Train Loss 640.13086 Test MSE 639.4306952401917 Test RE 0.4256826120213035 Lambda1 0.00025892167\n",
      "49 Train Loss 631.70087 Test MSE 628.2734971508775 Test RE 0.42195247694840315 Lambda1 0.00066071696\n",
      "50 Train Loss 612.7588 Test MSE 597.5452315728626 Test RE 0.4115044757162615 Lambda1 -7.6977456e-05\n",
      "51 Train Loss 572.9316 Test MSE 555.705840324628 Test RE 0.39683653708902367 Lambda1 8.5073916e-05\n",
      "52 Train Loss 547.72437 Test MSE 526.6191036002334 Test RE 0.3863113564764614 Lambda1 5.3263902e-05\n",
      "53 Train Loss 533.2808 Test MSE 499.9057062952537 Test RE 0.3763857890568906 Lambda1 4.4860535e-05\n",
      "54 Train Loss 522.8379 Test MSE 489.75816828230825 Test RE 0.37254609423145474 Lambda1 -6.4012354e-05\n",
      "55 Train Loss 480.41 Test MSE 442.88151801863916 Test RE 0.3542688349856687 Lambda1 -0.00027411742\n",
      "56 Train Loss 463.03052 Test MSE 433.5982387514472 Test RE 0.35053624115919163 Lambda1 -0.0002638996\n",
      "57 Train Loss 441.82535 Test MSE 419.5961747583605 Test RE 0.34482991157274795 Lambda1 -4.7073023e-05\n",
      "58 Train Loss 428.3441 Test MSE 413.02893802814157 Test RE 0.34212074649326907 Lambda1 0.00017042528\n",
      "59 Train Loss 411.69348 Test MSE 406.9564067078166 Test RE 0.33959642963101294 Lambda1 0.00013952248\n",
      "60 Train Loss 397.27896 Test MSE 389.51506252005015 Test RE 0.3322395262790713 Lambda1 6.149916e-06\n",
      "61 Train Loss 389.0034 Test MSE 367.7982745088143 Test RE 0.32284496254708794 Lambda1 -0.0009659084\n",
      "62 Train Loss 370.16736 Test MSE 363.06968333661547 Test RE 0.3207629240702545 Lambda1 -0.0005876913\n",
      "63 Train Loss 364.0686 Test MSE 365.0179876516887 Test RE 0.3216224114723376 Lambda1 -0.0010159378\n",
      "64 Train Loss 358.10416 Test MSE 361.3775892974411 Test RE 0.32001458990461684 Lambda1 -0.0006923511\n",
      "65 Train Loss 352.6749 Test MSE 364.75693753835156 Test RE 0.32150738346477364 Lambda1 -0.00040558004\n",
      "66 Train Loss 347.3401 Test MSE 358.0920303550237 Test RE 0.31855652002802193 Lambda1 -0.00053278956\n",
      "67 Train Loss 341.70468 Test MSE 359.9501640403119 Test RE 0.31938194292162103 Lambda1 -0.00024129756\n",
      "68 Train Loss 338.76886 Test MSE 359.78392148243745 Test RE 0.319308181317513 Lambda1 -0.00017380418\n",
      "69 Train Loss 330.2068 Test MSE 354.8250580579612 Test RE 0.3171000510050136 Lambda1 -0.0006509768\n",
      "70 Train Loss 327.36096 Test MSE 350.0005241729838 Test RE 0.31493687803268205 Lambda1 -0.0009886398\n",
      "71 Train Loss 326.152 Test MSE 350.5121427569451 Test RE 0.3151669758593772 Lambda1 -0.00067884347\n",
      "72 Train Loss 323.74573 Test MSE 345.6464160883906 Test RE 0.3129717942772324 Lambda1 -0.00096962653\n",
      "73 Train Loss 321.6572 Test MSE 340.8790162156002 Test RE 0.31080593589572497 Lambda1 -0.001139648\n",
      "74 Train Loss 318.47577 Test MSE 339.84783598388833 Test RE 0.3103354761873924 Lambda1 -0.0012117916\n",
      "Training time: 146.12\n",
      "Training time: 146.12\n",
      "inv_HT_atanh_tune16\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 12794.537 Test MSE 3604.2089471632526 Test RE 1.0106345725949228 Lambda1 0.0020684325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 6048.042 Test MSE 3618.978343112284 Test RE 1.01270315437622 Lambda1 0.0005784652\n",
      "2 Train Loss 4626.682 Test MSE 3622.27723514428 Test RE 1.0131646157429166 Lambda1 0.00028873346\n",
      "3 Train Loss 4032.023 Test MSE 3622.3239726474853 Test RE 1.0131711520498659 Lambda1 -0.00014070912\n",
      "4 Train Loss 3800.923 Test MSE 3611.694041948049 Test RE 1.011683453532301 Lambda1 -4.8722395e-05\n",
      "5 Train Loss 3675.0264 Test MSE 3585.370488600404 Test RE 1.0079899228262823 Lambda1 9.4265626e-05\n",
      "6 Train Loss 3569.1436 Test MSE 3513.5059349597864 Test RE 0.9978367995126837 Lambda1 -0.00054642453\n",
      "7 Train Loss 2081.685 Test MSE 2060.2944550243637 Test RE 0.7641063651971718 Lambda1 -0.029016588\n",
      "8 Train Loss 838.0959 Test MSE 858.2776373221549 Test RE 0.4931772891963162 Lambda1 -0.11801452\n",
      "9 Train Loss 837.99896 Test MSE 858.1470547969645 Test RE 0.4931397705760467 Lambda1 -0.105090894\n",
      "10 Train Loss 837.6717 Test MSE 857.307094701426 Test RE 0.49289836723153824 Lambda1 -0.08912624\n",
      "11 Train Loss 837.5487 Test MSE 857.3302314923837 Test RE 0.49290501829602545 Lambda1 -0.064779505\n",
      "12 Train Loss 836.75793 Test MSE 855.129283318201 Test RE 0.49227191595072084 Lambda1 0.0008068301\n",
      "13 Train Loss 832.34906 Test MSE 845.9055389848273 Test RE 0.48960980333580095 Lambda1 0.045874268\n",
      "14 Train Loss 813.3343 Test MSE 818.8119901746218 Test RE 0.48170512555061773 Lambda1 0.0049986895\n",
      "15 Train Loss 778.71277 Test MSE 782.6681159538477 Test RE 0.47095345971644564 Lambda1 0.00024704912\n",
      "16 Train Loss 739.80164 Test MSE 750.5935889819116 Test RE 0.4612024396758733 Lambda1 -2.4232431e-05\n",
      "17 Train Loss 723.3434 Test MSE 732.9522330962895 Test RE 0.4557503457579522 Lambda1 0.00036201117\n",
      "18 Train Loss 709.32275 Test MSE 717.9406935230154 Test RE 0.4510591069847226 Lambda1 9.008846e-05\n",
      "19 Train Loss 703.2562 Test MSE 710.740621407648 Test RE 0.44879162045636656 Lambda1 0.00016743227\n",
      "20 Train Loss 698.46387 Test MSE 708.723001374988 Test RE 0.4481541625233252 Lambda1 8.093549e-05\n",
      "21 Train Loss 694.0326 Test MSE 706.9912138824951 Test RE 0.44760628806243746 Lambda1 3.242028e-05\n",
      "22 Train Loss 690.4064 Test MSE 704.2994962389743 Test RE 0.44675339294130373 Lambda1 6.164453e-05\n",
      "23 Train Loss 687.5747 Test MSE 701.8957939402094 Test RE 0.44599037949625037 Lambda1 3.0133346e-05\n",
      "24 Train Loss 686.99335 Test MSE 701.3593036867097 Test RE 0.4458199017469978 Lambda1 2.5761787e-05\n",
      "25 Train Loss 685.8036 Test MSE 699.9505725578841 Test RE 0.4453719458608804 Lambda1 1.4819404e-05\n",
      "26 Train Loss 685.02606 Test MSE 699.186323651177 Test RE 0.4451287372706011 Lambda1 1.4177599e-05\n",
      "27 Train Loss 683.45856 Test MSE 697.9153333781508 Test RE 0.444723972747141 Lambda1 -1.8984943e-05\n",
      "28 Train Loss 682.61847 Test MSE 697.4432103852486 Test RE 0.44457352474580464 Lambda1 -1.700937e-05\n",
      "29 Train Loss 682.28076 Test MSE 697.5460338782242 Test RE 0.4446062950970672 Lambda1 -1.6825656e-05\n",
      "30 Train Loss 681.79565 Test MSE 697.1900189215032 Test RE 0.44449282108602073 Lambda1 -1.8933393e-05\n",
      "31 Train Loss 680.8856 Test MSE 695.5975751497062 Test RE 0.4439849003943211 Lambda1 -2.8431701e-05\n",
      "32 Train Loss 680.6296 Test MSE 695.3729047148324 Test RE 0.4439131934619817 Lambda1 -2.2081087e-05\n",
      "33 Train Loss 680.502 Test MSE 694.9860354705588 Test RE 0.443789691199059 Lambda1 -2.5761985e-05\n",
      "34 Train Loss 680.31024 Test MSE 694.3810347339248 Test RE 0.44359648476459684 Lambda1 -1.6947299e-05\n",
      "35 Train Loss 679.9042 Test MSE 695.1517279492989 Test RE 0.4438425902704024 Lambda1 -4.9636797e-06\n",
      "36 Train Loss 679.38544 Test MSE 693.904491987092 Test RE 0.4434442420058835 Lambda1 8.274006e-06\n",
      "37 Train Loss 678.523 Test MSE 692.3229405014029 Test RE 0.4429386032740369 Lambda1 5.3678585e-05\n",
      "38 Train Loss 676.88934 Test MSE 691.1449394775316 Test RE 0.4425616084914816 Lambda1 4.1531413e-05\n",
      "39 Train Loss 676.0245 Test MSE 690.0446658297144 Test RE 0.442209198455255 Lambda1 4.319588e-05\n",
      "40 Train Loss 674.6529 Test MSE 689.7815933090972 Test RE 0.4421248965368031 Lambda1 -1.2835534e-05\n",
      "41 Train Loss 674.082 Test MSE 689.6840707845192 Test RE 0.44209364123758305 Lambda1 -5.1702814e-06\n",
      "42 Train Loss 673.70197 Test MSE 688.9539249432518 Test RE 0.44185956427026646 Lambda1 -9.277845e-06\n",
      "43 Train Loss 673.5217 Test MSE 688.2004747802697 Test RE 0.4416178861116929 Lambda1 -1.8026205e-05\n",
      "44 Train Loss 673.10864 Test MSE 688.4680299751659 Test RE 0.44170372278429765 Lambda1 -2.6144975e-05\n",
      "45 Train Loss 672.55365 Test MSE 687.2350227757611 Test RE 0.4413080123716903 Lambda1 -3.9650906e-05\n",
      "46 Train Loss 671.9123 Test MSE 686.8339280725507 Test RE 0.44117921208161004 Lambda1 2.8174213e-06\n",
      "47 Train Loss 671.7723 Test MSE 687.1095050785619 Test RE 0.44126770993287184 Lambda1 7.534549e-06\n",
      "48 Train Loss 671.6287 Test MSE 686.8915019897366 Test RE 0.4411977026381381 Lambda1 1.7228788e-06\n",
      "49 Train Loss 671.3779 Test MSE 684.3981996784427 Test RE 0.4403962374107899 Lambda1 3.0169287e-05\n",
      "50 Train Loss 671.148 Test MSE 683.5283967491404 Test RE 0.4401162982451161 Lambda1 2.8216344e-05\n",
      "51 Train Loss 670.4347 Test MSE 677.9397711698472 Test RE 0.4383133786625418 Lambda1 6.5677734e-05\n",
      "52 Train Loss 665.055 Test MSE 674.0388750724071 Test RE 0.43705052190809973 Lambda1 0.00010301668\n",
      "53 Train Loss 663.6734 Test MSE 673.0241377574865 Test RE 0.4367214173640478 Lambda1 7.759025e-05\n",
      "54 Train Loss 663.2767 Test MSE 673.4746868296692 Test RE 0.4368675722334625 Lambda1 7.356761e-05\n",
      "55 Train Loss 663.088 Test MSE 673.3493488258064 Test RE 0.4368269183986704 Lambda1 9.726855e-05\n",
      "56 Train Loss 662.9377 Test MSE 672.8105799366722 Test RE 0.43665212365773326 Lambda1 0.00012558447\n",
      "57 Train Loss 662.7073 Test MSE 672.3384408373333 Test RE 0.4364988883140244 Lambda1 0.0001177194\n",
      "58 Train Loss 662.2679 Test MSE 671.4552659744031 Test RE 0.43621210452764225 Lambda1 7.458299e-05\n",
      "59 Train Loss 661.6264 Test MSE 670.7797625430603 Test RE 0.43599262831272617 Lambda1 8.001164e-05\n",
      "60 Train Loss 661.30945 Test MSE 670.0435459076892 Test RE 0.43575330002145 Lambda1 0.000113053604\n",
      "61 Train Loss 660.9865 Test MSE 669.3331866042882 Test RE 0.4355222527278763 Lambda1 0.00016752216\n",
      "62 Train Loss 660.6904 Test MSE 668.9901709840348 Test RE 0.4354106415876554 Lambda1 0.00017775156\n",
      "63 Train Loss 660.3602 Test MSE 668.0539611399317 Test RE 0.4351058699619922 Lambda1 0.00024711655\n",
      "64 Train Loss 660.1074 Test MSE 667.6736798141303 Test RE 0.43498201305798023 Lambda1 0.00025608923\n",
      "65 Train Loss 659.9964 Test MSE 667.4634511636611 Test RE 0.4349135268469784 Lambda1 0.00020812954\n",
      "66 Train Loss 659.4682 Test MSE 665.5132063235479 Test RE 0.4342776805383047 Lambda1 0.00024794857\n",
      "67 Train Loss 658.84216 Test MSE 664.518135335996 Test RE 0.43395289451651037 Lambda1 5.4645836e-05\n",
      "68 Train Loss 657.8101 Test MSE 662.4032441369005 Test RE 0.43326179634348716 Lambda1 4.8857135e-05\n",
      "69 Train Loss 656.13165 Test MSE 661.3385031140924 Test RE 0.43291344574403195 Lambda1 9.431469e-06\n",
      "70 Train Loss 654.60175 Test MSE 658.9744798484528 Test RE 0.43213900597585775 Lambda1 -6.877952e-06\n",
      "71 Train Loss 653.47186 Test MSE 657.9458562250406 Test RE 0.4318016014156929 Lambda1 -2.401623e-05\n",
      "72 Train Loss 652.86224 Test MSE 657.608486541497 Test RE 0.43169088147771106 Lambda1 -7.0993796e-05\n",
      "73 Train Loss 652.1737 Test MSE 656.8512645709255 Test RE 0.4314422685253623 Lambda1 -5.2394542e-05\n",
      "74 Train Loss 651.4484 Test MSE 656.7427074286973 Test RE 0.43140661503915156 Lambda1 -4.730618e-05\n",
      "Training time: 145.71\n",
      "Training time: 145.71\n",
      "inv_HT_atanh_tune16\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 13662.727 Test MSE 3351.004558516952 Test RE 0.9744884138301478 Lambda1 -0.0008940826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 4536.0796 Test MSE 3336.2601913818094 Test RE 0.972342183567293 Lambda1 0.0012381155\n",
      "2 Train Loss 3513.3105 Test MSE 3334.589056937359 Test RE 0.9720986297146804 Lambda1 -0.0007176358\n",
      "3 Train Loss 3315.0137 Test MSE 3265.9606820368367 Test RE 0.9620433604981585 Lambda1 -0.0002447996\n",
      "4 Train Loss 864.83936 Test MSE 890.0391743051157 Test RE 0.5022196854605723 Lambda1 -0.051796317\n",
      "5 Train Loss 838.0653 Test MSE 858.2798726717459 Test RE 0.49317793142610633 Lambda1 -0.049890906\n",
      "6 Train Loss 838.0557 Test MSE 858.22291509376 Test RE 0.4931615669001318 Lambda1 -0.049182966\n",
      "7 Train Loss 838.0367 Test MSE 858.2813981017725 Test RE 0.4931783696911262 Lambda1 -0.047952116\n",
      "8 Train Loss 837.89935 Test MSE 857.9478360106967 Test RE 0.49308252606974273 Lambda1 -0.039732505\n",
      "9 Train Loss 837.7815 Test MSE 857.7174448067702 Test RE 0.4930163160394447 Lambda1 -0.0015546847\n",
      "10 Train Loss 837.46844 Test MSE 857.616755188621 Test RE 0.4929873769745589 Lambda1 0.06519331\n",
      "11 Train Loss 836.0789 Test MSE 854.6177151573891 Test RE 0.49212464680773066 Lambda1 0.05994095\n",
      "12 Train Loss 833.81714 Test MSE 850.199557676753 Test RE 0.4908509184708839 Lambda1 0.07180085\n",
      "13 Train Loss 827.25116 Test MSE 842.1587177040548 Test RE 0.48852427058342723 Lambda1 0.0343226\n",
      "14 Train Loss 805.2937 Test MSE 817.6228246077718 Test RE 0.48135520683496935 Lambda1 -0.008453257\n",
      "15 Train Loss 785.2592 Test MSE 807.1613855272133 Test RE 0.4782658362381176 Lambda1 0.0014545475\n",
      "16 Train Loss 770.2096 Test MSE 792.0222821263095 Test RE 0.473759433308938 Lambda1 -0.0006786647\n",
      "17 Train Loss 758.9113 Test MSE 781.8004311457344 Test RE 0.47069233212377193 Lambda1 0.0001262303\n",
      "18 Train Loss 744.60675 Test MSE 768.9966519396924 Test RE 0.46682208619880755 Lambda1 0.00015029305\n",
      "19 Train Loss 727.1977 Test MSE 755.3436955395242 Test RE 0.46265949023576575 Lambda1 -5.1850002e-05\n",
      "20 Train Loss 719.45703 Test MSE 750.331043170578 Test RE 0.4611217719477549 Lambda1 -5.9467457e-05\n",
      "21 Train Loss 709.5537 Test MSE 740.1510491779961 Test RE 0.4579829921713064 Lambda1 -8.070649e-05\n",
      "22 Train Loss 701.3429 Test MSE 730.2903226306547 Test RE 0.4549220038489762 Lambda1 -6.15338e-05\n",
      "23 Train Loss 696.5013 Test MSE 728.1570781470813 Test RE 0.45425708363680833 Lambda1 -5.9827325e-05\n",
      "24 Train Loss 692.58 Test MSE 725.0595278204412 Test RE 0.45328985841096164 Lambda1 -6.4684726e-05\n",
      "25 Train Loss 688.6683 Test MSE 720.0776053212884 Test RE 0.45172988478860776 Lambda1 -0.0001191798\n",
      "26 Train Loss 686.4836 Test MSE 719.2839034403129 Test RE 0.4514808576625756 Lambda1 -8.018117e-05\n",
      "27 Train Loss 684.9219 Test MSE 719.3183339316039 Test RE 0.4514916632162974 Lambda1 -9.686398e-05\n",
      "28 Train Loss 683.5539 Test MSE 719.0169223956339 Test RE 0.45139706041810035 Lambda1 -7.675867e-05\n",
      "29 Train Loss 681.87036 Test MSE 718.1641863571137 Test RE 0.4511293082109172 Lambda1 -2.4078243e-05\n",
      "30 Train Loss 680.4675 Test MSE 717.1961091557479 Test RE 0.4508251470721232 Lambda1 -2.7518789e-05\n",
      "31 Train Loss 679.6659 Test MSE 716.8384330595601 Test RE 0.45071271653466294 Lambda1 1.9226948e-06\n",
      "32 Train Loss 679.1838 Test MSE 716.4183337102745 Test RE 0.4505806282848166 Lambda1 1.5120583e-05\n",
      "33 Train Loss 678.6035 Test MSE 716.226707785165 Test RE 0.4505203641206105 Lambda1 2.7042073e-05\n",
      "34 Train Loss 678.0012 Test MSE 715.4703044977556 Test RE 0.4502824051747915 Lambda1 4.541661e-05\n",
      "35 Train Loss 676.12115 Test MSE 714.784225638696 Test RE 0.4500664609685829 Lambda1 4.050503e-05\n",
      "36 Train Loss 674.6662 Test MSE 714.5488269561537 Test RE 0.44999234505200203 Lambda1 1.6561897e-05\n",
      "37 Train Loss 673.83105 Test MSE 713.9918062793847 Test RE 0.4498169169362503 Lambda1 5.7815864e-06\n",
      "38 Train Loss 672.4994 Test MSE 712.2956877037634 Test RE 0.4492823194368984 Lambda1 -7.721178e-07\n",
      "39 Train Loss 671.6407 Test MSE 710.9719395177354 Test RE 0.44886464652305086 Lambda1 -2.1025271e-05\n",
      "40 Train Loss 670.77875 Test MSE 710.83899871386 Test RE 0.4488226791729065 Lambda1 -1.96471e-05\n",
      "41 Train Loss 669.9113 Test MSE 710.5031615352109 Test RE 0.44871664310138326 Lambda1 -3.7651866e-05\n",
      "42 Train Loss 668.7422 Test MSE 709.6265181827478 Test RE 0.44843973661088765 Lambda1 -2.2691145e-05\n",
      "43 Train Loss 667.69226 Test MSE 709.0624734234432 Test RE 0.4482614806135488 Lambda1 -3.0039382e-05\n",
      "44 Train Loss 666.4404 Test MSE 708.2620951791532 Test RE 0.44800841402676833 Lambda1 -9.957798e-06\n",
      "45 Train Loss 665.73193 Test MSE 707.218213816858 Test RE 0.44767814075409174 Lambda1 2.8728944e-06\n",
      "46 Train Loss 664.6518 Test MSE 706.8493507515726 Test RE 0.4475613780169533 Lambda1 2.6511165e-05\n",
      "47 Train Loss 663.8132 Test MSE 705.7959331574265 Test RE 0.4472277533074234 Lambda1 2.5684632e-05\n",
      "48 Train Loss 663.0091 Test MSE 705.5917169316375 Test RE 0.44716304779766264 Lambda1 4.947825e-05\n",
      "49 Train Loss 662.4104 Test MSE 704.5965040486202 Test RE 0.44684758245965567 Lambda1 4.1566127e-05\n",
      "50 Train Loss 659.2828 Test MSE 702.1882769459191 Test RE 0.4460832928756022 Lambda1 5.6401237e-05\n",
      "51 Train Loss 654.34015 Test MSE 698.4491228359677 Test RE 0.44489401027425507 Lambda1 4.2326486e-05\n",
      "52 Train Loss 650.3125 Test MSE 695.4632942242163 Test RE 0.44394204401946136 Lambda1 4.688872e-05\n",
      "53 Train Loss 648.42816 Test MSE 691.6373086285834 Test RE 0.44271922006930375 Lambda1 -5.685426e-05\n",
      "54 Train Loss 647.39307 Test MSE 690.582390068096 Test RE 0.4423814628823695 Lambda1 -9.763546e-06\n",
      "55 Train Loss 646.99304 Test MSE 689.5897059516226 Test RE 0.442063395853177 Lambda1 -1.9761685e-06\n",
      "56 Train Loss 646.4221 Test MSE 688.6868768483458 Test RE 0.44177392052313763 Lambda1 -5.7371974e-05\n",
      "57 Train Loss 646.115 Test MSE 687.1766914998319 Test RE 0.44128928325803596 Lambda1 -7.8644756e-05\n",
      "58 Train Loss 645.8796 Test MSE 687.2002635999727 Test RE 0.4412968519271286 Lambda1 -7.0396934e-05\n",
      "59 Train Loss 645.5768 Test MSE 686.6092374383543 Test RE 0.4411070425647331 Lambda1 -8.496704e-05\n",
      "60 Train Loss 644.71625 Test MSE 687.0532806798693 Test RE 0.44124965566421037 Lambda1 -7.237261e-05\n",
      "61 Train Loss 644.186 Test MSE 686.8804237334247 Test RE 0.44119414478298374 Lambda1 -4.265926e-06\n",
      "62 Train Loss 643.3385 Test MSE 684.4207940137647 Test RE 0.4404035068470216 Lambda1 -1.2474814e-05\n",
      "63 Train Loss 642.5194 Test MSE 683.3914363667539 Test RE 0.4400722024060613 Lambda1 1.3194236e-05\n",
      "64 Train Loss 642.1791 Test MSE 683.1576975609204 Test RE 0.43999693752673263 Lambda1 2.8007382e-05\n",
      "65 Train Loss 641.7543 Test MSE 683.0466488840827 Test RE 0.43996117487363096 Lambda1 2.4732733e-05\n",
      "66 Train Loss 641.38965 Test MSE 682.4054337311416 Test RE 0.4397546179629458 Lambda1 2.565029e-05\n",
      "67 Train Loss 641.0736 Test MSE 682.2183918479903 Test RE 0.43969434721716527 Lambda1 2.7518068e-05\n",
      "68 Train Loss 640.8358 Test MSE 682.0163571543446 Test RE 0.43962923603602067 Lambda1 3.639468e-05\n",
      "69 Train Loss 640.0567 Test MSE 680.8266871588689 Test RE 0.43924563673987793 Lambda1 4.4176108e-05\n",
      "70 Train Loss 639.22266 Test MSE 680.6086271060918 Test RE 0.43917528873560946 Lambda1 3.965071e-05\n",
      "71 Train Loss 638.7314 Test MSE 681.1888004350492 Test RE 0.4393624326408798 Lambda1 2.477572e-05\n",
      "72 Train Loss 638.2048 Test MSE 680.564568000799 Test RE 0.43916107352977946 Lambda1 2.9537756e-05\n",
      "73 Train Loss 637.8214 Test MSE 679.9481661425746 Test RE 0.4389621496973373 Lambda1 3.0750172e-05\n",
      "74 Train Loss 637.3235 Test MSE 679.3831146978318 Test RE 0.4387797183286729 Lambda1 2.8617249e-05\n",
      "Training time: 149.19\n",
      "Training time: 149.19\n",
      "inv_HT_atanh_tune16\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 14615.248 Test MSE 3679.0863192051606 Test RE 1.0210785649689704 Lambda1 0.0017520722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 10453.865 Test MSE 3657.7809902505196 Test RE 1.018117775398207 Lambda1 -0.002709911\n",
      "2 Train Loss 7001.117 Test MSE 3626.350922926357 Test RE 1.0137341685800263 Lambda1 0.0016701833\n",
      "3 Train Loss 4445.3247 Test MSE 3598.9665104147302 Test RE 1.0098993050371081 Lambda1 -0.00034431095\n",
      "4 Train Loss 3862.0088 Test MSE 3578.5870252914756 Test RE 1.0070359210112543 Lambda1 -0.00059649604\n",
      "5 Train Loss 3684.371 Test MSE 3553.4379575375146 Test RE 1.0034911326521425 Lambda1 0.00023411636\n",
      "6 Train Loss 3582.7766 Test MSE 3497.3403631372767 Test RE 0.9955386392559957 Lambda1 -0.0017407705\n",
      "7 Train Loss 3417.7341 Test MSE 3352.678896912422 Test RE 0.974731836299199 Lambda1 0.0018391719\n",
      "8 Train Loss 2822.2532 Test MSE 2797.9311868151 Test RE 0.8904462006139354 Lambda1 -0.019308453\n",
      "9 Train Loss 838.06915 Test MSE 858.3580876528123 Test RE 0.4932004025483055 Lambda1 -0.30889568\n",
      "10 Train Loss 838.05927 Test MSE 858.2564963012528 Test RE 0.49317121520863927 Lambda1 -0.3080052\n",
      "11 Train Loss 838.0237 Test MSE 858.2082076970346 Test RE 0.4931573412183414 Lambda1 -0.2725841\n",
      "12 Train Loss 837.88684 Test MSE 857.955978450085 Test RE 0.4930848658886033 Lambda1 -0.21833989\n",
      "13 Train Loss 837.562 Test MSE 857.3953916835231 Test RE 0.49292374922010945 Lambda1 0.045824107\n",
      "14 Train Loss 836.50635 Test MSE 855.7268171934779 Test RE 0.492443876953621 Lambda1 0.007828943\n",
      "15 Train Loss 832.5008 Test MSE 848.7510250994657 Test RE 0.4904325951218341 Lambda1 -0.009050565\n",
      "16 Train Loss 821.5103 Test MSE 835.0647789719346 Test RE 0.4864623729113633 Lambda1 0.0034703305\n",
      "17 Train Loss 807.8296 Test MSE 817.063719046235 Test RE 0.4811905989072131 Lambda1 -0.0023894785\n",
      "18 Train Loss 776.8801 Test MSE 769.064319023046 Test RE 0.46684262451589226 Lambda1 0.0007686768\n",
      "19 Train Loss 754.2295 Test MSE 746.5512256969812 Test RE 0.45995884743394183 Lambda1 0.00033398689\n",
      "20 Train Loss 732.1976 Test MSE 725.9440002713226 Test RE 0.45356624964760217 Lambda1 0.00023912232\n",
      "21 Train Loss 703.66144 Test MSE 706.0377778876763 Test RE 0.4473043692288911 Lambda1 -0.00034154393\n",
      "22 Train Loss 685.8049 Test MSE 697.1210905822069 Test RE 0.4444708479449402 Lambda1 -5.0398234e-05\n",
      "23 Train Loss 678.556 Test MSE 690.5864878951559 Test RE 0.442382775397767 Lambda1 -2.929385e-05\n",
      "24 Train Loss 666.7018 Test MSE 686.5061693577346 Test RE 0.44107393365791564 Lambda1 -2.6919137e-05\n",
      "25 Train Loss 652.8034 Test MSE 673.4337400053084 Test RE 0.43685429139900267 Lambda1 6.091408e-06\n",
      "26 Train Loss 643.1264 Test MSE 664.7955758333767 Test RE 0.43404347409333666 Lambda1 -1.2359984e-06\n",
      "27 Train Loss 613.50903 Test MSE 631.1453533387927 Test RE 0.4229157558510144 Lambda1 2.0260625e-06\n",
      "28 Train Loss 571.3431 Test MSE 589.9792059575731 Test RE 0.4088909732996074 Lambda1 1.0761771e-06\n",
      "29 Train Loss 561.89996 Test MSE 582.4692017726213 Test RE 0.4062801967171406 Lambda1 2.6304378e-06\n",
      "30 Train Loss 543.8828 Test MSE 566.8786872710434 Test RE 0.40080601942798183 Lambda1 -3.2045175e-06\n",
      "31 Train Loss 533.9768 Test MSE 556.160643545878 Test RE 0.396998894229762 Lambda1 1.2892798e-07\n",
      "32 Train Loss 514.47217 Test MSE 534.7952331769467 Test RE 0.3892986829320666 Lambda1 5.2133255e-06\n",
      "33 Train Loss 503.8097 Test MSE 525.6303976615317 Test RE 0.38594854417880303 Lambda1 1.1556958e-05\n",
      "34 Train Loss 494.71674 Test MSE 518.7236400044529 Test RE 0.38340448687157785 Lambda1 1.6774426e-05\n",
      "35 Train Loss 485.27893 Test MSE 511.49678195638927 Test RE 0.3807243233545806 Lambda1 2.7350375e-06\n",
      "36 Train Loss 473.225 Test MSE 496.64701977042745 Test RE 0.3751570286834274 Lambda1 -1.014745e-05\n",
      "37 Train Loss 421.7931 Test MSE 450.1730747447089 Test RE 0.3571732524966226 Lambda1 -7.626321e-06\n",
      "38 Train Loss 411.37888 Test MSE 440.5920512705735 Test RE 0.353351955835532 Lambda1 3.785627e-06\n",
      "39 Train Loss 404.76657 Test MSE 437.84047028818895 Test RE 0.35224685272423834 Lambda1 -3.6098097e-06\n",
      "40 Train Loss 402.45963 Test MSE 435.7803265896476 Test RE 0.3514171729427408 Lambda1 -4.427709e-06\n",
      "41 Train Loss 398.3201 Test MSE 432.4665228769279 Test RE 0.35007848258362667 Lambda1 1.6086242e-06\n",
      "42 Train Loss 395.17734 Test MSE 430.8397835750459 Test RE 0.34941944547667736 Lambda1 -3.1810735e-06\n",
      "43 Train Loss 391.05692 Test MSE 425.05652139335257 Test RE 0.3470663532077811 Lambda1 -4.397823e-06\n",
      "44 Train Loss 385.54846 Test MSE 418.8077379918752 Test RE 0.3445057851465468 Lambda1 2.2962936e-06\n",
      "45 Train Loss 384.03134 Test MSE 417.50543148499673 Test RE 0.34396973792776026 Lambda1 -1.7327168e-06\n",
      "46 Train Loss 383.2635 Test MSE 416.5412105418683 Test RE 0.34357231251267034 Lambda1 -4.9329756e-06\n",
      "47 Train Loss 382.23447 Test MSE 415.9283144802928 Test RE 0.3433194544181239 Lambda1 -5.136378e-06\n",
      "48 Train Loss 381.80728 Test MSE 415.2873660176772 Test RE 0.3430548235804373 Lambda1 -2.2913307e-06\n",
      "49 Train Loss 380.71594 Test MSE 415.90404600191636 Test RE 0.3433094383143805 Lambda1 -5.532928e-06\n",
      "50 Train Loss 376.8191 Test MSE 414.19119212077953 Test RE 0.3426017683573121 Lambda1 -2.8550367e-06\n",
      "51 Train Loss 375.5931 Test MSE 413.9005304346087 Test RE 0.34248153561677747 Lambda1 -5.227834e-07\n",
      "52 Train Loss 374.69037 Test MSE 412.9516675152666 Test RE 0.34208874258307087 Lambda1 -6.2022895e-07\n",
      "53 Train Loss 371.83603 Test MSE 412.37043423371165 Test RE 0.34184791127966335 Lambda1 8.962928e-07\n",
      "54 Train Loss 370.32513 Test MSE 413.9013373222268 Test RE 0.3424818694457478 Lambda1 4.1846351e-07\n",
      "55 Train Loss 369.54312 Test MSE 413.1386859936966 Test RE 0.3421661967735917 Lambda1 1.1521976e-06\n",
      "56 Train Loss 368.88547 Test MSE 412.30857043557006 Test RE 0.34182226831203244 Lambda1 1.8975685e-06\n",
      "57 Train Loss 368.68253 Test MSE 412.38572350821795 Test RE 0.34185424849212553 Lambda1 1.6308492e-06\n",
      "58 Train Loss 368.5181 Test MSE 412.12845943018124 Test RE 0.34174760011224004 Lambda1 1.5967763e-06\n",
      "59 Train Loss 367.58563 Test MSE 413.8239353922907 Test RE 0.3424498449090875 Lambda1 1.0217284e-06\n",
      "60 Train Loss 366.95428 Test MSE 415.2896982562113 Test RE 0.3430557868707045 Lambda1 4.3522775e-07\n",
      "61 Train Loss 365.55554 Test MSE 415.5500558620177 Test RE 0.34316330601840045 Lambda1 7.722942e-07\n",
      "62 Train Loss 362.9214 Test MSE 413.7229251704792 Test RE 0.3424080480929579 Lambda1 1.0130638e-06\n",
      "63 Train Loss 361.59677 Test MSE 412.38572030251845 Test RE 0.34185424716341556 Lambda1 1.7348265e-06\n",
      "64 Train Loss 360.89612 Test MSE 409.56755948787776 Test RE 0.3406841632286797 Lambda1 1.3288613e-06\n",
      "65 Train Loss 358.91135 Test MSE 403.88059089088 Test RE 0.3383106441973144 Lambda1 1.8506531e-06\n",
      "66 Train Loss 357.74405 Test MSE 398.7956584308858 Test RE 0.336174201081363 Lambda1 6.4860483e-06\n",
      "67 Train Loss 356.2252 Test MSE 399.614064241179 Test RE 0.3365189715231457 Lambda1 2.8481923e-06\n",
      "68 Train Loss 354.63016 Test MSE 395.96026534387755 Test RE 0.3349769885524628 Lambda1 3.7568338e-07\n",
      "69 Train Loss 353.43597 Test MSE 399.85944842151895 Test RE 0.33662227589441296 Lambda1 2.577259e-07\n",
      "70 Train Loss 351.76083 Test MSE 399.241280155975 Test RE 0.33636197231013854 Lambda1 2.039371e-06\n",
      "71 Train Loss 350.03204 Test MSE 392.53974645717165 Test RE 0.33352699411868186 Lambda1 -1.7276149e-06\n",
      "72 Train Loss 349.41333 Test MSE 392.3370947886126 Test RE 0.33344089006403616 Lambda1 5.3750655e-06\n",
      "73 Train Loss 348.3415 Test MSE 387.52818352844446 Test RE 0.3313910819514193 Lambda1 1.3912459e-06\n",
      "74 Train Loss 346.622 Test MSE 383.73026945337614 Test RE 0.3297632084428765 Lambda1 1.5649904e-06\n",
      "Training time: 146.43\n",
      "Training time: 146.43\n",
      "inv_HT_atanh_tune17\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 2185400.5 Test MSE 3510.7871822495235 Test RE 0.997450661483918 Lambda1 -2.6884994e-05\n",
      "1 Train Loss 2106349.2 Test MSE 3510.9328221623546 Test RE 0.9974713501661591 Lambda1 3.5125395e-05\n",
      "2 Train Loss 2063495.4 Test MSE 3510.909032756583 Test RE 0.99746797082339 Lambda1 1.0935667e-05\n",
      "3 Train Loss 1948009.9 Test MSE 3511.3887741330927 Test RE 0.9975361170366874 Lambda1 4.528471e-06\n",
      "4 Train Loss 1889269.5 Test MSE 3511.7186439225475 Test RE 0.9975829716179214 Lambda1 7.426721e-06\n",
      "5 Train Loss 1834414.9 Test MSE 3512.3210034517197 Test RE 0.9976685248618201 Lambda1 3.2391097e-06\n",
      "6 Train Loss 1775188.6 Test MSE 3513.092497764235 Test RE 0.9977780896382129 Lambda1 1.3685619e-05\n",
      "7 Train Loss 1713141.0 Test MSE 3513.55379567493 Test RE 0.9978435957186773 Lambda1 8.0542495e-06\n",
      "8 Train Loss 1675325.6 Test MSE 3513.484092684047 Test RE 0.9978336979012458 Lambda1 -5.669344e-06\n",
      "9 Train Loss 1631446.2 Test MSE 3513.8441390220983 Test RE 0.9978848233875012 Lambda1 -8.114497e-06\n",
      "10 Train Loss 1575898.6 Test MSE 3514.205590541043 Test RE 0.9979361457706001 Lambda1 6.753554e-06\n",
      "11 Train Loss 1536025.1 Test MSE 3514.348620600757 Test RE 0.9979564538331945 Lambda1 -8.355295e-06\n",
      "12 Train Loss 1503315.8 Test MSE 3514.2597787129625 Test RE 0.99794383970392 Lambda1 -1.6075396e-06\n",
      "13 Train Loss 1471236.9 Test MSE 3514.2818916890055 Test RE 0.9979469794082811 Lambda1 3.5815835e-06\n",
      "14 Train Loss 1439573.1 Test MSE 3514.1154301251104 Test RE 0.9979233441696748 Lambda1 -7.745375e-06\n",
      "15 Train Loss 1412899.9 Test MSE 3514.3481341948623 Test RE 0.9979563847717593 Lambda1 -4.1179983e-06\n",
      "16 Train Loss 1386984.4 Test MSE 3514.3573861712694 Test RE 0.9979576983955804 Lambda1 -2.423441e-06\n",
      "17 Train Loss 1354357.0 Test MSE 3514.5600310422114 Test RE 0.9979864700985688 Lambda1 2.6300445e-06\n",
      "18 Train Loss 1334424.5 Test MSE 3514.7186264974216 Test RE 0.9980089870468637 Lambda1 2.0656114e-05\n",
      "19 Train Loss 1313175.5 Test MSE 3514.644419219925 Test RE 0.9979984513641205 Lambda1 -2.373137e-06\n",
      "20 Train Loss 1288807.1 Test MSE 3514.9404964364694 Test RE 0.9980404866805898 Lambda1 -2.9332077e-06\n",
      "21 Train Loss 1268508.5 Test MSE 3514.982889239744 Test RE 0.998046505218668 Lambda1 5.6072565e-07\n",
      "22 Train Loss 1260998.9 Test MSE 3515.031004565021 Test RE 0.998053336143514 Lambda1 -1.0058447e-05\n",
      "23 Train Loss 1246575.8 Test MSE 3515.1086563938984 Test RE 0.998064360262327 Lambda1 9.742319e-07\n",
      "24 Train Loss 1232437.1 Test MSE 3515.14496452764 Test RE 0.9980695148342391 Lambda1 7.391258e-06\n",
      "25 Train Loss 1221032.0 Test MSE 3515.014911174097 Test RE 0.9980510513726453 Lambda1 -6.6397257e-07\n",
      "26 Train Loss 1211526.4 Test MSE 3515.1104113256715 Test RE 0.9980646094060677 Lambda1 2.6706311e-06\n",
      "27 Train Loss 1203469.2 Test MSE 3515.161439447023 Test RE 0.9980718537271864 Lambda1 1.4944661e-05\n",
      "28 Train Loss 1190915.6 Test MSE 3515.0072524299876 Test RE 0.9980499640626381 Lambda1 3.8530234e-06\n",
      "29 Train Loss 1175843.2 Test MSE 3515.341905177138 Test RE 0.9980974735254763 Lambda1 5.3888925e-07\n",
      "30 Train Loss 1166305.6 Test MSE 3515.4595585092866 Test RE 0.9981141758142162 Lambda1 -1.3295729e-06\n",
      "31 Train Loss 1155325.1 Test MSE 3515.569790550414 Test RE 0.9981298243088964 Lambda1 1.2344321e-05\n",
      "32 Train Loss 1149198.2 Test MSE 3515.426741215139 Test RE 0.9981095170374046 Lambda1 9.84267e-06\n",
      "33 Train Loss 1137628.9 Test MSE 3515.532002306623 Test RE 0.9981244599331711 Lambda1 -3.8740736e-06\n",
      "34 Train Loss 1126333.4 Test MSE 3515.7146166193374 Test RE 0.9981503833844678 Lambda1 -1.2889702e-05\n",
      "35 Train Loss 1119509.6 Test MSE 3515.577075259683 Test RE 0.9981308584345505 Lambda1 -1.0457046e-05\n",
      "36 Train Loss 1113991.9 Test MSE 3515.5198269376265 Test RE 0.9981227315256185 Lambda1 -6.9715934e-06\n",
      "37 Train Loss 1106418.1 Test MSE 3515.440040013942 Test RE 0.9981114049511736 Lambda1 5.9439744e-06\n",
      "38 Train Loss 1097790.2 Test MSE 3515.3746907007157 Test RE 0.9981021278482757 Lambda1 1.042612e-06\n",
      "39 Train Loss 1091625.6 Test MSE 3515.4597462930096 Test RE 0.9981142024721238 Lambda1 -1.739864e-05\n",
      "40 Train Loss 1087304.2 Test MSE 3515.438108173577 Test RE 0.9981111307049705 Lambda1 -6.250838e-06\n",
      "41 Train Loss 1081679.8 Test MSE 3515.56251033002 Test RE 0.9981287908194059 Lambda1 5.120716e-06\n",
      "42 Train Loss 1075686.6 Test MSE 3515.6367000294767 Test RE 0.9981393226310463 Lambda1 2.5191812e-05\n",
      "43 Train Loss 1069989.4 Test MSE 3515.5751726463354 Test RE 0.9981305883427121 Lambda1 1.4644347e-05\n",
      "44 Train Loss 1062471.9 Test MSE 3515.61115813926 Test RE 0.9981356967712828 Lambda1 2.14466e-07\n",
      "45 Train Loss 1056134.4 Test MSE 3515.55313009708 Test RE 0.9981274592136131 Lambda1 -1.678691e-05\n",
      "46 Train Loss 1052935.2 Test MSE 3515.555365919049 Test RE 0.998127776608183 Lambda1 -1.8273942e-05\n",
      "47 Train Loss 1047548.9 Test MSE 3515.667474515574 Test RE 0.9981436912789052 Lambda1 -1.2119456e-05\n",
      "48 Train Loss 1041290.56 Test MSE 3515.746675884783 Test RE 0.9981549343646575 Lambda1 -6.709835e-06\n",
      "49 Train Loss 1033168.94 Test MSE 3515.979019473169 Test RE 0.998187916130568 Lambda1 -3.2957773e-06\n",
      "50 Train Loss 1027583.25 Test MSE 3516.140024926951 Test RE 0.9982107706268089 Lambda1 3.8331377e-06\n",
      "51 Train Loss 1019814.2 Test MSE 3516.0661109944313 Test RE 0.9982002787136925 Lambda1 1.4954463e-05\n",
      "52 Train Loss 1013468.75 Test MSE 3516.006613654557 Test RE 0.9981918331227088 Lambda1 1.9862725e-06\n",
      "53 Train Loss 1006546.7 Test MSE 3516.1393085619793 Test RE 0.9982106689409741 Lambda1 -5.395327e-06\n",
      "54 Train Loss 997307.8 Test MSE 3516.1258206625016 Test RE 0.9982087543726237 Lambda1 -1.0284639e-05\n",
      "55 Train Loss 986255.8 Test MSE 3515.9180300167523 Test RE 0.9981792586269138 Lambda1 -1.6825672e-05\n",
      "56 Train Loss 979340.56 Test MSE 3515.6546455159137 Test RE 0.998141870117369 Lambda1 1.4967736e-06\n",
      "57 Train Loss 970680.06 Test MSE 3515.6260694147622 Test RE 0.9981378135384554 Lambda1 -9.285763e-07\n",
      "58 Train Loss 962320.75 Test MSE 3515.5666735199607 Test RE 0.9981293818199214 Lambda1 -1.0324483e-05\n",
      "59 Train Loss 953706.2 Test MSE 3515.376777072574 Test RE 0.9981024240346098 Lambda1 -5.3557687e-06\n",
      "60 Train Loss 949106.75 Test MSE 3515.335067696014 Test RE 0.9980965028551777 Lambda1 -8.607799e-06\n",
      "61 Train Loss 943194.56 Test MSE 3515.2629875536354 Test RE 0.9980862700743133 Lambda1 -1.260058e-05\n",
      "62 Train Loss 934711.6 Test MSE 3515.1523751827626 Test RE 0.9980705669025239 Lambda1 -1.1187183e-05\n",
      "63 Train Loss 927775.94 Test MSE 3515.0766857784906 Test RE 0.9980598214546414 Lambda1 -5.155846e-06\n",
      "64 Train Loss 921343.8 Test MSE 3515.151526374971 Test RE 0.9980704463999063 Lambda1 2.4728317e-06\n",
      "65 Train Loss 914525.2 Test MSE 3515.1869074378005 Test RE 0.998075469327659 Lambda1 7.216558e-06\n",
      "66 Train Loss 909075.8 Test MSE 3515.169624995382 Test RE 0.9980730158019315 Lambda1 -5.195887e-06\n",
      "67 Train Loss 901127.06 Test MSE 3515.096168378195 Test RE 0.9980625873648589 Lambda1 1.0125648e-06\n",
      "68 Train Loss 895399.9 Test MSE 3515.323610804467 Test RE 0.9980948763967488 Lambda1 -5.452297e-06\n",
      "69 Train Loss 891516.25 Test MSE 3515.224969753403 Test RE 0.9980808728754182 Lambda1 -1.6336506e-05\n",
      "70 Train Loss 887462.1 Test MSE 3515.208870375674 Test RE 0.998078587317664 Lambda1 3.7315467e-06\n",
      "71 Train Loss 880886.3 Test MSE 3515.411626359735 Test RE 0.9981073713096803 Lambda1 -7.74634e-06\n",
      "72 Train Loss 875133.4 Test MSE 3515.584849405755 Test RE 0.9981319620386866 Lambda1 -2.0119433e-06\n",
      "73 Train Loss 867454.3 Test MSE 3515.6931238639704 Test RE 0.9981473323639592 Lambda1 -1.5677362e-05\n",
      "74 Train Loss 862646.75 Test MSE 3515.8866351275683 Test RE 0.9981748020672188 Lambda1 1.5260479e-06\n",
      "Training time: 142.75\n",
      "Training time: 142.75\n",
      "inv_HT_atanh_tune17\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.93\n",
      "Training time: 2.93\n",
      "inv_HT_atanh_tune17\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.89\n",
      "Training time: 2.89\n",
      "inv_HT_atanh_tune17\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.85\n",
      "Training time: 2.85\n",
      "inv_HT_atanh_tune17\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 423287.0 Test MSE 3518.3520735854313 Test RE 0.9985247148390775 Lambda1 -3.899612e-06\n",
      "1 Train Loss 411733.22 Test MSE 3518.313694626221 Test RE 0.9985192687604864 Lambda1 2.5327447e-05\n",
      "2 Train Loss 395452.16 Test MSE 3518.2287357531013 Test RE 0.9985072127598901 Lambda1 1.6736785e-05\n",
      "3 Train Loss 383473.75 Test MSE 3517.9876674843913 Test RE 0.9984730034252195 Lambda1 1.5088616e-05\n",
      "4 Train Loss 376533.3 Test MSE 3518.0460644466393 Test RE 0.998481290485015 Lambda1 -8.446512e-06\n",
      "5 Train Loss 371081.5 Test MSE 3518.054493094231 Test RE 0.9984824865810481 Lambda1 -4.1869744e-06\n",
      "6 Train Loss 363067.22 Test MSE 3518.0261173124463 Test RE 0.9984784598130798 Lambda1 5.464864e-06\n",
      "7 Train Loss 356288.5 Test MSE 3518.2255013463764 Test RE 0.998506753781889 Lambda1 2.3607225e-05\n",
      "8 Train Loss 347479.84 Test MSE 3517.900496555531 Test RE 0.9984606329500285 Lambda1 7.1216728e-06\n",
      "9 Train Loss 340049.94 Test MSE 3517.4013888324516 Test RE 0.9983898013431493 Lambda1 6.550906e-05\n",
      "10 Train Loss 335839.2 Test MSE 3517.196779709994 Test RE 0.9983607624861366 Lambda1 5.882197e-05\n",
      "11 Train Loss 331080.34 Test MSE 3516.840664383843 Test RE 0.9983102193134561 Lambda1 2.286347e-06\n",
      "12 Train Loss 327222.38 Test MSE 3516.726845245546 Test RE 0.9982940645115858 Lambda1 2.2991146e-05\n",
      "13 Train Loss 320153.7 Test MSE 3516.4053146554274 Test RE 0.9982484269868508 Lambda1 2.4399535e-05\n",
      "14 Train Loss 316472.03 Test MSE 3516.1999902711264 Test RE 0.9982192824887922 Lambda1 2.5563e-05\n",
      "15 Train Loss 313599.38 Test MSE 3515.8118703566843 Test RE 0.9981641889964822 Lambda1 4.0731375e-05\n",
      "16 Train Loss 309678.0 Test MSE 3515.650467082888 Test RE 0.9981412769603875 Lambda1 3.5449364e-05\n",
      "17 Train Loss 305369.47 Test MSE 3515.5564177244773 Test RE 0.9981279259211685 Lambda1 6.524665e-05\n",
      "18 Train Loss 302134.88 Test MSE 3515.4304492639817 Test RE 0.9981100434369196 Lambda1 4.9501334e-05\n",
      "19 Train Loss 298766.3 Test MSE 3515.182150238016 Test RE 0.998074793965845 Lambda1 -1.6745029e-05\n",
      "20 Train Loss 296583.53 Test MSE 3515.0048570082963 Test RE 0.9980496239849701 Lambda1 -1.0263853e-05\n",
      "21 Train Loss 292950.88 Test MSE 3514.787406951777 Test RE 0.9980187521494298 Lambda1 2.0165418e-05\n",
      "22 Train Loss 289662.62 Test MSE 3514.444915531335 Test RE 0.9979701259954554 Lambda1 2.1044834e-05\n",
      "23 Train Loss 286971.84 Test MSE 3514.1117705222964 Test RE 0.9979228245504275 Lambda1 5.011989e-07\n",
      "24 Train Loss 283607.22 Test MSE 3513.1866476862624 Test RE 0.9977914596391467 Lambda1 1.7751183e-05\n",
      "25 Train Loss 281205.16 Test MSE 3512.7291733301417 Test RE 0.9977264931411985 Lambda1 2.6498114e-05\n",
      "26 Train Loss 278218.97 Test MSE 3511.6991006677717 Test RE 0.997580195762667 Lambda1 3.2206488e-05\n",
      "27 Train Loss 276657.56 Test MSE 3511.5509946765696 Test RE 0.9975591590568864 Lambda1 3.0353602e-05\n",
      "28 Train Loss 273859.2 Test MSE 3511.4763704554634 Test RE 0.9975485594000874 Lambda1 8.191961e-06\n",
      "29 Train Loss 271553.9 Test MSE 3510.8650160302273 Test RE 0.9974617181104996 Lambda1 -3.6276715e-06\n",
      "30 Train Loss 267013.6 Test MSE 3510.2848582169445 Test RE 0.9973793012253506 Lambda1 -1.3182087e-05\n",
      "31 Train Loss 262631.84 Test MSE 3510.194917430967 Test RE 0.9973665236792951 Lambda1 -3.1288015e-05\n",
      "32 Train Loss 260196.97 Test MSE 3510.0041956337677 Test RE 0.9973394280159149 Lambda1 -1.2727864e-05\n",
      "33 Train Loss 255815.39 Test MSE 3509.65006073076 Test RE 0.9972891144562952 Lambda1 4.85257e-05\n",
      "34 Train Loss 250367.4 Test MSE 3509.3614441603236 Test RE 0.9972481075073499 Lambda1 1.6042664e-05\n",
      "35 Train Loss 246475.33 Test MSE 3509.8729841338372 Test RE 0.9973207864961333 Lambda1 1.9040535e-05\n",
      "36 Train Loss 243922.42 Test MSE 3509.850356305177 Test RE 0.9973175716732109 Lambda1 -1.535526e-05\n",
      "37 Train Loss 241202.66 Test MSE 3509.8954663956247 Test RE 0.9973239806275455 Lambda1 4.3117043e-06\n",
      "38 Train Loss 236877.62 Test MSE 3509.9369941160517 Test RE 0.9973298805851559 Lambda1 -7.849727e-06\n",
      "39 Train Loss 235220.45 Test MSE 3509.907013219367 Test RE 0.9973256211200984 Lambda1 -3.750165e-06\n",
      "40 Train Loss 232090.97 Test MSE 3509.9021359958188 Test RE 0.9973249281983662 Lambda1 7.837959e-06\n",
      "41 Train Loss 229705.27 Test MSE 3509.8961177161805 Test RE 0.9973240731627199 Lambda1 -2.5757508e-06\n",
      "42 Train Loss 227026.56 Test MSE 3510.377811489132 Test RE 0.9973925065718778 Lambda1 -1.5860182e-05\n",
      "43 Train Loss 223266.64 Test MSE 3511.0186086024582 Test RE 0.9974835362428902 Lambda1 -3.4240984e-05\n",
      "44 Train Loss 219702.67 Test MSE 3511.4593584015483 Test RE 0.9975461429847908 Lambda1 -3.0013093e-06\n",
      "45 Train Loss 217368.12 Test MSE 3511.7669762004048 Test RE 0.997589836531709 Lambda1 -3.4708924e-06\n",
      "46 Train Loss 214842.8 Test MSE 3511.6874291936115 Test RE 0.9975785379837945 Lambda1 -9.5320665e-06\n",
      "47 Train Loss 211360.86 Test MSE 3511.879870449946 Test RE 0.9976058713728009 Lambda1 8.034929e-06\n",
      "48 Train Loss 207581.36 Test MSE 3511.985620778334 Test RE 0.9976208912991957 Lambda1 6.3627976e-06\n",
      "49 Train Loss 204494.86 Test MSE 3512.552295737584 Test RE 0.997701373401956 Lambda1 -1.4303014e-05\n",
      "50 Train Loss 202417.92 Test MSE 3512.8038885949545 Test RE 0.9977371038371955 Lambda1 -2.8704158e-05\n",
      "51 Train Loss 200830.33 Test MSE 3512.8213092521087 Test RE 0.9977395778173856 Lambda1 -3.72653e-05\n",
      "52 Train Loss 199306.58 Test MSE 3512.9372821907455 Test RE 0.9977560474618168 Lambda1 -2.7905759e-05\n",
      "53 Train Loss 196756.73 Test MSE 3512.9271187080763 Test RE 0.9977546041278178 Lambda1 -3.7703576e-05\n",
      "54 Train Loss 194666.11 Test MSE 3512.982943503081 Test RE 0.9977625318791028 Lambda1 3.3363492e-06\n",
      "55 Train Loss 192101.98 Test MSE 3512.96685795032 Test RE 0.997760247555427 Lambda1 3.860885e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 Train Loss 189777.9 Test MSE 3513.458871367848 Test RE 0.9978301164527876 Lambda1 2.541953e-06\n",
      "57 Train Loss 188437.7 Test MSE 3513.665624725463 Test RE 0.997859475227074 Lambda1 1.8841682e-05\n",
      "58 Train Loss 185796.86 Test MSE 3514.1911509081237 Test RE 0.9979340955424545 Lambda1 -8.599417e-06\n",
      "59 Train Loss 183521.05 Test MSE 3514.3854233973957 Test RE 0.9979616791958767 Lambda1 -9.931458e-06\n",
      "60 Train Loss 181551.16 Test MSE 3514.4520659673253 Test RE 0.9979711412223332 Lambda1 5.602252e-06\n",
      "61 Train Loss 180366.95 Test MSE 3514.799772638344 Test RE 0.9980205077572083 Lambda1 1.565126e-05\n",
      "62 Train Loss 179435.5 Test MSE 3514.8057216738866 Test RE 0.9980213523653432 Lambda1 2.2256401e-05\n",
      "63 Train Loss 177997.45 Test MSE 3515.430069730377 Test RE 0.9981099895578405 Lambda1 -2.6903738e-06\n",
      "64 Train Loss 176311.62 Test MSE 3515.593705426976 Test RE 0.9981332192224339 Lambda1 7.474595e-06\n",
      "65 Train Loss 174953.5 Test MSE 3515.6168651763583 Test RE 0.9981365069284479 Lambda1 2.849727e-05\n",
      "66 Train Loss 173457.84 Test MSE 3515.7926523221963 Test RE 0.9981614609238874 Lambda1 2.6447131e-05\n",
      "67 Train Loss 172094.92 Test MSE 3515.489586973282 Test RE 0.9981184386667805 Lambda1 1.2825368e-05\n",
      "68 Train Loss 170800.08 Test MSE 3515.8115532275083 Test RE 0.9981641439788579 Lambda1 2.5650066e-05\n",
      "69 Train Loss 169929.9 Test MSE 3515.8773055010215 Test RE 0.9981734777064989 Lambda1 2.848559e-05\n",
      "70 Train Loss 168151.67 Test MSE 3515.9517338240876 Test RE 0.9981840429194525 Lambda1 3.223697e-05\n",
      "71 Train Loss 167291.17 Test MSE 3516.1545857340284 Test RE 0.99821283748696 Lambda1 2.8166989e-05\n",
      "72 Train Loss 166495.64 Test MSE 3516.222575664777 Test RE 0.9982224883842621 Lambda1 1.84593e-05\n",
      "73 Train Loss 165661.4 Test MSE 3516.3794754808723 Test RE 0.9982447593262102 Lambda1 2.3893432e-05\n",
      "74 Train Loss 164928.53 Test MSE 3516.37111834025 Test RE 0.9982435730951956 Lambda1 2.1164102e-05\n",
      "Training time: 139.75\n",
      "Training time: 139.75\n",
      "inv_HT_atanh_tune17\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.42\n",
      "Training time: 2.42\n",
      "inv_HT_atanh_tune17\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.37\n",
      "Training time: 3.37\n",
      "inv_HT_atanh_tune17\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.34\n",
      "Training time: 3.34\n",
      "inv_HT_atanh_tune17\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.31\n",
      "Training time: 3.31\n",
      "inv_HT_atanh_tune17\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.38\n",
      "Training time: 3.38\n",
      "inv_HT_atanh_tune18\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.10\n",
      "Training time: 3.10\n",
      "inv_HT_atanh_tune18\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.97\n",
      "Training time: 2.97\n",
      "inv_HT_atanh_tune18\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.03\n",
      "Training time: 3.03\n",
      "inv_HT_atanh_tune18\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.02\n",
      "Training time: 3.02\n",
      "inv_HT_atanh_tune18\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.82\n",
      "Training time: 2.82\n",
      "inv_HT_atanh_tune18\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.97\n",
      "Training time: 2.97\n",
      "inv_HT_atanh_tune18\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.77\n",
      "Training time: 2.77\n",
      "inv_HT_atanh_tune18\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.76\n",
      "Training time: 2.76\n",
      "inv_HT_atanh_tune18\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.77\n",
      "Training time: 2.77\n",
      "inv_HT_atanh_tune18\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.81\n",
      "Training time: 2.81\n",
      "inv_HT_atanh_tune19\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.70\n",
      "Training time: 2.70\n",
      "inv_HT_atanh_tune19\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.65\n",
      "Training time: 2.65\n",
      "inv_HT_atanh_tune19\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.71\n",
      "Training time: 2.71\n",
      "inv_HT_atanh_tune19\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.97\n",
      "Training time: 2.97\n",
      "inv_HT_atanh_tune19\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.97\n",
      "Training time: 2.97\n",
      "inv_HT_atanh_tune19\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.82\n",
      "Training time: 2.82\n",
      "inv_HT_atanh_tune19\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.83\n",
      "Training time: 2.83\n",
      "inv_HT_atanh_tune19\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.63\n",
      "Training time: 2.63\n",
      "inv_HT_atanh_tune19\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.97\n",
      "Training time: 2.97\n",
      "inv_HT_atanh_tune19\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 3.01\n",
      "Training time: 3.01\n",
      "inv_HT_atanh_tune20\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.06244 Test MSE 858.2756371476482 Test RE 0.4931767145330762 Lambda1 -0.046508815\n",
      "1 Train Loss 837.92584 Test MSE 858.0168057002911 Test RE 0.4931023449118122 Lambda1 -0.056181543\n",
      "2 Train Loss 837.74774 Test MSE 857.8300306771362 Test RE 0.4930486721784155 Lambda1 -1.1175126\n",
      "3 Train Loss 837.1741 Test MSE 856.1433845782252 Test RE 0.4925637230861007 Lambda1 -1.645812\n",
      "4 Train Loss 833.999 Test MSE 851.6841817281996 Test RE 0.49127929509272467 Lambda1 -1.4166214\n",
      "5 Train Loss 827.3418 Test MSE 843.7661274582177 Test RE 0.4889902660762432 Lambda1 -2.3989143\n",
      "6 Train Loss 815.1335 Test MSE 827.6753168910654 Test RE 0.48430524506550043 Lambda1 -2.698769\n",
      "7 Train Loss 806.49243 Test MSE 817.9808321007706 Test RE 0.4814605793280359 Lambda1 -2.8392344\n",
      "8 Train Loss 800.9946 Test MSE 810.3762480430537 Test RE 0.47921733799682475 Lambda1 -2.8428211\n",
      "9 Train Loss 788.5254 Test MSE 797.105423643496 Test RE 0.4752772786759903 Lambda1 -2.7024498\n",
      "10 Train Loss 779.2572 Test MSE 786.6121824233351 Test RE 0.47213859650633017 Lambda1 -2.5897784\n",
      "11 Train Loss 767.5035 Test MSE 768.953870236434 Test RE 0.4668091006274841 Lambda1 -2.9189117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 Train Loss 753.47284 Test MSE 748.2512311343568 Test RE 0.46048224614883154 Lambda1 -3.1766145\n",
      "13 Train Loss 733.1377 Test MSE 727.776629483043 Test RE 0.45413839765153213 Lambda1 -3.635517\n",
      "14 Train Loss 714.2657 Test MSE 705.3752418816201 Test RE 0.44709444787899166 Lambda1 -3.6826947\n",
      "15 Train Loss 697.59 Test MSE 687.8303073468538 Test RE 0.4414991020121437 Lambda1 -3.8783042\n",
      "16 Train Loss 682.0796 Test MSE 673.5647597330449 Test RE 0.4368967853692932 Lambda1 -3.9781861\n",
      "17 Train Loss 676.0037 Test MSE 666.5502757896152 Test RE 0.4346159163533556 Lambda1 -4.0103874\n",
      "18 Train Loss 670.5168 Test MSE 666.0424598778416 Test RE 0.43445032724608584 Lambda1 -4.094566\n",
      "19 Train Loss 667.0077 Test MSE 662.4920412759182 Test RE 0.4332908353911162 Lambda1 -4.1672053\n",
      "20 Train Loss 663.61334 Test MSE 658.0475083669994 Test RE 0.4318349566393943 Lambda1 -4.2335477\n",
      "21 Train Loss 659.8206 Test MSE 656.9068344270054 Test RE 0.43146051822586995 Lambda1 -4.279315\n",
      "22 Train Loss 657.3636 Test MSE 653.2865504869366 Test RE 0.4302699630708564 Lambda1 -4.351692\n",
      "23 Train Loss 654.0256 Test MSE 646.8504246460367 Test RE 0.428145224629505 Lambda1 -4.446545\n",
      "24 Train Loss 651.53766 Test MSE 641.2684385921285 Test RE 0.42629388554723896 Lambda1 -4.5720744\n",
      "25 Train Loss 649.4685 Test MSE 639.29456968252 Test RE 0.4256372987702397 Lambda1 -4.571689\n",
      "26 Train Loss 646.9628 Test MSE 639.333352067758 Test RE 0.4256502090780485 Lambda1 -4.587359\n",
      "27 Train Loss 645.4564 Test MSE 640.9280954070045 Test RE 0.4261807461303089 Lambda1 -4.5246954\n",
      "28 Train Loss 643.2559 Test MSE 640.3668588263162 Test RE 0.425994110054129 Lambda1 -4.5532384\n",
      "29 Train Loss 641.38477 Test MSE 636.7331925085248 Test RE 0.4247837705892396 Lambda1 -4.6504397\n",
      "30 Train Loss 639.65063 Test MSE 635.6734459415164 Test RE 0.4244301290552848 Lambda1 -4.6268334\n",
      "31 Train Loss 638.248 Test MSE 632.7300045117435 Test RE 0.42344634188138397 Lambda1 -4.6544037\n",
      "32 Train Loss 637.7123 Test MSE 632.3225524660393 Test RE 0.42330997892864675 Lambda1 -4.63436\n",
      "33 Train Loss 637.0952 Test MSE 631.4757683779455 Test RE 0.42302644306539466 Lambda1 -4.6206856\n",
      "34 Train Loss 636.3224 Test MSE 629.7647324139396 Test RE 0.4224529416995299 Lambda1 -4.7244687\n",
      "35 Train Loss 635.86743 Test MSE 628.7270193313813 Test RE 0.4221047436653221 Lambda1 -4.7880163\n",
      "36 Train Loss 634.38837 Test MSE 628.9006629969064 Test RE 0.42216302870207717 Lambda1 -4.7954044\n",
      "37 Train Loss 633.6047 Test MSE 627.0540097057425 Test RE 0.4215427702777977 Lambda1 -4.8541484\n",
      "38 Train Loss 633.1627 Test MSE 626.0458962388145 Test RE 0.4212037772425268 Lambda1 -4.9372816\n",
      "39 Train Loss 631.9184 Test MSE 624.5766275668647 Test RE 0.4207092248089552 Lambda1 -5.0303364\n",
      "40 Train Loss 630.62823 Test MSE 623.0250826705218 Test RE 0.4201863465034119 Lambda1 -5.099631\n",
      "41 Train Loss 629.5819 Test MSE 622.2709057546801 Test RE 0.41993195000258504 Lambda1 -5.152111\n",
      "42 Train Loss 629.4026 Test MSE 622.0850343433568 Test RE 0.41986922878827115 Lambda1 -5.1425366\n",
      "43 Train Loss 629.1576 Test MSE 622.773549251309 Test RE 0.4201015171914283 Lambda1 -5.092157\n",
      "44 Train Loss 628.6545 Test MSE 622.9462941161189 Test RE 0.42015977701009555 Lambda1 -5.0857773\n",
      "45 Train Loss 627.189 Test MSE 622.414350624768 Test RE 0.4199803482262674 Lambda1 -4.9736094\n",
      "46 Train Loss 626.9538 Test MSE 621.8310435932129 Test RE 0.4197835059513521 Lambda1 -4.961857\n",
      "47 Train Loss 626.74384 Test MSE 621.9028656011955 Test RE 0.41980774792519493 Lambda1 -4.98151\n",
      "48 Train Loss 626.6023 Test MSE 621.683544436588 Test RE 0.41973371639363594 Lambda1 -5.003416\n",
      "49 Train Loss 626.4628 Test MSE 621.0089997397909 Test RE 0.4195059429568718 Lambda1 -5.056059\n",
      "50 Train Loss 626.1743 Test MSE 620.9759802919991 Test RE 0.41949479010826524 Lambda1 -5.049821\n",
      "51 Train Loss 626.0365 Test MSE 620.8400935448981 Test RE 0.4194488890564486 Lambda1 -5.068865\n",
      "52 Train Loss 625.9038 Test MSE 620.8594387552372 Test RE 0.4194554239628285 Lambda1 -5.084816\n",
      "53 Train Loss 625.79755 Test MSE 620.9841031314108 Test RE 0.419497533755185 Lambda1 -5.118915\n",
      "54 Train Loss 625.7389 Test MSE 620.9705050751138 Test RE 0.41949294073719967 Lambda1 -5.1126513\n",
      "55 Train Loss 625.63074 Test MSE 620.7648835377695 Test RE 0.4194234817819077 Lambda1 -5.104353\n",
      "56 Train Loss 625.42584 Test MSE 620.5707746669312 Test RE 0.4193579012489311 Lambda1 -5.116647\n",
      "57 Train Loss 625.215 Test MSE 620.1605398800274 Test RE 0.4192192678753526 Lambda1 -5.110857\n",
      "58 Train Loss 625.1383 Test MSE 619.9298727009987 Test RE 0.41914129683945783 Lambda1 -5.107148\n",
      "59 Train Loss 624.66486 Test MSE 619.2763577464032 Test RE 0.41892031431839427 Lambda1 -5.09844\n",
      "60 Train Loss 624.3567 Test MSE 619.2357990531642 Test RE 0.41890659577536005 Lambda1 -5.090271\n",
      "61 Train Loss 624.0424 Test MSE 619.1153606773097 Test RE 0.4188658561382667 Lambda1 -5.11907\n",
      "62 Train Loss 623.85345 Test MSE 619.2010088199082 Test RE 0.41889482799408306 Lambda1 -5.1403813\n",
      "63 Train Loss 623.7568 Test MSE 618.7833110641799 Test RE 0.41875351608982403 Lambda1 -5.1577945\n",
      "64 Train Loss 623.3663 Test MSE 618.1817008648881 Test RE 0.4185499006680642 Lambda1 -5.139323\n",
      "65 Train Loss 622.96564 Test MSE 617.2716685460286 Test RE 0.41824171116732334 Lambda1 -5.1271443\n",
      "66 Train Loss 622.65094 Test MSE 616.909075832381 Test RE 0.41811885304686913 Lambda1 -5.138387\n",
      "67 Train Loss 622.0841 Test MSE 615.4887214936891 Test RE 0.4176372430529405 Lambda1 -5.249829\n",
      "68 Train Loss 621.75104 Test MSE 614.4312408396523 Test RE 0.4172783143259119 Lambda1 -5.3158965\n",
      "69 Train Loss 620.9946 Test MSE 613.6923559183961 Test RE 0.41702733961749117 Lambda1 -5.2990446\n",
      "70 Train Loss 620.2945 Test MSE 613.8329933060488 Test RE 0.41707512111201206 Lambda1 -5.3140416\n",
      "71 Train Loss 619.94794 Test MSE 612.8675435094606 Test RE 0.41674699966044737 Lambda1 -5.4098554\n",
      "72 Train Loss 619.50385 Test MSE 612.3484020998983 Test RE 0.4165704554278921 Lambda1 -5.4615192\n",
      "73 Train Loss 618.7886 Test MSE 610.4858026045504 Test RE 0.4159364251607104 Lambda1 -5.580308\n",
      "74 Train Loss 617.8493 Test MSE 609.9745960046754 Test RE 0.4157622409486067 Lambda1 -5.6126685\n",
      "Training time: 149.46\n",
      "Training time: 149.46\n",
      "inv_HT_atanh_tune20\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "1 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "2 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "3 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "4 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "5 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "6 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "7 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "8 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "9 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "10 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "11 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "12 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "13 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "15 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "16 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "17 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "18 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "19 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "20 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "21 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "22 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "23 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "24 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "25 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "26 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "27 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "28 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "29 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "30 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "31 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "32 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "33 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "34 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "35 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "36 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "37 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "38 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "39 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "40 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "41 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "42 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "43 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "44 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "45 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "46 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "47 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "48 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "49 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "50 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "51 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "52 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "53 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "54 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "55 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "56 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "57 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "58 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "59 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "60 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "61 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "62 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "63 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "64 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "65 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "66 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "67 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "68 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "69 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "70 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "71 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "72 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "73 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "74 Train Loss 838.0623 Test MSE 858.2832987246724 Test RE 0.49317891575072065 Lambda1 -0.07379224\n",
      "Training time: 116.39\n",
      "Training time: 116.39\n",
      "inv_HT_atanh_tune20\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 838.0629 Test MSE 858.2933869874762 Test RE 0.49318181415495316 Lambda1 -0.32856348\n",
      "1 Train Loss 837.9731 Test MSE 858.0256801979862 Test RE 0.49310489499257926 Lambda1 -0.32675374\n",
      "2 Train Loss 837.52594 Test MSE 857.4321766024765 Test RE 0.49293432308478363 Lambda1 -0.54362714\n",
      "3 Train Loss 836.28406 Test MSE 855.9952368290557 Test RE 0.49252110442824626 Lambda1 -0.6613012\n",
      "4 Train Loss 832.27216 Test MSE 850.8249217482555 Test RE 0.4910314079222541 Lambda1 -0.807345\n",
      "5 Train Loss 828.99963 Test MSE 844.5576455958352 Test RE 0.4892195677305692 Lambda1 -0.94787186\n",
      "6 Train Loss 821.3877 Test MSE 833.7742705733633 Test RE 0.48608633830580367 Lambda1 -1.2281367\n",
      "7 Train Loss 816.89374 Test MSE 829.6024098671221 Test RE 0.4848687260540821 Lambda1 -1.3400793\n",
      "8 Train Loss 811.5494 Test MSE 824.3059811881866 Test RE 0.48331847465225874 Lambda1 -1.3729432\n",
      "9 Train Loss 806.0351 Test MSE 813.9499136266464 Test RE 0.4802728221703296 Lambda1 -1.2545726\n",
      "10 Train Loss 799.59247 Test MSE 807.794004058416 Test RE 0.47845322167065313 Lambda1 -1.248411\n",
      "11 Train Loss 787.4962 Test MSE 797.9999571499249 Test RE 0.4755438884779995 Lambda1 -1.2805712\n",
      "12 Train Loss 779.1227 Test MSE 783.6533199522316 Test RE 0.4712497789995143 Lambda1 -1.3090749\n",
      "13 Train Loss 772.15625 Test MSE 764.5326845380954 Test RE 0.46546518069809906 Lambda1 -1.3468915\n",
      "14 Train Loss 763.7478 Test MSE 764.9194955866601 Test RE 0.46558291556448395 Lambda1 -1.3362862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Train Loss 758.94604 Test MSE 762.7895661926607 Test RE 0.4649342525171081 Lambda1 -1.3120111\n",
      "16 Train Loss 749.40515 Test MSE 756.5500441117164 Test RE 0.4630287962561413 Lambda1 -1.245833\n",
      "17 Train Loss 741.1725 Test MSE 745.6796649758253 Test RE 0.459690279704301 Lambda1 -1.2417581\n",
      "18 Train Loss 735.28064 Test MSE 737.3942109788452 Test RE 0.45712927258933983 Lambda1 -1.2406759\n",
      "19 Train Loss 730.82404 Test MSE 732.6362871587257 Test RE 0.4556521074386904 Lambda1 -1.2628697\n",
      "20 Train Loss 725.86896 Test MSE 723.9026721391344 Test RE 0.45292809481395796 Lambda1 -1.292039\n",
      "21 Train Loss 710.9847 Test MSE 705.2680799339097 Test RE 0.4470604848697729 Lambda1 -1.3269053\n",
      "22 Train Loss 702.7367 Test MSE 695.7292094595914 Test RE 0.4440269080747352 Lambda1 -1.3529319\n",
      "23 Train Loss 696.36646 Test MSE 691.0865199841331 Test RE 0.4425429041866039 Lambda1 -1.3663445\n",
      "24 Train Loss 691.04486 Test MSE 684.0631974907476 Test RE 0.4402884406914728 Lambda1 -1.3727295\n",
      "25 Train Loss 686.97003 Test MSE 678.4367962331416 Test RE 0.43847402185645284 Lambda1 -1.3773806\n",
      "26 Train Loss 680.2311 Test MSE 678.0985066729849 Test RE 0.4383646898744188 Lambda1 -1.3385998\n",
      "27 Train Loss 678.33887 Test MSE 675.4518626861877 Test RE 0.4375083765621467 Lambda1 -1.3250291\n",
      "28 Train Loss 675.7583 Test MSE 671.0303670328975 Test RE 0.4360740645106456 Lambda1 -1.3158447\n",
      "29 Train Loss 673.7161 Test MSE 670.0772892209023 Test RE 0.4357642721254728 Lambda1 -1.3331733\n",
      "30 Train Loss 667.55524 Test MSE 661.870999765955 Test RE 0.4330876973371721 Lambda1 -1.3711512\n",
      "31 Train Loss 664.4235 Test MSE 660.164382340992 Test RE 0.4325289841788664 Lambda1 -1.3718551\n",
      "32 Train Loss 661.92566 Test MSE 655.9453588102313 Test RE 0.43114465100464017 Lambda1 -1.3824397\n",
      "33 Train Loss 654.0637 Test MSE 652.9170069777113 Test RE 0.4301482508114215 Lambda1 -1.3958621\n",
      "34 Train Loss 651.92645 Test MSE 650.0291205702234 Test RE 0.42919591236718646 Lambda1 -1.3992633\n",
      "35 Train Loss 648.32404 Test MSE 647.0526747995891 Test RE 0.42821215329375845 Lambda1 -1.4136432\n",
      "36 Train Loss 644.0641 Test MSE 639.7889395504504 Test RE 0.42580184075191563 Lambda1 -1.427743\n",
      "37 Train Loss 638.35693 Test MSE 632.8043427634958 Test RE 0.4234712161057893 Lambda1 -1.4409164\n",
      "38 Train Loss 634.6637 Test MSE 629.3219523511428 Test RE 0.4223044047770879 Lambda1 -1.4494667\n",
      "39 Train Loss 630.5981 Test MSE 626.5858239606105 Test RE 0.42138536982571834 Lambda1 -1.4601012\n",
      "40 Train Loss 627.524 Test MSE 623.2918403497732 Test RE 0.42027629146894585 Lambda1 -1.470439\n",
      "41 Train Loss 623.5335 Test MSE 618.7824988552047 Test RE 0.41875324126383107 Lambda1 -1.4786695\n",
      "42 Train Loss 622.7861 Test MSE 616.3695905900113 Test RE 0.41793599118274594 Lambda1 -1.4814726\n",
      "43 Train Loss 620.9692 Test MSE 616.0941397963333 Test RE 0.41784259456912426 Lambda1 -1.4851552\n",
      "44 Train Loss 619.59814 Test MSE 613.4312987570663 Test RE 0.41693863103845796 Lambda1 -1.5035547\n",
      "45 Train Loss 618.94836 Test MSE 610.6458122624352 Test RE 0.4159909305107139 Lambda1 -1.5175745\n",
      "46 Train Loss 616.65814 Test MSE 607.555018244636 Test RE 0.41493682243722113 Lambda1 -1.516424\n",
      "47 Train Loss 610.5575 Test MSE 598.4662374561644 Test RE 0.41182148277835945 Lambda1 -1.5518931\n",
      "48 Train Loss 609.68634 Test MSE 598.2252227771456 Test RE 0.41173854993161285 Lambda1 -1.5524508\n",
      "49 Train Loss 606.5658 Test MSE 597.4241320970833 Test RE 0.41146277552461025 Lambda1 -1.5458978\n",
      "50 Train Loss 605.7303 Test MSE 596.158580200496 Test RE 0.41102673391625544 Lambda1 -1.5479218\n",
      "51 Train Loss 605.55145 Test MSE 594.9016576071101 Test RE 0.41059320716342673 Lambda1 -1.5543395\n",
      "52 Train Loss 605.10583 Test MSE 593.9736380539795 Test RE 0.410272828797969 Lambda1 -1.5604278\n",
      "53 Train Loss 604.8373 Test MSE 592.4431083512291 Test RE 0.40974389978300835 Lambda1 -1.5641698\n",
      "54 Train Loss 602.8227 Test MSE 589.8596931022685 Test RE 0.40884955641382187 Lambda1 -1.5727739\n",
      "55 Train Loss 600.61237 Test MSE 587.2979083772101 Test RE 0.4079607651933113 Lambda1 -1.592003\n",
      "56 Train Loss 600.16125 Test MSE 586.65930928329 Test RE 0.40773890655060063 Lambda1 -1.5992777\n",
      "57 Train Loss 597.63403 Test MSE 585.3473343355193 Test RE 0.40728272812047533 Lambda1 -1.5999372\n",
      "58 Train Loss 593.8889 Test MSE 581.0448009771267 Test RE 0.4057831231668885 Lambda1 -1.6272058\n",
      "59 Train Loss 587.7097 Test MSE 573.7011745255675 Test RE 0.4032106922697232 Lambda1 -1.6656823\n",
      "60 Train Loss 582.38153 Test MSE 563.385193610619 Test RE 0.3995690905575927 Lambda1 -1.7368438\n",
      "61 Train Loss 581.23376 Test MSE 562.4176466739714 Test RE 0.39922583696993186 Lambda1 -1.7403876\n",
      "62 Train Loss 580.5669 Test MSE 558.7667995162822 Test RE 0.3979279709326918 Lambda1 -1.7592139\n",
      "63 Train Loss 580.28925 Test MSE 558.7784889676961 Test RE 0.3979321332552914 Lambda1 -1.7556169\n",
      "64 Train Loss 580.0182 Test MSE 558.82552074569 Test RE 0.3979488796604582 Lambda1 -1.753918\n",
      "65 Train Loss 579.40314 Test MSE 559.7248655651366 Test RE 0.3982689700751604 Lambda1 -1.7346574\n",
      "66 Train Loss 575.436 Test MSE 555.0406397491548 Test RE 0.3965989519151435 Lambda1 -1.7486666\n",
      "67 Train Loss 571.9204 Test MSE 546.830517395637 Test RE 0.39365479191074465 Lambda1 -1.7695117\n",
      "68 Train Loss 571.34406 Test MSE 545.0745466612317 Test RE 0.3930222356673188 Lambda1 -1.7749194\n",
      "69 Train Loss 570.60956 Test MSE 547.0181087949969 Test RE 0.39372230818748577 Lambda1 -1.7701674\n",
      "70 Train Loss 570.4545 Test MSE 546.1566415730525 Test RE 0.3934121607708365 Lambda1 -1.7765726\n",
      "71 Train Loss 570.1437 Test MSE 544.9090468835706 Test RE 0.3929625649019699 Lambda1 -1.7784715\n",
      "72 Train Loss 569.9412 Test MSE 545.1405068930123 Test RE 0.39304601503263287 Lambda1 -1.775395\n",
      "73 Train Loss 569.6588 Test MSE 547.5881909242057 Test RE 0.39392741620783883 Lambda1 -1.7675041\n",
      "74 Train Loss 568.9155 Test MSE 545.3307682031011 Test RE 0.39311459820064876 Lambda1 -1.7764336\n",
      "Training time: 154.29\n",
      "Training time: 154.29\n",
      "inv_HT_atanh_tune20\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "1 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "2 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "3 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "4 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "5 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "6 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "7 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "8 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "9 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "10 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "11 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "12 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "13 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "14 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "15 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "16 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "18 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "19 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "20 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "21 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "22 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "23 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "24 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "25 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "26 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "27 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "28 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "29 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "30 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "31 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "32 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "33 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "34 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "35 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "36 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "37 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "38 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "39 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "40 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "41 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "42 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "43 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "44 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "45 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "46 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "47 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "48 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "49 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "50 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "51 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "52 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "53 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "54 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "55 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "56 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "57 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "58 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "59 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "60 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "61 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "62 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "63 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "64 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "65 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "66 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "67 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "68 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "69 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "70 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "71 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "72 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "73 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "74 Train Loss 838.0632 Test MSE 858.2832047313698 Test RE 0.4931788887459329 Lambda1 -0.044443887\n",
      "Training time: 107.39\n",
      "Training time: 107.39\n",
      "inv_HT_atanh_tune20\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 838.0623 Test MSE 858.2811926043648 Test RE 0.49317831065053486 Lambda1 -0.003534245\n",
      "1 Train Loss 837.8943 Test MSE 858.0552949144676 Test RE 0.49311340466542075 Lambda1 -0.005748454\n",
      "2 Train Loss 837.84753 Test MSE 857.7206596339245 Test RE 0.49301723998051916 Lambda1 -0.11102642\n",
      "3 Train Loss 836.412 Test MSE 855.7430436188163 Test RE 0.4924485458302781 Lambda1 -0.7010105\n",
      "4 Train Loss 831.89374 Test MSE 849.5603821884049 Test RE 0.49066637422956244 Lambda1 -0.739541\n",
      "5 Train Loss 822.83527 Test MSE 840.5228501105352 Test RE 0.48804956820848355 Lambda1 -0.760465\n",
      "6 Train Loss 814.5496 Test MSE 831.2222690914948 Test RE 0.4853418659859392 Lambda1 -0.74082047\n",
      "7 Train Loss 808.53815 Test MSE 823.6484085478733 Test RE 0.4831256576516644 Lambda1 -0.8264283\n",
      "8 Train Loss 801.47406 Test MSE 813.566096114205 Test RE 0.4801595726613933 Lambda1 -0.93244094\n",
      "9 Train Loss 790.2743 Test MSE 798.1402791419588 Test RE 0.4755856969592763 Lambda1 -0.9469194\n",
      "10 Train Loss 774.82635 Test MSE 785.1458543973301 Test RE 0.47169833317994997 Lambda1 -0.8652146\n",
      "11 Train Loss 760.7117 Test MSE 767.8910318785244 Test RE 0.4664863802431523 Lambda1 -0.9239516\n",
      "12 Train Loss 737.6012 Test MSE 742.3823675229288 Test RE 0.45867280868393934 Lambda1 -0.94443876\n",
      "13 Train Loss 723.1924 Test MSE 724.4496765714971 Test RE 0.4530991861083587 Lambda1 -0.93738836\n",
      "14 Train Loss 712.4738 Test MSE 712.9776098608162 Test RE 0.44949733005746534 Lambda1 -0.9626578\n",
      "15 Train Loss 702.7395 Test MSE 707.4360775811516 Test RE 0.44774709071377944 Lambda1 -0.9854453\n",
      "16 Train Loss 690.6268 Test MSE 693.2941528395902 Test RE 0.4432491784745859 Lambda1 -0.9755054\n",
      "17 Train Loss 682.9552 Test MSE 683.6951071402341 Test RE 0.44016996645210305 Lambda1 -0.92045236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 Train Loss 674.79865 Test MSE 669.4671914282536 Test RE 0.4355658477289562 Lambda1 -0.8958972\n",
      "19 Train Loss 662.5484 Test MSE 661.5724335970674 Test RE 0.43299000464468107 Lambda1 -0.902838\n",
      "20 Train Loss 654.72754 Test MSE 653.3709427784976 Test RE 0.4302977535523327 Lambda1 -0.8891324\n",
      "21 Train Loss 646.2259 Test MSE 647.1508623222554 Test RE 0.4282446417581637 Lambda1 -0.890414\n",
      "22 Train Loss 638.28906 Test MSE 637.5110256099947 Test RE 0.42504314927072234 Lambda1 -0.8571832\n",
      "23 Train Loss 633.08295 Test MSE 628.7217955521094 Test RE 0.4221029901327691 Lambda1 -0.8575719\n",
      "24 Train Loss 621.05853 Test MSE 621.4803708173725 Test RE 0.41966512378916454 Lambda1 -0.7877701\n",
      "25 Train Loss 612.8789 Test MSE 616.101839484743 Test RE 0.41784520557247806 Lambda1 -0.7530383\n",
      "26 Train Loss 602.52277 Test MSE 599.5172731258505 Test RE 0.4121829477766246 Lambda1 -0.71761835\n",
      "27 Train Loss 596.0888 Test MSE 591.5799216784569 Test RE 0.40944529356075016 Lambda1 -0.67510575\n",
      "28 Train Loss 589.01196 Test MSE 588.0407999419907 Test RE 0.4082187048407944 Lambda1 -0.64522946\n",
      "29 Train Loss 562.5758 Test MSE 567.5479410515028 Test RE 0.40104254430139663 Lambda1 -0.5466209\n",
      "30 Train Loss 550.45685 Test MSE 551.5347208149982 Test RE 0.3953444073605836 Lambda1 -0.5299474\n",
      "31 Train Loss 541.2203 Test MSE 536.6893211357748 Test RE 0.3899874645181025 Lambda1 -0.53017616\n",
      "32 Train Loss 536.33734 Test MSE 523.467716604121 Test RE 0.3851537424068903 Lambda1 -0.5115651\n",
      "33 Train Loss 516.71246 Test MSE 503.01638566744344 Test RE 0.3775550093525993 Lambda1 -0.47330347\n",
      "34 Train Loss 495.3509 Test MSE 491.7507973014671 Test RE 0.3733031950431743 Lambda1 -0.46395856\n",
      "35 Train Loss 475.1685 Test MSE 454.82170467215104 Test RE 0.35901265825628537 Lambda1 -0.4634049\n",
      "36 Train Loss 460.95367 Test MSE 441.6999460918516 Test RE 0.35379593912103924 Lambda1 -0.49296796\n",
      "37 Train Loss 447.30704 Test MSE 426.823420578604 Test RE 0.34778695772544177 Lambda1 -0.46231538\n",
      "38 Train Loss 437.41412 Test MSE 420.2095163495831 Test RE 0.34508184582134266 Lambda1 -0.44461045\n",
      "39 Train Loss 433.54016 Test MSE 405.946869685197 Test RE 0.3391749495353637 Lambda1 -0.4421687\n",
      "40 Train Loss 423.56473 Test MSE 398.4962378702747 Test RE 0.3360479755741052 Lambda1 -0.41675895\n",
      "41 Train Loss 404.66226 Test MSE 372.82352974632016 Test RE 0.32504300739919206 Lambda1 -0.42817274\n",
      "42 Train Loss 390.6026 Test MSE 365.34677564174893 Test RE 0.321767228697898 Lambda1 -0.41799447\n",
      "43 Train Loss 384.41415 Test MSE 372.71428922541514 Test RE 0.324995383703233 Lambda1 -0.40955952\n",
      "44 Train Loss 372.3775 Test MSE 356.1036266554094 Test RE 0.317670852843474 Lambda1 -0.40948758\n",
      "45 Train Loss 358.65396 Test MSE 348.2071796843915 Test RE 0.31412899974712827 Lambda1 -0.4399218\n",
      "46 Train Loss 349.68195 Test MSE 344.04845452531737 Test RE 0.31224750460113737 Lambda1 -0.46488634\n",
      "47 Train Loss 346.6792 Test MSE 341.75595662523926 Test RE 0.31120546652394226 Lambda1 -0.4621443\n",
      "48 Train Loss 340.75153 Test MSE 336.03215384878115 Test RE 0.30858839415502937 Lambda1 -0.4732456\n",
      "49 Train Loss 331.5063 Test MSE 332.6690356613162 Test RE 0.3070402848260013 Lambda1 -0.4794491\n",
      "50 Train Loss 329.0039 Test MSE 326.6006283073501 Test RE 0.30422694656661836 Lambda1 -0.50098544\n",
      "51 Train Loss 324.60022 Test MSE 324.9985932286949 Test RE 0.3034798852586792 Lambda1 -0.50266826\n",
      "52 Train Loss 321.54086 Test MSE 325.0238053224244 Test RE 0.3034916564095192 Lambda1 -0.50215334\n",
      "53 Train Loss 317.04495 Test MSE 319.62629947883846 Test RE 0.30096114050651357 Lambda1 -0.5009126\n",
      "54 Train Loss 314.29272 Test MSE 319.29122531353363 Test RE 0.30080334569031064 Lambda1 -0.48819807\n",
      "55 Train Loss 312.90546 Test MSE 320.3452027997009 Test RE 0.30129941123084714 Lambda1 -0.48659363\n",
      "56 Train Loss 311.8811 Test MSE 320.8509202427476 Test RE 0.3015371427119731 Lambda1 -0.48408312\n",
      "57 Train Loss 309.2715 Test MSE 319.715595604813 Test RE 0.3010031783287823 Lambda1 -0.48454222\n",
      "58 Train Loss 307.82367 Test MSE 316.1386239544835 Test RE 0.29931463344183173 Lambda1 -0.4975448\n",
      "59 Train Loss 304.93802 Test MSE 313.7153547070107 Test RE 0.29816527177451657 Lambda1 -0.50339\n",
      "60 Train Loss 302.0378 Test MSE 312.40978691360255 Test RE 0.29754419789366826 Lambda1 -0.500995\n",
      "61 Train Loss 300.61926 Test MSE 311.9859974217724 Test RE 0.29734231738024786 Lambda1 -0.497698\n",
      "62 Train Loss 297.4309 Test MSE 310.8078713371084 Test RE 0.29678037216904174 Lambda1 -0.49650878\n",
      "63 Train Loss 295.98157 Test MSE 308.1171465813712 Test RE 0.2954929367460299 Lambda1 -0.5036506\n",
      "64 Train Loss 293.41595 Test MSE 307.17567590083183 Test RE 0.2950411430414043 Lambda1 -0.5197887\n",
      "65 Train Loss 292.2557 Test MSE 305.77498220294143 Test RE 0.2943676937921751 Lambda1 -0.5216999\n",
      "66 Train Loss 289.53976 Test MSE 304.82953489884403 Test RE 0.293912253316521 Lambda1 -0.5064142\n",
      "67 Train Loss 287.3647 Test MSE 301.71524855310565 Test RE 0.29240702385654654 Lambda1 -0.5191899\n",
      "68 Train Loss 281.96854 Test MSE 296.3391576450266 Test RE 0.28979019803583217 Lambda1 -0.56008446\n",
      "69 Train Loss 279.76254 Test MSE 295.8079295351879 Test RE 0.2895303374187451 Lambda1 -0.5567825\n",
      "70 Train Loss 273.816 Test MSE 294.530626253633 Test RE 0.2889045628917325 Lambda1 -0.54763395\n",
      "71 Train Loss 270.19785 Test MSE 292.9792296905779 Test RE 0.2881426772343325 Lambda1 -0.54594564\n",
      "72 Train Loss 268.8782 Test MSE 291.496808785677 Test RE 0.2874127783545642 Lambda1 -0.56897235\n",
      "73 Train Loss 268.18167 Test MSE 291.65765063136166 Test RE 0.2874920616008953 Lambda1 -0.57727426\n",
      "74 Train Loss 267.25958 Test MSE 290.6773804420482 Test RE 0.2870085202003388 Lambda1 -0.58816785\n",
      "Training time: 152.87\n",
      "Training time: 152.87\n",
      "inv_HT_atanh_tune20\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 838.0633 Test MSE 858.2802711393331 Test RE 0.49317804590822567 Lambda1 0.0078104055\n",
      "1 Train Loss 837.8932 Test MSE 857.9217948782497 Test RE 0.49307504279146686 Lambda1 0.0011063778\n",
      "2 Train Loss 836.8983 Test MSE 856.3328897458993 Test RE 0.49261823393548476 Lambda1 -2.8015532\n",
      "3 Train Loss 831.57324 Test MSE 850.5420557771929 Test RE 0.4909497767788957 Lambda1 -2.4433637\n",
      "4 Train Loss 821.88727 Test MSE 838.558686472732 Test RE 0.48747898889674285 Lambda1 -3.4237819\n",
      "5 Train Loss 812.3704 Test MSE 825.2956693930895 Test RE 0.48360853144830895 Lambda1 -3.8010118\n",
      "6 Train Loss 804.2699 Test MSE 816.7488488107518 Test RE 0.4810978722363925 Lambda1 -4.054809\n",
      "7 Train Loss 798.1788 Test MSE 808.4945399193073 Test RE 0.47866063903321693 Lambda1 -3.9606452\n",
      "8 Train Loss 788.6542 Test MSE 795.9237850042897 Test RE 0.47492486965490316 Lambda1 -4.1895638\n",
      "9 Train Loss 775.7849 Test MSE 776.0706633450469 Test RE 0.46896432244761277 Lambda1 -4.2504864\n",
      "10 Train Loss 759.5122 Test MSE 760.456879883238 Test RE 0.4642228006002371 Lambda1 -4.2919645\n",
      "11 Train Loss 748.8734 Test MSE 748.1576785255011 Test RE 0.4604534585837166 Lambda1 -3.9916065\n",
      "12 Train Loss 733.15137 Test MSE 717.3263411648311 Test RE 0.4508660767417374 Lambda1 -3.97173\n",
      "13 Train Loss 721.63464 Test MSE 714.6681664121378 Test RE 0.4500299209303612 Lambda1 -3.9679058\n",
      "14 Train Loss 701.2658 Test MSE 691.3256088227171 Test RE 0.44261944881324217 Lambda1 -3.734129\n",
      "15 Train Loss 693.2398 Test MSE 682.939069100384 Test RE 0.43992652658723136 Lambda1 -3.5457451\n",
      "16 Train Loss 688.26245 Test MSE 675.2555774549973 Test RE 0.43744480232586047 Lambda1 -3.3779953\n",
      "17 Train Loss 680.7154 Test MSE 668.5866883277455 Test RE 0.43527931892622546 Lambda1 -3.3835206\n",
      "18 Train Loss 672.9895 Test MSE 661.1685870562542 Test RE 0.4328578284836002 Lambda1 -3.45222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 Train Loss 666.34485 Test MSE 654.3502548550965 Test RE 0.43062011101715875 Lambda1 -3.4162407\n",
      "20 Train Loss 662.00525 Test MSE 652.2386360829261 Test RE 0.4299247339754112 Lambda1 -3.4718864\n",
      "21 Train Loss 654.1702 Test MSE 647.2207443481974 Test RE 0.42826776294755803 Lambda1 -3.5210373\n",
      "22 Train Loss 650.0487 Test MSE 642.4018338740856 Test RE 0.4266704409417466 Lambda1 -3.5886712\n",
      "23 Train Loss 646.6845 Test MSE 637.0219482523459 Test RE 0.4248800784713569 Lambda1 -3.604772\n",
      "24 Train Loss 643.64734 Test MSE 632.2949437338457 Test RE 0.4233007374589477 Lambda1 -3.6038578\n",
      "25 Train Loss 639.92365 Test MSE 629.3325985787272 Test RE 0.42230797681980153 Lambda1 -3.5814462\n",
      "26 Train Loss 639.30396 Test MSE 629.6734799867356 Test RE 0.4224223340369476 Lambda1 -3.595488\n",
      "27 Train Loss 639.05676 Test MSE 630.223739988308 Test RE 0.4226068672609427 Lambda1 -3.567972\n",
      "28 Train Loss 638.57245 Test MSE 629.0713000842807 Test RE 0.4222202967160585 Lambda1 -3.5705657\n",
      "29 Train Loss 637.0406 Test MSE 627.4637561117044 Test RE 0.42168047566271794 Lambda1 -3.6088538\n",
      "30 Train Loss 635.85126 Test MSE 626.9263213216376 Test RE 0.4214998482529035 Lambda1 -3.6299229\n",
      "31 Train Loss 634.29645 Test MSE 625.5230052579445 Test RE 0.4210278399309195 Lambda1 -3.6068923\n",
      "32 Train Loss 633.333 Test MSE 626.4156906365741 Test RE 0.4213281577423146 Lambda1 -3.5630875\n",
      "33 Train Loss 632.8078 Test MSE 627.2666867691433 Test RE 0.4216142512639284 Lambda1 -3.5268984\n",
      "34 Train Loss 632.2895 Test MSE 625.4777291735778 Test RE 0.42101260241223926 Lambda1 -3.565708\n",
      "35 Train Loss 631.9913 Test MSE 625.2061208847756 Test RE 0.42092118194769074 Lambda1 -3.5473638\n",
      "36 Train Loss 631.30023 Test MSE 624.2778252455456 Test RE 0.42060857748559244 Lambda1 -3.5846238\n",
      "37 Train Loss 630.45404 Test MSE 623.0181027255655 Test RE 0.4201839927572579 Lambda1 -3.6433907\n",
      "38 Train Loss 630.1989 Test MSE 622.7458261433416 Test RE 0.4200921665620157 Lambda1 -3.6821444\n",
      "39 Train Loss 630.1345 Test MSE 622.9951791525483 Test RE 0.42017626247863504 Lambda1 -3.6657114\n",
      "40 Train Loss 629.95355 Test MSE 623.2075326363971 Test RE 0.42024786679764736 Lambda1 -3.6663082\n",
      "41 Train Loss 629.6022 Test MSE 622.5535944716645 Test RE 0.4200273236927793 Lambda1 -3.6713696\n",
      "42 Train Loss 629.2711 Test MSE 623.1532876420489 Test RE 0.420229576871704 Lambda1 -3.6416035\n",
      "43 Train Loss 628.70416 Test MSE 622.91888324102 Test RE 0.42015053297575994 Lambda1 -3.6538978\n",
      "44 Train Loss 628.44293 Test MSE 622.7735237253024 Test RE 0.4201015085819472 Lambda1 -3.6129446\n",
      "45 Train Loss 627.4743 Test MSE 621.6925600078222 Test RE 0.4197367598435825 Lambda1 -3.5559385\n",
      "46 Train Loss 626.5123 Test MSE 620.7340382395575 Test RE 0.41941306124795835 Lambda1 -3.5289884\n",
      "47 Train Loss 625.7734 Test MSE 619.8733154347992 Test RE 0.41912217691344505 Lambda1 -3.5095098\n",
      "48 Train Loss 624.89813 Test MSE 618.0539205460929 Test RE 0.4185066405685645 Lambda1 -3.5256884\n",
      "49 Train Loss 624.62244 Test MSE 617.1265690898733 Test RE 0.4181925511178003 Lambda1 -3.5731983\n",
      "50 Train Loss 624.218 Test MSE 616.785311094807 Test RE 0.41807690929222313 Lambda1 -3.6429996\n",
      "51 Train Loss 624.0058 Test MSE 617.3386008171815 Test RE 0.41826438603661176 Lambda1 -3.6563466\n",
      "52 Train Loss 623.41406 Test MSE 616.101451473911 Test RE 0.41784507399642923 Lambda1 -3.7266371\n",
      "53 Train Loss 622.6347 Test MSE 615.6280182351251 Test RE 0.41768449998377116 Lambda1 -3.713342\n",
      "54 Train Loss 622.2237 Test MSE 614.8962955671283 Test RE 0.4174362006229571 Lambda1 -3.7269561\n",
      "55 Train Loss 621.8556 Test MSE 615.2577905104488 Test RE 0.41755888708631866 Lambda1 -3.7066789\n",
      "56 Train Loss 621.2647 Test MSE 614.2012095249584 Test RE 0.4172001965004571 Lambda1 -3.7035568\n",
      "57 Train Loss 620.9537 Test MSE 613.6258951642187 Test RE 0.4170047576986662 Lambda1 -3.6987014\n",
      "58 Train Loss 620.51996 Test MSE 614.0005390822128 Test RE 0.4171320375761938 Lambda1 -3.6662848\n",
      "59 Train Loss 620.2701 Test MSE 613.6770725049693 Test RE 0.4170221467542733 Lambda1 -3.6952577\n",
      "60 Train Loss 619.74677 Test MSE 613.336740431769 Test RE 0.4169064949718704 Lambda1 -3.7165778\n",
      "61 Train Loss 619.56836 Test MSE 613.1917140184111 Test RE 0.4168572022878786 Lambda1 -3.6976335\n",
      "62 Train Loss 619.1305 Test MSE 612.5027830066585 Test RE 0.4166229635029939 Lambda1 -3.7566457\n",
      "63 Train Loss 618.85455 Test MSE 612.0385654053449 Test RE 0.41646505371275633 Lambda1 -3.8293426\n",
      "64 Train Loss 618.662 Test MSE 612.4398646297471 Test RE 0.4166015644868686 Lambda1 -3.849614\n",
      "65 Train Loss 618.24384 Test MSE 611.6551287465807 Test RE 0.41633457746802127 Lambda1 -3.8164277\n",
      "66 Train Loss 618.03217 Test MSE 611.2968705875136 Test RE 0.4162126320289638 Lambda1 -3.8039649\n",
      "67 Train Loss 617.41876 Test MSE 611.1069045102194 Test RE 0.4161479560683724 Lambda1 -3.8329086\n",
      "68 Train Loss 616.8601 Test MSE 610.4181008945388 Test RE 0.4159133612435199 Lambda1 -3.7926993\n",
      "69 Train Loss 616.3962 Test MSE 610.0291865865454 Test RE 0.41578084516180314 Lambda1 -3.707192\n",
      "70 Train Loss 615.9551 Test MSE 609.6137661760744 Test RE 0.4156392508998042 Lambda1 -3.7545414\n",
      "71 Train Loss 615.5434 Test MSE 609.0729151342086 Test RE 0.41545483183231097 Lambda1 -3.7921705\n",
      "72 Train Loss 615.18835 Test MSE 609.2054835422797 Test RE 0.4155000425022676 Lambda1 -3.7996593\n",
      "73 Train Loss 614.9275 Test MSE 608.7879081852345 Test RE 0.41535761738874316 Lambda1 -3.8316653\n",
      "74 Train Loss 614.8059 Test MSE 608.6378583505054 Test RE 0.4153064269973144 Lambda1 -3.818478\n",
      "Training time: 159.98\n",
      "Training time: 159.98\n",
      "inv_HT_atanh_tune20\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 838.0604 Test MSE 858.2715875429179 Test RE 0.4931755510530557 Lambda1 -0.018431686\n",
      "1 Train Loss 837.89026 Test MSE 857.8638824469507 Test RE 0.49305840044911203 Lambda1 -0.026940338\n",
      "2 Train Loss 837.84216 Test MSE 857.8868864606874 Test RE 0.49306501119844687 Lambda1 -0.3750387\n",
      "3 Train Loss 836.0556 Test MSE 854.5953888649767 Test RE 0.49211821855897514 Lambda1 -0.75764984\n",
      "4 Train Loss 832.84674 Test MSE 850.754373924198 Test RE 0.491011050082114 Lambda1 -0.60934657\n",
      "5 Train Loss 826.9566 Test MSE 844.307384351358 Test RE 0.4891470790320717 Lambda1 -1.0475689\n",
      "6 Train Loss 818.603 Test MSE 834.4773752006989 Test RE 0.4862912484021984 Lambda1 -0.82920086\n",
      "7 Train Loss 806.4649 Test MSE 818.4986822851712 Test RE 0.4816129575939376 Lambda1 -0.60510087\n",
      "8 Train Loss 786.9414 Test MSE 791.2301513238971 Test RE 0.4735224618700052 Lambda1 -0.53851616\n",
      "9 Train Loss 770.15674 Test MSE 776.6583382801446 Test RE 0.4691418490806032 Lambda1 -0.7284924\n",
      "10 Train Loss 754.2886 Test MSE 757.729131990152 Test RE 0.4633894723896204 Lambda1 -0.9798041\n",
      "11 Train Loss 738.6601 Test MSE 738.2539813019765 Test RE 0.4573956916094096 Lambda1 -1.1207665\n",
      "12 Train Loss 729.21875 Test MSE 728.8118269871766 Test RE 0.45446126861119274 Lambda1 -1.061368\n",
      "13 Train Loss 706.7752 Test MSE 711.4189300127341 Test RE 0.4490057257230937 Lambda1 -1.0076604\n",
      "14 Train Loss 694.28125 Test MSE 695.4271488753135 Test RE 0.4439305073584269 Lambda1 -1.1030641\n",
      "15 Train Loss 687.6556 Test MSE 692.2434327131426 Test RE 0.44291316855435675 Lambda1 -1.0707154\n",
      "16 Train Loss 682.1528 Test MSE 684.0977306990076 Test RE 0.44029955397792525 Lambda1 -1.1086508\n",
      "17 Train Loss 674.6789 Test MSE 676.8929413636743 Test RE 0.43797484064408565 Lambda1 -1.1845775\n",
      "18 Train Loss 670.06555 Test MSE 671.0902085515515 Test RE 0.4360935083046883 Lambda1 -1.1566501\n",
      "19 Train Loss 666.05365 Test MSE 671.3868840801543 Test RE 0.43618989175263806 Lambda1 -1.1608737\n",
      "20 Train Loss 661.4141 Test MSE 663.3358794974386 Test RE 0.43356669607419573 Lambda1 -1.2364799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Train Loss 657.96173 Test MSE 659.3926392394695 Test RE 0.4322760934742598 Lambda1 -1.2949969\n",
      "22 Train Loss 653.5742 Test MSE 656.0594918795001 Test RE 0.43118215849007935 Lambda1 -1.3087403\n",
      "23 Train Loss 650.77686 Test MSE 650.0540381788906 Test RE 0.4292041384858025 Lambda1 -1.4332327\n",
      "24 Train Loss 647.40063 Test MSE 645.0769079579572 Test RE 0.42755788336860134 Lambda1 -1.50562\n",
      "25 Train Loss 644.8433 Test MSE 645.4441936624011 Test RE 0.4276795847890297 Lambda1 -1.4748927\n",
      "26 Train Loss 643.4345 Test MSE 645.7769553862707 Test RE 0.4277898166721134 Lambda1 -1.4176971\n",
      "27 Train Loss 641.5524 Test MSE 644.3114192918297 Test RE 0.42730412458157785 Lambda1 -1.4421982\n",
      "28 Train Loss 640.1996 Test MSE 643.3376656068249 Test RE 0.42698110819374857 Lambda1 -1.4416604\n",
      "29 Train Loss 638.82983 Test MSE 640.9795573046472 Test RE 0.42619785540501753 Lambda1 -1.4378521\n",
      "30 Train Loss 636.4467 Test MSE 637.0314566466628 Test RE 0.424883249407821 Lambda1 -1.4188212\n",
      "31 Train Loss 635.1022 Test MSE 635.3823661197148 Test RE 0.42433294299757607 Lambda1 -1.343531\n",
      "32 Train Loss 633.25037 Test MSE 636.3862655281146 Test RE 0.42466803213573706 Lambda1 -1.2509187\n",
      "33 Train Loss 630.5981 Test MSE 634.7332832139883 Test RE 0.4241161462686848 Lambda1 -1.1327776\n",
      "34 Train Loss 627.5611 Test MSE 632.1312166707705 Test RE 0.42324592895989493 Lambda1 -1.1814514\n",
      "35 Train Loss 625.99603 Test MSE 631.6571758023886 Test RE 0.42308720123756416 Lambda1 -1.1118683\n",
      "36 Train Loss 624.09454 Test MSE 630.0800188620468 Test RE 0.42255867723394575 Lambda1 -1.0249525\n",
      "37 Train Loss 622.6157 Test MSE 627.9068032265421 Test RE 0.421829321989822 Lambda1 -0.9589623\n",
      "38 Train Loss 620.5152 Test MSE 627.0622224491092 Test RE 0.42154553081453067 Lambda1 -0.917394\n",
      "39 Train Loss 619.1342 Test MSE 626.9766099058709 Test RE 0.42151675311464715 Lambda1 -0.9011218\n",
      "40 Train Loss 616.6245 Test MSE 624.1035878309567 Test RE 0.4205498769660469 Lambda1 -0.8985446\n",
      "41 Train Loss 615.6744 Test MSE 623.1880690382693 Test RE 0.4202413042971716 Lambda1 -0.9028938\n",
      "42 Train Loss 615.0477 Test MSE 622.0093166339153 Test RE 0.4198436756066663 Lambda1 -0.9204756\n",
      "43 Train Loss 614.1981 Test MSE 621.4829117244545 Test RE 0.41966598168351266 Lambda1 -0.8919477\n",
      "44 Train Loss 613.72974 Test MSE 621.1516179914553 Test RE 0.4195541111553499 Lambda1 -0.88830614\n",
      "45 Train Loss 613.2019 Test MSE 620.9988519395956 Test RE 0.4195025154059967 Lambda1 -0.8994629\n",
      "46 Train Loss 612.3863 Test MSE 620.4210490502576 Test RE 0.41930730878578487 Lambda1 -0.93786156\n",
      "47 Train Loss 612.06226 Test MSE 619.7197521728704 Test RE 0.4190702584377961 Lambda1 -0.9581792\n",
      "48 Train Loss 611.6295 Test MSE 619.6542958894454 Test RE 0.4190481262515299 Lambda1 -0.94689953\n",
      "49 Train Loss 611.32526 Test MSE 618.9721030993 Test RE 0.41881739248878713 Lambda1 -0.9457372\n",
      "50 Train Loss 611.09937 Test MSE 618.6296335700516 Test RE 0.4187015132433178 Lambda1 -0.9371662\n",
      "51 Train Loss 610.5868 Test MSE 618.6090911408625 Test RE 0.418694561412105 Lambda1 -0.9354176\n",
      "52 Train Loss 610.20074 Test MSE 617.9792673855912 Test RE 0.41848136462914914 Lambda1 -0.9042931\n",
      "53 Train Loss 609.761 Test MSE 617.5170588848849 Test RE 0.41832483686573796 Lambda1 -0.90840757\n",
      "54 Train Loss 609.2299 Test MSE 617.095749772187 Test RE 0.41818210871372286 Lambda1 -0.9195726\n",
      "55 Train Loss 609.0891 Test MSE 616.8146279904596 Test RE 0.41808684514126304 Lambda1 -0.9092286\n",
      "56 Train Loss 608.2 Test MSE 616.006435993971 Test RE 0.4178128526139155 Lambda1 -0.9151737\n",
      "57 Train Loss 606.4699 Test MSE 614.630377407504 Test RE 0.417345928595939 Lambda1 -0.9300479\n",
      "58 Train Loss 604.6393 Test MSE 613.3192198633088 Test RE 0.41690054025689044 Lambda1 -0.921274\n",
      "59 Train Loss 602.90857 Test MSE 612.0712899251563 Test RE 0.4164761873549011 Lambda1 -0.8722921\n",
      "60 Train Loss 601.867 Test MSE 610.0554647809765 Test RE 0.41578980034990587 Lambda1 -0.8653317\n",
      "61 Train Loss 600.3704 Test MSE 609.0806739400658 Test RE 0.41545747800408867 Lambda1 -0.829619\n",
      "62 Train Loss 598.71655 Test MSE 607.1092419384446 Test RE 0.41478457042723893 Lambda1 -0.8966892\n",
      "63 Train Loss 593.245 Test MSE 597.7503702717047 Test RE 0.41157510488748533 Lambda1 -0.8624548\n",
      "64 Train Loss 579.9463 Test MSE 586.2712216447832 Test RE 0.40760402025028186 Lambda1 -0.8867774\n",
      "65 Train Loss 575.4894 Test MSE 580.229940466719 Test RE 0.40549848706623925 Lambda1 -0.890874\n",
      "66 Train Loss 568.3112 Test MSE 573.1236591009479 Test RE 0.40300769545173176 Lambda1 -0.8028136\n",
      "67 Train Loss 551.6438 Test MSE 556.2961237576395 Test RE 0.3970472455647211 Lambda1 -0.7550321\n",
      "68 Train Loss 537.13306 Test MSE 538.521706402642 Test RE 0.39065265230485785 Lambda1 -0.7027423\n",
      "69 Train Loss 524.40857 Test MSE 524.9862194089128 Test RE 0.38571197502335797 Lambda1 -0.6459417\n",
      "70 Train Loss 509.23703 Test MSE 509.70709752449216 Test RE 0.38005767843798327 Lambda1 -0.535059\n",
      "71 Train Loss 487.5867 Test MSE 486.05601642122724 Test RE 0.3711353586501479 Lambda1 -0.46740657\n",
      "72 Train Loss 469.7891 Test MSE 466.74783718142595 Test RE 0.3636891356462613 Lambda1 -0.5828392\n",
      "73 Train Loss 458.12897 Test MSE 457.5356386882697 Test RE 0.36008218451480306 Lambda1 -0.58972937\n",
      "74 Train Loss 435.4187 Test MSE 434.17135445149324 Test RE 0.35076782821850105 Lambda1 -0.55894154\n",
      "Training time: 159.22\n",
      "Training time: 159.22\n",
      "inv_HT_atanh_tune20\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "1 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "2 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "3 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "4 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "5 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "6 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "7 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "8 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "9 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "10 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "11 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "12 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "13 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "14 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "15 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "16 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "17 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "18 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "19 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "20 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "21 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "22 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "24 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "25 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "26 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "27 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "28 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "29 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "30 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "31 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "32 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "33 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "34 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "35 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "36 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "37 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "38 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "39 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "40 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "41 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "42 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "43 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "44 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "45 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "46 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "47 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "48 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "49 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "50 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "51 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "52 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "53 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "54 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "55 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "56 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "57 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "58 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "59 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "60 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "61 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "62 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "63 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "64 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "65 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "66 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "67 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "68 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "69 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "70 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "71 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "72 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "73 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "74 Train Loss 838.0632 Test MSE 858.2829692517647 Test RE 0.4931788210913539 Lambda1 -0.2268887\n",
      "Training time: 100.45\n",
      "Training time: 100.45\n",
      "inv_HT_atanh_tune20\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "1 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "2 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "3 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "4 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "5 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "6 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "7 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "8 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "9 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "10 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "11 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "12 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "13 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "14 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "15 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "16 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "17 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "18 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "19 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "20 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "21 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "22 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "23 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "24 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "26 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "27 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "28 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "29 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "30 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "31 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "32 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "33 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "34 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "35 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "36 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "37 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "38 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "39 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "40 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "41 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "42 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "43 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "44 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "45 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "46 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "47 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "48 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "49 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "50 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "51 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "52 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "53 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "54 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "55 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "56 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "57 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "58 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "59 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "60 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "61 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "62 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "63 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "64 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "65 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "66 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "67 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "68 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "69 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "70 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "71 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "72 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "73 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "74 Train Loss 838.0633 Test MSE 858.2833461553447 Test RE 0.4931789293778105 Lambda1 -0.039522726\n",
      "Training time: 113.35\n",
      "Training time: 113.35\n",
      "inv_HT_atanh_tune20\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 838.0619 Test MSE 858.273825889142 Test RE 0.493176194146063 Lambda1 0.038888182\n",
      "1 Train Loss 837.8928 Test MSE 857.9156447857945 Test RE 0.49307327546106844 Lambda1 0.03179491\n",
      "2 Train Loss 837.68085 Test MSE 857.5179898659218 Test RE 0.4929589893183143 Lambda1 -0.6608566\n",
      "3 Train Loss 835.5017 Test MSE 854.3269159836514 Test RE 0.49204091251689147 Lambda1 -0.6914403\n",
      "4 Train Loss 825.0046 Test MSE 839.9047857575197 Test RE 0.4878700956851852 Lambda1 -0.77199006\n",
      "5 Train Loss 818.54767 Test MSE 832.6895160599834 Test RE 0.48577003209372865 Lambda1 -0.92944276\n",
      "6 Train Loss 807.9722 Test MSE 820.6837667085704 Test RE 0.4822553920752246 Lambda1 -1.1333408\n",
      "7 Train Loss 796.85736 Test MSE 808.2904819263977 Test RE 0.47860023003332797 Lambda1 -1.1000348\n",
      "8 Train Loss 779.19037 Test MSE 785.2336400964786 Test RE 0.4717247022999994 Lambda1 -1.1257106\n",
      "9 Train Loss 761.65686 Test MSE 766.233856512512 Test RE 0.46598274936745754 Lambda1 -1.1855266\n",
      "10 Train Loss 748.4627 Test MSE 749.0404421599618 Test RE 0.46072502683083455 Lambda1 -1.1568735\n",
      "11 Train Loss 741.15656 Test MSE 746.1620548398257 Test RE 0.4598389454721166 Lambda1 -1.1572088\n",
      "12 Train Loss 728.10315 Test MSE 735.5196874047457 Test RE 0.4565478706352007 Lambda1 -1.2196155\n",
      "13 Train Loss 719.8712 Test MSE 725.153751424192 Test RE 0.45331931062478875 Lambda1 -1.1551862\n",
      "14 Train Loss 712.6422 Test MSE 721.2098693389636 Test RE 0.45208489970929994 Lambda1 -1.0867589\n",
      "15 Train Loss 703.7083 Test MSE 708.8080886745569 Test RE 0.448181063783029 Lambda1 -1.158454\n",
      "16 Train Loss 696.6709 Test MSE 701.8931633633722 Test RE 0.4459895437503524 Lambda1 -1.3338295\n",
      "17 Train Loss 690.8931 Test MSE 697.1504786206056 Test RE 0.44448021646704006 Lambda1 -1.3810914\n",
      "18 Train Loss 687.109 Test MSE 690.7358323910515 Test RE 0.4424306071047541 Lambda1 -1.41271\n",
      "19 Train Loss 682.5252 Test MSE 685.5789929924167 Test RE 0.44077598185609007 Lambda1 -1.4778372\n",
      "20 Train Loss 675.58356 Test MSE 680.2310943395194 Test RE 0.43905346684397445 Lambda1 -1.5822381\n",
      "21 Train Loss 669.03906 Test MSE 672.8012623992067 Test RE 0.43664910011980973 Lambda1 -1.6533325\n",
      "22 Train Loss 663.75354 Test MSE 668.3539445545545 Test RE 0.4352035491187416 Lambda1 -1.6728592\n",
      "23 Train Loss 659.1927 Test MSE 661.1018120556785 Test RE 0.4328359696021109 Lambda1 -1.6499635\n",
      "24 Train Loss 655.4291 Test MSE 656.363909261525 Test RE 0.4312821830477959 Lambda1 -1.5798532\n",
      "25 Train Loss 651.2682 Test MSE 651.5600766231491 Test RE 0.429701038694181 Lambda1 -1.615874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 Train Loss 648.3061 Test MSE 647.5713648687754 Test RE 0.4283837505269448 Lambda1 -1.6513373\n",
      "27 Train Loss 643.4686 Test MSE 644.3903861144485 Test RE 0.427330308981064 Lambda1 -1.6338041\n",
      "28 Train Loss 641.68054 Test MSE 642.2675301515696 Test RE 0.4266258376881191 Lambda1 -1.5907991\n",
      "29 Train Loss 637.737 Test MSE 640.536512991487 Test RE 0.42605053621669753 Lambda1 -1.6328274\n",
      "30 Train Loss 635.9059 Test MSE 638.0061226201194 Test RE 0.4252081634766213 Lambda1 -1.6953354\n",
      "31 Train Loss 633.57 Test MSE 636.0896988138732 Test RE 0.4245690693783705 Lambda1 -1.7971768\n",
      "32 Train Loss 632.55316 Test MSE 633.2804565406265 Test RE 0.42363049327622365 Lambda1 -1.8753806\n",
      "33 Train Loss 631.81256 Test MSE 631.2143452908502 Test RE 0.42293887016697074 Lambda1 -1.8835576\n",
      "34 Train Loss 630.2222 Test MSE 630.8369030330077 Test RE 0.42281240054984753 Lambda1 -1.914739\n",
      "35 Train Loss 628.0252 Test MSE 631.7957838898184 Test RE 0.4231336188935895 Lambda1 -2.002991\n",
      "36 Train Loss 626.22565 Test MSE 629.9399647654054 Test RE 0.4225117114826903 Lambda1 -2.0534248\n",
      "37 Train Loss 624.7696 Test MSE 630.012378898189 Test RE 0.422535995495185 Lambda1 -2.132029\n",
      "38 Train Loss 623.0959 Test MSE 628.0692086065887 Test RE 0.4218838706280839 Lambda1 -2.2330475\n",
      "39 Train Loss 619.754 Test MSE 624.5679978132508 Test RE 0.42070631833652716 Lambda1 -2.304453\n",
      "40 Train Loss 617.0556 Test MSE 615.1930745660858 Test RE 0.4175369260248286 Lambda1 -2.4304173\n",
      "41 Train Loss 616.2885 Test MSE 613.5327381429752 Test RE 0.4169731029104097 Lambda1 -2.4473324\n",
      "42 Train Loss 612.3341 Test MSE 611.0286486018791 Test RE 0.41612131009309244 Lambda1 -2.445889\n",
      "43 Train Loss 609.65356 Test MSE 604.0760375162376 Test RE 0.4137471115245383 Lambda1 -2.533696\n",
      "44 Train Loss 602.66583 Test MSE 598.5468490953499 Test RE 0.41184921741472763 Lambda1 -2.5671988\n",
      "45 Train Loss 596.10004 Test MSE 592.9827766200749 Test RE 0.4099304792569686 Lambda1 -2.613283\n",
      "46 Train Loss 589.41284 Test MSE 589.8754158130793 Test RE 0.4088550053202147 Lambda1 -2.6172092\n",
      "47 Train Loss 579.24506 Test MSE 578.0400232834826 Test RE 0.4047325428149292 Lambda1 -2.6975656\n",
      "48 Train Loss 572.8711 Test MSE 570.3533061862709 Test RE 0.4020324905433117 Lambda1 -2.726128\n",
      "49 Train Loss 564.08813 Test MSE 561.3848728906527 Test RE 0.39885911711607763 Lambda1 -2.6300855\n",
      "50 Train Loss 561.9387 Test MSE 560.7077130088405 Test RE 0.39861848639821323 Lambda1 -2.6478062\n",
      "51 Train Loss 557.9465 Test MSE 560.3996388182962 Test RE 0.39850896325662966 Lambda1 -2.6804185\n",
      "52 Train Loss 552.87787 Test MSE 554.6317073168758 Test RE 0.3964528256265226 Lambda1 -2.8029525\n",
      "53 Train Loss 549.7421 Test MSE 551.854346599639 Test RE 0.3954589459014528 Lambda1 -2.8101573\n",
      "54 Train Loss 545.6608 Test MSE 542.2641527041143 Test RE 0.3920077186976432 Lambda1 -2.844332\n",
      "55 Train Loss 539.74774 Test MSE 532.4412025468389 Test RE 0.3884409418240572 Lambda1 -2.9428043\n",
      "56 Train Loss 532.36804 Test MSE 521.4767679885487 Test RE 0.3844206008924326 Lambda1 -2.918077\n",
      "57 Train Loss 526.7803 Test MSE 510.45592565887284 Test RE 0.38033675384450066 Lambda1 -2.9250517\n",
      "58 Train Loss 521.95404 Test MSE 502.3961797547672 Test RE 0.3773221798869895 Lambda1 -2.97057\n",
      "59 Train Loss 515.94495 Test MSE 492.8177675462146 Test RE 0.3737079606115886 Lambda1 -2.9620929\n",
      "60 Train Loss 510.46924 Test MSE 501.55788744092246 Test RE 0.3770072508007223 Lambda1 -2.9356208\n",
      "61 Train Loss 493.7397 Test MSE 496.91534896101075 Test RE 0.3752583601971341 Lambda1 -2.8331714\n",
      "62 Train Loss 476.45804 Test MSE 469.68492786050484 Test RE 0.3648316292081254 Lambda1 -2.7314093\n",
      "63 Train Loss 470.66043 Test MSE 461.44543549009876 Test RE 0.36161742374242306 Lambda1 -2.6953578\n",
      "64 Train Loss 467.29636 Test MSE 461.0373597765882 Test RE 0.3614574915854531 Lambda1 -2.6422083\n",
      "65 Train Loss 462.23294 Test MSE 457.96081144059934 Test RE 0.36024945190339697 Lambda1 -2.6408343\n",
      "66 Train Loss 457.7635 Test MSE 450.8063325357767 Test RE 0.35742438175196245 Lambda1 -2.6638477\n",
      "67 Train Loss 445.3163 Test MSE 433.0460010828005 Test RE 0.3503129457499856 Lambda1 -2.6124296\n",
      "68 Train Loss 432.8999 Test MSE 427.9242727200881 Test RE 0.34823517055724545 Lambda1 -2.6224813\n",
      "69 Train Loss 423.3621 Test MSE 422.11088478846773 Test RE 0.3458616796494889 Lambda1 -2.601216\n",
      "70 Train Loss 413.9903 Test MSE 410.2152733764097 Test RE 0.34095344565167157 Lambda1 -2.5923736\n",
      "71 Train Loss 408.61948 Test MSE 406.4599646072059 Test RE 0.3393892312569379 Lambda1 -2.5772934\n",
      "72 Train Loss 402.45062 Test MSE 397.27942501412844 Test RE 0.3355345201302394 Lambda1 -2.5468352\n",
      "73 Train Loss 397.9279 Test MSE 390.2450087632694 Test RE 0.3325506868801368 Lambda1 -2.5520496\n",
      "74 Train Loss 393.2285 Test MSE 390.8949749844756 Test RE 0.33282750885533235 Lambda1 -2.5611327\n",
      "Training time: 156.98\n",
      "Training time: 156.98\n",
      "inv_HT_atanh_tune21\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 27620.814 Test MSE 3577.9082946843446 Test RE 1.006940417044254 Lambda1 -7.191058e-06\n",
      "1 Train Loss 21263.14 Test MSE 3574.0234508400163 Test RE 1.0063936075145268 Lambda1 -0.0004417915\n",
      "2 Train Loss 17506.697 Test MSE 3570.9449298739687 Test RE 1.0059600805220232 Lambda1 0.00048235978\n",
      "3 Train Loss 15318.776 Test MSE 3576.5359105014777 Test RE 1.0067472816242935 Lambda1 -0.00020491476\n",
      "4 Train Loss 12707.366 Test MSE 3584.8728487099147 Test RE 1.0079199672382402 Lambda1 9.728629e-05\n",
      "5 Train Loss 10883.585 Test MSE 3586.171969365111 Test RE 1.008102580554787 Lambda1 -0.00069159595\n",
      "6 Train Loss 9506.818 Test MSE 3584.0459467363553 Test RE 1.0078037149870926 Lambda1 0.00021559885\n",
      "7 Train Loss 8436.241 Test MSE 3582.120736112739 Test RE 1.0075330020732318 Lambda1 -0.0002509207\n",
      "8 Train Loss 7542.549 Test MSE 3579.7906165009736 Test RE 1.0072052556784457 Lambda1 5.2776726e-05\n",
      "9 Train Loss 6891.7295 Test MSE 3580.816996529352 Test RE 1.0073496358085303 Lambda1 0.00021409389\n",
      "10 Train Loss 6367.369 Test MSE 3580.5626013409146 Test RE 1.007313852151847 Lambda1 8.0841055e-05\n",
      "11 Train Loss 5781.8516 Test MSE 3577.22168532922 Test RE 1.006843795243783 Lambda1 4.249858e-05\n",
      "12 Train Loss 5352.546 Test MSE 3577.9676154959625 Test RE 1.0069487644179547 Lambda1 -4.284664e-05\n",
      "13 Train Loss 5009.813 Test MSE 3578.07125640973 Test RE 1.0069633481629106 Lambda1 0.00028696115\n",
      "14 Train Loss 4818.5117 Test MSE 3578.9293548190826 Test RE 1.0070840866538138 Lambda1 -2.6908223e-05\n",
      "15 Train Loss 4667.7246 Test MSE 3580.2851157599307 Test RE 1.0072748191120324 Lambda1 0.00029904785\n",
      "16 Train Loss 4552.5205 Test MSE 3580.196026233089 Test RE 1.0072622868426204 Lambda1 0.00012408858\n",
      "17 Train Loss 4411.1777 Test MSE 3579.040584468467 Test RE 1.0070997361300145 Lambda1 -0.00049499946\n",
      "18 Train Loss 4306.9746 Test MSE 3579.3795003521413 Test RE 1.0071474184640095 Lambda1 -0.00077288056\n",
      "19 Train Loss 4213.17 Test MSE 3578.587841507988 Test RE 1.0070360358553663 Lambda1 -0.00041047262\n",
      "20 Train Loss 4142.682 Test MSE 3576.872810115425 Test RE 1.0067946968875685 Lambda1 0.00048307457\n",
      "21 Train Loss 4080.003 Test MSE 3575.215576187528 Test RE 1.0065614361980144 Lambda1 -0.0001505471\n",
      "22 Train Loss 4021.5146 Test MSE 3574.431160707446 Test RE 1.0064510084953553 Lambda1 -5.312507e-05\n",
      "23 Train Loss 3963.0972 Test MSE 3572.5253082612358 Test RE 1.0061826577028956 Lambda1 5.8098114e-05\n",
      "24 Train Loss 3920.274 Test MSE 3571.670257535027 Test RE 1.0060622402638968 Lambda1 -2.1875152e-05\n",
      "25 Train Loss 3877.3516 Test MSE 3569.477525927828 Test RE 1.005753370323452 Lambda1 -0.00019581297\n",
      "26 Train Loss 3847.0496 Test MSE 3567.346757740478 Test RE 1.0054531377010243 Lambda1 -0.00023763365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 Train Loss 3824.2434 Test MSE 3565.786510200739 Test RE 1.005233236544808 Lambda1 4.9364415e-05\n",
      "28 Train Loss 3803.3179 Test MSE 3564.6842598697795 Test RE 1.0050778564757843 Lambda1 0.00021513434\n",
      "29 Train Loss 3781.3103 Test MSE 3562.519375547003 Test RE 1.0047726109621844 Lambda1 -0.00014210837\n",
      "30 Train Loss 3761.9019 Test MSE 3558.818137965396 Test RE 1.004250526983681 Lambda1 5.5168653e-05\n",
      "31 Train Loss 3746.1628 Test MSE 3557.110887600346 Test RE 1.0040096165628347 Lambda1 -0.0003117376\n",
      "32 Train Loss 3736.743 Test MSE 3556.0253581620304 Test RE 1.0038564072319287 Lambda1 -0.00010046833\n",
      "33 Train Loss 3726.035 Test MSE 3554.302913621417 Test RE 1.003613257050794 Lambda1 0.00021350503\n",
      "34 Train Loss 3710.227 Test MSE 3552.6964529427423 Test RE 1.003386426714321 Lambda1 0.00014529136\n",
      "35 Train Loss 3700.676 Test MSE 3551.2707293217627 Test RE 1.0031850732775631 Lambda1 -0.00030644384\n",
      "36 Train Loss 3693.6335 Test MSE 3549.1621006636433 Test RE 1.0028871997760616 Lambda1 -8.329179e-05\n",
      "37 Train Loss 3686.544 Test MSE 3547.228534831678 Test RE 1.0026139785833743 Lambda1 0.00016607354\n",
      "38 Train Loss 3678.2502 Test MSE 3544.955401130388 Test RE 1.0022926797386764 Lambda1 0.00015407571\n",
      "39 Train Loss 3667.6223 Test MSE 3540.864481890841 Test RE 1.0017141841620023 Lambda1 -8.9445595e-05\n",
      "40 Train Loss 3657.41 Test MSE 3537.5576461245923 Test RE 1.0012463212909117 Lambda1 6.8427136e-05\n",
      "41 Train Loss 3653.0967 Test MSE 3535.7137093479605 Test RE 1.0009853395985637 Lambda1 -0.000104137216\n",
      "42 Train Loss 3647.3284 Test MSE 3533.2613201803715 Test RE 1.0006381350990337 Lambda1 -0.00018311\n",
      "43 Train Loss 3639.4917 Test MSE 3530.798646913606 Test RE 1.0002893533214763 Lambda1 -0.0002373655\n",
      "44 Train Loss 3633.4429 Test MSE 3529.859158170664 Test RE 1.0001562640117718 Lambda1 -0.00023789692\n",
      "45 Train Loss 3628.032 Test MSE 3527.3175115916492 Test RE 0.9997961219505291 Lambda1 -0.00020313496\n",
      "46 Train Loss 3622.363 Test MSE 3524.746875695618 Test RE 0.9994317402039029 Lambda1 -0.00022624666\n",
      "47 Train Loss 3615.9333 Test MSE 3520.540467140243 Test RE 0.998835204710525 Lambda1 -0.00038913655\n",
      "48 Train Loss 3611.3042 Test MSE 3514.750239477242 Test RE 0.9980134753104173 Lambda1 -0.0004505501\n",
      "49 Train Loss 3605.6118 Test MSE 3511.377663951757 Test RE 0.9975345389123851 Lambda1 -0.00026422954\n",
      "50 Train Loss 3598.893 Test MSE 3508.6198010841495 Test RE 0.9971427263367866 Lambda1 -0.000112249065\n",
      "51 Train Loss 3594.6816 Test MSE 3505.9902496372006 Test RE 0.9967689996712625 Lambda1 -0.00037183284\n",
      "52 Train Loss 3590.2234 Test MSE 3500.4010650509967 Test RE 0.9959741674483953 Lambda1 -0.0001625412\n",
      "53 Train Loss 3583.9949 Test MSE 3497.236097775306 Test RE 0.9955237992691118 Lambda1 0.0001198333\n",
      "54 Train Loss 3579.9062 Test MSE 3492.7589217161953 Test RE 0.99488635835573 Lambda1 0.0001571236\n",
      "55 Train Loss 3572.987 Test MSE 3484.6851505349205 Test RE 0.9937358162938901 Lambda1 0.00019861096\n",
      "56 Train Loss 3564.343 Test MSE 3475.4782234409026 Test RE 0.9924221675495046 Lambda1 8.1939375e-05\n",
      "57 Train Loss 3559.1343 Test MSE 3472.1463192840456 Test RE 0.991946341170111 Lambda1 -0.00027694955\n",
      "58 Train Loss 3555.2688 Test MSE 3468.354349986243 Test RE 0.9914045354308376 Lambda1 -0.00053288\n",
      "59 Train Loss 3551.0283 Test MSE 3464.027339027821 Test RE 0.9907859197053053 Lambda1 -0.00039510633\n",
      "60 Train Loss 3545.192 Test MSE 3459.174417377139 Test RE 0.9900916567618525 Lambda1 -0.00047844983\n",
      "61 Train Loss 3539.6377 Test MSE 3453.0452032457397 Test RE 0.9892141100455435 Lambda1 -0.0003085673\n",
      "62 Train Loss 3533.8975 Test MSE 3446.159966095242 Test RE 0.9882273907974292 Lambda1 -0.00013064344\n",
      "63 Train Loss 3527.2954 Test MSE 3441.8442969862567 Test RE 0.987608412204312 Lambda1 0.00027958426\n",
      "64 Train Loss 3521.2837 Test MSE 3432.633199421843 Test RE 0.9862860031767192 Lambda1 0.00016566365\n",
      "65 Train Loss 3511.1274 Test MSE 3422.267216994454 Test RE 0.9847956671836668 Lambda1 -7.303977e-05\n",
      "66 Train Loss 3503.6 Test MSE 3411.212863893207 Test RE 0.9832038738510825 Lambda1 -7.686257e-06\n",
      "67 Train Loss 3496.357 Test MSE 3402.90600752505 Test RE 0.9820060137430701 Lambda1 -1.6823913e-05\n",
      "68 Train Loss 3489.6958 Test MSE 3396.5114203642966 Test RE 0.9810829092174967 Lambda1 0.0002727491\n",
      "69 Train Loss 3483.9277 Test MSE 3394.0026828266286 Test RE 0.9807205176524648 Lambda1 0.00028401945\n",
      "70 Train Loss 3476.9077 Test MSE 3385.402597859886 Test RE 0.9794772025658424 Lambda1 0.00021239463\n",
      "71 Train Loss 3468.7927 Test MSE 3364.2119890228846 Test RE 0.9764069178167631 Lambda1 0.00025429926\n",
      "72 Train Loss 3460.4236 Test MSE 3337.6837856604598 Test RE 0.972549612403552 Lambda1 0.00047818\n",
      "73 Train Loss 3451.0845 Test MSE 3327.218145081065 Test RE 0.971023652083664 Lambda1 0.0004261184\n",
      "74 Train Loss 3443.8096 Test MSE 3318.284411514115 Test RE 0.9697191542793377 Lambda1 0.0005613869\n",
      "Training time: 146.52\n",
      "Training time: 146.52\n",
      "inv_HT_atanh_tune21\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 5967.042 Test MSE 3509.1870677306324 Test RE 0.9972233311017978 Lambda1 -0.001955802\n",
      "1 Train Loss 3834.7085 Test MSE 3493.0714315762348 Test RE 0.9949308654129424 Lambda1 -0.00032934974\n",
      "2 Train Loss 3520.2766 Test MSE 3460.810101101634 Test RE 0.9903257133913628 Lambda1 0.00015611178\n",
      "3 Train Loss 2829.8533 Test MSE 2803.75819670142 Test RE 0.8913729460902837 Lambda1 0.0837103\n",
      "4 Train Loss 851.5642 Test MSE 859.2654974768388 Test RE 0.4934610261050522 Lambda1 1.0964348\n",
      "5 Train Loss 838.0642 Test MSE 858.2670742196301 Test RE 0.4931742543401864 Lambda1 1.1089464\n",
      "6 Train Loss 838.0501 Test MSE 858.2529398039171 Test RE 0.4931701933906062 Lambda1 1.1048838\n",
      "7 Train Loss 838.04254 Test MSE 858.2296360062331 Test RE 0.4931634979189724 Lambda1 1.0962332\n",
      "8 Train Loss 838.0317 Test MSE 858.1997438764624 Test RE 0.4931549094042993 Lambda1 1.0853138\n",
      "9 Train Loss 838.0166 Test MSE 858.1948509287935 Test RE 0.49315350356339743 Lambda1 1.0764141\n",
      "10 Train Loss 838.00525 Test MSE 858.1378005846791 Test RE 0.4931371115723239 Lambda1 1.0712209\n",
      "11 Train Loss 837.97516 Test MSE 858.1131207816665 Test RE 0.4931300202786702 Lambda1 1.0473955\n",
      "12 Train Loss 837.89087 Test MSE 857.9210527451113 Test RE 0.49307482952761805 Lambda1 1.0311638\n",
      "13 Train Loss 837.82294 Test MSE 857.7452366711469 Test RE 0.49302430336189196 Lambda1 1.0212693\n",
      "14 Train Loss 837.70404 Test MSE 857.4823241814137 Test RE 0.4929487376951333 Lambda1 1.0118026\n",
      "15 Train Loss 837.5089 Test MSE 857.1400527278848 Test RE 0.49285034550890017 Lambda1 1.0013415\n",
      "16 Train Loss 837.2102 Test MSE 856.7602672474688 Test RE 0.492741146234468 Lambda1 0.9927068\n",
      "17 Train Loss 836.63196 Test MSE 855.2947613852887 Test RE 0.4923195439914245 Lambda1 0.9645268\n",
      "18 Train Loss 835.9029 Test MSE 854.5573153605031 Test RE 0.4921072561350425 Lambda1 0.93488115\n",
      "19 Train Loss 834.71234 Test MSE 851.7013592738031 Test RE 0.49128424935283993 Lambda1 0.9192495\n",
      "20 Train Loss 833.8657 Test MSE 850.2834219321417 Test RE 0.4908751268060511 Lambda1 0.90607494\n",
      "21 Train Loss 831.9035 Test MSE 845.3487124595847 Test RE 0.48944863131790767 Lambda1 0.8924862\n",
      "22 Train Loss 830.6339 Test MSE 844.3123801545503 Test RE 0.4891485261821081 Lambda1 0.8657146\n",
      "23 Train Loss 828.3253 Test MSE 840.4619234419292 Test RE 0.48803187935375886 Lambda1 0.8143457\n",
      "24 Train Loss 824.8968 Test MSE 835.0095038068391 Test RE 0.48644627252392286 Lambda1 0.79029286\n",
      "25 Train Loss 821.54584 Test MSE 829.4598359183711 Test RE 0.4848270599388103 Lambda1 0.76985925\n",
      "26 Train Loss 816.511 Test MSE 824.0056098666051 Test RE 0.4832304076966084 Lambda1 0.7296778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 Train Loss 812.40497 Test MSE 819.393552024288 Test RE 0.4818761609056071 Lambda1 0.7248718\n",
      "28 Train Loss 809.7058 Test MSE 818.0112638358332 Test RE 0.48146953524970754 Lambda1 0.7072186\n",
      "29 Train Loss 806.3978 Test MSE 814.4153993388335 Test RE 0.4804101329430173 Lambda1 0.70215267\n",
      "30 Train Loss 803.42053 Test MSE 813.6491437057321 Test RE 0.4801840790159955 Lambda1 0.66723686\n",
      "31 Train Loss 798.84216 Test MSE 804.4398060407639 Test RE 0.47745884919732245 Lambda1 0.6560506\n",
      "32 Train Loss 795.23425 Test MSE 801.7711609829141 Test RE 0.4766662313562565 Lambda1 0.6509073\n",
      "33 Train Loss 791.9187 Test MSE 795.6043519410441 Test RE 0.47482955780897695 Lambda1 0.65173364\n",
      "34 Train Loss 788.44916 Test MSE 790.1945471248168 Test RE 0.4732124746850995 Lambda1 0.6613601\n",
      "35 Train Loss 785.9888 Test MSE 783.7581811448391 Test RE 0.47128130707424026 Lambda1 0.67417395\n",
      "36 Train Loss 781.8425 Test MSE 779.4885001779489 Test RE 0.46999585394572363 Lambda1 0.6767531\n",
      "37 Train Loss 776.07117 Test MSE 771.8472907442448 Test RE 0.46768653091407103 Lambda1 0.674848\n",
      "38 Train Loss 772.2296 Test MSE 766.7463364759051 Test RE 0.4661385548721228 Lambda1 0.6835597\n",
      "39 Train Loss 769.4918 Test MSE 766.4383196548546 Test RE 0.4660449170342402 Lambda1 0.67776567\n",
      "40 Train Loss 768.39435 Test MSE 764.940328648589 Test RE 0.46558925574258553 Lambda1 0.67345273\n",
      "41 Train Loss 766.14044 Test MSE 763.2128126606558 Test RE 0.46506322287774765 Lambda1 0.67512333\n",
      "42 Train Loss 764.41077 Test MSE 761.25601795543 Test RE 0.4644666544681421 Lambda1 0.6813605\n",
      "43 Train Loss 762.91766 Test MSE 760.2651149101472 Test RE 0.46416426520651954 Lambda1 0.669605\n",
      "44 Train Loss 759.59644 Test MSE 757.0508048059834 Test RE 0.4631820103546863 Lambda1 0.6641991\n",
      "45 Train Loss 756.57245 Test MSE 752.0332897156773 Test RE 0.4616445400394447 Lambda1 0.6614774\n",
      "46 Train Loss 755.1531 Test MSE 749.8453107247237 Test RE 0.46097249246040156 Lambda1 0.6607654\n",
      "47 Train Loss 752.3888 Test MSE 747.6512642592497 Test RE 0.46029759593652325 Lambda1 0.6603911\n",
      "48 Train Loss 751.0313 Test MSE 746.1338582882398 Test RE 0.45983025701432345 Lambda1 0.65979975\n",
      "49 Train Loss 748.0247 Test MSE 739.6955456926863 Test RE 0.4578420446205536 Lambda1 0.647611\n",
      "50 Train Loss 745.3761 Test MSE 735.443500882869 Test RE 0.4565242249764284 Lambda1 0.62898034\n",
      "51 Train Loss 741.44806 Test MSE 730.9721513925282 Test RE 0.45513432115066355 Lambda1 0.6176202\n",
      "52 Train Loss 737.83435 Test MSE 728.6204529949529 Test RE 0.4544015976685897 Lambda1 0.6086915\n",
      "53 Train Loss 735.88983 Test MSE 729.9035620476176 Test RE 0.4548015249138671 Lambda1 0.601023\n",
      "54 Train Loss 734.9307 Test MSE 730.32938964441 Test RE 0.45493417175395345 Lambda1 0.59342605\n",
      "55 Train Loss 732.3483 Test MSE 727.0880537868338 Test RE 0.453923508458947 Lambda1 0.5863095\n",
      "56 Train Loss 730.61743 Test MSE 724.8611776649589 Test RE 0.4532278522857215 Lambda1 0.59265906\n",
      "57 Train Loss 728.67084 Test MSE 720.5188973680042 Test RE 0.4518682825610889 Lambda1 0.60189044\n",
      "58 Train Loss 724.4903 Test MSE 716.8208228124377 Test RE 0.4507071802724446 Lambda1 0.5952732\n",
      "59 Train Loss 723.0592 Test MSE 714.8966481137755 Test RE 0.45010185318508356 Lambda1 0.6002712\n",
      "60 Train Loss 722.1721 Test MSE 713.0691479127939 Test RE 0.4495261842520483 Lambda1 0.60427505\n",
      "61 Train Loss 721.58997 Test MSE 713.9938582077037 Test RE 0.44981756329619377 Lambda1 0.59807\n",
      "62 Train Loss 721.0956 Test MSE 713.0999658257024 Test RE 0.4495358981130099 Lambda1 0.5963946\n",
      "63 Train Loss 720.16504 Test MSE 713.2738401692307 Test RE 0.44959069968285936 Lambda1 0.5910074\n",
      "64 Train Loss 719.54944 Test MSE 713.2226973609572 Test RE 0.44957458122888044 Lambda1 0.59394115\n",
      "65 Train Loss 718.43713 Test MSE 710.3004812508848 Test RE 0.4486526374019446 Lambda1 0.6005826\n",
      "66 Train Loss 717.4737 Test MSE 708.5145888658948 Test RE 0.4480882638570981 Lambda1 0.6049307\n",
      "67 Train Loss 715.83356 Test MSE 709.2646535289146 Test RE 0.44832538407571326 Lambda1 0.5920997\n",
      "68 Train Loss 715.2247 Test MSE 709.5937214542812 Test RE 0.4484293737468779 Lambda1 0.5874948\n",
      "69 Train Loss 714.01276 Test MSE 707.5428961219706 Test RE 0.44778089298024243 Lambda1 0.5853812\n",
      "70 Train Loss 713.2369 Test MSE 705.7691035525337 Test RE 0.44721925293402465 Lambda1 0.5840473\n",
      "71 Train Loss 711.78864 Test MSE 704.6013166571556 Test RE 0.446849108509533 Lambda1 0.58886147\n",
      "72 Train Loss 710.9766 Test MSE 705.773555603422 Test RE 0.44722066348013856 Lambda1 0.58156955\n",
      "73 Train Loss 710.39484 Test MSE 706.5523447435844 Test RE 0.4474673393193371 Lambda1 0.57375556\n",
      "74 Train Loss 708.72 Test MSE 706.8011766805835 Test RE 0.4475461263788591 Lambda1 0.57140595\n",
      "Training time: 160.52\n",
      "Training time: 160.52\n",
      "inv_HT_atanh_tune21\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 4289.156 Test MSE 3519.3178749623285 Test RE 0.9986617549010282 Lambda1 -0.00019557454\n",
      "1 Train Loss 3682.9814 Test MSE 3519.492978259518 Test RE 0.9986865987482141 Lambda1 -0.00091397145\n",
      "2 Train Loss 3494.7651 Test MSE 3358.8444803613356 Test RE 0.9756276915533254 Lambda1 0.0031468996\n",
      "3 Train Loss 3262.8528 Test MSE 3195.104483778268 Test RE 0.9515501958312181 Lambda1 0.00027248275\n",
      "4 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 9.37\n",
      "Training time: 9.37\n",
      "inv_HT_atanh_tune21\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 18042.76 Test MSE 3517.434580319762 Test RE 0.9983945119178866 Lambda1 0.00033615128\n",
      "1 Train Loss 9387.777 Test MSE 3477.2823121671304 Test RE 0.9926797127335055 Lambda1 -0.00020159918\n",
      "2 Train Loss 5763.044 Test MSE 3456.1424995994853 Test RE 0.9896576609296929 Lambda1 0.0012096664\n",
      "3 Train Loss 4276.2007 Test MSE 3462.555038481527 Test RE 0.9905753426022794 Lambda1 -0.0003913688\n",
      "4 Train Loss 3545.994 Test MSE 3451.0088982895227 Test RE 0.9889223909201317 Lambda1 -0.00022456778\n",
      "5 Train Loss 3201.9136 Test MSE 3171.671905675943 Test RE 0.9480544875540869 Lambda1 -0.0034006187\n",
      "6 Train Loss 838.03564 Test MSE 858.1427617764482 Test RE 0.49313853706846283 Lambda1 -0.12670983\n",
      "7 Train Loss 837.8985 Test MSE 857.9582793140864 Test RE 0.4930855270649837 Lambda1 -0.12527631\n",
      "8 Train Loss 837.7578 Test MSE 857.7788143405422 Test RE 0.49303395333973155 Lambda1 -0.08112355\n",
      "9 Train Loss 837.55396 Test MSE 857.3317195212448 Test RE 0.4929054460521851 Lambda1 -0.03512672\n",
      "10 Train Loss 837.283 Test MSE 856.8594518904137 Test RE 0.4927696670133551 Lambda1 0.0033123365\n",
      "11 Train Loss 836.5456 Test MSE 855.8412200399539 Test RE 0.492476793474006 Lambda1 0.014395016\n",
      "12 Train Loss 834.92554 Test MSE 852.202367889692 Test RE 0.4914287257308934 Lambda1 -0.01127112\n",
      "13 Train Loss 829.72675 Test MSE 841.6387829724671 Test RE 0.48837344393625304 Lambda1 0.00022582372\n",
      "14 Train Loss 815.92664 Test MSE 818.7873321832648 Test RE 0.4816978723774902 Lambda1 0.032486536\n",
      "15 Train Loss 797.62024 Test MSE 802.7784592843477 Test RE 0.47696556512920524 Lambda1 0.0053125094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 Train Loss 769.706 Test MSE 761.5667381752938 Test RE 0.4645614349716684 Lambda1 0.005454223\n",
      "17 Train Loss 749.75146 Test MSE 753.4643780096092 Test RE 0.4620835765193229 Lambda1 0.0005830625\n",
      "18 Train Loss 741.0687 Test MSE 743.9682734036909 Test RE 0.4591624646008119 Lambda1 0.0007115631\n",
      "19 Train Loss 731.6768 Test MSE 736.9724103849588 Test RE 0.4569985114731737 Lambda1 0.00043385694\n",
      "20 Train Loss 728.11426 Test MSE 738.2283507219187 Test RE 0.45738775164637335 Lambda1 0.00020430455\n",
      "21 Train Loss 725.25806 Test MSE 736.0639435827989 Test RE 0.4567167533002105 Lambda1 6.244462e-05\n",
      "22 Train Loss 721.732 Test MSE 728.3794271810282 Test RE 0.4543264339990699 Lambda1 2.621179e-05\n",
      "23 Train Loss 717.2849 Test MSE 725.925345782302 Test RE 0.45356042199355623 Lambda1 -9.760501e-05\n",
      "24 Train Loss 713.2103 Test MSE 716.4206113155683 Test RE 0.45058134451719106 Lambda1 -9.160245e-06\n",
      "25 Train Loss 707.94574 Test MSE 703.9581502055928 Test RE 0.44664511799488815 Lambda1 0.00011691896\n",
      "26 Train Loss 674.8445 Test MSE 637.6950793255346 Test RE 0.42510450124905946 Lambda1 0.0002197777\n",
      "27 Train Loss 638.9971 Test MSE 604.6557358364182 Test RE 0.4139455890140424 Lambda1 -3.552408e-05\n",
      "28 Train Loss 607.0633 Test MSE 588.6742701414275 Test RE 0.40843852358366406 Lambda1 -1.5852624e-05\n",
      "29 Train Loss 600.6619 Test MSE 589.2848141950138 Test RE 0.4086502748861054 Lambda1 4.7108188e-05\n",
      "30 Train Loss 597.27783 Test MSE 587.051323360524 Test RE 0.40787511226306006 Lambda1 3.7968453e-05\n",
      "31 Train Loss 590.5342 Test MSE 583.4578448629591 Test RE 0.4066248465393134 Lambda1 2.2748685e-05\n",
      "32 Train Loss 580.35266 Test MSE 574.6648613189413 Test RE 0.40354920105839936 Lambda1 3.447997e-05\n",
      "33 Train Loss 563.95154 Test MSE 568.0022095905617 Test RE 0.4012030105434821 Lambda1 2.2877393e-05\n",
      "34 Train Loss 550.9119 Test MSE 563.8680043129153 Test RE 0.39974026555624737 Lambda1 -6.69698e-06\n",
      "35 Train Loss 545.51483 Test MSE 559.1340701947812 Test RE 0.3980587260783835 Lambda1 -5.061256e-07\n",
      "36 Train Loss 542.8041 Test MSE 558.0972622834037 Test RE 0.3976894926876208 Lambda1 7.1879103e-06\n",
      "37 Train Loss 539.3938 Test MSE 555.9873362574631 Test RE 0.3969370342586631 Lambda1 4.363551e-06\n",
      "38 Train Loss 538.0588 Test MSE 555.4062928432319 Test RE 0.39672956735791737 Lambda1 4.460196e-06\n",
      "39 Train Loss 536.10754 Test MSE 555.1931718199576 Test RE 0.3966534433338999 Lambda1 6.4400983e-06\n",
      "40 Train Loss 533.99976 Test MSE 553.7260610979765 Test RE 0.3961290137283038 Lambda1 6.558771e-06\n",
      "41 Train Loss 532.7545 Test MSE 553.2953426123611 Test RE 0.395974918325165 Lambda1 5.998154e-06\n",
      "42 Train Loss 527.72 Test MSE 550.6207745258557 Test RE 0.3950167096186695 Lambda1 2.8621164e-06\n",
      "43 Train Loss 525.50574 Test MSE 548.999940838651 Test RE 0.39443488607812444 Lambda1 1.3114159e-06\n",
      "44 Train Loss 519.7546 Test MSE 545.2803099002732 Test RE 0.3930964107486806 Lambda1 6.2805947e-07\n",
      "45 Train Loss 518.5722 Test MSE 543.6355552387095 Test RE 0.3925031054049605 Lambda1 1.1610805e-06\n",
      "46 Train Loss 517.71606 Test MSE 542.8299749198263 Test RE 0.39221218440344957 Lambda1 1.0636618e-06\n",
      "47 Train Loss 516.3947 Test MSE 542.8295185668113 Test RE 0.39221201953852064 Lambda1 1.3133154e-06\n",
      "48 Train Loss 513.95593 Test MSE 541.2104532282366 Test RE 0.3916266690014547 Lambda1 1.4976006e-06\n",
      "49 Train Loss 507.22943 Test MSE 533.1043731280076 Test RE 0.38868277363664905 Lambda1 8.9036877e-07\n",
      "50 Train Loss 505.1468 Test MSE 530.2773921800738 Test RE 0.38765083746979373 Lambda1 -8.4584303e-07\n",
      "51 Train Loss 499.47577 Test MSE 521.3287064642005 Test RE 0.38436602325411373 Lambda1 4.3332352e-07\n",
      "52 Train Loss 493.81897 Test MSE 518.1768751082469 Test RE 0.38320236827841747 Lambda1 -1.7760927e-06\n",
      "53 Train Loss 486.77383 Test MSE 513.4011443860576 Test RE 0.38143240552328367 Lambda1 -2.4931563e-07\n",
      "54 Train Loss 484.66544 Test MSE 511.87221925336104 Test RE 0.3808640230520811 Lambda1 9.8108714e-08\n",
      "55 Train Loss 482.3741 Test MSE 508.2982614820041 Test RE 0.3795320731833001 Lambda1 8.787519e-08\n",
      "56 Train Loss 480.8112 Test MSE 505.40697878726263 Test RE 0.3784511139252217 Lambda1 4.999984e-08\n",
      "57 Train Loss 478.9311 Test MSE 502.35105853488295 Test RE 0.37730523547138994 Lambda1 1.3669833e-07\n",
      "58 Train Loss 478.07108 Test MSE 502.39247922760296 Test RE 0.3773207902530673 Lambda1 -4.6961352e-07\n",
      "59 Train Loss 476.77698 Test MSE 500.09303212557074 Test RE 0.3764563025314197 Lambda1 -5.1357154e-07\n",
      "60 Train Loss 476.04785 Test MSE 498.77274361900555 Test RE 0.37595903564072447 Lambda1 -8.053766e-07\n",
      "61 Train Loss 475.66208 Test MSE 498.32222379624636 Test RE 0.3757892035225106 Lambda1 -7.340304e-07\n",
      "62 Train Loss 475.20996 Test MSE 496.63830642219267 Test RE 0.3751537377262271 Lambda1 -8.4922084e-07\n",
      "63 Train Loss 473.74008 Test MSE 494.1068745432727 Test RE 0.37419641187210656 Lambda1 -9.618637e-07\n",
      "64 Train Loss 472.53522 Test MSE 491.5929071927121 Test RE 0.3732432606065707 Lambda1 -1.1612652e-06\n",
      "65 Train Loss 472.18127 Test MSE 491.4445969138159 Test RE 0.3731869538667706 Lambda1 -1.4435392e-06\n",
      "66 Train Loss 471.6977 Test MSE 491.1055474791934 Test RE 0.3730582001301149 Lambda1 -1.0992921e-06\n",
      "67 Train Loss 471.26926 Test MSE 488.97183051525764 Test RE 0.37224690091156826 Lambda1 -1.5787789e-06\n",
      "68 Train Loss 470.27954 Test MSE 486.22911766723945 Test RE 0.3712014397930577 Lambda1 -2.807893e-06\n",
      "69 Train Loss 465.43842 Test MSE 483.13161930085306 Test RE 0.37001719059154153 Lambda1 -1.9448014e-06\n",
      "70 Train Loss 464.042 Test MSE 482.60488935574256 Test RE 0.36981543161175123 Lambda1 -2.4610617e-06\n",
      "71 Train Loss 463.07553 Test MSE 483.0437297701195 Test RE 0.36998353297621644 Lambda1 -2.0860004e-06\n",
      "72 Train Loss 462.71777 Test MSE 483.35867032174804 Test RE 0.37010412643804746 Lambda1 -2.311757e-06\n",
      "73 Train Loss 462.49924 Test MSE 483.1127600550102 Test RE 0.3700099686327775 Lambda1 -2.166649e-06\n",
      "74 Train Loss 462.02902 Test MSE 483.60601811957724 Test RE 0.37019881050788184 Lambda1 -2.455676e-06\n",
      "Training time: 158.43\n",
      "Training time: 158.43\n",
      "inv_HT_atanh_tune21\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 9339.5625 Test MSE 3585.9885294911837 Test RE 1.00807679699271 Lambda1 -6.774483e-05\n",
      "1 Train Loss 9096.035 Test MSE 3585.782413260024 Test RE 1.0080478253473448 Lambda1 -0.00015875272\n",
      "2 Train Loss 4478.9814 Test MSE 3600.344589627386 Test RE 1.0100926366553276 Lambda1 -0.0012155988\n",
      "3 Train Loss 3863.5085 Test MSE 3573.4171188501923 Test RE 1.0063082367091016 Lambda1 0.0005839153\n",
      "4 Train Loss 3665.5989 Test MSE 3559.0235938769415 Test RE 1.0042795150102681 Lambda1 0.00092508056\n",
      "5 Train Loss 3575.457 Test MSE 3492.1007764302767 Test RE 0.9947926200489046 Lambda1 -9.191145e-05\n",
      "6 Train Loss 3459.07 Test MSE 3402.2852521946415 Test RE 0.9819164412910638 Lambda1 0.00050859497\n",
      "7 Train Loss 838.0918 Test MSE 858.3036570774611 Test RE 0.4931847647814729 Lambda1 -0.03939091\n",
      "8 Train Loss 837.87836 Test MSE 857.7519685988595 Test RE 0.4930262380840663 Lambda1 -0.033825766\n",
      "9 Train Loss 837.4421 Test MSE 857.2288716969335 Test RE 0.49287588003239996 Lambda1 -0.027907297\n",
      "10 Train Loss 835.9441 Test MSE 854.4056423014307 Test RE 0.49206358280630924 Lambda1 -0.016585276\n",
      "11 Train Loss 831.72565 Test MSE 848.5867095596153 Test RE 0.49038511971685284 Lambda1 -0.011640895\n",
      "12 Train Loss 817.28284 Test MSE 832.370627337518 Test RE 0.4856770073734424 Lambda1 -0.0041740886\n",
      "13 Train Loss 806.47845 Test MSE 823.1195869170732 Test RE 0.482970537871524 Lambda1 -0.0024786\n",
      "14 Train Loss 799.7164 Test MSE 819.2699192827046 Test RE 0.4818398060196423 Lambda1 0.00094780757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Train Loss 792.3062 Test MSE 813.2341093710252 Test RE 0.48006159483339295 Lambda1 0.001630554\n",
      "16 Train Loss 784.4989 Test MSE 806.7197270184203 Test RE 0.47813497078623785 Lambda1 0.00038753918\n",
      "17 Train Loss 779.28766 Test MSE 803.9109642762623 Test RE 0.4773018817706243 Lambda1 2.76133e-05\n",
      "18 Train Loss 776.8572 Test MSE 801.8190981565378 Test RE 0.47668048086501263 Lambda1 0.00013078234\n",
      "19 Train Loss 775.5485 Test MSE 801.2016556286554 Test RE 0.4764969111018468 Lambda1 8.9489935e-05\n",
      "20 Train Loss 774.3138 Test MSE 800.209876368698 Test RE 0.4762019004209641 Lambda1 0.00011624672\n",
      "21 Train Loss 773.5904 Test MSE 799.4761387459259 Test RE 0.47598352809573796 Lambda1 0.00019667426\n",
      "22 Train Loss 772.8839 Test MSE 798.4358450953707 Test RE 0.475673747852337 Lambda1 0.0002877729\n",
      "23 Train Loss 770.1864 Test MSE 795.8992498328032 Test RE 0.47491754957404864 Lambda1 0.00022421095\n",
      "24 Train Loss 768.2215 Test MSE 793.5083827180543 Test RE 0.4742036911529697 Lambda1 0.00014926621\n",
      "25 Train Loss 765.9535 Test MSE 792.0949203696041 Test RE 0.4737811576118818 Lambda1 0.00011392137\n",
      "26 Train Loss 764.66425 Test MSE 789.9126975960523 Test RE 0.4731280735659388 Lambda1 0.000108163855\n",
      "27 Train Loss 762.9985 Test MSE 787.7601895150855 Test RE 0.47248299801292676 Lambda1 0.000107743486\n",
      "28 Train Loss 761.53925 Test MSE 785.910806077273 Test RE 0.4719280602979691 Lambda1 0.0001519826\n",
      "29 Train Loss 759.44934 Test MSE 781.8363932658173 Test RE 0.470703157712274 Lambda1 3.978981e-05\n",
      "30 Train Loss 757.7504 Test MSE 779.9673012957737 Test RE 0.4701401793552143 Lambda1 -5.9728784e-05\n",
      "31 Train Loss 753.5058 Test MSE 777.0967440005462 Test RE 0.4692742402774436 Lambda1 1.34017555e-05\n",
      "32 Train Loss 749.4282 Test MSE 773.5017604499736 Test RE 0.46818751018858407 Lambda1 0.00014961204\n",
      "33 Train Loss 743.66833 Test MSE 768.2191245004733 Test RE 0.46658602614891687 Lambda1 0.0002007678\n",
      "34 Train Loss 736.89545 Test MSE 759.7364354863154 Test RE 0.46400284995323415 Lambda1 7.2499846e-05\n",
      "35 Train Loss 725.86096 Test MSE 749.0255170346437 Test RE 0.46072043668280654 Lambda1 0.00010563092\n",
      "36 Train Loss 721.34686 Test MSE 745.0162621299772 Test RE 0.4594857497217403 Lambda1 5.5868382e-05\n",
      "37 Train Loss 714.71893 Test MSE 740.8326375644936 Test RE 0.4581938167440519 Lambda1 4.3239426e-05\n",
      "38 Train Loss 712.0167 Test MSE 739.6183684928933 Test RE 0.4578181591930381 Lambda1 1.1326334e-05\n",
      "39 Train Loss 708.54913 Test MSE 738.9964952381006 Test RE 0.4576256513057059 Lambda1 2.281143e-05\n",
      "40 Train Loss 707.3095 Test MSE 739.8736641028086 Test RE 0.4578971653988472 Lambda1 2.5451722e-05\n",
      "41 Train Loss 705.43774 Test MSE 738.4066781142783 Test RE 0.457442991898147 Lambda1 8.4717485e-06\n",
      "42 Train Loss 703.89014 Test MSE 737.1830185349854 Test RE 0.4570638061398444 Lambda1 -1.1544218e-05\n",
      "43 Train Loss 703.0572 Test MSE 737.1139633364784 Test RE 0.45704239804290137 Lambda1 -1.1308738e-05\n",
      "44 Train Loss 701.83624 Test MSE 736.351142541569 Test RE 0.4568058459563124 Lambda1 -8.089949e-06\n",
      "45 Train Loss 701.3234 Test MSE 735.564251561604 Test RE 0.4565617012389892 Lambda1 -6.096766e-06\n",
      "46 Train Loss 701.00323 Test MSE 735.1753128130111 Test RE 0.45644097893889624 Lambda1 -3.7082127e-06\n",
      "47 Train Loss 699.89844 Test MSE 733.7536913514958 Test RE 0.45599945140482584 Lambda1 -4.4431354e-06\n",
      "48 Train Loss 696.1399 Test MSE 730.664641282045 Test RE 0.45503857664940944 Lambda1 -4.2327815e-06\n",
      "49 Train Loss 693.0944 Test MSE 728.3138495056951 Test RE 0.45430598151173945 Lambda1 4.473942e-06\n",
      "50 Train Loss 691.3391 Test MSE 726.6252736893916 Test RE 0.4537790278825776 Lambda1 8.173842e-06\n",
      "51 Train Loss 686.8377 Test MSE 722.5626479548985 Test RE 0.45250869054855486 Lambda1 1.7166303e-05\n",
      "52 Train Loss 684.65314 Test MSE 719.8025777901186 Test RE 0.4516436093515964 Lambda1 1.487783e-05\n",
      "53 Train Loss 682.88605 Test MSE 717.6933776946934 Test RE 0.45098140999223524 Lambda1 1.1552432e-05\n",
      "54 Train Loss 680.67444 Test MSE 711.9936789747011 Test RE 0.44918706295121696 Lambda1 1.530063e-05\n",
      "55 Train Loss 679.16077 Test MSE 711.7574463734376 Test RE 0.4491125388123695 Lambda1 1.2583231e-05\n",
      "56 Train Loss 677.67584 Test MSE 712.4605571641787 Test RE 0.44933431234453836 Lambda1 1.2810557e-05\n",
      "57 Train Loss 675.23706 Test MSE 711.2491816211881 Test RE 0.44895215493409046 Lambda1 1.3950239e-05\n",
      "58 Train Loss 672.9221 Test MSE 709.9352407325733 Test RE 0.44853727270996446 Lambda1 1.0963821e-05\n",
      "59 Train Loss 671.23883 Test MSE 709.6411659304019 Test RE 0.44844436481885014 Lambda1 5.2196683e-06\n",
      "60 Train Loss 670.76355 Test MSE 709.7875717918106 Test RE 0.44849062164604425 Lambda1 3.850064e-06\n",
      "61 Train Loss 670.4613 Test MSE 709.8371575604875 Test RE 0.4485062871526804 Lambda1 2.0227433e-06\n",
      "62 Train Loss 669.9273 Test MSE 709.5417596364177 Test RE 0.4484129547515241 Lambda1 1.033605e-06\n",
      "63 Train Loss 669.3025 Test MSE 709.1484129250994 Test RE 0.4482886447933614 Lambda1 1.1332049e-06\n",
      "64 Train Loss 668.80475 Test MSE 709.3005533772863 Test RE 0.44833673005888924 Lambda1 1.4136232e-06\n",
      "65 Train Loss 668.69116 Test MSE 709.3979033326674 Test RE 0.44836749562293504 Lambda1 1.6063937e-06\n",
      "66 Train Loss 668.6496 Test MSE 709.4035899405856 Test RE 0.44836929269974424 Lambda1 1.5416377e-06\n",
      "67 Train Loss 668.5746 Test MSE 709.3256292338921 Test RE 0.44834465499856335 Lambda1 1.4563553e-06\n",
      "68 Train Loss 668.5746 Test MSE 709.3256292338921 Test RE 0.44834465499856335 Lambda1 1.4563553e-06\n",
      "69 Train Loss 668.5746 Test MSE 709.3256292338921 Test RE 0.44834465499856335 Lambda1 1.4563553e-06\n",
      "70 Train Loss 668.5746 Test MSE 709.3256292338921 Test RE 0.44834465499856335 Lambda1 1.4563553e-06\n",
      "71 Train Loss 668.5746 Test MSE 709.3256292338921 Test RE 0.44834465499856335 Lambda1 1.4563553e-06\n",
      "72 Train Loss 668.5746 Test MSE 709.3256292338921 Test RE 0.44834465499856335 Lambda1 1.4563553e-06\n",
      "73 Train Loss 668.5746 Test MSE 709.3256292338921 Test RE 0.44834465499856335 Lambda1 1.4563553e-06\n",
      "74 Train Loss 668.5746 Test MSE 709.3256292338921 Test RE 0.44834465499856335 Lambda1 1.4563553e-06\n",
      "Training time: 176.72\n",
      "Training time: 176.72\n",
      "inv_HT_atanh_tune21\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 29460.635 Test MSE 3579.537098767096 Test RE 1.0071695903362086 Lambda1 -9.518405e-05\n",
      "1 Train Loss 18674.896 Test MSE 3581.12805741982 Test RE 1.0073933884312298 Lambda1 3.9982435e-05\n",
      "2 Train Loss 12518.465 Test MSE 3560.5276335926565 Test RE 1.004491696341565 Lambda1 5.562654e-05\n",
      "3 Train Loss 8893.143 Test MSE 3554.5091503363165 Test RE 1.0036423737163929 Lambda1 -3.1914584e-05\n",
      "4 Train Loss 7186.406 Test MSE 3556.9107640597203 Test RE 1.0039813733070369 Lambda1 5.135807e-05\n",
      "5 Train Loss 6023.514 Test MSE 3559.406468767996 Test RE 1.0043335330659164 Lambda1 -0.00032143822\n",
      "6 Train Loss 5098.9985 Test MSE 3559.37900779495 Test RE 1.0043296588203763 Lambda1 0.00010083276\n",
      "7 Train Loss 4645.055 Test MSE 3553.811891709123 Test RE 1.003543930781536 Lambda1 -4.4003293e-05\n",
      "8 Train Loss 4235.4556 Test MSE 3561.088014492979 Test RE 1.0045707402182906 Lambda1 0.00033994514\n",
      "9 Train Loss 4013.0132 Test MSE 3561.9012073684894 Test RE 1.0046854331344017 Lambda1 9.178486e-06\n",
      "10 Train Loss 3868.3137 Test MSE 3558.491102535902 Test RE 1.0042043834277108 Lambda1 -7.8243385e-05\n",
      "11 Train Loss 3765.165 Test MSE 3551.532460516132 Test RE 1.0032220403253873 Lambda1 0.00036219248\n",
      "12 Train Loss 3709.356 Test MSE 3547.643564078316 Test RE 1.0026726302813667 Lambda1 1.9359575e-05\n",
      "13 Train Loss 3677.3872 Test MSE 3540.9402039463557 Test RE 1.0017248950282227 Lambda1 -0.000120817705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 Train Loss 3645.5408 Test MSE 3529.3247042806065 Test RE 1.0000805446109342 Lambda1 0.00022686517\n",
      "15 Train Loss 3608.7224 Test MSE 3514.4565064493454 Test RE 0.9979717716864032 Lambda1 0.0001266248\n",
      "16 Train Loss 3582.1711 Test MSE 3501.273327295119 Test RE 0.9960982527363605 Lambda1 0.00035718904\n",
      "17 Train Loss 3553.6035 Test MSE 3476.4984265877656 Test RE 0.992567816272223 Lambda1 0.00027388608\n",
      "18 Train Loss 3510.132 Test MSE 3427.3380627461247 Test RE 0.9855249934084708 Lambda1 -2.0138752e-05\n",
      "19 Train Loss 3409.7678 Test MSE 3320.1975464830557 Test RE 0.9699986565868212 Lambda1 -8.629057e-05\n",
      "20 Train Loss 3163.788 Test MSE 3102.7763099599547 Test RE 0.9377010538159611 Lambda1 0.0002741874\n",
      "21 Train Loss 2812.328 Test MSE 2728.839120115433 Test RE 0.8793831440413967 Lambda1 0.008450017\n",
      "22 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 44.09\n",
      "Training time: 44.09\n",
      "inv_HT_atanh_tune21\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 17664.477 Test MSE 3558.0564350512805 Test RE 1.0041430500828987 Lambda1 -0.0005326762\n",
      "1 Train Loss 7388.0264 Test MSE 3555.9163485212453 Test RE 1.0038410205496096 Lambda1 -0.0010090487\n",
      "2 Train Loss 4435.149 Test MSE 3553.9431339845883 Test RE 1.0035624610494647 Lambda1 0.00070820126\n",
      "3 Train Loss 3827.644 Test MSE 3548.1354543223665 Test RE 1.002742139489528 Lambda1 0.0003989277\n",
      "4 Train Loss 3630.5278 Test MSE 3521.9047467235364 Test RE 0.9990287202371497 Lambda1 -0.0027390504\n",
      "5 Train Loss 3020.4744 Test MSE 2994.3090212285265 Test RE 0.9211651093997896 Lambda1 0.05687917\n",
      "6 Train Loss 838.10126 Test MSE 858.3252075440843 Test RE 0.4931909562354012 Lambda1 1.504458\n",
      "7 Train Loss 838.0624 Test MSE 858.2790911738456 Test RE 0.49317770689701174 Lambda1 1.5128744\n",
      "8 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "9 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "10 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "11 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "12 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "13 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "14 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "15 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "16 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "17 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "18 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "19 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "20 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "21 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "22 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "23 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "24 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "25 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "26 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "27 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "28 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "29 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "30 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "31 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "32 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "33 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "34 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "35 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "36 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "37 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "38 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "39 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "40 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "41 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "42 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "43 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "44 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "45 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "46 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "47 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "48 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "49 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "50 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "51 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "52 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "53 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "54 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "55 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "56 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "57 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "58 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "59 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "60 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "61 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "62 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "63 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "64 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "65 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "66 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "67 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "68 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "70 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "71 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "72 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "73 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "74 Train Loss 838.0615 Test MSE 858.2807815191027 Test RE 0.4931781925433451 Lambda1 1.5132846\n",
      "Training time: 141.18\n",
      "Training time: 141.18\n",
      "inv_HT_atanh_tune21\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 8143.0684 Test MSE 3607.3804349073494 Test RE 1.0110791237320085 Lambda1 8.838063e-05\n",
      "1 Train Loss 4379.341 Test MSE 3607.2311367775224 Test RE 1.0110582008231424 Lambda1 -0.00017932593\n",
      "2 Train Loss 3535.2847 Test MSE 3502.14753749511 Test RE 0.9962225996270588 Lambda1 -0.0061054155\n",
      "3 Train Loss 838.2574 Test MSE 858.261642039434 Test RE 0.49317269362806226 Lambda1 -2.081505\n",
      "4 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "5 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "6 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "7 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "8 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "9 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "10 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "11 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "12 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "13 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "14 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "15 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "16 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "17 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "18 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "19 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "20 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "21 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "22 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "23 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "24 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "25 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "26 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "27 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "28 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "29 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "30 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "31 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "32 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "33 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "34 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "35 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "36 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "37 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "38 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "39 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "40 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "41 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "42 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "43 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "44 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "45 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "46 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "47 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "48 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "49 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "50 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "51 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "52 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "53 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "54 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "55 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "56 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "57 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "58 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "59 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "60 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "61 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "62 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "63 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "64 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "65 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "66 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "67 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "68 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "69 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "71 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "72 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "73 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "74 Train Loss 838.06335 Test MSE 858.2805474926947 Test RE 0.49317812530618255 Lambda1 -2.0872598\n",
      "Training time: 114.67\n",
      "Training time: 114.67\n",
      "inv_HT_atanh_tune21\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 6650.4736 Test MSE 3392.3854557571954 Test RE 0.9804868353460374 Lambda1 -0.00078142027\n",
      "1 Train Loss 3821.9165 Test MSE 3386.742012032862 Test RE 0.9796709455360053 Lambda1 0.0015182387\n",
      "2 Train Loss 3425.73 Test MSE 3364.7422111369306 Test RE 0.9764838588871679 Lambda1 0.00025636467\n",
      "3 Train Loss 908.69714 Test MSE 937.016065336838 Test RE 0.5153030217029162 Lambda1 0.12372773\n",
      "4 Train Loss 838.0616 Test MSE 858.2766519484661 Test RE 0.4931770060920835 Lambda1 0.11003703\n",
      "5 Train Loss 837.8979 Test MSE 857.9575242698304 Test RE 0.49308531009553086 Lambda1 0.10762899\n",
      "6 Train Loss 837.7847 Test MSE 857.6864210450932 Test RE 0.49300739972376617 Lambda1 0.10489313\n",
      "7 Train Loss 837.7359 Test MSE 857.6022747993555 Test RE 0.49298321504604514 Lambda1 0.10466329\n",
      "8 Train Loss 837.4992 Test MSE 856.9072703723696 Test RE 0.49278341674129333 Lambda1 0.094105326\n",
      "9 Train Loss 836.95703 Test MSE 855.9173276908299 Test RE 0.49249869030354937 Lambda1 0.081138834\n",
      "10 Train Loss 833.9062 Test MSE 851.4544733908773 Test RE 0.4912130389834993 Lambda1 0.043009114\n",
      "11 Train Loss 820.7017 Test MSE 833.0753478636002 Test RE 0.4858825613245208 Lambda1 -0.013508793\n",
      "12 Train Loss 807.42096 Test MSE 818.9389544029643 Test RE 0.48174247047817437 Lambda1 0.00033825188\n",
      "13 Train Loss 797.9181 Test MSE 811.8372542336976 Test RE 0.47964912768556645 Lambda1 -0.0013051502\n",
      "14 Train Loss 768.1856 Test MSE 771.831590838567 Test RE 0.4676817743567512 Lambda1 0.0007610659\n",
      "15 Train Loss 746.7743 Test MSE 758.8233776304791 Test RE 0.4637239448000553 Lambda1 0.0007927232\n",
      "16 Train Loss 728.2241 Test MSE 726.8355462463123 Test RE 0.45384468096257874 Lambda1 0.00056741294\n",
      "17 Train Loss 694.98987 Test MSE 697.7074852777599 Test RE 0.444657745576293 Lambda1 0.00015754077\n",
      "18 Train Loss 660.436 Test MSE 664.1793011142054 Test RE 0.43384224528682747 Lambda1 -0.00034988395\n",
      "19 Train Loss 637.04675 Test MSE 650.0891617024737 Test RE 0.42921573364340954 Lambda1 -1.3566565e-05\n",
      "20 Train Loss 609.4886 Test MSE 627.4593145630193 Test RE 0.42167898321183267 Lambda1 -0.00012915618\n",
      "21 Train Loss 586.1852 Test MSE 610.2251520122312 Test RE 0.41584762239936485 Lambda1 -4.025193e-05\n",
      "22 Train Loss 572.4825 Test MSE 603.1296296320295 Test RE 0.41342287500940933 Lambda1 -8.812674e-06\n",
      "23 Train Loss 564.63794 Test MSE 598.6786528772835 Test RE 0.41189456081305353 Lambda1 -5.0831704e-06\n",
      "24 Train Loss 560.8355 Test MSE 595.2108183712031 Test RE 0.4106998826283904 Lambda1 -3.028734e-06\n",
      "25 Train Loss 559.0332 Test MSE 592.7309737227393 Test RE 0.409843434028391 Lambda1 -6.3181897e-06\n",
      "26 Train Loss 557.40784 Test MSE 590.114121713989 Test RE 0.4089377229824294 Lambda1 -2.5922975e-05\n",
      "27 Train Loss 555.4449 Test MSE 590.2444722391635 Test RE 0.4089828856905092 Lambda1 -2.7699537e-06\n",
      "28 Train Loss 552.5984 Test MSE 585.4224321394814 Test RE 0.4073088536836451 Lambda1 2.319401e-06\n",
      "29 Train Loss 548.84314 Test MSE 581.7051747512567 Test RE 0.406013649657156 Lambda1 -7.456902e-07\n",
      "30 Train Loss 542.37573 Test MSE 573.5788772439658 Test RE 0.4031677132747217 Lambda1 3.790246e-07\n",
      "31 Train Loss 539.76056 Test MSE 570.5670412080572 Test RE 0.40210781260550177 Lambda1 -9.842481e-06\n",
      "32 Train Loss 531.0899 Test MSE 560.5083145268618 Test RE 0.39854760188298993 Lambda1 -1.9144627e-05\n",
      "33 Train Loss 520.55945 Test MSE 552.5260519798228 Test RE 0.3956995448221818 Lambda1 -1.4968213e-05\n",
      "34 Train Loss 515.65924 Test MSE 549.7420357700462 Test RE 0.3947013790773018 Lambda1 -1.3758618e-05\n",
      "35 Train Loss 513.7238 Test MSE 546.9902792777731 Test RE 0.39371229275934455 Lambda1 -1.1470101e-05\n",
      "36 Train Loss 511.02872 Test MSE 545.967167392594 Test RE 0.39334391303088156 Lambda1 -2.1890615e-05\n",
      "37 Train Loss 508.722 Test MSE 542.3722399346973 Test RE 0.39204678537508325 Lambda1 -1.5658548e-05\n",
      "38 Train Loss 505.52615 Test MSE 538.3920754968868 Test RE 0.3906056312614854 Lambda1 -1.7025499e-05\n",
      "39 Train Loss 503.73956 Test MSE 537.5528581540584 Test RE 0.3903010847468901 Lambda1 -2.5876411e-06\n",
      "40 Train Loss 501.41925 Test MSE 534.830448643685 Test RE 0.3893115000892627 Lambda1 -4.6915065e-06\n",
      "41 Train Loss 496.63953 Test MSE 528.3625363928852 Test RE 0.3869502920677079 Lambda1 -1.9787462e-06\n",
      "42 Train Loss 492.53397 Test MSE 521.8369456626615 Test RE 0.3845533352984022 Lambda1 -4.0300316e-07\n",
      "43 Train Loss 487.03653 Test MSE 518.9297094765863 Test RE 0.38348063543023514 Lambda1 -4.5866295e-06\n",
      "44 Train Loss 483.43588 Test MSE 516.1159832465484 Test RE 0.3824395732925462 Lambda1 -1.9937147e-06\n",
      "45 Train Loss 480.58057 Test MSE 512.9109635244772 Test RE 0.3812502716138081 Lambda1 -4.609534e-06\n",
      "46 Train Loss 479.46213 Test MSE 511.43726648689733 Test RE 0.3807021730236169 Lambda1 -1.1290684e-05\n",
      "47 Train Loss 477.69208 Test MSE 509.41171018399484 Test RE 0.3799475362631203 Lambda1 -1.3691897e-05\n",
      "48 Train Loss 475.8673 Test MSE 508.56526197478695 Test RE 0.3796317409908798 Lambda1 -8.548132e-06\n",
      "49 Train Loss 474.07388 Test MSE 507.76387506307066 Test RE 0.3793325150346344 Lambda1 -3.6458148e-06\n",
      "50 Train Loss 469.39255 Test MSE 506.14350283833016 Test RE 0.37872676986449055 Lambda1 -8.6451666e-07\n",
      "51 Train Loss 467.06754 Test MSE 506.7671827151915 Test RE 0.3789600352771569 Lambda1 -1.12902676e-07\n",
      "52 Train Loss 465.79233 Test MSE 507.2766496322226 Test RE 0.3791504768722071 Lambda1 -1.09820895e-07\n",
      "53 Train Loss 465.1101 Test MSE 507.1969236703836 Test RE 0.37912068117368225 Lambda1 -9.185099e-08\n",
      "54 Train Loss 464.5563 Test MSE 506.830533491827 Test RE 0.3789837213633172 Lambda1 -7.4207264e-08\n",
      "55 Train Loss 463.46466 Test MSE 506.31380075622627 Test RE 0.37879047803793275 Lambda1 -1.2215169e-07\n",
      "56 Train Loss 462.9643 Test MSE 505.9604362991159 Test RE 0.37865827301627897 Lambda1 -1.3222292e-07\n",
      "57 Train Loss 462.5024 Test MSE 505.7747671476539 Test RE 0.37858878970667004 Lambda1 -1.05472566e-07\n",
      "58 Train Loss 461.9344 Test MSE 505.73194373301044 Test RE 0.3785727620111802 Lambda1 -1.6188733e-07\n",
      "59 Train Loss 461.70816 Test MSE 505.64674128362185 Test RE 0.37854087092178834 Lambda1 -1.8975564e-07\n",
      "60 Train Loss 461.62018 Test MSE 505.7167010791261 Test RE 0.3785670569166796 Lambda1 -2.0648439e-07\n",
      "61 Train Loss 461.31677 Test MSE 505.6584366841852 Test RE 0.37854524864357925 Lambda1 -2.3827374e-07\n",
      "62 Train Loss 461.21216 Test MSE 505.72796350837405 Test RE 0.3785712722816729 Lambda1 -2.2962605e-07\n",
      "63 Train Loss 460.66 Test MSE 505.6295134979439 Test RE 0.3785344222729486 Lambda1 -9.969885e-08\n",
      "64 Train Loss 460.24597 Test MSE 505.0878333675812 Test RE 0.3783316062632525 Lambda1 -4.0052953e-08\n",
      "65 Train Loss 459.5147 Test MSE 504.6214679624645 Test RE 0.37815690247068895 Lambda1 -1.4917182e-08\n",
      "66 Train Loss 458.30307 Test MSE 503.4827628080515 Test RE 0.37772999592886636 Lambda1 -1.907909e-08\n",
      "67 Train Loss 457.8576 Test MSE 503.121076650666 Test RE 0.37759429688860957 Lambda1 -4.7883102e-08\n",
      "68 Train Loss 457.56253 Test MSE 503.01897162030116 Test RE 0.37755597983609485 Lambda1 -5.614471e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 Train Loss 457.32867 Test MSE 502.7954655705994 Test RE 0.377472090931067 Lambda1 -6.179654e-08\n",
      "70 Train Loss 457.24557 Test MSE 502.9190911477968 Test RE 0.37751849383264036 Lambda1 -8.1112965e-08\n",
      "71 Train Loss 457.1813 Test MSE 503.0284739119089 Test RE 0.3775595459342722 Lambda1 -1.2793095e-07\n",
      "72 Train Loss 457.0611 Test MSE 503.01695235544724 Test RE 0.377555222025427 Lambda1 -9.569786e-08\n",
      "73 Train Loss 456.98114 Test MSE 502.84710358769547 Test RE 0.3774914739716684 Lambda1 -9.191785e-08\n",
      "74 Train Loss 456.6121 Test MSE 502.4172629102112 Test RE 0.3773300970040313 Lambda1 -4.4478448e-08\n",
      "Training time: 161.46\n",
      "Training time: 161.46\n",
      "inv_HT_atanh_tune21\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 8871.102 Test MSE 3640.625849887792 Test RE 1.0157274627249409 Lambda1 -0.00069307914\n",
      "1 Train Loss 4603.0317 Test MSE 3588.5087681292325 Test RE 1.0084309737903179 Lambda1 -0.0008507127\n",
      "2 Train Loss 3798.3027 Test MSE 3568.2508433259604 Test RE 1.0055805374414917 Lambda1 -0.0010488024\n",
      "3 Train Loss 3576.486 Test MSE 3482.9105621072868 Test RE 0.9934827523047007 Lambda1 0.0020704286\n",
      "4 Train Loss 3314.8723 Test MSE 3278.845524721735 Test RE 0.9639392154529649 Lambda1 -0.0029982538\n",
      "5 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 10.02\n",
      "Training time: 10.02\n",
      "inv_HT_atanh_tune22\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1585633.4 Test MSE 3553.580370733407 Test RE 1.0035112411990075 Lambda1 -7.320092e-05\n",
      "1 Train Loss 1469341.0 Test MSE 3553.793579970308 Test RE 1.0035413452960658 Lambda1 -2.7532715e-06\n",
      "2 Train Loss 1395263.9 Test MSE 3554.5287004068814 Test RE 1.0036451337672774 Lambda1 2.6275477e-05\n",
      "3 Train Loss 1315671.1 Test MSE 3554.887781898842 Test RE 1.0036958270266558 Lambda1 -1.3104497e-05\n",
      "4 Train Loss 1246176.9 Test MSE 3554.817797478217 Test RE 1.0036859471906432 Lambda1 -2.0786423e-05\n",
      "5 Train Loss 1192644.8 Test MSE 3555.33250792014 Test RE 1.0037586075867602 Lambda1 -3.6393474e-05\n",
      "6 Train Loss 1140088.2 Test MSE 3555.9925435309606 Test RE 1.0038517754801373 Lambda1 -4.8164704e-05\n",
      "7 Train Loss 1088677.9 Test MSE 3556.5698678644485 Test RE 1.0039332611032428 Lambda1 -9.467702e-06\n",
      "8 Train Loss 1056646.0 Test MSE 3556.6372926517615 Test RE 1.0039427772477743 Lambda1 -3.0307667e-05\n",
      "9 Train Loss 1009640.0 Test MSE 3557.231356505069 Test RE 1.0040266178482322 Lambda1 5.955537e-06\n",
      "10 Train Loss 976716.1 Test MSE 3557.32184086904 Test RE 1.0040393873483164 Lambda1 -1.5540612e-05\n",
      "11 Train Loss 949446.9 Test MSE 3557.552876785359 Test RE 1.0040719912852067 Lambda1 -1.185544e-05\n",
      "12 Train Loss 923918.25 Test MSE 3557.9136540785967 Test RE 1.0041229022912046 Lambda1 6.881197e-05\n",
      "13 Train Loss 903515.25 Test MSE 3558.0353476892055 Test RE 1.004140074475312 Lambda1 -3.2596225e-05\n",
      "14 Train Loss 888359.06 Test MSE 3558.29169307014 Test RE 1.0041762464084971 Lambda1 -2.0032963e-05\n",
      "15 Train Loss 868615.75 Test MSE 3558.505445170738 Test RE 1.0042064071678078 Lambda1 -1.7917006e-05\n",
      "16 Train Loss 844135.1 Test MSE 3558.3634601934696 Test RE 1.0041863729640694 Lambda1 -1.8331511e-05\n",
      "17 Train Loss 822833.56 Test MSE 3558.3818857276983 Test RE 1.0041889728455378 Lambda1 7.48199e-06\n",
      "18 Train Loss 802393.56 Test MSE 3558.1249684135855 Test RE 1.0041527206670136 Lambda1 3.4076864e-05\n",
      "19 Train Loss 784456.1 Test MSE 3558.3566241639755 Test RE 1.0041854083842792 Lambda1 -4.3054504e-05\n",
      "20 Train Loss 765985.44 Test MSE 3558.7260055995444 Test RE 1.0042375276434217 Lambda1 1.0925094e-05\n",
      "21 Train Loss 745306.6 Test MSE 3559.336314138557 Test RE 1.004323635489643 Lambda1 -2.0356874e-05\n",
      "22 Train Loss 730558.9 Test MSE 3559.734686728619 Test RE 1.0043798375141584 Lambda1 -6.877328e-06\n",
      "23 Train Loss 718460.0 Test MSE 3559.8152098625437 Test RE 1.0043911972589041 Lambda1 -9.911615e-07\n",
      "24 Train Loss 705368.0 Test MSE 3560.2330755868225 Test RE 1.0044501453102126 Lambda1 -1.4967733e-05\n",
      "25 Train Loss 692239.44 Test MSE 3560.4390595828527 Test RE 1.0044792020682647 Lambda1 1.3496658e-05\n",
      "26 Train Loss 678989.94 Test MSE 3560.4987001108475 Test RE 1.0044876149940436 Lambda1 -2.7074857e-05\n",
      "27 Train Loss 666247.7 Test MSE 3560.369324292244 Test RE 1.0044693650803416 Lambda1 -1.7191225e-05\n",
      "28 Train Loss 655749.8 Test MSE 3560.4974102618194 Test RE 1.0044874330479736 Lambda1 -3.3876255e-05\n",
      "29 Train Loss 645794.06 Test MSE 3560.6375364712967 Test RE 1.0045071990547119 Lambda1 4.143164e-05\n",
      "30 Train Loss 634547.94 Test MSE 3560.7339685254915 Test RE 1.0045208013992104 Lambda1 -2.4361365e-05\n",
      "31 Train Loss 622887.94 Test MSE 3560.9241086743036 Test RE 1.004547621317044 Lambda1 2.7426424e-05\n",
      "32 Train Loss 613541.3 Test MSE 3561.331981763184 Test RE 1.004605150794885 Lambda1 -1.3427511e-05\n",
      "33 Train Loss 603930.1 Test MSE 3561.6639074014524 Test RE 1.00465196564369 Lambda1 2.4096247e-05\n",
      "34 Train Loss 597129.75 Test MSE 3561.8367680127553 Test RE 1.0046763450700409 Lambda1 2.4091929e-05\n",
      "35 Train Loss 588972.3 Test MSE 3562.1146550023286 Test RE 1.004715535670009 Lambda1 -2.8039616e-05\n",
      "36 Train Loss 581668.4 Test MSE 3562.18000061854 Test RE 1.0047247511867816 Lambda1 1.1355523e-06\n",
      "37 Train Loss 573112.7 Test MSE 3562.2352217889265 Test RE 1.0047325388142523 Lambda1 3.0152478e-08\n",
      "38 Train Loss 563889.75 Test MSE 3562.5169291142724 Test RE 1.0047722659663032 Lambda1 -2.2444907e-05\n",
      "39 Train Loss 551917.3 Test MSE 3562.977609866065 Test RE 1.0048372290652772 Lambda1 -1.23798245e-05\n",
      "40 Train Loss 544378.3 Test MSE 3563.1095403832624 Test RE 1.0048558325316292 Lambda1 -6.6506295e-06\n",
      "41 Train Loss 538219.4 Test MSE 3563.2670614879644 Test RE 1.0048780440647915 Lambda1 -4.4429613e-05\n",
      "42 Train Loss 531684.0 Test MSE 3563.5390889900027 Test RE 1.0049164006126654 Lambda1 -4.5300956e-05\n",
      "43 Train Loss 524962.94 Test MSE 3563.622275286717 Test RE 1.0049281297931143 Lambda1 -4.52525e-05\n",
      "44 Train Loss 518261.0 Test MSE 3563.7534802925275 Test RE 1.004946629282864 Lambda1 -2.4505576e-05\n",
      "45 Train Loss 513152.34 Test MSE 3563.871242430284 Test RE 1.0049632330808551 Lambda1 -3.4890476e-05\n",
      "46 Train Loss 507450.22 Test MSE 3563.8417101993264 Test RE 1.0049590692284363 Lambda1 -8.7869585e-06\n",
      "47 Train Loss 499573.7 Test MSE 3563.892594508729 Test RE 1.0049662435742466 Lambda1 -2.6678706e-05\n",
      "48 Train Loss 491396.1 Test MSE 3564.0057087580635 Test RE 1.0049821917400878 Lambda1 4.0781642e-06\n",
      "49 Train Loss 484009.53 Test MSE 3564.0667957036144 Test RE 1.0049908043847329 Lambda1 -1.5578627e-05\n",
      "50 Train Loss 476933.1 Test MSE 3564.1557731746875 Test RE 1.005003349180784 Lambda1 -2.0684029e-05\n",
      "51 Train Loss 471084.5 Test MSE 3564.0787426234106 Test RE 1.0049924887715813 Lambda1 -2.8766539e-05\n",
      "52 Train Loss 465365.22 Test MSE 3563.807448366525 Test RE 1.0049542385113117 Lambda1 -3.454244e-05\n",
      "53 Train Loss 459290.06 Test MSE 3563.791358247549 Test RE 1.004951969891058 Lambda1 7.4790405e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 Train Loss 453109.75 Test MSE 3563.868330955895 Test RE 1.0049628225826759 Lambda1 -3.0802774e-05\n",
      "55 Train Loss 448797.56 Test MSE 3563.8976496261807 Test RE 1.0049669563090529 Lambda1 -6.3123814e-05\n",
      "56 Train Loss 444805.84 Test MSE 3563.9736443284364 Test RE 1.0049776709486136 Lambda1 -7.302364e-05\n",
      "57 Train Loss 440115.97 Test MSE 3563.971536764992 Test RE 1.0049773738007213 Lambda1 -6.487379e-05\n",
      "58 Train Loss 435237.25 Test MSE 3563.9785166583106 Test RE 1.0049783579038982 Lambda1 -5.9741084e-05\n",
      "59 Train Loss 428995.34 Test MSE 3564.0550142083584 Test RE 1.0049891433181966 Lambda1 -6.589682e-05\n",
      "60 Train Loss 423057.66 Test MSE 3564.140165121388 Test RE 1.0050011486368104 Lambda1 -7.586567e-05\n",
      "61 Train Loss 418837.56 Test MSE 3564.1817517825452 Test RE 1.0050070118349987 Lambda1 -7.236097e-05\n",
      "62 Train Loss 413444.03 Test MSE 3564.517705361236 Test RE 1.005054375826215 Lambda1 -7.661463e-05\n",
      "63 Train Loss 408965.66 Test MSE 3564.736988291028 Test RE 1.0050852899493772 Lambda1 -8.003971e-05\n",
      "64 Train Loss 404882.7 Test MSE 3565.0471193839135 Test RE 1.0051290100640873 Lambda1 -7.910167e-05\n",
      "65 Train Loss 400403.9 Test MSE 3565.3115128242143 Test RE 1.005166280902456 Lambda1 -8.031823e-05\n",
      "66 Train Loss 397603.12 Test MSE 3565.450036019484 Test RE 1.0051858075962765 Lambda1 -8.685898e-05\n",
      "67 Train Loss 393915.3 Test MSE 3565.4290330536523 Test RE 1.005182846972225 Lambda1 -8.1890845e-05\n",
      "68 Train Loss 390044.5 Test MSE 3565.4571017890034 Test RE 1.005186803600659 Lambda1 -7.327798e-05\n",
      "69 Train Loss 385055.1 Test MSE 3565.537206536144 Test RE 1.005198095249973 Lambda1 -8.087683e-05\n",
      "70 Train Loss 381607.4 Test MSE 3565.5005063402305 Test RE 1.0051929219675442 Lambda1 -7.962677e-05\n",
      "71 Train Loss 377529.8 Test MSE 3565.434214609899 Test RE 1.0051835773765492 Lambda1 -8.0312624e-05\n",
      "72 Train Loss 373115.34 Test MSE 3565.459056310314 Test RE 1.0051870791135458 Lambda1 -7.653796e-05\n",
      "73 Train Loss 369832.3 Test MSE 3565.5286486106797 Test RE 1.0051968889218605 Lambda1 -7.250013e-05\n",
      "74 Train Loss 365809.5 Test MSE 3565.72523172066 Test RE 1.0052245989792479 Lambda1 -7.412415e-05\n",
      "Training time: 145.29\n",
      "Training time: 145.29\n",
      "inv_HT_atanh_tune22\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.73\n",
      "Training time: 2.73\n",
      "inv_HT_atanh_tune22\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.83\n",
      "Training time: 2.83\n",
      "inv_HT_atanh_tune22\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.77\n",
      "Training time: 2.77\n",
      "inv_HT_atanh_tune22\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 418645.1 Test MSE 3517.4878357558146 Test RE 0.9984020699452811 Lambda1 -2.4081364e-05\n",
      "1 Train Loss 394111.5 Test MSE 3517.378482499663 Test RE 0.9983865504366213 Lambda1 -1.7313818e-05\n",
      "2 Train Loss 379221.84 Test MSE 3517.04959564825 Test RE 0.9983398730763846 Lambda1 8.457744e-06\n",
      "3 Train Loss 369921.8 Test MSE 3516.7801706052906 Test RE 0.9983016332240082 Lambda1 1.2742522e-05\n",
      "4 Train Loss 363612.28 Test MSE 3516.8017862776146 Test RE 0.9983047012189894 Lambda1 -4.9586306e-06\n",
      "5 Train Loss 354246.2 Test MSE 3516.5432813858233 Test RE 0.9982680100138883 Lambda1 3.479283e-05\n",
      "6 Train Loss 340834.6 Test MSE 3516.1724130254424 Test RE 0.9982153680083231 Lambda1 -4.933304e-06\n",
      "7 Train Loss 330887.44 Test MSE 3515.9074538397126 Test RE 0.9981777573222181 Lambda1 7.1793334e-06\n",
      "8 Train Loss 318260.03 Test MSE 3515.5556769489826 Test RE 0.9981278207616062 Lambda1 5.60815e-06\n",
      "9 Train Loss 309443.12 Test MSE 3515.5347429536364 Test RE 0.9981248489932459 Lambda1 1.4776785e-05\n",
      "10 Train Loss 299733.56 Test MSE 3515.2936157880276 Test RE 0.9980906181919 Lambda1 4.0307095e-06\n",
      "11 Train Loss 295143.78 Test MSE 3515.4067347441414 Test RE 0.9981066768875473 Lambda1 -1.2586276e-05\n",
      "12 Train Loss 292413.44 Test MSE 3515.6299243451067 Test RE 0.9981383607739424 Lambda1 1.9280393e-05\n",
      "13 Train Loss 288359.0 Test MSE 3515.3768306476077 Test RE 0.998102431640248 Lambda1 -2.2458204e-05\n",
      "14 Train Loss 283046.94 Test MSE 3514.830598982218 Test RE 0.9980248842875797 Lambda1 -8.353441e-06\n",
      "15 Train Loss 277942.34 Test MSE 3514.4197236737177 Test RE 0.9979665492191825 Lambda1 8.5179445e-06\n",
      "16 Train Loss 273696.06 Test MSE 3514.3589780613192 Test RE 0.9979579244168093 Lambda1 -3.4818368e-07\n",
      "17 Train Loss 266364.84 Test MSE 3514.601150789184 Test RE 0.9979923082163898 Lambda1 -5.5251903e-06\n",
      "18 Train Loss 262192.12 Test MSE 3515.1144265627563 Test RE 0.9980651794400665 Lambda1 5.973953e-06\n",
      "19 Train Loss 258533.25 Test MSE 3515.3164279330886 Test RE 0.9980938566910861 Lambda1 3.1243988e-05\n",
      "20 Train Loss 253422.05 Test MSE 3515.24958199417 Test RE 0.9980843669567304 Lambda1 9.223445e-07\n",
      "21 Train Loss 248040.7 Test MSE 3514.5766801591867 Test RE 0.9979888339184577 Lambda1 3.0261892e-06\n",
      "22 Train Loss 245097.02 Test MSE 3514.431091545215 Test RE 0.997968163247558 Lambda1 2.9841842e-06\n",
      "23 Train Loss 241354.48 Test MSE 3514.3447326029523 Test RE 0.9979559018029329 Lambda1 3.2554087e-06\n",
      "24 Train Loss 237499.8 Test MSE 3514.357323461394 Test RE 0.9979576894918469 Lambda1 -1.6667385e-05\n",
      "25 Train Loss 234208.95 Test MSE 3514.890013125034 Test RE 0.9980333194796542 Lambda1 -2.0902864e-05\n",
      "26 Train Loss 230658.17 Test MSE 3514.5690928125473 Test RE 0.997987756677583 Lambda1 -1.8222472e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 Train Loss 225540.88 Test MSE 3514.7237823253768 Test RE 0.9980097190486673 Lambda1 -1.5924295e-05\n",
      "28 Train Loss 222641.44 Test MSE 3514.6977501750266 Test RE 0.9980060231128309 Lambda1 2.5406132e-05\n",
      "29 Train Loss 220176.5 Test MSE 3514.6157267943727 Test RE 0.9979943776867733 Lambda1 1.733155e-05\n",
      "30 Train Loss 218033.83 Test MSE 3514.5861547179693 Test RE 0.9979901791014423 Lambda1 2.1622329e-05\n",
      "31 Train Loss 215322.6 Test MSE 3514.649204800154 Test RE 0.9979991308069691 Lambda1 2.8073087e-05\n",
      "32 Train Loss 213268.69 Test MSE 3514.7199965174223 Test RE 0.9980091815563372 Lambda1 5.1887255e-06\n",
      "33 Train Loss 210858.9 Test MSE 3514.7583059367867 Test RE 0.9980146205455204 Lambda1 1.6209353e-05\n",
      "34 Train Loss 208002.52 Test MSE 3514.759696328946 Test RE 0.9980148179462296 Lambda1 2.2287684e-05\n",
      "35 Train Loss 206109.58 Test MSE 3514.8395712875167 Test RE 0.9980261581154344 Lambda1 1.4959856e-05\n",
      "36 Train Loss 203240.06 Test MSE 3514.724382901694 Test RE 0.9980098043158195 Lambda1 -1.7414388e-05\n",
      "37 Train Loss 199943.94 Test MSE 3514.654151364684 Test RE 0.9979998331054173 Lambda1 -5.8296087e-07\n",
      "38 Train Loss 197536.56 Test MSE 3514.68568715531 Test RE 0.9980043104511004 Lambda1 -2.085022e-06\n",
      "39 Train Loss 196284.6 Test MSE 3514.798265323423 Test RE 0.9980202937576261 Lambda1 4.6159266e-06\n",
      "40 Train Loss 194687.2 Test MSE 3514.9667023184315 Test RE 0.9980442071535914 Lambda1 5.4980997e-06\n",
      "41 Train Loss 192974.81 Test MSE 3514.954130655975 Test RE 0.9980424223449187 Lambda1 5.700753e-06\n",
      "42 Train Loss 191047.66 Test MSE 3514.980024187557 Test RE 0.9980460984662036 Lambda1 2.2166573e-06\n",
      "43 Train Loss 189187.92 Test MSE 3514.8150981391236 Test RE 0.9980226835777934 Lambda1 5.5095807e-06\n",
      "44 Train Loss 187090.31 Test MSE 3514.8292421834944 Test RE 0.9980246916582365 Lambda1 1.3248257e-05\n",
      "45 Train Loss 185108.19 Test MSE 3515.141400933619 Test RE 0.9980690089211841 Lambda1 1.4951001e-05\n",
      "46 Train Loss 183168.88 Test MSE 3515.1438179188663 Test RE 0.9980693520535622 Lambda1 6.648269e-06\n",
      "47 Train Loss 180971.94 Test MSE 3515.175091152426 Test RE 0.9980737918130926 Lambda1 -3.5813796e-06\n",
      "48 Train Loss 179032.8 Test MSE 3515.5501196080245 Test RE 0.9981270318480858 Lambda1 1.6359409e-05\n",
      "49 Train Loss 177630.02 Test MSE 3515.830482075934 Test RE 0.998166830993231 Lambda1 2.1256097e-05\n",
      "50 Train Loss 176015.8 Test MSE 3515.968114761967 Test RE 0.9981863682033288 Lambda1 2.2531402e-05\n",
      "51 Train Loss 175279.9 Test MSE 3516.219635943016 Test RE 0.998222071104508 Lambda1 1.7382426e-05\n",
      "52 Train Loss 174094.39 Test MSE 3516.364986807276 Test RE 0.998242702770954 Lambda1 7.68886e-06\n",
      "53 Train Loss 172428.98 Test MSE 3516.32514133555 Test RE 0.9982370469922993 Lambda1 4.5352035e-06\n",
      "54 Train Loss 170954.8 Test MSE 3516.3945104363843 Test RE 0.9982468934174832 Lambda1 -1.2933418e-05\n",
      "55 Train Loss 169641.78 Test MSE 3516.5566070786463 Test RE 0.9982699014452158 Lambda1 -2.2692058e-05\n",
      "56 Train Loss 168366.42 Test MSE 3516.7733191073744 Test RE 0.9983006607627567 Lambda1 -1.4016201e-06\n",
      "57 Train Loss 166638.8 Test MSE 3517.0716628123632 Test RE 0.9983430050332516 Lambda1 -2.2790471e-05\n",
      "58 Train Loss 165136.94 Test MSE 3517.2019545931666 Test RE 0.9983614969344392 Lambda1 -1.8558958e-05\n",
      "59 Train Loss 164333.67 Test MSE 3517.2645124248534 Test RE 0.9983703754483239 Lambda1 -1.8539246e-05\n",
      "60 Train Loss 163709.3 Test MSE 3517.2456054924546 Test RE 0.9983676920921963 Lambda1 -2.2362807e-05\n",
      "61 Train Loss 162919.31 Test MSE 3517.2581180379316 Test RE 0.998369467929235 Lambda1 -3.1576943e-05\n",
      "62 Train Loss 161578.4 Test MSE 3517.356801385654 Test RE 0.9983834734055183 Lambda1 -3.6721307e-05\n",
      "63 Train Loss 161055.11 Test MSE 3517.26424595126 Test RE 0.9983703376292538 Lambda1 -3.6015157e-05\n",
      "64 Train Loss 160454.25 Test MSE 3517.2093282576175 Test RE 0.9983625434451194 Lambda1 -4.4400458e-05\n",
      "65 Train Loss 159679.55 Test MSE 3517.1834003875747 Test RE 0.9983588636155202 Lambda1 -3.055509e-05\n",
      "66 Train Loss 158469.31 Test MSE 3517.1835872752636 Test RE 0.9983588901397239 Lambda1 -1.9322144e-05\n",
      "67 Train Loss 156756.06 Test MSE 3517.2300334180736 Test RE 0.9983654820285679 Lambda1 -1.8840166e-05\n",
      "68 Train Loss 155302.9 Test MSE 3517.3292202395096 Test RE 0.9983795590152494 Lambda1 -1.6284435e-05\n",
      "69 Train Loss 153519.75 Test MSE 3517.5965495229616 Test RE 0.9984174984579839 Lambda1 -1.0300895e-05\n",
      "70 Train Loss 152753.97 Test MSE 3517.6142341973514 Test RE 0.9984200082208184 Lambda1 -3.4108012e-05\n",
      "71 Train Loss 151864.88 Test MSE 3517.6602026919395 Test RE 0.9984265319200855 Lambda1 -2.1057609e-05\n",
      "72 Train Loss 150601.22 Test MSE 3517.616464161501 Test RE 0.9984203246910628 Lambda1 -1.1242605e-05\n",
      "73 Train Loss 149708.98 Test MSE 3517.669558699217 Test RE 0.9984278596889928 Lambda1 -1.8421944e-05\n",
      "74 Train Loss 148699.75 Test MSE 3517.9765848137467 Test RE 0.9984714306857442 Lambda1 -1.7973989e-05\n",
      "Training time: 151.73\n",
      "Training time: 151.73\n",
      "inv_HT_atanh_tune22\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.80\n",
      "Training time: 2.80\n",
      "inv_HT_atanh_tune22\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.94\n",
      "Training time: 2.94\n",
      "inv_HT_atanh_tune22\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.80\n",
      "Training time: 2.80\n",
      "inv_HT_atanh_tune22\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.99\n",
      "Training time: 2.99\n",
      "inv_HT_atanh_tune22\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.88\n",
      "Training time: 2.88\n",
      "inv_HT_atanh_tune23\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.84\n",
      "Training time: 2.84\n",
      "inv_HT_atanh_tune23\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.88\n",
      "Training time: 2.88\n",
      "inv_HT_atanh_tune23\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.68\n",
      "Training time: 2.68\n",
      "inv_HT_atanh_tune23\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.71\n",
      "Training time: 2.71\n",
      "inv_HT_atanh_tune23\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.74\n",
      "Training time: 2.74\n",
      "inv_HT_atanh_tune23\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.83\n",
      "Training time: 2.83\n",
      "inv_HT_atanh_tune23\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.62\n",
      "Training time: 2.62\n",
      "inv_HT_atanh_tune23\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.65\n",
      "Training time: 2.65\n",
      "inv_HT_atanh_tune23\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.89\n",
      "Training time: 2.89\n",
      "inv_HT_atanh_tune23\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.89\n",
      "Training time: 2.89\n",
      "inv_HT_atanh_tune24\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.64\n",
      "Training time: 2.64\n",
      "inv_HT_atanh_tune24\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.83\n",
      "Training time: 2.83\n",
      "inv_HT_atanh_tune24\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.70\n",
      "Training time: 2.70\n",
      "inv_HT_atanh_tune24\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.78\n",
      "Training time: 2.78\n",
      "inv_HT_atanh_tune24\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.91\n",
      "Training time: 2.91\n",
      "inv_HT_atanh_tune24\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.74\n",
      "Training time: 2.74\n",
      "inv_HT_atanh_tune24\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.71\n",
      "Training time: 2.71\n",
      "inv_HT_atanh_tune24\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.73\n",
      "Training time: 2.73\n",
      "inv_HT_atanh_tune24\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.79\n",
      "Training time: 2.79\n",
      "inv_HT_atanh_tune24\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 2.86\n",
      "Training time: 2.86\n"
     ]
    }
   ],
   "source": [
    "nan_tune = []\n",
    "for tune_reps in range(25):\n",
    "    label = \"inv_HT_atanh_tune\" + str(tune_reps)\n",
    "    max_reps = 10 #10\n",
    "    max_iter = 75#75\n",
    "\n",
    "    train_loss_full = []\n",
    "    test_mse_full = []\n",
    "    test_re_full = []\n",
    "    alpha_full = []\n",
    "\n",
    "    lambda1_full = []\n",
    "    elapsed_time= np.zeros((max_reps,1))\n",
    "    \n",
    "    time_threshold = np.empty((max_reps,1))\n",
    "    time_threshold[:] = np.nan\n",
    "    epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "    \n",
    "    n_val = lrn_tune[tune_reps,1]\n",
    "\n",
    "    for reps in range(max_reps):\n",
    "        print(label)\n",
    "        'Generate Training data'\n",
    "        print(reps)\n",
    "        torch.manual_seed(reps*36)\n",
    "        \n",
    "        train_loss = []\n",
    "        test_mse_loss = []\n",
    "        test_re_loss = []   \n",
    "        alpha_val = []\n",
    "\n",
    "        lambda1_val = []\n",
    "\n",
    "        N_f = 50000 #Total number of collocation points \n",
    "        N_train = 500\n",
    "\n",
    "        layers = np.array([2,50,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "        \n",
    "        PINN = Sequentialmodel(layers,n_val)\n",
    "\n",
    "        PINN.to(device)\n",
    "\n",
    "        'Neural Network Summary'\n",
    "        print(PINN)\n",
    "\n",
    "        params = list(PINN.parameters())\n",
    "\n",
    "        optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lrn_tune[tune_reps,0], \n",
    "                                  max_iter = 10, \n",
    "                                  max_eval = 15, \n",
    "                                  tolerance_grad = -1, \n",
    "                                  tolerance_change = -1, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "        \n",
    "        \n",
    "        nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "        torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "        train_loss_full.append(train_loss)\n",
    "        test_mse_full.append(test_mse_loss)\n",
    "        test_re_full.append(test_re_loss)\n",
    "        #elapsed_time[reps] = time.time() - start_time\n",
    "        alpha_full.append(alpha_val)\n",
    "  \n",
    "        lambda1_full.append(lambda1_val)\n",
    "        \n",
    "        if(nan_flag == 1):\n",
    "            nan_tune.append(tune_reps)\n",
    "            break\n",
    "\n",
    "        print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "    mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"alpha\": alpha_full,\"lambda1\": lambda1_full, \"label\": label}\n",
    "    savemat(label+'.mat', mdic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -0.5674061\n",
      "1   -0.18346347\n",
      "2   [[nan]]\n",
      "3   nan\n",
      "4   [[nan]]\n",
      "5   -0.22076301\n",
      "6   -0.08117101\n",
      "7   [[nan]]\n",
      "8   nan\n",
      "9   [[nan]]\n",
      "10   [[ 2.9659973e-04  3.6526890e-04  3.6491931e-04 -1.7225918e-03\n",
      "  -2.8505474e-01 -6.6395712e-01 -6.7049617e-01 -1.0269625e+00\n",
      "  -1.2820612e+00 -1.3224475e+00 -1.2399526e+00 -1.1882937e+00\n",
      "  -8.2210255e-01 -8.6709797e-01 -8.5426378e-01 -9.1268712e-01\n",
      "  -9.7879589e-01 -1.0841706e+00 -1.2198769e+00 -1.2949730e+00\n",
      "  -1.3551496e+00 -1.4492692e+00 -1.4796087e+00 -1.4836943e+00\n",
      "  -1.5168719e+00 -1.5208520e+00 -1.5149412e+00 -1.5133348e+00\n",
      "  -1.5150899e+00 -1.5119671e+00 -1.5120102e+00 -1.5249616e+00\n",
      "  -1.5612912e+00 -1.6117815e+00 -1.6300784e+00 -1.6384304e+00\n",
      "  -1.6809042e+00 -1.6705692e+00 -1.6539471e+00 -1.6466869e+00\n",
      "  -1.6284616e+00 -1.5976505e+00 -1.5627764e+00 -1.5062680e+00\n",
      "  -1.5258645e+00 -1.5533000e+00 -1.5546029e+00 -1.5747427e+00\n",
      "  -1.5576591e+00 -1.5923307e+00 -1.5989798e+00 -1.6002145e+00\n",
      "  -1.5846990e+00 -1.5285442e+00 -1.4696697e+00 -1.4721315e+00\n",
      "  -1.4626455e+00 -1.4975855e+00 -1.5277629e+00 -1.5729176e+00\n",
      "  -1.6210574e+00 -1.6523020e+00 -1.6778897e+00 -1.6862158e+00\n",
      "  -1.6642292e+00 -1.6482680e+00 -1.6365525e+00 -1.6538883e+00\n",
      "  -1.6906916e+00 -1.6822938e+00 -1.6693641e+00 -1.6722440e+00\n",
      "  -1.6706530e+00 -1.6550143e+00 -1.6479146e+00]]\n",
      "11   -0.02915073\n",
      "12   [[nan]]\n",
      "13   nan\n",
      "14   nan\n",
      "15   -1.1692178\n",
      "16   [[ 1.7520722e-03 -2.7099110e-03  1.6701833e-03 -3.4431095e-04\n",
      "  -5.9649604e-04  2.3411636e-04 -1.7407705e-03  1.8391719e-03\n",
      "  -1.9308453e-02 -3.0889568e-01 -3.0800521e-01 -2.7258411e-01\n",
      "  -2.1833989e-01  4.5824107e-02  7.8289434e-03 -9.0505648e-03\n",
      "   3.4703305e-03 -2.3894785e-03  7.6867681e-04  3.3398689e-04\n",
      "   2.3912232e-04 -3.4154393e-04 -5.0398234e-05 -2.9293849e-05\n",
      "  -2.6919137e-05  6.0914081e-06 -1.2359984e-06  2.0260625e-06\n",
      "   1.0761771e-06  2.6304378e-06 -3.2045175e-06  1.2892798e-07\n",
      "   5.2133255e-06  1.1556958e-05  1.6774426e-05  2.7350375e-06\n",
      "  -1.0147450e-05 -7.6263209e-06  3.7856271e-06 -3.6098097e-06\n",
      "  -4.4277090e-06  1.6086242e-06 -3.1810735e-06 -4.3978230e-06\n",
      "   2.2962936e-06 -1.7327168e-06 -4.9329756e-06 -5.1363781e-06\n",
      "  -2.2913307e-06 -5.5329278e-06 -2.8550367e-06 -5.2278341e-07\n",
      "  -6.2022895e-07  8.9629282e-07  4.1846351e-07  1.1521976e-06\n",
      "   1.8975685e-06  1.6308492e-06  1.5967763e-06  1.0217284e-06\n",
      "   4.3522775e-07  7.7229419e-07  1.0130638e-06  1.7348265e-06\n",
      "   1.3288613e-06  1.8506531e-06  6.4860483e-06  2.8481923e-06\n",
      "   3.7568338e-07  2.5772590e-07  2.0393711e-06 -1.7276149e-06\n",
      "   5.3750655e-06  1.3912459e-06  1.5649904e-06]]\n",
      "17   [[nan]]\n",
      "18   nan\n",
      "19   nan\n",
      "20   -1.530047\n",
      "21   [[-0.00069308 -0.00085071 -0.0010488   0.00207043 -0.00299825         nan]]\n",
      "22   [[nan]]\n",
      "23   nan\n",
      "24   nan\n"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(25):\n",
    "    label = \"inv_HT_atanh_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"lambda1\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 3.  ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrn_tune[11]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
