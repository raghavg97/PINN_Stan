{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_2D_4(xt): #True function for 2D_4 Heat Transfer in a rod x \\in [0,1] t \\in [0,0.1]\n",
    "    term1 = 4*u0/np.pi\n",
    "    \n",
    "    resol_n = 10000\n",
    "    \n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "\n",
    "    u = np.zeros((np.shape(xt)[0],1))\n",
    "    \n",
    "    for i in range(resol_n):\n",
    "        j = 2*i-1\n",
    "        term2 = np.sin(j*np.pi*x)/j\n",
    "        term3 = np.exp(-1*np.square(j*np.pi)*t)\n",
    "        \n",
    "        u = u + term2*term3\n",
    "        \n",
    "    u = term1*u\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = 50.0\n",
    "loss_thresh = 0.1\n",
    "\n",
    "x_ll = np.array(0.0)\n",
    "x_ul = np.array(1.0)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,100).reshape(-1,1)\n",
    "t = np.linspace(0,0.1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = true_2D_4(xt)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_f,N_train,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #X_Train\n",
    "    np.random.seed(seed)\n",
    "    x_train = np.random.uniform(x_ll,x_ul,(N_train,1))\n",
    "    t_train = np.random.uniform(0,0.1,(N_train,1))\n",
    "    \n",
    "    xt_train = np.hstack((x_train,t_train))\n",
    "    u_train = true_2D_4(xt_train)\n",
    "    \n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requiresGrad = True\n",
    "        \n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    \n",
    "    def loss_PDE(self, xt_coll,f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_tt[:,[0]]\n",
    "                \n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        f = du_dt - self.lambda1*d2u_dx2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_train = self.loss_function(self.forward(xt_train),u_train)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "        \n",
    "        #print(self.iter,\"train_loss\",loss_train.cpu().detach().numpy(),\"F Loss\",(loss_f).cpu().detach().numpy())\n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                    \n",
    "    \n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_coll,f_hat, xt_train, u_train,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xt_coll, xt_train, u_train = trainingdata(N_f,N_train,123)\n",
    "    \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_train = torch.from_numpy(xt_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    nan_flag = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_coll,f_hat, xt_train, u_train,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_coll,f_hat, xt_train, u_train).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1],\"Lambda1\",lambda1_val[-1])\n",
    "\n",
    "        if(np.isnan(loss_np)):\n",
    "            nan_flag =1\n",
    "            print(\"NAN BREAK!\")\n",
    "            break\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_HT_swish_tune0\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.04877 Test MSE 858.267842986848 Test RE 0.49317447521322566 Lambda1 -2.7418824e-07\n",
      "1 Train Loss 838.04877 Test MSE 858.2678212496938 Test RE 0.4931744689679668 Lambda1 -2.9793296e-07\n",
      "2 Train Loss 838.04877 Test MSE 858.2677976494191 Test RE 0.4931744621874184 Lambda1 -3.2179142e-07\n",
      "3 Train Loss 838.04865 Test MSE 858.267771971816 Test RE 0.4931744548100367 Lambda1 -3.4561702e-07\n",
      "4 Train Loss 838.04865 Test MSE 858.2677420537935 Test RE 0.49317444621434847 Lambda1 -3.69476e-07\n",
      "5 Train Loss 838.04865 Test MSE 858.2677096788711 Test RE 0.49317443691277285 Lambda1 -3.9337343e-07\n",
      "6 Train Loss 838.0486 Test MSE 858.2676770494753 Test RE 0.49317442753808466 Lambda1 -4.1732653e-07\n",
      "7 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "8 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "9 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "10 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "11 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "12 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "13 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "14 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "15 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "16 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "17 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "18 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "19 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "20 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "21 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "22 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "23 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "24 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "25 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "26 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "27 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "28 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "29 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "30 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "31 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "32 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "33 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "34 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "35 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "36 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "37 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "38 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "39 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "40 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "41 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "42 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "43 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "44 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "45 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "46 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "47 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "48 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "49 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "50 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "51 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "52 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "53 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "54 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "55 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "56 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "57 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "58 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "59 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "60 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "61 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "62 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "63 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "64 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "65 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "66 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "67 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "68 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "69 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "70 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "71 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "72 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "73 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 Train Loss 838.0486 Test MSE 858.2676516458465 Test RE 0.49317442023941765 Lambda1 -4.3530383e-07\n",
      "Training time: 155.02\n",
      "Training time: 155.02\n",
      "inv_HT_swish_tune0\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 838.79974 Test MSE 859.829898873315 Test RE 0.4936230623870932 Lambda1 2.3040664e-06\n",
      "1 Train Loss 838.1017 Test MSE 858.420811564698 Test RE 0.49321842235490504 Lambda1 4.271708e-06\n",
      "2 Train Loss 838.09106 Test MSE 858.3335864389695 Test RE 0.4931933634728091 Lambda1 5.805126e-06\n",
      "3 Train Loss 838.09033 Test MSE 858.3210005890049 Test RE 0.4931897475823623 Lambda1 6.6094885e-06\n",
      "4 Train Loss 838.0888 Test MSE 858.2910752660293 Test RE 0.4931811499882695 Lambda1 1.130316e-05\n",
      "5 Train Loss 838.0831 Test MSE 858.2362142704094 Test RE 0.4931653879454984 Lambda1 4.0425934e-05\n",
      "6 Train Loss 838.015 Test MSE 858.1404185446868 Test RE 0.4931378637896547 Lambda1 0.0007375686\n",
      "7 Train Loss 837.8804 Test MSE 858.0503005105421 Test RE 0.4931119695533 Lambda1 0.0049546375\n",
      "8 Train Loss 837.86255 Test MSE 857.9022431161228 Test RE 0.4930694242485348 Lambda1 0.0068256245\n",
      "9 Train Loss 837.8619 Test MSE 857.8891505569758 Test RE 0.4930656618354612 Lambda1 0.0072071003\n",
      "10 Train Loss 837.8617 Test MSE 857.884836341812 Test RE 0.49306442205172696 Lambda1 0.007370351\n",
      "11 Train Loss 837.8612 Test MSE 857.8744285619239 Test RE 0.4930614311365583 Lambda1 0.007936304\n",
      "12 Train Loss 837.86096 Test MSE 857.8696087368178 Test RE 0.4930600460427011 Lambda1 0.008285058\n",
      "13 Train Loss 837.85876 Test MSE 857.831341143077 Test RE 0.4930490487817181 Lambda1 0.013079433\n",
      "14 Train Loss 837.8076 Test MSE 858.0320051161345 Test RE 0.49310671244538024 Lambda1 0.09039941\n",
      "15 Train Loss 837.5776 Test MSE 857.4184252438748 Test RE 0.49293037026845177 Lambda1 0.22625348\n",
      "16 Train Loss 836.3623 Test MSE 855.9722142452459 Test RE 0.4925144810358713 Lambda1 0.33918515\n",
      "17 Train Loss 821.11926 Test MSE 839.2373179756671 Test RE 0.4876762032955686 Lambda1 0.6906142\n",
      "18 Train Loss 813.9589 Test MSE 830.7922206735383 Test RE 0.485216299387961 Lambda1 0.642033\n",
      "19 Train Loss 775.24506 Test MSE 793.3654855776722 Test RE 0.47416099128748485 Lambda1 0.2579236\n",
      "20 Train Loss 725.1726 Test MSE 750.4607478421277 Test RE 0.46116162573230757 Lambda1 0.14088996\n",
      "21 Train Loss 692.5935 Test MSE 683.2747870438809 Test RE 0.44003464244228374 Lambda1 0.12918174\n",
      "22 Train Loss 679.2849 Test MSE 675.4147870869588 Test RE 0.43749636896491995 Lambda1 0.1303322\n",
      "23 Train Loss 635.01105 Test MSE 614.5817650876902 Test RE 0.41732942391631905 Lambda1 0.1178717\n",
      "24 Train Loss 533.40656 Test MSE 490.0970184316971 Test RE 0.3726749491239134 Lambda1 0.053506702\n",
      "25 Train Loss 492.80453 Test MSE 469.4766773415076 Test RE 0.3647507400895309 Lambda1 0.007858822\n",
      "26 Train Loss 358.63525 Test MSE 360.3799322268804 Test RE 0.3195725516024685 Lambda1 -0.00038855808\n",
      "27 Train Loss 347.03873 Test MSE 357.89816038628857 Test RE 0.3184702755732742 Lambda1 0.0010637697\n",
      "28 Train Loss 333.26575 Test MSE 344.563614894734 Test RE 0.31248118879271397 Lambda1 0.00064935\n",
      "29 Train Loss 313.86176 Test MSE 321.6454515306081 Test RE 0.30191026390728637 Lambda1 -0.0011697036\n",
      "30 Train Loss 288.87634 Test MSE 299.0137414835674 Test RE 0.29109499893714 Lambda1 0.0015537171\n",
      "31 Train Loss 286.7892 Test MSE 296.1747279740197 Test RE 0.2897097889548328 Lambda1 0.003695249\n",
      "32 Train Loss 276.05084 Test MSE 283.6122409066679 Test RE 0.2834990815951654 Lambda1 0.017554387\n",
      "33 Train Loss 268.0781 Test MSE 274.01859235173 Test RE 0.2786629216245762 Lambda1 0.026132517\n",
      "34 Train Loss 267.38324 Test MSE 273.7993991270391 Test RE 0.2785514451664375 Lambda1 0.027652266\n",
      "35 Train Loss 264.8321 Test MSE 271.32889366418294 Test RE 0.27729190580265717 Lambda1 0.038280446\n",
      "36 Train Loss 260.95142 Test MSE 266.20155528635223 Test RE 0.27465939866880495 Lambda1 0.048769735\n",
      "37 Train Loss 258.89316 Test MSE 265.64597989538726 Test RE 0.27437263531091927 Lambda1 0.058036372\n",
      "38 Train Loss 256.83304 Test MSE 264.13190359708454 Test RE 0.27358961060481835 Lambda1 0.0674786\n",
      "39 Train Loss 255.78244 Test MSE 263.7752613672726 Test RE 0.2734048419973086 Lambda1 0.074481785\n",
      "40 Train Loss 254.30344 Test MSE 263.30545282204486 Test RE 0.27316125361858223 Lambda1 0.07761503\n",
      "41 Train Loss 251.38394 Test MSE 263.0265799754498 Test RE 0.2730165596205698 Lambda1 0.076726824\n",
      "42 Train Loss 245.61482 Test MSE 258.3486907858717 Test RE 0.27057788836768826 Lambda1 0.08045181\n",
      "43 Train Loss 238.06839 Test MSE 256.94155944154215 Test RE 0.2698400125930149 Lambda1 0.07825291\n",
      "44 Train Loss 236.79822 Test MSE 256.271178616758 Test RE 0.269487765729018 Lambda1 0.07779456\n",
      "45 Train Loss 235.55186 Test MSE 253.8316461378078 Test RE 0.2682020257305778 Lambda1 0.08447301\n",
      "46 Train Loss 227.30235 Test MSE 242.53200886143944 Test RE 0.2621643911075165 Lambda1 0.13791753\n",
      "47 Train Loss 222.81415 Test MSE 238.43453378848758 Test RE 0.25994037975503537 Lambda1 0.15943524\n",
      "48 Train Loss 219.60573 Test MSE 234.2282917406816 Test RE 0.2576373634689179 Lambda1 0.19128929\n",
      "49 Train Loss 219.0856 Test MSE 234.21773440917423 Test RE 0.2576315571812052 Lambda1 0.20431052\n",
      "50 Train Loss 218.815 Test MSE 234.23476575308703 Test RE 0.25764092396101207 Lambda1 0.20934501\n",
      "51 Train Loss 216.38538 Test MSE 230.51901002510903 Test RE 0.2555892264313279 Lambda1 0.25630906\n",
      "52 Train Loss 211.35298 Test MSE 225.0041780732485 Test RE 0.2525134188114639 Lambda1 0.27623788\n",
      "53 Train Loss 210.78142 Test MSE 223.97855965391844 Test RE 0.2519372557044201 Lambda1 0.2785674\n",
      "54 Train Loss 208.91331 Test MSE 221.52488010479672 Test RE 0.2505534722550754 Lambda1 0.31553906\n",
      "55 Train Loss 205.75618 Test MSE 219.7161897771947 Test RE 0.24952852542505 Lambda1 0.3624051\n",
      "56 Train Loss 203.80252 Test MSE 218.22665268685807 Test RE 0.24868126405386226 Lambda1 0.39436755\n",
      "57 Train Loss 194.37445 Test MSE 210.62275483401265 Test RE 0.24431032139910325 Lambda1 0.42114785\n",
      "58 Train Loss 193.22179 Test MSE 206.9061245595039 Test RE 0.24214518842256408 Lambda1 0.4283579\n",
      "59 Train Loss 191.12234 Test MSE 202.83500650972087 Test RE 0.23975110958893509 Lambda1 0.44478163\n",
      "60 Train Loss 184.79135 Test MSE 195.44304741904568 Test RE 0.23534191524457793 Lambda1 0.5152575\n",
      "61 Train Loss 177.67447 Test MSE 177.45829853679766 Test RE 0.2242525175108248 Lambda1 0.5803733\n",
      "62 Train Loss 174.1713 Test MSE 169.5020158675662 Test RE 0.21916772743391924 Lambda1 0.5892514\n",
      "63 Train Loss 169.1124 Test MSE 164.09311773236354 Test RE 0.2156424979398898 Lambda1 0.5484654\n",
      "64 Train Loss 167.41727 Test MSE 160.9415085246513 Test RE 0.21356161896149714 Lambda1 0.516753\n",
      "65 Train Loss 165.03584 Test MSE 161.3631323833051 Test RE 0.2138411732537336 Lambda1 0.51046556\n",
      "66 Train Loss 161.21036 Test MSE 162.06205113062717 Test RE 0.2143037823835607 Lambda1 0.5113074\n",
      "67 Train Loss 157.61856 Test MSE 154.99138832968384 Test RE 0.20957667934591287 Lambda1 0.5227108\n",
      "68 Train Loss 152.85541 Test MSE 151.86580432026355 Test RE 0.20745273649040044 Lambda1 0.53147286\n",
      "69 Train Loss 145.47568 Test MSE 141.77053085275668 Test RE 0.20043896587056353 Lambda1 0.564138\n",
      "70 Train Loss 142.7624 Test MSE 139.12517504543405 Test RE 0.19856012269485823 Lambda1 0.597392\n",
      "71 Train Loss 141.6538 Test MSE 139.24107856830693 Test RE 0.19864281451025462 Lambda1 0.60552216\n",
      "72 Train Loss 139.9535 Test MSE 138.0142161019385 Test RE 0.1977657506991942 Lambda1 0.58997047\n",
      "73 Train Loss 138.66757 Test MSE 137.26488719982487 Test RE 0.19722814924286164 Lambda1 0.5841296\n",
      "74 Train Loss 137.29236 Test MSE 135.25387277602496 Test RE 0.19577806206455658 Lambda1 0.57670593\n",
      "Training time: 204.75\n",
      "Training time: 204.75\n",
      "inv_HT_swish_tune0\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 840.8417 Test MSE 862.6630928316092 Test RE 0.4944356531756279 Lambda1 2.4666165e-06\n",
      "1 Train Loss 838.0969 Test MSE 858.4571479951354 Test RE 0.4932288610622396 Lambda1 2.1281141e-06\n",
      "2 Train Loss 838.07605 Test MSE 858.3259134612 Test RE 0.4931911590442331 Lambda1 1.9005313e-06\n",
      "3 Train Loss 838.07477 Test MSE 858.3042909094149 Test RE 0.4931849468826457 Lambda1 1.7312166e-06\n",
      "4 Train Loss 838.07465 Test MSE 858.3003251680808 Test RE 0.49318380751621754 Lambda1 1.6500305e-06\n",
      "5 Train Loss 838.07434 Test MSE 858.2968086115511 Test RE 0.4931827971993948 Lambda1 1.5474662e-06\n",
      "6 Train Loss 838.074 Test MSE 858.2911278467852 Test RE 0.49318116509493787 Lambda1 1.335076e-06\n",
      "7 Train Loss 838.07196 Test MSE 858.2617307416783 Test RE 0.49317271911302024 Lambda1 -3.8571903e-07\n",
      "8 Train Loss 838.06757 Test MSE 858.2210150392402 Test RE 0.4931610209846321 Lambda1 -5.1892976e-06\n",
      "9 Train Loss 838.0379 Test MSE 858.1345232389923 Test RE 0.49313616989236997 Lambda1 -2.8673268e-05\n",
      "10 Train Loss 837.8422 Test MSE 857.9843672520025 Test RE 0.4930930236339564 Lambda1 0.00093215634\n",
      "11 Train Loss 837.83014 Test MSE 857.8383812938489 Test RE 0.4930510719839217 Lambda1 0.0018240662\n",
      "12 Train Loss 837.8299 Test MSE 857.833357871999 Test RE 0.4930496283511768 Lambda1 0.0019280078\n",
      "13 Train Loss 837.70197 Test MSE 857.4072748849144 Test RE 0.4929271650839825 Lambda1 0.042411927\n",
      "14 Train Loss 836.1962 Test MSE 857.1484279628394 Test RE 0.49285275335773693 Lambda1 0.11173\n",
      "15 Train Loss 832.5057 Test MSE 855.8512860514753 Test RE 0.492479689608467 Lambda1 0.19236559\n",
      "16 Train Loss 808.7436 Test MSE 827.6199469174513 Test RE 0.4842890452244557 Lambda1 0.16631615\n",
      "17 Train Loss 727.62085 Test MSE 750.8429946875357 Test RE 0.46127905701445976 Lambda1 0.010106991\n",
      "18 Train Loss 659.97864 Test MSE 694.9455802189909 Test RE 0.4437767744755833 Lambda1 0.0029798388\n",
      "19 Train Loss 621.8131 Test MSE 647.1069910785508 Test RE 0.42823012586661635 Lambda1 0.0013774942\n",
      "20 Train Loss 611.47156 Test MSE 644.9705044139937 Test RE 0.42752261970496264 Lambda1 0.00072794274\n",
      "21 Train Loss 596.3033 Test MSE 639.1811457136583 Test RE 0.4255995387019119 Lambda1 0.00023978924\n",
      "22 Train Loss 529.30316 Test MSE 534.022427189154 Test RE 0.38901730316292193 Lambda1 -0.00085623184\n",
      "23 Train Loss 384.5316 Test MSE 372.5971370326163 Test RE 0.3249443031365811 Lambda1 -0.002883475\n",
      "24 Train Loss 362.8836 Test MSE 360.6543039246224 Test RE 0.3196941801577147 Lambda1 -0.00053775584\n",
      "25 Train Loss 335.65155 Test MSE 334.29038777870875 Test RE 0.30778759712563064 Lambda1 0.0030378988\n",
      "26 Train Loss 295.98386 Test MSE 309.61944712916994 Test RE 0.2962124348519267 Lambda1 -0.00042293113\n",
      "27 Train Loss 284.02414 Test MSE 297.81975700939194 Test RE 0.2905132354315357 Lambda1 -0.0008278145\n",
      "28 Train Loss 262.85208 Test MSE 285.2028204286292 Test RE 0.28429294254679105 Lambda1 -0.000873162\n",
      "29 Train Loss 261.33002 Test MSE 282.7804748153308 Test RE 0.2830830592367376 Lambda1 -0.00021891916\n",
      "30 Train Loss 259.92947 Test MSE 281.6021918382719 Test RE 0.28249267170223397 Lambda1 0.00018566547\n",
      "31 Train Loss 258.8895 Test MSE 281.17352285132705 Test RE 0.2822775775542437 Lambda1 0.000379182\n",
      "32 Train Loss 257.98285 Test MSE 280.57499157777676 Test RE 0.28197697676565336 Lambda1 0.00010507158\n",
      "33 Train Loss 257.72452 Test MSE 280.7398808613387 Test RE 0.28205982119940337 Lambda1 8.591184e-05\n",
      "34 Train Loss 257.47003 Test MSE 280.49815977824557 Test RE 0.2819383662647884 Lambda1 0.00015140395\n",
      "35 Train Loss 257.004 Test MSE 280.36339704062937 Test RE 0.2818706307923631 Lambda1 0.00032663727\n",
      "36 Train Loss 256.41913 Test MSE 279.8182100348979 Test RE 0.28159643846510796 Lambda1 0.0006017814\n",
      "37 Train Loss 256.29495 Test MSE 279.8777859877519 Test RE 0.2816264141469486 Lambda1 0.00070557115\n",
      "38 Train Loss 256.1145 Test MSE 280.0072488253642 Test RE 0.28169154246578304 Lambda1 0.00064932957\n",
      "39 Train Loss 255.69872 Test MSE 280.0428015407439 Test RE 0.28170942518381387 Lambda1 0.0010133024\n",
      "40 Train Loss 255.4093 Test MSE 279.91579659967084 Test RE 0.2816455375455113 Lambda1 0.0015761157\n",
      "41 Train Loss 255.23473 Test MSE 280.3947110646201 Test RE 0.2818863715369501 Lambda1 0.0013422233\n",
      "42 Train Loss 255.21242 Test MSE 280.52833569922444 Test RE 0.2819535312859511 Lambda1 0.0011501173\n",
      "43 Train Loss 255.16022 Test MSE 280.4982073977205 Test RE 0.281938390196775 Lambda1 0.0012172639\n",
      "44 Train Loss 255.06104 Test MSE 280.4860567921976 Test RE 0.2819322836350034 Lambda1 0.0014229337\n",
      "45 Train Loss 254.98535 Test MSE 280.59751318007307 Test RE 0.281988293608002 Lambda1 0.0013897727\n",
      "46 Train Loss 254.88245 Test MSE 280.9395983868802 Test RE 0.2821601316260474 Lambda1 0.0011227102\n",
      "47 Train Loss 254.76744 Test MSE 281.3092675640282 Test RE 0.28234570819586546 Lambda1 0.0008949484\n",
      "48 Train Loss 254.71544 Test MSE 281.45274936938563 Test RE 0.28241770423761525 Lambda1 0.0008230226\n",
      "49 Train Loss 254.60878 Test MSE 281.109741744281 Test RE 0.282245559963603 Lambda1 0.000949623\n",
      "50 Train Loss 254.52734 Test MSE 280.52267334480996 Test RE 0.28195068571087767 Lambda1 0.0011002496\n",
      "51 Train Loss 254.48367 Test MSE 280.5473294376676 Test RE 0.28196307624148537 Lambda1 0.0010858327\n",
      "52 Train Loss 254.40755 Test MSE 280.71224469828206 Test RE 0.2820459378084277 Lambda1 0.0010449595\n",
      "53 Train Loss 254.32132 Test MSE 280.55350024444306 Test RE 0.28196617719792144 Lambda1 0.0011879108\n",
      "54 Train Loss 254.2868 Test MSE 280.5476934701719 Test RE 0.28196325917620035 Lambda1 0.0012457129\n",
      "55 Train Loss 254.18315 Test MSE 280.9546125689588 Test RE 0.28216767123067443 Lambda1 0.0011546446\n",
      "56 Train Loss 254.13968 Test MSE 281.1314564805154 Test RE 0.28225646099018387 Lambda1 0.0010445191\n",
      "57 Train Loss 254.08614 Test MSE 280.9341519366054 Test RE 0.2821573965566882 Lambda1 0.0011429661\n",
      "58 Train Loss 254.00299 Test MSE 280.81584537082773 Test RE 0.2820979794522669 Lambda1 0.001334578\n",
      "59 Train Loss 253.9244 Test MSE 280.9782785256087 Test RE 0.2821795550491964 Lambda1 0.0012815718\n",
      "60 Train Loss 253.68155 Test MSE 281.53761803841167 Test RE 0.2824602808479838 Lambda1 0.0007765305\n",
      "61 Train Loss 253.51736 Test MSE 281.6761517667882 Test RE 0.2825297661687349 Lambda1 0.0007128847\n",
      "62 Train Loss 253.33473 Test MSE 281.7335841923646 Test RE 0.28255856793741724 Lambda1 0.0007878572\n",
      "63 Train Loss 253.17192 Test MSE 281.74118625474574 Test RE 0.2825623800732263 Lambda1 0.00085695396\n",
      "64 Train Loss 253.04385 Test MSE 281.9549657577767 Test RE 0.28266956104952345 Lambda1 0.00079630263\n",
      "65 Train Loss 252.94035 Test MSE 282.1514539881622 Test RE 0.28276803700416236 Lambda1 0.0007367653\n",
      "66 Train Loss 252.80423 Test MSE 282.03597052801064 Test RE 0.28271016316911873 Lambda1 0.00059448107\n",
      "67 Train Loss 252.6716 Test MSE 281.89884761161545 Test RE 0.2826414294628263 Lambda1 0.0005062805\n",
      "68 Train Loss 252.57907 Test MSE 281.9831441906894 Test RE 0.28268368562122925 Lambda1 0.00046784966\n",
      "69 Train Loss 252.4961 Test MSE 282.16414074026136 Test RE 0.28277439417087347 Lambda1 0.00047066793\n",
      "70 Train Loss 252.38742 Test MSE 282.50560149647333 Test RE 0.28294544206515193 Lambda1 0.00040307923\n",
      "71 Train Loss 252.33502 Test MSE 282.7206524783059 Test RE 0.2830531144784992 Lambda1 0.00036215727\n",
      "72 Train Loss 252.16898 Test MSE 283.3450415083375 Test RE 0.28336550373203284 Lambda1 0.00031999964\n",
      "73 Train Loss 252.03398 Test MSE 283.82410406875175 Test RE 0.2836049511423467 Lambda1 0.0003424976\n",
      "74 Train Loss 251.99423 Test MSE 283.94640335338 Test RE 0.28366604698755127 Lambda1 0.00036739366\n",
      "Training time: 228.42\n",
      "Training time: 228.42\n",
      "inv_HT_swish_tune0\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 838.51776 Test MSE 859.3875448935053 Test RE 0.49349606970055565 Lambda1 2.6951108e-07\n",
      "1 Train Loss 838.05945 Test MSE 858.3171784032896 Test RE 0.49318864947052216 Lambda1 2.9899172e-07\n",
      "2 Train Loss 838.05664 Test MSE 858.2788747196801 Test RE 0.49317764470839687 Lambda1 3.3015294e-07\n",
      "3 Train Loss 838.0545 Test MSE 858.2502350588311 Test RE 0.49316941628828215 Lambda1 4.3680402e-07\n",
      "4 Train Loss 838.0396 Test MSE 858.1427457133328 Test RE 0.4931385324530645 Lambda1 2.0646523e-06\n",
      "5 Train Loss 837.931 Test MSE 857.7833667675741 Test RE 0.4930352616595927 Lambda1 0.0004920985\n",
      "6 Train Loss 837.885 Test MSE 857.8711809616748 Test RE 0.4930604978601436 Lambda1 0.0017639116\n",
      "7 Train Loss 837.8846 Test MSE 857.8807876761582 Test RE 0.4930632585765953 Lambda1 0.0018556062\n",
      "8 Train Loss 837.88446 Test MSE 857.8842022372168 Test RE 0.4930642398276916 Lambda1 0.0019063149\n",
      "9 Train Loss 837.88434 Test MSE 857.887645972114 Test RE 0.49306522946057024 Lambda1 0.0019751962\n",
      "10 Train Loss 837.8841 Test MSE 857.8915128961297 Test RE 0.4930663407037718 Lambda1 0.0020785208\n",
      "11 Train Loss 837.88385 Test MSE 857.8963554332463 Test RE 0.4930677323067021 Lambda1 0.0022470125\n",
      "12 Train Loss 837.8836 Test MSE 857.9009190740007 Test RE 0.49306904375941496 Lambda1 0.0024448112\n",
      "13 Train Loss 837.8828 Test MSE 857.9170274201248 Test RE 0.493073672784376 Lambda1 0.0033654803\n",
      "14 Train Loss 837.8819 Test MSE 857.9294684551159 Test RE 0.4930772479113295 Lambda1 0.004483128\n",
      "15 Train Loss 837.8771 Test MSE 857.9875906344832 Test RE 0.49309394988971444 Lambda1 0.012441315\n",
      "16 Train Loss 837.8159 Test MSE 858.2461844110038 Test RE 0.49316825249134977 Lambda1 0.07467615\n",
      "17 Train Loss 835.9606 Test MSE 855.3430537266337 Test RE 0.49233344266644713 Lambda1 0.38309512\n",
      "18 Train Loss 810.08435 Test MSE 822.4594982830511 Test RE 0.4827768434750831 Lambda1 0.31809017\n",
      "19 Train Loss 759.1735 Test MSE 776.6076827743491 Test RE 0.46912654955699506 Lambda1 0.01749629\n",
      "20 Train Loss 646.8584 Test MSE 676.4731922906775 Test RE 0.4378390229754329 Lambda1 0.0025495635\n",
      "21 Train Loss 631.3156 Test MSE 667.3742360347701 Test RE 0.4348844599647898 Lambda1 0.00057861354\n",
      "22 Train Loss 598.2247 Test MSE 638.0260042047653 Test RE 0.42521478860601736 Lambda1 0.00026084276\n",
      "23 Train Loss 586.12476 Test MSE 626.029956093308 Test RE 0.421198414942194 Lambda1 -8.924337e-05\n",
      "24 Train Loss 579.84503 Test MSE 624.290951546333 Test RE 0.420612999399568 Lambda1 -4.968065e-05\n",
      "25 Train Loss 564.87317 Test MSE 566.2462340831768 Test RE 0.4005823721174006 Lambda1 -9.845208e-05\n",
      "26 Train Loss 500.9065 Test MSE 459.9091624923151 Test RE 0.3610149622167872 Lambda1 -0.0004118439\n",
      "27 Train Loss 447.4897 Test MSE 458.8255645481798 Test RE 0.3605894153919117 Lambda1 -0.0011389711\n",
      "28 Train Loss 380.81375 Test MSE 364.3205906962995 Test RE 0.32131502151376845 Lambda1 -0.0013454432\n",
      "29 Train Loss 340.85068 Test MSE 344.39177258600427 Test RE 0.31240325806624103 Lambda1 -0.0014938817\n",
      "30 Train Loss 315.3802 Test MSE 319.08004668632344 Test RE 0.3007038538477871 Lambda1 -0.00127514\n",
      "31 Train Loss 292.35928 Test MSE 305.66603920504485 Test RE 0.2943152497436545 Lambda1 -0.0008827165\n",
      "32 Train Loss 270.20197 Test MSE 284.17764916613044 Test RE 0.28378153222488745 Lambda1 0.00032313383\n",
      "33 Train Loss 262.4899 Test MSE 281.2139755184749 Test RE 0.28229788257815003 Lambda1 0.00063053024\n",
      "34 Train Loss 260.50958 Test MSE 280.5639669906106 Test RE 0.28197143687366305 Lambda1 0.00085199485\n",
      "35 Train Loss 259.08026 Test MSE 280.01747956964334 Test RE 0.2816966885607944 Lambda1 0.0011447677\n",
      "36 Train Loss 258.8651 Test MSE 279.85060804568013 Test RE 0.2816127399423928 Lambda1 0.0012629543\n",
      "37 Train Loss 258.07217 Test MSE 278.9005365976046 Test RE 0.2811343066603897 Lambda1 0.0021483575\n",
      "38 Train Loss 257.71204 Test MSE 278.37107922721526 Test RE 0.28086733094522665 Lambda1 0.0028744182\n",
      "39 Train Loss 257.41568 Test MSE 278.16491675142834 Test RE 0.2807633060776391 Lambda1 0.002957771\n",
      "40 Train Loss 257.25958 Test MSE 278.42688950194463 Test RE 0.28089548490673955 Lambda1 0.0023370918\n",
      "41 Train Loss 257.04724 Test MSE 278.1368627585695 Test RE 0.28074914769548176 Lambda1 0.0030705698\n",
      "42 Train Loss 256.3255 Test MSE 277.0140673259494 Test RE 0.2801819042369377 Lambda1 0.007924663\n",
      "43 Train Loss 255.95901 Test MSE 275.81444225760407 Test RE 0.27957457422177373 Lambda1 0.0111265015\n",
      "44 Train Loss 252.37572 Test MSE 271.43286094270024 Test RE 0.27734502680140805 Lambda1 0.029248504\n",
      "45 Train Loss 246.3679 Test MSE 260.4942289981394 Test RE 0.2716991150067774 Lambda1 0.06226968\n",
      "46 Train Loss 233.5921 Test MSE 247.7319733036816 Test RE 0.2649599307798429 Lambda1 0.11485723\n",
      "47 Train Loss 229.92464 Test MSE 245.60330540508488 Test RE 0.2638191242254262 Lambda1 0.13707682\n",
      "48 Train Loss 225.1142 Test MSE 241.40769395060596 Test RE 0.26155602248097537 Lambda1 0.15399154\n",
      "49 Train Loss 224.07823 Test MSE 241.48140677214857 Test RE 0.2615959519474902 Lambda1 0.16424789\n",
      "50 Train Loss 221.94629 Test MSE 240.76029410826746 Test RE 0.2612050705201397 Lambda1 0.15866071\n",
      "51 Train Loss 218.88718 Test MSE 237.2974127076542 Test RE 0.2593197965246986 Lambda1 0.16854359\n",
      "52 Train Loss 208.85825 Test MSE 230.2279779552237 Test RE 0.2554278337673598 Lambda1 0.25331324\n",
      "53 Train Loss 205.70393 Test MSE 227.96168494889514 Test RE 0.25416754833573835 Lambda1 0.30129346\n",
      "54 Train Loss 202.54243 Test MSE 223.218978144952 Test RE 0.2515096937915343 Lambda1 0.3362955\n",
      "55 Train Loss 196.31868 Test MSE 211.1505702042641 Test RE 0.2446162476440559 Lambda1 0.30106947\n",
      "56 Train Loss 187.79514 Test MSE 189.9077932294827 Test RE 0.23198535235545975 Lambda1 0.28778973\n",
      "57 Train Loss 172.24391 Test MSE 168.49742616718063 Test RE 0.21851729021848887 Lambda1 0.31522328\n",
      "58 Train Loss 159.8889 Test MSE 147.07021144665075 Test RE 0.2041510082058128 Lambda1 0.38497525\n",
      "59 Train Loss 150.83493 Test MSE 136.45585114532872 Test RE 0.1966460612083868 Lambda1 0.4410356\n",
      "60 Train Loss 139.74698 Test MSE 122.9556380836799 Test RE 0.1866652148894905 Lambda1 0.5212261\n",
      "61 Train Loss 126.40369 Test MSE 121.52282249615108 Test RE 0.18557441261745675 Lambda1 0.5940219\n",
      "62 Train Loss 121.748634 Test MSE 117.30160249562135 Test RE 0.18232286775035814 Lambda1 0.633128\n",
      "63 Train Loss 109.13676 Test MSE 100.46946192059443 Test RE 0.16873540133559672 Lambda1 0.7446104\n",
      "64 Train Loss 105.19202 Test MSE 95.78370752570986 Test RE 0.1647536300708881 Lambda1 0.78551793\n",
      "65 Train Loss 100.48495 Test MSE 89.41905369911352 Test RE 0.15918575639643798 Lambda1 0.8306741\n",
      "66 Train Loss 97.85827 Test MSE 87.93759543030538 Test RE 0.15786158669792857 Lambda1 0.8589166\n",
      "67 Train Loss 91.36253 Test MSE 75.48262047435635 Test RE 0.14625565092514112 Lambda1 0.9012557\n",
      "68 Train Loss 78.993935 Test MSE 69.92480062937975 Test RE 0.1407682750975775 Lambda1 0.8881383\n",
      "69 Train Loss 72.9235 Test MSE 67.66065423087825 Test RE 0.13847050211650483 Lambda1 0.86546576\n",
      "70 Train Loss 71.39873 Test MSE 67.25114104351562 Test RE 0.13805082276328332 Lambda1 0.87082905\n",
      "71 Train Loss 69.00612 Test MSE 63.36059018985281 Test RE 0.1339981423307996 Lambda1 0.88880926\n",
      "72 Train Loss 61.826736 Test MSE 53.976444563955894 Test RE 0.12367767365767411 Lambda1 0.9019931\n",
      "73 Train Loss 57.951607 Test MSE 49.917473828785596 Test RE 0.11893658651905172 Lambda1 0.92251897\n",
      "74 Train Loss 53.10611 Test MSE 47.40643233941327 Test RE 0.11590650409305162 Lambda1 0.94577265\n",
      "Training time: 210.64\n",
      "Training time: 210.64\n",
      "inv_HT_swish_tune0\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 838.13104 Test MSE 858.1013830488736 Test RE 0.4931266476183158 Lambda1 -1.16651755e-07\n",
      "1 Train Loss 838.0646 Test MSE 858.2685968862116 Test RE 0.4931746918145142 Lambda1 -1.5335483e-07\n",
      "2 Train Loss 838.06433 Test MSE 858.2738676772005 Test RE 0.4931762061520654 Lambda1 -1.5412228e-07\n",
      "3 Train Loss 838.06415 Test MSE 858.2779320333661 Test RE 0.4931773738687214 Lambda1 -1.5125782e-07\n",
      "4 Train Loss 838.0634 Test MSE 858.288359492266 Test RE 0.49318036973462026 Lambda1 -1.1993005e-07\n",
      "5 Train Loss 838.0617 Test MSE 858.3115592511402 Test RE 0.49318703508678086 Lambda1 1.04903734e-07\n",
      "6 Train Loss 838.0541 Test MSE 858.3579775228216 Test RE 0.4932003709087378 Lambda1 2.7488825e-06\n",
      "7 Train Loss 837.978 Test MSE 858.369191012492 Test RE 0.4932035924546121 Lambda1 0.00011305725\n",
      "8 Train Loss 837.86145 Test MSE 857.8662728112783 Test RE 0.4930590873811619 Lambda1 0.0043454175\n",
      "9 Train Loss 837.85925 Test MSE 857.8704009763643 Test RE 0.49306027371225203 Lambda1 0.005794847\n",
      "10 Train Loss 837.85364 Test MSE 857.8792356128976 Test RE 0.4930628125555829 Lambda1 0.014906991\n",
      "11 Train Loss 837.8081 Test MSE 857.9608927942178 Test RE 0.4930862780736735 Lambda1 0.09076859\n",
      "12 Train Loss 836.90125 Test MSE 855.447707309971 Test RE 0.49236356092297817 Lambda1 0.49587828\n",
      "13 Train Loss 821.9027 Test MSE 837.7970202163078 Test RE 0.487257549030133 Lambda1 0.4093928\n",
      "14 Train Loss 737.3951 Test MSE 765.3288286152035 Test RE 0.4657074731006811 Lambda1 -0.004964513\n",
      "15 Train Loss 641.54236 Test MSE 679.8767560960991 Test RE 0.4389390985795212 Lambda1 0.0011108769\n",
      "16 Train Loss 607.99603 Test MSE 632.5428435342059 Test RE 0.42338370972209194 Lambda1 0.0005845377\n",
      "17 Train Loss 542.9898 Test MSE 557.7374335376866 Test RE 0.397561268456727 Lambda1 0.0023656988\n",
      "18 Train Loss 506.4 Test MSE 487.1464845539848 Test RE 0.37155144705293613 Lambda1 -0.00047329016\n",
      "19 Train Loss 388.27 Test MSE 369.60970581114856 Test RE 0.32363900267749224 Lambda1 0.0015113851\n",
      "20 Train Loss 321.39392 Test MSE 343.93164331936947 Test RE 0.31219449302163016 Lambda1 0.0013694615\n",
      "21 Train Loss 292.49435 Test MSE 313.08361763260893 Test RE 0.2978649087596228 Lambda1 0.004025639\n",
      "22 Train Loss 276.68686 Test MSE 295.72903488860385 Test RE 0.28949172466594764 Lambda1 0.0073475917\n",
      "23 Train Loss 269.26202 Test MSE 285.0342779455319 Test RE 0.2842089277402926 Lambda1 0.010542377\n",
      "24 Train Loss 261.70883 Test MSE 277.84603506541475 Test RE 0.2806023297204753 Lambda1 0.015958667\n",
      "25 Train Loss 253.19336 Test MSE 268.8013996486942 Test RE 0.27599736355064736 Lambda1 0.02640277\n",
      "26 Train Loss 243.2184 Test MSE 261.4690176413017 Test RE 0.27220699939228554 Lambda1 0.03614286\n",
      "27 Train Loss 235.16093 Test MSE 247.02283236114252 Test RE 0.26458043073290793 Lambda1 0.0612456\n",
      "28 Train Loss 227.09529 Test MSE 243.31661162137817 Test RE 0.2625881059319921 Lambda1 0.071241766\n",
      "29 Train Loss 209.92145 Test MSE 225.18975597268866 Test RE 0.2526175307701761 Lambda1 0.12108245\n",
      "30 Train Loss 200.08679 Test MSE 214.75943337870254 Test RE 0.2466978106783302 Lambda1 0.13300242\n",
      "31 Train Loss 186.31749 Test MSE 195.78936472235378 Test RE 0.23555033121399566 Lambda1 0.16961585\n",
      "32 Train Loss 142.24345 Test MSE 142.43431749791887 Test RE 0.2009076575441305 Lambda1 0.29753783\n",
      "33 Train Loss 97.17854 Test MSE 98.93363008042427 Test RE 0.16744074316618165 Lambda1 0.39253953\n",
      "34 Train Loss 71.768105 Test MSE 64.04701427290898 Test RE 0.1347220290114119 Lambda1 0.49357024\n",
      "35 Train Loss 52.904312 Test MSE 50.45515167228361 Test RE 0.11957542376530006 Lambda1 0.5680051\n",
      "36 Train Loss 49.912506 Test MSE 48.74682838842401 Test RE 0.1175336850531177 Lambda1 0.5813861\n",
      "37 Train Loss 46.621693 Test MSE 45.4592764831467 Test RE 0.11350119402455992 Lambda1 0.6182627\n",
      "38 Train Loss 44.43257 Test MSE 44.08850765562607 Test RE 0.11177685088910967 Lambda1 0.64586926\n",
      "39 Train Loss 42.021202 Test MSE 41.27415333720049 Test RE 0.10815043168410722 Lambda1 0.71294165\n",
      "40 Train Loss 40.28206 Test MSE 39.87647877008445 Test RE 0.1063035018901045 Lambda1 0.75819695\n",
      "41 Train Loss 39.261883 Test MSE 39.336704457573894 Test RE 0.10558158006183146 Lambda1 0.7788696\n",
      "42 Train Loss 38.514656 Test MSE 38.49660485363144 Test RE 0.10444806177880586 Lambda1 0.8134199\n",
      "43 Train Loss 38.236794 Test MSE 38.223138239965515 Test RE 0.10407641907980299 Lambda1 0.8347935\n",
      "44 Train Loss 38.10562 Test MSE 38.195671118634074 Test RE 0.10403901773721078 Lambda1 0.84680337\n",
      "45 Train Loss 37.243916 Test MSE 38.50653981127213 Test RE 0.10446153855334343 Lambda1 0.87909114\n",
      "46 Train Loss 36.11913 Test MSE 38.561450682880086 Test RE 0.10453599383659627 Lambda1 0.9138801\n",
      "47 Train Loss 35.61933 Test MSE 37.81196684051432 Test RE 0.10351512355042784 Lambda1 0.9239668\n",
      "48 Train Loss 35.186436 Test MSE 37.582089313296535 Test RE 0.10319998419679395 Lambda1 0.9349602\n",
      "49 Train Loss 35.075504 Test MSE 37.65397003255894 Test RE 0.1032986288649879 Lambda1 0.9424282\n",
      "50 Train Loss 34.92462 Test MSE 37.44354505187783 Test RE 0.10300958809093469 Lambda1 0.9466522\n",
      "51 Train Loss 34.628544 Test MSE 37.14888613389692 Test RE 0.10260347477609674 Lambda1 0.94084334\n",
      "52 Train Loss 34.47898 Test MSE 37.05087344736044 Test RE 0.10246803216678958 Lambda1 0.94347334\n",
      "53 Train Loss 34.185062 Test MSE 36.67205062283585 Test RE 0.10194284939867268 Lambda1 0.9690163\n",
      "54 Train Loss 33.951805 Test MSE 36.35098127183611 Test RE 0.10149560579096106 Lambda1 1.000074\n",
      "55 Train Loss 33.71247 Test MSE 36.397526977840215 Test RE 0.10156056515596994 Lambda1 1.038653\n",
      "56 Train Loss 33.540356 Test MSE 36.371105754981734 Test RE 0.10152369669687707 Lambda1 1.0608082\n",
      "57 Train Loss 33.30619 Test MSE 35.98289977373388 Test RE 0.10098043802360986 Lambda1 1.0890925\n",
      "58 Train Loss 32.999035 Test MSE 35.64392014233345 Test RE 0.10050366556825417 Lambda1 1.1303881\n",
      "59 Train Loss 32.59631 Test MSE 35.51385062195987 Test RE 0.10032012218787816 Lambda1 1.1758579\n",
      "60 Train Loss 32.3926 Test MSE 35.406048729471166 Test RE 0.10016774616710078 Lambda1 1.1891761\n",
      "61 Train Loss 32.23161 Test MSE 35.23788896850854 Test RE 0.0999295914914745 Lambda1 1.1838561\n",
      "62 Train Loss 32.097755 Test MSE 35.068110902357155 Test RE 0.09968856772031423 Lambda1 1.1666912\n",
      "63 Train Loss 32.040615 Test MSE 35.066432855428616 Test RE 0.09968618258904008 Lambda1 1.1631705\n",
      "64 Train Loss 32.02639 Test MSE 35.08684382298495 Test RE 0.09971519032204992 Lambda1 1.1600437\n",
      "65 Train Loss 32.01282 Test MSE 35.081362187344425 Test RE 0.09970740074008316 Lambda1 1.1552392\n",
      "66 Train Loss 31.967974 Test MSE 34.987530385684224 Test RE 0.099573968216869 Lambda1 1.1359408\n",
      "67 Train Loss 31.925608 Test MSE 34.92448765595563 Test RE 0.0994842184544878 Lambda1 1.1222705\n",
      "68 Train Loss 31.918877 Test MSE 34.91821591237415 Test RE 0.09947528535962044 Lambda1 1.1208502\n",
      "69 Train Loss 31.913784 Test MSE 34.923130772083546 Test RE 0.0994822858585963 Lambda1 1.1196146\n",
      "70 Train Loss 31.902073 Test MSE 34.949911773397446 Test RE 0.09952042282676277 Lambda1 1.1192675\n",
      "71 Train Loss 31.870049 Test MSE 35.01464617330503 Test RE 0.09961254629895612 Lambda1 1.123934\n",
      "72 Train Loss 31.842575 Test MSE 34.97746083320358 Test RE 0.09955963829073851 Lambda1 1.1208214\n",
      "73 Train Loss 31.806967 Test MSE 34.797334611832255 Test RE 0.09930295228783101 Lambda1 1.1081115\n",
      "74 Train Loss 31.787012 Test MSE 34.71616033583711 Test RE 0.09918705904676099 Lambda1 1.1066222\n",
      "Training time: 212.10\n",
      "Training time: 212.10\n",
      "inv_HT_swish_tune0\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 838.0745 Test MSE 858.2997950944672 Test RE 0.49318365522467367 Lambda1 -2.549023e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 837.9445 Test MSE 858.0276006675512 Test RE 0.4931054468364429 Lambda1 -0.0007643767\n",
      "2 Train Loss 837.90753 Test MSE 857.9408740132276 Test RE 0.4930805254549508 Lambda1 -0.002274623\n",
      "3 Train Loss 837.9054 Test MSE 857.9320232666269 Test RE 0.49307798207336584 Lambda1 -0.0028362204\n",
      "4 Train Loss 837.9052 Test MSE 857.9322845947923 Test RE 0.49307805716973185 Lambda1 -0.0029486986\n",
      "5 Train Loss 837.90497 Test MSE 857.9329451228358 Test RE 0.4930782469818112 Lambda1 -0.00309027\n",
      "6 Train Loss 837.8974 Test MSE 857.9850322920566 Test RE 0.49309321473682494 Lambda1 -0.010242059\n",
      "7 Train Loss 837.8917 Test MSE 857.9237427467555 Test RE 0.493075602542255 Lambda1 -0.013947747\n",
      "8 Train Loss 837.87036 Test MSE 857.7733216714997 Test RE 0.49303237479954 Lambda1 -0.037696447\n",
      "9 Train Loss 837.47406 Test MSE 857.5485948359869 Test RE 0.4929677861369686 Lambda1 -0.14229056\n",
      "10 Train Loss 836.6419 Test MSE 859.5314214866603 Test RE 0.49353737793346303 Lambda1 -0.24571404\n",
      "11 Train Loss 835.37177 Test MSE 857.9827560132654 Test RE 0.4930925606354543 Lambda1 -0.2733528\n",
      "12 Train Loss 834.475 Test MSE 857.5183426177746 Test RE 0.4929590907110436 Lambda1 -0.28148586\n",
      "13 Train Loss 823.52405 Test MSE 855.348147419413 Test RE 0.4923349086230346 Lambda1 -0.21064603\n",
      "14 Train Loss 748.5349 Test MSE 780.3879960904553 Test RE 0.47026695317185324 Lambda1 -0.00691714\n",
      "15 Train Loss 688.468 Test MSE 724.7309529680134 Test RE 0.45318713820650297 Lambda1 0.0034173252\n",
      "16 Train Loss 639.03174 Test MSE 678.6906050160134 Test RE 0.43855603255507764 Lambda1 0.0001688737\n",
      "17 Train Loss 632.94305 Test MSE 676.2336746891922 Test RE 0.43776150368992867 Lambda1 4.3805885e-05\n",
      "18 Train Loss 627.8656 Test MSE 670.2289760251891 Test RE 0.435813591771554 Lambda1 6.2155224e-05\n",
      "19 Train Loss 615.84265 Test MSE 659.652560928044 Test RE 0.43236128312852046 Lambda1 0.0001377385\n",
      "20 Train Loss 614.85767 Test MSE 658.5419422156826 Test RE 0.43199715900719576 Lambda1 0.00034307188\n",
      "21 Train Loss 613.5874 Test MSE 657.9746850684464 Test RE 0.43181106131598257 Lambda1 0.00028847271\n",
      "22 Train Loss 613.3443 Test MSE 658.0559994468108 Test RE 0.4318377427097061 Lambda1 0.00017735771\n",
      "23 Train Loss 612.9045 Test MSE 657.08252684194 Test RE 0.4315182123070973 Lambda1 0.0001620939\n",
      "24 Train Loss 611.68317 Test MSE 652.8273953116707 Test RE 0.43011873126783406 Lambda1 0.00025748127\n",
      "25 Train Loss 606.36774 Test MSE 644.0327075242399 Test RE 0.4272116944566652 Lambda1 -0.00019516787\n",
      "26 Train Loss 581.7778 Test MSE 608.49888722135 Test RE 0.4152590105432928 Lambda1 -0.0004630057\n",
      "27 Train Loss 508.66742 Test MSE 486.50575983659786 Test RE 0.37130702311326613 Lambda1 -0.0005178315\n",
      "28 Train Loss 384.00116 Test MSE 376.2351136646648 Test RE 0.32652680084227637 Lambda1 -0.0027079012\n",
      "29 Train Loss 307.76593 Test MSE 312.4974653616736 Test RE 0.2975859481595462 Lambda1 -0.0026483554\n",
      "30 Train Loss 298.78912 Test MSE 304.7420155640293 Test RE 0.2938700578456899 Lambda1 -0.001927593\n",
      "31 Train Loss 295.08307 Test MSE 304.7708780118993 Test RE 0.29388397389265936 Lambda1 -0.0012480611\n",
      "32 Train Loss 287.97964 Test MSE 304.84983504088666 Test RE 0.29392203970608693 Lambda1 0.0011967161\n",
      "33 Train Loss 283.4736 Test MSE 300.9344857270845 Test RE 0.29202844103247017 Lambda1 0.0025808692\n",
      "34 Train Loss 280.53342 Test MSE 300.827448697476 Test RE 0.2919765017594191 Lambda1 0.0009150649\n",
      "35 Train Loss 279.98523 Test MSE 300.0911014420781 Test RE 0.2916189416007304 Lambda1 0.0011384866\n",
      "36 Train Loss 277.5206 Test MSE 296.2462317632369 Test RE 0.2897447583421247 Lambda1 0.0041943304\n",
      "37 Train Loss 273.72897 Test MSE 292.6322592891156 Test RE 0.2879720054117753 Lambda1 0.006880134\n",
      "38 Train Loss 272.82288 Test MSE 291.2384347068356 Test RE 0.28728537305821483 Lambda1 0.0076051685\n",
      "39 Train Loss 271.36456 Test MSE 290.29406936789115 Test RE 0.2868192212666548 Lambda1 0.0053915093\n",
      "40 Train Loss 270.21152 Test MSE 290.05786357429395 Test RE 0.2867025083264504 Lambda1 0.0030257043\n",
      "41 Train Loss 269.29434 Test MSE 289.0290372221037 Test RE 0.28619359414484086 Lambda1 0.0038536214\n",
      "42 Train Loss 268.6294 Test MSE 287.5703673408925 Test RE 0.28547050067229157 Lambda1 0.003891358\n",
      "43 Train Loss 267.97983 Test MSE 286.1221046999218 Test RE 0.2847507496820327 Lambda1 0.0032492613\n",
      "44 Train Loss 264.74442 Test MSE 282.6867530334987 Test RE 0.28303614431555596 Lambda1 0.00242066\n",
      "45 Train Loss 261.42834 Test MSE 280.19190550759834 Test RE 0.2817844108698883 Lambda1 0.0015167443\n",
      "46 Train Loss 258.72464 Test MSE 279.2646707739495 Test RE 0.28131777209635556 Lambda1 0.0023306736\n",
      "47 Train Loss 257.8325 Test MSE 278.8641442707706 Test RE 0.28111596416189344 Lambda1 0.0031843632\n",
      "48 Train Loss 257.76086 Test MSE 278.8665658470319 Test RE 0.2811171847244535 Lambda1 0.0031245851\n",
      "49 Train Loss 257.71872 Test MSE 278.832670037147 Test RE 0.2811000995206624 Lambda1 0.003154513\n",
      "50 Train Loss 257.709 Test MSE 278.82094132977784 Test RE 0.28109418741667674 Lambda1 0.003168123\n",
      "51 Train Loss 257.63852 Test MSE 278.77023566808913 Test RE 0.28106862672032573 Lambda1 0.0031592655\n",
      "52 Train Loss 257.58212 Test MSE 278.66757919719544 Test RE 0.28101687053116986 Lambda1 0.003194045\n",
      "53 Train Loss 257.55643 Test MSE 278.53927657455114 Test RE 0.2809521709478145 Lambda1 0.003281909\n",
      "54 Train Loss 257.54816 Test MSE 278.4749524696072 Test RE 0.28091972841290636 Lambda1 0.0033695342\n",
      "55 Train Loss 257.5467 Test MSE 278.46316343266375 Test RE 0.28091378208258455 Lambda1 0.0033910384\n",
      "56 Train Loss 257.54074 Test MSE 278.41451347146693 Test RE 0.2808892419577909 Lambda1 0.0034796356\n",
      "57 Train Loss 257.46835 Test MSE 278.07837603090115 Test RE 0.280719628123573 Lambda1 0.0040703374\n",
      "58 Train Loss 257.3808 Test MSE 277.87590753157616 Test RE 0.2806174137188943 Lambda1 0.004477743\n",
      "59 Train Loss 257.31696 Test MSE 277.869347417993 Test RE 0.28061410128171904 Lambda1 0.004622825\n",
      "60 Train Loss 257.02295 Test MSE 277.18673843255465 Test RE 0.2802692134902256 Lambda1 0.0054853256\n",
      "61 Train Loss 256.509 Test MSE 276.34180667434396 Test RE 0.27984172352727155 Lambda1 0.0075561875\n",
      "62 Train Loss 255.97305 Test MSE 275.16524751623103 Test RE 0.27924535783330284 Lambda1 0.011017956\n",
      "63 Train Loss 255.31839 Test MSE 273.3498882104884 Test RE 0.2783226949394393 Lambda1 0.015326481\n",
      "64 Train Loss 254.6199 Test MSE 271.8263317901843 Test RE 0.2775459745848332 Lambda1 0.018691905\n",
      "65 Train Loss 254.31163 Test MSE 272.24370208903684 Test RE 0.27775896906160064 Lambda1 0.01946283\n",
      "66 Train Loss 253.82704 Test MSE 272.0350364257644 Test RE 0.2776525021916903 Lambda1 0.02154325\n",
      "67 Train Loss 253.63376 Test MSE 271.2628613026537 Test RE 0.27725816197005965 Lambda1 0.023985755\n",
      "68 Train Loss 252.68297 Test MSE 265.64664805892727 Test RE 0.2743729803672911 Lambda1 0.048093345\n",
      "69 Train Loss 246.009 Test MSE 255.10218661430864 Test RE 0.26887242318259796 Lambda1 0.11361694\n",
      "70 Train Loss 235.29129 Test MSE 247.70252184876148 Test RE 0.2649441805169805 Lambda1 0.15273944\n",
      "71 Train Loss 228.0686 Test MSE 242.0412588615585 Test RE 0.26189901928279663 Lambda1 0.17727523\n",
      "72 Train Loss 226.89038 Test MSE 240.41649349803683 Test RE 0.26101850623451345 Lambda1 0.1869635\n",
      "73 Train Loss 224.7528 Test MSE 240.77710712857908 Test RE 0.26121419073139507 Lambda1 0.1968968\n",
      "74 Train Loss 219.70215 Test MSE 238.85631107793614 Test RE 0.2601701881315456 Lambda1 0.21926068\n",
      "Training time: 214.40\n",
      "Training time: 214.40\n",
      "inv_HT_swish_tune0\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 838.06415 Test MSE 858.3128971012865 Test RE 0.4931874194519235 Lambda1 4.7054243e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 838.06305 Test MSE 858.2922400667471 Test RE 0.4931814846402186 Lambda1 4.870258e-07\n",
      "2 Train Loss 838.0626 Test MSE 858.2816103141163 Test RE 0.49317843066094996 Lambda1 5.229504e-07\n",
      "3 Train Loss 838.0621 Test MSE 858.2721784038021 Test RE 0.4931757208117289 Lambda1 5.799021e-07\n",
      "4 Train Loss 838.0596 Test MSE 858.2317340201423 Test RE 0.4931641007082507 Lambda1 1.1300399e-06\n",
      "5 Train Loss 838.03326 Test MSE 858.0857986062794 Test RE 0.49312216962831557 Lambda1 9.412261e-06\n",
      "6 Train Loss 837.9055 Test MSE 857.8350001393419 Test RE 0.49305010030701074 Lambda1 0.00016266305\n",
      "7 Train Loss 837.90125 Test MSE 857.8680430122178 Test RE 0.4930595960928559 Lambda1 0.00020715181\n",
      "8 Train Loss 837.8999 Test MSE 857.8940797663055 Test RE 0.4930670783473544 Lambda1 0.00024185317\n",
      "9 Train Loss 837.8998 Test MSE 857.8969726469189 Test RE 0.4930679096755003 Lambda1 0.0002485418\n",
      "10 Train Loss 837.8997 Test MSE 857.900536664412 Test RE 0.4930689338665666 Lambda1 0.00025997357\n",
      "11 Train Loss 837.8996 Test MSE 857.9040261912219 Test RE 0.49306993664919735 Lambda1 0.00027555454\n",
      "12 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "13 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "14 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "15 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "16 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "17 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "18 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "19 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "20 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "21 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "22 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "23 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "24 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "25 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "26 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "27 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "28 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "29 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "30 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "31 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "32 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "33 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "34 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "35 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "36 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "37 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "38 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "39 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "40 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "41 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "42 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "43 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "44 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "45 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "46 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "47 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "48 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "49 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "50 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "51 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "52 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "53 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "54 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "55 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "56 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "57 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "58 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "59 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "60 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "61 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "62 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "63 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "64 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "65 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "66 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "67 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "68 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "69 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "70 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "71 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "72 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "73 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "74 Train Loss 837.8995 Test MSE 857.9063943273517 Test RE 0.49307061717749073 Lambda1 0.00028909516\n",
      "Training time: 194.91\n",
      "Training time: 194.91\n",
      "inv_HT_swish_tune0\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 838.1521 Test MSE 858.0914710582771 Test RE 0.49312379953946084 Lambda1 -4.5175497e-07\n",
      "1 Train Loss 838.0688 Test MSE 858.2528628242248 Test RE 0.49317017127353036 Lambda1 -4.6008063e-07\n",
      "2 Train Loss 838.06757 Test MSE 858.2716839311742 Test RE 0.4931755787461137 Lambda1 -4.6348922e-07\n",
      "3 Train Loss 838.06726 Test MSE 858.2753908055876 Test RE 0.49317664375734177 Lambda1 -4.6520887e-07\n",
      "4 Train Loss 838.0671 Test MSE 858.278727830435 Test RE 0.4931776025062074 Lambda1 -4.6748022e-07\n",
      "5 Train Loss 838.0669 Test MSE 858.282569114916 Test RE 0.4931787061298262 Lambda1 -4.711945e-07\n",
      "6 Train Loss 838.06604 Test MSE 858.2979919774797 Test RE 0.49318313718406653 Lambda1 -4.914684e-07\n",
      "7 Train Loss 838.06445 Test MSE 858.3206507371938 Test RE 0.4931896470702322 Lambda1 -5.4010286e-07\n",
      "8 Train Loss 838.04877 Test MSE 858.3941744412833 Test RE 0.4932107699181689 Lambda1 -8.9012474e-07\n",
      "9 Train Loss 837.86523 Test MSE 857.8850319429627 Test RE 0.4930644782620473 Lambda1 -1.9329424e-05\n",
      "10 Train Loss 837.8605 Test MSE 857.8586022376202 Test RE 0.4930568830431307 Lambda1 -4.487706e-05\n",
      "11 Train Loss 837.85974 Test MSE 857.8557224600153 Test RE 0.49305605546189657 Lambda1 -6.143955e-05\n",
      "12 Train Loss 837.85956 Test MSE 857.855265046075 Test RE 0.4930559240116205 Lambda1 -6.7061235e-05\n",
      "13 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "14 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "15 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "16 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "17 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "18 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "19 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "20 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "21 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "22 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "23 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "24 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "25 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "26 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "27 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "28 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "29 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "30 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "31 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "32 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "33 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "34 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "35 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "36 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "37 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "38 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "39 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "40 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "41 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "42 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "43 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "44 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "45 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "46 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "47 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "48 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "49 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "50 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "51 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "52 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "53 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "54 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "55 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "56 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "57 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "58 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "59 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "60 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "61 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "62 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "63 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "64 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "65 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "66 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "67 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "68 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "69 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "70 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "71 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "72 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "73 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "74 Train Loss 837.85944 Test MSE 857.8549873084172 Test RE 0.4930558441961702 Lambda1 -7.125661e-05\n",
      "Training time: 197.28\n",
      "Training time: 197.28\n",
      "inv_HT_swish_tune0\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 840.11536 Test MSE 861.7083019590441 Test RE 0.4941619580624265 Lambda1 -5.4715326e-07\n",
      "1 Train Loss 838.08075 Test MSE 858.362739078648 Test RE 0.4932017388682048 Lambda1 -8.55028e-07\n",
      "2 Train Loss 838.0756 Test MSE 858.2959379844358 Test RE 0.49318254706539644 Lambda1 -1.176968e-06\n",
      "3 Train Loss 838.07556 Test MSE 858.2934825688658 Test RE 0.49318184161584383 Lambda1 -1.2296565e-06\n",
      "4 Train Loss 838.0749 Test MSE 858.2828764044843 Test RE 0.4931787944158195 Lambda1 -1.626552e-06\n",
      "5 Train Loss 838.0727 Test MSE 858.2559616406924 Test RE 0.493171061595331 Lambda1 -3.4771883e-06\n",
      "6 Train Loss 838.05786 Test MSE 858.16194050611 Test RE 0.49314404764214986 Lambda1 -2.0488955e-05\n",
      "7 Train Loss 837.985 Test MSE 858.1600407300009 Test RE 0.49314350178725097 Lambda1 -0.00021494203\n",
      "8 Train Loss 837.9719 Test MSE 858.2439724377696 Test RE 0.49316761696526334 Lambda1 -0.0003795696\n",
      "9 Train Loss 837.8976 Test MSE 857.9762395313558 Test RE 0.4930906880833371 Lambda1 -0.0016811597\n",
      "10 Train Loss 837.89514 Test MSE 857.9335068664834 Test RE 0.49307840840673917 Lambda1 -0.001889067\n",
      "11 Train Loss 837.89496 Test MSE 857.930274831742 Test RE 0.4930774796354333 Lambda1 -0.0019208203\n",
      "12 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "13 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "14 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "15 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "16 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "17 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "18 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "19 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "20 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "21 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "22 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "23 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "24 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "25 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "26 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "27 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "28 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "29 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "30 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "31 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "32 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "33 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "34 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "35 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "36 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "37 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "38 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "39 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "40 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "41 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "42 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "43 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "44 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "45 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "46 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "47 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "48 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "49 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "50 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "51 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "52 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "53 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "54 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "55 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "56 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "57 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "58 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "59 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "60 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "61 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "62 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "63 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "64 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "65 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "66 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "67 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "68 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "69 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "70 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "71 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "72 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "73 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "74 Train Loss 837.89484 Test MSE 857.928226484967 Test RE 0.4930768910128506 Lambda1 -0.0019489136\n",
      "Training time: 203.05\n",
      "Training time: 203.05\n",
      "inv_HT_swish_tune0\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 838.0847 Test MSE 858.3066668071995 Test RE 0.49318562948217604 Lambda1 7.268415e-07\n",
      "1 Train Loss 837.99243 Test MSE 858.1728796371233 Test RE 0.4931471907261646 Lambda1 0.00052904576\n",
      "2 Train Loss 837.9082 Test MSE 858.1317026931414 Test RE 0.49313535946330384 Lambda1 0.0024083045\n",
      "3 Train Loss 837.8781 Test MSE 857.9231439666104 Test RE 0.49307543047338825 Lambda1 0.0070643686\n",
      "4 Train Loss 837.87726 Test MSE 857.9014368458651 Test RE 0.49306919255121245 Lambda1 0.0077801873\n",
      "5 Train Loss 837.87714 Test MSE 857.8980925152099 Test RE 0.49306823149207896 Lambda1 0.007946621\n",
      "6 Train Loss 837.8767 Test MSE 857.8857282586738 Test RE 0.4930646783637278 Lambda1 0.008726521\n",
      "7 Train Loss 837.87585 Test MSE 857.863599117192 Test RE 0.4930583190270284 Lambda1 0.011043361\n",
      "8 Train Loss 837.85065 Test MSE 857.7712608562507 Test RE 0.49303178253977803 Lambda1 0.08239526\n",
      "9 Train Loss 837.12 Test MSE 857.5914851557167 Test RE 0.4929801138824305 Lambda1 0.5645456\n",
      "10 Train Loss 832.7861 Test MSE 852.0610195103272 Test RE 0.49138796925266504 Lambda1 0.9198222\n",
      "11 Train Loss 813.95123 Test MSE 829.5466565856334 Test RE 0.48485243302235387 Lambda1 0.80933505\n",
      "12 Train Loss 800.91016 Test MSE 813.0873451593682 Test RE 0.48001827456449586 Lambda1 0.6727224\n",
      "13 Train Loss 761.5514 Test MSE 781.2624305000711 Test RE 0.4705303493789366 Lambda1 0.614476\n",
      "14 Train Loss 735.0373 Test MSE 756.8406104476474 Test RE 0.463117704892204 Lambda1 0.60333776\n",
      "15 Train Loss 707.07104 Test MSE 724.6411280179329 Test RE 0.4531590527678134 Lambda1 0.61862576\n",
      "16 Train Loss 693.5985 Test MSE 702.8781155593139 Test RE 0.446302358010318 Lambda1 0.6545166\n",
      "17 Train Loss 674.6195 Test MSE 677.2762185356038 Test RE 0.43809882036975933 Lambda1 0.71788925\n",
      "18 Train Loss 658.5833 Test MSE 669.3540938628288 Test RE 0.4355290546506077 Lambda1 0.7661548\n",
      "19 Train Loss 639.40674 Test MSE 642.098765087028 Test RE 0.426569782957027 Lambda1 0.82115537\n",
      "20 Train Loss 619.6699 Test MSE 612.044734373379 Test RE 0.4164671525618118 Lambda1 0.90814453\n",
      "21 Train Loss 605.65936 Test MSE 603.7596091806637 Test RE 0.41363873240413385 Lambda1 0.937797\n",
      "22 Train Loss 589.53265 Test MSE 584.0702751166976 Test RE 0.40683819875112676 Lambda1 1.0309479\n",
      "23 Train Loss 566.8292 Test MSE 555.0752072250086 Test RE 0.39661130165160047 Lambda1 1.155803\n",
      "24 Train Loss 495.69815 Test MSE 486.60224152683446 Test RE 0.3713438392788587 Lambda1 1.2167711\n",
      "25 Train Loss 416.7879 Test MSE 377.01446695413205 Test RE 0.3268648183182151 Lambda1 1.1273108\n",
      "26 Train Loss 337.94855 Test MSE 302.22493398981476 Test RE 0.2926539001981889 Lambda1 1.0862845\n",
      "27 Train Loss 262.5404 Test MSE 226.54867787283555 Test RE 0.2533786025886174 Lambda1 1.0981792\n",
      "28 Train Loss 213.06036 Test MSE 162.31013063156544 Test RE 0.214467744412241 Lambda1 1.1289299\n",
      "29 Train Loss 168.18152 Test MSE 112.99488708111171 Test RE 0.17894458748594008 Lambda1 1.1887857\n",
      "30 Train Loss 108.18999 Test MSE 95.91205896837681 Test RE 0.16486397914428336 Lambda1 1.2076945\n",
      "31 Train Loss 96.944534 Test MSE 83.82660486522995 Test RE 0.1541274911117221 Lambda1 1.2136526\n",
      "32 Train Loss 80.71061 Test MSE 69.95482389628904 Test RE 0.14079849234450584 Lambda1 1.2305776\n",
      "33 Train Loss 64.805214 Test MSE 52.84267188078646 Test RE 0.12237185825468436 Lambda1 1.2236118\n",
      "34 Train Loss 52.53436 Test MSE 41.533216237683746 Test RE 0.10848931128981465 Lambda1 1.2098864\n",
      "35 Train Loss 47.78086 Test MSE 39.84365036326359 Test RE 0.10625973557407917 Lambda1 1.2001396\n",
      "36 Train Loss 46.415417 Test MSE 40.270209722144685 Test RE 0.10682702064954135 Lambda1 1.2063055\n",
      "37 Train Loss 43.256096 Test MSE 39.84225650548768 Test RE 0.10625787690585077 Lambda1 1.2093576\n",
      "38 Train Loss 42.238262 Test MSE 38.35487742702104 Test RE 0.10425561877159141 Lambda1 1.2068225\n",
      "39 Train Loss 40.68706 Test MSE 37.91353163772399 Test RE 0.10365405366880152 Lambda1 1.20246\n",
      "40 Train Loss 38.694233 Test MSE 38.033083715127376 Test RE 0.1038173502989188 Lambda1 1.1896867\n",
      "41 Train Loss 37.064224 Test MSE 38.1108624713903 Test RE 0.1039234508240453 Lambda1 1.1799483\n",
      "42 Train Loss 36.183254 Test MSE 38.41173109541121 Test RE 0.10433285952788934 Lambda1 1.1739587\n",
      "43 Train Loss 35.724827 Test MSE 38.27888613128854 Test RE 0.10415228839823794 Lambda1 1.1675262\n",
      "44 Train Loss 35.57245 Test MSE 38.32227209909761 Test RE 0.10421129570756876 Lambda1 1.1642784\n",
      "45 Train Loss 35.270184 Test MSE 38.60204951234781 Test RE 0.10459100891553859 Lambda1 1.1565039\n",
      "46 Train Loss 34.97633 Test MSE 38.520399637155236 Test RE 0.10448033650930688 Lambda1 1.1481869\n",
      "47 Train Loss 34.857113 Test MSE 38.428870623958865 Test RE 0.10435613388366359 Lambda1 1.1464895\n",
      "48 Train Loss 34.69234 Test MSE 38.41761489007701 Test RE 0.10434084992039236 Lambda1 1.1453716\n",
      "49 Train Loss 34.661846 Test MSE 38.39755441818166 Test RE 0.1043136046058931 Lambda1 1.1440911\n",
      "50 Train Loss 34.647423 Test MSE 38.38551499448345 Test RE 0.10429724973431731 Lambda1 1.1429738\n",
      "51 Train Loss 34.617058 Test MSE 38.38094710100839 Test RE 0.1042910438399721 Lambda1 1.1433111\n",
      "52 Train Loss 34.612743 Test MSE 38.376323105832476 Test RE 0.10428476135027903 Lambda1 1.1440519\n",
      "53 Train Loss 34.61188 Test MSE 38.37419030537852 Test RE 0.10428186344789139 Lambda1 1.1442214\n",
      "54 Train Loss 34.60928 Test MSE 38.3619373894305 Test RE 0.10426521346894588 Lambda1 1.1444716\n",
      "55 Train Loss 34.588547 Test MSE 38.280284084618685 Test RE 0.1041541902131 Lambda1 1.1424979\n",
      "56 Train Loss 34.51028 Test MSE 38.118490114376264 Test RE 0.1039338501074822 Lambda1 1.1340092\n",
      "57 Train Loss 34.482082 Test MSE 37.97303704965657 Test RE 0.10373536446507924 Lambda1 1.127025\n",
      "58 Train Loss 34.38326 Test MSE 37.64053724314992 Test RE 0.10328020169114942 Lambda1 1.1065029\n",
      "59 Train Loss 34.19868 Test MSE 37.31250234825177 Test RE 0.1028291766733638 Lambda1 1.0842189\n",
      "60 Train Loss 34.104893 Test MSE 37.12548643226379 Test RE 0.10257115524865734 Lambda1 1.0723149\n",
      "61 Train Loss 34.08316 Test MSE 37.05666953609975 Test RE 0.10247604669555198 Lambda1 1.0683713\n",
      "62 Train Loss 34.070175 Test MSE 37.02052006726939 Test RE 0.10242605085695851 Lambda1 1.0667852\n",
      "63 Train Loss 34.01006 Test MSE 36.99037993449402 Test RE 0.10238434745344199 Lambda1 1.0666292\n",
      "64 Train Loss 33.839405 Test MSE 36.72603018048531 Test RE 0.10201784938277504 Lambda1 1.0555472\n",
      "65 Train Loss 33.571144 Test MSE 36.38918859679748 Test RE 0.10154893113480512 Lambda1 1.0355967\n",
      "66 Train Loss 33.47504 Test MSE 36.279563152283174 Test RE 0.10139585347524022 Lambda1 1.0233616\n",
      "67 Train Loss 33.357075 Test MSE 35.86674550225117 Test RE 0.10081732178968288 Lambda1 0.9994323\n",
      "68 Train Loss 33.248676 Test MSE 35.47743098506723 Test RE 0.10026866959203364 Lambda1 0.97884655\n",
      "69 Train Loss 33.205982 Test MSE 35.27579270919917 Test RE 0.09998332182858263 Lambda1 0.9679374\n",
      "70 Train Loss 33.174194 Test MSE 35.095948030706865 Test RE 0.09972812635197571 Lambda1 0.9582393\n",
      "71 Train Loss 33.158924 Test MSE 35.02507121896284 Test RE 0.09962737420915477 Lambda1 0.95343167\n",
      "72 Train Loss 33.151333 Test MSE 35.034541583875544 Test RE 0.09964084233051067 Lambda1 0.95308626\n",
      "73 Train Loss 33.14983 Test MSE 35.056759346506624 Test RE 0.09967243180785296 Lambda1 0.9543504\n",
      "74 Train Loss 33.147858 Test MSE 35.09141429001825 Test RE 0.09972168463872087 Lambda1 0.95676756\n",
      "Training time: 209.55\n",
      "Training time: 209.55\n",
      "inv_HT_swish_tune1\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.05786 Test MSE 858.2838386258848 Test RE 0.4931790708672496 Lambda1 -3.1246884e-07\n",
      "1 Train Loss 838.0565 Test MSE 858.2582319496157 Test RE 0.4931717138773841 Lambda1 -9.097996e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Train Loss 838.00073 Test MSE 857.969252916915 Test RE 0.4930886804277772 Lambda1 -4.3948796e-05\n",
      "3 Train Loss 837.8822 Test MSE 857.8792101173017 Test RE 0.49306280522883467 Lambda1 0.0002425587\n",
      "4 Train Loss 837.882 Test MSE 857.885334429058 Test RE 0.4930645651881097 Lambda1 0.0002687374\n",
      "5 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "6 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "7 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "8 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "9 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "10 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "11 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "12 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "13 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "14 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "15 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "16 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "17 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "18 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "19 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "20 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "21 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "22 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "23 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "24 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "25 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "26 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "27 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "28 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "29 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "30 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "31 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "32 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "33 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "34 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "35 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "36 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "37 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "38 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "39 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "40 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "41 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "42 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "43 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "44 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "45 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "46 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "47 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "48 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "49 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "50 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "51 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "52 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "53 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "54 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "55 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "56 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "57 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "58 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "59 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "60 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "61 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "62 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "63 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "64 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "65 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "66 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "67 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "68 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "69 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "70 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "71 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "72 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "73 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "74 Train Loss 837.88196 Test MSE 857.886857993789 Test RE 0.49306500301786255 Lambda1 0.0002804266\n",
      "Training time: 211.00\n",
      "Training time: 211.00\n",
      "inv_HT_swish_tune1\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 841.26996 Test MSE 859.7764736386576 Test RE 0.493607726599375 Lambda1 4.341108e-06\n",
      "1 Train Loss 838.46954 Test MSE 858.1126911293829 Test RE 0.4931298968249783 Lambda1 1.2692335e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Train Loss 838.108 Test MSE 858.3108171028342 Test RE 0.4931868218670019 Lambda1 2.8143451e-05\n",
      "3 Train Loss 838.10565 Test MSE 858.3387012216257 Test RE 0.4931948329324361 Lambda1 3.289069e-05\n",
      "4 Train Loss 838.0279 Test MSE 858.4295463002045 Test RE 0.49322093168450715 Lambda1 0.00035740988\n",
      "5 Train Loss 837.8782 Test MSE 857.9818279947564 Test RE 0.49309229396392534 Lambda1 0.0036692764\n",
      "6 Train Loss 837.8059 Test MSE 857.8230565644677 Test RE 0.49304666794455976 Lambda1 0.028241517\n",
      "7 Train Loss 837.09546 Test MSE 856.7452637767025 Test RE 0.49273683180718214 Lambda1 0.2239059\n",
      "8 Train Loss 831.155 Test MSE 853.0260853488002 Test RE 0.4916661697153168 Lambda1 0.6439067\n",
      "9 Train Loss 814.30414 Test MSE 830.7668186187129 Test RE 0.4852088814169274 Lambda1 1.0652587\n",
      "10 Train Loss 780.8196 Test MSE 787.7044313505291 Test RE 0.47246627639428207 Lambda1 1.3706281\n",
      "11 Train Loss 738.2745 Test MSE 730.0365833857161 Test RE 0.45484296569757465 Lambda1 1.6320859\n",
      "12 Train Loss 666.4547 Test MSE 681.505000290853 Test RE 0.4394643942552695 Lambda1 1.7047116\n",
      "13 Train Loss 620.9172 Test MSE 618.7899634383855 Test RE 0.418755767037411 Lambda1 1.6906735\n",
      "14 Train Loss 545.67456 Test MSE 526.6369243023112 Test RE 0.38631789277689726 Lambda1 1.5953271\n",
      "15 Train Loss 437.79712 Test MSE 396.86112748497675 Test RE 0.3353578306064379 Lambda1 1.4912932\n",
      "16 Train Loss 379.3342 Test MSE 370.1717860855815 Test RE 0.3238849945836135 Lambda1 1.4377136\n",
      "17 Train Loss 328.4171 Test MSE 338.502998878626 Test RE 0.3097208417532479 Lambda1 1.2919784\n",
      "18 Train Loss 297.83374 Test MSE 312.3071729536399 Test RE 0.29749532827234487 Lambda1 1.170044\n",
      "19 Train Loss 281.3625 Test MSE 292.251451637417 Test RE 0.2877845728250507 Lambda1 1.0076575\n",
      "20 Train Loss 265.56815 Test MSE 270.1049986925158 Test RE 0.2766658026628642 Lambda1 0.8456888\n",
      "21 Train Loss 233.05144 Test MSE 237.73087200976167 Test RE 0.2595565317099183 Lambda1 0.5870062\n",
      "22 Train Loss 212.60432 Test MSE 224.46203472023038 Test RE 0.252209022164694 Lambda1 0.47329086\n",
      "23 Train Loss 196.82124 Test MSE 216.55524070234964 Test RE 0.24772710063539072 Lambda1 0.40771377\n",
      "24 Train Loss 179.66725 Test MSE 189.12964522509438 Test RE 0.2315095839969845 Lambda1 0.376973\n",
      "25 Train Loss 167.50978 Test MSE 176.45020859871877 Test RE 0.22361465306991934 Lambda1 0.40624085\n",
      "26 Train Loss 158.36317 Test MSE 160.78930874438183 Test RE 0.21346061419259735 Lambda1 0.47191748\n",
      "27 Train Loss 148.26483 Test MSE 137.25853022302005 Test RE 0.1972235821998544 Lambda1 0.55217606\n",
      "28 Train Loss 123.51337 Test MSE 118.65709658565929 Test RE 0.18337326988623587 Lambda1 0.62331283\n",
      "29 Train Loss 107.691154 Test MSE 104.06369258831981 Test RE 0.1717270805634254 Lambda1 0.60234046\n",
      "30 Train Loss 97.39346 Test MSE 93.80652397573004 Test RE 0.16304432694341298 Lambda1 0.58961594\n",
      "31 Train Loss 91.15373 Test MSE 89.84495306544326 Test RE 0.159564403748025 Lambda1 0.56100273\n",
      "32 Train Loss 88.00094 Test MSE 88.23600979862458 Test RE 0.15812920982301087 Lambda1 0.5290584\n",
      "33 Train Loss 83.20577 Test MSE 82.87313393522196 Test RE 0.1532484363198333 Lambda1 0.53425455\n",
      "34 Train Loss 76.35765 Test MSE 74.63653141437159 Test RE 0.14543364693473826 Lambda1 0.5504995\n",
      "35 Train Loss 71.176674 Test MSE 68.98047514630873 Test RE 0.13981451528133668 Lambda1 0.558869\n",
      "36 Train Loss 66.65129 Test MSE 63.87024405419577 Test RE 0.13453598366796984 Lambda1 0.5799309\n",
      "37 Train Loss 57.04329 Test MSE 53.97182258134043 Test RE 0.12367237830800858 Lambda1 0.65409654\n",
      "38 Train Loss 51.491505 Test MSE 46.39745637737251 Test RE 0.11466642081251417 Lambda1 0.70423514\n",
      "39 Train Loss 48.08079 Test MSE 45.28666597522114 Test RE 0.11328550502700305 Lambda1 0.737299\n",
      "40 Train Loss 45.363117 Test MSE 43.53193278967994 Test RE 0.11106907259832045 Lambda1 0.79738504\n",
      "41 Train Loss 43.9887 Test MSE 40.984103768810606 Test RE 0.10776975408674237 Lambda1 0.81192434\n",
      "42 Train Loss 40.997902 Test MSE 39.89115221566482 Test RE 0.10632305847120287 Lambda1 0.8617633\n",
      "43 Train Loss 39.469334 Test MSE 39.23068502584184 Test RE 0.10543920347349364 Lambda1 0.87949693\n",
      "44 Train Loss 38.77057 Test MSE 39.236429736209146 Test RE 0.10544692313875197 Lambda1 0.871656\n",
      "45 Train Loss 37.538612 Test MSE 38.10296726365084 Test RE 0.10391268565528669 Lambda1 0.8710484\n",
      "46 Train Loss 36.807842 Test MSE 37.738598624386675 Test RE 0.10341464732209912 Lambda1 0.8872963\n",
      "47 Train Loss 36.340477 Test MSE 37.88143628668647 Test RE 0.10361017068739391 Lambda1 0.8803162\n",
      "48 Train Loss 36.07842 Test MSE 37.67496443832445 Test RE 0.10332742252362177 Lambda1 0.8780114\n",
      "49 Train Loss 35.263016 Test MSE 37.20794037449026 Test RE 0.1026849949078769 Lambda1 0.8765647\n",
      "50 Train Loss 34.741344 Test MSE 36.851651690817086 Test RE 0.10219217663756067 Lambda1 0.90633124\n",
      "51 Train Loss 34.34523 Test MSE 36.706312292579035 Test RE 0.1019904594555124 Lambda1 0.9320863\n",
      "52 Train Loss 34.147438 Test MSE 36.73869210046813 Test RE 0.10203543405665737 Lambda1 0.94979465\n",
      "53 Train Loss 33.840366 Test MSE 36.36958727883285 Test RE 0.10152157739203534 Lambda1 0.96392804\n",
      "54 Train Loss 33.67076 Test MSE 36.093491560607 Test RE 0.10113549833739562 Lambda1 0.9664965\n",
      "55 Train Loss 33.096478 Test MSE 35.716098297820345 Test RE 0.10060537296325524 Lambda1 0.9579626\n",
      "56 Train Loss 32.5885 Test MSE 35.52323922877482 Test RE 0.10033338185876081 Lambda1 0.9713631\n",
      "57 Train Loss 32.287853 Test MSE 35.29755640643597 Test RE 0.10001415984886497 Lambda1 0.9802172\n",
      "58 Train Loss 32.026917 Test MSE 34.91424853393379 Test RE 0.09946963404979643 Lambda1 0.978439\n",
      "59 Train Loss 31.769087 Test MSE 34.62932933825872 Test RE 0.09906293956921372 Lambda1 0.98718876\n",
      "60 Train Loss 31.509272 Test MSE 34.43676017640051 Test RE 0.09878711757037277 Lambda1 1.0052958\n",
      "61 Train Loss 31.28019 Test MSE 34.498597697020834 Test RE 0.09887577298543525 Lambda1 1.0230833\n",
      "62 Train Loss 31.045456 Test MSE 34.319529771714166 Test RE 0.09861882754432846 Lambda1 1.011789\n",
      "63 Train Loss 30.842367 Test MSE 34.03320192959489 Test RE 0.09820657744609523 Lambda1 1.025004\n",
      "64 Train Loss 30.756924 Test MSE 33.82258027715803 Test RE 0.09790221975490469 Lambda1 1.0268975\n",
      "65 Train Loss 30.694595 Test MSE 33.79565692394489 Test RE 0.09786324607553537 Lambda1 1.0215318\n",
      "66 Train Loss 30.613766 Test MSE 33.897337592784645 Test RE 0.09801035561178842 Lambda1 1.0280715\n",
      "67 Train Loss 30.38959 Test MSE 33.49918933609064 Test RE 0.09743305485431551 Lambda1 1.045507\n",
      "68 Train Loss 30.296803 Test MSE 33.218237517267745 Test RE 0.09702361761584001 Lambda1 1.0550337\n",
      "69 Train Loss 30.226671 Test MSE 33.065701932871065 Test RE 0.0968005988158775 Lambda1 1.0489962\n",
      "70 Train Loss 30.13634 Test MSE 33.21186228897324 Test RE 0.09701430680588763 Lambda1 1.0380294\n",
      "71 Train Loss 30.102896 Test MSE 33.148979324593256 Test RE 0.09692242039554619 Lambda1 1.0346296\n",
      "72 Train Loss 30.066057 Test MSE 33.16674135360339 Test RE 0.09694838361265035 Lambda1 1.0286758\n",
      "73 Train Loss 29.920036 Test MSE 32.852959909233164 Test RE 0.0964886927413264 Lambda1 1.0187521\n",
      "74 Train Loss 29.695568 Test MSE 32.62895044926224 Test RE 0.09615917371422236 Lambda1 0.98938996\n",
      "Training time: 215.15\n",
      "Training time: 215.15\n",
      "inv_HT_swish_tune1\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 838.06116 Test MSE 858.2430836654914 Test RE 0.4931673616102397 Lambda1 -8.413539e-06\n",
      "1 Train Loss 837.81116 Test MSE 857.8767833297413 Test RE 0.4930621078348925 Lambda1 0.0012068166\n",
      "2 Train Loss 837.7728 Test MSE 857.5848812849339 Test RE 0.49297821578567097 Lambda1 0.0076275035\n",
      "3 Train Loss 837.5435 Test MSE 858.0233712357094 Test RE 0.49310423151513716 Lambda1 0.03966767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 836.91614 Test MSE 859.2056339697341 Test RE 0.4934438365272953 Lambda1 0.10691612\n",
      "5 Train Loss 834.42084 Test MSE 857.2240202478798 Test RE 0.4928744853257562 Lambda1 0.16552831\n",
      "6 Train Loss 826.8805 Test MSE 848.9868579014942 Test RE 0.4905007258554184 Lambda1 0.36146098\n",
      "7 Train Loss 796.6041 Test MSE 811.4758235944659 Test RE 0.47954234569865256 Lambda1 0.12443387\n",
      "8 Train Loss 714.06287 Test MSE 751.3530372615584 Test RE 0.4614357022906018 Lambda1 0.006013658\n",
      "9 Train Loss 567.32794 Test MSE 579.4280640596827 Test RE 0.40521819121510194 Lambda1 0.001067966\n",
      "10 Train Loss 506.53058 Test MSE 517.4164054171067 Test RE 0.3829210736105552 Lambda1 0.001660646\n",
      "11 Train Loss 369.41174 Test MSE 370.7691993142252 Test RE 0.32414624514589707 Lambda1 0.002518296\n",
      "12 Train Loss 320.03287 Test MSE 332.3661955050528 Test RE 0.30690049829769184 Lambda1 0.0012524\n",
      "13 Train Loss 289.8736 Test MSE 304.66654208678756 Test RE 0.2938336651472486 Lambda1 -0.008714335\n",
      "14 Train Loss 281.90536 Test MSE 294.0214794220385 Test RE 0.2886547442745848 Lambda1 -0.014073978\n",
      "15 Train Loss 270.73514 Test MSE 287.5641502564607 Test RE 0.2854674148122733 Lambda1 -0.008807492\n",
      "16 Train Loss 267.13266 Test MSE 286.1436988267035 Test RE 0.28476149479342977 Lambda1 -0.0023695554\n",
      "17 Train Loss 262.9233 Test MSE 283.82979274905915 Test RE 0.2836077932720206 Lambda1 0.0025453472\n",
      "18 Train Loss 261.54626 Test MSE 281.094085364798 Test RE 0.2822377000347784 Lambda1 0.0023100965\n",
      "19 Train Loss 260.17465 Test MSE 280.5348717427765 Test RE 0.28195681589144644 Lambda1 0.0024722458\n",
      "20 Train Loss 259.8048 Test MSE 280.4020670037219 Test RE 0.28189006904853225 Lambda1 0.0019467612\n",
      "21 Train Loss 259.33978 Test MSE 280.3972299408938 Test RE 0.2818876376722774 Lambda1 0.0016232254\n",
      "22 Train Loss 259.1124 Test MSE 280.10356012938684 Test RE 0.2817399836175499 Lambda1 0.0017257377\n",
      "23 Train Loss 259.06763 Test MSE 280.12294732005336 Test RE 0.2817497336761831 Lambda1 0.0016567691\n",
      "24 Train Loss 258.7033 Test MSE 279.9454584471779 Test RE 0.2816604597217218 Lambda1 0.0018882791\n",
      "25 Train Loss 257.8092 Test MSE 279.43350194210154 Test RE 0.28140279543952074 Lambda1 0.002503395\n",
      "26 Train Loss 257.40277 Test MSE 279.4928518101763 Test RE 0.28143267791953047 Lambda1 0.0023063892\n",
      "27 Train Loss 257.1579 Test MSE 279.29717103691854 Test RE 0.2813341412195121 Lambda1 0.0022899227\n",
      "28 Train Loss 257.12363 Test MSE 279.37745423307456 Test RE 0.2813745726730326 Lambda1 0.0021526255\n",
      "29 Train Loss 257.08316 Test MSE 279.30596415326454 Test RE 0.281338569807808 Lambda1 0.0023517574\n",
      "30 Train Loss 257.037 Test MSE 279.57828122780836 Test RE 0.28147568580451326 Lambda1 0.0021654754\n",
      "31 Train Loss 256.92197 Test MSE 280.16421223752843 Test RE 0.2817704851916891 Lambda1 0.0017955716\n",
      "32 Train Loss 256.87866 Test MSE 280.42602364165964 Test RE 0.2819021106753277 Lambda1 0.0016290729\n",
      "33 Train Loss 256.85992 Test MSE 280.6066758549783 Test RE 0.28199289761525026 Lambda1 0.0015167618\n",
      "34 Train Loss 256.84167 Test MSE 280.7370595748865 Test RE 0.2820584039202675 Lambda1 0.0014200028\n",
      "35 Train Loss 256.80563 Test MSE 280.37721205150933 Test RE 0.2818775753470077 Lambda1 0.0015744678\n",
      "36 Train Loss 256.78165 Test MSE 279.94563883085993 Test RE 0.28166055046608124 Lambda1 0.0018022653\n",
      "37 Train Loss 256.76526 Test MSE 280.0008951095413 Test RE 0.2816883464803732 Lambda1 0.0018371573\n",
      "38 Train Loss 256.66663 Test MSE 279.52691933165136 Test RE 0.2814498293808142 Lambda1 0.0026919472\n",
      "39 Train Loss 256.53232 Test MSE 279.0203536243856 Test RE 0.28119468850863333 Lambda1 0.0040755114\n",
      "40 Train Loss 256.47638 Test MSE 278.6556841722746 Test RE 0.2810108728145362 Lambda1 0.0051051965\n",
      "41 Train Loss 256.43588 Test MSE 278.461473172416 Test RE 0.2809129295135884 Lambda1 0.0060954257\n",
      "42 Train Loss 256.14816 Test MSE 278.5036886921368 Test RE 0.28093422229020637 Lambda1 0.008511608\n",
      "43 Train Loss 254.89302 Test MSE 277.75332201696244 Test RE 0.2805555094230533 Lambda1 0.009529047\n",
      "44 Train Loss 253.9581 Test MSE 277.8610399468905 Test RE 0.280609906484806 Lambda1 0.010608099\n",
      "45 Train Loss 253.15688 Test MSE 277.3474861971664 Test RE 0.28035046939717595 Lambda1 0.01113299\n",
      "46 Train Loss 252.96986 Test MSE 276.8352515004875 Test RE 0.28009145928811463 Lambda1 0.0119867325\n",
      "47 Train Loss 252.60123 Test MSE 275.01420663509145 Test RE 0.2791687070665794 Lambda1 0.015569233\n",
      "48 Train Loss 252.13509 Test MSE 273.2042930975802 Test RE 0.278248563170463 Lambda1 0.022579275\n",
      "49 Train Loss 250.38643 Test MSE 270.21924029369217 Test RE 0.2767243047326299 Lambda1 0.033221465\n",
      "50 Train Loss 247.8479 Test MSE 266.0471812385573 Test RE 0.2745797476840355 Lambda1 0.04875406\n",
      "51 Train Loss 245.70406 Test MSE 261.368786916977 Test RE 0.27215482089680576 Lambda1 0.06419523\n",
      "52 Train Loss 242.78079 Test MSE 257.5703370793505 Test RE 0.2701699819657788 Lambda1 0.0860281\n",
      "53 Train Loss 238.33057 Test MSE 253.79095697607588 Test RE 0.26818052850370044 Lambda1 0.11797568\n",
      "54 Train Loss 232.77657 Test MSE 250.89833285774546 Test RE 0.266647832888868 Lambda1 0.16956776\n",
      "55 Train Loss 229.33792 Test MSE 252.19190319285528 Test RE 0.2673343346171381 Lambda1 0.18406427\n",
      "56 Train Loss 227.99779 Test MSE 250.08114967749813 Test RE 0.2662132388485309 Lambda1 0.19045113\n",
      "57 Train Loss 226.90865 Test MSE 250.00645468725324 Test RE 0.266173479193837 Lambda1 0.2093735\n",
      "58 Train Loss 225.29463 Test MSE 246.83619542559885 Test RE 0.26448046059827035 Lambda1 0.21814066\n",
      "59 Train Loss 223.29182 Test MSE 245.2434110860699 Test RE 0.26362575994637355 Lambda1 0.23262815\n",
      "60 Train Loss 222.1575 Test MSE 245.4073316365592 Test RE 0.26371384887996613 Lambda1 0.24331775\n",
      "61 Train Loss 221.25629 Test MSE 245.2022483858364 Test RE 0.2636036349819083 Lambda1 0.2530106\n",
      "62 Train Loss 220.55052 Test MSE 243.71124893826732 Test RE 0.2628009666169115 Lambda1 0.25392008\n",
      "63 Train Loss 219.92685 Test MSE 242.87605957208578 Test RE 0.26235027560164215 Lambda1 0.2518134\n",
      "64 Train Loss 219.43721 Test MSE 243.02881551972646 Test RE 0.26243276472319715 Lambda1 0.24478516\n",
      "65 Train Loss 217.91435 Test MSE 242.2758885764773 Test RE 0.2620259282400152 Lambda1 0.2532616\n",
      "66 Train Loss 217.78734 Test MSE 242.49708300778988 Test RE 0.262145513919178 Lambda1 0.25143206\n",
      "67 Train Loss 217.23058 Test MSE 241.09697628707463 Test RE 0.2613876429183101 Lambda1 0.2520109\n",
      "68 Train Loss 216.77979 Test MSE 241.50197934028753 Test RE 0.2616070948052408 Lambda1 0.25576684\n",
      "69 Train Loss 216.53127 Test MSE 242.23154066325156 Test RE 0.26200194559153267 Lambda1 0.25570372\n",
      "70 Train Loss 216.04762 Test MSE 243.11722091154923 Test RE 0.2624804923184749 Lambda1 0.25088793\n",
      "71 Train Loss 215.55284 Test MSE 243.34766704178998 Test RE 0.26260486295441776 Lambda1 0.2524659\n",
      "72 Train Loss 215.04297 Test MSE 242.1436767509634 Test RE 0.261954423696112 Lambda1 0.26495153\n",
      "73 Train Loss 213.38106 Test MSE 239.6775386794131 Test RE 0.26061705847993033 Lambda1 0.26039308\n",
      "74 Train Loss 212.0737 Test MSE 236.3121633452532 Test RE 0.258780893857881 Lambda1 0.25483912\n",
      "Training time: 224.98\n",
      "Training time: 224.98\n",
      "inv_HT_swish_tune1\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 845.0877 Test MSE 862.747710750421 Test RE 0.4944599019758777 Lambda1 2.0600376e-07\n",
      "1 Train Loss 838.6505 Test MSE 858.1157485120857 Test RE 0.49313077531418453 Lambda1 -3.6821814e-07\n",
      "2 Train Loss 838.1043 Test MSE 858.073290830512 Test RE 0.49311857564971223 Lambda1 -1.6970082e-06\n",
      "3 Train Loss 838.0256 Test MSE 858.285083347689 Test RE 0.49317942848234897 Lambda1 -6.565752e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 837.91516 Test MSE 858.1216005098199 Test RE 0.49313245678621487 Lambda1 0.00038915843\n",
      "5 Train Loss 837.87683 Test MSE 857.8384872087578 Test RE 0.49305110242174016 Lambda1 0.0030786926\n",
      "6 Train Loss 837.799 Test MSE 857.7599542831651 Test RE 0.49302853311973616 Lambda1 0.030498868\n",
      "7 Train Loss 834.4627 Test MSE 851.0596129245827 Test RE 0.4910991261977185 Lambda1 0.14090544\n",
      "8 Train Loss 830.8187 Test MSE 844.2309074105927 Test RE 0.48912492518145356 Lambda1 0.17755991\n",
      "9 Train Loss 781.9878 Test MSE 809.9411191471586 Test RE 0.4790886636246378 Lambda1 0.012227422\n",
      "10 Train Loss 629.3156 Test MSE 602.9903036151247 Test RE 0.4133751208562949 Lambda1 -0.0062925206\n",
      "11 Train Loss 376.05112 Test MSE 382.9501023250179 Test RE 0.3294278148944172 Lambda1 -0.0024444268\n",
      "12 Train Loss 329.38455 Test MSE 344.91000903460116 Test RE 0.3126382199727783 Lambda1 -0.001595221\n",
      "13 Train Loss 286.10504 Test MSE 304.64083311690746 Test RE 0.29382126746129905 Lambda1 0.0016385992\n",
      "14 Train Loss 267.44614 Test MSE 289.2371950912212 Test RE 0.28629663349166473 Lambda1 0.0034314643\n",
      "15 Train Loss 259.93225 Test MSE 278.81894284122853 Test RE 0.2810931800236904 Lambda1 0.005660975\n",
      "16 Train Loss 258.30856 Test MSE 280.20928691630684 Test RE 0.28179315083339457 Lambda1 0.0036449907\n",
      "17 Train Loss 257.39865 Test MSE 279.1144207745762 Test RE 0.2812420846100878 Lambda1 0.00483225\n",
      "18 Train Loss 257.05734 Test MSE 278.2227002269792 Test RE 0.2807924661841317 Lambda1 0.00634613\n",
      "19 Train Loss 256.59537 Test MSE 278.33984756116445 Test RE 0.28085157463756016 Lambda1 0.006506073\n",
      "20 Train Loss 256.33377 Test MSE 277.6637428875748 Test RE 0.2805102643387325 Lambda1 0.0073377662\n",
      "21 Train Loss 255.68887 Test MSE 276.65942261735927 Test RE 0.2800024966460077 Lambda1 0.008638932\n",
      "22 Train Loss 254.28261 Test MSE 275.7413893983219 Test RE 0.27953754738153624 Lambda1 0.010304286\n",
      "23 Train Loss 252.9673 Test MSE 275.01388886931807 Test RE 0.27916854578348277 Lambda1 0.014913565\n",
      "24 Train Loss 251.93625 Test MSE 272.3459060258412 Test RE 0.2778111013902612 Lambda1 0.022031298\n",
      "25 Train Loss 250.58548 Test MSE 271.14327044123735 Test RE 0.27719703823644504 Lambda1 0.025675327\n",
      "26 Train Loss 248.4557 Test MSE 266.86228599278826 Test RE 0.27500004925011545 Lambda1 0.034264665\n",
      "27 Train Loss 247.5037 Test MSE 265.97261893358905 Test RE 0.27454126816801344 Lambda1 0.037840497\n",
      "28 Train Loss 246.52338 Test MSE 263.92546511067303 Test RE 0.27348267451652025 Lambda1 0.043225005\n",
      "29 Train Loss 243.65002 Test MSE 256.99983101208534 Test RE 0.2698706092585679 Lambda1 0.067098044\n",
      "30 Train Loss 238.23438 Test MSE 248.81515949171438 Test RE 0.26553855592863607 Lambda1 0.10149845\n",
      "31 Train Loss 232.67274 Test MSE 245.9894498675802 Test RE 0.2640264347165718 Lambda1 0.11571783\n",
      "32 Train Loss 226.44049 Test MSE 243.8285008344211 Test RE 0.26286417708924886 Lambda1 0.14130752\n",
      "33 Train Loss 222.74203 Test MSE 243.34072635530637 Test RE 0.2626011179605957 Lambda1 0.15676919\n",
      "34 Train Loss 220.41655 Test MSE 242.66701685214107 Test RE 0.2622373492369672 Lambda1 0.16421153\n",
      "35 Train Loss 218.44565 Test MSE 240.49125712879183 Test RE 0.2610590882553739 Lambda1 0.17786719\n",
      "36 Train Loss 214.32103 Test MSE 238.49137645234293 Test RE 0.2599713627315135 Lambda1 0.21914846\n",
      "37 Train Loss 213.30591 Test MSE 237.35046016103132 Test RE 0.2593487801658343 Lambda1 0.23874019\n",
      "38 Train Loss 212.2803 Test MSE 236.87232503083567 Test RE 0.2590874234444271 Lambda1 0.26735005\n",
      "39 Train Loss 210.83255 Test MSE 235.15966698038 Test RE 0.2581490843070352 Lambda1 0.27888742\n",
      "40 Train Loss 208.91681 Test MSE 231.74523017931418 Test RE 0.2562681139986994 Lambda1 0.29546148\n",
      "41 Train Loss 204.81802 Test MSE 228.50773479498193 Test RE 0.25447177739564775 Lambda1 0.31472227\n",
      "42 Train Loss 202.77814 Test MSE 224.42272436248456 Test RE 0.25218693633396466 Lambda1 0.32143706\n",
      "43 Train Loss 200.91022 Test MSE 220.64047906486596 Test RE 0.2500528256484515 Lambda1 0.3409226\n",
      "44 Train Loss 198.05954 Test MSE 214.49759328143 Test RE 0.24654737472583801 Lambda1 0.36413896\n",
      "45 Train Loss 192.60854 Test MSE 203.83043992239013 Test RE 0.2403386910378568 Lambda1 0.37494412\n",
      "46 Train Loss 187.95343 Test MSE 204.47383457841892 Test RE 0.24071770900129183 Lambda1 0.38896814\n",
      "47 Train Loss 183.01416 Test MSE 192.6845590297438 Test RE 0.23367520230039365 Lambda1 0.4067249\n",
      "48 Train Loss 172.61679 Test MSE 173.88678742000917 Test RE 0.22198440367943062 Lambda1 0.43256336\n",
      "49 Train Loss 163.44182 Test MSE 167.86997187525748 Test RE 0.21811005108177467 Lambda1 0.3924939\n",
      "50 Train Loss 158.4493 Test MSE 166.61200286088257 Test RE 0.21729128748469057 Lambda1 0.38018295\n",
      "51 Train Loss 146.00673 Test MSE 145.3301019872771 Test RE 0.20293967465447743 Lambda1 0.3707201\n",
      "52 Train Loss 138.34885 Test MSE 138.2073835495673 Test RE 0.19790410074993076 Lambda1 0.37698606\n",
      "53 Train Loss 129.92215 Test MSE 126.58085673740145 Test RE 0.189397039430616 Lambda1 0.39231485\n",
      "54 Train Loss 115.81037 Test MSE 102.60737007670745 Test RE 0.1705212269337423 Lambda1 0.46410015\n",
      "55 Train Loss 99.16603 Test MSE 87.03233404797427 Test RE 0.15704694260690208 Lambda1 0.5614236\n",
      "56 Train Loss 90.03473 Test MSE 78.18326239535767 Test RE 0.14884904921097414 Lambda1 0.62931526\n",
      "57 Train Loss 83.33065 Test MSE 74.44166981161382 Test RE 0.14524367324233078 Lambda1 0.65751266\n",
      "58 Train Loss 75.46844 Test MSE 69.36577570368554 Test RE 0.14020444876878907 Lambda1 0.65986574\n",
      "59 Train Loss 67.730705 Test MSE 62.26243294613208 Test RE 0.132831847819713 Lambda1 0.67275506\n",
      "60 Train Loss 64.70371 Test MSE 56.61420192399116 Test RE 0.1266636111229362 Lambda1 0.6931165\n",
      "61 Train Loss 59.495903 Test MSE 51.989578483264566 Test RE 0.12138005188224602 Lambda1 0.72697484\n",
      "62 Train Loss 57.17546 Test MSE 53.18865980882044 Test RE 0.12277182012339295 Lambda1 0.7231887\n",
      "63 Train Loss 52.71897 Test MSE 47.927265337266284 Test RE 0.11654147096223055 Lambda1 0.75267696\n",
      "64 Train Loss 50.839355 Test MSE 46.75437395472755 Test RE 0.11510661794573472 Lambda1 0.76358575\n",
      "65 Train Loss 48.940125 Test MSE 43.86135103266082 Test RE 0.11148852584606338 Lambda1 0.794221\n",
      "66 Train Loss 47.823517 Test MSE 44.33013657431737 Test RE 0.11208273122818471 Lambda1 0.7938764\n",
      "67 Train Loss 46.809578 Test MSE 42.98059789434776 Test RE 0.11036348270672439 Lambda1 0.81933916\n",
      "68 Train Loss 43.697777 Test MSE 40.12176880685021 Test RE 0.10662995014755518 Lambda1 0.8724299\n",
      "69 Train Loss 41.29428 Test MSE 39.38228399827632 Test RE 0.10564273117725638 Lambda1 0.88319033\n",
      "70 Train Loss 39.78329 Test MSE 38.03325379058064 Test RE 0.10381758242265747 Lambda1 0.9326529\n",
      "71 Train Loss 38.97359 Test MSE 37.46669555225117 Test RE 0.10304142742416227 Lambda1 0.98158175\n",
      "72 Train Loss 38.57975 Test MSE 38.02511881816955 Test RE 0.10380647900328052 Lambda1 1.0123868\n",
      "73 Train Loss 37.34177 Test MSE 37.018152338733 Test RE 0.10242277536324793 Lambda1 1.065725\n",
      "74 Train Loss 36.190704 Test MSE 37.06924309994166 Test RE 0.10249343060851447 Lambda1 1.0920795\n",
      "Training time: 243.38\n",
      "Training time: 243.38\n",
      "inv_HT_swish_tune1\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 838.0615 Test MSE 858.2793003139827 Test RE 0.49317776698425364 Lambda1 -1.0108273e-07\n",
      "1 Train Loss 837.8673 Test MSE 857.9570900295794 Test RE 0.4930851853122462 Lambda1 0.0015274483\n",
      "2 Train Loss 837.8635 Test MSE 857.8805892862152 Test RE 0.4930632015647128 Lambda1 0.0029273771\n",
      "3 Train Loss 837.8406 Test MSE 857.7009153912527 Test RE 0.4930115654593693 Lambda1 0.06279971\n",
      "4 Train Loss 837.5027 Test MSE 857.891619640881 Test RE 0.4930663713791203 Lambda1 0.24173093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 834.94714 Test MSE 852.9003431047503 Test RE 0.4916299307965834 Lambda1 0.5029313\n",
      "6 Train Loss 806.0721 Test MSE 828.4506951425271 Test RE 0.4845320440526836 Lambda1 0.5521174\n",
      "7 Train Loss 785.87305 Test MSE 806.0093456397956 Test RE 0.4779244063390446 Lambda1 0.44158825\n",
      "8 Train Loss 742.82965 Test MSE 759.2263147148352 Test RE 0.4638470477392195 Lambda1 0.3476962\n",
      "9 Train Loss 721.4686 Test MSE 733.2847506772882 Test RE 0.45585371389829976 Lambda1 0.24493721\n",
      "10 Train Loss 691.6614 Test MSE 717.7949943698392 Test RE 0.45101333561035434 Lambda1 0.22502911\n",
      "11 Train Loss 625.25037 Test MSE 616.4321353791585 Test RE 0.4179571952263589 Lambda1 0.117967434\n",
      "12 Train Loss 542.83856 Test MSE 479.217699511503 Test RE 0.3685153611279933 Lambda1 0.1150813\n",
      "13 Train Loss 473.40387 Test MSE 418.1322431897425 Test RE 0.34422784642071014 Lambda1 0.13922176\n",
      "14 Train Loss 318.71445 Test MSE 301.35834072647793 Test RE 0.29223402425266887 Lambda1 0.13037336\n",
      "15 Train Loss 276.8081 Test MSE 267.3719553884206 Test RE 0.2752625296736161 Lambda1 0.092654906\n",
      "16 Train Loss 266.42438 Test MSE 269.7894048827364 Test RE 0.27650412565816546 Lambda1 0.09013648\n",
      "17 Train Loss 253.35667 Test MSE 261.18021253071174 Test RE 0.2720566249928266 Lambda1 0.08216004\n",
      "18 Train Loss 243.36253 Test MSE 252.88593548282836 Test RE 0.26770193401825104 Lambda1 0.08713033\n",
      "19 Train Loss 234.24017 Test MSE 245.66295480644865 Test RE 0.26385115900898204 Lambda1 0.09378976\n",
      "20 Train Loss 226.83002 Test MSE 243.18178791595534 Test RE 0.26251534475294364 Lambda1 0.09674926\n",
      "21 Train Loss 211.63176 Test MSE 224.7206619209235 Test RE 0.25235427910131447 Lambda1 0.14306502\n",
      "22 Train Loss 197.28429 Test MSE 209.80009531426114 Test RE 0.24383273565758146 Lambda1 0.20442507\n",
      "23 Train Loss 189.18037 Test MSE 196.43852955742182 Test RE 0.235940506780486 Lambda1 0.27609012\n",
      "24 Train Loss 186.2429 Test MSE 192.7908572972959 Test RE 0.23373964919866858 Lambda1 0.31463903\n",
      "25 Train Loss 185.07022 Test MSE 190.3932672278404 Test RE 0.23228168293297516 Lambda1 0.35049978\n",
      "26 Train Loss 179.37753 Test MSE 179.76489090060522 Test RE 0.2257052226210882 Lambda1 0.5075788\n",
      "27 Train Loss 166.19899 Test MSE 170.092913256153 Test RE 0.21954941306381862 Lambda1 0.5979132\n",
      "28 Train Loss 153.31854 Test MSE 157.6726690121866 Test RE 0.21138169704548979 Lambda1 0.6864941\n",
      "29 Train Loss 149.09587 Test MSE 152.8387533518861 Test RE 0.2081162126792271 Lambda1 0.7246131\n",
      "30 Train Loss 145.75761 Test MSE 149.52799426460396 Test RE 0.2058497882268933 Lambda1 0.70467204\n",
      "31 Train Loss 123.783325 Test MSE 124.00444408285722 Test RE 0.18745964726432865 Lambda1 0.7941782\n",
      "32 Train Loss 100.20944 Test MSE 94.82214726596884 Test RE 0.1639245739267996 Lambda1 0.8373834\n",
      "33 Train Loss 85.99361 Test MSE 67.13763880411479 Test RE 0.13793427682668655 Lambda1 0.893723\n",
      "34 Train Loss 81.29532 Test MSE 61.360287403495704 Test RE 0.13186600929668574 Lambda1 0.8776611\n",
      "35 Train Loss 78.62881 Test MSE 62.885044197310044 Test RE 0.13349434102968336 Lambda1 0.86563903\n",
      "36 Train Loss 70.37537 Test MSE 53.81242309587655 Test RE 0.12348961729215382 Lambda1 0.8562033\n",
      "37 Train Loss 65.89438 Test MSE 54.59923329136986 Test RE 0.12438913355835782 Lambda1 0.84133923\n",
      "38 Train Loss 61.978203 Test MSE 55.61708505638635 Test RE 0.1255432254426021 Lambda1 0.82497966\n",
      "39 Train Loss 59.783745 Test MSE 55.81976118628596 Test RE 0.12577176561406492 Lambda1 0.81517065\n",
      "40 Train Loss 57.489372 Test MSE 54.806560716462194 Test RE 0.12462507865728414 Lambda1 0.82287824\n",
      "41 Train Loss 56.642426 Test MSE 54.02325495482632 Test RE 0.12373129098686755 Lambda1 0.8269712\n",
      "42 Train Loss 50.585968 Test MSE 53.14122654286965 Test RE 0.12271706439795065 Lambda1 0.82591087\n",
      "43 Train Loss 47.818733 Test MSE 51.35993484526181 Test RE 0.12064279847319115 Lambda1 0.8335238\n",
      "44 Train Loss 47.64688 Test MSE 51.11843657281946 Test RE 0.12035882851653812 Lambda1 0.8371876\n",
      "45 Train Loss 47.05701 Test MSE 49.185498453383886 Test RE 0.1180613402554291 Lambda1 0.84733015\n",
      "46 Train Loss 45.612206 Test MSE 47.99934098462152 Test RE 0.11662906876544364 Lambda1 0.8529667\n",
      "47 Train Loss 42.33596 Test MSE 45.53429059444446 Test RE 0.11359480178028428 Lambda1 0.86616147\n",
      "48 Train Loss 41.38758 Test MSE 44.75056990652166 Test RE 0.11261298125064209 Lambda1 0.8740318\n",
      "49 Train Loss 40.747013 Test MSE 42.9421209263355 Test RE 0.11031407200513178 Lambda1 0.8952491\n",
      "50 Train Loss 39.965446 Test MSE 42.030525089992 Test RE 0.10913689117050612 Lambda1 0.9080169\n",
      "51 Train Loss 39.373695 Test MSE 41.023151159710594 Test RE 0.10782108039971997 Lambda1 0.9222608\n",
      "52 Train Loss 38.920013 Test MSE 40.87239330050304 Test RE 0.10762277972986549 Lambda1 0.9268387\n",
      "53 Train Loss 38.661755 Test MSE 41.106849909990444 Test RE 0.1079310169964311 Lambda1 0.92806387\n",
      "54 Train Loss 38.525303 Test MSE 41.37310874571338 Test RE 0.10828000021995263 Lambda1 0.9288653\n",
      "55 Train Loss 37.884922 Test MSE 41.22292020595774 Test RE 0.10808328789871671 Lambda1 0.93015826\n",
      "56 Train Loss 37.83169 Test MSE 41.20609281774563 Test RE 0.10806122559599723 Lambda1 0.9315664\n",
      "57 Train Loss 37.693768 Test MSE 40.66300762509927 Test RE 0.10734675474643908 Lambda1 0.94914144\n",
      "58 Train Loss 37.176945 Test MSE 39.8679232431451 Test RE 0.10629209753232287 Lambda1 0.98115647\n",
      "59 Train Loss 36.78564 Test MSE 39.851700644529075 Test RE 0.10627046975062149 Lambda1 1.0051186\n",
      "60 Train Loss 36.502205 Test MSE 39.522283548840285 Test RE 0.1058303385550045 Lambda1 1.0232117\n",
      "61 Train Loss 36.21894 Test MSE 39.550210964129526 Test RE 0.10586772310891797 Lambda1 1.0166315\n",
      "62 Train Loss 36.207233 Test MSE 39.57166280555056 Test RE 0.10589643028497638 Lambda1 1.0158693\n",
      "63 Train Loss 36.203938 Test MSE 39.57104120539664 Test RE 0.10589559855980742 Lambda1 1.0166945\n",
      "64 Train Loss 36.148033 Test MSE 39.42535116641747 Test RE 0.10570047909871721 Lambda1 1.0284193\n",
      "65 Train Loss 36.036125 Test MSE 39.28046908803882 Test RE 0.1055060838708046 Lambda1 1.0292221\n",
      "66 Train Loss 35.95955 Test MSE 39.06655889831644 Test RE 0.10521841372922755 Lambda1 1.038755\n",
      "67 Train Loss 35.922955 Test MSE 38.85493996972771 Test RE 0.1049330489068604 Lambda1 1.0477395\n",
      "68 Train Loss 35.901062 Test MSE 38.764174465811244 Test RE 0.10481041495363241 Lambda1 1.0548831\n",
      "69 Train Loss 35.87628 Test MSE 38.68703262721646 Test RE 0.10470607512552857 Lambda1 1.0604596\n",
      "70 Train Loss 35.775517 Test MSE 38.419269879288485 Test RE 0.10434309734155212 Lambda1 1.0759537\n",
      "71 Train Loss 35.682896 Test MSE 38.37766006159112 Test RE 0.10428657787263772 Lambda1 1.0843339\n",
      "72 Train Loss 35.479237 Test MSE 38.352313705984386 Test RE 0.10425213438045863 Lambda1 1.0904719\n",
      "73 Train Loss 35.27919 Test MSE 38.12678484002363 Test RE 0.1039451576891235 Lambda1 1.0887281\n",
      "74 Train Loss 35.152943 Test MSE 37.882773478221 Test RE 0.10361199935908756 Lambda1 1.083701\n",
      "Training time: 213.74\n",
      "Training time: 213.74\n",
      "inv_HT_swish_tune1\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 838.08136 Test MSE 858.3105839505099 Test RE 0.4931867548821408 Lambda1 -1.2946609e-07\n",
      "1 Train Loss 838.02094 Test MSE 858.0715345430665 Test RE 0.49311807099677873 Lambda1 8.9993855e-06\n",
      "2 Train Loss 837.91296 Test MSE 857.9503125324002 Test RE 0.49308323772646867 Lambda1 -0.0010138262\n",
      "3 Train Loss 837.9069 Test MSE 857.9469174194628 Test RE 0.4930822621018251 Lambda1 -0.001660326\n",
      "4 Train Loss 837.90674 Test MSE 857.9468172064774 Test RE 0.49308223330444506 Lambda1 -0.0017616273\n",
      "5 Train Loss 837.90656 Test MSE 857.9466815245027 Test RE 0.4930821943146309 Lambda1 -0.0019542668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 837.9058 Test MSE 857.9462029420685 Test RE 0.49308205678830513 Lambda1 -0.0032339667\n",
      "7 Train Loss 837.8775 Test MSE 857.8922005552754 Test RE 0.4930665383170513 Lambda1 -0.030781537\n",
      "8 Train Loss 836.6034 Test MSE 857.7720233510621 Test RE 0.49303200167400985 Lambda1 -0.33491102\n",
      "9 Train Loss 834.66016 Test MSE 857.2264369731791 Test RE 0.49287518009245096 Lambda1 -0.4047502\n",
      "10 Train Loss 831.1559 Test MSE 858.4536235049229 Test RE 0.4932278485585395 Lambda1 -0.37042895\n",
      "11 Train Loss 754.51904 Test MSE 769.4220324161105 Test RE 0.46695118269562597 Lambda1 -0.012800813\n",
      "12 Train Loss 676.6387 Test MSE 716.9837096165681 Test RE 0.4507583855944429 Lambda1 -0.0010873938\n",
      "13 Train Loss 626.4492 Test MSE 664.8001227054528 Test RE 0.4340449584118621 Lambda1 -0.00031370576\n",
      "14 Train Loss 616.22675 Test MSE 656.9899107301169 Test RE 0.43148779988113706 Lambda1 -0.0007432634\n",
      "15 Train Loss 602.82587 Test MSE 641.3553123462142 Test RE 0.4263227599575335 Lambda1 0.0004132626\n",
      "16 Train Loss 551.101 Test MSE 548.4472034553461 Test RE 0.39423627602918326 Lambda1 0.0021395877\n",
      "17 Train Loss 516.4034 Test MSE 521.9074690648829 Test RE 0.38457931955799507 Lambda1 0.005192599\n",
      "18 Train Loss 396.60522 Test MSE 363.24850026220963 Test RE 0.32084190447251865 Lambda1 0.008358863\n",
      "19 Train Loss 343.54642 Test MSE 317.5766800901369 Test RE 0.29999462448791286 Lambda1 0.012370138\n",
      "20 Train Loss 296.62354 Test MSE 299.02350859052905 Test RE 0.2910997531212778 Lambda1 0.015649278\n",
      "21 Train Loss 281.97748 Test MSE 292.5035097113025 Test RE 0.2879086488405903 Lambda1 0.012970843\n",
      "22 Train Loss 276.4557 Test MSE 291.730069702534 Test RE 0.28752775175985673 Lambda1 0.014632677\n",
      "23 Train Loss 271.0512 Test MSE 285.63088014803725 Test RE 0.28450620959764095 Lambda1 0.019605353\n",
      "24 Train Loss 269.54373 Test MSE 285.1680757459659 Test RE 0.28427562510071686 Lambda1 0.022103397\n",
      "25 Train Loss 260.36456 Test MSE 279.2796480802097 Test RE 0.2813253157035128 Lambda1 0.029388525\n",
      "26 Train Loss 259.24008 Test MSE 277.0511937852554 Test RE 0.28020067912020546 Lambda1 0.03313228\n",
      "27 Train Loss 258.04117 Test MSE 274.92010549875215 Test RE 0.2791209416429866 Lambda1 0.041313685\n",
      "28 Train Loss 256.77576 Test MSE 270.72721360017067 Test RE 0.2769842835553024 Lambda1 0.05169246\n",
      "29 Train Loss 248.32483 Test MSE 263.680077842639 Test RE 0.2733555083629398 Lambda1 0.07582043\n",
      "30 Train Loss 242.1143 Test MSE 261.03616072094496 Test RE 0.27198158934202915 Lambda1 0.09643599\n",
      "31 Train Loss 233.49124 Test MSE 251.8485529629466 Test RE 0.2671522895805253 Lambda1 0.13905251\n",
      "32 Train Loss 228.96347 Test MSE 247.32174127124813 Test RE 0.2647404595400481 Lambda1 0.17324331\n",
      "33 Train Loss 224.03174 Test MSE 246.06741913732122 Test RE 0.2640682745542898 Lambda1 0.21391208\n",
      "34 Train Loss 222.3211 Test MSE 244.56426117036585 Test RE 0.26326047890142734 Lambda1 0.23397927\n",
      "35 Train Loss 219.4841 Test MSE 243.18620156644207 Test RE 0.26251772701547216 Lambda1 0.25840485\n",
      "36 Train Loss 214.567 Test MSE 237.6855293640962 Test RE 0.25953177779180325 Lambda1 0.27863252\n",
      "37 Train Loss 212.54678 Test MSE 233.17369691395396 Test RE 0.2570567129642655 Lambda1 0.28365868\n",
      "38 Train Loss 204.84225 Test MSE 225.3368099192304 Test RE 0.25269999975727453 Lambda1 0.34885255\n",
      "39 Train Loss 197.48697 Test MSE 215.08564806061793 Test RE 0.2468851037561943 Lambda1 0.40149194\n",
      "40 Train Loss 189.65813 Test MSE 200.1687802110209 Test RE 0.23817015646397005 Lambda1 0.47123772\n",
      "41 Train Loss 178.38237 Test MSE 187.65704060549274 Test RE 0.23060653069174789 Lambda1 0.5207885\n",
      "42 Train Loss 153.46106 Test MSE 152.33098575649703 Test RE 0.20777021867605236 Lambda1 0.5938703\n",
      "43 Train Loss 143.30753 Test MSE 136.63553290588013 Test RE 0.19677548800172395 Lambda1 0.6007426\n",
      "44 Train Loss 134.07455 Test MSE 132.4203341494207 Test RE 0.19371645387213007 Lambda1 0.58112836\n",
      "45 Train Loss 128.96484 Test MSE 132.5298605252832 Test RE 0.19379654986293327 Lambda1 0.5740492\n",
      "46 Train Loss 123.5498 Test MSE 127.27288478502649 Test RE 0.18991405841287687 Lambda1 0.5806699\n",
      "47 Train Loss 119.458496 Test MSE 122.8648662199902 Test RE 0.18659629947384332 Lambda1 0.57121986\n",
      "48 Train Loss 118.36061 Test MSE 122.14004317021319 Test RE 0.1860450867294084 Lambda1 0.5663161\n",
      "49 Train Loss 111.98852 Test MSE 116.91629397755254 Test RE 0.18202317730871886 Lambda1 0.5624195\n",
      "50 Train Loss 104.657135 Test MSE 109.2645167586745 Test RE 0.1759659932708715 Lambda1 0.55757153\n",
      "51 Train Loss 100.170906 Test MSE 97.82800304706726 Test RE 0.1665025023402794 Lambda1 0.56224215\n",
      "52 Train Loss 94.27878 Test MSE 90.04706676900528 Test RE 0.1597437796312527 Lambda1 0.5655906\n",
      "53 Train Loss 92.60263 Test MSE 88.86395231021045 Test RE 0.15869088535699888 Lambda1 0.5720873\n",
      "54 Train Loss 87.30468 Test MSE 85.94889738847141 Test RE 0.15606636869059753 Lambda1 0.5774301\n",
      "55 Train Loss 84.72404 Test MSE 85.40404133773684 Test RE 0.15557090626855696 Lambda1 0.5968924\n",
      "56 Train Loss 82.26861 Test MSE 78.53905569715467 Test RE 0.14918735294939406 Lambda1 0.6119821\n",
      "57 Train Loss 77.4525 Test MSE 74.30360682544399 Test RE 0.14510892289594898 Lambda1 0.6142096\n",
      "58 Train Loss 74.368576 Test MSE 72.26751254784797 Test RE 0.14310694936787746 Lambda1 0.6370408\n",
      "59 Train Loss 73.3576 Test MSE 70.39090136742186 Test RE 0.1412366584230835 Lambda1 0.6540507\n",
      "60 Train Loss 70.82726 Test MSE 65.52130499874416 Test RE 0.1362637824389543 Lambda1 0.6912567\n",
      "61 Train Loss 68.78575 Test MSE 64.33182196857847 Test RE 0.13502124131158585 Lambda1 0.7016357\n",
      "62 Train Loss 66.7036 Test MSE 64.62105517765387 Test RE 0.1353244258738577 Lambda1 0.70650035\n",
      "63 Train Loss 64.64737 Test MSE 61.276710334790984 Test RE 0.13177617326119115 Lambda1 0.7309081\n",
      "64 Train Loss 61.705196 Test MSE 59.23287142521937 Test RE 0.1295598879773227 Lambda1 0.7504724\n",
      "65 Train Loss 59.268345 Test MSE 56.39276990072042 Test RE 0.12641566222018175 Lambda1 0.76767796\n",
      "66 Train Loss 54.261707 Test MSE 51.23649783456003 Test RE 0.12049773652126497 Lambda1 0.79810303\n",
      "67 Train Loss 51.506912 Test MSE 48.786000011467536 Test RE 0.11758089900344186 Lambda1 0.81564337\n",
      "68 Train Loss 48.986702 Test MSE 46.920087278884594 Test RE 0.11531042591885694 Lambda1 0.832217\n",
      "69 Train Loss 46.872448 Test MSE 44.55714114636448 Test RE 0.1123693399011976 Lambda1 0.8436307\n",
      "70 Train Loss 43.864136 Test MSE 43.99748296494987 Test RE 0.11166140458841926 Lambda1 0.84256124\n",
      "71 Train Loss 42.189003 Test MSE 42.70815318794499 Test RE 0.11001314152618943 Lambda1 0.8471781\n",
      "72 Train Loss 40.73265 Test MSE 42.029167256112004 Test RE 0.10913512827360032 Lambda1 0.85988086\n",
      "73 Train Loss 40.269245 Test MSE 41.24873459969508 Test RE 0.10811712426780429 Lambda1 0.87250155\n",
      "74 Train Loss 39.346912 Test MSE 40.28304384127692 Test RE 0.1068440421835676 Lambda1 0.8915886\n",
      "Training time: 207.57\n",
      "Training time: 207.57\n",
      "inv_HT_swish_tune1\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 838.065 Test MSE 858.3354514845323 Test RE 0.4931938992945508 Lambda1 4.2778382e-07\n",
      "1 Train Loss 838.06036 Test MSE 858.2636910375653 Test RE 0.4931732823234408 Lambda1 7.356575e-07\n",
      "2 Train Loss 838.05005 Test MSE 858.1768248236148 Test RE 0.4931483242713074 Lambda1 4.875828e-06\n",
      "3 Train Loss 837.9081 Test MSE 857.8402511836579 Test RE 0.49305160935243575 Lambda1 0.00041911987\n",
      "4 Train Loss 837.9005 Test MSE 857.8999984905525 Test RE 0.4930687792117777 Lambda1 0.0006969957\n",
      "5 Train Loss 837.9002 Test MSE 857.9111965290366 Test RE 0.4930719971772498 Lambda1 0.00076372334\n",
      "6 Train Loss 837.9001 Test MSE 857.9158159140766 Test RE 0.49307332463768555 Lambda1 0.0008062653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "8 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "9 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "10 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "11 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "12 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "13 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "14 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "15 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "16 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "17 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "18 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "19 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "20 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "21 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "22 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "23 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "24 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "25 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "26 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "27 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "28 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "29 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "30 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "31 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "32 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "33 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "34 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "35 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "36 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "37 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "38 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "39 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "40 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "41 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "42 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "43 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "44 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "45 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "46 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "47 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "48 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "49 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "50 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "51 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "52 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "53 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "54 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "55 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "56 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "57 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "58 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "59 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "60 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "61 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "62 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "63 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "64 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "65 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "66 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "67 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "68 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "69 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "70 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "71 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "72 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "73 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "74 Train Loss 837.9 Test MSE 857.9173125352705 Test RE 0.4930737547169849 Lambda1 0.00082340627\n",
      "Training time: 161.78\n",
      "Training time: 161.78\n",
      "inv_HT_swish_tune1\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 847.3318 Test MSE 864.6184444849138 Test RE 0.49499569124503906 Lambda1 -4.0770826e-07\n",
      "1 Train Loss 839.0194 Test MSE 858.298684923666 Test RE 0.4931833362695288 Lambda1 -4.917034e-07\n",
      "2 Train Loss 838.1738 Test MSE 858.0758795186516 Test RE 0.49311931948433513 Lambda1 -5.3163643e-07\n",
      "3 Train Loss 838.0612 Test MSE 858.2807862433377 Test RE 0.4931781939006453 Lambda1 -4.6440874e-07\n",
      "4 Train Loss 838.05176 Test MSE 858.352104939051 Test RE 0.49319868375400533 Lambda1 5.0418316e-07\n",
      "5 Train Loss 837.86206 Test MSE 857.8999349378746 Test RE 0.493068760948669 Lambda1 0.00051641016\n",
      "6 Train Loss 837.8597 Test MSE 857.8515826338416 Test RE 0.49305486576952023 Lambda1 0.0006273555\n",
      "7 Train Loss 837.85913 Test MSE 857.8363685994102 Test RE 0.49305049357559083 Lambda1 0.00071395363\n",
      "8 Train Loss 837.8558 Test MSE 857.7982137953305 Test RE 0.49303952851578475 Lambda1 0.0012881311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 837.74554 Test MSE 857.7141733809151 Test RE 0.4930153758301305 Lambda1 0.006913236\n",
      "10 Train Loss 835.9683 Test MSE 858.842050329566 Test RE 0.4933394220059492 Lambda1 0.005134865\n",
      "11 Train Loss 777.2984 Test MSE 808.7426763776033 Test RE 0.4787340866806722 Lambda1 -0.019363698\n",
      "12 Train Loss 648.4102 Test MSE 671.4337619262485 Test RE 0.4362051194002021 Lambda1 -0.0008099883\n",
      "13 Train Loss 595.0839 Test MSE 625.5337128678715 Test RE 0.4210314434615234 Lambda1 -0.0008612083\n",
      "14 Train Loss 516.1547 Test MSE 491.5991570038923 Test RE 0.37324563319215404 Lambda1 -0.00051244785\n",
      "15 Train Loss 356.339 Test MSE 358.02514206086664 Test RE 0.3185267669261311 Lambda1 0.003864593\n",
      "16 Train Loss 303.307 Test MSE 326.3458426851261 Test RE 0.30410825761101257 Lambda1 0.0023168407\n",
      "17 Train Loss 283.43314 Test MSE 302.5649448837785 Test RE 0.29281847553959994 Lambda1 0.001139455\n",
      "18 Train Loss 264.4584 Test MSE 286.04972957680565 Test RE 0.2847147332861389 Lambda1 -0.00088613544\n",
      "19 Train Loss 259.42313 Test MSE 280.65738766784466 Test RE 0.2820183776309524 Lambda1 0.0029064761\n",
      "20 Train Loss 258.0888 Test MSE 280.63003833224644 Test RE 0.28200463631590755 Lambda1 0.0029816183\n",
      "21 Train Loss 257.02014 Test MSE 279.1926159371826 Test RE 0.28128147746986754 Lambda1 0.0030015693\n",
      "22 Train Loss 256.4532 Test MSE 278.0527184757439 Test RE 0.28070667719658343 Lambda1 0.0038684218\n",
      "23 Train Loss 254.39906 Test MSE 272.70092874489876 Test RE 0.2779921159176479 Lambda1 0.011336631\n",
      "24 Train Loss 247.01355 Test MSE 259.7026202271593 Test RE 0.2712859713993207 Lambda1 0.041148715\n",
      "25 Train Loss 237.5993 Test MSE 249.25862408494143 Test RE 0.26577508597955435 Lambda1 0.0683974\n",
      "26 Train Loss 217.53296 Test MSE 231.3081156283963 Test RE 0.256026315293095 Lambda1 0.102365896\n",
      "27 Train Loss 198.62456 Test MSE 206.86815561164616 Test RE 0.24212296960259258 Lambda1 0.1518738\n",
      "28 Train Loss 184.97276 Test MSE 191.08108619654033 Test RE 0.2327008776735475 Lambda1 0.17427638\n",
      "29 Train Loss 172.17462 Test MSE 173.66077227268613 Test RE 0.22184009095682783 Lambda1 0.21275021\n",
      "30 Train Loss 159.18341 Test MSE 157.89637960877027 Test RE 0.21153160116084122 Lambda1 0.24070255\n",
      "31 Train Loss 141.88165 Test MSE 141.75707242257184 Test RE 0.20042945170059928 Lambda1 0.27362895\n",
      "32 Train Loss 118.20105 Test MSE 109.91007556233853 Test RE 0.17648505062445344 Lambda1 0.3348084\n",
      "33 Train Loss 104.86254 Test MSE 85.92458658288132 Test RE 0.15604429529818858 Lambda1 0.38107276\n",
      "34 Train Loss 98.777176 Test MSE 82.16763951841301 Test RE 0.15259474439392628 Lambda1 0.3872137\n",
      "35 Train Loss 93.17168 Test MSE 79.56966759860556 Test RE 0.150163002203566 Lambda1 0.3982168\n",
      "36 Train Loss 82.40624 Test MSE 72.37106873079512 Test RE 0.14320944566078184 Lambda1 0.443735\n",
      "37 Train Loss 71.779 Test MSE 60.564645001819564 Test RE 0.13100828415067328 Lambda1 0.48336977\n",
      "38 Train Loss 62.783802 Test MSE 50.337508755887285 Test RE 0.1194359393858332 Lambda1 0.5577843\n",
      "39 Train Loss 57.959652 Test MSE 47.79080730799172 Test RE 0.11637544484727179 Lambda1 0.59238076\n",
      "40 Train Loss 51.875916 Test MSE 43.91223053829544 Test RE 0.11155317087942926 Lambda1 0.64220095\n",
      "41 Train Loss 46.88824 Test MSE 40.62633590888523 Test RE 0.10729833877938362 Lambda1 0.6647327\n",
      "42 Train Loss 43.099144 Test MSE 42.27768409158232 Test RE 0.10945730860540312 Lambda1 0.6509519\n",
      "43 Train Loss 40.994175 Test MSE 40.70517335373209 Test RE 0.10740239722833607 Lambda1 0.6762556\n",
      "44 Train Loss 39.37226 Test MSE 38.04921372953764 Test RE 0.10383936268499913 Lambda1 0.7175172\n",
      "45 Train Loss 37.05203 Test MSE 37.03998520177577 Test RE 0.1024529747744084 Lambda1 0.75283927\n",
      "46 Train Loss 35.86747 Test MSE 36.06285245669875 Test RE 0.10109256318765995 Lambda1 0.7954344\n",
      "47 Train Loss 35.624573 Test MSE 35.49004699402774 Test RE 0.10028649610820407 Lambda1 0.8108978\n",
      "48 Train Loss 35.51752 Test MSE 35.21934939398157 Test RE 0.09990330024773876 Lambda1 0.8181641\n",
      "49 Train Loss 34.95261 Test MSE 35.130228907453805 Test RE 0.09977682047999438 Lambda1 0.8311719\n",
      "50 Train Loss 34.194637 Test MSE 34.732090185489284 Test RE 0.09920981291283844 Lambda1 0.8524615\n",
      "51 Train Loss 33.152905 Test MSE 34.042488089764554 Test RE 0.09821997465464606 Lambda1 0.91041017\n",
      "52 Train Loss 32.73725 Test MSE 33.68135715698012 Test RE 0.09769761502924247 Lambda1 0.93630415\n",
      "53 Train Loss 32.67041 Test MSE 33.592820130203975 Test RE 0.09756912335065548 Lambda1 0.945261\n",
      "54 Train Loss 32.53051 Test MSE 33.516000205558505 Test RE 0.09745749916118103 Lambda1 0.96153927\n",
      "55 Train Loss 32.1119 Test MSE 33.560830832831606 Test RE 0.0975226564108671 Lambda1 0.9533026\n",
      "56 Train Loss 31.906712 Test MSE 33.59454088271619 Test RE 0.09757162225017482 Lambda1 0.9492126\n",
      "57 Train Loss 31.805092 Test MSE 33.595453335325374 Test RE 0.09757294729993657 Lambda1 0.9522083\n",
      "58 Train Loss 31.757471 Test MSE 33.54878503131116 Test RE 0.09750515320902098 Lambda1 0.9657187\n",
      "59 Train Loss 31.690392 Test MSE 33.47060068646168 Test RE 0.09739147065380693 Lambda1 0.9796617\n",
      "60 Train Loss 31.623842 Test MSE 33.47998707844037 Test RE 0.09740512577828195 Lambda1 0.9857454\n",
      "61 Train Loss 31.619648 Test MSE 33.47412623744369 Test RE 0.09739659977528534 Lambda1 0.9868832\n",
      "62 Train Loss 31.548693 Test MSE 33.46632267477973 Test RE 0.0973852464577266 Lambda1 0.9865857\n",
      "63 Train Loss 31.499231 Test MSE 33.49745173543749 Test RE 0.09743052789861065 Lambda1 0.98169196\n",
      "64 Train Loss 31.441065 Test MSE 33.560066010792205 Test RE 0.09752154517643004 Lambda1 0.9701738\n",
      "65 Train Loss 31.417467 Test MSE 33.6343694609717 Test RE 0.09762944396395612 Lambda1 0.96712214\n",
      "66 Train Loss 31.328283 Test MSE 33.72434691319811 Test RE 0.09775994412875669 Lambda1 0.9718797\n",
      "67 Train Loss 31.248186 Test MSE 33.65819714827541 Test RE 0.09766401978686683 Lambda1 0.98468494\n",
      "68 Train Loss 31.239595 Test MSE 33.62064875021478 Test RE 0.09760952859131901 Lambda1 0.9872329\n",
      "69 Train Loss 31.216965 Test MSE 33.60135945023277 Test RE 0.09758152363476914 Lambda1 0.994294\n",
      "70 Train Loss 31.172457 Test MSE 33.53580363103749 Test RE 0.09748628701329978 Lambda1 1.0095415\n",
      "71 Train Loss 31.16008 Test MSE 33.51615878790207 Test RE 0.09745772972300583 Lambda1 1.0163839\n",
      "72 Train Loss 31.157816 Test MSE 33.51782066073408 Test RE 0.09746014587616164 Lambda1 1.0201467\n",
      "73 Train Loss 31.157219 Test MSE 33.52417295889756 Test RE 0.09746938076320781 Lambda1 1.0203925\n",
      "74 Train Loss 31.156689 Test MSE 33.526834731530116 Test RE 0.09747325015293042 Lambda1 1.0195422\n",
      "Training time: 201.86\n",
      "Training time: 201.86\n",
      "inv_HT_swish_tune1\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 838.085 Test MSE 858.2578251369005 Test RE 0.49317159699615865 Lambda1 -1.2235671e-06\n",
      "1 Train Loss 838.08154 Test MSE 858.3144657914632 Test RE 0.4931878701371627 Lambda1 -2.7476656e-06\n",
      "2 Train Loss 838.03534 Test MSE 858.3591703443054 Test RE 0.49320071359780776 Lambda1 -8.127807e-05\n",
      "3 Train Loss 837.95276 Test MSE 858.2056569189708 Test RE 0.49315660833333097 Lambda1 -0.00064197916\n",
      "4 Train Loss 837.89685 Test MSE 857.9222147699132 Test RE 0.4930751634540237 Lambda1 -0.0012662032\n",
      "5 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "6 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "7 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "8 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "9 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "11 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "12 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "13 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "14 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "15 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "16 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "17 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "18 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "19 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "20 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "21 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "22 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "23 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "24 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "25 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "26 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "27 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "28 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "29 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "30 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "31 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "32 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "33 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "34 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "35 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "36 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "37 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "38 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "39 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "40 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "41 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "42 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "43 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "44 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "45 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "46 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "47 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "48 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "49 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "50 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "51 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "52 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "53 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "54 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "55 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "56 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "57 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "58 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "59 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "60 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "61 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "62 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "63 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "64 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "65 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "66 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "67 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "68 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "69 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "70 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "71 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "72 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "73 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "74 Train Loss 837.8967 Test MSE 857.9206720607627 Test RE 0.49307472013183457 Lambda1 -0.0012727488\n",
      "Training time: 185.50\n",
      "Training time: 185.50\n",
      "inv_HT_swish_tune1\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 838.0938 Test MSE 858.3398859034596 Test RE 0.4931951732868654 Lambda1 1.0594949e-05\n",
      "1 Train Loss 837.96655 Test MSE 858.2581887464246 Test RE 0.49317170146469097 Lambda1 0.0010976334\n",
      "2 Train Loss 837.8762 Test MSE 857.8808814781804 Test RE 0.49306328553274703 Lambda1 0.007984842\n",
      "3 Train Loss 837.8735 Test MSE 857.9116212258796 Test RE 0.4930721192214052 Lambda1 0.013714908\n",
      "4 Train Loss 837.8123 Test MSE 857.6562871317324 Test RE 0.4929987389985892 Lambda1 0.12550373\n",
      "5 Train Loss 836.79767 Test MSE 855.6279545602798 Test RE 0.492415429963523 Lambda1 0.73736334\n",
      "6 Train Loss 824.7534 Test MSE 846.5006268488733 Test RE 0.4897819913722657 Lambda1 0.7036692\n",
      "7 Train Loss 803.2854 Test MSE 812.1667439075954 Test RE 0.4797464522427671 Lambda1 0.36958408\n",
      "8 Train Loss 742.2179 Test MSE 752.59542908012 Test RE 0.46181704576147403 Lambda1 0.018439071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 648.187 Test MSE 687.6397445082134 Test RE 0.4414379392883492 Lambda1 0.0016552842\n",
      "10 Train Loss 609.66565 Test MSE 650.4423262326882 Test RE 0.429332304723402 Lambda1 0.00046172555\n",
      "11 Train Loss 603.50726 Test MSE 646.6139943797515 Test RE 0.4280669718085097 Lambda1 7.973438e-05\n",
      "12 Train Loss 601.1931 Test MSE 643.9393592923465 Test RE 0.42718073259632355 Lambda1 9.653679e-05\n",
      "13 Train Loss 589.02594 Test MSE 632.5113451221449 Test RE 0.4233731680799648 Lambda1 -3.8188413e-05\n",
      "14 Train Loss 560.0745 Test MSE 589.6151864475909 Test RE 0.40876481015970434 Lambda1 9.6420035e-06\n",
      "15 Train Loss 525.4426 Test MSE 539.7569337951488 Test RE 0.3911004229893468 Lambda1 -7.901793e-05\n",
      "16 Train Loss 498.67014 Test MSE 506.52098621974636 Test RE 0.3788679713372765 Lambda1 -0.00017852108\n",
      "17 Train Loss 467.17685 Test MSE 473.23246388633567 Test RE 0.36620682625988843 Lambda1 0.00092702825\n",
      "18 Train Loss 397.19052 Test MSE 378.2776658442424 Test RE 0.32741194586290606 Lambda1 0.0025967096\n",
      "19 Train Loss 366.198 Test MSE 329.9708624085366 Test RE 0.3057925964906442 Lambda1 0.0015897971\n",
      "20 Train Loss 289.6409 Test MSE 303.52980030961686 Test RE 0.29328499127234575 Lambda1 -0.0029289736\n",
      "21 Train Loss 284.0559 Test MSE 301.7627765651129 Test RE 0.2924300538116864 Lambda1 -0.002243263\n",
      "22 Train Loss 276.52863 Test MSE 295.71424358307434 Test RE 0.28948448490659695 Lambda1 -0.00023918145\n",
      "23 Train Loss 269.03595 Test MSE 292.0526111033325 Test RE 0.28768665547602373 Lambda1 0.0007628434\n",
      "24 Train Loss 266.8376 Test MSE 290.8248016385415 Test RE 0.28708129121553116 Lambda1 0.0008236639\n",
      "25 Train Loss 264.91708 Test MSE 290.438638009052 Test RE 0.2868906314473273 Lambda1 0.0003789842\n",
      "26 Train Loss 259.40665 Test MSE 286.41749751779344 Test RE 0.2848977002784422 Lambda1 -0.00020594051\n",
      "27 Train Loss 258.87958 Test MSE 285.3495529458552 Test RE 0.2843660653422997 Lambda1 -0.00014628789\n",
      "28 Train Loss 258.74982 Test MSE 285.3885476067757 Test RE 0.2843854948093709 Lambda1 -9.887125e-05\n",
      "29 Train Loss 258.39944 Test MSE 285.0664868748085 Test RE 0.2842249851189545 Lambda1 -6.371767e-05\n",
      "30 Train Loss 258.21872 Test MSE 284.0619886210457 Test RE 0.28372377668507465 Lambda1 3.671129e-05\n",
      "31 Train Loss 257.26266 Test MSE 282.45774481393056 Test RE 0.28292147545416857 Lambda1 5.8117483e-05\n",
      "32 Train Loss 257.1914 Test MSE 282.3258795079177 Test RE 0.2828554268453203 Lambda1 5.8822134e-05\n",
      "33 Train Loss 257.13397 Test MSE 281.92314583794774 Test RE 0.28265361031687863 Lambda1 0.00018115676\n",
      "34 Train Loss 256.87274 Test MSE 282.21447221437364 Test RE 0.2827996132108179 Lambda1 0.00016025065\n",
      "35 Train Loss 256.72046 Test MSE 282.11407287621086 Test RE 0.2827493049834309 Lambda1 0.00017341197\n",
      "36 Train Loss 256.56686 Test MSE 281.90254987881684 Test RE 0.2826432854666149 Lambda1 0.00029875568\n",
      "37 Train Loss 256.351 Test MSE 281.25898241419935 Test RE 0.2823204718591853 Lambda1 0.00048450864\n",
      "38 Train Loss 256.02832 Test MSE 281.0517705249588 Test RE 0.2822164557376074 Lambda1 0.0006365863\n",
      "39 Train Loss 255.9723 Test MSE 280.74329282210135 Test RE 0.28206153519549776 Lambda1 0.00080912036\n",
      "40 Train Loss 255.91777 Test MSE 280.33368158189074 Test RE 0.2818556927920071 Lambda1 0.0011335323\n",
      "41 Train Loss 255.7664 Test MSE 280.3807597184587 Test RE 0.2818793586670343 Lambda1 0.0010955201\n",
      "42 Train Loss 255.6597 Test MSE 280.15147412246137 Test RE 0.28176407954545646 Lambda1 0.0013439615\n",
      "43 Train Loss 255.55377 Test MSE 279.55761474896474 Test RE 0.281465282244579 Lambda1 0.0018684223\n",
      "44 Train Loss 255.52364 Test MSE 279.6362423388889 Test RE 0.28150486152934934 Lambda1 0.0018081195\n",
      "45 Train Loss 255.47177 Test MSE 279.7026329420133 Test RE 0.28153827666935993 Lambda1 0.0019892713\n",
      "46 Train Loss 255.34311 Test MSE 279.5365060538872 Test RE 0.2814546556748436 Lambda1 0.0023081067\n",
      "47 Train Loss 255.18178 Test MSE 279.82235705025624 Test RE 0.28159852514207634 Lambda1 0.0019980073\n",
      "48 Train Loss 255.09166 Test MSE 280.0304810846512 Test RE 0.2817032282261348 Lambda1 0.0018892995\n",
      "49 Train Loss 254.9784 Test MSE 280.2150250942708 Test RE 0.2817960361249353 Lambda1 0.0018795135\n",
      "50 Train Loss 254.94751 Test MSE 280.2786553292754 Test RE 0.28182802893152886 Lambda1 0.0017914993\n",
      "51 Train Loss 254.92366 Test MSE 280.3919717452119 Test RE 0.28188499458748584 Lambda1 0.0017704077\n",
      "52 Train Loss 254.90558 Test MSE 280.48153899058036 Test RE 0.28193001307783117 Lambda1 0.0017384924\n",
      "53 Train Loss 254.8776 Test MSE 280.5134457957557 Test RE 0.28194604841160353 Lambda1 0.0017888787\n",
      "54 Train Loss 254.85304 Test MSE 280.5456688239173 Test RE 0.2819622417433361 Lambda1 0.0017877234\n",
      "55 Train Loss 254.8305 Test MSE 280.70857757035026 Test RE 0.28204409552699167 Lambda1 0.0018345244\n",
      "56 Train Loss 254.79944 Test MSE 280.7742070991884 Test RE 0.2820770644862159 Lambda1 0.0018845658\n",
      "57 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "58 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "59 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "60 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "61 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "62 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "63 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "64 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "65 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "66 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "67 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "68 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "69 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "70 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "71 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "72 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "73 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "74 Train Loss 254.79608 Test MSE 280.8027881305369 Test RE 0.28209142094785467 Lambda1 0.00188528\n",
      "Training time: 200.62\n",
      "Training time: 200.62\n",
      "inv_HT_swish_tune2\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.0492 Test MSE 858.2701756499805 Test RE 0.49317514540560436 Lambda1 -3.2587482e-07\n",
      "1 Train Loss 838.04913 Test MSE 858.2699888409206 Test RE 0.4931750917339326 Lambda1 -4.3998972e-07\n",
      "2 Train Loss 838.049 Test MSE 858.2698415834022 Test RE 0.4931750494257172 Lambda1 -5.3810027e-07\n",
      "3 Train Loss 838.0488 Test MSE 858.2696554887045 Test RE 0.4931749959592767 Lambda1 -6.587175e-07\n",
      "4 Train Loss 838.0486 Test MSE 858.2694229008417 Test RE 0.4931749291349837 Lambda1 -8.2030107e-07\n",
      "5 Train Loss 838.0484 Test MSE 858.2692424024506 Test RE 0.49317487727639264 Lambda1 -9.439604e-07\n",
      "6 Train Loss 838.0483 Test MSE 858.2691197447755 Test RE 0.49317484203588585 Lambda1 -1.027037e-06\n",
      "7 Train Loss 838.04816 Test MSE 858.2689973214159 Test RE 0.4931748068626973 Lambda1 -1.110516e-06\n",
      "8 Train Loss 838.04803 Test MSE 858.2687932104938 Test RE 0.49317474822002766 Lambda1 -1.253971e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "10 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "11 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "12 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "13 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "14 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "15 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "16 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "17 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "18 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "19 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "20 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "21 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "22 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "23 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "24 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "25 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "26 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "27 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "28 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "29 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "30 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "31 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "32 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "33 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "34 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "35 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "36 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "37 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "38 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "39 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "40 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "41 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "42 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "43 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "44 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "45 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "46 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "47 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "48 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "49 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "50 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "51 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "52 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "53 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "54 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "55 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "56 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "57 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "58 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "59 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "60 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "61 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "62 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "63 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "64 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "65 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "66 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "67 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "68 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "69 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "70 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "71 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "72 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "73 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "74 Train Loss 838.0479 Test MSE 858.268728647681 Test RE 0.4931747296706234 Lambda1 -1.2982281e-06\n",
      "Training time: 177.30\n",
      "Training time: 177.30\n",
      "inv_HT_swish_tune2\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 838.1165 Test MSE 858.4758036903241 Test RE 0.4932342203730224 Lambda1 2.5992395e-06\n",
      "1 Train Loss 838.0937 Test MSE 858.3109586252212 Test RE 0.49318686252649957 Lambda1 6.9224425e-06\n",
      "2 Train Loss 838.0803 Test MSE 858.2112079027742 Test RE 0.4931582032307174 Lambda1 6.7597844e-05\n",
      "3 Train Loss 837.92847 Test MSE 858.1777740750298 Test RE 0.4931485970132423 Lambda1 0.0027039875\n",
      "4 Train Loss 837.8586 Test MSE 857.9327524131108 Test RE 0.4930781916039526 Lambda1 0.00821551\n",
      "5 Train Loss 837.8527 Test MSE 857.8398475599721 Test RE 0.49305149335920095 Lambda1 0.011902753\n",
      "6 Train Loss 837.76495 Test MSE 857.602537069593 Test RE 0.4929832904276169 Lambda1 0.152298\n",
      "7 Train Loss 834.7405 Test MSE 856.0458782615624 Test RE 0.4925356732040457 Lambda1 0.34026545\n",
      "8 Train Loss 821.06793 Test MSE 837.3395356268378 Test RE 0.48712449576456296 Lambda1 0.65833783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 802.61993 Test MSE 821.0075805130275 Test RE 0.4823505234514346 Lambda1 0.5123216\n",
      "10 Train Loss 756.9165 Test MSE 765.0383302583243 Test RE 0.4656190796605306 Lambda1 0.14620724\n",
      "11 Train Loss 692.97986 Test MSE 697.0416189887859 Test RE 0.4444455124517616 Lambda1 0.0019517643\n",
      "12 Train Loss 651.1964 Test MSE 684.7319603520697 Test RE 0.4405036084040263 Lambda1 0.00040704754\n",
      "13 Train Loss 552.1129 Test MSE 547.0277513709161 Test RE 0.39372577834729966 Lambda1 6.74451e-05\n",
      "14 Train Loss 504.98456 Test MSE 511.29829838372234 Test RE 0.38065044717502083 Lambda1 -0.00013964585\n",
      "15 Train Loss 449.1323 Test MSE 477.9898770708602 Test RE 0.3680429645260418 Lambda1 2.5314639e-05\n",
      "16 Train Loss 440.84964 Test MSE 463.823398370341 Test RE 0.36254798643137626 Lambda1 3.4929835e-05\n",
      "17 Train Loss 391.2626 Test MSE 389.3542957438862 Test RE 0.3321709556444502 Lambda1 7.7629826e-05\n",
      "18 Train Loss 328.37512 Test MSE 331.4087775591144 Test RE 0.3064581489282405 Lambda1 -3.1125324e-05\n",
      "19 Train Loss 294.3678 Test MSE 310.37421547746993 Test RE 0.29657325793044464 Lambda1 -0.00045696157\n",
      "20 Train Loss 289.4281 Test MSE 306.16444329250754 Test RE 0.29455510003421786 Lambda1 -0.00030847924\n",
      "21 Train Loss 288.18924 Test MSE 305.4881966542995 Test RE 0.2942296180678891 Lambda1 -0.00014296485\n",
      "22 Train Loss 278.00424 Test MSE 296.1082480823366 Test RE 0.2896772727499248 Lambda1 0.0007117408\n",
      "23 Train Loss 266.68845 Test MSE 286.78026423450183 Test RE 0.2850780640823003 Lambda1 4.5096356e-05\n",
      "24 Train Loss 263.06326 Test MSE 284.6577491526833 Test RE 0.28402114645050447 Lambda1 7.865344e-05\n",
      "25 Train Loss 261.99896 Test MSE 283.29977045949875 Test RE 0.28334286566867495 Lambda1 0.00012706645\n",
      "26 Train Loss 261.41116 Test MSE 282.35411848998683 Test RE 0.2828695724660066 Lambda1 0.00018976655\n",
      "27 Train Loss 261.3656 Test MSE 282.4584453477842 Test RE 0.28292182629593554 Lambda1 0.0002105893\n",
      "28 Train Loss 260.8547 Test MSE 282.62318964878904 Test RE 0.2830043215531292 Lambda1 0.00024620348\n",
      "29 Train Loss 259.54868 Test MSE 281.98723581806297 Test RE 0.282685736509969 Lambda1 8.395933e-05\n",
      "30 Train Loss 259.01318 Test MSE 281.90469163663977 Test RE 0.2826443591573605 Lambda1 -6.1583014e-05\n",
      "31 Train Loss 258.81262 Test MSE 281.7161479344339 Test RE 0.28254982414438323 Lambda1 0.000120251614\n",
      "32 Train Loss 258.73453 Test MSE 281.77146269213614 Test RE 0.2825775620075782 Lambda1 8.988654e-05\n",
      "33 Train Loss 258.69513 Test MSE 281.7333482892396 Test RE 0.2825584496404394 Lambda1 -2.254636e-05\n",
      "34 Train Loss 258.63043 Test MSE 281.6534855125336 Test RE 0.282518398468082 Lambda1 7.0667695e-05\n",
      "35 Train Loss 258.34323 Test MSE 281.7468249534318 Test RE 0.28256520762600734 Lambda1 0.00023553717\n",
      "36 Train Loss 257.67288 Test MSE 281.2275264380743 Test RE 0.2823046840712482 Lambda1 0.00020358138\n",
      "37 Train Loss 257.63925 Test MSE 281.1117804583259 Test RE 0.28224658343746123 Lambda1 0.0002307166\n",
      "38 Train Loss 257.62204 Test MSE 281.12220430103037 Test RE 0.28225181634999075 Lambda1 0.00021462292\n",
      "39 Train Loss 257.2844 Test MSE 281.2630373415982 Test RE 0.28232250696732775 Lambda1 9.737558e-05\n",
      "40 Train Loss 256.9468 Test MSE 281.2389115312607 Test RE 0.2823103983635995 Lambda1 6.164331e-05\n",
      "41 Train Loss 256.9148 Test MSE 281.19311372427904 Test RE 0.2822874112821477 Lambda1 6.086428e-05\n",
      "42 Train Loss 256.86023 Test MSE 281.1850225746151 Test RE 0.2822833499342482 Lambda1 3.6987407e-05\n",
      "43 Train Loss 256.714 Test MSE 281.1747977579864 Test RE 0.28227821750986926 Lambda1 4.8649918e-06\n",
      "44 Train Loss 256.56097 Test MSE 281.26619788960244 Test RE 0.2823240931894949 Lambda1 -2.4999638e-06\n",
      "45 Train Loss 256.43304 Test MSE 281.43507033305445 Test RE 0.28240883427405583 Lambda1 5.045358e-06\n",
      "46 Train Loss 256.26996 Test MSE 281.25216062832715 Test RE 0.2823170480725598 Lambda1 2.0490985e-05\n",
      "47 Train Loss 256.19614 Test MSE 281.1616032589411 Test RE 0.28227159429292975 Lambda1 3.063449e-05\n",
      "48 Train Loss 256.15518 Test MSE 281.1741868885764 Test RE 0.2822779108763744 Lambda1 6.192026e-05\n",
      "49 Train Loss 256.11288 Test MSE 281.31698473690443 Test RE 0.28234958097211016 Lambda1 9.543937e-05\n",
      "50 Train Loss 256.09866 Test MSE 281.33710127342755 Test RE 0.2823596759795744 Lambda1 0.00012510647\n",
      "51 Train Loss 256.03143 Test MSE 281.29012941830285 Test RE 0.28233610370387197 Lambda1 0.00023206908\n",
      "52 Train Loss 255.86792 Test MSE 281.4280702793663 Test RE 0.28240532211529695 Lambda1 0.00013125887\n",
      "53 Train Loss 255.78632 Test MSE 281.3666180116806 Test RE 0.28237448760239575 Lambda1 0.00015684585\n",
      "54 Train Loss 255.76929 Test MSE 281.40335420102247 Test RE 0.2823929208908608 Lambda1 0.0001484238\n",
      "55 Train Loss 255.75098 Test MSE 281.430943032378 Test RE 0.28240676347590554 Lambda1 0.00013734789\n",
      "56 Train Loss 255.69946 Test MSE 281.2842678505603 Test RE 0.28233316200658787 Lambda1 0.00016477016\n",
      "57 Train Loss 255.6293 Test MSE 281.2218690079695 Test RE 0.28230184450737095 Lambda1 0.0001699671\n",
      "58 Train Loss 255.34573 Test MSE 281.0924823188561 Test RE 0.2822368952497112 Lambda1 0.00017974523\n",
      "59 Train Loss 255.28903 Test MSE 281.20263615160763 Test RE 0.282292190984195 Lambda1 0.00020040521\n",
      "60 Train Loss 255.23918 Test MSE 281.22404252666036 Test RE 0.2823029354380871 Lambda1 0.00019371958\n",
      "61 Train Loss 255.22264 Test MSE 281.4385169055321 Test RE 0.2824105635175622 Lambda1 0.00023617987\n",
      "62 Train Loss 255.19537 Test MSE 281.39104460834795 Test RE 0.28238674438337796 Lambda1 0.00026712322\n",
      "63 Train Loss 255.17586 Test MSE 281.30958065729584 Test RE 0.2823458653192275 Lambda1 0.00029938\n",
      "64 Train Loss 255.16225 Test MSE 281.19137284382856 Test RE 0.28228653745314486 Lambda1 0.00036087714\n",
      "65 Train Loss 255.14561 Test MSE 281.0508553041708 Test RE 0.2822159962304909 Lambda1 0.0005345274\n",
      "66 Train Loss 255.0606 Test MSE 280.5448797407764 Test RE 0.28196184520930895 Lambda1 0.0010120606\n",
      "67 Train Loss 254.97964 Test MSE 280.1957477085268 Test RE 0.2817863428825414 Lambda1 0.0012738188\n",
      "68 Train Loss 254.95226 Test MSE 280.24598840524624 Test RE 0.2818116046999513 Lambda1 0.0012123081\n",
      "69 Train Loss 254.82957 Test MSE 280.783578157195 Test RE 0.28208177171788307 Lambda1 0.0008117235\n",
      "70 Train Loss 254.66978 Test MSE 281.4227232852211 Test RE 0.2824026393218306 Lambda1 0.0004750174\n",
      "71 Train Loss 254.6309 Test MSE 281.6682798304062 Test RE 0.2825258182450815 Lambda1 0.00031269874\n",
      "72 Train Loss 254.57521 Test MSE 281.6891144291593 Test RE 0.282536267066684 Lambda1 0.0002260023\n",
      "73 Train Loss 254.54625 Test MSE 281.6717780409355 Test RE 0.2825275726700457 Lambda1 0.0002008939\n",
      "74 Train Loss 254.49905 Test MSE 281.8451116220317 Test RE 0.28261448940809036 Lambda1 9.824908e-05\n",
      "Training time: 184.14\n",
      "Training time: 184.14\n",
      "inv_HT_swish_tune2\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 838.2405 Test MSE 858.8599789800858 Test RE 0.4933445713019819 Lambda1 1.7097097e-06\n",
      "1 Train Loss 838.06555 Test MSE 858.3005944538922 Test RE 0.49318388488272175 Lambda1 1.019019e-06\n",
      "2 Train Loss 838.05115 Test MSE 858.1824597938148 Test RE 0.4931499433266522 Lambda1 2.4654744e-06\n",
      "3 Train Loss 837.80725 Test MSE 857.7163564683761 Test RE 0.49301600325068234 Lambda1 0.009193601\n",
      "4 Train Loss 837.77844 Test MSE 857.919066916245 Test RE 0.4930742588674195 Lambda1 0.02452247\n",
      "5 Train Loss 837.466 Test MSE 858.2838526026218 Test RE 0.49317907488284096 Lambda1 0.13280956\n",
      "6 Train Loss 829.2676 Test MSE 851.1958858025007 Test RE 0.49113844236987436 Lambda1 0.31992677\n",
      "7 Train Loss 782.1425 Test MSE 796.7356357926437 Test RE 0.47516702214883977 Lambda1 0.099983476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 658.66693 Test MSE 687.073242183608 Test RE 0.4412560656057123 Lambda1 -0.003840747\n",
      "9 Train Loss 597.49536 Test MSE 627.0736879145519 Test RE 0.421549384653472 Lambda1 -0.0005223149\n",
      "10 Train Loss 532.5547 Test MSE 527.0359963733629 Test RE 0.38646423599669993 Lambda1 -0.0027981354\n",
      "11 Train Loss 450.39514 Test MSE 441.89465327190265 Test RE 0.35387390949478476 Lambda1 0.0005750996\n",
      "12 Train Loss 352.21088 Test MSE 335.30699349760243 Test RE 0.3082552460859613 Lambda1 0.004359597\n",
      "13 Train Loss 307.58444 Test MSE 303.70084110401797 Test RE 0.29336761352714424 Lambda1 0.007566481\n",
      "14 Train Loss 284.756 Test MSE 292.38536788197484 Test RE 0.2878504999832656 Lambda1 0.007393276\n",
      "15 Train Loss 270.89697 Test MSE 292.45267672726055 Test RE 0.2878836305225674 Lambda1 0.0020214377\n",
      "16 Train Loss 264.66846 Test MSE 287.8097030160716 Test RE 0.2855892699769037 Lambda1 0.00034983276\n",
      "17 Train Loss 263.4566 Test MSE 287.58714366437357 Test RE 0.2854788274614222 Lambda1 0.000286546\n",
      "18 Train Loss 262.14032 Test MSE 286.83015020535294 Test RE 0.2851028579390658 Lambda1 7.1667764e-06\n",
      "19 Train Loss 259.51373 Test MSE 285.0070778611421 Test RE 0.2841953667550859 Lambda1 0.00038702003\n",
      "20 Train Loss 258.54074 Test MSE 283.7219611238011 Test RE 0.2835539145121814 Lambda1 0.0003899589\n",
      "21 Train Loss 258.18164 Test MSE 282.7499957964186 Test RE 0.2830678030094017 Lambda1 0.00021005783\n",
      "22 Train Loss 257.8088 Test MSE 281.5164736179771 Test RE 0.28244967379106883 Lambda1 0.00068789045\n",
      "23 Train Loss 257.38748 Test MSE 280.7459922014117 Test RE 0.2820628912194199 Lambda1 0.0018638598\n",
      "24 Train Loss 257.14685 Test MSE 280.8920786013854 Test RE 0.2821362675000216 Lambda1 0.001856102\n",
      "25 Train Loss 257.0813 Test MSE 280.8460484211728 Test RE 0.2821131495192896 Lambda1 0.0016462805\n",
      "26 Train Loss 256.94687 Test MSE 280.0082797520531 Test RE 0.2816920610292543 Lambda1 0.0023513148\n",
      "27 Train Loss 256.86942 Test MSE 279.44480126414373 Test RE 0.28140848485872816 Lambda1 0.0029840532\n",
      "28 Train Loss 256.76315 Test MSE 279.0873672449079 Test RE 0.2812284544016666 Lambda1 0.0034986543\n",
      "29 Train Loss 256.66882 Test MSE 278.96306563481875 Test RE 0.2811658198165659 Lambda1 0.0041358126\n",
      "30 Train Loss 256.15747 Test MSE 278.2446663526794 Test RE 0.28080355047176503 Lambda1 0.006300924\n",
      "31 Train Loss 255.86903 Test MSE 277.88564421829267 Test RE 0.2806223300499549 Lambda1 0.0059889583\n",
      "32 Train Loss 255.66331 Test MSE 277.7579475852959 Test RE 0.28055784553062507 Lambda1 0.006835287\n",
      "33 Train Loss 254.85402 Test MSE 277.79824915043497 Test RE 0.2805781987019164 Lambda1 0.008630674\n",
      "34 Train Loss 254.45834 Test MSE 277.97771325206844 Test RE 0.2806688140760798 Lambda1 0.008683332\n",
      "35 Train Loss 253.98497 Test MSE 276.6226223640494 Test RE 0.27998387355812393 Lambda1 0.011557792\n",
      "36 Train Loss 253.84792 Test MSE 276.4000095123458 Test RE 0.27987119196921356 Lambda1 0.013007483\n",
      "37 Train Loss 253.36949 Test MSE 274.6599770569919 Test RE 0.2789888587623928 Lambda1 0.017691437\n",
      "38 Train Loss 251.71198 Test MSE 271.7396728204916 Test RE 0.2775017298545754 Lambda1 0.027290843\n",
      "39 Train Loss 248.16695 Test MSE 267.38635367575927 Test RE 0.275269941175758 Lambda1 0.046795852\n",
      "40 Train Loss 244.22414 Test MSE 262.26792053660455 Test RE 0.2726225382979597 Lambda1 0.069233164\n",
      "41 Train Loss 241.993 Test MSE 258.68667529857333 Test RE 0.27075482219459474 Lambda1 0.08772974\n",
      "42 Train Loss 238.77724 Test MSE 256.1785132566826 Test RE 0.26943903914411804 Lambda1 0.1107933\n",
      "43 Train Loss 234.96002 Test MSE 252.04387845509825 Test RE 0.267255866786259 Lambda1 0.13760258\n",
      "44 Train Loss 232.39218 Test MSE 248.92303567106873 Test RE 0.265596113074184 Lambda1 0.14163321\n",
      "45 Train Loss 228.8509 Test MSE 246.54651747990428 Test RE 0.2643252227340184 Lambda1 0.17896304\n",
      "46 Train Loss 226.15697 Test MSE 243.57785874033212 Test RE 0.2627290374977518 Lambda1 0.21410525\n",
      "47 Train Loss 224.57996 Test MSE 244.03259780886023 Test RE 0.26297416948316366 Lambda1 0.25808772\n",
      "48 Train Loss 223.5571 Test MSE 245.91279605871765 Test RE 0.26398529431648676 Lambda1 0.29620218\n",
      "49 Train Loss 221.43217 Test MSE 246.10132053054986 Test RE 0.26408646463837787 Lambda1 0.3325712\n",
      "50 Train Loss 220.06277 Test MSE 246.23904566767882 Test RE 0.2641603493645904 Lambda1 0.3525491\n",
      "51 Train Loss 219.37143 Test MSE 244.878920474857 Test RE 0.2634297815032006 Lambda1 0.36345208\n",
      "52 Train Loss 217.46898 Test MSE 239.59477888894165 Test RE 0.2605720595294977 Lambda1 0.36557382\n",
      "53 Train Loss 215.23135 Test MSE 231.35137326891495 Test RE 0.2560502543073656 Lambda1 0.35488477\n",
      "54 Train Loss 211.86125 Test MSE 221.59359710797455 Test RE 0.2505923300771584 Lambda1 0.34529793\n",
      "55 Train Loss 209.02756 Test MSE 223.96544439626715 Test RE 0.25192987939483613 Lambda1 0.34455898\n",
      "56 Train Loss 207.354 Test MSE 222.6800619282507 Test RE 0.2512059011601357 Lambda1 0.34592384\n",
      "57 Train Loss 204.2983 Test MSE 222.48631351665892 Test RE 0.25109659335924545 Lambda1 0.3539156\n",
      "58 Train Loss 200.90448 Test MSE 219.81774556674557 Test RE 0.2495861864896977 Lambda1 0.35258636\n",
      "59 Train Loss 198.0318 Test MSE 214.8060563839352 Test RE 0.24672458754433813 Lambda1 0.36323404\n",
      "60 Train Loss 195.41266 Test MSE 207.44564035833702 Test RE 0.24246068440635282 Lambda1 0.3754715\n",
      "61 Train Loss 185.74516 Test MSE 193.55640864926212 Test RE 0.23420326666810945 Lambda1 0.38733366\n",
      "62 Train Loss 173.49461 Test MSE 186.1442595691918 Test RE 0.22967514249521423 Lambda1 0.4637061\n",
      "63 Train Loss 166.51416 Test MSE 179.6922261480851 Test RE 0.2256596006095104 Lambda1 0.51061016\n",
      "64 Train Loss 156.72398 Test MSE 162.1541665740826 Test RE 0.21436467845230414 Lambda1 0.55635715\n",
      "65 Train Loss 148.08855 Test MSE 151.97716840161544 Test RE 0.2075287857028195 Lambda1 0.59508616\n",
      "66 Train Loss 141.60783 Test MSE 142.30191740183983 Test RE 0.20081425878297088 Lambda1 0.596278\n",
      "67 Train Loss 137.09805 Test MSE 138.16396703704135 Test RE 0.197873013478933 Lambda1 0.60399365\n",
      "68 Train Loss 135.0769 Test MSE 135.5549005060498 Test RE 0.19599580765988533 Lambda1 0.60870093\n",
      "69 Train Loss 133.20969 Test MSE 132.69150556537724 Test RE 0.19391469948109719 Lambda1 0.6051894\n",
      "70 Train Loss 130.00378 Test MSE 130.27432826170727 Test RE 0.1921403563705074 Lambda1 0.62512976\n",
      "71 Train Loss 127.41202 Test MSE 131.1062387970984 Test RE 0.19275286850803403 Lambda1 0.6607474\n",
      "72 Train Loss 124.86678 Test MSE 128.77338631688113 Test RE 0.19103028728735552 Lambda1 0.6879416\n",
      "73 Train Loss 122.18137 Test MSE 121.04686734358008 Test RE 0.1852106465809885 Lambda1 0.6928241\n",
      "74 Train Loss 113.60297 Test MSE 112.80209495089034 Test RE 0.17879186448543155 Lambda1 0.65964234\n",
      "Training time: 187.17\n",
      "Training time: 187.17\n",
      "inv_HT_swish_tune2\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 838.0577 Test MSE 858.2554134898098 Test RE 0.4931709041060692 Lambda1 5.724626e-07\n",
      "1 Train Loss 838.0558 Test MSE 858.2969423999965 Test RE 0.4931828356372384 Lambda1 3.3399633e-06\n",
      "2 Train Loss 838.012 Test MSE 858.4201713000082 Test RE 0.4932182384180788 Lambda1 0.0001397918\n",
      "3 Train Loss 837.8866 Test MSE 857.9156714172311 Test RE 0.4930732831140643 Lambda1 0.0055989753\n",
      "4 Train Loss 837.8836 Test MSE 857.8869730040864 Test RE 0.49306503606857915 Lambda1 0.007579032\n",
      "5 Train Loss 837.8762 Test MSE 857.884546687381 Test RE 0.493064338813103 Lambda1 0.02808955\n",
      "6 Train Loss 836.14136 Test MSE 854.8205732366532 Test RE 0.49218305042806737 Lambda1 0.64271975\n",
      "7 Train Loss 793.895 Test MSE 820.8194089372809 Test RE 0.48229524390132283 Lambda1 0.22273038\n",
      "8 Train Loss 703.2995 Test MSE 729.0537364841598 Test RE 0.454536685455122 Lambda1 0.009613261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 628.23285 Test MSE 660.6841133356394 Test RE 0.4326992103361472 Lambda1 0.00050368335\n",
      "10 Train Loss 607.9629 Test MSE 651.1024373648352 Test RE 0.4295501066358909 Lambda1 0.00029337054\n",
      "11 Train Loss 603.07196 Test MSE 643.830654684978 Test RE 0.42714467448536464 Lambda1 0.00029839386\n",
      "12 Train Loss 574.61694 Test MSE 603.6643738226904 Test RE 0.4136061080071781 Lambda1 9.906083e-05\n",
      "13 Train Loss 497.74362 Test MSE 506.4216601045109 Test RE 0.3788308225025599 Lambda1 -9.511959e-05\n",
      "14 Train Loss 381.50592 Test MSE 372.81779740487013 Test RE 0.3250405085431411 Lambda1 -0.00031783155\n",
      "15 Train Loss 334.13928 Test MSE 340.53309601198725 Test RE 0.31064819467761573 Lambda1 8.7908884e-05\n",
      "16 Train Loss 298.25995 Test MSE 312.85957934578556 Test RE 0.29775831569198 Lambda1 0.00036016724\n",
      "17 Train Loss 281.77408 Test MSE 308.21119681484583 Test RE 0.295538031703525 Lambda1 -0.00010663318\n",
      "18 Train Loss 275.4828 Test MSE 302.9309139357264 Test RE 0.2929955121007355 Lambda1 -0.0005557007\n",
      "19 Train Loss 272.01733 Test MSE 300.69658498006146 Test RE 0.2919129881291797 Lambda1 4.974165e-05\n",
      "20 Train Loss 269.89835 Test MSE 297.7255869069988 Test RE 0.2904673019038956 Lambda1 -0.00022565309\n",
      "21 Train Loss 268.1934 Test MSE 294.59503383373413 Test RE 0.28893614980486615 Lambda1 -0.00041394564\n",
      "22 Train Loss 264.7059 Test MSE 290.0353860347406 Test RE 0.28669139935021853 Lambda1 -0.00024992207\n",
      "23 Train Loss 263.33218 Test MSE 289.19059627083055 Test RE 0.28627357002850085 Lambda1 -0.00010982396\n",
      "24 Train Loss 261.57968 Test MSE 286.56709211573724 Test RE 0.28497209097349546 Lambda1 5.640091e-05\n",
      "25 Train Loss 261.346 Test MSE 286.25139381511997 Test RE 0.28481507713860355 Lambda1 6.125197e-05\n",
      "26 Train Loss 260.86542 Test MSE 286.3792167840662 Test RE 0.28487866083858177 Lambda1 0.0001388236\n",
      "27 Train Loss 260.09583 Test MSE 284.59388946948815 Test RE 0.28398928622917563 Lambda1 0.00031533366\n",
      "28 Train Loss 259.61417 Test MSE 283.434275965724 Test RE 0.28341012067028165 Lambda1 0.00058492256\n",
      "29 Train Loss 259.00333 Test MSE 283.09480855817526 Test RE 0.28324035058346336 Lambda1 0.0005407868\n",
      "30 Train Loss 258.6521 Test MSE 283.22419023724296 Test RE 0.2833050672877808 Lambda1 0.0003501291\n",
      "31 Train Loss 258.06415 Test MSE 282.32259974778333 Test RE 0.28285378388462423 Lambda1 0.0006854306\n",
      "32 Train Loss 257.48227 Test MSE 281.5290047474287 Test RE 0.282455960055588 Lambda1 0.0005249916\n",
      "33 Train Loss 257.3542 Test MSE 281.55289083718793 Test RE 0.28246794216968857 Lambda1 0.00045845401\n",
      "34 Train Loss 257.31998 Test MSE 281.3932633805096 Test RE 0.2823878576928172 Lambda1 0.0006497106\n",
      "35 Train Loss 257.2336 Test MSE 280.89215268493933 Test RE 0.2821363047058697 Lambda1 0.0009275734\n",
      "36 Train Loss 257.18036 Test MSE 280.88760514971733 Test RE 0.28213402085784517 Lambda1 0.0008630184\n",
      "37 Train Loss 257.0183 Test MSE 281.07783490701655 Test RE 0.28222954163096015 Lambda1 0.00075659563\n",
      "38 Train Loss 256.5207 Test MSE 280.86132852893405 Test RE 0.2821208239386335 Lambda1 0.00044385827\n",
      "39 Train Loss 256.42154 Test MSE 280.8375807796798 Test RE 0.2821088965646364 Lambda1 0.0003601302\n",
      "40 Train Loss 256.37238 Test MSE 280.8045923155591 Test RE 0.2820923271787053 Lambda1 0.00038419076\n",
      "41 Train Loss 256.00455 Test MSE 280.8132957405894 Test RE 0.28209669881375765 Lambda1 0.00013967649\n",
      "42 Train Loss 254.90738 Test MSE 282.3751059250602 Test RE 0.28288008514509033 Lambda1 9.567279e-05\n",
      "43 Train Loss 254.72823 Test MSE 282.03382186922653 Test RE 0.28270908626954583 Lambda1 0.00012480632\n",
      "44 Train Loss 254.62184 Test MSE 281.8264743808148 Test RE 0.2826051451951345 Lambda1 0.00015714823\n",
      "45 Train Loss 254.49002 Test MSE 282.19317718642975 Test RE 0.28278894341910327 Lambda1 0.00014509886\n",
      "46 Train Loss 254.4707 Test MSE 282.26494813025295 Test RE 0.2828249023661845 Lambda1 0.00012707435\n",
      "47 Train Loss 254.4509 Test MSE 282.3667925444346 Test RE 0.28287592099060377 Lambda1 0.00013492735\n",
      "48 Train Loss 254.43181 Test MSE 282.48177319265335 Test RE 0.28293350911214793 Lambda1 0.00013961614\n",
      "49 Train Loss 254.34775 Test MSE 282.54409294800274 Test RE 0.28296471709857013 Lambda1 0.00011544338\n",
      "50 Train Loss 254.25157 Test MSE 282.6374126372868 Test RE 0.283011442548004 Lambda1 0.00013376906\n",
      "51 Train Loss 254.24095 Test MSE 282.6480194948704 Test RE 0.28301675294529816 Lambda1 0.00013037714\n",
      "52 Train Loss 254.23859 Test MSE 282.59751500204834 Test RE 0.282991466626013 Lambda1 0.00013451763\n",
      "53 Train Loss 254.22702 Test MSE 282.51060561639656 Test RE 0.2829479480095409 Lambda1 0.0001414795\n",
      "54 Train Loss 254.19319 Test MSE 282.5475078680978 Test RE 0.28296642709535114 Lambda1 0.000116954485\n",
      "55 Train Loss 254.15327 Test MSE 282.7944793052165 Test RE 0.2830900688875273 Lambda1 0.000118907796\n",
      "56 Train Loss 254.13953 Test MSE 282.8048087322704 Test RE 0.28309523895223615 Lambda1 0.00012896143\n",
      "57 Train Loss 254.13138 Test MSE 282.79760737858663 Test RE 0.2830916345546848 Lambda1 0.00012925733\n",
      "58 Train Loss 254.09908 Test MSE 282.68795072916737 Test RE 0.28303674390291095 Lambda1 0.00012801355\n",
      "59 Train Loss 254.08687 Test MSE 282.71719915004036 Test RE 0.28305138577863737 Lambda1 0.00012786259\n",
      "60 Train Loss 254.08131 Test MSE 282.7742638596365 Test RE 0.2830799504188564 Lambda1 0.00012595722\n",
      "61 Train Loss 254.07118 Test MSE 282.7038105504208 Test RE 0.2830446834864616 Lambda1 0.00012633658\n",
      "62 Train Loss 254.04175 Test MSE 282.7169063842621 Test RE 0.283051239222678 Lambda1 0.00011927989\n",
      "63 Train Loss 253.98105 Test MSE 282.8272676077415 Test RE 0.28310647969902863 Lambda1 7.980815e-05\n",
      "64 Train Loss 253.95915 Test MSE 282.79382859011866 Test RE 0.2830897431897127 Lambda1 7.492992e-05\n",
      "65 Train Loss 253.94498 Test MSE 282.93456794095005 Test RE 0.2831601777374579 Lambda1 6.331945e-05\n",
      "66 Train Loss 253.91348 Test MSE 283.03102764115556 Test RE 0.2832084419323863 Lambda1 4.2878357e-05\n",
      "67 Train Loss 253.8311 Test MSE 282.9560898080241 Test RE 0.28317094704688633 Lambda1 3.203831e-05\n",
      "68 Train Loss 253.74135 Test MSE 283.204134012912 Test RE 0.283295036134414 Lambda1 4.435651e-05\n",
      "69 Train Loss 253.66667 Test MSE 283.54149900479604 Test RE 0.28346372255193814 Lambda1 3.0271054e-05\n",
      "70 Train Loss 253.65092 Test MSE 283.7378691235399 Test RE 0.2835618636895261 Lambda1 3.7605518e-05\n",
      "71 Train Loss 253.63838 Test MSE 283.8103900111694 Test RE 0.2835980993253965 Lambda1 3.8972325e-05\n",
      "72 Train Loss 253.54732 Test MSE 283.8007803902013 Test RE 0.2835932980682772 Lambda1 3.3082164e-05\n",
      "73 Train Loss 253.30531 Test MSE 283.53970284551315 Test RE 0.28346282471722123 Lambda1 1.2342787e-05\n",
      "74 Train Loss 252.9828 Test MSE 283.4258936489872 Test RE 0.28340592983811363 Lambda1 4.101563e-05\n",
      "Training time: 201.67\n",
      "Training time: 201.67\n",
      "inv_HT_swish_tune2\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 838.0683 Test MSE 858.2544992407084 Test RE 0.4931706414329947 Lambda1 -2.6413102e-07\n",
      "1 Train Loss 838.06415 Test MSE 858.3141041960582 Test RE 0.4931877662507122 Lambda1 -1.1442023e-06\n",
      "2 Train Loss 837.9989 Test MSE 858.3834258686187 Test RE 0.49320768198458736 Lambda1 1.8948867e-05\n",
      "3 Train Loss 837.85925 Test MSE 857.8650800128447 Test RE 0.49305874460028215 Lambda1 0.008249951\n",
      "4 Train Loss 837.8474 Test MSE 857.8682423944513 Test RE 0.4930596533903146 Lambda1 0.027397737\n",
      "5 Train Loss 837.56885 Test MSE 857.6884089628782 Test RE 0.49300797106172023 Lambda1 0.30253172\n",
      "6 Train Loss 823.9964 Test MSE 844.7392644956853 Test RE 0.4892721673037648 Lambda1 0.4656345\n",
      "7 Train Loss 762.1079 Test MSE 783.5772298680447 Test RE 0.4712269000651083 Lambda1 0.058727298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 669.22675 Test MSE 692.4872647436401 Test RE 0.4429911663393682 Lambda1 0.0018918293\n",
      "9 Train Loss 617.0746 Test MSE 642.4887449279872 Test RE 0.42669930225688785 Lambda1 0.00090226904\n",
      "10 Train Loss 579.9617 Test MSE 611.5066465820905 Test RE 0.41628404081065246 Lambda1 0.0029649313\n",
      "11 Train Loss 533.9975 Test MSE 498.49482283318366 Test RE 0.3758542771196618 Lambda1 0.0012026872\n",
      "12 Train Loss 393.80045 Test MSE 420.06827620719747 Test RE 0.3450238467716029 Lambda1 0.0019053335\n",
      "13 Train Loss 325.50974 Test MSE 352.2322334392115 Test RE 0.31593934898080134 Lambda1 0.0019091669\n",
      "14 Train Loss 283.01663 Test MSE 303.13235837272765 Test RE 0.29309291468378407 Lambda1 0.0018507884\n",
      "15 Train Loss 278.76596 Test MSE 296.38799224142633 Test RE 0.2898140747392917 Lambda1 0.0009986166\n",
      "16 Train Loss 269.7915 Test MSE 288.6868143887636 Test RE 0.2860241111879171 Lambda1 -0.0001711362\n",
      "17 Train Loss 265.3295 Test MSE 283.90242489858724 Test RE 0.28364407862070024 Lambda1 -0.0005601836\n",
      "18 Train Loss 260.43167 Test MSE 281.24565219330674 Test RE 0.28231378151491165 Lambda1 0.0012713154\n",
      "19 Train Loss 258.1908 Test MSE 278.36964889147754 Test RE 0.2808666093632681 Lambda1 0.0040364624\n",
      "20 Train Loss 257.34717 Test MSE 278.1760491831508 Test RE 0.28076892423232963 Lambda1 0.0044081057\n",
      "21 Train Loss 257.0868 Test MSE 278.8151906759186 Test RE 0.28109128863206434 Lambda1 0.0033586246\n",
      "22 Train Loss 256.60687 Test MSE 278.4642355537407 Test RE 0.2809143228602101 Lambda1 0.0034530254\n",
      "23 Train Loss 256.36633 Test MSE 277.8056516657529 Test RE 0.28058193697345685 Lambda1 0.00423249\n",
      "24 Train Loss 256.14978 Test MSE 277.3662069679365 Test RE 0.28035993097258516 Lambda1 0.0049711713\n",
      "25 Train Loss 255.91556 Test MSE 277.29764034912176 Test RE 0.2803252754878629 Lambda1 0.00493579\n",
      "26 Train Loss 255.14975 Test MSE 275.4767523319473 Test RE 0.2794033750064793 Lambda1 0.00861118\n",
      "27 Train Loss 250.33382 Test MSE 267.12751002656717 Test RE 0.27513667123261654 Lambda1 0.024817144\n",
      "28 Train Loss 239.48573 Test MSE 252.5729629768575 Test RE 0.26753622831446106 Lambda1 0.050356016\n",
      "29 Train Loss 231.208 Test MSE 241.55186232037408 Test RE 0.26163411128713426 Lambda1 0.07242813\n",
      "30 Train Loss 220.22034 Test MSE 235.1720547535644 Test RE 0.258155883623924 Lambda1 0.09440658\n",
      "31 Train Loss 203.29303 Test MSE 211.84957596312933 Test RE 0.245020809391112 Lambda1 0.14031962\n",
      "32 Train Loss 172.75655 Test MSE 164.34140173365103 Test RE 0.21580557712825324 Lambda1 0.2245207\n",
      "33 Train Loss 130.08475 Test MSE 119.88058314839711 Test RE 0.18431623648499768 Lambda1 0.30494973\n",
      "34 Train Loss 102.05572 Test MSE 82.26563087762347 Test RE 0.15268570788642163 Lambda1 0.40269777\n",
      "35 Train Loss 81.300224 Test MSE 62.392650608198345 Test RE 0.13297067967541173 Lambda1 0.4809648\n",
      "36 Train Loss 71.82375 Test MSE 59.825995029663666 Test RE 0.1302069409791562 Lambda1 0.4904941\n",
      "37 Train Loss 66.623924 Test MSE 59.09445409213453 Test RE 0.12940841952071974 Lambda1 0.5071282\n",
      "38 Train Loss 63.32573 Test MSE 54.49461437243549 Test RE 0.12426990389408422 Lambda1 0.5332485\n",
      "39 Train Loss 55.05948 Test MSE 48.75414738822155 Test RE 0.11754250815770322 Lambda1 0.6113724\n",
      "40 Train Loss 48.932796 Test MSE 45.35613582871384 Test RE 0.11337236184742128 Lambda1 0.6510513\n",
      "41 Train Loss 46.3395 Test MSE 43.02614743124784 Test RE 0.11042194716059861 Lambda1 0.6860282\n",
      "42 Train Loss 44.63817 Test MSE 42.32062913605521 Test RE 0.10951288705039638 Lambda1 0.70107317\n",
      "43 Train Loss 41.05749 Test MSE 40.644403062749085 Test RE 0.10732219473478849 Lambda1 0.7245632\n",
      "44 Train Loss 38.918583 Test MSE 39.491325791755806 Test RE 0.1057888820468669 Lambda1 0.74864966\n",
      "45 Train Loss 37.25237 Test MSE 38.3721310288316 Test RE 0.10427906536830969 Lambda1 0.7778273\n",
      "46 Train Loss 36.759468 Test MSE 38.0466468867276 Test RE 0.10383586006574903 Lambda1 0.7845833\n",
      "47 Train Loss 36.240433 Test MSE 37.771178425463425 Test RE 0.10345927671891521 Lambda1 0.7918707\n",
      "48 Train Loss 34.554577 Test MSE 36.59025220732329 Test RE 0.10182909221045776 Lambda1 0.8450608\n",
      "49 Train Loss 33.879852 Test MSE 35.9606171340286 Test RE 0.10094916679215468 Lambda1 0.8711136\n",
      "50 Train Loss 33.38257 Test MSE 35.748210042084054 Test RE 0.1006505891030442 Lambda1 0.88300854\n",
      "51 Train Loss 32.97216 Test MSE 35.56224885310656 Test RE 0.10038845698259526 Lambda1 0.90836227\n",
      "52 Train Loss 32.669342 Test MSE 35.16547598375987 Test RE 0.09982686227621913 Lambda1 0.93169457\n",
      "53 Train Loss 32.26673 Test MSE 34.67968821920571 Test RE 0.09913494336655322 Lambda1 0.95282024\n",
      "54 Train Loss 32.02931 Test MSE 34.50062207760033 Test RE 0.09887867396216588 Lambda1 0.9705795\n",
      "55 Train Loss 31.947989 Test MSE 34.50759254432951 Test RE 0.0988886621255127 Lambda1 0.9783296\n",
      "56 Train Loss 31.897392 Test MSE 34.48367875320933 Test RE 0.09885439122336558 Lambda1 0.97737765\n",
      "57 Train Loss 31.725374 Test MSE 34.169665590120715 Test RE 0.09840327098289729 Lambda1 0.9946584\n",
      "58 Train Loss 31.560667 Test MSE 33.853652077817436 Test RE 0.09794717936330485 Lambda1 1.0176609\n",
      "59 Train Loss 31.486126 Test MSE 33.785747071621074 Test RE 0.09784889687345937 Lambda1 1.0183016\n",
      "60 Train Loss 31.401617 Test MSE 33.89668213614696 Test RE 0.09800940801747472 Lambda1 1.014658\n",
      "61 Train Loss 31.355888 Test MSE 33.86583758734635 Test RE 0.09796480565812364 Lambda1 1.0155185\n",
      "62 Train Loss 31.30777 Test MSE 33.793253656882165 Test RE 0.09785976640212882 Lambda1 1.018009\n",
      "63 Train Loss 31.260912 Test MSE 33.66075292006917 Test RE 0.09766772768315964 Lambda1 1.0234442\n",
      "64 Train Loss 31.192825 Test MSE 33.56377152079485 Test RE 0.09752692891266199 Lambda1 1.0314177\n",
      "65 Train Loss 31.120949 Test MSE 33.571537146615924 Test RE 0.09753821062804799 Lambda1 1.0406779\n",
      "66 Train Loss 31.052681 Test MSE 33.411099315796044 Test RE 0.09730486474283614 Lambda1 1.0440419\n",
      "67 Train Loss 30.984308 Test MSE 33.23725515038287 Test RE 0.09705138693302309 Lambda1 1.044645\n",
      "68 Train Loss 30.962135 Test MSE 33.18689587358012 Test RE 0.09697783557724608 Lambda1 1.0514386\n",
      "69 Train Loss 30.939074 Test MSE 33.17510974772303 Test RE 0.0969606135014062 Lambda1 1.0573926\n",
      "70 Train Loss 30.91958 Test MSE 33.14645255760447 Test RE 0.09691872638981278 Lambda1 1.0566628\n",
      "71 Train Loss 30.859943 Test MSE 33.13247152719627 Test RE 0.09689828428406486 Lambda1 1.0616989\n",
      "72 Train Loss 30.825329 Test MSE 33.1633370320109 Test RE 0.09694340796638866 Lambda1 1.0639172\n",
      "73 Train Loss 30.79694 Test MSE 33.152576800692096 Test RE 0.09692767948102121 Lambda1 1.0595723\n",
      "74 Train Loss 30.765139 Test MSE 33.12343269665176 Test RE 0.09688506602906935 Lambda1 1.0577059\n",
      "Training time: 237.60\n",
      "Training time: 237.60\n",
      "inv_HT_swish_tune2\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 838.058 Test MSE 858.2779546306024 Test RE 0.49317738036105047 Lambda1 -8.194605e-07\n",
      "1 Train Loss 838.045 Test MSE 858.3530074335098 Test RE 0.493198943035109 Lambda1 -1.5661917e-05\n",
      "2 Train Loss 837.9141 Test MSE 857.9754287774308 Test RE 0.4930904551075933 Lambda1 -0.0010224695\n",
      "3 Train Loss 837.9102 Test MSE 857.942721722991 Test RE 0.49308105641765343 Lambda1 -0.0015138461\n",
      "4 Train Loss 837.9081 Test MSE 857.9354217563999 Test RE 0.4930789586768579 Lambda1 -0.007884255\n",
      "5 Train Loss 837.82184 Test MSE 857.8422014485581 Test RE 0.4930521698185015 Lambda1 -0.28732297\n",
      "6 Train Loss 837.4115 Test MSE 856.7515587857371 Test RE 0.49273864201684514 Lambda1 -0.8350889\n",
      "7 Train Loss 836.0804 Test MSE 855.5884907532785 Test RE 0.4924040740865529 Lambda1 -0.7303202\n",
      "8 Train Loss 830.2749 Test MSE 841.5319417796627 Test RE 0.48834244485542305 Lambda1 -1.082384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 814.0527 Test MSE 834.8372110530365 Test RE 0.4863960841780086 Lambda1 -1.1600634\n",
      "10 Train Loss 794.8139 Test MSE 815.5636825882098 Test RE 0.48074869026741984 Lambda1 -1.0161977\n",
      "11 Train Loss 766.38947 Test MSE 785.7037630099717 Test RE 0.47186589302118015 Lambda1 -0.8388557\n",
      "12 Train Loss 696.48804 Test MSE 707.3894448381341 Test RE 0.44773233318318584 Lambda1 -0.64550126\n",
      "13 Train Loss 621.33325 Test MSE 624.3519097923288 Test RE 0.4206335340595368 Lambda1 -0.24844524\n",
      "14 Train Loss 503.40958 Test MSE 493.7190746141024 Test RE 0.3740495389649086 Lambda1 -0.08252758\n",
      "15 Train Loss 351.39026 Test MSE 350.2446936200645 Test RE 0.31504671294897063 Lambda1 -0.0061690477\n",
      "16 Train Loss 302.18173 Test MSE 308.3478256890575 Test RE 0.29560352989924116 Lambda1 0.003299393\n",
      "17 Train Loss 283.64728 Test MSE 292.4165413006375 Test RE 0.2878658445353006 Lambda1 0.0131503\n",
      "18 Train Loss 277.2994 Test MSE 288.032952980039 Test RE 0.28570001230829367 Lambda1 0.012438419\n",
      "19 Train Loss 269.2677 Test MSE 282.9714488867907 Test RE 0.2831786323132903 Lambda1 0.024044259\n",
      "20 Train Loss 246.82867 Test MSE 265.3930748248549 Test RE 0.2742419976214021 Lambda1 0.06926936\n",
      "21 Train Loss 234.89873 Test MSE 248.24573354328996 Test RE 0.26523453274675496 Lambda1 0.09606001\n",
      "22 Train Loss 231.09363 Test MSE 241.3547659298882 Test RE 0.2615273481640363 Lambda1 0.11052025\n",
      "23 Train Loss 219.48691 Test MSE 232.81035565078082 Test RE 0.25685635648085475 Lambda1 0.16008112\n",
      "24 Train Loss 211.92154 Test MSE 227.40670020955037 Test RE 0.25385796765067253 Lambda1 0.2053374\n",
      "25 Train Loss 208.52844 Test MSE 223.97737728105736 Test RE 0.251936590720755 Lambda1 0.22526231\n",
      "26 Train Loss 202.95584 Test MSE 218.93469204648417 Test RE 0.24908436223563973 Lambda1 0.26357067\n",
      "27 Train Loss 191.47021 Test MSE 200.30191504789107 Test RE 0.23824934831939729 Lambda1 0.33999404\n",
      "28 Train Loss 187.57532 Test MSE 190.54730173906844 Test RE 0.2323756257552795 Lambda1 0.37292296\n",
      "29 Train Loss 183.50546 Test MSE 180.31591462231034 Test RE 0.2260508790270216 Lambda1 0.40111345\n",
      "30 Train Loss 171.39151 Test MSE 167.09072431925276 Test RE 0.21760323196213294 Lambda1 0.46286058\n",
      "31 Train Loss 163.30148 Test MSE 155.90393941121826 Test RE 0.21019274177937886 Lambda1 0.51838857\n",
      "32 Train Loss 142.38036 Test MSE 139.1604094804443 Test RE 0.19858526448115302 Lambda1 0.5853158\n",
      "33 Train Loss 132.06233 Test MSE 134.08762509427555 Test RE 0.19493217117293338 Lambda1 0.6085145\n",
      "34 Train Loss 127.3529 Test MSE 128.92199591334997 Test RE 0.19114048376875792 Lambda1 0.66476864\n",
      "35 Train Loss 124.41866 Test MSE 125.27655511238939 Test RE 0.18841872987379268 Lambda1 0.7005911\n",
      "36 Train Loss 118.04819 Test MSE 115.05456765959265 Test RE 0.18056813108588668 Lambda1 0.7271571\n",
      "37 Train Loss 109.13375 Test MSE 106.6393011531556 Test RE 0.17383924046000965 Lambda1 0.68425024\n",
      "38 Train Loss 101.19463 Test MSE 98.20304565561098 Test RE 0.16682135684026095 Lambda1 0.6753918\n",
      "39 Train Loss 90.89243 Test MSE 92.56131458036612 Test RE 0.16195856778592518 Lambda1 0.67478704\n",
      "40 Train Loss 87.73638 Test MSE 86.49146951078687 Test RE 0.1565581960871238 Lambda1 0.68304276\n",
      "41 Train Loss 85.745636 Test MSE 80.55275368315532 Test RE 0.1510877891556053 Lambda1 0.70012426\n",
      "42 Train Loss 81.08653 Test MSE 74.49050019032778 Test RE 0.14529130208321817 Lambda1 0.72244585\n",
      "43 Train Loss 76.497246 Test MSE 73.67061079375439 Test RE 0.14448950588606974 Lambda1 0.7369541\n",
      "44 Train Loss 72.7006 Test MSE 72.38042329196215 Test RE 0.14321870086631375 Lambda1 0.7726725\n",
      "45 Train Loss 69.50624 Test MSE 69.2460138351615 Test RE 0.14008336311492053 Lambda1 0.7963338\n",
      "46 Train Loss 67.951965 Test MSE 66.73289820218956 Test RE 0.13751787853462843 Lambda1 0.8053584\n",
      "47 Train Loss 60.474495 Test MSE 58.01276435516895 Test RE 0.12821857646331045 Lambda1 0.8271064\n",
      "48 Train Loss 58.416157 Test MSE 57.21511147932727 Test RE 0.12733404775778287 Lambda1 0.81654906\n",
      "49 Train Loss 54.7715 Test MSE 53.73937328648162 Test RE 0.12340577088635851 Lambda1 0.8106135\n",
      "50 Train Loss 52.997448 Test MSE 50.670583720913974 Test RE 0.11983043181165783 Lambda1 0.81066966\n",
      "51 Train Loss 50.942177 Test MSE 47.34413035870947 Test RE 0.11583031634247498 Lambda1 0.81852704\n",
      "52 Train Loss 48.924652 Test MSE 47.097465236110615 Test RE 0.11552818162508253 Lambda1 0.8333448\n",
      "53 Train Loss 46.22675 Test MSE 45.6908834344244 Test RE 0.11378996091462965 Lambda1 0.8472151\n",
      "54 Train Loss 45.3797 Test MSE 44.83922315839274 Test RE 0.11272447220415395 Lambda1 0.85371023\n",
      "55 Train Loss 44.470203 Test MSE 43.94280804676906 Test RE 0.11159200316364809 Lambda1 0.86976236\n",
      "56 Train Loss 43.125904 Test MSE 42.2488329105481 Test RE 0.1094199542464238 Lambda1 0.905301\n",
      "57 Train Loss 42.094967 Test MSE 42.211383962878784 Test RE 0.10937144911275903 Lambda1 0.92510647\n",
      "58 Train Loss 41.52778 Test MSE 41.72659939293859 Test RE 0.10874158696963888 Lambda1 0.93594444\n",
      "59 Train Loss 40.73974 Test MSE 41.30107052206665 Test RE 0.10818569141033685 Lambda1 0.94244504\n",
      "60 Train Loss 39.4918 Test MSE 40.85840262421291 Test RE 0.10760435844076027 Lambda1 0.9365108\n",
      "61 Train Loss 38.824394 Test MSE 40.03230359186736 Test RE 0.10651099981678205 Lambda1 0.94512856\n",
      "62 Train Loss 38.149574 Test MSE 40.07411890090917 Test RE 0.10656661275360535 Lambda1 0.95175916\n",
      "63 Train Loss 37.68581 Test MSE 40.250496959434216 Test RE 0.10680087087906186 Lambda1 0.9505709\n",
      "64 Train Loss 36.906902 Test MSE 39.48545786863268 Test RE 0.1057810222944307 Lambda1 0.95568883\n",
      "65 Train Loss 36.73787 Test MSE 39.480715623238154 Test RE 0.10577466989717298 Lambda1 0.9560538\n",
      "66 Train Loss 36.604687 Test MSE 39.44398710885599 Test RE 0.10572545789106487 Lambda1 0.95699275\n",
      "67 Train Loss 36.395382 Test MSE 39.37377145077018 Test RE 0.1056313131324216 Lambda1 0.97164637\n",
      "68 Train Loss 36.323853 Test MSE 39.29772830404604 Test RE 0.10552926017640134 Lambda1 0.9806813\n",
      "69 Train Loss 36.22547 Test MSE 39.08894185112167 Test RE 0.10524855154551833 Lambda1 0.99075294\n",
      "70 Train Loss 35.846325 Test MSE 38.86819043385916 Test RE 0.1049509397217881 Lambda1 1.0180371\n",
      "71 Train Loss 35.75945 Test MSE 38.85509142059104 Test RE 0.10493325341348651 Lambda1 1.0221678\n",
      "72 Train Loss 35.686413 Test MSE 38.75455067682173 Test RE 0.10479740376554869 Lambda1 1.0251873\n",
      "73 Train Loss 35.53934 Test MSE 38.66093310309396 Test RE 0.10467075011339044 Lambda1 1.0348369\n",
      "74 Train Loss 35.4537 Test MSE 38.634887352000945 Test RE 0.10463548599270132 Lambda1 1.041129\n",
      "Training time: 235.04\n",
      "Training time: 235.04\n",
      "inv_HT_swish_tune2\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 838.0682 Test MSE 858.2686061105987 Test RE 0.4931746944647534 Lambda1 3.7665006e-07\n",
      "1 Train Loss 838.0507 Test MSE 858.3574025469858 Test RE 0.49320020572221285 Lambda1 2.7072952e-06\n",
      "2 Train Loss 837.9002 Test MSE 857.9351814432988 Test RE 0.4930788896195962 Lambda1 0.0017247446\n",
      "3 Train Loss 837.8991 Test MSE 857.8900858372405 Test RE 0.493065930608214 Lambda1 0.0026279497\n",
      "4 Train Loss 837.8738 Test MSE 857.7656590487569 Test RE 0.4930301726271071 Lambda1 0.029238902\n",
      "5 Train Loss 837.7998 Test MSE 857.9138547040328 Test RE 0.4930727610503899 Lambda1 0.16031864\n",
      "6 Train Loss 836.15027 Test MSE 855.2919199767523 Test RE 0.49231872621363587 Lambda1 0.22793858\n",
      "7 Train Loss 806.3318 Test MSE 814.7522790754726 Test RE 0.4805094825540945 Lambda1 0.078873605\n",
      "8 Train Loss 698.1724 Test MSE 717.5696341490059 Test RE 0.45094252956909747 Lambda1 -0.00810254\n",
      "9 Train Loss 566.09015 Test MSE 591.881542437489 Test RE 0.4095496593938823 Lambda1 0.00014025104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Train Loss 477.52682 Test MSE 450.1089243620525 Test RE 0.3571478027102586 Lambda1 0.0011888798\n",
      "11 Train Loss 347.0465 Test MSE 336.3801076261075 Test RE 0.3087481210064111 Lambda1 -0.0014633972\n",
      "12 Train Loss 290.42807 Test MSE 308.3319328825363 Test RE 0.2955959118298836 Lambda1 0.000461131\n",
      "13 Train Loss 284.3615 Test MSE 296.6878387782036 Test RE 0.2899606356350772 Lambda1 0.00370383\n",
      "14 Train Loss 273.228 Test MSE 288.2415259296362 Test RE 0.2858034353888371 Lambda1 0.00558143\n",
      "15 Train Loss 265.12878 Test MSE 284.58622825672705 Test RE 0.28398546373497124 Lambda1 0.004970503\n",
      "16 Train Loss 264.8002 Test MSE 284.00402794072346 Test RE 0.2836948293730006 Lambda1 0.00473714\n",
      "17 Train Loss 264.25424 Test MSE 284.02938846303834 Test RE 0.28370749554625224 Lambda1 0.003244765\n",
      "18 Train Loss 263.73248 Test MSE 283.9893372381622 Test RE 0.2836874919238039 Lambda1 0.0026328375\n",
      "19 Train Loss 262.77734 Test MSE 282.0832589386435 Test RE 0.28273386290132513 Lambda1 0.006079332\n",
      "20 Train Loss 262.51465 Test MSE 281.76683240078705 Test RE 0.2825752402291348 Lambda1 0.0066649397\n",
      "21 Train Loss 262.33542 Test MSE 282.1803461896402 Test RE 0.2827825143032265 Lambda1 0.0057092938\n",
      "22 Train Loss 262.24158 Test MSE 282.0965571395923 Test RE 0.2827405272589098 Lambda1 0.005953619\n",
      "23 Train Loss 261.8559 Test MSE 282.1000123493369 Test RE 0.28274225880225706 Lambda1 0.0051898984\n",
      "24 Train Loss 261.58163 Test MSE 282.19328511029386 Test RE 0.2827889974949546 Lambda1 0.00452718\n",
      "25 Train Loss 261.32166 Test MSE 281.0607368973607 Test RE 0.2822209574663341 Lambda1 0.006348757\n",
      "26 Train Loss 261.02277 Test MSE 279.8498963603218 Test RE 0.28161238185885573 Lambda1 0.008244829\n",
      "27 Train Loss 260.047 Test MSE 277.71091862710426 Test RE 0.2805340930118326 Lambda1 0.0114595825\n",
      "28 Train Loss 259.30353 Test MSE 274.77006542818185 Test RE 0.2790447648910747 Lambda1 0.015403331\n",
      "29 Train Loss 257.00613 Test MSE 271.50812564486546 Test RE 0.2773834761695628 Lambda1 0.021620305\n",
      "30 Train Loss 255.56071 Test MSE 272.22938200652686 Test RE 0.27775166387211325 Lambda1 0.020288287\n",
      "31 Train Loss 255.41687 Test MSE 272.3270408030781 Test RE 0.2778014793269943 Lambda1 0.020844547\n",
      "32 Train Loss 253.99457 Test MSE 263.6988465997494 Test RE 0.27336523691711884 Lambda1 0.041039675\n",
      "33 Train Loss 246.14415 Test MSE 250.99894041246154 Test RE 0.2667012889986736 Lambda1 0.082748905\n",
      "34 Train Loss 232.97977 Test MSE 241.44487033823086 Test RE 0.2615761613042008 Lambda1 0.11062279\n",
      "35 Train Loss 219.37921 Test MSE 227.8672496058165 Test RE 0.2541148971940482 Lambda1 0.15040922\n",
      "36 Train Loss 206.0505 Test MSE 219.91843981108246 Test RE 0.24964334523939655 Lambda1 0.21169278\n",
      "37 Train Loss 194.51646 Test MSE 201.66136567008368 Test RE 0.2390564812013378 Lambda1 0.251289\n",
      "38 Train Loss 178.13005 Test MSE 193.55708718263426 Test RE 0.23420367718046123 Lambda1 0.28817233\n",
      "39 Train Loss 171.29282 Test MSE 186.89663302851233 Test RE 0.23013883453436917 Lambda1 0.28778583\n",
      "40 Train Loss 167.63483 Test MSE 177.2772170898948 Test RE 0.22413807277865128 Lambda1 0.2879499\n",
      "41 Train Loss 162.98952 Test MSE 166.37299987362837 Test RE 0.21713538063611812 Lambda1 0.31441906\n",
      "42 Train Loss 132.69005 Test MSE 128.93722484585712 Test RE 0.19115177268632982 Lambda1 0.57007253\n",
      "43 Train Loss 76.754654 Test MSE 76.17032212643684 Test RE 0.1469203880525679 Lambda1 0.7207231\n",
      "44 Train Loss 50.44554 Test MSE 49.71398119007241 Test RE 0.11869391161519852 Lambda1 0.8110186\n",
      "45 Train Loss 41.221516 Test MSE 44.36655241910548 Test RE 0.11212875803724338 Lambda1 0.82063293\n",
      "46 Train Loss 39.391636 Test MSE 42.10926269150005 Test RE 0.10923906877001306 Lambda1 0.8567086\n",
      "47 Train Loss 38.541435 Test MSE 41.29891064026363 Test RE 0.10818286253255797 Lambda1 0.8912962\n",
      "48 Train Loss 38.330837 Test MSE 41.43234259360896 Test RE 0.10835748469267655 Lambda1 0.89168704\n",
      "49 Train Loss 38.212193 Test MSE 41.35395691620117 Test RE 0.1082549356283514 Lambda1 0.9000781\n",
      "50 Train Loss 37.987835 Test MSE 41.49767946630771 Test RE 0.10844288838740165 Lambda1 0.913615\n",
      "51 Train Loss 37.80138 Test MSE 41.53614047210268 Test RE 0.10849313043293625 Lambda1 0.9339381\n",
      "52 Train Loss 37.532997 Test MSE 41.39480931577772 Test RE 0.10830839341747471 Lambda1 0.95560586\n",
      "53 Train Loss 37.412106 Test MSE 41.20341499142459 Test RE 0.10805771429614085 Lambda1 0.9578296\n",
      "54 Train Loss 37.334328 Test MSE 41.25805922589552 Test RE 0.10812934397357919 Lambda1 0.955625\n",
      "55 Train Loss 37.275944 Test MSE 41.20795723700671 Test RE 0.10806367024844664 Lambda1 0.95863724\n",
      "56 Train Loss 37.142223 Test MSE 40.97259627081463 Test RE 0.10775462325331843 Lambda1 0.9701002\n",
      "57 Train Loss 36.961914 Test MSE 40.86417857014893 Test RE 0.10761196391421607 Lambda1 0.986217\n",
      "58 Train Loss 36.815784 Test MSE 40.52344889152017 Test RE 0.10716238504187488 Lambda1 0.9995432\n",
      "59 Train Loss 36.712246 Test MSE 40.19528717590253 Test RE 0.10672759878591909 Lambda1 1.0099207\n",
      "60 Train Loss 36.586605 Test MSE 40.218940604059824 Test RE 0.10675899677418102 Lambda1 1.0225617\n",
      "61 Train Loss 36.478863 Test MSE 40.22454621978286 Test RE 0.10676643641643345 Lambda1 1.0234314\n",
      "62 Train Loss 36.400497 Test MSE 40.12362573170117 Test RE 0.10663241765483629 Lambda1 1.0170877\n",
      "63 Train Loss 36.304405 Test MSE 40.357171930911335 Test RE 0.10694230318503063 Lambda1 1.0181339\n",
      "64 Train Loss 36.193985 Test MSE 40.35302758080777 Test RE 0.1069368119959424 Lambda1 1.0293471\n",
      "65 Train Loss 36.16337 Test MSE 40.371759519941044 Test RE 0.10696162923473408 Lambda1 1.0395039\n",
      "66 Train Loss 36.139263 Test MSE 40.284999546755195 Test RE 0.10684663574307313 Lambda1 1.0465147\n",
      "67 Train Loss 36.109585 Test MSE 40.25458304894721 Test RE 0.10680629176671053 Lambda1 1.0585562\n",
      "68 Train Loss 36.087997 Test MSE 40.21835203885437 Test RE 0.10675821561411304 Lambda1 1.0734829\n",
      "69 Train Loss 36.040943 Test MSE 40.132998632223476 Test RE 0.10664487162258489 Lambda1 1.0997465\n",
      "70 Train Loss 35.938004 Test MSE 40.08277387412962 Test RE 0.10657811994840252 Lambda1 1.0950537\n",
      "71 Train Loss 35.900795 Test MSE 39.94399848640275 Test RE 0.10639346152010068 Lambda1 1.0812941\n",
      "72 Train Loss 35.887787 Test MSE 39.94232434502935 Test RE 0.10639123190402687 Lambda1 1.081528\n",
      "73 Train Loss 35.883232 Test MSE 39.97265043616695 Test RE 0.10643161285413245 Lambda1 1.0854437\n",
      "74 Train Loss 35.86899 Test MSE 39.939025969770455 Test RE 0.10638683900178972 Lambda1 1.0953438\n",
      "Training time: 211.59\n",
      "Training time: 211.59\n",
      "inv_HT_swish_tune2\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 838.08813 Test MSE 858.4470514570437 Test RE 0.4932259605571662 Lambda1 -4.560826e-07\n",
      "1 Train Loss 838.06665 Test MSE 858.2899301685743 Test RE 0.493180820996926 Lambda1 -4.5888294e-07\n",
      "2 Train Loss 838.06335 Test MSE 858.2364207260074 Test RE 0.49316544726293754 Lambda1 -4.8958753e-07\n",
      "3 Train Loss 837.967 Test MSE 857.9286438433248 Test RE 0.49307701094697226 Lambda1 2.6930109e-06\n",
      "4 Train Loss 837.85657 Test MSE 857.8421941705773 Test RE 0.4930521677269605 Lambda1 -0.00015150942\n",
      "5 Train Loss 837.8527 Test MSE 857.9231889398598 Test RE 0.493075443397159 Lambda1 -0.0007816804\n",
      "6 Train Loss 837.7822 Test MSE 858.0218628471548 Test RE 0.4931037980810776 Lambda1 -0.00794418\n",
      "7 Train Loss 836.69025 Test MSE 857.062962428187 Test RE 0.4928281817825383 Lambda1 -0.02912327\n",
      "8 Train Loss 832.3575 Test MSE 854.1256926466742 Test RE 0.49198296283553933 Lambda1 -0.091721594\n",
      "9 Train Loss 789.998 Test MSE 807.0594390175569 Test RE 0.47823563219638965 Lambda1 -0.1702994\n",
      "10 Train Loss 738.8833 Test MSE 758.852040655852 Test RE 0.46373270283655804 Lambda1 0.00591788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Train Loss 617.783 Test MSE 657.4641232915566 Test RE 0.43164349483312836 Lambda1 -4.7420835e-05\n",
      "12 Train Loss 582.7639 Test MSE 614.147410171491 Test RE 0.4171819243186241 Lambda1 1.0344051e-05\n",
      "13 Train Loss 541.2888 Test MSE 573.7632758296878 Test RE 0.40323251480848654 Lambda1 9.71896e-06\n",
      "14 Train Loss 522.46014 Test MSE 542.3115015276053 Test RE 0.39202483277297945 Lambda1 -4.587715e-05\n",
      "15 Train Loss 467.08273 Test MSE 489.1757893902563 Test RE 0.3723245282295742 Lambda1 7.926545e-06\n",
      "16 Train Loss 442.7189 Test MSE 464.0900511915456 Test RE 0.3626521861689948 Lambda1 6.4561397e-07\n",
      "17 Train Loss 434.54068 Test MSE 452.4841195223277 Test RE 0.3580888855565103 Lambda1 4.855631e-07\n",
      "18 Train Loss 424.74545 Test MSE 430.2897499627511 Test RE 0.34919633026406166 Lambda1 4.0572113e-06\n",
      "19 Train Loss 390.54684 Test MSE 409.0865427695272 Test RE 0.34048404615597516 Lambda1 2.598082e-06\n",
      "20 Train Loss 386.4421 Test MSE 400.7417211277186 Test RE 0.33699344256635133 Lambda1 6.3365917e-07\n",
      "21 Train Loss 385.46725 Test MSE 398.3846444254778 Test RE 0.3360009194498453 Lambda1 -4.6422323e-07\n",
      "22 Train Loss 384.94675 Test MSE 394.0203115998947 Test RE 0.3341553937783468 Lambda1 1.1683658e-06\n",
      "23 Train Loss 380.9771 Test MSE 393.82721543495336 Test RE 0.3340735045579826 Lambda1 -5.1465286e-06\n",
      "24 Train Loss 374.62872 Test MSE 381.8214770243707 Test RE 0.32894201412014334 Lambda1 -1.4042207e-06\n",
      "25 Train Loss 365.00653 Test MSE 374.07255264197556 Test RE 0.32558702710756615 Lambda1 -2.020218e-05\n",
      "26 Train Loss 357.28052 Test MSE 365.58356231627903 Test RE 0.32187148286973544 Lambda1 -5.208132e-05\n",
      "27 Train Loss 348.67474 Test MSE 362.9194460833723 Test RE 0.3206965517880493 Lambda1 1.0259685e-05\n",
      "28 Train Loss 333.10886 Test MSE 346.12437899418796 Test RE 0.313188109597997 Lambda1 0.00041539752\n",
      "29 Train Loss 309.67337 Test MSE 328.07404514388554 Test RE 0.3049124147569304 Lambda1 0.0005542712\n",
      "30 Train Loss 298.80157 Test MSE 309.87129254995926 Test RE 0.29633288042186784 Lambda1 0.000876484\n",
      "31 Train Loss 287.5915 Test MSE 301.49624481177636 Test RE 0.29230088096606005 Lambda1 0.0010297926\n",
      "32 Train Loss 274.02133 Test MSE 295.33144827086375 Test RE 0.2892970587160505 Lambda1 0.00034192594\n",
      "33 Train Loss 271.60947 Test MSE 295.48778307332583 Test RE 0.28937361882348084 Lambda1 0.000126562\n",
      "34 Train Loss 269.1581 Test MSE 293.3406288936384 Test RE 0.28832033904419174 Lambda1 -8.3030545e-07\n",
      "35 Train Loss 267.42133 Test MSE 291.5385718870388 Test RE 0.28743336660584884 Lambda1 0.00015035443\n",
      "36 Train Loss 266.1169 Test MSE 292.0567643813991 Test RE 0.2876887010636728 Lambda1 0.00014746975\n",
      "37 Train Loss 264.35532 Test MSE 291.279538010664 Test RE 0.28730564504150935 Lambda1 7.074074e-05\n",
      "38 Train Loss 263.53687 Test MSE 288.91499722033194 Test RE 0.2861371279532135 Lambda1 7.5720374e-05\n",
      "39 Train Loss 263.3438 Test MSE 289.10860732857765 Test RE 0.28623298618633 Lambda1 0.00011133072\n",
      "40 Train Loss 261.3518 Test MSE 288.7565636188855 Test RE 0.28605866205010083 Lambda1 0.00013587701\n",
      "41 Train Loss 260.33832 Test MSE 288.13450306985425 Test RE 0.28575037165925854 Lambda1 5.9467115e-05\n",
      "42 Train Loss 259.18542 Test MSE 287.42185668936366 Test RE 0.28539677804668917 Lambda1 -2.6966718e-06\n",
      "43 Train Loss 258.07852 Test MSE 285.8158186665712 Test RE 0.28459829985544044 Lambda1 9.743306e-05\n",
      "44 Train Loss 257.81488 Test MSE 285.44613095588335 Test RE 0.2844141838429751 Lambda1 5.6053173e-05\n",
      "45 Train Loss 257.67526 Test MSE 285.13255761795216 Test RE 0.2842579210634854 Lambda1 5.017253e-05\n",
      "46 Train Loss 257.36853 Test MSE 285.6681837596285 Test RE 0.2845247873554716 Lambda1 6.431364e-05\n",
      "47 Train Loss 257.30466 Test MSE 285.90044428703794 Test RE 0.28464042930324013 Lambda1 4.954223e-05\n",
      "48 Train Loss 257.23663 Test MSE 285.79273106001403 Test RE 0.2845868049943413 Lambda1 4.9188864e-05\n",
      "49 Train Loss 257.064 Test MSE 285.65173584498785 Test RE 0.2845165961969066 Lambda1 3.561232e-05\n",
      "50 Train Loss 256.54776 Test MSE 285.67147078409135 Test RE 0.2845264242847818 Lambda1 2.086516e-05\n",
      "51 Train Loss 256.23975 Test MSE 285.6271780060806 Test RE 0.28450436580927324 Lambda1 2.5788988e-05\n",
      "52 Train Loss 256.2006 Test MSE 285.6247051661019 Test RE 0.28450313424703627 Lambda1 3.5094203e-05\n",
      "53 Train Loss 256.07098 Test MSE 285.4006572226908 Test RE 0.2843915282725421 Lambda1 3.1611882e-05\n",
      "54 Train Loss 255.91399 Test MSE 284.7873579419708 Test RE 0.28408579855951804 Lambda1 2.3688754e-05\n",
      "55 Train Loss 255.87074 Test MSE 284.9840378689443 Test RE 0.2841838793362128 Lambda1 2.812022e-05\n",
      "56 Train Loss 255.8051 Test MSE 285.2082734715289 Test RE 0.2842956603568732 Lambda1 4.4019333e-05\n",
      "57 Train Loss 255.71394 Test MSE 285.1713544578807 Test RE 0.2842772593215138 Lambda1 2.4030736e-05\n",
      "58 Train Loss 255.56058 Test MSE 285.15603025711135 Test RE 0.2842696211414281 Lambda1 -6.0051e-06\n",
      "59 Train Loss 255.38348 Test MSE 285.1067212678871 Test RE 0.28424504222286195 Lambda1 1.0447752e-05\n",
      "60 Train Loss 255.35359 Test MSE 285.1936907117165 Test RE 0.284288392215112 Lambda1 2.6330513e-06\n",
      "61 Train Loss 255.3225 Test MSE 285.1847809206712 Test RE 0.2842839514262075 Lambda1 -2.791035e-06\n",
      "62 Train Loss 255.2776 Test MSE 285.1000494207697 Test RE 0.28424171636201934 Lambda1 9.716823e-06\n",
      "63 Train Loss 255.20636 Test MSE 284.93818603629217 Test RE 0.2841610168700563 Lambda1 3.1525144e-06\n",
      "64 Train Loss 255.14685 Test MSE 284.7941843439617 Test RE 0.28408920333186083 Lambda1 5.8547505e-07\n",
      "65 Train Loss 255.12175 Test MSE 284.6943872002634 Test RE 0.2840394239179084 Lambda1 1.2785096e-05\n",
      "66 Train Loss 255.04454 Test MSE 284.57346210431945 Test RE 0.28397909406207394 Lambda1 1.5089802e-05\n",
      "67 Train Loss 254.9583 Test MSE 284.53876127713016 Test RE 0.2839617793591815 Lambda1 4.8418847e-06\n",
      "68 Train Loss 254.64287 Test MSE 284.3415350360644 Test RE 0.28386334914480377 Lambda1 1.914346e-05\n",
      "69 Train Loss 254.30618 Test MSE 284.7308382982813 Test RE 0.2840576069555137 Lambda1 2.602447e-05\n",
      "70 Train Loss 254.1919 Test MSE 284.67987214378815 Test RE 0.2840321829939271 Lambda1 2.8201724e-05\n",
      "71 Train Loss 254.04666 Test MSE 284.7259195361565 Test RE 0.2840551533789341 Lambda1 2.3369907e-05\n",
      "72 Train Loss 253.9812 Test MSE 284.68695129652303 Test RE 0.28403571449527626 Lambda1 3.0324132e-05\n",
      "73 Train Loss 253.89267 Test MSE 284.83530102413596 Test RE 0.28410971004159663 Lambda1 2.3640236e-05\n",
      "74 Train Loss 253.62593 Test MSE 284.6380239942136 Test RE 0.2840113057572589 Lambda1 9.94931e-06\n",
      "Training time: 197.18\n",
      "Training time: 197.18\n",
      "inv_HT_swish_tune2\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 838.12256 Test MSE 858.355266402754 Test RE 0.4931995920224726 Lambda1 -9.37981e-06\n",
      "1 Train Loss 838.01886 Test MSE 858.3729470786682 Test RE 0.4932046715376712 Lambda1 -0.00033301482\n",
      "2 Train Loss 837.8971 Test MSE 857.9261514592719 Test RE 0.49307629472297143 Lambda1 -0.0017082631\n",
      "3 Train Loss 837.8463 Test MSE 857.7598491019575 Test RE 0.4930285028913851 Lambda1 -0.019812498\n",
      "4 Train Loss 837.5697 Test MSE 858.7132210439178 Test RE 0.4933024193062677 Lambda1 -0.09745126\n",
      "5 Train Loss 822.73645 Test MSE 850.6401177559708 Test RE 0.4909780776246246 Lambda1 -0.14742409\n",
      "6 Train Loss 733.1512 Test MSE 759.2218059434845 Test RE 0.4638456704269844 Lambda1 -0.0022287853\n",
      "7 Train Loss 644.2568 Test MSE 660.7049165125457 Test RE 0.43270602255334817 Lambda1 0.0046931943\n",
      "8 Train Loss 610.39844 Test MSE 626.0962609605435 Test RE 0.4212207195981268 Lambda1 0.002744065\n",
      "9 Train Loss 499.77747 Test MSE 513.1398612341898 Test RE 0.38133533275125775 Lambda1 0.0034709992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Train Loss 372.64328 Test MSE 384.84630736323714 Test RE 0.3302424005564978 Lambda1 -0.0029552446\n",
      "11 Train Loss 313.53107 Test MSE 326.9432460172052 Test RE 0.30438647813004965 Lambda1 -0.005883792\n",
      "12 Train Loss 283.05743 Test MSE 301.415441340493 Test RE 0.2922617088221671 Lambda1 -0.006964367\n",
      "13 Train Loss 271.02008 Test MSE 290.5138276846196 Test RE 0.2869277646223835 Lambda1 -0.0024778536\n",
      "14 Train Loss 262.83142 Test MSE 280.61327396694844 Test RE 0.2819962129493743 Lambda1 0.008248615\n",
      "15 Train Loss 260.07846 Test MSE 278.50825614962395 Test RE 0.28093652593986734 Lambda1 0.013186241\n",
      "16 Train Loss 259.4005 Test MSE 277.85229958654446 Test RE 0.2806054930359332 Lambda1 0.015349636\n",
      "17 Train Loss 256.81503 Test MSE 277.228189689399 Test RE 0.2802901688160893 Lambda1 0.02212878\n",
      "18 Train Loss 255.09958 Test MSE 273.9025080446807 Test RE 0.27860388946439296 Lambda1 0.030144598\n",
      "19 Train Loss 245.15889 Test MSE 259.2447627765692 Test RE 0.2710467264172194 Lambda1 0.087338395\n",
      "20 Train Loss 237.5471 Test MSE 247.59787509029152 Test RE 0.26488820918819045 Lambda1 0.1295588\n",
      "21 Train Loss 217.6183 Test MSE 227.34004827109166 Test RE 0.2538207625779823 Lambda1 0.20427823\n",
      "22 Train Loss 209.97665 Test MSE 228.1470455058943 Test RE 0.2542708618929215 Lambda1 0.22825135\n",
      "23 Train Loss 204.52965 Test MSE 222.962584244871 Test RE 0.2513652076913049 Lambda1 0.24120711\n",
      "24 Train Loss 195.35849 Test MSE 212.08465541074173 Test RE 0.2451567156970435 Lambda1 0.3052942\n",
      "25 Train Loss 186.71115 Test MSE 203.53836951522396 Test RE 0.24016643761128845 Lambda1 0.34391633\n",
      "26 Train Loss 173.50642 Test MSE 180.3844594222676 Test RE 0.22609384012692074 Lambda1 0.3784255\n",
      "27 Train Loss 162.93869 Test MSE 171.2391073519826 Test RE 0.2202879027531481 Lambda1 0.3752722\n",
      "28 Train Loss 145.30888 Test MSE 144.728877983889 Test RE 0.20251946351652425 Lambda1 0.4155893\n",
      "29 Train Loss 141.74115 Test MSE 137.6926478179526 Test RE 0.19753522270816054 Lambda1 0.4311538\n",
      "30 Train Loss 135.22571 Test MSE 125.84833404268136 Test RE 0.1888482244887997 Lambda1 0.48896533\n",
      "31 Train Loss 127.8094 Test MSE 120.23471478697923 Test RE 0.18458827419148802 Lambda1 0.54740345\n",
      "32 Train Loss 122.045494 Test MSE 122.62805285049863 Test RE 0.1864163871324559 Lambda1 0.5575317\n",
      "33 Train Loss 115.317085 Test MSE 119.32048292545872 Test RE 0.18388515570446812 Lambda1 0.5533503\n",
      "34 Train Loss 112.915184 Test MSE 115.52514171301516 Test RE 0.1809370167965311 Lambda1 0.5548784\n",
      "35 Train Loss 108.43066 Test MSE 110.74258647471184 Test RE 0.1771521803541173 Lambda1 0.572665\n",
      "36 Train Loss 104.521576 Test MSE 101.98263914425036 Test RE 0.1700013201575473 Lambda1 0.57625294\n",
      "37 Train Loss 98.56278 Test MSE 91.13595041553269 Test RE 0.16070671882803106 Lambda1 0.571105\n",
      "38 Train Loss 94.27848 Test MSE 88.46114277675562 Test RE 0.15833081357101225 Lambda1 0.5893988\n",
      "39 Train Loss 88.140686 Test MSE 86.16984619166894 Test RE 0.15626683981771983 Lambda1 0.6214571\n",
      "40 Train Loss 80.9071 Test MSE 79.02810618946866 Test RE 0.14965111535479872 Lambda1 0.656419\n",
      "41 Train Loss 71.41827 Test MSE 67.28371756912456 Test RE 0.13808425469924068 Lambda1 0.7055981\n",
      "42 Train Loss 66.961296 Test MSE 63.54623837651551 Test RE 0.13419430777452857 Lambda1 0.7274345\n",
      "43 Train Loss 62.976273 Test MSE 57.58627528043412 Test RE 0.12774639850592373 Lambda1 0.7641962\n",
      "44 Train Loss 58.43576 Test MSE 52.61095907302896 Test RE 0.12210326586341366 Lambda1 0.80396485\n",
      "45 Train Loss 54.214478 Test MSE 48.53762951860111 Test RE 0.1172812137473633 Lambda1 0.8179398\n",
      "46 Train Loss 51.908745 Test MSE 48.77373643501125 Test RE 0.11756611963052677 Lambda1 0.8182931\n",
      "47 Train Loss 49.361412 Test MSE 48.4264502779046 Test RE 0.1171468158396609 Lambda1 0.8242362\n",
      "48 Train Loss 47.17423 Test MSE 44.494376442429356 Test RE 0.1122901683741081 Lambda1 0.8276459\n",
      "49 Train Loss 44.859776 Test MSE 43.64564568309919 Test RE 0.11121404374521343 Lambda1 0.8268954\n",
      "50 Train Loss 43.488293 Test MSE 43.475518308199256 Test RE 0.11099708021132501 Lambda1 0.82933\n",
      "51 Train Loss 42.26176 Test MSE 42.52373191844487 Test RE 0.10977535656325053 Lambda1 0.8348985\n",
      "52 Train Loss 41.83323 Test MSE 42.00952378829512 Test RE 0.10910962166630477 Lambda1 0.8374848\n",
      "53 Train Loss 41.42066 Test MSE 42.04197664036359 Test RE 0.1091517577643327 Lambda1 0.8452374\n",
      "54 Train Loss 40.08172 Test MSE 41.09644267671633 Test RE 0.10791735340534535 Lambda1 0.873795\n",
      "55 Train Loss 38.712837 Test MSE 39.035030703304045 Test RE 0.10517594753388193 Lambda1 0.88474745\n",
      "56 Train Loss 37.258728 Test MSE 38.18277040123117 Test RE 0.10402144648793271 Lambda1 0.9020392\n",
      "57 Train Loss 36.436283 Test MSE 37.69484395563315 Test RE 0.10335467972772057 Lambda1 0.92053294\n",
      "58 Train Loss 36.081314 Test MSE 37.06982316196521 Test RE 0.10249423251744505 Lambda1 0.9337088\n",
      "59 Train Loss 35.446075 Test MSE 36.54161729783415 Test RE 0.10176139528096739 Lambda1 0.95520663\n",
      "60 Train Loss 35.13194 Test MSE 36.705491138703856 Test RE 0.1019893186390672 Lambda1 0.96036226\n",
      "61 Train Loss 34.872326 Test MSE 36.524677480383275 Test RE 0.10173780547551058 Lambda1 0.96255875\n",
      "62 Train Loss 34.5211 Test MSE 36.36605745264368 Test RE 0.10151665071778877 Lambda1 0.9629719\n",
      "63 Train Loss 34.39565 Test MSE 36.408910420177115 Test RE 0.10157644560930149 Lambda1 0.9657834\n",
      "64 Train Loss 34.26855 Test MSE 36.36707812642764 Test RE 0.10151807532447228 Lambda1 0.97055596\n",
      "65 Train Loss 34.17185 Test MSE 36.416917417753176 Test RE 0.1015876142714933 Lambda1 0.97021806\n",
      "66 Train Loss 33.984356 Test MSE 36.30373518707465 Test RE 0.1014296264285569 Lambda1 0.96996814\n",
      "67 Train Loss 33.752247 Test MSE 36.015347888497715 Test RE 0.1010259580693688 Lambda1 0.97026277\n",
      "68 Train Loss 33.567795 Test MSE 35.95244571932011 Test RE 0.10093769668348788 Lambda1 0.97229433\n",
      "69 Train Loss 33.440517 Test MSE 35.76897928613322 Test RE 0.10067982319763763 Lambda1 0.97050005\n",
      "70 Train Loss 33.338055 Test MSE 35.7962498508165 Test RE 0.10071819544880062 Lambda1 0.9678196\n",
      "71 Train Loss 33.209225 Test MSE 35.882427438614485 Test RE 0.10083935944635912 Lambda1 0.9648487\n",
      "72 Train Loss 33.128895 Test MSE 35.72966515331387 Test RE 0.10062447875912077 Lambda1 0.9607254\n",
      "73 Train Loss 32.98629 Test MSE 35.48866725482374 Test RE 0.10028454668059134 Lambda1 0.961441\n",
      "74 Train Loss 32.84946 Test MSE 35.371577549324 Test RE 0.10011897284462204 Lambda1 0.9646811\n",
      "Training time: 228.89\n",
      "Training time: 228.89\n",
      "inv_HT_swish_tune2\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 838.09155 Test MSE 858.3138336453778 Test RE 0.4931876885214197 Lambda1 1.4217153e-06\n",
      "1 Train Loss 837.916 Test MSE 858.1602727678886 Test RE 0.4931435684577805 Lambda1 0.0018363481\n",
      "2 Train Loss 837.87537 Test MSE 857.8951887976957 Test RE 0.4930673970502709 Lambda1 0.005695587\n",
      "3 Train Loss 837.872 Test MSE 857.8487744869338 Test RE 0.4930540587700008 Lambda1 0.014226697\n",
      "4 Train Loss 837.5899 Test MSE 857.4708375668677 Test RE 0.4929454359762851 Lambda1 0.20610507\n",
      "5 Train Loss 835.8776 Test MSE 858.1029918017688 Test RE 0.49312710987054265 Lambda1 0.5078627\n",
      "6 Train Loss 822.57605 Test MSE 841.7423624493243 Test RE 0.48840349477907286 Lambda1 0.5515846\n",
      "7 Train Loss 795.27704 Test MSE 808.1506618127461 Test RE 0.47855883350929873 Lambda1 0.46758962\n",
      "8 Train Loss 730.4185 Test MSE 744.5285247282393 Test RE 0.4593353200679267 Lambda1 0.06144078\n",
      "9 Train Loss 666.7897 Test MSE 693.8884106501292 Test RE 0.4434391035338896 Lambda1 0.0016232454\n",
      "10 Train Loss 647.6694 Test MSE 682.8873791592655 Test RE 0.43990987780582447 Lambda1 0.00081770984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Train Loss 617.7012 Test MSE 654.4594045948895 Test RE 0.43065602459159497 Lambda1 0.00035282836\n",
      "12 Train Loss 609.8393 Test MSE 645.8500113638385 Test RE 0.4278140136639287 Lambda1 0.0001173327\n",
      "13 Train Loss 601.24036 Test MSE 644.0650730807623 Test RE 0.4272224289817678 Lambda1 -0.0001342751\n",
      "14 Train Loss 545.38824 Test MSE 546.8907104789121 Test RE 0.393676457347183 Lambda1 -0.00070498744\n",
      "15 Train Loss 387.7272 Test MSE 390.6376475579122 Test RE 0.3327179401071161 Lambda1 -0.0004120414\n",
      "16 Train Loss 332.4326 Test MSE 324.4859432975498 Test RE 0.30324043754078844 Lambda1 0.00284118\n",
      "17 Train Loss 292.88156 Test MSE 291.0545507774136 Test RE 0.28719466472753796 Lambda1 0.0083538415\n",
      "18 Train Loss 266.97867 Test MSE 280.34582738944965 Test RE 0.28186179860117194 Lambda1 0.010804581\n",
      "19 Train Loss 261.48004 Test MSE 278.7878805767085 Test RE 0.28107752177281836 Lambda1 0.011744387\n",
      "20 Train Loss 260.96323 Test MSE 279.20413838772185 Test RE 0.28128728173954987 Lambda1 0.011758357\n",
      "21 Train Loss 260.13358 Test MSE 278.61117786846165 Test RE 0.28098843068358614 Lambda1 0.011553111\n",
      "22 Train Loss 258.76822 Test MSE 277.7346182517183 Test RE 0.28054606303247354 Lambda1 0.012156238\n",
      "23 Train Loss 258.68008 Test MSE 277.72757975480266 Test RE 0.2805425081369362 Lambda1 0.012303013\n",
      "24 Train Loss 258.50848 Test MSE 277.77522832332374 Test RE 0.28056657286186115 Lambda1 0.011272786\n",
      "25 Train Loss 257.60226 Test MSE 278.7205416937472 Test RE 0.2810435737593999 Lambda1 0.007468951\n",
      "26 Train Loss 257.18454 Test MSE 278.3084970643805 Test RE 0.28083575749098694 Lambda1 0.007048346\n",
      "27 Train Loss 256.9592 Test MSE 276.8540645875499 Test RE 0.2801009763121564 Lambda1 0.010105031\n",
      "28 Train Loss 256.7422 Test MSE 275.76214368436683 Test RE 0.27954806718973 Lambda1 0.013948622\n",
      "29 Train Loss 254.09096 Test MSE 272.4373692317093 Test RE 0.27785774677836406 Lambda1 0.024127567\n",
      "30 Train Loss 242.26247 Test MSE 260.5614405122234 Test RE 0.2717341640195568 Lambda1 0.059259012\n",
      "31 Train Loss 232.60767 Test MSE 247.89957185396193 Test RE 0.26504954253152524 Lambda1 0.10353188\n",
      "32 Train Loss 221.36382 Test MSE 234.54739068855386 Test RE 0.25781279878870456 Lambda1 0.15038693\n",
      "33 Train Loss 213.79773 Test MSE 228.35206755969767 Test RE 0.25438508521694303 Lambda1 0.18300867\n",
      "34 Train Loss 205.86115 Test MSE 220.32301320684854 Test RE 0.24987286816594065 Lambda1 0.22376412\n",
      "35 Train Loss 200.73373 Test MSE 216.1624632780966 Test RE 0.2475023409563212 Lambda1 0.26751828\n",
      "36 Train Loss 196.27155 Test MSE 211.01608032226287 Test RE 0.24453833250870333 Lambda1 0.27539468\n",
      "37 Train Loss 186.28403 Test MSE 199.78296267178223 Test RE 0.23794051389677634 Lambda1 0.3204242\n",
      "38 Train Loss 168.97646 Test MSE 174.80441028169048 Test RE 0.2225693530549725 Lambda1 0.4042291\n",
      "39 Train Loss 157.64487 Test MSE 159.5191379360589 Test RE 0.21261581603483445 Lambda1 0.41345623\n",
      "40 Train Loss 144.59706 Test MSE 149.03463050157592 Test RE 0.2055099096048035 Lambda1 0.4613174\n",
      "41 Train Loss 127.92854 Test MSE 131.6776855910434 Test RE 0.19317248334225628 Lambda1 0.5097444\n",
      "42 Train Loss 119.01263 Test MSE 118.61676546335411 Test RE 0.18334210327870182 Lambda1 0.5315157\n",
      "43 Train Loss 115.63995 Test MSE 114.64356090215824 Test RE 0.18024532286934028 Lambda1 0.5771008\n",
      "44 Train Loss 108.77808 Test MSE 104.62185312877985 Test RE 0.1721870060822786 Lambda1 0.6683465\n",
      "45 Train Loss 104.82197 Test MSE 104.04483119325508 Test RE 0.17171151721462843 Lambda1 0.74286824\n",
      "46 Train Loss 103.55733 Test MSE 101.39253279294988 Test RE 0.1695087637736084 Lambda1 0.79635715\n",
      "47 Train Loss 98.66704 Test MSE 88.72880597689037 Test RE 0.15857016907657998 Lambda1 0.90892315\n",
      "48 Train Loss 93.521576 Test MSE 85.74625527839547 Test RE 0.1558822809233394 Lambda1 0.97176856\n",
      "49 Train Loss 87.05811 Test MSE 81.81881045967754 Test RE 0.15227049207370763 Lambda1 1.0083593\n",
      "50 Train Loss 78.86256 Test MSE 77.29034331765133 Test RE 0.1479966172395012 Lambda1 1.0572847\n",
      "51 Train Loss 72.579185 Test MSE 65.93002709867848 Test RE 0.13668812864426808 Lambda1 1.126093\n",
      "52 Train Loss 68.35348 Test MSE 61.10634106655898 Test RE 0.13159285535113455 Lambda1 1.1220797\n",
      "53 Train Loss 66.579704 Test MSE 60.265979493922124 Test RE 0.13068486102915303 Lambda1 1.107183\n",
      "54 Train Loss 61.888344 Test MSE 56.73973014584095 Test RE 0.1268039562446358 Lambda1 1.1319929\n",
      "55 Train Loss 58.935547 Test MSE 52.00510502800725 Test RE 0.12139817543867594 Lambda1 1.1853299\n",
      "56 Train Loss 57.055775 Test MSE 51.85583214040398 Test RE 0.12122382256872823 Lambda1 1.1925875\n",
      "57 Train Loss 53.731663 Test MSE 50.07458078710918 Test RE 0.11912360605765311 Lambda1 1.225941\n",
      "58 Train Loss 50.201237 Test MSE 46.7604617870217 Test RE 0.11511411165078585 Lambda1 1.2721457\n",
      "59 Train Loss 48.421215 Test MSE 47.91188936967097 Test RE 0.11652277511537477 Lambda1 1.2669767\n",
      "60 Train Loss 45.633865 Test MSE 44.208578763290504 Test RE 0.11192895452782788 Lambda1 1.2821316\n",
      "61 Train Loss 44.00458 Test MSE 40.98064493005012 Test RE 0.10776520639588917 Lambda1 1.2996303\n",
      "62 Train Loss 43.527477 Test MSE 40.27615672932556 Test RE 0.10683490833637112 Lambda1 1.3125898\n",
      "63 Train Loss 42.214706 Test MSE 39.539740533229434 Test RE 0.10585370859387241 Lambda1 1.3325099\n",
      "64 Train Loss 40.77952 Test MSE 39.00926035156166 Test RE 0.105141223997014 Lambda1 1.3312007\n",
      "65 Train Loss 40.311718 Test MSE 38.548267077727246 Test RE 0.10451812263289267 Lambda1 1.3302004\n",
      "66 Train Loss 39.553898 Test MSE 37.28495369812248 Test RE 0.10279120913022423 Lambda1 1.3510894\n",
      "67 Train Loss 37.677746 Test MSE 35.88302751151569 Test RE 0.10084020262668879 Lambda1 1.3740226\n",
      "68 Train Loss 36.587116 Test MSE 35.38121505403967 Test RE 0.100132611356439 Lambda1 1.3698413\n",
      "69 Train Loss 35.544003 Test MSE 34.66226577545824 Test RE 0.09911003844320354 Lambda1 1.3636324\n",
      "70 Train Loss 34.777702 Test MSE 34.75398118551092 Test RE 0.09924107305153254 Lambda1 1.3572136\n",
      "71 Train Loss 33.920704 Test MSE 34.751166926843815 Test RE 0.09923705486865596 Lambda1 1.3459755\n",
      "72 Train Loss 33.27927 Test MSE 33.90562339753712 Test RE 0.09802233361813505 Lambda1 1.3598309\n",
      "73 Train Loss 32.99331 Test MSE 33.77557456731519 Test RE 0.09783416516932589 Lambda1 1.3686008\n",
      "74 Train Loss 32.65695 Test MSE 33.82350658729853 Test RE 0.09790356038588667 Lambda1 1.3582673\n",
      "Training time: 238.15\n",
      "Training time: 238.15\n",
      "inv_HT_swish_tune3\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 838.0459 Test MSE 858.2380998136215 Test RE 0.4931659296868764 Lambda1 -2.6590424e-06\n",
      "1 Train Loss 837.88654 Test MSE 857.8336217348262 Test RE 0.49304970418027444 Lambda1 0.00031962636\n",
      "2 Train Loss 837.8818 Test MSE 857.9196857490301 Test RE 0.4930744366991485 Lambda1 0.0013830981\n",
      "3 Train Loss 837.66284 Test MSE 858.2565555971079 Test RE 0.49317123224492654 Lambda1 0.21802327\n",
      "4 Train Loss 826.66644 Test MSE 847.4227011326562 Test RE 0.4900486730780295 Lambda1 0.3224163\n",
      "5 Train Loss 762.58777 Test MSE 784.4500227133547 Test RE 0.47148926667508434 Lambda1 0.046875924\n",
      "6 Train Loss 632.6653 Test MSE 672.7520493885248 Test RE 0.43663313016627253 Lambda1 0.0012342805\n",
      "7 Train Loss 612.62866 Test MSE 657.7476378686699 Test RE 0.4317365524072216 Lambda1 2.3519206e-05\n",
      "8 Train Loss 606.5746 Test MSE 651.4268107894303 Test RE 0.4296570923278747 Lambda1 -5.5092507e-05\n",
      "9 Train Loss 603.5411 Test MSE 649.3285138794187 Test RE 0.428964554582121 Lambda1 -5.208757e-05\n",
      "10 Train Loss 599.0768 Test MSE 633.0483186161596 Test RE 0.42355284227963047 Lambda1 -0.00076921034\n",
      "11 Train Loss 542.3983 Test MSE 538.6048216332016 Test RE 0.39068279773089676 Lambda1 -0.00071460503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 Train Loss 384.95993 Test MSE 365.70262689468336 Test RE 0.3219238927378652 Lambda1 0.00059578\n",
      "13 Train Loss 296.89966 Test MSE 299.693715256826 Test RE 0.2914257940398856 Lambda1 0.0018825127\n",
      "14 Train Loss 277.8676 Test MSE 297.3596126037818 Test RE 0.2902887209199764 Lambda1 0.003138366\n",
      "15 Train Loss 262.8207 Test MSE 281.37059438880345 Test RE 0.28237648290570055 Lambda1 0.0030933858\n",
      "16 Train Loss 258.77133 Test MSE 280.027738642114 Test RE 0.2817018488105718 Lambda1 0.0033253068\n",
      "17 Train Loss 257.78894 Test MSE 279.7706476516437 Test RE 0.2815725051283264 Lambda1 0.0033382266\n",
      "18 Train Loss 256.56802 Test MSE 278.7121431093442 Test RE 0.2810393394357615 Lambda1 0.00486211\n",
      "19 Train Loss 255.32883 Test MSE 277.19562693033146 Test RE 0.2802737071256795 Lambda1 0.008048964\n",
      "20 Train Loss 247.27043 Test MSE 269.4679111205841 Test RE 0.2763393288787812 Lambda1 0.031786077\n",
      "21 Train Loss 223.72757 Test MSE 225.79817404262144 Test RE 0.25295856181551307 Lambda1 0.118910074\n",
      "22 Train Loss 190.65146 Test MSE 194.33130426557727 Test RE 0.23467161025974784 Lambda1 0.19480962\n",
      "23 Train Loss 154.01614 Test MSE 128.73720299849973 Test RE 0.19100344713225756 Lambda1 0.31616274\n",
      "24 Train Loss 105.33723 Test MSE 89.31242276447047 Test RE 0.15909081472207606 Lambda1 0.4069347\n",
      "25 Train Loss 91.64102 Test MSE 81.35497436122232 Test RE 0.15183826301323974 Lambda1 0.40755248\n",
      "26 Train Loss 74.1661 Test MSE 62.56542473609472 Test RE 0.13315465976345828 Lambda1 0.4876003\n",
      "27 Train Loss 70.52471 Test MSE 60.04938445815281 Test RE 0.1304498099211197 Lambda1 0.5273298\n",
      "28 Train Loss 67.420715 Test MSE 57.70502910279555 Test RE 0.12787804932071253 Lambda1 0.5750254\n",
      "29 Train Loss 63.696632 Test MSE 53.12683427335499 Test RE 0.12270044550542916 Lambda1 0.57822406\n",
      "30 Train Loss 58.23404 Test MSE 49.46360342579366 Test RE 0.11839464138871572 Lambda1 0.597736\n",
      "31 Train Loss 56.782314 Test MSE 49.634412342276654 Test RE 0.11859888683965965 Lambda1 0.58673036\n",
      "32 Train Loss 54.234665 Test MSE 50.03944373970993 Test RE 0.11908180454632411 Lambda1 0.58347666\n",
      "33 Train Loss 53.11028 Test MSE 48.527025805135395 Test RE 0.11726840219963709 Lambda1 0.6021785\n",
      "34 Train Loss 52.200863 Test MSE 48.24505360426207 Test RE 0.11692720465966003 Lambda1 0.6055368\n",
      "35 Train Loss 50.94377 Test MSE 48.81570018964933 Test RE 0.11761668429344783 Lambda1 0.60147434\n",
      "36 Train Loss 49.23322 Test MSE 47.65403293312392 Test RE 0.1162087958153701 Lambda1 0.62734514\n",
      "37 Train Loss 48.28515 Test MSE 48.1272929470797 Test RE 0.11678441450707709 Lambda1 0.6212178\n",
      "38 Train Loss 47.423977 Test MSE 46.65203643541215 Test RE 0.11498057435058069 Lambda1 0.635997\n",
      "39 Train Loss 44.207195 Test MSE 46.584354896921184 Test RE 0.11489713869251385 Lambda1 0.6880277\n",
      "40 Train Loss 43.30547 Test MSE 46.71437727435238 Test RE 0.11505737263184442 Lambda1 0.717997\n",
      "41 Train Loss 41.4469 Test MSE 42.30938913022296 Test RE 0.10949834323032537 Lambda1 0.77020556\n",
      "42 Train Loss 39.50103 Test MSE 41.641978129938465 Test RE 0.10863126740582016 Lambda1 0.78881747\n",
      "43 Train Loss 38.43345 Test MSE 40.380726561480095 Test RE 0.10697350729171351 Lambda1 0.80523306\n",
      "44 Train Loss 37.403023 Test MSE 39.39954607620087 Test RE 0.10566588134891326 Lambda1 0.8703701\n",
      "45 Train Loss 36.842434 Test MSE 39.32960473255928 Test RE 0.10557205163257531 Lambda1 0.90005887\n",
      "46 Train Loss 36.543026 Test MSE 39.213275103235304 Test RE 0.10541580479873026 Lambda1 0.93084097\n",
      "47 Train Loss 36.293163 Test MSE 38.974041391023 Test RE 0.1050937506438229 Lambda1 0.9900631\n",
      "48 Train Loss 36.07364 Test MSE 38.33631905409324 Test RE 0.10423039318251309 Lambda1 1.0277551\n",
      "49 Train Loss 35.76012 Test MSE 38.17582120854 Test RE 0.1040119802029948 Lambda1 1.0415398\n",
      "50 Train Loss 35.326378 Test MSE 37.754804887333506 Test RE 0.10343684985660484 Lambda1 1.0322058\n",
      "51 Train Loss 35.006622 Test MSE 37.528165508127536 Test RE 0.10312592054630598 Lambda1 1.0556668\n",
      "52 Train Loss 34.93008 Test MSE 37.34783746051502 Test RE 0.10287785500434463 Lambda1 1.0676578\n",
      "53 Train Loss 34.90633 Test MSE 37.20420085901599 Test RE 0.10267983469513524 Lambda1 1.0718291\n",
      "54 Train Loss 34.800247 Test MSE 37.27539407914393 Test RE 0.10277803078761998 Lambda1 1.0566846\n",
      "55 Train Loss 34.74216 Test MSE 37.15677707512345 Test RE 0.10261437140158566 Lambda1 1.0529038\n",
      "56 Train Loss 34.725983 Test MSE 37.178483705108334 Test RE 0.10264434018727259 Lambda1 1.0490164\n",
      "57 Train Loss 34.67689 Test MSE 37.139157762012715 Test RE 0.10259003924383284 Lambda1 1.0484728\n",
      "58 Train Loss 34.58592 Test MSE 37.211724271030484 Test RE 0.1026902160987717 Lambda1 1.0614547\n",
      "59 Train Loss 34.318573 Test MSE 36.687739751146985 Test RE 0.10196465378907091 Lambda1 1.0976841\n",
      "60 Train Loss 34.09264 Test MSE 36.59992689738797 Test RE 0.10184255344309422 Lambda1 1.1413647\n",
      "61 Train Loss 33.826286 Test MSE 36.16378851487818 Test RE 0.10123393795732297 Lambda1 1.173194\n",
      "62 Train Loss 33.722935 Test MSE 36.06273802547117 Test RE 0.10109240279885966 Lambda1 1.1884186\n",
      "63 Train Loss 33.70012 Test MSE 35.96905106383534 Test RE 0.10096100402327532 Lambda1 1.1968349\n",
      "64 Train Loss 33.621876 Test MSE 35.741848815149 Test RE 0.10064163355361827 Lambda1 1.2088692\n",
      "65 Train Loss 33.57808 Test MSE 35.766314245922395 Test RE 0.10067607245075255 Lambda1 1.2120911\n",
      "66 Train Loss 33.521744 Test MSE 35.845063908661544 Test RE 0.10078684494252993 Lambda1 1.2194011\n",
      "67 Train Loss 33.351868 Test MSE 35.49096950079202 Test RE 0.10028779949292282 Lambda1 1.2326213\n",
      "68 Train Loss 33.303642 Test MSE 35.61426202101431 Test RE 0.1004618439869247 Lambda1 1.2334347\n",
      "69 Train Loss 33.212746 Test MSE 35.539918888184204 Test RE 0.10035693447208371 Lambda1 1.2220544\n",
      "70 Train Loss 33.1528 Test MSE 35.46288281691313 Test RE 0.10024810898490642 Lambda1 1.2178084\n",
      "71 Train Loss 33.052845 Test MSE 35.51690499334553 Test RE 0.10032443611494078 Lambda1 1.212386\n",
      "72 Train Loss 33.01784 Test MSE 35.58554521457231 Test RE 0.10042133317815509 Lambda1 1.2044483\n",
      "73 Train Loss 32.950172 Test MSE 35.602889466134656 Test RE 0.1004458026738123 Lambda1 1.1959343\n",
      "74 Train Loss 32.897152 Test MSE 35.65319437849524 Test RE 0.1005167398050812 Lambda1 1.1852617\n",
      "Training time: 166.79\n",
      "Training time: 166.79\n",
      "inv_HT_swish_tune3\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 838.1003 Test MSE 858.3218952528929 Test RE 0.4931900046184488 Lambda1 4.0296236e-06\n",
      "1 Train Loss 837.6303 Test MSE 857.9398440022892 Test RE 0.4930802294680697 Lambda1 0.052837677\n",
      "2 Train Loss 834.58875 Test MSE 857.1175130344011 Test RE 0.4928438653725931 Lambda1 0.26479244\n",
      "3 Train Loss 804.52045 Test MSE 812.5535296883047 Test RE 0.47986067571971075 Lambda1 0.38142452\n",
      "4 Train Loss 771.98615 Test MSE 772.4423732788049 Test RE 0.4678667857822324 Lambda1 0.2174489\n",
      "5 Train Loss 657.2246 Test MSE 634.763773635861 Test RE 0.42412633269286654 Lambda1 0.03214143\n",
      "6 Train Loss 575.1983 Test MSE 559.442765300149 Test RE 0.3981685940254733 Lambda1 0.017329296\n",
      "7 Train Loss 458.86624 Test MSE 456.2779827790847 Test RE 0.3595869541132296 Lambda1 0.0008857355\n",
      "8 Train Loss 377.75174 Test MSE 390.73995932674234 Test RE 0.3327615082738872 Lambda1 0.0017629702\n",
      "9 Train Loss 339.14767 Test MSE 353.5388549075773 Test RE 0.31652480222455665 Lambda1 0.0036134182\n",
      "10 Train Loss 336.05722 Test MSE 350.5086102440112 Test RE 0.3151653877058044 Lambda1 0.0046988344\n",
      "11 Train Loss 326.38443 Test MSE 337.5541602300808 Test RE 0.30928645663524323 Lambda1 0.0071418\n",
      "12 Train Loss 306.30673 Test MSE 318.0170104021819 Test RE 0.30020252855882634 Lambda1 0.005701955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Train Loss 301.22482 Test MSE 305.378636817108 Test RE 0.29417685229793583 Lambda1 0.009230018\n",
      "14 Train Loss 294.66504 Test MSE 300.0606819687382 Test RE 0.29160416089015073 Lambda1 0.0068678046\n",
      "15 Train Loss 290.63235 Test MSE 294.7675408571882 Test RE 0.28902073408977186 Lambda1 0.005447113\n",
      "16 Train Loss 288.10107 Test MSE 289.1281733868959 Test RE 0.286242671744639 Lambda1 0.007083051\n",
      "17 Train Loss 281.39935 Test MSE 286.37338494979974 Test RE 0.28487576018536254 Lambda1 0.006507353\n",
      "18 Train Loss 275.43256 Test MSE 284.8318113109123 Test RE 0.2841079696245112 Lambda1 0.008298273\n",
      "19 Train Loss 272.9829 Test MSE 285.7254248114826 Test RE 0.2845532918989568 Lambda1 0.00823026\n",
      "20 Train Loss 270.08508 Test MSE 278.18793283927846 Test RE 0.2807749213800407 Lambda1 0.02173313\n",
      "21 Train Loss 254.53586 Test MSE 254.56999987959946 Test RE 0.26859181985945 Lambda1 0.069838256\n",
      "22 Train Loss 233.0681 Test MSE 235.52352298048726 Test RE 0.2583487205564557 Lambda1 0.108280025\n",
      "23 Train Loss 225.7859 Test MSE 229.15284073454427 Test RE 0.2548307270752098 Lambda1 0.14374363\n",
      "24 Train Loss 222.4372 Test MSE 227.3279252092015 Test RE 0.25381399490580986 Lambda1 0.17669356\n",
      "25 Train Loss 198.6685 Test MSE 200.03517768054778 Test RE 0.23809065993382075 Lambda1 0.25995392\n",
      "26 Train Loss 189.74835 Test MSE 188.41967360672123 Test RE 0.23107464484130197 Lambda1 0.27692944\n",
      "27 Train Loss 183.0869 Test MSE 181.2386636689609 Test RE 0.22662853756791645 Lambda1 0.34995165\n",
      "28 Train Loss 174.74045 Test MSE 173.96510329158406 Test RE 0.2220343871924888 Lambda1 0.45286292\n",
      "29 Train Loss 165.45908 Test MSE 155.52966959579686 Test RE 0.20994029123302732 Lambda1 0.561512\n",
      "30 Train Loss 151.28302 Test MSE 134.884182654627 Test RE 0.19551031838000377 Lambda1 0.5827322\n",
      "31 Train Loss 150.01486 Test MSE 135.1729705967383 Test RE 0.19571950092775314 Lambda1 0.5778169\n",
      "32 Train Loss 147.20442 Test MSE 130.30168409480703 Test RE 0.19216052873942013 Lambda1 0.6009307\n",
      "33 Train Loss 146.18262 Test MSE 131.01531342292964 Test RE 0.1926860175076344 Lambda1 0.59786767\n",
      "34 Train Loss 139.59756 Test MSE 130.87678743567517 Test RE 0.19258412453273402 Lambda1 0.55177265\n",
      "35 Train Loss 133.8406 Test MSE 122.05273740294778 Test RE 0.1859785822798799 Lambda1 0.57910055\n",
      "36 Train Loss 130.92072 Test MSE 119.58222275916492 Test RE 0.18408672924588346 Lambda1 0.554836\n",
      "37 Train Loss 125.370026 Test MSE 107.91075723627722 Test RE 0.17487250880535118 Lambda1 0.57851315\n",
      "38 Train Loss 118.97591 Test MSE 94.73664877173083 Test RE 0.1638506541444954 Lambda1 0.61986655\n",
      "39 Train Loss 111.38405 Test MSE 91.12251102495557 Test RE 0.16069486905863367 Lambda1 0.62130773\n",
      "40 Train Loss 99.13271 Test MSE 84.73078917613374 Test RE 0.1549564992572999 Lambda1 0.6193772\n",
      "41 Train Loss 88.9152 Test MSE 76.44685192248649 Test RE 0.14718683735067764 Lambda1 0.60548306\n",
      "42 Train Loss 79.26177 Test MSE 69.61707049995557 Test RE 0.14045818193865378 Lambda1 0.61431575\n",
      "43 Train Loss 70.68758 Test MSE 55.96631353121443 Test RE 0.12593676152627747 Lambda1 0.7033451\n",
      "44 Train Loss 68.714485 Test MSE 55.75230389448705 Test RE 0.1256957460528494 Lambda1 0.69035745\n",
      "45 Train Loss 65.520485 Test MSE 55.28734219228791 Test RE 0.12517051151922806 Lambda1 0.66911167\n",
      "46 Train Loss 58.430447 Test MSE 53.45225738338607 Test RE 0.12307566644179567 Lambda1 0.65399045\n",
      "47 Train Loss 51.72203 Test MSE 45.39038451694811 Test RE 0.11341515784281753 Lambda1 0.7057698\n",
      "48 Train Loss 50.62172 Test MSE 46.28094208470661 Test RE 0.11452235391575584 Lambda1 0.69838905\n",
      "49 Train Loss 44.259445 Test MSE 42.90108048416054 Test RE 0.11026134498065834 Lambda1 0.77987015\n",
      "50 Train Loss 43.31796 Test MSE 42.53514221939215 Test RE 0.1097900834673216 Lambda1 0.78111655\n",
      "51 Train Loss 42.535336 Test MSE 42.96207309722001 Test RE 0.11033969660825564 Lambda1 0.77126676\n",
      "52 Train Loss 41.080402 Test MSE 42.06341574059846 Test RE 0.10917958491954677 Lambda1 0.7626461\n",
      "53 Train Loss 40.780678 Test MSE 41.35262099181663 Test RE 0.10825318704626842 Lambda1 0.7789739\n",
      "54 Train Loss 39.649494 Test MSE 40.41975075025217 Test RE 0.1070251847453615 Lambda1 0.79658085\n",
      "55 Train Loss 39.230515 Test MSE 40.39770311918933 Test RE 0.10699599142267156 Lambda1 0.7835625\n",
      "56 Train Loss 39.01821 Test MSE 39.85623721660946 Test RE 0.10627651829960173 Lambda1 0.80226815\n",
      "57 Train Loss 38.61355 Test MSE 39.88106711246762 Test RE 0.10630961756089907 Lambda1 0.81894785\n",
      "58 Train Loss 38.322117 Test MSE 40.134904449908596 Test RE 0.1066474037442359 Lambda1 0.82745546\n",
      "59 Train Loss 38.007336 Test MSE 39.824622087167626 Test RE 0.1062343591209698 Lambda1 0.8469932\n",
      "60 Train Loss 37.76898 Test MSE 40.24213966733153 Test RE 0.10678978266303567 Lambda1 0.8762334\n",
      "61 Train Loss 37.439636 Test MSE 40.353166277274454 Test RE 0.1069369957708179 Lambda1 0.9236886\n",
      "62 Train Loss 37.029987 Test MSE 39.69451397027333 Test RE 0.10606068189138775 Lambda1 0.90959483\n",
      "63 Train Loss 36.864433 Test MSE 39.31007752028016 Test RE 0.1055458400315624 Lambda1 0.9370392\n",
      "64 Train Loss 36.83597 Test MSE 39.212776866733684 Test RE 0.10541513509990383 Lambda1 0.94526654\n",
      "65 Train Loss 36.292885 Test MSE 38.472996576837275 Test RE 0.10441603016295196 Lambda1 0.9832969\n",
      "66 Train Loss 35.518955 Test MSE 38.240465981216005 Test RE 0.10410000695097232 Lambda1 0.97217625\n",
      "67 Train Loss 35.142094 Test MSE 38.09841540413014 Test RE 0.10390647865739906 Lambda1 0.9371223\n",
      "68 Train Loss 34.56109 Test MSE 37.90004060149459 Test RE 0.10363561005571992 Lambda1 0.9747781\n",
      "69 Train Loss 34.000366 Test MSE 37.26022323139395 Test RE 0.10275711365747164 Lambda1 1.0159044\n",
      "70 Train Loss 33.611935 Test MSE 36.84625118685108 Test RE 0.10218468837786494 Lambda1 0.99606365\n",
      "71 Train Loss 33.157906 Test MSE 36.42125298263242 Test RE 0.10159366127678135 Lambda1 1.021701\n",
      "72 Train Loss 32.879032 Test MSE 36.02815652371932 Test RE 0.10104392110027025 Lambda1 1.0443435\n",
      "73 Train Loss 32.731792 Test MSE 35.782392532218886 Test RE 0.10069869872459118 Lambda1 1.0693331\n",
      "74 Train Loss 32.705826 Test MSE 35.60949766962548 Test RE 0.10045512404514605 Lambda1 1.0844829\n",
      "Training time: 142.17\n",
      "Training time: 142.17\n",
      "inv_HT_swish_tune3\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 837.9969 Test MSE 858.2138994802289 Test RE 0.49315897656742547 Lambda1 -3.416468e-05\n",
      "1 Train Loss 837.2544 Test MSE 857.4064816104649 Test RE 0.49292693705546703 Lambda1 0.20753287\n",
      "2 Train Loss 817.4704 Test MSE 834.3445148295933 Test RE 0.48625253470575275 Lambda1 0.17047578\n",
      "3 Train Loss nan Test MSE nan Test RE nan Lambda1 nan\n",
      "NAN BREAK!\n",
      "Training time: 11.05\n",
      "Training time: 11.05\n",
      "inv_HT_swish_tune3\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 838.0568 Test MSE 858.2776432343543 Test RE 0.4931772908949287 Lambda1 7.4155224e-07\n",
      "1 Train Loss 838.0133 Test MSE 857.9783940550487 Test RE 0.49309130720007627 Lambda1 0.00015486771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Train Loss 837.8862 Test MSE 857.9068353584858 Test RE 0.49307074391594785 Lambda1 0.005196389\n",
      "3 Train Loss 837.8512 Test MSE 858.0283918893297 Test RE 0.49310567419257845 Lambda1 0.070225745\n",
      "4 Train Loss 835.0494 Test MSE 854.4344839362351 Test RE 0.49207188787692674 Lambda1 0.68640095\n",
      "5 Train Loss 801.71246 Test MSE 817.9978131960335 Test RE 0.48146557680793445 Lambda1 1.2002178\n",
      "6 Train Loss 767.35785 Test MSE 773.9659511853312 Test RE 0.4683279725119976 Lambda1 1.1339493\n",
      "7 Train Loss 693.2829 Test MSE 698.5074023179649 Test RE 0.44491257114774385 Lambda1 0.98183626\n",
      "8 Train Loss 642.328 Test MSE 626.8297051730186 Test RE 0.4214673681514079 Lambda1 0.78207916\n",
      "9 Train Loss 567.86725 Test MSE 569.5786917733819 Test RE 0.4017593914243617 Lambda1 0.57728636\n",
      "10 Train Loss 477.24466 Test MSE 467.7946117217959 Test RE 0.36409672973391566 Lambda1 0.3725875\n",
      "11 Train Loss 401.19202 Test MSE 359.27683369853844 Test RE 0.319083081804855 Lambda1 0.3480445\n",
      "12 Train Loss 266.0079 Test MSE 246.21025626158848 Test RE 0.26414490656214756 Lambda1 0.46898332\n",
      "13 Train Loss 207.75584 Test MSE 169.83518174453712 Test RE 0.21938301502830598 Lambda1 0.5588413\n",
      "14 Train Loss 142.74196 Test MSE 117.63168199308302 Test RE 0.1825792102141772 Lambda1 0.6675207\n",
      "15 Train Loss 119.37255 Test MSE 113.78698228591382 Test RE 0.1795706937571538 Lambda1 0.64266557\n",
      "16 Train Loss 99.87442 Test MSE 94.02420463295883 Test RE 0.16323339178020793 Lambda1 0.67297274\n",
      "17 Train Loss 89.82142 Test MSE 73.45474429228658 Test RE 0.14427766207506157 Lambda1 0.6905332\n",
      "18 Train Loss 79.931206 Test MSE 68.14730411416046 Test RE 0.138967585085056 Lambda1 0.68974006\n",
      "19 Train Loss 60.556328 Test MSE 53.01485043683667 Test RE 0.1225710597138743 Lambda1 0.66865224\n",
      "20 Train Loss 57.208538 Test MSE 54.429404276509885 Test RE 0.12419552885560987 Lambda1 0.67483145\n",
      "21 Train Loss 50.62401 Test MSE 46.046874245844606 Test RE 0.11423238597457672 Lambda1 0.69113296\n",
      "22 Train Loss 47.93063 Test MSE 45.107498586164155 Test RE 0.11306118752844717 Lambda1 0.7057161\n",
      "23 Train Loss 45.29632 Test MSE 46.4202383336726 Test RE 0.11469456895894727 Lambda1 0.7057873\n",
      "24 Train Loss 44.43423 Test MSE 45.14065629678659 Test RE 0.11310273451709871 Lambda1 0.73585474\n",
      "25 Train Loss 44.184685 Test MSE 45.30208034036073 Test RE 0.11330478305879496 Lambda1 0.72953\n",
      "26 Train Loss 43.3112 Test MSE 43.85861329363088 Test RE 0.11148504634481146 Lambda1 0.761224\n",
      "27 Train Loss 42.421227 Test MSE 43.817971931815244 Test RE 0.11143338084644487 Lambda1 0.7934109\n",
      "28 Train Loss 42.284935 Test MSE 43.51515652573238 Test RE 0.1110476687286243 Lambda1 0.8133922\n",
      "29 Train Loss 40.798935 Test MSE 42.17561470092124 Test RE 0.10932509946997818 Lambda1 0.8648591\n",
      "30 Train Loss 40.22993 Test MSE 42.128355224898016 Test RE 0.10926383071166494 Lambda1 0.8500451\n",
      "31 Train Loss 39.216763 Test MSE 42.01742376022534 Test RE 0.10911988032138009 Lambda1 0.8786852\n",
      "32 Train Loss 38.674168 Test MSE 41.26719204845335 Test RE 0.1081413109866556 Lambda1 0.90161127\n",
      "33 Train Loss 38.20394 Test MSE 41.065919157963876 Test RE 0.1078772692884051 Lambda1 0.92874557\n",
      "34 Train Loss 37.878334 Test MSE 41.01959475853036 Test RE 0.10781640665675069 Lambda1 0.94292474\n",
      "35 Train Loss 37.569355 Test MSE 40.816147527099695 Test RE 0.1075487027066026 Lambda1 0.95931804\n",
      "36 Train Loss 37.36915 Test MSE 40.36414759447185 Test RE 0.10695154517670889 Lambda1 0.9822026\n",
      "37 Train Loss 37.137962 Test MSE 39.51181603107259 Test RE 0.10581632298949745 Lambda1 1.0012517\n",
      "38 Train Loss 36.730362 Test MSE 38.964866814974066 Test RE 0.1050813802644246 Lambda1 0.99652565\n",
      "39 Train Loss 36.179886 Test MSE 38.211493657040556 Test RE 0.10406056456255283 Lambda1 0.96879643\n",
      "40 Train Loss 35.774406 Test MSE 38.25914479404697 Test RE 0.10412542802138725 Lambda1 0.9318578\n",
      "41 Train Loss 35.369144 Test MSE 38.163049106562866 Test RE 0.10399457962448445 Lambda1 0.9010532\n",
      "42 Train Loss 35.1932 Test MSE 38.02244346494244 Test RE 0.10380282715560318 Lambda1 0.89605504\n",
      "43 Train Loss 34.87821 Test MSE 37.45814385273314 Test RE 0.10302966725156293 Lambda1 0.9176232\n",
      "44 Train Loss 34.816616 Test MSE 37.38941839472735 Test RE 0.10293510821660168 Lambda1 0.924319\n",
      "45 Train Loss 34.490437 Test MSE 37.15769863423055 Test RE 0.10261564390998905 Lambda1 0.92864484\n",
      "46 Train Loss 33.754753 Test MSE 36.156795451202 Test RE 0.10122414958018622 Lambda1 0.9661135\n",
      "47 Train Loss 33.10698 Test MSE 35.44652011537716 Test RE 0.10022497889770335 Lambda1 1.0146737\n",
      "48 Train Loss 32.702904 Test MSE 35.170400382222816 Test RE 0.09983385166033451 Lambda1 1.0420407\n",
      "49 Train Loss 32.525227 Test MSE 35.24347418299385 Test RE 0.0999375106106972 Lambda1 1.0564286\n",
      "50 Train Loss 32.47199 Test MSE 35.17066634320375 Test RE 0.09983422913483908 Lambda1 1.0600269\n",
      "51 Train Loss 32.37012 Test MSE 35.17364284584455 Test RE 0.09983845354385248 Lambda1 1.0843688\n",
      "52 Train Loss 32.34399 Test MSE 35.145043531310684 Test RE 0.09979785649807753 Lambda1 1.0981534\n",
      "53 Train Loss 32.189972 Test MSE 35.21565941005967 Test RE 0.099898066601543 Lambda1 1.0931523\n",
      "54 Train Loss 32.126644 Test MSE 35.14985649493551 Test RE 0.09980468970927127 Lambda1 1.103211\n",
      "55 Train Loss 32.041 Test MSE 35.12956182156582 Test RE 0.09977587314735616 Lambda1 1.1049651\n",
      "56 Train Loss 32.016567 Test MSE 35.06529513059756 Test RE 0.09968456542476131 Lambda1 1.1050984\n",
      "57 Train Loss 32.0066 Test MSE 34.99564742447078 Test RE 0.09958551803019985 Lambda1 1.1082947\n",
      "58 Train Loss 31.985739 Test MSE 34.878800981254756 Test RE 0.09941912672615533 Lambda1 1.1092963\n",
      "59 Train Loss 31.945 Test MSE 34.80139291553479 Test RE 0.09930874281451822 Lambda1 1.0974834\n",
      "60 Train Loss 31.923698 Test MSE 34.74849744895092 Test RE 0.09923324325259865 Lambda1 1.0922831\n",
      "61 Train Loss 31.912714 Test MSE 34.7360261279793 Test RE 0.09921543412731053 Lambda1 1.0933863\n",
      "62 Train Loss 31.904436 Test MSE 34.63882207776665 Test RE 0.09907651641667028 Lambda1 1.0882179\n",
      "63 Train Loss 31.879707 Test MSE 34.57070528323375 Test RE 0.09897905213501272 Lambda1 1.0766969\n",
      "64 Train Loss 31.82093 Test MSE 34.54362577854403 Test RE 0.09894027900663838 Lambda1 1.0670211\n",
      "65 Train Loss 31.755215 Test MSE 34.5038502087898 Test RE 0.0988832997608617 Lambda1 1.0446285\n",
      "66 Train Loss 31.736752 Test MSE 34.49263719422889 Test RE 0.09886723097476352 Lambda1 1.0325264\n",
      "67 Train Loss 31.688263 Test MSE 34.310515102906585 Test RE 0.09860587465043281 Lambda1 1.0262312\n",
      "68 Train Loss 31.612745 Test MSE 34.142345281767156 Test RE 0.09836392401749823 Lambda1 1.0182438\n",
      "69 Train Loss 31.556824 Test MSE 34.02709069424759 Test RE 0.0981977597266597 Lambda1 1.0056273\n",
      "70 Train Loss 31.545807 Test MSE 33.98002698607104 Test RE 0.09812982635703668 Lambda1 0.9922986\n",
      "71 Train Loss 31.540064 Test MSE 33.98319956218124 Test RE 0.09813440724033756 Lambda1 0.9913394\n",
      "72 Train Loss 31.513895 Test MSE 33.91506389125543 Test RE 0.09803597906580207 Lambda1 0.9914917\n",
      "73 Train Loss 31.458498 Test MSE 33.7897817651377 Test RE 0.09785473925609524 Lambda1 0.98366386\n",
      "74 Train Loss 31.407692 Test MSE 33.711171872561124 Test RE 0.0977408463970961 Lambda1 0.9765859\n",
      "Training time: 138.62\n",
      "Training time: 138.62\n",
      "inv_HT_swish_tune3\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 838.0562 Test MSE 858.2710175982712 Test RE 0.49317538730371563 Lambda1 4.2754783e-07\n",
      "1 Train Loss 838.0267 Test MSE 858.4202130953174 Test RE 0.49321825042514095 Lambda1 0.000117003205\n",
      "2 Train Loss 837.86597 Test MSE 857.8801887216365 Test RE 0.4930630864533096 Lambda1 0.0017266675\n",
      "3 Train Loss 837.6171 Test MSE 857.1356418583348 Test RE 0.4928490773956137 Lambda1 0.43429673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 821.4248 Test MSE 835.2084363223605 Test RE 0.48650421450978565 Lambda1 0.51040876\n",
      "5 Train Loss 788.1791 Test MSE 790.8666902793005 Test RE 0.47341369027065866 Lambda1 0.3453791\n",
      "6 Train Loss 707.71985 Test MSE 733.8649047697984 Test RE 0.4560340075078166 Lambda1 0.00062048284\n",
      "7 Train Loss 517.2855 Test MSE 497.01251676511157 Test RE 0.3752950477824221 Lambda1 7.893739e-05\n",
      "8 Train Loss 386.46194 Test MSE 389.9219703128174 Test RE 0.3324130183681909 Lambda1 -0.0008515935\n",
      "9 Train Loss 304.67093 Test MSE 326.2022373146082 Test RE 0.30404134028179974 Lambda1 -0.0018015153\n",
      "10 Train Loss 279.58936 Test MSE 305.85624246573263 Test RE 0.2944068055713195 Lambda1 -0.0011468196\n",
      "11 Train Loss 266.37994 Test MSE 286.79288465417676 Test RE 0.28508433676852735 Lambda1 -0.0008004425\n",
      "12 Train Loss 261.696 Test MSE 282.36313952488734 Test RE 0.2828740911815783 Lambda1 -0.00038875823\n",
      "13 Train Loss 261.09155 Test MSE 282.50835960253875 Test RE 0.28294682326189824 Lambda1 -0.0005836951\n",
      "14 Train Loss 260.1523 Test MSE 281.56816084880546 Test RE 0.2824756018855317 Lambda1 -0.00013130967\n",
      "15 Train Loss 259.23083 Test MSE 281.614655384422 Test RE 0.28249892311219443 Lambda1 0.00041081867\n",
      "16 Train Loss 258.3813 Test MSE 280.8694026741727 Test RE 0.2821248790859699 Lambda1 0.0011294018\n",
      "17 Train Loss 258.07538 Test MSE 281.05820824421954 Test RE 0.28221968791775975 Lambda1 0.0013603022\n",
      "18 Train Loss 257.7527 Test MSE 280.7138186755328 Test RE 0.2820467285350096 Lambda1 0.0013562762\n",
      "19 Train Loss 257.4147 Test MSE 279.996489547181 Test RE 0.28168613041521096 Lambda1 0.0014484046\n",
      "20 Train Loss 257.09613 Test MSE 280.5454628738869 Test RE 0.2819621382483452 Lambda1 0.00090765074\n",
      "21 Train Loss 256.6278 Test MSE 280.773359944259 Test RE 0.2820766389429326 Lambda1 0.0008265448\n",
      "22 Train Loss 256.36642 Test MSE 280.21616272162544 Test RE 0.2817966081473557 Lambda1 0.0015419258\n",
      "23 Train Loss 256.05112 Test MSE 279.68251787345025 Test RE 0.2815281529470691 Lambda1 0.0023570885\n",
      "24 Train Loss 255.97076 Test MSE 279.7457615532875 Test RE 0.28155998166142693 Lambda1 0.0023225562\n",
      "25 Train Loss 255.86467 Test MSE 279.5597680484641 Test RE 0.2814663662391696 Lambda1 0.0026257674\n",
      "26 Train Loss 255.76913 Test MSE 278.7828717503008 Test RE 0.28107499677991976 Lambda1 0.0035524913\n",
      "27 Train Loss 255.71646 Test MSE 278.73009853368353 Test RE 0.28104839196441994 Lambda1 0.0034838724\n",
      "28 Train Loss 255.58113 Test MSE 277.88983697617624 Test RE 0.28062444706656414 Lambda1 0.0042719077\n",
      "29 Train Loss 254.78574 Test MSE 272.7269430454228 Test RE 0.27800537512805773 Lambda1 0.012767529\n",
      "30 Train Loss 242.78789 Test MSE 252.75132975449543 Test RE 0.26763067855267697 Lambda1 0.04839817\n",
      "31 Train Loss 237.35799 Test MSE 245.42817256152043 Test RE 0.2637250464342891 Lambda1 0.061870717\n",
      "32 Train Loss 231.73186 Test MSE 234.73603371658987 Test RE 0.2579164554778633 Lambda1 0.08385344\n",
      "33 Train Loss 193.2397 Test MSE 191.95608503225245 Test RE 0.23323306125333249 Lambda1 0.16144362\n",
      "34 Train Loss 156.56137 Test MSE 154.30021990992805 Test RE 0.20910886421758318 Lambda1 0.23384757\n",
      "35 Train Loss 132.12247 Test MSE 129.0493102595243 Test RE 0.19123483896642254 Lambda1 0.2808064\n",
      "36 Train Loss 106.20225 Test MSE 95.3537272988706 Test RE 0.16438341844290424 Lambda1 0.36441764\n",
      "37 Train Loss 93.481224 Test MSE 86.28002573972242 Test RE 0.1563667118169127 Lambda1 0.38778427\n",
      "38 Train Loss 82.62055 Test MSE 72.84295984985991 Test RE 0.14367558125084393 Lambda1 0.4357975\n",
      "39 Train Loss 75.29587 Test MSE 65.06249778035593 Test RE 0.13578585669031318 Lambda1 0.50365365\n",
      "40 Train Loss 71.598564 Test MSE 56.93205630994877 Test RE 0.12701868309571615 Lambda1 0.5408705\n",
      "41 Train Loss 69.56724 Test MSE 55.7507555889738 Test RE 0.12569400068311398 Lambda1 0.5479962\n",
      "42 Train Loss 65.36665 Test MSE 50.30880030712816 Test RE 0.119401876222492 Lambda1 0.62495524\n",
      "43 Train Loss 62.34123 Test MSE 49.99138364109151 Test RE 0.11902460508847322 Lambda1 0.63237584\n",
      "44 Train Loss 58.441666 Test MSE 50.412548915084216 Test RE 0.11952493022414531 Lambda1 0.6841261\n",
      "45 Train Loss 53.76898 Test MSE 48.09496981109612 Test RE 0.11674519068723818 Lambda1 0.74060863\n",
      "46 Train Loss 48.54403 Test MSE 45.01778479197587 Test RE 0.1129486985094761 Lambda1 0.73618823\n",
      "47 Train Loss 46.219666 Test MSE 44.03249040248144 Test RE 0.11170581851994509 Lambda1 0.7398422\n",
      "48 Train Loss 44.81686 Test MSE 44.11892233067413 Test RE 0.11181539914877078 Lambda1 0.7320078\n",
      "49 Train Loss 44.284237 Test MSE 44.3268977844971 Test RE 0.1120786367332462 Lambda1 0.7201452\n",
      "50 Train Loss 41.976524 Test MSE 43.82405432414316 Test RE 0.1114411146368475 Lambda1 0.73519033\n",
      "51 Train Loss 41.613747 Test MSE 43.16726338999362 Test RE 0.11060287834060877 Lambda1 0.7394333\n",
      "52 Train Loss 41.178593 Test MSE 42.652328069468794 Test RE 0.10994121726293067 Lambda1 0.7557884\n",
      "53 Train Loss 40.75632 Test MSE 42.124741401681945 Test RE 0.10925914421685334 Lambda1 0.7678021\n",
      "54 Train Loss 40.518555 Test MSE 41.90547278446801 Test RE 0.10897441417957796 Lambda1 0.76891094\n",
      "55 Train Loss 40.07978 Test MSE 41.984576695185794 Test RE 0.10907721982177913 Lambda1 0.7668677\n",
      "56 Train Loss 39.319706 Test MSE 41.528508846400484 Test RE 0.10848316300470509 Lambda1 0.7927481\n",
      "57 Train Loss 38.643345 Test MSE 40.88569550593741 Test RE 0.10764029159724142 Lambda1 0.8279227\n",
      "58 Train Loss 38.15414 Test MSE 40.627841284023205 Test RE 0.10730032668638621 Lambda1 0.84275293\n",
      "59 Train Loss 37.844807 Test MSE 40.26375561832056 Test RE 0.10681845972677857 Lambda1 0.8580243\n",
      "60 Train Loss 37.505608 Test MSE 39.85203622583244 Test RE 0.10627091718833549 Lambda1 0.887295\n",
      "61 Train Loss 37.19329 Test MSE 39.706923645430365 Test RE 0.1060772594436058 Lambda1 0.9068515\n",
      "62 Train Loss 36.797447 Test MSE 39.27047466293845 Test RE 0.10549266066379803 Lambda1 0.9375733\n",
      "63 Train Loss 36.39513 Test MSE 39.12305796648565 Test RE 0.10529447103617134 Lambda1 0.9588774\n",
      "64 Train Loss 36.022247 Test MSE 39.334987379828 Test RE 0.10557927567763832 Lambda1 0.9701275\n",
      "65 Train Loss 35.95768 Test MSE 39.44134662419481 Test RE 0.10572191906116314 Lambda1 0.97213495\n",
      "66 Train Loss 35.91277 Test MSE 39.4434213640075 Test RE 0.10572469967857141 Lambda1 0.9837092\n",
      "67 Train Loss 35.713158 Test MSE 39.17428458568629 Test RE 0.10536338327819413 Lambda1 1.0204968\n",
      "68 Train Loss 35.529236 Test MSE 39.32558555732852 Test RE 0.10556665717948113 Lambda1 1.044235\n",
      "69 Train Loss 35.449932 Test MSE 39.15743803011863 Test RE 0.1053407255451574 Lambda1 1.0538392\n",
      "70 Train Loss 35.36347 Test MSE 39.001323919110064 Test RE 0.10513052796406003 Lambda1 1.087783\n",
      "71 Train Loss 35.083427 Test MSE 38.97770249050663 Test RE 0.10509868661693667 Lambda1 1.114037\n",
      "72 Train Loss 34.916447 Test MSE 38.89895342508735 Test RE 0.10499246424632933 Lambda1 1.0887856\n",
      "73 Train Loss 34.874325 Test MSE 38.87367310947925 Test RE 0.10495834155426197 Lambda1 1.0870686\n",
      "74 Train Loss 34.825813 Test MSE 38.79718138673698 Test RE 0.10485502744728624 Lambda1 1.0903212\n",
      "Training time: 124.68\n",
      "Training time: 124.68\n",
      "inv_HT_swish_tune3\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 838.07935 Test MSE 858.317709114277 Test RE 0.4931888019436411 Lambda1 1.7781e-07\n",
      "1 Train Loss 837.94904 Test MSE 857.9328158361053 Test RE 0.4930782098294458 Lambda1 -0.00015119465\n",
      "2 Train Loss 837.90717 Test MSE 857.9533180634425 Test RE 0.49308410139867553 Lambda1 -0.0010191959\n",
      "3 Train Loss 837.8019 Test MSE 858.1618966123026 Test RE 0.49314403503032855 Lambda1 -0.16514781\n",
      "4 Train Loss 837.3787 Test MSE 857.7897360927369 Test RE 0.49303709213112584 Lambda1 -0.35810432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 835.23035 Test MSE 856.733277420906 Test RE 0.49273338496020946 Lambda1 -0.40812176\n",
      "6 Train Loss 821.95087 Test MSE 847.9116276720896 Test RE 0.4901900212222997 Lambda1 -0.3128498\n",
      "7 Train Loss 752.57904 Test MSE 779.8353979364474 Test RE 0.4701004240405049 Lambda1 -0.020165743\n",
      "8 Train Loss 651.35223 Test MSE 680.6163461697494 Test RE 0.4391777791627534 Lambda1 0.002386738\n",
      "9 Train Loss 635.7324 Test MSE 659.0669249424624 Test RE 0.4321693165047731 Lambda1 0.0005768322\n",
      "10 Train Loss 616.77045 Test MSE 661.2185907823304 Test RE 0.4328741965442468 Lambda1 0.00014166592\n",
      "11 Train Loss 611.9623 Test MSE 655.4842530906054 Test RE 0.43099308486270127 Lambda1 2.8520026e-06\n",
      "12 Train Loss 588.2654 Test MSE 596.3217875678814 Test RE 0.41108299243773494 Lambda1 7.927627e-05\n",
      "13 Train Loss 461.05475 Test MSE 452.5883568902363 Test RE 0.35813012909618624 Lambda1 0.0013416493\n",
      "14 Train Loss 319.4477 Test MSE 333.2946768810633 Test RE 0.30732887018364585 Lambda1 0.002618776\n",
      "15 Train Loss 282.16592 Test MSE 301.0007466268534 Test RE 0.2920605892294249 Lambda1 0.0021987597\n",
      "16 Train Loss 265.65417 Test MSE 285.9716897920655 Test RE 0.2846758928506246 Lambda1 0.0024474633\n",
      "17 Train Loss 259.24716 Test MSE 281.1095104242 Test RE 0.2822454438362165 Lambda1 0.0021897648\n",
      "18 Train Loss 258.4236 Test MSE 282.01779153719116 Test RE 0.2827010517989089 Lambda1 0.0022672482\n",
      "19 Train Loss 257.82962 Test MSE 280.95209305061064 Test RE 0.28216640602948717 Lambda1 0.0022414962\n",
      "20 Train Loss 256.81982 Test MSE 279.7612725581385 Test RE 0.2815677873519956 Lambda1 0.0038679084\n",
      "21 Train Loss 256.65308 Test MSE 279.03504629129014 Test RE 0.28120209199273793 Lambda1 0.00416158\n",
      "22 Train Loss 255.70905 Test MSE 278.028943285281 Test RE 0.28069467588003283 Lambda1 0.006145891\n",
      "23 Train Loss 255.56659 Test MSE 278.7401496655629 Test RE 0.28105345928402603 Lambda1 0.005174555\n",
      "24 Train Loss 254.82733 Test MSE 277.4166145993007 Test RE 0.28038540566572445 Lambda1 0.0075900527\n",
      "25 Train Loss 253.93063 Test MSE 274.6183609350849 Test RE 0.2789677219474951 Lambda1 0.018090317\n",
      "26 Train Loss 249.99095 Test MSE 267.30531373195095 Test RE 0.27522822335659614 Lambda1 0.037736416\n",
      "27 Train Loss 239.80159 Test MSE 256.49973154753826 Test RE 0.26960790895486914 Lambda1 0.063211\n",
      "28 Train Loss 227.62715 Test MSE 239.47910760383834 Test RE 0.2605091526000782 Lambda1 0.11569137\n",
      "29 Train Loss 212.17865 Test MSE 220.04337395900077 Test RE 0.24971424550211357 Lambda1 0.17134379\n",
      "30 Train Loss 204.88675 Test MSE 216.41796509672943 Test RE 0.2476485703752665 Lambda1 0.19715554\n",
      "31 Train Loss 194.71599 Test MSE 208.71807290291724 Test RE 0.2432031517280084 Lambda1 0.23261037\n",
      "32 Train Loss 178.87761 Test MSE 185.25458599013135 Test RE 0.2291256207242982 Lambda1 0.2923634\n",
      "33 Train Loss 164.58847 Test MSE 176.8424906469121 Test RE 0.22386308381534106 Lambda1 0.35274914\n",
      "34 Train Loss 150.185 Test MSE 149.00697711006725 Test RE 0.2054908425275175 Lambda1 0.42450738\n",
      "35 Train Loss 134.0477 Test MSE 125.69514824504664 Test RE 0.18873325405744565 Lambda1 0.47249153\n",
      "36 Train Loss 118.096085 Test MSE 113.07897270684214 Test RE 0.17901115629174658 Lambda1 0.4820245\n",
      "37 Train Loss 109.244156 Test MSE 102.83552590777573 Test RE 0.17071070556910514 Lambda1 0.50873625\n",
      "38 Train Loss 100.294304 Test MSE 99.78975378066926 Test RE 0.16816365812826456 Lambda1 0.5255907\n",
      "39 Train Loss 92.954384 Test MSE 96.23032300225172 Test RE 0.16513728585763304 Lambda1 0.54027164\n",
      "40 Train Loss 85.842514 Test MSE 82.50907784394578 Test RE 0.1529114608093444 Lambda1 0.5487756\n",
      "41 Train Loss 82.47528 Test MSE 78.18486082126839 Test RE 0.14885057078319883 Lambda1 0.5731127\n",
      "42 Train Loss 77.728516 Test MSE 70.78981169242604 Test RE 0.1416362922096214 Lambda1 0.61841667\n",
      "43 Train Loss 67.549614 Test MSE 60.03288860300698 Test RE 0.13043189109488185 Lambda1 0.6679923\n",
      "44 Train Loss 61.444508 Test MSE 57.194916401067935 Test RE 0.12731157338039387 Lambda1 0.66100913\n",
      "45 Train Loss 55.25946 Test MSE 51.49032910883206 Test RE 0.12079584731261515 Lambda1 0.6946413\n",
      "46 Train Loss 52.43044 Test MSE 49.26839033946139 Test RE 0.1181607822428265 Lambda1 0.7170148\n",
      "47 Train Loss 49.81722 Test MSE 45.720643522865316 Test RE 0.11382701259545851 Lambda1 0.7651346\n",
      "48 Train Loss 47.658398 Test MSE 43.25287795571703 Test RE 0.1107125045501249 Lambda1 0.7985011\n",
      "49 Train Loss 46.359825 Test MSE 41.74532393193228 Test RE 0.10876598277160468 Lambda1 0.8073503\n",
      "50 Train Loss 45.419174 Test MSE 40.85043332020978 Test RE 0.10759386398174813 Lambda1 0.8290694\n",
      "51 Train Loss 44.337776 Test MSE 41.38258039819597 Test RE 0.10829239392051003 Lambda1 0.8227426\n",
      "52 Train Loss 42.92821 Test MSE 41.080779878811 Test RE 0.10789678655491597 Lambda1 0.8371749\n",
      "53 Train Loss 40.751583 Test MSE 41.134805147759565 Test RE 0.10796771069159487 Lambda1 0.8699551\n",
      "54 Train Loss 40.185013 Test MSE 39.66362313864553 Test RE 0.10601940489863389 Lambda1 0.8804673\n",
      "55 Train Loss 39.360477 Test MSE 38.699876514604405 Test RE 0.10472345461092214 Lambda1 0.9124982\n",
      "56 Train Loss 38.90297 Test MSE 38.40051396740006 Test RE 0.1043176245923623 Lambda1 0.928536\n",
      "57 Train Loss 38.35272 Test MSE 37.85331894808058 Test RE 0.10357171144027896 Lambda1 0.92168546\n",
      "58 Train Loss 38.055096 Test MSE 37.53416485307564 Test RE 0.10313416319869519 Lambda1 0.9132445\n",
      "59 Train Loss 37.71832 Test MSE 37.06708886491383 Test RE 0.102490452423262 Lambda1 0.9194067\n",
      "60 Train Loss 36.972195 Test MSE 36.77707742153396 Test RE 0.10208872447276418 Lambda1 0.9203262\n",
      "61 Train Loss 36.622917 Test MSE 36.4611354261424 Test RE 0.10164927022098622 Lambda1 0.929102\n",
      "62 Train Loss 36.164093 Test MSE 36.602296432532164 Test RE 0.10184585011093339 Lambda1 0.9484189\n",
      "63 Train Loss 35.86648 Test MSE 36.65717320845773 Test RE 0.10192216880077662 Lambda1 0.9551222\n",
      "64 Train Loss 35.21667 Test MSE 36.54578408163612 Test RE 0.10176719696242055 Lambda1 0.96679956\n",
      "65 Train Loss 34.42415 Test MSE 36.407136331831126 Test RE 0.10157397083346273 Lambda1 0.975174\n",
      "66 Train Loss 34.04415 Test MSE 36.1746489530965 Test RE 0.10124913772540663 Lambda1 0.98072225\n",
      "67 Train Loss 33.695244 Test MSE 35.85006708437846 Test RE 0.10079387850093055 Lambda1 0.9813213\n",
      "68 Train Loss 33.45988 Test MSE 35.7182882306436 Test RE 0.10060845722556046 Lambda1 0.9829813\n",
      "69 Train Loss 33.359352 Test MSE 35.710013548514155 Test RE 0.10059680281453762 Lambda1 0.98313075\n",
      "70 Train Loss 33.316444 Test MSE 35.641389730488605 Test RE 0.10050009805690151 Lambda1 0.98289555\n",
      "71 Train Loss 33.16275 Test MSE 35.505605633893005 Test RE 0.100308476221459 Lambda1 0.9850333\n",
      "72 Train Loss 32.98281 Test MSE 35.41734904696061 Test RE 0.10018372983474308 Lambda1 0.98386586\n",
      "73 Train Loss 32.839886 Test MSE 35.18957848769011 Test RE 0.0998610672049376 Lambda1 0.9818645\n",
      "74 Train Loss 32.68741 Test MSE 35.0939492220716 Test RE 0.0997252864190262 Lambda1 0.9735729\n",
      "Training time: 139.53\n",
      "Training time: 139.53\n",
      "inv_HT_swish_tune3\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 838.06274 Test MSE 858.2722570849565 Test RE 0.49317574341739817 Lambda1 5.080225e-07\n",
      "1 Train Loss 837.9581 Test MSE 857.9190788561573 Test RE 0.49307426229854934 Lambda1 3.602161e-05\n",
      "2 Train Loss 837.89923 Test MSE 857.9313291512706 Test RE 0.4930777826093877 Lambda1 0.00044167668\n",
      "3 Train Loss 837.86487 Test MSE 858.0551727731732 Test RE 0.49311336956889945 Lambda1 0.027688023\n",
      "4 Train Loss 837.62445 Test MSE 858.1712616827909 Test RE 0.49314672584896513 Lambda1 -0.04572312\n",
      "5 Train Loss 833.3793 Test MSE 857.0019181276593 Test RE 0.4928106306289682 Lambda1 -0.38495943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 773.16174 Test MSE 788.3105152552472 Test RE 0.4726480064525424 Lambda1 -0.59709275\n",
      "7 Train Loss 721.51013 Test MSE 704.0188076576705 Test RE 0.44666436045358104 Lambda1 -0.8705631\n",
      "8 Train Loss 665.1167 Test MSE 646.3020563576827 Test RE 0.42796370581527604 Lambda1 -1.099232\n",
      "9 Train Loss 563.5038 Test MSE 527.5676467293738 Test RE 0.38665911078750936 Lambda1 -1.4256624\n",
      "10 Train Loss 476.63412 Test MSE 458.8064128591771 Test RE 0.36058188969048993 Lambda1 -1.4677876\n",
      "11 Train Loss 440.03964 Test MSE 439.9365011964763 Test RE 0.35308898453625903 Lambda1 -1.394042\n",
      "12 Train Loss 408.83292 Test MSE 408.89098990571875 Test RE 0.34040265678935216 Lambda1 -1.409996\n",
      "13 Train Loss 368.6162 Test MSE 384.0805440813242 Test RE 0.32991368046331215 Lambda1 -1.4977255\n",
      "14 Train Loss 347.3574 Test MSE 377.3004867641375 Test RE 0.32698878184849595 Lambda1 -1.7362977\n",
      "15 Train Loss 337.4466 Test MSE 368.44789441518935 Test RE 0.32312994755618485 Lambda1 -1.8639203\n",
      "16 Train Loss 329.57584 Test MSE 358.47282435020605 Test RE 0.3187258510492694 Lambda1 -1.910344\n",
      "17 Train Loss 325.0951 Test MSE 359.0937471935461 Test RE 0.31900176950566045 Lambda1 -1.9609128\n",
      "18 Train Loss 317.7565 Test MSE 351.8954982421889 Test RE 0.31578829333088876 Lambda1 -2.0947125\n",
      "19 Train Loss 313.41495 Test MSE 347.56940764507567 Test RE 0.3138411904702246 Lambda1 -2.1342673\n",
      "20 Train Loss 311.01065 Test MSE 346.0057829583833 Test RE 0.31313444962384235 Lambda1 -2.2123845\n",
      "21 Train Loss 303.2341 Test MSE 339.52113147537085 Test RE 0.3101862735635548 Lambda1 -2.327962\n",
      "22 Train Loss 296.87723 Test MSE 331.0899317460857 Test RE 0.306310692949718 Lambda1 -2.4886508\n",
      "23 Train Loss 289.9618 Test MSE 324.68596065526947 Test RE 0.30333388381839194 Lambda1 -2.706214\n",
      "24 Train Loss 289.03442 Test MSE 324.9317848333045 Test RE 0.3034486912063729 Lambda1 -2.7590017\n",
      "25 Train Loss 287.89484 Test MSE 324.583099626495 Test RE 0.3032858316839071 Lambda1 -2.7795095\n",
      "26 Train Loss 287.41476 Test MSE 323.92657036062195 Test RE 0.30297895062040436 Lambda1 -2.840485\n",
      "27 Train Loss 286.71823 Test MSE 323.6383995048773 Test RE 0.302844152894356 Lambda1 -2.8667617\n",
      "28 Train Loss 286.48605 Test MSE 323.56736649429706 Test RE 0.30281091655334547 Lambda1 -2.8789546\n",
      "29 Train Loss 286.05743 Test MSE 322.9609151677533 Test RE 0.3025270093528894 Lambda1 -2.8780742\n",
      "30 Train Loss 285.83566 Test MSE 323.44471692441226 Test RE 0.3027535202385928 Lambda1 -2.871497\n",
      "31 Train Loss 285.66275 Test MSE 323.15972983584646 Test RE 0.30262011280656936 Lambda1 -2.8545573\n",
      "32 Train Loss 285.5771 Test MSE 323.2603724070994 Test RE 0.30266723206762947 Lambda1 -2.869636\n",
      "33 Train Loss 285.2677 Test MSE 323.3805192346618 Test RE 0.30272347330929444 Lambda1 -2.9177692\n",
      "34 Train Loss 284.86255 Test MSE 323.0318357992836 Test RE 0.30256022425259477 Lambda1 -2.966562\n",
      "35 Train Loss 284.36633 Test MSE 321.91270547373205 Test RE 0.3020356658902884 Lambda1 -3.0516412\n",
      "36 Train Loss 284.22995 Test MSE 321.28011213393296 Test RE 0.3017387535504828 Lambda1 -3.0883524\n",
      "37 Train Loss 284.11575 Test MSE 321.69622812193404 Test RE 0.30193409351383815 Lambda1 -3.105628\n",
      "38 Train Loss 283.6839 Test MSE 321.52933975177734 Test RE 0.3018557652319945 Lambda1 -3.2050474\n",
      "39 Train Loss 283.58 Test MSE 321.34802198815777 Test RE 0.3017706415374709 Lambda1 -3.2728665\n",
      "40 Train Loss 283.4257 Test MSE 320.80866712750424 Test RE 0.301517287224067 Lambda1 -3.3378215\n",
      "41 Train Loss 283.39185 Test MSE 320.53239263193336 Test RE 0.30138742870225355 Lambda1 -3.3761375\n",
      "42 Train Loss 282.80756 Test MSE 319.0875693096374 Test RE 0.30070739852021716 Lambda1 -3.5128872\n",
      "43 Train Loss 282.47412 Test MSE 318.4452729474263 Test RE 0.30040459675047015 Lambda1 -3.5605795\n",
      "44 Train Loss 282.14703 Test MSE 318.91248995860565 Test RE 0.300624890010004 Lambda1 -3.4771757\n",
      "45 Train Loss 281.98987 Test MSE 318.984437763541 Test RE 0.3006587991259083 Lambda1 -3.442789\n",
      "46 Train Loss 281.92908 Test MSE 318.50449341742626 Test RE 0.3004325281972565 Lambda1 -3.4678512\n",
      "47 Train Loss 281.8285 Test MSE 317.8916030574517 Test RE 0.3001433315457856 Lambda1 -3.5032928\n",
      "48 Train Loss 281.47147 Test MSE 317.4789234276876 Test RE 0.2999484486629037 Lambda1 -3.5740712\n",
      "49 Train Loss 281.4068 Test MSE 317.2772764828614 Test RE 0.2998531773063993 Lambda1 -3.598942\n",
      "50 Train Loss 280.98816 Test MSE 317.4737557841483 Test RE 0.29994600750404876 Lambda1 -3.6169856\n",
      "51 Train Loss 280.69174 Test MSE 316.9836319413511 Test RE 0.2997143860382173 Lambda1 -3.6625597\n",
      "52 Train Loss 280.62775 Test MSE 317.0108608570196 Test RE 0.2997272585049188 Lambda1 -3.6800594\n",
      "53 Train Loss 280.5195 Test MSE 317.22697661700107 Test RE 0.2998294076047152 Lambda1 -3.6891992\n",
      "54 Train Loss 280.46637 Test MSE 317.337437358507 Test RE 0.2998816044519728 Lambda1 -3.705516\n",
      "55 Train Loss 280.44043 Test MSE 317.0998613184947 Test RE 0.29976932960653524 Lambda1 -3.7037733\n",
      "56 Train Loss 280.29767 Test MSE 316.74168420817637 Test RE 0.29959998098436447 Lambda1 -3.7151155\n",
      "57 Train Loss 280.1516 Test MSE 317.09384702908784 Test RE 0.2997664867985239 Lambda1 -3.7059605\n",
      "58 Train Loss 279.9571 Test MSE 317.1811362264497 Test RE 0.29980774363109847 Lambda1 -3.7044156\n",
      "59 Train Loss 279.80243 Test MSE 317.13953764981875 Test RE 0.2997880829657645 Lambda1 -3.7314627\n",
      "60 Train Loss 279.54144 Test MSE 316.9543622379401 Test RE 0.2997005481727519 Lambda1 -3.748847\n",
      "61 Train Loss 279.37262 Test MSE 316.35244489307007 Test RE 0.2994158373332262 Lambda1 -3.7829828\n",
      "62 Train Loss 279.30798 Test MSE 316.139357903273 Test RE 0.29931498088670044 Lambda1 -3.8137872\n",
      "63 Train Loss 279.2557 Test MSE 315.846681310816 Test RE 0.29917639837378107 Lambda1 -3.8222196\n",
      "64 Train Loss 279.21466 Test MSE 316.1542412869046 Test RE 0.2993220264619278 Lambda1 -3.8171554\n",
      "65 Train Loss 279.0456 Test MSE 315.7526724092528 Test RE 0.2991318714887975 Lambda1 -3.8232646\n",
      "66 Train Loss 278.93832 Test MSE 315.6434881068551 Test RE 0.2990801485241489 Lambda1 -3.8303888\n",
      "67 Train Loss 278.79233 Test MSE 315.7324242525288 Test RE 0.2991222801746946 Lambda1 -3.819675\n",
      "68 Train Loss 278.703 Test MSE 315.95872540050584 Test RE 0.29922945888842445 Lambda1 -3.7782032\n",
      "69 Train Loss 278.6255 Test MSE 315.6915290520008 Test RE 0.29910290766046305 Lambda1 -3.7766113\n",
      "70 Train Loss 278.47845 Test MSE 315.2633752198368 Test RE 0.2989000110120529 Lambda1 -3.7954547\n",
      "71 Train Loss 278.3706 Test MSE 314.96974204877387 Test RE 0.29876078233722897 Lambda1 -3.8053823\n",
      "72 Train Loss 278.10867 Test MSE 314.6342130191507 Test RE 0.2986016089107548 Lambda1 -3.8300326\n",
      "73 Train Loss 277.8557 Test MSE 313.97180067297944 Test RE 0.298287114171115 Lambda1 -3.848159\n",
      "74 Train Loss 277.6367 Test MSE 313.2283860019009 Test RE 0.29793376645617725 Lambda1 -3.8489451\n",
      "Training time: 145.32\n",
      "Training time: 145.32\n",
      "inv_HT_swish_tune3\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 838.0657 Test MSE 858.3166264352939 Test RE 0.4931884908902205 Lambda1 -4.792401e-07\n",
      "1 Train Loss 837.8588 Test MSE 857.8495485251256 Test RE 0.49305428121164646 Lambda1 -8.361698e-06\n",
      "2 Train Loss 837.84705 Test MSE 857.8844982996203 Test RE 0.4930643249078056 Lambda1 -0.001272845\n",
      "3 Train Loss 837.41266 Test MSE 857.7906638482092 Test RE 0.49303735875691845 Lambda1 -0.01708113\n",
      "4 Train Loss 836.6185 Test MSE 857.2448753494577 Test RE 0.49288048077425994 Lambda1 -0.0299722\n",
      "5 Train Loss 833.0926 Test MSE 855.4742503807291 Test RE 0.4923711994608748 Lambda1 -0.066567056\n",
      "6 Train Loss 785.11084 Test MSE 800.4914020541349 Test RE 0.4762856604951093 Lambda1 -0.115449496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 735.4386 Test MSE 731.3182290815679 Test RE 0.4552420497383682 Lambda1 0.0030283397\n",
      "8 Train Loss 566.3288 Test MSE 582.8860426734931 Test RE 0.40642554681820603 Lambda1 0.00021270709\n",
      "9 Train Loss 481.98047 Test MSE 508.4186104302028 Test RE 0.37957700111825643 Lambda1 -0.00021615611\n",
      "10 Train Loss 409.10602 Test MSE 424.0723772278493 Test RE 0.3466643345983449 Lambda1 0.0006797267\n",
      "11 Train Loss 279.10327 Test MSE 289.1848378910301 Test RE 0.2862707198664096 Lambda1 0.0019136409\n",
      "12 Train Loss 263.83963 Test MSE 281.56271308514647 Test RE 0.2824728692119273 Lambda1 0.0027190524\n",
      "13 Train Loss 261.2684 Test MSE 279.07707000899796 Test RE 0.2812232662369706 Lambda1 0.003453281\n",
      "14 Train Loss 258.83255 Test MSE 276.82931792250446 Test RE 0.2800884575868997 Lambda1 0.0051063993\n",
      "15 Train Loss 257.79587 Test MSE 275.5205400398811 Test RE 0.27942558005126483 Lambda1 0.0064046304\n",
      "16 Train Loss 253.19075 Test MSE 268.2477409386047 Test RE 0.27571297680327184 Lambda1 0.017757606\n",
      "17 Train Loss 245.7134 Test MSE 259.77691525876395 Test RE 0.2713247730073312 Lambda1 0.038518872\n",
      "18 Train Loss 242.7738 Test MSE 251.37025697883703 Test RE 0.26689848905143765 Lambda1 0.054318063\n",
      "19 Train Loss 236.98364 Test MSE 252.2820168403334 Test RE 0.26738209253492934 Lambda1 0.06806065\n",
      "20 Train Loss 221.60706 Test MSE 229.36530230997587 Test RE 0.254948834260477 Lambda1 0.09513494\n",
      "21 Train Loss 214.01471 Test MSE 228.49569697292023 Test RE 0.254465074501491 Lambda1 0.10170932\n",
      "22 Train Loss 201.0671 Test MSE 213.20023840835154 Test RE 0.24580064242875965 Lambda1 0.13310435\n",
      "23 Train Loss 193.15009 Test MSE 206.76478650263846 Test RE 0.2420624693219068 Lambda1 0.15051314\n",
      "24 Train Loss 183.63466 Test MSE 196.73766150292647 Test RE 0.23612008075468643 Lambda1 0.18498687\n",
      "25 Train Loss 180.63313 Test MSE 187.30396890248548 Test RE 0.23038948854397415 Lambda1 0.22264671\n",
      "26 Train Loss 162.75356 Test MSE 173.92496198114145 Test RE 0.22200876922286247 Lambda1 0.2809913\n",
      "27 Train Loss 140.84726 Test MSE 150.86617889969082 Test RE 0.20676885176562615 Lambda1 0.32807034\n",
      "28 Train Loss 130.84468 Test MSE 143.77981117610335 Test RE 0.20185435563182066 Lambda1 0.34945533\n",
      "29 Train Loss 126.85007 Test MSE 135.18058909142687 Test RE 0.19572501633132247 Lambda1 0.36099604\n",
      "30 Train Loss 115.71616 Test MSE 117.5828040300525 Test RE 0.18254127389057284 Lambda1 0.38338503\n",
      "31 Train Loss 108.81062 Test MSE 111.81960430323416 Test RE 0.17801153542255788 Lambda1 0.39371157\n",
      "32 Train Loss 80.26193 Test MSE 65.51157369258715 Test RE 0.13625366302951628 Lambda1 0.5354974\n",
      "33 Train Loss 63.546867 Test MSE 57.55221079969276 Test RE 0.1277086094803489 Lambda1 0.57877636\n",
      "34 Train Loss 56.26458 Test MSE 51.047074951604536 Test RE 0.12027478837980894 Lambda1 0.5905735\n",
      "35 Train Loss 49.39399 Test MSE 50.64745339378335 Test RE 0.11980307833289347 Lambda1 0.5829406\n",
      "36 Train Loss 46.501328 Test MSE 46.18911583250021 Test RE 0.11440868531317164 Lambda1 0.63734996\n",
      "37 Train Loss 45.499557 Test MSE 45.56009503098923 Test RE 0.11362698450067392 Lambda1 0.6509744\n",
      "38 Train Loss 44.740185 Test MSE 44.12266085303575 Test RE 0.11182013652100033 Lambda1 0.69000494\n",
      "39 Train Loss 43.536846 Test MSE 44.278801615055706 Test RE 0.11201781569230837 Lambda1 0.6895636\n",
      "40 Train Loss 42.61476 Test MSE 43.770400627073585 Test RE 0.11137287518355868 Lambda1 0.7236052\n",
      "41 Train Loss 42.40773 Test MSE 42.99998595041677 Test RE 0.11038837175256831 Lambda1 0.74626344\n",
      "42 Train Loss 41.85473 Test MSE 42.74240205047276 Test RE 0.11005724399376134 Lambda1 0.76431966\n",
      "43 Train Loss 40.678173 Test MSE 42.32339927325761 Test RE 0.10951647112705878 Lambda1 0.77913654\n",
      "44 Train Loss 40.40559 Test MSE 42.252234928447045 Test RE 0.10942435958881819 Lambda1 0.8019983\n",
      "45 Train Loss 39.863216 Test MSE 42.15427594691176 Test RE 0.10929743945158012 Lambda1 0.8047854\n",
      "46 Train Loss 39.33453 Test MSE 41.789014468346636 Test RE 0.10882288497003961 Lambda1 0.83600545\n",
      "47 Train Loss 39.23454 Test MSE 41.49390228367369 Test RE 0.10843795295581495 Lambda1 0.8652544\n",
      "48 Train Loss 39.152145 Test MSE 41.32299918220403 Test RE 0.10821440800789965 Lambda1 0.8909508\n",
      "49 Train Loss 38.472153 Test MSE 41.16462160697195 Test RE 0.10800683366400145 Lambda1 0.9211568\n",
      "50 Train Loss 38.049843 Test MSE 40.762844195860424 Test RE 0.10747845383044237 Lambda1 0.90034634\n",
      "51 Train Loss 37.828922 Test MSE 40.76830786389523 Test RE 0.10748565655302249 Lambda1 0.89956117\n",
      "52 Train Loss 37.54129 Test MSE 40.92997686405599 Test RE 0.10769856587090126 Lambda1 0.9010049\n",
      "53 Train Loss 37.1135 Test MSE 40.674032529182654 Test RE 0.10736130614752074 Lambda1 0.9176141\n",
      "54 Train Loss 36.90969 Test MSE 40.63649502577628 Test RE 0.10731175357799265 Lambda1 0.9211077\n",
      "55 Train Loss 36.855286 Test MSE 40.46410682975456 Test RE 0.10708389262760343 Lambda1 0.92626464\n",
      "56 Train Loss 36.819553 Test MSE 40.433441939744796 Test RE 0.10704330927600492 Lambda1 0.9386401\n",
      "57 Train Loss 36.733406 Test MSE 40.37331589277675 Test RE 0.10696369095525105 Lambda1 0.96677893\n",
      "58 Train Loss 36.705296 Test MSE 40.374013455177064 Test RE 0.10696461500031734 Lambda1 0.9631575\n",
      "59 Train Loss 36.500134 Test MSE 40.468146924556365 Test RE 0.10708923833165515 Lambda1 0.973515\n",
      "60 Train Loss 36.270107 Test MSE 40.421568859978215 Test RE 0.10702759175353978 Lambda1 1.0094931\n",
      "61 Train Loss 36.18321 Test MSE 40.32138520299501 Test RE 0.10689487711769528 Lambda1 1.0316154\n",
      "62 Train Loss 36.121136 Test MSE 40.405688365164984 Test RE 0.10700656562649047 Lambda1 1.0481298\n",
      "63 Train Loss 36.078465 Test MSE 40.38793305595856 Test RE 0.10698305231063407 Lambda1 1.0467038\n",
      "64 Train Loss 36.05119 Test MSE 40.37178911732153 Test RE 0.1069616684426296 Lambda1 1.0312346\n",
      "65 Train Loss 36.040512 Test MSE 40.38491777517686 Test RE 0.10697905866775262 Lambda1 1.019974\n",
      "66 Train Loss 36.035004 Test MSE 40.371910024756616 Test RE 0.10696182860956278 Lambda1 1.0146198\n",
      "67 Train Loss 36.01273 Test MSE 40.27039099202311 Test RE 0.10682726108160498 Lambda1 1.0053477\n",
      "68 Train Loss 35.92401 Test MSE 39.96909875314345 Test RE 0.1064268843742139 Lambda1 0.9899552\n",
      "69 Train Loss 35.68073 Test MSE 39.67799469242566 Test RE 0.10603861047624107 Lambda1 0.9684207\n",
      "70 Train Loss 35.62695 Test MSE 39.535400189630316 Test RE 0.10584789856490602 Lambda1 0.96168345\n",
      "71 Train Loss 35.432907 Test MSE 39.52675869912261 Test RE 0.10583633002640935 Lambda1 1.0024213\n",
      "72 Train Loss 35.28829 Test MSE 39.240993054672614 Test RE 0.10545305486256591 Lambda1 1.0375748\n",
      "73 Train Loss 35.266354 Test MSE 39.25821896248808 Test RE 0.10547619807519866 Lambda1 1.0540345\n",
      "74 Train Loss 35.24153 Test MSE 39.250130340935804 Test RE 0.10546533154811939 Lambda1 1.0721343\n",
      "Training time: 123.43\n",
      "Training time: 123.43\n",
      "inv_HT_swish_tune3\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 838.08905 Test MSE 858.3174382841029 Test RE 0.49318872413421333 Lambda1 -2.786131e-06\n",
      "1 Train Loss 837.99316 Test MSE 858.1832948519328 Test RE 0.4931501832574261 Lambda1 -0.00033808424\n",
      "2 Train Loss 837.8509 Test MSE 857.9530127610242 Test RE 0.49308401366675664 Lambda1 -0.01779565\n",
      "3 Train Loss 836.4222 Test MSE 856.4226507889728 Test RE 0.49264405144662515 Lambda1 -0.2046074\n",
      "4 Train Loss 830.13995 Test MSE 851.2594255243724 Test RE 0.491156773178442 Lambda1 -0.17773785\n",
      "5 Train Loss 750.31744 Test MSE 772.233744167341 Test RE 0.4678035983995958 Lambda1 -0.029672245\n",
      "6 Train Loss 619.9109 Test MSE 645.7654104565197 Test RE 0.4277859927314987 Lambda1 0.00031333882\n",
      "7 Train Loss 546.0326 Test MSE 578.7761711904959 Test RE 0.40499017914048435 Lambda1 2.8940442e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 458.23627 Test MSE 466.8127066003585 Test RE 0.3637144078396753 Lambda1 -0.00048361006\n",
      "9 Train Loss 337.7955 Test MSE 331.2672817107575 Test RE 0.3063927203878206 Lambda1 -0.004677211\n",
      "10 Train Loss 308.99738 Test MSE 316.63970241124 Test RE 0.29955174577299015 Lambda1 -0.005776814\n",
      "11 Train Loss 284.09564 Test MSE 289.65696133774503 Test RE 0.286504307493845 Lambda1 -0.003619602\n",
      "12 Train Loss 271.47873 Test MSE 285.66620716513404 Test RE 0.2845238030122142 Lambda1 -0.0019298805\n",
      "13 Train Loss 264.3935 Test MSE 282.2732980091931 Test RE 0.28282908555691927 Lambda1 0.0010612196\n",
      "14 Train Loss 261.79697 Test MSE 282.04842142571545 Test RE 0.2827164034307193 Lambda1 0.0009725835\n",
      "15 Train Loss 261.01187 Test MSE 281.96666335486105 Test RE 0.28267542461062417 Lambda1 0.0015370774\n",
      "16 Train Loss 260.15735 Test MSE 281.57975812205166 Test RE 0.2824814191501663 Lambda1 0.0028602397\n",
      "17 Train Loss 259.18985 Test MSE 280.59772955888377 Test RE 0.28198840233362665 Lambda1 0.0024848862\n",
      "18 Train Loss 258.92917 Test MSE 280.0432628520407 Test RE 0.2817096572120717 Lambda1 0.0038431634\n",
      "19 Train Loss 258.5667 Test MSE 279.51931550912747 Test RE 0.2814460012895676 Lambda1 0.005090292\n",
      "20 Train Loss 257.18622 Test MSE 276.28580039016947 Test RE 0.27981336428357156 Lambda1 0.014813544\n",
      "21 Train Loss 253.56197 Test MSE 270.46457713714193 Test RE 0.27684989768552465 Lambda1 0.033650648\n",
      "22 Train Loss 249.43599 Test MSE 267.3757059119287 Test RE 0.2752644602705752 Lambda1 0.044950128\n",
      "23 Train Loss 246.7825 Test MSE 262.9338216734589 Test RE 0.2729684147037986 Lambda1 0.071411625\n",
      "24 Train Loss 242.00816 Test MSE 254.12635033219163 Test RE 0.26835767482823686 Lambda1 0.09445703\n",
      "25 Train Loss 238.79478 Test MSE 251.63746683214632 Test RE 0.2670403096551684 Lambda1 0.10817743\n",
      "26 Train Loss 227.68182 Test MSE 242.64073734987858 Test RE 0.26222314942174496 Lambda1 0.17515385\n",
      "27 Train Loss 220.61443 Test MSE 237.61481725217877 Test RE 0.2594931692039869 Lambda1 0.2561665\n",
      "28 Train Loss 213.05827 Test MSE 229.00687540950463 Test RE 0.2547495533594356 Lambda1 0.29376492\n",
      "29 Train Loss 200.91682 Test MSE 216.84031245101056 Test RE 0.24789010008973578 Lambda1 0.29805982\n",
      "30 Train Loss 186.51212 Test MSE 206.19144373182885 Test RE 0.24172662608500997 Lambda1 0.3162722\n",
      "31 Train Loss 162.9254 Test MSE 166.0241923888178 Test RE 0.21690764481101363 Lambda1 0.34181264\n",
      "32 Train Loss 151.72144 Test MSE 154.8783548134697 Test RE 0.20950024442304319 Lambda1 0.3908184\n",
      "33 Train Loss 106.87058 Test MSE 93.3545875516203 Test RE 0.1626510993128106 Lambda1 0.64309716\n",
      "34 Train Loss 85.600746 Test MSE 61.792422490652115 Test RE 0.1323295334669074 Lambda1 0.71926403\n",
      "35 Train Loss 74.49485 Test MSE 61.291907975502404 Test RE 0.13179251358661914 Lambda1 0.6867625\n",
      "36 Train Loss 58.341576 Test MSE 52.990949821169544 Test RE 0.12254342732390447 Lambda1 0.70349777\n",
      "37 Train Loss 50.0478 Test MSE 45.95096431297933 Test RE 0.11411335801006457 Lambda1 0.7351685\n",
      "38 Train Loss 44.866722 Test MSE 41.70504959495322 Test RE 0.10871350342218512 Lambda1 0.81758463\n",
      "39 Train Loss 43.806908 Test MSE 42.48488538654705 Test RE 0.1097252037891272 Lambda1 0.8083063\n",
      "40 Train Loss 41.053257 Test MSE 41.35527152468731 Test RE 0.10825665628263302 Lambda1 0.7893032\n",
      "41 Train Loss 40.427734 Test MSE 41.42230697363849 Test RE 0.10834436088255592 Lambda1 0.7801003\n",
      "42 Train Loss 39.986485 Test MSE 40.8176927204176 Test RE 0.1075507384445893 Lambda1 0.8135381\n",
      "43 Train Loss 39.52348 Test MSE 41.04556724850878 Test RE 0.10785053446123029 Lambda1 0.83718795\n",
      "44 Train Loss 38.183746 Test MSE 39.878977348349316 Test RE 0.1063068322174932 Lambda1 0.8569082\n",
      "45 Train Loss 37.478348 Test MSE 39.20844711627556 Test RE 0.10540931513679735 Lambda1 0.8862539\n",
      "46 Train Loss 36.987 Test MSE 39.14828138234371 Test RE 0.10532840828993874 Lambda1 0.8921311\n",
      "47 Train Loss 35.88051 Test MSE 38.37512905568037 Test RE 0.10428313896707911 Lambda1 0.9164846\n",
      "48 Train Loss 35.47494 Test MSE 38.11713922525278 Test RE 0.1039320084244667 Lambda1 0.93229765\n",
      "49 Train Loss 34.477177 Test MSE 37.48565030614838 Test RE 0.10306748894234068 Lambda1 0.9883302\n",
      "50 Train Loss 34.35711 Test MSE 37.20818299608293 Test RE 0.10268532969604505 Lambda1 1.0064976\n",
      "51 Train Loss 34.163696 Test MSE 37.155830576723346 Test RE 0.10261306444004961 Lambda1 1.0423778\n",
      "52 Train Loss 33.938347 Test MSE 37.19270961957296 Test RE 0.10266397614067813 Lambda1 1.043811\n",
      "53 Train Loss 33.636856 Test MSE 36.68827964596684 Test RE 0.10196540403951203 Lambda1 1.0701147\n",
      "54 Train Loss 33.459812 Test MSE 36.649351303227306 Test RE 0.10191129414776905 Lambda1 1.0899237\n",
      "55 Train Loss 33.19535 Test MSE 36.219696029263034 Test RE 0.10131215918831196 Lambda1 1.1192403\n",
      "56 Train Loss 33.029488 Test MSE 35.8562185330585 Test RE 0.10080252565038068 Lambda1 1.1391394\n",
      "57 Train Loss 32.932323 Test MSE 35.71878752557279 Test RE 0.1006091604103919 Lambda1 1.1569694\n",
      "58 Train Loss 32.879566 Test MSE 35.395178277009755 Test RE 0.10015236811233934 Lambda1 1.1729287\n",
      "59 Train Loss 32.86109 Test MSE 35.22226634513335 Test RE 0.09990743727778198 Lambda1 1.1806148\n",
      "60 Train Loss 32.685272 Test MSE 34.974349115346556 Test RE 0.09955520960455758 Lambda1 1.1669052\n",
      "61 Train Loss 32.6124 Test MSE 34.864606361860666 Test RE 0.09939889437534999 Lambda1 1.1672641\n",
      "62 Train Loss 32.538273 Test MSE 34.90722069977015 Test RE 0.0994596225028881 Lambda1 1.1494102\n",
      "63 Train Loss 32.32382 Test MSE 34.905535955144046 Test RE 0.099457222339211 Lambda1 1.1354855\n",
      "64 Train Loss 31.883999 Test MSE 34.23370057124503 Test RE 0.09849543316930494 Lambda1 1.1139104\n",
      "65 Train Loss 31.824518 Test MSE 34.114741240559056 Test RE 0.09832415242552224 Lambda1 1.1169083\n",
      "66 Train Loss 31.790514 Test MSE 33.911731738176556 Test RE 0.09803116293284746 Lambda1 1.1111857\n",
      "67 Train Loss 31.749453 Test MSE 33.88987766381046 Test RE 0.09799957024375101 Lambda1 1.0911399\n",
      "68 Train Loss 31.658337 Test MSE 33.935772341303085 Test RE 0.0980659047556345 Lambda1 1.0794228\n",
      "69 Train Loss 31.64344 Test MSE 33.88239828102616 Test RE 0.09798875555814432 Lambda1 1.071646\n",
      "70 Train Loss 31.625772 Test MSE 33.943610222364924 Test RE 0.09807722886097732 Lambda1 1.0698929\n",
      "71 Train Loss 31.514772 Test MSE 33.93601145039267 Test RE 0.09806625023779517 Lambda1 1.0658606\n",
      "72 Train Loss 31.502924 Test MSE 33.931694876246425 Test RE 0.09806001315093438 Lambda1 1.0678781\n",
      "73 Train Loss 31.491543 Test MSE 33.87357000336174 Test RE 0.09797598892497618 Lambda1 1.0603955\n",
      "74 Train Loss 31.469995 Test MSE 33.86861279000194 Test RE 0.09796881953521393 Lambda1 1.064113\n",
      "Training time: 136.04\n",
      "Training time: 136.04\n",
      "inv_HT_swish_tune3\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 838.0896 Test MSE 858.3411009724165 Test RE 0.49319552237115155 Lambda1 1.0936446e-05\n",
      "1 Train Loss 837.877 Test MSE 857.8881839188863 Test RE 0.4930653840512142 Lambda1 0.005190837\n",
      "2 Train Loss 837.70605 Test MSE 858.1005089026723 Test RE 0.4931263964446761 Lambda1 0.26749015\n",
      "3 Train Loss 834.1405 Test MSE 852.8561467282476 Test RE 0.491617192765002 Lambda1 0.832443\n",
      "4 Train Loss 806.5246 Test MSE 819.5997794774337 Test RE 0.48193679711705706 Lambda1 0.71130824\n",
      "5 Train Loss 757.112 Test MSE 768.2686594500703 Test RE 0.46660106869471585 Lambda1 0.4381416\n",
      "6 Train Loss 714.6736 Test MSE 732.197397124604 Test RE 0.45551560646854156 Lambda1 0.5062005\n",
      "7 Train Loss 674.7533 Test MSE 678.9770236230011 Test RE 0.4386485617262212 Lambda1 0.5813901\n",
      "8 Train Loss 626.84875 Test MSE 634.9234371535932 Test RE 0.4241796700502472 Lambda1 0.72378534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 596.1566 Test MSE 580.7364885901727 Test RE 0.4056754511219844 Lambda1 0.8466934\n",
      "10 Train Loss 509.4459 Test MSE 475.5503158389222 Test RE 0.36710255558534505 Lambda1 0.90908474\n",
      "11 Train Loss 308.22363 Test MSE 191.55484337345086 Test RE 0.232989172722568 Lambda1 0.9402064\n",
      "12 Train Loss 206.5464 Test MSE 147.50220278206382 Test RE 0.20445061612516297 Lambda1 0.9345094\n",
      "13 Train Loss 144.76947 Test MSE 111.00105217301564 Test RE 0.17735879046774056 Lambda1 0.9688068\n",
      "14 Train Loss 116.38151 Test MSE 90.0712358992056 Test RE 0.15976521624941412 Lambda1 1.014846\n",
      "15 Train Loss 93.225586 Test MSE 81.42862563012355 Test RE 0.15190697762436078 Lambda1 1.0518965\n",
      "16 Train Loss 66.73242 Test MSE 49.938952062463606 Test RE 0.11896217147830433 Lambda1 1.0857978\n",
      "17 Train Loss 50.736553 Test MSE 43.41204179488976 Test RE 0.11091601986325422 Lambda1 1.1224855\n",
      "18 Train Loss 43.041348 Test MSE 40.15851939481644 Test RE 0.10667877422146664 Lambda1 1.1421726\n",
      "19 Train Loss 38.91474 Test MSE 40.35962932580712 Test RE 0.10694555905575943 Lambda1 1.1773984\n",
      "20 Train Loss 37.585514 Test MSE 40.247932689664104 Test RE 0.10679746880172991 Lambda1 1.1935776\n",
      "21 Train Loss 36.67939 Test MSE 39.873731876390735 Test RE 0.10629984046541471 Lambda1 1.2131006\n",
      "22 Train Loss 36.510265 Test MSE 39.76162654094212 Test RE 0.10615030383391309 Lambda1 1.2148787\n",
      "23 Train Loss 36.33313 Test MSE 39.66536709668276 Test RE 0.10602173564083932 Lambda1 1.2041231\n",
      "24 Train Loss 35.889282 Test MSE 39.602635167889524 Test RE 0.10593786424110117 Lambda1 1.1529937\n",
      "25 Train Loss 35.82637 Test MSE 39.68898896914268 Test RE 0.1060533004459346 Lambda1 1.145149\n",
      "26 Train Loss 35.70883 Test MSE 39.47150890660432 Test RE 0.10576233610107025 Lambda1 1.126511\n",
      "27 Train Loss 35.51838 Test MSE 39.633644609601006 Test RE 0.10597933157274093 Lambda1 1.1240835\n",
      "28 Train Loss 35.36753 Test MSE 39.58628560158901 Test RE 0.10591599427047267 Lambda1 1.1331701\n",
      "29 Train Loss 35.322895 Test MSE 39.45190320479548 Test RE 0.10573606649001764 Lambda1 1.1316785\n",
      "30 Train Loss 35.133514 Test MSE 39.31239804027703 Test RE 0.10554895523297433 Lambda1 1.0960108\n",
      "31 Train Loss 34.955055 Test MSE 38.827852053189986 Test RE 0.10489646522860414 Lambda1 1.0456393\n",
      "32 Train Loss 34.85464 Test MSE 38.44207402163294 Test RE 0.10437405969285854 Lambda1 1.0022391\n",
      "33 Train Loss 34.84036 Test MSE 38.28816055687226 Test RE 0.1041649049375852 Lambda1 0.97977114\n",
      "34 Train Loss 34.820965 Test MSE 38.34943228531735 Test RE 0.10424821806006032 Lambda1 0.99185133\n",
      "35 Train Loss 34.735214 Test MSE 38.19487569694412 Test RE 0.10403793442960378 Lambda1 1.0047909\n",
      "36 Train Loss 34.712395 Test MSE 37.99709288016913 Test RE 0.10376821731871971 Lambda1 0.9879401\n",
      "37 Train Loss 34.599506 Test MSE 37.64564765559031 Test RE 0.10328721257027855 Lambda1 0.9651205\n",
      "38 Train Loss 34.445423 Test MSE 37.31045892947913 Test RE 0.10282636091545841 Lambda1 0.95608795\n",
      "39 Train Loss 34.38 Test MSE 37.19548076972575 Test RE 0.10266780070733317 Lambda1 0.94431275\n",
      "40 Train Loss 34.31521 Test MSE 36.80188749913276 Test RE 0.10212315355647023 Lambda1 0.9338403\n",
      "41 Train Loss 34.078926 Test MSE 36.73648114399104 Test RE 0.1020323637337206 Lambda1 0.9242616\n",
      "42 Train Loss 33.483593 Test MSE 36.36548826969989 Test RE 0.10151585627131532 Lambda1 0.93659365\n",
      "43 Train Loss 33.099632 Test MSE 35.64393867632059 Test RE 0.10050369169800356 Lambda1 0.96629703\n",
      "44 Train Loss 32.955235 Test MSE 35.68202092489053 Test RE 0.10055736681009692 Lambda1 0.97704625\n",
      "45 Train Loss 32.908325 Test MSE 35.57623025542863 Test RE 0.1004081890513393 Lambda1 0.9852639\n",
      "46 Train Loss 32.658127 Test MSE 35.76010908752748 Test RE 0.10066733884082937 Lambda1 1.0508255\n",
      "47 Train Loss 32.447086 Test MSE 35.392606160511015 Test RE 0.10014872908223636 Lambda1 1.0847062\n",
      "48 Train Loss 32.400833 Test MSE 35.36638508636236 Test RE 0.10011162396206756 Lambda1 1.1020237\n",
      "49 Train Loss 32.364914 Test MSE 35.371667303283274 Test RE 0.10011909986848035 Lambda1 1.0776469\n",
      "50 Train Loss 32.31035 Test MSE 35.27617573828387 Test RE 0.09998386464297954 Lambda1 1.0739287\n",
      "51 Train Loss 32.239296 Test MSE 35.10898496990983 Test RE 0.09974664741886834 Lambda1 1.0513834\n",
      "52 Train Loss 31.954662 Test MSE 34.686954903231616 Test RE 0.09914532905070428 Lambda1 1.0092171\n",
      "53 Train Loss 31.590721 Test MSE 33.9502989580215 Test RE 0.09808689165670131 Lambda1 0.990984\n",
      "54 Train Loss 31.391094 Test MSE 33.6423022435381 Test RE 0.09764095640648583 Lambda1 0.9691236\n",
      "55 Train Loss 31.331139 Test MSE 33.71727025000721 Test RE 0.09774968669521694 Lambda1 0.9609244\n",
      "56 Train Loss 31.275671 Test MSE 33.49888285232668 Test RE 0.09743260914625093 Lambda1 0.96978724\n",
      "57 Train Loss 31.198626 Test MSE 33.58291552172537 Test RE 0.09755473850285114 Lambda1 0.9986115\n",
      "58 Train Loss 31.177195 Test MSE 33.533496347825235 Test RE 0.09748293339838628 Lambda1 1.0082142\n",
      "59 Train Loss 31.165247 Test MSE 33.59957790153312 Test RE 0.09757893670757654 Lambda1 1.0256081\n",
      "60 Train Loss 31.161612 Test MSE 33.61574973617465 Test RE 0.09760241677328638 Lambda1 1.0191369\n",
      "61 Train Loss 31.158625 Test MSE 33.65448861378669 Test RE 0.09765863921996693 Lambda1 1.0255429\n",
      "62 Train Loss 31.118322 Test MSE 33.560377691719054 Test RE 0.09752199802915992 Lambda1 1.0238643\n",
      "63 Train Loss 31.11081 Test MSE 33.54187497230877 Test RE 0.09749511110080979 Lambda1 1.0226312\n",
      "64 Train Loss 31.0938 Test MSE 33.60011047829473 Test RE 0.09757971005163957 Lambda1 1.0216644\n",
      "65 Train Loss 31.087822 Test MSE 33.57756292858892 Test RE 0.0975469638408875 Lambda1 1.0228508\n",
      "66 Train Loss 31.086964 Test MSE 33.56873127070337 Test RE 0.09753413446922814 Lambda1 1.0223229\n",
      "67 Train Loss 31.086964 Test MSE 33.56873127070337 Test RE 0.09753413446922814 Lambda1 1.0223229\n",
      "68 Train Loss 31.086964 Test MSE 33.56873127070337 Test RE 0.09753413446922814 Lambda1 1.0223229\n",
      "69 Train Loss 31.086964 Test MSE 33.56873127070337 Test RE 0.09753413446922814 Lambda1 1.0223229\n",
      "70 Train Loss 31.086964 Test MSE 33.56873127070337 Test RE 0.09753413446922814 Lambda1 1.0223229\n",
      "71 Train Loss 31.086964 Test MSE 33.56873127070337 Test RE 0.09753413446922814 Lambda1 1.0223229\n",
      "72 Train Loss 31.086964 Test MSE 33.56873127070337 Test RE 0.09753413446922814 Lambda1 1.0223229\n",
      "73 Train Loss 31.086964 Test MSE 33.56873127070337 Test RE 0.09753413446922814 Lambda1 1.0223229\n",
      "74 Train Loss 31.086964 Test MSE 33.56873127070337 Test RE 0.09753413446922814 Lambda1 1.0223229\n",
      "Training time: 125.97\n",
      "Training time: 125.97\n",
      "inv_HT_swish_tune4\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartlab/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 838.0218 Test MSE 858.3878020242794 Test RE 0.49320893920296643 Lambda1 -3.955351e-05\n",
      "1 Train Loss 837.8687 Test MSE 857.730634874204 Test RE 0.4930201068527792 Lambda1 0.015584373\n",
      "2 Train Loss 834.4434 Test MSE 854.5796701689009 Test RE 0.49211369273752675 Lambda1 0.37434727\n",
      "3 Train Loss 786.6299 Test MSE 819.3752571808711 Test RE 0.4818707813794334 Lambda1 0.057876896\n",
      "4 Train Loss 686.07 Test MSE 700.8476567493016 Test RE 0.445657257577346 Lambda1 0.0055778013\n",
      "5 Train Loss 604.9953 Test MSE 647.5639684719916 Test RE 0.42838130407438835 Lambda1 -0.00016829043\n",
      "6 Train Loss 598.9923 Test MSE 642.1337234528178 Test RE 0.42658139486188845 Lambda1 -0.0001018248\n",
      "7 Train Loss 487.33618 Test MSE 469.1526431623322 Test RE 0.36462484234691606 Lambda1 0.00031927016\n",
      "8 Train Loss 352.23538 Test MSE 361.1738845337278 Test RE 0.3199243827536194 Lambda1 -0.00088620273\n",
      "9 Train Loss 316.63806 Test MSE 317.0281948579883 Test RE 0.2997354528638687 Lambda1 -0.0001796651\n",
      "10 Train Loss 295.93796 Test MSE 311.1143678017054 Test RE 0.29692666788607863 Lambda1 0.005714658\n",
      "11 Train Loss 282.80206 Test MSE 299.98914413000097 Test RE 0.2915693979635213 Lambda1 0.0044401432\n",
      "12 Train Loss 273.8107 Test MSE 294.50562052207624 Test RE 0.288892298592805 Lambda1 0.00035693235\n",
      "13 Train Loss 272.3332 Test MSE 292.61742236587577 Test RE 0.28796470499877846 Lambda1 -0.0027444186\n",
      "14 Train Loss 270.07886 Test MSE 287.8126402685415 Test RE 0.2855907272690652 Lambda1 -0.0010928101\n",
      "15 Train Loss 267.94516 Test MSE 287.704364024919 Test RE 0.28553700204005883 Lambda1 0.0017726846\n",
      "16 Train Loss 263.36133 Test MSE 284.0751559848869 Test RE 0.28373035245208234 Lambda1 0.0030179399\n",
      "17 Train Loss 261.7866 Test MSE 282.0731581820053 Test RE 0.28272880082937 Lambda1 0.0018365387\n",
      "18 Train Loss 261.23126 Test MSE 280.96371784365647 Test RE 0.282172243487592 Lambda1 0.0019453992\n",
      "19 Train Loss 258.60287 Test MSE 273.8016561346227 Test RE 0.27855259325410775 Lambda1 0.012095944\n",
      "20 Train Loss 257.5213 Test MSE 269.90110437640334 Test RE 0.2765613595118834 Lambda1 0.020993736\n",
      "21 Train Loss 256.21997 Test MSE 263.30035950339834 Test RE 0.27315861162233807 Lambda1 0.032297857\n",
      "22 Train Loss 251.39778 Test MSE 247.25302956918011 Test RE 0.264703681472909 Lambda1 0.072490945\n",
      "23 Train Loss 234.78862 Test MSE 223.08148700032058 Test RE 0.2514322234918914 Lambda1 0.13631071\n",
      "24 Train Loss 208.87633 Test MSE 191.3259606594707 Test RE 0.2328499354876796 Lambda1 0.20430513\n",
      "25 Train Loss 204.55466 Test MSE 193.77770195760922 Test RE 0.234337110881495 Lambda1 0.1900508\n",
      "26 Train Loss 195.40627 Test MSE 196.30210368969497 Test RE 0.23585856262456079 Lambda1 0.17113031\n",
      "27 Train Loss 180.08948 Test MSE 167.2079457932889 Test RE 0.21767954768693712 Lambda1 0.24628548\n",
      "28 Train Loss 162.69426 Test MSE 148.28215808716385 Test RE 0.2049904456862547 Lambda1 0.29620498\n",
      "29 Train Loss 156.12152 Test MSE 136.59883631298064 Test RE 0.19674906195160435 Lambda1 0.34292895\n",
      "30 Train Loss 148.83409 Test MSE 122.53445667370076 Test RE 0.1863452321590433 Lambda1 0.3770601\n",
      "31 Train Loss 138.54062 Test MSE 103.37605017810922 Test RE 0.17115876252523976 Lambda1 0.4816437\n",
      "32 Train Loss 132.49142 Test MSE 92.92680901320296 Test RE 0.16227801352219318 Lambda1 0.5758928\n",
      "33 Train Loss 116.651794 Test MSE 79.55634912419706 Test RE 0.15015043443837642 Lambda1 0.7030716\n",
      "34 Train Loss 96.09394 Test MSE 65.78859066345464 Test RE 0.13654143477803996 Lambda1 0.7090494\n",
      "35 Train Loss 81.43241 Test MSE 55.972036787601944 Test RE 0.12594320066710518 Lambda1 0.8089633\n",
      "36 Train Loss 72.5627 Test MSE 56.08287310525493 Test RE 0.12606783591138374 Lambda1 0.86424786\n",
      "37 Train Loss 67.65203 Test MSE 51.467866775961866 Test RE 0.1207694962233486 Lambda1 0.84876573\n",
      "38 Train Loss 62.15484 Test MSE 49.18057867853666 Test RE 0.11805543557055546 Lambda1 0.86546063\n",
      "39 Train Loss 55.444153 Test MSE 50.582774916104725 Test RE 0.11972655764310743 Lambda1 0.800252\n",
      "40 Train Loss 54.39558 Test MSE 52.039358251570675 Test RE 0.1214381483831863 Lambda1 0.7813863\n",
      "41 Train Loss 50.36505 Test MSE 50.55946111839719 Test RE 0.11969896324509426 Lambda1 0.8465511\n",
      "42 Train Loss 48.28103 Test MSE 48.493662869306476 Test RE 0.11722808352357186 Lambda1 0.9326823\n",
      "43 Train Loss 44.515495 Test MSE 47.3058372334665 Test RE 0.11578346362782033 Lambda1 0.92554957\n",
      "44 Train Loss 43.606976 Test MSE 47.0151800185279 Test RE 0.11542721634695999 Lambda1 0.94400233\n",
      "45 Train Loss 43.4287 Test MSE 46.94259695438829 Test RE 0.1153380824002091 Lambda1 0.9602653\n",
      "46 Train Loss 42.670547 Test MSE 46.82942584624364 Test RE 0.11519896765071437 Lambda1 0.943052\n",
      "47 Train Loss 41.891804 Test MSE 46.56306141799998 Test RE 0.11487087623328489 Lambda1 0.9679272\n",
      "48 Train Loss 41.72826 Test MSE 46.737354177594796 Test RE 0.1150856651755425 Lambda1 0.9945531\n",
      "49 Train Loss 41.661823 Test MSE 46.85248827921372 Test RE 0.11522733060202747 Lambda1 1.0254042\n",
      "50 Train Loss 41.558315 Test MSE 47.036492540285295 Test RE 0.11545337562498803 Lambda1 1.0416955\n",
      "51 Train Loss 41.079426 Test MSE 46.248454990056565 Test RE 0.11448215214459907 Lambda1 1.0580044\n",
      "52 Train Loss 40.581596 Test MSE 45.43056928637403 Test RE 0.11346535078283182 Lambda1 1.0387744\n",
      "53 Train Loss 39.90843 Test MSE 45.205112254647574 Test RE 0.11318345492703691 Lambda1 1.0717181\n",
      "54 Train Loss 39.697243 Test MSE 44.86266874578566 Test RE 0.11275393910711262 Lambda1 1.0663279\n",
      "55 Train Loss 39.502274 Test MSE 44.75811453190356 Test RE 0.11262247372136808 Lambda1 1.0617263\n",
      "56 Train Loss 38.99135 Test MSE 43.451505987146994 Test RE 0.1109664231225266 Lambda1 1.0129361\n",
      "57 Train Loss 38.65191 Test MSE 43.25246097537099 Test RE 0.11071197088563937 Lambda1 0.9962328\n",
      "58 Train Loss 38.58871 Test MSE 42.946981957848685 Test RE 0.11032031558362895 Lambda1 0.9967475\n",
      "59 Train Loss 38.528244 Test MSE 42.77595255641309 Test RE 0.11010043005404699 Lambda1 0.9949877\n",
      "60 Train Loss 38.481274 Test MSE 42.600763991708405 Test RE 0.1098747410335696 Lambda1 1.0018259\n",
      "61 Train Loss 38.20556 Test MSE 42.60820082147848 Test RE 0.10988433105039597 Lambda1 1.0138173\n",
      "62 Train Loss 37.95072 Test MSE 42.39673138820762 Test RE 0.10961130753924501 Lambda1 1.0195996\n",
      "63 Train Loss 37.67659 Test MSE 41.705294550391706 Test RE 0.10871382268717888 Lambda1 1.0158578\n",
      "64 Train Loss 37.359123 Test MSE 41.36946994735768 Test RE 0.1082752384584796 Lambda1 1.0083143\n",
      "65 Train Loss 37.291153 Test MSE 41.00247364205258 Test RE 0.10779390363196441 Lambda1 1.0018659\n",
      "66 Train Loss 37.22969 Test MSE 40.870529568769136 Test RE 0.10762032596755455 Lambda1 0.9950974\n",
      "67 Train Loss 37.169994 Test MSE 40.74266248471353 Test RE 0.10745184420960262 Lambda1 0.9836786\n",
      "68 Train Loss 37.15731 Test MSE 40.73026093720446 Test RE 0.10743548947859909 Lambda1 0.97513884\n",
      "69 Train Loss 37.123913 Test MSE 40.815560499555716 Test RE 0.10754792930830762 Lambda1 0.976635\n",
      "70 Train Loss 37.076065 Test MSE 40.56395987353903 Test RE 0.1072159363699918 Lambda1 0.9747031\n",
      "71 Train Loss 37.047535 Test MSE 40.329220793498585 Test RE 0.10690526296867914 Lambda1 0.9666934\n",
      "72 Train Loss 36.966682 Test MSE 40.3400705140572 Test RE 0.10691964229719124 Lambda1 0.9511888\n",
      "73 Train Loss 36.864952 Test MSE 40.43348353449948 Test RE 0.10704336433487245 Lambda1 0.9419077\n",
      "74 Train Loss 36.78296 Test MSE 40.70865997686492 Test RE 0.10740699693438441 Lambda1 0.9499016\n",
      "Training time: 135.59\n",
      "Training time: 135.59\n",
      "inv_HT_swish_tune4\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 837.9628 Test MSE 858.1747202671227 Test RE 0.4931477195829066 Lambda1 0.00040853448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 837.4342 Test MSE 856.802589845403 Test RE 0.4927533163996131 Lambda1 0.30394\n",
      "2 Train Loss 814.3898 Test MSE 832.8060934152033 Test RE 0.48580403504595565 Lambda1 0.48880377\n",
      "3 Train Loss 753.5295 Test MSE 744.8234466165287 Test RE 0.45942628678225944 Lambda1 0.26464787\n",
      "4 Train Loss 651.2055 Test MSE 621.777629461643 Test RE 0.4197654762524657 Lambda1 0.2478902\n",
      "5 Train Loss 571.96606 Test MSE 553.4074764820762 Test RE 0.3960150415170772 Lambda1 0.4475834\n",
      "6 Train Loss 446.4657 Test MSE 396.57876116073703 Test RE 0.33523850598505384 Lambda1 0.334369\n",
      "7 Train Loss 318.8514 Test MSE 322.8213451988888 Test RE 0.3024616326368551 Lambda1 0.16224615\n",
      "8 Train Loss 294.6862 Test MSE 305.2506830002279 Test RE 0.29411521570931726 Lambda1 0.11261711\n",
      "9 Train Loss 279.0814 Test MSE 295.1029599407907 Test RE 0.28918512720278844 Lambda1 0.08993452\n",
      "10 Train Loss 260.4662 Test MSE 277.02677561011154 Test RE 0.28018833096907914 Lambda1 0.07931973\n",
      "11 Train Loss 253.85826 Test MSE 270.9100917923151 Test RE 0.27707782020588995 Lambda1 0.100620225\n",
      "12 Train Loss 248.63362 Test MSE 267.6623901165125 Test RE 0.2754119920568366 Lambda1 0.10057145\n",
      "13 Train Loss 235.84564 Test MSE 259.46688319784414 Test RE 0.2711628177165164 Lambda1 0.141706\n",
      "14 Train Loss 230.84877 Test MSE 255.11572825996979 Test RE 0.26887955939497965 Lambda1 0.20183396\n",
      "15 Train Loss 228.9029 Test MSE 254.29843846745757 Test RE 0.26844852207064546 Lambda1 0.2601585\n",
      "16 Train Loss 226.43085 Test MSE 252.3241748670259 Test RE 0.26740443227728455 Lambda1 0.22995125\n",
      "17 Train Loss 223.44565 Test MSE 249.96828395742372 Test RE 0.266153158870887 Lambda1 0.26208404\n",
      "18 Train Loss 219.34312 Test MSE 243.44023873893028 Test RE 0.26265480686113396 Lambda1 0.26158875\n",
      "19 Train Loss 216.9233 Test MSE 241.47171338253528 Test RE 0.26159070148749614 Lambda1 0.28033555\n",
      "20 Train Loss 214.69354 Test MSE 238.59835701202465 Test RE 0.260029664134464 Lambda1 0.24649133\n",
      "21 Train Loss 213.6326 Test MSE 237.3301483103681 Test RE 0.2593376827228463 Lambda1 0.20760739\n",
      "22 Train Loss 209.23723 Test MSE 235.53656703657023 Test RE 0.2583558745521572 Lambda1 0.22476879\n",
      "23 Train Loss 205.33856 Test MSE 228.17185578549035 Test RE 0.25428468709864294 Lambda1 0.23143563\n",
      "24 Train Loss 195.56442 Test MSE 204.150613337207 Test RE 0.24052737695051415 Lambda1 0.22613154\n",
      "25 Train Loss 186.49503 Test MSE 188.8577982014231 Test RE 0.23134314308113793 Lambda1 0.2721328\n",
      "26 Train Loss 179.12967 Test MSE 175.6549430876541 Test RE 0.2231101655715419 Lambda1 0.31616572\n",
      "27 Train Loss 171.25156 Test MSE 171.13241281389264 Test RE 0.22021926429012506 Lambda1 0.3230914\n",
      "28 Train Loss 157.1973 Test MSE 153.31407674863442 Test RE 0.20843957868067325 Lambda1 0.38710174\n",
      "29 Train Loss 135.94536 Test MSE 126.42198077790944 Test RE 0.189278142762443 Lambda1 0.5725341\n",
      "30 Train Loss 121.20461 Test MSE 114.83323084309475 Test RE 0.1803943630382559 Lambda1 0.67306316\n",
      "31 Train Loss 108.99522 Test MSE 93.7937724578297 Test RE 0.16303324491401774 Lambda1 0.7840438\n",
      "32 Train Loss 104.07616 Test MSE 88.36716614635415 Test RE 0.15824668991083568 Lambda1 0.7840327\n",
      "33 Train Loss 99.40504 Test MSE 87.84399087885045 Test RE 0.1577775470034231 Lambda1 0.79495335\n",
      "34 Train Loss 91.70648 Test MSE 75.51730788936439 Test RE 0.14628925235367274 Lambda1 0.7523117\n",
      "35 Train Loss 85.69629 Test MSE 71.14843013208694 Test RE 0.1419946009690229 Lambda1 0.68852997\n",
      "36 Train Loss 78.241066 Test MSE 62.959308057502426 Test RE 0.13357314259541606 Lambda1 0.64478195\n",
      "37 Train Loss 72.81479 Test MSE 66.20023881328349 Test RE 0.1369679477849733 Lambda1 0.650247\n",
      "38 Train Loss 68.6507 Test MSE 61.386208198940565 Test RE 0.13189385883174165 Lambda1 0.6583825\n",
      "39 Train Loss 65.6761 Test MSE 58.33200989018479 Test RE 0.12857088727650234 Lambda1 0.70888597\n",
      "40 Train Loss 63.168304 Test MSE 57.53846313212568 Test RE 0.1276933555018851 Lambda1 0.7960899\n",
      "41 Train Loss 61.121593 Test MSE 56.23386695724593 Test RE 0.1262374302265479 Lambda1 0.8915387\n",
      "42 Train Loss 54.913223 Test MSE 48.016189039872344 Test RE 0.1166495357193127 Lambda1 0.92811316\n",
      "43 Train Loss 50.57045 Test MSE 44.57342957671656 Test RE 0.11238987704604081 Lambda1 0.9489701\n",
      "44 Train Loss 48.936554 Test MSE 43.19627191615595 Test RE 0.11064003482681044 Lambda1 0.985215\n",
      "45 Train Loss 48.328674 Test MSE 41.00862339949776 Test RE 0.10780198706508297 Lambda1 1.0098836\n",
      "46 Train Loss 47.399723 Test MSE 41.07367025802882 Test RE 0.10788744960426884 Lambda1 0.9759937\n",
      "47 Train Loss 44.065563 Test MSE 43.44520044250095 Test RE 0.11095837128282791 Lambda1 0.9252612\n",
      "48 Train Loss 40.38877 Test MSE 38.81192471734087 Test RE 0.10487494855291955 Lambda1 0.93225527\n",
      "49 Train Loss 37.72016 Test MSE 37.03986975042675 Test RE 0.10245281510448277 Lambda1 0.91119456\n",
      "50 Train Loss 36.928127 Test MSE 36.980005806671095 Test RE 0.10236998935829657 Lambda1 0.8807358\n",
      "51 Train Loss 36.78892 Test MSE 36.97688345528755 Test RE 0.10236566753873225 Lambda1 0.8688283\n",
      "52 Train Loss 36.36301 Test MSE 37.1010732205029 Test RE 0.10253742500868228 Lambda1 0.8531697\n",
      "53 Train Loss 35.469402 Test MSE 37.33338687746684 Test RE 0.10285795038711336 Lambda1 0.8567543\n",
      "54 Train Loss 34.75846 Test MSE 36.70368670422894 Test RE 0.10198681172118207 Lambda1 0.87524706\n",
      "55 Train Loss 34.5752 Test MSE 36.271574975982766 Test RE 0.10138468999263875 Lambda1 0.9101037\n",
      "56 Train Loss 34.14789 Test MSE 35.84971321012426 Test RE 0.1007933810339869 Lambda1 0.99413615\n",
      "57 Train Loss 33.473644 Test MSE 35.6108981378469 Test RE 0.10045709940052751 Lambda1 1.0696405\n",
      "58 Train Loss 33.201767 Test MSE 35.5275427930242 Test RE 0.10033945926171885 Lambda1 1.1219516\n",
      "59 Train Loss 33.066883 Test MSE 35.47041228689287 Test RE 0.1002587507461775 Lambda1 1.0766572\n",
      "60 Train Loss 33.03851 Test MSE 35.48873208025539 Test RE 0.10028463827301451 Lambda1 1.0516301\n",
      "61 Train Loss 33.02303 Test MSE 35.49663852855524 Test RE 0.10029580873620683 Lambda1 1.0673814\n",
      "62 Train Loss 32.781467 Test MSE 35.23860469273364 Test RE 0.09993060633186868 Lambda1 1.0986727\n",
      "63 Train Loss 32.55858 Test MSE 34.98324129044626 Test RE 0.09956786468058161 Lambda1 1.1265268\n",
      "64 Train Loss 32.477764 Test MSE 34.70789845339815 Test RE 0.09917525588975237 Lambda1 1.1191899\n",
      "65 Train Loss 32.357178 Test MSE 34.69728201710878 Test RE 0.09916008688847193 Lambda1 1.10124\n",
      "66 Train Loss 32.14631 Test MSE 34.667078543560976 Test RE 0.099116918793706 Lambda1 1.0705205\n",
      "67 Train Loss 32.037914 Test MSE 34.45536675890581 Test RE 0.09881380188134928 Lambda1 1.0283899\n",
      "68 Train Loss 31.9832 Test MSE 34.280407298223686 Test RE 0.09856260135680019 Lambda1 1.0040019\n",
      "69 Train Loss 31.779707 Test MSE 33.90902929844043 Test RE 0.09802725678334284 Lambda1 0.98370236\n",
      "70 Train Loss 31.653301 Test MSE 33.595749135413215 Test RE 0.09757337685245017 Lambda1 0.97368336\n",
      "71 Train Loss 31.587517 Test MSE 33.42194580956709 Test RE 0.09732065786230693 Lambda1 0.979002\n",
      "72 Train Loss 31.571602 Test MSE 33.412351808324686 Test RE 0.0973066885749822 Lambda1 0.9832795\n",
      "73 Train Loss 31.445595 Test MSE 33.299193067849956 Test RE 0.09714177290067129 Lambda1 0.99435127\n",
      "74 Train Loss 31.307287 Test MSE 33.15878681961688 Test RE 0.09693675712440744 Lambda1 0.9993196\n",
      "Training time: 111.99\n",
      "Training time: 111.99\n",
      "inv_HT_swish_tune4\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 838.0746 Test MSE 858.318931865225 Test RE 0.49318915323946905 Lambda1 4.3185187e-06\n",
      "1 Train Loss 838.0463 Test MSE 858.2201230381638 Test RE 0.4931607646985075 Lambda1 7.7976314e-05\n",
      "2 Train Loss 837.8474 Test MSE 857.8325523779623 Test RE 0.49304939686762284 Lambda1 0.0023679489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Train Loss 837.2068 Test MSE 857.7929275451634 Test RE 0.49303800931566083 Lambda1 0.20646429\n",
      "4 Train Loss 829.1549 Test MSE 851.02552859331 Test RE 0.49108929201416174 Lambda1 0.43380308\n",
      "5 Train Loss 777.19336 Test MSE 788.3673653574664 Test RE 0.47266504897736905 Lambda1 0.10652804\n",
      "6 Train Loss 688.6019 Test MSE 719.9551736938355 Test RE 0.4516914803335776 Lambda1 0.007161909\n",
      "7 Train Loss 634.3945 Test MSE 664.1093922864147 Test RE 0.43381941244568606 Lambda1 0.0013960769\n",
      "8 Train Loss 619.8815 Test MSE 654.2979431644106 Test RE 0.43060289782275857 Lambda1 0.0004078237\n",
      "9 Train Loss 614.08386 Test MSE 655.9568178319608 Test RE 0.43114841692376854 Lambda1 0.00017037666\n",
      "10 Train Loss 610.15094 Test MSE 656.093975173742 Test RE 0.4311934900736128 Lambda1 0.00011821433\n",
      "11 Train Loss 588.6301 Test MSE 600.6124307039197 Test RE 0.41255924996002796 Lambda1 -6.439272e-05\n",
      "12 Train Loss 495.0977 Test MSE 511.8216312537676 Test RE 0.38084520231481295 Lambda1 0.00038728307\n",
      "13 Train Loss 414.251 Test MSE 416.5016639598314 Test RE 0.34355600268208786 Lambda1 0.0011550917\n",
      "14 Train Loss 311.77 Test MSE 302.3749730265217 Test RE 0.2927265349413707 Lambda1 0.0021126368\n",
      "15 Train Loss 273.02618 Test MSE 291.4385723630742 Test RE 0.28738406667129385 Lambda1 0.0025416387\n",
      "16 Train Loss 267.83255 Test MSE 286.37333954161556 Test RE 0.28487573760000345 Lambda1 0.003473282\n",
      "17 Train Loss 263.21667 Test MSE 283.160926862186 Test RE 0.283273424800475 Lambda1 0.0044242344\n",
      "18 Train Loss 259.4029 Test MSE 279.7748588928523 Test RE 0.281574624302165 Lambda1 0.006180401\n",
      "19 Train Loss 258.29297 Test MSE 277.8926851922221 Test RE 0.2806258851850173 Lambda1 0.00736684\n",
      "20 Train Loss 256.7683 Test MSE 273.4113149204518 Test RE 0.2783539652779917 Lambda1 0.016652858\n",
      "21 Train Loss 256.0575 Test MSE 271.7389765801984 Test RE 0.27750137435254746 Lambda1 0.024082812\n",
      "22 Train Loss 251.94728 Test MSE 269.60977356964963 Test RE 0.2764120592748191 Lambda1 0.027289532\n",
      "23 Train Loss 241.13545 Test MSE 252.42045157459648 Test RE 0.2674554427740311 Lambda1 0.06586146\n",
      "24 Train Loss 230.15283 Test MSE 240.5077246679997 Test RE 0.2610680260588354 Lambda1 0.09021075\n",
      "25 Train Loss 208.40738 Test MSE 212.6331437839917 Test RE 0.24547352028200606 Lambda1 0.16294368\n",
      "26 Train Loss 187.3174 Test MSE 187.4849100965291 Test RE 0.2305007432241306 Lambda1 0.2464782\n",
      "27 Train Loss 180.38979 Test MSE 179.45273641985227 Test RE 0.22550917346954671 Lambda1 0.26764265\n",
      "28 Train Loss 154.81104 Test MSE 160.13961644175308 Test RE 0.21302891852930755 Lambda1 0.3556752\n",
      "29 Train Loss 134.65184 Test MSE 133.221674077245 Test RE 0.19430170609514122 Lambda1 0.48747733\n",
      "30 Train Loss 126.98546 Test MSE 117.63696255906362 Test RE 0.18258330822047153 Lambda1 0.56794965\n",
      "31 Train Loss 106.604324 Test MSE 79.38844281973387 Test RE 0.14999190201975804 Lambda1 0.73417604\n",
      "32 Train Loss 95.743935 Test MSE 72.31876135542558 Test RE 0.14315768281966365 Lambda1 0.72086954\n",
      "33 Train Loss 83.7709 Test MSE 58.9192481492531 Test RE 0.12921643911370978 Lambda1 0.65706116\n",
      "34 Train Loss 78.28404 Test MSE 53.957543743910826 Test RE 0.12365601778401396 Lambda1 0.61256135\n",
      "35 Train Loss 72.598236 Test MSE 51.352445493152175 Test RE 0.12063400203155814 Lambda1 0.69322824\n",
      "36 Train Loss 62.879555 Test MSE 49.638827796167575 Test RE 0.1186041619728369 Lambda1 0.7763649\n",
      "37 Train Loss 58.245052 Test MSE 49.849518843532024 Test RE 0.11885560198646872 Lambda1 0.7519762\n",
      "38 Train Loss 51.76025 Test MSE 42.243464888067884 Test RE 0.10941300272383403 Lambda1 0.82683384\n",
      "39 Train Loss 47.43634 Test MSE 41.603996623954366 Test RE 0.10858171500016438 Lambda1 0.9136174\n",
      "40 Train Loss 40.303528 Test MSE 41.86977001407192 Test RE 0.10892798208749416 Lambda1 1.0197108\n",
      "41 Train Loss 39.324955 Test MSE 42.7585130226037 Test RE 0.11007798407913333 Lambda1 1.0555435\n",
      "42 Train Loss 38.902218 Test MSE 42.20017586438352 Test RE 0.10935692782439378 Lambda1 1.0786972\n",
      "43 Train Loss 38.57327 Test MSE 42.191109179912324 Test RE 0.1093451795555679 Lambda1 1.0772352\n",
      "44 Train Loss 38.16899 Test MSE 41.496690462503246 Test RE 0.10844159613359868 Lambda1 1.0373594\n",
      "45 Train Loss 38.022892 Test MSE 41.15396316028665 Test RE 0.10799285005914985 Lambda1 0.9989206\n",
      "46 Train Loss 37.888725 Test MSE 41.067858412208174 Test RE 0.10787981640030349 Lambda1 1.0027331\n",
      "47 Train Loss 37.062355 Test MSE 40.49285103044611 Test RE 0.1071219200882862 Lambda1 0.96769965\n",
      "48 Train Loss 36.78419 Test MSE 40.37788375080903 Test RE 0.10696974174801972 Lambda1 0.9541843\n",
      "49 Train Loss 36.62154 Test MSE 39.911181799026 Test RE 0.10634974783951734 Lambda1 0.92383623\n",
      "50 Train Loss 36.2741 Test MSE 39.61459333222951 Test RE 0.10595385720178477 Lambda1 0.90700907\n",
      "51 Train Loss 36.030373 Test MSE 38.930032788670836 Test RE 0.10503439914634187 Lambda1 0.88435566\n",
      "52 Train Loss 35.890957 Test MSE 38.85328748792233 Test RE 0.1049308175072246 Lambda1 0.87287134\n",
      "53 Train Loss 35.74104 Test MSE 38.6435376647696 Test RE 0.10464719922760163 Lambda1 0.88803285\n",
      "54 Train Loss 35.55506 Test MSE 38.04424911529615 Test RE 0.10383258804834361 Lambda1 0.92602926\n",
      "55 Train Loss 35.244793 Test MSE 37.85813631137058 Test RE 0.10357830170371028 Lambda1 0.93727434\n",
      "56 Train Loss 35.056126 Test MSE 37.914662898342634 Test RE 0.1036556000675295 Lambda1 0.93070877\n",
      "57 Train Loss 34.903553 Test MSE 38.27316458849419 Test RE 0.10414450428923143 Lambda1 0.9346426\n",
      "58 Train Loss 34.839066 Test MSE 38.335059735026505 Test RE 0.1042286812236883 Lambda1 0.954131\n",
      "59 Train Loss 34.593117 Test MSE 38.0015933552057 Test RE 0.10377436242626203 Lambda1 0.9727351\n",
      "60 Train Loss 34.425373 Test MSE 37.591839536885544 Test RE 0.10321337032944417 Lambda1 1.0224217\n",
      "61 Train Loss 34.325047 Test MSE 37.171533455570405 Test RE 0.10263474542907647 Lambda1 1.0775619\n",
      "62 Train Loss 34.182564 Test MSE 37.299262260490664 Test RE 0.1028109309375359 Lambda1 1.0600086\n",
      "63 Train Loss 34.047157 Test MSE 37.16190682125241 Test RE 0.102621454463538 Lambda1 1.0649104\n",
      "64 Train Loss 33.769455 Test MSE 36.87818867016841 Test RE 0.1022289644547407 Lambda1 1.1212872\n",
      "65 Train Loss 33.632973 Test MSE 36.78675449669952 Test RE 0.10210215478521341 Lambda1 1.1528554\n",
      "66 Train Loss 33.584232 Test MSE 36.7247439044232 Test RE 0.10201606285335822 Lambda1 1.157815\n",
      "67 Train Loss 33.535206 Test MSE 36.76214214756084 Test RE 0.10206799310955515 Lambda1 1.1623157\n",
      "68 Train Loss 33.49097 Test MSE 36.640096803586395 Test RE 0.10189842628588167 Lambda1 1.1792943\n",
      "69 Train Loss 33.42692 Test MSE 36.301712431091175 Test RE 0.10142680068293261 Lambda1 1.1986068\n",
      "70 Train Loss 33.408886 Test MSE 36.18574680427848 Test RE 0.10126466740888453 Lambda1 1.2128298\n",
      "71 Train Loss 33.35978 Test MSE 35.94898260673373 Test RE 0.10093283516406927 Lambda1 1.2346876\n",
      "72 Train Loss 33.26867 Test MSE 35.95404158647582 Test RE 0.10093993688389952 Lambda1 1.259984\n",
      "73 Train Loss 33.115967 Test MSE 35.64231327679644 Test RE 0.10050140013706643 Lambda1 1.2794821\n",
      "74 Train Loss 32.96981 Test MSE 35.549321020470295 Test RE 0.10037020837373974 Lambda1 1.2982205\n",
      "Training time: 75.02\n",
      "Training time: 75.02\n",
      "inv_HT_swish_tune4\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 837.8992 Test MSE 857.7794716417391 Test RE 0.4930341422414274 Lambda1 0.00029282292\n",
      "1 Train Loss 837.20966 Test MSE 858.686292639162 Test RE 0.49329468450550296 Lambda1 0.5618841\n",
      "2 Train Loss 831.5878 Test MSE 850.9653106103494 Test RE 0.49107191713598586 Lambda1 0.83714294\n",
      "3 Train Loss 810.03827 Test MSE 819.1820787177965 Test RE 0.481813974352737 Lambda1 0.89553607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 711.4513 Test MSE 707.7256074352456 Test RE 0.44783870527284353 Lambda1 0.28420258\n",
      "5 Train Loss 630.0802 Test MSE 647.1391304601837 Test RE 0.42824076002815864 Lambda1 0.323913\n",
      "6 Train Loss 523.27496 Test MSE 506.3519946236607 Test RE 0.3788047648304844 Lambda1 0.35596043\n",
      "7 Train Loss 412.34387 Test MSE 381.1862260225469 Test RE 0.32866826350242845 Lambda1 0.34340706\n",
      "8 Train Loss 298.75604 Test MSE 284.50780081350865 Test RE 0.2839463300976029 Lambda1 0.272422\n",
      "9 Train Loss 242.87585 Test MSE 241.8765854953809 Test RE 0.2618099123030026 Lambda1 0.2243405\n",
      "10 Train Loss 225.30269 Test MSE 232.90976519012926 Test RE 0.2569111891940489 Lambda1 0.15467973\n",
      "11 Train Loss 219.3893 Test MSE 230.4811377983386 Test RE 0.2555682300485379 Lambda1 0.16189805\n",
      "12 Train Loss 202.6846 Test MSE 218.41150386737326 Test RE 0.24878656579897615 Lambda1 0.18043678\n",
      "13 Train Loss 189.33229 Test MSE 196.93576995641862 Test RE 0.23623893347911398 Lambda1 0.30938053\n",
      "14 Train Loss 178.96988 Test MSE 178.5688692032911 Test RE 0.22495313234687403 Lambda1 0.376408\n",
      "15 Train Loss 152.9955 Test MSE 143.38201901217099 Test RE 0.20157492941912222 Lambda1 0.41271994\n",
      "16 Train Loss 120.33346 Test MSE 103.67784707576551 Test RE 0.1714084215987714 Lambda1 0.5360582\n",
      "17 Train Loss 101.185234 Test MSE 78.11156527386227 Test RE 0.14878078334541 Lambda1 0.6853034\n",
      "18 Train Loss 81.91524 Test MSE 68.93353497152484 Test RE 0.1397669363494546 Lambda1 0.8084761\n",
      "19 Train Loss 74.77641 Test MSE 69.87940573153897 Test RE 0.1407225745817142 Lambda1 0.8333493\n",
      "20 Train Loss 55.779324 Test MSE 56.38302009408383 Test RE 0.12640473367790103 Lambda1 0.81407875\n",
      "21 Train Loss 48.928886 Test MSE 46.70175422891283 Test RE 0.11504182631981838 Lambda1 0.8784812\n",
      "22 Train Loss 44.323936 Test MSE 40.84501024302783 Test RE 0.10758672196209551 Lambda1 0.9204993\n",
      "23 Train Loss 42.34836 Test MSE 40.57515510456208 Test RE 0.10723073059094694 Lambda1 0.87691855\n",
      "24 Train Loss 41.1621 Test MSE 40.370419909083765 Test RE 0.10695985462606965 Lambda1 0.8673206\n",
      "25 Train Loss 38.256893 Test MSE 38.2389143852901 Test RE 0.10409789501560515 Lambda1 0.9084184\n",
      "26 Train Loss 37.348553 Test MSE 37.9718062257655 Test RE 0.10373368325902495 Lambda1 0.9265283\n",
      "27 Train Loss 36.78922 Test MSE 37.29203555029018 Test RE 0.10280097067625994 Lambda1 0.9342584\n",
      "28 Train Loss 35.26599 Test MSE 37.03277545448812 Test RE 0.10244300317208348 Lambda1 0.9345125\n",
      "29 Train Loss 35.097206 Test MSE 37.04009021918918 Test RE 0.1024531200139208 Lambda1 0.9241713\n",
      "30 Train Loss 34.875732 Test MSE 37.18264636882892 Test RE 0.10265008627841528 Lambda1 0.9194297\n",
      "31 Train Loss 34.648895 Test MSE 37.13576221715882 Test RE 0.10258534935500184 Lambda1 0.94592255\n",
      "32 Train Loss 34.4847 Test MSE 36.96584338177159 Test RE 0.10235038489784198 Lambda1 0.96143526\n",
      "33 Train Loss 34.394794 Test MSE 36.984083902780654 Test RE 0.10237563380233132 Lambda1 0.96809226\n",
      "34 Train Loss 34.059868 Test MSE 37.01569457163101 Test RE 0.10241937520021506 Lambda1 1.0144321\n",
      "35 Train Loss 33.8752 Test MSE 37.01188875519417 Test RE 0.10241410987471206 Lambda1 1.0685422\n",
      "36 Train Loss 33.657722 Test MSE 37.20158854874429 Test RE 0.10267622977567262 Lambda1 1.1220777\n",
      "37 Train Loss 33.453945 Test MSE 37.22204486487603 Test RE 0.10270445556977477 Lambda1 1.1096722\n",
      "38 Train Loss 33.381104 Test MSE 37.171244194723535 Test RE 0.10263434608758612 Lambda1 1.1142899\n",
      "39 Train Loss 33.28241 Test MSE 37.026412957553184 Test RE 0.10243420257202329 Lambda1 1.1265205\n",
      "40 Train Loss 32.994183 Test MSE 36.46501409611502 Test RE 0.10165467671042791 Lambda1 1.1336294\n",
      "41 Train Loss 32.710045 Test MSE 35.919733909928716 Test RE 0.10089176648278551 Lambda1 1.1315035\n",
      "42 Train Loss 32.548176 Test MSE 35.61936113870027 Test RE 0.10046903560625874 Lambda1 1.1273854\n",
      "43 Train Loss 32.49263 Test MSE 35.321530997551974 Test RE 0.10004811958738427 Lambda1 1.1149516\n",
      "44 Train Loss 32.447304 Test MSE 35.17359369214433 Test RE 0.09983838378378879 Lambda1 1.1019318\n",
      "45 Train Loss 32.385197 Test MSE 34.75555438548901 Test RE 0.09924331918682171 Lambda1 1.0825642\n",
      "46 Train Loss 32.33723 Test MSE 34.58515595079182 Test RE 0.09899973675698114 Lambda1 1.0578753\n",
      "47 Train Loss 32.275177 Test MSE 34.32309962192119 Test RE 0.09862395647951112 Lambda1 1.0328423\n",
      "48 Train Loss 32.053432 Test MSE 34.10301576198665 Test RE 0.09830725361899025 Lambda1 1.0363404\n",
      "49 Train Loss 31.89522 Test MSE 33.93332390454148 Test RE 0.09806236700651681 Lambda1 1.0073739\n",
      "50 Train Loss 31.814556 Test MSE 33.93843942792252 Test RE 0.09806975828696154 Lambda1 1.0043563\n",
      "51 Train Loss 31.705347 Test MSE 33.950766033662006 Test RE 0.09808756637594652 Lambda1 1.0144159\n",
      "52 Train Loss 31.649216 Test MSE 34.027382817022875 Test RE 0.09819818123992967 Lambda1 1.0250235\n",
      "53 Train Loss 31.575397 Test MSE 33.88663841482705 Test RE 0.0979948866536054 Lambda1 1.0201379\n",
      "54 Train Loss 31.544655 Test MSE 33.92378581701089 Test RE 0.09804858419558374 Lambda1 1.0329883\n",
      "55 Train Loss 31.508743 Test MSE 33.87009557976671 Test RE 0.09797096408087091 Lambda1 1.0501187\n",
      "56 Train Loss 31.452192 Test MSE 33.879691382966165 Test RE 0.09798484127102661 Lambda1 1.0719885\n",
      "57 Train Loss 31.44242 Test MSE 33.84447217267427 Test RE 0.09793389856902192 Lambda1 1.0764881\n",
      "58 Train Loss 31.413364 Test MSE 33.73588933322356 Test RE 0.09777667224815466 Lambda1 1.062021\n",
      "59 Train Loss 31.37778 Test MSE 33.7396249001297 Test RE 0.09778208549199936 Lambda1 1.0452666\n",
      "60 Train Loss 31.364338 Test MSE 33.71400680370082 Test RE 0.09774495605464444 Lambda1 1.033764\n",
      "61 Train Loss 31.34051 Test MSE 33.75955502784874 Test RE 0.09781096135755646 Lambda1 1.0506585\n",
      "62 Train Loss 31.31673 Test MSE 33.62264117653569 Test RE 0.09761242081559095 Lambda1 1.0539651\n",
      "63 Train Loss 31.306227 Test MSE 33.59138587946035 Test RE 0.09756704046388255 Lambda1 1.0566263\n",
      "64 Train Loss 31.29888 Test MSE 33.54386444201744 Test RE 0.09749800242239486 Lambda1 1.052985\n",
      "65 Train Loss 31.257519 Test MSE 33.456533806750926 Test RE 0.09737100286941555 Lambda1 1.020199\n",
      "66 Train Loss 31.222101 Test MSE 33.44592321755033 Test RE 0.09735556125722908 Lambda1 1.0151795\n",
      "67 Train Loss 31.210606 Test MSE 33.36067038914445 Test RE 0.09723140363306419 Lambda1 1.004793\n",
      "68 Train Loss 31.193573 Test MSE 33.3633348708588 Test RE 0.09723528644059304 Lambda1 1.0017179\n",
      "69 Train Loss 31.192377 Test MSE 33.3767339046743 Test RE 0.09725480979028574 Lambda1 1.0037422\n",
      "70 Train Loss 31.178404 Test MSE 33.36364376938058 Test RE 0.0972357365719549 Lambda1 0.99680656\n",
      "71 Train Loss 31.161884 Test MSE 33.294065135812275 Test RE 0.09713429290577123 Lambda1 0.9983827\n",
      "72 Train Loss 31.156206 Test MSE 33.2535157525496 Test RE 0.09707512416731405 Lambda1 0.9975901\n",
      "73 Train Loss 31.129793 Test MSE 33.24755031258895 Test RE 0.09706641648956152 Lambda1 1.003556\n",
      "74 Train Loss 31.109755 Test MSE 33.26774088179617 Test RE 0.09709588525840239 Lambda1 1.0119308\n",
      "Training time: 74.19\n",
      "Training time: 74.19\n",
      "inv_HT_swish_tune4\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 838.0581 Test MSE 858.2762706357665 Test RE 0.49317689653843527 Lambda1 2.335109e-07\n",
      "1 Train Loss 837.7989 Test MSE 857.9253033089442 Test RE 0.4930760509939956 Lambda1 0.10975963\n",
      "2 Train Loss 835.8765 Test MSE 852.5144683008933 Test RE 0.49151870498532824 Lambda1 0.472391\n",
      "3 Train Loss 796.1291 Test MSE 813.3645381526628 Test RE 0.48010009010668425 Lambda1 0.48182893\n",
      "4 Train Loss 774.1535 Test MSE 785.7620976024556 Test RE 0.4718834095426557 Lambda1 0.3008834\n",
      "5 Train Loss 723.1035 Test MSE 760.5037707639627 Test RE 0.46423711270720847 Lambda1 0.0034704993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Train Loss 670.21466 Test MSE 676.4154513743003 Test RE 0.43782033652473673 Lambda1 -0.0031803448\n",
      "7 Train Loss 481.9897 Test MSE 468.7404524247978 Test RE 0.364464630088559 Lambda1 0.0029527606\n",
      "8 Train Loss 333.67087 Test MSE 354.9863120225117 Test RE 0.317172097540324 Lambda1 0.0007392877\n",
      "9 Train Loss 286.17648 Test MSE 311.54239236210907 Test RE 0.2971308503943421 Lambda1 0.002500546\n",
      "10 Train Loss 264.98035 Test MSE 286.7938891547494 Test RE 0.2850848360262567 Lambda1 0.00510872\n",
      "11 Train Loss 258.4392 Test MSE 278.3551947011075 Test RE 0.28085931734677627 Lambda1 0.013431348\n",
      "12 Train Loss 253.43953 Test MSE 270.5233835133745 Test RE 0.2768799934092483 Lambda1 0.02977297\n",
      "13 Train Loss 238.66496 Test MSE 255.25951801378858 Test RE 0.26895532242164555 Lambda1 0.057523187\n",
      "14 Train Loss 207.51486 Test MSE 219.908081166599 Test RE 0.24963746579355495 Lambda1 0.11957171\n",
      "15 Train Loss 187.6854 Test MSE 198.20376626342946 Test RE 0.23699824060847702 Lambda1 0.16900447\n",
      "16 Train Loss 132.35811 Test MSE 131.8758695967801 Test RE 0.1933177976462846 Lambda1 0.30810747\n",
      "17 Train Loss 82.31563 Test MSE 76.14011614881244 Test RE 0.14689125391025082 Lambda1 0.41625\n",
      "18 Train Loss 64.29665 Test MSE 58.267633130454165 Test RE 0.1284999205631294 Lambda1 0.48439884\n",
      "19 Train Loss 59.283783 Test MSE 56.121703165202725 Test RE 0.12611147110982165 Lambda1 0.501829\n",
      "20 Train Loss 55.296097 Test MSE 53.35122310142398 Test RE 0.12295929396493128 Lambda1 0.5275371\n",
      "21 Train Loss 51.231865 Test MSE 49.250748872319 Test RE 0.11813962551152583 Lambda1 0.5800748\n",
      "22 Train Loss 43.444042 Test MSE 44.804591206689196 Test RE 0.11268093194754587 Lambda1 0.65313894\n",
      "23 Train Loss 41.178093 Test MSE 43.61646573657685 Test RE 0.11117686062660413 Lambda1 0.72986746\n",
      "24 Train Loss 39.980446 Test MSE 42.14530213484631 Test RE 0.10928580520020925 Lambda1 0.76001376\n",
      "25 Train Loss 39.679012 Test MSE 42.34144231727312 Test RE 0.10953981282441413 Lambda1 0.7683572\n",
      "26 Train Loss 39.292545 Test MSE 41.481192929179926 Test RE 0.10842134470909713 Lambda1 0.81208485\n",
      "27 Train Loss 38.570835 Test MSE 41.36008279384905 Test RE 0.1082629533850898 Lambda1 0.8811372\n",
      "28 Train Loss 37.36296 Test MSE 40.91306309813752 Test RE 0.10767631107505084 Lambda1 0.9997718\n",
      "29 Train Loss 36.379665 Test MSE 40.52905280204831 Test RE 0.10716979442673888 Lambda1 1.0042981\n",
      "30 Train Loss 36.142517 Test MSE 40.41388868554136 Test RE 0.10701742354822194 Lambda1 1.0238363\n",
      "31 Train Loss 35.899338 Test MSE 39.86202387628906 Test RE 0.10628423307357068 Lambda1 1.0179722\n",
      "32 Train Loss 35.785194 Test MSE 39.56013369787155 Test RE 0.10588100282754975 Lambda1 1.0012934\n",
      "33 Train Loss 35.629696 Test MSE 39.127585408434435 Test RE 0.10530056336175883 Lambda1 0.9861078\n",
      "34 Train Loss 35.559258 Test MSE 38.82547797047977 Test RE 0.10489325829494552 Lambda1 0.9553756\n",
      "35 Train Loss 35.04341 Test MSE 38.79769592690681 Test RE 0.10485572275481655 Lambda1 1.0198995\n",
      "36 Train Loss 34.779076 Test MSE 38.51639075828001 Test RE 0.104474899650967 Lambda1 1.0733533\n",
      "37 Train Loss 34.630344 Test MSE 37.96295935352315 Test RE 0.10372159834400235 Lambda1 1.0371032\n",
      "38 Train Loss 34.617836 Test MSE 37.803194443233146 Test RE 0.10350311507085234 Lambda1 1.0325556\n",
      "39 Train Loss 34.56319 Test MSE 37.61351634045508 Test RE 0.10324312430720702 Lambda1 1.0661783\n",
      "40 Train Loss 34.396843 Test MSE 37.43549843014033 Test RE 0.10299851911060846 Lambda1 1.087236\n",
      "41 Train Loss 34.329556 Test MSE 37.67702131894747 Test RE 0.10333024308729905 Lambda1 1.1598941\n",
      "42 Train Loss 34.30167 Test MSE 37.46150501800236 Test RE 0.10303428963718489 Lambda1 1.180145\n",
      "43 Train Loss 34.218517 Test MSE 37.30351821950437 Test RE 0.10281679628983406 Lambda1 1.2245475\n",
      "44 Train Loss 34.128464 Test MSE 36.78388006489246 Test RE 0.10209816569432191 Lambda1 1.1962961\n",
      "45 Train Loss 33.98964 Test MSE 36.25710230057298 Test RE 0.10136446128590194 Lambda1 1.177005\n",
      "46 Train Loss 33.798992 Test MSE 35.683290025538774 Test RE 0.10055915505360691 Lambda1 1.148565\n",
      "47 Train Loss 33.42109 Test MSE 35.32819555694731 Test RE 0.10005755781340711 Lambda1 1.1902312\n",
      "48 Train Loss 33.28015 Test MSE 35.117529774714406 Test RE 0.09975878482158908 Lambda1 1.1767839\n",
      "49 Train Loss 33.15007 Test MSE 34.7171374976872 Test RE 0.09918845495470668 Lambda1 1.1570479\n",
      "50 Train Loss 32.74172 Test MSE 34.27991562671689 Test RE 0.09856189453036045 Lambda1 1.1104212\n",
      "51 Train Loss 32.592686 Test MSE 34.19576877015034 Test RE 0.09844085033693738 Lambda1 1.0681936\n",
      "52 Train Loss 32.537586 Test MSE 34.327372220341914 Test RE 0.09863009472827966 Lambda1 1.0737101\n",
      "53 Train Loss 32.272785 Test MSE 34.187268606114145 Test RE 0.09842861468013539 Lambda1 1.0444684\n",
      "54 Train Loss 32.031216 Test MSE 34.04817479141073 Test RE 0.09822817799698223 Lambda1 1.0066524\n",
      "55 Train Loss 31.954023 Test MSE 34.20181695493322 Test RE 0.09844955553863721 Lambda1 0.98636943\n",
      "56 Train Loss 31.92599 Test MSE 34.33635653799856 Test RE 0.09864300084476484 Lambda1 0.99691623\n",
      "57 Train Loss 31.904636 Test MSE 34.435335496539714 Test RE 0.09878507409311398 Lambda1 1.0172516\n",
      "58 Train Loss 31.890347 Test MSE 34.56044184102196 Test RE 0.09896435846326283 Lambda1 1.0316439\n",
      "59 Train Loss 31.85021 Test MSE 34.43674389498436 Test RE 0.09878709421750283 Lambda1 1.0413582\n",
      "60 Train Loss 31.82624 Test MSE 34.270140905896135 Test RE 0.09854784134976124 Lambda1 1.0424726\n",
      "61 Train Loss 31.808159 Test MSE 34.215937702281686 Test RE 0.09846987664968125 Lambda1 1.0237176\n",
      "62 Train Loss 31.769396 Test MSE 34.22129346935076 Test RE 0.09847758301580954 Lambda1 1.0158211\n",
      "63 Train Loss 31.61965 Test MSE 33.88023113229581 Test RE 0.09798562178347511 Lambda1 1.0024409\n",
      "64 Train Loss 31.58597 Test MSE 33.7402351137202 Test RE 0.09778296972956668 Lambda1 0.98943007\n",
      "65 Train Loss 31.555315 Test MSE 33.78059672355315 Test RE 0.09784143847564301 Lambda1 0.97045106\n",
      "66 Train Loss 31.543077 Test MSE 33.79133281950453 Test RE 0.09785698514628721 Lambda1 0.95398754\n",
      "67 Train Loss 31.514097 Test MSE 33.65776986429173 Test RE 0.09766339987244158 Lambda1 0.95098144\n",
      "68 Train Loss 31.479134 Test MSE 33.58298186824398 Test RE 0.09755483486753501 Lambda1 0.97074854\n",
      "69 Train Loss 31.428606 Test MSE 33.655513430049304 Test RE 0.09766012611523542 Lambda1 0.96868956\n",
      "70 Train Loss 31.406218 Test MSE 33.63594821826422 Test RE 0.0976317352420509 Lambda1 0.96156234\n",
      "71 Train Loss 31.378613 Test MSE 33.62007332299358 Test RE 0.09760869327995204 Lambda1 0.9813062\n",
      "72 Train Loss 31.335972 Test MSE 33.62538995212433 Test RE 0.09761641082271698 Lambda1 0.99362856\n",
      "73 Train Loss 31.292282 Test MSE 33.62904192046008 Test RE 0.09762171161421686 Lambda1 1.012372\n",
      "74 Train Loss 31.26095 Test MSE 33.65574414208895 Test RE 0.09766046084981496 Lambda1 1.0233616\n",
      "Training time: 75.04\n",
      "Training time: 75.04\n",
      "inv_HT_swish_tune4\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 838.0471 Test MSE 858.1844819327364 Test RE 0.49315052433194767 Lambda1 -2.3540007e-07\n",
      "1 Train Loss 837.88025 Test MSE 857.8171940776866 Test RE 0.49304498316557793 Lambda1 -0.066107005\n",
      "2 Train Loss 837.6254 Test MSE 857.6708243499901 Test RE 0.4930029171291573 Lambda1 -0.67820257\n",
      "3 Train Loss 835.65533 Test MSE 855.2247024676471 Test RE 0.4922993801354729 Lambda1 -0.41184863\n",
      "4 Train Loss 831.8415 Test MSE 853.718275992191 Test RE 0.4918656112777721 Lambda1 -0.4657504\n",
      "5 Train Loss 813.0637 Test MSE 829.2282299821495 Test RE 0.48475936729124824 Lambda1 -0.5174247\n",
      "6 Train Loss 784.74 Test MSE 794.1248506847148 Test RE 0.47438785721624965 Lambda1 -0.33454937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 718.9536 Test MSE 717.7706195999767 Test RE 0.45100567782527934 Lambda1 -0.068573624\n",
      "8 Train Loss 548.0785 Test MSE 500.65232537976607 Test RE 0.3766667540088279 Lambda1 -0.014808941\n",
      "9 Train Loss 377.43054 Test MSE 365.7031917990074 Test RE 0.32192414137721775 Lambda1 -0.009733399\n",
      "10 Train Loss 300.0711 Test MSE 312.17875266976284 Test RE 0.2974341571465618 Lambda1 -0.003158619\n",
      "11 Train Loss 271.2805 Test MSE 289.8964898534521 Test RE 0.28662274374633856 Lambda1 -0.0038145392\n",
      "12 Train Loss 265.36362 Test MSE 284.02149970817953 Test RE 0.28370355561166166 Lambda1 -0.0026326035\n",
      "13 Train Loss 261.4023 Test MSE 282.5159263849319 Test RE 0.2829506124996882 Lambda1 -0.0022991416\n",
      "14 Train Loss 257.90057 Test MSE 282.2641483050743 Test RE 0.28282450165996886 Lambda1 -0.00047587117\n",
      "15 Train Loss 257.30484 Test MSE 281.68984197687513 Test RE 0.28253663193432077 Lambda1 -6.480379e-05\n",
      "16 Train Loss 257.16245 Test MSE 281.83170122967994 Test RE 0.2826077658281621 Lambda1 -7.6002914e-05\n",
      "17 Train Loss 256.72418 Test MSE 282.5413881672436 Test RE 0.28296336269164196 Lambda1 0.00040015072\n",
      "18 Train Loss 256.46356 Test MSE 281.86849670657216 Test RE 0.28262621362534934 Lambda1 0.00080009585\n",
      "19 Train Loss 256.1058 Test MSE 280.4325238401852 Test RE 0.28190537786330433 Lambda1 0.002108583\n",
      "20 Train Loss 255.90617 Test MSE 280.07615988716645 Test RE 0.28172620312045477 Lambda1 0.0026428346\n",
      "21 Train Loss 255.70522 Test MSE 279.984718987237 Test RE 0.2816802095582475 Lambda1 0.0022713114\n",
      "22 Train Loss 255.6183 Test MSE 279.95594664669784 Test RE 0.2816657358984569 Lambda1 0.002744992\n",
      "23 Train Loss 255.04012 Test MSE 279.28358511116136 Test RE 0.2813272986309828 Lambda1 0.004054837\n",
      "24 Train Loss 254.66881 Test MSE 278.6073634659293 Test RE 0.2809865072025215 Lambda1 0.0046988204\n",
      "25 Train Loss 254.29797 Test MSE 277.78799399057317 Test RE 0.2805730197620816 Lambda1 0.0065150023\n",
      "26 Train Loss 253.69887 Test MSE 276.2540372605755 Test RE 0.27979727948370414 Lambda1 0.009348059\n",
      "27 Train Loss 252.55083 Test MSE 272.3183591441396 Test RE 0.2777970512009608 Lambda1 0.014261748\n",
      "28 Train Loss 248.36047 Test MSE 266.5810192678026 Test RE 0.274855089173548 Lambda1 0.026834413\n",
      "29 Train Loss 241.88431 Test MSE 258.4662758707433 Test RE 0.27063945691001756 Lambda1 0.042768635\n",
      "30 Train Loss 232.04062 Test MSE 248.7044944689915 Test RE 0.26547949783390057 Lambda1 0.07083701\n",
      "31 Train Loss 224.43497 Test MSE 239.60208593705175 Test RE 0.2605760329008443 Lambda1 0.089360386\n",
      "32 Train Loss 215.95308 Test MSE 223.41967281629582 Test RE 0.25162273371996585 Lambda1 0.12851879\n",
      "33 Train Loss 201.51704 Test MSE 201.27372421477904 Test RE 0.23882660876781872 Lambda1 0.18266295\n",
      "34 Train Loss 187.60716 Test MSE 184.18995712124214 Test RE 0.22846629771221666 Lambda1 0.2151654\n",
      "35 Train Loss 172.82817 Test MSE 169.96046080842652 Test RE 0.2194639141445112 Lambda1 0.22941898\n",
      "36 Train Loss 163.44301 Test MSE 154.29148588635897 Test RE 0.2091029459253239 Lambda1 0.2698469\n",
      "37 Train Loss 153.85823 Test MSE 151.9081433468258 Test RE 0.20748165259453197 Lambda1 0.30080616\n",
      "38 Train Loss 144.46596 Test MSE 134.0849851786808 Test RE 0.19493025225233526 Lambda1 0.36492825\n",
      "39 Train Loss 134.73755 Test MSE 110.54863150965062 Test RE 0.1769969798861136 Lambda1 0.45668522\n",
      "40 Train Loss 107.378334 Test MSE 88.28206806968224 Test RE 0.15817047533263529 Lambda1 0.56080747\n",
      "41 Train Loss 90.82167 Test MSE 79.38278386738419 Test RE 0.14998655607693115 Lambda1 0.6489357\n",
      "42 Train Loss 67.29185 Test MSE 62.180548605934426 Test RE 0.13274447228504402 Lambda1 0.72107834\n",
      "43 Train Loss 57.5785 Test MSE 50.8020445145571 Test RE 0.11998577637517914 Lambda1 0.7961618\n",
      "44 Train Loss 50.499313 Test MSE 47.459358659303426 Test RE 0.11597118722957593 Lambda1 0.82318145\n",
      "45 Train Loss 40.358105 Test MSE 40.169832197487366 Test RE 0.10669379906500631 Lambda1 0.8510229\n",
      "46 Train Loss 37.82097 Test MSE 38.39877812282168 Test RE 0.10431526679541143 Lambda1 0.86050826\n",
      "47 Train Loss 37.228306 Test MSE 38.214892511842926 Test RE 0.10406519247487879 Lambda1 0.86693543\n",
      "48 Train Loss 36.268215 Test MSE 37.445687287190545 Test RE 0.10301253476199193 Lambda1 0.86662906\n",
      "49 Train Loss 35.86149 Test MSE 37.32188107224732 Test RE 0.10284209923029888 Lambda1 0.86841136\n",
      "50 Train Loss 34.327286 Test MSE 36.8096737642077 Test RE 0.10213395620742083 Lambda1 0.86043346\n",
      "51 Train Loss 34.153553 Test MSE 36.51902237488241 Test RE 0.10172992915277947 Lambda1 0.8662304\n",
      "52 Train Loss 33.991627 Test MSE 36.399371987874424 Test RE 0.10156313920292877 Lambda1 0.87414104\n",
      "53 Train Loss 33.4412 Test MSE 35.84770802001769 Test RE 0.1007905621450949 Lambda1 0.9082898\n",
      "54 Train Loss 32.915157 Test MSE 35.695895635304666 Test RE 0.10057691543298568 Lambda1 0.93323654\n",
      "55 Train Loss 32.791904 Test MSE 35.43463924792373 Test RE 0.10020818092315538 Lambda1 0.95973855\n",
      "56 Train Loss 32.641903 Test MSE 35.38884647194063 Test RE 0.10014340963740156 Lambda1 0.98900175\n",
      "57 Train Loss 32.388382 Test MSE 35.10098387128693 Test RE 0.09973528098035446 Lambda1 0.98479635\n",
      "58 Train Loss 32.255615 Test MSE 34.84808598218325 Test RE 0.09937534180703346 Lambda1 1.0122658\n",
      "59 Train Loss 32.06357 Test MSE 34.65068443244694 Test RE 0.09909347975583381 Lambda1 1.0359092\n",
      "60 Train Loss 31.881739 Test MSE 34.35778990122345 Test RE 0.09867378339778564 Lambda1 1.0824107\n",
      "61 Train Loss 31.714226 Test MSE 34.32062943196254 Test RE 0.09862040749510888 Lambda1 1.0976585\n",
      "62 Train Loss 31.591808 Test MSE 34.0437505563795 Test RE 0.09822179588300323 Lambda1 1.1008401\n",
      "63 Train Loss 31.534029 Test MSE 34.04861734587037 Test RE 0.09822881637447593 Lambda1 1.0918711\n",
      "64 Train Loss 31.462143 Test MSE 34.06324846349716 Test RE 0.09824991918446456 Lambda1 1.065628\n",
      "65 Train Loss 31.433088 Test MSE 33.99303738564588 Test RE 0.0981486107162622 Lambda1 1.0733496\n",
      "66 Train Loss 31.364779 Test MSE 34.00932738224976 Test RE 0.09817212507621223 Lambda1 1.0763475\n",
      "67 Train Loss 31.324974 Test MSE 33.88333522615671 Test RE 0.09799011038332393 Lambda1 1.0922152\n",
      "68 Train Loss 31.202154 Test MSE 33.71246222516033 Test RE 0.09774271697799479 Lambda1 1.1003228\n",
      "69 Train Loss 31.051722 Test MSE 33.54385723856289 Test RE 0.09749799195367757 Lambda1 1.1213362\n",
      "70 Train Loss 30.971533 Test MSE 33.407015971401286 Test RE 0.09729891849409206 Lambda1 1.1011413\n",
      "71 Train Loss 30.86991 Test MSE 33.516615423145446 Test RE 0.09745839361893185 Lambda1 1.0811709\n",
      "72 Train Loss 30.83175 Test MSE 33.46862233611246 Test RE 0.09738859234727495 Lambda1 1.0846518\n",
      "73 Train Loss 30.785597 Test MSE 33.4430750155141 Test RE 0.09735141584599888 Lambda1 1.0882539\n",
      "74 Train Loss 30.651062 Test MSE 33.402864928806665 Test RE 0.09729287328935474 Lambda1 1.0978745\n",
      "Training time: 72.74\n",
      "Training time: 72.74\n",
      "inv_HT_swish_tune4\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 838.04224 Test MSE 858.3749224410776 Test RE 0.4932052390400669 Lambda1 1.6804559e-06\n",
      "1 Train Loss 837.8779 Test MSE 857.7543214095373 Test RE 0.49302691426823375 Lambda1 0.029065095\n",
      "2 Train Loss 837.82245 Test MSE 857.8172116058112 Test RE 0.49304498820287135 Lambda1 0.08900687\n",
      "3 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "4 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "5 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "6 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "7 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "9 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "10 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "11 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "12 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "13 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "14 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "15 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "16 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "17 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "18 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "19 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "20 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "21 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "22 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "23 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "24 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "25 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "26 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "27 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "28 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "29 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "30 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "31 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "32 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "33 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "34 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "35 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "36 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "37 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "38 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "39 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "40 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "41 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "42 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "43 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "44 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "45 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "46 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "47 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "48 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "49 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "50 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "51 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "52 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "53 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "54 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "55 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "56 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "57 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "58 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "59 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "60 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "61 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "62 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "63 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "64 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "65 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "66 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "67 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "68 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "69 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "70 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "71 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "72 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "73 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "74 Train Loss 837.6075 Test MSE 857.771431787369 Test RE 0.49303183166387066 Lambda1 -0.15938672\n",
      "Training time: 108.31\n",
      "Training time: 108.31\n",
      "inv_HT_swish_tune4\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 837.9306 Test MSE 858.2396576331772 Test RE 0.49316637726848644 Lambda1 -1.3288469e-06\n",
      "1 Train Loss 837.705 Test MSE 858.0277440697034 Test RE 0.49310548804279775 Lambda1 -0.005522359\n",
      "2 Train Loss 836.5539 Test MSE 857.4431492379792 Test RE 0.49293747713678765 Lambda1 -0.015545187\n",
      "3 Train Loss 828.6964 Test MSE 849.8459710801467 Test RE 0.49074883869853364 Lambda1 -0.11679585\n",
      "4 Train Loss 771.6837 Test MSE 767.4935388851237 Test RE 0.466365628038641 Lambda1 0.022941725\n",
      "5 Train Loss 577.7486 Test MSE 599.9643530015683 Test RE 0.41233660836860925 Lambda1 0.0012498627\n",
      "6 Train Loss 477.17328 Test MSE 502.04105199043823 Test RE 0.3771887978315539 Lambda1 -0.0003169807\n",
      "7 Train Loss 433.99103 Test MSE 436.6551701114717 Test RE 0.3517697369258094 Lambda1 -0.00021300015\n",
      "8 Train Loss 344.77618 Test MSE 356.19914230650363 Test RE 0.317713453510377 Lambda1 4.3947115e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Train Loss 286.56744 Test MSE 297.8795004055155 Test RE 0.29054237281506307 Lambda1 -0.0009810154\n",
      "10 Train Loss 268.00735 Test MSE 286.8252386833322 Test RE 0.28510041695620814 Lambda1 -0.0051085437\n",
      "11 Train Loss 265.2935 Test MSE 284.2813469325387 Test RE 0.28383330411309937 Lambda1 -0.0056731105\n",
      "12 Train Loss 261.32224 Test MSE 281.22885190507316 Test RE 0.2823053493423655 Lambda1 -0.002261241\n",
      "13 Train Loss 260.60043 Test MSE 279.34072667846834 Test RE 0.28135607701512005 Lambda1 0.0014019766\n",
      "14 Train Loss 258.12946 Test MSE 274.57929066565964 Test RE 0.2789478767126144 Lambda1 0.01048736\n",
      "15 Train Loss 256.8397 Test MSE 274.2553752316654 Test RE 0.27878329367780785 Lambda1 0.011551838\n",
      "16 Train Loss 256.41827 Test MSE 272.0003608398823 Test RE 0.27763480581305916 Lambda1 0.015955104\n",
      "17 Train Loss 253.91618 Test MSE 268.98772878991235 Test RE 0.27609300561740663 Lambda1 0.028071666\n",
      "18 Train Loss 248.59232 Test MSE 258.3849466282423 Test RE 0.2705968737263652 Lambda1 0.066970915\n",
      "19 Train Loss 244.8799 Test MSE 254.16282227760064 Test RE 0.2683769313423782 Lambda1 0.091448784\n",
      "20 Train Loss 235.4155 Test MSE 246.21945387397156 Test RE 0.26414984031229116 Lambda1 0.091349036\n",
      "21 Train Loss 231.90787 Test MSE 241.87779145546324 Test RE 0.26181056497450694 Lambda1 0.08806756\n",
      "22 Train Loss 224.64752 Test MSE 219.3036858198896 Test RE 0.24929417796609046 Lambda1 0.14126882\n",
      "23 Train Loss 193.9101 Test MSE 198.0377071965718 Test RE 0.2368989388794323 Lambda1 0.22449903\n",
      "24 Train Loss 177.41039 Test MSE 178.81003878216353 Test RE 0.2251049884544865 Lambda1 0.23469968\n",
      "25 Train Loss 153.1844 Test MSE 131.82113483547687 Test RE 0.19327767544009233 Lambda1 0.32956675\n",
      "26 Train Loss 140.50293 Test MSE 115.61593754051223 Test RE 0.18100810564900774 Lambda1 0.38374352\n",
      "27 Train Loss 118.044754 Test MSE 102.24749775095756 Test RE 0.17022193180610917 Lambda1 0.40598232\n",
      "28 Train Loss 102.61825 Test MSE 89.89392271556395 Test RE 0.15960788281014526 Lambda1 0.4708371\n",
      "29 Train Loss 92.95126 Test MSE 74.878785019259 Test RE 0.14566947838488112 Lambda1 0.5315461\n",
      "30 Train Loss 81.7168 Test MSE 67.7283225036173 Test RE 0.13853972785931 Lambda1 0.6091749\n",
      "31 Train Loss 64.14006 Test MSE 56.853040476093675 Test RE 0.12693050806189338 Lambda1 0.7184172\n",
      "32 Train Loss 60.863056 Test MSE 53.08334208547821 Test RE 0.12265021097207106 Lambda1 0.76805043\n",
      "33 Train Loss 48.77572 Test MSE 48.45175813443501 Test RE 0.1171774225385503 Lambda1 0.92030585\n",
      "34 Train Loss 42.643757 Test MSE 41.80088483295694 Test RE 0.10883833969682442 Lambda1 1.0375963\n",
      "35 Train Loss 40.25329 Test MSE 39.73888008658141 Test RE 0.10611993676052289 Lambda1 1.058357\n",
      "36 Train Loss 37.73482 Test MSE 38.45494911762955 Test RE 0.1043915368135904 Lambda1 1.0710341\n",
      "37 Train Loss 37.345318 Test MSE 38.57810570380308 Test RE 0.10455856639521116 Lambda1 1.0702397\n",
      "38 Train Loss 35.64864 Test MSE 37.7222719940187 Test RE 0.10339227506352873 Lambda1 1.0831134\n",
      "39 Train Loss 35.01557 Test MSE 37.132823142070706 Test RE 0.1025812897640236 Lambda1 1.0888381\n",
      "40 Train Loss 34.343975 Test MSE 36.99447229072431 Test RE 0.10239001083990829 Lambda1 1.0661027\n",
      "41 Train Loss 33.437675 Test MSE 35.73236694998025 Test RE 0.10062828318523011 Lambda1 1.0292537\n",
      "42 Train Loss 33.027805 Test MSE 35.27166934630156 Test RE 0.09997747816730583 Lambda1 1.0017111\n",
      "43 Train Loss 32.647224 Test MSE 34.961714926940296 Test RE 0.0995372262407545 Lambda1 0.98805827\n",
      "44 Train Loss 32.400677 Test MSE 34.98123279421175 Test RE 0.09956500638980598 Lambda1 0.98481494\n",
      "45 Train Loss 32.316883 Test MSE 34.76208255270675 Test RE 0.09925263923040002 Lambda1 0.9727271\n",
      "46 Train Loss 32.202244 Test MSE 34.593343146594066 Test RE 0.09901145395536093 Lambda1 0.9727257\n",
      "47 Train Loss 32.126743 Test MSE 34.70930264729189 Test RE 0.09917726205962184 Lambda1 0.9870201\n",
      "48 Train Loss 31.865309 Test MSE 34.6773997503625 Test RE 0.09913167241771238 Lambda1 1.0266607\n",
      "49 Train Loss 31.825798 Test MSE 34.574659541884174 Test RE 0.09898471267301176 Lambda1 1.0389109\n",
      "50 Train Loss 31.755207 Test MSE 34.13281916404045 Test RE 0.09835020071170315 Lambda1 1.0348864\n",
      "51 Train Loss 31.6989 Test MSE 34.01478235541877 Test RE 0.09817999798751416 Lambda1 1.0556082\n",
      "52 Train Loss 31.6518 Test MSE 34.128578914046756 Test RE 0.0983440916002872 Lambda1 1.0581062\n",
      "53 Train Loss 31.606936 Test MSE 34.08732066131087 Test RE 0.09828462922969951 Lambda1 1.0362555\n",
      "54 Train Loss 31.557833 Test MSE 34.037762258042335 Test RE 0.0982131568923675 Lambda1 1.0388787\n",
      "55 Train Loss 31.507664 Test MSE 33.81171253848132 Test RE 0.0978864897128247 Lambda1 1.0129216\n",
      "56 Train Loss 31.49026 Test MSE 33.8280512827889 Test RE 0.09791013757094402 Lambda1 1.0046793\n",
      "57 Train Loss 31.487421 Test MSE 33.77539809944205 Test RE 0.09783390959100013 Lambda1 0.99676037\n",
      "58 Train Loss 31.451508 Test MSE 33.8061174116143 Test RE 0.09787839030071355 Lambda1 1.0022155\n",
      "59 Train Loss 31.422976 Test MSE 33.85324095680577 Test RE 0.09794658462295552 Lambda1 1.0111605\n",
      "60 Train Loss 31.381115 Test MSE 33.840762501765965 Test RE 0.0979285311860045 Lambda1 1.0342261\n",
      "61 Train Loss 31.344688 Test MSE 33.89459094332568 Test RE 0.09800638471668617 Lambda1 1.0419084\n",
      "62 Train Loss 31.314983 Test MSE 33.86523712449355 Test RE 0.09796393716509909 Lambda1 1.0259594\n",
      "63 Train Loss 31.304789 Test MSE 33.831683411804796 Test RE 0.09791539375172935 Lambda1 1.0296241\n",
      "64 Train Loss 31.295603 Test MSE 33.82581607169344 Test RE 0.0979069027786217 Lambda1 1.0282388\n",
      "65 Train Loss 31.288086 Test MSE 33.78499652463889 Test RE 0.09784781001570875 Lambda1 1.0227605\n",
      "66 Train Loss 31.279922 Test MSE 33.74300101869368 Test RE 0.09778697759819603 Lambda1 1.0139232\n",
      "67 Train Loss 31.274277 Test MSE 33.75208376992052 Test RE 0.09780013758698639 Lambda1 1.0184793\n",
      "68 Train Loss 31.27219 Test MSE 33.77961118614977 Test RE 0.09784001121991216 Lambda1 1.0250901\n",
      "69 Train Loss 31.271488 Test MSE 33.79038969397929 Test RE 0.0978556195288819 Lambda1 1.025061\n",
      "70 Train Loss 31.271488 Test MSE 33.79038969397929 Test RE 0.0978556195288819 Lambda1 1.025061\n",
      "71 Train Loss 31.271488 Test MSE 33.79038969397929 Test RE 0.0978556195288819 Lambda1 1.025061\n",
      "72 Train Loss 31.271488 Test MSE 33.79038969397929 Test RE 0.0978556195288819 Lambda1 1.025061\n",
      "73 Train Loss 31.271488 Test MSE 33.79038969397929 Test RE 0.0978556195288819 Lambda1 1.025061\n",
      "74 Train Loss 31.271488 Test MSE 33.79038969397929 Test RE 0.0978556195288819 Lambda1 1.025061\n",
      "Training time: 74.77\n",
      "Training time: 74.77\n",
      "inv_HT_swish_tune4\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 838.0728 Test MSE 858.2227714369179 Test RE 0.4931615256252862 Lambda1 -1.2641498e-05\n",
      "1 Train Loss 837.918 Test MSE 858.1028981300926 Test RE 0.49312708295533286 Lambda1 -0.0011870229\n",
      "2 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "3 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "4 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "5 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "6 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "7 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "8 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "9 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "10 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "12 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "13 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "14 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "15 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "16 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "17 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "18 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "19 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "20 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "21 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "22 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "23 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "24 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "25 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "26 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "27 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "28 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "29 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "30 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "31 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "32 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "33 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "34 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "35 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "36 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "37 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "38 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "39 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "40 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "41 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "42 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "43 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "44 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "45 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "46 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "47 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "48 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "49 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "50 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "51 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "52 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "53 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "54 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "55 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "56 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "57 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "58 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "59 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "60 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "61 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "62 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "63 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "64 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "65 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "66 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "67 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "68 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "69 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "70 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "71 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "72 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "73 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "74 Train Loss 837.6879 Test MSE 858.2124534807334 Test RE 0.493158561106937 Lambda1 -0.06898803\n",
      "Training time: 98.94\n",
      "Training time: 98.94\n",
      "inv_HT_swish_tune4\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 838.092 Test MSE 858.3494553266181 Test RE 0.49319792253587286 Lambda1 1.5471729e-05\n",
      "1 Train Loss 837.94714 Test MSE 857.8756647567859 Test RE 0.4930617863865624 Lambda1 0.002148609\n",
      "2 Train Loss 837.7877 Test MSE 857.9777547787761 Test RE 0.4930911234998609 Lambda1 0.08846391\n",
      "3 Train Loss 836.5815 Test MSE 857.9674824180369 Test RE 0.49308817166047403 Lambda1 0.47068673\n",
      "4 Train Loss 817.65625 Test MSE 838.0205119472718 Test RE 0.4873225353985234 Lambda1 0.67089736\n",
      "5 Train Loss 779.36487 Test MSE 797.988954634797 Test RE 0.4755406101589875 Lambda1 0.40803164\n",
      "6 Train Loss 745.27966 Test MSE 755.21356912848 Test RE 0.4626196363172841 Lambda1 0.29417032\n",
      "7 Train Loss 707.07477 Test MSE 722.5754739938569 Test RE 0.4525127067181941 Lambda1 0.32451358\n",
      "8 Train Loss 685.99457 Test MSE 709.9634456343188 Test RE 0.4485461825536592 Lambda1 0.2758054\n",
      "9 Train Loss 631.67444 Test MSE 631.6124667442891 Test RE 0.42307222779491493 Lambda1 0.15681235\n",
      "10 Train Loss 571.99 Test MSE 572.1786033512597 Test RE 0.4026752873661504 Lambda1 0.15027016\n",
      "11 Train Loss 493.553 Test MSE 468.416267760682 Test RE 0.3643385749559365 Lambda1 0.1434399\n",
      "12 Train Loss 451.03018 Test MSE 438.46270640610504 Test RE 0.3524970613187844 Lambda1 0.14748739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Train Loss 359.33453 Test MSE 302.11068279339196 Test RE 0.2925985784575564 Lambda1 0.21999246\n",
      "14 Train Loss 298.96936 Test MSE 261.1931187209876 Test RE 0.2720633467332648 Lambda1 0.2666962\n",
      "15 Train Loss 248.43343 Test MSE 225.50302164135198 Test RE 0.2527931801829288 Lambda1 0.32650438\n",
      "16 Train Loss 211.48422 Test MSE 174.2987439768697 Test RE 0.22224720062200753 Lambda1 0.3701516\n",
      "17 Train Loss 176.08882 Test MSE 151.7814522656944 Test RE 0.20739511491056495 Lambda1 0.43772084\n",
      "18 Train Loss 140.61763 Test MSE 126.85083910186889 Test RE 0.18959891286246044 Lambda1 0.47019833\n",
      "19 Train Loss 122.88477 Test MSE 120.09150350595485 Test RE 0.1844783101114369 Lambda1 0.49125287\n",
      "20 Train Loss 111.402176 Test MSE 99.98187643710477 Test RE 0.16832546087844136 Lambda1 0.5577085\n",
      "21 Train Loss 80.052765 Test MSE 72.46947018471893 Test RE 0.14330677205800665 Lambda1 0.6099371\n",
      "22 Train Loss 69.04623 Test MSE 65.32316656214147 Test RE 0.1360575935110873 Lambda1 0.64492553\n",
      "23 Train Loss 62.806995 Test MSE 54.84845946271917 Test RE 0.12467270650417654 Lambda1 0.6793319\n",
      "24 Train Loss 57.795544 Test MSE 47.80879589999355 Test RE 0.11639734480626092 Lambda1 0.7236229\n",
      "25 Train Loss 51.39542 Test MSE 45.346878894449134 Test RE 0.11336079192379767 Lambda1 0.77150387\n",
      "26 Train Loss 48.549377 Test MSE 45.741974722224825 Test RE 0.11385356278559615 Lambda1 0.79162484\n",
      "27 Train Loss 40.787216 Test MSE 41.023706840407044 Test RE 0.10782181064457516 Lambda1 0.84751433\n",
      "28 Train Loss 38.35574 Test MSE 38.9807800294757 Test RE 0.10510283564187566 Lambda1 0.84131646\n",
      "29 Train Loss 37.442745 Test MSE 38.22024601536246 Test RE 0.10407248143773459 Lambda1 0.84076184\n",
      "30 Train Loss 37.080643 Test MSE 38.178794228498695 Test RE 0.10401603019672197 Lambda1 0.8488283\n",
      "31 Train Loss 36.48118 Test MSE 38.091492155223854 Test RE 0.10389703727918057 Lambda1 0.86726177\n",
      "32 Train Loss 35.92967 Test MSE 37.99257154489032 Test RE 0.10376204336139958 Lambda1 0.89969134\n",
      "33 Train Loss 35.60587 Test MSE 38.052904389943755 Test RE 0.1038443986173373 Lambda1 0.9186453\n",
      "34 Train Loss 35.05364 Test MSE 37.54073439031824 Test RE 0.10314318849716729 Lambda1 0.9380657\n",
      "35 Train Loss 34.80492 Test MSE 37.1621548169457 Test RE 0.10262179687917895 Lambda1 0.9262207\n",
      "36 Train Loss 34.689743 Test MSE 37.04014646169481 Test RE 0.10245319779745153 Lambda1 0.92569077\n",
      "37 Train Loss 34.32927 Test MSE 36.84788840528999 Test RE 0.10218695857893303 Lambda1 0.9266959\n",
      "38 Train Loss 34.1642 Test MSE 36.82114321527007 Test RE 0.10214986682352842 Lambda1 0.9405439\n",
      "39 Train Loss 33.875576 Test MSE 36.65284528286019 Test RE 0.10191615190769109 Lambda1 0.960542\n",
      "40 Train Loss 33.820442 Test MSE 36.588937762530556 Test RE 0.10182726317247597 Lambda1 0.9675782\n",
      "41 Train Loss 33.693806 Test MSE 36.5982159849962 Test RE 0.10184017303228707 Lambda1 0.9742017\n",
      "42 Train Loss 33.636044 Test MSE 36.530060089798276 Test RE 0.10174530170454661 Lambda1 0.98169255\n",
      "43 Train Loss 33.55342 Test MSE 36.54572310289431 Test RE 0.1017671120601757 Lambda1 0.9906716\n",
      "44 Train Loss 33.51622 Test MSE 36.502475524227286 Test RE 0.10170687951239407 Lambda1 0.9973065\n",
      "45 Train Loss 33.498558 Test MSE 36.46614378853597 Test RE 0.1016562513379341 Lambda1 1.0028533\n",
      "46 Train Loss 33.482277 Test MSE 36.457859241535466 Test RE 0.10164470331362449 Lambda1 1.0085068\n",
      "47 Train Loss 33.45107 Test MSE 36.484323436531824 Test RE 0.10168158777879176 Lambda1 1.0189576\n",
      "48 Train Loss 33.385067 Test MSE 36.40368343690941 Test RE 0.10156915402281003 Lambda1 1.0302801\n",
      "49 Train Loss 33.36969 Test MSE 36.3770234621989 Test RE 0.10153195549276205 Lambda1 1.0279408\n",
      "50 Train Loss 33.340786 Test MSE 36.32026724864304 Test RE 0.10145271840467276 Lambda1 1.0368232\n",
      "51 Train Loss 33.324726 Test MSE 36.23371583202982 Test RE 0.1013317650826638 Lambda1 1.0405803\n",
      "52 Train Loss 33.31127 Test MSE 36.15432185686366 Test RE 0.10122068699780352 Lambda1 1.0459756\n",
      "53 Train Loss 33.298817 Test MSE 36.12920764140143 Test RE 0.10118552495323732 Lambda1 1.0522982\n",
      "54 Train Loss 33.270046 Test MSE 36.064154242038114 Test RE 0.10109438777473656 Lambda1 1.0443103\n",
      "55 Train Loss 33.224495 Test MSE 36.085205021970694 Test RE 0.101123888054135 Lambda1 1.0594971\n",
      "56 Train Loss 33.136753 Test MSE 36.1286548010614 Test RE 0.10118475079214068 Lambda1 1.0806465\n",
      "57 Train Loss 32.864174 Test MSE 35.73860005356808 Test RE 0.10063705953026059 Lambda1 1.049934\n",
      "58 Train Loss 32.66647 Test MSE 35.27066102396317 Test RE 0.09997604911323893 Lambda1 1.0386602\n",
      "59 Train Loss 32.449226 Test MSE 34.73445284037999 Test RE 0.0992131872355803 Lambda1 1.0165136\n",
      "60 Train Loss 32.27901 Test MSE 34.4462450435434 Test RE 0.09880072103033416 Lambda1 1.0063504\n",
      "61 Train Loss 32.087833 Test MSE 34.23855278607854 Test RE 0.09850241319279342 Lambda1 0.9858735\n",
      "62 Train Loss 32.008446 Test MSE 34.137666621475226 Test RE 0.09835718418827902 Lambda1 0.9739336\n",
      "63 Train Loss 31.936302 Test MSE 34.13586034572302 Test RE 0.09835458203991083 Lambda1 0.9789843\n",
      "64 Train Loss 31.920448 Test MSE 34.09803692459007 Test RE 0.09830007721999076 Lambda1 0.98124164\n",
      "65 Train Loss 31.884045 Test MSE 34.23071461435182 Test RE 0.0984911375552446 Lambda1 0.9941082\n",
      "66 Train Loss 31.859154 Test MSE 34.18714346925473 Test RE 0.09842843453911371 Lambda1 1.0050097\n",
      "67 Train Loss 31.783924 Test MSE 34.02377984744404 Test RE 0.09819298227373655 Lambda1 1.0133718\n",
      "68 Train Loss 31.732391 Test MSE 34.01654154411986 Test RE 0.09818253680884519 Lambda1 1.0342238\n",
      "69 Train Loss 31.623266 Test MSE 34.05019665996887 Test RE 0.09823109447514163 Lambda1 1.0380065\n",
      "70 Train Loss 31.433578 Test MSE 33.92489449966905 Test RE 0.09805018637343905 Lambda1 1.0383571\n",
      "71 Train Loss 31.375029 Test MSE 33.76299657788359 Test RE 0.09781594680150828 Lambda1 1.0300772\n",
      "72 Train Loss 31.27894 Test MSE 33.63698855029965 Test RE 0.09763324506475364 Lambda1 1.0503722\n",
      "73 Train Loss 31.258522 Test MSE 33.55027642102794 Test RE 0.09750732044945323 Lambda1 1.0554712\n",
      "74 Train Loss 31.233027 Test MSE 33.51920940022752 Test RE 0.09746216488200722 Lambda1 1.0547124\n",
      "Training time: 71.91\n",
      "Training time: 71.91\n"
     ]
    }
   ],
   "source": [
    "nan_tune = []\n",
    "for tune_reps in range(5):\n",
    "    label = \"inv_HT_swish_tune\" + str(tune_reps)\n",
    "    max_reps = 10 #10\n",
    "    max_iter = 75 #75\n",
    "\n",
    "    train_loss_full = []\n",
    "    test_mse_full = []\n",
    "    test_re_full = []\n",
    "    beta_full = []\n",
    "   \n",
    "    lambda1_full = []\n",
    "    elapsed_time= np.zeros((max_reps,1))\n",
    "    \n",
    "    time_threshold = np.empty((max_reps,1))\n",
    "    time_threshold[:] = np.nan\n",
    "    epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "    for reps in range(max_reps):\n",
    "        print(label)\n",
    "        'Generate Training data'\n",
    "        print(reps)\n",
    "        torch.manual_seed(reps*36)\n",
    "        \n",
    "        train_loss = []\n",
    "        test_mse_loss = []\n",
    "        test_re_loss = []   \n",
    "        beta_val = []\n",
    "        \n",
    "        lambda1_val = []\n",
    "\n",
    "        N_f = 50000 #Total number of collocation points \n",
    "        N_train = 500\n",
    "\n",
    "        layers = np.array([2,50,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "        \n",
    "        PINN = Sequentialmodel(layers)\n",
    "\n",
    "        PINN.to(device)\n",
    "\n",
    "        'Neural Network Summary'\n",
    "        print(PINN)\n",
    "\n",
    "        params = list(PINN.parameters())\n",
    "\n",
    "        optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lr_tune[tune_reps], \n",
    "                                  max_iter = 10, \n",
    "                                  max_eval = 15, \n",
    "                                  tolerance_grad = -1, \n",
    "                                  tolerance_change = -1, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "        \n",
    "        \n",
    "        nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "        torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "        train_loss_full.append(train_loss)\n",
    "        test_mse_full.append(test_mse_loss)\n",
    "        test_re_full.append(test_re_loss)\n",
    "        #elapsed_time[reps] = time.time() - start_time\n",
    "        beta_full.append(beta_val)\n",
    "\n",
    "        lambda1_full.append(lambda1_val)\n",
    "        \n",
    "        if(nan_flag == 1):\n",
    "            nan_tune.append(tune_reps)\n",
    "            break\n",
    "\n",
    "        print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "    mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"lambda1\": lambda1_full, \"label\": label}\n",
    "    savemat(label+'.mat', mdic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3803765\n",
      "0.5332857\n",
      "0.6176917\n",
      "[[1.0936446e-05 5.1908372e-03 2.6749015e-01 8.3244300e-01 7.1130824e-01\n",
      "  4.3814161e-01 5.0620049e-01 5.8139008e-01 7.2378534e-01 8.4669340e-01\n",
      "  9.0908474e-01 9.4020641e-01 9.3450940e-01 9.6880680e-01 1.0148460e+00\n",
      "  1.0518965e+00 1.0857978e+00 1.1224855e+00 1.1421726e+00 1.1773984e+00\n",
      "  1.1935776e+00 1.2131006e+00 1.2148787e+00 1.2041231e+00 1.1529937e+00\n",
      "  1.1451490e+00 1.1265110e+00 1.1240835e+00 1.1331701e+00 1.1316785e+00\n",
      "  1.0960108e+00 1.0456393e+00 1.0022391e+00 9.7977114e-01 9.9185133e-01\n",
      "  1.0047909e+00 9.8794007e-01 9.6512049e-01 9.5608795e-01 9.4431275e-01\n",
      "  9.3384027e-01 9.2426163e-01 9.3659365e-01 9.6629703e-01 9.7704625e-01\n",
      "  9.8526388e-01 1.0508255e+00 1.0847062e+00 1.1020237e+00 1.0776469e+00\n",
      "  1.0739287e+00 1.0513834e+00 1.0092171e+00 9.9098402e-01 9.6912360e-01\n",
      "  9.6092439e-01 9.6978724e-01 9.9861151e-01 1.0082142e+00 1.0256081e+00\n",
      "  1.0191369e+00 1.0255429e+00 1.0238643e+00 1.0226312e+00 1.0216644e+00\n",
      "  1.0228508e+00 1.0223229e+00 1.0223229e+00 1.0223229e+00 1.0223229e+00\n",
      "  1.0223229e+00 1.0223229e+00 1.0223229e+00 1.0223229e+00 1.0223229e+00]]\n",
      "0.8232008\n"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"inv_HT_swish_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"lambda1\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
