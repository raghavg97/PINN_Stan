{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLsZ-c_nCQr2",
    "outputId": "0238c820-5951-4e75-a35b-19e4de8c9b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SV23gJi7JexL",
    "outputId": "6f051579-557f-463f-d7b4-955ed617736e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOyXTKXGJf97",
    "outputId": "11b7b7db-47b0-4cf8-c699-473f1c6b8c5f"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/MURI Aug17 Thin Plate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "APjvgycyCTj0",
    "outputId": "19bce659-211e-4bec-d94d-7c94148b0d09"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lxFUD2gACQr7"
   },
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CUcT7YuXCQr7"
   },
   "outputs": [],
   "source": [
    "label = \"3D_HTTP_atanh\"\n",
    "loss_thresh = 20000\n",
    "x = np.linspace(0,1,100).reshape(-1,1)\n",
    "y = np.linspace(0,1,100).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xyt = np.hstack((X,Y,T))\n",
    "\n",
    "initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "xyt_initial = xyt[initial_pts,:]\n",
    "xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "xyt_NBC_x0 = xyt[NBC_pts_x0,:]\n",
    "xyt_NBC_x1 = xyt[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xyt_NBC_y1 = xyt[NBC_pts_y1,:]\n",
    "\n",
    "u_initial = 300*np.ones((np.shape(xyt_initial)[0],1))\n",
    "u_DBC = 1000*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xyt_NBC_x = np.vstack((xyt_NBC_x0,xyt_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xyt_NBC_y = np.vstack((xyt_NBC_y1))\n",
    "\n",
    "u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../3D_HTTP_FEA.mat')\n",
    "xy = fea_data['xy']\n",
    "t = fea_data['t']/3000\n",
    "xyt = np.zeros((497*101,3))\n",
    "u_true = np.ones((497*101,1))\n",
    "\n",
    "\n",
    "for i in range(101):\n",
    "    t_temp = t[0,i]*np.ones((497,1))\n",
    "    xyt[497*i:497*(i+1)] = np.hstack((xy,t_temp))\n",
    "    u_true[497*i:497*(i+1)] = fea_data['u'][:,i].reshape(-1,1)\n",
    "    #print(i)\n",
    "#print(xyt)\n",
    "\n",
    "xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gp2G6x6BCQr8"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xyt_I_DBC.shape[0], N_D, replace=False) \n",
    "    xyt_D = xyt_I_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_I_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_x.shape[0], N_D, replace=False) \n",
    "    xyt_Nx = xyt_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_y.shape[0], N_D, replace=False) \n",
    "    xyt_Ny = xyt_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xyt_coll = lb_xyt + (ub_xyt - lb_xyt)*samples\n",
    "    xyt_coll = np.vstack((xyt_coll, xyt_D,xyt_Nx,xyt_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xyt_coll, xyt_D, u_D, xyt_Nx,xyt_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VRolFlBzCQr9"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "\n",
    "    \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/3000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "       \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xyt_coll_np_array, xyt_D_np_array, u_D_np_array,xyt_Nx_np_array,xyt_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "\n",
    "    xyt_coll = torch.from_numpy(xyt_coll_np_array).float().to(device)\n",
    "    xyt_D = torch.from_numpy(xyt_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xyt_Nx = torch.from_numpy(xyt_Nx_np_array).float().to(device)\n",
    "    xyt_Ny = torch.from_numpy(xyt_Ny_np_array).float().to(device)\n",
    "\n",
    "    N_hat = torch.zeros(xyt_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xyt_coll.shape[0],1).to(device)\n",
    "\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,i)\n",
    "\n",
    "        loss_np = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVnXJfj0CQr-",
    "outputId": "1f2921b0-e258-465d-aa27-cdeb80b78a0b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D_HTTP_atanh\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 894870.9 Test MSE 325713.2632569917 Test RE 1.0117124258788912\n",
      "1 Train Loss 528936.56 Test MSE 279647.29161598574 Test RE 0.9374425693316915\n",
      "2 Train Loss 403515.34 Test MSE 191058.0671917834 Test RE 0.7748578767582727\n",
      "3 Train Loss 244788.72 Test MSE 86939.00396886282 Test RE 0.5226930100802946\n",
      "4 Train Loss 234474.11 Test MSE 78744.80615752348 Test RE 0.49745101498990973\n",
      "5 Train Loss 221073.97 Test MSE 71411.37744185585 Test RE 0.4737214689445817\n",
      "6 Train Loss 207686.58 Test MSE 62850.170351116074 Test RE 0.44441897335280306\n",
      "7 Train Loss 198256.69 Test MSE 58057.34140417468 Test RE 0.42713772916609544\n",
      "8 Train Loss 192433.56 Test MSE 53774.136804125774 Test RE 0.4110797521973154\n",
      "9 Train Loss 186776.1 Test MSE 52468.17948251825 Test RE 0.40605733498443036\n",
      "10 Train Loss 183737.12 Test MSE 51169.54342110155 Test RE 0.4010007022001245\n",
      "11 Train Loss 180178.92 Test MSE 49589.64474933821 Test RE 0.3947615638397904\n",
      "12 Train Loss 175382.38 Test MSE 47385.990507862756 Test RE 0.3858907280698205\n",
      "13 Train Loss 171732.11 Test MSE 45032.9030543343 Test RE 0.3761874773951041\n",
      "14 Train Loss 168889.55 Test MSE 44252.875930394795 Test RE 0.37291522294870283\n",
      "15 Train Loss 167323.19 Test MSE 44533.28947054597 Test RE 0.3740948676910178\n",
      "16 Train Loss 166432.8 Test MSE 43631.832119320046 Test RE 0.3702892354371586\n",
      "17 Train Loss 165305.47 Test MSE 43047.060691914514 Test RE 0.36779948222153946\n",
      "18 Train Loss 164169.03 Test MSE 42494.50988764669 Test RE 0.36543132661829153\n",
      "19 Train Loss 162945.3 Test MSE 42210.43592227191 Test RE 0.3642078321107306\n",
      "20 Train Loss 161797.08 Test MSE 42406.81905498893 Test RE 0.3650540834600529\n",
      "21 Train Loss 161188.47 Test MSE 42210.595103970256 Test RE 0.3642085188504592\n",
      "22 Train Loss 160003.83 Test MSE 41418.46281074011 Test RE 0.36077492951366985\n",
      "23 Train Loss 158899.31 Test MSE 40878.61896777071 Test RE 0.35841606696570055\n",
      "24 Train Loss 158115.97 Test MSE 40707.319477325065 Test RE 0.3576643176904199\n",
      "25 Train Loss 157486.62 Test MSE 40352.462900499464 Test RE 0.35610197770916296\n",
      "26 Train Loss 156796.55 Test MSE 39937.68468650245 Test RE 0.35426708522285316\n",
      "27 Train Loss 155839.42 Test MSE 39262.179343550546 Test RE 0.3512582743897527\n",
      "28 Train Loss 155016.05 Test MSE 38870.31948827263 Test RE 0.3495009956832876\n",
      "29 Train Loss 154664.95 Test MSE 38523.33404643689 Test RE 0.34793754554596695\n",
      "30 Train Loss 154035.03 Test MSE 38225.72496087392 Test RE 0.3465909572470635\n",
      "31 Train Loss 153434.9 Test MSE 38157.90571433083 Test RE 0.34628336370089885\n",
      "32 Train Loss 152937.33 Test MSE 38091.3863946133 Test RE 0.34598140030783486\n",
      "33 Train Loss 152556.75 Test MSE 38010.79185111357 Test RE 0.3456151891931853\n",
      "34 Train Loss 152149.92 Test MSE 38093.63851915968 Test RE 0.34599162810158307\n",
      "35 Train Loss 151958.27 Test MSE 38047.5950160578 Test RE 0.3457824661003574\n",
      "36 Train Loss 151781.44 Test MSE 37895.937131074636 Test RE 0.34509263276116803\n",
      "37 Train Loss 151133.8 Test MSE 37592.56223088842 Test RE 0.34370854224913655\n",
      "38 Train Loss 150785.58 Test MSE 37510.525103515 Test RE 0.3433333049696559\n",
      "39 Train Loss 150687.7 Test MSE 37454.95041743546 Test RE 0.34307887353760097\n",
      "40 Train Loss 150353.83 Test MSE 37373.8977178491 Test RE 0.3427074602966796\n",
      "41 Train Loss 150087.62 Test MSE 37413.09974314174 Test RE 0.34288714861121006\n",
      "42 Train Loss 149943.5 Test MSE 37414.818564371686 Test RE 0.34289502492920615\n",
      "43 Train Loss 149636.0 Test MSE 37497.7356404328 Test RE 0.34327476909246124\n",
      "44 Train Loss 149452.28 Test MSE 37569.58507462831 Test RE 0.343603486201563\n",
      "45 Train Loss 149361.06 Test MSE 37546.63275303903 Test RE 0.343498511624548\n",
      "46 Train Loss 149015.7 Test MSE 37525.19405212449 Test RE 0.34340043074713383\n",
      "47 Train Loss 148808.38 Test MSE 37502.3245721683 Test RE 0.34329577324462063\n",
      "48 Train Loss 148662.27 Test MSE 37524.95734832212 Test RE 0.3433993476839021\n",
      "49 Train Loss 148510.75 Test MSE 37384.650240728944 Test RE 0.3427567554602604\n",
      "50 Train Loss 148216.38 Test MSE 37226.46693546549 Test RE 0.3420308442840618\n",
      "51 Train Loss 147952.92 Test MSE 37249.20352846738 Test RE 0.34213527843647373\n",
      "52 Train Loss 147828.39 Test MSE 37200.295872206 Test RE 0.3419105953671336\n",
      "53 Train Loss 147744.14 Test MSE 37236.32117903655 Test RE 0.3420761108982005\n",
      "54 Train Loss 147472.1 Test MSE 37045.44152480573 Test RE 0.34119821451061527\n",
      "55 Train Loss 147308.66 Test MSE 37014.25899327275 Test RE 0.34105458464027394\n",
      "56 Train Loss 147114.67 Test MSE 36795.67039276499 Test RE 0.3400460403962122\n",
      "57 Train Loss 146766.17 Test MSE 36624.35115144664 Test RE 0.33925349609300365\n",
      "58 Train Loss 146553.19 Test MSE 36524.34751367542 Test RE 0.33879000972844686\n",
      "59 Train Loss 146233.19 Test MSE 36398.799470612365 Test RE 0.33820723357605625\n",
      "60 Train Loss 145999.73 Test MSE 36363.75884360689 Test RE 0.33804440063560487\n",
      "61 Train Loss 145859.39 Test MSE 36144.11475600685 Test RE 0.3370219277608399\n",
      "62 Train Loss 145705.39 Test MSE 36081.42207024878 Test RE 0.3367295152893614\n",
      "63 Train Loss 145567.16 Test MSE 36032.47611372713 Test RE 0.33650104395230607\n",
      "64 Train Loss 145395.36 Test MSE 36185.98097522032 Test RE 0.3372170598101955\n",
      "65 Train Loss 145287.56 Test MSE 36231.86543748449 Test RE 0.3374307906685211\n",
      "66 Train Loss 145084.28 Test MSE 36090.48874756712 Test RE 0.3367718199701934\n",
      "67 Train Loss 144810.3 Test MSE 35996.16924967763 Test RE 0.3363314694724485\n",
      "68 Train Loss 144526.56 Test MSE 36091.350115908106 Test RE 0.33677583879707945\n",
      "69 Train Loss 144440.86 Test MSE 35986.40831478672 Test RE 0.3362858655619879\n",
      "70 Train Loss 144361.03 Test MSE 35984.64544868212 Test RE 0.33627762864363464\n",
      "71 Train Loss 144230.6 Test MSE 35927.66428213278 Test RE 0.3360112777761526\n",
      "72 Train Loss 144094.56 Test MSE 35822.71189695494 Test RE 0.3355201384587832\n",
      "73 Train Loss 143881.16 Test MSE 35711.51167619971 Test RE 0.33499897589590866\n",
      "74 Train Loss 143765.6 Test MSE 35736.07453147211 Test RE 0.3351141644798988\n",
      "75 Train Loss 143660.2 Test MSE 35667.92662276308 Test RE 0.33479448432822345\n",
      "76 Train Loss 143498.97 Test MSE 35654.61145080052 Test RE 0.33473198753250655\n",
      "77 Train Loss 143367.75 Test MSE 35654.31224993126 Test RE 0.33473058305342684\n",
      "78 Train Loss 143274.72 Test MSE 35574.59919024386 Test RE 0.3343561917449225\n",
      "79 Train Loss 143149.66 Test MSE 35487.964157377566 Test RE 0.33394881376063074\n",
      "80 Train Loss 143093.5 Test MSE 35408.37715294779 Test RE 0.3335741387919568\n",
      "81 Train Loss 142940.44 Test MSE 35436.434078993414 Test RE 0.3337062715306334\n",
      "82 Train Loss 142849.98 Test MSE 35394.37352455885 Test RE 0.3335081697952988\n",
      "83 Train Loss 142765.6 Test MSE 35364.04032590081 Test RE 0.33336522987380734\n",
      "84 Train Loss 142682.58 Test MSE 35336.71865225742 Test RE 0.3332364287554715\n",
      "85 Train Loss 142545.0 Test MSE 35300.7003749387 Test RE 0.3330665535852947\n",
      "86 Train Loss 142457.55 Test MSE 35170.39048316879 Test RE 0.3324512400947398\n",
      "87 Train Loss 142353.1 Test MSE 35146.08586812954 Test RE 0.332336349476151\n",
      "88 Train Loss 142312.0 Test MSE 35142.87492975068 Test RE 0.33232116804290246\n",
      "89 Train Loss 142200.48 Test MSE 35084.189835978425 Test RE 0.3320435805166068\n",
      "90 Train Loss 142127.88 Test MSE 35116.40913678084 Test RE 0.33219601038632557\n",
      "91 Train Loss 142075.73 Test MSE 35095.58034011607 Test RE 0.33209747711313126\n",
      "92 Train Loss 141995.5 Test MSE 34971.111912693515 Test RE 0.3315080529569124\n",
      "93 Train Loss 141888.4 Test MSE 34944.44461630108 Test RE 0.33138163276343086\n",
      "94 Train Loss 141812.88 Test MSE 34887.20101869902 Test RE 0.3311100981598448\n",
      "95 Train Loss 141754.58 Test MSE 34886.80971177601 Test RE 0.3311082412318997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 141687.5 Test MSE 34830.93812035957 Test RE 0.3308429983326183\n",
      "97 Train Loss 141633.4 Test MSE 34826.82821748274 Test RE 0.33082347872193535\n",
      "98 Train Loss 141588.86 Test MSE 34921.45484851346 Test RE 0.3312726077051339\n",
      "99 Train Loss 141508.8 Test MSE 34969.00208346655 Test RE 0.3314980527611742\n",
      "Training time: 210.94\n",
      "3D_HTTP_atanh\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 982198.44 Test MSE 307619.9221711107 Test RE 0.9832106886117778\n",
      "1 Train Loss 470018.12 Test MSE 228040.4622327914 Test RE 0.8465357792886279\n",
      "2 Train Loss 350447.5 Test MSE 138527.01996607214 Test RE 0.6597913355241275\n",
      "3 Train Loss 241873.47 Test MSE 87051.54279288951 Test RE 0.5230312025500714\n",
      "4 Train Loss 236705.95 Test MSE 83383.13604213801 Test RE 0.5118921559922444\n",
      "5 Train Loss 226874.27 Test MSE 75504.8054624989 Test RE 0.4871095662452625\n",
      "6 Train Loss 216452.28 Test MSE 71358.56593778166 Test RE 0.473546268778086\n",
      "7 Train Loss 210781.22 Test MSE 67770.7978045278 Test RE 0.4614882642718289\n",
      "8 Train Loss 207915.39 Test MSE 65884.25486181416 Test RE 0.45501968081710703\n",
      "9 Train Loss 204904.08 Test MSE 64225.64564665388 Test RE 0.44925570581842234\n",
      "10 Train Loss 198466.23 Test MSE 61089.47168546389 Test RE 0.43814972842348954\n",
      "11 Train Loss 195154.69 Test MSE 59118.59007263457 Test RE 0.4310239436723794\n",
      "12 Train Loss 191688.75 Test MSE 56611.09691241318 Test RE 0.4217840447231667\n",
      "13 Train Loss 189267.34 Test MSE 55430.72891090997 Test RE 0.4173636838224512\n",
      "14 Train Loss 184809.98 Test MSE 52607.80231421394 Test RE 0.4065972546841798\n",
      "15 Train Loss 180693.72 Test MSE 50501.501048810154 Test RE 0.3983744763296505\n",
      "16 Train Loss 178418.08 Test MSE 48284.24147771028 Test RE 0.38953103859529953\n",
      "17 Train Loss 175114.89 Test MSE 46341.2601045713 Test RE 0.381613106074408\n",
      "18 Train Loss 173077.84 Test MSE 46565.26371871353 Test RE 0.3825343117392509\n",
      "19 Train Loss 171805.23 Test MSE 45759.80230528388 Test RE 0.37921144118861966\n",
      "20 Train Loss 170567.52 Test MSE 45050.07890548398 Test RE 0.37625921076887936\n",
      "21 Train Loss 169285.23 Test MSE 43903.99872071803 Test RE 0.37144233495432455\n",
      "22 Train Loss 168068.66 Test MSE 43768.179748170965 Test RE 0.3708673532359937\n",
      "23 Train Loss 166816.52 Test MSE 43273.56643439974 Test RE 0.3687658593919206\n",
      "24 Train Loss 166074.1 Test MSE 43112.19252496472 Test RE 0.3680776243241392\n",
      "25 Train Loss 164652.53 Test MSE 42247.22777541789 Test RE 0.3643665246487311\n",
      "26 Train Loss 163969.47 Test MSE 41936.75596059592 Test RE 0.36302520419771256\n",
      "27 Train Loss 163165.3 Test MSE 41797.83080250107 Test RE 0.36242340310386134\n",
      "28 Train Loss 162291.23 Test MSE 41170.836951725476 Test RE 0.35969484197491247\n",
      "29 Train Loss 161556.17 Test MSE 40771.72263248251 Test RE 0.35794713669230216\n",
      "30 Train Loss 160116.94 Test MSE 40548.046448923495 Test RE 0.3569639263215626\n",
      "31 Train Loss 159520.97 Test MSE 40692.56755448386 Test RE 0.35759950484366715\n",
      "32 Train Loss 158862.36 Test MSE 40523.18438980968 Test RE 0.3568544732179183\n",
      "33 Train Loss 158536.55 Test MSE 40178.344656935435 Test RE 0.35533286875318965\n",
      "34 Train Loss 158251.44 Test MSE 40629.471868006396 Test RE 0.35732216010209206\n",
      "35 Train Loss 157786.16 Test MSE 40440.04032089128 Test RE 0.3564881944057269\n",
      "36 Train Loss 157007.45 Test MSE 40400.7587456602 Test RE 0.3563150143107485\n",
      "37 Train Loss 156405.28 Test MSE 39742.44244099267 Test RE 0.35340007648367355\n",
      "38 Train Loss 155871.4 Test MSE 39660.97321579555 Test RE 0.35303766795023933\n",
      "39 Train Loss 155238.81 Test MSE 39636.663006767114 Test RE 0.3529294540770113\n",
      "40 Train Loss 154488.64 Test MSE 39268.879345342204 Test RE 0.35128824382489027\n",
      "41 Train Loss 154167.44 Test MSE 38948.715696707004 Test RE 0.3498532664116581\n",
      "42 Train Loss 153671.69 Test MSE 38697.28132453724 Test RE 0.34872219494797674\n",
      "43 Train Loss 153424.69 Test MSE 38700.33936112916 Test RE 0.34873597348911506\n",
      "44 Train Loss 153133.92 Test MSE 38695.51040973253 Test RE 0.3487142155196791\n",
      "45 Train Loss 152836.77 Test MSE 38862.3642823926 Test RE 0.34946522938802754\n",
      "46 Train Loss 152135.62 Test MSE 39084.36200824687 Test RE 0.35046195218232395\n",
      "47 Train Loss 151784.39 Test MSE 38808.89072134579 Test RE 0.34922471927879756\n",
      "48 Train Loss 151494.92 Test MSE 38553.85419809423 Test RE 0.34807534520042704\n",
      "49 Train Loss 151062.73 Test MSE 38073.1923780297 Test RE 0.34589876294840494\n",
      "50 Train Loss 149592.19 Test MSE 38041.28592775866 Test RE 0.3457537959235027\n",
      "51 Train Loss 148748.08 Test MSE 38187.1909730338 Test RE 0.34641622021785234\n",
      "52 Train Loss 148345.4 Test MSE 38576.849823919125 Test RE 0.3481791353062702\n",
      "53 Train Loss 147942.92 Test MSE 38449.31396139122 Test RE 0.3476031151210673\n",
      "54 Train Loss 146180.97 Test MSE 37631.06633582658 Test RE 0.3438845185853471\n",
      "55 Train Loss 145472.33 Test MSE 37380.32104495567 Test RE 0.3427369090253166\n",
      "56 Train Loss 144489.6 Test MSE 37206.68518006073 Test RE 0.3419399564005214\n",
      "57 Train Loss 143842.55 Test MSE 36616.2799216987 Test RE 0.3392161118977866\n",
      "58 Train Loss 143422.61 Test MSE 36369.78043449066 Test RE 0.3380723884026009\n",
      "59 Train Loss 142572.98 Test MSE 35963.79997935628 Test RE 0.3361802137548388\n",
      "60 Train Loss 142418.48 Test MSE 36088.971558143014 Test RE 0.3367647412074598\n",
      "61 Train Loss 141961.31 Test MSE 36023.56316842401 Test RE 0.336459423153292\n",
      "62 Train Loss 141635.53 Test MSE 36025.811013360326 Test RE 0.33646992040489876\n",
      "63 Train Loss 141164.08 Test MSE 35984.62813436497 Test RE 0.3362775477422093\n",
      "64 Train Loss 140916.05 Test MSE 35987.5050074578 Test RE 0.3362909897109603\n",
      "65 Train Loss 140687.47 Test MSE 35979.48750917142 Test RE 0.3362535272275338\n",
      "66 Train Loss 140388.95 Test MSE 35898.10527476746 Test RE 0.33587302493169435\n",
      "67 Train Loss 140163.05 Test MSE 35607.27007875297 Test RE 0.33450968900477807\n",
      "68 Train Loss 139986.52 Test MSE 35442.242618454664 Test RE 0.3337336200304595\n",
      "69 Train Loss 139870.14 Test MSE 35364.72653710098 Test RE 0.3333684642022759\n",
      "70 Train Loss 139759.23 Test MSE 35375.13444897825 Test RE 0.3334175161064278\n",
      "71 Train Loss 139615.34 Test MSE 35348.92161840248 Test RE 0.33329396270318057\n",
      "72 Train Loss 139462.98 Test MSE 35485.540375257966 Test RE 0.33393740942955796\n",
      "73 Train Loss 139308.16 Test MSE 35445.59648954472 Test RE 0.33374941012977255\n",
      "74 Train Loss 139045.55 Test MSE 35275.76687092732 Test RE 0.3329489074325818\n",
      "75 Train Loss 138921.9 Test MSE 35091.352952755566 Test RE 0.3320774753501961\n",
      "76 Train Loss 138880.55 Test MSE 34992.02350742692 Test RE 0.3316071536953907\n",
      "77 Train Loss 138643.03 Test MSE 34624.737413292474 Test RE 0.32986224199508674\n",
      "78 Train Loss 138495.55 Test MSE 34566.600772431586 Test RE 0.32958519817162985\n",
      "79 Train Loss 138329.8 Test MSE 34439.23685460695 Test RE 0.32897744386749067\n",
      "80 Train Loss 137892.23 Test MSE 34150.10275991911 Test RE 0.32759357058424193\n",
      "81 Train Loss 137795.67 Test MSE 34190.32463479965 Test RE 0.3277864331093641\n",
      "82 Train Loss 137644.1 Test MSE 34280.25294068326 Test RE 0.32821722634404726\n",
      "83 Train Loss 137577.83 Test MSE 34360.00525367893 Test RE 0.3285987999013341\n",
      "84 Train Loss 137503.58 Test MSE 34265.51685876288 Test RE 0.32814667324985886\n",
      "85 Train Loss 137355.55 Test MSE 34223.80712925914 Test RE 0.32794689402760835\n",
      "86 Train Loss 137209.64 Test MSE 33995.41570833034 Test RE 0.3268507910045598\n",
      "87 Train Loss 137089.36 Test MSE 33950.52196211144 Test RE 0.3266349030124838\n",
      "88 Train Loss 136769.48 Test MSE 33951.65901560447 Test RE 0.3266403727111263\n",
      "89 Train Loss 136557.72 Test MSE 33843.28259468 Test RE 0.32611862486829374\n",
      "90 Train Loss 136482.92 Test MSE 33922.72792049206 Test RE 0.32650117365599607\n",
      "91 Train Loss 136398.1 Test MSE 33862.60426753623 Test RE 0.32621170476181605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Train Loss 136124.58 Test MSE 33725.78580308682 Test RE 0.3255520248675977\n",
      "93 Train Loss 135543.61 Test MSE 33560.13447764036 Test RE 0.32475153191972245\n",
      "94 Train Loss 135377.52 Test MSE 33535.554940623566 Test RE 0.3246325856682547\n",
      "95 Train Loss 135271.42 Test MSE 33472.85578907648 Test RE 0.3243289719314779\n",
      "96 Train Loss 135154.78 Test MSE 33424.303676718046 Test RE 0.3240936682707542\n",
      "97 Train Loss 134991.06 Test MSE 33260.61201052097 Test RE 0.3232990885850895\n",
      "98 Train Loss 134789.12 Test MSE 33108.88386148536 Test RE 0.3225608333276164\n",
      "99 Train Loss 134671.88 Test MSE 32873.637577386646 Test RE 0.32141285583640694\n",
      "Training time: 392.64\n",
      "3D_HTTP_atanh\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 1024737.1 Test MSE 297837.05881174863 Test RE 0.9674504466491505\n",
      "1 Train Loss 534420.2 Test MSE 267084.91303853074 Test RE 0.9161446302830595\n",
      "2 Train Loss 462221.97 Test MSE 229833.77524883987 Test RE 0.8498578448079811\n",
      "3 Train Loss 379625.34 Test MSE 162696.1997002079 Test RE 0.715036261327427\n",
      "4 Train Loss 247634.94 Test MSE 88440.727440095 Test RE 0.5271879988714573\n",
      "5 Train Loss 244890.92 Test MSE 86626.6469178462 Test RE 0.5217531916369598\n",
      "6 Train Loss 241741.7 Test MSE 84072.11878387458 Test RE 0.5140026505499568\n",
      "7 Train Loss 235411.53 Test MSE 78661.33868716292 Test RE 0.4971873024448949\n",
      "8 Train Loss 231257.55 Test MSE 76336.7050766676 Test RE 0.48978566206112645\n",
      "9 Train Loss 226265.88 Test MSE 72311.69121801422 Test RE 0.4766983203012551\n",
      "10 Train Loss 222817.72 Test MSE 70689.39397546287 Test RE 0.4713206754798852\n",
      "11 Train Loss 219779.42 Test MSE 68934.4852460183 Test RE 0.4654334912691652\n",
      "12 Train Loss 215575.83 Test MSE 66721.66843648237 Test RE 0.45790228534257454\n",
      "13 Train Loss 212945.9 Test MSE 65180.958947839536 Test RE 0.45258456118727347\n",
      "14 Train Loss 211307.14 Test MSE 64664.774341698016 Test RE 0.4507889329375296\n",
      "15 Train Loss 208630.5 Test MSE 63367.31692704832 Test RE 0.44624362164189924\n",
      "16 Train Loss 206185.69 Test MSE 61757.76517569767 Test RE 0.440539797789063\n",
      "17 Train Loss 204626.2 Test MSE 60995.738457178944 Test RE 0.43781345972440716\n",
      "18 Train Loss 201549.56 Test MSE 58643.05243500352 Test RE 0.42928691007588893\n",
      "19 Train Loss 200368.64 Test MSE 58243.71254958061 Test RE 0.4278227619447483\n",
      "20 Train Loss 197305.98 Test MSE 57167.329374112946 Test RE 0.4238510997172618\n",
      "21 Train Loss 195243.12 Test MSE 55731.05076098003 Test RE 0.41849278782186083\n",
      "22 Train Loss 194747.4 Test MSE 55659.06660506424 Test RE 0.41822243060143294\n",
      "23 Train Loss 194263.8 Test MSE 55193.59908095971 Test RE 0.4164699967393323\n",
      "24 Train Loss 193878.28 Test MSE 55057.43570337774 Test RE 0.41595596086920067\n",
      "25 Train Loss 192704.58 Test MSE 54778.26206145798 Test RE 0.4149000497321955\n",
      "26 Train Loss 192069.16 Test MSE 54676.02455713485 Test RE 0.4145126866177218\n",
      "27 Train Loss 191293.77 Test MSE 54282.86034777379 Test RE 0.4130196594088822\n",
      "28 Train Loss 191012.83 Test MSE 53852.96554040283 Test RE 0.41138094745796056\n",
      "29 Train Loss 190675.88 Test MSE 53256.63241426344 Test RE 0.4090969228035215\n",
      "30 Train Loss 190172.05 Test MSE 53272.657445009194 Test RE 0.4091584672288839\n",
      "31 Train Loss 189814.0 Test MSE 53146.879851910046 Test RE 0.4086751669599691\n",
      "32 Train Loss 189162.14 Test MSE 52140.07442247073 Test RE 0.4047857223038681\n",
      "33 Train Loss 188311.23 Test MSE 51614.67104229901 Test RE 0.4027410927484492\n",
      "34 Train Loss 187063.19 Test MSE 51098.33839272265 Test RE 0.40072159860920614\n",
      "35 Train Loss 186166.05 Test MSE 50965.71066124473 Test RE 0.4002012164478238\n",
      "36 Train Loss 185733.44 Test MSE 50438.13123714949 Test RE 0.39812445564333865\n",
      "37 Train Loss 185216.05 Test MSE 50130.37313022709 Test RE 0.396907980108848\n",
      "38 Train Loss 184564.88 Test MSE 50113.60586723373 Test RE 0.3968415970296001\n",
      "39 Train Loss 183966.02 Test MSE 49578.94729331662 Test RE 0.3947189826491302\n",
      "40 Train Loss 183122.67 Test MSE 48913.56595699915 Test RE 0.3920613445128738\n",
      "41 Train Loss 182394.94 Test MSE 48698.39843198091 Test RE 0.3911980681970045\n",
      "42 Train Loss 181510.28 Test MSE 48599.42021348927 Test RE 0.3908003160786074\n",
      "43 Train Loss 180572.58 Test MSE 47888.61597498794 Test RE 0.3879319103202285\n",
      "44 Train Loss 179990.95 Test MSE 47832.580671315176 Test RE 0.38770488094541516\n",
      "45 Train Loss 179333.11 Test MSE 47374.56328800771 Test RE 0.38584419613048787\n",
      "46 Train Loss 178499.16 Test MSE 47124.58987671808 Test RE 0.3848248900784889\n",
      "47 Train Loss 177337.39 Test MSE 46879.27692038933 Test RE 0.3838219560628525\n",
      "48 Train Loss 176932.11 Test MSE 46797.57191470082 Test RE 0.38348733218051906\n",
      "49 Train Loss 175970.8 Test MSE 46305.8224839567 Test RE 0.3814671665076574\n",
      "50 Train Loss 175284.58 Test MSE 45741.10228175171 Test RE 0.3791339497317439\n",
      "51 Train Loss 175064.39 Test MSE 45220.88869925927 Test RE 0.37697183941862156\n",
      "52 Train Loss 174228.3 Test MSE 44880.60872272736 Test RE 0.3755508346723958\n",
      "53 Train Loss 173863.72 Test MSE 44673.46171426087 Test RE 0.37468315253791745\n",
      "54 Train Loss 173075.61 Test MSE 44850.248782995615 Test RE 0.37542379061169034\n",
      "55 Train Loss 172971.11 Test MSE 44887.42314875798 Test RE 0.375579344382062\n",
      "56 Train Loss 172703.52 Test MSE 44879.752258891305 Test RE 0.37554725130697175\n",
      "57 Train Loss 172429.7 Test MSE 44776.6970977744 Test RE 0.3751158281873339\n",
      "58 Train Loss 171985.19 Test MSE 44658.52780499285 Test RE 0.37462052081321046\n",
      "59 Train Loss 171590.73 Test MSE 44286.47605959488 Test RE 0.3730567688017137\n",
      "60 Train Loss 171156.0 Test MSE 43594.9758414855 Test RE 0.37013280874660753\n",
      "61 Train Loss 170692.31 Test MSE 43463.62341666106 Test RE 0.36957477977797365\n",
      "62 Train Loss 170422.33 Test MSE 43478.13523798699 Test RE 0.3696364722365348\n",
      "63 Train Loss 170179.6 Test MSE 43389.64426200391 Test RE 0.36926012039485134\n",
      "64 Train Loss 170075.9 Test MSE 43436.24646230413 Test RE 0.36945836719149316\n",
      "65 Train Loss 169794.9 Test MSE 43085.64749670681 Test RE 0.367964290545198\n",
      "66 Train Loss 169543.9 Test MSE 43081.705560252165 Test RE 0.36794745750318364\n",
      "67 Train Loss 168907.11 Test MSE 42699.956925703445 Test RE 0.3663136318813182\n",
      "68 Train Loss 168771.84 Test MSE 42721.48587380849 Test RE 0.3664059663229923\n",
      "69 Train Loss 168511.83 Test MSE 42604.85262335939 Test RE 0.3659054649305196\n",
      "70 Train Loss 168256.39 Test MSE 42511.101859017734 Test RE 0.3655026609435323\n",
      "71 Train Loss 167908.62 Test MSE 42141.75372293809 Test RE 0.363911403298603\n",
      "72 Train Loss 167470.97 Test MSE 42134.26810293699 Test RE 0.3638790811570272\n",
      "73 Train Loss 166809.9 Test MSE 41919.623237127984 Test RE 0.3629510419782201\n",
      "74 Train Loss 166635.38 Test MSE 41974.910241935984 Test RE 0.36319030777414846\n",
      "75 Train Loss 166473.34 Test MSE 42013.959157871286 Test RE 0.3633592049898311\n",
      "76 Train Loss 166175.53 Test MSE 41774.30302029849 Test RE 0.3623213856184903\n",
      "77 Train Loss 165828.25 Test MSE 41623.12489124693 Test RE 0.36166518417597576\n",
      "78 Train Loss 165524.86 Test MSE 41531.38836886569 Test RE 0.3612664129715009\n",
      "79 Train Loss 165096.4 Test MSE 41322.66552990013 Test RE 0.3603574675615428\n",
      "80 Train Loss 164349.78 Test MSE 41000.26109552735 Test RE 0.3589489385299451\n",
      "81 Train Loss 164123.0 Test MSE 41001.55835884716 Test RE 0.35895461712315285\n",
      "82 Train Loss 163885.48 Test MSE 41022.92396252494 Test RE 0.35904812921822465\n",
      "83 Train Loss 163597.03 Test MSE 41131.91425829602 Test RE 0.3595247749612821\n",
      "84 Train Loss 163139.62 Test MSE 41122.94198224969 Test RE 0.35948556050317004\n",
      "85 Train Loss 162845.67 Test MSE 41140.869950943845 Test RE 0.35956391267464605\n",
      "86 Train Loss 162515.78 Test MSE 40815.149556661854 Test RE 0.35813771493072233\n",
      "87 Train Loss 162226.22 Test MSE 40623.53966732775 Test RE 0.35729607332269253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 Train Loss 161759.3 Test MSE 40373.90565270976 Test RE 0.3561965790271399\n",
      "89 Train Loss 161412.33 Test MSE 40237.60877599025 Test RE 0.35559483486805216\n",
      "90 Train Loss 161163.02 Test MSE 40090.9977428911 Test RE 0.35494641534466126\n",
      "91 Train Loss 161032.95 Test MSE 39944.841664876105 Test RE 0.3542988267760457\n",
      "92 Train Loss 160695.73 Test MSE 39731.88070247088 Test RE 0.3533531145081548\n",
      "93 Train Loss 160406.97 Test MSE 39604.153515003985 Test RE 0.35278469024105996\n",
      "94 Train Loss 160289.97 Test MSE 39638.88787613321 Test RE 0.35293935918572356\n",
      "95 Train Loss 160116.69 Test MSE 39432.69654156005 Test RE 0.35202021229473845\n",
      "96 Train Loss 159822.81 Test MSE 39206.499194806136 Test RE 0.35100911537260854\n",
      "97 Train Loss 159558.77 Test MSE 39297.79682673899 Test RE 0.3514175638101432\n",
      "98 Train Loss 159328.34 Test MSE 39242.27255202035 Test RE 0.3511692152505034\n",
      "99 Train Loss 159121.52 Test MSE 39229.04737477875 Test RE 0.35111003587360595\n",
      "Training time: 400.37\n",
      "3D_HTTP_atanh\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 760561.5 Test MSE 302370.84896632575 Test RE 0.9747860868268586\n",
      "1 Train Loss 520909.2 Test MSE 271575.876392941 Test RE 0.9238148882006183\n",
      "2 Train Loss 425997.53 Test MSE 185832.0555341484 Test RE 0.7641870566364902\n",
      "3 Train Loss 353651.3 Test MSE 153109.19820922462 Test RE 0.6936493814083663\n",
      "4 Train Loss 248876.19 Test MSE 86934.63843529295 Test RE 0.5226798867270028\n",
      "5 Train Loss 240318.6 Test MSE 82892.43380166595 Test RE 0.5103837138548981\n",
      "6 Train Loss 229682.8 Test MSE 75095.96894071222 Test RE 0.4857889980095799\n",
      "7 Train Loss 220578.3 Test MSE 71280.28689202057 Test RE 0.4732864616962601\n",
      "8 Train Loss 213057.31 Test MSE 67403.24506128639 Test RE 0.46023512949495726\n",
      "9 Train Loss 207915.25 Test MSE 63840.59174823849 Test RE 0.4479069632808875\n",
      "10 Train Loss 202274.23 Test MSE 60578.68736082641 Test RE 0.4363141437891151\n",
      "11 Train Loss 195474.88 Test MSE 56980.763836708706 Test RE 0.4231589156439665\n",
      "12 Train Loss 191384.12 Test MSE 54928.18098697235 Test RE 0.4154674177607641\n",
      "13 Train Loss 187662.78 Test MSE 53540.283481461614 Test RE 0.4101849251038805\n",
      "14 Train Loss 184964.92 Test MSE 51562.17668335352 Test RE 0.4025362380697006\n",
      "15 Train Loss 181945.34 Test MSE 50679.266907184065 Test RE 0.39907500175181626\n",
      "16 Train Loss 178992.14 Test MSE 48745.014470392285 Test RE 0.39138525856432155\n",
      "17 Train Loss 176857.89 Test MSE 48068.60315880998 Test RE 0.3886602388301023\n",
      "18 Train Loss 174596.38 Test MSE 47256.826162515295 Test RE 0.38536444029221384\n",
      "19 Train Loss 173275.66 Test MSE 46432.48885349466 Test RE 0.38198854870372917\n",
      "20 Train Loss 170965.69 Test MSE 46295.640237732434 Test RE 0.38142522355150227\n",
      "21 Train Loss 168915.22 Test MSE 45760.23060863575 Test RE 0.3792132158592158\n",
      "22 Train Loss 167799.19 Test MSE 45111.47712250979 Test RE 0.3765155230702319\n",
      "23 Train Loss 166299.92 Test MSE 44122.73469276275 Test RE 0.3723664746322486\n",
      "24 Train Loss 164882.39 Test MSE 43244.57760497166 Test RE 0.3686423211399064\n",
      "25 Train Loss 163242.2 Test MSE 42412.55123657448 Test RE 0.36507875502977916\n",
      "26 Train Loss 162232.9 Test MSE 41456.533973822305 Test RE 0.3609407005904016\n",
      "27 Train Loss 161087.4 Test MSE 41441.791841106024 Test RE 0.3608765188037086\n",
      "28 Train Loss 159910.3 Test MSE 41107.71462504127 Test RE 0.359418997635676\n",
      "29 Train Loss 159420.75 Test MSE 40966.50460894454 Test RE 0.3588011425189653\n",
      "30 Train Loss 158829.64 Test MSE 40557.50290545648 Test RE 0.35700554875913726\n",
      "31 Train Loss 158214.64 Test MSE 39848.70794413433 Test RE 0.3538722312444087\n",
      "32 Train Loss 157131.84 Test MSE 39486.76742941125 Test RE 0.352261478129879\n",
      "33 Train Loss 156149.34 Test MSE 39161.075920152856 Test RE 0.35080572302066465\n",
      "34 Train Loss 155511.05 Test MSE 39353.170259212486 Test RE 0.351665063012165\n",
      "35 Train Loss 154065.2 Test MSE 38567.107555152594 Test RE 0.34813516762984625\n",
      "36 Train Loss 153303.66 Test MSE 38219.501505111286 Test RE 0.3465627421995598\n",
      "37 Train Loss 152958.23 Test MSE 38132.75159543078 Test RE 0.3461692079483631\n",
      "38 Train Loss 152184.52 Test MSE 38048.56033513718 Test RE 0.3457868525575638\n",
      "39 Train Loss 151620.47 Test MSE 38179.04556038811 Test RE 0.346379272572924\n",
      "40 Train Loss 150889.8 Test MSE 37734.22245377986 Test RE 0.34435553253809953\n",
      "41 Train Loss 150001.05 Test MSE 37439.4778780598 Test RE 0.34300800373741275\n",
      "42 Train Loss 149436.72 Test MSE 37310.661249410645 Test RE 0.3424174078048234\n",
      "43 Train Loss 148792.38 Test MSE 36996.11091427814 Test RE 0.3409709649140709\n",
      "44 Train Loss 148005.66 Test MSE 36656.938551034524 Test RE 0.3394043920240569\n",
      "45 Train Loss 146714.86 Test MSE 36253.445140859585 Test RE 0.3375312626200513\n",
      "46 Train Loss 145836.33 Test MSE 36071.27179893757 Test RE 0.3366821483044514\n",
      "47 Train Loss 145036.06 Test MSE 35908.538030247524 Test RE 0.3359218273205871\n",
      "48 Train Loss 144156.14 Test MSE 35605.24646624987 Test RE 0.3345001835357347\n",
      "49 Train Loss 143522.03 Test MSE 35512.35838251041 Test RE 0.33406357130264414\n",
      "50 Train Loss 142376.31 Test MSE 34882.134166908814 Test RE 0.3310860528557817\n",
      "51 Train Loss 141454.56 Test MSE 34822.87433456671 Test RE 0.3308046990268683\n",
      "52 Train Loss 140516.58 Test MSE 34557.165497088456 Test RE 0.329540213428258\n",
      "53 Train Loss 139389.81 Test MSE 34191.258579990754 Test RE 0.3277909099968406\n",
      "54 Train Loss 138494.97 Test MSE 33934.90748607872 Test RE 0.32655978164280214\n",
      "55 Train Loss 137258.97 Test MSE 33608.37103174755 Test RE 0.3249848335776417\n",
      "56 Train Loss 136423.6 Test MSE 33642.295478072636 Test RE 0.32514881281040436\n",
      "57 Train Loss 136102.14 Test MSE 33469.14615161861 Test RE 0.32431099951455855\n",
      "58 Train Loss 135275.44 Test MSE 33251.202447667405 Test RE 0.3232533540345668\n",
      "59 Train Loss 134621.83 Test MSE 32978.078811965104 Test RE 0.32192302352395047\n",
      "60 Train Loss 134027.67 Test MSE 32784.60862570963 Test RE 0.32097733284682706\n",
      "61 Train Loss 133270.72 Test MSE 32802.08294953051 Test RE 0.32106286253440364\n",
      "62 Train Loss 132566.6 Test MSE 32787.007163661154 Test RE 0.32098907406116617\n",
      "63 Train Loss 131654.08 Test MSE 32575.889729342136 Test RE 0.3199539714978526\n",
      "64 Train Loss 130747.805 Test MSE 32266.02980645501 Test RE 0.3184286440649974\n",
      "65 Train Loss 130275.27 Test MSE 32213.889238165306 Test RE 0.3181712562883417\n",
      "66 Train Loss 129758.13 Test MSE 32032.399323261532 Test RE 0.31727371740426735\n",
      "67 Train Loss 129366.445 Test MSE 31758.825398119057 Test RE 0.31591596808322336\n",
      "68 Train Loss 129109.16 Test MSE 31474.927130779153 Test RE 0.3145007814333685\n",
      "69 Train Loss 128861.09 Test MSE 31227.203850832888 Test RE 0.3132606980651452\n",
      "70 Train Loss 127956.17 Test MSE 30967.69922234446 Test RE 0.31195635143628975\n",
      "71 Train Loss 127802.6 Test MSE 30810.088972189125 Test RE 0.3111614871242884\n",
      "72 Train Loss 127620.875 Test MSE 30587.947459447354 Test RE 0.3100377168012125\n",
      "73 Train Loss 127466.94 Test MSE 30647.528150305723 Test RE 0.3103395231718685\n",
      "74 Train Loss 127168.14 Test MSE 30483.589755557674 Test RE 0.30950838301843975\n",
      "75 Train Loss 127045.19 Test MSE 30351.88248458982 Test RE 0.3088390289182067\n",
      "76 Train Loss 126905.52 Test MSE 30318.745532738674 Test RE 0.3086703939278727\n",
      "77 Train Loss 126705.66 Test MSE 30096.546570325325 Test RE 0.30753722753591334\n",
      "78 Train Loss 126538.59 Test MSE 30038.149656033027 Test RE 0.307238722436724\n",
      "79 Train Loss 126377.016 Test MSE 30011.123362558294 Test RE 0.3071004750321344\n",
      "80 Train Loss 126103.484 Test MSE 29908.337663749197 Test RE 0.3065741266796925\n",
      "81 Train Loss 125891.46 Test MSE 29744.14881232794 Test RE 0.30573146321443\n",
      "82 Train Loss 125756.43 Test MSE 29721.397081413048 Test RE 0.3056145116299827\n",
      "83 Train Loss 125437.984 Test MSE 29741.32321745257 Test RE 0.30571694113517345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 Train Loss 125243.28 Test MSE 29662.142762256484 Test RE 0.3053097141376134\n",
      "85 Train Loss 125086.7 Test MSE 29583.613446746283 Test RE 0.3049052987719247\n",
      "86 Train Loss 124769.42 Test MSE 29414.0865040884 Test RE 0.30403042378108824\n",
      "87 Train Loss 124522.25 Test MSE 29142.77750050885 Test RE 0.30262502092738824\n",
      "88 Train Loss 124230.42 Test MSE 28950.005853016704 Test RE 0.30162246864815057\n",
      "89 Train Loss 124051.984 Test MSE 28817.969832883507 Test RE 0.30093385837327397\n",
      "90 Train Loss 123950.79 Test MSE 28777.824131390873 Test RE 0.30072417303862564\n",
      "91 Train Loss 123685.65 Test MSE 28920.205375316054 Test RE 0.3014671870218589\n",
      "92 Train Loss 123555.375 Test MSE 28930.577022189806 Test RE 0.30152123973145345\n",
      "93 Train Loss 123426.79 Test MSE 28786.245731456558 Test RE 0.3007681720818861\n",
      "94 Train Loss 123325.62 Test MSE 28800.201007072064 Test RE 0.30084106790508414\n",
      "95 Train Loss 123215.07 Test MSE 28732.43407193053 Test RE 0.3004869196133591\n",
      "96 Train Loss 123138.28 Test MSE 28724.38774674903 Test RE 0.30044484199545013\n",
      "97 Train Loss 122972.46 Test MSE 28607.690536884547 Test RE 0.29983391935628473\n",
      "98 Train Loss 122778.39 Test MSE 28546.300056268454 Test RE 0.2995120332874468\n",
      "99 Train Loss 122675.42 Test MSE 28560.463320245457 Test RE 0.2995863256114559\n",
      "Training time: 400.13\n",
      "3D_HTTP_atanh\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 883872.6 Test MSE 304308.84531826206 Test RE 0.9779049630815114\n",
      "1 Train Loss 546748.8 Test MSE 280134.14261907095 Test RE 0.9382582332087345\n",
      "2 Train Loss 341934.2 Test MSE 140027.38188125132 Test RE 0.6633547550717007\n",
      "3 Train Loss 245041.66 Test MSE 87040.11811422059 Test RE 0.5229968800136772\n",
      "4 Train Loss 244260.78 Test MSE 86489.05054681642 Test RE 0.5213386547766775\n",
      "5 Train Loss 242924.69 Test MSE 85387.77906732094 Test RE 0.5180088994827136\n",
      "6 Train Loss 240704.12 Test MSE 83685.99617763102 Test RE 0.5128209481646998\n",
      "7 Train Loss 239130.25 Test MSE 82590.00832022844 Test RE 0.5094518188890363\n",
      "8 Train Loss 234834.22 Test MSE 79591.90690051323 Test RE 0.5001195331848016\n",
      "9 Train Loss 232909.67 Test MSE 78247.46682530908 Test RE 0.49587761704931593\n",
      "10 Train Loss 230783.31 Test MSE 77612.1061687879 Test RE 0.4938602782379664\n",
      "11 Train Loss 228317.34 Test MSE 75036.83075154494 Test RE 0.4855976805509786\n",
      "12 Train Loss 224781.67 Test MSE 74078.57972275952 Test RE 0.4824870771945679\n",
      "13 Train Loss 221711.14 Test MSE 70860.39941969878 Test RE 0.4718904194805124\n",
      "14 Train Loss 219143.73 Test MSE 69908.48266934982 Test RE 0.4687100872909901\n",
      "15 Train Loss 216849.05 Test MSE 68889.33473484317 Test RE 0.465281042156684\n",
      "16 Train Loss 215009.62 Test MSE 67845.24123140916 Test RE 0.4617416576198833\n",
      "17 Train Loss 212971.1 Test MSE 66355.22053096154 Test RE 0.4566431114804969\n",
      "18 Train Loss 211658.83 Test MSE 65993.60315411605 Test RE 0.45539712302534335\n",
      "19 Train Loss 210708.2 Test MSE 65273.449435664996 Test RE 0.45290555161533347\n",
      "20 Train Loss 209649.89 Test MSE 64670.6188321296 Test RE 0.4508093039363023\n",
      "21 Train Loss 208024.77 Test MSE 63658.85787300383 Test RE 0.44726898454008485\n",
      "22 Train Loss 206890.14 Test MSE 62816.96972396655 Test RE 0.4443015755983739\n",
      "23 Train Loss 205909.9 Test MSE 62817.877069347116 Test RE 0.444304784393133\n",
      "24 Train Loss 204849.42 Test MSE 62015.5211372816 Test RE 0.4414581723760463\n",
      "25 Train Loss 202847.98 Test MSE 60998.25502278355 Test RE 0.4378224912974128\n",
      "26 Train Loss 202064.25 Test MSE 60444.919812057626 Test RE 0.4358321514306127\n",
      "27 Train Loss 201054.53 Test MSE 59994.82013702569 Test RE 0.43420641956855954\n",
      "28 Train Loss 199658.84 Test MSE 59448.57795498059 Test RE 0.4322252134575178\n",
      "29 Train Loss 198502.94 Test MSE 58606.73132083844 Test RE 0.429153948085843\n",
      "30 Train Loss 197375.73 Test MSE 58154.429297418625 Test RE 0.42749472604580785\n",
      "31 Train Loss 196645.44 Test MSE 57374.20990333897 Test RE 0.42461733589347306\n",
      "32 Train Loss 195388.05 Test MSE 56326.78736899498 Test RE 0.4207235797646312\n",
      "33 Train Loss 194404.14 Test MSE 55994.05237634401 Test RE 0.4194790847920614\n",
      "34 Train Loss 193288.2 Test MSE 55513.70893168426 Test RE 0.41767596453884653\n",
      "35 Train Loss 192632.06 Test MSE 55189.70950488402 Test RE 0.4164553218467488\n",
      "36 Train Loss 191957.86 Test MSE 54425.91593247504 Test RE 0.4135635317481474\n",
      "37 Train Loss 191083.58 Test MSE 54171.39725703775 Test RE 0.41259539929897593\n",
      "38 Train Loss 190265.56 Test MSE 53647.31030155974 Test RE 0.4105946994630853\n",
      "39 Train Loss 188931.95 Test MSE 52718.61411858173 Test RE 0.40702525274071755\n",
      "40 Train Loss 188475.02 Test MSE 52562.04490978071 Test RE 0.40642039040254363\n",
      "41 Train Loss 187624.44 Test MSE 51892.22064032166 Test RE 0.4038224787016048\n",
      "42 Train Loss 187172.25 Test MSE 51666.827094622706 Test RE 0.40294452407266707\n",
      "43 Train Loss 186656.94 Test MSE 51284.6158413439 Test RE 0.4014513433883425\n",
      "44 Train Loss 186345.06 Test MSE 51060.378989867975 Test RE 0.40057272900700125\n",
      "45 Train Loss 185995.84 Test MSE 50596.535082359245 Test RE 0.3987491319182389\n",
      "46 Train Loss 185297.97 Test MSE 50436.99411303902 Test RE 0.39811996777416087\n",
      "47 Train Loss 184966.9 Test MSE 50340.29618750209 Test RE 0.3977381464027659\n",
      "48 Train Loss 184390.94 Test MSE 50245.65126346786 Test RE 0.3973640762268436\n",
      "49 Train Loss 184047.05 Test MSE 50037.98417000868 Test RE 0.39654206595039065\n",
      "50 Train Loss 183460.61 Test MSE 49943.41037119428 Test RE 0.39616714850260043\n",
      "51 Train Loss 183280.89 Test MSE 49888.544761271936 Test RE 0.3959494828997607\n",
      "52 Train Loss 183155.17 Test MSE 49758.45236146152 Test RE 0.3954328949465706\n",
      "53 Train Loss 182761.83 Test MSE 49610.70334416062 Test RE 0.39484537409389636\n",
      "54 Train Loss 182587.5 Test MSE 49541.731645212065 Test RE 0.3945708100754135\n",
      "55 Train Loss 182314.92 Test MSE 49494.01544760437 Test RE 0.3943807485483183\n",
      "56 Train Loss 181480.1 Test MSE 49150.23826392929 Test RE 0.39300871042962543\n",
      "57 Train Loss 181188.67 Test MSE 48967.73430823584 Test RE 0.3922783746940051\n",
      "58 Train Loss 181121.56 Test MSE 48907.948821987746 Test RE 0.3920388321005068\n",
      "59 Train Loss 180607.0 Test MSE 48693.53706054391 Test RE 0.39117854181995876\n",
      "60 Train Loss 180227.38 Test MSE 48584.23660744707 Test RE 0.3907392636879784\n",
      "61 Train Loss 179978.03 Test MSE 48578.828583735325 Test RE 0.390717516037318\n",
      "62 Train Loss 179539.88 Test MSE 48007.27730486777 Test RE 0.3884122336201991\n",
      "63 Train Loss 179265.83 Test MSE 47857.09169173162 Test RE 0.3878042047223409\n",
      "64 Train Loss 178797.19 Test MSE 47593.96190593887 Test RE 0.38673661489765077\n",
      "65 Train Loss 178161.83 Test MSE 47150.10034619103 Test RE 0.38492903671829376\n",
      "66 Train Loss 177926.14 Test MSE 46798.92878635045 Test RE 0.38349289164969685\n",
      "67 Train Loss 177756.25 Test MSE 46374.3524061129 Test RE 0.3817493367493932\n",
      "68 Train Loss 177623.55 Test MSE 46061.17958695767 Test RE 0.3804581484650933\n",
      "69 Train Loss 177381.36 Test MSE 46179.5773177316 Test RE 0.38094680802950615\n",
      "70 Train Loss 177117.17 Test MSE 46411.8757142182 Test RE 0.38190374970377566\n",
      "71 Train Loss 176706.1 Test MSE 46207.998796543594 Test RE 0.3810640179183651\n",
      "72 Train Loss 176415.2 Test MSE 45877.88861504104 Test RE 0.37970041650548636\n",
      "73 Train Loss 175955.17 Test MSE 45610.80469438918 Test RE 0.37859356628110197\n",
      "74 Train Loss 175709.64 Test MSE 45526.3135510759 Test RE 0.3782427433668263\n",
      "75 Train Loss 175611.02 Test MSE 45470.622101472385 Test RE 0.3780113240946976\n",
      "76 Train Loss 175494.23 Test MSE 45552.8655555791 Test RE 0.3783530272699804\n",
      "77 Train Loss 175184.94 Test MSE 45736.717897994036 Test RE 0.3791157788910428\n",
      "78 Train Loss 174408.0 Test MSE 45578.80802029072 Test RE 0.37846074840740795\n",
      "79 Train Loss 174131.22 Test MSE 45344.773545096425 Test RE 0.377487852684868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 Train Loss 174016.28 Test MSE 45324.599572705876 Test RE 0.37740387082609494\n",
      "81 Train Loss 173930.58 Test MSE 45344.08933344636 Test RE 0.3774850046988634\n",
      "82 Train Loss 173739.03 Test MSE 45150.44718916798 Test RE 0.3766781165886822\n",
      "83 Train Loss 173479.5 Test MSE 44823.05445896146 Test RE 0.3753099568589267\n",
      "84 Train Loss 173236.52 Test MSE 44936.48474232702 Test RE 0.37578454093372976\n",
      "85 Train Loss 173054.94 Test MSE 44766.98602865766 Test RE 0.3750751488440472\n",
      "86 Train Loss 172945.56 Test MSE 44729.978941279325 Test RE 0.374920086935414\n",
      "87 Train Loss 172877.2 Test MSE 44665.59102297495 Test RE 0.37465014473850683\n",
      "88 Train Loss 172775.67 Test MSE 44664.21451509769 Test RE 0.374644371694459\n",
      "89 Train Loss 172692.16 Test MSE 44564.5197958004 Test RE 0.3742260174103355\n",
      "90 Train Loss 172510.61 Test MSE 44379.81061824299 Test RE 0.37344967388022465\n",
      "91 Train Loss 172409.9 Test MSE 44278.80976937531 Test RE 0.3730244780722078\n",
      "92 Train Loss 172181.61 Test MSE 44326.96000451846 Test RE 0.3732272425116259\n",
      "93 Train Loss 171989.62 Test MSE 44125.29225996326 Test RE 0.37237726655718684\n",
      "94 Train Loss 171767.14 Test MSE 44074.1494275223 Test RE 0.37216140453485613\n",
      "95 Train Loss 171661.86 Test MSE 44014.18628902094 Test RE 0.3719081544829187\n",
      "96 Train Loss 171472.11 Test MSE 43808.06545239302 Test RE 0.37103629945605177\n",
      "97 Train Loss 171315.95 Test MSE 43643.575475224716 Test RE 0.3703390631147487\n",
      "98 Train Loss 171058.08 Test MSE 43513.04553009984 Test RE 0.3697848402367406\n",
      "99 Train Loss 170848.08 Test MSE 43410.761619176774 Test RE 0.3693499672861504\n",
      "Training time: 392.84\n",
      "3D_HTTP_atanh\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 721999.3 Test MSE 308535.35604491516 Test RE 0.984672550608817\n",
      "1 Train Loss 511428.4 Test MSE 277495.94642478885 Test RE 0.9338297034320739\n",
      "2 Train Loss 477856.47 Test MSE 254086.80138902698 Test RE 0.8935737766849218\n",
      "3 Train Loss 447412.62 Test MSE 230175.66720229166 Test RE 0.85048971795711\n",
      "4 Train Loss 425388.88 Test MSE 215331.00942989596 Test RE 0.8226074726166893\n",
      "5 Train Loss 385985.72 Test MSE 181751.71871457697 Test RE 0.7557508171131201\n",
      "6 Train Loss 316901.34 Test MSE 130615.349322837 Test RE 0.6406730715542113\n",
      "7 Train Loss 243817.06 Test MSE 87647.36163517462 Test RE 0.5248180774089881\n",
      "8 Train Loss 236841.44 Test MSE 81539.56966005232 Test RE 0.506201665492699\n",
      "9 Train Loss 216946.98 Test MSE 68914.68690585041 Test RE 0.4653666490182721\n",
      "10 Train Loss 208334.84 Test MSE 62612.26332988536 Test RE 0.44357704532948794\n",
      "11 Train Loss 196680.4 Test MSE 57521.217008920816 Test RE 0.42516097589937996\n",
      "12 Train Loss 193190.75 Test MSE 55597.379860896515 Test RE 0.4179906091409456\n",
      "13 Train Loss 188927.77 Test MSE 53370.12152465987 Test RE 0.4095325806331444\n",
      "14 Train Loss 184229.98 Test MSE 51373.83551362154 Test RE 0.40180039341442797\n",
      "15 Train Loss 182134.05 Test MSE 50654.412742064924 Test RE 0.39897713241615573\n",
      "16 Train Loss 177518.33 Test MSE 48394.92945548234 Test RE 0.38997726825150764\n",
      "17 Train Loss 175000.66 Test MSE 47968.2916043986 Test RE 0.38825449088483915\n",
      "18 Train Loss 172912.69 Test MSE 46434.87285954804 Test RE 0.3819983548905831\n",
      "19 Train Loss 170922.1 Test MSE 45780.96640819312 Test RE 0.3792991245023506\n",
      "20 Train Loss 167915.67 Test MSE 44464.914796396675 Test RE 0.3738075720451141\n",
      "21 Train Loss 165029.47 Test MSE 43269.72473630444 Test RE 0.3687494900623266\n",
      "22 Train Loss 161671.69 Test MSE 41453.26536221438 Test RE 0.3609264712505826\n",
      "23 Train Loss 157812.11 Test MSE 39831.11529017713 Test RE 0.35379410777057413\n",
      "24 Train Loss 156459.88 Test MSE 39358.684191251494 Test RE 0.35168969875723505\n",
      "25 Train Loss 155269.64 Test MSE 38866.23733380425 Test RE 0.3494826429332924\n",
      "26 Train Loss 152709.44 Test MSE 38712.76178196866 Test RE 0.3487919393734984\n",
      "27 Train Loss 151549.56 Test MSE 38128.64156246743 Test RE 0.34615055200123956\n",
      "28 Train Loss 148936.1 Test MSE 38462.50644327353 Test RE 0.3476627436818324\n",
      "29 Train Loss 147557.38 Test MSE 38103.67069654345 Test RE 0.3460371845385796\n",
      "30 Train Loss 145824.94 Test MSE 37886.76842395756 Test RE 0.3450508836328717\n",
      "31 Train Loss 142097.7 Test MSE 36471.2756310325 Test RE 0.3385437800053433\n",
      "32 Train Loss 140106.25 Test MSE 35764.872806873034 Test RE 0.33524916484897266\n",
      "33 Train Loss 137445.31 Test MSE 34905.48545932359 Test RE 0.3311968544732178\n",
      "34 Train Loss 135844.61 Test MSE 34841.90685171879 Test RE 0.33089508768926523\n",
      "35 Train Loss 133167.34 Test MSE 33503.7069634336 Test RE 0.3244784008482584\n",
      "36 Train Loss 131781.06 Test MSE 33349.144666585125 Test RE 0.32372907925314914\n",
      "37 Train Loss 129803.24 Test MSE 32698.375401538513 Test RE 0.32055492220405596\n",
      "38 Train Loss 128447.83 Test MSE 32055.54214611445 Test RE 0.3173883089404381\n",
      "39 Train Loss 126361.39 Test MSE 31436.84238499514 Test RE 0.3143104504484769\n",
      "40 Train Loss 124786.766 Test MSE 30687.618478934437 Test RE 0.31054243590466146\n",
      "41 Train Loss 123341.984 Test MSE 30682.24944842879 Test RE 0.31051526884465663\n",
      "42 Train Loss 121393.57 Test MSE 30705.73139588793 Test RE 0.3106340689456472\n",
      "43 Train Loss 119945.56 Test MSE 30274.6749851037 Test RE 0.3084459746783632\n",
      "44 Train Loss 119120.32 Test MSE 30326.840977630698 Test RE 0.3087116004049002\n",
      "45 Train Loss 118623.25 Test MSE 30224.545427027966 Test RE 0.30819050263098696\n",
      "46 Train Loss 117567.805 Test MSE 29876.33873459462 Test RE 0.3064100809674811\n",
      "47 Train Loss 116871.3 Test MSE 29506.501458218343 Test RE 0.3045076598000507\n",
      "48 Train Loss 115274.87 Test MSE 28833.683420857098 Test RE 0.30101589236541254\n",
      "49 Train Loss 114357.93 Test MSE 28283.535605876823 Test RE 0.2981303645416444\n",
      "50 Train Loss 113966.97 Test MSE 28121.81258571274 Test RE 0.29727679971303367\n",
      "51 Train Loss 113678.42 Test MSE 28032.14620403605 Test RE 0.2968024878813173\n",
      "52 Train Loss 112463.7 Test MSE 27556.999505746237 Test RE 0.29427632676237947\n",
      "53 Train Loss 111867.125 Test MSE 27518.118160055714 Test RE 0.29406864997251964\n",
      "54 Train Loss 111396.6 Test MSE 27203.743891270427 Test RE 0.29238406580419923\n",
      "55 Train Loss 111044.12 Test MSE 27297.129166460447 Test RE 0.29288548527822256\n",
      "56 Train Loss 109952.766 Test MSE 26925.06897357905 Test RE 0.2908826207848233\n",
      "57 Train Loss 109090.38 Test MSE 26378.961528869968 Test RE 0.28791759731549327\n",
      "58 Train Loss 108388.65 Test MSE 25846.337160910967 Test RE 0.2849960657926992\n",
      "59 Train Loss 107717.05 Test MSE 25556.80896481983 Test RE 0.283395320988892\n",
      "60 Train Loss 107174.8 Test MSE 25182.291947462265 Test RE 0.28131117808282846\n",
      "61 Train Loss 106771.01 Test MSE 25007.78202471919 Test RE 0.28033475906633304\n",
      "62 Train Loss 106250.52 Test MSE 24960.932686471922 Test RE 0.28007204774816097\n",
      "63 Train Loss 105853.44 Test MSE 24972.132098186663 Test RE 0.28013487173105345\n",
      "64 Train Loss 104880.87 Test MSE 24940.715046576188 Test RE 0.2799585996062881\n",
      "65 Train Loss 103407.164 Test MSE 24623.740594462655 Test RE 0.2781738977550791\n",
      "66 Train Loss 102860.484 Test MSE 24627.637881134982 Test RE 0.2781959106683189\n",
      "67 Train Loss 102181.586 Test MSE 24571.76642520553 Test RE 0.2778801671087404\n",
      "68 Train Loss 101687.09 Test MSE 24336.074683127186 Test RE 0.27654424620575146\n",
      "69 Train Loss 100575.45 Test MSE 24120.557214069493 Test RE 0.2753170011123241\n",
      "70 Train Loss 100027.88 Test MSE 23960.296499134918 Test RE 0.2744008524366039\n",
      "71 Train Loss 99648.89 Test MSE 23556.881303017013 Test RE 0.27208102750508195\n",
      "72 Train Loss 99113.375 Test MSE 23411.49506487152 Test RE 0.27124012545716797\n",
      "73 Train Loss 98432.88 Test MSE 23244.96668558421 Test RE 0.27027372436720537\n",
      "74 Train Loss 96749.77 Test MSE 22963.54922628302 Test RE 0.2686326948019116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 Train Loss 96080.98 Test MSE 22760.064976440324 Test RE 0.2674398444496468\n",
      "76 Train Loss 95453.11 Test MSE 22468.246563889224 Test RE 0.26571982195186694\n",
      "77 Train Loss 94606.39 Test MSE 22115.263941933932 Test RE 0.263624291572978\n",
      "78 Train Loss 93852.84 Test MSE 21631.899454363447 Test RE 0.2607274097219589\n",
      "79 Train Loss 93615.14 Test MSE 21594.53787962715 Test RE 0.2605021545059758\n",
      "80 Train Loss 93188.89 Test MSE 21538.314350134624 Test RE 0.2601628118354561\n",
      "81 Train Loss 92614.03 Test MSE 21385.47609665906 Test RE 0.25923809644488716\n",
      "82 Train Loss 92015.2 Test MSE 21205.70761362203 Test RE 0.2581462060892158\n",
      "83 Train Loss 91724.86 Test MSE 21183.936982991578 Test RE 0.25801366043274415\n",
      "84 Train Loss 91512.31 Test MSE 21001.1308311407 Test RE 0.2568979877110249\n",
      "85 Train Loss 91416.54 Test MSE 21001.363274196723 Test RE 0.2568994093960904\n",
      "86 Train Loss 91026.55 Test MSE 20750.692355912644 Test RE 0.2553616395454036\n",
      "87 Train Loss 90612.4 Test MSE 20867.50597109487 Test RE 0.2560793951942316\n",
      "88 Train Loss 90550.94 Test MSE 20837.357504776715 Test RE 0.25589434213679635\n",
      "89 Train Loss 90326.375 Test MSE 20709.433408268542 Test RE 0.25510764332322633\n",
      "90 Train Loss 89917.27 Test MSE 20637.08161927305 Test RE 0.25466162330647546\n",
      "91 Train Loss 89493.27 Test MSE 20456.121952313413 Test RE 0.253542643612206\n",
      "92 Train Loss 89008.74 Test MSE 20192.879399674275 Test RE 0.25190598615239895\n",
      "93 Train Loss 88715.04 Test MSE 20014.514472360563 Test RE 0.25079096801535167\n",
      "94 Train Loss 88168.89 Test MSE 19723.83495665725 Test RE 0.2489631338745452\n",
      "95 Train Loss 87457.055 Test MSE 19437.82633062989 Test RE 0.24715147745097218\n",
      "96 Train Loss 87101.21 Test MSE 19338.92757702669 Test RE 0.2465219280564276\n",
      "97 Train Loss 86705.09 Test MSE 19178.54737877369 Test RE 0.24549758094492352\n",
      "98 Train Loss 86390.83 Test MSE 19136.60710221095 Test RE 0.24522900294897146\n",
      "99 Train Loss 85874.055 Test MSE 18993.70739351816 Test RE 0.2443116820739631\n",
      "Training time: 393.88\n",
      "3D_HTTP_atanh\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 724160.75 Test MSE 303461.2315975039 Test RE 0.9765420982300564\n",
      "1 Train Loss 515333.34 Test MSE 269145.80213716783 Test RE 0.9196724301937471\n",
      "2 Train Loss 450902.8 Test MSE 220616.05987155446 Test RE 0.8326412531792657\n",
      "3 Train Loss 380338.3 Test MSE 169963.77535985288 Test RE 0.7308319863838616\n",
      "4 Train Loss 300447.8 Test MSE 113229.56315096024 Test RE 0.5965121281724068\n",
      "5 Train Loss 248460.23 Test MSE 87353.25598216524 Test RE 0.5239368092089984\n",
      "6 Train Loss 243666.4 Test MSE 84400.03988491569 Test RE 0.5150041019159172\n",
      "7 Train Loss 227123.84 Test MSE 72458.00922293698 Test RE 0.4771803606064655\n",
      "8 Train Loss 216109.73 Test MSE 65159.88629454791 Test RE 0.4525113961966876\n",
      "9 Train Loss 209766.78 Test MSE 62288.25955594793 Test RE 0.44242785312780125\n",
      "10 Train Loss 203418.72 Test MSE 58919.14563065456 Test RE 0.43029627107181984\n",
      "11 Train Loss 196404.6 Test MSE 55442.585237795276 Test RE 0.41740831732973227\n",
      "12 Train Loss 190255.95 Test MSE 51067.31893405324 Test RE 0.4005999502887715\n",
      "13 Train Loss 186748.86 Test MSE 50472.545189438555 Test RE 0.39826025270230275\n",
      "14 Train Loss 183253.05 Test MSE 49381.242766215735 Test RE 0.3939311917946828\n",
      "15 Train Loss 178589.78 Test MSE 48916.47305336527 Test RE 0.392072995096441\n",
      "16 Train Loss 176333.62 Test MSE 48165.73561758972 Test RE 0.3890527244675052\n",
      "17 Train Loss 174051.38 Test MSE 47175.191536397164 Test RE 0.38503144416826446\n",
      "18 Train Loss 171794.3 Test MSE 45837.427642546994 Test RE 0.37953294544981775\n",
      "19 Train Loss 170675.44 Test MSE 45721.14309457072 Test RE 0.3790512229197315\n",
      "20 Train Loss 168756.28 Test MSE 44927.67273392527 Test RE 0.3757476936045674\n",
      "21 Train Loss 166566.84 Test MSE 43842.72754561224 Test RE 0.3711830572699734\n",
      "22 Train Loss 164814.55 Test MSE 42671.43641877354 Test RE 0.3661912758375589\n",
      "23 Train Loss 163147.83 Test MSE 42453.934974051495 Test RE 0.3652568230692399\n",
      "24 Train Loss 161955.0 Test MSE 42417.09844440988 Test RE 0.36509832523285946\n",
      "25 Train Loss 160773.6 Test MSE 42129.17514183316 Test RE 0.36385708863056954\n",
      "26 Train Loss 158713.44 Test MSE 41488.571941242604 Test RE 0.3610801427001061\n",
      "27 Train Loss 156460.33 Test MSE 40990.65612386812 Test RE 0.3589068912815532\n",
      "28 Train Loss 155007.7 Test MSE 40600.96028058578 Test RE 0.3571967633213692\n",
      "29 Train Loss 153090.27 Test MSE 40117.99688905576 Test RE 0.355065913957557\n",
      "30 Train Loss 151617.53 Test MSE 39558.115448424316 Test RE 0.35257958236254633\n",
      "31 Train Loss 148827.16 Test MSE 38525.26414745219 Test RE 0.3479462616425247\n",
      "32 Train Loss 148075.45 Test MSE 38451.02842211355 Test RE 0.34761086487240467\n",
      "33 Train Loss 147551.72 Test MSE 37976.8591831053 Test RE 0.34546088743542214\n",
      "34 Train Loss 146725.3 Test MSE 37370.87811743205 Test RE 0.3426936156008306\n",
      "35 Train Loss 146038.22 Test MSE 36940.35676281899 Test RE 0.3407139417553308\n",
      "36 Train Loss 144608.2 Test MSE 36527.05954016862 Test RE 0.3388025875086023\n",
      "37 Train Loss 143406.42 Test MSE 35937.95539741214 Test RE 0.3360593978429173\n",
      "38 Train Loss 142274.45 Test MSE 35376.51828389298 Test RE 0.3334240374991367\n",
      "39 Train Loss 141511.98 Test MSE 35299.92205260953 Test RE 0.3330628817804589\n",
      "40 Train Loss 140462.9 Test MSE 35058.666616837596 Test RE 0.33192278019320615\n",
      "41 Train Loss 139081.75 Test MSE 34533.30185769235 Test RE 0.3294264108809048\n",
      "42 Train Loss 137222.83 Test MSE 34268.21803094638 Test RE 0.3281596069989943\n",
      "43 Train Loss 136499.89 Test MSE 34200.30161012186 Test RE 0.3278342548052456\n",
      "44 Train Loss 136144.69 Test MSE 34067.07769167317 Test RE 0.3271951086824552\n",
      "45 Train Loss 135311.56 Test MSE 33788.83410123208 Test RE 0.32585618260329346\n",
      "46 Train Loss 134525.45 Test MSE 33371.20034043166 Test RE 0.3238361117260424\n",
      "47 Train Loss 133707.62 Test MSE 32990.55648190669 Test RE 0.3219839195720885\n",
      "48 Train Loss 132858.38 Test MSE 32823.55618108565 Test RE 0.3211679340518681\n",
      "49 Train Loss 132393.27 Test MSE 32596.5918432851 Test RE 0.32005562139310845\n",
      "50 Train Loss 131920.33 Test MSE 32483.119772087663 Test RE 0.319498062558606\n",
      "51 Train Loss 131334.86 Test MSE 32418.80132967576 Test RE 0.3191815935691612\n",
      "52 Train Loss 130908.3 Test MSE 32166.023033251753 Test RE 0.3179347851185444\n",
      "53 Train Loss 130476.45 Test MSE 31973.77886702774 Test RE 0.316983273239275\n",
      "54 Train Loss 129479.3 Test MSE 31503.953776307735 Test RE 0.3146457666613041\n",
      "55 Train Loss 128968.445 Test MSE 31294.54234008092 Test RE 0.31359827459654366\n",
      "56 Train Loss 128398.016 Test MSE 31384.712603902928 Test RE 0.31404974148562803\n",
      "57 Train Loss 128122.375 Test MSE 31284.686239196253 Test RE 0.3135488873966948\n",
      "58 Train Loss 127596.47 Test MSE 30919.593045132468 Test RE 0.31171395596411394\n",
      "59 Train Loss 127281.19 Test MSE 30611.1784333657 Test RE 0.3101554283786969\n",
      "60 Train Loss 126954.32 Test MSE 30642.537329053328 Test RE 0.3103142533990318\n",
      "61 Train Loss 126369.15 Test MSE 30471.25193292672 Test RE 0.3094457420035679\n",
      "62 Train Loss 125828.23 Test MSE 30220.864568595684 Test RE 0.30817173576229673\n",
      "63 Train Loss 125674.63 Test MSE 30260.662896547718 Test RE 0.30837458708268134\n",
      "64 Train Loss 125286.19 Test MSE 30220.1705986786 Test RE 0.3081681974263701\n",
      "65 Train Loss 125125.52 Test MSE 30167.82727566614 Test RE 0.30790119796825544\n",
      "66 Train Loss 124939.45 Test MSE 30105.202268054043 Test RE 0.30758144785718095\n",
      "67 Train Loss 124671.8 Test MSE 29930.20003698043 Test RE 0.3066861558681348\n",
      "68 Train Loss 124428.42 Test MSE 29814.895405895215 Test RE 0.30609483910956653\n",
      "69 Train Loss 123995.83 Test MSE 29685.531254129375 Test RE 0.3054300582201204\n",
      "70 Train Loss 123594.48 Test MSE 29844.3569473911 Test RE 0.3062460353320321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 Train Loss 123308.69 Test MSE 29974.926752602292 Test RE 0.30691522122191833\n",
      "72 Train Loss 123153.96 Test MSE 30023.4633875559 Test RE 0.30716360559237016\n",
      "73 Train Loss 122972.695 Test MSE 30041.174407031765 Test RE 0.3072541910532902\n",
      "74 Train Loss 122516.59 Test MSE 29877.518196663274 Test RE 0.3064161291567254\n",
      "75 Train Loss 122187.67 Test MSE 29607.374675064275 Test RE 0.3050277224619151\n",
      "76 Train Loss 121753.77 Test MSE 29465.88452161223 Test RE 0.3042980038435608\n",
      "77 Train Loss 121372.18 Test MSE 29480.8711824394 Test RE 0.3043753785996068\n",
      "78 Train Loss 120691.84 Test MSE 29164.064406723337 Test RE 0.3027355247136412\n",
      "79 Train Loss 120344.37 Test MSE 29085.13920433138 Test RE 0.30232560842851974\n",
      "80 Train Loss 120075.44 Test MSE 28827.31535026519 Test RE 0.3009826500585867\n",
      "81 Train Loss 119711.92 Test MSE 28644.710928508513 Test RE 0.30002785987004554\n",
      "82 Train Loss 119338.5 Test MSE 28637.025000207475 Test RE 0.29998760553947545\n",
      "83 Train Loss 118987.47 Test MSE 28619.954801029624 Test RE 0.299898182641592\n",
      "84 Train Loss 118751.56 Test MSE 28479.2709113132 Test RE 0.2991601867368872\n",
      "85 Train Loss 118552.164 Test MSE 28396.544336097882 Test RE 0.29872537054594916\n",
      "86 Train Loss 118238.984 Test MSE 28423.770635514153 Test RE 0.29886854356229803\n",
      "87 Train Loss 117872.69 Test MSE 28018.67630993485 Test RE 0.29673117014434264\n",
      "88 Train Loss 117409.945 Test MSE 27882.49206832565 Test RE 0.29600916365556434\n",
      "89 Train Loss 117076.984 Test MSE 27697.737096758927 Test RE 0.29502682568488353\n",
      "90 Train Loss 116639.914 Test MSE 27652.037483671324 Test RE 0.29478333688097813\n",
      "91 Train Loss 116356.195 Test MSE 27720.93959212667 Test RE 0.29515037234028535\n",
      "92 Train Loss 116127.42 Test MSE 27669.41694986777 Test RE 0.29487595884975454\n",
      "93 Train Loss 115807.98 Test MSE 27643.48892895813 Test RE 0.29473776761187626\n",
      "94 Train Loss 115266.23 Test MSE 27488.330317590426 Test RE 0.29390944500078525\n",
      "95 Train Loss 114943.266 Test MSE 27418.74365371416 Test RE 0.2935371936199985\n",
      "96 Train Loss 114524.74 Test MSE 27322.725712423475 Test RE 0.29302277261182913\n",
      "97 Train Loss 114272.266 Test MSE 27207.38944775708 Test RE 0.2924036562496193\n",
      "98 Train Loss 114108.51 Test MSE 27193.296288422625 Test RE 0.29232791533539204\n",
      "99 Train Loss 113967.086 Test MSE 27087.381807096375 Test RE 0.2917580696260063\n",
      "Training time: 400.14\n",
      "3D_HTTP_atanh\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 948221.3 Test MSE 297479.43647569383 Test RE 0.9668694480817364\n",
      "1 Train Loss 568132.0 Test MSE 274587.7803922639 Test RE 0.9289235334015785\n",
      "2 Train Loss 490752.53 Test MSE 235339.72788360613 Test RE 0.8599772959271053\n",
      "3 Train Loss 452495.22 Test MSE 210309.50050594014 Test RE 0.8129593085076892\n",
      "4 Train Loss 375673.84 Test MSE 152963.4514253388 Test RE 0.6933191555366476\n",
      "5 Train Loss 271980.16 Test MSE 96903.49902477341 Test RE 0.5518348089696127\n",
      "6 Train Loss 248091.31 Test MSE 87243.58450987193 Test RE 0.5236078062004863\n",
      "7 Train Loss 244138.88 Test MSE 84739.91622679844 Test RE 0.5160400128379774\n",
      "8 Train Loss 232105.39 Test MSE 75401.44874000327 Test RE 0.4867760557545542\n",
      "9 Train Loss 224360.97 Test MSE 69803.41733826156 Test RE 0.468357743089225\n",
      "10 Train Loss 220236.86 Test MSE 66992.23453125033 Test RE 0.4588297762849927\n",
      "11 Train Loss 215152.34 Test MSE 64994.9820336274 Test RE 0.45193843379994464\n",
      "12 Train Loss 209863.58 Test MSE 63267.080999801095 Test RE 0.4458905425786657\n",
      "13 Train Loss 204257.78 Test MSE 60647.958750168145 Test RE 0.4365635339039733\n",
      "14 Train Loss 197589.78 Test MSE 57377.96657015187 Test RE 0.42463123691055293\n",
      "15 Train Loss 187959.42 Test MSE 53243.83945643824 Test RE 0.4090477845669005\n",
      "16 Train Loss 183742.52 Test MSE 50562.18717371088 Test RE 0.39861376173952595\n",
      "17 Train Loss 181179.77 Test MSE 49712.31607761634 Test RE 0.395249528761379\n",
      "18 Train Loss 179028.52 Test MSE 49332.44285525946 Test RE 0.3937364968282534\n",
      "19 Train Loss 175332.34 Test MSE 48640.55298616463 Test RE 0.390965660651351\n",
      "20 Train Loss 173037.11 Test MSE 47162.76369996263 Test RE 0.38498072446959186\n",
      "21 Train Loss 171081.47 Test MSE 46179.297783473645 Test RE 0.38094565505398503\n",
      "22 Train Loss 168769.66 Test MSE 45316.94722148475 Test RE 0.3773720101046671\n",
      "23 Train Loss 165388.75 Test MSE 44254.351194084506 Test RE 0.37292143885773915\n",
      "24 Train Loss 164345.1 Test MSE 43598.418424919924 Test RE 0.3701474226774217\n",
      "25 Train Loss 163739.66 Test MSE 42952.88937250962 Test RE 0.36739695617746615\n",
      "26 Train Loss 162696.75 Test MSE 42332.614157548975 Test RE 0.3647345515777615\n",
      "27 Train Loss 161583.19 Test MSE 42262.63269570076 Test RE 0.3644329494294079\n",
      "28 Train Loss 160438.16 Test MSE 41675.57566579467 Test RE 0.361892985990191\n",
      "29 Train Loss 159284.42 Test MSE 40774.54947284323 Test RE 0.3579595453153549\n",
      "30 Train Loss 157736.7 Test MSE 40618.17998145013 Test RE 0.3572725025321841\n",
      "31 Train Loss 156665.61 Test MSE 40312.08742721133 Test RE 0.355923780603563\n",
      "32 Train Loss 155628.22 Test MSE 39920.5712981085 Test RE 0.3541911749656279\n",
      "33 Train Loss 155305.08 Test MSE 40135.66783943032 Test RE 0.35514410406998775\n",
      "34 Train Loss 154776.94 Test MSE 40030.937297562356 Test RE 0.35468044254290754\n",
      "35 Train Loss 154113.48 Test MSE 39771.754341488275 Test RE 0.35353037696240047\n",
      "36 Train Loss 151928.03 Test MSE 38390.21462170854 Test RE 0.3473358669885396\n",
      "37 Train Loss 151459.19 Test MSE 38082.28887238821 Test RE 0.3459400817613586\n",
      "38 Train Loss 149288.72 Test MSE 37864.91273114673 Test RE 0.34495134473930544\n",
      "39 Train Loss 148116.14 Test MSE 37807.337194474 Test RE 0.3446889868869219\n",
      "40 Train Loss 147471.14 Test MSE 37229.34872954013 Test RE 0.3420440827601599\n",
      "41 Train Loss 146851.84 Test MSE 36547.136998123424 Test RE 0.33889568780617674\n",
      "42 Train Loss 146737.81 Test MSE 36355.19170597307 Test RE 0.33800457742158335\n",
      "43 Train Loss 146009.75 Test MSE 35889.256650994794 Test RE 0.3358316272426265\n",
      "44 Train Loss 145445.95 Test MSE 35781.49956174056 Test RE 0.33532708289479835\n",
      "45 Train Loss 144837.92 Test MSE 35345.76522209979 Test RE 0.33327908203258877\n",
      "46 Train Loss 144316.42 Test MSE 35438.92617297343 Test RE 0.3337180053970804\n",
      "47 Train Loss 143786.56 Test MSE 35335.73519831958 Test RE 0.3332317915824284\n",
      "48 Train Loss 143644.42 Test MSE 35341.14901834006 Test RE 0.3332573179771616\n",
      "49 Train Loss 142675.48 Test MSE 35639.705248289894 Test RE 0.33466200914541405\n",
      "50 Train Loss 142252.88 Test MSE 35743.53419987266 Test RE 0.3351491390837612\n",
      "51 Train Loss 141780.11 Test MSE 35489.20788044474 Test RE 0.33395466555029824\n",
      "52 Train Loss 141025.1 Test MSE 35289.28743755392 Test RE 0.33301270798203836\n",
      "53 Train Loss 140504.55 Test MSE 34976.550028518795 Test RE 0.3315338272176009\n",
      "54 Train Loss 140184.47 Test MSE 35028.04103914795 Test RE 0.3317777725698538\n",
      "55 Train Loss 139928.0 Test MSE 34823.70001730217 Test RE 0.3308086208471508\n",
      "56 Train Loss 139732.27 Test MSE 34808.14204480535 Test RE 0.33073471591470577\n",
      "57 Train Loss 139577.6 Test MSE 34750.77566760934 Test RE 0.33046206596781647\n",
      "58 Train Loss 138836.5 Test MSE 34204.64129831709 Test RE 0.3278550536469192\n",
      "59 Train Loss 138457.6 Test MSE 34104.0108035725 Test RE 0.32737242151510915\n",
      "60 Train Loss 138075.72 Test MSE 34241.1368214163 Test RE 0.3280299137137901\n",
      "61 Train Loss 137762.48 Test MSE 34310.821243313636 Test RE 0.32836353226333576\n",
      "62 Train Loss 137498.33 Test MSE 34284.119921395824 Test RE 0.3282357380787003\n",
      "63 Train Loss 137307.08 Test MSE 34102.32009802798 Test RE 0.3273643066744333\n",
      "64 Train Loss 137090.58 Test MSE 33966.417032495905 Test RE 0.32671135658266554\n",
      "65 Train Loss 136925.25 Test MSE 33832.72899299651 Test RE 0.3260677729094836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 Train Loss 136701.88 Test MSE 33597.9043591981 Test RE 0.3249342245387787\n",
      "67 Train Loss 136469.28 Test MSE 33546.403151562365 Test RE 0.3246850881244887\n",
      "68 Train Loss 136372.31 Test MSE 33512.46675385319 Test RE 0.32452081670727567\n",
      "69 Train Loss 135954.95 Test MSE 33488.05447322201 Test RE 0.32440259597783766\n",
      "70 Train Loss 135511.45 Test MSE 33529.25254511152 Test RE 0.3246020798500421\n",
      "71 Train Loss 135310.75 Test MSE 33476.70005341805 Test RE 0.32434759553672976\n",
      "72 Train Loss 134896.06 Test MSE 33031.7802589382 Test RE 0.32218502635576707\n",
      "73 Train Loss 134795.48 Test MSE 33062.17352685573 Test RE 0.3223332170416229\n",
      "74 Train Loss 134627.52 Test MSE 33104.36394097425 Test RE 0.32253881508177645\n",
      "75 Train Loss 134569.44 Test MSE 33114.19060298531 Test RE 0.3225866825586385\n",
      "76 Train Loss 134517.88 Test MSE 33018.11000174824 Test RE 0.3221183510521663\n",
      "77 Train Loss 134471.75 Test MSE 32967.349096873295 Test RE 0.32187064898526985\n",
      "78 Train Loss 134180.83 Test MSE 32701.875897989456 Test RE 0.3205720801105871\n",
      "79 Train Loss 133931.17 Test MSE 32606.975721093644 Test RE 0.32010659533921787\n",
      "80 Train Loss 133761.64 Test MSE 32538.24799665286 Test RE 0.3197690632250795\n",
      "81 Train Loss 133642.38 Test MSE 32503.63071020973 Test RE 0.3195989175701475\n",
      "82 Train Loss 133511.78 Test MSE 32353.20830120693 Test RE 0.31885852967986167\n",
      "83 Train Loss 133302.61 Test MSE 32376.705633304697 Test RE 0.31897429818133083\n",
      "84 Train Loss 132948.77 Test MSE 32229.107779029 Test RE 0.3182464029197553\n",
      "85 Train Loss 132622.28 Test MSE 31936.941454211614 Test RE 0.31680062030761846\n",
      "86 Train Loss 132156.34 Test MSE 31952.89273428219 Test RE 0.3168797253245144\n",
      "87 Train Loss 131930.75 Test MSE 31778.272143186725 Test RE 0.3160126750149737\n",
      "88 Train Loss 131763.02 Test MSE 31670.562433674168 Test RE 0.31547667160733256\n",
      "89 Train Loss 131672.64 Test MSE 31541.429931743864 Test RE 0.3148328576064552\n",
      "90 Train Loss 131595.48 Test MSE 31514.51545979801 Test RE 0.3146985046532555\n",
      "91 Train Loss 131395.83 Test MSE 31398.463994934016 Test RE 0.3141185352945662\n",
      "92 Train Loss 131199.7 Test MSE 31390.670185855834 Test RE 0.31407954721166714\n",
      "93 Train Loss 130976.914 Test MSE 31408.430735887767 Test RE 0.31416838629467025\n",
      "94 Train Loss 130420.19 Test MSE 31490.961257366664 Test RE 0.31458087857320205\n",
      "95 Train Loss 130120.51 Test MSE 31328.82117986684 Test RE 0.3137699793427092\n",
      "96 Train Loss 129928.016 Test MSE 31232.683454979662 Test RE 0.31328818162294525\n",
      "97 Train Loss 129453.88 Test MSE 31280.675685876897 Test RE 0.31352878898775316\n",
      "98 Train Loss 129300.17 Test MSE 31261.86352094852 Test RE 0.3134344968695089\n",
      "99 Train Loss 129056.3 Test MSE 31527.915409235466 Test RE 0.3147654023323686\n",
      "Training time: 395.83\n",
      "3D_HTTP_atanh\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 900576.25 Test MSE 308668.21847127454 Test RE 0.9848845391225189\n",
      "1 Train Loss 506427.66 Test MSE 263933.9092374242 Test RE 0.9107243687103072\n",
      "2 Train Loss 454828.6 Test MSE 228518.45259125612 Test RE 0.8474225169349868\n",
      "3 Train Loss 431080.47 Test MSE 214804.1915921283 Test RE 0.8216005816963526\n",
      "4 Train Loss 382632.12 Test MSE 174317.49977862995 Test RE 0.7401331499244209\n",
      "5 Train Loss 340913.53 Test MSE 149276.13549602247 Test RE 0.6849116507954975\n",
      "6 Train Loss 264816.2 Test MSE 91956.42259049238 Test RE 0.5375642711578774\n",
      "7 Train Loss 243543.05 Test MSE 87342.21717188951 Test RE 0.523903703277805\n",
      "8 Train Loss 237151.11 Test MSE 81442.49410585138 Test RE 0.5059002508435041\n",
      "9 Train Loss 223307.94 Test MSE 74336.47192843581 Test RE 0.4833261966123677\n",
      "10 Train Loss 210851.11 Test MSE 66106.04208344003 Test RE 0.4557849073766553\n",
      "11 Train Loss 206388.83 Test MSE 62254.69869320973 Test RE 0.44230864718792706\n",
      "12 Train Loss 202744.8 Test MSE 61448.609599111995 Test RE 0.439435756790031\n",
      "13 Train Loss 198651.11 Test MSE 58685.98934756842 Test RE 0.4294440376561274\n",
      "14 Train Loss 195960.8 Test MSE 57997.64674656981 Test RE 0.42691808081949245\n",
      "15 Train Loss 194989.25 Test MSE 57101.90127510638 Test RE 0.4236084811506072\n",
      "16 Train Loss 192666.92 Test MSE 55582.32767109605 Test RE 0.4179340228422565\n",
      "17 Train Loss 191321.73 Test MSE 55149.74273686827 Test RE 0.41630450217565645\n",
      "18 Train Loss 189759.05 Test MSE 54104.37747985333 Test RE 0.4123400928912963\n",
      "19 Train Loss 188343.16 Test MSE 53298.20000801569 Test RE 0.4092565447793959\n",
      "20 Train Loss 187516.81 Test MSE 53184.87881501743 Test RE 0.40882123816729277\n",
      "21 Train Loss 186562.83 Test MSE 52274.222999672136 Test RE 0.4053061142176181\n",
      "22 Train Loss 185306.27 Test MSE 51277.89560863094 Test RE 0.4014250398390995\n",
      "23 Train Loss 184177.61 Test MSE 51067.29069169708 Test RE 0.4005998395145204\n",
      "24 Train Loss 182934.4 Test MSE 50339.24918612795 Test RE 0.39773401020788146\n",
      "25 Train Loss 182011.78 Test MSE 50205.99534500868 Test RE 0.39720723730229207\n",
      "26 Train Loss 181518.97 Test MSE 49738.69328112064 Test RE 0.3953543739536723\n",
      "27 Train Loss 180942.5 Test MSE 49386.0752924568 Test RE 0.3939504666867554\n",
      "28 Train Loss 180332.42 Test MSE 49109.84279741955 Test RE 0.39284717475771286\n",
      "29 Train Loss 179935.27 Test MSE 48805.296436936645 Test RE 0.39162719287733827\n",
      "30 Train Loss 179591.69 Test MSE 48780.37358882004 Test RE 0.3915271861962619\n",
      "31 Train Loss 179285.95 Test MSE 48705.23362198374 Test RE 0.39122552104341773\n",
      "32 Train Loss 179065.33 Test MSE 48600.51771281602 Test RE 0.3908047286895038\n",
      "33 Train Loss 178770.08 Test MSE 48283.99255134249 Test RE 0.38953003449262974\n",
      "34 Train Loss 178633.81 Test MSE 48164.83488576611 Test RE 0.3890490866759939\n",
      "35 Train Loss 178396.44 Test MSE 48103.16488848482 Test RE 0.38879993871241875\n",
      "36 Train Loss 178172.38 Test MSE 48000.360584504546 Test RE 0.3883842520752107\n",
      "37 Train Loss 178066.55 Test MSE 47857.47034801366 Test RE 0.3878057389171975\n",
      "38 Train Loss 177786.86 Test MSE 47823.262717496145 Test RE 0.3876671159735063\n",
      "39 Train Loss 177576.48 Test MSE 47723.93498390095 Test RE 0.3872643192102511\n",
      "40 Train Loss 177400.9 Test MSE 47614.175503371625 Test RE 0.3868187314948536\n",
      "41 Train Loss 177212.56 Test MSE 47502.1582566825 Test RE 0.3863634481299588\n",
      "42 Train Loss 177042.47 Test MSE 47498.89827367061 Test RE 0.38635019020722206\n",
      "43 Train Loss 176883.53 Test MSE 47363.84914777763 Test RE 0.3858005627722302\n",
      "44 Train Loss 176747.42 Test MSE 47278.46134472797 Test RE 0.38545264422502173\n",
      "45 Train Loss 176595.3 Test MSE 47081.96643786175 Test RE 0.38465081672467\n",
      "46 Train Loss 176232.67 Test MSE 47224.36145269194 Test RE 0.3852320478406157\n",
      "47 Train Loss 175966.9 Test MSE 47081.351404170164 Test RE 0.3846483043616329\n",
      "48 Train Loss 175793.08 Test MSE 46960.77594532423 Test RE 0.38415544597109963\n",
      "49 Train Loss 175633.45 Test MSE 47023.07177190743 Test RE 0.38441016226688013\n",
      "50 Train Loss 175341.33 Test MSE 46883.64370558885 Test RE 0.3838398320745751\n",
      "51 Train Loss 175204.62 Test MSE 46910.86480687899 Test RE 0.3839512464819354\n",
      "52 Train Loss 174917.14 Test MSE 46894.39455012331 Test RE 0.3838838385285846\n",
      "53 Train Loss 174675.16 Test MSE 46688.47516580305 Test RE 0.3830400693080022\n",
      "54 Train Loss 174281.56 Test MSE 46496.99537581071 Test RE 0.38225379617724237\n",
      "55 Train Loss 173923.94 Test MSE 46242.556062331576 Test RE 0.3812064831813869\n",
      "56 Train Loss 173721.7 Test MSE 46188.137036677086 Test RE 0.38098211201787285\n",
      "57 Train Loss 173650.92 Test MSE 46015.41002205415 Test RE 0.38026907675638166\n",
      "58 Train Loss 173501.84 Test MSE 45933.40353387248 Test RE 0.3799300768666958\n",
      "59 Train Loss 173350.27 Test MSE 45849.04872597084 Test RE 0.3795810535650796\n",
      "60 Train Loss 173151.89 Test MSE 45627.32265584656 Test RE 0.37866211394097316\n",
      "61 Train Loss 173031.0 Test MSE 45621.89598685672 Test RE 0.37863959525113683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 Train Loss 172724.52 Test MSE 45669.24885178359 Test RE 0.37883604718663266\n",
      "63 Train Loss 172372.34 Test MSE 45502.904866747594 Test RE 0.3781454885845056\n",
      "64 Train Loss 171881.69 Test MSE 45406.70314011401 Test RE 0.3777455416250753\n",
      "65 Train Loss 171462.66 Test MSE 45595.57154688594 Test RE 0.37853033945042425\n",
      "66 Train Loss 170683.78 Test MSE 45444.67061162429 Test RE 0.377903437324105\n",
      "67 Train Loss 169784.78 Test MSE 45944.45529975639 Test RE 0.37997578048760233\n",
      "68 Train Loss 169050.45 Test MSE 46158.14649723204 Test RE 0.38085840369001556\n",
      "69 Train Loss 168325.69 Test MSE 45797.790315714934 Test RE 0.3793688118390489\n",
      "70 Train Loss 167914.52 Test MSE 45399.80069320115 Test RE 0.37771682926033756\n",
      "71 Train Loss 167610.56 Test MSE 44974.58449878061 Test RE 0.3759438131386811\n",
      "72 Train Loss 167468.3 Test MSE 44812.04856500894 Test RE 0.3752638770544081\n",
      "73 Train Loss 167175.53 Test MSE 44587.02712984392 Test RE 0.37432050700429986\n",
      "74 Train Loss 166951.34 Test MSE 44620.4478091511 Test RE 0.37446076867957456\n",
      "75 Train Loss 166770.75 Test MSE 44599.34500933192 Test RE 0.374372209447608\n",
      "76 Train Loss 166568.17 Test MSE 44559.23448595984 Test RE 0.37420382532670904\n",
      "77 Train Loss 166267.1 Test MSE 44571.90105844148 Test RE 0.3742570078278786\n",
      "78 Train Loss 165934.81 Test MSE 44421.69765292555 Test RE 0.37362586897828126\n",
      "79 Train Loss 165681.3 Test MSE 44250.92660978428 Test RE 0.3729070094787282\n",
      "80 Train Loss 165458.98 Test MSE 44183.6031996405 Test RE 0.37262323092791916\n",
      "81 Train Loss 165258.98 Test MSE 44090.68094880466 Test RE 0.3722311939403354\n",
      "82 Train Loss 165119.36 Test MSE 44316.83013060439 Test RE 0.37318459396396897\n",
      "83 Train Loss 165054.72 Test MSE 44427.03031224669 Test RE 0.3736482945021363\n",
      "84 Train Loss 164941.25 Test MSE 44308.793055847345 Test RE 0.37315075300029477\n",
      "85 Train Loss 164596.72 Test MSE 43982.16427111345 Test RE 0.3717728411046083\n",
      "86 Train Loss 164510.27 Test MSE 43920.15671386879 Test RE 0.3715106796469859\n",
      "87 Train Loss 164383.92 Test MSE 43934.46914604358 Test RE 0.37157120753084527\n",
      "88 Train Loss 164277.5 Test MSE 43780.333765340874 Test RE 0.3709188428721137\n",
      "89 Train Loss 164074.7 Test MSE 43663.00630003314 Test RE 0.3704214944074632\n",
      "90 Train Loss 163797.66 Test MSE 43558.36724560285 Test RE 0.3699773678042557\n",
      "91 Train Loss 163465.52 Test MSE 43361.66640017616 Test RE 0.36914105080200776\n",
      "92 Train Loss 163160.75 Test MSE 43109.174219118664 Test RE 0.3680647394506751\n",
      "93 Train Loss 162543.55 Test MSE 42825.14969464022 Test RE 0.3668502395977598\n",
      "94 Train Loss 162215.03 Test MSE 42815.01250518215 Test RE 0.3668068182611593\n",
      "95 Train Loss 161868.67 Test MSE 42504.934776187256 Test RE 0.36547614825811425\n",
      "96 Train Loss 161496.05 Test MSE 42301.34325332036 Test RE 0.364599812836148\n",
      "97 Train Loss 161264.69 Test MSE 42247.849769442226 Test RE 0.3643692068716694\n",
      "98 Train Loss 161145.72 Test MSE 42257.34792578781 Test RE 0.3644101632897944\n",
      "99 Train Loss 160893.83 Test MSE 42291.9775591035 Test RE 0.3645594486405253\n",
      "Training time: 391.87\n",
      "3D_HTTP_atanh\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 796973.3 Test MSE 306435.9875881865 Test RE 0.9813168266775092\n",
      "1 Train Loss 586975.0 Test MSE 305287.4222668927 Test RE 0.9794760432113461\n",
      "2 Train Loss 478765.47 Test MSE 244430.3573073905 Test RE 0.8764293916941523\n",
      "3 Train Loss 408834.72 Test MSE 191143.76818523335 Test RE 0.7750316423722783\n",
      "4 Train Loss 326967.66 Test MSE 139126.47846128597 Test RE 0.6612173770152129\n",
      "5 Train Loss 292761.38 Test MSE 114867.84552207745 Test RE 0.6008120023469763\n",
      "6 Train Loss 242119.98 Test MSE 87181.71734410853 Test RE 0.5234221199057043\n",
      "7 Train Loss 232415.0 Test MSE 80384.34520547988 Test RE 0.5026030288011907\n",
      "8 Train Loss 217824.73 Test MSE 69880.96708694965 Test RE 0.46861783739644336\n",
      "9 Train Loss 212346.75 Test MSE 67147.40293973144 Test RE 0.4593608431174471\n",
      "10 Train Loss 207392.36 Test MSE 64624.63373388581 Test RE 0.4506489977802165\n",
      "11 Train Loss 203245.22 Test MSE 62039.874122441244 Test RE 0.4415448423651913\n",
      "12 Train Loss 197084.84 Test MSE 58586.991704104286 Test RE 0.4290816692903688\n",
      "13 Train Loss 192829.45 Test MSE 55797.3019564502 Test RE 0.4187414590040425\n",
      "14 Train Loss 189357.6 Test MSE 54630.09566913092 Test RE 0.4143385508179258\n",
      "15 Train Loss 186673.9 Test MSE 52975.33785114625 Test RE 0.40801509418751025\n",
      "16 Train Loss 182949.12 Test MSE 51733.565954259364 Test RE 0.40320468500111\n",
      "17 Train Loss 180652.08 Test MSE 51108.515404303915 Test RE 0.4007615015242722\n",
      "18 Train Loss 178486.34 Test MSE 49501.81559384723 Test RE 0.3944118240863821\n",
      "19 Train Loss 176447.27 Test MSE 47918.06259606996 Test RE 0.38805116129707284\n",
      "20 Train Loss 175130.73 Test MSE 47544.59655603177 Test RE 0.38653599762807367\n",
      "21 Train Loss 174595.72 Test MSE 47355.88234634121 Test RE 0.3857681147576384\n",
      "22 Train Loss 174008.22 Test MSE 47218.02134072911 Test RE 0.3852061872866345\n",
      "23 Train Loss 173356.84 Test MSE 46701.911339545324 Test RE 0.38309518165296874\n",
      "24 Train Loss 172998.45 Test MSE 46520.65628749748 Test RE 0.3823510424930341\n",
      "25 Train Loss 172570.1 Test MSE 46158.151542439715 Test RE 0.38085842450443125\n",
      "26 Train Loss 172241.45 Test MSE 46012.56383685744 Test RE 0.38025731620754466\n",
      "27 Train Loss 171937.97 Test MSE 45599.48771056885 Test RE 0.3785465949209567\n",
      "28 Train Loss 171570.38 Test MSE 45246.03839455153 Test RE 0.3770766517006502\n",
      "29 Train Loss 170847.9 Test MSE 44611.286037196405 Test RE 0.37442232329791625\n",
      "30 Train Loss 170242.67 Test MSE 44274.43984088066 Test RE 0.3730060705029292\n",
      "31 Train Loss 169913.52 Test MSE 44093.70529343767 Test RE 0.37224396008598254\n",
      "32 Train Loss 169441.39 Test MSE 43981.89076955384 Test RE 0.3717716851745668\n",
      "33 Train Loss 169052.36 Test MSE 44128.52247378441 Test RE 0.3723908963386383\n",
      "34 Train Loss 168717.16 Test MSE 44042.32636325899 Test RE 0.372027023553713\n",
      "35 Train Loss 168388.08 Test MSE 43690.672535869075 Test RE 0.3705388311006501\n",
      "36 Train Loss 168123.08 Test MSE 43343.26973156722 Test RE 0.36906273639917364\n",
      "37 Train Loss 167705.23 Test MSE 43369.77824904302 Test RE 0.3691755775699094\n",
      "38 Train Loss 167241.3 Test MSE 43973.327779406216 Test RE 0.37173549263955624\n",
      "39 Train Loss 166752.81 Test MSE 43878.75910086788 Test RE 0.37133555184367406\n",
      "40 Train Loss 166273.61 Test MSE 43756.27100132782 Test RE 0.37081689573569626\n",
      "41 Train Loss 165826.38 Test MSE 43274.47127037416 Test RE 0.3687697147578979\n",
      "42 Train Loss 165433.0 Test MSE 42776.117173067636 Test RE 0.36664016743135397\n",
      "43 Train Loss 164589.1 Test MSE 42903.0780552298 Test RE 0.36718386416717713\n",
      "44 Train Loss 163697.66 Test MSE 42817.29829609022 Test RE 0.3668166095989496\n",
      "45 Train Loss 162569.25 Test MSE 42886.65902146081 Test RE 0.3671135967008828\n",
      "46 Train Loss 160925.58 Test MSE 42507.21884991306 Test RE 0.3654859678620948\n",
      "47 Train Loss 158769.4 Test MSE 41291.86046957119 Test RE 0.36022312357611846\n",
      "48 Train Loss 157501.48 Test MSE 40636.17483413544 Test RE 0.3573516340215025\n",
      "49 Train Loss 156451.66 Test MSE 40974.896360341656 Test RE 0.3588378898060308\n",
      "50 Train Loss 155593.3 Test MSE 41007.212100837816 Test RE 0.35897936455858465\n",
      "51 Train Loss 154812.22 Test MSE 40672.414171666554 Test RE 0.3575109415913996\n",
      "52 Train Loss 153784.69 Test MSE 39921.511065106766 Test RE 0.35419534393425195\n",
      "53 Train Loss 153079.39 Test MSE 40164.08658029654 Test RE 0.35526981472654023\n",
      "54 Train Loss 152122.48 Test MSE 39551.32132327409 Test RE 0.35254930320686056\n",
      "55 Train Loss 151525.78 Test MSE 39177.26113107104 Test RE 0.3508782092578666\n",
      "56 Train Loss 150570.89 Test MSE 38614.61203391312 Test RE 0.34834950688490013\n",
      "57 Train Loss 150135.1 Test MSE 38207.646686122964 Test RE 0.3465089900959149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 Train Loss 149778.03 Test MSE 37819.73051523506 Test RE 0.34474547712760145\n",
      "59 Train Loss 149031.81 Test MSE 37577.743503460326 Test RE 0.34364079180945994\n",
      "60 Train Loss 148016.27 Test MSE 37299.014623725845 Test RE 0.3423639603650224\n",
      "61 Train Loss 147525.1 Test MSE 37017.25660796349 Test RE 0.3410683945822131\n",
      "62 Train Loss 146950.75 Test MSE 36677.99576281351 Test RE 0.3395018617579047\n",
      "63 Train Loss 145814.94 Test MSE 36901.952511381875 Test RE 0.34053678772038226\n",
      "64 Train Loss 145180.28 Test MSE 36843.103794259456 Test RE 0.3402651469565427\n",
      "65 Train Loss 144816.42 Test MSE 36974.4923987756 Test RE 0.3408713279107732\n",
      "66 Train Loss 144257.61 Test MSE 36598.6483344485 Test RE 0.3391344318548181\n",
      "67 Train Loss 143720.83 Test MSE 36295.95580379916 Test RE 0.33772909863043726\n",
      "68 Train Loss 143061.84 Test MSE 35830.436740580844 Test RE 0.33555631244944467\n",
      "69 Train Loss 142676.62 Test MSE 35687.57155386178 Test RE 0.3348866695186494\n",
      "70 Train Loss 142380.44 Test MSE 35339.37726386891 Test RE 0.3332489642937076\n",
      "71 Train Loss 141972.33 Test MSE 35125.782989980245 Test RE 0.3322403450568094\n",
      "72 Train Loss 141750.1 Test MSE 35128.82445114494 Test RE 0.3322547287110319\n",
      "73 Train Loss 141526.12 Test MSE 35194.76633718682 Test RE 0.3325664276023504\n",
      "74 Train Loss 141070.69 Test MSE 35306.617104152145 Test RE 0.33309446495841377\n",
      "75 Train Loss 140822.1 Test MSE 35589.79141509512 Test RE 0.33442757796375766\n",
      "76 Train Loss 140596.56 Test MSE 35538.14844842252 Test RE 0.3341848524322951\n",
      "77 Train Loss 140289.55 Test MSE 35130.61506246363 Test RE 0.3322631965647957\n",
      "78 Train Loss 139829.97 Test MSE 34745.80333702408 Test RE 0.3304384229643319\n",
      "79 Train Loss 139544.7 Test MSE 34906.25299158605 Test RE 0.3312004957758711\n",
      "80 Train Loss 139389.47 Test MSE 34856.93386496585 Test RE 0.3309664360926168\n",
      "81 Train Loss 139108.23 Test MSE 34496.895552900394 Test RE 0.32925271823839586\n",
      "82 Train Loss 138834.0 Test MSE 34444.2852126193 Test RE 0.3290015549571639\n",
      "83 Train Loss 138724.08 Test MSE 34513.49558267522 Test RE 0.32933192750461526\n",
      "84 Train Loss 138519.44 Test MSE 34545.17178618711 Test RE 0.3294830219168542\n",
      "85 Train Loss 138364.8 Test MSE 34492.55377171691 Test RE 0.32923199770373374\n",
      "86 Train Loss 138199.77 Test MSE 34424.187600361816 Test RE 0.32890555774832486\n",
      "87 Train Loss 137992.0 Test MSE 34408.56214588385 Test RE 0.32883090265107273\n",
      "88 Train Loss 137865.52 Test MSE 34388.37865290025 Test RE 0.32873444512175154\n",
      "89 Train Loss 137806.25 Test MSE 34369.929937118264 Test RE 0.3286462533807518\n",
      "90 Train Loss 137748.78 Test MSE 34304.51793087519 Test RE 0.32833336870368063\n",
      "91 Train Loss 137626.38 Test MSE 34257.191068951906 Test RE 0.3281068045063144\n",
      "92 Train Loss 137536.7 Test MSE 34243.348966435806 Test RE 0.3280405097115793\n",
      "93 Train Loss 137503.06 Test MSE 34219.16924209015 Test RE 0.327924672183336\n",
      "94 Train Loss 137344.62 Test MSE 34163.13688781803 Test RE 0.32765608121826456\n",
      "95 Train Loss 136935.5 Test MSE 34163.132792138815 Test RE 0.3276560615775881\n",
      "96 Train Loss 136779.67 Test MSE 34122.434562476024 Test RE 0.32746083657340697\n",
      "97 Train Loss 136641.94 Test MSE 33925.408410122276 Test RE 0.3265140730567205\n",
      "98 Train Loss 136555.73 Test MSE 33804.76585603248 Test RE 0.32593299568560335\n",
      "99 Train Loss 136507.62 Test MSE 33780.24512478947 Test RE 0.3258147643503112\n",
      "Training time: 396.28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 1.0 \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    alpha_val = []\n",
    "\n",
    "\n",
    "    print(reps)\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_D = 5000 #Total number of data points for 'y'\n",
    "    N_N = 3500\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "\n",
    "    layers = np.array([3,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,n_val)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.1, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    alpha_full.append(alpha_val)\n",
    "\n",
    "\n",
    "\n",
    "    #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"alpha\": alpha_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HT_stan_v3_15Aug2022_MP4Video.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
