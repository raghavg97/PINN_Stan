{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,100).reshape(-1,1)\n",
    "y = np.linspace(0,1,100).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xyt = np.hstack((X,Y,T))\n",
    "\n",
    "# initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "# DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "# NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "# NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "# NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "# NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "# xyt_initial = xyt[initial_pts,:]\n",
    "# xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "# xyt_NBC_x0 = xyt[NBC_pts_x0,:]\n",
    "# xyt_NBC_x1 = xyt[NBC_pts_x1,:]\n",
    "\n",
    "# #xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "# xyt_NBC_y1 = xyt[NBC_pts_y1,:]\n",
    "\n",
    "# u_initial = 300*np.ones((np.shape(xyt_initial)[0],1))\n",
    "# u_DBC = 1000*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "# xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "# #xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "# xyt_NBC_x = np.vstack((xyt_NBC_x0,xyt_NBC_x1))\n",
    "# #xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "# xyt_NBC_y = np.vstack((xyt_NBC_y1))\n",
    "\n",
    "# u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdic = {\"xyt\": xyt}\n",
    "savemat('HTTP_xyt.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "xy = np.hstack((X,Y))\n",
    "mdic = {\"xy\": xy}\n",
    "savemat('HTTP_xy.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_name = \"3D_HTTP_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stan = scipy.io.loadmat('./stan/'+prob_name+'stan.mat')\n",
    "data_tanh = scipy.io.loadmat('./tanh/'+prob_name+'tanh.mat')\n",
    "data_atanh = scipy.io.loadmat('./atanh/'+prob_name+'atanh.mat')\n",
    "data_swish = scipy.io.loadmat('./swish/'+prob_name+'swish.mat')\n",
    "data_rowdy = scipy.io.loadmat('./rowdy/'+prob_name+'rowdy.mat')\n",
    "\n",
    "num_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_stan =  data_stan['train_loss']\n",
    "train_loss_tanh =  data_tanh['train_loss']\n",
    "train_loss_atanh =  data_atanh['train_loss']\n",
    "train_loss_swish =  data_swish['train_loss']\n",
    "train_loss_rowdy =  data_rowdy['train_loss']\n",
    "\n",
    "\n",
    "mean_train_loss_stan = np.zeros((num_steps,1))\n",
    "mean_train_loss_tanh = np.zeros((num_steps,1))\n",
    "mean_train_loss_atanh = np.zeros((num_steps,1))\n",
    "mean_train_loss_swish = np.zeros((num_steps,1))\n",
    "mean_train_loss_rowdy = np.zeros((num_steps,1))\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    mean_train_loss_stan = mean_train_loss_stan + train_loss_stan[i,:num_steps].reshape(-1,1)\n",
    "    mean_train_loss_tanh = mean_train_loss_tanh + train_loss_tanh[i,:num_steps].reshape(-1,1)\n",
    "    mean_train_loss_atanh = mean_train_loss_atanh + train_loss_atanh[i,:num_steps].reshape(-1,1)\n",
    "    mean_train_loss_swish = mean_train_loss_swish + train_loss_swish[i,:num_steps].reshape(-1,1)\n",
    "    mean_train_loss_rowdy = mean_train_loss_rowdy + train_loss_rowdy[i,:num_steps].reshape(-1,1)\n",
    "    \n",
    "        \n",
    "mean_train_loss_stan = mean_train_loss_stan/10\n",
    "mean_train_loss_tanh = mean_train_loss_tanh/10\n",
    "mean_train_loss_atanh = mean_train_loss_atanh/10\n",
    "mean_train_loss_swish = mean_train_loss_swish/10\n",
    "mean_train_loss_rowdy = mean_train_loss_rowdy/10\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mean_train_loss_stan,'r',linewidth = 2,label = 'Stan (Proposed)')\n",
    "ax.plot(mean_train_loss_tanh,'k:.', linewidth = 2,label = 'tanh')\n",
    "ax.plot(mean_train_loss_atanh,'b--',linewidth = 2, label = 'N-LAAF')\n",
    "ax.plot(mean_train_loss_swish,'c-.',linewidth = 2,label = 'Swish')\n",
    "ax.plot(mean_train_loss_rowdy,'m:',linewidth = 2, label = 'Rowdy')\n",
    "\n",
    "#ax.set_fontvariant('normal')\n",
    "\n",
    "fig.dpi=300\n",
    "fig.tight_layout()\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.legend(prop={'size':14})\n",
    "#ax.set_ylim([0,5])\n",
    "ax.set_title('Training Loss', fontsize=14)\n",
    "ax.set_xlabel('Epochs',fontsize=14,alpha = 1)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    " \n",
    "fig.savefig(prob_name+'train_loss.eps', format='eps',pad_inches=0, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_stan =  data_stan['test_re_loss']\n",
    "test_loss_tanh =  data_tanh['test_re_loss']\n",
    "test_loss_atanh =  data_atanh['test_re_loss']\n",
    "test_loss_swish =  data_swish['test_re_loss']\n",
    "test_loss_rowdy =  data_rowdy['test_re_loss']\n",
    "\n",
    "\n",
    "mean_test_loss_stan = np.zeros((num_steps,1))\n",
    "mean_test_loss_tanh = np.zeros((num_steps,1))\n",
    "mean_test_loss_atanh = np.zeros((num_steps,1))\n",
    "mean_test_loss_swish = np.zeros((num_steps,1))\n",
    "mean_test_loss_rowdy = np.zeros((num_steps,1))\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    mean_test_loss_stan = mean_test_loss_stan + test_loss_stan[i,:num_steps].reshape(-1,1)\n",
    "    mean_test_loss_tanh = mean_test_loss_tanh + test_loss_tanh[i,:num_steps].reshape(-1,1)\n",
    "    mean_test_loss_atanh = mean_test_loss_atanh + test_loss_atanh[i,:num_steps].reshape(-1,1)\n",
    "    mean_test_loss_swish = mean_test_loss_swish + test_loss_swish[i,:num_steps].reshape(-1,1)\n",
    "    mean_test_loss_rowdy = mean_test_loss_rowdy + test_loss_rowdy[i,:num_steps].reshape(-1,1)\n",
    "    \n",
    "        \n",
    "mean_test_loss_stan = mean_test_loss_stan/10\n",
    "mean_test_loss_tanh = mean_test_loss_tanh/10\n",
    "mean_test_loss_atanh = mean_test_loss_atanh/10\n",
    "mean_test_loss_swish = mean_test_loss_swish/10\n",
    "mean_test_loss_rowdy = mean_test_loss_rowdy/10\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mean_test_loss_stan,'r',linewidth = 2,label = 'Stan (Proposed)')\n",
    "ax.plot(mean_test_loss_tanh,'k:.',linewidth = 2, label = 'tanh')\n",
    "ax.plot(mean_test_loss_atanh,'b--',linewidth = 2, label = 'N-LAAF')\n",
    "ax.plot(mean_test_loss_swish,'c-.',linewidth = 2,label = 'Swish')\n",
    "ax.plot(mean_test_loss_rowdy,'m:',linewidth = 2, label = 'Rowdy')\n",
    "\n",
    "#ax.set_fontvariant('normal')\n",
    "\n",
    "fig.dpi=300\n",
    "fig.tight_layout()\n",
    "\n",
    "#ax.set_yscale('log')\n",
    "ax.legend(prop={'size':14})\n",
    "#ax.set_ylim([0,6])\n",
    "ax.set_title('Test Loss', fontsize=14)\n",
    "ax.set_xlabel('Epochs',fontsize=14,alpha = 1)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    " \n",
    "fig.savefig(prob_name + 're.eps', format='eps',pad_inches=0, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_test_loss_stan[-1])\n",
    "print(mean_test_loss_tanh[-1])\n",
    "print(mean_test_loss_atanh[-1])\n",
    "print(mean_test_loss_swish[-1])\n",
    "print(mean_test_loss_rowdy[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_stan[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_swish[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_rowdy[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_tanhxtanh =  data_stan['beta']\n",
    "\n",
    "beta_history = beta_tanhxtanh[0,:num_steps,20,7] #For the first repetition\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(beta_history,'r')\n",
    "\n",
    "fig.dpi=300\n",
    "fig.tight_layout()\n",
    "\n",
    "ax.set_title('Value', fontsize=14, math_fontfamily='cm')\n",
    "ax.set_xlabel('Epochs', fontsize=14, math_fontfamily='cm')\n",
    "ax.set_ylabel('$\\\\beta $',fontsize=18, math_fontfamily='cm')\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "plt.savefig(prob_name + 'beta_history.eps', format='eps',pad_inches=0, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel_stan(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "\n",
    "    \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/3000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "       \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re \n",
    "\n",
    "class Sequentialmodel_tanh(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "    \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)         \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/3000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "       \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re\n",
    "    \n",
    "class Sequentialmodel_swish(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.iter = 0\n",
    "\n",
    "    \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/3000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "       \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re \n",
    "    \n",
    "class Sequentialmodel_atanh(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "\n",
    "    \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/3000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "       \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re\n",
    "\n",
    "class Sequentialmodel_rowdy(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "\n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        self.omega1.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        self.omega.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "                \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/3000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    def test(self): \n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "layers = np.array([3,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "beta_init = 1.0\n",
    "\n",
    "n_val_atanh = 1.0\n",
    "\n",
    "n_val_rowdy = 1.0\n",
    "rowdy_terms = 6\n",
    "\n",
    "\n",
    "model_stan = Sequentialmodel_stan(layers,beta_init)\n",
    "model_tanh = Sequentialmodel_tanh(layers)\n",
    "model_atanh = Sequentialmodel_atanh(layers,n_val_atanh)\n",
    "model_swish = Sequentialmodel_swish(layers)\n",
    "model_rowdy = Sequentialmodel_rowdy(layers,n_val_rowdy,rowdy_terms)\n",
    "\n",
    "model_stan.to(device)\n",
    "model_tanh.to(device)\n",
    "model_atanh.to(device)\n",
    "model_swish.to(device)\n",
    "model_rowdy.to(device)\n",
    "\n",
    "u_pred_stan = np.zeros((xyt.shape[0],1))\n",
    "u_pred_tanh = np.zeros((xyt.shape[0],1))\n",
    "u_pred_atanh = np.zeros((xyt.shape[0],1))\n",
    "u_pred_swish = np.zeros((xyt.shape[0],1))\n",
    "u_pred_rowdy = np.zeros((xyt.shape[0],1))\n",
    "\n",
    "models_n = 1\n",
    "\n",
    "for i in range(5,6):\n",
    "    model_stan.load_state_dict(torch.load('./stan/' + prob_name + 'stan_' +str(i)+'.pt'))\n",
    "    u_pred_stan = u_pred_stan + model_stan.test()\n",
    "    del model_stan\n",
    "    \n",
    "    model_tanh.load_state_dict(torch.load('./tanh/' + prob_name + 'tanh_' +str(i)+'.pt'))\n",
    "    u_pred_tanh = u_pred_tanh + model_tanh.test()\n",
    "    del model_tanh\n",
    "    \n",
    "    model_rowdy.load_state_dict(torch.load('./rowdy/' + prob_name + 'rowdy_'+str(i)+'.pt'))\n",
    "    u_pred_rowdy = u_pred_rowdy + model_rowdy.test()\n",
    "    del model_rowdy\n",
    "    \n",
    "    model_swish.load_state_dict(torch.load('./swish/' + prob_name + 'swish_'+str(i)+'.pt'))\n",
    "    u_pred_swish = u_pred_swish + model_swish.test()\n",
    "    del model_swish\n",
    "\n",
    "    model_atanh.load_state_dict(torch.load('./atanh/' + prob_name + 'atanh_' +str(i)+'.pt',map_location = device))\n",
    "    u_pred_atanh = u_pred_atanh + model_atanh.test()\n",
    "    del model_atanh\n",
    "    \n",
    "u_pred_stan = u_pred_stan/models_n\n",
    "u_pred_tanh = u_pred_tanh/models_n\n",
    "u_pred_atanh = u_pred_atanh/models_n\n",
    "u_pred_swish = u_pred_swish/models_n\n",
    "u_pred_rowdy = u_pred_rowdy/models_n\n",
    "\n",
    "u_pred = np.hstack((u_pred_stan,u_pred_tanh,u_pred_atanh,u_pred_swish,u_pred_rowdy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyt_test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred_stan_3d  = u_pred_stan.reshape(100,100,100,order = 'F')\n",
    "u_pred_tanh_3d  = u_pred_tanh.reshape(100,100,100,order = 'F')\n",
    "u_pred_atanh_3d  = u_pred_atanh.reshape(100,100,100,order = 'F')\n",
    "u_pred_swish_3d  = u_pred_swish.reshape(100,100,100,order = 'F')\n",
    "u_pred_rowdy_3d  = u_pred_rowdy.reshape(100,100,100,order = 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdic = {\"u_pred\": u_pred}\n",
    "savemat('HTTP_U_pred.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdic = {\"u_pred_stan_3d\": u_pred_stan_3d,\"u_pred_tanh_3d\": u_pred_tanh_3d,\"u_pred_atanh_3d\": u_pred_atanh_3d,\"u_pred_swish_3d\": u_pred_swish_3d,\"u_pred_rowdy_3d\": u_pred_rowdy_3d}\n",
    "savemat('HTTP_U_pred_3D.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, combinations\n",
    "from scipy.interpolate import griddata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\"Stan (Proposed)\",\"tanh\",\"N-LAAF\",\"Swish\",\"Rowdy\"]\n",
    "fig = plt.figure(figsize=plt.figaspect(0.2))\n",
    "#fig,ax = plt.subplots(1,5)\n",
    "\n",
    "r1 = [lb_xyt[0],ub_xyt[0]]\n",
    "r2 = [lb_xyt[2],3000*ub_xyt[2]]\n",
    "r3 = [lb_xyt[1],ub_xyt[1]]\n",
    "fig.dpi=300\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    u_pred_3d = u_pred[:,i].reshape(100,100,100,order = 'F')\n",
    "    ax = fig.add_subplot(1,5,i+1,projection='3d')\n",
    "\n",
    "\n",
    "    ax.grid(False)\n",
    "\n",
    "    X_plot, Y_plot = np.meshgrid(x,y)\n",
    "    X1 = np.hstack((X_plot.flatten('F').reshape(-1,1),Y_plot.flatten('F').reshape(-1,1)))\n",
    "\n",
    "\n",
    "    u_pred_plot = u_pred_3d[:,:,0] \n",
    "    UU_star = np.transpose(griddata(X1, u_pred_plot.flatten(), (X_plot, Y_plot), method='cubic'))\n",
    "    ax.contourf(X_plot,UU_star,Y_plot, zdir = 'y', offset = 0, cmap='jet', alpha = 0.8)\n",
    "\n",
    "\n",
    "    for n in range(4):\n",
    "        u_pred_plot = u_pred_3d[:,:,99-n*25] \n",
    "        UU_star = np.transpose(griddata(X1, u_pred_plot.flatten(), (X_plot, Y_plot), method='cubic'))\n",
    "        ax.contourf(X_plot,UU_star,Y_plot, zdir = 'y', offset = 3000 - n*750, cmap='jet', alpha = 0.8)\n",
    "\n",
    "    for s, e in combinations(np.array(list(product(r1,r2,r3))), 2):\n",
    "        if np.sum(np.abs(s-e)) == r1[1]-r1[0] or np.sum(np.abs(s-e)) == r2[1]-r2[0] or np.sum(np.abs(s-e)) == r3[1]-r3[0]:\n",
    "            ax.plot3D(*zip(s,e), color=\"k\", linewidth = 1)       \n",
    "\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_zticklabels([])\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_yticks([0,750,1500,2250,3000])\n",
    "    y_ticklabels = [\"$t = 0$\",\"$t = 750$\",\"$t = 1500$\",\"$t = 2250$\",\"$t =3000$\"] \n",
    "    ax.set_yticklabels(y_ticklabels,rotation=15,verticalalignment='baseline',horizontalalignment='left',math_fontfamily='cm')\n",
    "\n",
    "    tmp_planes = ax.zaxis._PLANES \n",
    "    ax.zaxis._PLANES = ( tmp_planes[2], tmp_planes[3], \n",
    "                         tmp_planes[0], tmp_planes[1], \n",
    "                         tmp_planes[4], tmp_planes[5])\n",
    "\n",
    "    ax.set_xlim3d(r1)\n",
    "    ax.set_ylim3d(r2)\n",
    "    ax.set_zlim3d(r3)\n",
    "    ax.set_box_aspect((1,3, 1))\n",
    "\n",
    "\n",
    "    ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "\n",
    "    ax.text(0.35, 0, -0.4, '$x$',math_fontfamily='cm',fontsize = 12)\n",
    "    ax.text(-0.35, 0, 0.4, '$y$',math_fontfamily='cm',fontsize = 12)\n",
    "    \n",
    "    ax.text(1.2, 0, 2.7,titles[i],math_fontfamily='cm',fontsize = 12)\n",
    "\n",
    "\n",
    "plt.savefig(prob_name + 'prediction.pdf', format='pdf',pad_inches=0, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\"Stan (Proposed)\",\"tanh\",\"N-LAAF\",\"Swish\",\"Rowdy\"]\n",
    "fig = plt.figure(figsize=plt.figaspect(0.2))\n",
    "#fig,ax = plt.subplots(1,5)\n",
    "\n",
    "r1 = [lb_xyt[0],ub_xyt[0]]\n",
    "r2 = [lb_xyt[2],3000*ub_xyt[2]]\n",
    "r3 = [lb_xyt[1],ub_xyt[1]]\n",
    "fig.dpi=300\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    u_pred_3d = u_pred[:,i].reshape(100,100,100,order = 'F')\n",
    "    ax = fig.add_subplot(1,5,i+1,projection='3d')\n",
    "\n",
    "\n",
    "    ax.grid(False)\n",
    "\n",
    "    X_plot, Y_plot = np.meshgrid(x,y)\n",
    "    X1 = np.hstack((X_plot.flatten('F').reshape(-1,1),Y_plot.flatten('F').reshape(-1,1)))\n",
    "\n",
    "\n",
    "    u_pred_plot = u_pred_3d[:,:,0] \n",
    "    UU_star = np.transpose(griddata(X1, u_pred_plot.flatten(), (X_plot, Y_plot), method='cubic'))\n",
    "    ax.contourf(X_plot,UU_star,Y_plot, zdir = 'y', offset = 0, cmap='jet', alpha = 0.8)\n",
    "\n",
    "\n",
    "    for n in range(4):\n",
    "        u_pred_plot = u_pred_3d[:,:,99-n*25] \n",
    "        UU_star = np.transpose(griddata(X1, u_pred_plot.flatten(), (X_plot, Y_plot), method='cubic'))\n",
    "        ax.contourf(X_plot,UU_star,Y_plot, zdir = 'y', offset = 3000 - n*750, cmap='jet', alpha = 0.8)\n",
    "\n",
    "    for s, e in combinations(np.array(list(product(r1,r2,r3))), 2):\n",
    "        if np.sum(np.abs(s-e)) == r1[1]-r1[0] or np.sum(np.abs(s-e)) == r2[1]-r2[0] or np.sum(np.abs(s-e)) == r3[1]-r3[0]:\n",
    "            ax.plot3D(*zip(s,e), color=\"k\", linewidth = 1)       \n",
    "\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_zticklabels([])\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_yticks([0,750,1500,2250,3000])\n",
    "    y_ticklabels = [\"$t = 0$\",\"$t = 750$\",\"$t = 1500$\",\"$t = 2250$\",\"$t =3000$\"] \n",
    "    ax.set_yticklabels(y_ticklabels,rotation=15,verticalalignment='baseline',horizontalalignment='left',math_fontfamily='cm')\n",
    "\n",
    "    tmp_planes = ax.zaxis._PLANES \n",
    "    ax.zaxis._PLANES = ( tmp_planes[2], tmp_planes[3], \n",
    "                         tmp_planes[0], tmp_planes[1], \n",
    "                         tmp_planes[4], tmp_planes[5])\n",
    "\n",
    "    ax.set_xlim3d(r1)\n",
    "    ax.set_ylim3d(r2)\n",
    "    ax.set_zlim3d(r3)\n",
    "    ax.set_box_aspect((1,3, 1))\n",
    "\n",
    "\n",
    "    ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "\n",
    "    ax.text(0.35, 0, -0.4, '$x$',math_fontfamily='cm',fontsize = 12)\n",
    "    ax.text(-0.35, 0, 0.4, '$y$',math_fontfamily='cm',fontsize = 12)\n",
    "    \n",
    "    ax.text(1.2, 0, 2.7,titles[i],math_fontfamily='cm',fontsize = 12)\n",
    "\n",
    "\n",
    "plt.savefig(prob_name + 'test.pdf', format='pdf',pad_inches=0, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = np.array([3,50,50,50,50,50,50,50,50,50,1])\n",
    "device = 'cpu'\n",
    "beta_init = 1.0\n",
    "\n",
    "model_stan = Sequentialmodel(layers,beta_init)\n",
    "model_stan.load_state_dict(torch.load('./stan/3D_HTTP_stan_8.pt'))\n",
    "xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "\n",
    "\n",
    "u_pred = model_stan.test()\n",
    "u_pred_3d = u_pred.reshape(100,100,100,order = 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.flipud(u_pred_3d[:,:,50]),vmax =1000,vmin=300,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n",
    "fig.colorbar(img3, orientation='vertical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "for i in range(100):\n",
    "    #plt.imshow(img[i], cmap=cm.Greys_r)\n",
    "    #fig, ax = \n",
    "    plt.subplots(1,1)\n",
    "    img3 = plt.imshow(np.flipud(u_pred_3d[:,:,i]),vmax =1000,vmin=300,cmap = cmap,extent=[0,1,0,1],aspect = 1)\n",
    "    plt.colorbar(img3, orientation='vertical')\n",
    "    plt.savefig(\"stan_heat_%02d.eps\" % i,pad_inches=0, bbox_inches='tight')\n",
    "\n",
    "video_name = '3D_HTTP_stan_8.mp4'\n",
    "# fourcc = cv2.CV_FOURCC(*'MP4V')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "\n",
    "images = []\n",
    "for i in range(100):\n",
    "    images.append(\"stan_heat_%02d.eps\" % i)\n",
    "#frame = cv2.imread(os.path.join(image_folder, images[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = cv2.imread(images[0])\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "video = cv2.VideoWriter(video_name, fourcc, 10, (width,height))\n",
    "# video._fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "\n",
    "for image in images:\n",
    "    video.write(cv2.imread(image))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relative Error\n",
    "print('Relative Error')\n",
    "print(mean_test_loss_stan[-1])\n",
    "print(mean_test_loss_tanh[-1])\n",
    "print(mean_test_loss_atanh[-1])\n",
    "print(mean_test_loss_swish[-1])\n",
    "print(mean_test_loss_rowdy[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_stan =  data_stan['test_mse_loss']\n",
    "test_loss_tanh =  data_tanh['test_mse_loss']\n",
    "test_loss_atanh =  data_atanh['test_mse_loss']\n",
    "test_loss_swish =  data_swish['test_mse_loss']\n",
    "test_loss_rowdy =  data_rowdy['test_mse_loss']\n",
    "\n",
    "\n",
    "mean_test_loss_stan = np.zeros((num_steps,1))\n",
    "mean_test_loss_tanh = np.zeros((num_steps,1))\n",
    "mean_test_loss_atanh = np.zeros((num_steps,1))\n",
    "mean_test_loss_swish = np.zeros((num_steps,1))\n",
    "mean_test_loss_rowdy = np.zeros((num_steps,1))\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    mean_test_loss_stan = mean_test_loss_stan + test_loss_stan[i,:num_steps].reshape(-1,1)\n",
    "    mean_test_loss_tanh = mean_test_loss_tanh + test_loss_tanh[i,:num_steps].reshape(-1,1)\n",
    "    mean_test_loss_atanh = mean_test_loss_atanh + test_loss_atanh[i,:num_steps].reshape(-1,1)\n",
    "    mean_test_loss_swish = mean_test_loss_swish + test_loss_swish[i,:num_steps].reshape(-1,1)\n",
    "    mean_test_loss_rowdy = mean_test_loss_rowdy + test_loss_rowdy[i,:num_steps].reshape(-1,1)\n",
    "    \n",
    "        \n",
    "mean_test_loss_stan = mean_test_loss_stan/10\n",
    "mean_test_loss_tanh = mean_test_loss_tanh/10\n",
    "mean_test_loss_atanh = mean_test_loss_atanh/10\n",
    "mean_test_loss_swish = mean_test_loss_swish/10\n",
    "mean_test_loss_rowdy = mean_test_loss_rowdy/10\n",
    "\n",
    "print('MSE')\n",
    "print(mean_test_loss_stan[-1])\n",
    "print(mean_test_loss_tanh[-1])\n",
    "print(mean_test_loss_atanh[-1])\n",
    "print(mean_test_loss_swish[-1])\n",
    "print(mean_test_loss_rowdy[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time')\n",
    "print(np.mean(data_stan['Time']))\n",
    "print(np.mean(data_tanh['Time']))\n",
    "print(np.mean(data_atanh['Time']))\n",
    "print(np.mean(data_swish['Time']))\n",
    "print(np.mean(data_rowdy['Time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = np.max(np.vstack((data_stan['Time'],data_tanh['Time'],data_atanh['Time'],data_swish['Time'],data_rowdy['Time'])))\n",
    "\n",
    "ttime_stan = data_stan['Thresh Time']\n",
    "ttime_tanh = data_tanh['Thresh Time']\n",
    "ttime_atanh = data_atanh['Thresh Time']\n",
    "ttime_swish = data_swish['Thresh Time']\n",
    "ttime_rowdy = data_rowdy['Thresh Time']\n",
    "\n",
    "ttime_stan[np.isnan(ttime_stan)] = max_time\n",
    "ttime_tanh[np.isnan(ttime_tanh)] = max_time\n",
    "ttime_atanh[np.isnan(ttime_atanh)] = max_time\n",
    "ttime_swish[np.isnan(ttime_swish)] = max_time\n",
    "ttime_rowdy[np.isnan(ttime_rowdy)] = max_time\n",
    "\n",
    "print('Threshold Time')\n",
    "print(np.mean(ttime_stan))\n",
    "print(np.mean(ttime_tanh))\n",
    "print(np.mean(ttime_atanh))\n",
    "print(np.mean(ttime_swish))\n",
    "print(np.mean(ttime_rowdy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tep_stan = data_stan['Thresh epoch']\n",
    "tep_tanh = data_tanh['Thresh epoch']\n",
    "tep_atanh = data_atanh['Thresh epoch']\n",
    "tep_swish = data_swish['Thresh epoch']\n",
    "tep_rowdy = data_rowdy['Thresh epoch']\n",
    "\n",
    "print('Threshold Epoch')\n",
    "print(np.mean(tep_stan))\n",
    "print(np.mean(tep_tanh))\n",
    "print(np.mean(tep_atanh))\n",
    "print(np.mean(tep_swish))\n",
    "print(np.mean(tep_rowdy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
