{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLsZ-c_nCQr2",
    "outputId": "0238c820-5951-4e75-a35b-19e4de8c9b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SV23gJi7JexL",
    "outputId": "6f051579-557f-463f-d7b4-955ed617736e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOyXTKXGJf97",
    "outputId": "11b7b7db-47b0-4cf8-c699-473f1c6b8c5f"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/MURI Aug17 Thin Plate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "APjvgycyCTj0",
    "outputId": "19bce659-211e-4bec-d94d-7c94148b0d09"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lxFUD2gACQr7"
   },
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CUcT7YuXCQr7"
   },
   "outputs": [],
   "source": [
    "label = \"3D_HTTP_swish\"\n",
    "loss_thresh = 20000\n",
    "x = np.linspace(0,1,100).reshape(-1,1)\n",
    "y = np.linspace(0,1,100).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xyt = np.hstack((X,Y,T))\n",
    "\n",
    "initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "xyt_initial = xyt[initial_pts,:]\n",
    "xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "xyt_NBC_x0 = xyt[NBC_pts_x0,:]\n",
    "xyt_NBC_x1 = xyt[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xyt_NBC_y1 = xyt[NBC_pts_y1,:]\n",
    "\n",
    "u_initial = 300*np.ones((np.shape(xyt_initial)[0],1))\n",
    "u_DBC = 1000*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xyt_NBC_x = np.vstack((xyt_NBC_x0,xyt_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xyt_NBC_y = np.vstack((xyt_NBC_y1))\n",
    "\n",
    "u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../3D_HTTP_FEA.mat')\n",
    "xy = fea_data['xy']\n",
    "t = fea_data['t']/3000\n",
    "xyt = np.zeros((497*101,3))\n",
    "u_true = np.ones((497*101,1))\n",
    "\n",
    "\n",
    "for i in range(101):\n",
    "    t_temp = t[0,i]*np.ones((497,1))\n",
    "    xyt[497*i:497*(i+1)] = np.hstack((xy,t_temp))\n",
    "    u_true[497*i:497*(i+1)] = fea_data['u'][:,i].reshape(-1,1)\n",
    "    #print(i)\n",
    "#print(xyt)\n",
    "\n",
    "xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gp2G6x6BCQr8"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xyt_I_DBC.shape[0], N_D, replace=False) \n",
    "    xyt_D = xyt_I_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_I_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_x.shape[0], N_D, replace=False) \n",
    "    xyt_Nx = xyt_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_y.shape[0], N_D, replace=False) \n",
    "    xyt_Ny = xyt_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xyt_coll = lb_xyt + (ub_xyt - lb_xyt)*samples\n",
    "    xyt_coll = np.vstack((xyt_coll, xyt_D,xyt_Nx,xyt_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xyt_coll, xyt_D, u_D, xyt_Nx,xyt_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VRolFlBzCQr9"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.iter = 0\n",
    "\n",
    "    \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/3000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "       \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xyt_coll_np_array, xyt_D_np_array, u_D_np_array,xyt_Nx_np_array,xyt_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "\n",
    "    xyt_coll = torch.from_numpy(xyt_coll_np_array).float().to(device)\n",
    "    xyt_D = torch.from_numpy(xyt_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xyt_Nx = torch.from_numpy(xyt_Nx_np_array).float().to(device)\n",
    "    xyt_Ny = torch.from_numpy(xyt_Ny_np_array).float().to(device)\n",
    "\n",
    "    N_hat = torch.zeros(xyt_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xyt_coll.shape[0],1).to(device)\n",
    "\n",
    "\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,i)\n",
    "\n",
    "        loss_np = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVnXJfj0CQr-",
    "outputId": "1f2921b0-e258-465d-aa27-cdeb80b78a0b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D_HTTP_swish\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 243755.5 Test MSE 85893.8507646575 Test RE 0.5195416853746291\n",
      "1 Train Loss 211980.2 Test MSE 66881.09219846984 Test RE 0.458449011406987\n",
      "2 Train Loss 196338.3 Test MSE 57818.99460653636 Test RE 0.4262600484870081\n",
      "3 Train Loss 187880.11 Test MSE 50438.01359747613 Test RE 0.39812399135910487\n",
      "4 Train Loss 183611.64 Test MSE 48401.31845139219 Test RE 0.3900030093877438\n",
      "5 Train Loss 181366.77 Test MSE 47776.66344613499 Test RE 0.3874781973707627\n",
      "6 Train Loss 173460.67 Test MSE 43521.68519884469 Test RE 0.3698215494571332\n",
      "7 Train Loss 166276.19 Test MSE 41219.2850274155 Test RE 0.35990641648416655\n",
      "8 Train Loss 159204.45 Test MSE 38390.64680399122 Test RE 0.3473378220698911\n",
      "9 Train Loss 150686.94 Test MSE 35110.3318808397 Test RE 0.3321672641736547\n",
      "10 Train Loss 140942.9 Test MSE 30548.29406378611 Test RE 0.3098366893332207\n",
      "11 Train Loss 133318.8 Test MSE 27727.032301721832 Test RE 0.2951828057053503\n",
      "12 Train Loss 126306.59 Test MSE 25161.73417902232 Test RE 0.2811963293067725\n",
      "13 Train Loss 121035.05 Test MSE 22891.62764893501 Test RE 0.2682116876098065\n",
      "14 Train Loss 115514.56 Test MSE 20189.57832485582 Test RE 0.2518853948717026\n",
      "15 Train Loss 110013.664 Test MSE 18390.188146045326 Test RE 0.240398884665982\n",
      "16 Train Loss 104375.08 Test MSE 15833.746443229342 Test RE 0.22306488316177636\n",
      "17 Train Loss 100555.64 Test MSE 14229.021225409242 Test RE 0.21145934167498504\n",
      "18 Train Loss 98216.37 Test MSE 14653.615218253053 Test RE 0.21459112369088307\n",
      "19 Train Loss 93983.766 Test MSE 13193.208074614506 Test RE 0.20361725021944513\n",
      "20 Train Loss 88990.59 Test MSE 13041.345007346396 Test RE 0.20244196975944204\n",
      "21 Train Loss 85519.39 Test MSE 13647.702271949518 Test RE 0.2070947703556956\n",
      "22 Train Loss 80287.945 Test MSE 10966.289135706907 Test RE 0.18563899488389313\n",
      "23 Train Loss 73578.18 Test MSE 9991.547838882032 Test RE 0.17719674841040448\n",
      "24 Train Loss 69259.73 Test MSE 9426.716425363446 Test RE 0.1721153417877449\n",
      "25 Train Loss 66610.55 Test MSE 8637.29733458729 Test RE 0.16475109027642096\n",
      "26 Train Loss 64793.883 Test MSE 8122.796092151719 Test RE 0.15976886052504932\n",
      "27 Train Loss 61327.906 Test MSE 8428.753417691822 Test RE 0.16275001474776923\n",
      "28 Train Loss 55658.195 Test MSE 7030.871257858997 Test RE 0.148642819663375\n",
      "29 Train Loss 54668.85 Test MSE 6938.187993348631 Test RE 0.1476598400819871\n",
      "30 Train Loss 53454.305 Test MSE 6676.281424856367 Test RE 0.14484605782002427\n",
      "31 Train Loss 51285.09 Test MSE 6513.3753072185145 Test RE 0.14306796964606527\n",
      "32 Train Loss 47887.457 Test MSE 6008.601112537787 Test RE 0.1374124393001003\n",
      "33 Train Loss 46419.402 Test MSE 6123.534980578866 Test RE 0.13872044194701938\n",
      "34 Train Loss 44284.95 Test MSE 5355.324219641451 Test RE 0.12972755908695516\n",
      "35 Train Loss 42888.586 Test MSE 5491.069981743044 Test RE 0.13136142516379237\n",
      "36 Train Loss 41714.766 Test MSE 5623.245107762649 Test RE 0.13293301938476415\n",
      "37 Train Loss 40808.59 Test MSE 5198.444644659596 Test RE 0.12781330767774593\n",
      "38 Train Loss 39847.832 Test MSE 5206.301609480379 Test RE 0.12790986016307015\n",
      "39 Train Loss 39372.113 Test MSE 5099.148546061031 Test RE 0.1265867337419422\n",
      "40 Train Loss 38754.41 Test MSE 4943.003863225667 Test RE 0.12463351318635214\n",
      "41 Train Loss 38467.402 Test MSE 4936.962250395741 Test RE 0.12455732291019693\n",
      "42 Train Loss 38023.21 Test MSE 4858.098566120052 Test RE 0.12355847037251987\n",
      "43 Train Loss 37628.5 Test MSE 4829.136529821486 Test RE 0.1231896167752341\n",
      "44 Train Loss 37167.21 Test MSE 4810.30015956963 Test RE 0.12294912736214533\n",
      "45 Train Loss 36722.145 Test MSE 4872.733649792516 Test RE 0.1237444411491689\n",
      "46 Train Loss 36392.53 Test MSE 4940.183667554989 Test RE 0.12459795373166617\n",
      "47 Train Loss 36111.23 Test MSE 4881.326845393374 Test RE 0.12385350639474044\n",
      "48 Train Loss 35637.773 Test MSE 4858.711235432325 Test RE 0.12356626129066256\n",
      "49 Train Loss 35236.336 Test MSE 4698.380064904926 Test RE 0.1215103955937535\n",
      "50 Train Loss 34947.094 Test MSE 4646.6871370292165 Test RE 0.12084010065171426\n",
      "51 Train Loss 34623.953 Test MSE 4614.336264675439 Test RE 0.12041871326475691\n",
      "52 Train Loss 34448.54 Test MSE 4677.251886079706 Test RE 0.12123687726833977\n",
      "53 Train Loss 34276.312 Test MSE 4586.301694535604 Test RE 0.12005235178974484\n",
      "54 Train Loss 34046.742 Test MSE 4647.398066231686 Test RE 0.12084934438473903\n",
      "55 Train Loss 33815.973 Test MSE 4605.4363490868045 Test RE 0.12030252822861154\n",
      "56 Train Loss 33487.953 Test MSE 4596.06262103902 Test RE 0.12018003629809705\n",
      "57 Train Loss 33279.246 Test MSE 4562.534006049925 Test RE 0.11974087279581934\n",
      "58 Train Loss 32871.63 Test MSE 4573.29259081228 Test RE 0.11988196585878\n",
      "59 Train Loss 32668.576 Test MSE 4565.263487526294 Test RE 0.11977668421419264\n",
      "60 Train Loss 32491.697 Test MSE 4541.366399586056 Test RE 0.11946278453140666\n",
      "61 Train Loss 32305.953 Test MSE 4653.788260418798 Test RE 0.1209324000523662\n",
      "62 Train Loss 32144.887 Test MSE 4605.930284749055 Test RE 0.12030897931286062\n",
      "63 Train Loss 32055.793 Test MSE 4601.14538267926 Test RE 0.12024647117280475\n",
      "64 Train Loss 31926.525 Test MSE 4648.476229734538 Test RE 0.12086336166866636\n",
      "65 Train Loss 31812.363 Test MSE 4610.841126796802 Test RE 0.12037309893247017\n",
      "66 Train Loss 31751.648 Test MSE 4640.768154643474 Test RE 0.12076311264670378\n",
      "67 Train Loss 31640.441 Test MSE 4654.560446293163 Test RE 0.12094243256915951\n",
      "68 Train Loss 31565.027 Test MSE 4657.338962702216 Test RE 0.12097852517155001\n",
      "69 Train Loss 31430.396 Test MSE 4696.386337426938 Test RE 0.12148461177923801\n",
      "70 Train Loss 31338.312 Test MSE 4662.499388725495 Test RE 0.12104552994571667\n",
      "71 Train Loss 31249.094 Test MSE 4628.509212927448 Test RE 0.1206035047036673\n",
      "72 Train Loss 31179.578 Test MSE 4672.764315522144 Test RE 0.12117870319351641\n",
      "73 Train Loss 31152.445 Test MSE 4682.240128144744 Test RE 0.12130150899540475\n",
      "74 Train Loss 31122.69 Test MSE 4687.722659725044 Test RE 0.12137250543860569\n",
      "75 Train Loss 31083.572 Test MSE 4708.247188600596 Test RE 0.12163792138569411\n",
      "76 Train Loss 31066.047 Test MSE 4684.282438980006 Test RE 0.12132796090419312\n",
      "77 Train Loss 31023.18 Test MSE 4702.271651194955 Test RE 0.12156070764880961\n",
      "78 Train Loss 31004.584 Test MSE 4709.523845478528 Test RE 0.12165441153131719\n",
      "79 Train Loss 30987.318 Test MSE 4692.162789961604 Test RE 0.12142997280808215\n",
      "80 Train Loss 30971.47 Test MSE 4700.0763762611805 Test RE 0.12153232877710886\n",
      "81 Train Loss 30959.178 Test MSE 4698.5798521651495 Test RE 0.1215129790342822\n",
      "82 Train Loss 30931.951 Test MSE 4733.79890756438 Test RE 0.12196754007331549\n",
      "83 Train Loss 30900.39 Test MSE 4688.886486823415 Test RE 0.12138757115971129\n",
      "84 Train Loss 30887.938 Test MSE 4693.978590369402 Test RE 0.12145346637344456\n",
      "85 Train Loss 30863.38 Test MSE 4696.88273687498 Test RE 0.12149103196025114\n",
      "86 Train Loss 30838.383 Test MSE 4665.854001414409 Test RE 0.12108906752332464\n",
      "87 Train Loss 30811.371 Test MSE 4668.111219273553 Test RE 0.12111835384013563\n",
      "88 Train Loss 30743.686 Test MSE 4677.497448454815 Test RE 0.12124005978079692\n",
      "89 Train Loss 30717.088 Test MSE 4629.281631043927 Test RE 0.12061356760312779\n",
      "90 Train Loss 30684.752 Test MSE 4623.78661185234 Test RE 0.12054196138873398\n",
      "91 Train Loss 30660.334 Test MSE 4652.561067409242 Test RE 0.12091645420646302\n",
      "92 Train Loss 30645.174 Test MSE 4660.7042015903435 Test RE 0.1210222248167981\n",
      "93 Train Loss 30624.488 Test MSE 4660.815678978954 Test RE 0.12102367214753731\n",
      "94 Train Loss 30597.572 Test MSE 4710.364414352405 Test RE 0.12166526765526958\n",
      "95 Train Loss 30565.982 Test MSE 4724.489361050914 Test RE 0.1218475496266549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 30521.314 Test MSE 4725.277133036145 Test RE 0.12185770777050342\n",
      "97 Train Loss 30433.447 Test MSE 4675.946597644751 Test RE 0.12121995919979299\n",
      "98 Train Loss 30339.951 Test MSE 4651.239589981686 Test RE 0.12089928090014711\n",
      "99 Train Loss 30277.746 Test MSE 4605.823391209542 Test RE 0.12030758325099535\n",
      "Training time: 557.76\n",
      "3D_HTTP_swish\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 239569.98 Test MSE 84823.7036986714 Test RE 0.5162950697659173\n",
      "1 Train Loss 212601.73 Test MSE 68303.06941748758 Test RE 0.4632969834197875\n",
      "2 Train Loss 193862.25 Test MSE 58448.49317691538 Test RE 0.4285741987729053\n",
      "3 Train Loss 182524.84 Test MSE 50826.41594473774 Test RE 0.3996539459762747\n",
      "4 Train Loss 171469.36 Test MSE 45498.9982435635 Test RE 0.3781292555149256\n",
      "5 Train Loss 165323.64 Test MSE 41765.23475171736 Test RE 0.36228205754203696\n",
      "6 Train Loss 157833.5 Test MSE 38541.3558786327 Test RE 0.3480189214059533\n",
      "7 Train Loss 144737.77 Test MSE 33782.827927780156 Test RE 0.3258272198440463\n",
      "8 Train Loss 126776.48 Test MSE 26455.07672952664 Test RE 0.2883326841610866\n",
      "9 Train Loss 107517.59 Test MSE 20262.250534628965 Test RE 0.25233831729798745\n",
      "10 Train Loss 87835.86 Test MSE 12424.420676326608 Test RE 0.19759567571784584\n",
      "11 Train Loss 76014.78 Test MSE 8718.54527956924 Test RE 0.16552415363206419\n",
      "12 Train Loss 65805.695 Test MSE 7861.352785390421 Test RE 0.15717664170591875\n",
      "13 Train Loss 52255.562 Test MSE 5856.240228019379 Test RE 0.13565906016357704\n",
      "14 Train Loss 47108.08 Test MSE 5497.4939942394085 Test RE 0.13143824268509166\n",
      "15 Train Loss 45043.484 Test MSE 5620.805992802069 Test RE 0.13290418602627863\n",
      "16 Train Loss 42762.887 Test MSE 4953.234969582894 Test RE 0.12476243070501358\n",
      "17 Train Loss 40874.402 Test MSE 4953.273117506671 Test RE 0.12476291114038181\n",
      "18 Train Loss 40149.098 Test MSE 4535.585143700872 Test RE 0.11938672098502243\n",
      "19 Train Loss 38752.676 Test MSE 4526.888515848051 Test RE 0.1192722087419311\n",
      "20 Train Loss 38066.95 Test MSE 4239.238103667754 Test RE 0.11542058396106636\n",
      "21 Train Loss 37488.168 Test MSE 4060.8716739142033 Test RE 0.11296632356204021\n",
      "22 Train Loss 36791.164 Test MSE 4450.517400376814 Test RE 0.11826183512823214\n",
      "23 Train Loss 35841.19 Test MSE 4373.716683781651 Test RE 0.1172369970845221\n",
      "24 Train Loss 34906.555 Test MSE 4379.0928201848055 Test RE 0.11730902833024975\n",
      "25 Train Loss 34381.01 Test MSE 4274.664501261428 Test RE 0.11590185303811\n",
      "26 Train Loss 33936.68 Test MSE 4275.59045466228 Test RE 0.11591440535699228\n",
      "27 Train Loss 33590.836 Test MSE 4243.552341915126 Test RE 0.11547930032232132\n",
      "28 Train Loss 33347.258 Test MSE 4239.049464229913 Test RE 0.11541801591513909\n",
      "29 Train Loss 32947.625 Test MSE 4255.67042973166 Test RE 0.11564406684029498\n",
      "30 Train Loss 32707.758 Test MSE 4462.073080320906 Test RE 0.11841526784835014\n",
      "31 Train Loss 32478.074 Test MSE 4386.78804521276 Test RE 0.1174120545935328\n",
      "32 Train Loss 32425.967 Test MSE 4330.6086115908465 Test RE 0.11665781283789245\n",
      "33 Train Loss 32297.291 Test MSE 4373.055704213601 Test RE 0.11722813800730694\n",
      "34 Train Loss 32201.844 Test MSE 4346.771233494427 Test RE 0.11687530422435885\n",
      "35 Train Loss 32108.508 Test MSE 4351.250315304434 Test RE 0.1169355051451534\n",
      "36 Train Loss 32077.84 Test MSE 4319.252708355633 Test RE 0.11650475993580248\n",
      "37 Train Loss 31927.678 Test MSE 4354.269171385073 Test RE 0.1169760624824021\n",
      "38 Train Loss 31756.994 Test MSE 4331.121299743407 Test RE 0.11666471802303599\n",
      "39 Train Loss 31503.576 Test MSE 4425.399997504426 Test RE 0.11792764554808811\n",
      "40 Train Loss 31355.283 Test MSE 4414.1386615331785 Test RE 0.11777750445680754\n",
      "41 Train Loss 31287.553 Test MSE 4490.555455623631 Test RE 0.11879260175849421\n",
      "42 Train Loss 31234.953 Test MSE 4499.327636501164 Test RE 0.11890857424588322\n",
      "43 Train Loss 31118.74 Test MSE 4420.279743465358 Test RE 0.11785940377763339\n",
      "44 Train Loss 31034.383 Test MSE 4389.983357237702 Test RE 0.11745480795354485\n",
      "45 Train Loss 30987.09 Test MSE 4435.866620107748 Test RE 0.11806701997775917\n",
      "46 Train Loss 30944.777 Test MSE 4438.807236044735 Test RE 0.11810614787201125\n",
      "47 Train Loss 30883.053 Test MSE 4445.7172229254065 Test RE 0.11819804131558302\n",
      "48 Train Loss 30790.828 Test MSE 4450.682182664005 Test RE 0.11826402445517406\n",
      "49 Train Loss 30717.697 Test MSE 4436.616856476227 Test RE 0.11807700386984951\n",
      "50 Train Loss 30673.623 Test MSE 4431.200967257922 Test RE 0.11800491208538222\n",
      "51 Train Loss 30631.71 Test MSE 4433.222656315239 Test RE 0.11803182826863158\n",
      "52 Train Loss 30590.242 Test MSE 4429.968291788445 Test RE 0.11798849758747768\n",
      "53 Train Loss 30539.363 Test MSE 4413.557094026145 Test RE 0.1177697455449174\n",
      "54 Train Loss 30485.828 Test MSE 4351.393936287359 Test RE 0.11693743496444566\n",
      "55 Train Loss 30449.342 Test MSE 4368.8360377208455 Test RE 0.11717156623401631\n",
      "56 Train Loss 30418.855 Test MSE 4413.027293423638 Test RE 0.11776267682989024\n",
      "57 Train Loss 30395.086 Test MSE 4464.8732245907795 Test RE 0.11845241737771686\n",
      "58 Train Loss 30349.344 Test MSE 4391.629210980163 Test RE 0.1174768234492843\n",
      "59 Train Loss 30226.193 Test MSE 4318.393291908067 Test RE 0.11649316868440165\n",
      "60 Train Loss 30147.596 Test MSE 4310.3769153268295 Test RE 0.11638499339390566\n",
      "61 Train Loss 30051.33 Test MSE 4234.7907275043235 Test RE 0.11536002432247022\n",
      "62 Train Loss 29871.71 Test MSE 4225.499016643803 Test RE 0.11523339697825588\n",
      "63 Train Loss 29776.73 Test MSE 4124.4831173989705 Test RE 0.1138476648981458\n",
      "64 Train Loss 29720.0 Test MSE 4054.623417182934 Test RE 0.11287938233754333\n",
      "65 Train Loss 29643.256 Test MSE 4059.256769697801 Test RE 0.11294385942781013\n",
      "66 Train Loss 29498.027 Test MSE 4026.3990039080863 Test RE 0.11248581704392614\n",
      "67 Train Loss 29364.482 Test MSE 3840.593284468839 Test RE 0.10985972847382376\n",
      "68 Train Loss 29219.12 Test MSE 3828.8450084736746 Test RE 0.10969157073704958\n",
      "69 Train Loss 29028.828 Test MSE 3746.4634876070763 Test RE 0.10850509071929046\n",
      "70 Train Loss 28854.508 Test MSE 3768.3786017418015 Test RE 0.10882198079138451\n",
      "71 Train Loss 28626.156 Test MSE 3749.5620295596395 Test RE 0.10854995143790253\n",
      "72 Train Loss 28426.201 Test MSE 3738.645069317809 Test RE 0.10839181305811652\n",
      "73 Train Loss 28264.473 Test MSE 3869.521197172404 Test RE 0.11027269211216309\n",
      "74 Train Loss 28107.52 Test MSE 3854.365701655983 Test RE 0.11005653140079664\n",
      "75 Train Loss 28040.287 Test MSE 3850.974824518134 Test RE 0.11000810964748332\n",
      "76 Train Loss 28004.707 Test MSE 3766.2049023143336 Test RE 0.10879059058040678\n",
      "77 Train Loss 27922.117 Test MSE 3615.488616613913 Test RE 0.10659157059352203\n",
      "78 Train Loss 27738.268 Test MSE 3433.626426910707 Test RE 0.10387615927726944\n",
      "79 Train Loss 27535.564 Test MSE 3317.9043129624893 Test RE 0.1021107087638283\n",
      "80 Train Loss 27408.08 Test MSE 3258.0348163481212 Test RE 0.10118525318318122\n",
      "81 Train Loss 27361.453 Test MSE 3342.225394289925 Test RE 0.10248427414665923\n",
      "82 Train Loss 27254.414 Test MSE 3234.8246935295065 Test RE 0.10082418892993855\n",
      "83 Train Loss 27094.201 Test MSE 3359.5845731463164 Test RE 0.10275007590641827\n",
      "84 Train Loss 26964.416 Test MSE 3343.9472601000734 Test RE 0.10251066994957898\n",
      "85 Train Loss 26692.387 Test MSE 3315.5935558053607 Test RE 0.10207514502757024\n",
      "86 Train Loss 26290.95 Test MSE 3428.9961805303137 Test RE 0.10380609713190198\n",
      "87 Train Loss 26031.334 Test MSE 3683.77099529132 Test RE 0.10759341055693994\n",
      "88 Train Loss 25698.463 Test MSE 3949.752917428324 Test RE 0.11141003905628588\n",
      "89 Train Loss 25166.428 Test MSE 3924.410495709025 Test RE 0.11105204911129371\n",
      "90 Train Loss 24757.205 Test MSE 4125.693095869043 Test RE 0.11386436312650151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 Train Loss 24439.656 Test MSE 3961.09131804233 Test RE 0.11156983467103622\n",
      "92 Train Loss 24164.42 Test MSE 4246.319490845054 Test RE 0.11551694523871636\n",
      "93 Train Loss 23927.828 Test MSE 4406.0314585092465 Test RE 0.11766929705273439\n",
      "94 Train Loss 23523.426 Test MSE 4004.058693626725 Test RE 0.1121733214949618\n",
      "95 Train Loss 23216.43 Test MSE 4171.22803687071 Test RE 0.11449099473361025\n",
      "96 Train Loss 23113.414 Test MSE 4195.035828719821 Test RE 0.1148172654841287\n",
      "97 Train Loss 22794.248 Test MSE 4145.231120835026 Test RE 0.11413365813691538\n",
      "98 Train Loss 22491.873 Test MSE 3869.545024535615 Test RE 0.11027303162489717\n",
      "99 Train Loss 22107.246 Test MSE 3961.444352360628 Test RE 0.11157480641995389\n",
      "Training time: 560.82\n",
      "3D_HTTP_swish\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 244316.05 Test MSE 86774.57873241471 Test RE 0.5221984990315384\n",
      "1 Train Loss 182177.0 Test MSE 52233.428444201745 Test RE 0.4051479338670721\n",
      "2 Train Loss 155607.17 Test MSE 37448.93101838519 Test RE 0.34305130426303126\n",
      "3 Train Loss 131320.56 Test MSE 27698.433658947262 Test RE 0.2950305354326974\n",
      "4 Train Loss 109656.45 Test MSE 19935.589252178375 Test RE 0.2502959951141886\n",
      "5 Train Loss 72798.26 Test MSE 9699.445373502744 Test RE 0.17458736604640587\n",
      "6 Train Loss 61591.26 Test MSE 8966.093186749311 Test RE 0.1678575911070689\n",
      "7 Train Loss 51362.035 Test MSE 4497.4972374588 Test RE 0.11888438482266103\n",
      "8 Train Loss 45632.453 Test MSE 6075.057207887119 Test RE 0.1381702515279341\n",
      "9 Train Loss 42325.07 Test MSE 4964.875787947444 Test RE 0.12490894954616735\n",
      "10 Train Loss 39094.465 Test MSE 5105.736570894579 Test RE 0.1266684814442235\n",
      "11 Train Loss 37742.645 Test MSE 5178.362413589318 Test RE 0.12756618952055634\n",
      "12 Train Loss 36865.34 Test MSE 4767.661539210954 Test RE 0.12240300238923363\n",
      "13 Train Loss 36350.523 Test MSE 4809.749169181035 Test RE 0.12294208562615849\n",
      "14 Train Loss 32959.72 Test MSE 3497.892421561267 Test RE 0.10484375972443763\n",
      "15 Train Loss 30675.166 Test MSE 4195.223900343862 Test RE 0.11481983919606477\n",
      "16 Train Loss 29567.61 Test MSE 3820.3810890188006 Test RE 0.10957026336243819\n",
      "17 Train Loss 29119.672 Test MSE 3626.1614284523075 Test RE 0.10674878218653149\n",
      "18 Train Loss 28035.098 Test MSE 3754.916270258279 Test RE 0.10862742651727486\n",
      "19 Train Loss 27196.486 Test MSE 3588.5502247980303 Test RE 0.10619373030059621\n",
      "20 Train Loss 27018.285 Test MSE 3662.941366380836 Test RE 0.10728878957391795\n",
      "21 Train Loss 26581.76 Test MSE 3090.1201058406145 Test RE 0.09854328646182128\n",
      "22 Train Loss 26175.975 Test MSE 3013.399906472022 Test RE 0.097312302262269\n",
      "23 Train Loss 25933.316 Test MSE 3112.5379577101426 Test RE 0.09890009081843142\n",
      "24 Train Loss 25755.418 Test MSE 3147.849182095209 Test RE 0.09945951118407331\n",
      "25 Train Loss 25315.928 Test MSE 3297.31094835207 Test RE 0.10179332828655402\n",
      "26 Train Loss 25037.166 Test MSE 3226.9801020632913 Test RE 0.10070186318804868\n",
      "27 Train Loss 24671.387 Test MSE 2903.3006826086335 Test RE 0.09551803296557286\n",
      "28 Train Loss 24581.297 Test MSE 2739.6374004234776 Test RE 0.092786737661326\n",
      "29 Train Loss 24055.492 Test MSE 2705.439070897411 Test RE 0.09220580016751237\n",
      "30 Train Loss 23865.287 Test MSE 2817.536493344656 Test RE 0.0940966445908773\n",
      "31 Train Loss 23786.787 Test MSE 2900.340201007333 Test RE 0.09546932090745774\n",
      "32 Train Loss 23740.066 Test MSE 2883.894239960352 Test RE 0.09519826360185667\n",
      "33 Train Loss 23580.271 Test MSE 2933.7503045655617 Test RE 0.09601761982114913\n",
      "34 Train Loss 23213.895 Test MSE 2492.4908948837838 Test RE 0.08850262506737122\n",
      "35 Train Loss 23046.951 Test MSE 2376.1700587149207 Test RE 0.08641280878664563\n",
      "36 Train Loss 22901.441 Test MSE 2431.637034200008 Test RE 0.08741555852782901\n",
      "37 Train Loss 21489.775 Test MSE 2202.889352434443 Test RE 0.08320237131320828\n",
      "38 Train Loss 20205.984 Test MSE 1787.341802580807 Test RE 0.07494508709042629\n",
      "39 Train Loss 19820.809 Test MSE 2021.1205507474115 Test RE 0.07969580673352841\n",
      "40 Train Loss 19410.537 Test MSE 1868.8184738147047 Test RE 0.0766342517760187\n",
      "41 Train Loss 19046.148 Test MSE 1864.4580789325046 Test RE 0.0765447966610253\n",
      "42 Train Loss 18862.24 Test MSE 1820.313785268984 Test RE 0.0756332027276759\n",
      "43 Train Loss 18753.557 Test MSE 1756.634706969179 Test RE 0.07429850783912118\n",
      "44 Train Loss 18628.17 Test MSE 1738.5126578131124 Test RE 0.0739142697780199\n",
      "45 Train Loss 18501.893 Test MSE 1835.422514083254 Test RE 0.07594643448516958\n",
      "46 Train Loss 18465.988 Test MSE 1816.8256622717875 Test RE 0.07556070302446682\n",
      "47 Train Loss 18391.115 Test MSE 1953.4553024914082 Test RE 0.07835037897119147\n",
      "48 Train Loss 18296.031 Test MSE 1809.9330302046878 Test RE 0.07541723658296015\n",
      "49 Train Loss 18031.2 Test MSE 1917.1322192208431 Test RE 0.07761852673252967\n",
      "50 Train Loss 17995.92 Test MSE 1887.0004586504754 Test RE 0.07700614186502776\n",
      "51 Train Loss 17817.238 Test MSE 1748.8646749364516 Test RE 0.07413400534275122\n",
      "52 Train Loss 17762.146 Test MSE 1848.075569255031 Test RE 0.07620776503521363\n",
      "53 Train Loss 17741.71 Test MSE 1825.8094559287533 Test RE 0.0757472879818937\n",
      "54 Train Loss 17632.734 Test MSE 1871.0541573949151 Test RE 0.07668007718135778\n",
      "55 Train Loss 17383.502 Test MSE 1715.6937857333435 Test RE 0.07342758612152164\n",
      "56 Train Loss 17315.744 Test MSE 1739.4191179768952 Test RE 0.07393353670989199\n",
      "57 Train Loss 17276.086 Test MSE 1754.7985042129317 Test RE 0.07425966572964444\n",
      "58 Train Loss 17248.385 Test MSE 1725.1276177679965 Test RE 0.07362918209169107\n",
      "59 Train Loss 17182.354 Test MSE 1764.8778858779551 Test RE 0.07447263028730404\n",
      "60 Train Loss 17128.729 Test MSE 1797.9793111059423 Test RE 0.07516777710006572\n",
      "61 Train Loss 17106.781 Test MSE 1783.0066632772443 Test RE 0.07485414347838887\n",
      "62 Train Loss 17059.8 Test MSE 1712.9392274650506 Test RE 0.07336861819435593\n",
      "63 Train Loss 17043.527 Test MSE 1702.4545967257945 Test RE 0.07314373467919805\n",
      "64 Train Loss 17039.555 Test MSE 1704.571459671605 Test RE 0.07318919467578622\n",
      "65 Train Loss 17037.328 Test MSE 1689.2355891042953 Test RE 0.07285921261251785\n",
      "66 Train Loss 17019.422 Test MSE 1721.5048146896252 Test RE 0.07355183007031008\n",
      "67 Train Loss 16974.176 Test MSE 1720.013496207778 Test RE 0.07351996464405074\n",
      "68 Train Loss 16961.568 Test MSE 1731.5905474014405 Test RE 0.07376697345455659\n",
      "69 Train Loss 16948.6 Test MSE 1722.7537913896047 Test RE 0.07357850669212257\n",
      "70 Train Loss 16930.557 Test MSE 1762.8007402993792 Test RE 0.07442879268303941\n",
      "71 Train Loss 16923.605 Test MSE 1760.693075359466 Test RE 0.07438428456836833\n",
      "72 Train Loss 16916.557 Test MSE 1746.278686355162 Test RE 0.07407917531067515\n",
      "73 Train Loss 16914.277 Test MSE 1742.5263900501623 Test RE 0.07399954412173076\n",
      "74 Train Loss 16912.123 Test MSE 1748.1192724055409 Test RE 0.07411820493078618\n",
      "75 Train Loss 16905.662 Test MSE 1736.0322166636065 Test RE 0.07386152197726976\n",
      "76 Train Loss 16897.482 Test MSE 1720.3933875470145 Test RE 0.0735280832012281\n",
      "77 Train Loss 16891.984 Test MSE 1726.0764753771637 Test RE 0.07364942813174655\n",
      "78 Train Loss 16886.738 Test MSE 1724.5975529730995 Test RE 0.0736178695269485\n",
      "79 Train Loss 16881.947 Test MSE 1740.459522503834 Test RE 0.07395564446032088\n",
      "80 Train Loss 16879.268 Test MSE 1725.3697389632082 Test RE 0.0736343488283199\n",
      "81 Train Loss 16875.469 Test MSE 1723.420804162112 Test RE 0.07359274931183003\n",
      "82 Train Loss 16873.693 Test MSE 1734.947918790756 Test RE 0.07383845201355987\n",
      "83 Train Loss 16866.152 Test MSE 1760.1153966590596 Test RE 0.07437208092463758\n",
      "84 Train Loss 16861.021 Test MSE 1751.309925233911 Test RE 0.07418581406205807\n",
      "85 Train Loss 16856.812 Test MSE 1762.8790178300426 Test RE 0.07443044517764427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 Train Loss 16839.98 Test MSE 1784.1861574755253 Test RE 0.07487889813388048\n",
      "87 Train Loss 16803.838 Test MSE 1767.2771202493352 Test RE 0.07452323339474878\n",
      "88 Train Loss 16784.822 Test MSE 1747.9691127096603 Test RE 0.07411502156501158\n",
      "89 Train Loss 16775.73 Test MSE 1720.307576310621 Test RE 0.07352624943040136\n",
      "90 Train Loss 16773.258 Test MSE 1718.7057103668503 Test RE 0.07349200944132\n",
      "91 Train Loss 16770.967 Test MSE 1730.03808059518 Test RE 0.07373389795124952\n",
      "92 Train Loss 16769.178 Test MSE 1726.8877187246442 Test RE 0.07366673344552728\n",
      "93 Train Loss 16765.81 Test MSE 1739.0382028048543 Test RE 0.07392544091873\n",
      "94 Train Loss 16756.156 Test MSE 1762.4164834030335 Test RE 0.07442068021322293\n",
      "95 Train Loss 16745.28 Test MSE 1763.8860012494642 Test RE 0.07445170004633171\n",
      "96 Train Loss 16723.168 Test MSE 1849.5754868213035 Test RE 0.07623868427464077\n",
      "97 Train Loss 16694.77 Test MSE 1789.4293557285996 Test RE 0.07498884094777952\n",
      "98 Train Loss 16658.229 Test MSE 1784.274653026016 Test RE 0.07488075510578807\n",
      "99 Train Loss 16643.336 Test MSE 1756.77510843107 Test RE 0.07430147698536525\n",
      "Training time: 387.75\n",
      "3D_HTTP_swish\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 243366.69 Test MSE 85922.06563612854 Test RE 0.5196270092839863\n",
      "1 Train Loss 208683.28 Test MSE 65424.887090627424 Test RE 0.4534306289633299\n",
      "2 Train Loss 191540.52 Test MSE 51742.169008934645 Test RE 0.40323820915208775\n",
      "3 Train Loss 181254.06 Test MSE 49246.74919141578 Test RE 0.3933943752319325\n",
      "4 Train Loss 161820.98 Test MSE 37859.0690342548 Test RE 0.34492472551934633\n",
      "5 Train Loss 159580.52 Test MSE 37229.95512551665 Test RE 0.342046868375784\n",
      "6 Train Loss 156338.02 Test MSE 36580.21388288423 Test RE 0.3390490114229043\n",
      "7 Train Loss 142583.98 Test MSE 31171.492955270336 Test RE 0.31298113693482493\n",
      "8 Train Loss 129462.37 Test MSE 25148.262775259183 Test RE 0.2811210440257789\n",
      "9 Train Loss 118369.164 Test MSE 20866.065525163576 Test RE 0.25607055669457557\n",
      "10 Train Loss 107654.29 Test MSE 17507.51721869779 Test RE 0.2345587525566213\n",
      "11 Train Loss 89227.57 Test MSE 13794.899046587923 Test RE 0.2082085815590933\n",
      "12 Train Loss 77875.97 Test MSE 10130.719452770756 Test RE 0.17842656166078702\n",
      "13 Train Loss 71216.016 Test MSE 8231.39253839293 Test RE 0.16083331690722769\n",
      "14 Train Loss 65255.805 Test MSE 7559.550766289198 Test RE 0.15413006307947075\n",
      "15 Train Loss 50383.92 Test MSE 4981.20896413739 Test RE 0.12511424015367126\n",
      "16 Train Loss 46912.37 Test MSE 4972.360860112835 Test RE 0.12500307077114428\n",
      "17 Train Loss 42538.19 Test MSE 5127.8089458687045 Test RE 0.12694198351548097\n",
      "18 Train Loss 42044.836 Test MSE 4744.382924973192 Test RE 0.12210381390253801\n",
      "19 Train Loss 40190.43 Test MSE 4413.19084602199 Test RE 0.11776485902955194\n",
      "20 Train Loss 39694.094 Test MSE 4616.892678202396 Test RE 0.1204520655590641\n",
      "21 Train Loss 38444.516 Test MSE 3886.856770859374 Test RE 0.11051942859623767\n",
      "22 Train Loss 37938.02 Test MSE 4371.8082616754555 Test RE 0.11721141677093992\n",
      "23 Train Loss 37479.64 Test MSE 4156.928291979376 Test RE 0.11429457804738755\n",
      "24 Train Loss 34399.61 Test MSE 4243.669027371452 Test RE 0.11548088798533811\n",
      "25 Train Loss 33768.086 Test MSE 4450.306954939022 Test RE 0.11825903905356903\n",
      "26 Train Loss 33401.195 Test MSE 4383.719727599351 Test RE 0.11737098576497892\n",
      "27 Train Loss 32683.344 Test MSE 3843.742714996126 Test RE 0.10990476379176592\n",
      "28 Train Loss 32342.803 Test MSE 4118.673874888577 Test RE 0.11376746069792036\n",
      "29 Train Loss 31832.035 Test MSE 3894.1942233392215 Test RE 0.11062369648584967\n",
      "30 Train Loss 31646.63 Test MSE 3941.687325617457 Test RE 0.11129622850576161\n",
      "31 Train Loss 31022.299 Test MSE 3851.511985490177 Test RE 0.11001578173121579\n",
      "32 Train Loss 30817.84 Test MSE 3834.696947333313 Test RE 0.10977536405864692\n",
      "33 Train Loss 30713.236 Test MSE 4008.0148191159424 Test RE 0.11222872305243414\n",
      "34 Train Loss 30345.92 Test MSE 3885.721483906216 Test RE 0.1105032869635361\n",
      "35 Train Loss 30105.549 Test MSE 3852.6644939228877 Test RE 0.11003224080401024\n",
      "36 Train Loss 29860.977 Test MSE 3946.8166255917668 Test RE 0.11136861960345705\n",
      "37 Train Loss 29771.832 Test MSE 3900.617700680269 Test RE 0.11071489584304778\n",
      "38 Train Loss 29634.078 Test MSE 3874.5641346912194 Test RE 0.11034452493110779\n",
      "39 Train Loss 29367.482 Test MSE 3939.74066939788 Test RE 0.1112687425291082\n",
      "40 Train Loss 29095.336 Test MSE 3910.100195609269 Test RE 0.11084938943337008\n",
      "41 Train Loss 28946.73 Test MSE 3988.6324426808455 Test RE 0.11195703049764191\n",
      "42 Train Loss 28878.488 Test MSE 4080.773189071868 Test RE 0.11324279785312653\n",
      "43 Train Loss 28817.6 Test MSE 3973.240642203348 Test RE 0.11174080526800277\n",
      "44 Train Loss 28684.44 Test MSE 4071.294883869739 Test RE 0.1131112083558079\n",
      "45 Train Loss 28458.424 Test MSE 3969.2550621974838 Test RE 0.11168474729175197\n",
      "46 Train Loss 28387.771 Test MSE 3877.0293309109393 Test RE 0.11037962276942224\n",
      "47 Train Loss 28326.291 Test MSE 3770.905696862452 Test RE 0.10885846298127032\n",
      "48 Train Loss 28231.66 Test MSE 3734.2816757230785 Test RE 0.1083285422458201\n",
      "49 Train Loss 28002.863 Test MSE 3773.451226779102 Test RE 0.10889519894712435\n",
      "50 Train Loss 27920.7 Test MSE 3754.987682712631 Test RE 0.10862845947163344\n",
      "51 Train Loss 27854.318 Test MSE 3715.466057503085 Test RE 0.1080552845388175\n",
      "52 Train Loss 27731.691 Test MSE 3618.756320370265 Test RE 0.10663972881595321\n",
      "53 Train Loss 27611.396 Test MSE 3442.2987540747345 Test RE 0.10400725681438959\n",
      "54 Train Loss 27485.322 Test MSE 3573.9192611436347 Test RE 0.10597702669867554\n",
      "55 Train Loss 27266.434 Test MSE 3522.517326347659 Test RE 0.10521215865597457\n",
      "56 Train Loss 27159.277 Test MSE 3455.0213784398175 Test RE 0.10419928326829903\n",
      "57 Train Loss 27073.8 Test MSE 3490.6537048741097 Test RE 0.10473521903297814\n",
      "58 Train Loss 26900.826 Test MSE 3454.077999075675 Test RE 0.1041850567044362\n",
      "59 Train Loss 26744.316 Test MSE 3454.6870285215455 Test RE 0.10419424135090814\n",
      "60 Train Loss 26672.383 Test MSE 3465.7466861903495 Test RE 0.10436088913022362\n",
      "61 Train Loss 26556.895 Test MSE 3410.6966761697545 Test RE 0.10352873588555539\n",
      "62 Train Loss 26151.904 Test MSE 3333.2886960307715 Test RE 0.10234716728257105\n",
      "63 Train Loss 25857.99 Test MSE 3222.87482503178 Test RE 0.10063778770593287\n",
      "64 Train Loss 25550.432 Test MSE 3115.852023050033 Test RE 0.0989527285963851\n",
      "65 Train Loss 25312.887 Test MSE 3130.4444038058314 Test RE 0.09918416918145329\n",
      "66 Train Loss 25141.285 Test MSE 2937.7442877907292 Test RE 0.09608295638542592\n",
      "67 Train Loss 24926.238 Test MSE 2939.0555861625535 Test RE 0.09610439789894522\n",
      "68 Train Loss 24739.037 Test MSE 2854.7502615643302 Test RE 0.09471601615182756\n",
      "69 Train Loss 24570.066 Test MSE 3015.983450452547 Test RE 0.09735400876601748\n",
      "70 Train Loss 24351.852 Test MSE 3058.4053426217465 Test RE 0.09803629365852756\n",
      "71 Train Loss 24204.08 Test MSE 3148.604596634062 Test RE 0.09947144451527724\n",
      "72 Train Loss 23810.926 Test MSE 3302.1813724925732 Test RE 0.10186847948545752\n",
      "73 Train Loss 23174.494 Test MSE 2956.518714893068 Test RE 0.09638948911572941\n",
      "74 Train Loss 22820.912 Test MSE 2625.4649493128018 Test RE 0.09083275245144215\n",
      "75 Train Loss 22709.793 Test MSE 2510.466665036276 Test RE 0.08882119087644545\n",
      "76 Train Loss 22511.674 Test MSE 2534.2800748931436 Test RE 0.08924145999254167\n",
      "77 Train Loss 22267.57 Test MSE 2468.745928131592 Test RE 0.08808005162966447\n",
      "78 Train Loss 22063.307 Test MSE 2416.0032838636284 Test RE 0.0871340944944829\n",
      "79 Train Loss 21884.078 Test MSE 2387.018322442095 Test RE 0.08660984043996617\n",
      "80 Train Loss 21720.363 Test MSE 2317.5303925653543 Test RE 0.08533989051270867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 Train Loss 21630.395 Test MSE 2285.1488476098853 Test RE 0.08474159015578203\n",
      "82 Train Loss 21567.945 Test MSE 2259.5011170127955 Test RE 0.08426469290682094\n",
      "83 Train Loss 21516.584 Test MSE 2233.4636316890246 Test RE 0.08377777166591667\n",
      "84 Train Loss 21379.86 Test MSE 2373.439185801001 Test RE 0.08636313846965277\n",
      "85 Train Loss 21151.367 Test MSE 2255.696286484348 Test RE 0.08419371530043664\n",
      "86 Train Loss 21051.463 Test MSE 2343.0973329483104 Test RE 0.08580933316902674\n",
      "87 Train Loss 20942.564 Test MSE 2362.0212541541887 Test RE 0.0861551539496047\n",
      "88 Train Loss 20620.951 Test MSE 2303.0723464582607 Test RE 0.08507327514061776\n",
      "89 Train Loss 20501.682 Test MSE 2342.0550504415005 Test RE 0.0857902457171334\n",
      "90 Train Loss 20177.512 Test MSE 2328.6606446416613 Test RE 0.08554457321632045\n",
      "91 Train Loss 20083.432 Test MSE 2250.555932046924 Test RE 0.08409772888962473\n",
      "92 Train Loss 19893.035 Test MSE 2201.173396842136 Test RE 0.0831699594745824\n",
      "93 Train Loss 19791.23 Test MSE 2157.186477538701 Test RE 0.08233475676051594\n",
      "94 Train Loss 19695.152 Test MSE 2125.9973566145695 Test RE 0.08173738179318947\n",
      "95 Train Loss 19506.133 Test MSE 2101.089917632266 Test RE 0.08125716790395728\n",
      "96 Train Loss 19396.91 Test MSE 2114.6089803831833 Test RE 0.08151816562451297\n",
      "97 Train Loss 19272.336 Test MSE 2060.49590590875 Test RE 0.08046837667095129\n",
      "98 Train Loss 19220.49 Test MSE 2100.9839575860433 Test RE 0.08125511893838223\n",
      "99 Train Loss 19106.697 Test MSE 2022.155448627811 Test RE 0.07971620790806985\n",
      "Training time: 366.15\n",
      "3D_HTTP_swish\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 241832.31 Test MSE 85314.20659233029 Test RE 0.5177856859844076\n",
      "1 Train Loss 213872.11 Test MSE 67131.24809005317 Test RE 0.459305581473345\n",
      "2 Train Loss 192565.47 Test MSE 53800.19600481623 Test RE 0.41117934572770143\n",
      "3 Train Loss 181748.3 Test MSE 47087.16024862336 Test RE 0.38467203236846104\n",
      "4 Train Loss 176191.62 Test MSE 44611.773795743815 Test RE 0.37442437016997\n",
      "5 Train Loss 174489.2 Test MSE 44351.28666997045 Test RE 0.37332964214511377\n",
      "6 Train Loss 170068.9 Test MSE 42140.500884324356 Test RE 0.3639059938696359\n",
      "7 Train Loss 163474.12 Test MSE 39726.005250761365 Test RE 0.35332698705229154\n",
      "8 Train Loss 155585.69 Test MSE 37131.39213224806 Test RE 0.34159379898842623\n",
      "9 Train Loss 143741.19 Test MSE 31601.691713151413 Test RE 0.31513346755918353\n",
      "10 Train Loss 137232.05 Test MSE 28642.683862425867 Test RE 0.3000172438254289\n",
      "11 Train Loss 132460.19 Test MSE 27051.959638872217 Test RE 0.29156724121039107\n",
      "12 Train Loss 119640.4 Test MSE 21655.781413045475 Test RE 0.26087129360970523\n",
      "13 Train Loss 116084.805 Test MSE 21009.16611631485 Test RE 0.25694712914047224\n",
      "14 Train Loss 110609.87 Test MSE 19171.27787531341 Test RE 0.24545104940565876\n",
      "15 Train Loss 106453.85 Test MSE 18771.8088615004 Test RE 0.2428803745482316\n",
      "16 Train Loss 103725.01 Test MSE 19226.81022716869 Test RE 0.24580628440205463\n",
      "17 Train Loss 100182.9 Test MSE 18531.429730625823 Test RE 0.24132028290616786\n",
      "18 Train Loss 94885.54 Test MSE 17340.023161178684 Test RE 0.23343404669917192\n",
      "19 Train Loss 90337.01 Test MSE 16484.952135585987 Test RE 0.2276057384788424\n",
      "20 Train Loss 87516.234 Test MSE 15934.889317403802 Test RE 0.22377619645136496\n",
      "21 Train Loss 81239.984 Test MSE 14294.964945135742 Test RE 0.211948774380428\n",
      "22 Train Loss 78585.164 Test MSE 13482.42192699556 Test RE 0.20583694107938577\n",
      "23 Train Loss 73326.305 Test MSE 12088.82663756167 Test RE 0.1949087952014295\n",
      "24 Train Loss 69200.734 Test MSE 10523.176719314322 Test RE 0.18184978589350162\n",
      "25 Train Loss 67813.336 Test MSE 9776.109107455233 Test RE 0.1752759711830468\n",
      "26 Train Loss 65232.477 Test MSE 9844.658437202062 Test RE 0.17588940857316396\n",
      "27 Train Loss 64162.137 Test MSE 9631.331333197451 Test RE 0.17397326897217127\n",
      "28 Train Loss 62568.203 Test MSE 8942.489415564993 Test RE 0.16763649797738334\n",
      "29 Train Loss 61325.51 Test MSE 8856.556130446193 Test RE 0.16682909808495766\n",
      "30 Train Loss 59926.203 Test MSE 8768.80467747218 Test RE 0.16600056278170666\n",
      "31 Train Loss 57760.23 Test MSE 8607.723329613358 Test RE 0.1644687955233912\n",
      "32 Train Loss 52876.133 Test MSE 7764.724505487523 Test RE 0.1562076820277974\n",
      "33 Train Loss 50410.51 Test MSE 7359.827199040964 Test RE 0.1520803737048438\n",
      "34 Train Loss 48749.836 Test MSE 7015.107470747005 Test RE 0.14847609149057456\n",
      "35 Train Loss 47925.41 Test MSE 7137.594078478183 Test RE 0.1497667084376852\n",
      "36 Train Loss 47352.344 Test MSE 7127.685025849483 Test RE 0.14966271249667246\n",
      "37 Train Loss 46984.37 Test MSE 7120.503979950392 Test RE 0.14958730192258346\n",
      "38 Train Loss 46547.266 Test MSE 7040.189234727101 Test RE 0.14874128482567814\n",
      "39 Train Loss 45566.785 Test MSE 6872.416762889791 Test RE 0.14695829559812268\n",
      "40 Train Loss 44200.164 Test MSE 6723.910522095868 Test RE 0.14536181097167203\n",
      "41 Train Loss 43394.914 Test MSE 6661.993881292857 Test RE 0.14469098626008753\n",
      "42 Train Loss 42394.234 Test MSE 6308.61502933416 Test RE 0.14080120621463652\n",
      "43 Train Loss 41977.332 Test MSE 6207.082384733907 Test RE 0.13966356292882592\n",
      "44 Train Loss 41332.934 Test MSE 6090.49799565864 Test RE 0.1383457316543988\n",
      "45 Train Loss 40711.543 Test MSE 6210.871597671135 Test RE 0.13970618635513263\n",
      "46 Train Loss 40037.24 Test MSE 5975.565459071705 Test RE 0.13703416767925466\n",
      "47 Train Loss 39465.375 Test MSE 5826.017488006029 Test RE 0.13530855439369116\n",
      "48 Train Loss 38834.5 Test MSE 5663.290593249821 Test RE 0.13340551554235033\n",
      "49 Train Loss 38612.48 Test MSE 5588.251803933408 Test RE 0.13251875457807838\n",
      "50 Train Loss 38450.13 Test MSE 5473.170971629419 Test RE 0.13114715374913816\n",
      "51 Train Loss 37858.8 Test MSE 5350.344234461508 Test RE 0.12966722739128222\n",
      "52 Train Loss 37619.523 Test MSE 5357.047655076572 Test RE 0.1297484316875887\n",
      "53 Train Loss 37215.87 Test MSE 5270.568385998513 Test RE 0.12869690078216112\n",
      "54 Train Loss 36989.797 Test MSE 5271.804327170494 Test RE 0.12871198952239588\n",
      "55 Train Loss 36790.79 Test MSE 5296.849104732954 Test RE 0.12901736349558468\n",
      "56 Train Loss 36610.312 Test MSE 5241.496606398809 Test RE 0.12834147217603373\n",
      "57 Train Loss 36539.367 Test MSE 5220.957451305204 Test RE 0.12808976803191433\n",
      "58 Train Loss 36379.562 Test MSE 5161.503679858137 Test RE 0.12735836728948413\n",
      "59 Train Loss 36213.66 Test MSE 5144.543771523154 Test RE 0.12714895510692317\n",
      "60 Train Loss 36028.543 Test MSE 5098.818212610086 Test RE 0.12658263339955425\n",
      "61 Train Loss 35921.8 Test MSE 5010.605063237363 Test RE 0.12548287158791877\n",
      "62 Train Loss 35805.266 Test MSE 4986.935262279698 Test RE 0.1251861339110607\n",
      "63 Train Loss 35549.953 Test MSE 5065.697872060883 Test RE 0.12617084284509938\n",
      "64 Train Loss 35384.055 Test MSE 5124.717358985285 Test RE 0.12690371070285106\n",
      "65 Train Loss 35295.69 Test MSE 5088.038129705268 Test RE 0.12644875009132095\n",
      "66 Train Loss 35257.79 Test MSE 5050.695296503892 Test RE 0.12598387046495335\n",
      "67 Train Loss 35116.266 Test MSE 4951.254767179364 Test RE 0.12473748947332544\n",
      "68 Train Loss 34889.918 Test MSE 4959.066101781681 Test RE 0.12483584658831492\n",
      "69 Train Loss 34677.92 Test MSE 4871.430834331501 Test RE 0.12372789736079427\n",
      "70 Train Loss 34606.86 Test MSE 4848.729229048206 Test RE 0.12343926533305369\n",
      "71 Train Loss 34569.56 Test MSE 4881.522027034804 Test RE 0.12385598253389223\n",
      "72 Train Loss 34428.984 Test MSE 4851.809161593635 Test RE 0.12347846367123581\n",
      "73 Train Loss 34280.83 Test MSE 4734.624013368557 Test RE 0.12197816914125215\n",
      "74 Train Loss 34177.98 Test MSE 4703.874914293604 Test RE 0.12158142924909154\n",
      "75 Train Loss 34126.85 Test MSE 4668.54149523457 Test RE 0.12112393566036377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 Train Loss 34056.07 Test MSE 4670.694444904948 Test RE 0.12115186126409636\n",
      "77 Train Loss 33804.13 Test MSE 4703.910937150091 Test RE 0.12158189479101937\n",
      "78 Train Loss 33636.83 Test MSE 4685.24526400799 Test RE 0.12134042936619441\n",
      "79 Train Loss 33582.293 Test MSE 4665.990248050217 Test RE 0.12109083545880239\n",
      "80 Train Loss 33508.336 Test MSE 4639.85723057104 Test RE 0.12075125992949906\n",
      "81 Train Loss 33316.87 Test MSE 4620.047970268695 Test RE 0.12049321840360486\n",
      "82 Train Loss 33271.957 Test MSE 4624.288964403934 Test RE 0.12054850936794544\n",
      "83 Train Loss 33248.777 Test MSE 4604.256091607882 Test RE 0.12028711198506119\n",
      "84 Train Loss 33218.703 Test MSE 4607.645874521811 Test RE 0.12033138321669284\n",
      "85 Train Loss 33178.32 Test MSE 4615.170508680701 Test RE 0.12042959825923227\n",
      "86 Train Loss 33154.2 Test MSE 4644.660127782981 Test RE 0.1208137409342723\n",
      "87 Train Loss 33082.207 Test MSE 4677.830315350619 Test RE 0.12124437363503565\n",
      "88 Train Loss 33024.91 Test MSE 4714.688639637078 Test RE 0.12172110063239187\n",
      "89 Train Loss 32944.945 Test MSE 4771.590054758188 Test RE 0.1224534215599429\n",
      "90 Train Loss 32906.797 Test MSE 4805.601035716326 Test RE 0.12288905893325956\n",
      "91 Train Loss 32848.586 Test MSE 4786.574729417995 Test RE 0.12264554686018968\n",
      "92 Train Loss 32804.188 Test MSE 4752.915881219155 Test RE 0.12221356879179238\n",
      "93 Train Loss 32729.545 Test MSE 4726.0843489633135 Test RE 0.1218681157613178\n",
      "94 Train Loss 32718.691 Test MSE 4736.393447620379 Test RE 0.1220009599888753\n",
      "95 Train Loss 32658.1 Test MSE 4757.486063746408 Test RE 0.12227231211158245\n",
      "96 Train Loss 32616.955 Test MSE 4778.194146008336 Test RE 0.12253813272929848\n",
      "97 Train Loss 32588.805 Test MSE 4770.218255936174 Test RE 0.1224358180428103\n",
      "98 Train Loss 32583.807 Test MSE 4763.017135417852 Test RE 0.12234336859608824\n",
      "99 Train Loss 32559.02 Test MSE 4724.825827796384 Test RE 0.12185188839381264\n",
      "Training time: 361.73\n",
      "3D_HTTP_swish\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 243149.34 Test MSE 87370.52957425894 Test RE 0.5239886093580496\n",
      "1 Train Loss 226810.08 Test MSE 77402.26019127613 Test RE 0.4931921818282735\n",
      "2 Train Loss 209034.1 Test MSE 61558.01614975019 Test RE 0.43982678084986604\n",
      "3 Train Loss 188477.48 Test MSE 54173.768619211034 Test RE 0.4126044299172079\n",
      "4 Train Loss 179145.34 Test MSE 48680.88360337508 Test RE 0.3911277128719612\n",
      "5 Train Loss 175234.2 Test MSE 45600.88075726987 Test RE 0.3785523771027784\n",
      "6 Train Loss 169397.25 Test MSE 43033.15795237651 Test RE 0.3677400840506383\n",
      "7 Train Loss 163487.47 Test MSE 41850.91505220156 Test RE 0.3626534732946381\n",
      "8 Train Loss 158672.03 Test MSE 39057.296933696045 Test RE 0.3503405875107271\n",
      "9 Train Loss 156451.44 Test MSE 37734.71441746659 Test RE 0.3443577773155739\n",
      "10 Train Loss 149654.62 Test MSE 34328.74770330216 Test RE 0.3284493015168888\n",
      "11 Train Loss 146761.5 Test MSE 32801.4253559511 Test RE 0.3210596442946431\n",
      "12 Train Loss 139738.62 Test MSE 29002.39244513483 Test RE 0.3018952463394826\n",
      "13 Train Loss 135597.31 Test MSE 28324.466273360445 Test RE 0.29834600701626435\n",
      "14 Train Loss 131832.48 Test MSE 27133.275539409864 Test RE 0.2920051256183776\n",
      "15 Train Loss 127818.56 Test MSE 27118.241373616947 Test RE 0.2919242164456207\n",
      "16 Train Loss 123576.6 Test MSE 25190.933681448074 Test RE 0.2813594423124678\n",
      "17 Train Loss 115942.75 Test MSE 22935.509699757033 Test RE 0.2684686384047103\n",
      "18 Train Loss 111856.03 Test MSE 20987.848105257362 Test RE 0.25681673386570064\n",
      "19 Train Loss 109198.85 Test MSE 19878.39164573136 Test RE 0.24993667251729104\n",
      "20 Train Loss 106187.35 Test MSE 18753.262745460393 Test RE 0.2427603647831638\n",
      "21 Train Loss 101958.586 Test MSE 17715.409493591218 Test RE 0.23594727175067323\n",
      "22 Train Loss 98982.91 Test MSE 17119.86409054604 Test RE 0.23194740524935223\n",
      "23 Train Loss 95512.39 Test MSE 17233.55712592835 Test RE 0.23271631226448433\n",
      "24 Train Loss 91647.03 Test MSE 15186.20674466378 Test RE 0.21845601957406308\n",
      "25 Train Loss 88309.49 Test MSE 14104.70359969892 Test RE 0.21053356490148203\n",
      "26 Train Loss 79127.69 Test MSE 12380.66951521291 Test RE 0.19724746373572977\n",
      "27 Train Loss 70951.39 Test MSE 9630.426956122945 Test RE 0.17396510078003657\n",
      "28 Train Loss 67192.05 Test MSE 9907.013560317191 Test RE 0.1764455626357285\n",
      "29 Train Loss 61763.156 Test MSE 8635.993999316832 Test RE 0.1647386596514565\n",
      "30 Train Loss 57720.008 Test MSE 8360.354425119202 Test RE 0.16208831470729748\n",
      "31 Train Loss 52176.477 Test MSE 7434.27137843947 Test RE 0.15284758013112573\n",
      "32 Train Loss 47630.32 Test MSE 6688.679228231473 Test RE 0.14498048445060782\n",
      "33 Train Loss 45083.266 Test MSE 6646.812902922769 Test RE 0.14452603538022432\n",
      "34 Train Loss 43580.094 Test MSE 6475.154587412407 Test RE 0.1426475880476967\n",
      "35 Train Loss 41956.242 Test MSE 5647.079868834762 Test RE 0.13321444731096396\n",
      "36 Train Loss 40114.68 Test MSE 5690.885198930545 Test RE 0.1337301324008671\n",
      "37 Train Loss 39090.703 Test MSE 5599.322542873508 Test RE 0.13264995434396204\n",
      "38 Train Loss 37972.125 Test MSE 5321.436149854903 Test RE 0.12931645480877066\n",
      "39 Train Loss 36568.71 Test MSE 5180.925328709765 Test RE 0.12759775363743328\n",
      "40 Train Loss 36010.1 Test MSE 5112.959635991027 Test RE 0.12675804847480934\n",
      "41 Train Loss 35532.414 Test MSE 5005.407288933685 Test RE 0.125417769581881\n",
      "42 Train Loss 35172.582 Test MSE 4958.105617063078 Test RE 0.12482375673844599\n",
      "43 Train Loss 34711.71 Test MSE 4906.118790118217 Test RE 0.1241676300383013\n",
      "44 Train Loss 34156.03 Test MSE 4725.488296181276 Test RE 0.12186043052819964\n",
      "45 Train Loss 33932.715 Test MSE 4794.638976687911 Test RE 0.12274881776334157\n",
      "46 Train Loss 33692.93 Test MSE 4599.551879548442 Test RE 0.12022564704039397\n",
      "47 Train Loss 33180.105 Test MSE 4684.992032293283 Test RE 0.12133715017178048\n",
      "48 Train Loss 32838.008 Test MSE 4531.505656713935 Test RE 0.11933301830680479\n",
      "49 Train Loss 32442.498 Test MSE 4588.206982186546 Test RE 0.12007728587909665\n",
      "50 Train Loss 32215.113 Test MSE 4499.196888089071 Test RE 0.1189068465188235\n",
      "51 Train Loss 32078.18 Test MSE 4613.102177225822 Test RE 0.12040260941453247\n",
      "52 Train Loss 31807.781 Test MSE 4509.950356374308 Test RE 0.11904886051808947\n",
      "53 Train Loss 31619.453 Test MSE 4602.457880616582 Test RE 0.12026362038011208\n",
      "54 Train Loss 31497.014 Test MSE 4601.05444539068 Test RE 0.12024528288802237\n",
      "55 Train Loss 31350.42 Test MSE 4560.031566466704 Test RE 0.1197080308052061\n",
      "56 Train Loss 31018.486 Test MSE 4587.638968996152 Test RE 0.12006985295447316\n",
      "57 Train Loss 30895.215 Test MSE 4495.278325530226 Test RE 0.11885505445174095\n",
      "58 Train Loss 30788.15 Test MSE 4544.306422171811 Test RE 0.11950144761513594\n",
      "59 Train Loss 30661.162 Test MSE 4589.238074724541 Test RE 0.12009077740626813\n",
      "60 Train Loss 30590.973 Test MSE 4603.181331265801 Test RE 0.12027307200115259\n",
      "61 Train Loss 30555.021 Test MSE 4617.413039844408 Test RE 0.12045885333527931\n",
      "62 Train Loss 30482.299 Test MSE 4608.188194326101 Test RE 0.12033846450873083\n",
      "63 Train Loss 30425.395 Test MSE 4594.116014439072 Test RE 0.12015458320407554\n",
      "64 Train Loss 30356.127 Test MSE 4530.658333808256 Test RE 0.11932186105195447\n",
      "65 Train Loss 30253.88 Test MSE 4524.045805838767 Test RE 0.11923475370568531\n",
      "66 Train Loss 30122.29 Test MSE 4499.876393804337 Test RE 0.11891582532474636\n",
      "67 Train Loss 29963.121 Test MSE 4360.9623118068175 Test RE 0.11706593250886427\n",
      "68 Train Loss 29885.113 Test MSE 4392.558287040836 Test RE 0.11748924926293451\n",
      "69 Train Loss 29815.982 Test MSE 4367.143107039321 Test RE 0.11714886195328618\n",
      "70 Train Loss 29718.973 Test MSE 4310.222695055023 Test RE 0.11638291131544493\n",
      "71 Train Loss 29595.34 Test MSE 4265.99973048495 Test RE 0.11578432657292509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 Train Loss 29325.41 Test MSE 4300.407788393529 Test RE 0.1162503267002111\n",
      "73 Train Loss 29019.809 Test MSE 4209.833644483513 Test RE 0.11501959379784463\n",
      "74 Train Loss 28820.459 Test MSE 4165.7289928693735 Test RE 0.11441550153941367\n",
      "75 Train Loss 28640.434 Test MSE 4052.224443067332 Test RE 0.11284598407150705\n",
      "76 Train Loss 28449.963 Test MSE 3922.6283015365384 Test RE 0.11102683019114717\n",
      "77 Train Loss 28303.236 Test MSE 3897.5312416002294 Test RE 0.11067108424107391\n",
      "78 Train Loss 28070.408 Test MSE 3874.4170610059036 Test RE 0.11034243064006226\n",
      "79 Train Loss 27673.799 Test MSE 3800.705540135339 Test RE 0.10928774730715098\n",
      "80 Train Loss 27421.393 Test MSE 3827.5813330752135 Test RE 0.10967346789166718\n",
      "81 Train Loss 27103.764 Test MSE 3656.786870984029 Test RE 0.10719861805706384\n",
      "82 Train Loss 26582.557 Test MSE 3734.6630428669732 Test RE 0.10833407368340552\n",
      "83 Train Loss 25695.178 Test MSE 3412.0074376805323 Test RE 0.10354862748833038\n",
      "84 Train Loss 25181.238 Test MSE 3185.6528058177164 Test RE 0.10005495086690061\n",
      "85 Train Loss 25005.615 Test MSE 3091.7049688907396 Test RE 0.09856855369872701\n",
      "86 Train Loss 24589.004 Test MSE 3101.732909521213 Test RE 0.09872827776862603\n",
      "87 Train Loss 24406.56 Test MSE 3081.1045648380396 Test RE 0.09839942959597346\n",
      "88 Train Loss 24270.33 Test MSE 3009.618593848242 Test RE 0.09725122776908177\n",
      "89 Train Loss 23972.246 Test MSE 2905.6191401092965 Test RE 0.09555616375716035\n",
      "90 Train Loss 23880.635 Test MSE 2772.153739361542 Test RE 0.09333574925968037\n",
      "91 Train Loss 23744.36 Test MSE 2645.1236707419753 Test RE 0.0911721829235959\n",
      "92 Train Loss 23387.584 Test MSE 2621.9361833195976 Test RE 0.09077168987622115\n",
      "93 Train Loss 23211.443 Test MSE 2699.9239876166994 Test RE 0.09211177068171454\n",
      "94 Train Loss 23152.709 Test MSE 2676.820948450156 Test RE 0.09171682736988243\n",
      "95 Train Loss 23064.85 Test MSE 2774.569189794907 Test RE 0.09337640335193283\n",
      "96 Train Loss 23014.053 Test MSE 2772.412447090829 Test RE 0.09334010437824233\n",
      "97 Train Loss 22878.021 Test MSE 2606.6003425923877 Test RE 0.09050583639735105\n",
      "98 Train Loss 22754.428 Test MSE 2632.5685347507047 Test RE 0.09095555019822692\n",
      "99 Train Loss 22684.541 Test MSE 2586.9998135647706 Test RE 0.09016491153383882\n",
      "Training time: 368.28\n",
      "3D_HTTP_swish\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 250030.58 Test MSE 89046.44577128584 Test RE 0.528990237224227\n",
      "1 Train Loss 240943.25 Test MSE 83978.07033855762 Test RE 0.5137150719935063\n",
      "2 Train Loss 207971.67 Test MSE 62413.05418708171 Test RE 0.44287083382875486\n",
      "3 Train Loss 190398.48 Test MSE 53948.69077057228 Test RE 0.41174640599355533\n",
      "4 Train Loss 184392.81 Test MSE 49438.82731163823 Test RE 0.3941608107553591\n",
      "5 Train Loss 177448.83 Test MSE 46058.542269745085 Test RE 0.38044725639507104\n",
      "6 Train Loss 172794.88 Test MSE 44663.508296579086 Test RE 0.3746414097948495\n",
      "7 Train Loss 166702.67 Test MSE 41027.43344124869 Test RE 0.35906786300667504\n",
      "8 Train Loss 155310.89 Test MSE 34743.741738212106 Test RE 0.3304286197435097\n",
      "9 Train Loss 129480.945 Test MSE 27848.003665812103 Test RE 0.2958260372333491\n",
      "10 Train Loss 112474.82 Test MSE 19125.9856942677 Test RE 0.24516093866815772\n",
      "11 Train Loss 94393.43 Test MSE 14002.675253197356 Test RE 0.20977072086696805\n",
      "12 Train Loss 85209.98 Test MSE 12750.510309461217 Test RE 0.20017191572938836\n",
      "13 Train Loss 72233.21 Test MSE 10937.196799739957 Test RE 0.18539259163187957\n",
      "14 Train Loss 68668.41 Test MSE 9506.379471069917 Test RE 0.1728410656691653\n",
      "15 Train Loss 64746.67 Test MSE 8875.185225267109 Test RE 0.1670044621013409\n",
      "16 Train Loss 58659.844 Test MSE 8737.047627221946 Test RE 0.1656996968114298\n",
      "17 Train Loss 53395.96 Test MSE 6971.808671605619 Test RE 0.1480171685678894\n",
      "18 Train Loss 51085.89 Test MSE 6576.220959613162 Test RE 0.1437565232075846\n",
      "19 Train Loss 46776.824 Test MSE 6105.917100519045 Test RE 0.13852074352818675\n",
      "20 Train Loss 44211.227 Test MSE 5682.365620108983 Test RE 0.13362999410148116\n",
      "21 Train Loss 42302.445 Test MSE 5236.984683988138 Test RE 0.12828622159800054\n",
      "22 Train Loss 41387.94 Test MSE 5118.525207823897 Test RE 0.12682701920759532\n",
      "23 Train Loss 39494.586 Test MSE 5014.8098942700235 Test RE 0.12553551229861384\n",
      "24 Train Loss 38004.312 Test MSE 4878.976458138423 Test RE 0.12382368471278771\n",
      "25 Train Loss 36046.67 Test MSE 4213.930133053482 Test RE 0.11507554160056\n",
      "26 Train Loss 35639.29 Test MSE 4272.29123510621 Test RE 0.11586967459061162\n",
      "27 Train Loss 34874.016 Test MSE 4234.983301827894 Test RE 0.11536264725327468\n",
      "28 Train Loss 34615.914 Test MSE 4298.279608679726 Test RE 0.11622155824205614\n",
      "29 Train Loss 34338.113 Test MSE 4222.225659848211 Test RE 0.11518875454655123\n",
      "30 Train Loss 34260.965 Test MSE 4305.913970611893 Test RE 0.11632472554236442\n",
      "31 Train Loss 34181.715 Test MSE 4237.49211260764 Test RE 0.11539681270109443\n",
      "32 Train Loss 33926.61 Test MSE 4155.6302934854375 Test RE 0.11427673244572736\n",
      "33 Train Loss 33680.2 Test MSE 4166.523838837708 Test RE 0.11442641659924836\n",
      "34 Train Loss 33578.543 Test MSE 4162.972892868111 Test RE 0.11437764589156854\n",
      "35 Train Loss 33338.805 Test MSE 4223.632251819319 Test RE 0.11520793993578374\n",
      "36 Train Loss 33243.633 Test MSE 4202.995940824135 Test RE 0.11492614717098729\n",
      "37 Train Loss 33161.32 Test MSE 4219.2270587889925 Test RE 0.11514784407690702\n",
      "38 Train Loss 33073.97 Test MSE 4256.912252595245 Test RE 0.11566093832666552\n",
      "39 Train Loss 32947.297 Test MSE 4290.83498402261 Test RE 0.11612086669511172\n",
      "40 Train Loss 32840.277 Test MSE 4339.283160532073 Test RE 0.11677459176536906\n",
      "41 Train Loss 32731.303 Test MSE 4321.967760932317 Test RE 0.1165413712291244\n",
      "42 Train Loss 32626.318 Test MSE 4340.4301294871075 Test RE 0.11679002380530355\n",
      "43 Train Loss 32579.775 Test MSE 4351.486596882084 Test RE 0.11693868001772569\n",
      "44 Train Loss 32515.865 Test MSE 4363.263248861894 Test RE 0.11709681168207282\n",
      "45 Train Loss 32451.145 Test MSE 4336.755204119224 Test RE 0.1167405718508021\n",
      "46 Train Loss 32329.191 Test MSE 4386.008511406612 Test RE 0.11740162204797748\n",
      "47 Train Loss 32211.947 Test MSE 4313.333837268262 Test RE 0.11642490664828077\n",
      "48 Train Loss 31982.34 Test MSE 4400.112208900212 Test RE 0.11759022952496025\n",
      "49 Train Loss 31943.234 Test MSE 4413.091137024172 Test RE 0.11776352866759769\n",
      "50 Train Loss 31844.969 Test MSE 4446.308093820672 Test RE 0.11820589578074336\n",
      "51 Train Loss 31738.318 Test MSE 4512.289987318718 Test RE 0.11907973605572958\n",
      "52 Train Loss 31676.463 Test MSE 4547.43901721053 Test RE 0.11954262938363458\n",
      "53 Train Loss 31548.072 Test MSE 4617.18435763115 Test RE 0.12045587037295509\n",
      "54 Train Loss 31353.023 Test MSE 4604.393441107122 Test RE 0.12028890611326094\n",
      "55 Train Loss 31184.5 Test MSE 4539.893803248214 Test RE 0.11944341429049689\n",
      "56 Train Loss 31121.492 Test MSE 4586.393883560987 Test RE 0.12005355836689867\n",
      "57 Train Loss 31063.88 Test MSE 4543.746174846698 Test RE 0.11949408098712017\n",
      "58 Train Loss 30989.643 Test MSE 4596.367673718149 Test RE 0.12018402456335446\n",
      "59 Train Loss 30929.72 Test MSE 4560.398576991065 Test RE 0.11971284801124416\n",
      "60 Train Loss 30802.139 Test MSE 4561.2920993380585 Test RE 0.11972457515133292\n",
      "61 Train Loss 30656.053 Test MSE 4484.957298928459 Test RE 0.11871853219219881\n",
      "62 Train Loss 30573.39 Test MSE 4511.953267431445 Test RE 0.11907529293887861\n",
      "63 Train Loss 30555.158 Test MSE 4480.471949359198 Test RE 0.11865915288449098\n",
      "64 Train Loss 30530.643 Test MSE 4501.322116861518 Test RE 0.11893492646543564\n",
      "65 Train Loss 30455.508 Test MSE 4450.5387113883835 Test RE 0.11826211827242561\n",
      "66 Train Loss 30437.82 Test MSE 4453.374705635399 Test RE 0.11829979205710604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 Train Loss 30413.477 Test MSE 4454.84402509164 Test RE 0.11831930600581879\n",
      "68 Train Loss 30384.697 Test MSE 4459.098735448659 Test RE 0.11837579442961119\n",
      "69 Train Loss 30372.484 Test MSE 4431.89264777335 Test RE 0.11801412161053325\n",
      "70 Train Loss 30354.732 Test MSE 4422.238215391483 Test RE 0.11788551058365367\n",
      "71 Train Loss 30280.578 Test MSE 4420.72959288547 Test RE 0.11786540086816404\n",
      "72 Train Loss 30208.188 Test MSE 4443.275453692108 Test RE 0.11816557726461548\n",
      "73 Train Loss 30151.984 Test MSE 4382.686624630137 Test RE 0.11735715465049938\n",
      "74 Train Loss 30106.895 Test MSE 4383.227835962386 Test RE 0.11736440055543566\n",
      "75 Train Loss 30034.357 Test MSE 4322.583365543847 Test RE 0.1165496707879132\n",
      "76 Train Loss 29965.047 Test MSE 4346.559262152295 Test RE 0.11687245446348976\n",
      "77 Train Loss 29911.213 Test MSE 4293.597876281335 Test RE 0.11615824611141677\n",
      "78 Train Loss 29840.738 Test MSE 4289.184365743479 Test RE 0.11609852959025226\n",
      "79 Train Loss 29759.203 Test MSE 4308.777494484276 Test RE 0.11636339831541681\n",
      "80 Train Loss 29653.45 Test MSE 4266.148966408964 Test RE 0.1157863517758012\n",
      "81 Train Loss 29569.55 Test MSE 4334.9682154479915 Test RE 0.11671651751015082\n",
      "82 Train Loss 29514.97 Test MSE 4365.22022438883 Test RE 0.11712306839553276\n",
      "83 Train Loss 29457.791 Test MSE 4349.083170478311 Test RE 0.11690638159238025\n",
      "84 Train Loss 29406.926 Test MSE 4303.774211044302 Test RE 0.11629581903419248\n",
      "85 Train Loss 29376.486 Test MSE 4224.367614578106 Test RE 0.11521796873756517\n",
      "86 Train Loss 29287.064 Test MSE 4281.0696035354595 Test RE 0.11598865345759451\n",
      "87 Train Loss 29208.4 Test MSE 4206.543986258955 Test RE 0.11497464557289329\n",
      "88 Train Loss 29168.404 Test MSE 4216.1227419621555 Test RE 0.11510547598921643\n",
      "89 Train Loss 28966.727 Test MSE 4189.161166455746 Test RE 0.11473684317141478\n",
      "90 Train Loss 28871.857 Test MSE 4170.99856676256 Test RE 0.1144878454665302\n",
      "91 Train Loss 28790.215 Test MSE 4106.744721866409 Test RE 0.11360258559343894\n",
      "92 Train Loss 28403.615 Test MSE 4100.280269245423 Test RE 0.11351313910880328\n",
      "93 Train Loss 28142.758 Test MSE 3898.235007965964 Test RE 0.11068107557485636\n",
      "94 Train Loss 27662.654 Test MSE 3736.986404169458 Test RE 0.1083677661555028\n",
      "95 Train Loss 26812.307 Test MSE 3392.320470876942 Test RE 0.10324946233165852\n",
      "96 Train Loss 26420.256 Test MSE 3031.9043926586114 Test RE 0.09761062944048508\n",
      "97 Train Loss 26143.328 Test MSE 3006.389103939474 Test RE 0.09719903574734819\n",
      "98 Train Loss 26002.89 Test MSE 2882.2286330823695 Test RE 0.09517076852562761\n",
      "99 Train Loss 25483.334 Test MSE 2782.8157789970955 Test RE 0.09351506731358462\n",
      "Training time: 367.25\n",
      "3D_HTTP_swish\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 248026.75 Test MSE 87378.50516215699 Test RE 0.5240125248697439\n",
      "1 Train Loss 237967.67 Test MSE 79663.17412802567 Test RE 0.500343388592658\n",
      "2 Train Loss 193314.97 Test MSE 54093.61492875164 Test RE 0.4122990790931041\n",
      "3 Train Loss 163665.19 Test MSE 39050.0996322803 Test RE 0.3503083064367907\n",
      "4 Train Loss 138258.97 Test MSE 27214.58476345647 Test RE 0.29244231849500074\n",
      "5 Train Loss 106707.29 Test MSE 18319.770784494096 Test RE 0.23993819087042795\n",
      "6 Train Loss 76358.164 Test MSE 7871.991955146068 Test RE 0.15728296332909852\n",
      "7 Train Loss 51050.887 Test MSE 6321.162256103259 Test RE 0.14094115667100446\n",
      "8 Train Loss 39925.777 Test MSE 5690.166246027017 Test RE 0.13372168479557997\n",
      "9 Train Loss 36157.133 Test MSE 4982.473846802682 Test RE 0.12513012432852913\n",
      "10 Train Loss 34842.69 Test MSE 4930.64753328564 Test RE 0.12447763869443765\n",
      "11 Train Loss 32707.18 Test MSE 4706.8060875107385 Test RE 0.12161930448332041\n",
      "12 Train Loss 32493.82 Test MSE 4630.479291017413 Test RE 0.12062916880346809\n",
      "13 Train Loss 31357.691 Test MSE 4531.002544875412 Test RE 0.11932639363000767\n",
      "14 Train Loss 31266.342 Test MSE 4539.554800924007 Test RE 0.1194389546761607\n",
      "15 Train Loss 30838.262 Test MSE 4508.642521392915 Test RE 0.11903159784986078\n",
      "16 Train Loss 30721.832 Test MSE 4515.837919861299 Test RE 0.11912654198532141\n",
      "17 Train Loss 30423.477 Test MSE 4329.988619747907 Test RE 0.11664946187758661\n",
      "18 Train Loss 30340.027 Test MSE 4377.457606413755 Test RE 0.1172871238791027\n",
      "19 Train Loss 29809.15 Test MSE 4185.229889597596 Test RE 0.11468299370706725\n",
      "20 Train Loss 29446.875 Test MSE 4162.541209026472 Test RE 0.11437171548283054\n",
      "21 Train Loss 29263.914 Test MSE 4074.890267529287 Test RE 0.11316114190968933\n",
      "22 Train Loss 29188.73 Test MSE 3984.7220674477953 Test RE 0.11190213682678347\n",
      "23 Train Loss 28550.553 Test MSE 3943.4429346277775 Test RE 0.11132101115511206\n",
      "24 Train Loss 28272.213 Test MSE 3662.981711486935 Test RE 0.1072893804331819\n",
      "25 Train Loss 28140.084 Test MSE 3624.393749351341 Test RE 0.10672276009716733\n",
      "26 Train Loss 27689.104 Test MSE 3534.0220674320126 Test RE 0.10538383303009699\n",
      "27 Train Loss 27580.01 Test MSE 3604.488408340736 Test RE 0.10642929339913411\n",
      "28 Train Loss 27294.719 Test MSE 3440.8046341834215 Test RE 0.10398468233892022\n",
      "29 Train Loss 27038.707 Test MSE 3507.8217122280744 Test RE 0.10499246161554071\n",
      "30 Train Loss 26877.934 Test MSE 3374.141328526655 Test RE 0.10297243837402006\n",
      "31 Train Loss 26556.762 Test MSE 3292.960940483384 Test RE 0.10172616023280351\n",
      "32 Train Loss 26258.432 Test MSE 3243.439765458089 Test RE 0.10095835850109651\n",
      "33 Train Loss 25993.625 Test MSE 2929.2286825837477 Test RE 0.09594359804882643\n",
      "34 Train Loss 25773.828 Test MSE 2705.9181422713245 Test RE 0.09221396357516463\n",
      "35 Train Loss 25240.184 Test MSE 2777.399737174919 Test RE 0.09342402136763939\n",
      "36 Train Loss 24867.725 Test MSE 2736.438719910137 Test RE 0.0927325549779783\n",
      "37 Train Loss 24269.559 Test MSE 2470.554096835859 Test RE 0.08811230169630127\n",
      "38 Train Loss 24084.195 Test MSE 2341.7901471064642 Test RE 0.0857853938318504\n",
      "39 Train Loss 23309.672 Test MSE 2397.971008734495 Test RE 0.08680831498298079\n",
      "40 Train Loss 23014.021 Test MSE 2723.4159291648084 Test RE 0.09251163337036945\n",
      "41 Train Loss 22393.164 Test MSE 2584.6948601516333 Test RE 0.09012473521993279\n",
      "42 Train Loss 21669.953 Test MSE 2378.562079736502 Test RE 0.08645629247177393\n",
      "43 Train Loss 21492.42 Test MSE 2203.8451896041215 Test RE 0.08322042017573222\n",
      "44 Train Loss 21294.35 Test MSE 2037.8750915922046 Test RE 0.08002545327782754\n",
      "45 Train Loss 20765.625 Test MSE 1948.1481757327138 Test RE 0.07824387584381741\n",
      "46 Train Loss 20591.656 Test MSE 1880.1911546040014 Test RE 0.07686707668158774\n",
      "47 Train Loss 20339.922 Test MSE 1924.7412851410543 Test RE 0.07777240751755286\n",
      "48 Train Loss 20249.166 Test MSE 1980.1302502273693 Test RE 0.07888351267402076\n",
      "49 Train Loss 20099.016 Test MSE 1994.6663519764352 Test RE 0.07917252448393296\n",
      "50 Train Loss 19695.943 Test MSE 1980.7665527097847 Test RE 0.07889618601791169\n",
      "51 Train Loss 19441.354 Test MSE 1913.4372197647697 Test RE 0.07754369132570235\n",
      "52 Train Loss 19240.84 Test MSE 1787.9053197485719 Test RE 0.07495690058767181\n",
      "53 Train Loss 19024.004 Test MSE 1755.236022673268 Test RE 0.07426892261857884\n",
      "54 Train Loss 18877.61 Test MSE 1850.2735231377128 Test RE 0.07625306929178648\n",
      "55 Train Loss 18783.926 Test MSE 1776.6281939635048 Test RE 0.07472013314619559\n",
      "56 Train Loss 18669.447 Test MSE 1960.7803181877787 Test RE 0.07849713962293382\n",
      "57 Train Loss 18432.969 Test MSE 1754.493938502102 Test RE 0.07425322113521711\n",
      "58 Train Loss 18078.686 Test MSE 1780.890813386266 Test RE 0.07480971651439422\n",
      "59 Train Loss 17862.055 Test MSE 1810.4977562191968 Test RE 0.07542900131487676\n",
      "60 Train Loss 17805.873 Test MSE 1687.1725472449525 Test RE 0.07281470800703674\n",
      "61 Train Loss 17700.527 Test MSE 1720.4536805264224 Test RE 0.0735293716240962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 Train Loss 17589.65 Test MSE 1603.9894844725302 Test RE 0.0709970200643028\n",
      "63 Train Loss 17540.188 Test MSE 1620.2326838761246 Test RE 0.07135559905308801\n",
      "64 Train Loss 17384.197 Test MSE 1645.1910764166666 Test RE 0.07190308676046617\n",
      "65 Train Loss 17196.201 Test MSE 1690.5811603480088 Test RE 0.07288822506841375\n",
      "66 Train Loss 17138.428 Test MSE 1680.4705368632851 Test RE 0.07266994198289393\n",
      "67 Train Loss 17110.477 Test MSE 1708.9085862699915 Test RE 0.07328224713750192\n",
      "68 Train Loss 17070.574 Test MSE 1770.889913552457 Test RE 0.07459936734611157\n",
      "69 Train Loss 17019.033 Test MSE 1918.129307273784 Test RE 0.07763870855533166\n",
      "70 Train Loss 16945.932 Test MSE 1866.8619405492575 Test RE 0.07659412569388162\n",
      "71 Train Loss 16859.44 Test MSE 1878.0392615237615 Test RE 0.07682307661136058\n",
      "72 Train Loss 16826.195 Test MSE 1805.411572200705 Test RE 0.0753229764384722\n",
      "73 Train Loss 16814.842 Test MSE 1808.4003906450243 Test RE 0.07538529840669123\n",
      "74 Train Loss 16738.654 Test MSE 1774.8157012447102 Test RE 0.07468200917172967\n",
      "75 Train Loss 16699.877 Test MSE 1837.5017234526713 Test RE 0.07598943925427291\n",
      "76 Train Loss 16645.959 Test MSE 1957.7756798615599 Test RE 0.07843697328616586\n",
      "77 Train Loss 16537.469 Test MSE 2041.4534475701498 Test RE 0.08009568181245837\n",
      "78 Train Loss 16468.85 Test MSE 2009.2656795286532 Test RE 0.0794617353396264\n",
      "79 Train Loss 16439.275 Test MSE 2011.1711226313455 Test RE 0.07949940430932469\n",
      "80 Train Loss 16382.681 Test MSE 2016.3748946894548 Test RE 0.07960218758731631\n",
      "81 Train Loss 16302.029 Test MSE 1809.7757638793871 Test RE 0.07541395998401457\n",
      "82 Train Loss 16064.156 Test MSE 1998.111118059364 Test RE 0.07924086001723682\n",
      "83 Train Loss 15895.79 Test MSE 1885.991641745218 Test RE 0.07698555483150907\n",
      "84 Train Loss 15826.141 Test MSE 1838.9122366655401 Test RE 0.07601859937568006\n",
      "85 Train Loss 15782.866 Test MSE 1785.6127936563164 Test RE 0.07490882875491486\n",
      "86 Train Loss 15682.134 Test MSE 1889.6904867893873 Test RE 0.07706101066871733\n",
      "87 Train Loss 15428.951 Test MSE 1940.50944604916 Test RE 0.07809032724352338\n",
      "88 Train Loss 15322.43 Test MSE 1931.790488903701 Test RE 0.0779146948223899\n",
      "89 Train Loss 15268.82 Test MSE 1924.0790364344675 Test RE 0.07775902673028291\n",
      "90 Train Loss 15183.313 Test MSE 1942.8509375720098 Test RE 0.0781374263996537\n",
      "91 Train Loss 15120.936 Test MSE 2051.982021371928 Test RE 0.08030195856950184\n",
      "92 Train Loss 15025.919 Test MSE 2094.386193739955 Test RE 0.08112743504308043\n",
      "93 Train Loss 14877.598 Test MSE 2180.2286463718174 Test RE 0.08277332154504968\n",
      "94 Train Loss 14733.325 Test MSE 2225.2172621601976 Test RE 0.08362296696430987\n",
      "95 Train Loss 14634.61 Test MSE 2152.1884962408603 Test RE 0.08223932082798456\n",
      "96 Train Loss 14584.444 Test MSE 2128.038947735293 Test RE 0.08177661849998404\n",
      "97 Train Loss 14544.737 Test MSE 2124.8531937309663 Test RE 0.08171538424360571\n",
      "98 Train Loss 14459.784 Test MSE 2188.6025414378896 Test RE 0.08293212847362524\n",
      "99 Train Loss 14315.903 Test MSE 2158.548474596965 Test RE 0.0823607447783945\n",
      "Training time: 369.21\n",
      "3D_HTTP_swish\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 243283.66 Test MSE 87288.16715500115 Test RE 0.5237415744291299\n",
      "1 Train Loss 232720.67 Test MSE 76943.42176971708 Test RE 0.4917281942924217\n",
      "2 Train Loss 198533.42 Test MSE 54705.79669178476 Test RE 0.4146255262774251\n",
      "3 Train Loss 181402.48 Test MSE 47524.80127593129 Test RE 0.38645552176435477\n",
      "4 Train Loss 174569.08 Test MSE 45216.105774977215 Test RE 0.3769519031071683\n",
      "5 Train Loss 167050.16 Test MSE 40998.68717387574 Test RE 0.3589420487820131\n",
      "6 Train Loss 159857.14 Test MSE 38265.79973943133 Test RE 0.34677258777764997\n",
      "7 Train Loss 144720.39 Test MSE 31988.648107524787 Test RE 0.3170569703880591\n",
      "8 Train Loss 129041.695 Test MSE 26676.10348677862 Test RE 0.28953465913214593\n",
      "9 Train Loss 112202.56 Test MSE 20305.574897612776 Test RE 0.2526079457682459\n",
      "10 Train Loss 105114.86 Test MSE 15713.176496040023 Test RE 0.22221396905116048\n",
      "11 Train Loss 97145.78 Test MSE 13572.585385203776 Test RE 0.20652405967191445\n",
      "12 Train Loss 86250.46 Test MSE 9993.906097219002 Test RE 0.1772176586368057\n",
      "13 Train Loss 72567.71 Test MSE 9817.553055406172 Test RE 0.17564710277033585\n",
      "14 Train Loss 64084.87 Test MSE 7804.877901785594 Test RE 0.15661105635067865\n",
      "15 Train Loss 59681.258 Test MSE 6495.793738115961 Test RE 0.14287474731511968\n",
      "16 Train Loss 49412.832 Test MSE 5558.0048043264 Test RE 0.13215963213551193\n",
      "17 Train Loss 44473.266 Test MSE 5413.116257280708 Test RE 0.13042565891209945\n",
      "18 Train Loss 42258.637 Test MSE 4848.843236050156 Test RE 0.12344071652337349\n",
      "19 Train Loss 40233.457 Test MSE 4426.9885010940725 Test RE 0.1179488087967139\n",
      "20 Train Loss 36259.74 Test MSE 4805.300928509291 Test RE 0.1228852216954429\n",
      "21 Train Loss 34924.08 Test MSE 4610.54319174318 Test RE 0.12036920984325018\n",
      "22 Train Loss 33608.82 Test MSE 4528.120778874784 Test RE 0.11928844116424596\n",
      "23 Train Loss 33059.4 Test MSE 4548.70254823625 Test RE 0.11955923601978509\n",
      "24 Train Loss 32310.52 Test MSE 4419.002100234984 Test RE 0.11784236943130157\n",
      "25 Train Loss 32009.992 Test MSE 4379.600747032503 Test RE 0.11731583141478442\n",
      "26 Train Loss 31276.81 Test MSE 4348.498323590875 Test RE 0.11689852078247182\n",
      "27 Train Loss 31018.379 Test MSE 4384.577037152763 Test RE 0.11738246213145344\n",
      "28 Train Loss 30261.896 Test MSE 4206.857946179625 Test RE 0.11497893612081143\n",
      "29 Train Loss 29407.123 Test MSE 4277.126665432857 Test RE 0.11593522738938566\n",
      "30 Train Loss 28914.73 Test MSE 4235.350977779217 Test RE 0.11536765496493082\n",
      "31 Train Loss 28386.158 Test MSE 4052.9988956613624 Test RE 0.11285676699963504\n",
      "32 Train Loss 28160.475 Test MSE 4023.1097072937023 Test RE 0.11243986099066398\n",
      "33 Train Loss 27922.889 Test MSE 3806.774021539592 Test RE 0.10937496086971855\n",
      "34 Train Loss 27856.406 Test MSE 3786.725008831921 Test RE 0.10908655986899442\n",
      "35 Train Loss 27686.586 Test MSE 3736.5954959923442 Test RE 0.10836209809182536\n",
      "36 Train Loss 27385.564 Test MSE 3802.2914877002977 Test RE 0.1093105465687039\n",
      "37 Train Loss 26647.871 Test MSE 3681.5010238016334 Test RE 0.10756025545431243\n",
      "38 Train Loss 26389.215 Test MSE 3725.976470031647 Test RE 0.10820801148532096\n",
      "39 Train Loss 26012.076 Test MSE 3846.164233046281 Test RE 0.10993937777174062\n",
      "40 Train Loss 25616.203 Test MSE 3905.0224830031457 Test RE 0.11077739074182061\n",
      "41 Train Loss 24830.93 Test MSE 3392.328849775717 Test RE 0.10324958984264498\n",
      "42 Train Loss 24028.363 Test MSE 3243.8448920580363 Test RE 0.10096466348012433\n",
      "43 Train Loss 23580.496 Test MSE 3070.1588925612396 Test RE 0.09822449133176227\n",
      "44 Train Loss 23173.436 Test MSE 2929.374990243182 Test RE 0.09594599409050017\n",
      "45 Train Loss 22853.12 Test MSE 2840.8788213330745 Test RE 0.0944856199093886\n",
      "46 Train Loss 22162.535 Test MSE 2364.832718704661 Test RE 0.08620641304086268\n",
      "47 Train Loss 21170.736 Test MSE 2199.901716328828 Test RE 0.0831459311773855\n",
      "48 Train Loss 20874.043 Test MSE 2204.1904204106004 Test RE 0.0832269381307712\n",
      "49 Train Loss 20586.254 Test MSE 2034.7200010804197 Test RE 0.07996348055142477\n",
      "50 Train Loss 20461.172 Test MSE 2135.2766080139063 Test RE 0.08191556543643891\n",
      "51 Train Loss 20334.918 Test MSE 2122.382698108792 Test RE 0.08166786655795523\n",
      "52 Train Loss 20066.854 Test MSE 1973.022698452361 Test RE 0.07874181172364632\n",
      "53 Train Loss 19940.469 Test MSE 2008.6927519343847 Test RE 0.07945040556200393\n",
      "54 Train Loss 19867.666 Test MSE 1966.411692202099 Test RE 0.07860978095578311\n",
      "55 Train Loss 19718.902 Test MSE 1887.3297481250327 Test RE 0.0770128605188928\n",
      "56 Train Loss 19610.785 Test MSE 1883.7978198113885 Test RE 0.07694076626023219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 Train Loss 19498.742 Test MSE 1990.9829235110149 Test RE 0.07909938917319354\n",
      "58 Train Loss 19383.535 Test MSE 2115.9276957517063 Test RE 0.08154357989834204\n",
      "59 Train Loss 19181.053 Test MSE 2021.6506531680454 Test RE 0.07970625741403442\n",
      "60 Train Loss 18992.723 Test MSE 1959.0114895848642 Test RE 0.07846172532575287\n",
      "61 Train Loss 18897.95 Test MSE 1854.739252858708 Test RE 0.07634503417486874\n",
      "62 Train Loss 18690.541 Test MSE 1804.5823611079902 Test RE 0.07530567683158543\n",
      "63 Train Loss 18577.15 Test MSE 1797.2193579766658 Test RE 0.07515188981356868\n",
      "64 Train Loss 18541.379 Test MSE 1798.4869155713995 Test RE 0.07517838701266297\n",
      "65 Train Loss 18503.084 Test MSE 1831.9460486089968 Test RE 0.0758744754958606\n",
      "66 Train Loss 18241.5 Test MSE 1892.4823426831397 Test RE 0.07711791518113247\n",
      "67 Train Loss 18077.023 Test MSE 1964.8507801320156 Test RE 0.07857857505010231\n",
      "68 Train Loss 18002.598 Test MSE 1846.8537625249714 Test RE 0.0761825694868437\n",
      "69 Train Loss 17927.816 Test MSE 1888.2244959118332 Test RE 0.0770311135352556\n",
      "70 Train Loss 17884.486 Test MSE 1864.2350783909285 Test RE 0.0765402189122042\n",
      "71 Train Loss 17778.3 Test MSE 2025.9797684496173 Test RE 0.07979155233119421\n",
      "72 Train Loss 17706.568 Test MSE 2060.9836651043365 Test RE 0.08047790031719515\n",
      "73 Train Loss 17673.99 Test MSE 2036.6419670416283 Test RE 0.08000123778882838\n",
      "74 Train Loss 17581.059 Test MSE 2104.764430721821 Test RE 0.08132819058953723\n",
      "75 Train Loss 17475.412 Test MSE 2129.521402107511 Test RE 0.08180509753698856\n",
      "76 Train Loss 17424.465 Test MSE 2094.4960767964394 Test RE 0.08112956321161746\n",
      "77 Train Loss 17347.234 Test MSE 2059.201273291204 Test RE 0.08044309310842286\n",
      "78 Train Loss 17305.857 Test MSE 2081.141470322276 Test RE 0.0808705066376807\n",
      "79 Train Loss 17257.787 Test MSE 2048.0994079839816 Test RE 0.08022595179202999\n",
      "80 Train Loss 17113.736 Test MSE 2001.581717917629 Test RE 0.0793096484840612\n",
      "81 Train Loss 17038.293 Test MSE 1939.4579116755267 Test RE 0.0780691663595302\n",
      "82 Train Loss 16994.197 Test MSE 2005.1231478928582 Test RE 0.07937977938029313\n",
      "83 Train Loss 16924.658 Test MSE 2129.6033677238024 Test RE 0.08180667186742473\n",
      "84 Train Loss 16860.049 Test MSE 2153.563095887968 Test RE 0.0822655797027873\n",
      "85 Train Loss 16822.883 Test MSE 2096.0106920905987 Test RE 0.08115889195368177\n",
      "86 Train Loss 16791.428 Test MSE 2060.7675993656985 Test RE 0.08047368170716174\n",
      "87 Train Loss 16755.967 Test MSE 2116.0215021718227 Test RE 0.08154538743329488\n",
      "88 Train Loss 16546.117 Test MSE 2006.4959688881722 Test RE 0.07940694867953293\n",
      "89 Train Loss 16466.57 Test MSE 2126.7660304258197 Test RE 0.08175215690720376\n",
      "90 Train Loss 16445.037 Test MSE 2080.8878725161885 Test RE 0.08086557924370961\n",
      "91 Train Loss 16332.82 Test MSE 2063.830476593056 Test RE 0.08053346270581511\n",
      "92 Train Loss 16129.306 Test MSE 2317.1990184331376 Test RE 0.08533378909474774\n",
      "93 Train Loss 16031.902 Test MSE 2282.7589816000127 Test RE 0.08469726612318076\n",
      "94 Train Loss 15966.594 Test MSE 2383.7961031391465 Test RE 0.08655136369068088\n",
      "95 Train Loss 15740.749 Test MSE 2332.342644209606 Test RE 0.08561217659891132\n",
      "96 Train Loss 15609.477 Test MSE 2519.174740329762 Test RE 0.08897510489993164\n",
      "97 Train Loss 15489.607 Test MSE 2473.5238182869407 Test RE 0.08816524334316175\n",
      "98 Train Loss 15361.651 Test MSE 2443.29657037775 Test RE 0.08762488376728132\n",
      "99 Train Loss 15125.211 Test MSE 2381.488717757997 Test RE 0.08650946503565116\n",
      "Training time: 368.36\n",
      "3D_HTTP_swish\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 242764.72 Test MSE 88214.19339072962 Test RE 0.5265123904224736\n",
      "1 Train Loss 236566.08 Test MSE 84318.61147268064 Test RE 0.5147556062628615\n",
      "2 Train Loss 211526.38 Test MSE 69691.69190439957 Test RE 0.4679827727131472\n",
      "3 Train Loss 184087.36 Test MSE 51333.82696314798 Test RE 0.4016439073144494\n",
      "4 Train Loss 159325.34 Test MSE 39765.72567172681 Test RE 0.3535035815808294\n",
      "5 Train Loss 151028.89 Test MSE 35798.066450263796 Test RE 0.3354047023851336\n",
      "6 Train Loss 137913.75 Test MSE 26417.98253242594 Test RE 0.2881304692640844\n",
      "7 Train Loss 122142.3 Test MSE 24309.784477787325 Test RE 0.27639483078595256\n",
      "8 Train Loss 108094.055 Test MSE 19412.251375839096 Test RE 0.24698883147731693\n",
      "9 Train Loss 87629.83 Test MSE 12284.083417171449 Test RE 0.19647655772196757\n",
      "10 Train Loss 76237.97 Test MSE 10907.798680393014 Test RE 0.1851432653665504\n",
      "11 Train Loss 63786.004 Test MSE 6942.7737694822845 Test RE 0.14770862970260104\n",
      "12 Train Loss 57167.58 Test MSE 7525.009388962888 Test RE 0.15377753154259796\n",
      "13 Train Loss 50256.793 Test MSE 6414.351214776497 Test RE 0.1419762595108366\n",
      "14 Train Loss 46285.75 Test MSE 5509.093762078984 Test RE 0.13157683762407707\n",
      "15 Train Loss 42752.062 Test MSE 5018.854825682644 Test RE 0.12558613038734368\n",
      "16 Train Loss 40745.695 Test MSE 6457.0226127308215 Test RE 0.14244772442658563\n",
      "17 Train Loss 38416.508 Test MSE 5673.367762868781 Test RE 0.13352415262169406\n",
      "18 Train Loss 35837.957 Test MSE 5207.300002390995 Test RE 0.12792212397195088\n",
      "19 Train Loss 34022.633 Test MSE 4716.445222633244 Test RE 0.12174377374192695\n",
      "20 Train Loss 32547.5 Test MSE 4194.019660325087 Test RE 0.11480335848220134\n",
      "21 Train Loss 31490.604 Test MSE 4301.221813436907 Test RE 0.11626132870329427\n",
      "22 Train Loss 30553.27 Test MSE 4153.558388807805 Test RE 0.1142482409776157\n",
      "23 Train Loss 30386.469 Test MSE 4074.5354812742094 Test RE 0.11315621553265254\n",
      "24 Train Loss 30132.256 Test MSE 3845.698929804287 Test RE 0.10993272741919945\n",
      "25 Train Loss 29536.166 Test MSE 4132.72834489468 Test RE 0.11396140414756845\n",
      "26 Train Loss 29307.205 Test MSE 4030.774999659561 Test RE 0.1125469267086043\n",
      "27 Train Loss 29208.998 Test MSE 3893.181934658758 Test RE 0.11060931733733802\n",
      "28 Train Loss 28813.385 Test MSE 3921.554523049841 Test RE 0.11101163293406051\n",
      "29 Train Loss 28433.541 Test MSE 3922.5503629399673 Test RE 0.11102572719111185\n",
      "30 Train Loss 28355.611 Test MSE 3925.779424282325 Test RE 0.11107141623259398\n",
      "31 Train Loss 28118.088 Test MSE 3971.645645803007 Test RE 0.11171837470207983\n",
      "32 Train Loss 27760.227 Test MSE 3677.4083572427962 Test RE 0.10750045231030882\n",
      "33 Train Loss 26232.17 Test MSE 3693.884318343747 Test RE 0.10774100131593108\n",
      "34 Train Loss 25301.934 Test MSE 3222.508753891264 Test RE 0.10063207205796\n",
      "35 Train Loss 24230.975 Test MSE 3484.6847590175166 Test RE 0.10464563318667357\n",
      "36 Train Loss 23673.078 Test MSE 3338.7069386210655 Test RE 0.10243031588791603\n",
      "37 Train Loss 23268.637 Test MSE 3552.7186250708714 Test RE 0.10566222911531274\n",
      "38 Train Loss 22495.89 Test MSE 3207.4012297665913 Test RE 0.10039590706936735\n",
      "39 Train Loss 22236.213 Test MSE 3064.5073921530848 Test RE 0.09813404463669217\n",
      "40 Train Loss 21701.36 Test MSE 2574.2337991793206 Test RE 0.08994216894268066\n",
      "41 Train Loss 21401.246 Test MSE 2292.9150812279117 Test RE 0.08488546804064127\n",
      "42 Train Loss 21354.865 Test MSE 2111.049424497423 Test RE 0.0814495262961185\n",
      "43 Train Loss 21307.7 Test MSE 2132.5364754816783 Test RE 0.08186298874413062\n",
      "44 Train Loss 21238.07 Test MSE 2181.4881908252555 Test RE 0.08279722766723095\n",
      "45 Train Loss 21149.162 Test MSE 2147.5815484455507 Test RE 0.08215125344261416\n",
      "46 Train Loss 21127.766 Test MSE 2188.842977616491 Test RE 0.08293668374030011\n",
      "47 Train Loss 21091.922 Test MSE 2296.2294091413883 Test RE 0.08494679536273649\n",
      "48 Train Loss 20968.395 Test MSE 2228.4104291797776 Test RE 0.08368294456286013\n",
      "49 Train Loss 20769.46 Test MSE 2161.0049736192173 Test RE 0.08240759606820552\n",
      "50 Train Loss 20658.297 Test MSE 2211.976845842638 Test RE 0.08337381043378828\n",
      "51 Train Loss 20540.455 Test MSE 2130.1624741169294 Test RE 0.08181740993073539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 Train Loss 20511.777 Test MSE 2184.3202390329134 Test RE 0.08285095467962805\n",
      "53 Train Loss 20492.91 Test MSE 2208.7407973013455 Test RE 0.08331280155656658\n",
      "54 Train Loss 20368.914 Test MSE 2310.595013938009 Test RE 0.08521210191545926\n",
      "55 Train Loss 20317.45 Test MSE 2328.8117879351444 Test RE 0.08554734933545415\n",
      "56 Train Loss 20216.559 Test MSE 2159.2084141250048 Test RE 0.08237333401554331\n",
      "57 Train Loss 20078.422 Test MSE 2072.1410390468754 Test RE 0.08069544450986724\n",
      "58 Train Loss 19791.29 Test MSE 2053.3690306175877 Test RE 0.08032909349144701\n",
      "59 Train Loss 19592.025 Test MSE 1808.8945699671629 Test RE 0.07539559792640146\n",
      "60 Train Loss 19507.629 Test MSE 1883.7552614446274 Test RE 0.07693989714046544\n",
      "61 Train Loss 19269.836 Test MSE 1883.4080237829621 Test RE 0.07693280554480808\n",
      "62 Train Loss 18704.24 Test MSE 1839.2771654599503 Test RE 0.07602614187800832\n",
      "63 Train Loss 18235.424 Test MSE 1891.630960766318 Test RE 0.07710056648968658\n",
      "64 Train Loss 18093.193 Test MSE 1757.9004733020852 Test RE 0.07432527140421392\n",
      "65 Train Loss 18061.887 Test MSE 1808.1284698215131 Test RE 0.07537963052385785\n",
      "66 Train Loss 17973.191 Test MSE 1720.5540629505133 Test RE 0.07353151668298023\n",
      "67 Train Loss 17863.71 Test MSE 1759.9439299974797 Test RE 0.07436845825221325\n",
      "68 Train Loss 17832.342 Test MSE 1784.879832371602 Test RE 0.0748934528270782\n",
      "69 Train Loss 17792.9 Test MSE 1827.045152612439 Test RE 0.07577291629719234\n",
      "70 Train Loss 17740.44 Test MSE 1839.0108543367257 Test RE 0.0760206377211162\n",
      "71 Train Loss 17723.809 Test MSE 1791.3983615678958 Test RE 0.07503008674208678\n",
      "72 Train Loss 17714.738 Test MSE 1749.8330040324277 Test RE 0.07415452613585136\n",
      "73 Train Loss 17655.434 Test MSE 1774.8528208123082 Test RE 0.07468279013997085\n",
      "74 Train Loss 17615.697 Test MSE 1827.518041714319 Test RE 0.07578272171114082\n",
      "75 Train Loss 17604.922 Test MSE 1811.0292482490945 Test RE 0.07544007201935694\n",
      "76 Train Loss 17565.02 Test MSE 1838.5722156143697 Test RE 0.0760115710035519\n",
      "77 Train Loss 17551.494 Test MSE 1785.0213946977165 Test RE 0.07489642274163233\n",
      "78 Train Loss 17528.074 Test MSE 1818.4687114342241 Test RE 0.07559486202394569\n",
      "79 Train Loss 17491.918 Test MSE 1834.9115248564156 Test RE 0.07593586184835688\n",
      "80 Train Loss 17466.943 Test MSE 1837.8102034893625 Test RE 0.07599581754523278\n",
      "81 Train Loss 17462.258 Test MSE 1854.6540787361853 Test RE 0.07634328118022528\n",
      "82 Train Loss 17445.38 Test MSE 1842.9872810973638 Test RE 0.07610278168072164\n",
      "83 Train Loss 17419.938 Test MSE 1824.1889953177372 Test RE 0.07571366652612332\n",
      "84 Train Loss 17398.48 Test MSE 1858.212688004414 Test RE 0.07641648774654079\n",
      "85 Train Loss 17359.762 Test MSE 1835.0445069920515 Test RE 0.07593861346062508\n",
      "86 Train Loss 17342.586 Test MSE 1820.6279878105422 Test RE 0.07563972993175679\n",
      "87 Train Loss 17310.662 Test MSE 1821.5541255949233 Test RE 0.07565896612547919\n",
      "88 Train Loss 17286.572 Test MSE 1843.870234352053 Test RE 0.07612100946592869\n",
      "89 Train Loss 17265.404 Test MSE 1842.1875789807748 Test RE 0.0760862687726748\n",
      "90 Train Loss 17234.785 Test MSE 1863.6448208142533 Test RE 0.07652810079970215\n",
      "91 Train Loss 17161.795 Test MSE 1799.9352915718987 Test RE 0.0752086526367751\n",
      "92 Train Loss 17128.107 Test MSE 1843.60776262486 Test RE 0.07611559142632798\n",
      "93 Train Loss 17113.174 Test MSE 1903.644180074991 Test RE 0.07734500108969879\n",
      "94 Train Loss 17097.621 Test MSE 1907.3150618375466 Test RE 0.0774195390765552\n",
      "95 Train Loss 17078.764 Test MSE 1848.723760961782 Test RE 0.07622112837150935\n",
      "96 Train Loss 17057.928 Test MSE 1901.5311115106122 Test RE 0.07730206221721601\n",
      "97 Train Loss 17045.908 Test MSE 1898.8631399632118 Test RE 0.07724781327697561\n",
      "98 Train Loss 17036.496 Test MSE 1895.776029991654 Test RE 0.0771849942423374\n",
      "99 Train Loss 17031.078 Test MSE 1878.7898822749455 Test RE 0.07683842752437775\n",
      "Training time: 231.81\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    beta_val = []\n",
    "\n",
    "\n",
    "    print(reps)\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_D = 5000 #Total number of data points for 'y'\n",
    "    N_N = 3500\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "\n",
    "    layers = np.array([3,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.05, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "    if(nan_flag == 1):\n",
    "        nan_tune.append(tune_reps)\n",
    "        break\n",
    "\n",
    "    #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HT_stan_v3_15Aug2022_MP4Video.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
