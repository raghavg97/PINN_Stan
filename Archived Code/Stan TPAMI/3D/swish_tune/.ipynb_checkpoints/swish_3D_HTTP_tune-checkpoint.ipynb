{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLsZ-c_nCQr2",
    "outputId": "0238c820-5951-4e75-a35b-19e4de8c9b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SV23gJi7JexL",
    "outputId": "6f051579-557f-463f-d7b4-955ed617736e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOyXTKXGJf97",
    "outputId": "11b7b7db-47b0-4cf8-c699-473f1c6b8c5f"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/MURI Aug17 Thin Plate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "APjvgycyCTj0",
    "outputId": "19bce659-211e-4bec-d94d-7c94148b0d09"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1]).reshape(-1,1)\n",
    "b_value = np.array([0.0,0.25,0.5,1.0]).reshape(-1,1)\n",
    "\n",
    "\n",
    "LR_tune, B_value = np.meshgrid(lr_tune,b_value)\n",
    "\n",
    "LR_tune = LR_tune.flatten('F').reshape(-1,1)\n",
    "B_value = B_value.flatten('F').reshape(-1,1)\n",
    "\n",
    "\n",
    "lrb_tune = np.hstack((LR_tune,B_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lxFUD2gACQr7"
   },
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CUcT7YuXCQr7"
   },
   "outputs": [],
   "source": [
    "label = \"Thinplate_stan\"\n",
    "loss_thresh = 10000\n",
    "x = np.linspace(0,1,100).reshape(-1,1)\n",
    "y = np.linspace(0,1,100).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xyt = np.hstack((X,Y,T))\n",
    "\n",
    "initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "xyt_initial = xyt[initial_pts,:]\n",
    "xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "xyt_NBC_x0 = xyt[NBC_pts_x0,:]\n",
    "xyt_NBC_x1 = xyt[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xyt_NBC_y1 = xyt[NBC_pts_y1,:]\n",
    "\n",
    "u_initial = 300*np.ones((np.shape(xyt_initial)[0],1))\n",
    "u_DBC = 1000*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xyt_NBC_x = np.vstack((xyt_NBC_x0,xyt_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xyt_NBC_y = np.vstack((xyt_NBC_y1))\n",
    "\n",
    "u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../3D_HTTP_FEA.mat')\n",
    "xy = fea_data['xy']\n",
    "t = fea_data['t']/3000\n",
    "xyt = np.zeros((497*101,3))\n",
    "u_true = np.ones((497*101,1))\n",
    "\n",
    "\n",
    "for i in range(101):\n",
    "    t_temp = t[0,i]*np.ones((497,1))\n",
    "    xyt[497*i:497*(i+1)] = np.hstack((xy,t_temp))\n",
    "    u_true[497*i:497*(i+1)] = fea_data['u'][:,i].reshape(-1,1)\n",
    "    #print(i)\n",
    "#print(xyt)\n",
    "\n",
    "xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gp2G6x6BCQr8"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xyt_I_DBC.shape[0], N_D, replace=False) \n",
    "    xyt_D = xyt_I_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_I_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_x.shape[0], N_D, replace=False) \n",
    "    xyt_Nx = xyt_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_y.shape[0], N_D, replace=False) \n",
    "    xyt_Ny = xyt_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xyt_coll = lb_xyt + (ub_xyt - lb_xyt)*samples\n",
    "    xyt_coll = np.vstack((xyt_coll, xyt_D,xyt_Nx,xyt_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xyt_coll, xyt_D, u_D, xyt_Nx,xyt_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VRolFlBzCQr9"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.iter = 0\n",
    "\n",
    "    \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/3000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "       \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xyt_coll_np_array, xyt_D_np_array, u_D_np_array,xyt_Nx_np_array,xyt_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "\n",
    "    xyt_coll = torch.from_numpy(xyt_coll_np_array).float().to(device)\n",
    "    xyt_D = torch.from_numpy(xyt_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xyt_Nx = torch.from_numpy(xyt_Nx_np_array).float().to(device)\n",
    "    xyt_Ny = torch.from_numpy(xyt_Ny_np_array).float().to(device)\n",
    "\n",
    "    N_hat = torch.zeros(xyt_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xyt_coll.shape[0],1).to(device)\n",
    "\n",
    "    nan_flag = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,i)\n",
    "\n",
    "        loss_np = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "        if(np.isnan(loss_np)):\n",
    "            nan_flag =1\n",
    "            print(\"NAN BREAK!\")\n",
    "            break\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVnXJfj0CQr-",
    "outputId": "1f2921b0-e258-465d-aa27-cdeb80b78a0b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D_HTTP_stan_tune0\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1235712.9 Test MSE 309898.57204455783 Test RE 0.9868454651639196\n",
      "1 Train Loss 755159.2 Test MSE 294277.9055950005 Test RE 0.9616525563817746\n",
      "2 Train Loss 640241.5 Test MSE 295495.6320661277 Test RE 0.9636401688000372\n",
      "3 Train Loss 587557.25 Test MSE 287451.5633690657 Test RE 0.950433421618312\n",
      "4 Train Loss 553865.44 Test MSE 282291.9660937826 Test RE 0.9418649192608525\n",
      "5 Train Loss 521658.8 Test MSE 250578.79244898513 Test RE 0.8873838453650151\n",
      "6 Train Loss 376643.72 Test MSE 151775.08362335345 Test RE 0.6906207179435279\n",
      "7 Train Loss 278862.28 Test MSE 103313.80375943064 Test RE 0.5697948740474739\n",
      "8 Train Loss 246399.84 Test MSE 85826.64175522426 Test RE 0.5193383837709111\n",
      "9 Train Loss 225682.02 Test MSE 71779.42255083314 Test RE 0.4749406500290026\n",
      "10 Train Loss 215757.08 Test MSE 66894.83965201904 Test RE 0.4584961262375317\n",
      "11 Train Loss 210446.27 Test MSE 63249.757812067124 Test RE 0.4458294936550154\n",
      "12 Train Loss 206907.4 Test MSE 63207.17239592882 Test RE 0.4456793821606057\n",
      "13 Train Loss 201447.05 Test MSE 58547.06040977634 Test RE 0.42893541918941347\n",
      "14 Train Loss 195907.4 Test MSE 55962.76406628225 Test RE 0.4193618703282944\n",
      "15 Train Loss 190343.95 Test MSE 52701.02755859264 Test RE 0.4069573566927765\n",
      "16 Train Loss 189083.66 Test MSE 51872.826663885964 Test RE 0.40374701020724296\n",
      "17 Train Loss 183883.86 Test MSE 50013.81945581569 Test RE 0.3964463038706476\n",
      "18 Train Loss 179886.83 Test MSE 47633.642611446114 Test RE 0.3868977990460818\n",
      "19 Train Loss 175380.16 Test MSE 44456.197573469195 Test RE 0.3737709282814166\n",
      "20 Train Loss 171477.58 Test MSE 43939.7663004967 Test RE 0.37159360692237264\n",
      "21 Train Loss 170038.89 Test MSE 42288.709342098715 Test RE 0.3645453622525482\n",
      "22 Train Loss 168806.02 Test MSE 41331.11286193802 Test RE 0.36039429848215543\n",
      "23 Train Loss 168061.81 Test MSE 40786.33864020747 Test RE 0.3580112900930657\n",
      "24 Train Loss 167622.06 Test MSE 40830.076066747424 Test RE 0.3582031962239808\n",
      "25 Train Loss 167126.8 Test MSE 40359.12245227382 Test RE 0.35613136106709326\n",
      "26 Train Loss 165499.14 Test MSE 39672.77834344227 Test RE 0.35309020504547267\n",
      "27 Train Loss 163793.97 Test MSE 38656.08021954169 Test RE 0.34853650275388753\n",
      "28 Train Loss 162770.38 Test MSE 38340.264157602884 Test RE 0.34710982976659094\n",
      "29 Train Loss 161085.84 Test MSE 38606.81218948935 Test RE 0.3483143231935943\n",
      "30 Train Loss 160878.94 Test MSE 38717.11964657687 Test RE 0.3488115704357723\n",
      "31 Train Loss 160745.06 Test MSE 38640.18922788448 Test RE 0.3484648560691618\n",
      "32 Train Loss 159571.97 Test MSE 37208.459574762746 Test RE 0.3419481098978086\n",
      "33 Train Loss 158171.97 Test MSE 36785.91835019777 Test RE 0.3400009758114963\n",
      "34 Train Loss 157473.0 Test MSE 36278.127740466596 Test RE 0.3376461445495705\n",
      "35 Train Loss 156942.53 Test MSE 36610.81966245142 Test RE 0.33919081881815466\n",
      "36 Train Loss 156693.27 Test MSE 36669.26035440899 Test RE 0.33946143064987244\n",
      "37 Train Loss 155999.77 Test MSE 36340.31104535443 Test RE 0.33793539547043194\n",
      "38 Train Loss 154674.53 Test MSE 35064.59872753368 Test RE 0.3319508605450892\n",
      "39 Train Loss 152942.53 Test MSE 33833.23346831843 Test RE 0.32607020387701746\n",
      "40 Train Loss 151876.17 Test MSE 32432.457846379002 Test RE 0.3192488145895416\n",
      "41 Train Loss 151467.67 Test MSE 31608.05335261466 Test RE 0.3151651852380734\n",
      "42 Train Loss 151074.77 Test MSE 31162.381146408956 Test RE 0.312935389483182\n",
      "43 Train Loss 149067.9 Test MSE 31635.626383455805 Test RE 0.31530262117767466\n",
      "44 Train Loss 148649.25 Test MSE 32270.62098935981 Test RE 0.31845129810940903\n",
      "45 Train Loss 147877.12 Test MSE 32272.221841456816 Test RE 0.31845919673500955\n",
      "46 Train Loss 145305.4 Test MSE 30351.8466352941 Test RE 0.30883884652976745\n",
      "47 Train Loss 143809.28 Test MSE 30291.834795792216 Test RE 0.30853337652070195\n",
      "48 Train Loss 142459.58 Test MSE 30506.4427244597 Test RE 0.30962437756414757\n",
      "49 Train Loss 137369.98 Test MSE 28052.426670288256 Test RE 0.2969098325792898\n",
      "50 Train Loss 134132.72 Test MSE 26357.767174450524 Test RE 0.28780190940628486\n",
      "51 Train Loss 130644.58 Test MSE 23238.13327931801 Test RE 0.2702339947788422\n",
      "52 Train Loss 127109.06 Test MSE 22312.16614456745 Test RE 0.26479527408460535\n",
      "53 Train Loss 124342.86 Test MSE 22751.381117720422 Test RE 0.26738882017988774\n",
      "54 Train Loss 122821.55 Test MSE 23039.161463300006 Test RE 0.2690745957023762\n",
      "55 Train Loss 120734.82 Test MSE 22269.486996380554 Test RE 0.26454190006994843\n",
      "56 Train Loss 118999.96 Test MSE 21367.15402608689 Test RE 0.2591270211417375\n",
      "57 Train Loss 116392.17 Test MSE 18968.6758349889 Test RE 0.24415064141306572\n",
      "58 Train Loss 110552.94 Test MSE 17851.81010010046 Test RE 0.23685387340263422\n",
      "59 Train Loss 108881.484 Test MSE 17519.113548127498 Test RE 0.23463642120233053\n",
      "60 Train Loss 104268.31 Test MSE 16970.084285967823 Test RE 0.23093053509635972\n",
      "61 Train Loss 101057.6 Test MSE 17086.848128447964 Test RE 0.231723639967232\n",
      "62 Train Loss 99447.734 Test MSE 16907.6912935895 Test RE 0.2305056192572493\n",
      "63 Train Loss 95397.46 Test MSE 16554.858075181677 Test RE 0.22808781936516315\n",
      "64 Train Loss 93484.625 Test MSE 15801.325508414699 Test RE 0.2228363940333221\n",
      "65 Train Loss 92898.06 Test MSE 15351.406657555832 Test RE 0.21964101922033538\n",
      "66 Train Loss 91020.79 Test MSE 14584.64229616723 Test RE 0.21408549984386047\n",
      "67 Train Loss 89218.945 Test MSE 14583.126750861888 Test RE 0.21407437633815668\n",
      "68 Train Loss 86270.17 Test MSE 13965.098946756909 Test RE 0.2094890709685311\n",
      "69 Train Loss 78554.54 Test MSE 10528.819509628016 Test RE 0.1818985355588412\n",
      "70 Train Loss 75322.234 Test MSE 9406.874746542633 Test RE 0.17193410920875726\n",
      "71 Train Loss 74375.57 Test MSE 9117.5028614316 Test RE 0.16926895628446778\n",
      "72 Train Loss 72954.42 Test MSE 9047.9522212579 Test RE 0.16862210692744148\n",
      "73 Train Loss 71553.67 Test MSE 9109.5733702572 Test RE 0.16919533367783096\n",
      "74 Train Loss 69260.62 Test MSE 8544.070326330822 Test RE 0.16385955435854127\n",
      "75 Train Loss 67092.92 Test MSE 8061.98648174863 Test RE 0.15916969900077446\n",
      "76 Train Loss 63621.195 Test MSE 8175.633364528975 Test RE 0.16028765154050734\n",
      "77 Train Loss 62161.6 Test MSE 8275.278562394371 Test RE 0.16126149183724728\n",
      "78 Train Loss 59524.34 Test MSE 7426.785492408783 Test RE 0.15277060636411782\n",
      "79 Train Loss 57848.293 Test MSE 7022.840431830318 Test RE 0.1485579037510209\n",
      "80 Train Loss 53900.234 Test MSE 6800.409808229981 Test RE 0.14618637761003028\n",
      "81 Train Loss 50912.445 Test MSE 6084.9748401153 Test RE 0.13828298815578996\n",
      "82 Train Loss 49491.008 Test MSE 5921.204931081363 Test RE 0.13640943442441553\n",
      "83 Train Loss 47488.766 Test MSE 6235.783063537679 Test RE 0.1399860829050969\n",
      "84 Train Loss 45194.348 Test MSE 5744.322002846881 Test RE 0.13435652123223574\n",
      "85 Train Loss 43278.61 Test MSE 5049.117757606816 Test RE 0.1259641939683697\n",
      "86 Train Loss 42189.35 Test MSE 4882.561910877058 Test RE 0.12386917401145325\n",
      "87 Train Loss 41147.277 Test MSE 4934.795779102231 Test RE 0.12452999036625309\n",
      "88 Train Loss 40108.125 Test MSE 4970.65501085065 Test RE 0.12498162676358567\n",
      "89 Train Loss 39505.28 Test MSE 5120.4548933626675 Test RE 0.1268509238669347\n",
      "90 Train Loss 38291.797 Test MSE 5275.938739162199 Test RE 0.1287624508129238\n",
      "91 Train Loss 35992.6 Test MSE 4873.207953547696 Test RE 0.1237504635412337\n",
      "92 Train Loss 35124.914 Test MSE 4696.429704544807 Test RE 0.1214851726812739\n",
      "93 Train Loss 34749.465 Test MSE 4850.3274390567685 Test RE 0.1234596073243197\n",
      "94 Train Loss 34182.527 Test MSE 4711.59198741989 Test RE 0.12168112027973674\n",
      "95 Train Loss 33593.477 Test MSE 4398.289000586433 Test RE 0.11756586495343845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 33266.445 Test MSE 4182.9469918487475 Test RE 0.11465171164290115\n",
      "97 Train Loss 32895.246 Test MSE 4014.139147872383 Test RE 0.11231443421686425\n",
      "98 Train Loss 31824.242 Test MSE 3811.833936185768 Test RE 0.10944762662070608\n",
      "99 Train Loss 31483.512 Test MSE 3672.8853241717616 Test RE 0.10743432181092802\n",
      "Training time: 88.88\n",
      "3D_HTTP_stan_tune0\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 1417869.0 Test MSE 312425.24694374204 Test RE 0.9908602882557818\n",
      "1 Train Loss 1123803.8 Test MSE 311655.56067382614 Test RE 0.9896390011470559\n",
      "2 Train Loss 590086.0 Test MSE 232849.81197642628 Test RE 0.8554158794581727\n",
      "3 Train Loss 325045.3 Test MSE 115593.79121493941 Test RE 0.6027075282121741\n",
      "4 Train Loss 234273.86 Test MSE 75876.46134566258 Test RE 0.4883069397277848\n",
      "5 Train Loss 216515.72 Test MSE 66821.72454804665 Test RE 0.4582454928765554\n",
      "6 Train Loss 199974.78 Test MSE 59066.01514310235 Test RE 0.43083224344854804\n",
      "7 Train Loss 190165.58 Test MSE 51007.07133139441 Test RE 0.40036357299485714\n",
      "8 Train Loss 183794.06 Test MSE 50662.63520431114 Test RE 0.3990095130234124\n",
      "9 Train Loss 177537.81 Test MSE 47094.35042844524 Test RE 0.3847014008363441\n",
      "10 Train Loss 175818.25 Test MSE 45531.67438837942 Test RE 0.37826501222589015\n",
      "11 Train Loss 170718.89 Test MSE 43512.55478232383 Test RE 0.3697827549816476\n",
      "12 Train Loss 164525.28 Test MSE 40895.520457087725 Test RE 0.3584901538533591\n",
      "13 Train Loss 158775.67 Test MSE 36714.161116486284 Test RE 0.33966919886957575\n",
      "14 Train Loss 153933.12 Test MSE 33239.575729006036 Test RE 0.32319683420782647\n",
      "15 Train Loss 147318.75 Test MSE 31223.520020192987 Test RE 0.3132422200515635\n",
      "16 Train Loss 139083.17 Test MSE 32090.98135675384 Test RE 0.3175637058184389\n",
      "17 Train Loss 130866.55 Test MSE 30059.888635986612 Test RE 0.3073498785584002\n",
      "18 Train Loss 116578.016 Test MSE 25433.350851581334 Test RE 0.2827099887981158\n",
      "19 Train Loss 107055.06 Test MSE 21393.75945488936 Test RE 0.2592882976682049\n",
      "20 Train Loss 100043.05 Test MSE 18933.49997291951 Test RE 0.2439241576408025\n",
      "21 Train Loss 93545.7 Test MSE 16129.333188416576 Test RE 0.2251373598498295\n",
      "22 Train Loss 86031.36 Test MSE 14422.333105294114 Test RE 0.21289091237826327\n",
      "23 Train Loss 81739.94 Test MSE 13673.62278613806 Test RE 0.2072913403144238\n",
      "24 Train Loss 75811.07 Test MSE 10785.506850923137 Test RE 0.18410248122765616\n",
      "25 Train Loss 72620.51 Test MSE 8890.97434105347 Test RE 0.16715294807722866\n",
      "26 Train Loss 69991.31 Test MSE 8646.4096716336 Test RE 0.16483797345614423\n",
      "27 Train Loss 64567.465 Test MSE 7118.359559163487 Test RE 0.14956477526746637\n",
      "28 Train Loss 56074.094 Test MSE 5268.468990970309 Test RE 0.12867126668308604\n",
      "29 Train Loss 51881.387 Test MSE 4894.389042891043 Test RE 0.12401910871660743\n",
      "30 Train Loss 49154.5 Test MSE 4997.9680795331815 Test RE 0.12532453481375735\n",
      "31 Train Loss 46641.97 Test MSE 4729.769778116939 Test RE 0.12191562324826563\n",
      "32 Train Loss 44000.24 Test MSE 4652.43566653197 Test RE 0.12091482465970493\n",
      "33 Train Loss 42197.613 Test MSE 5404.347074841747 Test RE 0.13031997210207868\n",
      "34 Train Loss 39390.07 Test MSE 4687.211712242156 Test RE 0.12136589064176294\n",
      "35 Train Loss 37869.883 Test MSE 3970.7865993846317 Test RE 0.1117062919952755\n",
      "36 Train Loss 37179.914 Test MSE 3664.360336576387 Test RE 0.10730956861984033\n",
      "37 Train Loss 36522.938 Test MSE 3797.2412659828096 Test RE 0.10923792905289487\n",
      "38 Train Loss 34952.867 Test MSE 3712.1997811638316 Test RE 0.10800777824819985\n",
      "39 Train Loss 33716.0 Test MSE 3587.9020707689974 Test RE 0.10618413965840266\n",
      "40 Train Loss 33314.367 Test MSE 3481.937010218084 Test RE 0.10460436738530961\n",
      "41 Train Loss 32083.744 Test MSE 3170.9865672614123 Test RE 0.09982436663188564\n",
      "42 Train Loss 31294.707 Test MSE 3257.082793164602 Test RE 0.1011704685429816\n",
      "43 Train Loss 30700.96 Test MSE 3372.6953663326267 Test RE 0.1029503719882783\n",
      "44 Train Loss 30483.334 Test MSE 3388.992732845114 Test RE 0.10319880799808759\n",
      "45 Train Loss 29639.43 Test MSE 3250.596411909517 Test RE 0.10106967938381994\n",
      "46 Train Loss 29473.033 Test MSE 3279.2430663307546 Test RE 0.10151405279526873\n",
      "47 Train Loss 29297.514 Test MSE 3276.453231784272 Test RE 0.10147086178038751\n",
      "48 Train Loss 29144.176 Test MSE 3280.213866405426 Test RE 0.10152907799397284\n",
      "49 Train Loss 28815.709 Test MSE 3639.2111839774057 Test RE 0.10694069234715518\n",
      "50 Train Loss 28651.936 Test MSE 3821.4184137836965 Test RE 0.10958513782531676\n",
      "51 Train Loss 28400.22 Test MSE 3967.876221259876 Test RE 0.11166534706736067\n",
      "52 Train Loss 27812.254 Test MSE 3896.834409182054 Test RE 0.11066119045937196\n",
      "53 Train Loss 27551.104 Test MSE 3803.6181416852833 Test RE 0.10932961462613998\n",
      "54 Train Loss 27414.441 Test MSE 3742.3218433780203 Test RE 0.10844509897629487\n",
      "55 Train Loss 27320.54 Test MSE 3668.695650330191 Test RE 0.10737302896304451\n",
      "56 Train Loss 27067.998 Test MSE 3689.0238510619583 Test RE 0.10767009438656007\n",
      "57 Train Loss 26798.88 Test MSE 3750.1448450199437 Test RE 0.10855838737404824\n",
      "58 Train Loss 26418.514 Test MSE 3643.285500026975 Test RE 0.10700053887356352\n",
      "59 Train Loss 26273.818 Test MSE 3704.5342438591797 Test RE 0.10789620484722325\n",
      "60 Train Loss 26174.645 Test MSE 3796.48799292111 Test RE 0.10922709354564665\n",
      "61 Train Loss 25737.885 Test MSE 3706.447103316983 Test RE 0.1079240576927454\n",
      "62 Train Loss 25055.107 Test MSE 3695.86388359041 Test RE 0.10776986683481382\n",
      "63 Train Loss 24375.621 Test MSE 3698.4341558748915 Test RE 0.10780733436176375\n",
      "64 Train Loss 24152.39 Test MSE 3744.301391138497 Test RE 0.10847377687746311\n",
      "65 Train Loss 23997.717 Test MSE 3638.1611654342673 Test RE 0.1069252634801307\n",
      "66 Train Loss 23770.473 Test MSE 3842.1751119329915 Test RE 0.10988235014095807\n",
      "67 Train Loss 23561.695 Test MSE 3878.5119467938594 Test RE 0.11040072590348143\n",
      "68 Train Loss 23309.08 Test MSE 3838.439679619328 Test RE 0.10982892234528503\n",
      "69 Train Loss 23154.107 Test MSE 4120.884878377759 Test RE 0.11379799315946475\n",
      "70 Train Loss 22930.86 Test MSE 4245.193769573918 Test RE 0.1155016321537711\n",
      "71 Train Loss 22802.246 Test MSE 4094.6858681970084 Test RE 0.11343567430868405\n",
      "72 Train Loss 22596.826 Test MSE 4071.397198397092 Test RE 0.11311262962931468\n",
      "73 Train Loss 22433.125 Test MSE 4007.9016328236044 Test RE 0.11222713837232288\n",
      "74 Train Loss 22085.752 Test MSE 3909.9033939241795 Test RE 0.11084659978348288\n",
      "75 Train Loss 21852.4 Test MSE 3658.739605465987 Test RE 0.1072272364188909\n",
      "76 Train Loss 21791.895 Test MSE 3595.890201670466 Test RE 0.10630227850369336\n",
      "77 Train Loss 21699.254 Test MSE 3634.3951321585555 Test RE 0.10686990744375802\n",
      "78 Train Loss 21591.105 Test MSE 3678.4670084976074 Test RE 0.10751592479640258\n",
      "79 Train Loss 21518.205 Test MSE 3639.8638924262223 Test RE 0.10695028205772521\n",
      "80 Train Loss 21306.46 Test MSE 3681.5514975339256 Test RE 0.10756099278249681\n",
      "81 Train Loss 21109.824 Test MSE 3801.1523813974663 Test RE 0.10929417148802656\n",
      "82 Train Loss 20990.04 Test MSE 3806.11666585517 Test RE 0.10936551699992499\n",
      "83 Train Loss 20957.863 Test MSE 3770.730745817287 Test RE 0.10885593770958409\n",
      "84 Train Loss 20909.26 Test MSE 3664.4946704354275 Test RE 0.10731153556291202\n",
      "85 Train Loss 20757.008 Test MSE 3694.5757714448087 Test RE 0.10775108478865117\n",
      "86 Train Loss 20474.965 Test MSE 4098.300304550614 Test RE 0.1134857288914547\n",
      "87 Train Loss 20360.049 Test MSE 4199.371010171629 Test RE 0.11487657667607071\n",
      "88 Train Loss 20253.156 Test MSE 4130.892296760964 Test RE 0.11393608650620381\n",
      "89 Train Loss 20157.68 Test MSE 4065.6619058479173 Test RE 0.11303293184795166\n",
      "90 Train Loss 20093.33 Test MSE 4229.742230885703 Test RE 0.11529124071439051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 Train Loss 20036.709 Test MSE 4281.280354897657 Test RE 0.11599150840602343\n",
      "92 Train Loss 19916.111 Test MSE 4198.498428069143 Test RE 0.11486464102520207\n",
      "93 Train Loss 19788.193 Test MSE 4328.421137202722 Test RE 0.1166283460546253\n",
      "94 Train Loss 19725.062 Test MSE 4309.234776750082 Test RE 0.11636957286339342\n",
      "95 Train Loss 19637.174 Test MSE 4205.64452957453 Test RE 0.11496235279257945\n",
      "96 Train Loss 19468.879 Test MSE 4101.56866916381 Test RE 0.11353097189370868\n",
      "97 Train Loss 19259.562 Test MSE 3973.453755671631 Test RE 0.11174380195929215\n",
      "98 Train Loss 19152.203 Test MSE 3898.9151439224233 Test RE 0.11069073057203227\n",
      "99 Train Loss 19093.783 Test MSE 3910.645107897042 Test RE 0.11085711316000994\n",
      "Training time: 88.98\n",
      "3D_HTTP_stan_tune0\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 1433412.0 Test MSE 313766.34665642894 Test RE 0.9929846675817794\n",
      "1 Train Loss 1096139.2 Test MSE 308353.7568000694 Test RE 0.9843827262695299\n",
      "2 Train Loss 596094.25 Test MSE 231331.59934354204 Test RE 0.8526226045919773\n",
      "3 Train Loss 362735.94 Test MSE 137956.33539549253 Test RE 0.6584308741033704\n",
      "4 Train Loss 261471.42 Test MSE 89576.52588484279 Test RE 0.5305624006866686\n",
      "5 Train Loss 242429.78 Test MSE 84807.65439142882 Test RE 0.5162462239254821\n",
      "6 Train Loss 228149.67 Test MSE 74065.24444343442 Test RE 0.4824436477068913\n",
      "7 Train Loss 213555.17 Test MSE 64347.387293225154 Test RE 0.44968129309920407\n",
      "8 Train Loss 203218.94 Test MSE 58748.75210974686 Test RE 0.42967361452609154\n",
      "9 Train Loss 186385.67 Test MSE 49698.448223619635 Test RE 0.39519439508919113\n",
      "10 Train Loss 180188.28 Test MSE 49122.367966730104 Test RE 0.3928972682155843\n",
      "11 Train Loss 171738.22 Test MSE 42961.87253894559 Test RE 0.36743537286503813\n",
      "12 Train Loss 166643.89 Test MSE 39211.49958284472 Test RE 0.3510314984705347\n",
      "13 Train Loss 161238.88 Test MSE 36600.86345140208 Test RE 0.3391446946800483\n",
      "14 Train Loss 153303.95 Test MSE 32599.94907151858 Test RE 0.3200721027496328\n",
      "15 Train Loss 148447.08 Test MSE 32085.456521568507 Test RE 0.3175363685015099\n",
      "16 Train Loss 142855.48 Test MSE 28999.34438083739 Test RE 0.30187938178102586\n",
      "17 Train Loss 141245.02 Test MSE 28488.75329608849 Test RE 0.2992099863950107\n",
      "18 Train Loss 139273.02 Test MSE 27484.550979388212 Test RE 0.2938892396743187\n",
      "19 Train Loss 135485.03 Test MSE 26315.173405640657 Test RE 0.28756927353213\n",
      "20 Train Loss 130324.28 Test MSE 23349.958149397757 Test RE 0.27088341471981386\n",
      "21 Train Loss 126381.5 Test MSE 20526.191739552534 Test RE 0.25397651107812663\n",
      "22 Train Loss 120056.69 Test MSE 19604.538009218366 Test RE 0.24820908203269376\n",
      "23 Train Loss 114133.02 Test MSE 18399.66740160467 Test RE 0.2404608337028348\n",
      "24 Train Loss 108555.61 Test MSE 18045.729987038325 Test RE 0.23813684178377786\n",
      "25 Train Loss 103364.88 Test MSE 18050.47253552364 Test RE 0.2381681317711279\n",
      "26 Train Loss 99926.68 Test MSE 15644.516210384361 Test RE 0.2217279445304744\n",
      "27 Train Loss 96329.664 Test MSE 14341.411280745348 Test RE 0.21229282071347091\n",
      "28 Train Loss 94353.234 Test MSE 13734.58017332858 Test RE 0.2077528817491025\n",
      "29 Train Loss 89900.7 Test MSE 12534.323709441627 Test RE 0.19846769026230832\n",
      "30 Train Loss 86945.766 Test MSE 12884.290056634163 Test RE 0.20121928841528572\n",
      "31 Train Loss 81828.086 Test MSE 11586.243671038641 Test RE 0.1908142001309826\n",
      "32 Train Loss 72448.53 Test MSE 11761.559945323585 Test RE 0.19225242449700525\n",
      "33 Train Loss 64463.023 Test MSE 8773.987829349944 Test RE 0.1660496161620449\n",
      "34 Train Loss 62796.098 Test MSE 8237.672221238665 Test RE 0.16089465462514754\n",
      "35 Train Loss 57299.438 Test MSE 6943.86096444281 Test RE 0.14772019437378936\n",
      "36 Train Loss 51291.297 Test MSE 4887.819122493161 Test RE 0.12393584303408332\n",
      "37 Train Loss 47424.0 Test MSE 4755.398446259166 Test RE 0.12224548220373671\n",
      "38 Train Loss 46197.61 Test MSE 4347.050119063248 Test RE 0.11687905347997302\n",
      "39 Train Loss 43911.51 Test MSE 4042.883572414973 Test RE 0.1127158471674032\n",
      "40 Train Loss 39652.05 Test MSE 3711.545057953851 Test RE 0.10799825312539621\n",
      "41 Train Loss 38127.08 Test MSE 3411.1272504615813 Test RE 0.10353527053369162\n",
      "42 Train Loss 36596.05 Test MSE 3425.8329855817487 Test RE 0.10375820634022155\n",
      "43 Train Loss 35262.19 Test MSE 3300.5327591773193 Test RE 0.1018430474029657\n",
      "44 Train Loss 34397.277 Test MSE 3597.4264065079406 Test RE 0.10632498284502713\n",
      "45 Train Loss 33633.24 Test MSE 3623.1327850895696 Test RE 0.10670419350332688\n",
      "46 Train Loss 32785.79 Test MSE 3288.1318904506065 Test RE 0.10165154335202144\n",
      "47 Train Loss 32192.844 Test MSE 3447.2347072739694 Test RE 0.10408179872473651\n",
      "48 Train Loss 31375.291 Test MSE 3393.2584092699103 Test RE 0.10326373500174166\n",
      "49 Train Loss 30117.41 Test MSE 3234.532865912976 Test RE 0.10081964093288115\n",
      "50 Train Loss 29649.537 Test MSE 2898.341227830272 Test RE 0.09543641554245957\n",
      "51 Train Loss 29405.066 Test MSE 2817.581611801054 Test RE 0.09409739799342776\n",
      "52 Train Loss 29107.48 Test MSE 3092.200251599831 Test RE 0.09857644858937392\n",
      "53 Train Loss 28709.053 Test MSE 2962.8811806817685 Test RE 0.09649314907812777\n",
      "54 Train Loss 27818.738 Test MSE 3023.1822271909073 Test RE 0.09747012546314285\n",
      "55 Train Loss 27449.518 Test MSE 3050.310939182556 Test RE 0.09790647583344961\n",
      "56 Train Loss 27350.64 Test MSE 3100.458887371577 Test RE 0.09870799959911768\n",
      "57 Train Loss 27087.072 Test MSE 3101.661945900746 Test RE 0.09872714837478228\n",
      "58 Train Loss 26429.787 Test MSE 3223.5527112863233 Test RE 0.10064837101892585\n",
      "59 Train Loss 26235.295 Test MSE 3150.36531625274 Test RE 0.09949925315880352\n",
      "60 Train Loss 26044.014 Test MSE 3038.1436843031124 Test RE 0.09771101324302074\n",
      "61 Train Loss 25604.254 Test MSE 3061.17937967369 Test RE 0.09808074405686144\n",
      "62 Train Loss 25051.445 Test MSE 2820.215340940543 Test RE 0.09414136640433185\n",
      "63 Train Loss 24848.215 Test MSE 2854.320888120033 Test RE 0.09470889292428074\n",
      "64 Train Loss 24710.283 Test MSE 2862.5792124898785 Test RE 0.09484580322144247\n",
      "65 Train Loss 24565.055 Test MSE 2819.509190916848 Test RE 0.0941295796997843\n",
      "66 Train Loss 24522.697 Test MSE 2829.457592213309 Test RE 0.0942954976285842\n",
      "67 Train Loss 24243.074 Test MSE 2867.613637572339 Test RE 0.09492916935687115\n",
      "68 Train Loss 23499.146 Test MSE 2875.012671676538 Test RE 0.09505155886952382\n",
      "69 Train Loss 23013.998 Test MSE 2632.865055400202 Test RE 0.09096067246552146\n",
      "70 Train Loss 22719.69 Test MSE 2448.4895744725336 Test RE 0.08771795369128123\n",
      "71 Train Loss 22233.75 Test MSE 2305.5717356894625 Test RE 0.08511942513896907\n",
      "72 Train Loss 22025.516 Test MSE 2393.542325295975 Test RE 0.0867281171380125\n",
      "73 Train Loss 21949.117 Test MSE 2393.246926438979 Test RE 0.08672276520065512\n",
      "74 Train Loss 21809.045 Test MSE 2311.527707718965 Test RE 0.08522929852028732\n",
      "75 Train Loss 21649.354 Test MSE 2258.1179550421402 Test RE 0.08423889748672289\n",
      "76 Train Loss 21531.986 Test MSE 2358.396452186492 Test RE 0.08608902091277973\n",
      "77 Train Loss 21338.979 Test MSE 2428.772215285492 Test RE 0.08736404928816109\n",
      "78 Train Loss 21131.408 Test MSE 2441.9173255531264 Test RE 0.08760014808238732\n",
      "79 Train Loss 21004.31 Test MSE 2427.641446377792 Test RE 0.0873437097826811\n",
      "80 Train Loss 20955.002 Test MSE 2426.6582828631617 Test RE 0.08732602145293214\n",
      "81 Train Loss 20854.762 Test MSE 2423.8047958272264 Test RE 0.08727466338405585\n",
      "82 Train Loss 20739.174 Test MSE 2417.5764255671806 Test RE 0.0871624578606669\n",
      "83 Train Loss 20611.775 Test MSE 2387.9815945277333 Test RE 0.0866273142116804\n",
      "84 Train Loss 20554.232 Test MSE 2326.614486801975 Test RE 0.08550698161465031\n",
      "85 Train Loss 20516.262 Test MSE 2284.0217382772053 Test RE 0.08472068893141194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 Train Loss 20412.783 Test MSE 2201.5038524676365 Test RE 0.083176202269806\n",
      "87 Train Loss 20229.074 Test MSE 2081.6950568651214 Test RE 0.08088126175646512\n",
      "88 Train Loss 20125.42 Test MSE 2020.2956012005639 Test RE 0.07967954057620037\n",
      "89 Train Loss 20020.85 Test MSE 2013.434798193765 Test RE 0.07954413204194695\n",
      "90 Train Loss 19908.47 Test MSE 1980.2152148644825 Test RE 0.0788852050468161\n",
      "91 Train Loss 19829.47 Test MSE 1891.6516628584302 Test RE 0.07710098838452108\n",
      "92 Train Loss 19739.75 Test MSE 1884.8104856727887 Test RE 0.076961443855827\n",
      "93 Train Loss 19659.715 Test MSE 1865.1657253138947 Test RE 0.07655932139351561\n",
      "94 Train Loss 19589.758 Test MSE 1830.0882505952109 Test RE 0.07583599313845953\n",
      "95 Train Loss 19489.924 Test MSE 1817.3662578541155 Test RE 0.07557194371337646\n",
      "96 Train Loss 19387.584 Test MSE 1777.8462748986592 Test RE 0.07474574333717326\n",
      "97 Train Loss 19235.99 Test MSE 1741.0829474615925 Test RE 0.07396888856796707\n",
      "98 Train Loss 19035.11 Test MSE 1861.159049968662 Test RE 0.07647704632952058\n",
      "99 Train Loss 18768.008 Test MSE 2052.2938735638286 Test RE 0.08030806032627799\n",
      "Training time: 89.43\n",
      "3D_HTTP_stan_tune0\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 1458479.9 Test MSE 313716.70110207674 Test RE 0.9929061071737316\n",
      "1 Train Loss 1206651.2 Test MSE 310854.9268126506 Test RE 0.9883670071207992\n",
      "2 Train Loss 768184.9 Test MSE 292228.6047474747 Test RE 0.9582983148795663\n",
      "3 Train Loss 564946.25 Test MSE 296718.6385352648 Test RE 0.9656322815541245\n",
      "4 Train Loss 373002.2 Test MSE 125435.58192230083 Test RE 0.6278410919169597\n",
      "5 Train Loss 273052.8 Test MSE 96079.39570152908 Test RE 0.5494832947376127\n",
      "6 Train Loss 241799.53 Test MSE 83054.29857520101 Test RE 0.5108817861755535\n",
      "7 Train Loss 220123.22 Test MSE 69874.67256771587 Test RE 0.4685967315750678\n",
      "8 Train Loss 198088.81 Test MSE 55656.236078400194 Test RE 0.41821179617236126\n",
      "9 Train Loss 183253.66 Test MSE 48427.38084960515 Test RE 0.39010799667046686\n",
      "10 Train Loss 177308.5 Test MSE 45818.38552863642 Test RE 0.37945410310752004\n",
      "11 Train Loss 167378.45 Test MSE 40764.56846283766 Test RE 0.35791573101797536\n",
      "12 Train Loss 161145.67 Test MSE 39014.94127699763 Test RE 0.3501505726862215\n",
      "13 Train Loss 155323.52 Test MSE 36080.68947273068 Test RE 0.3367260967924562\n",
      "14 Train Loss 148496.14 Test MSE 29470.948400895035 Test RE 0.30432415038869\n",
      "15 Train Loss 142109.42 Test MSE 27867.351085945866 Test RE 0.2959287820646553\n",
      "16 Train Loss 136188.69 Test MSE 26327.649440914687 Test RE 0.2876374338138497\n",
      "17 Train Loss 131381.0 Test MSE 25552.46033148185 Test RE 0.2833712093173016\n",
      "18 Train Loss 126012.625 Test MSE 24982.6360132183 Test RE 0.28019378146930835\n",
      "19 Train Loss 119034.24 Test MSE 22321.712529345565 Test RE 0.26485191509444583\n",
      "20 Train Loss 107387.42 Test MSE 19881.304362125615 Test RE 0.2499549830524395\n",
      "21 Train Loss 99338.766 Test MSE 17880.460135622856 Test RE 0.2370438583789886\n",
      "22 Train Loss 88319.51 Test MSE 14641.949163036128 Test RE 0.21450568640056095\n",
      "23 Train Loss 83701.08 Test MSE 12053.371480478358 Test RE 0.19462276262993256\n",
      "24 Train Loss 80946.83 Test MSE 12160.763902741228 Test RE 0.195487859194146\n",
      "25 Train Loss 76201.59 Test MSE 11220.958809443811 Test RE 0.1877821669028627\n",
      "26 Train Loss 73975.2 Test MSE 10815.562162194221 Test RE 0.18435881632714576\n",
      "27 Train Loss 67265.55 Test MSE 8946.136817157958 Test RE 0.16767068170781954\n",
      "28 Train Loss 62703.63 Test MSE 8207.481148824343 Test RE 0.16059954449128652\n",
      "29 Train Loss 60831.473 Test MSE 7469.438657611412 Test RE 0.1532086707781531\n",
      "30 Train Loss 58811.984 Test MSE 6992.008298280482 Test RE 0.14823144072764907\n",
      "31 Train Loss 55967.402 Test MSE 7218.123544152468 Test RE 0.1506092055802107\n",
      "32 Train Loss 51041.906 Test MSE 5833.191573916665 Test RE 0.1353918374037265\n",
      "33 Train Loss 47567.062 Test MSE 4581.13340202331 Test RE 0.11998468936683208\n",
      "34 Train Loss 43316.86 Test MSE 5082.345476834062 Test RE 0.1263779929275318\n",
      "35 Train Loss 41957.12 Test MSE 5272.163482653529 Test RE 0.12871637386849286\n",
      "36 Train Loss 39345.594 Test MSE 5263.076769708909 Test RE 0.12860540299836376\n",
      "37 Train Loss 37365.45 Test MSE 5676.500502334045 Test RE 0.13356101244392163\n",
      "38 Train Loss 35937.703 Test MSE 4978.551917555098 Test RE 0.12508086685918557\n",
      "39 Train Loss 34936.414 Test MSE 4721.174616878458 Test RE 0.12180479745886871\n",
      "40 Train Loss 34135.742 Test MSE 4816.076878247548 Test RE 0.12302293038807043\n",
      "41 Train Loss 33294.86 Test MSE 4439.501512596187 Test RE 0.11811538403823657\n",
      "42 Train Loss 32862.215 Test MSE 4541.877874202008 Test RE 0.11946951163247492\n",
      "43 Train Loss 30987.094 Test MSE 4651.244813649386 Test RE 0.12089934878930644\n",
      "44 Train Loss 30354.693 Test MSE 4395.500860265978 Test RE 0.11752859567752642\n",
      "45 Train Loss 29796.764 Test MSE 4314.663697590376 Test RE 0.11644285296748175\n",
      "46 Train Loss 29359.258 Test MSE 4298.571420865505 Test RE 0.11622550334270436\n",
      "47 Train Loss 28737.592 Test MSE 4191.809656712354 Test RE 0.11477310716130638\n",
      "48 Train Loss 28162.498 Test MSE 4659.68723061833 Test RE 0.12100902050272325\n",
      "49 Train Loss 27960.088 Test MSE 4533.594629030445 Test RE 0.11936052071637837\n",
      "50 Train Loss 27704.229 Test MSE 4230.190662389753 Test RE 0.11529735206248586\n",
      "51 Train Loss 27089.092 Test MSE 3872.8673685484555 Test RE 0.11032036100583216\n",
      "52 Train Loss 26841.736 Test MSE 3754.9823697896145 Test RE 0.10862838262253358\n",
      "53 Train Loss 26528.193 Test MSE 3836.0035604137165 Test RE 0.10979406458601826\n",
      "54 Train Loss 26469.725 Test MSE 3780.2597514338604 Test RE 0.10899339572579445\n",
      "55 Train Loss 26199.709 Test MSE 3525.635368693539 Test RE 0.10525871391334123\n",
      "56 Train Loss 25978.658 Test MSE 3629.1094907931692 Test RE 0.10679216664884071\n",
      "57 Train Loss 25740.97 Test MSE 3660.835981604113 Test RE 0.10725795142645198\n",
      "58 Train Loss 25647.293 Test MSE 3456.063857731862 Test RE 0.10421500204258802\n",
      "59 Train Loss 25583.746 Test MSE 3329.3421330052633 Test RE 0.1022865605946293\n",
      "60 Train Loss 25443.738 Test MSE 3318.8693542735955 Test RE 0.10212555757781308\n",
      "61 Train Loss 25158.021 Test MSE 3276.690671872472 Test RE 0.10147453844112111\n",
      "62 Train Loss 24909.195 Test MSE 3202.7956845587137 Test RE 0.1003238013429086\n",
      "63 Train Loss 24835.016 Test MSE 3168.995497436586 Test RE 0.09979302173498909\n",
      "64 Train Loss 24773.346 Test MSE 3148.574743390034 Test RE 0.0994709729488651\n",
      "65 Train Loss 24651.268 Test MSE 3111.3618324907825 Test RE 0.09888140351518697\n",
      "66 Train Loss 24554.752 Test MSE 2968.121858423173 Test RE 0.0965784488340462\n",
      "67 Train Loss 24489.502 Test MSE 2892.829317118904 Test RE 0.09534562441486577\n",
      "68 Train Loss 24219.516 Test MSE 3046.1674038649167 Test RE 0.0978399552703795\n",
      "69 Train Loss 24013.941 Test MSE 3121.055828990157 Test RE 0.09903532494162752\n",
      "70 Train Loss 23956.39 Test MSE 3146.3111811003987 Test RE 0.09943521085906731\n",
      "71 Train Loss 23829.295 Test MSE 3279.726700275926 Test RE 0.10152153833854592\n",
      "72 Train Loss 23664.514 Test MSE 3272.6623549609994 Test RE 0.10141214358118208\n",
      "73 Train Loss 23492.566 Test MSE 3206.6946792217536 Test RE 0.10038484847633515\n",
      "74 Train Loss 23388.57 Test MSE 3250.5447142309417 Test RE 0.10106887567152974\n",
      "75 Train Loss 23042.398 Test MSE 3162.0214624896507 Test RE 0.09968315358516473\n",
      "76 Train Loss 22874.242 Test MSE 3033.682772764801 Test RE 0.09763925226819814\n",
      "77 Train Loss 22768.982 Test MSE 3114.801730924794 Test RE 0.09893604968633915\n",
      "78 Train Loss 22636.408 Test MSE 3100.844103857076 Test RE 0.09871413139590639\n",
      "79 Train Loss 22548.385 Test MSE 3039.7884497560917 Test RE 0.09773745866008188\n",
      "80 Train Loss 22369.207 Test MSE 3029.68493114974 Test RE 0.09757489567896899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 Train Loss 22044.018 Test MSE 3030.61967730175 Test RE 0.09758994686850898\n",
      "82 Train Loss 21783.8 Test MSE 3082.6727229397534 Test RE 0.09842446708344035\n",
      "83 Train Loss 21598.703 Test MSE 3324.430995743377 Test RE 0.1022110909209811\n",
      "84 Train Loss 21491.484 Test MSE 3228.302853260733 Test RE 0.10072250010841181\n",
      "85 Train Loss 21429.17 Test MSE 3115.119515720372 Test RE 0.09894109648757449\n",
      "86 Train Loss 21335.523 Test MSE 3099.53584690285 Test RE 0.09869330528067044\n",
      "87 Train Loss 21146.67 Test MSE 3035.5063224847645 Test RE 0.09766859338524826\n",
      "88 Train Loss 20971.512 Test MSE 2984.403234905472 Test RE 0.09684297295015318\n",
      "89 Train Loss 20859.084 Test MSE 3065.1374877800426 Test RE 0.09814413282483644\n",
      "90 Train Loss 20780.785 Test MSE 3086.521123369956 Test RE 0.09848588434262681\n",
      "91 Train Loss 20618.137 Test MSE 3108.4970693997634 Test RE 0.0988358708761748\n",
      "92 Train Loss 20420.135 Test MSE 3088.3084846187708 Test RE 0.09851439611517969\n",
      "93 Train Loss 20314.016 Test MSE 3133.143822357032 Test RE 0.09922692379675366\n",
      "94 Train Loss 20258.816 Test MSE 3165.5235514719334 Test RE 0.09973834022297125\n",
      "95 Train Loss 20197.86 Test MSE 3277.3503128875705 Test RE 0.10148475200806596\n",
      "96 Train Loss 20071.447 Test MSE 3370.0397221377652 Test RE 0.10290983269142485\n",
      "97 Train Loss 19915.38 Test MSE 3317.5705986824028 Test RE 0.10210557349539863\n",
      "98 Train Loss 19896.744 Test MSE 3270.145618536387 Test RE 0.1013731421970639\n",
      "99 Train Loss 19868.318 Test MSE 3270.2746483193523 Test RE 0.10137514211190547\n",
      "Training time: 115.61\n",
      "3D_HTTP_stan_tune0\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 1301268.2 Test MSE 309276.7544469878 Test RE 0.985854905574363\n",
      "1 Train Loss 1067768.6 Test MSE 310843.0178906545 Test RE 0.9883480746585637\n",
      "2 Train Loss 688904.9 Test MSE 292177.0920347033 Test RE 0.9582138489500299\n",
      "3 Train Loss 548231.5 Test MSE 281073.4627722389 Test RE 0.9398299575631386\n",
      "4 Train Loss 376460.03 Test MSE 158669.42748422408 Test RE 0.7061321569995466\n",
      "5 Train Loss 246032.44 Test MSE 72561.65230892859 Test RE 0.47752151524786124\n",
      "6 Train Loss 217128.94 Test MSE 61282.879121813865 Test RE 0.4388427647777479\n",
      "7 Train Loss 204218.7 Test MSE 59052.10156298268 Test RE 0.4307814970762434\n",
      "8 Train Loss 196569.78 Test MSE 56379.710168625956 Test RE 0.4209211823897521\n",
      "9 Train Loss 192011.12 Test MSE 52466.69548730273 Test RE 0.40605159253815915\n",
      "10 Train Loss 186702.03 Test MSE 49945.2400115631 Test RE 0.39617440508323737\n",
      "11 Train Loss 176668.1 Test MSE 44869.37238985115 Test RE 0.37550382017702455\n",
      "12 Train Loss 169026.73 Test MSE 42699.77865524601 Test RE 0.3663128672088035\n",
      "13 Train Loss 165128.77 Test MSE 39736.84061768132 Test RE 0.3533751691751299\n",
      "14 Train Loss 164455.23 Test MSE 39624.247442387896 Test RE 0.35287417493374157\n",
      "15 Train Loss 163021.78 Test MSE 39304.085159461625 Test RE 0.351445679154367\n",
      "16 Train Loss 159833.06 Test MSE 38418.86133750467 Test RE 0.34746543355235354\n",
      "17 Train Loss 156321.36 Test MSE 36158.11346143762 Test RE 0.33708718615767647\n",
      "18 Train Loss 150235.03 Test MSE 32030.09879077298 Test RE 0.3172623240710224\n",
      "19 Train Loss 146678.28 Test MSE 30456.650679262202 Test RE 0.30937159281559296\n",
      "20 Train Loss 139026.28 Test MSE 27929.535987065137 Test RE 0.29625877481187796\n",
      "21 Train Loss 133865.2 Test MSE 25026.760533093035 Test RE 0.2804411124915164\n",
      "22 Train Loss 128727.875 Test MSE 21154.92318409086 Test RE 0.2578369104396626\n",
      "23 Train Loss 116777.18 Test MSE 20328.508884275794 Test RE 0.2527505586321801\n",
      "24 Train Loss 112227.03 Test MSE 17249.52686622035 Test RE 0.23282411236840114\n",
      "25 Train Loss 108509.59 Test MSE 15101.66470272676 Test RE 0.21784709550088527\n",
      "26 Train Loss 104471.33 Test MSE 13376.723993656213 Test RE 0.20502850515943374\n",
      "27 Train Loss 98151.984 Test MSE 11043.551400317614 Test RE 0.18629180070557047\n",
      "28 Train Loss 93052.02 Test MSE 10196.935809626824 Test RE 0.17900872731462092\n",
      "29 Train Loss 88613.81 Test MSE 12429.458763038603 Test RE 0.19763573405439344\n",
      "30 Train Loss 85034.8 Test MSE 13487.460300327593 Test RE 0.2058753980602247\n",
      "31 Train Loss 78184.06 Test MSE 11522.877386704922 Test RE 0.1902916941085996\n",
      "32 Train Loss 73002.37 Test MSE 9791.689669876912 Test RE 0.17541558761744624\n",
      "33 Train Loss 65273.99 Test MSE 6942.076484273473 Test RE 0.14770121208881393\n",
      "34 Train Loss 62747.91 Test MSE 7101.749663453377 Test RE 0.1493901770208738\n",
      "35 Train Loss 61543.82 Test MSE 7427.3703120070895 Test RE 0.15277662117940918\n",
      "36 Train Loss 58449.492 Test MSE 6303.292136941971 Test RE 0.14074179318786062\n",
      "37 Train Loss 56019.18 Test MSE 6417.905209495786 Test RE 0.14201558640342216\n"
     ]
    }
   ],
   "source": [
    "nan_tune = []\n",
    "for tune_reps in range(20):\n",
    "    label = \"3D_HTTP_stan_tune\"+str(tune_reps)\n",
    "    max_reps = 10\n",
    "    max_iter = 100\n",
    "\n",
    "    train_loss_full = []\n",
    "    test_mse_full = []\n",
    "    test_re_full = []\n",
    "    beta_full = []\n",
    "    elapsed_time= np.zeros((max_reps,1))\n",
    "    \n",
    "    time_threshold = np.empty((max_reps,1))\n",
    "    time_threshold[:] = np.nan\n",
    "    epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "    \n",
    "    beta_init = lrb_tune[tune_reps,1]\n",
    "    \n",
    "    for reps in range(max_reps):\n",
    "        print(label)\n",
    "        train_loss = []\n",
    "        test_mse_loss = []\n",
    "        test_re_loss = []\n",
    "        beta_val = []\n",
    "\n",
    "\n",
    "        print(reps)\n",
    "\n",
    "        torch.manual_seed(reps*36)\n",
    "        N_D = 5000 #Total number of data points for 'y'\n",
    "        N_N = 3500\n",
    "        N_f = 10000 #Total number of collocation points \n",
    "\n",
    "        layers = np.array([3,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    " \n",
    "        PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "        PINN.to(device)\n",
    "\n",
    "        'Neural Network Summary'\n",
    "        print(PINN)\n",
    "\n",
    "        params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "        optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lrb_tune[tune_reps,0], \n",
    "                                  max_iter = 10, \n",
    "                                  max_eval = 15, \n",
    "                                  tolerance_grad = -1, \n",
    "                                  tolerance_change = -1, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "        nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "        torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "        train_loss_full.append(train_loss)\n",
    "        test_mse_full.append(test_mse_loss)\n",
    "        test_re_full.append(test_re_loss)\n",
    "        #elapsed_time[reps] = time.time() - start_time\n",
    "        beta_full.append(beta_val)\n",
    "        \n",
    "        if(nan_flag == 1):\n",
    "            nan_tune.append(tune_reps)\n",
    "            break\n",
    "\n",
    "        #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "    mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "    savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFzPEF73CQsD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tune_reps in range(20):\n",
    "    label = \"3D_HTTP_stan_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HT_stan_v3_15Aug2022_MP4Video.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
