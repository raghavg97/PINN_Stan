{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLsZ-c_nCQr2",
    "outputId": "0238c820-5951-4e75-a35b-19e4de8c9b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SV23gJi7JexL",
    "outputId": "6f051579-557f-463f-d7b4-955ed617736e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOyXTKXGJf97",
    "outputId": "11b7b7db-47b0-4cf8-c699-473f1c6b8c5f"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/MURI Aug17 Thin Plate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "APjvgycyCTj0",
    "outputId": "19bce659-211e-4bec-d94d-7c94148b0d09"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lxFUD2gACQr7"
   },
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CUcT7YuXCQr7"
   },
   "outputs": [],
   "source": [
    "label = \"3D_HTTP_tanh\"\n",
    "loss_thresh = 20000\n",
    "x = np.linspace(0,1,100).reshape(-1,1)\n",
    "y = np.linspace(0,1,100).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xyt = np.hstack((X,Y,T))\n",
    "\n",
    "initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "xyt_initial = xyt[initial_pts,:]\n",
    "xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "xyt_NBC_x0 = xyt[NBC_pts_x0,:]\n",
    "xyt_NBC_x1 = xyt[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xyt_NBC_y1 = xyt[NBC_pts_y1,:]\n",
    "\n",
    "u_initial = 300*np.ones((np.shape(xyt_initial)[0],1))\n",
    "u_DBC = 1000*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xyt_NBC_x = np.vstack((xyt_NBC_x0,xyt_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xyt_NBC_y = np.vstack((xyt_NBC_y1))\n",
    "\n",
    "u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../3D_HTTP_FEA.mat')\n",
    "xy = fea_data['xy']\n",
    "t = fea_data['t']/3000\n",
    "xyt = np.zeros((497*101,3))\n",
    "u_true = np.ones((497*101,1))\n",
    "\n",
    "\n",
    "for i in range(101):\n",
    "    t_temp = t[0,i]*np.ones((497,1))\n",
    "    xyt[497*i:497*(i+1)] = np.hstack((xy,t_temp))\n",
    "    u_true[497*i:497*(i+1)] = fea_data['u'][:,i].reshape(-1,1)\n",
    "    #print(i)\n",
    "#print(xyt)\n",
    "\n",
    "xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gp2G6x6BCQr8"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xyt_I_DBC.shape[0], N_D, replace=False) \n",
    "    xyt_D = xyt_I_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_I_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_x.shape[0], N_D, replace=False) \n",
    "    xyt_Nx = xyt_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_y.shape[0], N_D, replace=False) \n",
    "    xyt_Ny = xyt_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xyt_coll = lb_xyt + (ub_xyt - lb_xyt)*samples\n",
    "    xyt_coll = np.vstack((xyt_coll, xyt_D,xyt_Nx,xyt_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xyt_coll, xyt_D, u_D, xyt_Nx,xyt_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VRolFlBzCQr9"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "    \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)         \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/3000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "       \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xyt_coll_np_array, xyt_D_np_array, u_D_np_array,xyt_Nx_np_array,xyt_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "\n",
    "    xyt_coll = torch.from_numpy(xyt_coll_np_array).float().to(device)\n",
    "    xyt_D = torch.from_numpy(xyt_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xyt_Nx = torch.from_numpy(xyt_Nx_np_array).float().to(device)\n",
    "    xyt_Ny = torch.from_numpy(xyt_Ny_np_array).float().to(device)\n",
    "\n",
    "    N_hat = torch.zeros(xyt_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xyt_coll.shape[0],1).to(device)\n",
    "\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,i)\n",
    "\n",
    "        loss_np = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVnXJfj0CQr-",
    "outputId": "1f2921b0-e258-465d-aa27-cdeb80b78a0b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D_HTTP_tanh\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 669958.2 Test MSE 301439.9805083001 Test RE 0.9732844588219348\n",
      "1 Train Loss 427549.66 Test MSE 189295.05827936932 Test RE 0.7712745489175916\n",
      "2 Train Loss 237088.25 Test MSE 80406.88213272895 Test RE 0.502673479919789\n",
      "3 Train Loss 222040.33 Test MSE 69917.21104648277 Test RE 0.46873934662092004\n",
      "4 Train Loss 212197.78 Test MSE 64851.731754364366 Test RE 0.45144011826517055\n",
      "5 Train Loss 204258.78 Test MSE 59915.616277075016 Test RE 0.4339197099628782\n",
      "6 Train Loss 198799.16 Test MSE 56286.71569671364 Test RE 0.4205738987894323\n",
      "7 Train Loss 193653.6 Test MSE 53417.60294047651 Test RE 0.4097147130998103\n",
      "8 Train Loss 190504.75 Test MSE 51240.291494862504 Test RE 0.4012778223959225\n",
      "9 Train Loss 186081.69 Test MSE 49084.976969175936 Test RE 0.39274770684405624\n",
      "10 Train Loss 182993.67 Test MSE 48597.158053275125 Test RE 0.390791220669541\n",
      "11 Train Loss 180580.8 Test MSE 48120.79322354426 Test RE 0.38887117381499453\n",
      "12 Train Loss 178022.0 Test MSE 46874.982142214845 Test RE 0.3838043740092838\n",
      "13 Train Loss 176067.33 Test MSE 45682.72532407522 Test RE 0.37889193815312083\n",
      "14 Train Loss 173044.19 Test MSE 44120.29656747329 Test RE 0.37235618641385926\n",
      "15 Train Loss 169548.23 Test MSE 42655.614702643164 Test RE 0.36612338133885497\n",
      "16 Train Loss 167769.89 Test MSE 41778.39294742383 Test RE 0.3623391217826759\n",
      "17 Train Loss 165350.17 Test MSE 40696.8678388584 Test RE 0.357618399435855\n",
      "18 Train Loss 162508.84 Test MSE 39857.05747613722 Test RE 0.35390930287000055\n",
      "19 Train Loss 158226.52 Test MSE 39108.514565580896 Test RE 0.3505702211251264\n",
      "20 Train Loss 155198.48 Test MSE 37294.87870037278 Test RE 0.3423449782234212\n",
      "21 Train Loss 149570.55 Test MSE 35354.18099340318 Test RE 0.33331875628468355\n",
      "22 Train Loss 145264.12 Test MSE 33649.53504292421 Test RE 0.32518379569816025\n",
      "23 Train Loss 142287.97 Test MSE 33262.717134088816 Test RE 0.32330931951172004\n",
      "24 Train Loss 138222.48 Test MSE 31649.743268215316 Test RE 0.31537296266969383\n",
      "25 Train Loss 134565.77 Test MSE 30895.924321981784 Test RE 0.31159462574229474\n",
      "26 Train Loss 131671.97 Test MSE 30059.19371932549 Test RE 0.30734632592072425\n",
      "27 Train Loss 129231.29 Test MSE 29614.666512797718 Test RE 0.30506528195090027\n",
      "28 Train Loss 125146.125 Test MSE 27644.860381223458 Test RE 0.29474507880487893\n",
      "29 Train Loss 122192.8 Test MSE 27065.28064931612 Test RE 0.2916390196008589\n",
      "30 Train Loss 118990.77 Test MSE 26079.800293924058 Test RE 0.286280319303932\n",
      "31 Train Loss 116499.35 Test MSE 25466.84363146947 Test RE 0.28289607572435826\n",
      "32 Train Loss 114803.71 Test MSE 25212.838640388545 Test RE 0.2814817448020232\n",
      "33 Train Loss 112258.58 Test MSE 24349.24584328262 Test RE 0.2766190716634695\n",
      "34 Train Loss 110280.83 Test MSE 23758.990767223037 Test RE 0.2732457127704706\n",
      "35 Train Loss 108682.27 Test MSE 23593.592367105866 Test RE 0.27229295062560005\n",
      "36 Train Loss 106114.18 Test MSE 22744.16025328827 Test RE 0.2673463846979759\n",
      "37 Train Loss 104174.805 Test MSE 22472.520621548312 Test RE 0.2657450942362868\n",
      "38 Train Loss 102758.875 Test MSE 22366.184024201946 Test RE 0.2651156157379569\n",
      "39 Train Loss 100951.88 Test MSE 21635.304744105844 Test RE 0.26074793074548525\n",
      "40 Train Loss 100061.234 Test MSE 21440.59870764395 Test RE 0.25957198389790603\n",
      "41 Train Loss 99346.03 Test MSE 21166.597231583615 Test RE 0.25790804246811755\n",
      "42 Train Loss 98470.375 Test MSE 20786.814357870164 Test RE 0.2555838047316161\n",
      "43 Train Loss 97003.445 Test MSE 20625.473798538525 Test RE 0.2545899929684174\n",
      "44 Train Loss 95999.36 Test MSE 20254.20013425688 Test RE 0.2522881840145068\n",
      "45 Train Loss 95004.98 Test MSE 20119.057485096633 Test RE 0.25144510068075804\n",
      "46 Train Loss 94175.54 Test MSE 19974.01513062226 Test RE 0.25053710194130335\n",
      "47 Train Loss 93289.125 Test MSE 19586.996014102835 Test RE 0.24809800935812248\n",
      "48 Train Loss 92478.0 Test MSE 19258.948238942892 Test RE 0.24601163377583135\n",
      "49 Train Loss 91813.43 Test MSE 18763.154574744523 Test RE 0.2428243810437143\n",
      "50 Train Loss 91256.695 Test MSE 18606.22528926595 Test RE 0.2418067944571803\n",
      "51 Train Loss 90254.97 Test MSE 18549.96954049135 Test RE 0.2414409674343549\n",
      "52 Train Loss 88949.55 Test MSE 18342.54391147975 Test RE 0.2400872769562395\n",
      "53 Train Loss 88191.87 Test MSE 18122.739782092332 Test RE 0.23864442296335192\n",
      "54 Train Loss 87173.125 Test MSE 17813.707818116105 Test RE 0.2366009720538153\n",
      "55 Train Loss 86623.81 Test MSE 17680.0785563566 Test RE 0.23571187220287632\n",
      "56 Train Loss 86272.07 Test MSE 17608.42878390402 Test RE 0.2352337677821665\n",
      "57 Train Loss 85759.18 Test MSE 17425.668671677715 Test RE 0.23400982309666044\n",
      "58 Train Loss 85073.04 Test MSE 17213.407880344897 Test RE 0.23258022806866996\n",
      "59 Train Loss 84797.99 Test MSE 17042.6336772697 Test RE 0.2314236381766045\n",
      "60 Train Loss 84240.28 Test MSE 17021.665472885717 Test RE 0.2312812296814969\n",
      "61 Train Loss 83656.4 Test MSE 16958.124291458535 Test RE 0.23084914437704182\n",
      "62 Train Loss 83144.34 Test MSE 16879.46977323091 Test RE 0.2303131643640103\n",
      "63 Train Loss 82741.92 Test MSE 16888.985293418475 Test RE 0.2303780728243776\n",
      "64 Train Loss 81995.06 Test MSE 16828.033942277823 Test RE 0.22996198664345555\n",
      "65 Train Loss 81121.55 Test MSE 16671.799675187667 Test RE 0.22889199468302673\n",
      "66 Train Loss 80741.734 Test MSE 16629.811782888904 Test RE 0.22860358097343342\n",
      "67 Train Loss 80358.78 Test MSE 16375.894802994126 Test RE 0.22685161848170401\n",
      "68 Train Loss 79669.516 Test MSE 16017.911582862504 Test RE 0.22435838658900253\n",
      "69 Train Loss 79249.03 Test MSE 15898.337149726935 Test RE 0.2235193951360668\n",
      "70 Train Loss 78792.94 Test MSE 15928.743862350633 Test RE 0.22373304148661743\n",
      "71 Train Loss 78340.09 Test MSE 15791.963916102195 Test RE 0.2227703738584645\n",
      "72 Train Loss 77880.31 Test MSE 15644.167191457156 Test RE 0.22172547121394898\n",
      "73 Train Loss 77427.805 Test MSE 15474.789125146895 Test RE 0.22052190321595086\n",
      "74 Train Loss 77137.45 Test MSE 15458.760944302901 Test RE 0.22040766965090797\n",
      "75 Train Loss 76799.82 Test MSE 15436.052382873815 Test RE 0.22024572327680592\n",
      "76 Train Loss 76347.52 Test MSE 15306.489916761617 Test RE 0.21931945957618781\n",
      "77 Train Loss 75892.3 Test MSE 15183.918989248232 Test RE 0.21843956409001714\n",
      "78 Train Loss 75572.77 Test MSE 15008.372330540085 Test RE 0.2171731645892406\n",
      "79 Train Loss 75304.49 Test MSE 14929.391014534907 Test RE 0.21660097568411577\n",
      "80 Train Loss 74987.836 Test MSE 14828.40112691986 Test RE 0.2158671337492505\n",
      "81 Train Loss 74646.66 Test MSE 14661.960314479868 Test RE 0.21465221880850144\n",
      "82 Train Loss 74439.375 Test MSE 14633.97528830911 Test RE 0.21444726950576223\n",
      "83 Train Loss 74252.15 Test MSE 14581.157268600315 Test RE 0.2140599202503079\n",
      "84 Train Loss 74091.38 Test MSE 14684.913276908328 Test RE 0.21482016966489387\n",
      "85 Train Loss 73959.914 Test MSE 14749.053593231341 Test RE 0.2152888009965727\n",
      "86 Train Loss 73797.2 Test MSE 14740.910502705367 Test RE 0.2152293613101736\n",
      "87 Train Loss 73623.51 Test MSE 14717.488201534892 Test RE 0.21505830094621214\n",
      "88 Train Loss 73265.14 Test MSE 14661.811183418406 Test RE 0.21465112716066073\n",
      "89 Train Loss 73050.84 Test MSE 14628.087178255015 Test RE 0.21440412278254187\n",
      "90 Train Loss 72932.914 Test MSE 14596.192804104821 Test RE 0.21417025704316614\n",
      "91 Train Loss 72761.69 Test MSE 14591.439134085458 Test RE 0.2141353788509529\n",
      "92 Train Loss 72590.22 Test MSE 14579.01917862098 Test RE 0.21404422546898588\n",
      "93 Train Loss 72326.59 Test MSE 14374.22032071198 Test RE 0.21253551456351943\n",
      "94 Train Loss 72129.16 Test MSE 14247.24994462638 Test RE 0.21159474800958714\n",
      "95 Train Loss 71823.57 Test MSE 14127.477653595271 Test RE 0.21070346456822822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 71687.99 Test MSE 14127.331900407955 Test RE 0.2107023776516001\n",
      "97 Train Loss 71566.51 Test MSE 14149.395550615476 Test RE 0.21086684783934262\n",
      "98 Train Loss 71236.41 Test MSE 14132.023213811557 Test RE 0.21073735909210764\n",
      "99 Train Loss 70944.484 Test MSE 14048.688090164482 Test RE 0.21011509183753424\n",
      "Training time: 520.70\n",
      "3D_HTTP_tanh\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 577317.94 Test MSE 300848.08719228895 Test RE 0.9723284415844197\n",
      "1 Train Loss 237281.75 Test MSE 82131.25920446182 Test RE 0.5080349646043812\n",
      "2 Train Loss 223264.5 Test MSE 73450.07512615515 Test RE 0.4804359357181559\n",
      "3 Train Loss 209433.84 Test MSE 63560.29256689245 Test RE 0.446922589052083\n",
      "4 Train Loss 195770.6 Test MSE 55189.55428892192 Test RE 0.41645473622526724\n",
      "5 Train Loss 188430.78 Test MSE 51690.83128019923 Test RE 0.40303811634941383\n",
      "6 Train Loss 180622.53 Test MSE 47979.414463507186 Test RE 0.3882995023866269\n",
      "7 Train Loss 174676.16 Test MSE 46051.0080264672 Test RE 0.3804161383953156\n",
      "8 Train Loss 168944.9 Test MSE 42853.08213052331 Test RE 0.36696985798234466\n",
      "9 Train Loss 166031.72 Test MSE 41793.6935866637 Test RE 0.3624054660372714\n",
      "10 Train Loss 162398.16 Test MSE 41100.54599106293 Test RE 0.3593876573434602\n",
      "11 Train Loss 158302.36 Test MSE 39095.73613722717 Test RE 0.35051294328558635\n",
      "12 Train Loss 151830.6 Test MSE 36632.07008981349 Test RE 0.33928924469716165\n",
      "13 Train Loss 146501.4 Test MSE 35267.04739143118 Test RE 0.33290775566110736\n",
      "14 Train Loss 141343.14 Test MSE 34224.752485114805 Test RE 0.32795142339476085\n",
      "15 Train Loss 138353.11 Test MSE 33619.94720209973 Test RE 0.32504079814510045\n",
      "16 Train Loss 134722.0 Test MSE 32202.54902911469 Test RE 0.3181152486741957\n",
      "17 Train Loss 131323.22 Test MSE 31166.851262569104 Test RE 0.31295783332939414\n",
      "18 Train Loss 128945.24 Test MSE 29860.718063769156 Test RE 0.3063299681255949\n",
      "19 Train Loss 123524.63 Test MSE 27606.2076235316 Test RE 0.2945389519691213\n",
      "20 Train Loss 120805.91 Test MSE 27359.704230917083 Test RE 0.2932209937630976\n",
      "21 Train Loss 117990.03 Test MSE 26522.563996511013 Test RE 0.2887002202474769\n",
      "22 Train Loss 115311.836 Test MSE 25183.155823804518 Test RE 0.28131600321923617\n",
      "23 Train Loss 111790.164 Test MSE 24255.288640662897 Test RE 0.2760848564505452\n",
      "24 Train Loss 110008.05 Test MSE 24162.62122015429 Test RE 0.27555696014988845\n",
      "25 Train Loss 108938.34 Test MSE 23880.901457831398 Test RE 0.2739458467024056\n",
      "26 Train Loss 105585.81 Test MSE 22523.659016327656 Test RE 0.26604728675263023\n",
      "27 Train Loss 104411.75 Test MSE 22289.39721404026 Test RE 0.2646601315780831\n",
      "28 Train Loss 103264.64 Test MSE 21302.693267191393 Test RE 0.2587358567097853\n",
      "29 Train Loss 102176.016 Test MSE 20752.473007774464 Test RE 0.25537259581656435\n",
      "30 Train Loss 101281.24 Test MSE 20555.535855338803 Test RE 0.2541579878604662\n",
      "31 Train Loss 99541.92 Test MSE 20204.99614709387 Test RE 0.25198155297483904\n",
      "32 Train Loss 97304.734 Test MSE 19417.94190035461 Test RE 0.2470250300868028\n",
      "33 Train Loss 93657.66 Test MSE 18207.299469043868 Test RE 0.2392005257757179\n",
      "34 Train Loss 91756.32 Test MSE 17878.499644308813 Test RE 0.23703086276363206\n",
      "35 Train Loss 90594.49 Test MSE 17403.830215025086 Test RE 0.2338631424838378\n",
      "36 Train Loss 89842.59 Test MSE 17373.95855795675 Test RE 0.23366235680346545\n",
      "37 Train Loss 88662.734 Test MSE 17241.24196163794 Test RE 0.2327681932339059\n",
      "38 Train Loss 86322.77 Test MSE 16729.350394797068 Test RE 0.22928671962863098\n",
      "39 Train Loss 84480.66 Test MSE 15814.027329626122 Test RE 0.22292593903563332\n",
      "40 Train Loss 82767.17 Test MSE 15255.03384173776 Test RE 0.21895050434501467\n",
      "41 Train Loss 79438.46 Test MSE 15220.1562237747 Test RE 0.21870006760589625\n",
      "42 Train Loss 77820.375 Test MSE 14506.371191196093 Test RE 0.21351026286538016\n",
      "43 Train Loss 76530.445 Test MSE 14366.792289318151 Test RE 0.2124805924721948\n",
      "44 Train Loss 74943.445 Test MSE 13924.325949062217 Test RE 0.2091830315672792\n",
      "45 Train Loss 73968.984 Test MSE 13606.23408173045 Test RE 0.2067799049455603\n",
      "46 Train Loss 72404.17 Test MSE 13035.003267662709 Test RE 0.20239274207678742\n",
      "47 Train Loss 71639.234 Test MSE 12668.25249610583 Test RE 0.1995251829267302\n",
      "48 Train Loss 70532.414 Test MSE 12533.700683269859 Test RE 0.19846275772247934\n",
      "49 Train Loss 69899.766 Test MSE 12294.163537978418 Test RE 0.1965571539391954\n",
      "50 Train Loss 68905.51 Test MSE 12229.412793966689 Test RE 0.19603885824190784\n",
      "51 Train Loss 68228.23 Test MSE 11874.543901364817 Test RE 0.19317362557445855\n",
      "52 Train Loss 67479.03 Test MSE 11950.664166647304 Test RE 0.19379179405976946\n",
      "53 Train Loss 66143.625 Test MSE 10907.68930538072 Test RE 0.18514233712711906\n",
      "54 Train Loss 64688.164 Test MSE 10682.454095214785 Test RE 0.18322084418996498\n",
      "55 Train Loss 63278.875 Test MSE 10544.508294713858 Test RE 0.18203400681158213\n",
      "56 Train Loss 60702.992 Test MSE 10375.031163995924 Test RE 0.18056520577467586\n",
      "57 Train Loss 59460.547 Test MSE 9818.298553589548 Test RE 0.1756537715456182\n",
      "58 Train Loss 58760.14 Test MSE 9768.059772786952 Test RE 0.1752037980173912\n",
      "59 Train Loss 57926.81 Test MSE 9452.208791037927 Test RE 0.17234790765201524\n",
      "60 Train Loss 57075.758 Test MSE 9407.999728938767 Test RE 0.17194438983164104\n",
      "61 Train Loss 56342.777 Test MSE 8960.461855183556 Test RE 0.16780486969313432\n",
      "62 Train Loss 55073.87 Test MSE 8721.140245731433 Test RE 0.1655487848983853\n",
      "63 Train Loss 53715.246 Test MSE 8622.061859227113 Test RE 0.16460572252182287\n",
      "64 Train Loss 53389.805 Test MSE 8316.97654463179 Test RE 0.16166726847476642\n",
      "65 Train Loss 52238.266 Test MSE 7907.496926688146 Test RE 0.15763726022081265\n",
      "66 Train Loss 51159.316 Test MSE 7575.735605597263 Test RE 0.15429496947419777\n",
      "67 Train Loss 50139.86 Test MSE 7512.158863527685 Test RE 0.15364617180869553\n",
      "68 Train Loss 49560.746 Test MSE 7603.317528360612 Test RE 0.15457559472146118\n",
      "69 Train Loss 48339.14 Test MSE 7307.300491222691 Test RE 0.15153670703878697\n",
      "70 Train Loss 47879.215 Test MSE 7155.156849437968 Test RE 0.1499508532850529\n",
      "71 Train Loss 47494.965 Test MSE 7082.630361029598 Test RE 0.14918894766980267\n",
      "72 Train Loss 47046.113 Test MSE 7167.096722522308 Test RE 0.15007591328762837\n",
      "73 Train Loss 46304.62 Test MSE 6985.597402883668 Test RE 0.14816346925583165\n",
      "74 Train Loss 45751.7 Test MSE 6718.555540703291 Test RE 0.14530391573085708\n",
      "75 Train Loss 45371.848 Test MSE 6538.44666639585 Test RE 0.14334305462271957\n",
      "76 Train Loss 45106.71 Test MSE 6648.156620574395 Test RE 0.14454064331140312\n",
      "77 Train Loss 44821.168 Test MSE 6651.3441115731985 Test RE 0.1445752895159973\n",
      "78 Train Loss 44377.72 Test MSE 6515.956166044958 Test RE 0.1430963114539105\n",
      "79 Train Loss 43785.277 Test MSE 6448.693699659159 Test RE 0.14235582312565664\n",
      "80 Train Loss 43428.484 Test MSE 6276.726893252262 Test RE 0.1404449016894738\n",
      "81 Train Loss 42706.945 Test MSE 5976.7146444484815 Test RE 0.13704734384577225\n",
      "82 Train Loss 41962.664 Test MSE 5645.734649049059 Test RE 0.13319857951841485\n",
      "83 Train Loss 41620.29 Test MSE 5620.650875757036 Test RE 0.13290235213921067\n",
      "84 Train Loss 41210.965 Test MSE 5424.0239610300205 Test RE 0.13055699993888722\n",
      "85 Train Loss 40151.9 Test MSE 5258.547379201374 Test RE 0.1285500523454668\n",
      "86 Train Loss 39593.004 Test MSE 5113.01772874373 Test RE 0.1267587685766262\n",
      "87 Train Loss 39420.645 Test MSE 5017.303234321265 Test RE 0.12556671625559943\n",
      "88 Train Loss 39245.594 Test MSE 4916.332679575406 Test RE 0.12429681311700029\n",
      "89 Train Loss 38600.344 Test MSE 4870.076934455569 Test RE 0.12371070253329602\n",
      "90 Train Loss 38286.867 Test MSE 4811.585965460109 Test RE 0.12296555857682813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 Train Loss 37063.54 Test MSE 4615.467181767097 Test RE 0.12043346893382474\n",
      "92 Train Loss 36642.457 Test MSE 4724.57817649688 Test RE 0.12184869492435513\n",
      "93 Train Loss 36332.81 Test MSE 4607.945952686108 Test RE 0.12033530151181346\n",
      "94 Train Loss 36124.266 Test MSE 4545.066530332033 Test RE 0.1195114414644727\n",
      "95 Train Loss 35965.902 Test MSE 4372.975035377841 Test RE 0.11722705676095856\n",
      "96 Train Loss 35802.52 Test MSE 4379.685959231397 Test RE 0.11731697269362606\n",
      "97 Train Loss 35622.17 Test MSE 4443.202425646244 Test RE 0.11816460619757904\n",
      "98 Train Loss 35445.56 Test MSE 4393.284257072865 Test RE 0.1174989577453938\n",
      "99 Train Loss 35068.367 Test MSE 4187.083980729814 Test RE 0.1147083936472094\n",
      "Training time: 571.02\n",
      "3D_HTTP_tanh\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 698017.2 Test MSE 297738.5165646289 Test RE 0.9672903882798556\n",
      "1 Train Loss 530934.1 Test MSE 267525.1261002635 Test RE 0.9168993205877769\n",
      "2 Train Loss 490667.2 Test MSE 243173.81829285395 Test RE 0.8741737662002917\n",
      "3 Train Loss 453755.53 Test MSE 222544.30954774946 Test RE 0.8362721022927772\n",
      "4 Train Loss 389245.75 Test MSE 163596.28692388322 Test RE 0.7170114374996511\n",
      "5 Train Loss 354607.6 Test MSE 153203.51071813633 Test RE 0.693862986283108\n",
      "6 Train Loss 281856.28 Test MSE 101950.24313099345 Test RE 0.5660222392221853\n",
      "7 Train Loss 245461.34 Test MSE 87196.66671634954 Test RE 0.5234669945476506\n",
      "8 Train Loss 245173.56 Test MSE 86881.8781331364 Test RE 0.5225212564354559\n",
      "9 Train Loss 242074.77 Test MSE 83660.84482951603 Test RE 0.5127438796703385\n",
      "10 Train Loss 239291.34 Test MSE 81291.35585460372 Test RE 0.5054306164935508\n",
      "11 Train Loss 236187.92 Test MSE 79787.3062188039 Test RE 0.500733057318703\n",
      "12 Train Loss 234183.34 Test MSE 78081.11453163094 Test RE 0.49535022445661137\n",
      "13 Train Loss 232001.06 Test MSE 77494.09371773753 Test RE 0.49348466780439615\n",
      "14 Train Loss 230799.06 Test MSE 77660.89154690654 Test RE 0.494015469075283\n",
      "15 Train Loss 229260.78 Test MSE 76497.63778710924 Test RE 0.49030167224557064\n",
      "16 Train Loss 227863.12 Test MSE 75553.06487888044 Test RE 0.48726521109575904\n",
      "17 Train Loss 226803.88 Test MSE 75115.47932251298 Test RE 0.4858520993546573\n",
      "18 Train Loss 226106.3 Test MSE 74216.44332009029 Test RE 0.4829358336715697\n",
      "19 Train Loss 225772.53 Test MSE 74249.41694774746 Test RE 0.48304310355212005\n",
      "20 Train Loss 225297.42 Test MSE 73977.39224695403 Test RE 0.48215743849883613\n",
      "21 Train Loss 224995.95 Test MSE 73547.78503787065 Test RE 0.48075538910938126\n",
      "22 Train Loss 224542.14 Test MSE 73501.976434232 Test RE 0.48060564861555677\n",
      "23 Train Loss 224281.56 Test MSE 73477.3105069126 Test RE 0.4805250005888927\n",
      "24 Train Loss 223996.56 Test MSE 73160.26718269638 Test RE 0.4794871840387379\n",
      "25 Train Loss 223712.42 Test MSE 72798.41017978657 Test RE 0.47829992162209284\n",
      "26 Train Loss 223513.64 Test MSE 72958.83928089375 Test RE 0.47882665701168436\n",
      "27 Train Loss 222945.44 Test MSE 72779.4476404098 Test RE 0.4782376237520114\n",
      "28 Train Loss 222806.34 Test MSE 72606.9086538563 Test RE 0.4776704059436884\n",
      "29 Train Loss 222599.53 Test MSE 72472.12800395061 Test RE 0.47722684875129356\n",
      "30 Train Loss 222523.75 Test MSE 72450.80377719652 Test RE 0.4771566338803479\n",
      "31 Train Loss 222335.53 Test MSE 72427.23254808385 Test RE 0.47707900821787086\n",
      "32 Train Loss 222182.11 Test MSE 72265.93236735185 Test RE 0.4765474690431911\n",
      "33 Train Loss 222050.56 Test MSE 72239.91090438627 Test RE 0.4764616638899251\n",
      "34 Train Loss 221926.08 Test MSE 72153.36399664446 Test RE 0.47617616629364923\n",
      "35 Train Loss 221889.53 Test MSE 72177.01527945099 Test RE 0.4762542032263007\n",
      "36 Train Loss 221743.08 Test MSE 72159.35361190194 Test RE 0.4761959301181286\n",
      "37 Train Loss 221201.12 Test MSE 71914.73342235727 Test RE 0.4753880928566849\n",
      "38 Train Loss 220798.42 Test MSE 71544.61925957196 Test RE 0.474163205938517\n",
      "39 Train Loss 220527.8 Test MSE 71130.40799143765 Test RE 0.472788617303007\n",
      "40 Train Loss 220328.16 Test MSE 71022.77011748002 Test RE 0.4724307588997875\n",
      "41 Train Loss 220117.72 Test MSE 70820.90560365234 Test RE 0.4717588978472641\n",
      "42 Train Loss 220018.64 Test MSE 70562.61410150325 Test RE 0.4708978341710079\n",
      "43 Train Loss 219829.6 Test MSE 70476.97407861294 Test RE 0.4706119891423441\n",
      "44 Train Loss 219538.77 Test MSE 70269.47939114751 Test RE 0.4699187026574167\n",
      "45 Train Loss 219374.81 Test MSE 70138.70925656786 Test RE 0.46948124425379834\n",
      "46 Train Loss 219033.77 Test MSE 69742.00277283844 Test RE 0.4681516620839415\n",
      "47 Train Loss 218799.12 Test MSE 69324.10263559142 Test RE 0.4667469519283415\n",
      "48 Train Loss 217886.11 Test MSE 68727.19749441245 Test RE 0.46473317923170343\n",
      "49 Train Loss 217308.58 Test MSE 68558.1552625839 Test RE 0.4641612957366841\n",
      "50 Train Loss 216312.14 Test MSE 68148.85454945805 Test RE 0.4627736712331743\n",
      "51 Train Loss 215762.73 Test MSE 67529.4761024707 Test RE 0.46066588609149184\n",
      "52 Train Loss 214614.47 Test MSE 66925.831348542 Test RE 0.45860232222715397\n",
      "53 Train Loss 213969.33 Test MSE 66252.04783382127 Test RE 0.45628796663418586\n",
      "54 Train Loss 213259.06 Test MSE 64605.00245079512 Test RE 0.45058054485832627\n",
      "55 Train Loss 212151.94 Test MSE 64143.85136070211 Test RE 0.44896954087329566\n",
      "56 Train Loss 211521.25 Test MSE 64185.25345823912 Test RE 0.449114412765237\n",
      "57 Train Loss 210897.0 Test MSE 63942.67755027668 Test RE 0.44826493832112124\n",
      "58 Train Loss 209767.16 Test MSE 63123.16588029274 Test RE 0.44538311498956806\n",
      "59 Train Loss 209165.72 Test MSE 62653.6928461544 Test RE 0.4437237749194558\n",
      "60 Train Loss 208078.33 Test MSE 61823.22118905987 Test RE 0.4407731963112018\n",
      "61 Train Loss 207225.58 Test MSE 61153.48279182556 Test RE 0.43837922055196427\n",
      "62 Train Loss 206457.05 Test MSE 60612.072814660256 Test RE 0.43643435553315924\n",
      "63 Train Loss 205805.77 Test MSE 60590.994606624874 Test RE 0.436358462615912\n",
      "64 Train Loss 204006.33 Test MSE 59113.829460485 Test RE 0.4310065889018158\n",
      "65 Train Loss 202840.16 Test MSE 58225.83772192554 Test RE 0.4277571081203067\n",
      "66 Train Loss 202119.7 Test MSE 58132.94724388552 Test RE 0.42741576118497937\n",
      "67 Train Loss 201611.5 Test MSE 57714.23885022966 Test RE 0.42587372701999815\n",
      "68 Train Loss 201132.31 Test MSE 57488.525485643295 Test RE 0.42504014104850624\n",
      "69 Train Loss 200814.97 Test MSE 57642.35153016587 Test RE 0.42560841588746245\n",
      "70 Train Loss 200007.58 Test MSE 57612.74326990875 Test RE 0.4254990939823507\n",
      "71 Train Loss 199300.98 Test MSE 57579.70141758137 Test RE 0.4253770611314952\n",
      "72 Train Loss 198986.11 Test MSE 57462.09702218813 Test RE 0.4249424306875641\n",
      "73 Train Loss 198726.34 Test MSE 57519.337354299154 Test RE 0.4251540292250607\n",
      "74 Train Loss 198328.72 Test MSE 57244.68115490775 Test RE 0.42413775430511913\n",
      "75 Train Loss 198158.67 Test MSE 57119.22051461232 Test RE 0.42367271737005807\n",
      "76 Train Loss 197925.73 Test MSE 57140.500950354755 Test RE 0.423751632141342\n",
      "77 Train Loss 197425.39 Test MSE 56703.20194294652 Test RE 0.4221270220358052\n",
      "78 Train Loss 197012.98 Test MSE 56586.33416184071 Test RE 0.4216917865258084\n",
      "79 Train Loss 196751.45 Test MSE 56486.83354194404 Test RE 0.4213208749278725\n",
      "80 Train Loss 196065.56 Test MSE 56039.087860376196 Test RE 0.4196477424065588\n",
      "81 Train Loss 195225.66 Test MSE 55153.373589806375 Test RE 0.4163182059163462\n",
      "82 Train Loss 195079.23 Test MSE 55185.666049363674 Test RE 0.41644006583770954\n",
      "83 Train Loss 194475.89 Test MSE 54816.99618407783 Test RE 0.41504671328865467\n",
      "84 Train Loss 194081.55 Test MSE 54352.183090030536 Test RE 0.4132833017438995\n",
      "85 Train Loss 192922.27 Test MSE 53444.927191159804 Test RE 0.409819488638581\n",
      "86 Train Loss 192457.78 Test MSE 53716.20749234184 Test RE 0.41085827040866013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 Train Loss 192421.03 Test MSE 53592.23146623412 Test RE 0.4103838698303741\n",
      "88 Train Loss 191951.2 Test MSE 52928.52748219896 Test RE 0.4078347880403371\n",
      "89 Train Loss 191684.19 Test MSE 52980.298582043615 Test RE 0.4080341974701358\n",
      "90 Train Loss 191406.05 Test MSE 53016.94353707649 Test RE 0.4081752858612573\n",
      "91 Train Loss 191325.97 Test MSE 52997.25735399516 Test RE 0.40809949726487704\n",
      "92 Train Loss 191179.55 Test MSE 52910.689846730966 Test RE 0.4077660593069203\n",
      "93 Train Loss 190914.62 Test MSE 52555.98154974799 Test RE 0.4063969481617851\n",
      "94 Train Loss 190379.45 Test MSE 51959.28082311921 Test RE 0.4040833238299138\n",
      "95 Train Loss 189827.7 Test MSE 51706.85531898904 Test RE 0.4031005819532103\n",
      "96 Train Loss 189678.66 Test MSE 51850.87512552741 Test RE 0.4036615723547375\n",
      "97 Train Loss 189645.58 Test MSE 51842.675674607715 Test RE 0.40362965453172917\n",
      "98 Train Loss 189572.19 Test MSE 51754.76139485203 Test RE 0.403287273796476\n",
      "99 Train Loss 189488.45 Test MSE 51738.222159073106 Test RE 0.40322282952023647\n",
      "Training time: 445.93\n",
      "3D_HTTP_tanh\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 650755.8 Test MSE 314059.5081234393 Test RE 0.9934484472058256\n",
      "1 Train Loss 461405.34 Test MSE 222154.2582000146 Test RE 0.8355389177108798\n",
      "2 Train Loss 330662.44 Test MSE 134138.88598584707 Test RE 0.6492571033377899\n",
      "3 Train Loss 242136.1 Test MSE 83066.3221780367 Test RE 0.5109187644987713\n",
      "4 Train Loss 234488.0 Test MSE 77056.20592517256 Test RE 0.49208845147633656\n",
      "5 Train Loss 219143.0 Test MSE 66828.50556661325 Test RE 0.45826874349143304\n",
      "6 Train Loss 212890.28 Test MSE 64496.00721335821 Test RE 0.450200296712102\n",
      "7 Train Loss 206315.27 Test MSE 60371.54864876291 Test RE 0.4355675533309391\n",
      "8 Train Loss 201822.47 Test MSE 58299.733496664194 Test RE 0.42802846033633946\n",
      "9 Train Loss 197826.31 Test MSE 55989.629960431885 Test RE 0.41946251921472505\n",
      "10 Train Loss 195176.05 Test MSE 54797.65756395705 Test RE 0.41497349568001446\n",
      "11 Train Loss 191473.78 Test MSE 54037.43349171586 Test RE 0.41208491728949487\n",
      "12 Train Loss 189774.27 Test MSE 53039.52786685469 Test RE 0.40826221452160627\n",
      "13 Train Loss 187860.56 Test MSE 52167.01832386343 Test RE 0.4048902973170442\n",
      "14 Train Loss 186345.31 Test MSE 51360.052111429264 Test RE 0.40174648904963056\n",
      "15 Train Loss 183841.12 Test MSE 50394.37746929083 Test RE 0.39795173686934665\n",
      "16 Train Loss 182127.44 Test MSE 49073.196380750116 Test RE 0.3927005735150719\n",
      "17 Train Loss 180602.39 Test MSE 48423.4343155823 Test RE 0.39009210064406424\n",
      "18 Train Loss 178472.38 Test MSE 47795.420800143234 Test RE 0.38755425283345885\n",
      "19 Train Loss 176208.5 Test MSE 47169.88525978556 Test RE 0.3850097893457271\n",
      "20 Train Loss 174737.52 Test MSE 45655.34368540553 Test RE 0.37877836965265776\n",
      "21 Train Loss 172772.97 Test MSE 44342.78582440934 Test RE 0.3732938622393142\n",
      "22 Train Loss 171076.8 Test MSE 44202.64299988497 Test RE 0.37270350853337825\n",
      "23 Train Loss 169864.66 Test MSE 43942.452911255146 Test RE 0.37160496692956485\n",
      "24 Train Loss 166837.73 Test MSE 41895.15609569385 Test RE 0.36284510506887174\n",
      "25 Train Loss 162772.06 Test MSE 40222.01656730725 Test RE 0.35552593109526853\n",
      "26 Train Loss 160055.3 Test MSE 39774.52955733954 Test RE 0.35354271116801395\n",
      "27 Train Loss 158039.8 Test MSE 38480.22258631372 Test RE 0.34774280260648754\n",
      "28 Train Loss 155411.58 Test MSE 38625.71236338566 Test RE 0.34839957234254415\n",
      "29 Train Loss 153042.7 Test MSE 39194.89685197168 Test RE 0.3509571746291391\n",
      "30 Train Loss 151536.08 Test MSE 38397.81303021643 Test RE 0.3473702386279587\n",
      "31 Train Loss 149116.84 Test MSE 37051.815617623295 Test RE 0.3412275667798277\n",
      "32 Train Loss 147012.03 Test MSE 35977.88687033923 Test RE 0.33624604759849297\n",
      "33 Train Loss 145736.44 Test MSE 35847.6469533778 Test RE 0.33563689062218527\n",
      "34 Train Loss 144036.12 Test MSE 35257.58491431692 Test RE 0.3328630915400239\n",
      "35 Train Loss 143064.61 Test MSE 34565.335792550046 Test RE 0.32957916745917876\n",
      "36 Train Loss 142118.39 Test MSE 34214.42675224014 Test RE 0.3279019475913706\n",
      "37 Train Loss 140496.2 Test MSE 33630.16605012924 Test RE 0.3250901927818723\n",
      "38 Train Loss 139426.86 Test MSE 33302.947038952465 Test RE 0.32350477517338155\n",
      "39 Train Loss 137749.02 Test MSE 32788.04185538437 Test RE 0.32099413890463085\n",
      "40 Train Loss 136979.48 Test MSE 32416.900828291997 Test RE 0.3191722376718741\n",
      "41 Train Loss 135969.06 Test MSE 32469.575419181052 Test RE 0.31943144571683285\n",
      "42 Train Loss 133909.56 Test MSE 32205.99085915724 Test RE 0.3181322484055613\n",
      "43 Train Loss 132882.61 Test MSE 31394.538879103704 Test RE 0.3140989007326518\n",
      "44 Train Loss 131559.45 Test MSE 30695.591820323516 Test RE 0.31058277627916175\n",
      "45 Train Loss 130706.11 Test MSE 30443.96183860779 Test RE 0.309307140951895\n",
      "46 Train Loss 128986.6 Test MSE 30139.268787563222 Test RE 0.30775542554269136\n",
      "47 Train Loss 128429.53 Test MSE 29985.607284796664 Test RE 0.3069698956833122\n",
      "48 Train Loss 127264.64 Test MSE 29119.265705709706 Test RE 0.3025029204672808\n",
      "49 Train Loss 126424.95 Test MSE 28728.954237507263 Test RE 0.300468722819\n",
      "50 Train Loss 125160.76 Test MSE 28988.36579870014 Test RE 0.3018222335697119\n",
      "51 Train Loss 124038.31 Test MSE 28416.06563721143 Test RE 0.2988280327917761\n",
      "52 Train Loss 123351.586 Test MSE 28509.37680015515 Test RE 0.29931826845866044\n",
      "53 Train Loss 122764.586 Test MSE 28334.68374055437 Test RE 0.29839981324958903\n",
      "54 Train Loss 122256.75 Test MSE 28293.908241411573 Test RE 0.29818502733528685\n",
      "55 Train Loss 121384.93 Test MSE 27856.38347293821 Test RE 0.2958705427337237\n",
      "56 Train Loss 121032.17 Test MSE 27940.490145148306 Test RE 0.29631686649146743\n",
      "57 Train Loss 120741.08 Test MSE 27685.11276432058 Test RE 0.29495958298545044\n",
      "58 Train Loss 120355.92 Test MSE 27523.46666816744 Test RE 0.29409722663850163\n",
      "59 Train Loss 119377.71 Test MSE 27909.462379587323 Test RE 0.2961522916375242\n",
      "60 Train Loss 119049.13 Test MSE 27773.88930323752 Test RE 0.295432120929707\n",
      "61 Train Loss 118730.984 Test MSE 27389.6209015045 Test RE 0.29338126229360306\n",
      "62 Train Loss 118318.48 Test MSE 27348.345850213816 Test RE 0.29316012209966724\n",
      "63 Train Loss 118046.0 Test MSE 27424.531759658206 Test RE 0.29356817488485815\n",
      "64 Train Loss 117543.09 Test MSE 27226.309366823727 Test RE 0.29250530679714254\n",
      "65 Train Loss 117079.51 Test MSE 27113.022920296196 Test RE 0.29189612712060387\n",
      "66 Train Loss 116667.77 Test MSE 27106.566838949286 Test RE 0.29186137228445397\n",
      "67 Train Loss 116197.97 Test MSE 26848.05453232667 Test RE 0.29046631349795826\n",
      "68 Train Loss 115784.0 Test MSE 26764.11092850436 Test RE 0.2900118694360381\n",
      "69 Train Loss 115477.734 Test MSE 26822.521584038313 Test RE 0.2903281614861107\n",
      "70 Train Loss 115102.09 Test MSE 26877.42399248683 Test RE 0.2906251426430321\n",
      "71 Train Loss 114586.27 Test MSE 26287.39856470373 Test RE 0.28741747328787687\n",
      "72 Train Loss 113911.42 Test MSE 26001.793533817578 Test RE 0.28585185505174326\n",
      "73 Train Loss 113687.14 Test MSE 25855.444949210127 Test RE 0.28504627513550246\n",
      "74 Train Loss 113466.26 Test MSE 25575.535793914303 Test RE 0.2834991313649849\n",
      "75 Train Loss 113231.39 Test MSE 25189.479047288434 Test RE 0.2813513187357556\n",
      "76 Train Loss 112666.58 Test MSE 24890.98719716738 Test RE 0.2796793637178977\n",
      "77 Train Loss 112133.04 Test MSE 24825.918707698565 Test RE 0.2793135641940055\n",
      "78 Train Loss 111721.836 Test MSE 24691.71445898269 Test RE 0.278557582830766\n",
      "79 Train Loss 111433.336 Test MSE 25013.838499325542 Test RE 0.28036870325144825\n",
      "80 Train Loss 111186.53 Test MSE 25034.090019405226 Test RE 0.28048217531343034\n",
      "81 Train Loss 110891.36 Test MSE 24973.505603554193 Test RE 0.28014257554781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 Train Loss 110239.29 Test MSE 25064.29708581044 Test RE 0.2806513444225859\n",
      "83 Train Loss 109743.03 Test MSE 24588.137946840277 Test RE 0.2779727238172826\n",
      "84 Train Loss 109502.15 Test MSE 24600.954433995998 Test RE 0.27804516056951395\n",
      "85 Train Loss 109397.15 Test MSE 24631.82155435918 Test RE 0.2782195392307326\n",
      "86 Train Loss 109008.05 Test MSE 24328.235169609252 Test RE 0.2764997002588802\n",
      "87 Train Loss 108818.77 Test MSE 24408.0681064565 Test RE 0.27695299462783823\n",
      "88 Train Loss 108594.07 Test MSE 24430.55814813475 Test RE 0.27708056003407133\n",
      "89 Train Loss 108530.09 Test MSE 24321.907540747823 Test RE 0.2764637399590383\n",
      "90 Train Loss 108324.14 Test MSE 24263.412747176877 Test RE 0.27613108873892633\n",
      "91 Train Loss 108021.02 Test MSE 24202.54654756538 Test RE 0.27578452571105966\n",
      "92 Train Loss 107786.69 Test MSE 24005.409729773794 Test RE 0.27465905640906235\n",
      "93 Train Loss 107618.0 Test MSE 23872.787026750237 Test RE 0.27389930106511695\n",
      "94 Train Loss 107499.164 Test MSE 23641.32760074703 Test RE 0.27256826689774505\n",
      "95 Train Loss 107493.164 Test MSE 23668.11611324621 Test RE 0.27272264974988253\n",
      "96 Train Loss 107234.01 Test MSE 23553.613271756407 Test RE 0.2720621540359568\n",
      "97 Train Loss 106871.39 Test MSE 23652.31032584505 Test RE 0.2726315711844143\n",
      "98 Train Loss 106640.37 Test MSE 23496.566561485906 Test RE 0.2717324878487792\n",
      "99 Train Loss 106570.05 Test MSE 23539.207663474597 Test RE 0.2719789434428432\n",
      "Training time: 355.31\n",
      "3D_HTTP_tanh\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 947129.4 Test MSE 312399.7595440998 Test RE 0.990819870639914\n",
      "1 Train Loss 487334.56 Test MSE 239211.93477362784 Test RE 0.8670233307082065\n",
      "2 Train Loss 362685.0 Test MSE 156200.94488437046 Test RE 0.7006178381752869\n",
      "3 Train Loss 245196.67 Test MSE 86631.97113342863 Test RE 0.5217692252933996\n",
      "4 Train Loss 238403.25 Test MSE 81750.7486311601 Test RE 0.5068567463473954\n",
      "5 Train Loss 226989.9 Test MSE 73948.61444308521 Test RE 0.48206364780512634\n",
      "6 Train Loss 218632.2 Test MSE 69583.7074618752 Test RE 0.46762007202352035\n",
      "7 Train Loss 213354.83 Test MSE 66206.9183202368 Test RE 0.4561325332356996\n",
      "8 Train Loss 205200.05 Test MSE 60499.74603838324 Test RE 0.43602976618666484\n",
      "9 Train Loss 200326.36 Test MSE 58758.58269359726 Test RE 0.4297095622327078\n",
      "10 Train Loss 196497.44 Test MSE 55652.073178608814 Test RE 0.4181961554600378\n",
      "11 Train Loss 193985.2 Test MSE 53965.434929622745 Test RE 0.41181029829883636\n",
      "12 Train Loss 191107.36 Test MSE 52796.18895635571 Test RE 0.40732460915860275\n",
      "13 Train Loss 189179.33 Test MSE 52241.115001697486 Test RE 0.405177743113944\n",
      "14 Train Loss 186050.16 Test MSE 50948.54709731285 Test RE 0.4001338235136808\n",
      "15 Train Loss 182771.75 Test MSE 48909.306645874865 Test RE 0.3920442741197399\n",
      "16 Train Loss 180597.06 Test MSE 47976.722694734264 Test RE 0.38828860993257186\n",
      "17 Train Loss 178406.69 Test MSE 46849.56360927682 Test RE 0.38370029858804383\n",
      "18 Train Loss 176559.31 Test MSE 45664.170403791955 Test RE 0.37881498320267165\n",
      "19 Train Loss 173726.61 Test MSE 44752.336481247716 Test RE 0.37501377403130953\n",
      "20 Train Loss 172188.45 Test MSE 44384.47436539301 Test RE 0.3734692957428856\n",
      "21 Train Loss 170938.1 Test MSE 43643.781150373215 Test RE 0.37033993574549945\n",
      "22 Train Loss 169417.56 Test MSE 42570.51291606877 Test RE 0.36575797446067676\n",
      "23 Train Loss 167066.22 Test MSE 40655.92994982197 Test RE 0.35743848600890515\n",
      "24 Train Loss 165207.56 Test MSE 40365.65924915624 Test RE 0.3561602004468239\n",
      "25 Train Loss 164104.06 Test MSE 40143.820708652616 Test RE 0.3551801729404774\n",
      "26 Train Loss 163244.19 Test MSE 40083.96593410258 Test RE 0.35491528585279886\n",
      "27 Train Loss 162038.02 Test MSE 39299.33691704304 Test RE 0.35142444981297194\n",
      "28 Train Loss 160808.02 Test MSE 38798.652799988035 Test RE 0.3491786528881678\n",
      "29 Train Loss 159827.9 Test MSE 38502.17603547473 Test RE 0.34784198427554336\n",
      "30 Train Loss 159024.19 Test MSE 38355.09504308717 Test RE 0.3471769582607401\n",
      "31 Train Loss 158161.97 Test MSE 37665.796790341454 Test RE 0.3440431708957346\n",
      "32 Train Loss 157271.89 Test MSE 37211.2107758226 Test RE 0.34196075152048816\n",
      "33 Train Loss 155935.86 Test MSE 36698.24255648316 Test RE 0.33959555384291695\n",
      "34 Train Loss 155137.22 Test MSE 36440.80848488478 Test RE 0.33840234516857937\n",
      "35 Train Loss 154475.98 Test MSE 36459.017246216055 Test RE 0.3384868811358339\n",
      "36 Train Loss 154023.2 Test MSE 36268.489367238195 Test RE 0.33760128865503714\n",
      "37 Train Loss 153480.1 Test MSE 36050.24679613983 Test RE 0.33658401238407293\n",
      "38 Train Loss 153164.33 Test MSE 36103.169858961846 Test RE 0.33683098051406596\n",
      "39 Train Loss 152476.22 Test MSE 35818.71082657391 Test RE 0.33550140066502326\n",
      "40 Train Loss 152028.52 Test MSE 35532.16934875113 Test RE 0.33415673886027375\n",
      "41 Train Loss 151450.62 Test MSE 35445.54672657819 Test RE 0.33374917585008484\n",
      "42 Train Loss 151294.67 Test MSE 35523.192144661894 Test RE 0.3341145238365933\n",
      "43 Train Loss 150682.53 Test MSE 35422.683371150306 Test RE 0.3336415197744404\n",
      "44 Train Loss 150209.31 Test MSE 35285.336265711594 Test RE 0.3329940645442813\n",
      "45 Train Loss 149646.72 Test MSE 34991.149521928666 Test RE 0.3316030124422692\n",
      "46 Train Loss 149089.14 Test MSE 34609.226455254226 Test RE 0.3297883489793385\n",
      "47 Train Loss 148677.66 Test MSE 34365.94714020031 Test RE 0.3286272110200388\n",
      "48 Train Loss 148346.48 Test MSE 34363.6493130055 Test RE 0.328616224255111\n",
      "49 Train Loss 147938.88 Test MSE 34296.8425844665 Test RE 0.3282966357385882\n",
      "50 Train Loss 147351.8 Test MSE 34106.310887491105 Test RE 0.32738346085225595\n",
      "51 Train Loss 147024.75 Test MSE 33919.790705479674 Test RE 0.3264870382228003\n",
      "52 Train Loss 146781.5 Test MSE 33781.88591311566 Test RE 0.3258226770603643\n",
      "53 Train Loss 146352.22 Test MSE 33604.525167932996 Test RE 0.3249662387583904\n",
      "54 Train Loss 145898.53 Test MSE 33610.647347457125 Test RE 0.32499583910295465\n",
      "55 Train Loss 145665.52 Test MSE 33465.18271205015 Test RE 0.3242917963789527\n",
      "56 Train Loss 145285.92 Test MSE 33328.623382623446 Test RE 0.3236294611260328\n",
      "57 Train Loss 144972.39 Test MSE 33061.14117533386 Test RE 0.3223281846473596\n",
      "58 Train Loss 144748.73 Test MSE 32925.374959802146 Test RE 0.3216656803394576\n",
      "59 Train Loss 144075.7 Test MSE 32690.081059642718 Test RE 0.32051426329182187\n",
      "60 Train Loss 143584.05 Test MSE 32415.430953994382 Test RE 0.31916500150223925\n",
      "61 Train Loss 143182.9 Test MSE 32149.66607846563 Test RE 0.3178539372904502\n",
      "62 Train Loss 142628.3 Test MSE 31839.690674232872 Test RE 0.31631790975514495\n",
      "63 Train Loss 141907.94 Test MSE 31636.66157670728 Test RE 0.3153077798628791\n",
      "64 Train Loss 141541.05 Test MSE 31451.580499619915 Test RE 0.314384118798928\n",
      "65 Train Loss 141015.45 Test MSE 31037.205039047312 Test RE 0.31230624226799125\n",
      "66 Train Loss 140738.2 Test MSE 30975.969913398938 Test RE 0.3119980064901597\n",
      "67 Train Loss 140038.64 Test MSE 30604.63161393326 Test RE 0.3101222601004946\n",
      "68 Train Loss 139484.75 Test MSE 30543.56771470268 Test RE 0.3098127198605585\n",
      "69 Train Loss 139083.12 Test MSE 30324.128832055445 Test RE 0.308697795974661\n",
      "70 Train Loss 138587.97 Test MSE 29888.0393027163 Test RE 0.30647007528452136\n",
      "71 Train Loss 137960.92 Test MSE 29771.128727113344 Test RE 0.30587009113778857\n",
      "72 Train Loss 137607.98 Test MSE 29563.297345856397 Test RE 0.3048005862309633\n",
      "73 Train Loss 137133.31 Test MSE 28998.9378114711 Test RE 0.3018772656066475\n",
      "74 Train Loss 136660.34 Test MSE 28861.207636520154 Test RE 0.3011595307955522\n",
      "75 Train Loss 135975.67 Test MSE 28708.379699240653 Test RE 0.3003611116541848\n",
      "76 Train Loss 135254.08 Test MSE 28593.044657997714 Test RE 0.2997571586431224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 Train Loss 134318.94 Test MSE 28358.44774123554 Test RE 0.29852491941329834\n",
      "78 Train Loss 133765.1 Test MSE 28055.89119598169 Test RE 0.29692816646529485\n",
      "79 Train Loss 132917.81 Test MSE 27784.406033375624 Test RE 0.2954880491037035\n",
      "80 Train Loss 132268.12 Test MSE 27473.770747589915 Test RE 0.29383159811316933\n",
      "81 Train Loss 131072.5 Test MSE 27140.828305835432 Test RE 0.29204576378333236\n",
      "82 Train Loss 130114.94 Test MSE 26834.927963107904 Test RE 0.2903952973220634\n",
      "83 Train Loss 129101.74 Test MSE 26398.685364988964 Test RE 0.2880252167599356\n",
      "84 Train Loss 126641.32 Test MSE 25643.156917648164 Test RE 0.283873666528389\n",
      "85 Train Loss 124047.84 Test MSE 25028.76016222598 Test RE 0.28045231583956515\n",
      "86 Train Loss 121902.016 Test MSE 24740.6601545458 Test RE 0.278833534601681\n",
      "87 Train Loss 120863.13 Test MSE 24378.920515496276 Test RE 0.2767875795689163\n",
      "88 Train Loss 119952.85 Test MSE 24129.96946280627 Test RE 0.27537071254454587\n",
      "89 Train Loss 117783.516 Test MSE 23595.824956025233 Test RE 0.27230583344213455\n",
      "90 Train Loss 116544.72 Test MSE 23134.619543826277 Test RE 0.26963144743371087\n",
      "91 Train Loss 114411.39 Test MSE 23077.795950487376 Test RE 0.2693001074814274\n",
      "92 Train Loss 112740.66 Test MSE 22548.621241068788 Test RE 0.2661946716233818\n",
      "93 Train Loss 111352.36 Test MSE 22718.441208732722 Test RE 0.2671951846005834\n",
      "94 Train Loss 109225.484 Test MSE 21573.920338948305 Test RE 0.26037776665407053\n",
      "95 Train Loss 107007.77 Test MSE 20621.121409320727 Test RE 0.2545631297502956\n",
      "96 Train Loss 103769.516 Test MSE 19821.07669169199 Test RE 0.24957609380728143\n",
      "97 Train Loss 100699.375 Test MSE 19628.142961923746 Test RE 0.24835846584044313\n",
      "98 Train Loss 98276.79 Test MSE 18980.14929114026 Test RE 0.24422446913402554\n",
      "99 Train Loss 95401.72 Test MSE 18681.743387907452 Test RE 0.2422970147346649\n",
      "Training time: 351.77\n",
      "3D_HTTP_tanh\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 648593.5 Test MSE 289272.82544675656 Test RE 0.9534395892328283\n",
      "1 Train Loss 510567.03 Test MSE 277552.3410060099 Test RE 0.9339245881689119\n",
      "2 Train Loss 474721.38 Test MSE 252526.27823778088 Test RE 0.8908255225095629\n",
      "3 Train Loss 434383.12 Test MSE 219789.97983181043 Test RE 0.8310809102730824\n",
      "4 Train Loss 390239.44 Test MSE 177003.772779888 Test RE 0.7458141587224884\n",
      "5 Train Loss 252070.03 Test MSE 91034.48129012718 Test RE 0.5348627133700928\n",
      "6 Train Loss 238866.66 Test MSE 83352.75156157807 Test RE 0.5117988817693355\n",
      "7 Train Loss 224633.55 Test MSE 74812.14052938369 Test RE 0.48487009883436244\n",
      "8 Train Loss 215549.0 Test MSE 68183.77200904962 Test RE 0.4628922118291163\n",
      "9 Train Loss 209615.4 Test MSE 66081.28066744667 Test RE 0.4556995374439039\n",
      "10 Train Loss 205016.98 Test MSE 63554.17769513253 Test RE 0.44690109025195895\n",
      "11 Train Loss 201932.06 Test MSE 62457.37829032642 Test RE 0.44302806350533913\n",
      "12 Train Loss 198956.77 Test MSE 60820.920568574904 Test RE 0.4371856081253199\n",
      "13 Train Loss 196558.14 Test MSE 59665.98063922658 Test RE 0.43301481324782537\n",
      "14 Train Loss 192430.55 Test MSE 56762.67609552883 Test RE 0.42234834170015173\n",
      "15 Train Loss 188356.4 Test MSE 54964.903531880285 Test RE 0.4156062760976202\n",
      "16 Train Loss 185831.36 Test MSE 53437.7143480126 Test RE 0.4097918334093071\n",
      "17 Train Loss 183663.27 Test MSE 51821.811372865785 Test RE 0.4035484251364758\n",
      "18 Train Loss 182195.94 Test MSE 51124.41320278962 Test RE 0.40082382705027175\n",
      "19 Train Loss 180832.62 Test MSE 50834.14056575498 Test RE 0.3996843146135736\n",
      "20 Train Loss 179502.89 Test MSE 50256.368609561294 Test RE 0.3974064526430476\n",
      "21 Train Loss 178622.81 Test MSE 49473.85452794932 Test RE 0.39430041673065075\n",
      "22 Train Loss 177768.42 Test MSE 49085.974434241754 Test RE 0.3927516973738604\n",
      "23 Train Loss 176156.98 Test MSE 48758.243530591775 Test RE 0.39143836458916753\n",
      "24 Train Loss 175195.84 Test MSE 47967.57062831855 Test RE 0.3882515730901526\n",
      "25 Train Loss 173271.53 Test MSE 47270.67092577125 Test RE 0.3854208859865854\n",
      "26 Train Loss 172512.36 Test MSE 46965.521754762056 Test RE 0.3841748566648736\n",
      "27 Train Loss 171492.25 Test MSE 46473.51752107837 Test RE 0.38215727776577646\n",
      "28 Train Loss 170275.02 Test MSE 45799.84796536989 Test RE 0.3793773340770391\n",
      "29 Train Loss 168830.22 Test MSE 45010.55402141457 Test RE 0.3760941182163799\n",
      "30 Train Loss 167650.64 Test MSE 44254.7874280712 Test RE 0.3729232768759305\n",
      "31 Train Loss 166028.75 Test MSE 43711.61203704427 Test RE 0.37062761399658173\n",
      "32 Train Loss 163985.1 Test MSE 42827.04879214049 Test RE 0.3668583735640676\n",
      "33 Train Loss 162551.55 Test MSE 41209.76427009125 Test RE 0.3598648488109548\n",
      "34 Train Loss 160990.86 Test MSE 40594.527929921416 Test RE 0.35716846712060046\n",
      "35 Train Loss 158913.19 Test MSE 39816.58020080684 Test RE 0.3537295489681849\n",
      "36 Train Loss 157028.98 Test MSE 38741.628475104306 Test RE 0.34892195584862906\n",
      "37 Train Loss 156033.5 Test MSE 38760.104948175984 Test RE 0.34900514903007757\n",
      "38 Train Loss 154606.67 Test MSE 39030.46965277426 Test RE 0.3502202476437823\n",
      "39 Train Loss 153705.05 Test MSE 38808.56094794106 Test RE 0.34922323553026463\n",
      "40 Train Loss 153101.72 Test MSE 38862.49572411896 Test RE 0.3494658203746422\n",
      "41 Train Loss 152023.61 Test MSE 38554.16093001123 Test RE 0.3480767298298986\n",
      "42 Train Loss 151393.28 Test MSE 38440.832614279694 Test RE 0.3475647749665929\n",
      "43 Train Loss 150097.19 Test MSE 38405.999403366106 Test RE 0.3474072661383515\n",
      "44 Train Loss 149280.62 Test MSE 38261.4266580566 Test RE 0.34675277232777413\n",
      "45 Train Loss 148498.39 Test MSE 38308.85476804931 Test RE 0.3469676197176565\n",
      "46 Train Loss 147712.58 Test MSE 38131.52580011969 Test RE 0.346163644017302\n",
      "47 Train Loss 146974.61 Test MSE 37962.432308640506 Test RE 0.34539526333718146\n",
      "48 Train Loss 145863.12 Test MSE 37399.541335603586 Test RE 0.3428250122872304\n",
      "49 Train Loss 144619.61 Test MSE 36704.246586571295 Test RE 0.33962333253737736\n",
      "50 Train Loss 144229.14 Test MSE 36512.35978656868 Test RE 0.3387344077018822\n",
      "51 Train Loss 143740.58 Test MSE 36267.60196279055 Test RE 0.33759715847588045\n",
      "52 Train Loss 143084.44 Test MSE 35789.81351669498 Test RE 0.33536603783479424\n",
      "53 Train Loss 142642.36 Test MSE 35791.29778958655 Test RE 0.3353729919022926\n",
      "54 Train Loss 141189.83 Test MSE 35268.564766366624 Test RE 0.332914917310412\n",
      "55 Train Loss 140653.16 Test MSE 35044.11606563523 Test RE 0.3318538933658221\n",
      "56 Train Loss 139605.14 Test MSE 33970.47426479734 Test RE 0.32673086856543904\n",
      "57 Train Loss 138038.64 Test MSE 33079.23721924574 Test RE 0.32241638588359234\n",
      "58 Train Loss 136870.55 Test MSE 33272.1511222291 Test RE 0.32335516483513177\n",
      "59 Train Loss 135501.12 Test MSE 32971.44587869244 Test RE 0.32189064745186774\n",
      "60 Train Loss 134307.2 Test MSE 32958.50658388536 Test RE 0.32182747996582833\n",
      "61 Train Loss 133852.56 Test MSE 32917.750692344045 Test RE 0.32162843540036573\n",
      "62 Train Loss 133476.8 Test MSE 32800.43194915495 Test RE 0.3210547825356796\n",
      "63 Train Loss 133040.1 Test MSE 32590.67790370138 Test RE 0.3200265865253441\n",
      "64 Train Loss 132201.27 Test MSE 32178.90242787132 Test RE 0.3179984298803574\n",
      "65 Train Loss 131797.64 Test MSE 31806.055435339895 Test RE 0.3161507875316559\n",
      "66 Train Loss 131030.71 Test MSE 31388.333522903526 Test RE 0.31406785724621383\n",
      "67 Train Loss 130541.73 Test MSE 31208.488098129274 Test RE 0.3131668089593661\n",
      "68 Train Loss 130036.22 Test MSE 31342.388495814106 Test RE 0.3138379128859478\n",
      "69 Train Loss 129718.02 Test MSE 31387.026139092668 Test RE 0.3140613164162028\n",
      "70 Train Loss 129494.414 Test MSE 31130.911360374055 Test RE 0.31277733838225924\n",
      "71 Train Loss 129136.35 Test MSE 31148.618802130473 Test RE 0.31286628051249726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 Train Loss 128383.86 Test MSE 30714.07018547214 Test RE 0.3106762457019279\n",
      "73 Train Loss 127584.02 Test MSE 30323.88350620295 Test RE 0.3086965472709711\n",
      "74 Train Loss 127156.79 Test MSE 30630.224356421837 Test RE 0.31025190094468325\n",
      "75 Train Loss 126446.78 Test MSE 30567.47440949841 Test RE 0.30993394259185036\n",
      "76 Train Loss 125442.39 Test MSE 29898.792619225114 Test RE 0.3065252022420726\n",
      "77 Train Loss 125015.91 Test MSE 29684.362815073375 Test RE 0.305424047212306\n",
      "78 Train Loss 123951.69 Test MSE 29198.048094983376 Test RE 0.3029118560067036\n",
      "79 Train Loss 123249.09 Test MSE 29007.32858776288 Test RE 0.3019209361958588\n",
      "80 Train Loss 122788.164 Test MSE 28631.541288444412 Test RE 0.2999588818099443\n",
      "81 Train Loss 121254.01 Test MSE 28334.566825128622 Test RE 0.298399197615804\n",
      "82 Train Loss 120739.38 Test MSE 28128.21925660605 Test RE 0.29731066037107806\n",
      "83 Train Loss 120202.25 Test MSE 28030.438340944682 Test RE 0.2967934463735472\n",
      "84 Train Loss 119377.6 Test MSE 27679.59630278514 Test RE 0.2949301950925687\n",
      "85 Train Loss 116986.93 Test MSE 27119.19018594885 Test RE 0.29192932331944077\n",
      "86 Train Loss 116343.95 Test MSE 26579.856056143268 Test RE 0.2890118663543288\n",
      "87 Train Loss 115751.33 Test MSE 26705.62096371889 Test RE 0.28969480189394187\n",
      "88 Train Loss 114976.26 Test MSE 26415.36687153773 Test RE 0.2881162049224748\n",
      "89 Train Loss 113160.125 Test MSE 25875.001529571055 Test RE 0.28515405662365245\n",
      "90 Train Loss 111960.12 Test MSE 25767.706910031324 Test RE 0.28456222517722307\n",
      "91 Train Loss 111378.3 Test MSE 25701.16957260666 Test RE 0.2841945896031115\n",
      "92 Train Loss 110795.39 Test MSE 25498.21122314747 Test RE 0.2830702441045923\n",
      "93 Train Loss 110382.67 Test MSE 25413.69252194328 Test RE 0.2826007094405381\n",
      "94 Train Loss 109351.49 Test MSE 24899.95028630781 Test RE 0.2797297145822197\n",
      "95 Train Loss 108242.13 Test MSE 24776.533262429966 Test RE 0.279035610903737\n",
      "96 Train Loss 107759.66 Test MSE 24520.41794468025 Test RE 0.27758966730574675\n",
      "97 Train Loss 107379.734 Test MSE 24265.198021096676 Test RE 0.2761412472555499\n",
      "98 Train Loss 107103.586 Test MSE 24181.24458126055 Test RE 0.27566313257783326\n",
      "99 Train Loss 105651.06 Test MSE 23717.941608779343 Test RE 0.27300956310326885\n",
      "Training time: 357.00\n",
      "3D_HTTP_tanh\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 691679.5 Test MSE 286797.81524031353 Test RE 0.949352025947671\n",
      "1 Train Loss 459685.06 Test MSE 229697.5931930173 Test RE 0.8496060268638584\n",
      "2 Train Loss 347933.06 Test MSE 142637.93605727685 Test RE 0.6695097180162098\n",
      "3 Train Loss 245906.52 Test MSE 85210.88180719047 Test RE 0.517472043595199\n",
      "4 Train Loss 234736.95 Test MSE 77478.69193731368 Test RE 0.49343562587218753\n",
      "5 Train Loss 227202.81 Test MSE 73958.91991922053 Test RE 0.4820972368208651\n",
      "6 Train Loss 220285.23 Test MSE 68114.85213356087 Test RE 0.4626582079124741\n",
      "7 Train Loss 213246.39 Test MSE 65031.586638396584 Test RE 0.4520656797708279\n",
      "8 Train Loss 206690.0 Test MSE 61475.062877808254 Test RE 0.43953033391566787\n",
      "9 Train Loss 200910.06 Test MSE 58915.80343390768 Test RE 0.43028406659077095\n",
      "10 Train Loss 196558.6 Test MSE 56464.02773499742 Test RE 0.4212358149947716\n",
      "11 Train Loss 193516.27 Test MSE 55102.43121430712 Test RE 0.4161258954872743\n",
      "12 Train Loss 190214.97 Test MSE 53602.57282549603 Test RE 0.41042346253071343\n",
      "13 Train Loss 187808.22 Test MSE 52378.78573521719 Test RE 0.4057112732225739\n",
      "14 Train Loss 184850.33 Test MSE 50164.98478759247 Test RE 0.3970449756241775\n",
      "15 Train Loss 180078.77 Test MSE 48144.28375943593 Test RE 0.3889660774639278\n",
      "16 Train Loss 176358.92 Test MSE 47312.371518518936 Test RE 0.3855908511684316\n",
      "17 Train Loss 172477.5 Test MSE 45065.41192623999 Test RE 0.376323236178144\n",
      "18 Train Loss 169517.33 Test MSE 43835.49931292351 Test RE 0.37115245803206065\n",
      "19 Train Loss 168148.92 Test MSE 43530.1818304747 Test RE 0.369857647371453\n",
      "20 Train Loss 165609.66 Test MSE 42816.90242018383 Test RE 0.36681491385681686\n",
      "21 Train Loss 164195.64 Test MSE 42089.73252343414 Test RE 0.363686721694876\n",
      "22 Train Loss 162664.28 Test MSE 41623.381912568104 Test RE 0.36166630080909984\n",
      "23 Train Loss 161665.5 Test MSE 41343.55942438864 Test RE 0.36044855945139226\n",
      "24 Train Loss 160223.16 Test MSE 41135.17416660355 Test RE 0.35953902174022084\n",
      "25 Train Loss 159333.34 Test MSE 40610.80385296575 Test RE 0.3572400613010088\n",
      "26 Train Loss 158761.69 Test MSE 40342.73543450119 Test RE 0.3560590537038537\n",
      "27 Train Loss 158127.64 Test MSE 40094.44914400194 Test RE 0.3549616935387217\n",
      "28 Train Loss 157833.44 Test MSE 40018.3351290151 Test RE 0.35462460954404174\n",
      "29 Train Loss 157353.31 Test MSE 39688.13005840289 Test RE 0.35315851404890136\n",
      "30 Train Loss 156682.27 Test MSE 39581.67482116642 Test RE 0.3526845585135322\n",
      "31 Train Loss 155985.61 Test MSE 39410.18438817097 Test RE 0.35191971365918706\n",
      "32 Train Loss 155719.69 Test MSE 39304.40873202054 Test RE 0.3514471257971678\n",
      "33 Train Loss 155219.25 Test MSE 39341.86833303849 Test RE 0.35161456164269694\n",
      "34 Train Loss 154365.53 Test MSE 38951.07221496542 Test RE 0.34986384985613717\n",
      "35 Train Loss 154021.36 Test MSE 38889.853786919135 Test RE 0.34958880560319455\n",
      "36 Train Loss 153821.5 Test MSE 38626.67765617434 Test RE 0.3484039257314274\n",
      "37 Train Loss 153341.89 Test MSE 38246.1461732205 Test RE 0.34668352400029223\n",
      "38 Train Loss 152939.12 Test MSE 37966.3917332417 Test RE 0.3454132749708243\n",
      "39 Train Loss 152703.92 Test MSE 37921.9605352994 Test RE 0.3452111011847748\n",
      "40 Train Loss 152463.28 Test MSE 37812.57803360476 Test RE 0.34471287638914694\n",
      "41 Train Loss 152014.39 Test MSE 37716.178156853115 Test RE 0.34427318823517306\n",
      "42 Train Loss 151916.94 Test MSE 37803.449121149606 Test RE 0.34467126267500975\n",
      "43 Train Loss 151613.75 Test MSE 37429.11563572182 Test RE 0.34296053274916694\n",
      "44 Train Loss 151510.64 Test MSE 37437.110917148806 Test RE 0.3429971609129998\n",
      "45 Train Loss 151154.11 Test MSE 37354.03616648245 Test RE 0.3426163859533388\n",
      "46 Train Loss 150917.4 Test MSE 37296.694654332794 Test RE 0.34235331281483217\n",
      "47 Train Loss 150734.98 Test MSE 37181.73385431985 Test RE 0.3418252823027081\n",
      "48 Train Loss 150440.84 Test MSE 37176.895976774504 Test RE 0.34180304339017537\n",
      "49 Train Loss 150151.25 Test MSE 37115.10007223621 Test RE 0.34151885058288406\n",
      "50 Train Loss 150028.03 Test MSE 36871.806562481834 Test RE 0.340397663617931\n",
      "51 Train Loss 149878.5 Test MSE 36774.06235720994 Test RE 0.33994618074021565\n",
      "52 Train Loss 149343.84 Test MSE 36722.790302113564 Test RE 0.3397091139360329\n",
      "53 Train Loss 148812.06 Test MSE 36857.589756637775 Test RE 0.34033203306551124\n",
      "54 Train Loss 148621.86 Test MSE 37000.25482448638 Test RE 0.3409900603468547\n",
      "55 Train Loss 148316.45 Test MSE 36761.598332454334 Test RE 0.3398885659918448\n",
      "56 Train Loss 148116.11 Test MSE 36605.032603853426 Test RE 0.339164009875634\n",
      "57 Train Loss 147695.67 Test MSE 36528.1212754494 Test RE 0.3388075114752581\n",
      "58 Train Loss 146661.45 Test MSE 36238.506296060965 Test RE 0.3374617127237801\n",
      "59 Train Loss 146234.11 Test MSE 36109.68577298605 Test RE 0.33686137483538686\n",
      "60 Train Loss 145733.48 Test MSE 35749.08529339099 Test RE 0.3351751629790971\n",
      "61 Train Loss 145118.23 Test MSE 35484.8940734052 Test RE 0.333934368397255\n",
      "62 Train Loss 144668.42 Test MSE 35296.3159117595 Test RE 0.3330458689618765\n",
      "63 Train Loss 144126.95 Test MSE 34943.48626221076 Test RE 0.331377088648849\n",
      "64 Train Loss 143061.27 Test MSE 34719.09449526163 Test RE 0.3303113957638614\n",
      "65 Train Loss 141842.31 Test MSE 34005.4340600612 Test RE 0.32689894845326584\n",
      "66 Train Loss 140996.3 Test MSE 33798.329798460494 Test RE 0.325901967170435\n",
      "67 Train Loss 140011.5 Test MSE 33496.50859888356 Test RE 0.324443541432663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 Train Loss 139300.94 Test MSE 33593.16113035229 Test RE 0.3249112872184811\n",
      "69 Train Loss 138418.69 Test MSE 33090.04657153974 Test RE 0.32246905982548124\n",
      "70 Train Loss 137759.88 Test MSE 32890.36321311474 Test RE 0.321494610566461\n",
      "71 Train Loss 137213.7 Test MSE 32777.656652336686 Test RE 0.3209432994349492\n",
      "72 Train Loss 136625.27 Test MSE 32501.222600566598 Test RE 0.3195870782236806\n",
      "73 Train Loss 135939.73 Test MSE 32676.548086117917 Test RE 0.3204479135016059\n",
      "74 Train Loss 135412.94 Test MSE 32554.35839926349 Test RE 0.31984821577872435\n",
      "75 Train Loss 134879.53 Test MSE 32388.645479550818 Test RE 0.31903310825701625\n",
      "76 Train Loss 134128.17 Test MSE 32040.06572338141 Test RE 0.31731168211810223\n",
      "77 Train Loss 133671.66 Test MSE 31860.008496255497 Test RE 0.3164188194356086\n",
      "78 Train Loss 133222.5 Test MSE 31773.196546734926 Test RE 0.3159874373800358\n",
      "79 Train Loss 133016.55 Test MSE 31813.868182497317 Test RE 0.316189614328474\n",
      "80 Train Loss 132510.38 Test MSE 31626.557098748264 Test RE 0.3152574225433855\n",
      "81 Train Loss 132110.19 Test MSE 31473.113473257657 Test RE 0.3144917201745049\n",
      "82 Train Loss 131815.34 Test MSE 31489.966479415893 Test RE 0.31457590983644945\n",
      "83 Train Loss 131575.23 Test MSE 31499.21527729957 Test RE 0.3146221028899039\n",
      "84 Train Loss 131221.73 Test MSE 31433.35591982406 Test RE 0.31429302085409117\n",
      "85 Train Loss 130635.805 Test MSE 31199.600865302225 Test RE 0.3131222155701221\n",
      "86 Train Loss 130354.305 Test MSE 31087.898713429502 Test RE 0.3125611861578867\n",
      "87 Train Loss 129847.03 Test MSE 31020.209738000132 Test RE 0.3122207244934486\n",
      "88 Train Loss 129409.984 Test MSE 30792.801125421734 Test RE 0.31107417697529294\n",
      "89 Train Loss 129263.02 Test MSE 30781.693129883406 Test RE 0.3110180644730123\n",
      "90 Train Loss 129122.91 Test MSE 30753.0522056706 Test RE 0.31087333692629365\n",
      "91 Train Loss 128732.24 Test MSE 30815.913347580787 Test RE 0.31119089690086077\n",
      "92 Train Loss 128548.55 Test MSE 30823.484816235497 Test RE 0.3112291243482331\n",
      "93 Train Loss 128336.28 Test MSE 30810.915653155742 Test RE 0.31116566156133285\n",
      "94 Train Loss 127983.29 Test MSE 30605.73707669726 Test RE 0.3101278609768489\n",
      "95 Train Loss 127839.81 Test MSE 30442.46216092884 Test RE 0.3092995225819486\n",
      "96 Train Loss 127424.96 Test MSE 30233.67740346622 Test RE 0.3082370571087734\n",
      "97 Train Loss 127278.57 Test MSE 30228.731817002445 Test RE 0.30821184556499703\n",
      "98 Train Loss 126901.28 Test MSE 30165.3123238383 Test RE 0.3078883635534193\n",
      "99 Train Loss 126734.0 Test MSE 30142.19782435592 Test RE 0.3077703795396023\n",
      "Training time: 351.60\n",
      "3D_HTTP_tanh\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 638041.94 Test MSE 307940.82736847486 Test RE 0.9837233913892885\n",
      "1 Train Loss 543850.25 Test MSE 294594.94429554406 Test RE 0.9621704325113887\n",
      "2 Train Loss 480547.34 Test MSE 240329.78135887903 Test RE 0.8690467862523849\n",
      "3 Train Loss 422524.78 Test MSE 196042.02647225367 Test RE 0.7848993216974173\n",
      "4 Train Loss 264365.9 Test MSE 86054.17510453486 Test RE 0.5200263320913896\n",
      "5 Train Loss 247201.89 Test MSE 85919.81686073478 Test RE 0.5196202093306239\n",
      "6 Train Loss 243097.38 Test MSE 83249.60826561156 Test RE 0.5114821257940814\n",
      "7 Train Loss 235444.72 Test MSE 76681.72663304057 Test RE 0.49089126452279674\n",
      "8 Train Loss 228092.67 Test MSE 71425.76799227209 Test RE 0.4737691978186156\n",
      "9 Train Loss 221912.77 Test MSE 67780.25336880545 Test RE 0.46152045719187534\n",
      "10 Train Loss 217572.16 Test MSE 65470.96283546676 Test RE 0.45359026605129255\n",
      "11 Train Loss 214115.42 Test MSE 64055.04963190661 Test RE 0.44865865308727043\n",
      "12 Train Loss 212079.84 Test MSE 62647.1298425558 Test RE 0.4437005341744508\n",
      "13 Train Loss 209571.77 Test MSE 61049.04697253768 Test RE 0.43800473610629614\n",
      "14 Train Loss 208369.12 Test MSE 60127.63790945663 Test RE 0.4346867814283871\n",
      "15 Train Loss 206348.08 Test MSE 58979.21633046846 Test RE 0.43051556832114674\n",
      "16 Train Loss 204491.23 Test MSE 57725.7660272906 Test RE 0.42591625445219145\n",
      "17 Train Loss 202243.89 Test MSE 56113.38142237804 Test RE 0.4199258232994745\n",
      "18 Train Loss 199964.67 Test MSE 56471.087959568926 Test RE 0.4212621496950151\n",
      "19 Train Loss 199194.48 Test MSE 55782.1988498838 Test RE 0.41868478310146834\n",
      "20 Train Loss 197689.83 Test MSE 55244.293509336945 Test RE 0.41666121332130196\n",
      "21 Train Loss 196146.5 Test MSE 54674.02528429355 Test RE 0.4145051080534487\n",
      "22 Train Loss 195104.89 Test MSE 54104.58226525012 Test RE 0.4123408732454336\n",
      "23 Train Loss 194179.08 Test MSE 53245.63983424561 Test RE 0.40905470024330215\n",
      "24 Train Loss 193245.97 Test MSE 53144.14160586511 Test RE 0.40866463889531973\n",
      "25 Train Loss 192666.14 Test MSE 52675.64536260963 Test RE 0.40685934422605147\n",
      "26 Train Loss 191989.89 Test MSE 52635.42055311255 Test RE 0.4067039691465036\n",
      "27 Train Loss 191404.0 Test MSE 52195.26434873842 Test RE 0.4049998971571983\n",
      "28 Train Loss 190639.5 Test MSE 51887.569864336096 Test RE 0.4038043822514489\n",
      "29 Train Loss 190220.05 Test MSE 51625.88317020301 Test RE 0.4027848336011197\n",
      "30 Train Loss 188970.1 Test MSE 51450.23308382631 Test RE 0.4020990393086372\n",
      "31 Train Loss 188530.27 Test MSE 51170.16657778897 Test RE 0.40100314394077474\n",
      "32 Train Loss 187570.67 Test MSE 50531.432843577306 Test RE 0.3984925153633424\n",
      "33 Train Loss 187001.36 Test MSE 50397.30849283502 Test RE 0.3979633094793207\n",
      "34 Train Loss 186007.6 Test MSE 49888.92567937686 Test RE 0.39595099450968524\n",
      "35 Train Loss 185360.4 Test MSE 49365.33922853577 Test RE 0.39386775268607804\n",
      "36 Train Loss 185030.66 Test MSE 49340.66410769996 Test RE 0.39376930355844475\n",
      "37 Train Loss 184730.67 Test MSE 49399.41510292781 Test RE 0.394003668623754\n",
      "38 Train Loss 184129.44 Test MSE 49104.54943231403 Test RE 0.39282600242803156\n",
      "39 Train Loss 183783.72 Test MSE 48849.88288536588 Test RE 0.3918060390350904\n",
      "40 Train Loss 183114.03 Test MSE 48297.59599499678 Test RE 0.38958490336758556\n",
      "41 Train Loss 182840.78 Test MSE 47994.09241590961 Test RE 0.3883588925005782\n",
      "42 Train Loss 182258.69 Test MSE 47673.920712605155 Test RE 0.3870613411871667\n",
      "43 Train Loss 181529.98 Test MSE 47369.09907387592 Test RE 0.385821943723705\n",
      "44 Train Loss 181084.78 Test MSE 46837.322138704585 Test RE 0.3836501661810469\n",
      "45 Train Loss 180506.4 Test MSE 46782.247957976106 Test RE 0.3834245402001604\n",
      "46 Train Loss 180062.28 Test MSE 46355.40383542865 Test RE 0.38167133734908043\n",
      "47 Train Loss 179575.53 Test MSE 46628.25397395285 Test RE 0.3827929572261543\n",
      "48 Train Loss 178923.19 Test MSE 46190.49605246986 Test RE 0.380991841045729\n",
      "49 Train Loss 178436.28 Test MSE 45874.5869928318 Test RE 0.37968675360685356\n",
      "50 Train Loss 178100.03 Test MSE 45656.38908596534 Test RE 0.37878270619709054\n",
      "51 Train Loss 177952.33 Test MSE 45405.052305400684 Test RE 0.37773867478518325\n",
      "52 Train Loss 177492.0 Test MSE 45261.47818999125 Test RE 0.3771409831867182\n",
      "53 Train Loss 177275.64 Test MSE 45179.761135584435 Test RE 0.3768003759587626\n",
      "54 Train Loss 176909.97 Test MSE 45264.989253651685 Test RE 0.37715561086101235\n",
      "55 Train Loss 176715.44 Test MSE 45366.7662753393 Test RE 0.37757938453594425\n",
      "56 Train Loss 176583.77 Test MSE 45167.45403568956 Test RE 0.3767490516976476\n",
      "57 Train Loss 176131.31 Test MSE 44855.385715835924 Test RE 0.37544528961820056\n",
      "58 Train Loss 176036.5 Test MSE 44770.77609685159 Test RE 0.37509102583768994\n",
      "59 Train Loss 175745.66 Test MSE 44603.63990369096 Test RE 0.37439023494083373\n",
      "60 Train Loss 175451.2 Test MSE 44725.50119226714 Test RE 0.37490132054981207\n",
      "61 Train Loss 175290.83 Test MSE 44779.46525948824 Test RE 0.37512742311609243\n",
      "62 Train Loss 175100.77 Test MSE 44613.86488485615 Test RE 0.3744331452699333\n",
      "63 Train Loss 174591.02 Test MSE 44598.417588164666 Test RE 0.3743683169855508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 Train Loss 174267.44 Test MSE 44360.21255086381 Test RE 0.3733672073267263\n",
      "65 Train Loss 174152.86 Test MSE 44189.23836557784 Test RE 0.37264699230814624\n",
      "66 Train Loss 173922.81 Test MSE 44038.61850165956 Test RE 0.37201136300779425\n",
      "67 Train Loss 173815.47 Test MSE 43804.177413771344 Test RE 0.371019834046575\n",
      "68 Train Loss 173289.36 Test MSE 43665.50327678102 Test RE 0.3704320859896004\n",
      "69 Train Loss 173022.2 Test MSE 43633.645525822416 Test RE 0.3702969302541974\n",
      "70 Train Loss 172937.48 Test MSE 43530.5729717848 Test RE 0.3698593090493244\n",
      "71 Train Loss 172621.67 Test MSE 43402.24397138027 Test RE 0.3693137303353113\n",
      "72 Train Loss 172114.12 Test MSE 43031.352982996585 Test RE 0.3677323717819676\n",
      "73 Train Loss 171874.83 Test MSE 43052.729591820826 Test RE 0.36782369931995335\n",
      "74 Train Loss 171783.14 Test MSE 43013.34470029016 Test RE 0.3676554171775352\n",
      "75 Train Loss 171619.25 Test MSE 42781.255703160576 Test RE 0.3666621883051776\n",
      "76 Train Loss 171387.19 Test MSE 42754.17143757578 Test RE 0.36654610535968835\n",
      "77 Train Loss 171140.47 Test MSE 42586.336881301555 Test RE 0.36582594643625255\n",
      "78 Train Loss 170923.14 Test MSE 42767.52515201587 Test RE 0.3666033438666226\n",
      "79 Train Loss 170740.12 Test MSE 42692.171656736755 Test RE 0.3662802362954858\n",
      "80 Train Loss 170586.08 Test MSE 42631.61667226986 Test RE 0.36602037642123203\n",
      "81 Train Loss 170077.58 Test MSE 42182.71880824378 Test RE 0.364088235528746\n",
      "82 Train Loss 169789.72 Test MSE 42216.26231439267 Test RE 0.3642329674164367\n",
      "83 Train Loss 169571.1 Test MSE 41980.83463772706 Test RE 0.3632159374562707\n",
      "84 Train Loss 169169.58 Test MSE 41742.775968959 Test RE 0.3621846379035947\n",
      "85 Train Loss 168959.56 Test MSE 41723.86787161784 Test RE 0.3621025997795093\n",
      "86 Train Loss 168639.16 Test MSE 41663.39337045486 Test RE 0.3618400891868224\n",
      "87 Train Loss 168376.2 Test MSE 41535.08585206087 Test RE 0.3612824941430453\n",
      "88 Train Loss 168184.47 Test MSE 41610.15154892744 Test RE 0.3616088168090771\n",
      "89 Train Loss 167955.62 Test MSE 41439.419214726295 Test RE 0.3608661882006409\n",
      "90 Train Loss 167211.36 Test MSE 41113.93850183274 Test RE 0.35944620536192196\n",
      "91 Train Loss 166940.55 Test MSE 40932.2513373728 Test RE 0.3586511091783615\n",
      "92 Train Loss 166714.3 Test MSE 40916.761558060185 Test RE 0.35858324151866605\n",
      "93 Train Loss 166453.03 Test MSE 40989.12369511741 Test RE 0.35890018238211674\n",
      "94 Train Loss 166253.9 Test MSE 40973.011107474405 Test RE 0.3588296346546778\n",
      "95 Train Loss 165837.28 Test MSE 40782.70450955356 Test RE 0.35799534003686945\n",
      "96 Train Loss 165594.12 Test MSE 40544.27203616947 Test RE 0.3569473119508382\n",
      "97 Train Loss 165384.28 Test MSE 40503.94509761477 Test RE 0.35676975082157997\n",
      "98 Train Loss 165121.33 Test MSE 40497.789038227274 Test RE 0.3567426376696798\n",
      "99 Train Loss 164908.55 Test MSE 40624.29313958799 Test RE 0.35729938681326767\n",
      "Training time: 350.87\n",
      "3D_HTTP_tanh\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 622613.25 Test MSE 306403.4285250093 Test RE 0.9812646924527542\n",
      "1 Train Loss 489392.03 Test MSE 249471.50828908573 Test RE 0.885421041649671\n",
      "2 Train Loss 393703.72 Test MSE 181626.91816935875 Test RE 0.755491302894408\n",
      "3 Train Loss 301746.4 Test MSE 117239.39543934251 Test RE 0.606982468261759\n",
      "4 Train Loss 240326.73 Test MSE 83916.49887053404 Test RE 0.5135267133492594\n",
      "5 Train Loss 234235.9 Test MSE 79194.66241268387 Test RE 0.4988699197061072\n",
      "6 Train Loss 220862.6 Test MSE 69813.28142694033 Test RE 0.46839083429941086\n",
      "7 Train Loss 209788.61 Test MSE 62751.94779005295 Test RE 0.4440715674689145\n",
      "8 Train Loss 201057.64 Test MSE 57738.78962343521 Test RE 0.4259642975431876\n",
      "9 Train Loss 195306.66 Test MSE 55745.96836061483 Test RE 0.4185487933167126\n",
      "10 Train Loss 191327.8 Test MSE 53728.49588487065 Test RE 0.4109052627339062\n",
      "11 Train Loss 188150.4 Test MSE 52827.773803870485 Test RE 0.4074464300926834\n",
      "12 Train Loss 185275.22 Test MSE 50688.02209688216 Test RE 0.3991094717300561\n",
      "13 Train Loss 182649.98 Test MSE 49391.06815694915 Test RE 0.39397038010945556\n",
      "14 Train Loss 179984.1 Test MSE 48324.00769852118 Test RE 0.3896914117186962\n",
      "15 Train Loss 177239.33 Test MSE 47073.689781722795 Test RE 0.3846170058761539\n",
      "16 Train Loss 175333.39 Test MSE 46410.16685353557 Test RE 0.3818967188921593\n",
      "17 Train Loss 173653.77 Test MSE 45636.421270369174 Test RE 0.37869986685597545\n",
      "18 Train Loss 172173.9 Test MSE 44886.830991006624 Test RE 0.3755768670405494\n",
      "19 Train Loss 170810.34 Test MSE 44331.87316146644 Test RE 0.37324792601000156\n",
      "20 Train Loss 169717.83 Test MSE 43827.17151285843 Test RE 0.371117200873743\n",
      "21 Train Loss 168720.61 Test MSE 43547.32093352736 Test RE 0.3699304520810162\n",
      "22 Train Loss 167819.95 Test MSE 43218.02277610104 Test RE 0.36852911923629234\n",
      "23 Train Loss 166950.16 Test MSE 42881.787545398605 Test RE 0.3670927459756504\n",
      "24 Train Loss 166162.95 Test MSE 42396.313879553854 Test RE 0.3650088643796384\n",
      "25 Train Loss 165543.12 Test MSE 41745.27321108963 Test RE 0.3621954715052058\n",
      "26 Train Loss 163621.67 Test MSE 40845.753172506884 Test RE 0.3582719574288924\n",
      "27 Train Loss 162757.52 Test MSE 40290.2854630637 Test RE 0.355827520555147\n",
      "28 Train Loss 161920.72 Test MSE 39808.797138839334 Test RE 0.35369497501007896\n",
      "29 Train Loss 161332.44 Test MSE 39751.648476182425 Test RE 0.35344100533649475\n",
      "30 Train Loss 160634.55 Test MSE 39630.032884500266 Test RE 0.35289993515291784\n",
      "31 Train Loss 159968.72 Test MSE 39222.68315486154 Test RE 0.3510815540180798\n",
      "32 Train Loss 159353.12 Test MSE 38556.09779830226 Test RE 0.34808547298884085\n",
      "33 Train Loss 158903.98 Test MSE 38089.99403113719 Test RE 0.34597507687998025\n",
      "34 Train Loss 158240.48 Test MSE 38107.32343057125 Test RE 0.3460537702307834\n",
      "35 Train Loss 157576.17 Test MSE 37827.017478571724 Test RE 0.3447786876584799\n",
      "36 Train Loss 157186.39 Test MSE 37462.111236517376 Test RE 0.34311166771174734\n",
      "37 Train Loss 156601.97 Test MSE 37113.012895474094 Test RE 0.3415092477496853\n",
      "38 Train Loss 156217.61 Test MSE 37132.4766904528 Test RE 0.3415987877005489\n",
      "39 Train Loss 155425.23 Test MSE 37041.69422575264 Test RE 0.34118095727155484\n",
      "40 Train Loss 155168.42 Test MSE 36894.001175337646 Test RE 0.3405000976792654\n",
      "41 Train Loss 154830.86 Test MSE 36931.01211778517 Test RE 0.34067084455081553\n",
      "42 Train Loss 154474.89 Test MSE 36769.12642229033 Test RE 0.3399233656305218\n",
      "43 Train Loss 154126.8 Test MSE 36505.728161758765 Test RE 0.3387036446736325\n",
      "44 Train Loss 153764.39 Test MSE 36372.69760832692 Test RE 0.3380859463089098\n",
      "45 Train Loss 153516.78 Test MSE 36216.8115146031 Test RE 0.3373606840816315\n",
      "46 Train Loss 153300.83 Test MSE 36227.34633531614 Test RE 0.3374097465992882\n",
      "47 Train Loss 152534.45 Test MSE 35764.544938550745 Test RE 0.33524762817591175\n",
      "48 Train Loss 152240.11 Test MSE 35866.0796486836 Test RE 0.33572317100564325\n",
      "49 Train Loss 151955.33 Test MSE 35764.608803581796 Test RE 0.33524792750294996\n",
      "50 Train Loss 151657.72 Test MSE 35694.149651175656 Test RE 0.3349175320220603\n",
      "51 Train Loss 151254.14 Test MSE 35542.81798609617 Test RE 0.33420680683298504\n",
      "52 Train Loss 150842.48 Test MSE 35289.66289426826 Test RE 0.33301447950441365\n",
      "53 Train Loss 150636.83 Test MSE 35000.06278410642 Test RE 0.3316452442127279\n",
      "54 Train Loss 150220.42 Test MSE 34824.52182210439 Test RE 0.33081252420189483\n",
      "55 Train Loss 149623.1 Test MSE 34679.016959437504 Test RE 0.3301206953799153\n",
      "56 Train Loss 149168.05 Test MSE 34661.51807280399 Test RE 0.33003739611072347\n",
      "57 Train Loss 148769.81 Test MSE 34639.37615762119 Test RE 0.32993196467188773\n",
      "58 Train Loss 148537.28 Test MSE 34522.664832062874 Test RE 0.32937567163986586\n",
      "59 Train Loss 147862.84 Test MSE 34261.65247243155 Test RE 0.32812816891386754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 Train Loss 146948.5 Test MSE 33955.13516439677 Test RE 0.32665709385978553\n",
      "61 Train Loss 146624.77 Test MSE 33837.57571218926 Test RE 0.32609112754840097\n",
      "62 Train Loss 146438.12 Test MSE 33955.184356931306 Test RE 0.32665733048208845\n",
      "63 Train Loss 146245.45 Test MSE 33895.76233803234 Test RE 0.32637137801135396\n",
      "64 Train Loss 146016.16 Test MSE 33918.74117552899 Test RE 0.3264819871807887\n",
      "65 Train Loss 145617.58 Test MSE 33781.52750540473 Test RE 0.32582094865375766\n",
      "66 Train Loss 145249.5 Test MSE 33651.67687356894 Test RE 0.32519414469013286\n",
      "67 Train Loss 144785.81 Test MSE 33338.3757636363 Test RE 0.3236768067203347\n",
      "68 Train Loss 144438.2 Test MSE 33222.13187440607 Test RE 0.32311201756499786\n",
      "69 Train Loss 143927.86 Test MSE 32980.96397869982 Test RE 0.3219371053218955\n",
      "70 Train Loss 143571.95 Test MSE 32979.14601614944 Test RE 0.32192823236010304\n",
      "71 Train Loss 142604.98 Test MSE 32521.021777236074 Test RE 0.3196844068339656\n",
      "72 Train Loss 142125.6 Test MSE 32333.49191740533 Test RE 0.3187613570102479\n",
      "73 Train Loss 141642.52 Test MSE 32074.868307574365 Test RE 0.31748397061149886\n",
      "74 Train Loss 141185.77 Test MSE 31721.610093647658 Test RE 0.3157308171050769\n",
      "75 Train Loss 140410.19 Test MSE 31696.491009199523 Test RE 0.3156057849946104\n",
      "76 Train Loss 139393.83 Test MSE 31573.419992066614 Test RE 0.31499247259107577\n",
      "77 Train Loss 138917.62 Test MSE 31365.37259889739 Test RE 0.31395296412095114\n",
      "78 Train Loss 138023.72 Test MSE 30946.073132944228 Test RE 0.3118474060778785\n",
      "79 Train Loss 137332.61 Test MSE 30759.98291020607 Test RE 0.31090836515460535\n",
      "80 Train Loss 136236.11 Test MSE 30128.13124854122 Test RE 0.3076985569636518\n",
      "81 Train Loss 134904.66 Test MSE 29492.229731628668 Test RE 0.30443400864728015\n",
      "82 Train Loss 134176.05 Test MSE 29295.599897662214 Test RE 0.30341745413897586\n",
      "83 Train Loss 133065.98 Test MSE 28802.294414263066 Test RE 0.3008520013602164\n",
      "84 Train Loss 131929.5 Test MSE 28730.84572832435 Test RE 0.3004786139635051\n",
      "85 Train Loss 131157.88 Test MSE 28497.807460007985 Test RE 0.2992575293854164\n",
      "86 Train Loss 130173.664 Test MSE 27944.197954472464 Test RE 0.2963365270268987\n",
      "87 Train Loss 128601.11 Test MSE 27370.252962318686 Test RE 0.2932775150506307\n",
      "88 Train Loss 127261.555 Test MSE 26792.380620524087 Test RE 0.2901649920721545\n",
      "89 Train Loss 125902.445 Test MSE 26349.11209422036 Test RE 0.28775465287804547\n",
      "90 Train Loss 124738.08 Test MSE 26231.127374416406 Test RE 0.2871096834588303\n",
      "91 Train Loss 123474.83 Test MSE 26042.331102842938 Test RE 0.28607459404087865\n",
      "92 Train Loss 122144.44 Test MSE 25933.221970827468 Test RE 0.2854746839808139\n",
      "93 Train Loss 121022.28 Test MSE 26013.98521100335 Test RE 0.2859188620664543\n",
      "94 Train Loss 119369.31 Test MSE 25643.92175743842 Test RE 0.2838778999435129\n",
      "95 Train Loss 118611.36 Test MSE 25823.291449309574 Test RE 0.2848689800509916\n",
      "96 Train Loss 117080.14 Test MSE 25260.367376643553 Test RE 0.2817469305828604\n",
      "97 Train Loss 115454.26 Test MSE 24991.4111960797 Test RE 0.2802429863609723\n",
      "98 Train Loss 114051.78 Test MSE 24673.252600750813 Test RE 0.2784534253762068\n",
      "99 Train Loss 112260.195 Test MSE 24385.83829049909 Test RE 0.27682684747629094\n",
      "Training time: 348.26\n",
      "3D_HTTP_tanh\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 648851.1 Test MSE 306440.8423724118 Test RE 0.9813246000180669\n",
      "1 Train Loss 513665.97 Test MSE 280345.6619118641 Test RE 0.938612389020955\n",
      "2 Train Loss 470904.72 Test MSE 253452.87390820563 Test RE 0.8924583808223318\n",
      "3 Train Loss 432213.84 Test MSE 226974.3659584067 Test RE 0.8445546705523974\n",
      "4 Train Loss 390521.9 Test MSE 190365.75267210748 Test RE 0.7734527222612159\n",
      "5 Train Loss 358248.28 Test MSE 153811.77597432942 Test RE 0.6952390469711445\n",
      "6 Train Loss 243225.48 Test MSE 87387.63872417332 Test RE 0.5240399113294897\n",
      "7 Train Loss 237712.75 Test MSE 83963.35811370531 Test RE 0.5136700708243506\n",
      "8 Train Loss 235133.86 Test MSE 82397.76656714262 Test RE 0.5088585572414049\n",
      "9 Train Loss 230776.66 Test MSE 79389.58029205368 Test RE 0.49948346426337653\n",
      "10 Train Loss 225656.73 Test MSE 76879.4890504504 Test RE 0.4915238619856736\n",
      "11 Train Loss 222904.44 Test MSE 74481.82210131524 Test RE 0.4837984899376828\n",
      "12 Train Loss 220326.4 Test MSE 73042.17303362541 Test RE 0.47910003742023427\n",
      "13 Train Loss 217289.27 Test MSE 71042.04626864452 Test RE 0.4724948653045911\n",
      "14 Train Loss 215179.95 Test MSE 69382.64799667269 Test RE 0.46694399812830956\n",
      "15 Train Loss 213573.72 Test MSE 68787.42847280353 Test RE 0.46493677551554197\n",
      "16 Train Loss 211968.86 Test MSE 68022.71288435684 Test RE 0.46234518215936393\n",
      "17 Train Loss 210173.88 Test MSE 66821.5402621205 Test RE 0.4582448609843539\n",
      "18 Train Loss 208831.28 Test MSE 65607.92458531623 Test RE 0.4540644614618433\n",
      "19 Train Loss 207038.5 Test MSE 64533.422134215885 Test RE 0.4503308611224032\n",
      "20 Train Loss 205728.7 Test MSE 63535.58538352888 Test RE 0.44683571664163907\n",
      "21 Train Loss 204471.69 Test MSE 63403.21464317334 Test RE 0.4463700027130436\n",
      "22 Train Loss 202318.03 Test MSE 61623.62637710053 Test RE 0.4400611081706893\n",
      "23 Train Loss 201506.27 Test MSE 60911.275447175874 Test RE 0.43751022662984923\n",
      "24 Train Loss 200635.73 Test MSE 60217.86117802981 Test RE 0.4350127892540404\n",
      "25 Train Loss 199627.03 Test MSE 60067.42389186613 Test RE 0.4344690712816337\n",
      "26 Train Loss 198768.36 Test MSE 59409.840800660735 Test RE 0.43208436985994736\n",
      "27 Train Loss 198230.0 Test MSE 59124.45779651648 Test RE 0.43104533344784396\n",
      "28 Train Loss 197497.55 Test MSE 59069.0858330274 Test RE 0.4308434422320817\n",
      "29 Train Loss 196871.7 Test MSE 59101.86915739268 Test RE 0.4309629846364653\n",
      "30 Train Loss 195976.97 Test MSE 58700.64787626107 Test RE 0.4294976673620939\n",
      "31 Train Loss 195254.42 Test MSE 58162.38873615158 Test RE 0.4275239800656901\n",
      "32 Train Loss 194773.72 Test MSE 57806.23140704341 Test RE 0.4262129987039637\n",
      "33 Train Loss 194099.34 Test MSE 57643.201561350186 Test RE 0.42561155402359374\n",
      "34 Train Loss 193223.53 Test MSE 57476.103887100275 Test RE 0.42499421916079727\n",
      "35 Train Loss 192653.53 Test MSE 57455.145231976916 Test RE 0.4249167250473605\n",
      "36 Train Loss 192180.0 Test MSE 57171.96530597142 Test RE 0.4238682852758947\n",
      "37 Train Loss 191773.83 Test MSE 56889.9770522989 Test RE 0.4228216741847945\n",
      "38 Train Loss 191244.9 Test MSE 56331.30238210109 Test RE 0.42074044149686124\n",
      "39 Train Loss 190773.7 Test MSE 56008.49451737391 Test RE 0.4195331779078439\n",
      "40 Train Loss 190302.8 Test MSE 55662.612774888825 Test RE 0.4182357533557642\n",
      "41 Train Loss 189679.38 Test MSE 55406.64029123344 Test RE 0.41727298676915536\n",
      "42 Train Loss 189093.47 Test MSE 55052.56718724326 Test RE 0.4159375697740605\n",
      "43 Train Loss 188693.67 Test MSE 54551.8271107632 Test RE 0.41404163293334606\n",
      "44 Train Loss 188011.66 Test MSE 54471.24592133978 Test RE 0.41373571928188607\n",
      "45 Train Loss 187525.5 Test MSE 54442.58698747306 Test RE 0.4136268656616303\n",
      "46 Train Loss 187034.55 Test MSE 54110.1753487904 Test RE 0.4123621856487005\n",
      "47 Train Loss 186321.34 Test MSE 53904.990005927575 Test RE 0.41157960604062155\n",
      "48 Train Loss 185062.03 Test MSE 53513.75781057674 Test RE 0.41008330275912375\n",
      "49 Train Loss 184501.0 Test MSE 53280.38327529776 Test RE 0.40918813511509583\n",
      "50 Train Loss 183957.77 Test MSE 53205.59994252828 Test RE 0.40890086993711383\n",
      "51 Train Loss 183632.12 Test MSE 53119.84727803764 Test RE 0.40857121968374743\n",
      "52 Train Loss 183348.48 Test MSE 53182.218187824874 Test RE 0.4088110121920595\n",
      "53 Train Loss 182809.08 Test MSE 52992.44155290442 Test RE 0.4080809550727812\n",
      "54 Train Loss 182572.4 Test MSE 52979.35258853756 Test RE 0.40803055461199367\n",
      "55 Train Loss 182277.23 Test MSE 52650.41497427718 Test RE 0.4067618945538678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 Train Loss 181829.92 Test MSE 52557.24725632875 Test RE 0.40640184176456456\n",
      "57 Train Loss 181637.17 Test MSE 52293.333796483195 Test RE 0.40538019484995824\n",
      "58 Train Loss 181135.88 Test MSE 52011.78534900847 Test RE 0.404287434106247\n",
      "59 Train Loss 180523.97 Test MSE 51511.86434381974 Test RE 0.4023398006420476\n",
      "60 Train Loss 180357.22 Test MSE 51594.403829629075 Test RE 0.402662014061675\n",
      "61 Train Loss 179866.23 Test MSE 51366.60560647857 Test RE 0.4017721194718857\n",
      "62 Train Loss 179583.89 Test MSE 51237.37469543834 Test RE 0.40126640107547096\n",
      "63 Train Loss 179303.7 Test MSE 50704.55970399272 Test RE 0.39917457367222403\n",
      "64 Train Loss 178533.66 Test MSE 50477.97108953492 Test RE 0.3982816590159997\n",
      "65 Train Loss 178256.34 Test MSE 50274.64029780333 Test RE 0.3974786885320812\n",
      "66 Train Loss 177761.95 Test MSE 50162.657399719756 Test RE 0.3970357651322036\n",
      "67 Train Loss 177448.39 Test MSE 50142.0153550912 Test RE 0.3969540661781274\n",
      "68 Train Loss 177225.17 Test MSE 49877.23686234578 Test RE 0.3959046067614035\n",
      "69 Train Loss 176885.6 Test MSE 49768.898661807725 Test RE 0.39547440140234114\n",
      "70 Train Loss 176282.48 Test MSE 49932.92394307164 Test RE 0.396125555463708\n",
      "71 Train Loss 176039.34 Test MSE 49811.08547519515 Test RE 0.39564197865634465\n",
      "72 Train Loss 175882.6 Test MSE 49729.747178588026 Test RE 0.3953188177339093\n",
      "73 Train Loss 175535.83 Test MSE 49482.13821448615 Test RE 0.39433342532048105\n",
      "74 Train Loss 175048.8 Test MSE 49300.815584369666 Test RE 0.39361026339389354\n",
      "75 Train Loss 174707.95 Test MSE 49167.58505886967 Test RE 0.3930780573985277\n",
      "76 Train Loss 174496.7 Test MSE 49099.175418064464 Test RE 0.39280450635159103\n",
      "77 Train Loss 174027.33 Test MSE 48982.46816833196 Test RE 0.39233738640927673\n",
      "78 Train Loss 173821.2 Test MSE 48879.07582628029 Test RE 0.3919230942001685\n",
      "79 Train Loss 173605.67 Test MSE 48681.6928394134 Test RE 0.39113096377151396\n",
      "80 Train Loss 173434.58 Test MSE 48477.66476550694 Test RE 0.39031047580470907\n",
      "81 Train Loss 173214.47 Test MSE 48385.73348521443 Test RE 0.38994021488885927\n",
      "82 Train Loss 172937.89 Test MSE 48409.07580014488 Test RE 0.39003426130653485\n",
      "83 Train Loss 172720.75 Test MSE 48544.06462051024 Test RE 0.39057768845531965\n",
      "84 Train Loss 172394.89 Test MSE 48327.48640674734 Test RE 0.3897054378558918\n",
      "85 Train Loss 171997.16 Test MSE 48008.37185581283 Test RE 0.3884166614338352\n",
      "86 Train Loss 171377.33 Test MSE 47601.43650097678 Test RE 0.38676698204907406\n",
      "87 Train Loss 170948.92 Test MSE 47332.17689618287 Test RE 0.38567154859648434\n",
      "88 Train Loss 170741.81 Test MSE 47221.17979212555 Test RE 0.3852190704469252\n",
      "89 Train Loss 170518.69 Test MSE 47009.579069137566 Test RE 0.3843550073824091\n",
      "90 Train Loss 170288.89 Test MSE 46993.48762561738 Test RE 0.38428921912845354\n",
      "91 Train Loss 170024.53 Test MSE 47166.72898073265 Test RE 0.38499690804810033\n",
      "92 Train Loss 169681.67 Test MSE 47213.80629733883 Test RE 0.38518899367088966\n",
      "93 Train Loss 169333.34 Test MSE 47203.7810168294 Test RE 0.3851480963886346\n",
      "94 Train Loss 169016.62 Test MSE 46886.50168094707 Test RE 0.38385153112329795\n",
      "95 Train Loss 168845.33 Test MSE 46692.9257350913 Test RE 0.3830583254808411\n",
      "96 Train Loss 168409.97 Test MSE 46277.48082839627 Test RE 0.3813504094211989\n",
      "97 Train Loss 168078.08 Test MSE 46071.11661272418 Test RE 0.38049918539224953\n",
      "98 Train Loss 167881.62 Test MSE 45813.4988678705 Test RE 0.3794338676398144\n",
      "99 Train Loss 167569.03 Test MSE 45735.83264000785 Test RE 0.3791121098816517\n",
      "Training time: 311.69\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "\n",
    "    print(reps)\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_D = 5000 #Total number of data points for 'y'\n",
    "    N_N = 3500\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "\n",
    "    layers = np.array([3,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "\n",
    "    #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jFzPEF73CQsD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '3D_HTTP_tanh_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '3D_HTTP_tanh_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-32c8100bd565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"3D_HTTP_tanh_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '3D_HTTP_tanh_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"3D_HTTP_tanh_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HT_stan_v3_15Aug2022_MP4Video.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
