{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EG0W1AFlwR_K"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMtToePZL2nl",
    "outputId": "ed6ca883-db49-404e-df85-58c0227e5aa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIONgQSowWqL",
    "outputId": "220b4aa0-31cc-4133-973d-97d73f894c56"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bS0UTdDMwXb7",
    "outputId": "e0fddb95-0b36-482f-b039-f3345278893f"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/MURI Aug17 Thin Plate/Rowdy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7xfa_6UCyRdu",
    "outputId": "a091937a-a627-4f7d-f35f-3a5130dfca86"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1]).reshape(-1,1)\n",
    "n_value = np.array([1.0,3.0,5.0,8.0,10.0]).reshape(-1,1)\n",
    "r_value = np.array([2,6,8]).reshape(-1,1)\n",
    "\n",
    "LR_tune,N_value,R_value = np.meshgrid(lr_tune,n_value,r_value)\n",
    "\n",
    "LR_tune = LR_tune.flatten('F').reshape(-1,1)\n",
    "N_value = N_value.flatten('F').reshape(-1,1)\n",
    "R_value = R_value.flatten('F').reshape(-1,1)\n",
    "\n",
    "\n",
    "lrnr_tune = np.hstack((LR_tune,N_value,R_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uwHGknZlwR_O"
   },
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9MUkWfTQwR_P"
   },
   "outputs": [],
   "source": [
    "loss_thresh = 10000\n",
    "\n",
    "x = np.linspace(0,1,100).reshape(-1,1)\n",
    "y = np.linspace(0,1,100).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 3000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xyt = np.hstack((X,Y,T))\n",
    "\n",
    "initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "xyt_initial = xyt[initial_pts,:]\n",
    "xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "xyt_NBC_x0 = xyt[NBC_pts_x0,:]\n",
    "xyt_NBC_x1 = xyt[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xyt_NBC_y1 = xyt[NBC_pts_y1,:]\n",
    "\n",
    "u_initial = 300*np.ones((np.shape(xyt_initial)[0],1))\n",
    "u_DBC = 1000*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xyt_NBC_x = np.vstack((xyt_NBC_x0,xyt_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xyt_NBC_y = np.vstack((xyt_NBC_y1))\n",
    "\n",
    "u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../3D_HTTP_FEA.mat')\n",
    "xy = fea_data['xy']\n",
    "t = fea_data['t']/3000\n",
    "xyt = np.zeros((497*101,3))\n",
    "u_true = np.ones((497*101,1))\n",
    "\n",
    "\n",
    "for i in range(101):\n",
    "    t_temp = t[0,i]*np.ones((497,1))\n",
    "    xyt[497*i:497*(i+1)] = np.hstack((xy,t_temp))\n",
    "    u_true[497*i:497*(i+1)] = fea_data['u'][:,i].reshape(-1,1)\n",
    "    #print(i)\n",
    "#print(xyt)\n",
    "\n",
    "xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IfjvaqHTwR_Q"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xyt_I_DBC.shape[0], N_D, replace=False) \n",
    "    xyt_D = xyt_I_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_I_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_x.shape[0], N_D, replace=False) \n",
    "    xyt_Nx = xyt_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_y.shape[0], N_D, replace=False) \n",
    "    xyt_Ny = xyt_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xyt_coll = lb_xyt + (ub_xyt - lb_xyt)*samples\n",
    "    xyt_coll = np.vstack((xyt_coll, xyt_D,xyt_Nx,xyt_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xyt_coll, xyt_D, u_D, xyt_Nx,xyt_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KNZCQobEwR_Q"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "\n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        self.omega1.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        self.omega.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "                \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/3000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xyt_coll_np_array, xyt_D_np_array, u_D_np_array,xyt_Nx_np_array,xyt_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "\n",
    "    xyt_coll = torch.from_numpy(xyt_coll_np_array).float().to(device)\n",
    "    xyt_D = torch.from_numpy(xyt_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xyt_Nx = torch.from_numpy(xyt_Nx_np_array).float().to(device)\n",
    "    xyt_Ny = torch.from_numpy(xyt_Ny_np_array).float().to(device)\n",
    "\n",
    "    N_hat = torch.zeros(xyt_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xyt_coll.shape[0],1).to(device)\n",
    "\n",
    "    nan_flag = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,i)\n",
    "\n",
    "        loss_np = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "        if(np.isnan(loss_np)):\n",
    "            nan_flag =1\n",
    "            print(\"NAN BREAK!\")\n",
    "            break\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    \n",
    "    return nan_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2QpZV2SwR_S",
    "outputId": "9326a244-267a-4324-8765-4f710090a2eb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D_HTTP_rowdy_tune0\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1348241.0 Test MSE 309544.74322853494 Test RE 0.9862819354818919\n",
      "1 Train Loss 653680.75 Test MSE 303633.8539962303 Test RE 0.9768198092780221\n",
      "2 Train Loss 497970.6 Test MSE 246792.58687006592 Test RE 0.8806542135209325\n",
      "3 Train Loss 336251.03 Test MSE 119318.58549495389 Test RE 0.6123411004103002\n",
      "4 Train Loss 252733.53 Test MSE 82077.1766791193 Test RE 0.5078676693581256\n",
      "5 Train Loss 226328.84 Test MSE 76379.99039274812 Test RE 0.4899245043302734\n",
      "6 Train Loss 223293.33 Test MSE 73468.01245792182 Test RE 0.4804945960577399\n",
      "7 Train Loss 220118.2 Test MSE 68867.66002655645 Test RE 0.46520784051906183\n",
      "8 Train Loss 212124.19 Test MSE 63461.67598936429 Test RE 0.44657574447882054\n",
      "9 Train Loss 210411.6 Test MSE 64702.37888954649 Test RE 0.4509199876763871\n",
      "10 Train Loss 204182.73 Test MSE 61517.515245300674 Test RE 0.4396820692875331\n",
      "11 Train Loss 198867.86 Test MSE 55995.889753713185 Test RE 0.4194859670860007\n",
      "12 Train Loss 187289.78 Test MSE 52330.21582815724 Test RE 0.4055231252082354\n",
      "13 Train Loss 175463.56 Test MSE 46379.91348809728 Test RE 0.38177222521692517\n",
      "14 Train Loss 172226.9 Test MSE 42969.69333121833 Test RE 0.3674688153663485\n",
      "15 Train Loss 170350.12 Test MSE 41318.99873658883 Test RE 0.3603414789309822\n",
      "16 Train Loss 169326.94 Test MSE 42008.85082859411 Test RE 0.3633371145355326\n",
      "17 Train Loss 164713.7 Test MSE 39773.6520569454 Test RE 0.35353881124030956\n",
      "18 Train Loss 163442.1 Test MSE 38425.91097027938 Test RE 0.347497311011317\n",
      "19 Train Loss 162302.72 Test MSE 37659.28481779178 Test RE 0.34401342910452415\n",
      "20 Train Loss 161391.05 Test MSE 37879.96593816987 Test RE 0.3450199056793821\n",
      "21 Train Loss 160518.23 Test MSE 37879.879272210186 Test RE 0.34501951099187916\n",
      "22 Train Loss 158588.1 Test MSE 36167.798685904534 Test RE 0.3371323288124863\n",
      "23 Train Loss 158215.78 Test MSE 35392.169645142196 Test RE 0.33349778646089434\n",
      "24 Train Loss 156275.25 Test MSE 34739.16597082169 Test RE 0.3304068602234878\n",
      "25 Train Loss 155529.78 Test MSE 35163.90537969199 Test RE 0.3324205881733593\n",
      "26 Train Loss 154617.08 Test MSE 35368.79290593359 Test RE 0.33338762962923685\n",
      "27 Train Loss 153885.92 Test MSE 35504.81236456337 Test RE 0.3340280768473708\n",
      "28 Train Loss 153546.69 Test MSE 35669.75452443213 Test RE 0.3348030629535361\n",
      "29 Train Loss 152793.92 Test MSE 35736.778480299676 Test RE 0.33511746509509716\n",
      "30 Train Loss 152303.89 Test MSE 35622.17838044823 Test RE 0.33457970910835316\n",
      "31 Train Loss 151455.28 Test MSE 34975.89007761273 Test RE 0.33153069944940217\n",
      "32 Train Loss 150290.88 Test MSE 33903.64449148105 Test RE 0.3264093231639226\n",
      "33 Train Loss 148661.52 Test MSE 33578.11036132952 Test RE 0.32483849394105463\n",
      "34 Train Loss 147729.88 Test MSE 33393.24745694743 Test RE 0.3239430673281451\n",
      "35 Train Loss 147398.36 Test MSE 33244.56372154693 Test RE 0.323221083057107\n",
      "36 Train Loss 146088.86 Test MSE 31643.86907058726 Test RE 0.31534369467436524\n",
      "37 Train Loss 144194.73 Test MSE 30687.225939145075 Test RE 0.3105404497509904\n",
      "38 Train Loss 143048.23 Test MSE 30264.065358148688 Test RE 0.30839192317348335\n",
      "39 Train Loss 141932.38 Test MSE 30123.53535705655 Test RE 0.3076750871523226\n",
      "40 Train Loss 139625.38 Test MSE 30199.89266511241 Test RE 0.3080647886311005\n",
      "41 Train Loss 139191.0 Test MSE 30287.401146232227 Test RE 0.30851079652619184\n",
      "42 Train Loss 138917.58 Test MSE 30189.282130223983 Test RE 0.3080106656018188\n",
      "43 Train Loss 138078.72 Test MSE 29696.85411029342 Test RE 0.3054883022662646\n",
      "44 Train Loss 137682.97 Test MSE 29365.298893266834 Test RE 0.303778179439856\n",
      "45 Train Loss 137451.66 Test MSE 29104.997290780833 Test RE 0.30242879830194624\n",
      "46 Train Loss 136833.69 Test MSE 28558.545109776627 Test RE 0.29957626486298106\n",
      "47 Train Loss 136083.16 Test MSE 28222.875122392277 Test RE 0.29781048870600535\n",
      "48 Train Loss 135161.66 Test MSE 27817.533704485297 Test RE 0.2956641535487597\n",
      "49 Train Loss 134812.14 Test MSE 27582.18656582667 Test RE 0.29441078012812233\n",
      "50 Train Loss 134566.05 Test MSE 27428.793508891446 Test RE 0.29359098412283247\n",
      "51 Train Loss 134398.53 Test MSE 27307.784611562885 Test RE 0.2929426436829809\n",
      "52 Train Loss 134194.03 Test MSE 27148.842673556854 Test RE 0.29208887944208173\n",
      "53 Train Loss 134011.69 Test MSE 27043.86205722704 Test RE 0.29152359989419696\n",
      "54 Train Loss 133809.64 Test MSE 27013.340662995783 Test RE 0.29135904834861015\n",
      "55 Train Loss 133572.98 Test MSE 26971.470497364087 Test RE 0.2911331602849289\n",
      "56 Train Loss 133016.06 Test MSE 26757.945848859305 Test RE 0.28997846557568724\n",
      "57 Train Loss 132328.52 Test MSE 26467.69312534262 Test RE 0.28840142872956\n",
      "58 Train Loss 131805.89 Test MSE 26352.03279081205 Test RE 0.28777060067814964\n",
      "59 Train Loss 131025.05 Test MSE 26222.666886351664 Test RE 0.2870633780920366\n",
      "60 Train Loss 129890.67 Test MSE 25891.06511317875 Test RE 0.28524255681793403\n",
      "61 Train Loss 128572.49 Test MSE 25589.337467610367 Test RE 0.283575615290949\n",
      "62 Train Loss 127533.64 Test MSE 25300.522780057803 Test RE 0.28197078261240766\n",
      "63 Train Loss 126189.09 Test MSE 25208.221993716823 Test RE 0.28145597298649633\n",
      "64 Train Loss 125370.305 Test MSE 25011.38254971206 Test RE 0.28035493910423753\n",
      "65 Train Loss 124397.84 Test MSE 24762.828263815674 Test RE 0.2789584267473056\n",
      "66 Train Loss 123484.27 Test MSE 24871.196408522363 Test RE 0.2795681552752176\n",
      "67 Train Loss 122558.12 Test MSE 24815.89137478199 Test RE 0.27925715031063164\n",
      "68 Train Loss 121532.68 Test MSE 24620.192671672 Test RE 0.27815385662726405\n",
      "69 Train Loss 120053.7 Test MSE 24258.368319721292 Test RE 0.2761023830563103\n",
      "70 Train Loss 118245.445 Test MSE 23634.616133994456 Test RE 0.2725295748508897\n",
      "71 Train Loss 115885.67 Test MSE 22600.614409366703 Test RE 0.2665013940404629\n",
      "72 Train Loss 114166.625 Test MSE 21988.86249199577 Test RE 0.26286982977585494\n",
      "73 Train Loss 112506.73 Test MSE 21718.491555693367 Test RE 0.2612487320790976\n",
      "74 Train Loss 110348.98 Test MSE 20788.294139321148 Test RE 0.25559290187935907\n",
      "75 Train Loss 108889.65 Test MSE 19821.43724458849 Test RE 0.24957836373882217\n",
      "76 Train Loss 106372.586 Test MSE 19070.91714999871 Test RE 0.2448077441142861\n",
      "77 Train Loss 104445.35 Test MSE 18157.119130592164 Test RE 0.2388706733139086\n",
      "78 Train Loss 103642.63 Test MSE 18123.719778668055 Test RE 0.2386508752867464\n",
      "79 Train Loss 102598.52 Test MSE 17737.017231802456 Test RE 0.2360911220301576\n",
      "80 Train Loss 101241.59 Test MSE 16652.00236679404 Test RE 0.22875605280700795\n",
      "81 Train Loss 99738.72 Test MSE 15961.069652543674 Test RE 0.2239599483227193\n",
      "82 Train Loss 98629.41 Test MSE 15444.687783356721 Test RE 0.2203073207648058\n",
      "83 Train Loss 97943.55 Test MSE 14835.978832377945 Test RE 0.21592228361387875\n",
      "84 Train Loss 97645.13 Test MSE 14408.444349501036 Test RE 0.212788380351592\n",
      "85 Train Loss 97040.25 Test MSE 13888.091191386515 Test RE 0.2089106796414573\n",
      "86 Train Loss 95694.805 Test MSE 12939.453934945608 Test RE 0.20164958688348567\n",
      "87 Train Loss 94879.75 Test MSE 11976.558569406692 Test RE 0.19400163208014612\n",
      "88 Train Loss 94231.375 Test MSE 11477.94667739096 Test RE 0.18992033323197224\n",
      "89 Train Loss 91695.516 Test MSE 11526.271518089632 Test RE 0.19031971781461418\n",
      "90 Train Loss 88606.1 Test MSE 11575.137911312648 Test RE 0.19072272765825668\n",
      "91 Train Loss 83610.4 Test MSE 11457.824298675028 Test RE 0.18975378230548268\n",
      "92 Train Loss 78496.664 Test MSE 11692.269736239397 Test RE 0.19168528508774688\n",
      "93 Train Loss 74991.84 Test MSE 11339.320531232144 Test RE 0.18876995751968575\n",
      "94 Train Loss 74424.484 Test MSE 11585.633600872683 Test RE 0.19080917643277334\n",
      "95 Train Loss 74332.2 Test MSE 11718.645319498246 Test RE 0.19190136645088574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 74150.664 Test MSE 11510.076858650253 Test RE 0.1901859691692987\n",
      "97 Train Loss 73990.64 Test MSE 11263.0913559586 Test RE 0.18813437958625398\n",
      "98 Train Loss 73686.28 Test MSE 11042.48658414854 Test RE 0.18628281938714267\n",
      "99 Train Loss 73035.86 Test MSE 11155.097784001535 Test RE 0.18723026543759552\n",
      "Training time: 365.40\n",
      "3D_HTTP_rowdy_tune0\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 1462395.5 Test MSE 313515.91477944964 Test RE 0.9925883142819084\n",
      "1 Train Loss 1349088.8 Test MSE 312809.8482598305 Test RE 0.9914699844402817\n",
      "2 Train Loss 574592.9 Test MSE 297103.8509965717 Test RE 0.9662588902163629\n",
      "3 Train Loss 496048.12 Test MSE 271925.99393493147 Test RE 0.9244101907415522\n",
      "4 Train Loss 447402.2 Test MSE 227501.79366220318 Test RE 0.8455353607224645\n",
      "5 Train Loss 316968.34 Test MSE 121034.10146378809 Test RE 0.6167273910418618\n",
      "6 Train Loss 245162.38 Test MSE 88945.6425638514 Test RE 0.5286907361505983\n",
      "7 Train Loss 234464.86 Test MSE 80104.0792029404 Test RE 0.501726082314709\n",
      "8 Train Loss 226802.62 Test MSE 76873.40729991361 Test RE 0.4915044199688866\n",
      "9 Train Loss 221740.34 Test MSE 72665.10584624967 Test RE 0.47786180307626375\n",
      "10 Train Loss 218348.48 Test MSE 69016.05230821755 Test RE 0.46570877307297964\n",
      "11 Train Loss 212780.55 Test MSE 63556.069206992666 Test RE 0.4469077405812814\n",
      "12 Train Loss 203024.34 Test MSE 60603.48586513719 Test RE 0.43640343947676113\n",
      "13 Train Loss 199697.73 Test MSE 59349.68193118057 Test RE 0.4318655484445723\n",
      "14 Train Loss 196662.1 Test MSE 56541.86903033556 Test RE 0.42152607278956905\n",
      "15 Train Loss 190370.78 Test MSE 52959.77138258414 Test RE 0.4079551434534604\n",
      "16 Train Loss 183427.75 Test MSE 49865.544822696094 Test RE 0.3958582007858812\n",
      "17 Train Loss 176852.86 Test MSE 49633.76081311525 Test RE 0.3949371191888\n",
      "18 Train Loss 172854.11 Test MSE 45996.30429529697 Test RE 0.3801901241688374\n",
      "19 Train Loss 170155.14 Test MSE 45420.652550144834 Test RE 0.3778035608430234\n",
      "20 Train Loss 159357.56 Test MSE 39004.87696581474 Test RE 0.35010540727647\n",
      "21 Train Loss 147868.64 Test MSE 35117.8012204005 Test RE 0.3322025947731262\n",
      "22 Train Loss 145481.62 Test MSE 34949.14148012006 Test RE 0.33140390242776885\n",
      "23 Train Loss 142101.97 Test MSE 33930.97673851002 Test RE 0.32654086806182975\n",
      "24 Train Loss 138443.73 Test MSE 32177.075683025476 Test RE 0.31798940362101674\n",
      "25 Train Loss 137483.44 Test MSE 31087.46368485438 Test RE 0.31255899923743263\n",
      "26 Train Loss 136834.14 Test MSE 30681.51085221673 Test RE 0.3105115313940831\n",
      "27 Train Loss 134553.72 Test MSE 29852.924640708447 Test RE 0.3062899906069079\n",
      "28 Train Loss 131541.27 Test MSE 28761.128521189883 Test RE 0.3006369270055919\n",
      "29 Train Loss 129238.445 Test MSE 26794.71367573684 Test RE 0.2901776254439337\n",
      "30 Train Loss 127079.33 Test MSE 26263.51544210468 Test RE 0.2872868786496704\n",
      "31 Train Loss 122582.805 Test MSE 25394.468894043042 Test RE 0.282493805679674\n",
      "32 Train Loss 117636.42 Test MSE 22593.61278283411 Test RE 0.26646011003515946\n",
      "33 Train Loss 106532.79 Test MSE 18594.64064213957 Test RE 0.24173150560873674\n",
      "34 Train Loss 95850.76 Test MSE 17024.86586473626 Test RE 0.2313029712606239\n",
      "35 Train Loss 92542.4 Test MSE 16515.116700732735 Test RE 0.22781388255523022\n",
      "36 Train Loss 85269.46 Test MSE 14123.494121034619 Test RE 0.21067375638859934\n",
      "37 Train Loss 79595.164 Test MSE 11964.166922188164 Test RE 0.19390124339494177\n",
      "38 Train Loss 77432.75 Test MSE 11994.892031896603 Test RE 0.19415006209559513\n",
      "39 Train Loss 70286.375 Test MSE 10010.545978194092 Test RE 0.1773651312198434\n",
      "40 Train Loss 68116.5 Test MSE 10118.707750411519 Test RE 0.17832075266947212\n",
      "41 Train Loss 63767.555 Test MSE 9543.955315178908 Test RE 0.17318232301985612\n",
      "42 Train Loss 62722.21 Test MSE 9007.068246772971 Test RE 0.16824070862233206\n",
      "43 Train Loss 62284.715 Test MSE 9018.983307477114 Test RE 0.16835195102054124\n",
      "44 Train Loss 61129.15 Test MSE 8755.854264597567 Test RE 0.16587793659559585\n",
      "45 Train Loss 60054.79 Test MSE 8554.547599684565 Test RE 0.16395999099396466\n",
      "46 Train Loss 58113.723 Test MSE 8757.585578667276 Test RE 0.1658943354872225\n",
      "47 Train Loss 56880.867 Test MSE 8640.064838255834 Test RE 0.16477748237060805\n",
      "48 Train Loss 56262.65 Test MSE 8821.118602390197 Test RE 0.16649499888684824\n",
      "49 Train Loss 54981.797 Test MSE 8500.985438425852 Test RE 0.16344588775034763\n",
      "50 Train Loss 54354.207 Test MSE 8307.37827912478 Test RE 0.16157395491834914\n",
      "51 Train Loss 53559.99 Test MSE 8214.736487114182 Test RE 0.1606705130752944\n",
      "52 Train Loss 52464.246 Test MSE 7341.446858364294 Test RE 0.15189035317746083\n",
      "53 Train Loss 50622.254 Test MSE 6892.367509358929 Test RE 0.1471714522664187\n",
      "54 Train Loss 49434.74 Test MSE 6588.613171994119 Test RE 0.1438919066536605\n",
      "55 Train Loss 48633.414 Test MSE 6347.798458201034 Test RE 0.1412377944159842\n",
      "56 Train Loss 47776.754 Test MSE 6198.5732900174735 Test RE 0.139567799897196\n",
      "57 Train Loss 47468.176 Test MSE 6229.515725209754 Test RE 0.13991571798944608\n",
      "58 Train Loss 47179.19 Test MSE 6145.352750772758 Test RE 0.13896734832175378\n",
      "59 Train Loss 46815.598 Test MSE 5964.954087834285 Test RE 0.13691244141220102\n",
      "60 Train Loss 46610.273 Test MSE 5887.142449389213 Test RE 0.13601651224717545\n",
      "61 Train Loss 46397.773 Test MSE 5841.6839250324565 Test RE 0.13549035781267216\n",
      "62 Train Loss 45921.953 Test MSE 5748.258436558369 Test RE 0.1344025488541595\n",
      "63 Train Loss 45195.59 Test MSE 5601.522585934421 Test RE 0.13267601168826804\n",
      "64 Train Loss 43458.85 Test MSE 5611.65712459558 Test RE 0.13279597929669745\n",
      "65 Train Loss 42856.754 Test MSE 5649.361347286676 Test RE 0.13324135459818007\n",
      "66 Train Loss 42206.527 Test MSE 5575.53409825179 Test RE 0.1323678760441418\n",
      "67 Train Loss 41510.086 Test MSE 5377.706359125099 Test RE 0.12999836928514066\n",
      "68 Train Loss 40810.242 Test MSE 5257.771865654452 Test RE 0.12854057292313703\n",
      "69 Train Loss 39365.758 Test MSE 5342.104064491842 Test RE 0.12956733739727375\n",
      "70 Train Loss 39219.63 Test MSE 5271.341729171961 Test RE 0.1287063421945272\n",
      "71 Train Loss 39021.49 Test MSE 5341.175002105975 Test RE 0.12955607017256562\n",
      "72 Train Loss 37955.92 Test MSE 5467.987469073972 Test RE 0.13108503595287915\n",
      "73 Train Loss 37562.367 Test MSE 5168.835731929885 Test RE 0.12744879314721208\n",
      "74 Train Loss 37325.46 Test MSE 5035.036466082695 Test RE 0.12578842296960743\n",
      "75 Train Loss 37191.953 Test MSE 5059.991126565618 Test RE 0.12609975414153282\n",
      "76 Train Loss 36704.266 Test MSE 4882.985890356925 Test RE 0.12387455201268201\n",
      "77 Train Loss 36637.36 Test MSE 4867.04067021827 Test RE 0.12367213261435109\n",
      "78 Train Loss 36350.14 Test MSE 5008.803429712505 Test RE 0.1254603099939512\n",
      "79 Train Loss 36069.355 Test MSE 4912.86315708384 Test RE 0.12425294640490468\n",
      "80 Train Loss 35817.273 Test MSE 4803.237254482128 Test RE 0.12285883185253303\n",
      "81 Train Loss 35706.87 Test MSE 4789.158459861365 Test RE 0.12267864362565196\n",
      "82 Train Loss 35651.63 Test MSE 4762.65519870227 Test RE 0.12233872013514593\n",
      "83 Train Loss 35588.04 Test MSE 4763.057439759356 Test RE 0.12234388622581088\n",
      "84 Train Loss 35183.92 Test MSE 4874.568159197259 Test RE 0.12376773289820865\n",
      "85 Train Loss 34765.473 Test MSE 4978.557796680238 Test RE 0.12508094071257347\n",
      "86 Train Loss 34507.402 Test MSE 4985.107546535883 Test RE 0.1251631913998852\n",
      "87 Train Loss 34420.586 Test MSE 4992.080080893704 Test RE 0.1252506919904323\n",
      "88 Train Loss 34382.113 Test MSE 4985.359865307549 Test RE 0.1251663588965553\n",
      "89 Train Loss 34344.887 Test MSE 4979.134953189327 Test RE 0.12508819072250693\n",
      "90 Train Loss 34275.46 Test MSE 4951.03834357957 Test RE 0.12473476325211658\n",
      "91 Train Loss 34004.844 Test MSE 4813.6698884524785 Test RE 0.12299218420679184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Train Loss 33640.863 Test MSE 4582.410084391943 Test RE 0.12000140702741419\n",
      "93 Train Loss 33519.594 Test MSE 4559.059353985109 Test RE 0.1196952690683749\n",
      "94 Train Loss 33324.59 Test MSE 4609.811314055321 Test RE 0.12035965575909192\n",
      "95 Train Loss 33081.5 Test MSE 4645.966170819018 Test RE 0.1208307256920405\n",
      "96 Train Loss 32883.33 Test MSE 4717.012890559744 Test RE 0.12175110001764508\n",
      "97 Train Loss 32844.05 Test MSE 4808.862903476281 Test RE 0.12293075817745744\n",
      "98 Train Loss 32692.336 Test MSE 4802.1992285285505 Test RE 0.12284555564523492\n",
      "99 Train Loss 32625.842 Test MSE 4697.585426235135 Test RE 0.12150011961130788\n",
      "Training time: 364.43\n",
      "3D_HTTP_rowdy_tune0\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 1450944.5 Test MSE 314304.3435633808 Test RE 0.9938356095031747\n",
      "1 Train Loss 749125.4 Test MSE 296764.8650016272 Test RE 0.9657074976409581\n",
      "2 Train Loss 483793.03 Test MSE 258505.18817281642 Test RE 0.9013095942931597\n",
      "3 Train Loss 373725.5 Test MSE 164909.80467011602 Test RE 0.7198841321349798\n",
      "4 Train Loss 248170.48 Test MSE 87769.82217230258 Test RE 0.525184586273901\n",
      "5 Train Loss 245746.16 Test MSE 87354.0091472135 Test RE 0.5239390679115343\n",
      "6 Train Loss 245392.4 Test MSE 87514.53599409341 Test RE 0.5244202575166362\n",
      "7 Train Loss 244274.05 Test MSE 86142.28335474478 Test RE 0.520292483548063\n",
      "8 Train Loss 242969.89 Test MSE 84600.13281437656 Test RE 0.5156142181891797\n",
      "9 Train Loss 235490.31 Test MSE 78220.94619484823 Test RE 0.4957935753411102\n",
      "10 Train Loss 231541.72 Test MSE 76503.0908024234 Test RE 0.4903191471308095\n",
      "11 Train Loss 227425.25 Test MSE 71269.16192818945 Test RE 0.47324952651784685\n",
      "12 Train Loss 220954.1 Test MSE 68029.27752416818 Test RE 0.46236749129874344\n",
      "13 Train Loss 216262.92 Test MSE 61014.70436124967 Test RE 0.437881520894887\n",
      "14 Train Loss 202442.6 Test MSE 49350.9290483295 Test RE 0.3938102617456734\n",
      "15 Train Loss 196548.14 Test MSE 51905.293885609135 Test RE 0.4038733431451648\n",
      "16 Train Loss 192356.55 Test MSE 50462.57614177019 Test RE 0.3982209197194447\n",
      "17 Train Loss 188419.55 Test MSE 49360.67306015035 Test RE 0.3938491374317688\n",
      "18 Train Loss 185854.14 Test MSE 48220.80513769308 Test RE 0.38927506952346536\n",
      "19 Train Loss 185223.89 Test MSE 47342.19550316196 Test RE 0.38571236319112057\n",
      "20 Train Loss 184415.23 Test MSE 47203.56490207224 Test RE 0.3851472147189802\n",
      "21 Train Loss 183074.8 Test MSE 45857.780310544564 Test RE 0.3796171959320569\n",
      "22 Train Loss 181779.0 Test MSE 45566.02494720898 Test RE 0.3784076729665065\n",
      "23 Train Loss 175448.9 Test MSE 44807.872698679195 Test RE 0.3752463919321277\n",
      "24 Train Loss 172020.83 Test MSE 43102.81250131736 Test RE 0.36803758037792816\n",
      "25 Train Loss 164892.56 Test MSE 40197.280180432324 Test RE 0.35541659073547294\n",
      "26 Train Loss 162091.61 Test MSE 38427.18518101923 Test RE 0.3475030725030283\n",
      "27 Train Loss 161090.11 Test MSE 37874.42535106791 Test RE 0.3449946722519154\n",
      "28 Train Loss 157280.84 Test MSE 36159.66657104593 Test RE 0.3370944255802541\n",
      "29 Train Loss 156480.16 Test MSE 35995.908057615845 Test RE 0.3363302492416415\n",
      "30 Train Loss 156042.75 Test MSE 35348.559128592715 Test RE 0.3332922537970904\n",
      "31 Train Loss 155661.52 Test MSE 34864.9476250815 Test RE 0.3310044792149691\n",
      "32 Train Loss 154173.55 Test MSE 34495.974805885664 Test RE 0.32924832421285444\n",
      "33 Train Loss 151851.27 Test MSE 33643.61146716759 Test RE 0.3251551721878105\n",
      "34 Train Loss 151396.02 Test MSE 33128.71255728562 Test RE 0.3226574086656692\n",
      "35 Train Loss 151118.97 Test MSE 32981.09568146716 Test RE 0.32193774811640835\n",
      "36 Train Loss 151046.45 Test MSE 32887.03484351167 Test RE 0.3214783431881558\n",
      "37 Train Loss 150895.38 Test MSE 32747.466873860238 Test RE 0.32079546337547643\n",
      "38 Train Loss 150733.67 Test MSE 33019.50913012642 Test RE 0.3221251757939787\n",
      "39 Train Loss 150469.1 Test MSE 33656.009465089344 Test RE 0.3252150780904592\n",
      "40 Train Loss 149833.89 Test MSE 33699.8670041092 Test RE 0.32542690488773973\n",
      "41 Train Loss 148971.11 Test MSE 33962.188973875214 Test RE 0.3266910218244252\n",
      "42 Train Loss 147374.52 Test MSE 33250.38969475027 Test RE 0.32324940339981406\n",
      "43 Train Loss 144784.05 Test MSE 30852.804542342703 Test RE 0.31137711189369355\n",
      "44 Train Loss 141851.3 Test MSE 30153.421995292916 Test RE 0.30782767705056735\n",
      "45 Train Loss 141186.56 Test MSE 29848.87939018433 Test RE 0.30626923783751525\n",
      "46 Train Loss 140594.94 Test MSE 29239.982123418893 Test RE 0.303129297886221\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7e445121e72e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mnan_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-95f1f7121058>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxyt_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxyt_Nx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxyt_Ny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxyt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxyt_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxyt_Nx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxyt_Ny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxyt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b4fc61822cea>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(xyt_D, u_D, xyt_Nx, xyt_Ny, N_hat, xyt_coll, f_hat, seed)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_old\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_stps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0;31m# multiply by initial Hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nan_tune = []\n",
    "for tune_reps in range(75):\n",
    "    label = \"3D_HTTP_rowdy_tune\"+str(tune_reps)\n",
    "    \n",
    "    max_reps = 10 #10\n",
    "    max_iter = 100 #1000\n",
    "\n",
    "    train_loss_full = []\n",
    "    test_mse_full = []\n",
    "    test_re_full = []\n",
    "    alpha_full = []\n",
    "    omega_full = []\n",
    "    elapsed_time= np.zeros((max_reps,1))\n",
    "    \n",
    "    time_threshold = np.empty((max_reps,1))\n",
    "    time_threshold[:] = np.nan\n",
    "    epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "    \n",
    "    n_val = lrnr_tune[tune_reps,1]\n",
    "    rowdy_terms = int(lrnr_tune[tune_reps,2])\n",
    "\n",
    "    for reps in range(max_reps):\n",
    "        print(label)\n",
    "        print(reps)\n",
    "\n",
    "        train_loss = []\n",
    "        test_mse_loss = []\n",
    "        test_re_loss = []\n",
    "\n",
    "        alpha_val = []    \n",
    "        omega_val = []\n",
    "\n",
    "        torch.manual_seed(reps*36)\n",
    "        N_D = 5000 #Total number of data points for 'y'\n",
    "        N_N = 3500\n",
    "        N_f = 10000#Total number of collocation points \n",
    "\n",
    "        layers = np.array([3,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "\n",
    "        PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "\n",
    "        PINN.to(device)\n",
    "\n",
    "        'Neural Network Summary'\n",
    "        print(PINN)\n",
    "\n",
    "        params = list(PINN.parameters())\n",
    "\n",
    "        optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lrnr_tune[tune_reps,0], \n",
    "                                  max_iter = 10, \n",
    "                                  max_eval = 15, \n",
    "                                  tolerance_grad = 1e-6, \n",
    "                                  tolerance_change = 1e-6, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "        nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "        torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "        train_loss_full.append(train_loss)\n",
    "        test_mse_full.append(test_mse_loss)\n",
    "        test_re_full.append(test_re_loss)\n",
    "        #elapsed_time[reps] = time.time() - start_time\n",
    "        alpha_full.append(alpha_val)\n",
    "        omega_full.append(omega_val)\n",
    "        \n",
    "        if(nan_flag == 1):\n",
    "            nan_tune.append(tune_reps)\n",
    "            break\n",
    "\n",
    "\n",
    "        #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "    mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time,\"alpha\": alpha_full,\"omega\": omega_full,  \"label\": label}\n",
    "    savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rH35Ss9wR_Y"
   },
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HT_Rowdy_v3_15Aug2022_MP4Video.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
