{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLsZ-c_nCQr2",
    "outputId": "0238c820-5951-4e75-a35b-19e4de8c9b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SV23gJi7JexL",
    "outputId": "6f051579-557f-463f-d7b4-955ed617736e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOyXTKXGJf97",
    "outputId": "11b7b7db-47b0-4cf8-c699-473f1c6b8c5f"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/MURI Aug17 Thin Plate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "APjvgycyCTj0",
    "outputId": "19bce659-211e-4bec-d94d-7c94148b0d09"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lxFUD2gACQr7"
   },
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CUcT7YuXCQr7"
   },
   "outputs": [],
   "source": [
    "label = \"3D_HTTP_stan\"\n",
    "loss_thresh = 20000\n",
    "x = np.linspace(0,1,100).reshape(-1,1)\n",
    "y = np.linspace(0,1,100).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xyt = np.hstack((X,Y,T))\n",
    "\n",
    "initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "xyt_initial = xyt[initial_pts,:]\n",
    "xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "xyt_NBC_x0 = xyt[NBC_pts_x0,:]\n",
    "xyt_NBC_x1 = xyt[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xyt_NBC_y1 = xyt[NBC_pts_y1,:]\n",
    "\n",
    "u_initial = 300*np.ones((np.shape(xyt_initial)[0],1))\n",
    "u_DBC = 1000*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xyt_NBC_x = np.vstack((xyt_NBC_x0,xyt_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xyt_NBC_y = np.vstack((xyt_NBC_y1))\n",
    "\n",
    "u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../3D_HTTP_FEA.mat')\n",
    "xy = fea_data['xy']\n",
    "t = fea_data['t']/3000\n",
    "xyt = np.zeros((497*101,3))\n",
    "u_true = np.ones((497*101,1))\n",
    "\n",
    "\n",
    "for i in range(101):\n",
    "    t_temp = t[0,i]*np.ones((497,1))\n",
    "    xyt[497*i:497*(i+1)] = np.hstack((xy,t_temp))\n",
    "    u_true[497*i:497*(i+1)] = fea_data['u'][:,i].reshape(-1,1)\n",
    "    #print(i)\n",
    "#print(xyt)\n",
    "\n",
    "xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gp2G6x6BCQr8"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xyt_I_DBC.shape[0], N_D, replace=False) \n",
    "    xyt_D = xyt_I_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_I_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_x.shape[0], N_D, replace=False) \n",
    "    xyt_Nx = xyt_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_y.shape[0], N_D, replace=False) \n",
    "    xyt_Ny = xyt_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xyt_coll = lb_xyt + (ub_xyt - lb_xyt)*samples\n",
    "    xyt_coll = np.vstack((xyt_coll, xyt_D,xyt_Nx,xyt_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xyt_coll, xyt_D, u_D, xyt_Nx,xyt_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VRolFlBzCQr9"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "\n",
    "    \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/3000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "       \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xyt_coll_np_array, xyt_D_np_array, u_D_np_array,xyt_Nx_np_array,xyt_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "\n",
    "    xyt_coll = torch.from_numpy(xyt_coll_np_array).float().to(device)\n",
    "    xyt_D = torch.from_numpy(xyt_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xyt_Nx = torch.from_numpy(xyt_Nx_np_array).float().to(device)\n",
    "    xyt_Ny = torch.from_numpy(xyt_Ny_np_array).float().to(device)\n",
    "\n",
    "    N_hat = torch.zeros(xyt_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xyt_coll.shape[0],1).to(device)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,i)\n",
    "\n",
    "        loss_np = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVnXJfj0CQr-",
    "outputId": "1f2921b0-e258-465d-aa27-cdeb80b78a0b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D_HTTP_stan\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 269782.94 Test MSE 90171.70689853322 Test RE 0.5323221131650155\n",
      "1 Train Loss 196405.66 Test MSE 54599.01442797229 Test RE 0.41422066718156164\n",
      "2 Train Loss 173943.77 Test MSE 42601.2425661199 Test RE 0.36588996238122734\n",
      "3 Train Loss 157464.78 Test MSE 38006.07939031519 Test RE 0.34559376437625244\n",
      "4 Train Loss 151753.6 Test MSE 33656.67583444886 Test RE 0.32521829760896487\n",
      "5 Train Loss 144745.27 Test MSE 30199.951408590918 Test RE 0.3080650882478689\n",
      "6 Train Loss 134527.47 Test MSE 24480.136231431574 Test RE 0.27736156386371874\n",
      "7 Train Loss 119116.49 Test MSE 20044.47911782305 Test RE 0.25097863311820967\n",
      "8 Train Loss 108813.72 Test MSE 15662.025760940836 Test RE 0.22185199026360936\n",
      "9 Train Loss 97236.63 Test MSE 14502.433023432086 Test RE 0.2134812791761237\n",
      "10 Train Loss 84754.49 Test MSE 11328.510843753333 Test RE 0.18867995958809486\n",
      "11 Train Loss 71579.805 Test MSE 8274.924826444738 Test RE 0.16125804514985548\n",
      "12 Train Loss 63768.164 Test MSE 7303.515319818154 Test RE 0.15149745405403764\n",
      "13 Train Loss 57831.07 Test MSE 7952.172299196106 Test RE 0.1580819384984783\n",
      "14 Train Loss 51473.684 Test MSE 6054.491836570875 Test RE 0.13793618530435992\n",
      "15 Train Loss 47606.54 Test MSE 4870.973104439989 Test RE 0.1237220843574503\n",
      "16 Train Loss 45025.45 Test MSE 4553.102384346628 Test RE 0.11961704524050049\n",
      "17 Train Loss 43289.97 Test MSE 4140.688866187924 Test RE 0.11407110839948195\n",
      "18 Train Loss 41774.46 Test MSE 4279.373633574248 Test RE 0.11596567640087113\n",
      "19 Train Loss 39873.22 Test MSE 4208.077695939029 Test RE 0.11499560359177537\n",
      "20 Train Loss 38319.875 Test MSE 4821.364001610289 Test RE 0.12309043958799241\n",
      "21 Train Loss 36971.934 Test MSE 5032.024323826229 Test RE 0.12575079173192397\n",
      "22 Train Loss 36276.633 Test MSE 4866.201470710241 Test RE 0.12366147007071596\n",
      "23 Train Loss 35418.89 Test MSE 4690.5611911765645 Test RE 0.12140924689720302\n",
      "24 Train Loss 33932.875 Test MSE 4623.7099460614145 Test RE 0.12054096204729663\n",
      "25 Train Loss 33435.855 Test MSE 4490.763643917271 Test RE 0.11879535542033678\n",
      "26 Train Loss 32947.9 Test MSE 4613.7414578752105 Test RE 0.12041095178252181\n",
      "27 Train Loss 32725.537 Test MSE 4608.080446092026 Test RE 0.12033705762903212\n",
      "28 Train Loss 32316.84 Test MSE 4845.144213625643 Test RE 0.12339362311648376\n",
      "29 Train Loss 32080.207 Test MSE 4757.990850692305 Test RE 0.12227879871280828\n",
      "30 Train Loss 31895.7 Test MSE 4791.04143563643 Test RE 0.12270275832289329\n",
      "31 Train Loss 31702.3 Test MSE 4629.124130179603 Test RE 0.12061151578340241\n",
      "32 Train Loss 31589.47 Test MSE 4567.85453953528 Test RE 0.11981066950294224\n",
      "33 Train Loss 31479.334 Test MSE 4557.951542637755 Test RE 0.11968072573573474\n",
      "34 Train Loss 31103.523 Test MSE 4377.225316419465 Test RE 0.11728401191443852\n",
      "35 Train Loss 30774.432 Test MSE 4403.943157309115 Test RE 0.1176414082303156\n",
      "36 Train Loss 30632.53 Test MSE 4392.883794029115 Test RE 0.11749360240530711\n",
      "37 Train Loss 30322.102 Test MSE 4451.893020403648 Test RE 0.1182801106190441\n",
      "38 Train Loss 29867.523 Test MSE 4041.1286137821985 Test RE 0.11269138033364899\n",
      "39 Train Loss 29735.092 Test MSE 3978.5390757683494 Test RE 0.11181528527545276\n",
      "40 Train Loss 29546.533 Test MSE 4054.196274041687 Test RE 0.11287343641866195\n",
      "41 Train Loss 29161.486 Test MSE 3855.8877895247992 Test RE 0.11007825990161658\n",
      "42 Train Loss 28892.87 Test MSE 4003.4481958104648 Test RE 0.11216476964999694\n",
      "43 Train Loss 28596.121 Test MSE 4207.631047680783 Test RE 0.11498950057363395\n",
      "44 Train Loss 28270.201 Test MSE 4150.322620365017 Test RE 0.11420373060303433\n",
      "45 Train Loss 27994.748 Test MSE 4114.135801925058 Test RE 0.11370476729877967\n",
      "46 Train Loss 27463.65 Test MSE 3727.5855816373214 Test RE 0.10823137448475216\n",
      "47 Train Loss 27005.076 Test MSE 3646.305969861026 Test RE 0.10704488414131644\n",
      "48 Train Loss 26671.44 Test MSE 3520.7546206534344 Test RE 0.10518583071190893\n",
      "49 Train Loss 26087.93 Test MSE 3671.035071113891 Test RE 0.10740725784000928\n",
      "50 Train Loss 25895.719 Test MSE 3620.034121788678 Test RE 0.10665855467183938\n",
      "51 Train Loss 25647.33 Test MSE 3447.819204104355 Test RE 0.10409062215976421\n",
      "52 Train Loss 25430.268 Test MSE 3310.2549260728024 Test RE 0.10199293336135505\n",
      "53 Train Loss 25165.98 Test MSE 3318.904212911679 Test RE 0.10212609389728261\n",
      "54 Train Loss 25084.88 Test MSE 3329.1597511587283 Test RE 0.10228375891992263\n",
      "55 Train Loss 24976.086 Test MSE 3339.8806874682523 Test RE 0.1024483193998337\n",
      "56 Train Loss 24773.326 Test MSE 3226.977236722071 Test RE 0.1007018184797991\n",
      "57 Train Loss 24528.799 Test MSE 2972.180189650993 Test RE 0.09664445243279386\n",
      "58 Train Loss 24413.068 Test MSE 2950.781271943971 Test RE 0.09629591660496944\n",
      "59 Train Loss 24110.09 Test MSE 2886.7507662117123 Test RE 0.09524539935144571\n",
      "60 Train Loss 23880.549 Test MSE 2643.1373240439184 Test RE 0.091137943772832\n",
      "61 Train Loss 23728.455 Test MSE 2493.3286464560438 Test RE 0.08851749713459241\n",
      "62 Train Loss 23465.684 Test MSE 2413.524766183137 Test RE 0.0870893886756791\n",
      "63 Train Loss 23167.736 Test MSE 2633.331037905991 Test RE 0.0909687215310666\n",
      "64 Train Loss 23017.861 Test MSE 2664.2964439557695 Test RE 0.0915020101191572\n",
      "65 Train Loss 22923.057 Test MSE 2581.065708621518 Test RE 0.09006144124704991\n",
      "66 Train Loss 22876.94 Test MSE 2594.0587721417924 Test RE 0.09028784095791452\n",
      "67 Train Loss 22704.268 Test MSE 2492.5093830925043 Test RE 0.08850295330366963\n",
      "68 Train Loss 22423.842 Test MSE 2401.838786184582 Test RE 0.08687829497084687\n",
      "69 Train Loss 22322.047 Test MSE 2388.7158493614697 Test RE 0.08664063123968199\n",
      "70 Train Loss 22051.133 Test MSE 2460.4162326546952 Test RE 0.08793133241244105\n",
      "71 Train Loss 21942.174 Test MSE 2363.0012343099534 Test RE 0.0861730245732324\n",
      "72 Train Loss 21744.863 Test MSE 2259.954621954899 Test RE 0.08427314887461733\n",
      "73 Train Loss 21572.828 Test MSE 2210.9021530622576 Test RE 0.08335355431804559\n",
      "74 Train Loss 21511.691 Test MSE 2265.718455439193 Test RE 0.08438054640352174\n",
      "75 Train Loss 21339.23 Test MSE 2342.4248748419636 Test RE 0.08579701884439135\n",
      "76 Train Loss 21174.352 Test MSE 2394.9899846892126 Test RE 0.08675434057130538\n",
      "77 Train Loss 20931.83 Test MSE 2280.7794811581302 Test RE 0.08466053543037338\n",
      "78 Train Loss 20790.896 Test MSE 2142.1115664359304 Test RE 0.08204656536168085\n",
      "79 Train Loss 20625.59 Test MSE 2107.8086963086544 Test RE 0.08138698461614159\n",
      "80 Train Loss 20561.027 Test MSE 1982.08777269032 Test RE 0.07892249447926901\n",
      "81 Train Loss 20489.625 Test MSE 1954.5865190366344 Test RE 0.07837306145012074\n",
      "82 Train Loss 20291.246 Test MSE 2036.2796396126628 Test RE 0.07999412118890603\n",
      "83 Train Loss 20077.143 Test MSE 2016.719950982791 Test RE 0.0796089983398372\n",
      "84 Train Loss 19949.652 Test MSE 2095.7517655916968 Test RE 0.08115387889796262\n",
      "85 Train Loss 19845.121 Test MSE 1983.8754731122747 Test RE 0.07895807766074059\n",
      "86 Train Loss 19781.191 Test MSE 1970.0077561044984 Test RE 0.07868162671291272\n",
      "87 Train Loss 19721.188 Test MSE 1973.1360349091035 Test RE 0.07874407327638691\n",
      "88 Train Loss 19636.982 Test MSE 2007.1071265445923 Test RE 0.0794190410212392\n",
      "89 Train Loss 19529.55 Test MSE 2086.1854049879917 Test RE 0.0809684477682637\n",
      "90 Train Loss 19396.18 Test MSE 2118.084818150235 Test RE 0.0815851348803091\n",
      "91 Train Loss 19324.873 Test MSE 2130.134030319871 Test RE 0.08181686368001485\n",
      "92 Train Loss 19215.502 Test MSE 2075.9125030609325 Test RE 0.08076884723786379\n",
      "93 Train Loss 19148.941 Test MSE 1976.68063032994 Test RE 0.07881477054110618\n",
      "94 Train Loss 19084.686 Test MSE 2009.8868824878393 Test RE 0.07947401794887417\n",
      "95 Train Loss 19047.17 Test MSE 2035.1854955327378 Test RE 0.07997262687816864\n",
      "96 Train Loss 19009.836 Test MSE 2062.1016443134 Test RE 0.08049972494947588\n",
      "97 Train Loss 18902.084 Test MSE 2191.8453320051676 Test RE 0.08299354482847258\n",
      "98 Train Loss 18732.348 Test MSE 2173.540108295117 Test RE 0.08264625740044644\n",
      "99 Train Loss 18603.195 Test MSE 2156.646249986908 Test RE 0.08232444650462196\n",
      "Training time: 396.57\n",
      "3D_HTTP_stan\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 352244.72 Test MSE 150864.53670689557 Test RE 0.6885459750197099\n",
      "1 Train Loss 192704.88 Test MSE 56985.83718511527 Test RE 0.4231777534471264\n",
      "2 Train Loss 161114.05 Test MSE 40901.349953262536 Test RE 0.35851570362615504\n",
      "3 Train Loss 142855.7 Test MSE 32919.811413570584 Test RE 0.3216385025549755\n",
      "4 Train Loss 131123.92 Test MSE 26801.87517164898 Test RE 0.2902164011342968\n",
      "5 Train Loss 104634.44 Test MSE 16450.36087541814 Test RE 0.2273668144054488\n",
      "6 Train Loss 86120.92 Test MSE 12885.664879137492 Test RE 0.20123002371483395\n",
      "7 Train Loss 70345.44 Test MSE 10810.219676680425 Test RE 0.1843132775030386\n",
      "8 Train Loss 56637.105 Test MSE 8169.351641404027 Test RE 0.1602260614418832\n",
      "9 Train Loss 49945.676 Test MSE 6347.043746250582 Test RE 0.141229398038837\n",
      "10 Train Loss 42406.62 Test MSE 5693.22565125839 Test RE 0.1337576287230677\n",
      "11 Train Loss 37741.82 Test MSE 4238.505237749108 Test RE 0.11541060675922488\n",
      "12 Train Loss 34770.582 Test MSE 3932.803231132906 Test RE 0.11117073352135447\n",
      "13 Train Loss 32538.83 Test MSE 4296.971569116378 Test RE 0.11620387279838496\n",
      "14 Train Loss 29429.553 Test MSE 3978.9377393935238 Test RE 0.11182088727778318\n",
      "15 Train Loss 28045.629 Test MSE 3551.4014353094017 Test RE 0.1056426398762507\n",
      "16 Train Loss 26832.123 Test MSE 3710.959033336872 Test RE 0.10798972673916504\n",
      "17 Train Loss 26014.555 Test MSE 3174.226185577907 Test RE 0.09987534608005082\n",
      "18 Train Loss 25417.8 Test MSE 3292.711070377544 Test RE 0.10172230066551925\n",
      "19 Train Loss 25125.969 Test MSE 3094.6240276074454 Test RE 0.0986150748764843\n",
      "20 Train Loss 23988.244 Test MSE 3047.3938767904388 Test RE 0.09785964985115035\n",
      "21 Train Loss 23503.89 Test MSE 2815.738146248594 Test RE 0.09406661029703547\n",
      "22 Train Loss 23211.377 Test MSE 2761.978721691846 Test RE 0.093164300264163\n",
      "23 Train Loss 22892.406 Test MSE 3003.6548035987926 Test RE 0.09715482460161952\n",
      "24 Train Loss 22477.113 Test MSE 2872.310276899985 Test RE 0.09500687607069677\n",
      "25 Train Loss 22215.355 Test MSE 2513.981029496638 Test RE 0.08888333885748681\n",
      "26 Train Loss 21988.654 Test MSE 2445.4928022126724 Test RE 0.08766425707440463\n",
      "27 Train Loss 21736.688 Test MSE 2497.4323269038023 Test RE 0.08859031107784009\n",
      "28 Train Loss 21555.295 Test MSE 2211.2367182031007 Test RE 0.08335986082479732\n",
      "29 Train Loss 21435.658 Test MSE 2293.399382341045 Test RE 0.08489443216592532\n",
      "30 Train Loss 21301.492 Test MSE 2359.9533961017173 Test RE 0.08611743294308828\n",
      "31 Train Loss 21112.201 Test MSE 2426.542391121386 Test RE 0.08732393618084539\n",
      "32 Train Loss 21046.08 Test MSE 2435.419324647635 Test RE 0.08748351738789108\n",
      "33 Train Loss 20915.46 Test MSE 2236.745191695798 Test RE 0.08383929513816518\n",
      "34 Train Loss 20840.033 Test MSE 2180.7479959064035 Test RE 0.08278317962263032\n",
      "35 Train Loss 20756.451 Test MSE 2192.308060066491 Test RE 0.08300230489353812\n",
      "36 Train Loss 20615.693 Test MSE 2042.09731092279 Test RE 0.08010831168864987\n",
      "37 Train Loss 20498.027 Test MSE 2013.5527449567073 Test RE 0.07954646185056534\n",
      "38 Train Loss 20454.805 Test MSE 2044.9657109785246 Test RE 0.08016455338950815\n",
      "39 Train Loss 20366.512 Test MSE 2011.6220102138777 Test RE 0.07950831535746736\n",
      "40 Train Loss 20279.766 Test MSE 1955.842034302017 Test RE 0.07839822860894113\n",
      "41 Train Loss 20050.586 Test MSE 1754.1049358876844 Test RE 0.07424498904681905\n",
      "42 Train Loss 19856.646 Test MSE 1885.1429131106129 Test RE 0.07696823047119439\n",
      "43 Train Loss 19808.748 Test MSE 1887.4512693879967 Test RE 0.07701533982846816\n",
      "44 Train Loss 19675.12 Test MSE 1954.5254044200665 Test RE 0.07837183618406211\n",
      "45 Train Loss 19467.902 Test MSE 1870.4157639411933 Test RE 0.07666699465369116\n",
      "46 Train Loss 19398.984 Test MSE 1860.0169941032145 Test RE 0.07645357857104607\n",
      "47 Train Loss 19354.32 Test MSE 1785.139907178061 Test RE 0.07489890899012525\n",
      "48 Train Loss 19227.842 Test MSE 1746.500570734115 Test RE 0.07408388145805067\n",
      "49 Train Loss 19115.752 Test MSE 1743.7659462680494 Test RE 0.07402585944435634\n",
      "50 Train Loss 19032.184 Test MSE 1751.3278811374987 Test RE 0.07418619436879055\n",
      "51 Train Loss 18886.043 Test MSE 1774.0420879537792 Test RE 0.07466573106127126\n",
      "52 Train Loss 18797.887 Test MSE 1858.2834108948427 Test RE 0.07641794192439229\n",
      "53 Train Loss 18672.453 Test MSE 1856.5492495413016 Test RE 0.07638227675819108\n",
      "54 Train Loss 18566.564 Test MSE 1720.985463593144 Test RE 0.0735407345146279\n",
      "55 Train Loss 18469.734 Test MSE 1783.9837006490534 Test RE 0.07487464964946593\n",
      "56 Train Loss 18439.695 Test MSE 1794.8088084571054 Test RE 0.07510147355889492\n",
      "57 Train Loss 18356.246 Test MSE 1794.1735322558607 Test RE 0.07508818122348532\n",
      "58 Train Loss 18292.314 Test MSE 1781.5844881723865 Test RE 0.07482428466237562\n",
      "59 Train Loss 18136.195 Test MSE 1775.9253032261086 Test RE 0.07470535085090524\n",
      "60 Train Loss 18056.955 Test MSE 1836.4170614378418 Test RE 0.07596700797592124\n",
      "61 Train Loss 17956.354 Test MSE 1862.705602552369 Test RE 0.07650881449386152\n",
      "62 Train Loss 17809.79 Test MSE 1863.3248683231952 Test RE 0.07652153130555245\n",
      "63 Train Loss 17670.076 Test MSE 1864.5850086468874 Test RE 0.07654740214836057\n",
      "64 Train Loss 17509.074 Test MSE 1838.8579869159685 Test RE 0.07601747805506656\n",
      "65 Train Loss 17457.3 Test MSE 1760.8842188994406 Test RE 0.07438832209478531\n",
      "66 Train Loss 17390.49 Test MSE 1747.6979781796852 Test RE 0.07410927320232312\n",
      "67 Train Loss 17283.137 Test MSE 1843.639986843371 Test RE 0.07611625663147142\n",
      "68 Train Loss 17145.506 Test MSE 1961.907869717001 Test RE 0.07851970636543082\n",
      "69 Train Loss 16989.861 Test MSE 1907.7643956535233 Test RE 0.07742865796022305\n",
      "70 Train Loss 16929.377 Test MSE 1850.6424170602563 Test RE 0.07626067030092705\n",
      "71 Train Loss 16761.182 Test MSE 1786.5736080852887 Test RE 0.0749289797653668\n",
      "72 Train Loss 16667.686 Test MSE 1880.1936394692211 Test RE 0.07686712747542906\n",
      "73 Train Loss 16576.307 Test MSE 1934.1043601003335 Test RE 0.07796134341495549\n",
      "74 Train Loss 16490.875 Test MSE 1820.8229306679466 Test RE 0.07564377936703168\n",
      "75 Train Loss 16376.623 Test MSE 1851.789623602938 Test RE 0.07628430349584749\n",
      "76 Train Loss 16348.634 Test MSE 1852.9427680072902 Test RE 0.076308051638241\n",
      "77 Train Loss 16327.751 Test MSE 1814.9168957804661 Test RE 0.07552100035995012\n",
      "78 Train Loss 16307.121 Test MSE 1875.43033249806 Test RE 0.07676969763955299\n",
      "79 Train Loss 16282.506 Test MSE 1865.1277305095246 Test RE 0.07655854160456345\n",
      "80 Train Loss 16256.535 Test MSE 1882.8739299995775 Test RE 0.0769218965314436\n",
      "81 Train Loss 16240.614 Test MSE 1906.4990388062427 Test RE 0.07740297577063734\n",
      "82 Train Loss 16211.225 Test MSE 1962.5943949672483 Test RE 0.07853344326076202\n",
      "83 Train Loss 16189.923 Test MSE 1973.9080907951982 Test RE 0.07875947740421957\n",
      "84 Train Loss 16177.931 Test MSE 1930.114092635968 Test RE 0.07788088053104211\n",
      "85 Train Loss 16164.738 Test MSE 1920.5764915013892 Test RE 0.07768821920635399\n",
      "86 Train Loss 16146.164 Test MSE 1925.12291891137 Test RE 0.07778011741244403\n",
      "87 Train Loss 16132.549 Test MSE 1896.7057417404617 Test RE 0.0772039181550333\n",
      "88 Train Loss 16116.265 Test MSE 1913.9862165533457 Test RE 0.07755481481172856\n",
      "89 Train Loss 16084.734 Test MSE 1907.867886580578 Test RE 0.07743075807671047\n",
      "90 Train Loss 16062.35 Test MSE 1972.913208362799 Test RE 0.07873962686088809\n",
      "91 Train Loss 15992.351 Test MSE 1914.7514295239312 Test RE 0.07757031649592559\n",
      "92 Train Loss 15954.422 Test MSE 1919.9460344401114 Test RE 0.07767546701794156\n",
      "93 Train Loss 15909.917 Test MSE 1890.9638075251771 Test RE 0.07708696911523549\n",
      "94 Train Loss 15870.379 Test MSE 1837.5036678028862 Test RE 0.07598947945832865\n",
      "95 Train Loss 15786.69 Test MSE 1922.4651454027228 Test RE 0.0777264082878156\n",
      "96 Train Loss 15705.797 Test MSE 1892.7065707772829 Test RE 0.07712248364937684\n",
      "97 Train Loss 15676.881 Test MSE 1954.4808805971893 Test RE 0.07837094352909398\n",
      "98 Train Loss 15651.678 Test MSE 2000.317038165654 Test RE 0.07928458901377274\n",
      "99 Train Loss 15620.791 Test MSE 1923.1611633767413 Test RE 0.0777404772246908\n",
      "Training time: 579.11\n",
      "3D_HTTP_stan\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 543061.75 Test MSE 258055.72241601412 Test RE 0.9005256950485192\n",
      "1 Train Loss 255688.08 Test MSE 88619.80209034574 Test RE 0.5277214537020575\n",
      "2 Train Loss 188427.81 Test MSE 51028.368660034954 Test RE 0.4004471475317099\n",
      "3 Train Loss 176428.25 Test MSE 44724.137992646014 Test RE 0.374895607151205\n",
      "4 Train Loss 161245.55 Test MSE 38702.606588677416 Test RE 0.348746188544583\n",
      "5 Train Loss 150216.33 Test MSE 34669.78596459067 Test RE 0.3300767560540573\n",
      "6 Train Loss 129164.836 Test MSE 27022.05262074325 Test RE 0.291406027045701\n",
      "7 Train Loss 100668.64 Test MSE 16676.84485705859 Test RE 0.2289266254491055\n",
      "8 Train Loss 79639.28 Test MSE 9732.755317623716 Test RE 0.17488689405180138\n",
      "9 Train Loss 63782.098 Test MSE 6860.680575092779 Test RE 0.14683275991068032\n",
      "10 Train Loss 53220.367 Test MSE 5705.980858537224 Test RE 0.13390738141643985\n",
      "11 Train Loss 46225.48 Test MSE 5140.246527773637 Test RE 0.1270958401734508\n",
      "12 Train Loss 41372.78 Test MSE 3827.7555426023687 Test RE 0.10967596371656609\n",
      "13 Train Loss 38008.086 Test MSE 3893.0075773832473 Test RE 0.11060684047452084\n",
      "14 Train Loss 35437.902 Test MSE 3760.2530365842836 Test RE 0.10870459379764125\n",
      "15 Train Loss 33518.516 Test MSE 3746.659126874124 Test RE 0.10850792373492892\n",
      "16 Train Loss 32205.87 Test MSE 3398.4844003116687 Test RE 0.10334322316130662\n",
      "17 Train Loss 31114.14 Test MSE 3044.0756646946315 Test RE 0.09780635718055475\n",
      "18 Train Loss 29857.729 Test MSE 3002.209961176523 Test RE 0.09713145468958845\n",
      "19 Train Loss 29264.83 Test MSE 3266.5488209398172 Test RE 0.10131737725079552\n",
      "20 Train Loss 28514.895 Test MSE 3241.8733831261015 Test RE 0.10093397721179015\n",
      "21 Train Loss 27904.143 Test MSE 2930.5603546338966 Test RE 0.09596540428248544\n",
      "22 Train Loss 27323.412 Test MSE 2810.623070649383 Test RE 0.09398113066171392\n",
      "23 Train Loss 26377.486 Test MSE 2753.863652311222 Test RE 0.09302733488765999\n",
      "24 Train Loss 26083.68 Test MSE 2693.6973711239307 Test RE 0.09200549440598825\n",
      "25 Train Loss 25682.59 Test MSE 2831.2866060722095 Test RE 0.09432596988409393\n",
      "26 Train Loss 25076.742 Test MSE 2770.572568227482 Test RE 0.09330912720758748\n",
      "27 Train Loss 24881.082 Test MSE 2674.0197797948676 Test RE 0.09166882611123497\n",
      "28 Train Loss 24673.303 Test MSE 2612.825375439532 Test RE 0.09061384409994279\n",
      "29 Train Loss 24220.568 Test MSE 2619.6267492341603 Test RE 0.09073170465123376\n",
      "30 Train Loss 23811.936 Test MSE 2615.657160850723 Test RE 0.09066293453595468\n",
      "31 Train Loss 23609.584 Test MSE 2585.870022875278 Test RE 0.0901452210411457\n",
      "32 Train Loss 23203.814 Test MSE 2432.5015060278056 Test RE 0.08743109570946109\n",
      "33 Train Loss 22933.219 Test MSE 2446.7677870316693 Test RE 0.0876871064633751\n",
      "34 Train Loss 22601.248 Test MSE 2257.146800773833 Test RE 0.08422078112515154\n",
      "35 Train Loss 22231.146 Test MSE 2221.2661980189987 Test RE 0.08354869410444558\n",
      "36 Train Loss 21887.777 Test MSE 2245.463931306285 Test RE 0.08400253725566703\n",
      "37 Train Loss 21626.975 Test MSE 2109.207881763147 Test RE 0.0814139929005259\n",
      "38 Train Loss 21488.61 Test MSE 2155.1937235914324 Test RE 0.08229671859776386\n",
      "39 Train Loss 21428.986 Test MSE 2192.797250794071 Test RE 0.08301156492725012\n",
      "40 Train Loss 21356.617 Test MSE 2147.6159783494363 Test RE 0.08215191196206036\n",
      "41 Train Loss 21098.045 Test MSE 2067.537629530006 Test RE 0.08060575932101582\n",
      "42 Train Loss 20829.652 Test MSE 2070.67834269246 Test RE 0.08066695857018073\n",
      "43 Train Loss 20583.045 Test MSE 1916.0392556232753 Test RE 0.07759639828518458\n",
      "44 Train Loss 20389.457 Test MSE 1902.3864741289178 Test RE 0.07731944659231299\n",
      "45 Train Loss 20160.52 Test MSE 1757.3609586962832 Test RE 0.07431386500000936\n",
      "46 Train Loss 19802.229 Test MSE 1833.6284323950008 Test RE 0.07590930749699125\n",
      "47 Train Loss 19699.111 Test MSE 1815.3815390102363 Test RE 0.07553066693917275\n",
      "48 Train Loss 19593.076 Test MSE 1791.1441576461555 Test RE 0.07502476307444811\n",
      "49 Train Loss 19531.125 Test MSE 1804.6653644182736 Test RE 0.07530740868620202\n",
      "50 Train Loss 19373.346 Test MSE 1710.9621615409403 Test RE 0.07332626512306788\n",
      "51 Train Loss 19295.639 Test MSE 1713.673287002518 Test RE 0.07338433713066532\n",
      "52 Train Loss 19086.72 Test MSE 1632.4030878198475 Test RE 0.07162309204010929\n",
      "53 Train Loss 18997.58 Test MSE 1695.5732039911063 Test RE 0.07299575997933809\n",
      "54 Train Loss 18876.969 Test MSE 1696.290687271209 Test RE 0.07301120245563668\n",
      "55 Train Loss 18775.572 Test MSE 1736.1390140147275 Test RE 0.07386379385146312\n",
      "56 Train Loss 18601.668 Test MSE 1766.8696428062476 Test RE 0.07451464156530802\n",
      "57 Train Loss 18463.42 Test MSE 1772.4164446992056 Test RE 0.07463151325375125\n",
      "58 Train Loss 18419.486 Test MSE 1805.6148593348512 Test RE 0.07532721695660172\n",
      "59 Train Loss 18366.512 Test MSE 1767.6342281052164 Test RE 0.07453076234635748\n",
      "60 Train Loss 18318.14 Test MSE 1746.1918627531982 Test RE 0.07407733370863605\n",
      "61 Train Loss 18229.2 Test MSE 1706.5669917552102 Test RE 0.07323202323100511\n",
      "62 Train Loss 18136.734 Test MSE 1697.2506550295286 Test RE 0.07303185884653333\n",
      "63 Train Loss 18112.799 Test MSE 1647.6027452373694 Test RE 0.07195576846193906\n",
      "64 Train Loss 18053.889 Test MSE 1613.332171697373 Test RE 0.07120348646686243\n",
      "65 Train Loss 17936.533 Test MSE 1661.9083089595504 Test RE 0.07226747681239523\n",
      "66 Train Loss 17871.848 Test MSE 1655.0986259999097 Test RE 0.07211926654315295\n",
      "67 Train Loss 17790.016 Test MSE 1672.3982928603107 Test RE 0.0724951946007564\n",
      "68 Train Loss 17747.209 Test MSE 1642.7831643209101 Test RE 0.07185044858607123\n",
      "69 Train Loss 17692.281 Test MSE 1677.8856801461425 Test RE 0.07261403095310909\n",
      "70 Train Loss 17631.059 Test MSE 1685.9688610115222 Test RE 0.07278872912901353\n",
      "71 Train Loss 17590.256 Test MSE 1658.5346781957526 Test RE 0.072194088997838\n",
      "72 Train Loss 17469.568 Test MSE 1686.3218769760679 Test RE 0.07279634915067927\n",
      "73 Train Loss 17384.717 Test MSE 1689.4773503577558 Test RE 0.07286442618500102\n",
      "74 Train Loss 17360.28 Test MSE 1686.0738946290335 Test RE 0.07279099641420561\n",
      "75 Train Loss 17326.943 Test MSE 1675.27269363922 Test RE 0.07255746768015557\n",
      "76 Train Loss 17297.096 Test MSE 1664.0855770059995 Test RE 0.07231480017140873\n",
      "77 Train Loss 17269.559 Test MSE 1664.423716928164 Test RE 0.07232214693265274\n",
      "78 Train Loss 17193.656 Test MSE 1691.6166530810424 Test RE 0.07291054392402818\n",
      "79 Train Loss 17129.268 Test MSE 1672.563820571928 Test RE 0.07249878216324183\n",
      "80 Train Loss 17106.693 Test MSE 1685.5482651560155 Test RE 0.07277964932093073\n",
      "81 Train Loss 17069.943 Test MSE 1743.0320982029766 Test RE 0.07401028124980266\n",
      "82 Train Loss 17014.807 Test MSE 1710.3760084758785 Test RE 0.07331370374158934\n",
      "83 Train Loss 16979.447 Test MSE 1705.2976134285923 Test RE 0.07320478244989373\n",
      "84 Train Loss 16932.188 Test MSE 1693.555144470936 Test RE 0.07295230752152373\n",
      "85 Train Loss 16892.303 Test MSE 1720.4962653528364 Test RE 0.07353028162132147\n",
      "86 Train Loss 16871.47 Test MSE 1739.1604524828606 Test RE 0.07392803925225545\n",
      "87 Train Loss 16853.719 Test MSE 1714.2800794851714 Test RE 0.07339732826594776\n",
      "88 Train Loss 16829.03 Test MSE 1706.8914207063283 Test RE 0.07323898383071889\n",
      "89 Train Loss 16809.7 Test MSE 1733.0847540901068 Test RE 0.07379879371870748\n",
      "90 Train Loss 16777.148 Test MSE 1656.0364161546663 Test RE 0.07213969528361919\n",
      "91 Train Loss 16732.014 Test MSE 1625.845202826444 Test RE 0.07147908083244497\n",
      "92 Train Loss 16708.838 Test MSE 1641.2377554198504 Test RE 0.07181664484019258\n",
      "93 Train Loss 16661.902 Test MSE 1618.0268966682613 Test RE 0.07130701070056251\n",
      "94 Train Loss 16617.31 Test MSE 1637.7873429368117 Test RE 0.07174111421658565\n",
      "95 Train Loss 16592.762 Test MSE 1668.9187150580105 Test RE 0.07241973888654275\n",
      "96 Train Loss 16573.354 Test MSE 1681.3168633795349 Test RE 0.07268823886935817\n",
      "97 Train Loss 16547.271 Test MSE 1743.0507658643376 Test RE 0.07401067756928807\n",
      "98 Train Loss 16521.102 Test MSE 1740.017618923641 Test RE 0.07394625517595352\n",
      "99 Train Loss 16479.758 Test MSE 1712.9915751020878 Test RE 0.07336973926286527\n",
      "Training time: 565.83\n",
      "3D_HTTP_stan\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 921761.6 Test MSE 306249.1724455429 Test RE 0.9810176568682071\n",
      "1 Train Loss 584566.7 Test MSE 291492.25791941345 Test RE 0.9570902110435848\n",
      "2 Train Loss 257645.5 Test MSE 86539.34064003776 Test RE 0.5214902020492711\n",
      "3 Train Loss 222401.27 Test MSE 69957.58075267103 Test RE 0.46887465049477367\n",
      "4 Train Loss 199835.53 Test MSE 56170.23269661579 Test RE 0.4201384933772961\n",
      "5 Train Loss 188118.88 Test MSE 50117.234652578634 Test RE 0.3968559646537072\n",
      "6 Train Loss 181005.95 Test MSE 47291.905062166144 Test RE 0.38550744241396306\n",
      "7 Train Loss 176905.3 Test MSE 45072.779623880495 Test RE 0.37635399726947955\n",
      "8 Train Loss 172224.3 Test MSE 43816.785138467945 Test RE 0.37107322369642126\n",
      "9 Train Loss 166294.36 Test MSE 41225.80966121324 Test RE 0.35993490029517783\n",
      "10 Train Loss 163763.97 Test MSE 39821.48321785672 Test RE 0.35375132744102744\n",
      "11 Train Loss 161335.33 Test MSE 38997.10287516984 Test RE 0.35007051565502106\n",
      "12 Train Loss 158634.1 Test MSE 37918.73074487383 Test RE 0.3451964001614787\n",
      "13 Train Loss 156056.67 Test MSE 37241.51163893555 Test RE 0.34209995147119737\n",
      "14 Train Loss 152501.88 Test MSE 36077.913625244306 Test RE 0.3367131436271842\n",
      "15 Train Loss 148866.66 Test MSE 34517.74320121703 Test RE 0.3293521925247233\n",
      "16 Train Loss 145656.48 Test MSE 32955.58394788353 Test RE 0.3218132104261917\n",
      "17 Train Loss 142473.6 Test MSE 31472.765416752973 Test RE 0.3144899812109988\n",
      "18 Train Loss 138799.61 Test MSE 29615.68753977322 Test RE 0.305070540784346\n",
      "19 Train Loss 134224.88 Test MSE 29921.801287650207 Test RE 0.3066431230640347\n",
      "20 Train Loss 129239.24 Test MSE 27656.761143948206 Test RE 0.29480851399275526\n",
      "21 Train Loss 123623.96 Test MSE 25973.052696229934 Test RE 0.2856938295438088\n",
      "22 Train Loss 118589.6 Test MSE 23960.685773756155 Test RE 0.27440308147525644\n",
      "23 Train Loss 114390.63 Test MSE 21927.471184984828 Test RE 0.2625026165523882\n",
      "24 Train Loss 109832.0 Test MSE 21085.585132320422 Test RE 0.2574140163868956\n",
      "25 Train Loss 102363.37 Test MSE 17456.830003811327 Test RE 0.23421896286845026\n",
      "26 Train Loss 92415.39 Test MSE 15853.708629283989 Test RE 0.2232054519187659\n",
      "27 Train Loss 87805.33 Test MSE 14826.186384029139 Test RE 0.21585101238698357\n",
      "28 Train Loss 81850.42 Test MSE 13165.82357349285 Test RE 0.20340582123159276\n",
      "29 Train Loss 74671.69 Test MSE 11647.632382461143 Test RE 0.19131903858336272\n",
      "30 Train Loss 69403.086 Test MSE 10561.693362544424 Test RE 0.18218228272545098\n",
      "31 Train Loss 64388.35 Test MSE 8984.490680718565 Test RE 0.16802971604673914\n",
      "32 Train Loss 58481.78 Test MSE 7883.040053127187 Test RE 0.15739329552536838\n",
      "33 Train Loss 54469.523 Test MSE 7369.912277502515 Test RE 0.15218453494412865\n",
      "34 Train Loss 50886.9 Test MSE 6444.935189703995 Test RE 0.14231433226048337\n",
      "35 Train Loss 47585.67 Test MSE 5616.735998141538 Test RE 0.1328560597181632\n",
      "36 Train Loss 44781.594 Test MSE 4922.6835328030975 Test RE 0.12437706969254059\n",
      "37 Train Loss 42445.77 Test MSE 4551.720696762669 Test RE 0.11959889432792672\n",
      "38 Train Loss 40460.016 Test MSE 4239.419880761725 Test RE 0.11542305853231194\n",
      "39 Train Loss 39212.43 Test MSE 4133.882500983217 Test RE 0.11397731616205935\n",
      "40 Train Loss 37957.008 Test MSE 3990.2389986022054 Test RE 0.1119795754582044\n",
      "41 Train Loss 36595.695 Test MSE 3809.5139765941944 Test RE 0.10941431552669173\n",
      "42 Train Loss 35332.53 Test MSE 3394.328650918411 Test RE 0.10328001852979668\n",
      "43 Train Loss 33807.234 Test MSE 3375.5662840472914 Test RE 0.10299417955818727\n",
      "44 Train Loss 31611.828 Test MSE 2988.160006103092 Test RE 0.09690390681892679\n",
      "45 Train Loss 30974.996 Test MSE 2956.010148426628 Test RE 0.09638119852553571\n",
      "46 Train Loss 30255.82 Test MSE 2694.4356543664276 Test RE 0.09201810188406645\n",
      "47 Train Loss 29592.57 Test MSE 2614.1899429777313 Test RE 0.09063750288928082\n",
      "48 Train Loss 28694.875 Test MSE 2373.9333227634156 Test RE 0.08637212816658879\n",
      "49 Train Loss 28062.512 Test MSE 2196.826240636027 Test RE 0.08308779159708193\n",
      "50 Train Loss 27879.27 Test MSE 2131.226404437523 Test RE 0.08183783962980856\n",
      "51 Train Loss 27052.74 Test MSE 2129.752966168498 Test RE 0.08180954515738718\n",
      "52 Train Loss 26646.703 Test MSE 2064.518843323154 Test RE 0.08054689208754741\n",
      "53 Train Loss 26143.395 Test MSE 2114.1955483993706 Test RE 0.08151019633451567\n",
      "54 Train Loss 25904.4 Test MSE 2017.0166125547307 Test RE 0.07961485340715126\n",
      "55 Train Loss 25526.05 Test MSE 2184.397203518642 Test RE 0.08285241429274624\n",
      "56 Train Loss 25244.424 Test MSE 2098.0658064381396 Test RE 0.08119866988511817\n",
      "57 Train Loss 24916.676 Test MSE 2000.6786323916199 Test RE 0.07929175476639085\n",
      "58 Train Loss 24171.723 Test MSE 1993.4904733360322 Test RE 0.07914918448903133\n",
      "59 Train Loss 23846.008 Test MSE 1951.894627816609 Test RE 0.07831907447064393\n",
      "60 Train Loss 23500.49 Test MSE 1986.0964644098456 Test RE 0.07900226293141613\n",
      "61 Train Loss 22975.096 Test MSE 1941.024814236379 Test RE 0.07810069632436009\n",
      "62 Train Loss 22703.883 Test MSE 1870.96154442718 Test RE 0.07667817941229375\n",
      "63 Train Loss 22488.965 Test MSE 1843.704098947037 Test RE 0.07611758008153391\n",
      "64 Train Loss 22358.506 Test MSE 1791.2774821928317 Test RE 0.07502755527192505\n",
      "65 Train Loss 22265.541 Test MSE 1848.2483593778657 Test RE 0.07621132756304606\n",
      "66 Train Loss 22075.572 Test MSE 1917.1154594584193 Test RE 0.0776181874573097\n",
      "67 Train Loss 21866.139 Test MSE 1924.391704808844 Test RE 0.07776534450631398\n",
      "68 Train Loss 21734.531 Test MSE 1964.192987737694 Test RE 0.07856542068837444\n",
      "69 Train Loss 21576.035 Test MSE 2036.2371365048161 Test RE 0.07999328632898894\n",
      "70 Train Loss 21454.328 Test MSE 2126.2667580684324 Test RE 0.08174256041485604\n",
      "71 Train Loss 20986.977 Test MSE 2043.5703482261395 Test RE 0.0801371989651308\n",
      "72 Train Loss 20775.84 Test MSE 2009.87711887902 Test RE 0.07947382491458659\n",
      "73 Train Loss 20662.8 Test MSE 2010.1927956820239 Test RE 0.079480065857805\n",
      "74 Train Loss 20429.934 Test MSE 1980.2971246119505 Test RE 0.07888683653625198\n",
      "75 Train Loss 19893.25 Test MSE 2114.641346149479 Test RE 0.08151878947219032\n",
      "76 Train Loss 19631.672 Test MSE 2249.884336632649 Test RE 0.08408518002059535\n",
      "77 Train Loss 19540.014 Test MSE 2217.0928544366784 Test RE 0.0834701710225211\n",
      "78 Train Loss 19479.979 Test MSE 2165.32976425605 Test RE 0.08249001546797183\n",
      "79 Train Loss 19389.045 Test MSE 2215.323212160361 Test RE 0.08343685220363684\n",
      "80 Train Loss 19185.785 Test MSE 2110.648374487971 Test RE 0.08144178917625965\n",
      "81 Train Loss 19038.385 Test MSE 2098.9520890558083 Test RE 0.08121581838710248\n",
      "82 Train Loss 18915.428 Test MSE 2022.4560932887669 Test RE 0.07972213360522235\n",
      "83 Train Loss 18752.158 Test MSE 1994.217831267473 Test RE 0.0791636226159025\n",
      "84 Train Loss 18667.494 Test MSE 1992.6988447870808 Test RE 0.07913346759033117\n",
      "85 Train Loss 18592.387 Test MSE 1942.383834713192 Test RE 0.07812803288197628\n",
      "86 Train Loss 18547.203 Test MSE 1899.9916182261645 Test RE 0.07727076372787706\n",
      "87 Train Loss 18486.89 Test MSE 1858.7573618112858 Test RE 0.0764276864134024\n",
      "88 Train Loss 18350.488 Test MSE 1867.395195705506 Test RE 0.076605064182498\n",
      "89 Train Loss 18297.986 Test MSE 1870.8683786241695 Test RE 0.07667627026747081\n",
      "90 Train Loss 18240.924 Test MSE 1820.7127089556705 Test RE 0.07564148982175926\n",
      "91 Train Loss 18216.918 Test MSE 1798.0381154969052 Test RE 0.07516900630197704\n",
      "92 Train Loss 18087.527 Test MSE 1816.219630596671 Test RE 0.07554809972418988\n",
      "93 Train Loss 18053.184 Test MSE 1855.3754694451227 Test RE 0.07635812706982854\n",
      "94 Train Loss 18019.875 Test MSE 1816.9732514188217 Test RE 0.07556377203473803\n",
      "95 Train Loss 17944.11 Test MSE 1843.9386858583778 Test RE 0.07612242240413482\n",
      "96 Train Loss 17873.79 Test MSE 1853.0158683235304 Test RE 0.07630955683521418\n",
      "97 Train Loss 17800.795 Test MSE 1907.4131042658448 Test RE 0.07742152886372254\n",
      "98 Train Loss 17760.766 Test MSE 1950.5652111027532 Test RE 0.07829239874242311\n",
      "99 Train Loss 17643.7 Test MSE 1965.0782629400173 Test RE 0.07858312367988358\n",
      "Training time: 371.61\n",
      "3D_HTTP_stan\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 366348.6 Test MSE 155559.61226681023 Test RE 0.6991780541922451\n",
      "1 Train Loss 194151.6 Test MSE 52825.38485337324 Test RE 0.4074372173217654\n",
      "2 Train Loss 176310.25 Test MSE 45155.21729128546 Test RE 0.3766980139068545\n",
      "3 Train Loss 159356.6 Test MSE 36408.21608386236 Test RE 0.3382509789853798\n",
      "4 Train Loss 135490.58 Test MSE 30180.578485063656 Test RE 0.3079662622805219\n",
      "5 Train Loss 109235.52 Test MSE 20172.07607921749 Test RE 0.25177619209795243\n",
      "6 Train Loss 93428.1 Test MSE 16768.059060513482 Test RE 0.22955183018883218\n",
      "7 Train Loss 77838.64 Test MSE 11273.670930692646 Test RE 0.18822271743628224\n",
      "8 Train Loss 63510.832 Test MSE 10103.198836379112 Test RE 0.17818404441598876\n",
      "9 Train Loss 54884.547 Test MSE 8217.383628287664 Test RE 0.1606963984626599\n",
      "10 Train Loss 48036.875 Test MSE 6285.739644477597 Test RE 0.1405456979259956\n",
      "11 Train Loss 43518.8 Test MSE 4863.433337353254 Test RE 0.12362629272170357\n",
      "12 Train Loss 36344.105 Test MSE 3666.5957655445327 Test RE 0.10734229552473823\n",
      "13 Train Loss 30225.875 Test MSE 2983.417307661486 Test RE 0.09682697510989687\n",
      "14 Train Loss 26833.63 Test MSE 2611.017996916795 Test RE 0.09058249836805225\n",
      "15 Train Loss 25532.184 Test MSE 2473.7211403965225 Test RE 0.08816875990620004\n",
      "16 Train Loss 24122.74 Test MSE 2242.1994000385644 Test RE 0.08394145218345898\n",
      "17 Train Loss 23212.53 Test MSE 1912.1632519668312 Test RE 0.0775178727063495\n",
      "18 Train Loss 22489.633 Test MSE 1751.5879866752987 Test RE 0.07419170319548712\n",
      "19 Train Loss 21941.13 Test MSE 1783.4926876729176 Test RE 0.07486434491557142\n",
      "20 Train Loss 21562.553 Test MSE 1944.298436252298 Test RE 0.07816652867419713\n",
      "21 Train Loss 21309.74 Test MSE 2124.0203579660197 Test RE 0.08169936851022784\n",
      "22 Train Loss 20967.748 Test MSE 1907.814063567404 Test RE 0.07742966586636024\n",
      "23 Train Loss 20781.309 Test MSE 1989.4106615684111 Test RE 0.07906815095412274\n",
      "24 Train Loss 20352.752 Test MSE 1818.7469515166135 Test RE 0.0756006451083599\n",
      "25 Train Loss 19995.748 Test MSE 1871.5846066729719 Test RE 0.07669094592317575\n",
      "26 Train Loss 19246.541 Test MSE 1989.796767402014 Test RE 0.07907582337536131\n",
      "27 Train Loss 18905.828 Test MSE 1875.620331679798 Test RE 0.07677358629648405\n",
      "28 Train Loss 18722.268 Test MSE 1791.1045508211394 Test RE 0.07502393357418781\n",
      "29 Train Loss 18455.484 Test MSE 1815.8100698450032 Test RE 0.07553958112860584\n",
      "30 Train Loss 18245.51 Test MSE 1847.9897403859227 Test RE 0.07620599538294845\n",
      "31 Train Loss 18086.45 Test MSE 1824.8090564789584 Test RE 0.07572653337142561\n",
      "32 Train Loss 18024.957 Test MSE 1808.6495443157128 Test RE 0.0753904913598148\n",
      "33 Train Loss 17885.96 Test MSE 1806.7589833042741 Test RE 0.0753510786416735\n",
      "34 Train Loss 17817.23 Test MSE 1906.7552456785588 Test RE 0.07740817653594405\n",
      "35 Train Loss 17785.42 Test MSE 1912.6664062586049 Test RE 0.07752807081178317\n",
      "36 Train Loss 17707.008 Test MSE 1901.5589461765596 Test RE 0.07730262799002716\n",
      "37 Train Loss 17636.416 Test MSE 1868.0924308820795 Test RE 0.07661936398386772\n",
      "38 Train Loss 17565.164 Test MSE 1779.6526599965773 Test RE 0.07478370649503083\n",
      "39 Train Loss 17504.652 Test MSE 1808.1583556512867 Test RE 0.07538025348111313\n",
      "40 Train Loss 17448.932 Test MSE 1821.705015701213 Test RE 0.0756620997005051\n",
      "41 Train Loss 17415.883 Test MSE 1792.0917488186574 Test RE 0.07504460609004651\n",
      "42 Train Loss 17384.137 Test MSE 1721.6363749565041 Test RE 0.07355464049333198\n",
      "43 Train Loss 17348.797 Test MSE 1729.7635691394821 Test RE 0.0737280479058453\n",
      "44 Train Loss 17316.281 Test MSE 1739.1595111186514 Test RE 0.07392801924455016\n",
      "45 Train Loss 17298.262 Test MSE 1745.6801571614662 Test RE 0.07406647907001328\n",
      "46 Train Loss 17260.78 Test MSE 1734.7605004359305 Test RE 0.07383446369356432\n",
      "47 Train Loss 17184.746 Test MSE 1645.9240458127163 Test RE 0.07191910219298187\n",
      "48 Train Loss 17086.742 Test MSE 1576.5128563532521 Test RE 0.07038629746335333\n",
      "49 Train Loss 17070.924 Test MSE 1623.9772892561573 Test RE 0.0714380083134166\n",
      "50 Train Loss 17039.977 Test MSE 1606.096573689262 Test RE 0.07104363756377802\n",
      "51 Train Loss 17003.803 Test MSE 1622.8385085387586 Test RE 0.07141295670292241\n",
      "52 Train Loss 16937.11 Test MSE 1663.6865956234508 Test RE 0.07230613054859529\n",
      "53 Train Loss 16903.723 Test MSE 1654.2523399254535 Test RE 0.07210082615116127\n",
      "54 Train Loss 16880.115 Test MSE 1707.6107301332345 Test RE 0.07325441420368395\n",
      "55 Train Loss 16825.84 Test MSE 1732.1911508802768 Test RE 0.07377976541043581\n",
      "56 Train Loss 16774.287 Test MSE 1694.3784758087847 Test RE 0.07297003845364539\n",
      "57 Train Loss 16754.682 Test MSE 1753.4951226305343 Test RE 0.07423208231776292\n",
      "58 Train Loss 16700.086 Test MSE 1752.970888903458 Test RE 0.0742209850898129\n",
      "59 Train Loss 16652.049 Test MSE 1715.4112929159623 Test RE 0.07342154086456629\n",
      "60 Train Loss 16591.014 Test MSE 1691.7313226357103 Test RE 0.07291301507430525\n",
      "61 Train Loss 16568.27 Test MSE 1704.7574101513617 Test RE 0.0731931866453342\n",
      "62 Train Loss 16533.037 Test MSE 1707.5363196726687 Test RE 0.07325281812736917\n",
      "63 Train Loss 16486.197 Test MSE 1645.2541522407118 Test RE 0.07190446511190629\n",
      "64 Train Loss 16405.4 Test MSE 1774.5990889076204 Test RE 0.07467745164551519\n",
      "65 Train Loss 16357.698 Test MSE 1826.0956575094265 Test RE 0.07575322456634535\n",
      "66 Train Loss 16337.466 Test MSE 1830.1492098189797 Test RE 0.07583725615535997\n",
      "67 Train Loss 16314.777 Test MSE 1853.7167419009227 Test RE 0.0763239869047709\n",
      "68 Train Loss 16281.578 Test MSE 1824.8762928712324 Test RE 0.07572792845766367\n",
      "69 Train Loss 16247.196 Test MSE 1816.7684339059751 Test RE 0.07555951296817903\n",
      "70 Train Loss 16217.247 Test MSE 1803.982803327245 Test RE 0.07529316594367488\n",
      "71 Train Loss 16188.967 Test MSE 1704.8939831282569 Test RE 0.07319611844405549\n",
      "72 Train Loss 16149.819 Test MSE 1734.7964857032978 Test RE 0.07383522948778712\n",
      "73 Train Loss 16116.807 Test MSE 1758.3424666078522 Test RE 0.07433461471124381\n",
      "74 Train Loss 16074.79 Test MSE 1829.180387717859 Test RE 0.07581718059661118\n",
      "75 Train Loss 16038.545 Test MSE 1880.5494821352997 Test RE 0.07687440101070346\n",
      "76 Train Loss 16008.263 Test MSE 1914.6736410483002 Test RE 0.07756874079845709\n",
      "77 Train Loss 15980.541 Test MSE 1911.5465481543263 Test RE 0.07750537130963914\n",
      "78 Train Loss 15944.143 Test MSE 1867.1381984248517 Test RE 0.07659979267633017\n",
      "79 Train Loss 15921.443 Test MSE 1883.9898262209165 Test RE 0.0769446872605916\n",
      "80 Train Loss 15893.24 Test MSE 1895.7713989977744 Test RE 0.0771848999686868\n",
      "81 Train Loss 15834.727 Test MSE 1943.2411717701395 Test RE 0.07814527320981689\n",
      "82 Train Loss 15812.616 Test MSE 2000.378659949382 Test RE 0.07928581022523011\n",
      "83 Train Loss 15804.117 Test MSE 2030.976949185913 Test RE 0.07988989665910375\n",
      "84 Train Loss 15776.936 Test MSE 2019.0060032853098 Test RE 0.07965410593876414\n",
      "85 Train Loss 15743.205 Test MSE 1934.6663011656224 Test RE 0.07797266816546626\n",
      "86 Train Loss 15682.036 Test MSE 2036.0448289180445 Test RE 0.07998950885169921\n",
      "87 Train Loss 15652.839 Test MSE 2041.1633327520904 Test RE 0.0800899903356813\n",
      "88 Train Loss 15605.893 Test MSE 2051.616528218893 Test RE 0.08029480667373244\n",
      "89 Train Loss 15559.856 Test MSE 2179.414458166143 Test RE 0.08275786459853214\n",
      "90 Train Loss 15542.7 Test MSE 2198.636325497043 Test RE 0.08312201482872052\n",
      "91 Train Loss 15527.561 Test MSE 2213.362233028816 Test RE 0.08339991534735888\n",
      "92 Train Loss 15507.394 Test MSE 2262.10482820314 Test RE 0.08431322967286932\n",
      "93 Train Loss 15480.838 Test MSE 2314.0942049448854 Test RE 0.08527660058679891\n",
      "94 Train Loss 15452.824 Test MSE 2259.6089035016053 Test RE 0.08426670275027599\n",
      "95 Train Loss 15420.799 Test MSE 2236.146372140617 Test RE 0.08382807169388518\n",
      "96 Train Loss 15372.147 Test MSE 2387.54754949255 Test RE 0.086619441063818\n",
      "97 Train Loss 15315.026 Test MSE 2470.108722360966 Test RE 0.08810435919892338\n",
      "98 Train Loss 15271.068 Test MSE 2433.4022110743276 Test RE 0.08744728117541917\n",
      "99 Train Loss 15202.715 Test MSE 2403.137419070437 Test RE 0.08690177859625117\n",
      "Training time: 368.09\n",
      "3D_HTTP_stan\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 263430.53 Test MSE 84332.36735303699 Test RE 0.5147975935989565\n",
      "1 Train Loss 186479.77 Test MSE 49623.144776236215 Test RE 0.3948948808900373\n",
      "2 Train Loss 161633.45 Test MSE 38067.391533835595 Test RE 0.3458724113196181\n",
      "3 Train Loss 141296.72 Test MSE 26788.712444165536 Test RE 0.2901451279769697\n",
      "4 Train Loss 121403.5 Test MSE 22115.41499973592 Test RE 0.26362519191127237\n",
      "5 Train Loss 94094.34 Test MSE 15401.980265434808 Test RE 0.22000251395165077\n",
      "6 Train Loss 69487.625 Test MSE 10000.160201238354 Test RE 0.1772731006390364\n",
      "7 Train Loss 63306.414 Test MSE 8608.075896215725 Test RE 0.16447216375494647\n",
      "8 Train Loss 56801.24 Test MSE 7080.1698370378235 Test RE 0.149163031106798\n",
      "9 Train Loss 50547.473 Test MSE 5363.730260144722 Test RE 0.12982933327280563\n",
      "10 Train Loss 46735.273 Test MSE 5180.457935759845 Test RE 0.12759199794401874\n",
      "11 Train Loss 44117.367 Test MSE 5304.985632768999 Test RE 0.12911641772106522\n",
      "12 Train Loss 40446.37 Test MSE 5092.597666786266 Test RE 0.126505394582064\n",
      "13 Train Loss 37418.105 Test MSE 4998.53886681776 Test RE 0.12533169088273788\n",
      "14 Train Loss 34321.41 Test MSE 4159.239139506741 Test RE 0.1143263419640152\n",
      "15 Train Loss 32087.137 Test MSE 3688.5305370198316 Test RE 0.10766289506399597\n",
      "16 Train Loss 30610.828 Test MSE 3812.2694249980764 Test RE 0.10945387844810249\n",
      "17 Train Loss 29694.143 Test MSE 3805.762288409706 Test RE 0.10936042551454585\n",
      "18 Train Loss 28260.662 Test MSE 3433.498585060867 Test RE 0.10387422548449234\n",
      "19 Train Loss 26911.576 Test MSE 3152.3003477767934 Test RE 0.09952980590688662\n",
      "20 Train Loss 26069.074 Test MSE 3248.9315971036613 Test RE 0.10104379431120213\n",
      "21 Train Loss 25716.396 Test MSE 3256.7574262556655 Test RE 0.10116541519511905\n",
      "22 Train Loss 24465.373 Test MSE 2871.7755303543636 Test RE 0.09499803180271699\n",
      "23 Train Loss 24207.027 Test MSE 2830.8978150384764 Test RE 0.09431949326135816\n",
      "24 Train Loss 23881.422 Test MSE 2782.6880332568494 Test RE 0.09351292087459123\n",
      "25 Train Loss 23032.402 Test MSE 2465.059741545921 Test RE 0.08801426908198591\n",
      "26 Train Loss 22686.506 Test MSE 2271.5899700749396 Test RE 0.08448980999012615\n",
      "27 Train Loss 22423.307 Test MSE 2296.7856779784574 Test RE 0.08495708405458385\n",
      "28 Train Loss 22131.332 Test MSE 2210.7496290494223 Test RE 0.08335067910309216\n",
      "29 Train Loss 21755.312 Test MSE 2176.1453307879074 Test RE 0.08269577279882902\n",
      "30 Train Loss 21449.953 Test MSE 2199.9181449696157 Test RE 0.08314624163945658\n",
      "31 Train Loss 20920.758 Test MSE 2012.227879837396 Test RE 0.07952028779718315\n",
      "32 Train Loss 20675.746 Test MSE 1985.49642396095 Test RE 0.07899032792846548\n",
      "33 Train Loss 20578.07 Test MSE 1949.5452432011534 Test RE 0.07827192616984502\n",
      "34 Train Loss 19973.143 Test MSE 1728.70834331502 Test RE 0.07370555592763679\n",
      "35 Train Loss 19735.611 Test MSE 1722.6841782680092 Test RE 0.07357702009511065\n",
      "36 Train Loss 19663.049 Test MSE 1754.8243922405052 Test RE 0.07426021349315666\n",
      "37 Train Loss 19517.305 Test MSE 1736.254075043535 Test RE 0.07386624143867677\n",
      "38 Train Loss 19420.186 Test MSE 1787.045869600024 Test RE 0.07493888244588007\n",
      "39 Train Loss 19278.816 Test MSE 1734.7892928283368 Test RE 0.07383507641849966\n",
      "40 Train Loss 19180.262 Test MSE 1708.352495091984 Test RE 0.07327032288151138\n",
      "41 Train Loss 19097.982 Test MSE 1680.9885037608185 Test RE 0.07268114055006517\n",
      "42 Train Loss 18991.146 Test MSE 1667.5510794232644 Test RE 0.07239005975539493\n",
      "43 Train Loss 18947.184 Test MSE 1709.4546772712458 Test RE 0.07329395507187933\n",
      "44 Train Loss 18808.266 Test MSE 1720.4877093136245 Test RE 0.0735300987878078\n",
      "45 Train Loss 18743.744 Test MSE 1734.769768843271 Test RE 0.07383466093317495\n",
      "46 Train Loss 18617.393 Test MSE 1821.233894690341 Test RE 0.07565231537495896\n",
      "47 Train Loss 18465.107 Test MSE 1802.9671929468134 Test RE 0.07527196860010725\n",
      "48 Train Loss 18416.508 Test MSE 1778.5972942262968 Test RE 0.07476152917073776\n",
      "49 Train Loss 18383.838 Test MSE 1837.4013400705508 Test RE 0.07598736356068185\n",
      "50 Train Loss 18196.158 Test MSE 1730.468391550958 Test RE 0.07374306726715955\n",
      "51 Train Loss 18031.545 Test MSE 1568.3336702823124 Test RE 0.07020347266976905\n",
      "52 Train Loss 17985.975 Test MSE 1535.3771901040661 Test RE 0.06946193933417054\n",
      "53 Train Loss 17957.938 Test MSE 1545.3763954423662 Test RE 0.0696877590943605\n",
      "54 Train Loss 17928.717 Test MSE 1572.6876803656637 Test RE 0.07030085461712612\n",
      "55 Train Loss 17872.838 Test MSE 1637.2852655727734 Test RE 0.07173011695536491\n",
      "56 Train Loss 17839.846 Test MSE 1634.17513867438 Test RE 0.07166195662469954\n",
      "57 Train Loss 17748.14 Test MSE 1649.4175613637353 Test RE 0.07199538679507984\n",
      "58 Train Loss 17658.871 Test MSE 1719.2289373291947 Test RE 0.07350319520584633\n",
      "59 Train Loss 17603.277 Test MSE 1722.67353432062 Test RE 0.07357679278962119\n",
      "60 Train Loss 17590.668 Test MSE 1732.4765604720433 Test RE 0.07378584343057487\n",
      "61 Train Loss 17572.572 Test MSE 1702.3245906697405 Test RE 0.07314094185581396\n",
      "62 Train Loss 17544.787 Test MSE 1673.5794854056223 Test RE 0.07252079127505545\n",
      "63 Train Loss 17520.945 Test MSE 1647.9727831803457 Test RE 0.07196384834338393\n",
      "64 Train Loss 17482.004 Test MSE 1646.5502318484873 Test RE 0.07193278157724646\n",
      "65 Train Loss 17426.248 Test MSE 1562.4486997334116 Test RE 0.07007163412210705\n",
      "66 Train Loss 17400.047 Test MSE 1530.2630373377863 Test RE 0.06934615824809064\n",
      "67 Train Loss 17380.184 Test MSE 1540.7107332493667 Test RE 0.06958248204887975\n",
      "68 Train Loss 17362.105 Test MSE 1574.961765530567 Test RE 0.07035166330018286\n",
      "69 Train Loss 17340.705 Test MSE 1562.7947570303745 Test RE 0.07007939356335174\n",
      "70 Train Loss 17316.031 Test MSE 1539.459573507961 Test RE 0.06955422350561989\n",
      "71 Train Loss 17287.055 Test MSE 1517.5472884449073 Test RE 0.06905744063125159\n",
      "72 Train Loss 17258.44 Test MSE 1530.8713972628618 Test RE 0.069359941248304\n",
      "73 Train Loss 17238.318 Test MSE 1536.5876525096014 Test RE 0.06948931518072769\n",
      "74 Train Loss 17171.4 Test MSE 1526.7911598783608 Test RE 0.06926744691759447\n",
      "75 Train Loss 17116.393 Test MSE 1497.7219811960342 Test RE 0.06860487289768527\n",
      "76 Train Loss 17100.395 Test MSE 1506.237958099177 Test RE 0.06879963847710882\n",
      "77 Train Loss 17092.254 Test MSE 1491.720789220851 Test RE 0.06846728919879627\n",
      "78 Train Loss 17082.605 Test MSE 1495.4991496520056 Test RE 0.06855394432038332\n",
      "79 Train Loss 17068.541 Test MSE 1481.732130521676 Test RE 0.0682376734786422\n",
      "80 Train Loss 17052.584 Test MSE 1490.9857567737208 Test RE 0.06845041878966715\n",
      "81 Train Loss 17041.451 Test MSE 1512.346868825061 Test RE 0.06893901405235396\n",
      "82 Train Loss 17035.967 Test MSE 1496.6644519565214 Test RE 0.06858064795088936\n",
      "83 Train Loss 17027.615 Test MSE 1490.9269704202043 Test RE 0.06844906935015581\n",
      "84 Train Loss 17002.611 Test MSE 1484.4590476203018 Test RE 0.06830043547883698\n",
      "85 Train Loss 16989.652 Test MSE 1489.978679840369 Test RE 0.06842729768275466\n",
      "86 Train Loss 16980.076 Test MSE 1492.7829978085026 Test RE 0.0684916615884245\n",
      "87 Train Loss 16971.918 Test MSE 1502.146694852629 Test RE 0.06870613770289145\n",
      "88 Train Loss 16964.104 Test MSE 1491.9337475246812 Test RE 0.06847217622508601\n",
      "89 Train Loss 16955.895 Test MSE 1472.4954893426186 Test RE 0.06802465514127334\n",
      "90 Train Loss 16949.836 Test MSE 1463.5984369958085 Test RE 0.06781883586994075\n",
      "91 Train Loss 16943.574 Test MSE 1494.2493649683704 Test RE 0.06852529315823847\n",
      "92 Train Loss 16937.666 Test MSE 1498.6457159310983 Test RE 0.0686260260010194\n",
      "93 Train Loss 16923.676 Test MSE 1499.9418067499294 Test RE 0.06865569490096293\n",
      "94 Train Loss 16910.125 Test MSE 1499.3868957021402 Test RE 0.06864299396560221\n",
      "95 Train Loss 16885.375 Test MSE 1500.466941046349 Test RE 0.06866771213550497\n",
      "96 Train Loss 16869.629 Test MSE 1497.5315374218574 Test RE 0.06860051101128518\n",
      "97 Train Loss 16847.629 Test MSE 1498.381763813561 Test RE 0.06861998228356511\n",
      "98 Train Loss 16834.787 Test MSE 1520.246385873728 Test RE 0.06911882585341983\n",
      "99 Train Loss 16817.58 Test MSE 1513.1817124292613 Test RE 0.06895803923623102\n",
      "Training time: 372.97\n",
      "3D_HTTP_stan\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 445469.7 Test MSE 208156.34861319253 Test RE 0.8087870572045293\n",
      "1 Train Loss 224009.88 Test MSE 70117.2252443919 Test RE 0.4694093359354558\n",
      "2 Train Loss 187890.73 Test MSE 47007.029656913066 Test RE 0.3843445851167849\n",
      "3 Train Loss 168340.33 Test MSE 38460.07038040221 Test RE 0.3476517337166659\n",
      "4 Train Loss 153434.97 Test MSE 32313.005228569666 Test RE 0.31866035648271895\n",
      "5 Train Loss 126981.586 Test MSE 26024.97419597046 Test RE 0.28597924547773107\n",
      "6 Train Loss 91579.66 Test MSE 16074.154451217135 Test RE 0.2247519304606747\n",
      "7 Train Loss 73164.69 Test MSE 11782.851412820608 Test RE 0.19242635913840403\n",
      "8 Train Loss 63102.598 Test MSE 8383.34129522174 Test RE 0.1623109933917054\n",
      "9 Train Loss 54500.266 Test MSE 6605.342806761015 Test RE 0.1440744741231777\n",
      "10 Train Loss 50358.734 Test MSE 5220.86068157041 Test RE 0.1280885809632172\n",
      "11 Train Loss 47041.492 Test MSE 5422.618084266453 Test RE 0.13054007901919837\n",
      "12 Train Loss 41701.023 Test MSE 5481.342860897929 Test RE 0.13124502391260998\n",
      "13 Train Loss 37633.562 Test MSE 5645.705240589488 Test RE 0.13319823260429425\n",
      "14 Train Loss 35855.594 Test MSE 4777.437330242508 Test RE 0.12252842796841662\n",
      "15 Train Loss 34499.816 Test MSE 4358.248461270641 Test RE 0.11702950145731009\n",
      "16 Train Loss 33486.01 Test MSE 4628.501484520431 Test RE 0.12060340401537675\n",
      "17 Train Loss 31626.723 Test MSE 4874.529561324473 Test RE 0.12376724288755435\n",
      "18 Train Loss 30573.492 Test MSE 4744.620199750789 Test RE 0.12210686717558927\n",
      "19 Train Loss 28573.852 Test MSE 3919.14541399113 Test RE 0.1109775290837621\n",
      "20 Train Loss 27466.727 Test MSE 3351.638464710529 Test RE 0.10262849144042258\n",
      "21 Train Loss 26332.031 Test MSE 3698.570357577073 Test RE 0.1078093194460784\n",
      "22 Train Loss 25909.678 Test MSE 3588.262626254018 Test RE 0.10618947485309697\n",
      "23 Train Loss 25662.004 Test MSE 3632.4361810326222 Test RE 0.10684110194298163\n",
      "24 Train Loss 25231.566 Test MSE 3755.787879579833 Test RE 0.10864003334748333\n",
      "25 Train Loss 24769.793 Test MSE 3702.044848233319 Test RE 0.10785994637839408\n",
      "26 Train Loss 24367.578 Test MSE 3523.6141179560077 Test RE 0.10522853711785839\n",
      "27 Train Loss 24144.146 Test MSE 3484.435398771214 Test RE 0.10464188895591668\n",
      "28 Train Loss 24070.855 Test MSE 3527.4680617512404 Test RE 0.1052860681107289\n",
      "29 Train Loss 23912.252 Test MSE 3458.8276131712823 Test RE 0.10425666319693912\n",
      "30 Train Loss 23777.285 Test MSE 3343.2599655660065 Test RE 0.1025001346991316\n",
      "31 Train Loss 23750.822 Test MSE 3336.499632212444 Test RE 0.10239645061034668\n",
      "32 Train Loss 23437.602 Test MSE 3224.033495195871 Test RE 0.10065587645120254\n",
      "33 Train Loss 23311.332 Test MSE 3300.881721510318 Test RE 0.10184843114714844\n",
      "34 Train Loss 22855.684 Test MSE 3299.1063964160776 Test RE 0.10182103870940104\n",
      "35 Train Loss 22723.668 Test MSE 3243.700903333751 Test RE 0.10096242263093047\n",
      "36 Train Loss 22549.719 Test MSE 3314.5834915615956 Test RE 0.10205959572909211\n",
      "37 Train Loss 22451.176 Test MSE 3335.141442732181 Test RE 0.10237560721855204\n",
      "38 Train Loss 22345.557 Test MSE 3408.338015846507 Test RE 0.10349293215151449\n",
      "39 Train Loss 22218.236 Test MSE 3389.0973128547507 Test RE 0.10320040027784831\n",
      "40 Train Loss 22065.936 Test MSE 3407.116358618306 Test RE 0.1034743829026621\n",
      "41 Train Loss 21988.918 Test MSE 3377.6758335891436 Test RE 0.10302635747603357\n",
      "42 Train Loss 21793.736 Test MSE 3122.905586543324 Test RE 0.09906466824969279\n",
      "43 Train Loss 21616.758 Test MSE 2997.4519058571636 Test RE 0.09705445472983988\n",
      "44 Train Loss 21531.4 Test MSE 2923.9941246282956 Test RE 0.09585783368070486\n",
      "45 Train Loss 21450.334 Test MSE 2984.58136040802 Test RE 0.09684586296609243\n",
      "46 Train Loss 21398.547 Test MSE 3049.095215929233 Test RE 0.09788696322574456\n",
      "47 Train Loss 21321.67 Test MSE 3228.129654083332 Test RE 0.10071979817972876\n",
      "48 Train Loss 21256.451 Test MSE 3188.889991789003 Test RE 0.10010577471067977\n",
      "49 Train Loss 21214.7 Test MSE 3225.5572420726216 Test RE 0.10067965969661778\n",
      "50 Train Loss 21155.594 Test MSE 3282.8207531353837 Test RE 0.10156941412013572\n",
      "51 Train Loss 21099.99 Test MSE 3197.0719318114193 Test RE 0.10023411635012824\n",
      "52 Train Loss 20902.48 Test MSE 3105.809651076611 Test RE 0.098793137883739\n",
      "53 Train Loss 20481.057 Test MSE 3156.317649777608 Test RE 0.09959320627101043\n",
      "54 Train Loss 20373.863 Test MSE 3109.928161632827 Test RE 0.09885861932475147\n",
      "55 Train Loss 20218.033 Test MSE 3169.4936681514155 Test RE 0.09980086523104165\n",
      "56 Train Loss 20013.225 Test MSE 3334.58459647842 Test RE 0.10236706037667549\n",
      "57 Train Loss 19840.773 Test MSE 3351.423045935145 Test RE 0.10262519328350223\n",
      "58 Train Loss 19665.332 Test MSE 3455.8535743714406 Test RE 0.10421183152620053\n",
      "59 Train Loss 19510.473 Test MSE 3376.9113929521286 Test RE 0.10301469827680106\n",
      "60 Train Loss 19344.023 Test MSE 3541.2452987228526 Test RE 0.10549147571316791\n",
      "61 Train Loss 19225.32 Test MSE 3583.6673293184185 Test RE 0.10612145746566734\n",
      "62 Train Loss 19058.855 Test MSE 3558.8328961245816 Test RE 0.10575311300623816\n",
      "63 Train Loss 18966.99 Test MSE 3517.5883180040196 Test RE 0.10513852194942282\n",
      "64 Train Loss 18914.041 Test MSE 3504.217025141827 Test RE 0.1049385018820408\n",
      "65 Train Loss 18801.617 Test MSE 3376.8038502263676 Test RE 0.10301305793621153\n",
      "66 Train Loss 18762.38 Test MSE 3327.6110558535943 Test RE 0.102259965408096\n",
      "67 Train Loss 18724.744 Test MSE 3421.41085496185 Test RE 0.10369121817905283\n",
      "68 Train Loss 18684.588 Test MSE 3413.2642143534144 Test RE 0.10356769625069807\n",
      "69 Train Loss 18668.592 Test MSE 3407.791803634925 Test RE 0.10348463905263795\n",
      "70 Train Loss 18635.428 Test MSE 3468.6199040534825 Test RE 0.10440413947092257\n",
      "71 Train Loss 18518.51 Test MSE 3405.172163123756 Test RE 0.10344485600733667\n",
      "72 Train Loss 18446.145 Test MSE 3556.9722240214187 Test RE 0.1057254638303249\n",
      "73 Train Loss 18278.842 Test MSE 3582.0957319947274 Test RE 0.1060981854272283\n",
      "74 Train Loss 18087.484 Test MSE 3787.4830745114723 Test RE 0.10909747835907288\n",
      "75 Train Loss 17964.248 Test MSE 3843.1605748190254 Test RE 0.10989644086200064\n",
      "76 Train Loss 17878.113 Test MSE 3814.6097634647736 Test RE 0.10948746996430724\n",
      "77 Train Loss 17809.633 Test MSE 3759.3607883799427 Test RE 0.10869169609740775\n",
      "78 Train Loss 17737.459 Test MSE 3820.134601500964 Test RE 0.10956672861848758\n",
      "79 Train Loss 17610.33 Test MSE 3988.196453673981 Test RE 0.11195091143688643\n",
      "80 Train Loss 17487.27 Test MSE 4233.600506085794 Test RE 0.11534381175972815\n",
      "81 Train Loss 17374.996 Test MSE 4209.828588474871 Test RE 0.11501952472857922\n",
      "82 Train Loss 17279.38 Test MSE 4236.342400172999 Test RE 0.11538115696093441\n",
      "83 Train Loss 17192.145 Test MSE 4473.884690100983 Test RE 0.11857189353793886\n",
      "84 Train Loss 17120.932 Test MSE 4386.617770243507 Test RE 0.117409775872999\n",
      "85 Train Loss 17036.564 Test MSE 4291.553904294267 Test RE 0.11613059418974159\n",
      "86 Train Loss 16969.826 Test MSE 4359.311155765506 Test RE 0.11704376854507763\n",
      "87 Train Loss 16885.107 Test MSE 4261.161090672029 Test RE 0.11571864471190883\n",
      "88 Train Loss 16836.514 Test MSE 4183.871765368968 Test RE 0.11466438464684746\n",
      "89 Train Loss 16762.229 Test MSE 4076.474384441633 Test RE 0.1131831355154582\n",
      "90 Train Loss 16679.508 Test MSE 4221.201491568357 Test RE 0.11517478326303705\n",
      "91 Train Loss 16614.594 Test MSE 4197.829002719235 Test RE 0.11485548342170214\n",
      "92 Train Loss 16580.955 Test MSE 4253.604473090059 Test RE 0.1156159931636657\n",
      "93 Train Loss 16466.09 Test MSE 4266.827226467158 Test RE 0.11579555564293384\n",
      "94 Train Loss 16364.405 Test MSE 4375.3622136820895 Test RE 0.11725904913500211\n",
      "95 Train Loss 16255.795 Test MSE 4468.482240082307 Test RE 0.1185002810274719\n",
      "96 Train Loss 16150.319 Test MSE 4450.570261820781 Test RE 0.11826253745924736\n",
      "97 Train Loss 16046.144 Test MSE 4431.984298707967 Test RE 0.11801534186218246\n",
      "98 Train Loss 16003.006 Test MSE 4579.064748607902 Test RE 0.11995759620647038\n",
      "99 Train Loss 15973.684 Test MSE 4614.43901105023 Test RE 0.12042005392532665\n",
      "Training time: 369.57\n",
      "3D_HTTP_stan\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 280988.5 Test MSE 92512.09277248474 Test RE 0.5391860096441944\n",
      "1 Train Loss 196481.86 Test MSE 55241.31420927454 Test RE 0.41664997799329856\n",
      "2 Train Loss 181977.64 Test MSE 47298.09149696099 Test RE 0.3855326564401519\n",
      "3 Train Loss 165663.83 Test MSE 37428.83081005388 Test RE 0.3429592278272232\n",
      "4 Train Loss 153646.12 Test MSE 34379.918679788025 Test RE 0.32869400623305983\n",
      "5 Train Loss 146916.17 Test MSE 31051.39550785298 Test RE 0.31237762861766644\n",
      "6 Train Loss 132454.38 Test MSE 24324.32347423466 Test RE 0.2764774704080303\n",
      "7 Train Loss 124766.164 Test MSE 24147.907468682246 Test RE 0.27547304760539953\n",
      "8 Train Loss 114131.79 Test MSE 20648.58867565125 Test RE 0.25473261196011615\n",
      "9 Train Loss 98830.69 Test MSE 14579.818360429583 Test RE 0.21405009204700934\n",
      "10 Train Loss 93103.445 Test MSE 13395.1359000005 Test RE 0.20516955866219258\n",
      "11 Train Loss 81616.84 Test MSE 12925.694948255536 Test RE 0.20154234773942506\n",
      "12 Train Loss 67157.86 Test MSE 8844.680565098519 Test RE 0.1667172117938109\n",
      "13 Train Loss 62421.46 Test MSE 8398.266729208706 Test RE 0.16245541580203468\n",
      "14 Train Loss 55931.125 Test MSE 6639.310454015308 Test RE 0.14444444700435138\n",
      "15 Train Loss 50965.137 Test MSE 6303.180549843991 Test RE 0.1407405474072882\n",
      "16 Train Loss 48804.703 Test MSE 5938.152048382655 Test RE 0.13660450409012181\n",
      "17 Train Loss 45419.125 Test MSE 5129.148923323918 Test RE 0.1269585684045499\n",
      "18 Train Loss 42837.46 Test MSE 4762.556439653134 Test RE 0.12233745171261651\n",
      "19 Train Loss 40697.28 Test MSE 4842.980047924527 Test RE 0.12336606211295836\n",
      "20 Train Loss 38109.125 Test MSE 4559.561609407339 Test RE 0.11970186208873203\n",
      "21 Train Loss 35754.508 Test MSE 4235.482373659022 Test RE 0.11536944451165744\n",
      "22 Train Loss 34800.016 Test MSE 4585.543573174665 Test RE 0.12004242897830682\n",
      "23 Train Loss 33585.633 Test MSE 4490.9356915901 Test RE 0.1187976310097811\n",
      "24 Train Loss 32834.105 Test MSE 4487.832490743076 Test RE 0.11875657980791979\n",
      "25 Train Loss 32429.41 Test MSE 4324.569741223989 Test RE 0.11657644700264978\n",
      "26 Train Loss 31918.104 Test MSE 4390.016087275341 Test RE 0.11745524580180156\n",
      "27 Train Loss 31725.447 Test MSE 4448.243526466612 Test RE 0.11823161989318391\n",
      "28 Train Loss 31621.512 Test MSE 4459.874532510346 Test RE 0.1183860915327798\n",
      "29 Train Loss 31438.475 Test MSE 4531.9691758135905 Test RE 0.11933912132454304\n",
      "30 Train Loss 31102.156 Test MSE 4443.415954687635 Test RE 0.11816744550913148\n",
      "31 Train Loss 30953.748 Test MSE 4321.612798944455 Test RE 0.1165365853759548\n",
      "32 Train Loss 30688.908 Test MSE 4339.052044304547 Test RE 0.11677148193576216\n",
      "33 Train Loss 30225.246 Test MSE 4234.874593502209 Test RE 0.11536116661453126\n",
      "34 Train Loss 29640.734 Test MSE 4035.059495363627 Test RE 0.11260672646822216\n",
      "35 Train Loss 29521.148 Test MSE 4016.2916975221983 Test RE 0.11234454403431735\n",
      "36 Train Loss 29325.86 Test MSE 3886.9211514967387 Test RE 0.11052034389646738\n",
      "37 Train Loss 29214.787 Test MSE 3900.8523350438536 Test RE 0.1107182257167336\n",
      "38 Train Loss 28939.93 Test MSE 3893.471082882993 Test RE 0.11061342476083025\n",
      "39 Train Loss 28607.693 Test MSE 3894.0936931945043 Test RE 0.11062226857967024\n",
      "40 Train Loss 28050.525 Test MSE 3685.548255218465 Test RE 0.10761936200707033\n",
      "41 Train Loss 27782.084 Test MSE 3801.0822790548164 Test RE 0.10929316366039743\n",
      "42 Train Loss 27292.936 Test MSE 3716.551262714947 Test RE 0.1080710636624816\n",
      "43 Train Loss 26985.168 Test MSE 3812.115954096322 Test RE 0.10945167527820338\n",
      "44 Train Loss 26612.848 Test MSE 3369.571616058715 Test RE 0.10290268523988276\n",
      "45 Train Loss 26185.871 Test MSE 3200.702396404104 Test RE 0.10029101109167265\n",
      "46 Train Loss 25403.395 Test MSE 2805.9635688677877 Test RE 0.09390319653871895\n",
      "47 Train Loss 25013.977 Test MSE 2493.7854230759995 Test RE 0.08852560494492086\n",
      "48 Train Loss 24740.87 Test MSE 2405.435326433663 Test RE 0.08694331690386968\n",
      "49 Train Loss 24298.025 Test MSE 2533.251044294515 Test RE 0.08922334014900268\n",
      "50 Train Loss 23725.738 Test MSE 2407.435895847244 Test RE 0.08697946420505138\n",
      "51 Train Loss 23356.215 Test MSE 2328.9508207042036 Test RE 0.08554990293540822\n",
      "52 Train Loss 23275.43 Test MSE 2390.233336764243 Test RE 0.08666814711025549\n",
      "53 Train Loss 23043.48 Test MSE 2413.7273667943095 Test RE 0.0870930439091681\n",
      "54 Train Loss 22821.688 Test MSE 2343.269593919671 Test RE 0.08581248739723686\n",
      "55 Train Loss 22698.316 Test MSE 2322.490777128634 Test RE 0.08543117138733718\n",
      "56 Train Loss 22585.246 Test MSE 2221.2389656024525 Test RE 0.08354818195515537\n",
      "57 Train Loss 22262.32 Test MSE 2125.4605254332987 Test RE 0.08172706147331815\n",
      "58 Train Loss 21877.924 Test MSE 2009.3418613100748 Test RE 0.07946324173055205\n",
      "59 Train Loss 21797.55 Test MSE 1937.836408770147 Test RE 0.07803652428736882\n",
      "60 Train Loss 21746.854 Test MSE 1858.3885381915459 Test RE 0.07642010346175263\n",
      "61 Train Loss 21680.957 Test MSE 1847.5326525220767 Test RE 0.07619657027926305\n",
      "62 Train Loss 21603.357 Test MSE 1813.7537838778792 Test RE 0.07549679719881187\n",
      "63 Train Loss 21404.777 Test MSE 1874.2696912609829 Test RE 0.07674593886133105\n",
      "64 Train Loss 21279.531 Test MSE 1917.1920104839414 Test RE 0.07761973710120486\n",
      "65 Train Loss 21217.764 Test MSE 1994.3655777386466 Test RE 0.07916655507620776\n",
      "66 Train Loss 21113.541 Test MSE 2119.560826097573 Test RE 0.08161355662591813\n",
      "67 Train Loss 20952.969 Test MSE 2075.424247014671 Test RE 0.08075934823512054\n",
      "68 Train Loss 20700.244 Test MSE 2092.96396572265 Test RE 0.08109988489352833\n",
      "69 Train Loss 20634.88 Test MSE 2047.6887423164885 Test RE 0.08021790831142842\n",
      "70 Train Loss 20577.734 Test MSE 2054.4169188610053 Test RE 0.08034958790206262\n",
      "71 Train Loss 20500.725 Test MSE 2036.1676499227665 Test RE 0.07999192143209759\n",
      "72 Train Loss 20457.256 Test MSE 2115.4399263495015 Test RE 0.08153418053280746\n",
      "73 Train Loss 20341.096 Test MSE 2056.7742074958487 Test RE 0.08039567223557152\n",
      "74 Train Loss 20112.482 Test MSE 1900.3293633815665 Test RE 0.07727763130196516\n",
      "75 Train Loss 20028.25 Test MSE 1889.6602982933923 Test RE 0.07706039512735767\n",
      "76 Train Loss 19986.193 Test MSE 1879.7801617446798 Test RE 0.07685867499695952\n",
      "77 Train Loss 19920.91 Test MSE 1881.5835801833048 Test RE 0.07689553439307092\n",
      "78 Train Loss 19731.46 Test MSE 1840.4522954464587 Test RE 0.07605042487765475\n",
      "79 Train Loss 19608.805 Test MSE 1719.4904218939412 Test RE 0.0735087846942018\n",
      "80 Train Loss 19501.598 Test MSE 1776.6090826305367 Test RE 0.07471973125986138\n",
      "81 Train Loss 19371.912 Test MSE 1639.9509310594217 Test RE 0.07178848513889183\n",
      "82 Train Loss 19260.54 Test MSE 1689.7074425444612 Test RE 0.07286938776804894\n",
      "83 Train Loss 19221.32 Test MSE 1652.344707311748 Test RE 0.07205924193838366\n",
      "84 Train Loss 19182.53 Test MSE 1733.0177629591567 Test RE 0.0737973673855698\n",
      "85 Train Loss 18939.068 Test MSE 1757.4130077927214 Test RE 0.0743149654969981\n",
      "86 Train Loss 18790.182 Test MSE 1751.9092963355226 Test RE 0.07419850771173131\n",
      "87 Train Loss 18695.422 Test MSE 1843.2432415390003 Test RE 0.07610806620588285\n",
      "88 Train Loss 18505.637 Test MSE 1828.3015866077492 Test RE 0.07579896581925352\n",
      "89 Train Loss 18255.547 Test MSE 1575.1258811510827 Test RE 0.07035532862919859\n",
      "90 Train Loss 17960.855 Test MSE 1656.4703563211256 Test RE 0.07214914624153093\n",
      "91 Train Loss 17699.926 Test MSE 1647.9361809559489 Test RE 0.07196304916405936\n",
      "92 Train Loss 17605.746 Test MSE 1651.578576393436 Test RE 0.07204253440339523\n",
      "93 Train Loss 17521.8 Test MSE 1723.7863012829343 Test RE 0.07360055254594666\n",
      "94 Train Loss 17343.277 Test MSE 1748.9584042704116 Test RE 0.07413599189950032\n",
      "95 Train Loss 17263.88 Test MSE 1671.7283002234826 Test RE 0.07248067171096476\n",
      "96 Train Loss 17089.244 Test MSE 1669.2062349152447 Test RE 0.07242597682254022\n",
      "97 Train Loss 17046.686 Test MSE 1660.6108218579793 Test RE 0.07223926092738849\n",
      "98 Train Loss 16937.777 Test MSE 1691.5132507766336 Test RE 0.07290831551327777\n",
      "99 Train Loss 16884.35 Test MSE 1745.9800046253667 Test RE 0.07407283982637254\n",
      "Training time: 367.94\n",
      "3D_HTTP_stan\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 309047.06 Test MSE 116432.38546092123 Test RE 0.6048897980985084\n",
      "1 Train Loss 199054.08 Test MSE 57986.29375323062 Test RE 0.42687629433665863\n",
      "2 Train Loss 182589.31 Test MSE 49448.13337628013 Test RE 0.39419790622854617\n",
      "3 Train Loss 165536.92 Test MSE 40354.870756674834 Test RE 0.35611260196222977\n",
      "4 Train Loss 151601.56 Test MSE 33352.25595463801 Test RE 0.3237441799541708\n",
      "5 Train Loss 125455.04 Test MSE 24917.59288403562 Test RE 0.2798287968071037\n",
      "6 Train Loss 102240.68 Test MSE 18022.03759532995 Test RE 0.23798046450161375\n",
      "7 Train Loss 82993.96 Test MSE 13240.016512538092 Test RE 0.20397813919039043\n",
      "8 Train Loss 65547.62 Test MSE 10527.834908626199 Test RE 0.18189003025264605\n",
      "9 Train Loss 54453.758 Test MSE 8257.500253080734 Test RE 0.1610881745202878\n",
      "10 Train Loss 47897.074 Test MSE 5365.7463308470715 Test RE 0.12985373052168325\n",
      "11 Train Loss 42988.13 Test MSE 4868.9215804441565 Test RE 0.12369602739198461\n",
      "12 Train Loss 40015.676 Test MSE 4099.990472835856 Test RE 0.11350912764138436\n",
      "13 Train Loss 37030.105 Test MSE 3970.284813207344 Test RE 0.11169923364011272\n",
      "14 Train Loss 35586.41 Test MSE 3938.334887637955 Test RE 0.11124888925223589\n",
      "15 Train Loss 34258.324 Test MSE 4078.537526577421 Test RE 0.1132117734186593\n",
      "16 Train Loss 33288.258 Test MSE 3798.5658497496674 Test RE 0.10925698001136963\n",
      "17 Train Loss 31627.365 Test MSE 3773.477716139399 Test RE 0.10889558116474476\n",
      "18 Train Loss 30758.97 Test MSE 3630.25875337811 Test RE 0.1068090747259688\n",
      "19 Train Loss 29980.357 Test MSE 3785.2739058840257 Test RE 0.10906565645031574\n",
      "20 Train Loss 29268.885 Test MSE 4060.6116422891823 Test RE 0.11296270669239927\n",
      "21 Train Loss 28900.57 Test MSE 4176.232478519397 Test RE 0.1145596545799173\n",
      "22 Train Loss 28251.465 Test MSE 3954.4498665372075 Test RE 0.11147626241467552\n",
      "23 Train Loss 27966.227 Test MSE 3905.217764292279 Test RE 0.11078016056982042\n",
      "24 Train Loss 27647.893 Test MSE 3856.131281144199 Test RE 0.11008173545797346\n",
      "25 Train Loss 27463.55 Test MSE 3797.9114346083934 Test RE 0.10924756823537643\n",
      "26 Train Loss 27019.645 Test MSE 3557.166365139419 Test RE 0.1057283490622566\n",
      "27 Train Loss 26687.73 Test MSE 3625.224244985999 Test RE 0.10673498665362731\n",
      "28 Train Loss 26072.926 Test MSE 3398.0069581348516 Test RE 0.10333596372729804\n",
      "29 Train Loss 25821.633 Test MSE 3242.876454913307 Test RE 0.10094959105333347\n",
      "30 Train Loss 25109.49 Test MSE 2834.9627276597616 Test RE 0.09438718609067734\n",
      "31 Train Loss 24470.77 Test MSE 2848.3644990425228 Test RE 0.09461002218627496\n",
      "32 Train Loss 23899.162 Test MSE 2813.8629687029747 Test RE 0.09403528263735758\n",
      "33 Train Loss 23163.324 Test MSE 2998.1714930066 Test RE 0.0970661037820365\n",
      "34 Train Loss 22862.799 Test MSE 2892.104539492994 Test RE 0.09533367958565671\n",
      "35 Train Loss 22223.477 Test MSE 2411.846604353711 Test RE 0.08705910609813464\n",
      "36 Train Loss 21891.924 Test MSE 2222.968062635784 Test RE 0.08358069417308284\n",
      "37 Train Loss 21503.625 Test MSE 2225.993778856006 Test RE 0.08363755631860811\n",
      "38 Train Loss 21077.715 Test MSE 2378.041466638313 Test RE 0.08644683029673422\n",
      "39 Train Loss 20905.715 Test MSE 2283.696257709469 Test RE 0.08471465222871334\n",
      "40 Train Loss 20632.56 Test MSE 2142.4488700272027 Test RE 0.08205302476189025\n",
      "41 Train Loss 20325.797 Test MSE 2137.8650811327225 Test RE 0.08196520116452963\n",
      "42 Train Loss 20029.285 Test MSE 2077.0631093983125 Test RE 0.08079122782315333\n",
      "43 Train Loss 19764.719 Test MSE 2116.7816806455126 Test RE 0.08156003366484707\n",
      "44 Train Loss 19579.049 Test MSE 1956.7768932875772 Test RE 0.07841696287563528\n",
      "45 Train Loss 19420.219 Test MSE 1857.485456774614 Test RE 0.07640153308203945\n",
      "46 Train Loss 19230.908 Test MSE 1815.8957201126843 Test RE 0.07554136267755676\n",
      "47 Train Loss 19104.543 Test MSE 1904.3308770451192 Test RE 0.07735895006964195\n",
      "48 Train Loss 19034.793 Test MSE 1844.5796793290128 Test RE 0.07613565216507463\n",
      "49 Train Loss 18924.057 Test MSE 1902.493632206239 Test RE 0.07732162419572766\n",
      "50 Train Loss 18774.986 Test MSE 1944.6928514942417 Test RE 0.07817445659986326\n",
      "51 Train Loss 18679.543 Test MSE 1946.9344355316728 Test RE 0.07821949819761227\n",
      "52 Train Loss 18560.965 Test MSE 1931.2627057721822 Test RE 0.07790405058561134\n",
      "53 Train Loss 18523.508 Test MSE 1914.526267463706 Test RE 0.07756575548463616\n",
      "54 Train Loss 18433.68 Test MSE 1877.2079101725362 Test RE 0.07680607109943789\n",
      "55 Train Loss 18279.012 Test MSE 1823.2398557726576 Test RE 0.07569396676219636\n",
      "56 Train Loss 18231.873 Test MSE 1971.0945616342096 Test RE 0.07870332709361232\n",
      "57 Train Loss 18138.934 Test MSE 1922.2825827407244 Test RE 0.07772271764173715\n",
      "58 Train Loss 18004.12 Test MSE 1894.5062214843454 Test RE 0.07715914029695986\n",
      "59 Train Loss 17918.885 Test MSE 1783.2539810759852 Test RE 0.07485933474335975\n",
      "60 Train Loss 17880.002 Test MSE 1738.3176699003866 Test RE 0.07391012462749184\n",
      "61 Train Loss 17818.877 Test MSE 1808.2223217526487 Test RE 0.07538158680965601\n",
      "62 Train Loss 17793.879 Test MSE 1760.3949452075324 Test RE 0.07437798672543411\n",
      "63 Train Loss 17752.201 Test MSE 1803.9058933317597 Test RE 0.07529156092317144\n",
      "64 Train Loss 17646.113 Test MSE 1694.235528979211 Test RE 0.07296696031738206\n",
      "65 Train Loss 17554.21 Test MSE 1766.0780303298136 Test RE 0.07449794725833651\n",
      "66 Train Loss 17426.88 Test MSE 1820.3432305936888 Test RE 0.07563381444506902\n",
      "67 Train Loss 17265.512 Test MSE 1919.240561575218 Test RE 0.07766119501029506\n",
      "68 Train Loss 17177.332 Test MSE 1757.8391466056923 Test RE 0.0743239749249258\n",
      "69 Train Loss 17075.824 Test MSE 1799.8179733836148 Test RE 0.07520620158014794\n",
      "70 Train Loss 16968.688 Test MSE 1757.477940507657 Test RE 0.07431633837516526\n",
      "71 Train Loss 16919.406 Test MSE 1751.4524048852813 Test RE 0.07418883173294484\n",
      "72 Train Loss 16884.111 Test MSE 1823.7724338261326 Test RE 0.07570502126001073\n",
      "73 Train Loss 16841.635 Test MSE 1758.7757053376477 Test RE 0.07434377181550109\n",
      "74 Train Loss 16802.012 Test MSE 1798.232759699065 Test RE 0.0751730748517991\n",
      "75 Train Loss 16730.902 Test MSE 1861.055206923444 Test RE 0.0764749127880163\n",
      "76 Train Loss 16668.139 Test MSE 1917.3377064835972 Test RE 0.07762268638074576\n",
      "77 Train Loss 16621.371 Test MSE 1893.099363465004 Test RE 0.07713048583433928\n",
      "78 Train Loss 16555.47 Test MSE 1834.4407451836787 Test RE 0.07592611986542842\n",
      "79 Train Loss 16480.176 Test MSE 1927.0991180425906 Test RE 0.07782002903896974\n",
      "80 Train Loss 16418.682 Test MSE 1959.0362595790632 Test RE 0.0784622213642774\n",
      "81 Train Loss 16336.255 Test MSE 1964.4727984363426 Test RE 0.07857101653932393\n",
      "82 Train Loss 16173.269 Test MSE 2036.8374068627977 Test RE 0.08000507622797894\n",
      "83 Train Loss 15999.811 Test MSE 1937.224102798939 Test RE 0.0780241945559582\n",
      "84 Train Loss 15899.591 Test MSE 1935.706072832454 Test RE 0.07799361825926354\n",
      "85 Train Loss 15804.746 Test MSE 1986.8942518660926 Test RE 0.07901812839603785\n",
      "86 Train Loss 15733.516 Test MSE 1912.3514412870068 Test RE 0.07752168714971389\n",
      "87 Train Loss 15625.226 Test MSE 1948.2453229167986 Test RE 0.07824582669066428\n",
      "88 Train Loss 15562.763 Test MSE 1934.315106167607 Test RE 0.07796559075530587\n",
      "89 Train Loss 15456.612 Test MSE 2069.4163487326086 Test RE 0.08064237321377218\n",
      "90 Train Loss 15271.414 Test MSE 2069.0136613992026 Test RE 0.08063452674001556\n",
      "91 Train Loss 15186.508 Test MSE 2186.3180173518235 Test RE 0.08288883374371031\n",
      "92 Train Loss 15120.546 Test MSE 2195.4406317036905 Test RE 0.08306158439338811\n",
      "93 Train Loss 15065.144 Test MSE 2179.2063229672362 Test RE 0.08275391279491597\n",
      "94 Train Loss 14910.335 Test MSE 2394.976262002971 Test RE 0.08675409203074598\n",
      "95 Train Loss 14744.708 Test MSE 2389.088183081855 Test RE 0.08664738339724724\n",
      "96 Train Loss 14702.382 Test MSE 2323.6427252568124 Test RE 0.0854523555571339\n",
      "97 Train Loss 14593.765 Test MSE 2613.0941374374324 Test RE 0.09061850436766891\n",
      "98 Train Loss 14421.5625 Test MSE 2564.155652673139 Test RE 0.0897659341175713\n",
      "99 Train Loss 14266.467 Test MSE 2862.5207027026513 Test RE 0.09484483391444498\n",
      "Training time: 367.50\n",
      "3D_HTTP_stan\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 601162.7 Test MSE 320700.5494613754 Test RE 1.0038971336107303\n",
      "1 Train Loss 478651.7 Test MSE 231237.19451655328 Test RE 0.8524486121426034\n",
      "2 Train Loss 240872.81 Test MSE 82373.10759975426 Test RE 0.5087824091488774\n",
      "3 Train Loss 200789.39 Test MSE 59440.38464840507 Test RE 0.43219542741525535\n",
      "4 Train Loss 178847.78 Test MSE 48625.58620016652 Test RE 0.3909055056036099\n",
      "5 Train Loss 163052.1 Test MSE 41407.50847696449 Test RE 0.3607272175747047\n",
      "6 Train Loss 155487.81 Test MSE 38225.82651870645 Test RE 0.34659141765694523\n",
      "7 Train Loss 149125.9 Test MSE 34452.99642583725 Test RE 0.3290431557842851\n",
      "8 Train Loss 141532.19 Test MSE 32161.222709532314 Test RE 0.31791106059976454\n",
      "9 Train Loss 125391.55 Test MSE 25856.741486999843 Test RE 0.28505342195960953\n",
      "10 Train Loss 113595.69 Test MSE 21590.854805991192 Test RE 0.26047993848227363\n",
      "11 Train Loss 105430.25 Test MSE 19581.184827418638 Test RE 0.24806120303010074\n",
      "12 Train Loss 97214.78 Test MSE 16197.827059229467 Test RE 0.22561488092129295\n",
      "13 Train Loss 89896.33 Test MSE 14182.299924786696 Test RE 0.21111189055121488\n",
      "14 Train Loss 83032.305 Test MSE 12032.095123110137 Test RE 0.19445091476096843\n",
      "15 Train Loss 74830.27 Test MSE 10426.245481642156 Test RE 0.1810103196056301\n",
      "16 Train Loss 66021.77 Test MSE 8221.629813292797 Test RE 0.1607379115855452\n",
      "17 Train Loss 57325.88 Test MSE 6577.571459670024 Test RE 0.14377128344964582\n",
      "18 Train Loss 53547.395 Test MSE 5510.679292643057 Test RE 0.1315957703286629\n",
      "19 Train Loss 50245.81 Test MSE 5220.988163774006 Test RE 0.1280901447776794\n",
      "20 Train Loss 47479.953 Test MSE 4308.532793925287 Test RE 0.11636009406114373\n",
      "21 Train Loss 44118.25 Test MSE 3956.6256414735517 Test RE 0.1115069258333788\n",
      "22 Train Loss 41110.754 Test MSE 3608.448450372994 Test RE 0.1064877411913729\n",
      "23 Train Loss 37757.09 Test MSE 3300.6575058496146 Test RE 0.10184497201061904\n",
      "24 Train Loss 35101.75 Test MSE 2756.1522323789663 Test RE 0.09306598173391611\n",
      "25 Train Loss 33424.344 Test MSE 2539.7789057527343 Test RE 0.08933822471021674\n",
      "26 Train Loss 31204.777 Test MSE 2323.4901618946324 Test RE 0.08544955023931841\n",
      "27 Train Loss 30338.5 Test MSE 2288.4800842775603 Test RE 0.08480333481786635\n",
      "28 Train Loss 29448.95 Test MSE 2384.8348625227964 Test RE 0.08657021938242529\n",
      "29 Train Loss 28316.61 Test MSE 2344.644902986396 Test RE 0.08583766618598856\n",
      "30 Train Loss 26907.475 Test MSE 2172.5038271338562 Test RE 0.08262655337705685\n",
      "31 Train Loss 26091.598 Test MSE 2220.6560492494164 Test RE 0.08353721852486416\n",
      "32 Train Loss 25653.824 Test MSE 2209.522973371005 Test RE 0.08332755193188646\n",
      "33 Train Loss 24921.107 Test MSE 2182.275891820698 Test RE 0.08281217470489101\n",
      "34 Train Loss 24342.314 Test MSE 2214.5769953542235 Test RE 0.08342279844727295\n",
      "35 Train Loss 23792.21 Test MSE 2151.318102475929 Test RE 0.08222268942444513\n",
      "36 Train Loss 23242.3 Test MSE 1978.6664889492838 Test RE 0.078854350961513\n",
      "37 Train Loss 22945.605 Test MSE 1966.2589381635755 Test RE 0.07860672762906507\n",
      "38 Train Loss 22479.117 Test MSE 2012.3544189371514 Test RE 0.07952278807747991\n",
      "39 Train Loss 22214.25 Test MSE 2076.4126800513627 Test RE 0.08077857700386222\n",
      "40 Train Loss 22049.643 Test MSE 2218.523449537706 Test RE 0.08349709654367246\n",
      "41 Train Loss 21809.578 Test MSE 2203.8905361673155 Test RE 0.08322127634762856\n",
      "42 Train Loss 21457.102 Test MSE 2095.2402203539996 Test RE 0.08114397400022884\n",
      "43 Train Loss 21144.512 Test MSE 2105.8318682065133 Test RE 0.08134881089124037\n",
      "44 Train Loss 20724.518 Test MSE 2057.980772554124 Test RE 0.08041925002710021\n",
      "45 Train Loss 20519.738 Test MSE 2005.4441146500308 Test RE 0.07938613241922822\n",
      "46 Train Loss 20281.633 Test MSE 2135.1954241142034 Test RE 0.08191400819363587\n",
      "47 Train Loss 20117.46 Test MSE 2179.5771596601567 Test RE 0.08276095363395412\n",
      "48 Train Loss 19961.195 Test MSE 2143.7505164759036 Test RE 0.08207794666609279\n",
      "49 Train Loss 19856.229 Test MSE 2229.7833174241878 Test RE 0.08370871845974374\n",
      "50 Train Loss 19759.805 Test MSE 2250.8596420801023 Test RE 0.08410340314593455\n",
      "51 Train Loss 19698.51 Test MSE 2236.984906404774 Test RE 0.0838437875975364\n",
      "52 Train Loss 19617.736 Test MSE 2216.5283472855895 Test RE 0.08345954392855957\n",
      "53 Train Loss 19558.16 Test MSE 2158.506433087665 Test RE 0.08235994271466544\n",
      "54 Train Loss 19442.229 Test MSE 2146.5074068068293 Test RE 0.0821307063489114\n",
      "55 Train Loss 19338.78 Test MSE 2086.586889484666 Test RE 0.08097623854576556\n",
      "56 Train Loss 19267.303 Test MSE 2063.0587322210363 Test RE 0.08051840404218692\n",
      "57 Train Loss 19107.227 Test MSE 2027.5754181163013 Test RE 0.07982296782377138\n",
      "58 Train Loss 19019.824 Test MSE 2076.095095572173 Test RE 0.0807723992812048\n",
      "59 Train Loss 18968.6 Test MSE 2065.0430666829416 Test RE 0.08055711768622585\n",
      "60 Train Loss 18870.822 Test MSE 2071.2958846346314 Test RE 0.0806789863959591\n",
      "61 Train Loss 18775.865 Test MSE 2005.9119993557426 Test RE 0.07939539256034119\n",
      "62 Train Loss 18677.307 Test MSE 1920.6609019446132 Test RE 0.07768992640859078\n",
      "63 Train Loss 18623.217 Test MSE 1884.2023988273772 Test RE 0.07694902801422705\n",
      "64 Train Loss 18534.057 Test MSE 1852.0346858734888 Test RE 0.07628935098772491\n",
      "65 Train Loss 18438.633 Test MSE 1825.6323460259807 Test RE 0.07574361401686906\n",
      "66 Train Loss 18369.592 Test MSE 1766.4923370875765 Test RE 0.07450668503563304\n",
      "67 Train Loss 18318.043 Test MSE 1761.0954604812935 Test RE 0.07439278389738908\n",
      "68 Train Loss 18258.67 Test MSE 1714.201363088019 Test RE 0.07339564311553502\n",
      "69 Train Loss 18164.64 Test MSE 1809.711667724135 Test RE 0.07541262451810127\n",
      "70 Train Loss 18114.98 Test MSE 1821.5878605164935 Test RE 0.07565966671884483\n",
      "71 Train Loss 18092.354 Test MSE 1831.9360768648207 Test RE 0.07587426899363055\n",
      "72 Train Loss 18038.492 Test MSE 1842.9743409282412 Test RE 0.07610251450997\n",
      "73 Train Loss 17990.979 Test MSE 1858.115715457748 Test RE 0.076414493788004\n",
      "74 Train Loss 17952.334 Test MSE 1836.4771484816881 Test RE 0.07596825077520067\n",
      "75 Train Loss 17910.96 Test MSE 1814.658935528199 Test RE 0.07551563314213462\n",
      "76 Train Loss 17876.12 Test MSE 1845.5787890767133 Test RE 0.07615626867066569\n",
      "77 Train Loss 17842.834 Test MSE 1865.2881947026221 Test RE 0.07656183484828724\n",
      "78 Train Loss 17808.074 Test MSE 1888.609686289113 Test RE 0.07703897015682756\n",
      "79 Train Loss 17763.56 Test MSE 1924.9015390763248 Test RE 0.07777564511500111\n",
      "80 Train Loss 17720.945 Test MSE 1891.3468419287826 Test RE 0.07709477610393257\n",
      "81 Train Loss 17678.332 Test MSE 1903.45341056651 Test RE 0.0773411255131399\n",
      "82 Train Loss 17646.232 Test MSE 1956.5137955592386 Test RE 0.07841169093625182\n",
      "83 Train Loss 17584.973 Test MSE 1949.1792343378688 Test RE 0.07826457841437244\n",
      "84 Train Loss 17503.523 Test MSE 1908.7045087695033 Test RE 0.07744773335854144\n",
      "85 Train Loss 17473.611 Test MSE 1867.3588127747653 Test RE 0.07660431792098715\n",
      "86 Train Loss 17444.084 Test MSE 1831.8780788831752 Test RE 0.07587306791752689\n",
      "87 Train Loss 17415.34 Test MSE 1843.4423573653735 Test RE 0.076112176870964\n",
      "88 Train Loss 17345.646 Test MSE 1806.4140456418918 Test RE 0.07534388546709335\n",
      "89 Train Loss 17301.566 Test MSE 1768.8033432136117 Test RE 0.07455540563394362\n",
      "90 Train Loss 17227.79 Test MSE 1751.5535464454224 Test RE 0.07419097380253263\n",
      "91 Train Loss 17205.963 Test MSE 1741.1497686967398 Test RE 0.07397030798485374\n",
      "92 Train Loss 17186.348 Test MSE 1754.1167482606156 Test RE 0.07424523903415003\n",
      "93 Train Loss 17152.65 Test MSE 1786.350727095007 Test RE 0.07492430580006793\n",
      "94 Train Loss 17122.434 Test MSE 1852.98276657866 Test RE 0.07630887524613401\n",
      "95 Train Loss 17098.193 Test MSE 1838.3126724669 Test RE 0.07600620570473396\n",
      "96 Train Loss 17038.643 Test MSE 1883.8176553004866 Test RE 0.07694117133388102\n",
      "97 Train Loss 17000.459 Test MSE 1910.4101445934682 Test RE 0.07748232963317694\n",
      "98 Train Loss 16984.793 Test MSE 1896.8884700812755 Test RE 0.07720763697229627\n",
      "99 Train Loss 16966.871 Test MSE 1928.351379776978 Test RE 0.0778453093209822\n",
      "Training time: 366.46\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 100\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 0.5\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    beta_val = []\n",
    "\n",
    "    print(reps)\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_D = 5000 #Total number of data points for 'y'\n",
    "    N_N = 3500\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "\n",
    "    layers = np.array([3,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "    #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HT_stan_v3_15Aug2022_MP4Video.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
