{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EG0W1AFlwR_K"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMtToePZL2nl",
    "outputId": "ed6ca883-db49-404e-df85-58c0227e5aa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIONgQSowWqL",
    "outputId": "220b4aa0-31cc-4133-973d-97d73f894c56"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bS0UTdDMwXb7",
    "outputId": "e0fddb95-0b36-482f-b039-f3345278893f"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/MURI Aug17 Thin Plate/Rowdy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7xfa_6UCyRdu",
    "outputId": "a091937a-a627-4f7d-f35f-3a5130dfca86"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uwHGknZlwR_O"
   },
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9MUkWfTQwR_P"
   },
   "outputs": [],
   "source": [
    "loss_thresh = 20000\n",
    "label = \"3D_HTTP_rowdy\"\n",
    "\n",
    "x = np.linspace(0,1,100).reshape(-1,1)\n",
    "y = np.linspace(0,1,100).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 3000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xyt = np.hstack((X,Y,T))\n",
    "\n",
    "initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "xyt_initial = xyt[initial_pts,:]\n",
    "xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "xyt_NBC_x0 = xyt[NBC_pts_x0,:]\n",
    "xyt_NBC_x1 = xyt[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xyt_NBC_y1 = xyt[NBC_pts_y1,:]\n",
    "\n",
    "u_initial = 300*np.ones((np.shape(xyt_initial)[0],1))\n",
    "u_DBC = 1000*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xyt_NBC_x = np.vstack((xyt_NBC_x0,xyt_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xyt_NBC_y = np.vstack((xyt_NBC_y1))\n",
    "\n",
    "u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../3D_HTTP_FEA.mat')\n",
    "xy = fea_data['xy']\n",
    "t = fea_data['t']/3000\n",
    "xyt = np.zeros((497*101,3))\n",
    "u_true = np.ones((497*101,1))\n",
    "\n",
    "\n",
    "for i in range(101):\n",
    "    t_temp = t[0,i]*np.ones((497,1))\n",
    "    xyt[497*i:497*(i+1)] = np.hstack((xy,t_temp))\n",
    "    u_true[497*i:497*(i+1)] = fea_data['u'][:,i].reshape(-1,1)\n",
    "    #print(i)\n",
    "#print(xyt)\n",
    "\n",
    "xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IfjvaqHTwR_Q"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xyt_I_DBC.shape[0], N_D, replace=False) \n",
    "    xyt_D = xyt_I_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_I_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_x.shape[0], N_D, replace=False) \n",
    "    xyt_Nx = xyt_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_y.shape[0], N_D, replace=False) \n",
    "    xyt_Ny = xyt_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xyt_coll = lb_xyt + (ub_xyt - lb_xyt)*samples\n",
    "    xyt_coll = np.vstack((xyt_coll, xyt_D,xyt_Nx,xyt_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xyt_coll, xyt_D, u_D, xyt_Nx,xyt_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KNZCQobEwR_Q"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "\n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        self.omega1.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        self.omega.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "                \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/3000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xyt_coll_np_array, xyt_D_np_array, u_D_np_array,xyt_Nx_np_array,xyt_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "\n",
    "    xyt_coll = torch.from_numpy(xyt_coll_np_array).float().to(device)\n",
    "    xyt_D = torch.from_numpy(xyt_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xyt_Nx = torch.from_numpy(xyt_Nx_np_array).float().to(device)\n",
    "    xyt_Ny = torch.from_numpy(xyt_Ny_np_array).float().to(device)\n",
    "\n",
    "    N_hat = torch.zeros(xyt_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xyt_coll.shape[0],1).to(device)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat,i)\n",
    "\n",
    "        loss_np = PINN.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2QpZV2SwR_S",
    "outputId": "9326a244-267a-4324-8765-4f710090a2eb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D_HTTP_rowdy\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1034684.94 Test MSE 316745.0362455329 Test RE 0.9976869028849932\n",
      "1 Train Loss 570587.44 Test MSE 312602.04782212566 Test RE 0.9911406116192567\n",
      "2 Train Loss 514153.72 Test MSE 263075.74414584204 Test RE 0.9092425807305725\n",
      "3 Train Loss 381633.66 Test MSE 163027.76034999685 Test RE 0.7157644812390641\n",
      "4 Train Loss 277071.84 Test MSE 103228.45194098241 Test RE 0.5695594598266046\n",
      "5 Train Loss 230235.94 Test MSE 76220.58666066272 Test RE 0.4894130052927064\n",
      "6 Train Loss 208567.97 Test MSE 62017.26056433436 Test RE 0.44146436339798106\n",
      "7 Train Loss 201398.25 Test MSE 58451.74108518159 Test RE 0.4285861062684648\n",
      "8 Train Loss 192781.31 Test MSE 53123.59405826112 Test RE 0.4085856286066941\n",
      "9 Train Loss 179310.05 Test MSE 48139.1505099236 Test RE 0.3889453407014667\n",
      "10 Train Loss 170860.23 Test MSE 43267.32128240867 Test RE 0.36873924866708563\n",
      "11 Train Loss 164665.73 Test MSE 40907.318092711066 Test RE 0.3585418591646527\n",
      "12 Train Loss 158710.31 Test MSE 39282.64729636895 Test RE 0.3513498205182231\n",
      "13 Train Loss 152078.44 Test MSE 35436.21458261012 Test RE 0.3337052380260637\n",
      "14 Train Loss 145137.47 Test MSE 32761.634835500572 Test RE 0.320864850823837\n",
      "15 Train Loss 142884.4 Test MSE 32108.890120546108 Test RE 0.3176523036102215\n",
      "16 Train Loss 138237.67 Test MSE 30427.98960144907 Test RE 0.3092259922632908\n",
      "17 Train Loss 134935.92 Test MSE 29359.99030402165 Test RE 0.30375072004857306\n",
      "18 Train Loss 127739.164 Test MSE 25638.082132127915 Test RE 0.2838455758128462\n",
      "19 Train Loss 122695.28 Test MSE 23894.327055716847 Test RE 0.2740228406553852\n",
      "20 Train Loss 117743.86 Test MSE 23382.169164970204 Test RE 0.27107019070349436\n",
      "21 Train Loss 113443.33 Test MSE 23091.83570242781 Test RE 0.2693820115732552\n",
      "22 Train Loss 107792.61 Test MSE 20565.44625629791 Test RE 0.2542192488283287\n",
      "23 Train Loss 103578.5 Test MSE 19159.492597065786 Test RE 0.24537559398656408\n",
      "24 Train Loss 100188.59 Test MSE 18376.591967702007 Test RE 0.24031000274638362\n",
      "25 Train Loss 96887.96 Test MSE 16786.11551115939 Test RE 0.22967539177803617\n",
      "26 Train Loss 91891.12 Test MSE 15701.253606909697 Test RE 0.22212964697619222\n",
      "27 Train Loss 88088.95 Test MSE 15521.361246085467 Test RE 0.22085348955048686\n",
      "28 Train Loss 84249.14 Test MSE 14633.650785635318 Test RE 0.21444489185045154\n",
      "29 Train Loss 79120.09 Test MSE 13781.222611045623 Test RE 0.2081053456687839\n",
      "30 Train Loss 74666.46 Test MSE 12635.604229062279 Test RE 0.19926791169317115\n",
      "31 Train Loss 68957.3 Test MSE 11652.235263965842 Test RE 0.19135683733302652\n",
      "32 Train Loss 66597.44 Test MSE 10643.755902229024 Test RE 0.182888675733398\n",
      "33 Train Loss 64028.816 Test MSE 10229.96343749206 Test RE 0.17929839541113482\n",
      "34 Train Loss 62344.016 Test MSE 9479.845732933156 Test RE 0.17259968438403375\n",
      "35 Train Loss 59977.754 Test MSE 9547.68375396968 Test RE 0.17321614739454158\n",
      "36 Train Loss 58035.707 Test MSE 10005.342355937872 Test RE 0.1773190267857333\n",
      "37 Train Loss 56433.656 Test MSE 9338.955541283594 Test RE 0.1713122882843798\n",
      "38 Train Loss 54942.13 Test MSE 9085.156025142513 Test RE 0.16896842552349856\n",
      "39 Train Loss 53041.35 Test MSE 8596.228539142334 Test RE 0.16435894267091036\n",
      "40 Train Loss 51580.414 Test MSE 8088.38580300236 Test RE 0.1594300900199926\n",
      "41 Train Loss 51120.516 Test MSE 7891.792119256863 Test RE 0.15748064345143162\n",
      "42 Train Loss 49808.668 Test MSE 7466.1259546990395 Test RE 0.1531746929172538\n",
      "43 Train Loss 49234.492 Test MSE 7264.07901040496 Test RE 0.15108788502541234\n",
      "44 Train Loss 48880.105 Test MSE 7044.725096000494 Test RE 0.14878919271400556\n",
      "45 Train Loss 48222.14 Test MSE 6656.080324722504 Test RE 0.14462675411652126\n",
      "46 Train Loss 47329.58 Test MSE 6759.1724278380425 Test RE 0.14574246980541639\n",
      "47 Train Loss 46590.02 Test MSE 6680.004484822993 Test RE 0.14488643923627534\n",
      "48 Train Loss 46034.254 Test MSE 6634.207767828721 Test RE 0.1443889294652124\n",
      "49 Train Loss 45443.96 Test MSE 6637.4047739767275 Test RE 0.14442371558529893\n",
      "50 Train Loss 44779.74 Test MSE 6342.8264563200455 Test RE 0.14118247034207335\n",
      "51 Train Loss 43883.88 Test MSE 5950.916855502462 Test RE 0.1367512495798011\n",
      "52 Train Loss 43236.1 Test MSE 5653.010377917049 Test RE 0.13328437922236883\n",
      "53 Train Loss 42686.977 Test MSE 5638.707511113204 Test RE 0.13311565884576948\n",
      "54 Train Loss 41876.695 Test MSE 5531.689154081113 Test RE 0.13184639085175442\n",
      "55 Train Loss 41129.445 Test MSE 5745.734222898804 Test RE 0.1343730357410806\n",
      "56 Train Loss 40511.375 Test MSE 5687.665433754342 Test RE 0.13369229641152816\n",
      "57 Train Loss 40154.727 Test MSE 5462.845156533586 Test RE 0.13102338266768337\n",
      "58 Train Loss 39587.105 Test MSE 5460.039370610359 Test RE 0.1309897307181668\n",
      "59 Train Loss 38706.4 Test MSE 5624.202801548516 Test RE 0.13294433880123321\n",
      "60 Train Loss 37509.01 Test MSE 5481.615111263764 Test RE 0.1312482832473213\n",
      "61 Train Loss 36717.98 Test MSE 5004.226760357048 Test RE 0.12540297877830994\n",
      "62 Train Loss 35680.277 Test MSE 5229.462742998524 Test RE 0.12819405900900888\n",
      "63 Train Loss 34905.31 Test MSE 4976.247156176745 Test RE 0.12505191115803632\n",
      "64 Train Loss 33978.09 Test MSE 4872.196803626482 Test RE 0.12373762428173589\n",
      "65 Train Loss 33261.4 Test MSE 4268.407931569542 Test RE 0.11581700268856829\n",
      "66 Train Loss 32873.43 Test MSE 3986.667414495431 Test RE 0.11192944888586069\n",
      "67 Train Loss 32484.285 Test MSE 3944.6567973816027 Test RE 0.11133814314285204\n",
      "68 Train Loss 31963.477 Test MSE 3945.023372235705 Test RE 0.11134331631982518\n",
      "69 Train Loss 31438.502 Test MSE 3852.7562731999105 Test RE 0.1100335514059824\n",
      "70 Train Loss 30898.629 Test MSE 3730.148151189851 Test RE 0.1082685705153047\n",
      "71 Train Loss 30275.066 Test MSE 3571.828253410477 Test RE 0.10594601995389699\n",
      "72 Train Loss 30010.879 Test MSE 3685.423258800398 Test RE 0.10761753702094981\n",
      "73 Train Loss 29550.264 Test MSE 3667.7696964528736 Test RE 0.10735947799614162\n",
      "74 Train Loss 29325.559 Test MSE 3633.127529072179 Test RE 0.1068512687936078\n",
      "75 Train Loss 28959.746 Test MSE 3785.6879377207815 Test RE 0.109071621067468\n",
      "76 Train Loss 28516.598 Test MSE 3780.2160351567486 Test RE 0.10899276550464833\n",
      "77 Train Loss 28202.55 Test MSE 3834.308611918482 Test RE 0.10976980550454936\n",
      "78 Train Loss 28009.201 Test MSE 3811.782468024312 Test RE 0.10944688772609171\n",
      "79 Train Loss 27770.898 Test MSE 3829.053336964893 Test RE 0.10969455487045597\n",
      "80 Train Loss 27374.24 Test MSE 3846.562685111083 Test RE 0.10994507233325004\n",
      "81 Train Loss 27177.771 Test MSE 3695.6299830020366 Test RE 0.10776645655850298\n",
      "82 Train Loss 26880.156 Test MSE 3526.864305058116 Test RE 0.10527705741418221\n",
      "83 Train Loss 26592.645 Test MSE 3494.2796011757423 Test RE 0.10478960146511003\n",
      "84 Train Loss 26477.734 Test MSE 3541.075997653422 Test RE 0.10548895399670737\n",
      "85 Train Loss 26273.61 Test MSE 3483.2098677888644 Test RE 0.10462348523473285\n",
      "86 Train Loss 25928.215 Test MSE 3377.478469896458 Test RE 0.10302334742409266\n",
      "87 Train Loss 25694.568 Test MSE 3389.703911629316 Test RE 0.1032096355445933\n",
      "88 Train Loss 25489.67 Test MSE 3435.0808919440133 Test RE 0.10389815762922408\n",
      "89 Train Loss 25061.45 Test MSE 3409.836611047147 Test RE 0.10351568179757654\n",
      "90 Train Loss 24927.777 Test MSE 3375.4878189580113 Test RE 0.10299298250060074\n",
      "91 Train Loss 24778.232 Test MSE 3318.0459764959965 Test RE 0.10211288863522384\n",
      "92 Train Loss 24620.066 Test MSE 3287.3357897828187 Test RE 0.10163923700862021\n",
      "93 Train Loss 24561.148 Test MSE 3253.7703891485316 Test RE 0.10111901103740087\n",
      "94 Train Loss 24351.703 Test MSE 3212.6669489919536 Test RE 0.10047828526574311\n",
      "95 Train Loss 24258.219 Test MSE 3197.035646501149 Test RE 0.10023354754273063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 24150.934 Test MSE 3180.6100491867987 Test RE 0.09997572807844245\n",
      "97 Train Loss 24032.0 Test MSE 3368.4268671806562 Test RE 0.10288520412455204\n",
      "98 Train Loss 23828.008 Test MSE 3352.6095024334195 Test RE 0.10264335714276861\n",
      "99 Train Loss 23631.104 Test MSE 3270.6750869692282 Test RE 0.10138134851454743\n",
      "Training time: 651.73\n",
      "3D_HTTP_rowdy\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 1292889.0 Test MSE 312883.27774372324 Test RE 0.9915863472365269\n",
      "1 Train Loss 582389.25 Test MSE 306482.5266653317 Test RE 0.9813913411723764\n",
      "2 Train Loss 521599.47 Test MSE 282901.52382472705 Test RE 0.9428812631863805\n",
      "3 Train Loss 415990.7 Test MSE 199152.30072223154 Test RE 0.7911011688823978\n",
      "4 Train Loss 312131.06 Test MSE 127917.73836053399 Test RE 0.6340226139943094\n",
      "5 Train Loss 244427.67 Test MSE 88124.20413048496 Test RE 0.5262437683995616\n",
      "6 Train Loss 238971.45 Test MSE 83998.1259626157 Test RE 0.5137764109910334\n",
      "7 Train Loss 233474.8 Test MSE 79421.69601793194 Test RE 0.4995844828855989\n",
      "8 Train Loss 224218.55 Test MSE 74144.51318919112 Test RE 0.48270174768512103\n",
      "9 Train Loss 218920.98 Test MSE 70794.78852291087 Test RE 0.47167190304502\n",
      "10 Train Loss 215647.92 Test MSE 69265.30994370206 Test RE 0.4665489895364977\n",
      "11 Train Loss 212018.66 Test MSE 67707.59724563705 Test RE 0.4612730305708831\n",
      "12 Train Loss 208421.28 Test MSE 65074.99175693172 Test RE 0.45221651948094843\n",
      "13 Train Loss 202244.94 Test MSE 62093.72851165658 Test RE 0.4417364447010989\n",
      "14 Train Loss 198551.55 Test MSE 59746.07938741625 Test RE 0.4333053666882999\n",
      "15 Train Loss 193943.31 Test MSE 55091.16904015971 Test RE 0.41608336812789626\n",
      "16 Train Loss 190496.56 Test MSE 54491.55803119507 Test RE 0.4138128522898147\n",
      "17 Train Loss 184981.72 Test MSE 52361.77962166251 Test RE 0.40564540560071544\n",
      "18 Train Loss 182013.66 Test MSE 50557.451502909506 Test RE 0.3985950941554991\n",
      "19 Train Loss 178490.05 Test MSE 48484.35021046218 Test RE 0.39033738829375114\n",
      "20 Train Loss 175892.88 Test MSE 47375.50022544981 Test RE 0.38584801157554605\n",
      "21 Train Loss 173339.25 Test MSE 45007.63738421547 Test RE 0.3760819327646568\n",
      "22 Train Loss 170315.27 Test MSE 43897.16946930695 Test RE 0.37141344496917095\n",
      "23 Train Loss 168918.69 Test MSE 42727.13993495516 Test RE 0.3664302118888363\n",
      "24 Train Loss 165684.39 Test MSE 42559.70098293114 Test RE 0.36571152444613975\n",
      "25 Train Loss 164432.25 Test MSE 42709.27644152513 Test RE 0.3663536047487408\n",
      "26 Train Loss 162324.88 Test MSE 42223.53400519055 Test RE 0.3642643353695446\n",
      "27 Train Loss 157284.81 Test MSE 40280.12364140636 Test RE 0.3557826451740808\n",
      "28 Train Loss 153831.08 Test MSE 39596.63417596197 Test RE 0.3527511983799342\n",
      "29 Train Loss 151717.45 Test MSE 39216.94286648624 Test RE 0.35105586246739584\n",
      "30 Train Loss 148852.6 Test MSE 37986.765840407774 Test RE 0.34550594302369686\n",
      "31 Train Loss 145728.4 Test MSE 37060.013538509615 Test RE 0.34126531394328646\n",
      "32 Train Loss 144380.48 Test MSE 35973.76347487944 Test RE 0.33622677860752903\n",
      "33 Train Loss 142568.55 Test MSE 35260.395932332314 Test RE 0.3328763605359\n",
      "34 Train Loss 141307.38 Test MSE 35815.20596396115 Test RE 0.3354849858492033\n",
      "35 Train Loss 137619.25 Test MSE 34682.46920092197 Test RE 0.3301371264684821\n",
      "36 Train Loss 133214.16 Test MSE 33621.02052762737 Test RE 0.3250459866095506\n",
      "37 Train Loss 127870.53 Test MSE 31139.49496826414 Test RE 0.31282045586377905\n",
      "38 Train Loss 123261.51 Test MSE 30203.37554741995 Test RE 0.30808255231126136\n",
      "39 Train Loss 118246.3 Test MSE 28511.638843137196 Test RE 0.2993301427511694\n",
      "40 Train Loss 116557.945 Test MSE 27505.43061960713 Test RE 0.29400085031368156\n",
      "41 Train Loss 114979.46 Test MSE 26872.26768913472 Test RE 0.29059726379428175\n",
      "42 Train Loss 114069.06 Test MSE 26636.088631118626 Test RE 0.28931742284105805\n",
      "43 Train Loss 113098.45 Test MSE 26918.099205375493 Test RE 0.2908449697083965\n",
      "44 Train Loss 111607.6 Test MSE 26449.6903718824 Test RE 0.2883033298327326\n",
      "45 Train Loss 110036.625 Test MSE 25844.28869610116 Test RE 0.28498477181367304\n",
      "46 Train Loss 106102.33 Test MSE 23101.05564319133 Test RE 0.26943578465465706\n",
      "47 Train Loss 101878.625 Test MSE 21954.732868856652 Test RE 0.26266574618061006\n",
      "48 Train Loss 100700.42 Test MSE 21329.822232051327 Test RE 0.25890055425648767\n",
      "49 Train Loss 99633.02 Test MSE 20970.914818485762 Test RE 0.2567131113100976\n",
      "50 Train Loss 98750.67 Test MSE 20973.182623769022 Test RE 0.2567269914773868\n",
      "51 Train Loss 95142.59 Test MSE 21203.584825798556 Test RE 0.2581332849609116\n",
      "52 Train Loss 92324.12 Test MSE 20477.399207900704 Test RE 0.25367446941872485\n",
      "53 Train Loss 89977.805 Test MSE 18769.200326346916 Test RE 0.2428634986037958\n",
      "54 Train Loss 86481.63 Test MSE 17786.438909122673 Test RE 0.23641981040496463\n",
      "55 Train Loss 84389.44 Test MSE 17275.161572580564 Test RE 0.23299704934310905\n",
      "56 Train Loss 83140.74 Test MSE 16735.632133550476 Test RE 0.22932976326321924\n",
      "57 Train Loss 82517.484 Test MSE 16527.799378679778 Test RE 0.2279013398728682\n",
      "58 Train Loss 81603.32 Test MSE 16152.185357234735 Test RE 0.22529679160867955\n",
      "59 Train Loss 79490.3 Test MSE 15759.171452243152 Test RE 0.22253895914102864\n",
      "60 Train Loss 77767.445 Test MSE 15708.162202862632 Test RE 0.2221785104379307\n",
      "61 Train Loss 77097.0 Test MSE 15654.274816360452 Test RE 0.2217970875547232\n",
      "62 Train Loss 76530.38 Test MSE 15296.343002896374 Test RE 0.2192467523580245\n",
      "63 Train Loss 75508.484 Test MSE 14699.129048613659 Test RE 0.21492412314835424\n",
      "64 Train Loss 74920.64 Test MSE 14668.387187842589 Test RE 0.2146992586126904\n",
      "65 Train Loss 74441.914 Test MSE 14493.028312176708 Test RE 0.21341204750339063\n",
      "66 Train Loss 74228.07 Test MSE 14332.38078800823 Test RE 0.21222597196950352\n",
      "67 Train Loss 72326.27 Test MSE 13362.574757747818 Test RE 0.20492004197265307\n",
      "68 Train Loss 70002.36 Test MSE 13161.223167041577 Test RE 0.20337028106185892\n",
      "69 Train Loss 68027.625 Test MSE 12417.592072759478 Test RE 0.19754136783419474\n",
      "70 Train Loss 67147.98 Test MSE 11774.240991181256 Test RE 0.192356037667279\n",
      "71 Train Loss 66668.82 Test MSE 11738.935781907394 Test RE 0.19206743016089797\n",
      "72 Train Loss 65970.3 Test MSE 11306.185728030625 Test RE 0.18849395194174295\n",
      "73 Train Loss 65688.2 Test MSE 10821.873586001504 Test RE 0.18441259979695623\n",
      "74 Train Loss 65389.12 Test MSE 10575.490207532941 Test RE 0.1823012371534048\n",
      "75 Train Loss 64710.914 Test MSE 10277.410702701043 Test RE 0.17971371346753298\n",
      "76 Train Loss 63134.086 Test MSE 10304.547852055699 Test RE 0.17995082099073273\n",
      "77 Train Loss 62596.14 Test MSE 10187.86034857819 Test RE 0.17892904904649012\n",
      "78 Train Loss 61785.656 Test MSE 10087.119946129427 Test RE 0.17804220109802274\n",
      "79 Train Loss 60864.504 Test MSE 10081.20558537243 Test RE 0.17798999788194872\n",
      "80 Train Loss 60572.266 Test MSE 10009.677529829843 Test RE 0.17735743754363076\n",
      "81 Train Loss 59850.31 Test MSE 9803.319522178212 Test RE 0.17551972960140702\n",
      "82 Train Loss 58932.684 Test MSE 9969.395148783755 Test RE 0.17700020414532142\n",
      "83 Train Loss 58166.414 Test MSE 9759.858099278836 Test RE 0.17513022833504052\n",
      "84 Train Loss 57876.75 Test MSE 9393.448382667579 Test RE 0.1718113652437427\n",
      "85 Train Loss 57652.188 Test MSE 9398.9453847553 Test RE 0.17186162949515643\n",
      "86 Train Loss 57110.22 Test MSE 9104.069622185081 Test RE 0.16914421442401478\n",
      "87 Train Loss 56637.566 Test MSE 8935.006381803216 Test RE 0.16756634457541464\n",
      "88 Train Loss 56532.16 Test MSE 8814.897980341173 Test RE 0.16643628268736885\n",
      "89 Train Loss 56420.76 Test MSE 8712.886730954568 Test RE 0.16547043030650435\n",
      "90 Train Loss 56268.887 Test MSE 8595.994432331925 Test RE 0.16435670460746019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 Train Loss 56043.46 Test MSE 8508.14494608839 Test RE 0.16351470011420546\n",
      "92 Train Loss 55909.96 Test MSE 8526.643304927824 Test RE 0.16369235995086084\n",
      "93 Train Loss 55764.613 Test MSE 8454.97081813036 Test RE 0.1630029328984526\n",
      "94 Train Loss 54923.996 Test MSE 8681.8974444089 Test RE 0.1651759023039308\n",
      "95 Train Loss 54089.453 Test MSE 8991.33930145159 Test RE 0.168093745974611\n",
      "96 Train Loss 53615.18 Test MSE 8992.870679510046 Test RE 0.16810805997757042\n",
      "97 Train Loss 53031.81 Test MSE 8774.079140976348 Test RE 0.1660504802059302\n",
      "98 Train Loss 52719.027 Test MSE 8842.671212641279 Test RE 0.16669827314107993\n",
      "99 Train Loss 52298.41 Test MSE 8777.527423659094 Test RE 0.16608310657840786\n",
      "Training time: 645.73\n",
      "3D_HTTP_rowdy\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 987764.9 Test MSE 313867.1108924239 Test RE 0.9931441004058078\n",
      "1 Train Loss 559260.06 Test MSE 308670.22827139706 Test RE 0.9848877455067966\n",
      "2 Train Loss 495226.5 Test MSE 261600.0855357435 Test RE 0.9066889084580425\n",
      "3 Train Loss 401034.22 Test MSE 184429.47414768854 Test RE 0.7612977148469853\n",
      "4 Train Loss 327471.22 Test MSE 138363.16006918222 Test RE 0.6594009953210037\n",
      "5 Train Loss 256931.8 Test MSE 92524.05594700004 Test RE 0.5392208708586456\n",
      "6 Train Loss 244985.69 Test MSE 86607.91864160728 Test RE 0.5216967882873755\n",
      "7 Train Loss 243233.31 Test MSE 85609.25428706304 Test RE 0.5186802592455729\n",
      "8 Train Loss 230637.08 Test MSE 74549.64677585756 Test RE 0.4840187182258882\n",
      "9 Train Loss 221328.9 Test MSE 69836.03387132708 Test RE 0.46846715336104927\n",
      "10 Train Loss 212767.4 Test MSE 63273.029305141674 Test RE 0.44591150316919165\n",
      "11 Train Loss 208555.02 Test MSE 61567.63424901529 Test RE 0.43986113975773056\n",
      "12 Train Loss 203576.73 Test MSE 58046.48677931591 Test RE 0.42709779763950495\n",
      "13 Train Loss 195734.64 Test MSE 54000.33204487703 Test RE 0.41194342674427037\n",
      "14 Train Loss 192682.19 Test MSE 52985.597911886536 Test RE 0.40805460367573676\n",
      "15 Train Loss 189781.58 Test MSE 51671.03958497455 Test RE 0.40296095013769584\n",
      "16 Train Loss 183991.08 Test MSE 48006.43920716108 Test RE 0.38840884320898156\n",
      "17 Train Loss 179661.47 Test MSE 47091.47937174473 Test RE 0.38468967420290834\n",
      "18 Train Loss 177869.64 Test MSE 46326.168628637344 Test RE 0.38155096303010233\n",
      "19 Train Loss 176579.38 Test MSE 45256.85364665534 Test RE 0.37712171570537384\n",
      "20 Train Loss 173204.47 Test MSE 43527.36045019275 Test RE 0.36984566113640094\n",
      "21 Train Loss 169696.17 Test MSE 42901.18641481578 Test RE 0.36717576932264995\n",
      "22 Train Loss 168127.8 Test MSE 41853.55965128973 Test RE 0.3626649313224124\n",
      "23 Train Loss 166516.52 Test MSE 40589.76535187927 Test RE 0.3571475148804784\n",
      "24 Train Loss 164413.08 Test MSE 39499.20902135 Test RE 0.3523169694840556\n",
      "25 Train Loss 163447.55 Test MSE 39644.89032538987 Test RE 0.3529660806770863\n",
      "26 Train Loss 162388.64 Test MSE 39635.57038678022 Test RE 0.35292458963581286\n",
      "27 Train Loss 161306.19 Test MSE 38305.822904560315 Test RE 0.346953889481149\n",
      "28 Train Loss 158615.16 Test MSE 38037.666365243465 Test RE 0.3457373465945818\n",
      "29 Train Loss 155408.12 Test MSE 35539.886609679714 Test RE 0.3341930247778766\n",
      "30 Train Loss 151528.25 Test MSE 34053.02208878877 Test RE 0.327127603641137\n",
      "31 Train Loss 148108.31 Test MSE 33126.732106655094 Test RE 0.32264776421272434\n",
      "32 Train Loss 146838.88 Test MSE 32843.208640882716 Test RE 0.32126406616202674\n",
      "33 Train Loss 144199.67 Test MSE 31853.266337075598 Test RE 0.31638533766607174\n",
      "34 Train Loss 142751.05 Test MSE 31282.273904261074 Test RE 0.31353679842270316\n",
      "35 Train Loss 141025.12 Test MSE 31459.145363223255 Test RE 0.31442192500690214\n",
      "36 Train Loss 138061.73 Test MSE 30282.1974214031 Test RE 0.3084842925312322\n",
      "37 Train Loss 134535.69 Test MSE 29846.984076299712 Test RE 0.30625951409636176\n",
      "38 Train Loss 132764.72 Test MSE 29546.65501780738 Test RE 0.30471478211079184\n",
      "39 Train Loss 130431.32 Test MSE 28471.58316432476 Test RE 0.2991198060849315\n",
      "40 Train Loss 129013.3 Test MSE 27808.997022066687 Test RE 0.29561878315972034\n",
      "41 Train Loss 128029.3 Test MSE 27363.351547357775 Test RE 0.29324053772640146\n",
      "42 Train Loss 126629.516 Test MSE 26836.237004083483 Test RE 0.2904023801565696\n",
      "43 Train Loss 125785.83 Test MSE 26882.312521012045 Test RE 0.2906515712305492\n",
      "44 Train Loss 124188.1 Test MSE 26697.755222732296 Test RE 0.2896521361178626\n",
      "45 Train Loss 123176.945 Test MSE 26061.040228251815 Test RE 0.28617733531446016\n",
      "46 Train Loss 120212.19 Test MSE 24902.9732692511 Test RE 0.27974669438503336\n",
      "47 Train Loss 113914.71 Test MSE 23257.102919974084 Test RE 0.27034427033286274\n",
      "48 Train Loss 110461.26 Test MSE 22390.117788631767 Test RE 0.2652574262075354\n",
      "49 Train Loss 107810.805 Test MSE 22331.92805331674 Test RE 0.2649125128554906\n",
      "50 Train Loss 107180.26 Test MSE 21680.01201055706 Test RE 0.2610171969144466\n",
      "51 Train Loss 106001.38 Test MSE 20731.009491393972 Test RE 0.2552405004302973\n",
      "52 Train Loss 104345.39 Test MSE 20372.831690252395 Test RE 0.2530259480948463\n",
      "53 Train Loss 103031.58 Test MSE 19980.367601986225 Test RE 0.2505769387801701\n",
      "54 Train Loss 100489.76 Test MSE 20441.523359032934 Test RE 0.25345215660417547\n",
      "55 Train Loss 98100.09 Test MSE 20651.73987659813 Test RE 0.2547520487128461\n",
      "56 Train Loss 95430.41 Test MSE 20370.62531157032 Test RE 0.2530122463625002\n",
      "57 Train Loss 94152.164 Test MSE 19231.691619525332 Test RE 0.24583748564541735\n",
      "58 Train Loss 88856.266 Test MSE 17114.667525180186 Test RE 0.23191219990756864\n",
      "59 Train Loss 85664.914 Test MSE 16267.5433282955 Test RE 0.2260998885515588\n",
      "60 Train Loss 83486.39 Test MSE 15172.929628072356 Test RE 0.2183605019662553\n",
      "61 Train Loss 81484.14 Test MSE 14762.08774575447 Test RE 0.21538390836620405\n",
      "62 Train Loss 79990.31 Test MSE 15215.880813424734 Test RE 0.2186693485320652\n",
      "63 Train Loss 78802.17 Test MSE 14720.37424604113 Test RE 0.21507938597766552\n",
      "64 Train Loss 77191.91 Test MSE 13816.025501213195 Test RE 0.20836795301600572\n",
      "65 Train Loss 76167.28 Test MSE 13950.503904241961 Test RE 0.20937957295559806\n",
      "66 Train Loss 74797.14 Test MSE 13338.860845736495 Test RE 0.20473813040964867\n",
      "67 Train Loss 71377.01 Test MSE 12820.973404575976 Test RE 0.20072425831500454\n",
      "68 Train Loss 68904.375 Test MSE 11793.262669457496 Test RE 0.19251135375413123\n",
      "69 Train Loss 66699.445 Test MSE 11546.85757292367 Test RE 0.19048959860968123\n",
      "70 Train Loss 65146.1 Test MSE 11355.919037166881 Test RE 0.18890806779968716\n",
      "71 Train Loss 63338.28 Test MSE 10886.182013907921 Test RE 0.18495971939423317\n",
      "72 Train Loss 61560.965 Test MSE 10268.347207081675 Test RE 0.17963445256361782\n",
      "73 Train Loss 60163.246 Test MSE 10077.708864519644 Test RE 0.1779591268065422\n",
      "74 Train Loss 59297.617 Test MSE 10575.480017942602 Test RE 0.1823011493288531\n",
      "75 Train Loss 58206.465 Test MSE 10489.9307630052 Test RE 0.18156229890794698\n",
      "76 Train Loss 56374.766 Test MSE 9869.337171945774 Test RE 0.17610973166208557\n",
      "77 Train Loss 55461.75 Test MSE 9522.856544506893 Test RE 0.17299079049138666\n",
      "78 Train Loss 53326.61 Test MSE 9366.417732763442 Test RE 0.1715639843823384\n",
      "79 Train Loss 51258.527 Test MSE 8896.226092670055 Test RE 0.16720230803135952\n",
      "80 Train Loss 50878.387 Test MSE 8628.347266388699 Test RE 0.16466570965231678\n",
      "81 Train Loss 50736.906 Test MSE 8664.295526588336 Test RE 0.16500837626398535\n",
      "82 Train Loss 50457.133 Test MSE 8557.171398147444 Test RE 0.1639851334685063\n",
      "83 Train Loss 49461.867 Test MSE 8244.85650713633 Test RE 0.1609647995187306\n",
      "84 Train Loss 48431.734 Test MSE 8026.987565465995 Test RE 0.15882382728954006\n",
      "85 Train Loss 47851.887 Test MSE 7936.180336668798 Test RE 0.15792290568298345\n",
      "86 Train Loss 47626.74 Test MSE 7839.865739557854 Test RE 0.15696169316709258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 Train Loss 47339.125 Test MSE 7824.027602702299 Test RE 0.15680306560396254\n",
      "88 Train Loss 46799.44 Test MSE 7715.803257796131 Test RE 0.15571481524439088\n",
      "89 Train Loss 46136.555 Test MSE 7542.7283716253105 Test RE 0.15395847346331168\n",
      "90 Train Loss 45896.39 Test MSE 7435.568881630265 Test RE 0.15286091779129307\n",
      "91 Train Loss 45253.035 Test MSE 7331.017524874488 Test RE 0.1517824263491251\n",
      "92 Train Loss 44897.773 Test MSE 7449.30801744939 Test RE 0.15300207764635665\n",
      "93 Train Loss 44720.926 Test MSE 7481.937750927038 Test RE 0.15333680417074552\n",
      "94 Train Loss 44651.55 Test MSE 7487.288455680779 Test RE 0.15339162374816198\n",
      "95 Train Loss 44651.55 Test MSE 7487.288455680779 Test RE 0.15339162374816198\n",
      "96 Train Loss 44651.55 Test MSE 7487.288455680779 Test RE 0.15339162374816198\n",
      "97 Train Loss 44651.55 Test MSE 7487.288455680779 Test RE 0.15339162374816198\n",
      "98 Train Loss 44651.55 Test MSE 7487.288455680779 Test RE 0.15339162374816198\n",
      "99 Train Loss 44651.55 Test MSE 7487.288455680779 Test RE 0.15339162374816198\n",
      "Training time: 619.56\n",
      "3D_HTTP_rowdy\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 1599808.9 Test MSE 316335.8582183935 Test RE 0.9970422779695386\n",
      "1 Train Loss 1220575.6 Test MSE 312647.18364453124 Test RE 0.9912121631849382\n",
      "2 Train Loss 580077.75 Test MSE 302912.54736682604 Test RE 0.9756588623954736\n",
      "3 Train Loss 491658.6 Test MSE 250204.3522128114 Test RE 0.8867205880419903\n",
      "4 Train Loss 394237.16 Test MSE 180642.80151001198 Test RE 0.7534417679322329\n",
      "5 Train Loss 247239.83 Test MSE 87048.08417957867 Test RE 0.5230208122652513\n",
      "6 Train Loss 245651.19 Test MSE 86117.16616249968 Test RE 0.5202166251045147\n",
      "7 Train Loss 235975.72 Test MSE 77600.92540922304 Test RE 0.493824704329584\n",
      "8 Train Loss 226036.67 Test MSE 71629.7246629233 Test RE 0.4744451408889372\n",
      "9 Train Loss 209663.48 Test MSE 61757.17914821298 Test RE 0.4405377076143188\n",
      "10 Train Loss 194307.36 Test MSE 55152.48108345229 Test RE 0.4163148374175299\n",
      "11 Train Loss 187860.97 Test MSE 51512.418041714205 Test RE 0.40234196299925085\n",
      "12 Train Loss 181613.84 Test MSE 48444.59194835708 Test RE 0.3901773127338901\n",
      "13 Train Loss 180084.69 Test MSE 47678.601513731584 Test RE 0.387080342274823\n",
      "14 Train Loss 175453.31 Test MSE 45998.91994239605 Test RE 0.38020093404882016\n",
      "15 Train Loss 172255.9 Test MSE 44331.709431006326 Test RE 0.37324723675296445\n",
      "16 Train Loss 169777.23 Test MSE 42113.65426223575 Test RE 0.36379005785818336\n",
      "17 Train Loss 166076.6 Test MSE 41173.338230189205 Test RE 0.35970576819555566\n",
      "18 Train Loss 161910.19 Test MSE 38788.04931539562 Test RE 0.3491309352072328\n",
      "19 Train Loss 158979.47 Test MSE 37677.583660179625 Test RE 0.3440969979114676\n",
      "20 Train Loss 156980.88 Test MSE 36545.694803484665 Test RE 0.33888900112144577\n",
      "21 Train Loss 155117.0 Test MSE 36286.07322668926 Test RE 0.33768311745970825\n",
      "22 Train Loss 152315.6 Test MSE 35017.77819076503 Test RE 0.33172916530666624\n",
      "23 Train Loss 145600.52 Test MSE 33018.855917807305 Test RE 0.32212198953865073\n",
      "24 Train Loss 140482.61 Test MSE 31424.33368777742 Test RE 0.31424791227458315\n",
      "25 Train Loss 133162.7 Test MSE 29327.793323175807 Test RE 0.30358412362431275\n",
      "26 Train Loss 131932.1 Test MSE 28432.421359718523 Test RE 0.2989140201580864\n",
      "27 Train Loss 131198.58 Test MSE 28222.095653091634 Test RE 0.2978063761601508\n",
      "28 Train Loss 129304.73 Test MSE 27311.66473422325 Test RE 0.29296345483688135\n",
      "29 Train Loss 128832.2 Test MSE 27186.621532180954 Test RE 0.29229203632120426\n",
      "30 Train Loss 128155.04 Test MSE 27405.89608417688 Test RE 0.2934684143675768\n",
      "31 Train Loss 126704.28 Test MSE 27178.609555313524 Test RE 0.29224896348862095\n",
      "32 Train Loss 125126.5 Test MSE 26727.027409688784 Test RE 0.2898108840836846\n",
      "33 Train Loss 123308.23 Test MSE 26065.14658102198 Test RE 0.2861998804392142\n",
      "34 Train Loss 121445.35 Test MSE 25302.71380071032 Test RE 0.28198299165722923\n",
      "35 Train Loss 119561.0 Test MSE 24544.11696041626 Test RE 0.27772378029516126\n",
      "36 Train Loss 118926.7 Test MSE 24386.873349178197 Test RE 0.2768327223817847\n",
      "37 Train Loss 116510.23 Test MSE 23975.631589598437 Test RE 0.27448864953074714\n",
      "38 Train Loss 114878.05 Test MSE 22834.245036277425 Test RE 0.26787531256667707\n",
      "39 Train Loss 113923.445 Test MSE 22462.548331299415 Test RE 0.26568612485499\n",
      "40 Train Loss 111264.31 Test MSE 21411.619887953046 Test RE 0.25939650761507754\n",
      "41 Train Loss 105762.91 Test MSE 19064.528020230777 Test RE 0.24476673299099544\n",
      "42 Train Loss 103922.24 Test MSE 19239.455035081137 Test RE 0.24588710026143776\n",
      "43 Train Loss 101462.375 Test MSE 19395.181796295317 Test RE 0.24688021649408792\n",
      "44 Train Loss 96684.49 Test MSE 16753.562608774497 Test RE 0.2294525817741412\n",
      "45 Train Loss 92405.15 Test MSE 15540.343354487739 Test RE 0.22098849652076072\n",
      "46 Train Loss 89417.53 Test MSE 14846.460606361714 Test RE 0.21599854582041841\n",
      "47 Train Loss 85949.69 Test MSE 13468.878640158235 Test RE 0.20573353201923777\n",
      "48 Train Loss 84375.98 Test MSE 12950.765647223076 Test RE 0.2017377089874824\n",
      "49 Train Loss 81446.266 Test MSE 11440.312032511662 Test RE 0.1896087159577819\n",
      "50 Train Loss 79415.26 Test MSE 11213.647168590882 Test RE 0.18772097696703954\n",
      "51 Train Loss 78403.805 Test MSE 10982.735370036535 Test RE 0.1857781448987533\n",
      "52 Train Loss 77453.99 Test MSE 11076.300067846085 Test RE 0.1865678121020162\n",
      "53 Train Loss 75762.805 Test MSE 10875.148611233903 Test RE 0.1848659651098823\n",
      "54 Train Loss 71507.77 Test MSE 9355.974012189281 Test RE 0.17146830927441534\n",
      "55 Train Loss 68241.83 Test MSE 8614.212656236441 Test RE 0.16453078001172167\n",
      "56 Train Loss 66442.41 Test MSE 8548.22825161582 Test RE 0.163899420188804\n",
      "57 Train Loss 64806.387 Test MSE 8244.585731673707 Test RE 0.1609621563146992\n",
      "58 Train Loss 63873.027 Test MSE 8308.529430709435 Test RE 0.16158514916430017\n",
      "59 Train Loss 62383.016 Test MSE 8213.77724547106 Test RE 0.16066113198639992\n",
      "60 Train Loss 61550.16 Test MSE 7996.542236871201 Test RE 0.1585223419975509\n",
      "61 Train Loss 60898.004 Test MSE 8200.190873994292 Test RE 0.16052820257136233\n",
      "62 Train Loss 59967.08 Test MSE 8017.809708111216 Test RE 0.15873300371957685\n",
      "63 Train Loss 59502.832 Test MSE 7896.315058114197 Test RE 0.15752576459028844\n",
      "64 Train Loss 59078.973 Test MSE 8018.70422124226 Test RE 0.15874185805768637\n",
      "65 Train Loss 58417.402 Test MSE 7966.756234475572 Test RE 0.15822682976936153\n",
      "66 Train Loss 56846.08 Test MSE 8000.7179534612105 Test RE 0.15856372600835753\n",
      "67 Train Loss 56064.496 Test MSE 7924.0547237401215 Test RE 0.1578022151272746\n",
      "68 Train Loss 55240.652 Test MSE 7369.191105740888 Test RE 0.1521770888667507\n",
      "69 Train Loss 54415.297 Test MSE 7131.250278608068 Test RE 0.14970013830103707\n",
      "70 Train Loss 53948.594 Test MSE 7208.675526408312 Test RE 0.1505106048579276\n",
      "71 Train Loss 53200.008 Test MSE 7070.069168296791 Test RE 0.14905659410495972\n",
      "72 Train Loss 52737.258 Test MSE 6690.609166801175 Test RE 0.14500139913635335\n",
      "73 Train Loss 52160.266 Test MSE 6751.847690246301 Test RE 0.14566347975687105\n",
      "74 Train Loss 51737.465 Test MSE 6464.38140897349 Test RE 0.1425288721437612\n",
      "75 Train Loss 50226.07 Test MSE 6507.000664300304 Test RE 0.14299794216943318\n",
      "76 Train Loss 48172.625 Test MSE 6200.927028338231 Test RE 0.13959429590556904\n",
      "77 Train Loss 47270.0 Test MSE 6001.6776309286215 Test RE 0.13733324892832352\n",
      "78 Train Loss 47016.305 Test MSE 6111.69308713199 Test RE 0.1385862459597787\n",
      "79 Train Loss 46687.188 Test MSE 6031.769816024598 Test RE 0.13767711030337507\n",
      "80 Train Loss 46095.73 Test MSE 6213.515103864794 Test RE 0.13973591445964106\n",
      "81 Train Loss 44848.55 Test MSE 6415.916537983672 Test RE 0.14199358200921655\n",
      "82 Train Loss 44245.656 Test MSE 6645.680961898257 Test RE 0.14451372858623712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 Train Loss 43963.44 Test MSE 6614.063701320968 Test RE 0.14416955199340065\n",
      "84 Train Loss 43631.81 Test MSE 6607.777654807745 Test RE 0.14410102589652057\n",
      "85 Train Loss 42969.65 Test MSE 6332.766528792391 Test RE 0.14107046593589362\n",
      "86 Train Loss 42365.83 Test MSE 6053.41564408632 Test RE 0.1379239256063398\n",
      "87 Train Loss 41942.574 Test MSE 5876.280221891599 Test RE 0.13589097388624094\n",
      "88 Train Loss 41566.984 Test MSE 5908.678069150853 Test RE 0.13626506457906506\n",
      "89 Train Loss 41219.14 Test MSE 6002.70912528144 Test RE 0.13734504999406963\n",
      "90 Train Loss 40897.11 Test MSE 6013.421408220619 Test RE 0.13746754661920532\n",
      "91 Train Loss 40497.59 Test MSE 5995.662558379627 Test RE 0.13726441179739346\n",
      "92 Train Loss 40022.04 Test MSE 5846.675374418587 Test RE 0.1355482305826086\n",
      "93 Train Loss 39868.668 Test MSE 5704.757954217715 Test RE 0.13389303115009826\n",
      "94 Train Loss 39775.58 Test MSE 5668.275719554486 Test RE 0.13346421790333757\n",
      "95 Train Loss 39442.688 Test MSE 5559.5025357106615 Test RE 0.13217743765230325\n",
      "96 Train Loss 38679.957 Test MSE 5350.475020261077 Test RE 0.1296688121985053\n",
      "97 Train Loss 38381.484 Test MSE 5147.843425966803 Test RE 0.1271897245494238\n",
      "98 Train Loss 38275.41 Test MSE 5169.729445352376 Test RE 0.12745981088697836\n",
      "99 Train Loss 38236.234 Test MSE 5157.1059745991415 Test RE 0.12730409977869275\n",
      "Training time: 656.57\n",
      "3D_HTTP_rowdy\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 981350.1 Test MSE 311168.0795659424 Test RE 0.9888647182919234\n",
      "1 Train Loss 563286.75 Test MSE 306990.4145353772 Test RE 0.9822041614627038\n",
      "2 Train Loss 450017.38 Test MSE 210888.84289656513 Test RE 0.814078273364644\n",
      "3 Train Loss 280120.0 Test MSE 103077.27779412916 Test RE 0.5691422579291319\n",
      "4 Train Loss 240978.31 Test MSE 83249.43185995346 Test RE 0.5114815838792863\n",
      "5 Train Loss 234821.08 Test MSE 78207.91338441754 Test RE 0.4957522702101529\n",
      "6 Train Loss 221663.58 Test MSE 71203.1822167786 Test RE 0.473030412832891\n",
      "7 Train Loss 194089.31 Test MSE 53914.22456388389 Test RE 0.41161485874135123\n",
      "8 Train Loss 179974.72 Test MSE 46883.74105172602 Test RE 0.3838402305643531\n",
      "9 Train Loss 176783.08 Test MSE 45428.68591948254 Test RE 0.3778369696690109\n",
      "10 Train Loss 171398.0 Test MSE 42419.61474064847 Test RE 0.3651091543790261\n",
      "11 Train Loss 161954.06 Test MSE 39740.224536415735 Test RE 0.353390215255541\n",
      "12 Train Loss 156380.2 Test MSE 37745.25862420666 Test RE 0.34440588588134685\n",
      "13 Train Loss 151867.08 Test MSE 35490.12374715396 Test RE 0.33395897469109836\n",
      "14 Train Loss 146148.23 Test MSE 33342.189525725014 Test RE 0.3236953197853085\n",
      "15 Train Loss 144739.4 Test MSE 32910.57828083379 Test RE 0.32159339386325314\n",
      "16 Train Loss 142794.31 Test MSE 31617.972550615716 Test RE 0.31521463372221653\n",
      "17 Train Loss 138123.6 Test MSE 30483.74507433775 Test RE 0.3095091715148673\n",
      "18 Train Loss 136009.47 Test MSE 28341.709923558687 Test RE 0.29843680822290286\n",
      "19 Train Loss 132427.17 Test MSE 27973.142645937784 Test RE 0.29648996047650883\n",
      "20 Train Loss 129536.48 Test MSE 26681.29885878247 Test RE 0.289562852286921\n",
      "21 Train Loss 128232.52 Test MSE 25858.631285166826 Test RE 0.2850638386548462\n",
      "22 Train Loss 121995.016 Test MSE 25398.303147928902 Test RE 0.2825151314276953\n",
      "23 Train Loss 120904.42 Test MSE 25446.856835612885 Test RE 0.28278504319897013\n",
      "24 Train Loss 119889.48 Test MSE 24925.319844540674 Test RE 0.2798721809828998\n",
      "25 Train Loss 118261.02 Test MSE 24429.159623979103 Test RE 0.2770726292000779\n",
      "26 Train Loss 116238.27 Test MSE 23599.31077413789 Test RE 0.272325946609486\n",
      "27 Train Loss 114398.07 Test MSE 22337.099013737155 Test RE 0.2649431813411538\n",
      "28 Train Loss 112998.234 Test MSE 21718.34819489093 Test RE 0.261247869844039\n",
      "29 Train Loss 111029.805 Test MSE 21632.412218772693 Test RE 0.2607304998561435\n",
      "30 Train Loss 108497.44 Test MSE 21231.62147531507 Test RE 0.25830388822721045\n",
      "31 Train Loss 106486.35 Test MSE 21179.493171649538 Test RE 0.257986596904201\n",
      "32 Train Loss 101594.07 Test MSE 21359.048758118188 Test RE 0.25907786875679095\n",
      "33 Train Loss 98661.72 Test MSE 19319.62453293935 Test RE 0.24639886508749356\n",
      "34 Train Loss 97069.44 Test MSE 18896.72431697848 Test RE 0.24368714834208716\n",
      "35 Train Loss 94131.2 Test MSE 17114.275583249175 Test RE 0.23190954438896944\n",
      "36 Train Loss 89387.44 Test MSE 15722.031343244884 Test RE 0.22227657235772008\n",
      "37 Train Loss 85955.84 Test MSE 14773.569239443768 Test RE 0.2154676515457703\n",
      "38 Train Loss 81606.81 Test MSE 13074.065061388907 Test RE 0.2026957688357126\n",
      "39 Train Loss 75370.414 Test MSE 13229.595881702962 Test RE 0.20389785230829896\n",
      "40 Train Loss 68844.64 Test MSE 12813.605854959911 Test RE 0.2006665771077856\n",
      "41 Train Loss 65457.94 Test MSE 11260.184704667632 Test RE 0.18811010222290814\n",
      "42 Train Loss 62537.516 Test MSE 10486.285145023532 Test RE 0.1815307465404631\n",
      "43 Train Loss 60331.438 Test MSE 9535.575613493138 Test RE 0.17310627829614153\n",
      "44 Train Loss 58719.56 Test MSE 8901.962331443721 Test RE 0.16725620491946616\n",
      "45 Train Loss 57607.348 Test MSE 9171.368742815977 Test RE 0.16976823741184768\n",
      "46 Train Loss 56816.664 Test MSE 8805.694042976096 Test RE 0.16634936907800446\n",
      "47 Train Loss 55398.543 Test MSE 8414.454689696928 Test RE 0.16261190974821085\n",
      "48 Train Loss 54613.156 Test MSE 8283.19906683667 Test RE 0.16133864735960157\n",
      "49 Train Loss 53782.082 Test MSE 8068.232189886556 Test RE 0.1592313423079685\n",
      "50 Train Loss 52770.707 Test MSE 8166.003769393491 Test RE 0.1601932270543881\n",
      "51 Train Loss 52047.266 Test MSE 7878.9408452301195 Test RE 0.15735236767694935\n",
      "52 Train Loss 51918.23 Test MSE 7824.487838615268 Test RE 0.1568076773809451\n",
      "53 Train Loss 51042.812 Test MSE 7415.031127505021 Test RE 0.1526496635591849\n",
      "54 Train Loss 50343.254 Test MSE 7698.930273829065 Test RE 0.1555444628224439\n",
      "55 Train Loss 50099.86 Test MSE 7494.627177173381 Test RE 0.1534667793073027\n",
      "56 Train Loss 49720.625 Test MSE 7562.252979857866 Test RE 0.15415760804528966\n",
      "57 Train Loss 49509.72 Test MSE 7395.398975897592 Test RE 0.15244745081473\n",
      "58 Train Loss 49265.734 Test MSE 7475.872543208976 Test RE 0.15327464058727086\n",
      "59 Train Loss 48680.836 Test MSE 7107.284034062612 Test RE 0.14944837532622482\n",
      "60 Train Loss 47647.46 Test MSE 6715.007185512554 Test RE 0.14526554007148199\n",
      "61 Train Loss 46863.094 Test MSE 6487.524663150014 Test RE 0.1427837793538394\n",
      "62 Train Loss 46728.03 Test MSE 6528.578604901177 Test RE 0.1432348445031216\n",
      "63 Train Loss 46496.004 Test MSE 6550.40243286196 Test RE 0.14347404853185072\n",
      "64 Train Loss 45805.996 Test MSE 6610.688155710985 Test RE 0.1441327581985572\n",
      "65 Train Loss 45135.86 Test MSE 6315.247009013597 Test RE 0.1408751959452959\n",
      "66 Train Loss 44015.11 Test MSE 6042.569304274175 Test RE 0.13780030610261712\n",
      "67 Train Loss 42482.766 Test MSE 6207.33497304857 Test RE 0.13966640460410182\n",
      "68 Train Loss 41361.07 Test MSE 5905.116452704311 Test RE 0.1362239896510517\n",
      "69 Train Loss 41084.1 Test MSE 5928.8585230603485 Test RE 0.1364975655577205\n",
      "70 Train Loss 40621.957 Test MSE 5786.263994097558 Test RE 0.13484612915668112\n",
      "71 Train Loss 40047.824 Test MSE 5542.258369334115 Test RE 0.13197228800964442\n",
      "72 Train Loss 39932.863 Test MSE 5413.701130144754 Test RE 0.13043270479515737\n",
      "73 Train Loss 39774.74 Test MSE 5464.063579117866 Test RE 0.1310379934559609\n",
      "74 Train Loss 39568.25 Test MSE 5269.964077964106 Test RE 0.12868952256458835\n",
      "75 Train Loss 39292.746 Test MSE 5213.051441111925 Test RE 0.12799274917258566\n",
      "76 Train Loss 39157.29 Test MSE 5201.23599724329 Test RE 0.12784761833707323\n",
      "77 Train Loss 38876.7 Test MSE 5027.852696043413 Test RE 0.1256986562263855\n",
      "78 Train Loss 38440.535 Test MSE 4906.333668577059 Test RE 0.12417034915885107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 Train Loss 38329.49 Test MSE 4914.190720947993 Test RE 0.12426973321275589\n",
      "80 Train Loss 38285.08 Test MSE 4909.936341883914 Test RE 0.12421592933566476\n",
      "81 Train Loss 38079.656 Test MSE 4751.623636052556 Test RE 0.12219695366187511\n",
      "82 Train Loss 37269.926 Test MSE 4748.037367669421 Test RE 0.12215083113360911\n",
      "83 Train Loss 36897.78 Test MSE 4648.527626651814 Test RE 0.12086402984321684\n",
      "84 Train Loss 36592.57 Test MSE 4618.30236302132 Test RE 0.12047045308729362\n",
      "85 Train Loss 36469.074 Test MSE 4550.39429485916 Test RE 0.11958146709872482\n",
      "86 Train Loss 36398.742 Test MSE 4604.830635667367 Test RE 0.12029461679022355\n",
      "87 Train Loss 36295.258 Test MSE 4711.879280440002 Test RE 0.12168483002410188\n",
      "88 Train Loss 36111.55 Test MSE 4749.783540541589 Test RE 0.12217329060937375\n",
      "89 Train Loss 35958.324 Test MSE 4734.626710025944 Test RE 0.12197820387825375\n",
      "90 Train Loss 35586.16 Test MSE 4773.577567025186 Test RE 0.12247892169055143\n",
      "91 Train Loss 35299.137 Test MSE 4687.597535582024 Test RE 0.12137088559746184\n",
      "92 Train Loss 34899.902 Test MSE 4561.068013973368 Test RE 0.11972163422428697\n",
      "93 Train Loss 34727.7 Test MSE 4553.579319900939 Test RE 0.11962330999396507\n",
      "94 Train Loss 34667.305 Test MSE 4484.216011809846 Test RE 0.11870872071022681\n",
      "95 Train Loss 34622.996 Test MSE 4513.499068500736 Test RE 0.11909568886737876\n",
      "96 Train Loss 34417.77 Test MSE 4505.106098989289 Test RE 0.11898490656932362\n",
      "97 Train Loss 34312.75 Test MSE 4549.956922996501 Test RE 0.1195757200334764\n",
      "98 Train Loss 34219.883 Test MSE 4751.127357447666 Test RE 0.122190572125596\n",
      "99 Train Loss 34174.79 Test MSE 4775.844804307984 Test RE 0.12250800426112717\n",
      "Training time: 640.66\n",
      "3D_HTTP_rowdy\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 1359563.1 Test MSE 313667.6383541849 Test RE 0.9928284629064033\n",
      "1 Train Loss 706423.75 Test MSE 309760.42562454304 Test RE 0.9866254829229658\n",
      "2 Train Loss 544248.0 Test MSE 305130.94088539935 Test RE 0.9792249856854987\n",
      "3 Train Loss 472647.66 Test MSE 242964.78248569282 Test RE 0.8737979590778037\n",
      "4 Train Loss 385212.94 Test MSE 175541.51129018195 Test RE 0.7427271142501105\n",
      "5 Train Loss 277356.2 Test MSE 107755.67519197901 Test RE 0.5819148476965371\n",
      "6 Train Loss 236635.56 Test MSE 81942.7807167402 Test RE 0.5074516991045145\n",
      "7 Train Loss 228880.1 Test MSE 74732.9447330649 Test RE 0.4846133902231021\n",
      "8 Train Loss 220558.95 Test MSE 69340.02283563993 Test RE 0.46680054280078015\n",
      "9 Train Loss 210257.64 Test MSE 64896.039336814516 Test RE 0.45159430693862535\n",
      "10 Train Loss 203175.19 Test MSE 60654.115147822864 Test RE 0.43658569120793533\n",
      "11 Train Loss 195111.97 Test MSE 56280.45002043733 Test RE 0.42055048959592456\n",
      "12 Train Loss 189249.52 Test MSE 54186.34119457088 Test RE 0.41265230548022536\n",
      "13 Train Loss 185115.39 Test MSE 51779.22299465168 Test RE 0.4033825682799206\n",
      "14 Train Loss 180677.02 Test MSE 48845.8429382077 Test RE 0.3917898372723812\n",
      "15 Train Loss 177141.36 Test MSE 45921.318941765414 Test RE 0.3798800957889265\n",
      "16 Train Loss 173719.58 Test MSE 44521.588239188495 Test RE 0.37404571728884056\n",
      "17 Train Loss 170199.27 Test MSE 42484.09932360488 Test RE 0.3653865610795169\n",
      "18 Train Loss 166873.77 Test MSE 40705.05009260938 Test RE 0.3576543478709971\n",
      "19 Train Loss 164542.08 Test MSE 39378.84251816824 Test RE 0.3517797496385729\n",
      "20 Train Loss 161946.33 Test MSE 38375.13846321515 Test RE 0.3472676594408981\n",
      "21 Train Loss 159095.66 Test MSE 37668.240449995465 Test RE 0.3440543310310833\n",
      "22 Train Loss 156478.86 Test MSE 37275.21797856421 Test RE 0.3422547294156855\n",
      "23 Train Loss 155306.94 Test MSE 37273.724938464875 Test RE 0.34224787492656394\n",
      "24 Train Loss 153448.92 Test MSE 37003.48868334416 Test RE 0.3410049614554928\n",
      "25 Train Loss 151025.17 Test MSE 35638.40080334834 Test RE 0.3346558846256333\n",
      "26 Train Loss 147569.98 Test MSE 34802.83717855239 Test RE 0.3307095124682839\n",
      "27 Train Loss 146299.67 Test MSE 34796.85053364904 Test RE 0.3306810675809385\n",
      "28 Train Loss 144555.84 Test MSE 33303.07310789076 Test RE 0.3235053874895362\n",
      "29 Train Loss 141006.14 Test MSE 32378.515304674696 Test RE 0.31898321246982986\n",
      "30 Train Loss 138514.66 Test MSE 31891.518662097133 Test RE 0.3165752529384949\n",
      "31 Train Loss 134619.12 Test MSE 31079.422920777484 Test RE 0.3125185749730095\n",
      "32 Train Loss 133378.58 Test MSE 30794.047417051683 Test RE 0.3110804720378754\n",
      "33 Train Loss 131874.06 Test MSE 30423.564992588508 Test RE 0.30920350878959924\n",
      "34 Train Loss 130508.53 Test MSE 29619.558776709684 Test RE 0.30509047889574054\n",
      "35 Train Loss 127994.45 Test MSE 29683.402621762616 Test RE 0.30541910743137657\n",
      "36 Train Loss 126986.34 Test MSE 29593.594351348038 Test RE 0.30495672883247954\n",
      "37 Train Loss 124493.234 Test MSE 29476.803677933633 Test RE 0.3043543803916017\n",
      "38 Train Loss 119720.31 Test MSE 27916.943059237314 Test RE 0.29619197839248956\n",
      "39 Train Loss 117313.805 Test MSE 26922.876512297757 Test RE 0.29087077751215945\n",
      "40 Train Loss 113505.97 Test MSE 24343.372354389787 Test RE 0.27658570683062594\n",
      "41 Train Loss 110990.6 Test MSE 23186.47207328143 Test RE 0.26993344607322356\n",
      "42 Train Loss 108924.35 Test MSE 22362.075543588297 Test RE 0.265091264860952\n",
      "43 Train Loss 106902.414 Test MSE 22514.477203417133 Test RE 0.2659930538810093\n",
      "44 Train Loss 103460.57 Test MSE 21587.09615047132 Test RE 0.2604572646006488\n",
      "45 Train Loss 100796.28 Test MSE 20629.32451027497 Test RE 0.25461375743876485\n",
      "46 Train Loss 99159.39 Test MSE 19973.551142154585 Test RE 0.2505341919855343\n",
      "47 Train Loss 97384.55 Test MSE 19692.131946592137 Test RE 0.2487629685681075\n",
      "48 Train Loss 95901.13 Test MSE 18906.56166216962 Test RE 0.24375056998720834\n",
      "49 Train Loss 92861.54 Test MSE 18484.371397555715 Test RE 0.2410136862494577\n",
      "50 Train Loss 88097.87 Test MSE 17426.187325371997 Test RE 0.2340133055786943\n",
      "51 Train Loss 86569.66 Test MSE 16550.109664904474 Test RE 0.22805510594005818\n",
      "52 Train Loss 83149.805 Test MSE 15972.833136126423 Test RE 0.22404246359232555\n",
      "53 Train Loss 79398.66 Test MSE 14901.133000378419 Test RE 0.21639588987510555\n",
      "54 Train Loss 74056.59 Test MSE 13807.269030019794 Test RE 0.20830191169434104\n",
      "55 Train Loss 70501.55 Test MSE 12814.597354510792 Test RE 0.20067434061224518\n",
      "56 Train Loss 68591.2 Test MSE 12550.652763052894 Test RE 0.1985969247880063\n",
      "57 Train Loss 66604.76 Test MSE 11947.15994474039 Test RE 0.19376337977127092\n",
      "58 Train Loss 64625.605 Test MSE 11124.095496087979 Test RE 0.18696990890196322\n",
      "59 Train Loss 63370.79 Test MSE 10831.291345546986 Test RE 0.1844928250841566\n",
      "60 Train Loss 61625.94 Test MSE 10472.846606185443 Test RE 0.1814143902696904\n",
      "61 Train Loss 60745.25 Test MSE 10359.1333387943 Test RE 0.18042681127055007\n",
      "62 Train Loss 59941.406 Test MSE 10350.976458559824 Test RE 0.18035576238701118\n",
      "63 Train Loss 59109.12 Test MSE 10436.128440157181 Test RE 0.1810960884357471\n",
      "64 Train Loss 58355.92 Test MSE 10185.144300064196 Test RE 0.1789051965222304\n",
      "65 Train Loss 57499.547 Test MSE 9906.78155969075 Test RE 0.1764434966387218\n",
      "66 Train Loss 56865.28 Test MSE 9699.762877138983 Test RE 0.17459022351236408\n",
      "67 Train Loss 56459.15 Test MSE 9569.121769276975 Test RE 0.1734105049142998\n",
      "68 Train Loss 55877.61 Test MSE 9529.665829824524 Test RE 0.17305262766956955\n",
      "69 Train Loss 54760.68 Test MSE 9159.27959109971 Test RE 0.16965631132943654\n",
      "70 Train Loss 54091.902 Test MSE 8759.402007286617 Test RE 0.16591153883582477\n",
      "71 Train Loss 53276.76 Test MSE 8740.047682074639 Test RE 0.1657281426626385\n",
      "72 Train Loss 52625.805 Test MSE 8283.732291179464 Test RE 0.16134384029949309\n",
      "73 Train Loss 51593.336 Test MSE 8077.572601671778 Test RE 0.15932348492798376\n",
      "74 Train Loss 50360.715 Test MSE 7745.86467936313 Test RE 0.1560178593998728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 Train Loss 48365.297 Test MSE 7255.264591305129 Test RE 0.15099619025045416\n",
      "76 Train Loss 47060.133 Test MSE 7357.586477643811 Test RE 0.15205722128025068\n",
      "77 Train Loss 46520.1 Test MSE 7203.166027998778 Test RE 0.15045307719978804\n",
      "78 Train Loss 45689.035 Test MSE 7125.037316366259 Test RE 0.14963491243337076\n",
      "79 Train Loss 44931.477 Test MSE 6702.278353456791 Test RE 0.145127793550595\n",
      "80 Train Loss 44485.496 Test MSE 6635.872861773779 Test RE 0.14440704813566266\n",
      "81 Train Loss 44116.3 Test MSE 6697.005040762928 Test RE 0.14507068947426094\n",
      "82 Train Loss 43717.92 Test MSE 6647.536602722132 Test RE 0.14453390310701883\n",
      "83 Train Loss 43570.805 Test MSE 6593.608376684635 Test RE 0.14394644266335066\n",
      "84 Train Loss 43334.574 Test MSE 6424.788736403619 Test RE 0.14209172543135595\n",
      "85 Train Loss 42999.65 Test MSE 6348.965862726414 Test RE 0.14125078112794556\n",
      "86 Train Loss 42350.81 Test MSE 6201.067494093264 Test RE 0.1395958769681296\n",
      "87 Train Loss 41781.797 Test MSE 6080.945746567163 Test RE 0.13823719935887746\n",
      "88 Train Loss 41235.53 Test MSE 5799.319019959838 Test RE 0.13499816403228374\n",
      "89 Train Loss 40841.85 Test MSE 5722.431699739681 Test RE 0.13410027582201484\n",
      "90 Train Loss 40208.164 Test MSE 5599.663429352661 Test RE 0.1326539921438338\n",
      "91 Train Loss 39902.363 Test MSE 5254.601449451833 Test RE 0.12850181234371727\n",
      "92 Train Loss 39712.598 Test MSE 5244.275201659722 Test RE 0.12837548552992206\n",
      "93 Train Loss 39561.24 Test MSE 5169.4285988419615 Test RE 0.1274561021438402\n",
      "94 Train Loss 39277.504 Test MSE 5084.950176594688 Test RE 0.12641037311149408\n",
      "95 Train Loss 39031.83 Test MSE 5141.606548682379 Test RE 0.12711265274918476\n",
      "96 Train Loss 38869.773 Test MSE 5071.3428900012395 Test RE 0.1262411232267584\n",
      "97 Train Loss 38670.33 Test MSE 4928.919550056778 Test RE 0.12445582471165649\n",
      "98 Train Loss 38473.56 Test MSE 5043.340268913029 Test RE 0.1258921056307823\n",
      "99 Train Loss 38295.65 Test MSE 5050.280366554837 Test RE 0.1259786953799843\n",
      "Training time: 649.52\n",
      "3D_HTTP_rowdy\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 1085624.6 Test MSE 311172.8998363799 Test RE 0.9888723774594629\n",
      "1 Train Loss 583264.06 Test MSE 307388.40279910207 Test RE 0.9828406294075932\n",
      "2 Train Loss 526527.75 Test MSE 287541.5037150616 Test RE 0.9505820999281779\n",
      "3 Train Loss 459465.0 Test MSE 224789.5661941436 Test RE 0.8404801032389628\n",
      "4 Train Loss 356141.97 Test MSE 157003.7336570545 Test RE 0.7024159301004309\n",
      "5 Train Loss 255703.33 Test MSE 91464.56255204784 Test RE 0.536124671339138\n",
      "6 Train Loss 247032.6 Test MSE 86237.50911229312 Test RE 0.5205799820860137\n",
      "7 Train Loss 238421.75 Test MSE 80696.49035827722 Test RE 0.5035779269071966\n",
      "8 Train Loss 230635.67 Test MSE 76704.74952888714 Test RE 0.4909649515064569\n",
      "9 Train Loss 222283.06 Test MSE 70415.64494595966 Test RE 0.47040718106704393\n",
      "10 Train Loss 210367.44 Test MSE 63109.03911261721 Test RE 0.4453332745247692\n",
      "11 Train Loss 195887.94 Test MSE 55662.024182073525 Test RE 0.4182335420760737\n",
      "12 Train Loss 188125.75 Test MSE 50350.60233215233 Test RE 0.3977788586889219\n",
      "13 Train Loss 182588.03 Test MSE 47426.503256722746 Test RE 0.38605565185786317\n",
      "14 Train Loss 174832.56 Test MSE 44283.217368268226 Test RE 0.37304304340144656\n",
      "15 Train Loss 171634.03 Test MSE 42529.41958394984 Test RE 0.36558139868310974\n",
      "16 Train Loss 169740.58 Test MSE 42100.988076442445 Test RE 0.363735346636127\n",
      "17 Train Loss 163825.55 Test MSE 40153.10268097894 Test RE 0.3552212325846725\n",
      "18 Train Loss 159799.0 Test MSE 38824.346652538086 Test RE 0.3492942530355027\n",
      "19 Train Loss 155710.1 Test MSE 36493.8908940537 Test RE 0.3386487265692268\n",
      "20 Train Loss 153565.55 Test MSE 36010.375528464574 Test RE 0.33639783135745627\n",
      "21 Train Loss 151724.1 Test MSE 34677.886775559426 Test RE 0.3301153160445089\n",
      "22 Train Loss 149851.44 Test MSE 33345.27264606844 Test RE 0.3237102853375103\n",
      "23 Train Loss 147201.03 Test MSE 32461.110770658728 Test RE 0.31938980594907257\n",
      "24 Train Loss 145640.8 Test MSE 32149.385433130105 Test RE 0.31785254996002915\n",
      "25 Train Loss 144516.2 Test MSE 31422.535481025407 Test RE 0.31423892098128337\n",
      "26 Train Loss 141136.98 Test MSE 30719.300227033746 Test RE 0.3107026958028556\n",
      "27 Train Loss 138236.83 Test MSE 29661.56463339605 Test RE 0.30530673880945525\n",
      "28 Train Loss 135714.11 Test MSE 28513.790964622654 Test RE 0.29934143958821113\n",
      "29 Train Loss 132991.52 Test MSE 28079.342033168006 Test RE 0.2970522359446151\n",
      "30 Train Loss 131605.12 Test MSE 28240.870838037023 Test RE 0.297905419835574\n",
      "31 Train Loss 130446.06 Test MSE 28044.525155320545 Test RE 0.2968680144018666\n",
      "32 Train Loss 127324.734 Test MSE 26959.795167004737 Test RE 0.29107014103371065\n",
      "33 Train Loss 124407.375 Test MSE 26115.118732512696 Test RE 0.28647410056173706\n",
      "34 Train Loss 122940.97 Test MSE 25320.512269740902 Test RE 0.282082150650602\n",
      "35 Train Loss 122350.664 Test MSE 25297.41295213014 Test RE 0.2819534527815302\n",
      "36 Train Loss 121163.2 Test MSE 25142.31569628943 Test RE 0.2810878022087295\n",
      "37 Train Loss 120438.14 Test MSE 24973.867143388554 Test RE 0.2801446033434949\n",
      "38 Train Loss 118031.4 Test MSE 24633.478333153646 Test RE 0.2782288958363941\n",
      "39 Train Loss 115992.21 Test MSE 23867.374856953407 Test RE 0.27386825162061496\n",
      "40 Train Loss 114849.75 Test MSE 23549.354690795808 Test RE 0.2720375579990264\n",
      "41 Train Loss 114369.234 Test MSE 23698.458431301828 Test RE 0.2728974077852779\n",
      "42 Train Loss 114033.82 Test MSE 23636.316157298068 Test RE 0.2725393761164255\n",
      "43 Train Loss 113481.234 Test MSE 23296.159854930527 Test RE 0.27057117712635764\n",
      "44 Train Loss 112525.3 Test MSE 22818.046189146746 Test RE 0.2677802789650536\n",
      "45 Train Loss 112076.08 Test MSE 22580.045553198193 Test RE 0.266380094758038\n",
      "46 Train Loss 111434.83 Test MSE 22707.00184885376 Test RE 0.2671279060688697\n",
      "47 Train Loss 110064.39 Test MSE 22636.603826555558 Test RE 0.2667134992165721\n",
      "48 Train Loss 108643.125 Test MSE 22328.888687314226 Test RE 0.2648944850013607\n",
      "49 Train Loss 108396.55 Test MSE 22145.482796137974 Test RE 0.26380434152692905\n",
      "50 Train Loss 107883.414 Test MSE 21934.368108957282 Test RE 0.2625438962383453\n",
      "51 Train Loss 106399.33 Test MSE 21388.838653078135 Test RE 0.2592584763620414\n",
      "52 Train Loss 104940.43 Test MSE 20978.769791016904 Test RE 0.25676118469328807\n",
      "53 Train Loss 101117.97 Test MSE 19965.11356803791 Test RE 0.25048126889505756\n",
      "54 Train Loss 99592.14 Test MSE 19818.89646463414 Test RE 0.2495623673199414\n",
      "55 Train Loss 99123.625 Test MSE 19905.86804706034 Test RE 0.2501093471738567\n",
      "56 Train Loss 98793.04 Test MSE 19693.277043010603 Test RE 0.24877020123961646\n",
      "57 Train Loss 98160.08 Test MSE 19480.247165275046 Test RE 0.24742102039719677\n",
      "58 Train Loss 97162.73 Test MSE 19181.96934576391 Test RE 0.24551948164291487\n",
      "59 Train Loss 95788.81 Test MSE 18515.14160024048 Test RE 0.24121420581502026\n",
      "60 Train Loss 95247.99 Test MSE 18518.225008665402 Test RE 0.24123429021317302\n",
      "61 Train Loss 94512.09 Test MSE 18268.45783164066 Test RE 0.23960192655172896\n",
      "62 Train Loss 92854.805 Test MSE 17242.54937814925 Test RE 0.23277701856130223\n",
      "63 Train Loss 91963.766 Test MSE 16953.82761757494 Test RE 0.23081989744180004\n",
      "64 Train Loss 89765.01 Test MSE 16115.267346680052 Test RE 0.22503917112932428\n",
      "65 Train Loss 89251.79 Test MSE 15990.45496578656 Test RE 0.22416601555679763\n",
      "66 Train Loss 88625.17 Test MSE 16097.759156008153 Test RE 0.22491689269112033\n",
      "67 Train Loss 88341.445 Test MSE 15686.146174200583 Test RE 0.22202275690263384\n",
      "68 Train Loss 87865.29 Test MSE 15551.285907828045 Test RE 0.22106628607863352\n",
      "69 Train Loss 86365.05 Test MSE 15295.423907615475 Test RE 0.21924016543452068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 Train Loss 85378.75 Test MSE 14953.454720920074 Test RE 0.21677546785387933\n",
      "71 Train Loss 84726.34 Test MSE 14930.589918286074 Test RE 0.2166096725729821\n",
      "72 Train Loss 83883.07 Test MSE 14882.415406426913 Test RE 0.21625993769119953\n",
      "73 Train Loss 82620.945 Test MSE 14395.099767520556 Test RE 0.21268981906006187\n",
      "74 Train Loss 81619.86 Test MSE 14017.461546859666 Test RE 0.2098814466043282\n",
      "75 Train Loss 80937.09 Test MSE 14289.051526320245 Test RE 0.21190493126783883\n",
      "76 Train Loss 80319.75 Test MSE 13819.534389298691 Test RE 0.20839441118269217\n",
      "77 Train Loss 79045.2 Test MSE 12901.918458623566 Test RE 0.20135689638588028\n",
      "78 Train Loss 75405.945 Test MSE 11390.084022733756 Test RE 0.18919202519369166\n",
      "79 Train Loss 74510.836 Test MSE 11773.810907001478 Test RE 0.19235252448777826\n",
      "80 Train Loss 74099.36 Test MSE 11294.345406810071 Test RE 0.1883952266237771\n",
      "81 Train Loss 73934.72 Test MSE 10909.608715486402 Test RE 0.18515862602346886\n",
      "82 Train Loss 73468.16 Test MSE 10838.463917087613 Test RE 0.18455390131892205\n",
      "83 Train Loss 72071.14 Test MSE 10575.76882617882 Test RE 0.1823036385640394\n",
      "84 Train Loss 70460.69 Test MSE 10455.13234440206 Test RE 0.18126089895116604\n",
      "85 Train Loss 68249.54 Test MSE 10309.588903303396 Test RE 0.1799948321596839\n",
      "86 Train Loss 67712.35 Test MSE 10173.05831548121 Test RE 0.1787990179934952\n",
      "87 Train Loss 67257.09 Test MSE 10011.305306720686 Test RE 0.17737185791837776\n",
      "88 Train Loss 66789.21 Test MSE 9733.672581252447 Test RE 0.17489513496623743\n",
      "89 Train Loss 65896.22 Test MSE 9985.941448339352 Test RE 0.177147027707121\n",
      "90 Train Loss 65555.02 Test MSE 9817.57881724758 Test RE 0.17564733322439152\n",
      "91 Train Loss 64261.016 Test MSE 9612.244974076531 Test RE 0.1738008025254986\n",
      "92 Train Loss 63454.746 Test MSE 9438.982368016623 Test RE 0.1722272827070217\n",
      "93 Train Loss 63119.547 Test MSE 9157.922475461663 Test RE 0.16964374201324564\n",
      "94 Train Loss 62916.043 Test MSE 9231.481851893535 Test RE 0.17032369586953655\n",
      "95 Train Loss 62608.29 Test MSE 9316.616272330755 Test RE 0.17110727164210082\n",
      "96 Train Loss 62350.668 Test MSE 9265.048540488127 Test RE 0.17063307278666157\n",
      "97 Train Loss 62092.617 Test MSE 9180.548172544044 Test RE 0.16985317488997986\n",
      "98 Train Loss 61791.39 Test MSE 8831.147039618467 Test RE 0.16658961332508904\n",
      "99 Train Loss 60673.54 Test MSE 8821.67578494016 Test RE 0.16650025709996996\n",
      "Training time: 647.11\n",
      "3D_HTTP_rowdy\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 1630989.0 Test MSE 317831.0500221866 Test RE 0.9993958082254304\n",
      "1 Train Loss 1400742.9 Test MSE 314217.5483619975 Test RE 0.9936983760865077\n",
      "2 Train Loss 1158431.0 Test MSE 313775.88011413836 Test RE 0.9929997528602493\n",
      "3 Train Loss 841122.94 Test MSE 310394.4596814532 Test RE 0.9876347054199351\n",
      "4 Train Loss 653714.25 Test MSE 306454.55124819675 Test RE 0.9813465499423809\n",
      "5 Train Loss 560020.1 Test MSE 302643.61795872974 Test RE 0.9752256653798789\n",
      "6 Train Loss 535501.8 Test MSE 295380.70673071657 Test RE 0.963452759194447\n",
      "7 Train Loss 522126.1 Test MSE 286956.447428749 Test RE 0.9496145401000659\n",
      "8 Train Loss 509149.6 Test MSE 274893.14860318584 Test RE 0.9294399163511631\n",
      "9 Train Loss 496010.6 Test MSE 260784.00127654785 Test RE 0.9052735562437888\n",
      "10 Train Loss 487817.0 Test MSE 258356.26722367216 Test RE 0.9010499414446745\n",
      "11 Train Loss 478813.34 Test MSE 250445.3377952785 Test RE 0.8871475099697489\n",
      "12 Train Loss 458175.62 Test MSE 231813.1753607284 Test RE 0.8535096194393859\n",
      "13 Train Loss 446220.47 Test MSE 228783.85029362276 Test RE 0.8479144656688637\n",
      "14 Train Loss 429349.28 Test MSE 215662.24329829286 Test RE 0.8232399192680622\n",
      "15 Train Loss 417241.88 Test MSE 206497.53260779238 Test RE 0.8055579639837201\n",
      "16 Train Loss 405932.88 Test MSE 197511.76434713262 Test RE 0.787836044514403\n",
      "17 Train Loss 386856.03 Test MSE 179936.6391180401 Test RE 0.7519676620577552\n",
      "18 Train Loss 371738.2 Test MSE 166390.298493921 Test RE 0.7231083268583262\n",
      "19 Train Loss 363301.03 Test MSE 161642.95903323637 Test RE 0.7127180508555105\n",
      "20 Train Loss 342627.62 Test MSE 146902.27937053517 Test RE 0.6794439398992007\n",
      "21 Train Loss 326462.3 Test MSE 138173.1380123117 Test RE 0.6589480431637716\n",
      "22 Train Loss 310279.34 Test MSE 127651.7472803104 Test RE 0.6333630802860342\n",
      "23 Train Loss 260443.0 Test MSE 96160.10730531033 Test RE 0.549714043315684\n",
      "24 Train Loss 243708.12 Test MSE 84506.44835516233 Test RE 0.515328648781663\n",
      "25 Train Loss 236763.23 Test MSE 79964.37654934743 Test RE 0.5012883826805828\n",
      "26 Train Loss 232891.75 Test MSE 77246.94504313775 Test RE 0.4926971143610179\n",
      "27 Train Loss 224553.2 Test MSE 72197.83567574891 Test RE 0.4763228891246375\n",
      "28 Train Loss 217987.4 Test MSE 68168.87723412651 Test RE 0.46284164956420315\n",
      "29 Train Loss 212688.44 Test MSE 64580.21705371148 Test RE 0.4504941050423358\n",
      "30 Train Loss 208769.56 Test MSE 61521.55689181978 Test RE 0.4396965124124809\n",
      "31 Train Loss 204814.53 Test MSE 59601.99665963708 Test RE 0.4327825750240359\n",
      "32 Train Loss 200925.25 Test MSE 57983.23745912904 Test RE 0.4268650444652862\n",
      "33 Train Loss 198072.39 Test MSE 55795.81514813094 Test RE 0.41873587994904743\n",
      "34 Train Loss 195930.6 Test MSE 54457.43189521887 Test RE 0.41368325381740795\n",
      "35 Train Loss 194553.03 Test MSE 53770.99242122936 Test RE 0.4110677333057537\n",
      "36 Train Loss 193874.88 Test MSE 53445.830409320166 Test RE 0.4098229515943763\n",
      "37 Train Loss 192203.61 Test MSE 51906.60604897009 Test RE 0.4038784480623558\n",
      "38 Train Loss 189789.56 Test MSE 51427.74624267187 Test RE 0.40201115899336354\n",
      "39 Train Loss 187786.94 Test MSE 49554.11454160242 Test RE 0.39462011824478577\n",
      "40 Train Loss 186331.12 Test MSE 49415.73253738804 Test RE 0.39406873617771726\n",
      "41 Train Loss 184070.62 Test MSE 49885.55903385441 Test RE 0.39593763433892115\n",
      "42 Train Loss 182426.22 Test MSE 49072.820986054765 Test RE 0.39269907149355154\n",
      "43 Train Loss 180096.75 Test MSE 48245.61158278944 Test RE 0.38937518490953354\n",
      "44 Train Loss 178308.6 Test MSE 46713.72930718467 Test RE 0.3831436499056559\n",
      "45 Train Loss 177373.03 Test MSE 46636.34676013991 Test RE 0.3828261745020377\n",
      "46 Train Loss 176063.7 Test MSE 45825.4404475096 Test RE 0.37948331534232077\n",
      "47 Train Loss 175495.02 Test MSE 45561.26245194103 Test RE 0.37838789713740667\n",
      "48 Train Loss 174372.33 Test MSE 45078.78982886542 Test RE 0.37637908879133897\n",
      "49 Train Loss 173173.89 Test MSE 44580.77485134816 Test RE 0.3742942612747928\n",
      "50 Train Loss 171472.3 Test MSE 44267.51654708455 Test RE 0.3729769054633926\n",
      "51 Train Loss 169552.66 Test MSE 43822.81136741199 Test RE 0.3710987401202377\n",
      "52 Train Loss 167834.1 Test MSE 43208.732561210614 Test RE 0.36848950730471014\n",
      "53 Train Loss 166473.08 Test MSE 42697.37518310857 Test RE 0.36630255761016806\n",
      "54 Train Loss 164278.34 Test MSE 42106.58902948048 Test RE 0.36375954080513473\n",
      "55 Train Loss 163016.9 Test MSE 41993.9363474577 Test RE 0.3632726106810178\n",
      "56 Train Loss 161212.97 Test MSE 40664.83750193303 Test RE 0.35747764053775993\n",
      "57 Train Loss 159589.97 Test MSE 40626.61928855575 Test RE 0.3573096161568238\n",
      "58 Train Loss 157749.67 Test MSE 39929.9534425206 Test RE 0.354232793577543\n",
      "59 Train Loss 154637.86 Test MSE 38365.18311830521 Test RE 0.34722261213019706\n",
      "60 Train Loss 151650.39 Test MSE 37744.745345531526 Test RE 0.3444035441726894\n",
      "61 Train Loss 149555.08 Test MSE 36816.422384000296 Test RE 0.3401419163200186\n",
      "62 Train Loss 147356.77 Test MSE 36407.02986234045 Test RE 0.338245468637682\n",
      "63 Train Loss 143265.9 Test MSE 34925.324278070635 Test RE 0.3312909603270277\n",
      "64 Train Loss 140727.88 Test MSE 33225.90073978181 Test RE 0.32313034467243446\n",
      "65 Train Loss 138321.7 Test MSE 32060.00149902254 Test RE 0.31741038464347965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 Train Loss 136838.28 Test MSE 32128.82413426155 Test RE 0.31775089161619535\n",
      "67 Train Loss 135244.19 Test MSE 31279.955104749897 Test RE 0.31352517774496086\n",
      "68 Train Loss 132679.33 Test MSE 30834.407053136885 Test RE 0.31128426115649854\n",
      "69 Train Loss 131734.17 Test MSE 30228.068467825342 Test RE 0.3082084637956754\n",
      "70 Train Loss 130661.71 Test MSE 29452.750587267717 Test RE 0.30423017836460636\n",
      "71 Train Loss 127810.86 Test MSE 28662.281570304764 Test RE 0.3001198641828894\n",
      "72 Train Loss 125385.01 Test MSE 27972.03463524679 Test RE 0.296484088463784\n",
      "73 Train Loss 123437.26 Test MSE 27328.65928688793 Test RE 0.2930545882160695\n",
      "74 Train Loss 121370.43 Test MSE 26563.059283666753 Test RE 0.2889205333967201\n",
      "75 Train Loss 120273.664 Test MSE 26169.560017308468 Test RE 0.28677254642744926\n",
      "76 Train Loss 119383.98 Test MSE 25959.846520156938 Test RE 0.28562118881932147\n",
      "77 Train Loss 117827.35 Test MSE 25297.971504907055 Test RE 0.2819565654518854\n",
      "78 Train Loss 116395.14 Test MSE 25098.63261775242 Test RE 0.28084351049919365\n",
      "79 Train Loss 115953.8 Test MSE 24864.842984430456 Test RE 0.2795324447192541\n",
      "80 Train Loss 115048.3 Test MSE 24452.897677685 Test RE 0.27720721360771844\n",
      "81 Train Loss 114221.42 Test MSE 24094.224615661285 Test RE 0.2751666772265487\n",
      "82 Train Loss 112232.93 Test MSE 23729.054136925337 Test RE 0.27307351197431634\n",
      "83 Train Loss 111434.02 Test MSE 22960.03382178136 Test RE 0.2686121320239212\n",
      "84 Train Loss 110642.85 Test MSE 22003.858103829698 Test RE 0.2629594483748903\n",
      "85 Train Loss 109923.12 Test MSE 21760.109836760694 Test RE 0.26149892255331375\n",
      "86 Train Loss 109203.766 Test MSE 21911.234475491252 Test RE 0.26240541044982246\n",
      "87 Train Loss 107501.83 Test MSE 21412.863387725152 Test RE 0.25940403985263577\n",
      "88 Train Loss 106045.06 Test MSE 20877.351123908677 Test RE 0.25613979636261613\n",
      "89 Train Loss 101696.69 Test MSE 19930.103560633368 Test RE 0.25026155567336367\n",
      "90 Train Loss 99766.28 Test MSE 19653.164391694285 Test RE 0.24851671577610246\n",
      "91 Train Loss 99038.73 Test MSE 19205.212723988396 Test RE 0.2456681883364438\n",
      "92 Train Loss 98081.555 Test MSE 18979.401474681013 Test RE 0.24421965787374628\n",
      "93 Train Loss 97343.875 Test MSE 19074.685563604125 Test RE 0.24483192992710134\n",
      "94 Train Loss 96808.164 Test MSE 18892.03358774047 Test RE 0.24365690126441095\n",
      "95 Train Loss 96018.16 Test MSE 18854.93998786127 Test RE 0.24341757942548628\n",
      "96 Train Loss 95102.69 Test MSE 18359.851455894157 Test RE 0.24020052028492922\n",
      "97 Train Loss 94750.77 Test MSE 18140.474431284834 Test RE 0.23876116138892908\n",
      "98 Train Loss 94274.89 Test MSE 17893.32099665578 Test RE 0.23712909220704767\n",
      "99 Train Loss 94053.91 Test MSE 18010.3945323412 Test RE 0.2379035789333838\n",
      "Training time: 640.07\n",
      "3D_HTTP_rowdy\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 1611828.8 Test MSE 316128.41132843954 Test RE 0.9967153039089303\n",
      "1 Train Loss 1332648.0 Test MSE 313473.0580954188 Test RE 0.9925204700424429\n",
      "2 Train Loss 1161595.4 Test MSE 311280.204023611 Test RE 0.9890428630651713\n",
      "3 Train Loss 968841.7 Test MSE 307192.38271364995 Test RE 0.9825272030937945\n",
      "4 Train Loss 819921.8 Test MSE 297916.0697872153 Test RE 0.9675787620021592\n",
      "5 Train Loss 640706.1 Test MSE 278361.5077012346 Test RE 0.9352849634502646\n",
      "6 Train Loss 532646.94 Test MSE 271272.78881328297 Test RE 0.9232992404426748\n",
      "7 Train Loss 462040.47 Test MSE 236976.57593049135 Test RE 0.8629627948051549\n",
      "8 Train Loss 431380.75 Test MSE 220067.087066802 Test RE 0.8316046511413931\n",
      "9 Train Loss 411156.34 Test MSE 200647.43826862442 Test RE 0.7940652154210891\n",
      "10 Train Loss 332083.66 Test MSE 144289.4776526526 Test RE 0.6733745414460224\n",
      "11 Train Loss 270110.4 Test MSE 98592.08523778849 Test RE 0.5566220266684834\n",
      "12 Train Loss 242546.72 Test MSE 86073.51353198405 Test RE 0.5200847599745381\n",
      "13 Train Loss 238619.11 Test MSE 83159.79004431826 Test RE 0.511206131625958\n",
      "14 Train Loss 212925.6 Test MSE 66258.91923896705 Test RE 0.4563116282287501\n",
      "15 Train Loss 197365.58 Test MSE 59336.47628856312 Test RE 0.4318174994982072\n",
      "16 Train Loss 188123.12 Test MSE 53653.77464922823 Test RE 0.4106194364622132\n",
      "17 Train Loss 182903.61 Test MSE 49234.22548161531 Test RE 0.3933443509121714\n",
      "18 Train Loss 175271.34 Test MSE 46210.768642654686 Test RE 0.3810754388075918\n",
      "19 Train Loss 171321.94 Test MSE 44566.30599563326 Test RE 0.3742335170508001\n",
      "20 Train Loss 165722.77 Test MSE 42994.46489641398 Test RE 0.36757472101142435\n",
      "21 Train Loss 160996.9 Test MSE 40225.44686088956 Test RE 0.3555410911049197\n",
      "22 Train Loss 157223.84 Test MSE 37815.93624018019 Test RE 0.34472818335057265\n",
      "23 Train Loss 145623.83 Test MSE 33623.38376071305 Test RE 0.32505741020585716\n",
      "24 Train Loss 131551.86 Test MSE 28691.462196215085 Test RE 0.30027259901632164\n",
      "25 Train Loss 128202.195 Test MSE 27584.91531677387 Test RE 0.29442534303857965\n",
      "26 Train Loss 125281.586 Test MSE 27882.325366774334 Test RE 0.2960082787766157\n",
      "27 Train Loss 117943.86 Test MSE 24108.176791009293 Test RE 0.27524633569590773\n",
      "28 Train Loss 112031.67 Test MSE 22322.278578886344 Test RE 0.26485527322286656\n",
      "29 Train Loss 106339.55 Test MSE 22076.562736353448 Test RE 0.2633935223311941\n",
      "30 Train Loss 101337.47 Test MSE 19837.50788699572 Test RE 0.24967951866364216\n",
      "31 Train Loss 98532.4 Test MSE 18262.502577505787 Test RE 0.2395628699799495\n",
      "32 Train Loss 94854.07 Test MSE 17375.640130549924 Test RE 0.23367366426419814\n",
      "33 Train Loss 92616.73 Test MSE 17037.90173037998 Test RE 0.23139150815463513\n",
      "34 Train Loss 90663.86 Test MSE 16195.95716906454 Test RE 0.22560185796390936\n",
      "35 Train Loss 88791.98 Test MSE 15710.036298203586 Test RE 0.22219176377995914\n",
      "36 Train Loss 86250.11 Test MSE 15827.256690458267 Test RE 0.22301916485012174\n",
      "37 Train Loss 83977.375 Test MSE 15270.794760181721 Test RE 0.2190635807961624\n",
      "38 Train Loss 79893.43 Test MSE 13430.19692200825 Test RE 0.2054378931344571\n",
      "39 Train Loss 74138.38 Test MSE 12366.243760988458 Test RE 0.19713251547792088\n",
      "40 Train Loss 68585.01 Test MSE 11999.02110031519 Test RE 0.19418347589830923\n",
      "41 Train Loss 64990.598 Test MSE 11003.67454287241 Test RE 0.18595515855595582\n",
      "42 Train Loss 63653.766 Test MSE 11152.693936879665 Test RE 0.187210090931153\n",
      "43 Train Loss 62156.734 Test MSE 10739.133912692769 Test RE 0.18370627502116887\n",
      "44 Train Loss 60985.32 Test MSE 10575.026308702003 Test RE 0.18229723874498244\n",
      "45 Train Loss 59585.89 Test MSE 9895.722807643588 Test RE 0.1763449888788384\n",
      "46 Train Loss 57972.8 Test MSE 9674.16318286911 Test RE 0.17435968131996815\n",
      "47 Train Loss 55914.426 Test MSE 8722.271040586225 Test RE 0.1655595171903272\n",
      "48 Train Loss 54432.06 Test MSE 8395.728696338258 Test RE 0.16243086619228805\n",
      "49 Train Loss 53303.832 Test MSE 7799.93382520425 Test RE 0.1565614450866588\n",
      "50 Train Loss 52435.742 Test MSE 7710.469845082661 Test RE 0.15566098838328557\n",
      "51 Train Loss 50160.625 Test MSE 7201.2757030339335 Test RE 0.15043333422369198\n",
      "52 Train Loss 47738.516 Test MSE 6794.872850948568 Test RE 0.14612685233296627\n",
      "53 Train Loss 46437.574 Test MSE 6599.499788891127 Test RE 0.1440107366304856\n",
      "54 Train Loss 45617.273 Test MSE 6332.54296089803 Test RE 0.14106797578366365\n",
      "55 Train Loss 44811.945 Test MSE 6292.337539469364 Test RE 0.14061944124001907\n",
      "56 Train Loss 44112.33 Test MSE 6070.6326358462375 Test RE 0.13811992643924462\n",
      "57 Train Loss 42727.06 Test MSE 5879.485061961251 Test RE 0.13592802534208284\n",
      "58 Train Loss 41693.348 Test MSE 5592.827422277609 Test RE 0.13257299615457527\n",
      "59 Train Loss 40411.605 Test MSE 5111.067238598088 Test RE 0.12673458859860204\n",
      "60 Train Loss 39680.86 Test MSE 4847.468150416628 Test RE 0.12342321197810391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 Train Loss 39501.78 Test MSE 4952.379978275144 Test RE 0.12475166244971955\n",
      "62 Train Loss 38746.496 Test MSE 5034.497639071669 Test RE 0.12578169213321197\n",
      "63 Train Loss 38261.8 Test MSE 4928.539767283705 Test RE 0.12445102983843158\n",
      "64 Train Loss 38112.027 Test MSE 4851.778786692494 Test RE 0.12347807715026264\n",
      "65 Train Loss 37787.473 Test MSE 4826.897101116714 Test RE 0.12316104993386981\n",
      "66 Train Loss 37096.25 Test MSE 4801.802984027616 Test RE 0.12284048735488146\n",
      "67 Train Loss 36768.387 Test MSE 4793.336967827713 Test RE 0.12273215009549981\n",
      "68 Train Loss 36650.645 Test MSE 4647.707636541438 Test RE 0.12085336929776079\n",
      "69 Train Loss 36210.953 Test MSE 4497.452674083452 Test RE 0.11888379583924341\n",
      "70 Train Loss 35711.957 Test MSE 4310.88115040907 Test RE 0.11639180065017232\n",
      "71 Train Loss 35596.29 Test MSE 4313.440086645274 Test RE 0.11642634057364923\n",
      "72 Train Loss 35399.953 Test MSE 4394.693915915805 Test RE 0.11751780698786204\n",
      "73 Train Loss 35192.01 Test MSE 4456.234898628251 Test RE 0.11833777515395784\n",
      "74 Train Loss 35035.508 Test MSE 4470.042099340456 Test RE 0.11852096228141992\n",
      "75 Train Loss 34687.137 Test MSE 4361.0452055545 Test RE 0.11706704510593166\n",
      "76 Train Loss 34430.0 Test MSE 4360.651229217371 Test RE 0.11706175707460448\n",
      "77 Train Loss 34280.766 Test MSE 4414.43091205288 Test RE 0.11778140328847296\n",
      "78 Train Loss 33915.68 Test MSE 4214.786538695731 Test RE 0.11508723452537256\n",
      "79 Train Loss 33358.43 Test MSE 4202.923178077603 Test RE 0.11492515235937825\n",
      "80 Train Loss 32909.414 Test MSE 4247.251845141497 Test RE 0.1155296264334638\n",
      "81 Train Loss 32658.982 Test MSE 4254.1745977403125 Test RE 0.11562374110068148\n",
      "82 Train Loss 32516.936 Test MSE 4176.0083999035305 Test RE 0.11455658115029935\n",
      "83 Train Loss 32276.203 Test MSE 4204.554651794915 Test RE 0.11494745578554472\n",
      "84 Train Loss 32129.879 Test MSE 4348.989042763405 Test RE 0.11690511647630089\n",
      "85 Train Loss 31976.766 Test MSE 4279.40460299974 Test RE 0.11596609601647201\n",
      "86 Train Loss 31871.537 Test MSE 4380.436548480226 Test RE 0.11732702513651379\n",
      "87 Train Loss 31689.125 Test MSE 4383.842032520125 Test RE 0.11737262306730238\n",
      "88 Train Loss 31563.824 Test MSE 4340.341318271146 Test RE 0.11678882895631683\n",
      "89 Train Loss 31465.08 Test MSE 4365.079837671826 Test RE 0.11712118502488784\n",
      "90 Train Loss 31300.867 Test MSE 4467.040400377069 Test RE 0.1184811613145605\n",
      "91 Train Loss 30989.207 Test MSE 4676.3889064400855 Test RE 0.12122569230485879\n",
      "92 Train Loss 30706.2 Test MSE 4589.341914909528 Test RE 0.12009213603898644\n",
      "93 Train Loss 30295.844 Test MSE 4771.097639678855 Test RE 0.1224471029673642\n",
      "94 Train Loss 29675.293 Test MSE 4419.0225950341555 Test RE 0.11784264270033837\n",
      "95 Train Loss 29337.857 Test MSE 4392.183111771129 Test RE 0.11748423168557355\n",
      "96 Train Loss 29209.771 Test MSE 4267.683507539599 Test RE 0.11580717417857998\n",
      "97 Train Loss 29071.46 Test MSE 3988.603806845599 Test RE 0.11195662860690866\n",
      "98 Train Loss 29041.207 Test MSE 3959.8324845209913 Test RE 0.11155210483386262\n",
      "99 Train Loss 28983.535 Test MSE 3964.333454056129 Test RE 0.11161548504397396\n",
      "Training time: 648.92\n",
      "3D_HTTP_rowdy\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 1006578.06 Test MSE 310414.87258226634 Test RE 0.9876671804806245\n",
      "1 Train Loss 553407.06 Test MSE 276932.05103129504 Test RE 0.9328804101865045\n",
      "2 Train Loss 456345.16 Test MSE 220500.34328341088 Test RE 0.8324228578306649\n",
      "3 Train Loss 399272.7 Test MSE 179204.46426596757 Test RE 0.750436197883189\n",
      "4 Train Loss 359128.3 Test MSE 159421.89520256768 Test RE 0.7078045434109282\n",
      "5 Train Loss 325910.28 Test MSE 131009.52058127381 Test RE 0.6416390555446991\n",
      "6 Train Loss 290492.3 Test MSE 114706.03939012882 Test RE 0.6003886927393135\n",
      "7 Train Loss 272841.56 Test MSE 105178.23782747416 Test RE 0.5749132360033454\n",
      "8 Train Loss 244448.62 Test MSE 87550.34069047568 Test RE 0.5245275241995631\n",
      "9 Train Loss 241409.62 Test MSE 86571.92712439709 Test RE 0.5215883766641615\n",
      "10 Train Loss 238088.2 Test MSE 83857.72787108032 Test RE 0.5133468573931671\n",
      "11 Train Loss 235389.47 Test MSE 81718.63319113493 Test RE 0.5067571782901953\n",
      "12 Train Loss 231442.2 Test MSE 76761.02963700079 Test RE 0.4911450348395943\n",
      "13 Train Loss 227866.55 Test MSE 76170.52791517739 Test RE 0.4892522650909452\n",
      "14 Train Loss 218166.48 Test MSE 68864.15936968752 Test RE 0.46519601672747707\n",
      "15 Train Loss 215392.67 Test MSE 65828.73625404676 Test RE 0.45482792496155666\n",
      "16 Train Loss 209319.38 Test MSE 64526.50623575174 Test RE 0.45030673001441146\n",
      "17 Train Loss 197258.05 Test MSE 57678.999916852044 Test RE 0.4257436930138982\n",
      "18 Train Loss 192376.95 Test MSE 55411.88556203758 Test RE 0.41729273763299835\n",
      "19 Train Loss 182183.14 Test MSE 51827.2699724968 Test RE 0.40356967826543405\n",
      "20 Train Loss 180663.25 Test MSE 51003.5661107138 Test RE 0.40034981620829907\n",
      "21 Train Loss 178181.4 Test MSE 49638.08927450516 Test RE 0.3949543396530122\n",
      "22 Train Loss 177087.56 Test MSE 48574.489367612194 Test RE 0.3907000655794117\n",
      "23 Train Loss 174623.61 Test MSE 47134.780668073865 Test RE 0.38486649742381696\n",
      "24 Train Loss 173371.14 Test MSE 46425.41802721169 Test RE 0.38195946263089664\n",
      "25 Train Loss 172661.5 Test MSE 45888.55811668855 Test RE 0.37974456608198676\n",
      "26 Train Loss 169268.69 Test MSE 44726.5599036863 Test RE 0.3749057577271461\n",
      "27 Train Loss 168704.12 Test MSE 44627.811864078765 Test RE 0.37449166747192986\n",
      "28 Train Loss 167555.47 Test MSE 44648.61515890142 Test RE 0.3745789421169028\n",
      "29 Train Loss 165652.8 Test MSE 44251.760086483395 Test RE 0.3729105213581804\n",
      "30 Train Loss 163031.38 Test MSE 42705.581003231375 Test RE 0.36633775495521304\n",
      "31 Train Loss 161863.3 Test MSE 41894.70301817489 Test RE 0.36284314305918175\n",
      "32 Train Loss 160153.62 Test MSE 41144.20447180392 Test RE 0.35957848394009545\n",
      "33 Train Loss 158705.84 Test MSE 40230.62211289191 Test RE 0.3555639616471851\n",
      "34 Train Loss 157501.72 Test MSE 39384.24441488173 Test RE 0.3518038769692344\n",
      "35 Train Loss 155872.56 Test MSE 39093.22593619391 Test RE 0.3505016904973932\n",
      "36 Train Loss 155082.6 Test MSE 38925.61431953618 Test RE 0.3497494980217207\n",
      "37 Train Loss 154517.03 Test MSE 38694.17877316505 Test RE 0.34870821528100576\n",
      "38 Train Loss 153114.77 Test MSE 37890.5736950714 Test RE 0.3450682113075123\n",
      "39 Train Loss 152612.75 Test MSE 37437.480450783856 Test RE 0.34299885373426475\n",
      "40 Train Loss 152248.62 Test MSE 37286.42763373638 Test RE 0.34230618812351243\n",
      "41 Train Loss 151637.17 Test MSE 37275.2867950088 Test RE 0.3422550453460009\n",
      "42 Train Loss 151110.05 Test MSE 36897.58538124318 Test RE 0.34051663685613526\n",
      "43 Train Loss 150851.73 Test MSE 36718.1294963305 Test RE 0.33968755554639146\n",
      "44 Train Loss 150533.02 Test MSE 36508.89806187883 Test RE 0.33871834967298187\n",
      "45 Train Loss 150194.39 Test MSE 36465.104256175386 Test RE 0.33851513597074234\n",
      "46 Train Loss 149892.64 Test MSE 36185.06026400643 Test RE 0.3372127697302378\n",
      "47 Train Loss 149521.92 Test MSE 35976.55295304961 Test RE 0.3362398142060827\n",
      "48 Train Loss 149420.08 Test MSE 35847.989157436794 Test RE 0.3356384926245757\n",
      "49 Train Loss 148902.98 Test MSE 35730.41607499801 Test RE 0.3350876324110204\n",
      "50 Train Loss 148366.77 Test MSE 35394.56915130578 Test RE 0.3335090914534846\n",
      "51 Train Loss 148139.33 Test MSE 35236.4758250339 Test RE 0.3327634322929303\n",
      "52 Train Loss 147863.58 Test MSE 35230.01903515733 Test RE 0.33273294283637805\n",
      "53 Train Loss 147658.45 Test MSE 35094.73102545818 Test RE 0.3320934587017119\n",
      "54 Train Loss 147404.08 Test MSE 34900.45252943512 Test RE 0.3311729764119851\n",
      "55 Train Loss 147263.0 Test MSE 34842.80141316161 Test RE 0.3308993355062456\n",
      "56 Train Loss 147169.31 Test MSE 34751.48043874687 Test RE 0.33046541695697906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 Train Loss 147100.7 Test MSE 34713.20007330722 Test RE 0.3302833553236383\n",
      "58 Train Loss 146948.1 Test MSE 34695.19283769401 Test RE 0.33019767809291667\n",
      "59 Train Loss 146908.88 Test MSE 34725.970855442356 Test RE 0.3303441043887479\n",
      "60 Train Loss 146839.8 Test MSE 34823.04247946424 Test RE 0.33080549768373024\n",
      "61 Train Loss 146538.42 Test MSE 34690.15767210069 Test RE 0.3301737171320797\n",
      "62 Train Loss 146112.05 Test MSE 34312.13576712961 Test RE 0.32836982237143597\n",
      "63 Train Loss 145715.14 Test MSE 34082.10754583103 Test RE 0.32726727736869465\n",
      "64 Train Loss 145443.6 Test MSE 33706.37821674131 Test RE 0.32545834153158887\n",
      "65 Train Loss 145020.19 Test MSE 33442.284792130646 Test RE 0.3241808321225992\n",
      "66 Train Loss 144849.05 Test MSE 33423.15863257101 Test RE 0.3240881168501491\n",
      "67 Train Loss 144579.28 Test MSE 33340.81728777169 Test RE 0.3236886586814668\n",
      "68 Train Loss 144358.67 Test MSE 33069.16323738408 Test RE 0.3223672876512487\n",
      "69 Train Loss 143784.55 Test MSE 33027.06427903753 Test RE 0.32216202619754425\n",
      "70 Train Loss 143356.12 Test MSE 32828.24764941586 Test RE 0.3211908854856941\n",
      "71 Train Loss 143259.06 Test MSE 32899.925975334365 Test RE 0.32154134390689487\n",
      "72 Train Loss 143115.5 Test MSE 32775.5564953375 Test RE 0.3209330173986078\n",
      "73 Train Loss 142471.3 Test MSE 32501.752275631334 Test RE 0.3195896823813573\n",
      "74 Train Loss 142054.48 Test MSE 32387.44061630468 Test RE 0.3190271741582654\n",
      "75 Train Loss 141856.36 Test MSE 32302.69344906022 Test RE 0.31860950671638\n",
      "76 Train Loss 141334.81 Test MSE 32035.54665718828 Test RE 0.31728930383907544\n",
      "77 Train Loss 140910.17 Test MSE 31756.195391513596 Test RE 0.31590288702190555\n",
      "78 Train Loss 140422.67 Test MSE 31713.01998874379 Test RE 0.31568806479098915\n",
      "79 Train Loss 140146.06 Test MSE 31737.918540716393 Test RE 0.31581196711091314\n",
      "80 Train Loss 139654.95 Test MSE 32123.747022819745 Test RE 0.3177257845593839\n",
      "81 Train Loss 139422.44 Test MSE 32108.15851740048 Test RE 0.3176486847254287\n",
      "82 Train Loss 138788.16 Test MSE 31858.953951380787 Test RE 0.3164135827671139\n",
      "83 Train Loss 138595.05 Test MSE 31890.50185476828 Test RE 0.31657020616404447\n",
      "84 Train Loss 138209.31 Test MSE 31564.550208676428 Test RE 0.3149482247514327\n",
      "85 Train Loss 137504.69 Test MSE 30732.220071989752 Test RE 0.31076802620821203\n",
      "86 Train Loss 137379.48 Test MSE 30433.34818922252 Test RE 0.30925321952503976\n",
      "87 Train Loss 136663.92 Test MSE 30578.470529476646 Test RE 0.30998968426564566\n",
      "88 Train Loss 135961.88 Test MSE 30322.559881664652 Test RE 0.30868980996135753\n",
      "89 Train Loss 135010.14 Test MSE 29821.988269855075 Test RE 0.30613124641394773\n",
      "90 Train Loss 134794.67 Test MSE 29829.585109540167 Test RE 0.30617023579808844\n",
      "91 Train Loss 134438.44 Test MSE 29676.7287747144 Test RE 0.30538477115567164\n",
      "92 Train Loss 133012.8 Test MSE 29292.976017729685 Test RE 0.3034038659414449\n",
      "93 Train Loss 132204.6 Test MSE 29149.51721829188 Test RE 0.30266001226145134\n",
      "94 Train Loss 131651.89 Test MSE 29020.22104465942 Test RE 0.301988023901465\n",
      "95 Train Loss 131407.89 Test MSE 28872.279426219724 Test RE 0.30121729094387195\n",
      "96 Train Loss 131273.38 Test MSE 28802.645389553127 Test RE 0.3008538343964192\n",
      "97 Train Loss 131034.73 Test MSE 28841.0145723819 Test RE 0.30105415755739395\n",
      "98 Train Loss 130639.96 Test MSE 28612.63059335669 Test RE 0.29985980632107134\n",
      "99 Train Loss 130510.88 Test MSE 28609.704006636788 Test RE 0.2998444706432608\n",
      "Training time: 637.81\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10 #10\n",
    "max_iter = 100 #1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 1.0\n",
    "rowdy_terms = 6\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "\n",
    "    alpha_val = []    \n",
    "    omega_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_D = 5000 #Total number of data points for 'y'\n",
    "    N_N = 3500\n",
    "    N_f = 10000#Total number of collocation points \n",
    "\n",
    "    layers = np.array([3,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.1, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)\n",
    "\n",
    "\n",
    "\n",
    "    #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time,\"alpha\": alpha_full,\"omega\": omega_full,  \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8rH35Ss9wR_Y"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '3D_HTTP_rowdy_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '3D_HTTP_rowdy_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fd36ee311c39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"3D_HTTP_rowdy_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '3D_HTTP_rowdy_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(50):\n",
    "    label = \"3D_HTTP_rowdy_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_tune[30]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HT_Rowdy_v3_15Aug2022_MP4Video.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
