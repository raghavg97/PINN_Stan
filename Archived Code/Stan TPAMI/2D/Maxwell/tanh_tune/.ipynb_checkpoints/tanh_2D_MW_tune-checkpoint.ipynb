{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 0.5\n",
    "\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../ES_FEA.mat')\n",
    "\n",
    "xy = np.array(fea_data['xy'])\n",
    "u_true = np.array(fea_data['u'])\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N_t = int(N_T/4)\n",
    "    \n",
    "    x_BC1 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC1 = np.zeros((N_t,1))\n",
    "    u_BC1 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC2 = np.ones((N_t,1))\n",
    "    y_BC2 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC2 = 1000*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC3 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC3 = np.ones((N_t,1)) \n",
    "    u_BC3 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC4 = np.zeros((N_t,1))\n",
    "    y_BC4 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC4 = 1000*np.ones((N_t,1))\n",
    "    \n",
    "    XY_1 = np.hstack((x_BC1,y_BC1))\n",
    "    XY_2 = np.hstack((x_BC2,y_BC2))\n",
    "    XY_3 = np.hstack((x_BC3,y_BC3))\n",
    "    XY_4 = np.hstack((x_BC4,y_BC4))\n",
    "    \n",
    "    xy_BC = np.vstack((XY_1,XY_2,XY_3,XY_4)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_BC1,u_BC2,u_BC3,u_BC4))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a =self.activation(z)\n",
    "     \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2u_dx2 + d2u_dy2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "     \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "    \n",
    "    nan_flag = 0\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "        if(np.isnan(loss_np)):\n",
    "            nan_flag =1\n",
    "            print(\"NAN BREAK!\")\n",
    "            break\n",
    "            \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    \n",
    "    return nan_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MW_stan_tune\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 250025.95 Test MSE 75123.7797622448 Test RE 0.47953990979729494\n",
      "1 Train Loss 250000.45 Test MSE 75084.18008874205 Test RE 0.4794135042366389\n",
      "2 Train Loss 250000.2 Test MSE 75083.13466735637 Test RE 0.4794101667101861\n",
      "3 Train Loss 250000.2 Test MSE 75083.04151326505 Test RE 0.4794098693129576\n",
      "4 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "5 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "6 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "7 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "8 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "9 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "10 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "11 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "12 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "13 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "14 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "15 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "16 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "17 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "18 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "19 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "20 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "21 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "22 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "23 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "24 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "25 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "26 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "27 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "28 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "29 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "30 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "31 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "32 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "33 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "34 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "35 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "36 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "37 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "38 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "39 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "40 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "41 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "42 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "43 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "44 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "45 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "46 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "47 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "48 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "49 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "50 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "51 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "52 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "53 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "54 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "55 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "56 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "57 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "58 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "59 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "60 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "61 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "62 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "63 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "64 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "65 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "66 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "67 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "68 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "69 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "70 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "71 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "72 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "73 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "74 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "75 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "76 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "77 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "78 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "79 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "80 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "81 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "82 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "83 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "84 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "85 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "86 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "87 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "88 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "89 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "90 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "91 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "92 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "93 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "94 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "95 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "96 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "97 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "99 Train Loss 250000.1 Test MSE 75082.6459637665 Test RE 0.47940860650727385\n",
      "Training time: 22.51\n",
      "Training time: 22.51\n",
      "MW_stan_tune\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 250011.08 Test MSE 75103.20043520787 Test RE 0.47947422297613257\n",
      "1 Train Loss 250000.53 Test MSE 75084.3907180701 Test RE 0.4794141766717198\n",
      "2 Train Loss 250000.22 Test MSE 75083.18031026497 Test RE 0.47941031242647236\n",
      "3 Train Loss 250000.1 Test MSE 75082.60551027823 Test RE 0.47940847735790254\n",
      "4 Train Loss 250000.08 Test MSE 75082.47371582885 Test RE 0.479408056598636\n",
      "5 Train Loss 250000.08 Test MSE 75082.41933699025 Test RE 0.4794078829918224\n",
      "6 Train Loss 250000.05 Test MSE 75082.32151805673 Test RE 0.4794075707004529\n",
      "7 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "8 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "9 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "10 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "11 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "12 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "13 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "14 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "15 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "16 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "17 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "18 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "19 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "20 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "21 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "22 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "23 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "24 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "25 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "26 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "27 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "28 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "29 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "30 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "31 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "32 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "33 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "34 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "35 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "36 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "37 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "38 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "39 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "40 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "41 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "42 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "43 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "44 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "45 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "46 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "47 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "48 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "49 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "50 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "51 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "52 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "53 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "54 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "55 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "56 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "57 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "58 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "59 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "60 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "61 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "62 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "63 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "64 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "65 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "66 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "67 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "68 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "69 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "70 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "71 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "72 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "73 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "74 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "75 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "76 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "77 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "78 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "79 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "80 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "81 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "82 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "83 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "84 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "85 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "86 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "87 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "88 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "89 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "90 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "91 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "92 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "94 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "95 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "96 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "97 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "98 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "99 Train Loss 250000.02 Test MSE 75082.27458211237 Test RE 0.47940742085525545\n",
      "Training time: 34.15\n",
      "Training time: 34.15\n",
      "MW_stan_tune\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 250027.05 Test MSE 75125.20600964753 Test RE 0.4795444618799167\n",
      "1 Train Loss 250000.28 Test MSE 75083.45662637232 Test RE 0.4794111945725731\n",
      "2 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "3 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "4 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "5 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "6 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "7 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "8 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "9 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "10 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "11 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "12 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "13 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "14 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "15 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "16 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "17 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "18 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "19 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "20 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "21 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "22 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "23 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "24 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "25 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "26 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "27 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "28 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "29 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "30 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "31 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "32 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "33 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "34 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "35 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "36 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "37 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "38 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "39 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "40 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "41 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "42 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "43 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "44 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "45 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "46 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "47 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "48 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "49 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "50 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "51 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "52 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "53 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "54 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "55 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "56 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "57 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "58 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "59 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "60 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "61 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "62 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "63 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "64 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "65 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "66 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "67 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "68 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "69 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "70 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "71 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "72 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "73 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "74 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "75 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "76 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "77 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "78 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "79 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "80 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "81 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "82 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "83 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "84 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "85 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "86 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "87 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "88 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "90 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "91 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "92 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "93 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "94 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "95 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "96 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "97 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "98 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "99 Train Loss 250000.1 Test MSE 75082.67849216337 Test RE 0.47940871035545013\n",
      "Training time: 33.82\n",
      "Training time: 33.82\n",
      "MW_stan_tune\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 250014.25 Test MSE 75107.82267407292 Test RE 0.47948897740921037\n",
      "1 Train Loss 250000.14 Test MSE 75082.89477475442 Test RE 0.4794094008454904\n",
      "2 Train Loss 250000.08 Test MSE 75082.57414694034 Test RE 0.47940837722917407\n",
      "3 Train Loss 250000.08 Test MSE 75082.51390594352 Test RE 0.4794081849072894\n",
      "4 Train Loss 250000.08 Test MSE 75082.45830047717 Test RE 0.47940800738446\n",
      "5 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "6 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "7 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "8 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "9 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "10 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "11 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "12 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "13 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "14 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "15 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "16 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "17 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "18 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "19 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "20 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "21 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "22 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "23 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "24 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "25 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "26 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "27 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "28 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "29 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "30 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "31 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "32 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "33 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "34 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "35 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "36 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "37 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "38 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "39 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "40 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "41 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "42 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "43 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "44 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "45 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "46 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "47 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "48 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "49 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "50 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "51 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "52 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "53 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "54 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "55 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "56 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "57 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "58 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "59 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "60 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "61 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "62 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "63 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "64 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "65 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "66 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "67 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "68 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "69 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "70 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "71 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "72 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "73 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "74 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "75 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "76 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "77 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "78 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "79 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "80 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "81 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "82 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "83 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "85 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "86 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "87 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "88 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "89 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "90 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "91 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "92 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "93 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "94 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "95 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "96 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "97 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "98 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "99 Train Loss 250000.02 Test MSE 75082.40472417336 Test RE 0.47940783633975653\n",
      "Training time: 21.75\n",
      "Training time: 21.75\n",
      "MW_stan_tune\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 250007.92 Test MSE 75098.46526890084 Test RE 0.47945910760182087\n",
      "1 Train Loss 250000.14 Test MSE 75082.92454563818 Test RE 0.47940949589004195\n",
      "2 Train Loss 250000.14 Test MSE 75082.8442735418 Test RE 0.47940923961862086\n",
      "3 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "4 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "5 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "6 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "7 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "8 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "9 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "10 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "11 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "12 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "13 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "14 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "15 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "16 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "17 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "18 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "19 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "20 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "21 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "22 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "23 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "24 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "25 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "26 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "27 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "28 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "29 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "30 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "31 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "32 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "33 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "34 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "35 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "36 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "37 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "38 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "39 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "40 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "41 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "42 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "43 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "44 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "45 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "46 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "47 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "48 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "49 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "50 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "51 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "52 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "53 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "54 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "55 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "56 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "57 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "58 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "59 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "60 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "61 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "62 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "63 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "64 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "65 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "66 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "67 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "68 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "69 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "70 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "71 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "72 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "73 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "74 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "75 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "76 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "77 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "78 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "80 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "81 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "82 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "83 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "84 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "85 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "86 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "87 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "88 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "89 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "90 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "91 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "92 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "93 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "94 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "95 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "96 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "97 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "98 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "99 Train Loss 250000.08 Test MSE 75082.63441677393 Test RE 0.47940856964304407\n",
      "Training time: 23.51\n",
      "Training time: 23.51\n",
      "MW_stan_tune\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 250006.75 Test MSE 75096.56475903165 Test RE 0.47945304074993694\n",
      "1 Train Loss 250000.53 Test MSE 75084.42763372228 Test RE 0.4794142945250091\n",
      "2 Train Loss 250000.1 Test MSE 75082.62523909562 Test RE 0.4794085403429416\n",
      "3 Train Loss 250000.08 Test MSE 75082.5040685728 Test RE 0.47940815350106747\n",
      "4 Train Loss 250000.08 Test MSE 75082.44830990912 Test RE 0.47940797548913716\n",
      "5 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "6 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "7 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "8 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "9 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "10 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "11 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "12 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "13 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "14 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "15 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "16 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "17 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "18 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "19 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "20 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "21 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "22 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "23 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "24 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "25 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "26 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "27 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "28 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "29 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "30 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "31 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "32 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "33 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "34 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "35 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "36 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "37 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "38 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "39 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "40 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "41 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "42 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "43 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "44 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "45 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "46 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "47 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "48 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "49 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "50 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "51 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "52 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "53 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "54 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "55 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "56 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "57 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "58 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "59 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "60 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "61 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "62 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "63 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "64 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "65 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "66 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "67 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "68 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "69 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "70 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "71 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "72 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "73 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "74 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "76 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "77 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "78 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "79 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "80 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "81 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "82 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "83 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "84 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "85 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "86 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "87 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "88 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "89 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "90 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "91 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "92 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "93 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "94 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "95 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "96 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "97 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "98 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "99 Train Loss 250000.02 Test MSE 75082.39355686236 Test RE 0.47940780068761785\n",
      "Training time: 20.53\n",
      "Training time: 20.53\n",
      "MW_stan_tune\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 250019.89 Test MSE 75115.67870742193 Test RE 0.4795140532453029\n",
      "1 Train Loss 250000.25 Test MSE 75083.28508455114 Test RE 0.4794106469211989\n",
      "2 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "3 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "4 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "5 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "6 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "7 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "8 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "9 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "10 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "11 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "12 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "13 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "14 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "15 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "16 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "17 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "18 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "19 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "20 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "21 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "22 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "23 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "24 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "25 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "26 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "27 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "28 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "29 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "30 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "31 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "32 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "33 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "34 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "35 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "36 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "37 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "38 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "39 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "40 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "41 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "42 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "43 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "44 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "45 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "46 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "47 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "48 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "49 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "50 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "51 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "52 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "53 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "54 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "55 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "56 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "57 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "58 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "59 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "60 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "61 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "62 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "63 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "64 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "65 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "66 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "67 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "68 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "69 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "71 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "72 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "73 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "74 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "75 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "76 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "77 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "78 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "79 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "80 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "81 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "82 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "83 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "84 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "85 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "86 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "87 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "88 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "89 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "90 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "91 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "92 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "93 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "94 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "95 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "96 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "97 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "98 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "99 Train Loss 250000.17 Test MSE 75083.14759343369 Test RE 0.47941020797706074\n",
      "Training time: 36.60\n",
      "Training time: 36.60\n",
      "MW_stan_tune\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 250019.58 Test MSE 75115.24183517422 Test RE 0.4795126588181311\n",
      "1 Train Loss 250000.1 Test MSE 75082.56271166175 Test RE 0.4794083407215782\n",
      "2 Train Loss 250000.08 Test MSE 75082.43810426108 Test RE 0.47940794290716\n",
      "3 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "4 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "5 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "6 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "7 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "8 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "9 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "10 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "11 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "12 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "13 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "14 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "15 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "16 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "17 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "18 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "19 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "20 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "21 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "22 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "23 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "24 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "25 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "26 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "27 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "28 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "29 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "30 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "31 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "32 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "33 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "34 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "35 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "36 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "37 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "38 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "39 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "40 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "41 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "42 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "43 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "44 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "45 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "46 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "47 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "48 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "49 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "50 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "51 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "52 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "53 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "54 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "55 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "56 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "57 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "58 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "59 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "60 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "61 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "62 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "63 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "64 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "65 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "67 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "68 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "69 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "70 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "71 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "72 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "73 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "74 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "75 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "76 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "77 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "78 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "79 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "80 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "81 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "82 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "83 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "84 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "85 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "86 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "87 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "88 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "89 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "90 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "91 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "92 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "93 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "94 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "95 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "96 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "97 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "98 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "99 Train Loss 250000.05 Test MSE 75082.33505261672 Test RE 0.47940761391015996\n",
      "Training time: 21.56\n",
      "Training time: 21.56\n",
      "MW_stan_tune\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 250004.1 Test MSE 75092.06685029069 Test RE 0.47943868211501023\n",
      "1 Train Loss 250000.2 Test MSE 75083.16846733977 Test RE 0.47941027461760044\n",
      "2 Train Loss 250000.2 Test MSE 75083.05094329601 Test RE 0.4794098994186224\n",
      "3 Train Loss 250000.14 Test MSE 75082.87604201697 Test RE 0.4794093410405841\n",
      "4 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "5 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "6 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "7 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "8 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "9 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "10 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "11 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "12 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "13 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "14 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "15 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "16 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "17 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "18 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "19 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "20 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "21 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "22 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "23 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "24 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "25 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "26 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "27 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "28 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "29 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "30 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "31 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "32 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "33 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "34 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "35 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "36 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "37 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "38 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "39 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "40 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "41 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "42 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "43 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "44 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "45 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "46 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "47 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "48 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "49 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "50 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "51 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "52 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "53 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "54 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "55 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "56 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "57 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "58 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "59 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "60 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "61 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "63 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "64 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "65 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "66 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "67 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "68 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "69 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "70 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "71 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "72 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "73 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "74 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "75 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "76 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "77 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "78 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "79 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "80 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "81 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "82 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "83 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "84 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "85 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "86 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "87 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "88 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "89 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "90 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "91 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "92 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "93 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "94 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "95 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "96 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "97 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "98 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "99 Train Loss 250000.1 Test MSE 75082.79662001376 Test RE 0.47940908748303324\n",
      "Training time: 24.96\n",
      "Training time: 24.96\n",
      "MW_stan_tune\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 250051.06 Test MSE 75155.45707244257 Test RE 0.4796410025106745\n",
      "1 Train Loss 250000.22 Test MSE 75083.29466088899 Test RE 0.4794106774939035\n",
      "2 Train Loss 250000.22 Test MSE 75083.17804119673 Test RE 0.47941030518240846\n",
      "3 Train Loss 250000.17 Test MSE 75082.98232684158 Test RE 0.47940968035843096\n",
      "4 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "5 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "6 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "7 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "8 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "9 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "10 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "11 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "12 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "13 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "14 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "15 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "16 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "17 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "18 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "19 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "20 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "21 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "22 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "23 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "24 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "25 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "26 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "27 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "28 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "29 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "30 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "31 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "32 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "33 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "34 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "35 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "36 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "37 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "38 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "39 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "40 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "41 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "42 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "43 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "44 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "45 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "46 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "47 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "48 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "49 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "50 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "51 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "52 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "53 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "54 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "55 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "56 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "57 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "59 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "60 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "61 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "62 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "63 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "64 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "65 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "66 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "67 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "68 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "69 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "70 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "71 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "72 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "73 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "74 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "75 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "76 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "77 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "78 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "79 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "80 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "81 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "82 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "83 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "84 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "85 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "86 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "87 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "88 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "89 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "90 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "91 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "92 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "93 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "94 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "95 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "96 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "97 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "98 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "99 Train Loss 250000.08 Test MSE 75082.63571197365 Test RE 0.4794085737780204\n",
      "Training time: 26.08\n",
      "Training time: 26.08\n",
      "MW_stan_tune\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 250000.14 Test MSE 75082.96523166062 Test RE 0.4794096257815119\n",
      "1 Train Loss 250000.02 Test MSE 75081.98288239025 Test RE 0.4794064895892007\n",
      "2 Train Loss 250000.02 Test MSE 75081.9307093975 Test RE 0.4794063230241029\n",
      "3 Train Loss 250000.02 Test MSE 75081.88371530206 Test RE 0.4794061729928649\n",
      "4 Train Loss 250000.02 Test MSE 75081.84220600133 Test RE 0.4794060404720972\n",
      "5 Train Loss 250000.0 Test MSE 75081.77150564904 Test RE 0.4794058147571777\n",
      "6 Train Loss 250000.0 Test MSE 75081.74176319981 Test RE 0.47940571980267516\n",
      "7 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "8 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "9 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "10 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "11 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "12 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "13 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "14 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "15 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "16 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "17 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "18 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "19 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "20 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "21 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "22 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "23 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "24 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "25 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "26 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "27 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "28 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "29 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "30 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "31 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "32 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "33 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "34 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "35 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "36 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "37 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "38 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "39 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "40 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "41 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "42 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "43 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "44 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "45 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "46 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "47 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "48 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "49 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "50 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "51 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "52 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "54 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "55 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "56 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "57 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "58 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "59 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "60 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "61 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "62 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "63 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "64 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "65 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "66 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "67 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "68 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "69 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "70 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "71 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "72 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "73 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "74 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "75 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "76 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "77 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "78 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "79 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "80 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "81 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "82 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "83 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "84 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "85 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "86 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "87 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "88 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "89 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "90 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "91 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "92 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "93 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "94 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "95 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "96 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "97 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "98 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "99 Train Loss 249999.97 Test MSE 75081.71503606332 Test RE 0.47940563447471835\n",
      "Training time: 42.85\n",
      "Training time: 42.85\n",
      "MW_stan_tune\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 251955.55 Test MSE 77178.77696769853 Test RE 0.4860545219729224\n",
      "1 Train Loss 250000.12 Test MSE 75082.66414518333 Test RE 0.47940866455215925\n",
      "2 Train Loss 250000.08 Test MSE 75082.42093711857 Test RE 0.47940788810030266\n",
      "3 Train Loss 250000.05 Test MSE 75082.23052784805 Test RE 0.47940728020991374\n",
      "4 Train Loss 250000.02 Test MSE 75082.08096159606 Test RE 0.47940680271220715\n",
      "5 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "6 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "7 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "8 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "9 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "10 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "11 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "12 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "13 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "14 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "15 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "16 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "17 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "18 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "19 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "20 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "21 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "22 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "23 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "24 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "25 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "26 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "27 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "28 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "29 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "30 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "31 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "32 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "33 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "34 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "35 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "36 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "37 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "38 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "39 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "40 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "41 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "42 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "43 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "44 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "45 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "46 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "47 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "49 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "50 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "51 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "52 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "53 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "54 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "55 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "56 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "57 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "58 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "59 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "60 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "61 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "62 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "63 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "64 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "65 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "66 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "67 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "68 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "69 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "70 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "71 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "72 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "73 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "74 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "75 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "76 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "77 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "78 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "79 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "80 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "81 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "82 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "83 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "84 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "85 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "86 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "87 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "88 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "89 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "90 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "91 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "92 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "93 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "94 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "95 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "96 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "97 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "98 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "99 Train Loss 250000.02 Test MSE 75082.01794868847 Test RE 0.4794066015402197\n",
      "Training time: 50.94\n",
      "Training time: 50.94\n",
      "MW_stan_tune\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 252094.12 Test MSE 77322.28233917465 Test RE 0.48650619433557574\n",
      "1 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "2 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "3 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "4 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "5 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "6 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "7 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "8 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "9 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "10 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "11 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "12 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "13 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "14 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "15 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "16 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "17 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "18 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "19 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "20 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "21 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "22 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "23 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "24 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "25 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "26 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "27 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "28 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "29 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "30 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "31 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "32 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "33 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "34 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "35 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "36 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "37 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "38 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "39 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "40 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "41 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "42 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "43 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "45 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "46 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "47 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "48 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "49 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "50 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "51 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "52 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "53 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "54 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "55 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "56 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "57 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "58 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "59 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "60 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "61 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "62 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "63 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "64 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "65 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "66 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "67 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "68 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "69 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "70 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "71 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "72 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "73 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "74 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "75 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "76 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "77 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "78 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "79 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "80 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "81 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "82 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "83 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "84 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "85 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "86 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "87 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "88 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "89 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "90 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "91 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "92 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "93 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "94 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "95 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "96 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "97 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "98 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "99 Train Loss 250000.02 Test MSE 75082.19107223996 Test RE 0.4794071542459665\n",
      "Training time: 24.43\n",
      "Training time: 24.43\n",
      "MW_stan_tune\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "1 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "2 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "3 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "4 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "5 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "6 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "7 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "8 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "9 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "10 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "11 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "12 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "13 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "14 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "15 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "16 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "17 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "18 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "19 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "20 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "21 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "22 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "23 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "24 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "25 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "26 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "27 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "28 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "29 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "30 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "31 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "32 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "33 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "34 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "35 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "36 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "37 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "38 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "39 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "41 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "42 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "43 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "44 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "45 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "46 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "47 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "48 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "49 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "50 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "51 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "52 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "53 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "54 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "55 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "56 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "57 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "58 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "59 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "60 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "61 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "62 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "63 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "64 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "65 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "66 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "67 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "68 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "69 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "70 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "71 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "72 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "73 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "74 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "75 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "76 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "77 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "78 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "79 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "80 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "81 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "82 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "83 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "84 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "85 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "86 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "87 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "88 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "89 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "90 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "91 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "92 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "93 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "94 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "95 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "96 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "97 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "98 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "99 Train Loss 249999.97 Test MSE 75081.53989994267 Test RE 0.47940507534196564\n",
      "Training time: 21.55\n",
      "Training time: 21.55\n",
      "MW_stan_tune\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 250144.45 Test MSE 75264.43486796318 Test RE 0.479988623871032\n",
      "1 Train Loss 250000.05 Test MSE 75082.25926961355 Test RE 0.4794073719693768\n",
      "2 Train Loss 250000.05 Test MSE 75082.1761718741 Test RE 0.4794071066758155\n",
      "3 Train Loss 250000.02 Test MSE 75082.03399296309 Test RE 0.4794066527624095\n",
      "4 Train Loss 250000.02 Test MSE 75081.97604256697 Test RE 0.47940646775269863\n",
      "5 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "6 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "7 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "8 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "9 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "10 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "11 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "12 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "13 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "14 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "15 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "16 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "17 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "18 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "19 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "20 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "21 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "22 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "23 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "24 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "25 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n",
      "26 Train Loss 250000.0 Test MSE 75081.92233969722 Test RE 0.4794062963033772\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22242/1838433073.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mnan_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22242/131485505.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnan_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22242/896708044.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(xy_BC, u_BC, xy_coll, f_hat, seed)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_old\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_stps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0;31m# multiply by initial Hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nan_tune = []\n",
    "for tune_reps in range(5):      \n",
    "    \n",
    "    max_reps = 10\n",
    "    max_iter = 100 #200\n",
    "\n",
    "    label = \"MW_tanh_tune\" + str(tune_reps)\n",
    "\n",
    "    train_loss_full = []\n",
    "    test_mse_full = []\n",
    "    test_re_full = []\n",
    "\n",
    "    elapsed_time= np.zeros((max_reps,1))\n",
    "    time_threshold = np.empty((max_reps,1))\n",
    "    time_threshold[:] = np.nan\n",
    "    epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "    N_T = 5000 #Total number of data points for 'y'\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "    for reps in range(max_reps):\n",
    "        print(label)\n",
    "        print(reps)\n",
    "        train_loss = []\n",
    "        test_mse_loss = []\n",
    "        test_re_loss = []\n",
    "   \n",
    "\n",
    "        torch.manual_seed(reps*36)\n",
    "\n",
    "        layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "        PINN = Sequentialmodel(layers)\n",
    "\n",
    "        PINN.to(device)\n",
    "\n",
    "        'Neural Network Summary'\n",
    "        print(PINN)\n",
    "\n",
    "        params = list(PINN.parameters())\n",
    "\n",
    "        optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lr_tune[tune_reps], \n",
    "                                  max_iter = 20, \n",
    "                                  max_eval = 30, \n",
    "                                  tolerance_grad = 1e-08, \n",
    "                                  tolerance_change = 1e-08, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "        torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "        train_loss_full.append(train_loss)\n",
    "        test_mse_full.append(test_mse_loss)\n",
    "        test_re_full.append(test_re_loss)\n",
    "      \n",
    "        \n",
    "        if(nan_flag == 1):\n",
    "            nan_tune.append(tune_reps)\n",
    "            break\n",
    "\n",
    "\n",
    "        print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "    mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "    savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(np.flipud(np.transpose(u_pred.reshape(500,500)))),vmin = 0,vmax = 1000,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "for tune_reps in range(20):\n",
    "    label = \"MW_stan_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrb_tune[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
