{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 25000\n",
    "label = \"ES_rowdy\"\n",
    "\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../ES_FEA.mat')\n",
    "\n",
    "xy = np.array(fea_data['xy'])\n",
    "u_true = np.array(fea_data['u'])\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N_t = int(N_T/4)\n",
    "    \n",
    "    x_BC1 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC1 = np.zeros((N_t,1))\n",
    "    u_BC1 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC2 = np.ones((N_t,1))\n",
    "    y_BC2 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC2 = 1000*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC3 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC3 = np.ones((N_t,1)) \n",
    "    u_BC3 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC4 = np.zeros((N_t,1))\n",
    "    y_BC4 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC4 = 1000*np.ones((N_t,1))\n",
    "    \n",
    "    XY_corners = np.array([[0,0],[1,0],[0,1],[1,1]]).reshape(-1,2)\n",
    "    U_corners = 1000*np.ones((4,1))\n",
    "    \n",
    "    XY_1 = np.hstack((x_BC1,y_BC1))\n",
    "    XY_2 = np.hstack((x_BC2,y_BC2))\n",
    "    XY_3 = np.hstack((x_BC3,y_BC3))\n",
    "    XY_4 = np.hstack((x_BC4,y_BC4))\n",
    "    \n",
    "    xy_BC = np.vstack((XY_1,XY_2,XY_3,XY_4,XY_corners)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_BC1,u_BC2,u_BC3,u_BC4,U_corners))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "\n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        self.omega1.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        self.omega.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "                \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2u_dx2 + d2u_dy2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'  \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(xy_BC, u_BC, xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "     \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "\n",
    "            \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES_rowdy\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 495952.53 Test MSE 322159.50877912925 Test RE 0.993050922178195\n",
      "1 Train Loss 488412.06 Test MSE 314665.4660189879 Test RE 0.9814328335438246\n",
      "2 Train Loss 470641.56 Test MSE 296825.463465602 Test RE 0.9532056746527949\n",
      "3 Train Loss 441350.97 Test MSE 266399.7326689107 Test RE 0.9030315713827232\n",
      "4 Train Loss 372628.94 Test MSE 198584.74059869707 Test RE 0.7796666877023077\n",
      "5 Train Loss 295745.22 Test MSE 120237.3106070427 Test RE 0.6066743991372984\n",
      "6 Train Loss 251750.36 Test MSE 77084.08040103315 Test RE 0.4857562416447785\n",
      "7 Train Loss 248727.62 Test MSE 74354.93928013949 Test RE 0.4770797175883742\n",
      "8 Train Loss 246862.89 Test MSE 73494.89618721815 Test RE 0.47431256778729664\n",
      "9 Train Loss 242588.42 Test MSE 72209.115949826 Test RE 0.47014525342705243\n",
      "10 Train Loss 237120.02 Test MSE 70693.35134101182 Test RE 0.4651845981196434\n",
      "11 Train Loss 231968.31 Test MSE 68976.28954235044 Test RE 0.45950046595956456\n",
      "12 Train Loss 227044.2 Test MSE 66547.29751530061 Test RE 0.4513373281152908\n",
      "13 Train Loss 223572.16 Test MSE 65366.89008787583 Test RE 0.4473165364506541\n",
      "14 Train Loss 218443.44 Test MSE 62980.014543424106 Test RE 0.4390736959409861\n",
      "15 Train Loss 213601.88 Test MSE 60080.24060380315 Test RE 0.42884650344642666\n",
      "16 Train Loss 208605.42 Test MSE 57062.30782714675 Test RE 0.4179368912628474\n",
      "17 Train Loss 200584.42 Test MSE 55315.60890303663 Test RE 0.41149057327978694\n",
      "18 Train Loss 193382.52 Test MSE 50581.04479163088 Test RE 0.3934865931950827\n",
      "19 Train Loss 184310.6 Test MSE 48189.709814072776 Test RE 0.38407248679051337\n",
      "20 Train Loss 181113.45 Test MSE 47042.087703354606 Test RE 0.37947164982215054\n",
      "21 Train Loss 177764.23 Test MSE 46039.4199097747 Test RE 0.3754057871646002\n",
      "22 Train Loss 172778.44 Test MSE 42584.960518887034 Test RE 0.3610473538145257\n",
      "23 Train Loss 169103.83 Test MSE 39374.80657469351 Test RE 0.34717245340402225\n",
      "24 Train Loss 164258.67 Test MSE 39399.84488250554 Test RE 0.3472828187625717\n",
      "25 Train Loss 160102.72 Test MSE 38654.15563996721 Test RE 0.34398074862738887\n",
      "26 Train Loss 156549.7 Test MSE 37936.607898917384 Test RE 0.34077308801791345\n",
      "27 Train Loss 155400.16 Test MSE 37929.29517613244 Test RE 0.3407402424452023\n",
      "28 Train Loss 153090.97 Test MSE 36203.55279295948 Test RE 0.3328983462051811\n",
      "29 Train Loss 151039.7 Test MSE 34837.91726714376 Test RE 0.3265593580041948\n",
      "30 Train Loss 149872.88 Test MSE 34027.31389807468 Test RE 0.3227378307110833\n",
      "31 Train Loss 148199.89 Test MSE 34652.77211526987 Test RE 0.3256904566709411\n",
      "32 Train Loss 147719.39 Test MSE 34678.40771351297 Test RE 0.32581090484282205\n",
      "33 Train Loss 147436.34 Test MSE 34765.72990936831 Test RE 0.3262208520985532\n",
      "34 Train Loss 145792.17 Test MSE 33725.781761384016 Test RE 0.32130468234489773\n",
      "35 Train Loss 144856.78 Test MSE 33406.531450264025 Test RE 0.3197803214649905\n",
      "36 Train Loss 142910.4 Test MSE 33070.424192121856 Test RE 0.31816757996505374\n",
      "37 Train Loss 142382.97 Test MSE 32898.80126978157 Test RE 0.31734092170121225\n",
      "38 Train Loss 142127.42 Test MSE 32562.148687292247 Test RE 0.31571307584461455\n",
      "39 Train Loss 141519.58 Test MSE 32246.428546772673 Test RE 0.3141787825884943\n",
      "40 Train Loss 140836.06 Test MSE 31803.00041032574 Test RE 0.3120111320818925\n",
      "41 Train Loss 137892.39 Test MSE 31398.793049396172 Test RE 0.3100220038498754\n",
      "42 Train Loss 136302.77 Test MSE 30574.039613260706 Test RE 0.30592322846542414\n",
      "43 Train Loss 135708.86 Test MSE 30595.820812988783 Test RE 0.3060321801886273\n",
      "44 Train Loss 135281.36 Test MSE 30382.503418357424 Test RE 0.3049634692943954\n",
      "45 Train Loss 133715.89 Test MSE 29571.040041362146 Test RE 0.3008633878336343\n",
      "46 Train Loss 129200.58 Test MSE 26822.368459741203 Test RE 0.2865395693978982\n",
      "47 Train Loss 126482.516 Test MSE 26318.248964477676 Test RE 0.28383407820572637\n",
      "48 Train Loss 124559.914 Test MSE 26026.417051432043 Test RE 0.282256033520363\n",
      "49 Train Loss 123448.44 Test MSE 25670.67137567692 Test RE 0.28032036849858954\n",
      "50 Train Loss 121396.01 Test MSE 24812.06874546412 Test RE 0.27559258583494295\n",
      "51 Train Loss 118759.03 Test MSE 24251.226873756612 Test RE 0.27246009211114103\n",
      "52 Train Loss 117820.08 Test MSE 24546.740979354443 Test RE 0.2741151011966616\n",
      "53 Train Loss 115975.69 Test MSE 24174.30568598082 Test RE 0.27202764802734875\n",
      "54 Train Loss 114708.44 Test MSE 24133.5859035328 Test RE 0.27179844650908797\n",
      "55 Train Loss 113567.42 Test MSE 23788.507136140102 Test RE 0.26984826871030687\n",
      "56 Train Loss 112678.766 Test MSE 23228.361658929953 Test RE 0.26665229843092664\n",
      "57 Train Loss 112576.91 Test MSE 23103.907941795507 Test RE 0.2659369995908601\n",
      "58 Train Loss 112419.33 Test MSE 23192.630310239816 Test RE 0.2664471287056481\n",
      "59 Train Loss 111755.77 Test MSE 22657.2719754804 Test RE 0.26335395858135713\n",
      "60 Train Loss 111369.95 Test MSE 22382.737752123696 Test RE 0.26175358890309997\n",
      "61 Train Loss 110590.03 Test MSE 22003.023391793846 Test RE 0.25952381820905607\n",
      "62 Train Loss 110168.7 Test MSE 21655.748356982152 Test RE 0.25746763274035345\n",
      "63 Train Loss 109170.73 Test MSE 21923.432057882997 Test RE 0.2590540064037149\n",
      "64 Train Loss 108328.1 Test MSE 21272.06980352068 Test RE 0.25517664113963057\n",
      "65 Train Loss 108057.22 Test MSE 21163.931509430553 Test RE 0.25452720914043503\n",
      "66 Train Loss 108029.336 Test MSE 21201.851270683717 Test RE 0.2547551273708027\n",
      "67 Train Loss 107889.86 Test MSE 21269.209486402142 Test RE 0.25515948458977306\n",
      "68 Train Loss 107555.945 Test MSE 21228.397444153277 Test RE 0.2549145629355807\n",
      "69 Train Loss 106781.72 Test MSE 21318.17831065954 Test RE 0.25545304689631615\n",
      "70 Train Loss 106050.77 Test MSE 20944.10775892785 Test RE 0.2532019078307172\n",
      "71 Train Loss 104946.81 Test MSE 20734.831393596018 Test RE 0.2519337179938543\n",
      "72 Train Loss 104476.32 Test MSE 20823.737874952825 Test RE 0.2524732589548586\n",
      "73 Train Loss 104367.98 Test MSE 20893.634799503645 Test RE 0.2528966296455321\n",
      "74 Train Loss 104128.67 Test MSE 21076.616474187224 Test RE 0.25400162102334356\n",
      "75 Train Loss 103017.69 Test MSE 20535.306276835698 Test RE 0.25071864623286605\n",
      "76 Train Loss 102443.695 Test MSE 20640.686782274443 Test RE 0.25136112626678175\n",
      "77 Train Loss 101866.375 Test MSE 20447.458524972117 Test RE 0.25018179825891523\n",
      "78 Train Loss 100730.59 Test MSE 20060.2000166059 Test RE 0.24780135187604507\n",
      "79 Train Loss 100176.56 Test MSE 19851.8365294655 Test RE 0.24651104742213306\n",
      "80 Train Loss 99496.72 Test MSE 19344.11284480927 Test RE 0.24333828916798994\n",
      "81 Train Loss 98434.44 Test MSE 18936.968563515675 Test RE 0.24076384531076536\n",
      "82 Train Loss 97751.016 Test MSE 19018.041536738274 Test RE 0.24127867411391704\n",
      "83 Train Loss 97611.69 Test MSE 18793.089361094964 Test RE 0.23984746421982459\n",
      "84 Train Loss 97401.625 Test MSE 18848.958491161797 Test RE 0.24020371551458886\n",
      "85 Train Loss 97357.625 Test MSE 18889.51647808475 Test RE 0.24046200416586797\n",
      "86 Train Loss 97243.766 Test MSE 18936.575421689122 Test RE 0.2407613461032267\n",
      "87 Train Loss 97220.78 Test MSE 18982.13671489488 Test RE 0.24105080731483217\n",
      "88 Train Loss 97210.234 Test MSE 18991.353834089306 Test RE 0.2411093234980656\n",
      "89 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "90 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "91 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "92 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "93 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "94 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "95 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "97 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "98 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "99 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "100 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "101 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "102 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "103 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "104 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "105 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "106 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "107 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "108 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "109 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "110 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "111 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "112 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "113 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "114 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "115 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "116 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "117 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "118 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "119 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "120 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "121 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "122 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "123 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "124 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "125 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "126 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "127 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "128 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "129 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "130 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "131 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "132 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "133 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "134 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "135 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "136 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "137 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "138 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "139 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "140 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "141 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "142 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "143 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "144 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "145 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "146 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "147 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "148 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "149 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "150 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "151 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "152 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "153 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "154 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "155 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "156 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "157 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "158 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "159 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "160 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "161 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "162 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "163 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "164 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "165 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "166 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "167 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "168 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "169 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "170 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "171 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "172 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "173 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "174 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "175 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "176 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "177 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "178 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "179 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "180 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "181 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "182 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "183 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "184 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "185 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "186 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "187 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "188 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "189 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "190 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "191 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "192 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "193 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "194 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "195 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "196 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "197 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "198 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "199 Train Loss 97204.445 Test MSE 18992.98387042245 Test RE 0.2411196705362008\n",
      "Training time: 370.67\n",
      "Training time: 370.67\n",
      "ES_rowdy\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 490978.5 Test MSE 317076.5626178919 Test RE 0.9851857298067924\n",
      "1 Train Loss 463791.7 Test MSE 288802.5641318613 Test RE 0.9402353260254865\n",
      "2 Train Loss 437196.22 Test MSE 262801.24605726515 Test RE 0.8969118292030378\n",
      "3 Train Loss 381122.2 Test MSE 206984.15740436327 Test RE 0.7959844703321247\n",
      "4 Train Loss 263375.06 Test MSE 87846.51968681093 Test RE 0.5185591740653198\n",
      "5 Train Loss 249991.84 Test MSE 75076.70464541108 Test RE 0.4793896382392645\n",
      "6 Train Loss 249952.89 Test MSE 75059.48760694882 Test RE 0.4793346668394913\n",
      "7 Train Loss 249748.22 Test MSE 74968.94191511789 Test RE 0.4790454643190384\n",
      "8 Train Loss 248657.5 Test MSE 74506.0710962471 Test RE 0.47756432105878727\n",
      "9 Train Loss 246891.6 Test MSE 73462.61523974869 Test RE 0.47420839088843425\n",
      "10 Train Loss 246395.16 Test MSE 73105.85150807186 Test RE 0.4730555170310807\n",
      "11 Train Loss 245516.48 Test MSE 72731.65518713757 Test RE 0.4718432834816632\n",
      "12 Train Loss 244858.61 Test MSE 72332.75956770855 Test RE 0.4705475959595371\n",
      "13 Train Loss 244203.89 Test MSE 72221.63327788729 Test RE 0.4701860011033064\n",
      "14 Train Loss 243413.39 Test MSE 71906.66824438138 Test RE 0.4691596191406497\n",
      "15 Train Loss 241315.69 Test MSE 70964.01019502115 Test RE 0.46607425779575556\n",
      "16 Train Loss 238258.77 Test MSE 70148.26575258911 Test RE 0.4633877099245009\n",
      "17 Train Loss 236251.3 Test MSE 69673.63237319143 Test RE 0.46181737478058615\n",
      "18 Train Loss 234843.34 Test MSE 69494.47600847864 Test RE 0.4612232420018128\n",
      "19 Train Loss 231854.8 Test MSE 67746.61947726371 Test RE 0.4553861902556594\n",
      "20 Train Loss 230975.19 Test MSE 67193.30193752416 Test RE 0.4535227041130354\n",
      "21 Train Loss 228319.22 Test MSE 66250.38812917804 Test RE 0.45032935258500345\n",
      "22 Train Loss 224039.53 Test MSE 65177.63813582718 Test RE 0.4466685257323307\n",
      "23 Train Loss 223075.8 Test MSE 64156.32252729731 Test RE 0.4431551215834759\n",
      "24 Train Loss 219973.1 Test MSE 62616.18709232075 Test RE 0.4378036228616857\n",
      "25 Train Loss 217412.1 Test MSE 62370.63020011904 Test RE 0.4369443298551934\n",
      "26 Train Loss 214291.53 Test MSE 60020.15456756011 Test RE 0.42863200586738553\n",
      "27 Train Loss 211105.33 Test MSE 59184.78759994022 Test RE 0.42563868083478007\n",
      "28 Train Loss 208398.97 Test MSE 58277.972717170414 Test RE 0.4223653280217739\n",
      "29 Train Loss 204572.2 Test MSE 57783.604148697064 Test RE 0.42057006268118025\n",
      "30 Train Loss 200837.08 Test MSE 56153.588039181435 Test RE 0.4145957039554118\n",
      "31 Train Loss 196686.6 Test MSE 54574.505862687525 Test RE 0.4087247604006702\n",
      "32 Train Loss 193600.34 Test MSE 52970.724724888736 Test RE 0.40267438054385124\n",
      "33 Train Loss 190127.27 Test MSE 51128.38982480053 Test RE 0.3956098531921497\n",
      "34 Train Loss 188623.11 Test MSE 51120.74433872685 Test RE 0.39558027331769985\n",
      "35 Train Loss 186774.36 Test MSE 50721.15484554172 Test RE 0.39403119742511317\n",
      "36 Train Loss 185247.28 Test MSE 49499.492469370634 Test RE 0.3892569853356918\n",
      "37 Train Loss 177063.02 Test MSE 46302.66533487611 Test RE 0.37647750998631774\n",
      "38 Train Loss 173271.62 Test MSE 44847.32264352353 Test RE 0.37051372680469846\n",
      "39 Train Loss 166800.94 Test MSE 42544.42653218784 Test RE 0.3608754835981306\n",
      "40 Train Loss 163105.34 Test MSE 40505.30054455699 Test RE 0.3521210362890681\n",
      "41 Train Loss 160138.53 Test MSE 40407.86248635303 Test RE 0.35169725658450673\n",
      "42 Train Loss 158745.78 Test MSE 39536.42376897235 Test RE 0.3478842229912839\n",
      "43 Train Loss 156985.6 Test MSE 38558.43654578746 Test RE 0.3435545857254426\n",
      "44 Train Loss 154591.75 Test MSE 38170.23821357381 Test RE 0.34182079270536786\n",
      "45 Train Loss 148908.34 Test MSE 36304.211429535244 Test RE 0.33336081233923326\n",
      "46 Train Loss 147373.42 Test MSE 35291.79686532925 Test RE 0.32867973432374364\n",
      "47 Train Loss 143050.69 Test MSE 34466.86125538489 Test RE 0.32481562301323086\n",
      "48 Train Loss 140550.23 Test MSE 34073.38374580536 Test RE 0.32295623545392294\n",
      "49 Train Loss 137702.28 Test MSE 32765.05169392562 Test RE 0.31669519238844607\n",
      "50 Train Loss 133900.5 Test MSE 31630.725215302882 Test RE 0.31116491052407047\n",
      "51 Train Loss 132293.25 Test MSE 30801.97668624459 Test RE 0.30706147781695026\n",
      "52 Train Loss 131410.22 Test MSE 30332.652019587462 Test RE 0.3047131756201916\n",
      "53 Train Loss 130049.14 Test MSE 29450.542035446462 Test RE 0.30024977320180984\n",
      "54 Train Loss 124935.0 Test MSE 27884.075466533664 Test RE 0.2921555672706674\n",
      "55 Train Loss 123503.766 Test MSE 27018.676649758887 Test RE 0.28758622413608576\n",
      "56 Train Loss 122635.87 Test MSE 26547.37812801853 Test RE 0.28506694381857767\n",
      "57 Train Loss 120294.125 Test MSE 25201.66950759746 Test RE 0.27774784488625787\n",
      "58 Train Loss 117983.41 Test MSE 24974.050940536905 Test RE 0.27649070664458686\n",
      "59 Train Loss 116036.51 Test MSE 25256.058318479176 Test RE 0.2780473931633436\n",
      "60 Train Loss 114504.36 Test MSE 24754.627381032387 Test RE 0.27527339465785106\n",
      "61 Train Loss 113393.06 Test MSE 24662.481243943188 Test RE 0.2747605808613909\n",
      "62 Train Loss 112695.28 Test MSE 24821.36600377707 Test RE 0.27564421424825775\n",
      "63 Train Loss 111002.78 Test MSE 24194.117979916282 Test RE 0.27213909668673525\n",
      "64 Train Loss 109612.07 Test MSE 23952.581801731063 Test RE 0.2707772715737917\n",
      "65 Train Loss 108327.99 Test MSE 23556.521762743523 Test RE 0.26852926578713904\n",
      "66 Train Loss 106377.695 Test MSE 23700.955625399107 Test RE 0.2693512345424248\n",
      "67 Train Loss 105355.36 Test MSE 23492.060887743 Test RE 0.26816160774518505\n",
      "68 Train Loss 104343.66 Test MSE 23686.01391355498 Test RE 0.2692663180712547\n",
      "69 Train Loss 102011.43 Test MSE 22074.582002358293 Test RE 0.2599454895550325\n",
      "70 Train Loss 99492.04 Test MSE 20345.70452398031 Test RE 0.24955852400724116\n",
      "71 Train Loss 99068.52 Test MSE 20426.555392363553 Test RE 0.2500538869951814\n",
      "72 Train Loss 98754.69 Test MSE 20517.94424688131 Test RE 0.25061263600203904\n",
      "73 Train Loss 98125.23 Test MSE 20291.483886531074 Test RE 0.2492257695102091\n",
      "74 Train Loss 96579.46 Test MSE 19702.231673339054 Test RE 0.24558042838495375\n",
      "75 Train Loss 95679.71 Test MSE 18806.5271294196 Test RE 0.2399331988922119\n",
      "76 Train Loss 95054.9 Test MSE 18485.880228291942 Test RE 0.23787900281857968\n",
      "77 Train Loss 93500.03 Test MSE 17416.12343581019 Test RE 0.23089354351036312\n",
      "78 Train Loss 92498.31 Test MSE 17077.28180950181 Test RE 0.2286364216603893\n",
      "79 Train Loss 91345.19 Test MSE 17133.665826224016 Test RE 0.22901355476002314\n",
      "80 Train Loss 90052.52 Test MSE 16442.426552109562 Test RE 0.22434634428798705\n",
      "81 Train Loss 89013.57 Test MSE 16331.492254638873 Test RE 0.22358824983892595\n",
      "82 Train Loss 88215.75 Test MSE 15882.960534607268 Test RE 0.22049653556466436\n",
      "83 Train Loss 87441.44 Test MSE 15828.213615087761 Test RE 0.22011619317399855\n",
      "84 Train Loss 86467.09 Test MSE 15371.29958376587 Test RE 0.21691587414327043\n",
      "85 Train Loss 85361.0 Test MSE 15718.800594846094 Test RE 0.21935409385631677\n",
      "86 Train Loss 84699.61 Test MSE 15826.994660112248 Test RE 0.22010771726820905\n",
      "87 Train Loss 84498.73 Test MSE 15810.22707018687 Test RE 0.21999109203255915\n",
      "88 Train Loss 83860.88 Test MSE 15360.91656793155 Test RE 0.21684260052679805\n",
      "89 Train Loss 83342.67 Test MSE 15382.880281269065 Test RE 0.2169975706890303\n",
      "90 Train Loss 83117.78 Test MSE 15139.084385448028 Test RE 0.21527115785749057\n",
      "91 Train Loss 82840.81 Test MSE 14957.93182110475 Test RE 0.21397932663810773\n",
      "92 Train Loss 81946.414 Test MSE 14861.947887047449 Test RE 0.21329167701533372\n",
      "93 Train Loss 81443.734 Test MSE 14983.116015334275 Test RE 0.21415938597440118\n",
      "94 Train Loss 80290.53 Test MSE 15080.46340813905 Test RE 0.2148539712846153\n",
      "95 Train Loss 79777.41 Test MSE 14985.872562681567 Test RE 0.21417908525914753\n",
      "96 Train Loss 79534.56 Test MSE 14872.992766255084 Test RE 0.2133709177482995\n",
      "97 Train Loss 78933.766 Test MSE 14450.678902196012 Test RE 0.21031980350301077\n",
      "98 Train Loss 78014.53 Test MSE 14100.277928004858 Test RE 0.20775423158950165\n",
      "99 Train Loss 76830.79 Test MSE 13558.031899045913 Test RE 0.20372032837055487\n",
      "100 Train Loss 75773.516 Test MSE 13405.376618001112 Test RE 0.20257019760343983\n",
      "101 Train Loss 74989.695 Test MSE 13770.405473830373 Test RE 0.2053096705163964\n",
      "102 Train Loss 74157.94 Test MSE 13650.192904726435 Test RE 0.2044115523062487\n",
      "103 Train Loss 73574.55 Test MSE 13320.78418848103 Test RE 0.20193004311115118\n",
      "104 Train Loss 73235.805 Test MSE 13269.621662938793 Test RE 0.20154188314936264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 72556.516 Test MSE 12728.546972119371 Test RE 0.19739014071065447\n",
      "106 Train Loss 71772.93 Test MSE 12794.485228351108 Test RE 0.19790075472462093\n",
      "107 Train Loss 71475.266 Test MSE 12865.48783778017 Test RE 0.1984491171282741\n",
      "108 Train Loss 70815.95 Test MSE 13135.129969813775 Test RE 0.20051793758270367\n",
      "109 Train Loss 70608.58 Test MSE 13254.488052238448 Test RE 0.20142692408730734\n",
      "110 Train Loss 70406.64 Test MSE 13403.262386314187 Test RE 0.20255422277368623\n",
      "111 Train Loss 69772.875 Test MSE 13913.926284480734 Test RE 0.20637680796149332\n",
      "112 Train Loss 68507.05 Test MSE 14064.95495267001 Test RE 0.20749384312882688\n",
      "113 Train Loss 67853.7 Test MSE 13564.062412547766 Test RE 0.20376562999025746\n",
      "114 Train Loss 67355.09 Test MSE 13449.804230951751 Test RE 0.20290559534320587\n",
      "115 Train Loss 66970.086 Test MSE 13552.704432400371 Test RE 0.2036802997034642\n",
      "116 Train Loss 66532.93 Test MSE 13158.784623846675 Test RE 0.20069840967964617\n",
      "117 Train Loss 65817.96 Test MSE 13027.861613515734 Test RE 0.19969749180997204\n",
      "118 Train Loss 65658.766 Test MSE 12793.262463110828 Test RE 0.19789129783981882\n",
      "119 Train Loss 65548.72 Test MSE 13081.695004498444 Test RE 0.20010965885379714\n",
      "120 Train Loss 65386.875 Test MSE 12722.97001276618 Test RE 0.19734689314272497\n",
      "121 Train Loss 65292.797 Test MSE 12661.031522979236 Test RE 0.1968659409196061\n",
      "122 Train Loss 65241.45 Test MSE 12831.013877110936 Test RE 0.19818305974097322\n",
      "123 Train Loss 65200.57 Test MSE 12621.178026618358 Test RE 0.19655585639402354\n",
      "124 Train Loss 65069.742 Test MSE 12441.821385711439 Test RE 0.19515425421555874\n",
      "125 Train Loss 64654.44 Test MSE 12034.782435012736 Test RE 0.191935436007796\n",
      "126 Train Loss 64213.844 Test MSE 11431.787331300531 Test RE 0.18706524631114013\n",
      "127 Train Loss 63551.664 Test MSE 11386.20577351227 Test RE 0.1866919345248088\n",
      "128 Train Loss 62964.6 Test MSE 11419.800429208746 Test RE 0.18696714612609597\n",
      "129 Train Loss 62364.79 Test MSE 11447.95907170016 Test RE 0.187197513532174\n",
      "130 Train Loss 61687.53 Test MSE 11301.691314496848 Test RE 0.185997780747287\n",
      "131 Train Loss 61222.848 Test MSE 11319.912989226295 Test RE 0.1861476621222088\n",
      "132 Train Loss 60860.688 Test MSE 11204.348480264061 Test RE 0.18519503778759352\n",
      "133 Train Loss 60637.844 Test MSE 11177.697348450776 Test RE 0.18497465032338295\n",
      "134 Train Loss 60412.016 Test MSE 11019.861602696716 Test RE 0.18366403106169485\n",
      "135 Train Loss 60179.484 Test MSE 10830.414144371744 Test RE 0.18207846096323618\n",
      "136 Train Loss 59878.33 Test MSE 10705.386499298522 Test RE 0.18102444202593532\n",
      "137 Train Loss 59743.11 Test MSE 10778.14369541241 Test RE 0.18163855008593482\n",
      "138 Train Loss 59694.305 Test MSE 10668.108080460344 Test RE 0.1807089844767271\n",
      "139 Train Loss 59444.3 Test MSE 10425.276357599776 Test RE 0.17864046065742395\n",
      "140 Train Loss 59063.582 Test MSE 10368.890923324167 Test RE 0.1781567144132475\n",
      "141 Train Loss 58939.99 Test MSE 10348.276762868627 Test RE 0.17797953161374533\n",
      "142 Train Loss 58718.934 Test MSE 10321.002798481193 Test RE 0.17774483504625024\n",
      "143 Train Loss 58490.98 Test MSE 10171.16101474394 Test RE 0.17644985531352395\n",
      "144 Train Loss 58247.99 Test MSE 9933.654545726653 Test RE 0.17437754868112934\n",
      "145 Train Loss 58191.305 Test MSE 9902.631538604677 Test RE 0.17410504342229396\n",
      "146 Train Loss 58065.426 Test MSE 9884.621516812771 Test RE 0.1739466480192469\n",
      "147 Train Loss 57957.723 Test MSE 10020.926182129304 Test RE 0.1751418663479497\n",
      "148 Train Loss 57611.98 Test MSE 9997.357884601332 Test RE 0.17493578631862305\n",
      "149 Train Loss 57166.375 Test MSE 10043.096041903265 Test RE 0.17533549742285706\n",
      "150 Train Loss 56801.047 Test MSE 9720.857408347774 Test RE 0.172499693671204\n",
      "151 Train Loss 56597.426 Test MSE 9838.952835459619 Test RE 0.17354435083197606\n",
      "152 Train Loss 56254.457 Test MSE 9812.293914565282 Test RE 0.17330907969273818\n",
      "153 Train Loss 55903.395 Test MSE 9532.33213617148 Test RE 0.17081878358737354\n",
      "154 Train Loss 55476.742 Test MSE 8953.621937410484 Test RE 0.1655523760043358\n",
      "155 Train Loss 55289.426 Test MSE 8885.42644740607 Test RE 0.16492070397019867\n",
      "156 Train Loss 55156.52 Test MSE 8735.733365001353 Test RE 0.16352559076953277\n",
      "157 Train Loss 55047.15 Test MSE 8633.799080431543 Test RE 0.16256872916393508\n",
      "158 Train Loss 55021.43 Test MSE 8598.566318888848 Test RE 0.16223668526257215\n",
      "159 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "160 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "161 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "162 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "163 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "164 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "165 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "166 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "167 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "168 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "169 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "170 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "171 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "172 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "173 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "174 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "175 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "176 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "177 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "178 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "179 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "180 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "181 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "182 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "183 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "184 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "185 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "186 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "187 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "188 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "189 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "190 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "191 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "192 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "193 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "194 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "195 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "196 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "197 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "198 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "199 Train Loss 55013.395 Test MSE 8636.133027127958 Test RE 0.16259070101583972\n",
      "Training time: 542.78\n",
      "Training time: 542.78\n",
      "ES_rowdy\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 498875.25 Test MSE 324723.9189197389 Test RE 0.9969954622298202\n",
      "1 Train Loss 458147.6 Test MSE 284247.7870835425 Test RE 0.9327915172886199\n",
      "2 Train Loss 414288.47 Test MSE 240335.46186257538 Test RE 0.8577188803890914\n",
      "3 Train Loss 350127.38 Test MSE 175991.95588752316 Test RE 0.7339769995764227\n",
      "4 Train Loss 278293.47 Test MSE 102721.50196841024 Test RE 0.5607466946646077\n",
      "5 Train Loss 258370.56 Test MSE 83664.78650773478 Test RE 0.5060662740468972\n",
      "6 Train Loss 249739.27 Test MSE 75016.69314381582 Test RE 0.47919800327422196\n",
      "7 Train Loss 248623.94 Test MSE 74309.27060469381 Test RE 0.47693318431500054\n",
      "8 Train Loss 246361.66 Test MSE 73705.99646058663 Test RE 0.47499326618692694\n",
      "9 Train Loss 243732.84 Test MSE 72463.07452792299 Test RE 0.47097127534184174\n",
      "10 Train Loss 239416.53 Test MSE 70625.61378193818 Test RE 0.4649616774314988\n",
      "11 Train Loss 232782.52 Test MSE 68386.95106502643 Test RE 0.4575332520976019\n",
      "12 Train Loss 230659.48 Test MSE 68129.99907977105 Test RE 0.45667289266373695\n",
      "13 Train Loss 228879.38 Test MSE 66955.25116494345 Test RE 0.4527186267905089\n",
      "14 Train Loss 225377.4 Test MSE 65237.54605897665 Test RE 0.4468737559188859\n",
      "15 Train Loss 222394.05 Test MSE 63920.298104455025 Test RE 0.442339209729736\n",
      "16 Train Loss 219649.61 Test MSE 61757.53506444705 Test RE 0.4347914734140202\n",
      "17 Train Loss 217607.23 Test MSE 61741.1580935208 Test RE 0.43473382021076834\n",
      "18 Train Loss 216128.02 Test MSE 60991.31259375914 Test RE 0.43208583764551145\n",
      "19 Train Loss 214036.02 Test MSE 60588.37337886236 Test RE 0.43065618455462096\n",
      "20 Train Loss 211521.66 Test MSE 59798.35896717611 Test RE 0.4278392997797681\n",
      "21 Train Loss 208348.31 Test MSE 58551.12681711667 Test RE 0.4233540030150227\n",
      "22 Train Loss 206238.3 Test MSE 58072.34405079225 Test RE 0.4216195302397618\n",
      "23 Train Loss 202705.38 Test MSE 56866.508217737224 Test RE 0.417219235406227\n",
      "24 Train Loss 201438.66 Test MSE 57471.59344855426 Test RE 0.4194330621434647\n",
      "25 Train Loss 199033.3 Test MSE 56132.71065108803 Test RE 0.41451862535375555\n",
      "26 Train Loss 197691.2 Test MSE 54896.844147967204 Test RE 0.40993002713209864\n",
      "27 Train Loss 196446.48 Test MSE 54499.31487219035 Test RE 0.40844309946832286\n",
      "28 Train Loss 195275.44 Test MSE 54284.09871813461 Test RE 0.4076358369630981\n",
      "29 Train Loss 194158.14 Test MSE 54176.87536070857 Test RE 0.40723305153646594\n",
      "30 Train Loss 192922.78 Test MSE 54521.81162236733 Test RE 0.40852739130156396\n",
      "31 Train Loss 191320.33 Test MSE 53080.18731073421 Test RE 0.403090223741079\n",
      "32 Train Loss 189170.44 Test MSE 52483.78341356543 Test RE 0.40081928514917226\n",
      "33 Train Loss 188402.98 Test MSE 52712.22689397994 Test RE 0.4016906507952431\n",
      "34 Train Loss 186550.5 Test MSE 51352.10800908624 Test RE 0.39647442680268746\n",
      "35 Train Loss 185446.0 Test MSE 51048.49415362054 Test RE 0.39530063289041695\n",
      "36 Train Loss 183232.23 Test MSE 50248.40104372837 Test RE 0.39219058630643533\n",
      "37 Train Loss 181780.56 Test MSE 50477.02273643485 Test RE 0.3930817740560093\n",
      "38 Train Loss 180160.73 Test MSE 49499.26428989901 Test RE 0.3892560881491634\n",
      "39 Train Loss 179520.16 Test MSE 49303.15975145055 Test RE 0.3884842520219967\n",
      "40 Train Loss 177839.45 Test MSE 48755.423427693626 Test RE 0.38632028083705067\n",
      "41 Train Loss 177019.03 Test MSE 48382.38665485427 Test RE 0.3848395390386702\n",
      "42 Train Loss 176597.11 Test MSE 48261.62951612178 Test RE 0.3843589803230326\n",
      "43 Train Loss 174893.5 Test MSE 47783.16097283586 Test RE 0.3824489560997962\n",
      "44 Train Loss 172803.1 Test MSE 46644.10097530595 Test RE 0.37786303217949563\n",
      "45 Train Loss 171909.88 Test MSE 46130.7307171078 Test RE 0.3757778772784752\n",
      "46 Train Loss 171156.89 Test MSE 45739.70015321213 Test RE 0.37418183329596777\n",
      "47 Train Loss 169969.97 Test MSE 45880.07606599986 Test RE 0.3747555786097831\n",
      "48 Train Loss 169263.47 Test MSE 45750.18826525592 Test RE 0.3742247307792841\n",
      "49 Train Loss 168076.52 Test MSE 44833.79243894106 Test RE 0.3704578315539413\n",
      "50 Train Loss 167061.28 Test MSE 44961.956185923154 Test RE 0.3709869567748071\n",
      "51 Train Loss 166448.97 Test MSE 44697.94157552049 Test RE 0.3698961436464846\n",
      "52 Train Loss 165795.22 Test MSE 44731.835352801216 Test RE 0.37003636041352445\n",
      "53 Train Loss 165061.72 Test MSE 44507.256315425046 Test RE 0.3691062959878318\n",
      "54 Train Loss 163781.69 Test MSE 43772.99680244779 Test RE 0.36604896402510745\n",
      "55 Train Loss 162733.6 Test MSE 43888.66002287038 Test RE 0.3665322582231214\n",
      "56 Train Loss 161579.84 Test MSE 42707.13305382055 Test RE 0.36156488956217675\n",
      "57 Train Loss 160647.4 Test MSE 43004.17467224418 Test RE 0.3628201098147166\n",
      "58 Train Loss 159922.28 Test MSE 42964.54006880527 Test RE 0.36265287551504294\n",
      "59 Train Loss 159442.28 Test MSE 42978.36800998179 Test RE 0.3627112299067575\n",
      "60 Train Loss 159019.55 Test MSE 42774.68648652692 Test RE 0.361850735234515\n",
      "61 Train Loss 158372.56 Test MSE 42434.213745014305 Test RE 0.3604077503611324\n",
      "62 Train Loss 157807.38 Test MSE 42175.293574997195 Test RE 0.35930652082064196\n",
      "63 Train Loss 157131.48 Test MSE 42146.556883220735 Test RE 0.3591840908470404\n",
      "64 Train Loss 156487.9 Test MSE 42021.372414797006 Test RE 0.3586502666048407\n",
      "65 Train Loss 156169.97 Test MSE 41713.82538144683 Test RE 0.35733540727246915\n",
      "66 Train Loss 155421.42 Test MSE 41682.62576555451 Test RE 0.3572017487937001\n",
      "67 Train Loss 154927.58 Test MSE 41547.47847721532 Test RE 0.3566222022714032\n",
      "68 Train Loss 154387.44 Test MSE 41154.20968120303 Test RE 0.35493038064261184\n",
      "69 Train Loss 153279.03 Test MSE 40523.884649502994 Test RE 0.3522018047784405\n",
      "70 Train Loss 152365.64 Test MSE 40239.2169969283 Test RE 0.35096257071130876\n",
      "71 Train Loss 152062.44 Test MSE 40445.75350881857 Test RE 0.3518621136848373\n",
      "72 Train Loss 151809.14 Test MSE 40136.72064312273 Test RE 0.350515304055517\n",
      "73 Train Loss 151456.6 Test MSE 39552.78816739982 Test RE 0.3479562113823243\n",
      "74 Train Loss 151169.03 Test MSE 39342.05769747846 Test RE 0.34702804795688197\n",
      "75 Train Loss 150852.73 Test MSE 39191.57958891969 Test RE 0.3463637441964271\n",
      "76 Train Loss 150214.47 Test MSE 39060.60835630588 Test RE 0.34578451710514685\n",
      "77 Train Loss 149651.58 Test MSE 38728.828733473965 Test RE 0.3443128437524679\n",
      "78 Train Loss 149090.69 Test MSE 38066.4765833917 Test RE 0.3413558752585187\n",
      "79 Train Loss 148332.72 Test MSE 37618.20546413364 Test RE 0.339340018298081\n",
      "80 Train Loss 147790.1 Test MSE 37086.481079820805 Test RE 0.3369332380013367\n",
      "81 Train Loss 146977.88 Test MSE 37055.02049314818 Test RE 0.3367902968808645\n",
      "82 Train Loss 145932.7 Test MSE 36699.373061657956 Test RE 0.3351701737049321\n",
      "83 Train Loss 145380.64 Test MSE 36387.46115026943 Test RE 0.3337428107619073\n",
      "84 Train Loss 144861.78 Test MSE 36151.427790377566 Test RE 0.3326586103756523\n",
      "85 Train Loss 144538.86 Test MSE 36093.82466239158 Test RE 0.3323934781672258\n",
      "86 Train Loss 144202.22 Test MSE 35894.529338451815 Test RE 0.3314745375511405\n",
      "87 Train Loss 143433.69 Test MSE 36205.82445380435 Test RE 0.33290879021160613\n",
      "88 Train Loss 143062.84 Test MSE 36608.57708484262 Test RE 0.33475530354892247\n",
      "89 Train Loss 142651.17 Test MSE 36705.23643133492 Test RE 0.3351969472977836\n",
      "90 Train Loss 142174.45 Test MSE 37186.26846833819 Test RE 0.33738622115215444\n",
      "91 Train Loss 141600.86 Test MSE 37227.293628045765 Test RE 0.3375722778435778\n",
      "92 Train Loss 140775.92 Test MSE 37045.30817278174 Test RE 0.33674615671332647\n",
      "93 Train Loss 140215.0 Test MSE 37210.017284800924 Test RE 0.33749393893202906\n",
      "94 Train Loss 139769.67 Test MSE 36754.02681345339 Test RE 0.33541965334167384\n",
      "95 Train Loss 139371.7 Test MSE 36556.5371063204 Test RE 0.33451728754869975\n",
      "96 Train Loss 139297.92 Test MSE 36614.57864731953 Test RE 0.33478274209877873\n",
      "97 Train Loss 139114.17 Test MSE 36791.57117523758 Test RE 0.3355909257649953\n",
      "98 Train Loss 138753.86 Test MSE 36707.839683410326 Test RE 0.3352088337036779\n",
      "99 Train Loss 138417.64 Test MSE 36238.20371781647 Test RE 0.333057618926725\n",
      "100 Train Loss 138179.53 Test MSE 36436.5719104418 Test RE 0.33396795475366264\n",
      "101 Train Loss 137772.19 Test MSE 36513.18656541103 Test RE 0.33431888518064673\n",
      "102 Train Loss 136893.03 Test MSE 36052.60022163876 Test RE 0.332203602887385\n",
      "103 Train Loss 136475.48 Test MSE 35585.00913560802 Test RE 0.33004228305784306\n",
      "104 Train Loss 136140.75 Test MSE 35655.68457644914 Test RE 0.33036986922420125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 135883.88 Test MSE 35179.18618015677 Test RE 0.328154932157946\n",
      "106 Train Loss 135260.25 Test MSE 34705.86504604346 Test RE 0.3259398630169803\n",
      "107 Train Loss 134607.22 Test MSE 34035.737772059605 Test RE 0.32277777706980976\n",
      "108 Train Loss 133733.84 Test MSE 33685.27523965702 Test RE 0.32111167206198465\n",
      "109 Train Loss 133301.47 Test MSE 34008.86266438281 Test RE 0.32265031691802126\n",
      "110 Train Loss 133028.22 Test MSE 33905.876376313085 Test RE 0.32216141858566566\n",
      "111 Train Loss 132191.12 Test MSE 34044.79863054571 Test RE 0.3228207385175566\n",
      "112 Train Loss 131797.97 Test MSE 34058.0094423168 Test RE 0.3228833664451863\n",
      "113 Train Loss 131587.1 Test MSE 33638.5968680988 Test RE 0.32088910944279003\n",
      "114 Train Loss 131350.39 Test MSE 33167.03082349365 Test RE 0.31863196292934015\n",
      "115 Train Loss 131117.78 Test MSE 32784.66007755889 Test RE 0.3167899420027716\n",
      "116 Train Loss 130779.664 Test MSE 32501.11784401773 Test RE 0.3154170684701775\n",
      "117 Train Loss 130328.164 Test MSE 32395.257845316944 Test RE 0.314902974096143\n",
      "118 Train Loss 130124.01 Test MSE 32444.892153511846 Test RE 0.3151441206408314\n",
      "119 Train Loss 129980.1 Test MSE 32273.6343034069 Test RE 0.3143112882612979\n",
      "120 Train Loss 129368.516 Test MSE 32450.65100316094 Test RE 0.31517208786405465\n",
      "121 Train Loss 128538.98 Test MSE 32114.514128752828 Test RE 0.31353549919576296\n",
      "122 Train Loss 128296.96 Test MSE 31977.715304666926 Test RE 0.3128669998813261\n",
      "123 Train Loss 127819.05 Test MSE 31581.998553113357 Test RE 0.310925145689427\n",
      "124 Train Loss 127594.47 Test MSE 31450.538723400463 Test RE 0.31027735910310233\n",
      "125 Train Loss 127377.12 Test MSE 31603.194675835086 Test RE 0.31102946622703015\n",
      "126 Train Loss 127171.04 Test MSE 31478.07006120456 Test RE 0.3104131355077193\n",
      "127 Train Loss 126922.46 Test MSE 31487.213155190384 Test RE 0.3104582134049045\n",
      "128 Train Loss 126628.766 Test MSE 31590.686760115183 Test RE 0.3109679104983306\n",
      "129 Train Loss 126152.48 Test MSE 31658.63971352273 Test RE 0.31130218364615597\n",
      "130 Train Loss 125470.08 Test MSE 31332.64696861703 Test RE 0.30969527867974816\n",
      "131 Train Loss 124698.45 Test MSE 31326.09584332128 Test RE 0.3096629009710132\n",
      "132 Train Loss 123626.3 Test MSE 31072.59266740633 Test RE 0.3084073983788781\n",
      "133 Train Loss 122769.09 Test MSE 30257.538420126897 Test RE 0.3043356568560197\n",
      "134 Train Loss 122155.414 Test MSE 30077.527860032373 Test RE 0.30342901740381883\n",
      "135 Train Loss 121903.9 Test MSE 30064.829844058488 Test RE 0.3033649603897369\n",
      "136 Train Loss 121392.69 Test MSE 30051.723472657828 Test RE 0.30329882917797224\n",
      "137 Train Loss 121184.03 Test MSE 29799.88281901376 Test RE 0.3020252969554442\n",
      "138 Train Loss 120769.41 Test MSE 29902.249359790534 Test RE 0.3025436006569836\n",
      "139 Train Loss 120417.92 Test MSE 30049.667667480895 Test RE 0.3032884548317597\n",
      "140 Train Loss 119863.125 Test MSE 30429.802601565378 Test RE 0.30520075904606636\n",
      "141 Train Loss 119565.016 Test MSE 30530.95687616179 Test RE 0.30570760997755136\n",
      "142 Train Loss 119244.58 Test MSE 30412.21952174602 Test RE 0.3051125700951517\n",
      "143 Train Loss 118869.4 Test MSE 29881.315276473484 Test RE 0.3024376791634904\n",
      "144 Train Loss 118764.68 Test MSE 30002.87849855536 Test RE 0.30305224351784943\n",
      "145 Train Loss 118690.17 Test MSE 30124.85871550288 Test RE 0.30366766583161\n",
      "146 Train Loss 118523.9 Test MSE 30162.59054753771 Test RE 0.30385778044601836\n",
      "147 Train Loss 118292.11 Test MSE 29881.35628843545 Test RE 0.3024378867105516\n",
      "148 Train Loss 117942.82 Test MSE 29565.80721663045 Test RE 0.3008367666016838\n",
      "149 Train Loss 117751.55 Test MSE 29530.776366327038 Test RE 0.3006584915566624\n",
      "150 Train Loss 117600.836 Test MSE 29415.736955869623 Test RE 0.30007230097367255\n",
      "151 Train Loss 117465.875 Test MSE 29464.813615705873 Test RE 0.30032251413353545\n",
      "152 Train Loss 117114.03 Test MSE 29239.522118426914 Test RE 0.29917216000412633\n",
      "153 Train Loss 116935.375 Test MSE 29135.0059111956 Test RE 0.2986369882690553\n",
      "154 Train Loss 116351.06 Test MSE 28862.239661821735 Test RE 0.29723575902478755\n",
      "155 Train Loss 116059.78 Test MSE 29111.473251020176 Test RE 0.29851635773876567\n",
      "156 Train Loss 115911.92 Test MSE 28978.247040075268 Test RE 0.2978325069324766\n",
      "157 Train Loss 115345.65 Test MSE 28751.210413760145 Test RE 0.2966634946603208\n",
      "158 Train Loss 114510.84 Test MSE 28033.313689920953 Test RE 0.2929363461525882\n",
      "159 Train Loss 114298.016 Test MSE 27746.421696448793 Test RE 0.29143354097220303\n",
      "160 Train Loss 114124.234 Test MSE 27581.640501146354 Test RE 0.29056686562228523\n",
      "161 Train Loss 113889.02 Test MSE 27515.392514559575 Test RE 0.2902177014295114\n",
      "162 Train Loss 113637.805 Test MSE 26964.033540511893 Test RE 0.2872952669013085\n",
      "163 Train Loss 113368.25 Test MSE 27041.56859690071 Test RE 0.2877080290424919\n",
      "164 Train Loss 113001.836 Test MSE 26991.31580518249 Test RE 0.2874405731266943\n",
      "165 Train Loss 112753.46 Test MSE 26918.50838502645 Test RE 0.28705263467523007\n",
      "166 Train Loss 112289.18 Test MSE 26858.855079716875 Test RE 0.2867343938660847\n",
      "167 Train Loss 112115.64 Test MSE 26749.988823376716 Test RE 0.2861526975198772\n",
      "168 Train Loss 112021.73 Test MSE 26710.554202628475 Test RE 0.2859416977123553\n",
      "169 Train Loss 111898.26 Test MSE 26647.264956673076 Test RE 0.2856027349374033\n",
      "170 Train Loss 111819.83 Test MSE 26662.074297064788 Test RE 0.2856820864324878\n",
      "171 Train Loss 111683.47 Test MSE 26619.816181358707 Test RE 0.2854556004160047\n",
      "172 Train Loss 111539.92 Test MSE 26197.999202939944 Test RE 0.2831849078098243\n",
      "173 Train Loss 111344.46 Test MSE 26001.811241121333 Test RE 0.28212257718069333\n",
      "174 Train Loss 111210.34 Test MSE 25899.378846700092 Test RE 0.281566327311238\n",
      "175 Train Loss 111172.336 Test MSE 25784.37023939934 Test RE 0.28094047097634234\n",
      "176 Train Loss 111088.164 Test MSE 25712.141589047525 Test RE 0.28054670177544283\n",
      "177 Train Loss 111024.6 Test MSE 25642.523879211516 Test RE 0.28016664286057835\n",
      "178 Train Loss 110574.24 Test MSE 25258.164537279314 Test RE 0.2780589867467596\n",
      "179 Train Loss 110376.61 Test MSE 25171.751562355923 Test RE 0.27758293294545283\n",
      "180 Train Loss 110338.38 Test MSE 25105.49158286067 Test RE 0.27721734934350134\n",
      "181 Train Loss 110258.08 Test MSE 25138.987871803034 Test RE 0.2774022223852119\n",
      "182 Train Loss 109849.89 Test MSE 24901.45015407116 Test RE 0.27608852814611334\n",
      "183 Train Loss 109660.39 Test MSE 24702.68769102289 Test RE 0.27498445630724405\n",
      "184 Train Loss 109406.875 Test MSE 24513.149923050118 Test RE 0.27392748019885854\n",
      "185 Train Loss 109012.92 Test MSE 24489.112172078265 Test RE 0.2737931397409553\n",
      "186 Train Loss 108839.36 Test MSE 24661.6713992922 Test RE 0.27475606965241695\n",
      "187 Train Loss 108778.37 Test MSE 24530.82362352685 Test RE 0.27402621169469527\n",
      "188 Train Loss 108658.77 Test MSE 24458.300615586064 Test RE 0.2736208458682819\n",
      "189 Train Loss 108579.445 Test MSE 24370.987659453793 Test RE 0.2731320137728801\n",
      "190 Train Loss 108483.125 Test MSE 24213.33303297673 Test RE 0.27224714214934265\n",
      "191 Train Loss 108316.57 Test MSE 24346.726270593303 Test RE 0.27299602806518475\n",
      "192 Train Loss 108274.59 Test MSE 24326.95923591339 Test RE 0.2728851832301634\n",
      "193 Train Loss 108253.62 Test MSE 24349.309955059238 Test RE 0.2730105129055182\n",
      "194 Train Loss 108195.06 Test MSE 24336.13255814735 Test RE 0.27293662878390673\n",
      "195 Train Loss 108138.98 Test MSE 24283.434745057963 Test RE 0.27264095817491\n",
      "196 Train Loss 108107.15 Test MSE 24276.38262550202 Test RE 0.27260136665366735\n",
      "197 Train Loss 108105.58 Test MSE 24275.654395083282 Test RE 0.2725972779453284\n",
      "198 Train Loss 108090.66 Test MSE 24267.240205795868 Test RE 0.2725500313531945\n",
      "199 Train Loss 108086.46 Test MSE 24265.32830866908 Test RE 0.2725392946999673\n",
      "Training time: 705.67\n",
      "Training time: 705.67\n",
      "ES_rowdy\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 492383.16 Test MSE 318549.008983012 Test RE 0.9874705926712224\n",
      "1 Train Loss 470446.03 Test MSE 296502.46144175075 Test RE 0.9526868998079226\n",
      "2 Train Loss 405959.3 Test MSE 230580.61910719736 Test RE 0.8401318362108752\n",
      "3 Train Loss 332486.62 Test MSE 157615.11892901533 Test RE 0.6946003196818793\n",
      "4 Train Loss 266613.22 Test MSE 91810.94470676032 Test RE 0.5301310843589702\n",
      "5 Train Loss 249932.47 Test MSE 75044.77685988694 Test RE 0.4792876926541064\n",
      "6 Train Loss 249783.62 Test MSE 74968.32824813147 Test RE 0.47904350367386633\n",
      "7 Train Loss 249523.06 Test MSE 74850.28213107392 Test RE 0.478666200983754\n",
      "8 Train Loss 249166.11 Test MSE 74590.72604795298 Test RE 0.47783555202478833\n",
      "9 Train Loss 248765.88 Test MSE 74303.81166185768 Test RE 0.47691566564750293\n",
      "10 Train Loss 247708.69 Test MSE 73857.37852402992 Test RE 0.4754808016729985\n",
      "11 Train Loss 244794.39 Test MSE 72742.36018003424 Test RE 0.4718780062797227\n",
      "12 Train Loss 243183.16 Test MSE 72056.2101439841 Test RE 0.46964721356973416\n",
      "13 Train Loss 241563.6 Test MSE 71061.71187487291 Test RE 0.4663949878056291\n",
      "14 Train Loss 241217.38 Test MSE 71083.3174855191 Test RE 0.4664658838087688\n",
      "15 Train Loss 240049.12 Test MSE 70477.1575238475 Test RE 0.4644727416264068\n",
      "16 Train Loss 238930.31 Test MSE 69976.04653348125 Test RE 0.4628185346906836\n",
      "17 Train Loss 238615.89 Test MSE 70078.9128249585 Test RE 0.46315858635939056\n",
      "18 Train Loss 236989.19 Test MSE 68824.35431267017 Test RE 0.45899411227640136\n",
      "19 Train Loss 236590.58 Test MSE 69004.11385425237 Test RE 0.4595931354498078\n",
      "20 Train Loss 235773.55 Test MSE 68283.17385979941 Test RE 0.45718596691785884\n",
      "21 Train Loss 234897.77 Test MSE 68218.33024590704 Test RE 0.45696883705954894\n",
      "22 Train Loss 232857.86 Test MSE 67123.13498651343 Test RE 0.4532858455539536\n",
      "23 Train Loss 227806.16 Test MSE 65808.59000777033 Test RE 0.4488253050914947\n",
      "24 Train Loss 226667.42 Test MSE 65199.83836556643 Test RE 0.44674458939410666\n",
      "25 Train Loss 221339.6 Test MSE 63777.00172467754 Test RE 0.4418431140170152\n",
      "26 Train Loss 218595.31 Test MSE 63064.01573407517 Test RE 0.43936641120463965\n",
      "27 Train Loss 216161.36 Test MSE 62150.80130283632 Test RE 0.43617363236936846\n",
      "28 Train Loss 213530.88 Test MSE 61452.48380401007 Test RE 0.43371631814271455\n",
      "29 Train Loss 210208.53 Test MSE 59274.568010950934 Test RE 0.42596139496490176\n",
      "30 Train Loss 209142.05 Test MSE 58875.34353933887 Test RE 0.42452450961065225\n",
      "31 Train Loss 208520.45 Test MSE 58841.30565259255 Test RE 0.4244017756662333\n",
      "32 Train Loss 203227.9 Test MSE 57710.812711131235 Test RE 0.4203050779397451\n",
      "33 Train Loss 201274.25 Test MSE 57025.83797465152 Test RE 0.41780331330630605\n",
      "34 Train Loss 200908.28 Test MSE 56785.5675923371 Test RE 0.41692220634605653\n",
      "35 Train Loss 200529.95 Test MSE 56670.0036143168 Test RE 0.41649775230761327\n",
      "36 Train Loss 198343.2 Test MSE 56628.67539983535 Test RE 0.41634585318309714\n",
      "37 Train Loss 197672.81 Test MSE 56573.52957795699 Test RE 0.41614308200355976\n",
      "38 Train Loss 197105.69 Test MSE 56636.54793679753 Test RE 0.41637479244563796\n",
      "39 Train Loss 196904.48 Test MSE 56155.251100556125 Test RE 0.41460184330483185\n",
      "40 Train Loss 196266.62 Test MSE 55621.81031994404 Test RE 0.4126279114751109\n",
      "41 Train Loss 191365.8 Test MSE 53913.46163891484 Test RE 0.40624184005136355\n",
      "42 Train Loss 189863.58 Test MSE 52816.025607926524 Test RE 0.40208595250793555\n",
      "43 Train Loss 189065.25 Test MSE 52644.48827305732 Test RE 0.40143246858979853\n",
      "44 Train Loss 188620.62 Test MSE 52195.91028374806 Test RE 0.399718528385877\n",
      "45 Train Loss 187422.64 Test MSE 51511.23474294288 Test RE 0.3970882368683109\n",
      "46 Train Loss 186672.8 Test MSE 51720.771110319285 Test RE 0.3978950509976127\n",
      "47 Train Loss 185665.44 Test MSE 51578.5294341619 Test RE 0.39734753184231086\n",
      "48 Train Loss 184946.7 Test MSE 51642.53749310659 Test RE 0.3975940060903679\n",
      "49 Train Loss 183789.53 Test MSE 51888.019335428726 Test RE 0.3985378636276465\n",
      "50 Train Loss 182736.9 Test MSE 51158.72791097742 Test RE 0.395727207425016\n",
      "51 Train Loss 182056.11 Test MSE 50801.4352673992 Test RE 0.3943429064519634\n",
      "52 Train Loss 179846.47 Test MSE 49980.97793169808 Test RE 0.3911455705247121\n",
      "53 Train Loss 176881.78 Test MSE 48902.012932475394 Test RE 0.38690060597641907\n",
      "54 Train Loss 175075.89 Test MSE 47571.93975107907 Test RE 0.38160272906440107\n",
      "55 Train Loss 173688.97 Test MSE 47121.00147072097 Test RE 0.37978980098172294\n",
      "56 Train Loss 172808.14 Test MSE 47135.00396518538 Test RE 0.37984622603004037\n",
      "57 Train Loss 167773.98 Test MSE 44583.09888645907 Test RE 0.36942064983887757\n",
      "58 Train Loss 160386.16 Test MSE 38744.60134442988 Test RE 0.34438294863166236\n",
      "59 Train Loss 158092.77 Test MSE 38907.42511755117 Test RE 0.34510582279092855\n",
      "60 Train Loss 157825.67 Test MSE 38967.29401608119 Test RE 0.3453712369561109\n",
      "61 Train Loss 157693.3 Test MSE 38977.99080102001 Test RE 0.34541863707000153\n",
      "62 Train Loss 157167.2 Test MSE 38848.29119763141 Test RE 0.3448434664404832\n",
      "63 Train Loss 156944.97 Test MSE 38559.84757239953 Test RE 0.34356087177165895\n",
      "64 Train Loss 156714.84 Test MSE 38795.390177485235 Test RE 0.3446085940018887\n",
      "65 Train Loss 155964.73 Test MSE 38772.741346176 Test RE 0.34450798770109564\n",
      "66 Train Loss 154514.69 Test MSE 38721.51433371198 Test RE 0.3442803284300917\n",
      "67 Train Loss 153668.02 Test MSE 38192.143263931874 Test RE 0.3419188603111016\n",
      "68 Train Loss 153469.39 Test MSE 38415.54329130067 Test RE 0.34291740740926874\n",
      "69 Train Loss 153250.1 Test MSE 38254.06680676923 Test RE 0.3421959363023353\n",
      "70 Train Loss 153093.17 Test MSE 38253.94732611964 Test RE 0.34219540190392883\n",
      "71 Train Loss 150054.22 Test MSE 36349.29905136203 Test RE 0.3335677550301452\n",
      "72 Train Loss 146807.44 Test MSE 35343.384246591864 Test RE 0.3289198685275503\n",
      "73 Train Loss 146605.39 Test MSE 35373.236597757226 Test RE 0.32905874825177883\n",
      "74 Train Loss 146453.19 Test MSE 35103.47598085593 Test RE 0.3278016258472603\n",
      "75 Train Loss 146058.4 Test MSE 34519.79999702799 Test RE 0.3250649745943615\n",
      "76 Train Loss 144898.2 Test MSE 34317.36480220007 Test RE 0.32411043038777154\n",
      "77 Train Loss 144724.77 Test MSE 34164.72065026432 Test RE 0.3233888030200271\n",
      "78 Train Loss 144601.6 Test MSE 34009.07578507612 Test RE 0.3226513278802547\n",
      "79 Train Loss 144190.61 Test MSE 33327.24309468951 Test RE 0.31940060652359925\n",
      "80 Train Loss 143963.7 Test MSE 33370.47607998712 Test RE 0.3196077068582221\n",
      "81 Train Loss 143783.97 Test MSE 33558.7141457016 Test RE 0.32050786982047696\n",
      "82 Train Loss 143569.3 Test MSE 33488.76136885421 Test RE 0.32017364796427505\n",
      "83 Train Loss 143481.22 Test MSE 33509.56744204371 Test RE 0.3202730921019539\n",
      "84 Train Loss 143415.78 Test MSE 33380.462030673385 Test RE 0.3196555237976784\n",
      "85 Train Loss 143198.16 Test MSE 33304.49144698505 Test RE 0.3192915646399276\n",
      "86 Train Loss 143143.83 Test MSE 33301.88854029254 Test RE 0.31927908730799226\n",
      "87 Train Loss 142946.94 Test MSE 33187.44818327009 Test RE 0.3187300214888112\n",
      "88 Train Loss 142552.92 Test MSE 33196.445842109206 Test RE 0.318773225016048\n",
      "89 Train Loss 142235.42 Test MSE 32942.54379362911 Test RE 0.31755182120504927\n",
      "90 Train Loss 141259.42 Test MSE 31782.48191966335 Test RE 0.3119104650009177\n",
      "91 Train Loss 139620.75 Test MSE 30929.987905855527 Test RE 0.3076988810522706\n",
      "92 Train Loss 138279.72 Test MSE 30952.390154357247 Test RE 0.3078102923323575\n",
      "93 Train Loss 137898.94 Test MSE 31100.914122738046 Test RE 0.3085479170194955\n",
      "94 Train Loss 137842.77 Test MSE 31213.931873914036 Test RE 0.3091080254648257\n",
      "95 Train Loss 137680.38 Test MSE 31108.08685523754 Test RE 0.3085834948175574\n",
      "96 Train Loss 137474.5 Test MSE 31280.15783661854 Test RE 0.3094357658138938\n",
      "97 Train Loss 136982.8 Test MSE 30682.27420144214 Test RE 0.30646424663792043\n",
      "98 Train Loss 136810.42 Test MSE 30450.175976713617 Test RE 0.3053029110255714\n",
      "99 Train Loss 136747.2 Test MSE 30499.079997598128 Test RE 0.30554797611186685\n",
      "100 Train Loss 136733.2 Test MSE 30514.444195712487 Test RE 0.3056249277537375\n",
      "101 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "102 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "103 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "104 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "106 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "107 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "108 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "109 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "110 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "111 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "112 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "113 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "114 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "115 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "116 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "117 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "118 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "119 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "120 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "121 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "122 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "123 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "124 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "125 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "126 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "127 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "128 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "129 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "130 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "131 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "132 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "133 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "134 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "135 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "136 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "137 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "138 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "139 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "140 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "141 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "142 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "143 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "144 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "145 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "146 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "147 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "148 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "149 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "150 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "151 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "152 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "153 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "154 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "155 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "156 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "157 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "158 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "159 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "160 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "161 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "162 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "163 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "164 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "165 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "166 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "167 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "168 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "169 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "170 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "171 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "172 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "173 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "174 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "175 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "176 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "177 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "178 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "179 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "180 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "181 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "182 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "183 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "184 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "185 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "186 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "187 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "188 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "189 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "190 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "191 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "192 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "193 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "194 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "195 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "196 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "197 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "198 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "199 Train Loss 136727.77 Test MSE 30501.67219177441 Test RE 0.305560960484767\n",
      "Training time: 412.78\n",
      "Training time: 412.78\n",
      "ES_rowdy\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 499768.84 Test MSE 325968.12480182317 Test RE 0.9989036703712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 465387.4 Test MSE 291307.82944534946 Test RE 0.9443046330549321\n",
      "2 Train Loss 394153.6 Test MSE 216729.65033124725 Test RE 0.8145077238239398\n",
      "3 Train Loss 283814.5 Test MSE 106724.43446739455 Test RE 0.5715680877576018\n",
      "4 Train Loss 250030.2 Test MSE 75095.68906371383 Test RE 0.4794502453091054\n",
      "5 Train Loss 249996.52 Test MSE 75079.7703579739 Test RE 0.4793994259346649\n",
      "6 Train Loss 249926.69 Test MSE 75024.44484297145 Test RE 0.47922276111575357\n",
      "7 Train Loss 249545.5 Test MSE 74816.93398781106 Test RE 0.47855955871768724\n",
      "8 Train Loss 247723.61 Test MSE 73611.65852790952 Test RE 0.47468919167200924\n",
      "9 Train Loss 245738.62 Test MSE 72540.05878631755 Test RE 0.4712213872078566\n",
      "10 Train Loss 241554.8 Test MSE 71167.2195473184 Test RE 0.46674109541107595\n",
      "11 Train Loss 235523.08 Test MSE 68729.54939387437 Test RE 0.4586778732382668\n",
      "12 Train Loss 229909.38 Test MSE 67558.17270847753 Test RE 0.45475238876656304\n",
      "13 Train Loss 221860.39 Test MSE 64028.94574477 Test RE 0.4427149800888086\n",
      "14 Train Loss 216466.2 Test MSE 61605.87647068709 Test RE 0.43425728438392414\n",
      "15 Train Loss 200732.17 Test MSE 55247.43271610185 Test RE 0.4112369151515459\n",
      "16 Train Loss 186169.36 Test MSE 49649.86620297437 Test RE 0.38984779582479295\n",
      "17 Train Loss 171699.64 Test MSE 43802.59274544962 Test RE 0.3661726902316606\n",
      "18 Train Loss 164158.19 Test MSE 40219.19589479881 Test RE 0.35087524878700643\n",
      "19 Train Loss 158900.23 Test MSE 37275.25429071781 Test RE 0.3377896583913974\n",
      "20 Train Loss 144250.56 Test MSE 33387.75667181285 Test RE 0.31969044909253236\n",
      "21 Train Loss 137915.88 Test MSE 30597.417382120304 Test RE 0.3060401648601478\n",
      "22 Train Loss 131010.29 Test MSE 29102.04599275218 Test RE 0.29846801908811965\n",
      "23 Train Loss 124087.81 Test MSE 26968.196316550122 Test RE 0.28731744273224336\n",
      "24 Train Loss 117262.266 Test MSE 25375.824705289353 Test RE 0.2787058756910244\n",
      "25 Train Loss 111233.91 Test MSE 22740.140427882572 Test RE 0.2638351245501002\n",
      "26 Train Loss 105476.92 Test MSE 21383.867275439887 Test RE 0.25584631547649805\n",
      "27 Train Loss 99119.55 Test MSE 18346.96706835627 Test RE 0.2369835399943924\n",
      "28 Train Loss 91114.09 Test MSE 16243.393644840586 Test RE 0.22298437206593902\n",
      "29 Train Loss 85853.07 Test MSE 14069.589861881552 Test RE 0.2075280286598578\n",
      "30 Train Loss 80928.61 Test MSE 12646.467392928515 Test RE 0.19675267976180297\n",
      "31 Train Loss 74634.17 Test MSE 10899.683062861757 Test RE 0.1826597995734753\n",
      "32 Train Loss 72839.086 Test MSE 10766.168479546142 Test RE 0.18153761594409962\n",
      "33 Train Loss 70650.89 Test MSE 10117.504120468888 Test RE 0.17598381850638015\n",
      "34 Train Loss 68810.664 Test MSE 9538.200677624081 Test RE 0.1708713574410676\n",
      "35 Train Loss 66947.66 Test MSE 9324.745629611572 Test RE 0.16894857715094608\n",
      "36 Train Loss 65408.688 Test MSE 9434.548790220444 Test RE 0.16994038946334264\n",
      "37 Train Loss 63475.375 Test MSE 8678.201421965141 Test RE 0.16298622643982583\n",
      "38 Train Loss 61393.055 Test MSE 9001.660463430268 Test RE 0.16599589781939758\n",
      "39 Train Loss 60013.934 Test MSE 9077.362258853518 Test RE 0.1666924292208825\n",
      "40 Train Loss 58576.32 Test MSE 8935.920421566625 Test RE 0.16538864461768857\n",
      "41 Train Loss 58135.61 Test MSE 8637.179317946106 Test RE 0.16260054986860922\n",
      "42 Train Loss 57406.824 Test MSE 8254.156392328741 Test RE 0.1589543393847437\n",
      "43 Train Loss 56979.523 Test MSE 8135.08210578206 Test RE 0.15780363833283617\n",
      "44 Train Loss 56230.61 Test MSE 8071.814152584171 Test RE 0.15718880737070873\n",
      "45 Train Loss 55289.414 Test MSE 7980.818468496471 Test RE 0.15630028073896657\n",
      "46 Train Loss 54593.195 Test MSE 7944.938972311816 Test RE 0.1559485441017208\n",
      "47 Train Loss 54237.312 Test MSE 8053.26194083486 Test RE 0.1570080625247163\n",
      "48 Train Loss 53352.848 Test MSE 8027.89217545968 Test RE 0.15676056034795188\n",
      "49 Train Loss 52420.17 Test MSE 7899.1491733912035 Test RE 0.15549849840966135\n",
      "50 Train Loss 52101.715 Test MSE 7898.20135923839 Test RE 0.15548916904420157\n",
      "51 Train Loss 51606.418 Test MSE 7722.885771462617 Test RE 0.1537537962414976\n",
      "52 Train Loss 51319.68 Test MSE 7666.978801914171 Test RE 0.15319626358155902\n",
      "53 Train Loss 50788.406 Test MSE 7655.518066236219 Test RE 0.15308172051651958\n",
      "54 Train Loss 50126.656 Test MSE 7564.3088325098115 Test RE 0.15216706629271468\n",
      "55 Train Loss 49708.074 Test MSE 7532.750817690777 Test RE 0.15184931686905698\n",
      "56 Train Loss 49325.47 Test MSE 7519.992298125701 Test RE 0.15172066575781004\n",
      "57 Train Loss 49150.57 Test MSE 7478.675268394807 Test RE 0.1513032928943441\n",
      "58 Train Loss 48853.695 Test MSE 7455.641258709973 Test RE 0.15107010927313733\n",
      "59 Train Loss 48618.758 Test MSE 7452.886809546257 Test RE 0.15104220064920396\n",
      "60 Train Loss 48287.746 Test MSE 7532.856838304156 Test RE 0.1518503854760943\n",
      "61 Train Loss 47978.484 Test MSE 7476.252647937696 Test RE 0.15127878453363378\n",
      "62 Train Loss 47828.074 Test MSE 7475.211911366922 Test RE 0.1512682547367941\n",
      "63 Train Loss 47604.844 Test MSE 7463.455461437512 Test RE 0.15114925627502526\n",
      "64 Train Loss 47345.24 Test MSE 7547.532659285096 Test RE 0.15199823407782925\n",
      "65 Train Loss 47275.758 Test MSE 7539.197024914399 Test RE 0.15191427606268693\n",
      "66 Train Loss 47198.8 Test MSE 7539.881488735353 Test RE 0.1519211718544802\n",
      "67 Train Loss 47064.98 Test MSE 7603.77284722599 Test RE 0.15256348792604119\n",
      "68 Train Loss 46982.793 Test MSE 7593.381172697226 Test RE 0.15245920205499822\n",
      "69 Train Loss 46941.75 Test MSE 7665.547315842319 Test RE 0.15318196143201718\n",
      "70 Train Loss 46824.55 Test MSE 7632.122716418425 Test RE 0.15284763180037053\n",
      "71 Train Loss 46627.344 Test MSE 7698.787122544364 Test RE 0.15351372058358434\n",
      "72 Train Loss 46444.26 Test MSE 7619.060561542437 Test RE 0.15271677866218264\n",
      "73 Train Loss 46357.875 Test MSE 7565.8851935343355 Test RE 0.15218292086331842\n",
      "74 Train Loss 46327.895 Test MSE 7580.30023710923 Test RE 0.15232782654623858\n",
      "75 Train Loss 46259.83 Test MSE 7553.620912069528 Test RE 0.15205952676568785\n",
      "76 Train Loss 46191.336 Test MSE 7586.159916736779 Test RE 0.1523866909582029\n",
      "77 Train Loss 46094.824 Test MSE 7553.223015626956 Test RE 0.15205552174993017\n",
      "78 Train Loss 45874.44 Test MSE 7408.690910222486 Test RE 0.1505936917741879\n",
      "79 Train Loss 45711.008 Test MSE 7446.671727160271 Test RE 0.15097920925177777\n",
      "80 Train Loss 45604.492 Test MSE 7409.629757568982 Test RE 0.15060323327151665\n",
      "81 Train Loss 45508.492 Test MSE 7343.777459012959 Test RE 0.14993250455715504\n",
      "82 Train Loss 45475.535 Test MSE 7319.846039848145 Test RE 0.14968801011476413\n",
      "83 Train Loss 45393.203 Test MSE 7150.615981408547 Test RE 0.1479475471966893\n",
      "84 Train Loss 45328.406 Test MSE 7080.665933871444 Test RE 0.14722212914502852\n",
      "85 Train Loss 45271.48 Test MSE 7043.93050036529 Test RE 0.14683972855262462\n",
      "86 Train Loss 45242.13 Test MSE 7009.180907257639 Test RE 0.14647708091305717\n",
      "87 Train Loss 45179.684 Test MSE 6964.667232842119 Test RE 0.1460112191883625\n",
      "88 Train Loss 45067.168 Test MSE 6928.628011400361 Test RE 0.14563295591441996\n",
      "89 Train Loss 44968.125 Test MSE 6826.4540571693515 Test RE 0.1445551696419146\n",
      "90 Train Loss 44777.41 Test MSE 6683.688783353794 Test RE 0.1430356031142171\n",
      "91 Train Loss 44640.414 Test MSE 6695.880809161685 Test RE 0.14316600260365528\n",
      "92 Train Loss 44573.33 Test MSE 6705.702381219256 Test RE 0.1432709626532257\n",
      "93 Train Loss 44453.207 Test MSE 6626.050241249225 Test RE 0.14241751516050188\n",
      "94 Train Loss 44381.86 Test MSE 6715.789501986445 Test RE 0.14337868055829991\n",
      "95 Train Loss 44315.7 Test MSE 6687.017910385186 Test RE 0.14307122150000137\n",
      "96 Train Loss 44219.164 Test MSE 6644.9188122296455 Test RE 0.1426201475405159\n",
      "97 Train Loss 44144.273 Test MSE 6634.152803241528 Test RE 0.14250456507315465\n",
      "98 Train Loss 44088.43 Test MSE 6681.613512123228 Test RE 0.1430133952640944\n",
      "99 Train Loss 44016.395 Test MSE 6649.575404499479 Test RE 0.14267011109806554\n",
      "100 Train Loss 43991.12 Test MSE 6617.6887010222545 Test RE 0.14232762709305236\n",
      "101 Train Loss 43960.977 Test MSE 6601.648576891225 Test RE 0.142155033763413\n",
      "102 Train Loss 43934.562 Test MSE 6586.0646725537645 Test RE 0.14198714847320767\n",
      "103 Train Loss 43902.516 Test MSE 6590.8278315460975 Test RE 0.14203848300767447\n",
      "104 Train Loss 43878.406 Test MSE 6611.137054406374 Test RE 0.14225715602177116\n",
      "105 Train Loss 43841.98 Test MSE 6604.671064878582 Test RE 0.14218757205360072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 Train Loss 43776.203 Test MSE 6549.851553733282 Test RE 0.14159625609614854\n",
      "107 Train Loss 43709.22 Test MSE 6522.038817715992 Test RE 0.1412953050862438\n",
      "108 Train Loss 43641.914 Test MSE 6594.234645040525 Test RE 0.14207518826815874\n",
      "109 Train Loss 43588.023 Test MSE 6559.694656157447 Test RE 0.14170261142370957\n",
      "110 Train Loss 43547.85 Test MSE 6539.230263780215 Test RE 0.14148140272329843\n",
      "111 Train Loss 43512.0 Test MSE 6528.159795831091 Test RE 0.14136159284295519\n",
      "112 Train Loss 43487.73 Test MSE 6525.662424307004 Test RE 0.1413345510580166\n",
      "113 Train Loss 43453.707 Test MSE 6530.145235137967 Test RE 0.14138308768491373\n",
      "114 Train Loss 43420.844 Test MSE 6473.385185082327 Test RE 0.1407672954486582\n",
      "115 Train Loss 43371.465 Test MSE 6488.983337646614 Test RE 0.14093678858039757\n",
      "116 Train Loss 43310.02 Test MSE 6481.883502390393 Test RE 0.14085966541763187\n",
      "117 Train Loss 43271.445 Test MSE 6456.300356429543 Test RE 0.1405814132567575\n",
      "118 Train Loss 43233.086 Test MSE 6482.2155789888675 Test RE 0.140863273597218\n",
      "119 Train Loss 43109.887 Test MSE 6498.640552832031 Test RE 0.14104162402248896\n",
      "120 Train Loss 43070.523 Test MSE 6474.49997630295 Test RE 0.14077941579876035\n",
      "121 Train Loss 43031.46 Test MSE 6397.82012829339 Test RE 0.1399432819632131\n",
      "122 Train Loss 42988.695 Test MSE 6437.554888716746 Test RE 0.14037718019670542\n",
      "123 Train Loss 42960.594 Test MSE 6439.060281088862 Test RE 0.14039359251373992\n",
      "124 Train Loss 42927.953 Test MSE 6407.833551832565 Test RE 0.1400527538962737\n",
      "125 Train Loss 42898.99 Test MSE 6433.969264423986 Test RE 0.140338080729289\n",
      "126 Train Loss 42871.586 Test MSE 6402.871380546987 Test RE 0.13999851556874646\n",
      "127 Train Loss 42854.29 Test MSE 6417.114979391642 Test RE 0.14015414691105235\n",
      "128 Train Loss 42836.1 Test MSE 6404.840405637292 Test RE 0.14002004023976503\n",
      "129 Train Loss 42812.92 Test MSE 6441.6731495914855 Test RE 0.14042207437086954\n",
      "130 Train Loss 42728.445 Test MSE 6374.668213015039 Test RE 0.13968984474708485\n",
      "131 Train Loss 42686.65 Test MSE 6417.271966915988 Test RE 0.14015586125761278\n",
      "132 Train Loss 42627.41 Test MSE 6383.325629878939 Test RE 0.13978466873251547\n",
      "133 Train Loss 42601.266 Test MSE 6361.072916292665 Test RE 0.13954080651676684\n",
      "134 Train Loss 42568.11 Test MSE 6394.538052879366 Test RE 0.13990738197566513\n",
      "135 Train Loss 42523.387 Test MSE 6431.849572741348 Test RE 0.14031496141093477\n",
      "136 Train Loss 42450.785 Test MSE 6462.10040531445 Test RE 0.14064454508005134\n",
      "137 Train Loss 42415.4 Test MSE 6397.162318721216 Test RE 0.13993608745047587\n",
      "138 Train Loss 42380.74 Test MSE 6427.240759255767 Test RE 0.14026468027733452\n",
      "139 Train Loss 42364.688 Test MSE 6392.74841764816 Test RE 0.13988780271138399\n",
      "140 Train Loss 42352.297 Test MSE 6419.159771602573 Test RE 0.14017647495742244\n",
      "141 Train Loss 42324.97 Test MSE 6438.762057426752 Test RE 0.14039034132690018\n",
      "142 Train Loss 42290.09 Test MSE 6366.190137592006 Test RE 0.13959692265048804\n",
      "143 Train Loss 42272.523 Test MSE 6391.413361209571 Test RE 0.13987319492056308\n",
      "144 Train Loss 42257.688 Test MSE 6365.801677572938 Test RE 0.13959266353716257\n",
      "145 Train Loss 42254.57 Test MSE 6364.802904832693 Test RE 0.1395817122993409\n",
      "146 Train Loss 42241.363 Test MSE 6373.589403829891 Test RE 0.13967802412686461\n",
      "147 Train Loss 42237.69 Test MSE 6358.665665434076 Test RE 0.13951440047515193\n",
      "148 Train Loss 42226.246 Test MSE 6371.92137290457 Test RE 0.13965974537569106\n",
      "149 Train Loss 42215.45 Test MSE 6377.629563656276 Test RE 0.13972228742221737\n",
      "150 Train Loss 42202.996 Test MSE 6408.592991452367 Test RE 0.14006105299284607\n",
      "151 Train Loss 42153.473 Test MSE 6402.393052429459 Test RE 0.13999328615891304\n",
      "152 Train Loss 42134.77 Test MSE 6392.012420740647 Test RE 0.13987974984061569\n",
      "153 Train Loss 42097.156 Test MSE 6366.399176202997 Test RE 0.13959921451631643\n",
      "154 Train Loss 42072.848 Test MSE 6396.638436326761 Test RE 0.13993035744474305\n",
      "155 Train Loss 42046.035 Test MSE 6382.463645489064 Test RE 0.13977523037096415\n",
      "156 Train Loss 42035.605 Test MSE 6365.699151417968 Test RE 0.1395915394087805\n",
      "157 Train Loss 42031.797 Test MSE 6373.743113405164 Test RE 0.13967970839917643\n",
      "158 Train Loss 42028.914 Test MSE 6368.751388140761 Test RE 0.13962500119766738\n",
      "159 Train Loss 42014.26 Test MSE 6354.901803793183 Test RE 0.13947310323938117\n",
      "160 Train Loss 41976.34 Test MSE 6322.261309609736 Test RE 0.13911445633206512\n",
      "161 Train Loss 41944.402 Test MSE 6293.555553484472 Test RE 0.1387982776185773\n",
      "162 Train Loss 41933.32 Test MSE 6270.620360410661 Test RE 0.1385451400401801\n",
      "163 Train Loss 41925.93 Test MSE 6306.0072220227985 Test RE 0.1389355145222554\n",
      "164 Train Loss 41916.36 Test MSE 6307.791932145085 Test RE 0.13895517371932253\n",
      "165 Train Loss 41893.285 Test MSE 6275.355387329584 Test RE 0.13859743878697514\n",
      "166 Train Loss 41882.094 Test MSE 6259.588014477666 Test RE 0.13842321025442927\n",
      "167 Train Loss 41874.33 Test MSE 6269.634131489285 Test RE 0.13853424457935712\n",
      "168 Train Loss 41864.496 Test MSE 6256.199540089741 Test RE 0.1383857391784542\n",
      "169 Train Loss 41860.168 Test MSE 6252.986953128514 Test RE 0.13835020376225685\n",
      "170 Train Loss 41853.78 Test MSE 6238.904518121822 Test RE 0.13819432578292684\n",
      "171 Train Loss 41852.22 Test MSE 6247.265042716156 Test RE 0.1382868893288021\n",
      "172 Train Loss 41851.47 Test MSE 6250.100296189902 Test RE 0.13831826577158338\n",
      "173 Train Loss 41851.348 Test MSE 6249.374535870138 Test RE 0.13831023479460658\n",
      "174 Train Loss 41849.484 Test MSE 6242.211393546744 Test RE 0.13823094526303428\n",
      "175 Train Loss 41847.207 Test MSE 6246.71952106219 Test RE 0.13828085147551758\n",
      "176 Train Loss 41834.582 Test MSE 6261.413047232073 Test RE 0.13844338797866113\n",
      "177 Train Loss 41814.22 Test MSE 6245.710626033642 Test RE 0.13826968429435868\n",
      "178 Train Loss 41800.367 Test MSE 6265.706466556548 Test RE 0.1384908448110269\n",
      "179 Train Loss 41786.668 Test MSE 6237.648065852132 Test RE 0.1381804096124897\n",
      "180 Train Loss 41776.79 Test MSE 6228.344155651041 Test RE 0.13807731804132922\n",
      "181 Train Loss 41755.41 Test MSE 6209.213437654092 Test RE 0.1378650987353479\n",
      "182 Train Loss 41721.176 Test MSE 6165.582858297761 Test RE 0.13737987317592515\n",
      "183 Train Loss 41692.973 Test MSE 6165.861883439005 Test RE 0.13738298172265312\n",
      "184 Train Loss 41668.082 Test MSE 6186.592839835418 Test RE 0.13761374351300665\n",
      "185 Train Loss 41639.266 Test MSE 6171.0825542805105 Test RE 0.13744113089819568\n",
      "186 Train Loss 41621.3 Test MSE 6183.300206611237 Test RE 0.13757711819268692\n",
      "187 Train Loss 41612.08 Test MSE 6191.201608524296 Test RE 0.1376649923872708\n",
      "188 Train Loss 41609.89 Test MSE 6182.827276211881 Test RE 0.13757185679153888\n",
      "189 Train Loss 41607.2 Test MSE 6177.642986261652 Test RE 0.13751416781428932\n",
      "190 Train Loss 41605.133 Test MSE 6181.272670855452 Test RE 0.13755456022304194\n",
      "191 Train Loss 41597.227 Test MSE 6167.675218649448 Test RE 0.13740318190628328\n",
      "192 Train Loss 41591.78 Test MSE 6166.94750625557 Test RE 0.1373950756957203\n",
      "193 Train Loss 41570.402 Test MSE 6153.413390375644 Test RE 0.1372442277899201\n",
      "194 Train Loss 41522.6 Test MSE 6122.8633291335445 Test RE 0.13690311332913652\n",
      "195 Train Loss 41513.434 Test MSE 6129.906877099786 Test RE 0.13698183519770513\n",
      "196 Train Loss 41506.824 Test MSE 6133.69842700162 Test RE 0.1370241925439606\n",
      "197 Train Loss 41497.836 Test MSE 6140.340813346344 Test RE 0.13709836649927512\n",
      "198 Train Loss 41485.15 Test MSE 6089.325898118574 Test RE 0.13652766127627722\n",
      "199 Train Loss 41479.848 Test MSE 6082.766757726284 Test RE 0.13645411082162928\n",
      "Training time: 648.83\n",
      "Training time: 648.83\n",
      "ES_rowdy\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 488325.2 Test MSE 314742.44238322077 Test RE 0.9815528697723424\n",
      "1 Train Loss 452123.97 Test MSE 278283.7132911679 Test RE 0.9229537457835781\n",
      "2 Train Loss 402085.34 Test MSE 226532.7950734775 Test RE 0.8327249624526493\n",
      "3 Train Loss 350027.62 Test MSE 175265.21249297968 Test RE 0.7324599849380754\n",
      "4 Train Loss 290530.28 Test MSE 114046.02834306512 Test RE 0.5908484827333447\n",
      "5 Train Loss 256685.17 Test MSE 81839.43006558888 Test RE 0.5005152795184216\n",
      "6 Train Loss 249661.94 Test MSE 74904.95118799424 Test RE 0.47884097289246813\n",
      "7 Train Loss 249450.08 Test MSE 74764.70450698608 Test RE 0.4783924890470528\n",
      "8 Train Loss 248516.05 Test MSE 74034.74685942505 Test RE 0.47605139237072064\n",
      "9 Train Loss 244166.67 Test MSE 72335.96269986783 Test RE 0.47055801454069074\n",
      "10 Train Loss 241020.67 Test MSE 70891.23923812422 Test RE 0.46583522557953794\n",
      "11 Train Loss 239069.53 Test MSE 70861.2036845121 Test RE 0.4657365314294114\n",
      "12 Train Loss 237847.5 Test MSE 70930.03818162804 Test RE 0.46596268450461814\n",
      "13 Train Loss 236573.39 Test MSE 70386.59912529156 Test RE 0.46417423761815846\n",
      "14 Train Loss 235570.83 Test MSE 69896.12420210491 Test RE 0.46255415776487774\n",
      "15 Train Loss 231325.88 Test MSE 67495.76997171872 Test RE 0.4545423153693675\n",
      "16 Train Loss 223715.11 Test MSE 64613.02553697676 Test RE 0.44472964582707053\n",
      "17 Train Loss 219257.88 Test MSE 62779.079652948836 Test RE 0.43837271398843386\n",
      "18 Train Loss 214887.45 Test MSE 62197.66229579323 Test RE 0.4363380363492452\n",
      "19 Train Loss 211225.55 Test MSE 61474.92143118376 Test RE 0.43379549050731525\n",
      "20 Train Loss 209324.48 Test MSE 60039.43887620022 Test RE 0.42870085947256165\n",
      "21 Train Loss 207545.7 Test MSE 59935.13812111933 Test RE 0.4283283271810499\n",
      "22 Train Loss 205282.89 Test MSE 59628.22589225693 Test RE 0.4272302407324181\n",
      "23 Train Loss 203124.92 Test MSE 59924.550638508714 Test RE 0.42829049362340954\n",
      "24 Train Loss 198766.05 Test MSE 56829.14421095603 Test RE 0.4170821464205717\n",
      "25 Train Loss 196654.05 Test MSE 56268.03815953469 Test RE 0.41501799553562485\n",
      "26 Train Loss 194022.81 Test MSE 53078.031511546294 Test RE 0.40308203810304327\n",
      "27 Train Loss 188214.08 Test MSE 51141.59260838422 Test RE 0.3956609286723838\n",
      "28 Train Loss 184928.12 Test MSE 49164.82281906314 Test RE 0.3879388562434568\n",
      "29 Train Loss 175129.92 Test MSE 48396.52596560983 Test RE 0.3848957678516783\n",
      "30 Train Loss 168929.81 Test MSE 47732.01074355027 Test RE 0.3822442020671395\n",
      "31 Train Loss 163778.66 Test MSE 45543.59298424832 Test RE 0.37337882669543876\n",
      "32 Train Loss 160086.48 Test MSE 43212.79202147215 Test RE 0.36369908257227573\n",
      "33 Train Loss 155057.12 Test MSE 40725.163274551065 Test RE 0.3530753992840894\n",
      "34 Train Loss 148628.33 Test MSE 38323.325404634204 Test RE 0.34250556734634735\n",
      "35 Train Loss 141071.44 Test MSE 34902.894235103464 Test RE 0.32686375266799017\n",
      "36 Train Loss 137194.0 Test MSE 32581.43195121321 Test RE 0.31580654445393014\n",
      "37 Train Loss 134977.39 Test MSE 31882.80049051151 Test RE 0.3124023359583646\n",
      "38 Train Loss 132139.27 Test MSE 31548.93609159252 Test RE 0.3107623529118009\n",
      "39 Train Loss 130671.05 Test MSE 30392.370796109655 Test RE 0.305012987028755\n",
      "40 Train Loss 129095.195 Test MSE 29403.69998843289 Test RE 0.3000108996546854\n",
      "41 Train Loss 128231.89 Test MSE 28291.68894120441 Test RE 0.2942832062546229\n",
      "42 Train Loss 126255.3 Test MSE 27431.536167178943 Test RE 0.28977512793483473\n",
      "43 Train Loss 123535.89 Test MSE 25703.118016466247 Test RE 0.28049746908776496\n",
      "44 Train Loss 120196.47 Test MSE 24532.91777086708 Test RE 0.27403790797983774\n",
      "45 Train Loss 115544.28 Test MSE 22875.61353298721 Test RE 0.26461984899088964\n",
      "46 Train Loss 112155.72 Test MSE 22453.64364999678 Test RE 0.2621678634616185\n",
      "47 Train Loss 109095.41 Test MSE 20706.24313036219 Test RE 0.2517599805857051\n",
      "48 Train Loss 105543.76 Test MSE 18690.62802195188 Test RE 0.23919273737211058\n",
      "49 Train Loss 104156.76 Test MSE 18165.874690229 Test RE 0.23581107540823423\n",
      "50 Train Loss 102305.16 Test MSE 18686.989751547422 Test RE 0.23916945591226596\n",
      "51 Train Loss 94617.94 Test MSE 19352.115596813514 Test RE 0.2433886190701398\n",
      "52 Train Loss 92013.195 Test MSE 19088.916290106725 Test RE 0.2417278440215281\n",
      "53 Train Loss 89821.75 Test MSE 21055.400463514063 Test RE 0.25387374808112767\n",
      "54 Train Loss 88109.0 Test MSE 21064.984305709244 Test RE 0.2539315196998101\n",
      "55 Train Loss 85884.16 Test MSE 20044.66402361826 Test RE 0.24770537611943166\n",
      "56 Train Loss 83917.664 Test MSE 18990.749582117485 Test RE 0.2411054877540658\n",
      "57 Train Loss 83116.08 Test MSE 17855.780697900813 Test RE 0.23378974836691094\n",
      "58 Train Loss 82138.54 Test MSE 17651.385584177056 Test RE 0.2324478014241206\n",
      "59 Train Loss 80626.02 Test MSE 16932.629402545685 Test RE 0.22766603472351904\n",
      "60 Train Loss 80190.09 Test MSE 16563.3047019533 Test RE 0.22516948654379618\n",
      "61 Train Loss 79564.55 Test MSE 16330.73467740662 Test RE 0.2235830639278891\n",
      "62 Train Loss 79136.0 Test MSE 15567.299814781969 Test RE 0.21829444623249544\n",
      "63 Train Loss 78070.445 Test MSE 14334.374943532808 Test RE 0.20947173115236714\n",
      "64 Train Loss 77672.96 Test MSE 13909.954473240723 Test RE 0.2063473501286203\n",
      "65 Train Loss 77136.61 Test MSE 14150.111938000977 Test RE 0.20812103623316522\n",
      "66 Train Loss 76188.24 Test MSE 13791.867466324255 Test RE 0.20546960186560018\n",
      "67 Train Loss 75342.2 Test MSE 13375.269024766209 Test RE 0.20234259007071756\n",
      "68 Train Loss 74964.62 Test MSE 13634.385277868652 Test RE 0.20429315846373117\n",
      "69 Train Loss 73935.96 Test MSE 14181.150409607179 Test RE 0.20834916942351275\n",
      "70 Train Loss 73547.69 Test MSE 13593.417105011622 Test RE 0.20398600072108272\n",
      "71 Train Loss 73151.91 Test MSE 13165.929333278878 Test RE 0.20075288800998364\n",
      "72 Train Loss 72860.13 Test MSE 13205.277892305847 Test RE 0.20105265581779438\n",
      "73 Train Loss 72271.555 Test MSE 12793.617941019274 Test RE 0.19789404715791498\n",
      "74 Train Loss 70804.2 Test MSE 11450.922475742804 Test RE 0.18722174081887213\n",
      "75 Train Loss 70260.98 Test MSE 10878.868186433836 Test RE 0.18248530559793302\n",
      "76 Train Loss 70119.28 Test MSE 11039.079676387139 Test RE 0.18382411162624462\n",
      "77 Train Loss 68221.08 Test MSE 10238.175296004707 Test RE 0.17703018468603643\n",
      "78 Train Loss 66596.36 Test MSE 10420.596390352785 Test RE 0.17860035978517363\n",
      "79 Train Loss 65695.6 Test MSE 10092.636918387137 Test RE 0.17576741545626476\n",
      "80 Train Loss 65195.996 Test MSE 10154.44763764304 Test RE 0.17630482342131285\n",
      "81 Train Loss 64710.055 Test MSE 9838.302456038296 Test RE 0.17353861487909267\n",
      "82 Train Loss 64430.76 Test MSE 10115.384142935307 Test RE 0.17596538010089047\n",
      "83 Train Loss 63878.95 Test MSE 9246.863697627288 Test RE 0.16824155349409817\n",
      "84 Train Loss 63465.277 Test MSE 9049.474874102152 Test RE 0.16643617681906367\n",
      "85 Train Loss 62735.15 Test MSE 8526.657411383183 Test RE 0.1615568768026844\n",
      "86 Train Loss 62149.363 Test MSE 8443.574651755805 Test RE 0.1607678543388032\n",
      "87 Train Loss 61362.54 Test MSE 8333.619948388521 Test RE 0.15971764103362332\n",
      "88 Train Loss 61015.867 Test MSE 8004.513252712716 Test RE 0.15653213393921733\n",
      "89 Train Loss 60346.812 Test MSE 7873.800095393666 Test RE 0.1552487936050953\n",
      "90 Train Loss 59945.2 Test MSE 8071.949279775387 Test RE 0.1571901230844216\n",
      "91 Train Loss 59374.582 Test MSE 7843.707535468003 Test RE 0.1549518400444294\n",
      "92 Train Loss 59200.004 Test MSE 7830.292978531269 Test RE 0.15481928157266223\n",
      "93 Train Loss 58938.582 Test MSE 7808.122506433779 Test RE 0.15459995072327\n",
      "94 Train Loss 58526.695 Test MSE 7836.467050898876 Test RE 0.15488030592409388\n",
      "95 Train Loss 58112.18 Test MSE 7685.213669291843 Test RE 0.15337833363780196\n",
      "96 Train Loss 56497.277 Test MSE 7683.24965878429 Test RE 0.15335873392853516\n",
      "97 Train Loss 56016.484 Test MSE 7614.650791536314 Test RE 0.15267257745540525\n",
      "98 Train Loss 55716.562 Test MSE 7604.782104797771 Test RE 0.15257361255426535\n",
      "99 Train Loss 55600.844 Test MSE 7688.190880692195 Test RE 0.15340803974141395\n",
      "100 Train Loss 55158.617 Test MSE 7505.888690215489 Test RE 0.1515783243210214\n",
      "101 Train Loss 54629.57 Test MSE 7371.725737701504 Test RE 0.1502175333584351\n",
      "102 Train Loss 54078.395 Test MSE 7499.764029569053 Test RE 0.15151646920328626\n",
      "103 Train Loss 53998.71 Test MSE 7402.389260017579 Test RE 0.1505296325049715\n",
      "104 Train Loss 53913.785 Test MSE 7359.4987304185215 Test RE 0.15009290357834013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 53770.527 Test MSE 7308.03524913061 Test RE 0.1495671983189831\n",
      "106 Train Loss 53611.684 Test MSE 7065.9971015959745 Test RE 0.14706955194503712\n",
      "107 Train Loss 53264.586 Test MSE 6853.537665399222 Test RE 0.14484164341130848\n",
      "108 Train Loss 53208.938 Test MSE 6856.035691037763 Test RE 0.14486803745719648\n",
      "109 Train Loss 53194.21 Test MSE 6874.207399779326 Test RE 0.14505989452076673\n",
      "110 Train Loss 53170.363 Test MSE 6809.735602937595 Test RE 0.14437804836694754\n",
      "111 Train Loss 53076.23 Test MSE 6779.760169466087 Test RE 0.14405993266300185\n",
      "112 Train Loss 52983.445 Test MSE 6781.323884664397 Test RE 0.14407654502927933\n",
      "113 Train Loss 52791.5 Test MSE 6922.272742608949 Test RE 0.14556614983735888\n",
      "114 Train Loss 52393.996 Test MSE 6716.472972438987 Test RE 0.1433859762454657\n",
      "115 Train Loss 52162.152 Test MSE 6707.40845175183 Test RE 0.1432891870545332\n",
      "116 Train Loss 52108.773 Test MSE 6718.954911066696 Test RE 0.14341246651230694\n",
      "117 Train Loss 52059.484 Test MSE 6689.636855001622 Test RE 0.1430992354017508\n",
      "118 Train Loss 51907.723 Test MSE 6516.85764970636 Test RE 0.14123917078647952\n",
      "119 Train Loss 51634.727 Test MSE 6588.551695192082 Test RE 0.14201395445976842\n",
      "120 Train Loss 51439.188 Test MSE 6545.643124096668 Test RE 0.14155075936801875\n",
      "121 Train Loss 51286.0 Test MSE 6514.069103178008 Test RE 0.14120894961535563\n",
      "122 Train Loss 51156.42 Test MSE 6485.431792431542 Test RE 0.1408982145959179\n",
      "123 Train Loss 50992.05 Test MSE 6417.717462306485 Test RE 0.14016072607453617\n",
      "124 Train Loss 50737.03 Test MSE 6324.934857985163 Test RE 0.13914386747932764\n",
      "125 Train Loss 50304.316 Test MSE 6480.804887859425 Test RE 0.14084794508950416\n",
      "126 Train Loss 50008.867 Test MSE 6424.605076923359 Test RE 0.14023591746431552\n",
      "127 Train Loss 49686.152 Test MSE 6420.603140326569 Test RE 0.14019223363728378\n",
      "128 Train Loss 49590.28 Test MSE 6363.487741168175 Test RE 0.13956729062101753\n",
      "129 Train Loss 49580.43 Test MSE 6378.01870032135 Test RE 0.13972654999619347\n",
      "130 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "131 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "132 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "133 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "134 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "135 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "136 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "137 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "138 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "139 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "140 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "141 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "142 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "143 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "144 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "145 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "146 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "147 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "148 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "149 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "150 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "151 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "152 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "153 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "154 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "155 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "156 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "157 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "158 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "159 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "160 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "161 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "162 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "163 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "164 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "165 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "166 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "167 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "168 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "169 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "170 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "171 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "172 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "173 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "174 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "175 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "176 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "177 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "178 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "179 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "180 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "181 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "182 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "183 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "184 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "185 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "186 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "187 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "188 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "189 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "190 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "191 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "192 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "193 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "194 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "195 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "196 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "197 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "198 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "199 Train Loss 49576.99 Test MSE 6397.1499565798595 Test RE 0.13993595224126562\n",
      "Training time: 462.71\n",
      "Training time: 462.71\n",
      "ES_rowdy\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 487600.7 Test MSE 313724.9126585284 Test RE 0.9799649559489938\n",
      "1 Train Loss 475391.2 Test MSE 300651.95311736705 Test RE 0.9593300678670478\n",
      "2 Train Loss 449963.28 Test MSE 276127.21712851746 Test RE 0.9193706809787938\n",
      "3 Train Loss 429179.6 Test MSE 255278.41688865202 Test RE 0.8839813262228708\n",
      "4 Train Loss 413824.34 Test MSE 239477.8958163848 Test RE 0.8561872547495509\n",
      "5 Train Loss 400042.6 Test MSE 226126.97816993608 Test RE 0.8319787452276578\n",
      "6 Train Loss 392988.88 Test MSE 218965.54536115826 Test RE 0.8186983845184265\n",
      "7 Train Loss 381574.12 Test MSE 207227.99637517537 Test RE 0.7964531895502426\n",
      "8 Train Loss 363105.78 Test MSE 188212.9785503951 Test RE 0.7590332932957916\n",
      "9 Train Loss 349254.47 Test MSE 174985.11785854542 Test RE 0.7318744718715596\n",
      "10 Train Loss 340492.22 Test MSE 166021.42345210395 Test RE 0.7128827539837145\n",
      "11 Train Loss 331942.38 Test MSE 157088.857652386 Test RE 0.6934397492669369\n",
      "12 Train Loss 327971.66 Test MSE 153532.70733939696 Test RE 0.6855458338697525\n",
      "13 Train Loss 322331.5 Test MSE 147863.58858091928 Test RE 0.6727700706960583\n",
      "14 Train Loss 317640.03 Test MSE 142934.39425366826 Test RE 0.6614612611868371\n",
      "15 Train Loss 309524.34 Test MSE 134791.55448093155 Test RE 0.6423435724614934\n",
      "16 Train Loss 304654.3 Test MSE 130244.882101003 Test RE 0.631417152437244\n",
      "17 Train Loss 297321.03 Test MSE 122536.28930576096 Test RE 0.6124468484818846\n",
      "18 Train Loss 293299.03 Test MSE 117364.95100476326 Test RE 0.5993841477697368\n",
      "19 Train Loss 290901.16 Test MSE 114945.92485428313 Test RE 0.5931749896058784\n",
      "20 Train Loss 286347.72 Test MSE 110861.84838854361 Test RE 0.5825418081471988\n",
      "21 Train Loss 283544.53 Test MSE 108099.12648457751 Test RE 0.575237425788506\n",
      "22 Train Loss 276927.72 Test MSE 102156.25132447397 Test RE 0.5592017421999724\n",
      "23 Train Loss 274469.75 Test MSE 99804.33646421735 Test RE 0.552727086341889\n",
      "24 Train Loss 270759.7 Test MSE 95972.77451857732 Test RE 0.5420134541208094\n",
      "25 Train Loss 265930.34 Test MSE 91269.49212399521 Test RE 0.5285655558579617\n",
      "26 Train Loss 260261.31 Test MSE 85283.57337006184 Test RE 0.5109386266417661\n",
      "27 Train Loss 258681.3 Test MSE 83586.01214366058 Test RE 0.5058279752433589\n",
      "28 Train Loss 257999.89 Test MSE 83099.01454225223 Test RE 0.5043522685906067\n",
      "29 Train Loss 257052.6 Test MSE 82150.72269733572 Test RE 0.5014662810043623\n",
      "30 Train Loss 256056.31 Test MSE 81229.19193802637 Test RE 0.4986457344352186\n",
      "31 Train Loss 255778.47 Test MSE 80958.76821017828 Test RE 0.4978150105752089\n",
      "32 Train Loss 255475.27 Test MSE 80690.16311960807 Test RE 0.49698849885935187\n",
      "33 Train Loss 254494.36 Test MSE 79460.21251840552 Test RE 0.4931861852471323\n",
      "34 Train Loss 253701.39 Test MSE 78767.99169471853 Test RE 0.4910332806097529\n",
      "35 Train Loss 251841.81 Test MSE 76890.4199136471 Test RE 0.48514566842945694\n",
      "36 Train Loss 250630.17 Test MSE 75657.53087029923 Test RE 0.48124044942554317\n",
      "37 Train Loss 249427.05 Test MSE 74995.62789986172 Test RE 0.4791307173731449\n",
      "38 Train Loss 249129.83 Test MSE 74783.24642943179 Test RE 0.4784518069203021\n",
      "39 Train Loss 248952.62 Test MSE 74636.35290782756 Test RE 0.4779816747660391\n",
      "40 Train Loss 248277.92 Test MSE 74155.36064318616 Test RE 0.4764390143682861\n",
      "41 Train Loss 248187.83 Test MSE 74102.03701554151 Test RE 0.476267684708784\n",
      "42 Train Loss 248106.78 Test MSE 74029.2958038009 Test RE 0.47603386661087355\n",
      "43 Train Loss 247987.64 Test MSE 73980.67777972181 Test RE 0.47587752561872143\n",
      "44 Train Loss 247832.25 Test MSE 73889.69573860086 Test RE 0.47558481656526186\n",
      "45 Train Loss 247579.56 Test MSE 73799.67136060414 Test RE 0.47529501109494426\n",
      "46 Train Loss 246888.8 Test MSE 73439.3552782389 Test RE 0.4741333122779354\n",
      "47 Train Loss 246215.61 Test MSE 73260.70916824885 Test RE 0.4735562807589678\n",
      "48 Train Loss 244620.67 Test MSE 72595.93507284815 Test RE 0.47140283889981777\n",
      "49 Train Loss 244114.4 Test MSE 72277.97185315617 Test RE 0.4703693564514815\n",
      "50 Train Loss 243221.17 Test MSE 72055.93411972377 Test RE 0.4696463140348528\n",
      "51 Train Loss 242622.34 Test MSE 71587.73595693309 Test RE 0.4681180159124172\n",
      "52 Train Loss 241762.33 Test MSE 71255.52122385791 Test RE 0.4670305632672751\n",
      "53 Train Loss 241527.95 Test MSE 71206.37592854632 Test RE 0.46686947881015967\n",
      "54 Train Loss 241421.62 Test MSE 71193.49166091776 Test RE 0.46682723860790587\n",
      "55 Train Loss 241322.94 Test MSE 71070.8588925314 Test RE 0.4664250038705566\n",
      "56 Train Loss 241180.77 Test MSE 71113.66278021103 Test RE 0.4665654397583391\n",
      "57 Train Loss 241083.23 Test MSE 71099.72451606533 Test RE 0.4665197141497163\n",
      "58 Train Loss 240772.81 Test MSE 70903.94988016518 Test RE 0.46587698531939725\n",
      "59 Train Loss 239286.75 Test MSE 70580.53463451761 Test RE 0.4648132651164507\n",
      "60 Train Loss 238050.23 Test MSE 70258.54939408595 Test RE 0.4637518245142989\n",
      "61 Train Loss 237352.47 Test MSE 69882.01502890381 Test RE 0.4625074700108663\n",
      "62 Train Loss 236233.38 Test MSE 69728.75135576619 Test RE 0.46200001109935873\n",
      "63 Train Loss 235135.45 Test MSE 69155.39484448497 Test RE 0.46009665353914897\n",
      "64 Train Loss 233571.88 Test MSE 68133.03413953444 Test RE 0.45668306449776314\n",
      "65 Train Loss 232950.42 Test MSE 67676.11586854095 Test RE 0.45514916937098515\n",
      "66 Train Loss 232545.08 Test MSE 67823.15434031295 Test RE 0.4556433476016733\n",
      "67 Train Loss 232437.61 Test MSE 67773.12830110009 Test RE 0.45547527641020885\n",
      "68 Train Loss 232301.17 Test MSE 67524.62165665711 Test RE 0.45463945412725554\n",
      "69 Train Loss 232196.62 Test MSE 67608.45992061679 Test RE 0.4549216057168978\n",
      "70 Train Loss 230934.02 Test MSE 67770.90163899028 Test RE 0.4554677941093641\n",
      "71 Train Loss 228603.88 Test MSE 66860.45449698764 Test RE 0.4523980289450784\n",
      "72 Train Loss 227766.55 Test MSE 66097.88359426225 Test RE 0.44981073798712334\n",
      "73 Train Loss 227213.25 Test MSE 65812.8698786706 Test RE 0.4488398995626087\n",
      "74 Train Loss 226873.44 Test MSE 65380.096599877616 Test RE 0.44736172135119695\n",
      "75 Train Loss 226203.48 Test MSE 65395.18484539816 Test RE 0.44741333884989276\n",
      "76 Train Loss 225174.5 Test MSE 64668.552006512975 Test RE 0.44492069835733444\n",
      "77 Train Loss 223861.0 Test MSE 64639.38960570588 Test RE 0.444820368147131\n",
      "78 Train Loss 222945.25 Test MSE 64326.408734076686 Test RE 0.44374216180025106\n",
      "79 Train Loss 222611.66 Test MSE 64231.54853080369 Test RE 0.4434148545368569\n",
      "80 Train Loss 222493.8 Test MSE 64110.319347523735 Test RE 0.4429962112861935\n",
      "81 Train Loss 222354.3 Test MSE 63908.565452644405 Test RE 0.44229861190514147\n",
      "82 Train Loss 222172.98 Test MSE 63880.53068083674 Test RE 0.4422015896929018\n",
      "83 Train Loss 221862.42 Test MSE 63882.92113335387 Test RE 0.44220986335627843\n",
      "84 Train Loss 221430.12 Test MSE 63596.48519388186 Test RE 0.44121736725267274\n",
      "85 Train Loss 221223.94 Test MSE 63712.423059593166 Test RE 0.44161935886670683\n",
      "86 Train Loss 221150.36 Test MSE 63680.99541422367 Test RE 0.44151042588396255\n",
      "87 Train Loss 221003.27 Test MSE 63697.02315132858 Test RE 0.44156598380813783\n",
      "88 Train Loss 220849.3 Test MSE 63511.696021915726 Test RE 0.44092314554623674\n",
      "89 Train Loss 220757.98 Test MSE 63281.06388815205 Test RE 0.4401218479996543\n",
      "90 Train Loss 220586.77 Test MSE 62998.79918262394 Test RE 0.4391391708865256\n",
      "91 Train Loss 220424.05 Test MSE 62958.44884632078 Test RE 0.4389985154164104\n",
      "92 Train Loss 220312.17 Test MSE 62897.0068734977 Test RE 0.43878425093427204\n",
      "93 Train Loss 220117.16 Test MSE 62982.86579612828 Test RE 0.43908363477557555\n",
      "94 Train Loss 219756.5 Test MSE 63020.85350357146 Test RE 0.43921603004000914\n",
      "95 Train Loss 218761.75 Test MSE 62371.916150080375 Test RE 0.436948834264128\n",
      "96 Train Loss 216948.88 Test MSE 62461.756199675496 Test RE 0.4372634099714272\n",
      "97 Train Loss 216742.11 Test MSE 62375.48684951425 Test RE 0.43696134142057064\n",
      "98 Train Loss 216712.06 Test MSE 62315.88329218339 Test RE 0.4367525200035561\n",
      "99 Train Loss 216692.12 Test MSE 62226.99526422116 Test RE 0.43644091466240403\n",
      "100 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "101 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "102 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "103 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "104 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "106 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "107 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "108 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "109 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "110 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "111 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "112 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "113 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "114 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "115 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "116 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "117 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "118 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "119 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "120 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "121 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "122 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "123 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "124 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "125 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "126 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "127 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "128 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "129 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "130 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "131 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "132 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "133 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "134 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "135 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "136 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "137 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "138 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "139 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "140 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "141 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "142 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "143 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "144 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "145 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "146 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "147 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "148 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "149 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "150 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "151 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "152 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "153 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "154 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "155 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "156 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "157 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "158 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "159 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "160 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "161 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "162 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "163 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "164 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "165 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "166 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "167 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "168 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "169 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "170 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "171 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "172 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "173 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "174 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "175 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "176 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "177 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "178 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "179 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "180 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "181 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "182 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "183 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "184 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "185 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "186 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "187 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "188 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "189 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "190 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "191 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "192 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "193 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "194 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "195 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "196 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "197 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "198 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "199 Train Loss 216675.69 Test MSE 62232.430223815005 Test RE 0.4364599738097604\n",
      "Training time: 392.96\n",
      "Training time: 392.96\n",
      "ES_rowdy\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "1 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "2 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "3 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "4 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "5 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "6 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "7 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "8 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "9 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "10 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "11 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "12 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "13 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "14 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "15 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "16 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "17 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "18 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "19 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "20 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "21 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "22 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "23 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "24 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "25 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "26 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "27 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "28 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "29 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "30 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "31 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "32 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "33 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "34 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "35 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "36 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "37 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "38 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "39 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "40 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "41 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "42 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "43 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "44 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "45 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "46 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "47 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "48 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "49 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "50 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "51 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "52 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "53 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "54 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "55 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "56 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "57 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "58 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "59 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "60 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "61 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "62 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "63 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "64 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "65 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "66 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "67 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "68 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "69 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "70 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "71 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "72 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "73 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "74 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "75 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "76 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "77 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "78 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "79 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "80 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "81 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "82 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "83 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "84 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "85 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "86 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "87 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "88 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "89 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "90 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "91 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "92 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "93 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "94 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "95 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "96 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "97 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "98 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "99 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "100 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "101 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "102 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "103 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "105 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "106 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "107 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "108 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "109 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "110 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "111 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "112 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "113 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "114 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "115 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "116 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "117 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "118 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "119 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "120 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "121 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "122 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "123 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "124 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "125 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "126 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "127 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "128 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "129 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "130 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "131 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "132 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "133 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "134 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "135 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "136 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "137 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "138 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "139 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "140 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "141 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "142 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "143 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "144 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "145 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "146 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "147 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "148 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "149 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "150 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "151 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "152 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "153 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "154 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "155 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "156 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "157 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "158 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "159 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "160 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "161 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "162 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "163 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "164 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "165 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "166 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "167 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "168 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "169 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "170 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "171 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "172 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "173 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "174 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "175 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "176 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "177 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "178 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "179 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "180 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "181 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "182 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "183 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "184 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "185 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "186 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "187 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "188 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "189 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "190 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "191 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "192 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "193 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "194 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "195 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "196 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "197 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "198 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "199 Train Loss 499576.22 Test MSE 325793.11751090613 Test RE 0.9986354863339163\n",
      "Training time: 112.92\n",
      "Training time: 112.92\n",
      "ES_rowdy\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 487012.7 Test MSE 312962.6688413985 Test RE 0.9787737427681799\n",
      "1 Train Loss 444202.56 Test MSE 267468.5536256148 Test RE 0.9048412821097866\n",
      "2 Train Loss 397432.84 Test MSE 223423.7870883002 Test RE 0.8269909297079684\n",
      "3 Train Loss 319764.28 Test MSE 140445.8498310731 Test RE 0.6556778263955924\n",
      "4 Train Loss 262298.56 Test MSE 86867.2576829518 Test RE 0.5156607756963276\n",
      "5 Train Loss 250087.67 Test MSE 75102.80671057783 Test RE 0.4794729661651219\n",
      "6 Train Loss 249657.5 Test MSE 74807.89411505147 Test RE 0.47853064649171473\n",
      "7 Train Loss 248985.64 Test MSE 73940.97098057435 Test RE 0.4757498023403106\n",
      "8 Train Loss 246759.8 Test MSE 73220.2935924359 Test RE 0.47342563985805486\n",
      "9 Train Loss 240660.64 Test MSE 70630.94663885118 Test RE 0.4649792314544468\n",
      "10 Train Loss 234888.31 Test MSE 69783.60960845258 Test RE 0.4621817118321614\n",
      "11 Train Loss 227899.44 Test MSE 66489.16412584674 Test RE 0.45114014879882364\n",
      "12 Train Loss 218035.61 Test MSE 63609.35000875397 Test RE 0.4412619915169212\n",
      "13 Train Loss 204033.86 Test MSE 57226.82422283309 Test RE 0.41853893476775533\n",
      "14 Train Loss 200163.4 Test MSE 56025.27985677564 Test RE 0.41412176769056147\n",
      "15 Train Loss 178758.47 Test MSE 48660.18260436165 Test RE 0.38594276952588996\n",
      "16 Train Loss 173937.1 Test MSE 46505.20819584767 Test RE 0.377300028783584\n",
      "17 Train Loss 165064.61 Test MSE 40833.82754240681 Test RE 0.35354612939226765\n",
      "18 Train Loss 155785.67 Test MSE 37243.121636229545 Test RE 0.33764403338155\n",
      "19 Train Loss 143397.52 Test MSE 34459.29130530639 Test RE 0.32477995145822514\n",
      "20 Train Loss 137357.28 Test MSE 31782.127186897666 Test RE 0.31190872433807537\n",
      "21 Train Loss 129122.88 Test MSE 27680.807425655883 Test RE 0.2910887484466082\n",
      "22 Train Loss 127783.766 Test MSE 27012.433689346035 Test RE 0.2875529972471655\n",
      "23 Train Loss 124081.11 Test MSE 27484.144659661903 Test RE 0.29005286175308365\n",
      "24 Train Loss 117546.43 Test MSE 25025.308958629725 Test RE 0.2767743030284645\n",
      "25 Train Loss 114516.71 Test MSE 24894.98169770972 Test RE 0.2760526671304817\n",
      "26 Train Loss 108894.23 Test MSE 22417.87049280704 Test RE 0.26195893722227\n",
      "27 Train Loss 106522.3 Test MSE 21323.357263546197 Test RE 0.2554840743822318\n",
      "28 Train Loss 103815.53 Test MSE 19973.742679142284 Test RE 0.24726677647247275\n",
      "29 Train Loss 96842.98 Test MSE 17222.443680771707 Test RE 0.2296061034778979\n",
      "30 Train Loss 93874.03 Test MSE 16582.753794985656 Test RE 0.22530164789500207\n",
      "31 Train Loss 92183.03 Test MSE 16588.822651757568 Test RE 0.2253428713965427\n",
      "32 Train Loss 90959.3 Test MSE 16781.885478705663 Test RE 0.2266503627059917\n",
      "33 Train Loss 84399.32 Test MSE 13771.056184115618 Test RE 0.20531452133710382\n",
      "34 Train Loss 82023.74 Test MSE 13159.02192158616 Test RE 0.20070021930969886\n",
      "35 Train Loss 80150.09 Test MSE 12627.270922354495 Test RE 0.1966032945109924\n",
      "36 Train Loss 78830.29 Test MSE 11976.268410372926 Test RE 0.19146826513991266\n",
      "37 Train Loss 76774.12 Test MSE 11775.757741122366 Test RE 0.18985868698940156\n",
      "38 Train Loss 74626.31 Test MSE 11851.86581881389 Test RE 0.19047123810990974\n",
      "39 Train Loss 73324.24 Test MSE 11036.364981699626 Test RE 0.1838015075216722\n",
      "40 Train Loss 71802.2 Test MSE 10696.646919699406 Test RE 0.18095053528079086\n",
      "41 Train Loss 69962.84 Test MSE 10779.96439623502 Test RE 0.18165389110855376\n",
      "42 Train Loss 68920.04 Test MSE 10036.727490524738 Test RE 0.17527989653067183\n",
      "43 Train Loss 68493.32 Test MSE 10112.18216365418 Test RE 0.17593752737237442\n",
      "44 Train Loss 67734.414 Test MSE 9711.815788367267 Test RE 0.1724194518029187\n",
      "45 Train Loss 64756.05 Test MSE 9337.144855791727 Test RE 0.16906086631067288\n",
      "46 Train Loss 64106.93 Test MSE 9261.490987117597 Test RE 0.1683745686193742\n",
      "47 Train Loss 63254.133 Test MSE 8870.537925375125 Test RE 0.16478247453369477\n",
      "48 Train Loss 61318.297 Test MSE 8464.415170888986 Test RE 0.16096613651217695\n",
      "49 Train Loss 58812.777 Test MSE 8130.003072777972 Test RE 0.15775436931404474\n",
      "50 Train Loss 57746.867 Test MSE 8031.200291601731 Test RE 0.15679285579461136\n",
      "51 Train Loss 57428.836 Test MSE 8094.379426225638 Test RE 0.15740836971886027\n",
      "52 Train Loss 57180.934 Test MSE 7938.467308028152 Test RE 0.1558850160976476\n",
      "53 Train Loss 56759.137 Test MSE 7905.7903378282335 Test RE 0.15556385191222857\n",
      "54 Train Loss 55377.33 Test MSE 7448.38137803669 Test RE 0.15099653960641796\n",
      "55 Train Loss 54574.273 Test MSE 7319.992318950539 Test RE 0.1496895057827227\n",
      "56 Train Loss 54301.152 Test MSE 7245.400636280958 Test RE 0.1489248750156236\n",
      "57 Train Loss 54049.566 Test MSE 7154.619643802756 Test RE 0.14798895965286235\n",
      "58 Train Loss 53962.047 Test MSE 7080.980705743618 Test RE 0.14722540149756097\n",
      "59 Train Loss 53733.016 Test MSE 7100.593301189993 Test RE 0.1474291498005138\n",
      "60 Train Loss 53530.383 Test MSE 6995.829427270265 Test RE 0.14633750554374308\n",
      "61 Train Loss 53301.26 Test MSE 6963.138178654273 Test RE 0.14599519033056668\n",
      "62 Train Loss 52612.65 Test MSE 7038.457411756936 Test RE 0.14678267070564402\n",
      "63 Train Loss 51743.074 Test MSE 6751.9428110813615 Test RE 0.14376408991290324\n",
      "64 Train Loss 51393.703 Test MSE 6752.917884787269 Test RE 0.1437744702972765\n",
      "65 Train Loss 51261.816 Test MSE 6793.73771270459 Test RE 0.14420835732076878\n",
      "66 Train Loss 51079.91 Test MSE 6816.98397697847 Test RE 0.1444548668976646\n",
      "67 Train Loss 50860.87 Test MSE 6932.571281664013 Test RE 0.14567439185372763\n",
      "68 Train Loss 50465.25 Test MSE 6749.549858397155 Test RE 0.1437386119755949\n",
      "69 Train Loss 49641.52 Test MSE 6520.430017664708 Test RE 0.1412778772603575\n",
      "70 Train Loss 48842.85 Test MSE 6467.564506142935 Test RE 0.14070399429494815\n",
      "71 Train Loss 48206.848 Test MSE 6417.263535440912 Test RE 0.14015576918413938\n",
      "72 Train Loss 47628.8 Test MSE 6267.788196465492 Test RE 0.13851384912769077\n",
      "73 Train Loss 47338.25 Test MSE 6276.058184798747 Test RE 0.1386051995587682\n",
      "74 Train Loss 47210.496 Test MSE 6258.999806340215 Test RE 0.13841670634623543\n",
      "75 Train Loss 47057.156 Test MSE 6245.03611445484 Test RE 0.13826221780841166\n",
      "76 Train Loss 47000.08 Test MSE 6200.806798388203 Test RE 0.13777173951073424\n",
      "77 Train Loss 46742.934 Test MSE 6217.57483291012 Test RE 0.13795789283725318\n",
      "78 Train Loss 46493.332 Test MSE 6192.028651708161 Test RE 0.13767418697491118\n",
      "79 Train Loss 46395.414 Test MSE 6183.125238707703 Test RE 0.13757517167954425\n",
      "80 Train Loss 46059.484 Test MSE 6151.0533578922195 Test RE 0.13721790647216056\n",
      "81 Train Loss 45850.0 Test MSE 6192.964938231175 Test RE 0.13768459532576566\n",
      "82 Train Loss 45748.516 Test MSE 6190.77461205472 Test RE 0.13766024504717297\n",
      "83 Train Loss 45646.31 Test MSE 6196.886775796517 Test RE 0.1377281843951699\n",
      "84 Train Loss 45396.594 Test MSE 6168.361601598017 Test RE 0.13741082729729664\n",
      "85 Train Loss 45251.363 Test MSE 6189.603736593545 Test RE 0.137647226431979\n",
      "86 Train Loss 45013.06 Test MSE 6147.010984382006 Test RE 0.13717281035929849\n",
      "87 Train Loss 44981.645 Test MSE 6144.884905663709 Test RE 0.13714908619356345\n",
      "88 Train Loss 44956.887 Test MSE 6132.0457269914305 Test RE 0.1370057309955296\n",
      "89 Train Loss 44935.688 Test MSE 6146.097150643975 Test RE 0.13716261371237862\n",
      "90 Train Loss 44920.21 Test MSE 6144.681485411316 Test RE 0.1371468160832968\n",
      "91 Train Loss 44868.66 Test MSE 6172.440173560926 Test RE 0.13745624838092738\n",
      "92 Train Loss 44824.91 Test MSE 6172.196383432105 Test RE 0.13745353382991912\n",
      "93 Train Loss 44733.363 Test MSE 6166.558924021076 Test RE 0.13739074696367612\n",
      "94 Train Loss 44632.06 Test MSE 6183.258337154978 Test RE 0.13757665239863753\n",
      "95 Train Loss 44583.11 Test MSE 6191.160663666339 Test RE 0.13766453717007437\n",
      "96 Train Loss 44551.566 Test MSE 6194.826494012148 Test RE 0.1377052872155817\n",
      "97 Train Loss 44525.31 Test MSE 6191.863068087226 Test RE 0.137672346161175\n",
      "98 Train Loss 44484.83 Test MSE 6198.066991463721 Test RE 0.137741299143605\n",
      "99 Train Loss 44425.258 Test MSE 6205.726277770443 Test RE 0.13782638005019585\n",
      "100 Train Loss 44355.664 Test MSE 6200.927031889899 Test RE 0.1377730751996952\n",
      "101 Train Loss 44300.59 Test MSE 6214.09846498696 Test RE 0.1379193199641941\n",
      "102 Train Loss 44025.34 Test MSE 6237.44390522537 Test RE 0.13817814824493752\n",
      "103 Train Loss 43618.547 Test MSE 6234.64307927916 Test RE 0.13814712140093235\n",
      "104 Train Loss 43453.65 Test MSE 6204.321431101353 Test RE 0.13781077866192484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 43210.094 Test MSE 6320.938115031616 Test RE 0.13909989784438948\n",
      "106 Train Loss 43107.836 Test MSE 6366.792425514433 Test RE 0.13960352593677153\n",
      "107 Train Loss 43025.85 Test MSE 6361.098881356493 Test RE 0.1395410913100948\n",
      "108 Train Loss 43006.75 Test MSE 6360.649904791766 Test RE 0.1395361667061452\n",
      "109 Train Loss 43003.2 Test MSE 6374.8113360186735 Test RE 0.13969141288500758\n",
      "110 Train Loss 42989.926 Test MSE 6363.758216214027 Test RE 0.13957025668850653\n",
      "111 Train Loss 42986.164 Test MSE 6365.730855878765 Test RE 0.13959188702723316\n",
      "112 Train Loss 42983.457 Test MSE 6367.998988261253 Test RE 0.13961675335357632\n",
      "113 Train Loss 42980.04 Test MSE 6368.4111124449355 Test RE 0.1396212711392796\n",
      "114 Train Loss 42976.6 Test MSE 6364.7014530026 Test RE 0.13958059986296223\n",
      "115 Train Loss 42960.7 Test MSE 6371.167857831058 Test RE 0.13965148736146699\n",
      "116 Train Loss 42959.0 Test MSE 6374.762406373125 Test RE 0.13969087678565703\n",
      "117 Train Loss 42958.566 Test MSE 6375.958201971496 Test RE 0.1397039779721909\n",
      "118 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "119 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "120 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "121 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "122 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "123 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "124 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "125 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "126 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "127 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "128 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "129 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "130 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "131 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "132 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "133 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "134 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "135 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "136 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "137 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "138 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "139 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "140 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "141 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "142 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "143 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "144 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "145 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "146 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "147 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "148 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "149 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "150 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "151 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "152 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "153 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "154 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "155 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "156 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "157 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "158 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "159 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "160 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "161 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "162 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "163 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "164 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "165 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "166 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "167 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "168 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "169 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "170 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "171 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "172 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "173 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "174 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "175 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "176 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "177 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "178 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "179 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "180 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "181 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "182 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "183 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "184 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "185 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "186 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "187 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "188 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "189 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "190 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "191 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "192 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "193 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "194 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "195 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "196 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "197 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "198 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "199 Train Loss 42958.438 Test MSE 6376.061333645992 Test RE 0.13970510782957576\n",
      "Training time: 429.41\n",
      "Training time: 429.41\n",
      "ES_rowdy\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 496205.12 Test MSE 321646.15263491723 Test RE 0.9922594011682532\n",
      "1 Train Loss 485962.56 Test MSE 312099.59946849756 Test RE 0.9774232097354066\n",
      "2 Train Loss 469119.25 Test MSE 294774.235369287 Test RE 0.9499063756097199\n",
      "3 Train Loss 441450.38 Test MSE 266872.0053894196 Test RE 0.9038316628948541\n",
      "4 Train Loss 404196.38 Test MSE 230221.03165640365 Test RE 0.8394764933225695\n",
      "5 Train Loss 362641.5 Test MSE 187715.04226500512 Test RE 0.75802857903812\n",
      "6 Train Loss 269059.9 Test MSE 94415.1956552937 Test RE 0.5375971906946615\n",
      "7 Train Loss 250276.62 Test MSE 75250.13175200886 Test RE 0.47994301362260683\n",
      "8 Train Loss 249555.72 Test MSE 74782.3427098742 Test RE 0.4784489159816075\n",
      "9 Train Loss 248492.73 Test MSE 74308.00808563904 Test RE 0.4769291327357982\n",
      "10 Train Loss 246294.34 Test MSE 73124.57421854032 Test RE 0.47311608888887174\n",
      "11 Train Loss 244218.83 Test MSE 72036.17839750052 Test RE 0.46958192767806145\n",
      "12 Train Loss 239926.97 Test MSE 69982.1926636763 Test RE 0.4628388593635834\n",
      "13 Train Loss 233316.8 Test MSE 67964.20154874005 Test RE 0.45611688679707213\n",
      "14 Train Loss 228444.27 Test MSE 65886.16508027552 Test RE 0.4490897644946897\n",
      "15 Train Loss 224543.23 Test MSE 63378.591337932965 Test RE 0.4404608707115563\n",
      "16 Train Loss 218095.84 Test MSE 61190.29633972739 Test RE 0.43279010226412434\n",
      "17 Train Loss 212683.73 Test MSE 58120.28953350431 Test RE 0.4217935423408928\n",
      "18 Train Loss 206844.33 Test MSE 57068.47116353021 Test RE 0.417959461468943\n",
      "19 Train Loss 198389.61 Test MSE 53708.659320080165 Test RE 0.4054695057261296\n",
      "20 Train Loss 193120.88 Test MSE 51846.22657402472 Test RE 0.39837733183950436\n",
      "21 Train Loss 188143.44 Test MSE 49932.40560958835 Test RE 0.3909554635328023\n",
      "22 Train Loss 182717.92 Test MSE 48428.832633657 Test RE 0.3850242132832548\n",
      "23 Train Loss 177530.6 Test MSE 46246.11860278475 Test RE 0.3762475547954546\n",
      "24 Train Loss 174501.16 Test MSE 45223.491370942094 Test RE 0.3720643727037581\n",
      "25 Train Loss 171744.38 Test MSE 44039.8311820769 Test RE 0.3671629618628923\n",
      "26 Train Loss 168030.39 Test MSE 42902.23143826689 Test RE 0.36238981436669365\n",
      "27 Train Loss 164352.61 Test MSE 41391.56520875813 Test RE 0.3559524336454297\n",
      "28 Train Loss 162170.89 Test MSE 40485.85426504837 Test RE 0.35203650085729776\n",
      "29 Train Loss 159351.38 Test MSE 38886.93174657035 Test RE 0.34501492351949575\n",
      "30 Train Loss 154946.06 Test MSE 37929.718235118395 Test RE 0.34074214272859554\n",
      "31 Train Loss 152584.19 Test MSE 36923.167995514224 Test RE 0.33619056417807225\n",
      "32 Train Loss 148870.83 Test MSE 35459.55644635018 Test RE 0.32945999785045066\n",
      "33 Train Loss 145664.69 Test MSE 35182.113489787844 Test RE 0.3281685849909856\n",
      "34 Train Loss 143841.95 Test MSE 33942.59100885023 Test RE 0.3223357959964514\n",
      "35 Train Loss 142394.84 Test MSE 33606.52923142486 Test RE 0.32073612135915175\n",
      "36 Train Loss 139760.92 Test MSE 32162.965678703244 Test RE 0.3137719274247761\n",
      "37 Train Loss 133021.84 Test MSE 30430.659970963352 Test RE 0.30520505858016694\n",
      "38 Train Loss 127609.195 Test MSE 28191.046504462483 Test RE 0.2937593109585856\n",
      "39 Train Loss 125424.87 Test MSE 27244.86208198411 Test RE 0.28878747181418984\n",
      "40 Train Loss 122264.88 Test MSE 26809.29400378558 Test RE 0.2864697246033892\n",
      "41 Train Loss 120018.984 Test MSE 25653.756963887234 Test RE 0.28022800170049983\n",
      "42 Train Loss 116745.77 Test MSE 25589.68832851961 Test RE 0.2798778570931895\n",
      "43 Train Loss 111952.27 Test MSE 21834.892485439294 Test RE 0.25853037179325283\n",
      "44 Train Loss 104651.97 Test MSE 19613.22586262202 Test RE 0.24502508958748864\n",
      "45 Train Loss 102138.56 Test MSE 19420.413999104974 Test RE 0.24381773011080465\n",
      "46 Train Loss 100178.99 Test MSE 19495.79386968421 Test RE 0.2442904581666372\n",
      "47 Train Loss 99000.88 Test MSE 18618.156070091696 Test RE 0.2387285581896404\n",
      "48 Train Loss 97442.484 Test MSE 18380.12248386995 Test RE 0.23719757376687894\n",
      "49 Train Loss 96707.75 Test MSE 18576.39304514735 Test RE 0.23846065775954797\n",
      "50 Train Loss 95648.53 Test MSE 18213.634584593787 Test RE 0.23612085733515142\n",
      "51 Train Loss 93527.13 Test MSE 17836.258256159977 Test RE 0.2336619075560552\n",
      "52 Train Loss 91249.445 Test MSE 17138.778648020463 Test RE 0.22904772194197628\n",
      "53 Train Loss 89923.586 Test MSE 16650.165472494653 Test RE 0.225759127940484\n",
      "54 Train Loss 89350.24 Test MSE 16920.380917822567 Test RE 0.22758367691740436\n",
      "55 Train Loss 88650.44 Test MSE 17019.338430246837 Test RE 0.22824820930422443\n",
      "56 Train Loss 87870.6 Test MSE 17118.580374816418 Test RE 0.22891271429580148\n",
      "57 Train Loss 86689.73 Test MSE 16400.907839225303 Test RE 0.22406291717614957\n",
      "58 Train Loss 86172.3 Test MSE 16174.991627555453 Test RE 0.22251437569342977\n",
      "59 Train Loss 85586.31 Test MSE 16166.008013182563 Test RE 0.22245257470377938\n",
      "60 Train Loss 85113.4 Test MSE 16191.24756726602 Test RE 0.22262616159091395\n",
      "61 Train Loss 84474.4 Test MSE 16280.060289688417 Test RE 0.22323590447752945\n",
      "62 Train Loss 83856.484 Test MSE 15368.72349317271 Test RE 0.2168976968136944\n",
      "63 Train Loss 83372.14 Test MSE 15169.958796465084 Test RE 0.21549055636481992\n",
      "64 Train Loss 82984.68 Test MSE 14719.014438104605 Test RE 0.21226354227143512\n",
      "65 Train Loss 82330.65 Test MSE 14903.332869944252 Test RE 0.2135884394581516\n",
      "66 Train Loss 81347.66 Test MSE 14221.307509168088 Test RE 0.20864395452293727\n",
      "67 Train Loss 80072.03 Test MSE 13777.47300150214 Test RE 0.2053623503610881\n",
      "68 Train Loss 78784.4 Test MSE 13327.2963430223 Test RE 0.20197939602278916\n",
      "69 Train Loss 77847.14 Test MSE 13445.890292933678 Test RE 0.20287607009160138\n",
      "70 Train Loss 76997.83 Test MSE 13377.405942688498 Test RE 0.20235875319377028\n",
      "71 Train Loss 76483.37 Test MSE 13130.486946218938 Test RE 0.20048249477377464\n",
      "72 Train Loss 75810.12 Test MSE 12764.39380425618 Test RE 0.19766789576139945\n",
      "73 Train Loss 74640.484 Test MSE 12181.230030590892 Test RE 0.1930997066843372\n",
      "74 Train Loss 73236.35 Test MSE 11008.794846400624 Test RE 0.18357178509230498\n",
      "75 Train Loss 72221.68 Test MSE 10902.379584951757 Test RE 0.1826823926948589\n",
      "76 Train Loss 71512.3 Test MSE 10187.160629588863 Test RE 0.17658858187437024\n",
      "77 Train Loss 71147.41 Test MSE 9906.916393979927 Test RE 0.17414270685780592\n",
      "78 Train Loss 69950.52 Test MSE 9680.166750909973 Test RE 0.17213828076164506\n",
      "79 Train Loss 68970.195 Test MSE 9198.04958463821 Test RE 0.16779689302949813\n",
      "80 Train Loss 68574.266 Test MSE 9171.811503709074 Test RE 0.16755739591466742\n",
      "81 Train Loss 67346.87 Test MSE 8814.389965730688 Test RE 0.16426013376523554\n",
      "82 Train Loss 66065.99 Test MSE 8469.570037762414 Test RE 0.16101514360214206\n",
      "83 Train Loss 65652.9 Test MSE 8221.671445398779 Test RE 0.15864124174477343\n",
      "84 Train Loss 64589.4 Test MSE 7915.75268336304 Test RE 0.15566183660789074\n",
      "85 Train Loss 63134.113 Test MSE 7679.944141542682 Test RE 0.15332574108717656\n",
      "86 Train Loss 61956.418 Test MSE 7376.0809820297845 Test RE 0.15026190135861367\n",
      "87 Train Loss 60288.953 Test MSE 7267.516702199426 Test RE 0.14915199308351848\n",
      "88 Train Loss 59426.29 Test MSE 7335.97843021663 Test RE 0.14985286989052096\n",
      "89 Train Loss 58675.08 Test MSE 7180.1478129006655 Test RE 0.14825274189662355\n",
      "90 Train Loss 58151.336 Test MSE 7293.932255357956 Test RE 0.14942281177544012\n",
      "91 Train Loss 57057.383 Test MSE 7047.351297659994 Test RE 0.14687537966928038\n",
      "92 Train Loss 56597.953 Test MSE 7055.364756539432 Test RE 0.1469588610634493\n",
      "93 Train Loss 55620.285 Test MSE 7163.277309069283 Test RE 0.14807847185430673\n",
      "94 Train Loss 55045.062 Test MSE 7089.7838132859115 Test RE 0.1473168887177173\n",
      "95 Train Loss 54354.477 Test MSE 7006.840249302515 Test RE 0.146452621466425\n",
      "96 Train Loss 54217.156 Test MSE 6968.32370763481 Test RE 0.14604954236087758\n",
      "97 Train Loss 54138.824 Test MSE 7065.706929906039 Test RE 0.1470665321406169\n",
      "98 Train Loss 54074.82 Test MSE 7083.29960335483 Test RE 0.14724950639912168\n",
      "99 Train Loss 53813.08 Test MSE 6959.253753356089 Test RE 0.14595446253591046\n",
      "100 Train Loss 53567.164 Test MSE 6883.201753289341 Test RE 0.14515476316481524\n",
      "101 Train Loss 53311.727 Test MSE 6868.982021606807 Test RE 0.1450047509338131\n",
      "102 Train Loss 53150.73 Test MSE 6871.929824639807 Test RE 0.1450358617733715\n",
      "103 Train Loss 52880.062 Test MSE 6821.418459023872 Test RE 0.14450184356462842\n",
      "104 Train Loss 52678.406 Test MSE 6889.638928079121 Test RE 0.1452226217179929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 52462.418 Test MSE 6800.712698377429 Test RE 0.14428236615284934\n",
      "106 Train Loss 52253.24 Test MSE 6774.644663569516 Test RE 0.14400557391880972\n",
      "107 Train Loss 52112.625 Test MSE 6716.634407480531 Test RE 0.14338769942532786\n",
      "108 Train Loss 52015.926 Test MSE 6714.217620630126 Test RE 0.14336190014372768\n",
      "109 Train Loss 51819.28 Test MSE 6723.6506680241455 Test RE 0.14346257195841952\n",
      "110 Train Loss 51680.26 Test MSE 6705.78500742478 Test RE 0.14327184532731915\n",
      "111 Train Loss 51510.008 Test MSE 6779.363254226914 Test RE 0.14405571566873468\n",
      "112 Train Loss 51387.277 Test MSE 6709.732623339237 Test RE 0.143314010338775\n",
      "113 Train Loss 51115.137 Test MSE 6749.898794214346 Test RE 0.14374232740128035\n",
      "114 Train Loss 51006.043 Test MSE 6750.46229642789 Test RE 0.1437483273008119\n",
      "115 Train Loss 50760.684 Test MSE 6641.320977290236 Test RE 0.14258153207809854\n",
      "116 Train Loss 50598.13 Test MSE 6672.279454753015 Test RE 0.14291346717307726\n",
      "117 Train Loss 50511.484 Test MSE 6629.580868605068 Test RE 0.14245545301136198\n",
      "118 Train Loss 50464.832 Test MSE 6633.75497526645 Test RE 0.14250029224780816\n",
      "119 Train Loss 50210.99 Test MSE 6593.352762415436 Test RE 0.14206568770925668\n",
      "120 Train Loss 49844.96 Test MSE 6544.322707171777 Test RE 0.141536481516195\n",
      "121 Train Loss 49575.9 Test MSE 6588.202883066717 Test RE 0.1420101951476135\n",
      "122 Train Loss 49239.773 Test MSE 6483.522481848182 Test RE 0.1408774728578515\n",
      "123 Train Loss 48980.72 Test MSE 6486.899656420041 Test RE 0.14091415861635503\n",
      "124 Train Loss 48901.855 Test MSE 6488.043417851946 Test RE 0.14092658096636673\n",
      "125 Train Loss 48776.234 Test MSE 6437.009089919247 Test RE 0.14037122923228487\n",
      "126 Train Loss 48632.8 Test MSE 6451.954920633398 Test RE 0.1405340958848639\n",
      "127 Train Loss 48451.46 Test MSE 6438.671970850765 Test RE 0.14038935920259762\n",
      "128 Train Loss 48230.305 Test MSE 6481.599313525934 Test RE 0.14085657748901279\n",
      "129 Train Loss 48016.86 Test MSE 6424.923489284605 Test RE 0.1402393925647945\n",
      "130 Train Loss 47912.656 Test MSE 6376.200792357957 Test RE 0.13970663565269725\n",
      "131 Train Loss 47758.055 Test MSE 6334.769180402774 Test RE 0.1392519993566184\n",
      "132 Train Loss 47601.59 Test MSE 6382.535105528126 Test RE 0.1397760128521516\n",
      "133 Train Loss 47335.273 Test MSE 6361.673159176932 Test RE 0.13954739002881522\n",
      "134 Train Loss 47183.023 Test MSE 6393.626231181868 Test RE 0.13989740665504855\n",
      "135 Train Loss 47064.043 Test MSE 6389.805730534207 Test RE 0.1398556026786042\n",
      "136 Train Loss 46938.62 Test MSE 6424.626899257353 Test RE 0.14023615563245145\n",
      "137 Train Loss 46801.914 Test MSE 6413.663445684595 Test RE 0.1401164499219253\n",
      "138 Train Loss 46633.695 Test MSE 6426.064165779192 Test RE 0.1402518410149176\n",
      "139 Train Loss 46592.76 Test MSE 6404.176119906969 Test RE 0.14001277887799235\n",
      "140 Train Loss 46562.508 Test MSE 6402.754374931102 Test RE 0.139997236401448\n",
      "141 Train Loss 46478.113 Test MSE 6451.982932435202 Test RE 0.1405344009558863\n",
      "142 Train Loss 46295.51 Test MSE 6439.7910836612855 Test RE 0.1404015592886337\n",
      "143 Train Loss 46091.18 Test MSE 6462.352983678366 Test RE 0.14064729367723702\n",
      "144 Train Loss 45980.59 Test MSE 6426.022668769428 Test RE 0.14025138816841137\n",
      "145 Train Loss 45937.117 Test MSE 6386.949347164093 Test RE 0.139824339922837\n",
      "146 Train Loss 45913.777 Test MSE 6399.162090823052 Test RE 0.13995795796145186\n",
      "147 Train Loss 45882.79 Test MSE 6419.64281045255 Test RE 0.14018174896616456\n",
      "148 Train Loss 45861.41 Test MSE 6424.755450138659 Test RE 0.1402375586237459\n",
      "149 Train Loss 45826.887 Test MSE 6443.726435203086 Test RE 0.140444452380881\n",
      "150 Train Loss 45782.27 Test MSE 6432.0089405819235 Test RE 0.14031669975646277\n",
      "151 Train Loss 45716.473 Test MSE 6389.666563861681 Test RE 0.13985407967887184\n",
      "152 Train Loss 45655.8 Test MSE 6387.9638975905555 Test RE 0.13983544484969315\n",
      "153 Train Loss 45619.21 Test MSE 6388.432600877493 Test RE 0.13984057482001602\n",
      "154 Train Loss 45615.39 Test MSE 6390.740399251102 Test RE 0.13986583099244904\n",
      "155 Train Loss 45605.98 Test MSE 6382.1642158091045 Test RE 0.1397719515944832\n",
      "156 Train Loss 45598.34 Test MSE 6394.061514715238 Test RE 0.1399021687414503\n",
      "157 Train Loss 45581.63 Test MSE 6397.434390462288 Test RE 0.13993906316441393\n",
      "158 Train Loss 45537.97 Test MSE 6413.345027046304 Test RE 0.140112971703639\n",
      "159 Train Loss 45487.99 Test MSE 6414.076341556021 Test RE 0.14012096002547958\n",
      "160 Train Loss 45468.64 Test MSE 6412.748816786323 Test RE 0.14010645882054107\n",
      "161 Train Loss 45456.668 Test MSE 6416.898929729205 Test RE 0.14015178755555555\n",
      "162 Train Loss 45416.79 Test MSE 6387.607594004822 Test RE 0.13983154497126618\n",
      "163 Train Loss 45294.105 Test MSE 6407.961646749167 Test RE 0.14005415374194924\n",
      "164 Train Loss 45233.934 Test MSE 6409.850363900958 Test RE 0.14007479237932685\n",
      "165 Train Loss 45186.176 Test MSE 6375.150649577191 Test RE 0.13969513052912183\n",
      "166 Train Loss 45150.77 Test MSE 6348.449965512127 Test RE 0.1394022849603185\n",
      "167 Train Loss 45128.15 Test MSE 6348.744272422362 Test RE 0.1394055161884536\n",
      "168 Train Loss 45086.188 Test MSE 6342.058947316329 Test RE 0.13933209878246386\n",
      "169 Train Loss 45006.59 Test MSE 6375.171657555743 Test RE 0.13969536069700722\n",
      "170 Train Loss 44919.766 Test MSE 6374.238147975344 Test RE 0.13968513260129936\n",
      "171 Train Loss 44868.73 Test MSE 6416.256380427947 Test RE 0.1401447704053215\n",
      "172 Train Loss 44856.348 Test MSE 6421.098811593106 Test RE 0.14019764496075254\n",
      "173 Train Loss 44845.83 Test MSE 6447.157490518007 Test RE 0.1404818382446114\n",
      "174 Train Loss 44814.73 Test MSE 6448.657753847893 Test RE 0.14049818246228726\n",
      "175 Train Loss 44784.84 Test MSE 6440.781314333826 Test RE 0.14041235347314318\n",
      "176 Train Loss 44765.742 Test MSE 6404.796959784073 Test RE 0.14001956534109092\n",
      "177 Train Loss 44681.176 Test MSE 6390.303559721589 Test RE 0.13986105064118062\n",
      "178 Train Loss 44653.305 Test MSE 6378.3951220928775 Test RE 0.13973067316844617\n",
      "179 Train Loss 44647.67 Test MSE 6378.086658859547 Test RE 0.13972729439560944\n",
      "180 Train Loss 44598.375 Test MSE 6409.098046524394 Test RE 0.14006657192279273\n",
      "181 Train Loss 44469.863 Test MSE 6373.472641729208 Test RE 0.13967674469284572\n",
      "182 Train Loss 44424.117 Test MSE 6380.399855255347 Test RE 0.13975263015776165\n",
      "183 Train Loss 44392.914 Test MSE 6417.567805657865 Test RE 0.14015909184032882\n",
      "184 Train Loss 44388.156 Test MSE 6402.085290358631 Test RE 0.13998992139037433\n",
      "185 Train Loss 44367.4 Test MSE 6403.577336608328 Test RE 0.14000623320593525\n",
      "186 Train Loss 44322.477 Test MSE 6417.254748809833 Test RE 0.14015567323222375\n",
      "187 Train Loss 44280.457 Test MSE 6418.121061523657 Test RE 0.1401651332387892\n",
      "188 Train Loss 44271.707 Test MSE 6407.760301194754 Test RE 0.140051953393047\n",
      "189 Train Loss 44260.48 Test MSE 6414.857167084041 Test RE 0.1401294886652388\n",
      "190 Train Loss 44218.81 Test MSE 6412.091536899856 Test RE 0.14009927847403236\n",
      "191 Train Loss 44193.43 Test MSE 6428.003230563337 Test RE 0.1402729999145268\n",
      "192 Train Loss 44178.15 Test MSE 6437.315541939688 Test RE 0.140374570577173\n",
      "193 Train Loss 44165.29 Test MSE 6427.932680849089 Test RE 0.1402722301383724\n",
      "194 Train Loss 44161.09 Test MSE 6432.048094372788 Test RE 0.14031712683316672\n",
      "195 Train Loss 44149.797 Test MSE 6433.56004795337 Test RE 0.1403336177325828\n",
      "196 Train Loss 44121.18 Test MSE 6459.9855322797575 Test RE 0.14062152859333077\n",
      "197 Train Loss 44102.25 Test MSE 6444.773238382712 Test RE 0.14045585973425073\n",
      "198 Train Loss 44074.832 Test MSE 6411.521622338895 Test RE 0.14009305223782112\n",
      "199 Train Loss 44045.562 Test MSE 6420.179574264503 Test RE 0.14018760933249486\n",
      "Training time: 649.60\n",
      "Training time: 649.60\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200 #200\n",
    "\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "n_val = 5.0\n",
    "rowdy_terms = 6\n",
    "\n",
    "N_T = 5000 #Total number of data points for 'y'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "\n",
    "    alpha_val = []    \n",
    "    omega_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.05, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-08, \n",
    "                              tolerance_change = 1e-08, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time,\"alpha\": alpha_full,\"omega\": omega_full,  \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1541 into shape (500,500)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2210/3689713413.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflipud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mextent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1541 into shape (500,500)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(np.flipud(np.transpose(u_pred.reshape(500,500)))),vmin = 0,vmax = 1000,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "for tune_reps in range(75):\n",
    "    label = \"MW_rowdy_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_tune[31]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
