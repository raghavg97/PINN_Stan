{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 0.5\n",
    "\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../ES_FEA.mat')\n",
    "\n",
    "xy = np.array(fea_data['xy'])\n",
    "u_true = np.array(fea_data['u'])\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N_t = int(N_T/4)\n",
    "    \n",
    "    x_BC1 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC1 = np.zeros((N_t,1))\n",
    "    u_BC1 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC2 = np.ones((N_t,1))\n",
    "    y_BC2 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC2 = 1000*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC3 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC3 = np.ones((N_t,1)) \n",
    "    u_BC3 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC4 = np.zeros((N_t,1))\n",
    "    y_BC4 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC4 = 1000*np.ones((N_t,1))\n",
    "    \n",
    "    XY_1 = np.hstack((x_BC1,y_BC1))\n",
    "    XY_2 = np.hstack((x_BC2,y_BC2))\n",
    "    XY_3 = np.hstack((x_BC3,y_BC3))\n",
    "    XY_4 = np.hstack((x_BC4,y_BC4))\n",
    "    \n",
    "    xy_BC = np.vstack((XY_1,XY_2,XY_3,XY_4)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_BC1,u_BC2,u_BC3,u_BC4))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2u_dx2 + d2u_dy2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'  \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(xy_BC, u_BC, xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "     \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "    \n",
    "    nan_flag = 0\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "        if(np.isnan(loss_np)):\n",
    "            nan_flag =1\n",
    "            print(\"NAN BREAK!\")\n",
    "            break\n",
    "            \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    \n",
    "    return nan_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MW_stan_tune\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 250000.12 Test MSE 75079.02148585906 Test RE 0.47939703507920906\n",
      "1 Train Loss 249999.3 Test MSE 75080.21649417964 Test RE 0.47940085026722606\n",
      "2 Train Loss 249999.25 Test MSE 75080.36619002427 Test RE 0.4794013281845974\n",
      "3 Train Loss 249999.25 Test MSE 75080.4068477551 Test RE 0.47940145798795686\n",
      "4 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "5 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "6 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "7 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "8 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "9 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "10 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "11 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "12 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "13 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "14 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "15 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "16 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "17 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "18 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "19 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "20 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "21 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "22 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "23 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "24 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "25 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "26 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "27 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "28 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "29 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "30 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "31 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "32 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "33 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "34 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "35 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "36 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "37 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "38 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "39 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "40 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "41 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "42 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "43 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "44 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "45 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "46 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "47 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "48 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "49 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "50 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "51 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "52 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "53 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "54 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "55 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "56 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "57 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "58 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "59 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "60 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "61 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "62 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "63 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "64 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "65 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "66 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "67 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "68 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "69 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "70 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "71 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "72 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "73 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "74 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "75 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "76 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "77 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "78 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "79 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "80 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "81 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "82 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "83 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "84 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "85 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "86 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "87 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "88 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "89 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "90 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "91 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "92 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "93 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "94 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "95 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "97 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "98 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "99 Train Loss 249999.2 Test MSE 75080.49044514255 Test RE 0.47940172487981303\n",
      "Training time: 63.71\n",
      "Training time: 63.71\n",
      "MW_stan_tune\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 250000.66 Test MSE 75080.17687817161 Test RE 0.4794007237894977\n",
      "1 Train Loss 250000.53 Test MSE 75080.61721102207 Test RE 0.47940212959051265\n",
      "2 Train Loss 250000.53 Test MSE 75080.65945927714 Test RE 0.47940226447153356\n",
      "3 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "4 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "5 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "6 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "7 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "8 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "9 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "10 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "11 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "12 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "13 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "14 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "15 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "16 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "17 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "18 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "19 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "20 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "21 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "22 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "23 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "24 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "25 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "26 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "27 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "28 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "29 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "30 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "31 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "32 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "33 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "34 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "35 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "36 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "37 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "38 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "39 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "40 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "41 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "42 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "43 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "44 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "45 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "46 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "47 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "48 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "49 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "50 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "51 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "52 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "53 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "54 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "55 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "56 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "57 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "58 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "59 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "60 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "61 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "62 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "63 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "64 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "65 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "66 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "67 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "68 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "69 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "70 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "71 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "72 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "73 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "74 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "75 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "76 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "77 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "78 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "79 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "80 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "81 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "82 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "83 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "84 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "85 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "86 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "87 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "88 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "89 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "90 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "92 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "93 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "94 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "95 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "96 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "97 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "98 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "99 Train Loss 250000.48 Test MSE 75080.75254841306 Test RE 0.47940256166610445\n",
      "Training time: 55.22\n",
      "Training time: 55.22\n",
      "MW_stan_tune\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 250002.05 Test MSE 75080.39190037994 Test RE 0.4794014102671593\n",
      "1 Train Loss 250001.77 Test MSE 75080.95781203419 Test RE 0.47940321698611116\n",
      "2 Train Loss 250001.7 Test MSE 75081.07060564941 Test RE 0.4794035770880814\n",
      "3 Train Loss 250001.66 Test MSE 75081.25851333598 Test RE 0.479404176996687\n",
      "4 Train Loss 250001.62 Test MSE 75081.35428892633 Test RE 0.47940448276675757\n",
      "5 Train Loss 250001.61 Test MSE 75081.44402395232 Test RE 0.4794047692517395\n",
      "6 Train Loss 250001.61 Test MSE 75081.4845065646 Test RE 0.47940489849509005\n",
      "7 Train Loss 250001.61 Test MSE 75081.52448482932 Test RE 0.4794050261282445\n",
      "8 Train Loss 250001.56 Test MSE 75081.60302846867 Test RE 0.47940527688371254\n",
      "9 Train Loss 250001.56 Test MSE 75081.6417261344 Test RE 0.4794054004283759\n",
      "10 Train Loss 250001.56 Test MSE 75081.68104974301 Test RE 0.4794055259713677\n",
      "11 Train Loss 250001.56 Test MSE 75081.71868945063 Test RE 0.47940564613837333\n",
      "12 Train Loss 250001.56 Test MSE 75081.758063735 Test RE 0.47940577184308614\n",
      "13 Train Loss 250001.53 Test MSE 75081.83371169365 Test RE 0.47940601335353944\n",
      "14 Train Loss 250001.5 Test MSE 75081.91318458629 Test RE 0.47940626707518325\n",
      "15 Train Loss 250001.5 Test MSE 75081.95236161475 Test RE 0.4794063921499832\n",
      "16 Train Loss 250001.5 Test MSE 75081.99252020182 Test RE 0.47940652035842846\n",
      "17 Train Loss 250001.48 Test MSE 75082.08103720355 Test RE 0.47940680295358784\n",
      "18 Train Loss 250001.45 Test MSE 75082.1646958917 Test RE 0.4794070700381738\n",
      "19 Train Loss 250001.45 Test MSE 75082.21030310192 Test RE 0.4794072156414326\n",
      "20 Train Loss 250001.45 Test MSE 75082.2552106657 Test RE 0.47940735901099246\n",
      "21 Train Loss 250001.45 Test MSE 75082.30048054566 Test RE 0.4794075035372208\n",
      "22 Train Loss 250001.4 Test MSE 75082.46426533222 Test RE 0.47940802642751784\n",
      "23 Train Loss 250001.4 Test MSE 75082.51532730332 Test RE 0.4794081894450405\n",
      "24 Train Loss 250001.34 Test MSE 75082.69226125516 Test RE 0.4794087543138063\n",
      "25 Train Loss 250001.25 Test MSE 75083.12786114376 Test RE 0.4794101449811545\n",
      "26 Train Loss 250001.2 Test MSE 75083.27315981535 Test RE 0.479410608851171\n",
      "27 Train Loss 250001.2 Test MSE 75083.34474988095 Test RE 0.47941083740426477\n",
      "28 Train Loss 250000.89 Test MSE 75084.2342469275 Test RE 0.4794136771369807\n",
      "29 Train Loss 250000.56 Test MSE 75084.92984175977 Test RE 0.4794158978222373\n",
      "30 Train Loss 249998.94 Test MSE 75087.57093424254 Test RE 0.4794243294115876\n",
      "31 Train Loss 249991.34 Test MSE 75079.11366948237 Test RE 0.47939732938606894\n",
      "32 Train Loss 249990.94 Test MSE 75077.64286543061 Test RE 0.47939263365277335\n",
      "33 Train Loss 249990.94 Test MSE 75077.57733966217 Test RE 0.4793924244521621\n",
      "34 Train Loss 249987.2 Test MSE 75074.62254088713 Test RE 0.47938299072949647\n",
      "35 Train Loss nan Test MSE nan Test RE nan\n",
      "NAN BREAK!\n",
      "Training time: 28.22\n",
      "MW_stan_tune\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartlab/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio5.py:493: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  narr = np.asanyarray(source)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 249996.48 Test MSE 75079.7683155964 Test RE 0.47939941941416947\n",
      "1 Train Loss 249996.48 Test MSE 75079.82719455186 Test RE 0.47939960739111825\n",
      "2 Train Loss 249996.48 Test MSE 75079.88620800622 Test RE 0.4793997957973943\n",
      "3 Train Loss 249996.48 Test MSE 75079.9473567149 Test RE 0.47939999102060227\n",
      "4 Train Loss 249996.48 Test MSE 75080.00897326454 Test RE 0.47940018773735765\n",
      "5 Train Loss 249996.45 Test MSE 75080.16092451697 Test RE 0.479400672855987\n",
      "6 Train Loss 249996.44 Test MSE 75080.32260863252 Test RE 0.4794011890471559\n",
      "7 Train Loss 249996.33 Test MSE 75080.95009411978 Test RE 0.47940319234608786\n",
      "8 Train Loss 249996.28 Test MSE 75081.14827739181 Test RE 0.4794038250607142\n",
      "9 Train Loss 249116.98 Test MSE 74588.89257685725 Test RE 0.47782967929031206\n",
      "10 Train Loss 235395.89 Test MSE 69379.58528919285 Test RE 0.46084182902816045\n",
      "11 Train Loss 162393.9 Test MSE 48124.861618910014 Test RE 0.38381397939996514\n",
      "12 Train Loss 94980.92 Test MSE 22494.103070789697 Test RE 0.26240395845999837\n",
      "13 Train Loss 73991.42 Test MSE 15197.138490230314 Test RE 0.2156835149056551\n",
      "14 Train Loss 61989.312 Test MSE 12755.101628214097 Test RE 0.19759593389170532\n",
      "15 Train Loss 54998.652 Test MSE 9199.593646799673 Test RE 0.16781097633915296\n",
      "16 Train Loss 51569.734 Test MSE 9051.941962836878 Test RE 0.1664588623804279\n",
      "17 Train Loss 50396.51 Test MSE 8459.203630615302 Test RE 0.16091657545859017\n",
      "18 Train Loss 49436.375 Test MSE 8283.64407327871 Test RE 0.15923801562458909\n",
      "19 Train Loss 48842.19 Test MSE 8422.785333781927 Test RE 0.16056981535661147\n",
      "20 Train Loss 48160.703 Test MSE 8143.753820315561 Test RE 0.15788772252560934\n",
      "21 Train Loss 47716.215 Test MSE 7697.1537988136615 Test RE 0.1534974354922482\n",
      "22 Train Loss 47290.67 Test MSE 7966.047782582122 Test RE 0.1561555755588802\n",
      "23 Train Loss 47058.65 Test MSE 7750.273958371553 Test RE 0.15402618864559178\n",
      "24 Train Loss 46557.883 Test MSE 7679.909116750915 Test RE 0.153325391461699\n",
      "25 Train Loss 46295.457 Test MSE 7364.683914938695 Test RE 0.15014576875265537\n",
      "26 Train Loss 46137.582 Test MSE 7404.052504548536 Test RE 0.15054654282385702\n",
      "27 Train Loss 46016.13 Test MSE 7383.393291199117 Test RE 0.15033636429504396\n",
      "28 Train Loss 45880.945 Test MSE 7335.652973208808 Test RE 0.1498495457799033\n",
      "29 Train Loss 45776.535 Test MSE 7428.397086431583 Test RE 0.15079383881131284\n",
      "30 Train Loss 45664.812 Test MSE 7260.056940218111 Test RE 0.14907542469865376\n",
      "31 Train Loss 45456.51 Test MSE 7211.94105200486 Test RE 0.14858060622587843\n",
      "32 Train Loss 45350.508 Test MSE 7105.382060907088 Test RE 0.14747885577217215\n",
      "33 Train Loss 45233.473 Test MSE 7081.472584042837 Test RE 0.14723051489396302\n",
      "34 Train Loss 45026.39 Test MSE 6997.026899480374 Test RE 0.14635002926233326\n",
      "35 Train Loss 44951.285 Test MSE 7067.590470038495 Test RE 0.14708613295762948\n",
      "36 Train Loss 44858.5 Test MSE 6848.043016200687 Test RE 0.14478357022074304\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27151/2107231491.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mnan_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27151/131485505.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnan_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27151/896708044.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(xy_BC, u_BC, xy_coll, f_hat, seed)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_old\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_stps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0;31m# multiply by initial Hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nan_tune = []\n",
    "for tune_reps in range(20):      \n",
    "    \n",
    "    max_reps = 10\n",
    "    max_iter = 100 #200\n",
    "\n",
    "    label = \"MW_swish_tune\" + str(tune_reps)\n",
    "\n",
    "    train_loss_full = []\n",
    "    test_mse_full = []\n",
    "    test_re_full = []\n",
    "    beta_full = []\n",
    "    elapsed_time= np.zeros((max_reps,1))\n",
    "    time_threshold = np.empty((max_reps,1))\n",
    "    time_threshold[:] = np.nan\n",
    "    epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "    N_T = 5000 #Total number of data points for 'y'\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "    for reps in range(max_reps):\n",
    "        print(label)\n",
    "        print(reps)\n",
    "        train_loss = []\n",
    "        test_mse_loss = []\n",
    "        test_re_loss = []\n",
    "        beta_val = []\n",
    "\n",
    "        torch.manual_seed(reps*36)\n",
    "\n",
    "        layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "        PINN = Sequentialmodel(layers)\n",
    "\n",
    "        PINN.to(device)\n",
    "\n",
    "        'Neural Network Summary'\n",
    "        print(PINN)\n",
    "\n",
    "        params = list(PINN.parameters())\n",
    "\n",
    "        optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lr_tune[tune_reps], \n",
    "                                  max_iter = 20, \n",
    "                                  max_eval = 30, \n",
    "                                  tolerance_grad = 1e-08, \n",
    "                                  tolerance_change = 1e-08, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "        torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "        train_loss_full.append(train_loss)\n",
    "        test_mse_full.append(test_mse_loss)\n",
    "        test_re_full.append(test_re_loss)\n",
    "        #elapsed_time[reps] = time.time() - start_time\n",
    "        beta_full.append(beta_val)\n",
    "        \n",
    "        if(nan_flag == 1):\n",
    "            nan_tune.append(tune_reps)\n",
    "            break\n",
    "\n",
    "\n",
    "        print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "    mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "    savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(np.flipud(np.transpose(u_pred.reshape(500,500)))),vmin = 0,vmax = 1000,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "for tune_reps in range(20):\n",
    "    label = \"MW_stan_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrb_tune[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
