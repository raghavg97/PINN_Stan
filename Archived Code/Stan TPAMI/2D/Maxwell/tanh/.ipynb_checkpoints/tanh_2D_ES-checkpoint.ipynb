{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 25000\n",
    "label = \"ES_tanh\"\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "# xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../ES_FEA.mat')\n",
    "\n",
    "xy = np.array(fea_data['xy'])\n",
    "u_true = np.array(fea_data['u'])\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N_t = int(N_T/4)\n",
    "    \n",
    "    x_BC1 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC1 = np.zeros((N_t,1))\n",
    "    u_BC1 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC2 = np.ones((N_t,1))\n",
    "    y_BC2 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC2 = 1000*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC3 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC3 = np.ones((N_t,1)) \n",
    "    u_BC3 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC4 = np.zeros((N_t,1))\n",
    "    y_BC4 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC4 = 1000*np.ones((N_t,1))\n",
    "    \n",
    "    XY_corners = np.array([[0,0],[1,0],[0,1],[1,1]]).reshape(-1,2)\n",
    "    U_corners = 1000*np.ones((4,1))\n",
    "    \n",
    "    XY_1 = np.hstack((x_BC1,y_BC1))\n",
    "    XY_2 = np.hstack((x_BC2,y_BC2))\n",
    "    XY_3 = np.hstack((x_BC3,y_BC3))\n",
    "    XY_4 = np.hstack((x_BC4,y_BC4))\n",
    "    \n",
    "    xy_BC = np.vstack((XY_1,XY_2,XY_3,XY_4,XY_corners)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_BC1,u_BC2,u_BC3,u_BC4,U_corners))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a =self.activation(z)\n",
    "       \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2u_dx2 + d2u_dy2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "    \n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES_stan\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 248126.16 Test MSE 74382.51381440704 Test RE 0.4771681719110999\n",
      "1 Train Loss 239420.86 Test MSE 70071.56275786147 Test RE 0.4631342970559795\n",
      "2 Train Loss 222422.36 Test MSE 63941.105364545525 Test RE 0.44241119874083185\n",
      "3 Train Loss 197389.17 Test MSE 54548.57806499005 Test RE 0.4086276583591155\n",
      "4 Train Loss 175988.33 Test MSE 46129.22448025694 Test RE 0.3757717423755804\n",
      "5 Train Loss 163107.81 Test MSE 42417.797034226554 Test RE 0.36033802735170534\n",
      "6 Train Loss 151883.69 Test MSE 39783.8186337546 Test RE 0.3489709494383978\n",
      "7 Train Loss 142897.03 Test MSE 36828.679144026435 Test RE 0.3357601215717553\n",
      "8 Train Loss 133209.77 Test MSE 36095.2105378784 Test RE 0.3323998594742205\n",
      "9 Train Loss 125165.57 Test MSE 33350.990768228876 Test RE 0.31951438237051205\n",
      "10 Train Loss 117950.836 Test MSE 32405.248664665596 Test RE 0.31495152898353906\n",
      "11 Train Loss 113219.914 Test MSE 30949.25733671794 Test RE 0.3077947145706672\n",
      "12 Train Loss 107321.3 Test MSE 29730.615942174278 Test RE 0.3016740787955787\n",
      "13 Train Loss 103550.05 Test MSE 27055.72966613772 Test RE 0.2877833523367995\n",
      "14 Train Loss 99182.414 Test MSE 25644.564140644874 Test RE 0.2801777884449464\n",
      "15 Train Loss 94917.45 Test MSE 26038.945055188626 Test RE 0.2823239583371228\n",
      "16 Train Loss 92024.29 Test MSE 26157.009133582724 Test RE 0.2829632818579382\n",
      "17 Train Loss 87697.516 Test MSE 23905.29715507507 Test RE 0.2705098696536005\n",
      "18 Train Loss 84152.53 Test MSE 23032.45167092498 Test RE 0.26552543324003525\n",
      "19 Train Loss 81196.9 Test MSE 21895.03018111018 Test RE 0.25888614937184484\n",
      "20 Train Loss 77994.44 Test MSE 19811.63963958727 Test RE 0.2462613476391267\n",
      "21 Train Loss 75610.85 Test MSE 17980.71623945793 Test RE 0.2346062272039201\n",
      "22 Train Loss 73243.69 Test MSE 17353.472841369094 Test RE 0.23047787550991433\n",
      "23 Train Loss 70625.63 Test MSE 15838.907188528947 Test RE 0.2201905360943655\n",
      "24 Train Loss 68679.664 Test MSE 15113.837940652624 Test RE 0.2150915862604741\n",
      "25 Train Loss 66796.125 Test MSE 14128.85006336413 Test RE 0.207964616732312\n",
      "26 Train Loss 64728.594 Test MSE 13664.273082310432 Test RE 0.20451695041878645\n",
      "27 Train Loss 62048.21 Test MSE 12777.858827993583 Test RE 0.1977721271645967\n",
      "28 Train Loss 60310.62 Test MSE 12259.592418930668 Test RE 0.1937198204297838\n",
      "29 Train Loss 59106.73 Test MSE 12198.088707182755 Test RE 0.19323328432448508\n",
      "30 Train Loss 57449.926 Test MSE 11326.89306246897 Test RE 0.18620504436894172\n",
      "31 Train Loss 56397.758 Test MSE 11161.2914900877 Test RE 0.1848388539175282\n",
      "32 Train Loss 55457.105 Test MSE 10876.793190864382 Test RE 0.18246790147777486\n",
      "33 Train Loss 54500.79 Test MSE 11288.66128805049 Test RE 0.18589052888190336\n",
      "34 Train Loss 53937.895 Test MSE 11082.34725014011 Test RE 0.18418400779682845\n",
      "35 Train Loss 53124.77 Test MSE 10826.136269563542 Test RE 0.18204249808218495\n",
      "36 Train Loss 52555.773 Test MSE 11028.8989436838 Test RE 0.18373932666745163\n",
      "37 Train Loss 51900.254 Test MSE 10615.14451300104 Test RE 0.1802598466733376\n",
      "38 Train Loss 51257.31 Test MSE 10281.512446138146 Test RE 0.1774044643758715\n",
      "39 Train Loss 50777.16 Test MSE 10263.807426158777 Test RE 0.177251651111943\n",
      "40 Train Loss 50298.527 Test MSE 10121.6253655457 Test RE 0.17601965731574676\n",
      "41 Train Loss 49902.35 Test MSE 10063.622876108228 Test RE 0.1755145878910207\n",
      "42 Train Loss 49453.336 Test MSE 10209.951354056646 Test RE 0.1767860035812077\n",
      "43 Train Loss 49058.07 Test MSE 9959.262008795798 Test RE 0.17460216352757058\n",
      "44 Train Loss 48432.504 Test MSE 9729.41065394833 Test RE 0.17257556701106047\n",
      "45 Train Loss 48179.05 Test MSE 9646.370750060869 Test RE 0.17183752808894237\n",
      "46 Train Loss 47976.89 Test MSE 9541.08887495005 Test RE 0.17089722567664306\n",
      "47 Train Loss 47781.86 Test MSE 9407.596501992195 Test RE 0.16969747597915008\n",
      "48 Train Loss 47469.637 Test MSE 9275.190436662686 Test RE 0.16849905108958907\n",
      "49 Train Loss 47247.02 Test MSE 9381.746081705332 Test RE 0.16946416618917154\n",
      "50 Train Loss 46970.95 Test MSE 9155.720389629187 Test RE 0.16741034924001308\n",
      "51 Train Loss 46796.258 Test MSE 9208.137599427291 Test RE 0.16788888392644116\n",
      "52 Train Loss 46538.297 Test MSE 9355.146666226352 Test RE 0.16922376065491224\n",
      "53 Train Loss 46246.133 Test MSE 9254.964680686819 Test RE 0.16831523380577895\n",
      "54 Train Loss 45927.477 Test MSE 9090.381209979712 Test RE 0.1668119233515478\n",
      "55 Train Loss 45666.24 Test MSE 9033.203460179937 Test RE 0.1662864791622069\n",
      "56 Train Loss 45476.652 Test MSE 8810.219564102588 Test RE 0.16422127050985263\n",
      "57 Train Loss 45289.246 Test MSE 8788.070058145031 Test RE 0.16401470874126017\n",
      "58 Train Loss 44995.688 Test MSE 8763.5037597783 Test RE 0.16378530376198855\n",
      "59 Train Loss 44855.25 Test MSE 8865.028040855339 Test RE 0.1647312897325993\n",
      "60 Train Loss 44700.58 Test MSE 8711.668673905213 Test RE 0.16330020003740436\n",
      "61 Train Loss 44471.44 Test MSE 8656.16955587378 Test RE 0.16277920354760733\n",
      "62 Train Loss 44252.445 Test MSE 8614.736815311575 Test RE 0.16238916507044984\n",
      "63 Train Loss 44073.137 Test MSE 8576.81583777942 Test RE 0.16203136264401907\n",
      "64 Train Loss 43880.438 Test MSE 8516.228873341852 Test RE 0.16145805041508057\n",
      "65 Train Loss 43737.855 Test MSE 8418.55773497811 Test RE 0.16052951337018811\n",
      "66 Train Loss 43602.863 Test MSE 8444.461584712062 Test RE 0.16077629783399874\n",
      "67 Train Loss 43394.996 Test MSE 8345.464251849608 Test RE 0.1598311014823112\n",
      "68 Train Loss 43245.75 Test MSE 8285.713377035245 Test RE 0.15925790368623596\n",
      "69 Train Loss 43093.566 Test MSE 8313.742171010204 Test RE 0.15952704395902678\n",
      "70 Train Loss 42930.92 Test MSE 8336.342191262727 Test RE 0.15974372541902626\n",
      "71 Train Loss 42810.715 Test MSE 8268.071357621435 Test RE 0.15908826662130965\n",
      "72 Train Loss 42687.055 Test MSE 8194.35358616371 Test RE 0.15837746660954394\n",
      "73 Train Loss 42485.266 Test MSE 8136.988878710047 Test RE 0.15782213095970896\n",
      "74 Train Loss 42318.93 Test MSE 8106.998953224892 Test RE 0.15753102554206766\n",
      "75 Train Loss 42128.957 Test MSE 8106.194600622553 Test RE 0.1575232104656015\n",
      "76 Train Loss 41985.46 Test MSE 8084.474028677193 Test RE 0.15731202695432267\n",
      "77 Train Loss 41834.895 Test MSE 7947.127753112218 Test RE 0.1559700240447729\n",
      "78 Train Loss 41638.11 Test MSE 7829.782980368494 Test RE 0.15481423969012495\n",
      "79 Train Loss 41450.676 Test MSE 7725.556784095455 Test RE 0.15378038234165656\n",
      "80 Train Loss 41180.055 Test MSE 7618.906272442501 Test RE 0.15271523236566592\n",
      "81 Train Loss 41046.383 Test MSE 7615.929378705816 Test RE 0.15268539465545838\n",
      "82 Train Loss 40893.816 Test MSE 7513.200652288644 Test RE 0.15165213737706562\n",
      "83 Train Loss 40641.375 Test MSE 7429.4965737991515 Test RE 0.150804998001141\n",
      "84 Train Loss 40519.37 Test MSE 7284.3770863490545 Test RE 0.14932490655271094\n",
      "85 Train Loss 40400.355 Test MSE 7268.358037074784 Test RE 0.14916062623477713\n",
      "86 Train Loss 40295.016 Test MSE 7238.0655800906325 Test RE 0.1488494720811528\n",
      "87 Train Loss 40215.19 Test MSE 7242.367492387965 Test RE 0.14889369952356507\n",
      "88 Train Loss 40067.305 Test MSE 7330.245022243847 Test RE 0.1497942999545106\n",
      "89 Train Loss 39933.1 Test MSE 7382.885804520276 Test RE 0.15033119763181435\n",
      "90 Train Loss 39761.76 Test MSE 7255.053416709453 Test RE 0.14902404555943308\n",
      "91 Train Loss 39634.055 Test MSE 7263.897354299463 Test RE 0.14911484833357683\n",
      "92 Train Loss 39511.367 Test MSE 7210.021718787069 Test RE 0.14856083383262395\n",
      "93 Train Loss 39294.832 Test MSE 7105.879170676966 Test RE 0.14748401467124186\n",
      "94 Train Loss 39115.85 Test MSE 7032.483555011404 Test RE 0.1467203669412963\n",
      "95 Train Loss 38963.42 Test MSE 7045.392668988894 Test RE 0.14685496814811225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 38854.008 Test MSE 7081.302149641181 Test RE 0.14722874313707213\n",
      "97 Train Loss 38723.47 Test MSE 7191.091334871767 Test RE 0.14836567755330116\n",
      "98 Train Loss 38604.566 Test MSE 7252.106528409123 Test RE 0.14899377687670654\n",
      "99 Train Loss 38521.56 Test MSE 7213.894352502449 Test RE 0.14860072583871817\n",
      "100 Train Loss 38449.098 Test MSE 7175.486441097793 Test RE 0.14820461103881183\n",
      "101 Train Loss 38372.465 Test MSE 7149.364109921127 Test RE 0.14793459590525632\n",
      "102 Train Loss 38277.727 Test MSE 7144.11490988881 Test RE 0.14788027772651094\n",
      "103 Train Loss 38171.082 Test MSE 7138.209930553907 Test RE 0.14781914975530358\n",
      "104 Train Loss 38032.664 Test MSE 7108.743554652108 Test RE 0.14751373712177202\n",
      "105 Train Loss 37959.082 Test MSE 7057.566068993769 Test RE 0.1469817852603267\n",
      "106 Train Loss 37801.67 Test MSE 6957.342184809546 Test RE 0.1459344157657104\n",
      "107 Train Loss 37691.01 Test MSE 7023.79547904397 Test RE 0.14662970824081517\n",
      "108 Train Loss 37550.625 Test MSE 7043.194561703146 Test RE 0.1468320575615396\n",
      "109 Train Loss 37373.79 Test MSE 7096.664429807628 Test RE 0.14738835670833836\n",
      "110 Train Loss 37247.32 Test MSE 7093.555131115759 Test RE 0.1473560651552584\n",
      "111 Train Loss 37127.445 Test MSE 6951.260644438266 Test RE 0.14587061984705377\n",
      "112 Train Loss 37019.254 Test MSE 6960.150452684347 Test RE 0.1459638653438508\n",
      "113 Train Loss 36868.684 Test MSE 6857.950550656642 Test RE 0.1448882665371576\n",
      "114 Train Loss 36731.93 Test MSE 6854.871763366764 Test RE 0.1448557400391261\n",
      "115 Train Loss 36582.77 Test MSE 6752.189089130528 Test RE 0.1437667117965404\n",
      "116 Train Loss 36439.56 Test MSE 6669.213853667414 Test RE 0.1428806323661664\n",
      "117 Train Loss 36295.805 Test MSE 6632.811396033522 Test RE 0.14249015733186157\n",
      "118 Train Loss 36206.824 Test MSE 6514.315947044157 Test RE 0.14121162507306873\n",
      "119 Train Loss 35974.754 Test MSE 6554.475650670665 Test RE 0.14164622969385482\n",
      "120 Train Loss 35817.316 Test MSE 6549.771836577725 Test RE 0.1415953944212673\n",
      "121 Train Loss 35725.945 Test MSE 6541.750293396587 Test RE 0.14150866151121996\n",
      "122 Train Loss 35562.734 Test MSE 6483.70832192365 Test RE 0.14087949186007262\n",
      "123 Train Loss 35379.59 Test MSE 6504.749318398567 Test RE 0.14110789848629732\n",
      "124 Train Loss 35200.473 Test MSE 6591.02504649477 Test RE 0.14204060807476054\n",
      "125 Train Loss 34864.855 Test MSE 6702.333066123807 Test RE 0.1432349645100094\n",
      "126 Train Loss 34692.31 Test MSE 6726.802215789256 Test RE 0.14349619031559113\n",
      "127 Train Loss 34512.047 Test MSE 6723.200136602605 Test RE 0.14345776538216737\n",
      "128 Train Loss 34322.605 Test MSE 6675.985101168466 Test RE 0.14295314726074135\n",
      "129 Train Loss 34104.86 Test MSE 6684.709217447448 Test RE 0.14304652169825954\n",
      "130 Train Loss 33823.59 Test MSE 6561.132101858261 Test RE 0.14171813643384293\n",
      "131 Train Loss 33616.926 Test MSE 6472.813059132382 Test RE 0.14076107471561922\n",
      "132 Train Loss 33346.926 Test MSE 6503.014883517618 Test RE 0.14108908463460837\n",
      "133 Train Loss 33078.387 Test MSE 6556.236566219791 Test RE 0.14166525564735685\n",
      "134 Train Loss 32870.66 Test MSE 6433.841716503031 Test RE 0.1403366896813678\n",
      "135 Train Loss 32708.24 Test MSE 6384.237929601974 Test RE 0.1397946573313923\n",
      "136 Train Loss 32511.23 Test MSE 6295.572175430875 Test RE 0.13882051316030286\n",
      "137 Train Loss 32311.557 Test MSE 6214.33639274442 Test RE 0.13792196029252118\n",
      "138 Train Loss 32131.402 Test MSE 6090.07740429888 Test RE 0.13653608570761333\n",
      "139 Train Loss 31938.867 Test MSE 6001.977137240386 Test RE 0.1355449089674273\n",
      "140 Train Loss 31749.309 Test MSE 6128.402208636856 Test RE 0.13696502214480233\n",
      "141 Train Loss 31553.541 Test MSE 6127.9729493070345 Test RE 0.13696022525474957\n",
      "142 Train Loss 31368.188 Test MSE 5955.1590089560095 Test RE 0.135015218281383\n",
      "143 Train Loss 31209.496 Test MSE 5907.282578465241 Test RE 0.1344713964373035\n",
      "144 Train Loss 30976.652 Test MSE 5821.052396691553 Test RE 0.1334863309589164\n",
      "145 Train Loss 30698.598 Test MSE 5830.041034268387 Test RE 0.1335893533448936\n",
      "146 Train Loss 30516.506 Test MSE 5718.899300392565 Test RE 0.13230987726591525\n",
      "147 Train Loss 30303.934 Test MSE 5598.591541691135 Test RE 0.13091078698712838\n",
      "148 Train Loss 30115.887 Test MSE 5532.20223827274 Test RE 0.13013228805918278\n",
      "149 Train Loss 30017.889 Test MSE 5526.019085238642 Test RE 0.13005954552654742\n",
      "150 Train Loss 29858.438 Test MSE 5481.013254073363 Test RE 0.12952883754769084\n",
      "151 Train Loss 29675.88 Test MSE 5405.621421162177 Test RE 0.12863491246212877\n",
      "152 Train Loss 29517.246 Test MSE 5289.6791656978585 Test RE 0.12724792456607445\n",
      "153 Train Loss 29298.725 Test MSE 5234.734405420413 Test RE 0.12658532697354788\n",
      "154 Train Loss 29138.605 Test MSE 5122.700188889734 Test RE 0.12522340578159055\n",
      "155 Train Loss 29041.598 Test MSE 5061.711080466512 Test RE 0.12447574033340424\n",
      "156 Train Loss 28880.67 Test MSE 5060.745092518756 Test RE 0.12446386215620288\n",
      "157 Train Loss 28741.07 Test MSE 4963.798562113941 Test RE 0.12326594692590695\n",
      "158 Train Loss 28620.578 Test MSE 4968.850281110839 Test RE 0.12332865561209903\n",
      "159 Train Loss 28472.508 Test MSE 4991.255904877267 Test RE 0.1236064006914418\n",
      "160 Train Loss 28371.184 Test MSE 4995.870229016662 Test RE 0.12366352341249956\n",
      "161 Train Loss 28219.812 Test MSE 4981.594329594371 Test RE 0.12348671027207908\n",
      "162 Train Loss 28021.516 Test MSE 5008.425059552714 Test RE 0.12381881171176802\n",
      "163 Train Loss 27871.484 Test MSE 4968.083655706575 Test RE 0.12331914128562312\n",
      "164 Train Loss 27733.223 Test MSE 5024.468754045657 Test RE 0.12401697009874996\n",
      "165 Train Loss 27605.049 Test MSE 5021.463962843439 Test RE 0.12397988151794904\n",
      "166 Train Loss 27451.941 Test MSE 4985.746941553716 Test RE 0.12353816825276945\n",
      "167 Train Loss 27261.328 Test MSE 4970.304679564749 Test RE 0.12334670363853852\n",
      "168 Train Loss 27124.082 Test MSE 4861.50216473958 Test RE 0.1219891720086464\n",
      "169 Train Loss 26997.959 Test MSE 4851.078913412921 Test RE 0.12185832705201513\n",
      "170 Train Loss 26803.459 Test MSE 4815.231392867907 Test RE 0.12140725019408173\n",
      "171 Train Loss 26548.227 Test MSE 4861.565440907637 Test RE 0.12198996589723995\n",
      "172 Train Loss 26205.809 Test MSE 4900.563442026692 Test RE 0.12247827181693591\n",
      "173 Train Loss 25974.896 Test MSE 5006.766225414025 Test RE 0.12379830507750698\n",
      "174 Train Loss 25845.676 Test MSE 4999.4979364167475 Test RE 0.12370841385685864\n",
      "175 Train Loss 25668.555 Test MSE 5002.16051912798 Test RE 0.12374135116824836\n",
      "176 Train Loss 25527.027 Test MSE 4980.205498100347 Test RE 0.12346949548339645\n",
      "177 Train Loss 25398.52 Test MSE 5041.161789318483 Test RE 0.12422281305551983\n",
      "178 Train Loss 25225.297 Test MSE 5045.678280278224 Test RE 0.12427844761231867\n",
      "179 Train Loss 25071.305 Test MSE 4924.346279213801 Test RE 0.12277511066712728\n",
      "180 Train Loss 24873.05 Test MSE 4969.754035006348 Test RE 0.1233398708509324\n",
      "181 Train Loss 24767.834 Test MSE 4951.97075570255 Test RE 0.12311899945306574\n",
      "182 Train Loss 24546.473 Test MSE 4946.180068107202 Test RE 0.12304699254472494\n",
      "183 Train Loss 24368.469 Test MSE 4858.661658506983 Test RE 0.12195352853654612\n",
      "184 Train Loss 24210.709 Test MSE 4845.337649937605 Test RE 0.12178619589335411\n",
      "185 Train Loss 24082.51 Test MSE 4740.052230446468 Test RE 0.12045576924690236\n",
      "186 Train Loss 23963.283 Test MSE 4766.63418789947 Test RE 0.12079305177355362\n",
      "187 Train Loss 23855.654 Test MSE 4710.9820077951945 Test RE 0.12008583013685004\n",
      "188 Train Loss 23737.645 Test MSE 4682.3201623932255 Test RE 0.1197199687704911\n",
      "189 Train Loss 23629.268 Test MSE 4711.595283663148 Test RE 0.12009364627209942\n",
      "190 Train Loss 23502.098 Test MSE 4668.532757538019 Test RE 0.11954357709824702\n",
      "191 Train Loss 23391.969 Test MSE 4647.174834789972 Test RE 0.11926981556999573\n",
      "192 Train Loss 23279.73 Test MSE 4712.707169647974 Test RE 0.12010781584281899\n",
      "193 Train Loss 23136.629 Test MSE 4611.088049819326 Test RE 0.11880582911079528\n",
      "194 Train Loss 22987.46 Test MSE 4642.657489806042 Test RE 0.11921183261060475\n",
      "195 Train Loss 22839.79 Test MSE 4736.298899714366 Test RE 0.12040806936375391\n",
      "196 Train Loss 22686.09 Test MSE 4770.538404805031 Test RE 0.12084251075516289\n",
      "197 Train Loss 22526.955 Test MSE 4803.682004523808 Test RE 0.12126156443553991\n",
      "198 Train Loss 22365.72 Test MSE 4815.652985529593 Test RE 0.12141256492158706\n",
      "199 Train Loss 22271.7 Test MSE 4814.074388068999 Test RE 0.1213926634381591\n",
      "Training time: 190.05\n",
      "Training time: 190.05\n",
      "ES_stan\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 247227.25 Test MSE 73905.65630611402 Test RE 0.47563617821751075\n",
      "1 Train Loss 237245.06 Test MSE 69946.53929286299 Test RE 0.4627209445973497\n",
      "2 Train Loss 222221.2 Test MSE 63880.97190806032 Test RE 0.44220311684867747\n",
      "3 Train Loss 201602.2 Test MSE 55046.36582673254 Test RE 0.4104879075306705\n",
      "4 Train Loss 177882.6 Test MSE 47360.860906988826 Test RE 0.3807551936136863\n",
      "5 Train Loss 158396.11 Test MSE 41825.457322267364 Test RE 0.35781322718601494\n",
      "6 Train Loss 151115.14 Test MSE 39360.92826632275 Test RE 0.34711126464627334\n",
      "7 Train Loss 146365.47 Test MSE 37653.55682397235 Test RE 0.3394994266743815\n",
      "8 Train Loss 135070.53 Test MSE 36392.6416790676 Test RE 0.3337665676090924\n",
      "9 Train Loss 130452.766 Test MSE 33487.76806262245 Test RE 0.32016889961251355\n",
      "10 Train Loss 123857.21 Test MSE 30841.601179318197 Test RE 0.30725892041942604\n",
      "11 Train Loss 119034.32 Test MSE 29129.368274104243 Test RE 0.2986080936736739\n",
      "12 Train Loss 116564.914 Test MSE 28209.95785568613 Test RE 0.2938578254663534\n",
      "13 Train Loss 113003.305 Test MSE 27225.71548770996 Test RE 0.2886859798646068\n",
      "14 Train Loss 109590.55 Test MSE 26202.698164065674 Test RE 0.2832103031692114\n",
      "15 Train Loss 105299.6 Test MSE 25696.56511088522 Test RE 0.28046171096307465\n",
      "16 Train Loss 102707.59 Test MSE 24852.814163886917 Test RE 0.27581877674848126\n",
      "17 Train Loss 99796.39 Test MSE 23084.382076290432 Test RE 0.2658245998253481\n",
      "18 Train Loss 97109.97 Test MSE 22271.78554774878 Test RE 0.26110402095748025\n",
      "19 Train Loss 94837.336 Test MSE 21822.322011810924 Test RE 0.2584559423657469\n",
      "20 Train Loss 92018.11 Test MSE 20669.563697752747 Test RE 0.2515368955404511\n",
      "21 Train Loss 89065.66 Test MSE 19033.506725275594 Test RE 0.24137675629294758\n",
      "22 Train Loss 85433.65 Test MSE 17628.062568954305 Test RE 0.23229418244598765\n",
      "23 Train Loss 82257.04 Test MSE 16999.03701678912 Test RE 0.2281120364456182\n",
      "24 Train Loss 79502.09 Test MSE 16554.07409190296 Test RE 0.22510673513578378\n",
      "25 Train Loss 77555.53 Test MSE 16451.423083770664 Test RE 0.22440771184140829\n",
      "26 Train Loss 75493.734 Test MSE 15826.544600131601 Test RE 0.2201045877296787\n",
      "27 Train Loss 74296.35 Test MSE 16248.479893904312 Test RE 0.22301928057424422\n",
      "28 Train Loss 73409.76 Test MSE 16054.6993514454 Test RE 0.22168541970745848\n",
      "29 Train Loss 71648.625 Test MSE 15631.226766866892 Test RE 0.2187421989481247\n",
      "30 Train Loss 69830.14 Test MSE 15223.041569805542 Test RE 0.2158672497784039\n",
      "31 Train Loss 68289.37 Test MSE 14660.700859468141 Test RE 0.21184265367339625\n",
      "32 Train Loss 66845.43 Test MSE 14142.998383765316 Test RE 0.20806871627982773\n",
      "33 Train Loss 65924.83 Test MSE 14137.822166410622 Test RE 0.20803063710336828\n",
      "34 Train Loss 64886.387 Test MSE 13485.76926884014 Test RE 0.20317670098130686\n",
      "35 Train Loss 63582.72 Test MSE 13176.460872659009 Test RE 0.2008331639351466\n",
      "36 Train Loss 62171.688 Test MSE 12535.142015852436 Test RE 0.19588477006979976\n",
      "37 Train Loss 61128.703 Test MSE 12074.591518967258 Test RE 0.19225261937815175\n",
      "38 Train Loss 60073.188 Test MSE 12096.53683769132 Test RE 0.1924272476351255\n",
      "39 Train Loss 59313.97 Test MSE 11531.265547900643 Test RE 0.18787739442625195\n",
      "40 Train Loss 58307.22 Test MSE 10688.902808506544 Test RE 0.18088502153639344\n",
      "41 Train Loss 57409.63 Test MSE 10342.014592519126 Test RE 0.17792567207579738\n",
      "42 Train Loss 56802.957 Test MSE 10184.481082212262 Test RE 0.17656535613916388\n",
      "43 Train Loss 56141.066 Test MSE 9888.563188140082 Test RE 0.17398132674592115\n",
      "44 Train Loss 55007.895 Test MSE 9524.826914417388 Test RE 0.17075152379817465\n",
      "45 Train Loss 53830.76 Test MSE 9738.99187841606 Test RE 0.1726605196576766\n",
      "46 Train Loss 53237.055 Test MSE 9188.208772532633 Test RE 0.16770710771254332\n",
      "47 Train Loss 52479.176 Test MSE 8904.914186138672 Test RE 0.16510145901446646\n",
      "48 Train Loss 51738.805 Test MSE 8479.480630582142 Test RE 0.1611093212863124\n",
      "49 Train Loss 51190.63 Test MSE 8549.658703471076 Test RE 0.16177463588748642\n",
      "50 Train Loss 50607.13 Test MSE 8360.018077196286 Test RE 0.15997040713339838\n",
      "51 Train Loss 49905.707 Test MSE 8057.661807792325 Test RE 0.15705094702675187\n",
      "52 Train Loss 49179.684 Test MSE 8025.927925775679 Test RE 0.1567413812339432\n",
      "53 Train Loss 48384.805 Test MSE 7860.854694518353 Test RE 0.1551211179853016\n",
      "54 Train Loss 48091.367 Test MSE 7897.373181563771 Test RE 0.1554810168062154\n",
      "55 Train Loss 47749.324 Test MSE 7795.396795896479 Test RE 0.1544739155277995\n",
      "56 Train Loss 47381.117 Test MSE 7680.301468535831 Test RE 0.1533293079613467\n",
      "57 Train Loss 47127.06 Test MSE 7775.378809948072 Test RE 0.15427544940257204\n",
      "58 Train Loss 46864.582 Test MSE 7550.849376056173 Test RE 0.15203162775376913\n",
      "59 Train Loss 46587.703 Test MSE 7643.477591522376 Test RE 0.15296129092121835\n",
      "60 Train Loss 46371.21 Test MSE 7685.053789661605 Test RE 0.15337673822355188\n",
      "61 Train Loss 46285.723 Test MSE 7693.759461982902 Test RE 0.15346358665241375\n",
      "62 Train Loss 46108.746 Test MSE 7646.58454747735 Test RE 0.1529923759674548\n",
      "63 Train Loss 45895.43 Test MSE 7641.750617038587 Test RE 0.15294400983639111\n",
      "64 Train Loss 45751.98 Test MSE 7600.980364920892 Test RE 0.15253547091586134\n",
      "65 Train Loss 45582.344 Test MSE 7550.4553021367565 Test RE 0.152027660486105\n",
      "66 Train Loss 45380.406 Test MSE 7457.118009342355 Test RE 0.15108506987985557\n",
      "67 Train Loss 45120.69 Test MSE 7510.197713114621 Test RE 0.1516218275478019\n",
      "68 Train Loss 44898.81 Test MSE 7710.150490721659 Test RE 0.15362697152043417\n",
      "69 Train Loss 44689.5 Test MSE 7826.678880941524 Test RE 0.15478354877362924\n",
      "70 Train Loss 44547.727 Test MSE 7644.110173105748 Test RE 0.1529676204023041\n",
      "71 Train Loss 44403.92 Test MSE 7683.183963631931 Test RE 0.1533580782849355\n",
      "72 Train Loss 44293.105 Test MSE 7661.070620252738 Test RE 0.15313722560743442\n",
      "73 Train Loss 44182.82 Test MSE 7505.733440530032 Test RE 0.15157675671125326\n",
      "74 Train Loss 44070.965 Test MSE 7456.612198563854 Test RE 0.1510799457989003\n",
      "75 Train Loss 43984.613 Test MSE 7361.716732664836 Test RE 0.15011551933862974\n",
      "76 Train Loss 43838.273 Test MSE 7474.908022910419 Test RE 0.15126517996485495\n",
      "77 Train Loss 43650.82 Test MSE 7313.685757229388 Test RE 0.14962500916265356\n",
      "78 Train Loss 43277.344 Test MSE 7104.006491557557 Test RE 0.14746457946644614\n",
      "79 Train Loss 43084.65 Test MSE 6917.777074550629 Test RE 0.14551887321291782\n",
      "80 Train Loss 42705.73 Test MSE 6782.298829879063 Test RE 0.1440869015387292\n",
      "81 Train Loss 42592.547 Test MSE 6820.514567512359 Test RE 0.14449226943296414\n",
      "82 Train Loss 42498.63 Test MSE 6808.6170107354865 Test RE 0.14436618984547378\n",
      "83 Train Loss 42370.312 Test MSE 6731.979385173917 Test RE 0.14355139940109593\n",
      "84 Train Loss 42303.547 Test MSE 6735.924888454403 Test RE 0.1435934598057909\n",
      "85 Train Loss 42229.17 Test MSE 6673.449332996532 Test RE 0.1429259954274123\n",
      "86 Train Loss 42187.582 Test MSE 6686.020559007409 Test RE 0.14306055175133772\n",
      "87 Train Loss 42138.04 Test MSE 6650.6907692039695 Test RE 0.14268207596230958\n",
      "88 Train Loss 42067.465 Test MSE 6674.291322903687 Test RE 0.14293501163806707\n",
      "89 Train Loss 41969.098 Test MSE 6601.211768590947 Test RE 0.14215033073165292\n",
      "90 Train Loss 41851.9 Test MSE 6583.564264201157 Test RE 0.14196019310816288\n",
      "91 Train Loss 41748.52 Test MSE 6570.8979017431975 Test RE 0.14182356613162647\n",
      "92 Train Loss 41659.36 Test MSE 6511.359343783133 Test RE 0.14117957611059975\n",
      "93 Train Loss 41576.914 Test MSE 6514.270094818446 Test RE 0.1412111281000364\n",
      "94 Train Loss 41500.78 Test MSE 6488.507438656302 Test RE 0.1409316203666968\n",
      "95 Train Loss 41418.445 Test MSE 6467.714210076249 Test RE 0.14070562271469325\n",
      "96 Train Loss 41368.67 Test MSE 6424.328768354398 Test RE 0.140232901807957\n",
      "97 Train Loss 41305.527 Test MSE 6448.751416667851 Test RE 0.140499202783778\n",
      "98 Train Loss 41150.99 Test MSE 6346.214963099044 Test RE 0.13937774417699839\n",
      "99 Train Loss 41009.53 Test MSE 6284.350557920209 Test RE 0.13869673684157496\n",
      "100 Train Loss 40892.305 Test MSE 6225.066958380624 Test RE 0.13804098686417104\n",
      "101 Train Loss 40761.758 Test MSE 6178.348429393217 Test RE 0.1375220191627836\n",
      "102 Train Loss 40688.46 Test MSE 6112.9717305574195 Test RE 0.13679248387847998\n",
      "103 Train Loss 40562.125 Test MSE 6056.747606375063 Test RE 0.1361619555251007\n",
      "104 Train Loss 40426.17 Test MSE 6095.998449244394 Test RE 0.13660244281956022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 40340.695 Test MSE 6046.97485620623 Test RE 0.13605206040762904\n",
      "106 Train Loss 40172.355 Test MSE 6118.638293467566 Test RE 0.1368558706974113\n",
      "107 Train Loss 40059.13 Test MSE 6041.185130565204 Test RE 0.13598691272964564\n",
      "108 Train Loss 39743.016 Test MSE 5821.918698782527 Test RE 0.1334962634571688\n",
      "109 Train Loss 39597.74 Test MSE 5687.114200141181 Test RE 0.13194168207609575\n",
      "110 Train Loss 39418.35 Test MSE 5693.24975645842 Test RE 0.13201283568474886\n",
      "111 Train Loss 39281.89 Test MSE 5636.381626451793 Test RE 0.13135186306535465\n",
      "112 Train Loss 39123.65 Test MSE 5631.360593769387 Test RE 0.1312933442330361\n",
      "113 Train Loss 38976.754 Test MSE 5554.994106662762 Test RE 0.13040007557876754\n",
      "114 Train Loss 38797.375 Test MSE 5601.441153174747 Test RE 0.13094409870691165\n",
      "115 Train Loss 38480.316 Test MSE 5403.676710968704 Test RE 0.12861177172501023\n",
      "116 Train Loss 38240.746 Test MSE 5374.805421104299 Test RE 0.12826773182127518\n",
      "117 Train Loss 38111.09 Test MSE 5304.377544163121 Test RE 0.12742459319050173\n",
      "118 Train Loss 37796.094 Test MSE 5313.048421966816 Test RE 0.12752869889450982\n",
      "119 Train Loss 37561.953 Test MSE 5174.3596278101295 Test RE 0.12585322437479166\n",
      "120 Train Loss 37386.55 Test MSE 5265.551590770534 Test RE 0.12695738777565363\n",
      "121 Train Loss 37279.562 Test MSE 5244.142106928265 Test RE 0.12669902351550755\n",
      "122 Train Loss 37143.25 Test MSE 5283.174140813532 Test RE 0.12716965842461736\n",
      "123 Train Loss 36842.61 Test MSE 5292.900814598474 Test RE 0.1272866684788244\n",
      "124 Train Loss 36605.645 Test MSE 5268.864053131025 Test RE 0.12699731478455137\n",
      "125 Train Loss 36421.348 Test MSE 5236.7269953554205 Test RE 0.12660941689201685\n",
      "126 Train Loss 36042.492 Test MSE 5249.760380030607 Test RE 0.12676687438129985\n",
      "127 Train Loss 35812.75 Test MSE 5516.093999931288 Test RE 0.12994269538552528\n",
      "128 Train Loss 35318.79 Test MSE 5485.55434890382 Test RE 0.12958248464944497\n",
      "129 Train Loss 34943.3 Test MSE 5679.19259003346 Test RE 0.1318497589370417\n",
      "130 Train Loss 34359.25 Test MSE 5709.440319127549 Test RE 0.13220041261977117\n",
      "131 Train Loss 33963.617 Test MSE 5845.943655744389 Test RE 0.1337714253219183\n",
      "132 Train Loss 33836.31 Test MSE 6045.133312597644 Test RE 0.13603134220667526\n",
      "133 Train Loss 33617.633 Test MSE 5773.972421345402 Test RE 0.1329454243042697\n",
      "134 Train Loss 33352.41 Test MSE 5682.769128076303 Test RE 0.1318912693675448\n",
      "135 Train Loss 33123.227 Test MSE 5640.298692704452 Test RE 0.13139749735907158\n",
      "136 Train Loss 32883.516 Test MSE 5549.564708149233 Test RE 0.130336334101805\n",
      "137 Train Loss 32530.086 Test MSE 5327.494294378112 Test RE 0.1277019527983299\n",
      "138 Train Loss 32355.45 Test MSE 5239.520717034617 Test RE 0.12664318457845586\n",
      "139 Train Loss 32138.875 Test MSE 5107.81876682244 Test RE 0.1250413867661006\n",
      "140 Train Loss 31979.305 Test MSE 4929.76058455771 Test RE 0.12284258757472032\n",
      "141 Train Loss 31753.658 Test MSE 5020.40968469872 Test RE 0.12396686577772849\n",
      "142 Train Loss 31554.568 Test MSE 4980.060015171661 Test RE 0.12346769206032507\n",
      "143 Train Loss 31326.047 Test MSE 4894.198195349905 Test RE 0.12239870364689932\n",
      "144 Train Loss 31134.318 Test MSE 4786.507982460195 Test RE 0.12104460445702692\n",
      "145 Train Loss 30948.908 Test MSE 4809.245142150255 Test RE 0.12133176054472328\n",
      "146 Train Loss 30781.768 Test MSE 4856.416585094063 Test RE 0.12192534935063022\n",
      "147 Train Loss 30535.854 Test MSE 4784.978215172473 Test RE 0.12102525999187823\n",
      "148 Train Loss 30237.756 Test MSE 4832.959295829519 Test RE 0.12163053319166539\n",
      "149 Train Loss 30081.799 Test MSE 4767.523003176198 Test RE 0.1208043131480476\n",
      "150 Train Loss 29765.549 Test MSE 4858.481074158941 Test RE 0.1219512621610884\n",
      "151 Train Loss 29505.658 Test MSE 4850.030593044821 Test RE 0.12184515952054221\n",
      "152 Train Loss 29279.453 Test MSE 5231.191116980847 Test RE 0.12654247816627012\n",
      "153 Train Loss 29094.746 Test MSE 5248.479272741766 Test RE 0.12675140587759612\n",
      "154 Train Loss 28757.555 Test MSE 5256.660459576091 Test RE 0.12685015573359376\n",
      "155 Train Loss 28523.346 Test MSE 5051.426657006653 Test RE 0.12434922065206512\n",
      "156 Train Loss 28309.05 Test MSE 5001.662529333457 Test RE 0.12373519148349064\n",
      "157 Train Loss 28129.09 Test MSE 5116.774902850139 Test RE 0.1251509635974719\n",
      "158 Train Loss 27772.84 Test MSE 5149.053458141568 Test RE 0.1255450928501476\n",
      "159 Train Loss 27540.379 Test MSE 5081.1045669460955 Test RE 0.12471397111096554\n",
      "160 Train Loss 27434.896 Test MSE 5148.765387582358 Test RE 0.12554158090846276\n",
      "161 Train Loss 27067.084 Test MSE 5168.620846732602 Test RE 0.12578341433815107\n",
      "162 Train Loss 26923.96 Test MSE 5103.738429379311 Test RE 0.12499143266525886\n",
      "163 Train Loss 26865.055 Test MSE 5073.633607634781 Test RE 0.12462225131511138\n",
      "164 Train Loss 26766.512 Test MSE 4980.3035153256 Test RE 0.12347071050131177\n",
      "165 Train Loss 26463.102 Test MSE 5018.833416968074 Test RE 0.12394740319172504\n",
      "166 Train Loss 26328.812 Test MSE 4996.203369410535 Test RE 0.12366764648077798\n",
      "167 Train Loss 26145.736 Test MSE 5036.573767269819 Test RE 0.12416627184887476\n",
      "168 Train Loss 26067.572 Test MSE 5041.727274726763 Test RE 0.12422978012196824\n",
      "169 Train Loss 25949.018 Test MSE 5056.681028861158 Test RE 0.12441387636925505\n",
      "170 Train Loss 25642.914 Test MSE 5137.748112525694 Test RE 0.12540719268952344\n",
      "171 Train Loss 25435.219 Test MSE 5163.939775995396 Test RE 0.12572644223156942\n",
      "172 Train Loss 25258.652 Test MSE 5367.977225335066 Test RE 0.1281862297519366\n",
      "173 Train Loss 25095.328 Test MSE 5588.019270336798 Test RE 0.13078712388706548\n",
      "174 Train Loss 24915.05 Test MSE 5562.028934366856 Test RE 0.13048261858427218\n",
      "175 Train Loss 24616.521 Test MSE 5802.627047347068 Test RE 0.13327490169443598\n",
      "176 Train Loss 24483.23 Test MSE 5805.23882551645 Test RE 0.13330489201746573\n",
      "177 Train Loss 24389.613 Test MSE 5791.606784691093 Test RE 0.13314828469682558\n",
      "178 Train Loss 24213.262 Test MSE 5778.2820165663215 Test RE 0.13299502915643593\n",
      "179 Train Loss 23833.768 Test MSE 6139.318690240252 Test RE 0.13708695530529327\n",
      "180 Train Loss 23653.94 Test MSE 6184.070569892453 Test RE 0.13758568813557892\n",
      "181 Train Loss 23375.682 Test MSE 6000.834663032046 Test RE 0.1355320078909195\n",
      "182 Train Loss 23193.254 Test MSE 6199.794262653201 Test RE 0.13776049061125614\n",
      "183 Train Loss 22957.64 Test MSE 6086.644429220448 Test RE 0.13649759760458266\n",
      "184 Train Loss 22860.693 Test MSE 5882.70957860023 Test RE 0.13419141921533276\n",
      "185 Train Loss 22752.639 Test MSE 5836.405175032343 Test RE 0.13366224730514287\n",
      "186 Train Loss 22666.73 Test MSE 5747.934094406046 Test RE 0.13264532000067647\n",
      "187 Train Loss 22440.838 Test MSE 5847.964904103028 Test RE 0.13379454921108352\n",
      "188 Train Loss 22360.303 Test MSE 5783.335133141425 Test RE 0.1330531686279646\n",
      "189 Train Loss 22253.496 Test MSE 5750.959024312636 Test RE 0.13268021862797819\n",
      "190 Train Loss 22211.027 Test MSE 5694.466515288134 Test RE 0.13202694179364258\n",
      "191 Train Loss 22032.082 Test MSE 5731.279096928287 Test RE 0.13245300654634898\n",
      "192 Train Loss 21873.297 Test MSE 5843.16594524917 Test RE 0.13373964068085165\n",
      "193 Train Loss 21691.812 Test MSE 5686.2915390793105 Test RE 0.13193213881689275\n",
      "194 Train Loss 21435.367 Test MSE 5875.4660883252745 Test RE 0.13410877756642417\n",
      "195 Train Loss 21294.31 Test MSE 5703.400038157132 Test RE 0.13213046364897843\n",
      "196 Train Loss 21139.24 Test MSE 5768.88062362656 Test RE 0.1328867921821213\n",
      "197 Train Loss 21081.084 Test MSE 5743.120097943768 Test RE 0.1325897619647947\n",
      "198 Train Loss 21015.666 Test MSE 5816.758504728046 Test RE 0.13343708886450678\n",
      "199 Train Loss 20952.525 Test MSE 5915.087385619409 Test RE 0.1345602001189959\n",
      "Training time: 187.18\n",
      "Training time: 187.18\n",
      "ES_stan\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 244512.22 Test MSE 72882.15128727636 Test RE 0.47233119949362823\n",
      "1 Train Loss 221325.4 Test MSE 63647.5268593802 Test RE 0.44139438924028956\n",
      "2 Train Loss 183421.97 Test MSE 51621.73889717503 Test RE 0.39751393421305403\n",
      "3 Train Loss 155774.12 Test MSE 42292.83156526332 Test RE 0.35980684671735735\n",
      "4 Train Loss 139853.56 Test MSE 38793.67267162123 Test RE 0.34460096585548977\n",
      "5 Train Loss 129766.02 Test MSE 37013.917059224244 Test RE 0.3366034520407117\n",
      "6 Train Loss 120336.79 Test MSE 34826.66999579811 Test RE 0.32650663960867843\n",
      "7 Train Loss 110877.086 Test MSE 31380.10548544978 Test RE 0.30992973249561745\n",
      "8 Train Loss 104167.28 Test MSE 27729.79161784279 Test RE 0.29134619116591526\n",
      "9 Train Loss 98127.24 Test MSE 25876.787043922235 Test RE 0.2814434965908649\n",
      "10 Train Loss 91963.11 Test MSE 24607.555901194908 Test RE 0.27445445329265555\n",
      "11 Train Loss 86931.24 Test MSE 22303.066185215546 Test RE 0.2612873164191224\n",
      "12 Train Loss 82460.33 Test MSE 19408.13520018684 Test RE 0.24374063952340547\n",
      "13 Train Loss 78049.73 Test MSE 19297.570856934526 Test RE 0.24304537658350495\n",
      "14 Train Loss 74876.46 Test MSE 16886.849487898282 Test RE 0.22735806170732953\n",
      "15 Train Loss 72606.61 Test MSE 16731.687681819316 Test RE 0.22631113176602324\n",
      "16 Train Loss 70818.836 Test MSE 16718.556385405056 Test RE 0.22622230803498575\n",
      "17 Train Loss 68863.086 Test MSE 16066.837982805266 Test RE 0.2217692097889924\n",
      "18 Train Loss 66708.67 Test MSE 15787.29049356416 Test RE 0.2198314588438579\n",
      "19 Train Loss 65446.145 Test MSE 15254.785954398298 Test RE 0.2160922049653818\n",
      "20 Train Loss 63419.87 Test MSE 14256.759058170608 Test RE 0.20890385141943532\n",
      "21 Train Loss 62275.49 Test MSE 14516.366756445132 Test RE 0.21079728252619573\n",
      "22 Train Loss 61612.953 Test MSE 14399.224264877224 Test RE 0.20994502596130585\n",
      "23 Train Loss 60726.77 Test MSE 13883.085086551659 Test RE 0.2061479567072437\n",
      "24 Train Loss 60005.258 Test MSE 13586.529139717253 Test RE 0.2039343129623335\n",
      "25 Train Loss 59505.973 Test MSE 14081.569564453375 Test RE 0.20761636083936766\n",
      "26 Train Loss 59139.242 Test MSE 13826.767415360602 Test RE 0.2057294052589015\n",
      "27 Train Loss 58443.14 Test MSE 13102.29774091924 Test RE 0.2002671760773171\n",
      "28 Train Loss 57872.82 Test MSE 12621.418811124684 Test RE 0.19655773131326734\n",
      "29 Train Loss 57309.316 Test MSE 12562.222734368697 Test RE 0.19609624906048717\n",
      "30 Train Loss 56908.46 Test MSE 12229.01964814644 Test RE 0.1934781219921181\n",
      "31 Train Loss 56498.22 Test MSE 11852.941285901434 Test RE 0.19047987982523154\n",
      "32 Train Loss 56148.664 Test MSE 11700.906058313889 Test RE 0.189254314100935\n",
      "33 Train Loss 55521.445 Test MSE 11691.132013415607 Test RE 0.1891752532740746\n",
      "34 Train Loss 54989.61 Test MSE 11574.793047014631 Test RE 0.18823165434349615\n",
      "35 Train Loss 54328.105 Test MSE 11718.971015947374 Test RE 0.18940035204762198\n",
      "36 Train Loss 53868.67 Test MSE 11653.769341633195 Test RE 0.18887272700858176\n",
      "37 Train Loss 53454.66 Test MSE 11257.094728222159 Test RE 0.18563044346405286\n",
      "38 Train Loss 53130.66 Test MSE 11208.75021928222 Test RE 0.18523141206576135\n",
      "39 Train Loss 52930.66 Test MSE 10956.766678920829 Test RE 0.18313748619852052\n",
      "40 Train Loss 52771.914 Test MSE 11010.267866719822 Test RE 0.18358406599718766\n",
      "41 Train Loss 52603.27 Test MSE 11070.563512966255 Test RE 0.18408606134507\n",
      "42 Train Loss 52498.184 Test MSE 10989.106871683554 Test RE 0.18340756302997013\n",
      "43 Train Loss 52331.156 Test MSE 11060.771643443735 Test RE 0.184004631639427\n",
      "44 Train Loss 52079.33 Test MSE 10907.392790625465 Test RE 0.18272438899229426\n",
      "45 Train Loss 51879.74 Test MSE 10936.620018431548 Test RE 0.18296903749071897\n",
      "46 Train Loss 51677.227 Test MSE 10900.115105331679 Test RE 0.18266341967914704\n",
      "47 Train Loss 51531.94 Test MSE 10807.857768879023 Test RE 0.181888755808597\n",
      "48 Train Loss 51389.598 Test MSE 10907.901783676594 Test RE 0.1827286523557329\n",
      "49 Train Loss 51280.82 Test MSE 10711.27906484142 Test RE 0.18107425581099892\n",
      "50 Train Loss 51138.445 Test MSE 10497.52022102691 Test RE 0.17925835296406314\n",
      "51 Train Loss 51009.434 Test MSE 10325.73326248376 Test RE 0.17778556360910303\n",
      "52 Train Loss 50827.85 Test MSE 10300.8770878406 Test RE 0.17757145137539698\n",
      "53 Train Loss 50687.363 Test MSE 9867.78948807016 Test RE 0.17379848238288012\n",
      "54 Train Loss 50579.207 Test MSE 9988.788551747886 Test RE 0.17486079628753676\n",
      "55 Train Loss 50460.7 Test MSE 9910.438942972049 Test RE 0.1741736635992208\n",
      "56 Train Loss 50317.496 Test MSE 9618.612194198231 Test RE 0.1715901087110882\n",
      "57 Train Loss 50131.73 Test MSE 9752.123144488554 Test RE 0.17277688116216025\n",
      "58 Train Loss 49933.59 Test MSE 9610.98849620067 Test RE 0.1715220941964047\n",
      "59 Train Loss 49787.26 Test MSE 9625.38606981851 Test RE 0.17165051895847505\n",
      "60 Train Loss 49609.086 Test MSE 9258.391129232195 Test RE 0.16834638844212793\n",
      "61 Train Loss 49446.72 Test MSE 9109.189533780136 Test RE 0.16698440409779022\n",
      "62 Train Loss 49314.18 Test MSE 9008.497156222715 Test RE 0.16605892216721901\n",
      "63 Train Loss 49226.31 Test MSE 9101.38576714382 Test RE 0.1669128616995037\n",
      "64 Train Loss 49168.336 Test MSE 9065.18216247417 Test RE 0.16658055688769255\n",
      "65 Train Loss 48877.066 Test MSE 9055.690105908428 Test RE 0.16649332167445524\n",
      "66 Train Loss 48697.84 Test MSE 8908.066726912994 Test RE 0.16513068125134006\n",
      "67 Train Loss 48538.062 Test MSE 8886.92215721856 Test RE 0.1649345841782408\n",
      "68 Train Loss 48450.11 Test MSE 8891.817732977079 Test RE 0.16498000702360724\n",
      "69 Train Loss 48232.008 Test MSE 8817.294168630553 Test RE 0.16428719210225337\n",
      "70 Train Loss 48072.85 Test MSE 8673.640930735206 Test RE 0.1629433952759394\n",
      "71 Train Loss 47930.836 Test MSE 8709.597730902035 Test RE 0.1632807889739134\n",
      "72 Train Loss 47716.645 Test MSE 8626.597099728524 Test RE 0.16250091075028858\n",
      "73 Train Loss 47534.74 Test MSE 8566.516213104416 Test RE 0.16193404427002686\n",
      "74 Train Loss 47437.008 Test MSE 8502.621680204511 Test RE 0.16132901036721067\n",
      "75 Train Loss 47284.438 Test MSE 8510.781142322534 Test RE 0.16140640074995907\n",
      "76 Train Loss 47189.766 Test MSE 8468.037773799013 Test RE 0.1610005779948748\n",
      "77 Train Loss 47021.535 Test MSE 8613.724247336599 Test RE 0.16237962125596186\n",
      "78 Train Loss 46795.914 Test MSE 8652.812649940326 Test RE 0.16274763719146756\n",
      "79 Train Loss 46574.816 Test MSE 8727.549405279098 Test RE 0.1634489743952338\n",
      "80 Train Loss 46328.95 Test MSE 8517.328665700252 Test RE 0.1614684754870325\n",
      "81 Train Loss 46140.16 Test MSE 8425.210089550046 Test RE 0.16059292615390355\n",
      "82 Train Loss 45922.758 Test MSE 8141.316461555892 Test RE 0.15786409350695374\n",
      "83 Train Loss 45766.45 Test MSE 8112.862974492569 Test RE 0.15758798856275105\n",
      "84 Train Loss 45530.96 Test MSE 8026.612648646466 Test RE 0.15674806719727083\n",
      "85 Train Loss 45432.46 Test MSE 7968.471190726362 Test RE 0.15617932635269677\n",
      "86 Train Loss 45282.805 Test MSE 7575.073906586404 Test RE 0.1522753053521782\n",
      "87 Train Loss 45070.652 Test MSE 7569.133069274208 Test RE 0.15221558182567574\n",
      "88 Train Loss 44974.37 Test MSE 7515.0352039381805 Test RE 0.151670651237561\n",
      "89 Train Loss 44900.95 Test MSE 7530.236694844928 Test RE 0.15182397422160776\n",
      "90 Train Loss 44774.473 Test MSE 7399.778823843289 Test RE 0.15050308819384714\n",
      "91 Train Loss 44640.902 Test MSE 7253.838032267708 Test RE 0.14901156259878962\n",
      "92 Train Loss 44553.562 Test MSE 7224.1406673480005 Test RE 0.14870622153005983\n",
      "93 Train Loss 44297.113 Test MSE 6888.104703593186 Test RE 0.14520645131271598\n",
      "94 Train Loss 44136.312 Test MSE 6794.9226682396775 Test RE 0.14422093309627676\n",
      "95 Train Loss 43945.27 Test MSE 6951.989730444341 Test RE 0.1458782694982835\n",
      "96 Train Loss 43819.15 Test MSE 6836.83868524152 Test RE 0.1446650789149781\n",
      "97 Train Loss 43736.152 Test MSE 6895.543750092343 Test RE 0.14528484051395393\n",
      "98 Train Loss 43589.105 Test MSE 6722.680738970132 Test RE 0.14345222389438395\n",
      "99 Train Loss 43468.438 Test MSE 6637.478826206035 Test RE 0.14254028282630601\n",
      "100 Train Loss 43387.543 Test MSE 6544.863263335195 Test RE 0.14154232679927742\n",
      "101 Train Loss 43274.863 Test MSE 6475.76149466415 Test RE 0.1407931301526507\n",
      "102 Train Loss 43120.273 Test MSE 6380.219895546778 Test RE 0.13975065927658603\n",
      "103 Train Loss 42965.254 Test MSE 6335.324307752708 Test RE 0.13925810067599617\n",
      "104 Train Loss 42848.508 Test MSE 6308.398935134426 Test RE 0.13896185943261688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 42627.52 Test MSE 6254.717702447945 Test RE 0.1383693492487201\n",
      "106 Train Loss 42539.184 Test MSE 6219.336444891667 Test RE 0.13797743510829705\n",
      "107 Train Loss 42470.832 Test MSE 6154.075452143205 Test RE 0.13725161082323248\n",
      "108 Train Loss 42346.062 Test MSE 6175.562899787509 Test RE 0.13749101453045978\n",
      "109 Train Loss 42221.793 Test MSE 6143.939173287115 Test RE 0.1371385317792626\n",
      "110 Train Loss 42022.004 Test MSE 6101.121548432827 Test RE 0.1366598313587024\n",
      "111 Train Loss 41899.39 Test MSE 6127.331079843907 Test RE 0.1369530521740969\n",
      "112 Train Loss 41773.684 Test MSE 6117.123214999086 Test RE 0.13683892573267378\n",
      "113 Train Loss 41673.23 Test MSE 6142.648603644135 Test RE 0.13712412765543786\n",
      "114 Train Loss 41592.125 Test MSE 6184.691997523777 Test RE 0.1375926008478042\n",
      "115 Train Loss 41510.875 Test MSE 6182.403723056422 Test RE 0.1375671445466869\n",
      "116 Train Loss 41411.37 Test MSE 6200.543606535882 Test RE 0.13776881563443297\n",
      "117 Train Loss 41324.035 Test MSE 6171.252067455421 Test RE 0.13744301856719587\n",
      "118 Train Loss 41253.43 Test MSE 6179.121387500923 Test RE 0.13753062141599315\n",
      "119 Train Loss 41179.582 Test MSE 6196.45980323422 Test RE 0.13772343949898994\n",
      "120 Train Loss 41138.062 Test MSE 6203.489320387229 Test RE 0.1378015369042297\n",
      "121 Train Loss 41088.285 Test MSE 6200.222604582296 Test RE 0.13776524944458313\n",
      "122 Train Loss 41065.668 Test MSE 6177.784367042587 Test RE 0.13751574137148936\n",
      "123 Train Loss 40999.97 Test MSE 6165.61314560815 Test RE 0.13738021060239772\n",
      "124 Train Loss 40964.54 Test MSE 6146.597623855422 Test RE 0.1371681981356878\n",
      "125 Train Loss 40940.742 Test MSE 6134.87414609607 Test RE 0.13703732444489675\n",
      "126 Train Loss 40912.98 Test MSE 6135.248112656658 Test RE 0.13704150110726346\n",
      "127 Train Loss 40884.754 Test MSE 6120.553049728228 Test RE 0.13687728274408228\n",
      "128 Train Loss 40866.67 Test MSE 6119.183903148525 Test RE 0.13686197240009668\n",
      "129 Train Loss 40853.402 Test MSE 6104.3273123435765 Test RE 0.1366957298093823\n",
      "130 Train Loss 40819.266 Test MSE 6079.614173951378 Test RE 0.1364187454372306\n",
      "131 Train Loss 40794.113 Test MSE 6068.779620786989 Test RE 0.13629713449271608\n",
      "132 Train Loss 40767.11 Test MSE 6061.189069251245 Test RE 0.13621187071737761\n",
      "133 Train Loss 40741.188 Test MSE 6061.625871203671 Test RE 0.13621677870972884\n",
      "134 Train Loss 40718.93 Test MSE 6050.157755295005 Test RE 0.1360878620291352\n",
      "135 Train Loss 40699.32 Test MSE 6039.376529137155 Test RE 0.1359665554214562\n",
      "136 Train Loss 40673.246 Test MSE 6029.039318213485 Test RE 0.13585014299852255\n",
      "137 Train Loss 40649.047 Test MSE 6034.923921058319 Test RE 0.13591642463335624\n",
      "138 Train Loss 40621.137 Test MSE 6013.622610836577 Test RE 0.135676342301107\n",
      "139 Train Loss 40597.832 Test MSE 6002.354302886895 Test RE 0.1355491677374072\n",
      "140 Train Loss 40567.703 Test MSE 5989.578605690928 Test RE 0.1354048362389968\n",
      "141 Train Loss 40535.33 Test MSE 5989.520287480531 Test RE 0.13540417704513105\n",
      "142 Train Loss 40510.668 Test MSE 5998.400645387445 Test RE 0.13550451831858615\n",
      "143 Train Loss 40477.336 Test MSE 5970.074981225706 Test RE 0.135184199812782\n",
      "144 Train Loss 40422.22 Test MSE 5981.101254967524 Test RE 0.1353089796845778\n",
      "145 Train Loss 40378.816 Test MSE 5945.349667806637 Test RE 0.13490397388165404\n",
      "146 Train Loss 40343.434 Test MSE 5950.232473194984 Test RE 0.1349593595777308\n",
      "147 Train Loss 40319.953 Test MSE 5925.050452857821 Test RE 0.13467347556076276\n",
      "148 Train Loss 40288.15 Test MSE 5910.163726179241 Test RE 0.13450418517920396\n",
      "149 Train Loss 40252.77 Test MSE 5906.607316828093 Test RE 0.1344637105031041\n",
      "150 Train Loss 40231.266 Test MSE 5902.53300494011 Test RE 0.1344173267186305\n",
      "151 Train Loss 40179.844 Test MSE 5863.972456723999 Test RE 0.1339775410464639\n",
      "152 Train Loss 40129.82 Test MSE 5856.036208832464 Test RE 0.13388684834734135\n",
      "153 Train Loss 40087.96 Test MSE 5837.924542220302 Test RE 0.1336796440426532\n",
      "154 Train Loss 40049.055 Test MSE 5823.693691104856 Test RE 0.13351661214262378\n",
      "155 Train Loss 40002.367 Test MSE 5809.544177070156 Test RE 0.1333543144504143\n",
      "156 Train Loss 39909.94 Test MSE 5767.8831325853225 Test RE 0.13287530302817965\n",
      "157 Train Loss 39795.965 Test MSE 5707.051177291857 Test RE 0.1321727497870325\n",
      "158 Train Loss 39714.367 Test MSE 5712.588029877826 Test RE 0.1322368497570259\n",
      "159 Train Loss 39616.832 Test MSE 5707.918279373693 Test RE 0.13218279025364585\n",
      "160 Train Loss 39530.684 Test MSE 5704.1295718077035 Test RE 0.13213891391869234\n",
      "161 Train Loss 39457.01 Test MSE 5680.799254465767 Test RE 0.13186840800857733\n",
      "162 Train Loss 39368.85 Test MSE 5648.766196372487 Test RE 0.1314960906772943\n",
      "163 Train Loss 39319.26 Test MSE 5614.621057364226 Test RE 0.1310980605734\n",
      "164 Train Loss 39210.023 Test MSE 5603.658717982828 Test RE 0.13097001599171326\n",
      "165 Train Loss 39118.277 Test MSE 5604.197196437005 Test RE 0.13097630856243694\n",
      "166 Train Loss 38974.27 Test MSE 5605.916565339211 Test RE 0.13099639880141617\n",
      "167 Train Loss 38910.387 Test MSE 5613.190125642771 Test RE 0.13108135380677574\n",
      "168 Train Loss 38847.066 Test MSE 5614.267163959759 Test RE 0.13109392890809282\n",
      "169 Train Loss 38801.6 Test MSE 5617.183875283806 Test RE 0.13112797729656298\n",
      "170 Train Loss 38682.85 Test MSE 5596.926466469347 Test RE 0.13089131847299149\n",
      "171 Train Loss 38561.6 Test MSE 5603.196469438338 Test RE 0.13096461399001486\n",
      "172 Train Loss 38358.062 Test MSE 5616.651723261595 Test RE 0.13112176585034793\n",
      "173 Train Loss 38283.164 Test MSE 5650.990634726765 Test RE 0.1315219791763507\n",
      "174 Train Loss 38229.69 Test MSE 5630.113743085845 Test RE 0.13127880846892181\n",
      "175 Train Loss 38165.4 Test MSE 5548.252766085584 Test RE 0.13032092714331012\n",
      "176 Train Loss 38069.293 Test MSE 5532.4412975287505 Test RE 0.1301350996874058\n",
      "177 Train Loss 37750.363 Test MSE 5438.351762510542 Test RE 0.1290237586590627\n",
      "178 Train Loss 37559.2 Test MSE 5428.18557103437 Test RE 0.1289031068637839\n",
      "179 Train Loss 37218.29 Test MSE 5402.556801624363 Test RE 0.128598443671155\n",
      "180 Train Loss 36932.91 Test MSE 5310.477204022169 Test RE 0.12749783678537627\n",
      "181 Train Loss 36656.477 Test MSE 5261.279025050811 Test RE 0.12690586953930336\n",
      "182 Train Loss 36446.934 Test MSE 5298.242881458375 Test RE 0.12735088679985254\n",
      "183 Train Loss 36314.36 Test MSE 5298.467862360554 Test RE 0.12735359064090832\n",
      "184 Train Loss 36091.453 Test MSE 5278.233262579527 Test RE 0.1271101793354917\n",
      "185 Train Loss 35858.027 Test MSE 5241.558765480681 Test RE 0.1266678127711082\n",
      "186 Train Loss 35517.055 Test MSE 5145.107570613929 Test RE 0.125496978981821\n",
      "187 Train Loss 35259.367 Test MSE 5214.782968940266 Test RE 0.1263438658063952\n",
      "188 Train Loss 35057.082 Test MSE 5377.337127891063 Test RE 0.12829793738505116\n",
      "189 Train Loss 34796.09 Test MSE 5259.317651988579 Test RE 0.12688221246322862\n",
      "190 Train Loss 34654.914 Test MSE 5169.038164429537 Test RE 0.12578849215163662\n",
      "191 Train Loss 34468.035 Test MSE 5206.898585402839 Test RE 0.12624831817659782\n",
      "192 Train Loss 34145.016 Test MSE 5248.304890024513 Test RE 0.12674930017830457\n",
      "193 Train Loss 33737.027 Test MSE 5282.384160008574 Test RE 0.1271601503767867\n",
      "194 Train Loss 33182.68 Test MSE 4964.903731334288 Test RE 0.12327966848882416\n",
      "195 Train Loss 32838.168 Test MSE 4925.105112999277 Test RE 0.12278457002556852\n",
      "196 Train Loss 32200.559 Test MSE 4933.435616751101 Test RE 0.12288836731908813\n",
      "197 Train Loss 31938.78 Test MSE 5159.417011693816 Test RE 0.12567137229942432\n",
      "198 Train Loss 31558.719 Test MSE 4860.365296526847 Test RE 0.12197490751629893\n",
      "199 Train Loss 31204.482 Test MSE 4709.60550565926 Test RE 0.12006828491190467\n",
      "Training time: 194.96\n",
      "Training time: 194.96\n",
      "ES_stan\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 254749.31 Test MSE 76171.20110621725 Test RE 0.48287135600444764\n",
      "1 Train Loss 248492.52 Test MSE 74620.0081445395 Test RE 0.4779293348218222\n",
      "2 Train Loss 247589.11 Test MSE 74165.99517408047 Test RE 0.47647317591597527\n",
      "3 Train Loss 245614.61 Test MSE 72914.25746096278 Test RE 0.4724352241246805\n",
      "4 Train Loss 244805.14 Test MSE 72555.27837353203 Test RE 0.4712708179567551\n",
      "5 Train Loss 244318.83 Test MSE 72202.5610232708 Test RE 0.4701239137561013\n",
      "6 Train Loss 243578.86 Test MSE 72183.93557777243 Test RE 0.47006327302552486\n",
      "7 Train Loss 242234.8 Test MSE 71646.02445626666 Test RE 0.4683085537492511\n",
      "8 Train Loss 240592.84 Test MSE 71054.85971542375 Test RE 0.46637250108400674\n",
      "9 Train Loss 238622.7 Test MSE 70063.28619228341 Test RE 0.463106944486505\n",
      "10 Train Loss 238307.89 Test MSE 69948.86256542253 Test RE 0.4627286291658612\n",
      "11 Train Loss 237460.34 Test MSE 69621.18446238784 Test RE 0.4616435219497371\n",
      "12 Train Loss 236884.88 Test MSE 69410.98750317963 Test RE 0.4609461091023459\n",
      "13 Train Loss 236666.8 Test MSE 69240.7763165219 Test RE 0.4603805909809602\n",
      "14 Train Loss 235461.62 Test MSE 68854.6849085465 Test RE 0.45909523949809905\n",
      "15 Train Loss 234707.0 Test MSE 68523.92707441025 Test RE 0.4579912322210239\n",
      "16 Train Loss 233572.72 Test MSE 67805.62754214699 Test RE 0.45558447032690513\n",
      "17 Train Loss 232645.36 Test MSE 67520.31069359278 Test RE 0.45462494116050955\n",
      "18 Train Loss 232405.98 Test MSE 67390.03197538412 Test RE 0.4541861357751815\n",
      "19 Train Loss 232096.03 Test MSE 67437.0679171756 Test RE 0.4543446113363676\n",
      "20 Train Loss 231653.95 Test MSE 67438.14490044404 Test RE 0.45434823930838664\n",
      "21 Train Loss 229551.69 Test MSE 66540.00283742885 Test RE 0.45131259044169486\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "\n",
    "N_T = 5000 #Total number of data points for 'y'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    " \n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    \n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-08, \n",
    "                              tolerance_change = 1e-08, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "    \n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(u_pred.reshape(500,500)),vmin = 0,vmax = 500,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
